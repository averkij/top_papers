{
    "paper_title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action",
    "authors": [
        "Jen-Hao Cheng",
        "Vivian Wang",
        "Huayu Wang",
        "Huapeng Zhou",
        "Yi-Hao Peng",
        "Hou-I Liu",
        "Hsiang-Wei Huang",
        "Kuang-Ming Chen",
        "Cheng-Yen Yang",
        "Wenhao Chai",
        "Yi-Ling Chen",
        "Vibhav Vineet",
        "Qin Cai",
        "Jenq-Neng Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding."
        },
        {
            "title": "Start",
            "content": "arXiv Preprint TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Jen-Hao Cheng 1, Vivian Wang 1, Huayu Wang 1, Huapeng Zhou 1, Yi-Hao Peng 2, Hou-I Liu 3, Hsiang-Wei Huang 1, Kuang-Ming Chen 1, Cheng-Yen Yang 1, Wenhao Chai a1, Yi-Ling Chen 4, Vibhav Vineet 4, Qin Cai, Jenq-Neng Hwang 1 1 University of Washington 2 Carnegie Mellon University 3 National Yang Ming Chiao Tung University 4 Microsoft Link: Project Page VER Data Code & Model Fig. 1: Our model, TEMPURA, is trained using two-stage process for video understanding. The model first infers event structures and causal relationships by filling in missing details and reasoning about event sequences (e.g., recognizing that shrimp must be battered before frying). Second, it is learned to partition video into non-overlapping events and describe them in details. To achieve TEMPURA, we propose new large-scale dataset consisting of 500k videos with dense event captions. Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), 5 2 0 2 2 ] . [ 1 3 8 5 1 0 . 5 0 5 2 : r TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding. 1."
        },
        {
            "title": "Introduction",
            "content": "Recent video Large Multi-modal Models (LMMs) [1, 21, 26, 27] have extended Large Language Models (LLMs) with video understanding capabilities. However, understanding and reasoning over the temporal relationships in long videos remains challenging for current models, particularly when analyzing events over time. Recent methods compress video tokens by consolidating key features from adjacent frames [18, 40, 45], which reduces computational and memory costs but leads to fine-grained temporal information loss. Some other works construct synthetic datasets and develop training pipelines to improve temporal reasoning. For example, LLaVA-Video [55] curates large-scale, high-quality video data for video-language instruction fine-tuning, and TPO [24] uses contrast training pairs with preference learning to steer models toward contextually appropriate responses. However, these approaches still struggle to capture fine-grained event dependencies and achieve long-video temporal understanding. To address these limitations, we introduce TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), two-stage training pipeline that unifies dense event segmentation with masked event prediction to build robust video temporal understanding LMMs. In the first stage, TEMPURA enhances video reasoning by teaching the model to infer missing events and generate step-by-step causal explanations. Drawing inspiration from the Fill-in-the-Middle (FIM) paradigm [2, 39], our training pipeline masks segments of dense video captions and leverages strong LLM to predict pseudo-events and associated reasoning steps. This training objective maximizes the likelihood of reconstructing both the absent event and its causal narrative from the surrounding context, thereby aligning vision-based inference with language-based reasoning. The second stage focuses on video segmentation and dense captioning, where the model learns to partition untrimmed videos into non-overlapping events with precise start and end timestamps, each enriched with detailed descriptions. This stage eliminates the need for auxiliary temporal encoders by directly grounding each event in its corresponding video segment. To support TEMPURAs training pipeline, we introduce VER, large-scale dataset constructed through multi-step event annotation pipeline (see Figure 4). The pipeline begins by filtering dynamic content from YT-1B [50] and categorizing videos into 10 common categories using Llama-3-72B [7] while discarding videos dominated by interviews, lectures, or speeches. We then applied GPT-4o [16] to segment each video by sampling frames at 1 FPS and arranging them into chronological frame sequence image, which facilitates accurate event boundary detection and dense caption generation. temporal coherence check further refines the data by filtering out events lacking causal relevance, and masked event prediction subset reinforces the training signal for temporal inference. The resulting 2 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Table 1: Video Dataset Characteristics Comparison across mainstream benchmarks. Dataset Video Hours Events per Video Events per Minute Coverage Event Details Temporal Reasoning Youcook2 [58] Charades [9] ActivityNet Captions [19] Finevideos [8] ViTT [15] Moment-10M [30] VER (Ours) 175 476 849 3,425 541 7,260 18, 7.7 6.8 3.6 - 7.1 22.5 10.5 1.5 2.3 2.0 - 1.5 3.3 6.0 Sparse Sparse Sparse Dense Sparse Dense Dense VER dataset comprises 500K untrimmed videos spanning total duration of 18K hours, providing dense, timestamp-aligned event captions and structured reasoning that capture fine-grained temporal dynamics across diverse video types. Our experiments demonstrate the effectiveness of TEMPURA in video temporal understanding tasks. On the Charades-STA benchmark [9], TEMPURA achieves mIoU of 39.2, outperforming the baseline by 6.3 points. On the QVHighlights dataset [20], it attains HIT@1 score of 51.7, surpassing the baseline by 6.9 points. Ablation studies reveal that sequentially applying masked event prediction followed by dense video captioning is crucial for unlocking fine-grained temporal reasoning, thereby enhancing the models performance in video understanding. In summary, TEMPURA advances video understanding by integrating dense video captioning with structured causal reasoning to capture fine-grained temporal dynamics in long videos. By decomposing videos into non-overlapping events with precise timestamps and enabling the model to infer missing events through masked prediction, TEMPURA goes beyond holistic processing to achieve robust temporal grounding and causal inference. Our contributions are twofold: We develop TEMPURA, novel training pipeline that leverages masked event prediction to reconstruct missing events with step-by-step causal explanations, and then refines temporal grounding via dense event segmentation and captioning. We curate VER, large-scale dataset of 500K videos spanning 18K hours, annotated with diverse, timestamp-aligned event captions and structured reasoning across 10 common video categories. 2. Related Work 2.1. Video Large Multi-modal Models Researchers have developed video Large Multi-modal Models (LMMs) that address broad range of video understanding tasks and its application [56, 57]. Many models integrate vision foundation models [34, 52] with Large Language Models [3, 7, 33] to enhance video question answering. Several approaches [4, 17, 40, 40, 41, 46, 48] rely on token merging strategies to fuse visual tokens to enable long video question answering. Models such as LLaVA-OneVision [21] and LLaVA-Next-Interleave [22], which extend the LLaVA architecture [26] with simple projector design, demonstrate strong performance across both image and video question answering. The Video-LLaMA series [6, 53] further incorporates an audio modality, supporting more fine-grained multi-modal video comprehension. Recent works [11, 13, 36] reveal, however, that many LMMs still struggle with temporal reasoning. The limited ability to capture the order of events arises from shortage of temporally structured video training data and training methods that overlook time causality. To enhance temporal reasoning and TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action understanding in LMMs, we propose masked temporal event learning in our training pipeline, which strengthens models ability to predict event order in videos. 2.2. LLM with Reasoning Recent advancements in LLMs have significantly improved their reasoning capabilities, enabling them to handle complex multi-step problems across various domains. Latest models like DeepSeek-R1 [10] involve reinforcement learning during the training process and achieve state-of-the-art performance across various LLM evaluation benchmarks with its strong reasoning ability. On the other hand, parallel efforts in LMMs have similarly advanced image-based reasoning, as demonstrated by studies training the multi-modal models to generate step-by-step solutions for math problems [54] or perform chain-of-thought reasoning for object localization or visual reasoning [29, 38, 42]. This emphasis on step-by-step reasoning in static domains naturally aligns with approaches like LLaVA-CoT [47], which leverages four sequential stages during model inference to guide models in systematically breaking down problems and delivering more accurate responses. However, despite these advances, the application of such reasoning capabilities to the video domain, particularly for temporal understanding across dynamic sequences, remains largely unexplored, with few works developing large multi-modal models to address these challenges. 2.3. Temporal Understanding with LMMs Temporal understanding in videos is essential for comprehending event relationships and causal dependencies, enabling models to interpret actions, anticipate future occurrences, and infer missing visual events. Video LMMs have been developed to facilitate temporal grounding through timestampbased event localization and video captioning. Models such as TimeMarker [5], VTimeLLM [13], and Momentor [31] enhance video comprehension through adaptive token compression, segment-level event alignment, and fine-grained moment localization. Additionally, Trace [11], TimeSuite [51], and TimeChat [37] introduce refined temporal modeling techniques, incorporating structured temporal embeddings and improved event localization. However, these works primarily focus on timestamp retrieval and event segmentation, lacking the ability to infer missing events and reason about causal dependencies between actions. In this work, we address these limitations by incorporating masked event prediction and structured temporal reasoning, enhancing the coherence of event transitions and improving long video comprehension, thereby advance the fine-grained temporal reasoning ability of video LMMs. 3. Method Understanding and reasoning about video content require the ability to segment video into meaningful events, establish their temporal order, and infer relationships among them. We define video reasoning as the capability to: 1) Comprehend video progression by identifying distinct events and their temporal boundaries, and 2) Analyze event relationships to infer missing or implicit information based on context and logical flow. To develop video LMMs with robust video reasoning capabilities, we propose structured training framework comprising two key stages: Masked Event Prediction Reasoning and Video Segmentation and Dense Captioning. The first stage enables the model to infer missing events and reason about causality within the video context, while the second stage focuses on enhancing the 4 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. 2: Overview of TEMPURAs two-stage training pipeline. (a) Masked Event Prediction Reasoning: The model infers missing events by analyzing the masked video context, generating both textual description and step-by-step causal explanations. (b) Video Event Segmentation and Temporal Dense Captioning: The model partitions an untrimmed video into non-overlapping events, each aligned with precise start/end timestamps and enriched with detailed captions, thereby reinforcing structured understanding of temporal progressions. video LMMs ability to decompose video into temporally grounded event sequences. Together, these stages equip the video LMM with structured understanding of video narratives, improving its generalization to downstream tasks such as temporal grounding and highlight detection. 3.1. Masked Event Prediction To enhance the video LMMs ability to reason from video input, we introduce Masked Event Prediction, novel training stage that aims to enhance the models understanding of event logical Inspired by Fill-in-theflow, causality, and inductive reasoning with that of language model. Middle (FIM) [2, 39], which is widely used in code and text infilling tasks, we extend this concept to the video domain. FIM typically trains model to predict missing content based on preceding and succeeding contexts. Similarly, we formulate video event infilling task where the video LMM learns to reconstruct masked video events through inferred text description. To enable this capability, we leverage the strong reasoning ability of LLMs to generate pseudoevents and reasoning steps based on our dense video caption data, detailed in Section 4. Specifically, we prompt the LLM to infer and construct plausible intermediate events that are masked within video sequence, ensuring logical consistency with the surrounding context. As shown in Figure 2a, we apply segment-level masking to dense video captions and use the LLM to produce pseudo-events with step-by-step reasoning explanations for the missing segments. These generated pseudo-events and reasoning steps serve as supervised fine-tuning data for the video LMM, enabling it to align its video-based reasoning capability with the strong contextual understanding of LLMs. By training the video LMM on this curated data, we reinforce its ability to infer missing content and establish logical event progression solely from video input. Formally, given masked video input, Vmasked, the training objective is to maximize the likelihood of predicting the pseudo-event, E, along with intermediate reasoning steps, R, in predefined TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action structured format: EVmasked (cid:2)Pθ( E, Vmasked)(cid:3) max θ This stage bridges the gap between vision and language-based reasoning by aligning the strong logical filling ability of the LLM with the video understanding of the video LMM. Making the model more effective on downstream tasks that require complex video comprehension. 3.2. Video Segmentation and Dense Captioning Dense video captioning [13, 14, 51] is crucial task for fine-grained video understanding. The resulting video events, grounded with timestamps, provide the necessary context for language model to establish relationships between events, assisting it in extracting facts and reasoning in response to queries. In the second training stage, Video Event Segmentation and Temporal Dense Captioning, we teach the model to break down video into non-overlapping events and describe each event in detail. As illustrated in Figure 2b, we develop the video LMMs temporal awareness by learning to segment video into non-overlapping events, each defined by its start and end timestamps. We design an instruction, I, to guide the video LMM in transforming video input, V, into structured event sequence, {Ei 1 N}, where each event is represented by its timestamp and caption, = (T, C). Unlike Trace [11], which utilizes extra encoders to model time and saliency scores, we eliminate these components and instead train the model to ground all video segments using their enclosing timestamps. This is achieved by leveraging dense video captions from our VER dataset, which consists of 500K annotated videos. This design choice reduces the need for additional parameters, making the video LMM more versatile for various tasks while ensuring that it learns the structural and temporal progression of videos in this initial training stage. 4. VER Data Pipeline Our TEMPURA training pipeline equips video LMM with three key capabilities: (1) segmenting an untrimmed video into non-overlapping events while ensuring full video coverage, (2) generating detailed descriptions for each segmented event, and (3) building strong understanding of event logical flow, allowing the model to infer missing events in masked video segments based on contextual cues. Existing datasets, as summarized in Table 1, lack large-scale timestamp-aligned dense event captions [8, 9, 15, 30, 58] and dense video coverage, where all events comprehensively describe the entire video [8, 30]. To support TEMPURA training, we construct Video Event Reasoning (VER), large-scale dataset consisting of 500K untrimmed videos spanning total duration of 18K hours. Our dataset provides non-overlapping video events with corresponding detailed descriptions. Compared to existing datasets, VER offers longer video hours, diverse range of video types, and fine-grained event segmentation and captions. Additionally, our TEMPURA masked event prediction training leverages temporal event reasoning data generated from our dense event captions. 4.1. Dataset Construction Figure 4 presents our VER data pipeline. Our video data is filtered from YT-1B [50]. Firstly, we remove static videos following the method in [8] to ensure richer temporal structure. Next, we categorize 6 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. 3: Structured Training Data for Masked Event Prediction and Dense Event Caption videos into 10 of our predefined common video categories using Llama-3-72B [7] to classify based on video captions. To define event boundaries, we apply GPT-4o [16] by sampling the video at 1 FPS and arranging the frames into frame sequence images. Each frame is indexed with marker at the top-left corner, and frame sequence images are ordered chronologically. We then ensure event time boundaries: (1) do not overlap, (2) cover the entire video, and (3) fall within the video length range. Once event boundaries are established, GPT-4o is further utilized to generate detailed event descriptions, compiling them into structured narrative describing the videos progression and event sequences. After filtering and alignment, we retain 500K videos with dense event captions. Each annotated video contains series of events, where each event includes an event ID, description, and start and end timestamps. Figure 3 showcases an example of video-dense event caption in our fine-tuning format. 4.2. Masked Event Prediction To enhance video LMMs temporal reasoning and event inference, we leverage strong LLMs for causal understanding and masked event prediction. Specifically, we randomly mask an event from the dense event caption and employ GPT-4o to analyze the structured captions and predict the missing event within the masked time window. To ensure that masked events are logically inferable, we filter out videos with uncorrelated event captions using GPT-4o. We achieve this by prompting GPT-4o to determine whether causal relationship exists between event captions, applying step-by-step reasoning to arrive at binary decision. During training, we align LMMs reasoning capabilities with LLM event inference by fine-tuning on these structured reasoning processes, as shown in Figure 3. We provide additional dataset statistics, annotation details, and more data examples in the supplementary material. TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. 4: VER Data Pipeline: The pipeline begins by filtering and categorizing large video pool. GPT-4o then generates event captions with start/end times, followed by temporal coherence check that discards invalid events. For valid events, subset is masked to form fill-in-the-blank task, and GPT-4o infers the missing segmentsultimately creating dataset for video temporal understanding. 5. Experiments 5.1."
        },
        {
            "title": "Implementation",
            "content": "We adopted Qwen2.5-VL [1] as our base model and conduct training on our collected data. Additionally, we train our model using DeepSpeed Zero2 [35], with the global batch size is set to 64. To fine-tune the LLM and MLP adapter, we use learning rate of 1 105, while the vision encoder is trained with lower learning rate of 2 106. We observed that the original temporal encoding scheme of Qwen2.5-VL tends to misalign for fine-grained temporal grounding, especially in longer videos (see the supplementary material for more examples). To overcome this issue, we introduced two key modifications. First, we overlay visual timestamps on the upper left corner of each sampled video frame to explicitly mark the temporal context. Second, we adjusted the temporal encoding in M-RoPE by assigning fixed position ID to every sampled frame, ensuring that the model reliably associates each frame with its corresponding timestamp. We conducted training on 8 NVIDIA H100 GPUs for 1 epoch in each training stage. More training details can be found in the supplementary material. 5.2. Video Temporal Understanding Our evaluation focuses on video temporal understanding benchmarks, where the goal is to accurately localize temporal events within videos based on textual queries. In the following, we detail our evaluation of two specific tasks: Video Temporal Grounding and Highlight Detection. 8 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Table 2: Video Temporal Grounding on Charades-STA and Highlight Detection on QVHighlight. The top half reports models fine-tuned with the benchmark training sets while the bottom half shows zero-shot performance. FT denotes fine-tuned models. Method LLM Size Charades-STA QVHighlight mIoU R@1 (IoU=0.3) R@1 (IoU=0.5) R@1 (IoU=0.7) mAP HIT@1 QD-DETR (FT) [28] UnLoc-L (FT) [49] HawkEye (FT) [44] TimeChat (FT) [37] VideoChat-T (FT) [51] MovieChat [40] GroundingGPT [25] VTimeLLM [13] HawkEye [44] TimeChat [37] Trace [11] ChatVTG [32] VideoChat2 [23] Momentor [31] Grounded-VideoLLM [43] Qwen-VL-2.5 [1] TEMPURA (Ours) - - 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 4B 3B 3B - - 49.3 - - - - 31.2 33.7 - - - 34.9 28.5 36.8 - - 72.5 - 79.4 8.8 - 51.0 50.6 - - 52.7 9.6 42.6 54. 57.3 60.8 58.3 46.7 67.1 2.9 29.6 27.5 31.4 32.2 40.3 33.0 3.4 26.6 36.4 32.6 38.4 28.8 23.7 43.0 1.3 11.9 11.4 14.5 13.4 19.4 15.9 1.4 11.6 19.7 38.9 - - 21.7 27.0 11.7 - - - 14.5 26.8 - 13.4 7.6 36. 64.2 - - 37.9 55.3 16.1 - - - 23.9 42.7 - 18.6 - 46.2 33.1 39.2 (+6.3) 52.4 63.8 (+11.4) 34.3 39.3 (+5.0) 12.5 15.0 (+2.5) 42.1 48.3 (+6.2) 44.8 51.7 (+6.9) Video Temporal Grounding. Video temporal grounding aims to localize specific moments in video based on language query. We evaluate our model on Charades-STA [9] using mean Intersection over Union (mIoU) and Recall@1 at different IoU thresholds following previous work [43], assessing both temporal localization accuracy and recall. Our model is capable of providing more granular descriptions of videos, including more detailed content descriptions and greater sensitivity to temporal intervals. In the example shown in Figure 5, given the same prompt, our model not only expresses the same meaning but also provides more detailed grounding and descriptions for each step. We also analyzed performance on Youcook2[58]. Compared to the baseline models average of 15.53 events per video, our model achieves 27.49 events, demonstrating significantly more detailed temporal understanding and description capabilities. Highlight Detection. The goal of highlight detection is to identify relevant time windows within video and predict saliency scores based on given language query. We evaluate our model on QVHighlights [20], reporting mean Average Precision (mAP) and HIT@1 as evaluation metrics. HIT@1 measures whether the highest-ranked retrieved time window aligns with the ground truth. Unlike video temporal grounding, which focuses on localizing single moment, highlight detection aims to retrieve all relevant time windows corresponding to the query. Table 2 shows that TEMPURA improves the baseline model by 6.3 mIoU, and either matches or exceeds the state-of-the-art in video temporal groundingall without any target-task fine-tuning and with smaller model size. In contrast to previous approaches that rely on various forms of instruction tuning data for video temporal grounding [13, 25, 37, 43, 44], our method trains the model to segment video into series of events, infer their relationships, and describe them in detail. As result, TEMPURA not only eliminates the need for extra components such as time prediction models [11], temporal encoding tokens [43], and video-specific vision encoders [37], but also outperforms methods like [32] that are optimized for generating dense captions and extracting time windows from model TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Table 3: TEMPURA Training Stages S1: Masked Event Prediction. S2 Event Segmentation and Temporal Captioning. Training Stages mIoU R@1 (IoU=0.3) R@1 (IoU=0.5) S2 S2 S1 S1 S2 38.4 34.0 39.2 59.1 55.6 63. 32.8 32.6 39.3 Fig. 5: Our model can segment videos into more fine-grained events, capturing subtle transitions and shortduration activities. In contrast, the baseline model (QwenVL2.5) tends to generate coarser segments. This difference suggests that our approach is more adept at recognizing and differentiating fine-grained patterns within video sequences, leading to detailed and structured event representation. outputs. TEMPURA also enhances the performance in highlight detection by 6.9 HIT@1 over the baseline model and surpasses other methods. The superior performance of our model in two tasks demonstrates that the models learned fine-grained temporal understanding ability trained with our TEMPURA pipeline and the data curated in VER can be easily adapted in downstream video temporal understanding tasks without fine-tuning on the benchmark datasets. 5.3. Ablation Study To study the effectiveness of each component in TEMPURA, we split our ablation analysis into three parts and report mIoU and R@1 (IoU=0.5) on Charades-STA. TEMPURA Training Stage. TEMPURA uses masked event prediction as the first training stage, and video dense captioning as the second training stage. As shown in Table 3, we found that using mask event prediction as the pre-training stage before dense captioning will enhance the models temporal understanding of videos. On the contrary, training the model first on dense captioning and continuing to fine-tune the model on the masked event prediction tasks would not improve the model to follow temporal grounding instructions since the model was not explicitly trained to segment video into fine-grained events. Nonetheless, we can still observe the model starts to extract facts around the masked video time windows and generate longer reasoning steps to predict plausible infill as shown in the supplementary material. We compare our models generated masked event prediction and reasoning steps with the baseline model in the supplementary material. 10 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Table 4: Temporal Encoding Scheme. We found adding visual timestamp on sampled video frames provide the most accurate and robust way to encode time. V.T. means adding visual timestamp to the images. T.M. means using temporal MRoPE for the encoding. T.I. means appending time instruction in the prompt. mIoU R@1 (IoU=0.3) R@1 (IoU=0.5) V.T. T.M. T.I. 25.6 38.4 26.7 (+1.1) 38.9 (+0.5) 33.1 39.0 (+5.9) 59.1 64.2 (+5.1) 16.7 22.1 (+4.4) 32.8 38.5 (+5.7) Table 5: Dynamic Scene and Relevant Segment Filtering. D.S.: Dynamic Scene Filtering, T.R.: Relevant Segment Filtering Data Filtering mIoU R@1 (IoU=0.3) R@1 (IoU=0.5) No D.S. D.S. No T.R. T.R. 34.8 38.7 (+3.9) 33.0 37.5 (+4.5) 51.3 63.7 (+12.4) 47.3 57.1 (+9.8) 27.4 38.3 (+10.9) 28.3 34.8 (+6.5) Temporal Encoding Scheme. In Table 4, we compared three different temporal encoding schemes during model training: using absolute temporal encoding in M-RoPE, appending time instruction ahead of user queries, and adding visual timestamps. We found that adding visual timestamps provides the best-grounded captions with the timestamps. Since our model was pre-trained with large amount of OCR data, and the LLM is good at understanding structured information, overlaying the visual timestamp on each sampled video frame will naturally allow the model to understand videos progression. In addition, we show that the baseline model Qwen2.5-VL tends to misalign the description with the correct timestamps. On the contrary, our models temporal grounding and captioning are robust when the video gets longer. Dynamic Scene and Relevant Event Filtering. Our TEMPURA model is learned to partition the videos into non-overlapping segments and describe the segments focusing on the video progression. We found that fine-tuning the base model without filtering out static scenes would weaken the models grounding and captioning ability. Static scenes contain redundant video frames and fewer semantics, and training on such video-text pairs will make the model leans to generate shorter descriptions. During the masked event prediction stage, it is crucial to filter out videos with non-relevant events. Since we train the model to predict possible events in masked time windows, the model learns to build casual bidirectional thinking around past and future video content. Training with masked event data generated from videos with non-relevant events weakens the models temporal understanding as shown in Table 5. 6. Conclusions In this work, we present TEMPURA, two-stage training framework to enhance video LMMs temporal understanding by intergrating coarse visual extraction with deep causal reasoning. Furthermore, we proposed VER, large-scale video event reasoning dataset that aims to enhance the temporal 11 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action understanding ability of video LMMs. After trained on VER, our model substantially outperforms strong baseline model Qwen2.5-VL on multiple temporal understanding benchmarks for temporal grounding and highlight detection tasks. Meanwhile, our ablation studies reveal that the integration of masked event prediction and follow-up fine-grained segmentation further improve video LMMs performance on video temporal understanding."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge Ms. Grace Chao for sponsoring the GPU training hours that supported this work. 12 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action"
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. [2] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle, 2022. URL https://arxiv.org/abs/2207.14255. [3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, et al. Internlm2 technical report, 2024. [4] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, JenqNeng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. [5] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile videollm for long and short video understanding with superior temporal localization ability, 2024. URL https://arxiv.org/abs/2411.18211. [6] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [8] Miquel Farre, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https://huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. [9] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 52775285, 2017. doi: 10.1109/ICCV.2017.563. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, and Xi Chen. Trace: Temporal grounding video llm via causal event modeling, 2024. URL https://arxiv.org/abs/2410. 05643. [12] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989, 2024. URL https://arxiv.org/abs/2410.10989. 13 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action [13] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. [14] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, 2024. [15] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. In AACL-IJCNLP 2020, 2020. [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [17] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. [18] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. [19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning In Proceedings of the IEEE international conference on computer vision, pages events in videos. 706715, 2017. [20] Jie Lei, Tamara L. Berg, and Mohit Bansal. Qvhighlights: detecting moments and highlights in videos via natural language queries. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [22] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. [24] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for long-form video understanding. arXiv preprint arXiv:2501.13919, 2025. 14 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action [25] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al. Groundinggpt: Language enhanced multi-modal grounding model. arXiv preprint arXiv:2401.06071, 2024. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [27] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. [28] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2302323033, 2023. [29] Yi-Hao Peng, Faria Huq, Yue Jiang, Jason Wu, Xin Yue Li, Jeffrey Bigham, and Amy Pavel. Dreamstruct: Understanding slides and user interfaces via synthetic data generation. In European Conference on Computer Vision, pages 466485. Springer, 2024. [30] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning, 2024. [31] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: advancing video large language model with fine-grained temporal reasoning. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [32] Mengxue Qu, Xiaodong Chen, Wu Liu, Alicia Li, and Yao Zhao. Chatvtg: Video temporal grounding via chat with video dialogue large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18471856, 2024. [33] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2.5 technical report, 2025. URL https://arxiv.org/ abs/2412.15115. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 20. IEEE Press, 2020. ISBN 9781728199986. [36] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 15 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action [37] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding, 2024. URL https://arxiv.org/abs/2312. 02051. [38] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning, 2024. URL https://arxiv.org/abs/ 2403.16999. [39] Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, and Yejin Choi. Film: Fill-in language models for any-order generation, 2023. URL https://arxiv.org/abs/2310.09930. [40] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [41] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. arXiv preprint arXiv:2404.17176, 2024. [42] Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. Sketchagent: Language-driven sequential sketch generation. arXiv preprint arXiv:2411.17673, 2024. [43] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models, 2024. URL https://arxiv.org/abs/2410.03290. [44] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024. [45] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. arXiv preprint arXiv:2405.19209, 2024. [46] Wenhao Wu. Freeva: Offline mllm as training-free video assistant. arXiv preprint arXiv:2405.07798, 2024. [47] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. URL https://arxiv.org/abs/2411.10440. [48] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024. [49] Shen Yan, Xuehan Xiong, Arsha Nagrani, Anurag Arnab, Zhonghao Wang, Weina Ge, David Ross, and Cordelia Schmid. Unloc: unified framework for video localization tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1362313633, 2023. 16 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action [50] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1637516387, 2022. [51] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, and Limin Wang. Timesuite: Improving mllms for long video understanding via grounded tuning, 2024. URL https://arxiv. org/abs/2410.19702. [52] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [53] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [54] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Chunyuan Li, and Hongsheng Li. Mavis: Mathematical visual instruction tuning with an automatic data engine, 2024. URL https://arxiv.org/abs/2407.08739. [55] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713. [56] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, and Gaoang Wang. See and think: Embodied agent in virtual environment. In European Conference on Computer Vision, pages 187204. Springer, 2024. [57] Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting Zhang, and Gaoang Wang. Hierarchical auto-organizing system for open-ended multi-agent navigation. arXiv preprint arXiv:2403.08282, 2024. [58] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, pages 75907598, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344. 17 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary material is structured as follows: VER data creation pipeline and statistics in Section S1. Implementation details in Section S2. Qualitative analysis in Section S3. S1. VER Data Creation Pipeline and Statistics After data filtering, we uniformly sampled video frames and arranged them into frame sequence images. The example input to GPT4-o is shown on the right of Figure S1. We added the timestamp in each image, and combined multiple frame sequence images to obtain grid-formatted composite image as the input. These inputs were first temporally segmented by GPT4-o, and we consider these segments as events. Based on the generated time segments, GPT4-o then generated the descriptions of these events separately. Next, we used these temporally aligned event descriptions to construct our masked event prediction data. To create this dataset, we first need to filter out data with weak event correlations. The bottom half of Figure S1 shows an example of the data pipeline described in Figure 4. By inputting the event information into the model in text form, we have it determine whether each correlation is logically valid. After filtering, we obtained 200K reasoning data from 500K dense video captioning data. Our dataset contains videos across 10 domains like travel, DIY, tech reviews, etc. (Figure S2). The average duration and the length of the caption also varied between different domains (Figure S3). Figure S4 presents the percentage of videos with temporal relevance in each category. S2."
        },
        {
            "title": "Implementation Details",
            "content": "We fully fine-tuned the 3B model from Qwen2.5-VL checkpoint on two tasks, masked event prediction and event segmentation and temporal captioning, in two sequential stages: Stage 1: We trained the model using masked event prediction with supervised fine-tuning (SFT) on our VER dataset. Stage 2: We fine-tuned the checkpoints from Stage 1 on the event segmentation and temporal captioning task, utilizing our VER dataset to enhance temporal event understanding. Our training configuration includes: Global batch size: 64, with 2 samples per device across 8 devices, resulting in gradient accumulation steps of 4 Learning rates: 1 105 for the LLM and MLP adapter; 2 106 for the vision encoder Weight decay: 0.1 to prevent overfitting Warm-up ratio: 0.03 in the cosine learning rate schedule Gradient checkpoint: Enabled to reduce memory consumption Liger kernel integration [12]: Significantly reduced memory overhead during full fine-tuning, making it feasible to process long video input frames efficiently. During training, we adopted uniform sampling rate at 1 frame per second (FPS) and fixed every sampled frame to 320 180 pixels. TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action S3. Qualitative Analysis Figure S5 and S6 compare the performance of Qwen2.5-VL-3B (our baseline), Grounded-Video-LLMPhi, VideoQA, and TEMPURA (our proposed model) on long video temporal grounding tasks. The red text highlights errors in timestamp predictions when other models segment videos into finegrained events and identify their start and end times. While other models often struggle, especially toward the end of long videos, TEMPURA consistently segments events accurately and assigns precise timestamps. For instance, the green text shows that TEMPURA correctly identifies person filling and wrapping spring rolls from 161.00 to 183.00 seconds, followed by placing them into oil for frying from 185.00 to 205.00 seconds. Additionally, TEMPURA has better performance in producing more fine-grained event captions, as shown by the larger number of event captions produced by our model. Figure S7 compares the performance of Qwen2.5-VL-3B (our baseline), Grounded-Video-LLMPhi, VideoQA, and TEMPURA (our proposed model) on fine-grained video segmentation. The red highlights indicate errors in timestamp predictions and the failure of other models to produce detailed event captions, even in shorter videos. In contrast, TEMPURA demonstrates better performance, as indicated by the green text, by producing more accurate timestamps, fine-grained events, and descriptive event captions. 19 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. S1: Our mask event prediction data example and generation process. 20 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. S2: Video Class Distribution Fig. S3: Event Caption Distribution Fig. S4: Video Frame Temporal Relevance Distribution TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. S5: Comparison of long video temporal grounding responses on cooking tutorial video, generated by baseline models: Qwen2.5-VL-3B and Grounded Video LLM-Meta-Llama-3-8B-Instruct. Each of the baseline models are prompted to segment videos and describe each of the video segments in detail with correct start and end times. The text highlighted in red indicates incorrect determination of start and end times for frame descriptions. 22 TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. S6: Comparison of long video temporal grounding responses on cooking tutorial video, generated by TEMPURA (our model) and baseline models: Grounded-Video-LLM-Phi and VideoQA. Each of the models are prompted to segment videos and describe each of the video segments in detail with correct start and end times. The text highlighted in red indicates incorrect determination of start/end times and video segment descriptions. The text in green indicates correct determination of start/end times and and video segment descriptions. TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action Fig. S7: Comparing TEMPURA (our model) and other baseline model (Qwen2-5-VL-3B, Grounded-Video-LLMMeta-Llama-3-8B-Instruct, Grounded-Video-LLM-Phi, and VideoQA) abilities to generate detailed descriptions on fine-grained events on short videos. Each model is prompted to segment the video into fine-grained events and describe the events in detail with correct start/end timestamps. Red text indicates incorrect responses with incorrect start/end timestamps and/or poor descriptions of the event segment."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Microsoft",
        "National Yang Ming Chiao Tung University",
        "University of Washington"
    ]
}