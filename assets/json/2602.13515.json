{
    "paper_title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning",
    "authors": [
        "Jintao Zhang",
        "Kai Jiang",
        "Chendong Xiang",
        "Weiqi Feng",
        "Yuezhou Hu",
        "Haocheng Xi",
        "Jianfei Chen",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods."
        },
        {
            "title": "Start",
            "content": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning Jintao Zhang * 1 Kai Jiang * 1 Chendong Xiang * 1 Weiqi Feng 1 Yuezhou Hu 2 Haocheng Xi 2 Jianfei Chen 1 Jun Zhu"
        },
        {
            "title": "Abstract",
            "content": "Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and 16.2 attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods. 6 2 0 2 3 1 ] . [ 1 5 1 5 3 1 . 2 0 6 2 : r 1. Introduction Motivation and core problem. Attention efficiency in video diffusion models (Blattmann et al., 2023; Yang et al., 2024; Zheng et al., 2024; Kong et al., 2024; Wan et al., 2025) is critical because of their long sequence length and O(N 2) time complexity of the attention operator. Sparse attention has been shown to work well in diffusion models. For example, SpargeAttention (Zhang et al., 2025f), SVG (Xi et al., *Equal contribution 1Tsinghua University 2UC Berkeley. Preprint. 1 2025), and other training-free sparse attention methods (Li et al., 2025; Chen et al., 2025a) can save certain portion of attention computation for video generation. More recently, studies show that trainable sparse attention (Zhang et al., 2025c;i; Wu et al., 2025; Zhan et al., 2025) can achieve even higher sparsity after pre-training or fine-tuning. The core points of sparse attention methods are (i) designing reasonable sparse masker, i.e., selecting which tokens in the query, key, and value participate in the computation. For trainable sparse attention, it further requires (ii) an efficient, trainable sparse-attention kernel implementation, and (iii) suitable training objective that enables the trained sparse attention to maintain high generation quality under high sparsity. In this work, we mainly focus on these three aspects. Limitation. Current trainable sparse attention methods have two main limitations. (L1) Under very high attention sparsity (e.g., > 90%), we observe that both Top-k and Top-p maskers can fail to preserve the most important attention computation. This is closely related to the distribution of each row of the attention weights matrix (P ), which is often either (i) relatively uniform or (ii) highly skewed. With Top-k masker, if the row is close to uniform, the probability is spread over many tokens. Then, keeping fixed tokens captures only small fraction of the total probability, which may miss useful context. With Top-p masker, highly skewed row may satisfy the cumulative-probability threshold with only few tokens; these tokens can be dominated by attention sinks (Xiao et al., 2024; Gu et al., 2024), causing other informative tokens to be dropped. (L2) Most existing sparse attention methods fine-tune video diffusion models using promptvideo pairs collected from real-world sources and optimize the standard diffusion loss. However, in practice, this setting is problematic for widely used opensource video diffusion models, whose pre-training datasets are typically not publicly available (e.g., Wan2.1 (Wan et al., 2025)). As result, it is difficult for the community to collect fine-tuning data that matches the distribution of the original pre-training data. In this setting, even fine-tuning with full attention can noticeably degrade performance relative to the original model. This is because the diffusion loss is data-driven and forces the model to fit the fine-tuning dataset, which is typically lower quality than the original SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning Figure 1. Qualitative examples of text-to-video generation. We compare the original full-attention model with SpargeAttention2 under high attention sparsity. SpargeAttention2 preserves visual quality, temporal coherence, and textvideo alignment comparable to full attention, while substantially reducing attention computation. The prompts used for generation is in Appendix training data. Our approach. We propose SpargeAttention2, an accurate and efficient trainable sparse attention method for diffusion models. To address (L1), we analyze how Top-k and Top-p masking affect the information preserved by sparse attention, especially at very high sparsity. Based on this analysis, we propose simple and effective unified masker that combines Top-k and Top-p, and works well for both uniform and skewed attention weight distributions. To address (L2), inspired by distillation, we introduce velocity-level distillation loss that aligns the model using sparse attention with frozen full-attention model during fine-tuning. Specifically, the velocity distillation loss uses the output of the full-attention model as the supervision signal, which helps maintain the original generation quality even when the fine-tuning data distribution differs from the pre-training distribution. This design matches the goal of trainable sparse attention, which aims to preserve generation quality while pushing sparsity as high as possible. In contrast, conventional fine-tuning typically aims to enhance or specialize model capabilities. Result. SpargeAttention2 achieves 95% attention sparsity, 16.2 attention runtime speedup, and up to 4.7 end-toend video generation speedup while maintaining the endto-end generation quality comparable to full attention, as shown in Figure 1. Contribution. Our contributions are summarized as: (1) We study three key questions in sparse attention for diffusion models: when Top-k and Top-p masking fail, why trainable methods can reach higher sparsity, and why finetuning with diffusion loss can be suboptimal. This analysis yields several important insights. (2) We propose an efficient trainable sparse-attention, SpargeAttention2. It contains (1) hybrid Top-k and Top-p masker for accurate sparse masking and (2) distillationstyle fine-tuning for trainable sparse attention for enhancing end-to-end generation quality. (3) SpargeAttention2 achieves 95% attention sparsity, 16.2 attention speedup, and 4.7 end-to-end generation speedup without degrading video generation quality, outperforming prior methods. 2. Preliminaries 2.1. Block Sparse Attention Let Q, K, RN be the query, key, and value matrices, where is the number of tokens and is the head dimension. Standard attention forms the score matrix and applies row-wise softmax to obtain attention weights = Softmax(S) RN , and produces the attention output O. = QK / RN , = RN d. The two matrix multiplications cost O(N 2d), which is expensive for large . Sparse attention reduces this cost by masking out lowimportance attention weights. It introduces binary mask {0, 1}N and keep only the selected weights via , where denotes element-wise multiplication. typical choice is thresholding: Mij = 1 if Pij > τ and Mij = 0 otherwise. When Mij = 0, we can skip computing the corresponding score and contribution, i.e., the dot and the value update PijVj, where Qi Rd product QiK is the i-th row of and Kj, Vj Rd are the j-th rows of and . In practice, however, fine-grained (element-wise) sparsity maps poorly to modern GPUs. Efficient kernels such as FlashAttention (Dao, 2023) therefore exploit block 2 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning structure. Concretely, we partition tensors into tiles: = {Qi}, = {Kj}, = {Vj}, = {Sij}, = {Pij}, = {Mij}. where Qi Rbqd, Kj, Vj Rbkvd, Sij, Pij, Mij Rbqbkv . Block-sparse attention restricts the mask to be constant within each tile: every Mij is either an all-one block (keep) or an all-zero block (drop). Ermon, 2019; Song et al., 2020), following the pre-training setup of Wan video models (Wan et al., 2025). Flow matching provides continuous-time perspective for diffusion modeling, where the generative process is defined by velocity field rather than discrete denoising steps. Given clean image or video latent x1, noise sample x0 (0, I), and time step [0, 1] sampled from predefined schedule, an intermediate latent xt is constructed as linear interpolation between x0 and x1: Mij[:, :] = 0 skip QiK and PijVj. xt = tx1 + (1 t)x0. This block-wise gating aligns sparsity with GPU-friendly tiling, enabling practical speedups. 2.2. Masking for Sparse Attention in Diffusion Models Diffusion models do not use autoregressive decoding, so sparse attention is usually implemented in block-sparse form. The masking problem is therefore to decide, for each block pair (i, j), Mij[:, :] {0, 1}. In practice, forming the full attention weights RN is prohibitively expensive. To obtain block mask efficiently, common approach is to compute block-pooled attention map at the block granularity. Specifically, queries and keys are pooled within each block (e.g., mean pooling over bq query tokens and bkv key tokens) to produce and K. The pooled attention scores and weights can be obtained by: = K / d, = Softmax( S) RN/bqN/bkv , where Pij measures the importance of keeping tile (i, j). We define block sparse mask Mij as: Mij[:, :] = 1 or 0 Mij = 1 or 0. The block mask is determined by applying Top-k or Top-p to each row of : Top-k. For each row i, keep the k% largest positions in Pi,:: (1) (2) The ground-truth velocity is defined as vt = dxt dt = x1 x0. The diffusion model θ is trained to predict this velocity vt conditioned on the noisy latent xt, timestep t, and text prompt ctxt. Formally, the training objective is formulated as the mean squared error (MSE): Loss = Ex0,x1,ctxt,t (cid:104) u(xt, ctxt, t; θ) vt2(cid:105) . (3) where E[] denotes expectation taken over the data sample (x1, ctxt), noise sample x0, and timestep t. 3. Analysis 3.1. Error of Sparse Attention Notation (one attention row). Consider the i-th query token. Let R1N denote the attention weights for this row (i.e., the i-th row of ), let RN be the value matrix, and let {0, 1}1N be the binary mask for this row (e.g., the i-th row of ). We use for element-wise multiplication. Sparse-attention error. The full-attention output token is Mij = 1 if Top-k(Pi,:, k%), Mij = 0 otherwise. = pV R1d. (4) Top-p. For each row i, keep the smallest set of positions whose cumulative probabilities reach p%: After masking and renormalization, define the retained probability sum Mij = 1 if Top-p(Pi,:, p%), Mij = 0 otherwise, where TopP( Pi,:, p%) denotes the minimal prefix of indices after sorting Pi,: in descending order such that the summed probability is at least p%. 2.3. Diffusion Loss We adopt the flow matching (Lipman et al., 2022; Liu et al., 2022) formulation as the training objective for diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & τ = (p m)1 = (cid:88) j=1 pjmj R, (5) and the sparse-attention output os = (p m/τ )V R1d. The error is therefore = os = (p (p m)/τ ) V. (6) (7) SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning The sparse-attention error admits the decomposition For skewed , the accuracy satisfies: Top-k Top-k+Top-p > Top-p. = (1 m) (cid:123)(cid:122) (cid:125) dropped error (cid:124) V, + (1 1/τ ) (p m) (cid:125) (cid:123)(cid:122) renormalization error (cid:124) (8) which separates the dropped contribution (first term) from the renormalization effect (second term). 3.2. Analysis for Different Cases This is because when the distribution is highly concentrated, Top-p may reach the cumulative threshold with only few probabilities corresponding to attention sinks (Xiao et al., 2024; Gu et al., 2024). For example, for row like [0.6 (sink), 0.2, 0.1, . . .], Top-p(60%) selects only the sink probabilities and ignores other important probabilities, which increases the error of sparse attention. In contrast, Top-k could select not only the attention sink probabilities. (a) uniform . We keep the largest probabilities whose sum reaches 60% in each row. (b) skewed , where we keep the largest probabilities whose sum reaches 60% in each row. Figure 2. Uniform and skewed heatmap examples for Case 1. Table 1. L1 error of three masking methods on with uniform or skewed row distributions. distribution Top-k 0.4150 (a) Uniform 0.1664 (b) Skewed Top-p Top-k + Top-p 0.3726 0.2160 0.3707 0.1671 CASE 1 (FAILURE OF TOP-K AND TOP-P MASKING). In Figure 2, we select two representative attention-weight matrices to analyze the accuracy of different masking strategies. For the left (Figure 2a, each row has an almost uniform probability distribution. We call it uniform . For the right , each row is highly concentrated. We call it skewed . Under the same attention sparsity (i.e., each masking strategy keeps the same number of attention weights), we compare three masking methods: Top-k, Top-p, and their combination (Top-k+Top-p). We measure accuracy by the relative L1 distance between the sparse attention output and the full attention output. As shown in Table 1, for uniform , the accuracy satisfies: Top-p Top-k+Top-p > Top-k. This is because when the probabilities are spread across many tokens, Top-k keeps only fixed number of probabilities and may miss many important ones. For example, if row contains ten probabilities of 0.1, Top-20% keeps only two of them, i.e., 2 high-probabilities. This significantly increases the dropped error, i.e., the first term in Equation 8. 4 (a) before fine-tuning using sparse attention. Each row keeps the largest probabilities whose sum reaches 60%. (b) sparser after finetuning using sparse attention. Each row keeps the probabilities whose sum reaches 60%. Figure 3. Heatmaps before and after fine-tuning for Case 2. Table 2. Attention sparsity before and after sparse-attention finetuning, and the corresponding L1 error of sparse attention at the same sparsity level. Fine-tuning (a) Before (b) After Sparsity 51.3% 56.9% L1 Error 0.4901 0.4119 CASE 2 (ATTETION BE SPARSER AFTER TRAINING). As shown in Figure 3, we visualize two heatmaps of from diffusion model: (i) before fine-tuning with sparse attention and (ii) after fine-tuning with sparse attention. For fair comparison, we keep the largest probabilities until their sum reaches 60% for each row. Table 2 shows that, after sparse-attention fine-tuning, becomes more sparse (i.e., probabilities are more concentrated). We further compare the attention L1 error at the same 60% sparsity. Table 2 shows that the fine-tuned model achieves smaller error, which helps explain why trainable sparse attention performs better in practice. This observation also matches the error decomposition in Equation 8. Specifically, if fine-tuning makes the attention distribution more concentrated, the dropped error and the renormalization error will reduce under the same attention sparsity before fine-tuning. The dropped term SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning τ (p (1 m))V becomes smaller because, when is more concentrated, the probabilities masked out by (1 m) carry less probability. Meanwhile, the remained probability sum τ = (p m)1 becomes larger, so the factor (cid:0)1 1 (cid:1) decreases, reducing the renormalization term. simple example could illustrate this effect: Suppose that before fine-tuning, = [0.6, 0.2, 0.2], and after fine-tuning, = [0.8, 0.1, 0.1]. At 2/3 sparsity, the sparse attention mask = [1, 0, 0]. The dropped probability is (1 m), which equals [0, 0.2, 0.2] before fine-tuning and [0, 0.1, 0.1] after fine-tuning. Additionally, the 1/τ will also decrease. As result, the sparse-attention error is smaller after finetuning. Table 3. Full-attention diffusion fine-tuning degrades alignment under distribution mismatch. Without access to the original pretraining data, optimizing the diffusion loss alone leads to consistent degradation in aesthetic quality, vision reward, and VQA accuracy, even when full attention is used. Model Scale-Res. AQ VR VA VT Original Fine-tuning Original Fine-tuning 1.3B-480p 14B-720p 0.6441 0.6183 0.6466 0. 0.1084 0.0936 0.1238 0.1181 81.28 75.45 86.15 79.86 85.80 80.87 87.51 81. CASE 3 (DIFFUSION LOSS FAILED IN FINE-TUNING). Table 3 compares the original pre-trained models with the full-attention same models after fine-tuning using the standard diffusion-loss-based optimization adopted by prior sparse-attention methods. Despite keeping full attention, diffusion-loss-based fine-tuning degrades performance across several key metrics for both the 1.3B and 14B models. This degradation comes mainly from the quality of the fine-tuning data. With diffusion loss, the model is trained to fit the fine-tuning set, so the result strongly depends on the data. Compared with continuous pre-training, the only major change in our fine-tuning is the dataset. If the finetuning dataset has similar quality to the pre-training data, the model should keep similar performance after fine-tuning. However, pre-training data are usually closed and highquality, so it is hard to collect matching dataset. In this setting, this degradation is related to the dataset, not to the use of full or sparse attention. Therefore, fine-tuning with sparse attention will also be affected by this issue. We propose an effective and simple solution in Section 4. 4. Method 4.1. Hybrid Top-k+Top-p Masking To make sparse attention stably work at high sparsity, we need to avoid the two failure conditions in Case 1 in Section 3. In particular, high sparsity attention should not keep fixed number of tokens for uniform , and should not rely on fixed cumulative-probability threshold for skewed . This can be achieved by using Top-k and Top-p masking together. Specifically, for rows of with relatively uniform probability distribution, Top-p helps prevent the Top-k failure where fixed may keep too few useful tokens. For rows of with highly skewed distribution, Top-k helps prevent the Top-p failure where the cumulative threshold can be met by too few tokens corresponding to the attention sink, leading to an ineffective selection. Formally, we can determine the = Top-kp(P, k%, p%) as follows. Mij = (cid:40) 1, 0, Top-k(Pi,:, k%) Top-p(Pi,:, p%), otherwise. (9) 4.2. Velocity Distillation Loss Data distribution mismatch introduces additional performance degradation during sparse-attention adaptation. As analyzed in Case 3, even for full-attention models, optimizing the standard diffusion objective under such distribution mismatch can cause significant behavior drift, because the diffusion loss encourages the model to fit the fine-tuning data distribution. This drift directly conflicts with the goal of sparse-attention adaptation, which aims to adapt the new attention structure while keeping the original generation behavior. Therefore, this issue is not caused by sparse attention itself and cannot be resolved by modifying the attention structure alone, but instead requires different fine-tuning objective. To address this issue, we replace the data-driven diffusion objective with velocity distillation loss that directly constrains sparse-attention model to match frozen fullattention reference model. Instead of using supervision derived from the fine-tuning data, the sparse-attention model is trained to match the diffusion behavior of the original full-attention model. We adopt teacherstudent setup (Hinton et al., 2015), where the original full-attention diffusion model serves as frozen teacher, and the sparse-attention model serves as student. Both models share the same initialization and differ only in the attention operator. During training, the teacher and student receive identical inputs: noisy latent xt constructed following Eq. 1, timestep t, and text conditioning ctxt. We then train the student to align its diffusion dynamics with those of the teacher under these identical noisy inputs. Let ufull(xt, ctxt, t) and usparse(xt, ctxt, t) denote the teachers and students velocity predictions, respectively. We minimize the following velocity distillation loss: LVD = Ex0,x1,ctxt,t (cid:104) usparse(xt, ctxt, t) ufull(xt, ctxt, t)2(cid:105) . Under the flow matching framework, the diffusion dynamics 5 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning are parameterized by the velocity field u(xt, ctxt, t). As result, minimizing the velocity distillation loss directly aligns the sampling dynamics of the teacher and student models. Overall, velocity distillation uses the teachers predictions as supervision to guide sparse-attention adaptation. We do not use the standard diffusion loss during fine-tuning; the fine-tuning data are only used to construct noisy inputs xt for distillation. This design avoids introducing optimization gradients that push the model toward the mismatched fine-tuning data distribution, thereby significantly reducing behavior drift while enabling stable adaptation to sparse attention under high sparsity. 4.3. Kernel Implementation and Model Adaptation Algorithm 1 shows the kernel implementation of SpargeAttention2. We denote SpargeAttention2 as an attention operator = SpargeAttn2(Q, K, V, k%, p%), which computes sparse-attention outputs using the hybrid Top-k+Top-p masking strategy in Section 3. We implement mask construction and the block-sparse attention forward/backward passes in CUDA, building on FlashAttention. This implementation efficiently skips the masked-out matrix multiplications and softmax computations. Algorithm 2 summarizes the procedure for adapting pretrained diffusion model to sparse attention using SpargeAttention2. Starting from diffusion model with full-attention, we replace all attention layers with SpargeAttention2. The diffusion model using sparse attention is then adapted by minimizing the difference between its velocity predictions and those of frozen full-attention teacher. 5. Experiments 5.1. Setup Models and dataset. We conduct video generation experiments using the Wan2.1 (Wan et al., 2025) under two configurations: Wan2.1-1.3B at 480p resolution and Wan2.1-14B at 720p resolution. For training, we use private video dataset consisting of 3,000 videos, each approximately 5 seconds long, collected from publicly available sources. All videos are stored at native resolution of 720p. For the 1.3B model, videos are resized to 480p during training, while for the 14B model, both training and evaluation are performed at 720p resolution. To obtain textvideo pairs, we automatically generate captions for each video using Qwen3-VL-Flash (Bai et al., 2025). For evaluation, we adopt the prompts provided by VBench (Huang et al., 2024) as text inputs for video generation. Baselines and ablations. We compare our method with representative trainable sparse attention approaches for d) ; for = 1 to Tn do M2 = Top-p(P, p%) ; Algorithm 1 SpargeAttention2 Implementation. 1: Input: Matrices Q, K, RN d, bq, bkv, k%, p%. 2: Divide to Tm = N/bq blocks {Qi} ; 3: Divide K, to Tn = N/bkv blocks {Ki}, {Vi} ; 4: = softmax(pool(Q)pool(K)/ 5: M1 = Top-k(P, k%) , 6: = M1 M2 ; 7: for = 1 to Tm do 8: 9: 10: 11: 12: 13: 14: end if 15: end for 16: 17: Oi = diag(lTn 18: end for 19: return = {Oi} ; if [i, j] = 1 then Sij = QiK ; / mij = max(mi,j1, rowmax(Sij)) ; Pij = exp(Sij mij) ; lij = emi,j1mij li,j1 + rowsum(Pij) ; Oij = diag(emi,j1mij )Oi,j1 + PijVj ; )1Oi,Tn ; Algorithm 2 Adapting Diffusion Models with SpargeAttention2 via Velocity Distillation. 1: Input: Pre-trained diffusion model θfull, sparsity hyperparameters k%, p%, training data D. 2: Output: Sparse-attention model θsparse. 3: Initialize θsparse θfull and freeze θfull. 4: Replace in attention SpargeAttn2(, , , k%, p%) (Alg. 1). layers all θsparse with 5: for each training iteration do 6: Sample (x1, ctxt) D, noise x0 (0, I), and select timestep [0, 1] according to predefined schedule. Construct noisy latent: Compute teacher velocity with full attention: xt = tx1 + (1 t)x0. ufull = uθfull (xt, ctxt, t). Compute student velocity with SpargeAttention2: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return θsparse. usparse = uθsparse(xt, ctxt, t). Velocity distillation loss: LVD = usparse ufull2. Update θsparse by minimizing LVD. diffusion models, including VSA (Zhang et al., 2025i), VMoBA (Wu et al., 2025), SLA (Zhang et al., 2025c), and SpargeAttention (Zhang et al., 2025f). In addition, we conduct controlled ablation studies by modifying only one component at time. Specifically, we study three aspects: (1) Sparse masker design, by replacing the unified Top-k+Top-p masker with Top-k-only or Top-p-only variants; (2) Effect of training, by comparing trainable sparse attention with 6 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning Table 4. Effectiveness comparison on Wan2.1-1.3B at 480p resolution. Method IQ OC AQ VR VQA-a VQA-t Sparsity Attn Time E2E Time Full Attention 63.67 20.27 64.41 0.1084 SpargeAttn VSA VMoBA SLA SpargeAttn2 35.28 59.57 65.31 63.14 67. 13.41 19.27 20.82 21.09 21.57 40.53 50.60 64.14 62.91 65.05 -0.1398 -0.0881 0.0936 0.0881 0.1010 81.28 3.258 33.35 78.99 72.66 83.86 85. 0.608 48.36 86.69 80.51 87.73 0% 89% 90% 90% 95% 95% 97s 12.6s 25s 36s 11s 6s 159s 74.6s 87s 98s 73s 68s Table 5. Effectiveness comparison on Wan2.1-14B at 720p resolution. Method IQ OC AQ VR VQA-a VQA-t Sparsity Attn Time E2E Time Full Attention 68. 22.44 64.66 0.1238 SpargeAttn VSA VMoBA SLA SpargeAttn2 38.46 64.03 67.18 64.43 69.08 16.26 21.27 20.85 20.89 21. 42.16 63.37 63.64 61.89 64.92 -0.1306 0.1074 0.1117 0.1078 0.1149 86.15 9.926 77.63 81.66 76.90 85.21 87.00 4.451 85.60 83.21 82.60 87. 0% 86% 90% 90% 95% 95% 2550s 415s 651s 832s 285s 157s 3043s 908s 1144s 1325s 778s 650s training-free variant where sparse-attention parameters are frozen; and (3) Training objective, by replacing the proposed velocity distillation loss with standard diffusion-loss-based training. Metrics. For video generation quality, we report Imaging Quality (IQ), Overall Consistency (OC), and Aesthetic Quality (AQ) from the VBench benchmark (Huang et al., 2024), together with Vision Reward (VR) (Xu et al., 2024) and VQA accuracy (VA and VT) (Liu et al., 2024), where VA and VT denote VQA-a and VQA-t, respectively, following prior work (Zhang et al., 2025c; Wu et al., 2025). All training hyper-parameters and sparse-attention settings are provided in Appendix A. For efficiency evaluation, we report attention latency and end-to-end generation latency in seconds (s), measured on an RTX 5090 GPU. 5.2. Effectiveness We evaluate SpargeAttention2 against prior trainable sparseattention methods on Wan2.1 under high attention sparsity. Results on Wan2.1-1.3B at 480p and Wan2.1-14B at 720p are reported in Tables 4 and 5, respectively. Across both settings, SpargeAttention2 consistently achieves the best overall performance, matching or exceeding the full-attention model on generation quality while remaining stable under high sparsity. In contrast, existing sparse-attention baselines exhibit noticeable degradation under the same or even lower sparsity levels. These results indicate that SpargeAttention2 performs robustly across different model sizes and resolutions. Figure 4 provides qualitative comparison. 5.3. Efficiency We evaluate the efficiency of SpargeAttention2 on Wan2.1 under high attention sparsities, focusing on both attention operator latency and end-to-end video generation time. Results on Wan2.1-1.3B at 480p and Wan2.1-14B at 720p are reported in Tables 4 and 5, respectively. Under sparsity of 85% - 95%, SpargeAttention2 is the only method that simultaneously achieves strong generation quality and substantial efficiency gains. In contrast, other sparse attention baselines are significantly slower than SpargeAttention2 and exhibit clear degradation in generation quality. Notably, SpargeAttention2 achieves higher video generation quality than all baselines, even at higher attention sparsity. For efficiency, on Wan2.1-1.3B at 480p, SpargeAttention2 reduces attention latency from 97s to 6s, achieving 16.2 speedup over full attention. It is 1.8 faster than SLA and more than 4 faster than VSA and VMoBA, while also delivering clearly superior generation quality. This reduction in attention cost result in an end-toend generation speedup from 159s to 68s, corresponding to 2.3 overall acceleration. Similar trends are observed on Wan2.1-14B at 720p. SpargeAttention2 reduces attention latency from 2550s to 157s, achieving 16.2 speedup over full attention. Compared with prior sparse-attention methods, it is 1.8 faster than SLA and more than 4 faster than VSA and VMoBA, while maintaining generation quality comparable to or better than full attention. As result, end-to-end generation time is reduced from 3043s to 650s, yielding 4.7 speedup. 7 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning Figure 4. representative example of text-to-video generation under high attention sparsity, evaluated on Wan2.1-14B at 720p. SpargeAttention2 produces semantically correct video. In contrast, SLA and VSA produce videos in which the male character walks backward, while VMoBA fails to generate the female character specified in the prompt. The prompt used for generation is in Appendix Table 6. Ablation studies on SpargeAttention2. VD replaces velocity distillation with standard diffusion fine-tuning. VQA denotes the overall score combining VQA-a and VQA-t. loss-based fine-tuning consistently underperforms velocity distillation. This confirms the effectiveness of the proposed velocity distillation for sparse-attention adaptation. Variant IQ OC AQ VR VQA Wan2.1-1.3B-480p 6. Related Work SpargeAttn2 67.68 Top-k only Top-p only Training-free VD 21.57 65.84 21.51 60.56 21.12 53.18 19.87 67.23 21.26 65.05 64.57 60.12 48.93 63.34 0.1010 0.0916 0.0312 -0.0650 0. Wan2.1-14B-720p SpargeAttn2 68.41 Top-k only Top-p only Training-free VD 21.06 65.24 20.64 63.37 21.33 62.17 19.41 66.40 20.56 65.02 63.99 63.62 57.01 64.59 0.1119 0.0935 0.1090 -0.0272 0.1082 86.73 86.90 62.57 20.40 85. 88.22 84.25 86.43 45.85 85.05 5.4. Ablation We analyze the contribution of individual design choices in SpargeAttention2 through ablation studies, focusing on the sparse masker, trainability, and training objective. Results are reported in Table 6. Sparse masker design. We compare the proposed hybrid Top-k/Top-p masking with variants that use only Top-k or only Top-p masking. As shown in Table 6, the unified Top-k+Top-p masker consistently achieves the best overall generation quality and alignment across both model scales, validating its robustness under high sparsity. Effect of training. We evaluate the impact of training sparse attention by comparing SpargeAttention2 with trainingfree variant. Table 6 shows that disabling training leads to substantial degradation in generation quality and alignment for both the 1.3B and 14B models, highlighting the necessity of adapting sparse attention under high sparsity. Training objective. We examine the role of the training objective by replacing the proposed velocity distillation loss with standard diffusion loss. As shown in Table 6, diffusion8 Sparse attention methods can be grouped by whether they require training. First, training-free approaches (Gao et al., 2024; Xi et al., 2025; Zhang et al., 2025f; Ribar et al., 2023; Yang et al., 2025; Li et al., 2025; Chen et al., 2025a; Lai et al., 2025; Zhang et al., 2023; Xiao et al., 2024; Jiang et al., 2024; Tang et al., 2024; Zhu et al., 2025; Lin et al., 2025; Xu et al., 2025; Xia et al., 2025; Chen et al., 2025b; Zhang et al., 2025j) reduce inference cost by applying test-time attention mask. Among them, vAttention (Desai et al., 2025) uses hybrid of Top-k and random sampling, which differs from our Top-k and Top-p hybrid and is not designed for diffusion models. Second, trainable sparse attention methods (Zhang et al., 2025i; Wu et al., 2025; Zhang et al., 2025c; Zhan et al., 2025; Zhou et al., 2025; Lu et al., 2025; Yuan et al., 2025; Liu et al., 2025a; Zhang et al., 2026; Cai et al., 2025; Liu et al., 2025b; Sun et al., 2025; Tan et al., 2025; Ding et al., 2023) enhance attention sparsity by directly using sparse attention during training. Some methods in the second category are designed for diffusion models, and SpargeAttention2 belongs to this group. Among them, SpargeAttention2 achieves state-of-the-art performance. 7. Conclusion In this paper, we analyze key challenges in sparse attention for diffusion models and propose SpargeAttention2. It is an efficient and accurate trainable sparse attention method that achieves high sparsity without degrading video generation quality. Specifically, by combining hybrid Top-k and Top-p sparse masking, an efficient implementation, and distillation-style fine-tuning method, SpargeAttention2 achieves very high sparsity while preserving generation quality, surpassing baselines. SpargeAttention2 achieves 95% attention sparsity, 16.2 attention runtime speedup, and up to 4.7 end-to-end video generation speedup. SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning"
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Cai, S., Yang, C., Zhang, L., Guo, Y., Xiao, J., Yang, Z., Xu, Y., Yang, Z., Yuille, A., Guibas, L., et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. Chen, P., Zeng, X., Zhao, M., Ye, P., Shen, M., Cheng, W., Yu, G., and Chen, T. Sparse-vdit: Unleashing the power of sparse attention to accelerate video diffusion transformers. arXiv preprint arXiv:2506.03065, 2025a. Chen, R., Mills, K. G., Jiang, L., Gao, C., and Niu, D. Re-ttention: Ultra sparse visual generation via attention statistical reshape. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Desai, A., Agrawal, K. K., Yang, S., Cuadron, A., Schroeder, L. G., Zaharia, M., Gonzalez, J. E., and Stoica, I. vattention: Verified sparse attention. arXiv preprint arXiv:2510.05688, 2025. Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Gao, Y., Zeng, Z., Du, D., Cao, S., Zhou, P., Qi, J., Lai, J., So, H. K.-H., Cao, T., Yang, F., et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Gu, X., Pang, T., Du, C., Liu, Q., Zhang, F., Du, C., Wang, Y., and Lin, M. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hu, Y., Huang, W., Liang, Z., Chen, C., Zhang, J., Zhu, J., and Chen, J. Identifying sensitive weights via postquantization integral. arXiv preprint arXiv:2503.01901, 2025. Hu, Y., Singh, H., Maheswaran, M., Xi, H., Hooper, C., Zhang, J., Tomar, A., Mahoney, M. W., Min, S., Farajtabar, M., et al. Residual context diffusion language models. arXiv preprint arXiv:2601.22954, 2026. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. Jiang, Y., Fu, F., Zhao, W., Rabanser, S., Lane, N. D., and Yuan, B. Cascadia: cascade serving system for large language models. arXiv preprint arXiv:2506.04203, 2025. Jiang, Y., Li, W., Peng, Y., Zhang, J., Yan, R., Chen, J., Han, X., Fu, F., and Yuan, B. Hexgen-3: fully disaggregated llm serving framework with fine-grained heterogeneous resource autoscaling. 2026. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Lai, X., Lu, J., Luo, Y., Ma, Y., and Zhou, X. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. arXiv preprint arXiv:2502.20766, 2025. Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., et al. Radial attention: (nlog n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. Lin, C., Tang, J., Yang, S., Wang, H., Tang, T., Tian, B., Stoica, I., Han, S., and Gao, M. Twilight: Adaptive attention sparsity with hierarchical top-p pruning. arXiv preprint arXiv:2502.02770, 2025. Hinton, G., Vinyals, O., and Dean, J. the knowledge in neural network. arXiv:1503.02531, 2015. Distilling arXiv preprint Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 9 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Liu, A., Zhang, Z., Li, Z., Bai, X., Han, Y., Tang, J., Xing, Y., Wu, J., Yang, M., Chen, W., et al. Fpsattention: Trainingaware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025b. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D. Sparq attention: Bandwidthefficient llm inference. 2023. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Sun, W., Tu, R.-C., Ding, Y., Jin, Z., Liao, J., Liu, S., and Tao, D. Vorta: Efficient video diffusion via routing sparse attention. arXiv preprint arXiv:2505.18809, 2025. Tan, X., Chen, Y., Jiang, Y., Chen, X., Yan, K., Duan, N., Zhu, Y., Jiang, D., and Xu, H. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training. arXiv preprint arXiv:2502.07590, 2025. Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wu, J., Hou, L., Yang, H., Tao, X., Tian, Y., Wan, P., Zhang, D., and Tong, Y. Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. Xi, H., Yang, S., Zhao, Y., Li, M., Cai, H., Li, X., Lin, Y., Zhang, Z., Zhang, J., Li, X., et al. Quant videogen: Auto-regressive long video generation via 2-bit kv-cache quantization. arXiv preprint arXiv:2602.02958, 2026. Xia, Y., Ling, S., Fu, F., Wang, Y., Li, H., Xiao, X., and Cui, B. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. Xiang, C., Liu, J., Zhang, J., Yang, X., Fang, Z., Wang, S., Wang, Z., Zou, Y., Su, H., and Zhu, J. Geometry-aware rotary position embedding for consistent video world model. arXiv preprint arXiv:2602.07854, 2026. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. Xu, J., Huang, Y., Cheng, J., Yang, Y., Xu, J., Wang, Y., Duan, W., Yang, S., Jin, Q., Li, S., et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. 10 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Zhang, J., Zheng, K., Jiang, K., Wang, H., Stoica, I., Gonzalez, J. E., Chen, J., and Zhu, J. Turbodiffusion: Accelerating video diffusion models by 100-200 times. arXiv preprint arXiv:2512.16093, 2025h. Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2307823097, 2025. Zhan, C., Li, W., Shen, C., Zhang, J., Wu, S., and Zhang, H. Bidirectional sparse attention for faster video diffusion training. arXiv preprint arXiv:2509.01085, 2025. Zhang, J., Su, R., Liu, C., Wei, J., Wang, Z., Wang, H., Zhang, P., Jiang, H., Huang, H., Xiang, C., et al. Efficient attention methods: Hardware-efficient, sparse, compact, and linear attention. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML 2025), 2025a. Zhang, J., Li, G., and Su, J. Sage: framework of precise retrieval for rag. arXiv preprint arXiv:2503.01713, 2025b. Zhang, J., Wang, H., Jiang, K., Yang, S., Zheng, K., Xi, H., Wang, Z., Zhu, H., Zhao, M., Stoica, I., et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025c. Zhang, J., Wei, J., Huang, H., Zhang, P., Zhu, J., and Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR 2025), 2025d. Zhang, J., Wei, J., Zhang, P., Xu, X., Huang, H., Wang, H., Jiang, K., Zhu, J., and Chen, J. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. Advances in Neural Information Processing Systems (NeurIPS 2025), 2025e. Zhang, J., Xiang, C., Huang, H., Xi, H., Zhu, J., Chen, J., et al. Spargeattention: Accurate and training-free sparse In Fortyattention accelerating any model inference. second International Conference on Machine Learning, 2025f. Zhang, J., Xu, X., Wei, J., Huang, H., Zhang, P., Xiang, C., Zhu, J., and Chen, J. Sageattention2++: more efficient implementation of sageattention2. arXiv preprint arXiv:2505.21136, 2025g. Zhang, J., Wang, H., Jiang, K., Zheng, K., Jiang, Y., Stoica, I., Chen, J., Zhu, J., and Gonzalez, J. E. SLA2: SparseLinear Attention with Learnable Routing and QAT. 2026. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025i. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025j. Zhang, P., Wei, J., Zhang, J., Zhu, J., and Chen, J. Accurate int8 training through dynamic block-level fallback. arXiv preprint arXiv:2503.08040, 2025k. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhao, M., Yan, B., Yang, X., Zhu, H., Zhang, J., Liu, S., Li, C., and Zhu, J. Ultraimage: Rethinking resolution extrapolation in image diffusion transformers. arXiv preprint arXiv:2512.04504, 2025a. Zhao, M., Zhu, H., Wang, Y., Yan, B., Zhang, J., He, G., Yang, L., Li, C., and Zhu, J. Ultravico: Breaking extrapolation limits in video diffusion transformers. arXiv preprint arXiv:2511.20123, 2025b. Zheng, K., Wang, Y., Ma, Q., Chen, H., Zhang, J., Balaji, Y., Chen, J., Liu, M.-Y., Zhu, J., and Zhang, Q. Large scale diffusion distillation via score-regularized continuoustime consistency. arXiv preprint arXiv:2510.08431, 2025. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Zhou, Y., Xiao, Z., Wei, T., Yang, S., and Pan, X. Trainable log-linear sparse attention for efficient diffusion transformers. arXiv preprint arXiv:2512.16615, 2025. Zhu, K., Tang, T., Xu, Q., Gu, Y., Zeng, Z., Kadekodi, R., Zhao, L., Li, A., Krishnamurthy, A., and Kasikci, B. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms. arXiv preprint arXiv:2502.12216, 2025. 11 SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p masking and Distillation Fine-Tuning A. Hyper-parameters Unless otherwise stated, all models are trained for 500 steps. We use batch size of 64 for Wan2.1-1.3B at 480p resolution and 16 for Wan2.1-14B at 720p resolution. To reduce computational cost, ablation studies on the 14B model are conducted for 100 training steps, while all main results are reported using models trained for 500 steps. Sparse Attention Settings. We calibrate Top-k and Top-p to achieve target sparsity of approximately 95% across model scales. Specifically, for Wan2.1-1.3B at 480p resolution, we use Top-k = 0.03 and Top-p = 0.2. while for Wan2.1-14B at 720p resolution, we use Top-k = 0.03 and Top-p = 0.16. We set bq = 128 and bkv = 64, respectively. Ablation Settings. For ablation studies on sparse masker design, we vary Top-k or Top-p individually while keeping all other settings unchanged. In the Top-k ablation, we set Top-k = 0.05 for both Wan2.1-1.3B and Wan2.1-14B, resulting in approximately 95% sparsity. In the Top-p ablation, we use Top-p = 0.4 for Wan2.1-1.3B at 480p and Top-p = 0.3 for Wan2.1-14B at 720p, corresponding to sparsity levels of 94% and 93%, respectively. B. Prompts for Qualitative Visualizations This subsection lists the text prompts used to generate the qualitative video samples shown in the main paper. Prompts for Figure 1. From top to bottom, the prompts are: large polar bear sitting on rocky Arctic shoreline, casually playing an acoustic guitar with its massive paws. The bear has thick, white fur glistening under soft golden sunlight, curious expression, and gentle eyes focused on the instrument. Melodic notes seem to ring out across the tundra. Behind, ice floes drift in calm blue water beneath clear pastel sky. Natural daylight, medium shot from the front, slight low angle emphasizing the bears size. Slow camera pan from left to right. Realistic wildlife style with whimsical twist. fluffy brown teddy bear with cheerful expression is energetically playing red drum kit in the heart of New York Citys Times Square. Bright neon billboards and towering digital screens flash colorful advertisements all around, casting vibrant reflections on the wet pavement. The teddy bear, wearing tiny sunglasses and denim vest, rhythmically bangs the drums with animated motion, cymbals shimmering. Pedestrians stop to watch in delight, capturing the whimsical scene on their phones. Dynamic camera circling the bear, wide-angle view emphasizing the bustling urban spectacle and lively performance. Cartoon-style 3D animation, high detail, vivid colors. Pacific Coast, Carmel-by-the-Sea, scenic ocean shoreline with rolling waves gently crashing against rocky outcrops and sandy coves. The deep blue Pacific Ocean stretches to the horizon under soft golden-hour sky, with seagulls gliding above the surf. Waves foam and ripple over smooth stones and tide pools teeming with marine life. Coastal pine trees and native shrubs line the bluffs, framing the pristine beach. Gentle wave motion, sparkling water surface, and calm breeze enhance the serene atmosphere. Wide-angle landscape shot, natural daylight, realistic detail, peaceful coastal ambiance. vibrant clownfish with bright orange body and white stripes edged in black darts playfully through lush coral reef. It weaves gracefully between swaying sea anemones and colorful coral formations in vivid pinks, purples, and blues. Sunlight filters through the crystal-clear turquoise water, creating shimmering patterns on the ocean floor. Small bubbles rise as the fish flicks its tail, exploring its intricate underwater home. Wide-angle underwater shot with natural lighting, showcasing rich detail and lively marine biodiversity. Gentle current moves the corals softly. Prompt for Figure 4. An oil painting of couple in elegant formal evening wear walking home as sudden heavy downpour surrounds them, clutching umbrellas that barely shield them from the rain. The man wears sleek black tuxedo, the woman flowing satin gown shimmering with raindrops, her hair slightly damp and clinging to her shoulders. Rain streaks through the air in silvery lines under dim streetlights, puddles rippling at their feet. Warm glows from nearby lampposts reflect on wet cobblestones, creating romantic, cinematic atmosphere. Loose brushstrokes and rich textures evoke emotion and movement. Medium shot, side view, captured from slight distance."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "UC Berkeley"
    ]
}