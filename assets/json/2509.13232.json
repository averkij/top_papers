{
    "paper_title": "Single-stream Policy Optimization",
    "authors": [
        "Zhongwen Xu",
        "Zihan Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning."
        },
        {
            "title": "Start",
            "content": "Single-stream Policy Optimization Zhongwen Xu1* and Zihan Ding1* 1Tencent, *Equal contribution. 5 2 0 2 6 1 ] . [ 1 2 3 2 3 1 . 9 0 5 2 : r Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from singlestream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPOs gains stem from its principled approach to baseline estimation and advantage normalization, offering more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3-8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@ğ‘˜ across the evaluated ğ‘˜ values. SPOs success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning. 1. Introduction Reinforcement learning (RL) [29] has become cornerstone for advancing the reasoning capabilities of Large Language Models (LLMs), notably the Reinforcement Learning with Verifiable Reward (RLVR) paradigm [18, 31]. Methods like Group Relative Policy Optimization (GRPO) [27, 31] have achieved remarkable success by adopting multi-outcome approach, generating group of responses for each prompt to construct an on-the-fly baseline for variance reduction. While this group-based paradigm has pushed the state of the art, it suffers from fundamental inefficiencies. When all responses in group share the same outcome (e.g., all correct or all incorrect), the relative advantage collapses to zero, yielding no learning signal. This degeneracy represents fundamental waste of computation and data. To counteract this, series of engineering heuristics like dynamic sampling [40] have been developed. These workarounds, while functional, add significant complexity and create less principled, more convoluted optimization process. Group-based architectural choice also imposes critical synchronization barrier. In distributed training, the entire group must wait for its slowest member, bottleneck that becomes particularly acute in complex agentic tasks requiring multi-turn tool use or long-horizon reasoning [13, 39, 41]. In these settings, interaction times are highly variable (e.g., number of interaction turns, time per interaction, etc), and single slow-running agentic trajectory can stall its entire group, severely hindering training throughput and scalability. Corresponding authors: zhongwenxu@tencent.com, dingzihan737@gmail.com Single-stream Policy Optimization We advocate for returning to the classic single-stream paradigm for policy gradient optimization [29], where each training sample is single stream of prompt-response pair. This is not mere simplification, but deliberate re-alignment with foundational RL principles to address the aforementioned architectural flaws. To overcome the critical challenge of high gradient variance in this setting, we introduce Singlestream Policy Optimization (SPO). SPO replaces the noisy, on-the-fly group baseline with three synergistic components for stable and efficient learning. First, it employs lightweight Bayesian value tracker to maintain persistent, temporally-informed estimate of the success probability for each prompt, serving as low-variance baseline. Second, it normalizes advantages globally across the entire batch, avoiding the instability of per-group statistics. Finally, this architecture naturally enables an adaptive curriculum via prioritized sampling, focusing computational resources on the most informative prompts. The benefits of this principled approach are clear: SPO is inherently more scalable and eliminates the computational waste of degenerate groups. Our experiments confirm these advantages, demonstrating that SPO consistently outperforms GRPO on challenging reasoning benchmarks, improving the absolute point gains on challenging datasets, including 7.3 percentage points (pp) on BRUMO 25, 4.4 pp on AIME 25, 3.3 pp on HMMT 25, and the pass@ğ‘˜ curves of SPO are above GRPO for all ğ‘˜s. The scalability benefit is particularly pronounced in agentic settings. Our simulations, designed to model these variabletime scenarios, show that SPOs group-free design can achieve 4.35 training throughput speedup by eliminating group synchronization bottlenecks. SPO thus provides more robust foundation for modern LLM optimization, prompting re-evaluation of essential versus incidental complexity in the field. 2. Related Work Group Relative Policy Optimization (GRPO) [27] addresses the computational overhead and training instability of PPO-style algorithms [25] by eliminating the need for separate critic network. Instead, GRPO constructs baselines on-the-fly using multiple responses generated for each prompt. Specifically, GRPO samples group of multiple responses for each prompt and normalizes the rewards within this group to have zero mean and unit variance, creating relative advantages for policy updates. However, this approach can be inefficient if all responses in group receive the same reward (e.g., all incorrect or all correct), resulting in zero-advantage for all samples and providing no learning signal. To address this, DAPO [40] enhances GRPO with engineering treatments like dynamic sampling, which continues generating responses until non-zero advantages are achieved, ensuring meaningful gradients. Several other works have proposed improvements to group-based methods. Zheng et al. [43] introduce GRESO, an online filtering algorithm that leverages reward training dynamics to predict and skip uninformative prompts before generation, significantly reducing rollout overhead. Liu et al. [22] propose Lite PPO, which simplifies RLVR training to only advantage normalization and token-level loss aggregation. Other group-based approaches include RLOO [1], which returns to the simpler REINFORCE [29, 37] algorithm using Leave-One-Out baseline that treats entire generations as single actions. Similarly, Hao et al. [15] propose On-Policy RL with Optimal Baseline (OPO), which uses length-weighted average of rewards as an optimal simplified baseline. Despite these improvements, all group-based methods share fundamental limitations. They construct baselines from concurrently generated responses rather than persistent, historical estimates, inheriting the same core architectural constraints as GRPO: synchronization overhead and increased generation costs in distributed settings. Moving beyond group-based methods, Brantley et al. [5] propose ğ´*-PO, two-stage framework that Work in Progress 2 Single-stream Policy Optimization achieves single-sample efficiency through different approach. In the first stage, ğ´*-PO performs offline estimation to approximate the optimal value function ğ‘‰ * rather than the policy-specific value function ğ‘‰ğœ‹. The second stage uses this pre-computed optimal value to construct optimal advantage estimates ğ´* for least-squares regression objective during online training. However, ğ´*-PO has key limitations compared to our approach. It relies on fixed, offline-computed estimate that does not adapt as the policy evolves during training. Additionally, ğ´*-PO is constrained by KL-regularized policy optimization, which restricts how far the optimized policy can deviate from the reference policy. 3. Background Reinforcement learning (RL) algorithms have been used to align Large Language Models (LLMs) with human preferences (RLHF) and to optimize verifiable reward signals (RLVR; e.g., [18, 27]). 3.1. Policy Gradient and the REINFORCE Algorithm The foundational method for this optimization is the policy gradient theorem [29, 37]. For LLMs, trajectory consists of generating single response ğ‘¦ from prompt ğ‘¥. The objective function is the expected reward: ğ½(ğœƒ) = Eğ‘¥ğ’Ÿ,ğ‘¦ğœ‹ğœƒ(ğ‘¥)[ğ‘…(ğ‘¥, ğ‘¦)], where ğ’Ÿ is the prompt distribution and ğ‘…(ğ‘¥, ğ‘¦) is the reward for generating response ğ‘¦ for prompt ğ‘¥. The gradient of this objective is given by: (1) ğœƒğ½(ğœƒ) = Eğ‘¥ğ’Ÿ,ğ‘¦ğœ‹ğœƒ(ğ‘¥)[ğ‘…(ğ‘¥, ğ‘¦)ğœƒ log ğœ‹ğœƒ(ğ‘¦ğ‘¥)]. This formulation gives rise to the REINFORCE algorithm [29, 37], which updates the policy by taking step in the direction of this estimated gradient. significant drawback of REINFORCE is the high variance of its gradient estimator. The raw reward ğ‘…(ğ‘¥, ğ‘¦) can fluctuate widely, leading to noisy updates and unstable training. (2) To mitigate high variance, baseline ğ‘(ğ‘¥) that is conditionally independent of the action ğ‘¦ can be subtracted from the reward. This results in an unbiased gradient estimator with provably lower variance [14]: ğœƒğ½(ğœƒ) = Eğ‘¥ğ’Ÿ,ğ‘¦ğœ‹ğœƒ(ğ‘¥)[(ğ‘…(ğ‘¥, ğ‘¦) ğ‘(ğ‘¥))ğœƒ log ğœ‹ğœƒ(ğ‘¦ğ‘¥)]. The term ğ´(ğ‘¥, ğ‘¦) = ğ‘…(ğ‘¥, ğ‘¦) ğ‘(ğ‘¥) is known as the advantage. The optimal baseline that minimizes variance is the true value function ğ‘‰ğœ‹(ğ‘¥) = Eğ‘¦ğœ‹ğœƒ(ğ‘¥)[ğ‘…(ğ‘¥, ğ‘¦)], which is the expected reward for given prompt ğ‘¥. In practice, ğ‘‰ğœ‹(ğ‘¥) is unknown and must be estimated. The quality of this estimation is crucial for the stability and efficiency of the RL algorithm. (3) 3.2. Variance Reduction Baselines for Large Language Models Several strategies have been developed to estimate the baseline ğ‘(ğ‘¥) in the context of LLM training. PPO [25] trains parameterized critic network ğ‘£ğœ‘. However, learning ğ‘£ğœ‘ is notoriously unstable and resource-intensive, as ğœ‘ typically matches the size of the LLM policy parameters ğœƒ. common approach is to construct an empirical, on-the-fly baseline from multiple samples. Group Relative Policy Optimization (GRPO) [27, 31] generates group of ğº responses {ğ‘¦1, . . . , ğ‘¦ğº} for single Work in Progress 3 Single-stream Policy Optimization prompt ğ‘¥, then uses the mean rewards of the group as the baseline ğ‘GRPO. Another popular baseline is the Leave-One-Out (RLOO). For given sample ğ‘¦ğ‘–, the baseline is the average reward of the other ğº 1 samples in the group, denoted as ğ‘RLOO: ğ‘GRPO(ğ‘¥) = 1 ğº ğ‘— ğ‘…(ğ‘¥, ğ‘¦ğ‘—), ğ‘RLOO(ğ‘¥, ğ‘¦ğ‘–) = 1 ğº ğ‘—=ğ‘– ğ‘…(ğ‘¥, ğ‘¦ğ‘—). (4) The raw advantage for sample ğ‘¦ğ‘– is then ğ´(ğ‘¥, ğ‘¦ğ‘–) = ğ‘…(ğ‘¥, ğ‘¦ğ‘–) ğ‘GRPO(ğ‘¥), then it is normalized with the standard deviation ğœğº. While simple to implement, this approach suffers from two key limitations. First, it is sample-inefficient, requiring ğº > 1 generations per prompt for each gradient step. Second, the baseline is estimated from very small group (ğº), making it high-variance estimate of the true value function, which in turn leads to noisy advantage estimates. 4. Method We introduce Single-stream Policy Optimization (SPO), method designed for policy optimization in settings with verifiable feedback (RLVR) [18]. We assume the feedback is binary1, i.e., +1 for success and 0 for failure. SPO addresses the challenge of estimating non-stationary success probability for policy that evolves over training iterations. It integrates Bayesian value tracker with an adaptive memory mechanism into policy gradient framework. The core components are: (1) KL-adaptive tracker that provides low-variance, single-sample estimate of the success probability; (2) global advantage normalization scheme that ensures high sample efficiency and stable learning dynamics; and (3) prioritized sampling across training prompts to focus on prompts with high learning potential. The following subsections detail each component. 4.1. KL-Adaptive Value Tracker The definition of value function is the expected reward of the prompt ğ‘¥ under policy ğœ‹, i.e., ğ‘‰ğœ‹(ğ‘¥) = Eğ‘¦ğœ‹(ğ‘¥)[ğ‘…(ğ‘¥, ğ‘¦)]. We use Ë†ğ‘£(ğ‘¥) to denote the trackers running estimate of ğ‘‰ğœ‹(ğ‘¥); that is, Ë†ğ‘£(ğ‘¥) ğ‘‰ğœ‹(ğ‘¥). To estimate the non-stationary success probability of prompt ğ‘¥, we use Bayesian tabular tracker instead of separate value network2. For the binary success/failure rewards common in RLVR, this is elegantly modeled using Beta distribution, which is the conjugate prior for the Bernoulli process governing the outcomes. We therefore model the success probability Ë†ğ‘£(ğ‘¥) using Beta distribution: Ë†ğ‘£(ğ‘¥) Beta(ğ›¼(ğ‘¥), ğ›½(ğ‘¥)), where the value estimate is the posterior mean Ë†ğ‘£(ğ‘¥) = ğ›¼(ğ‘¥)/(ğ›¼(ğ‘¥) + ğ›½(ğ‘¥)). The tracker adapts to policy changes by dynamically adjusting its memory of past rewards. When the policy changes significantly, older observations become less relevant and should be downweighted. After each new observation ğ‘Ÿ(ğ‘¥, ğ‘¦) {0, 1}, we discount the prior Beta parameters (ğ›¼1, ğ›½1) by factor ğœŒ(ğ‘¥) before incorporating the new evidence ğ‘Ÿ(ğ‘¥, ğ‘¦): ğ›¼(ğ‘¥) = ğœŒ(ğ‘¥)ğ›¼1(ğ‘¥) + ğ‘Ÿ(ğ‘¥, ğ‘¦), ğ›½(ğ‘¥) = ğœŒ(ğ‘¥)ğ›½1(ğ‘¥) + (1 ğ‘Ÿ(ğ‘¥, ğ‘¦)), Ë†ğ‘£(ğ‘¥) = ğ›¼(ğ‘¥) ğ›¼(ğ‘¥) + ğ›½(ğ‘¥) . (5) 1Generalizing to non-binary rewards is straightforward, as discussed at the end of Section 4.1. 2The development of core RL algorithms was on tabular representation [29]. Work in Progress 4 Single-stream Policy Optimization The discount factor ğœŒ(ğ‘¥) = 2ğ·(ğ‘¥)/ğ·half is determined by the KL divergence ğ·(ğ‘¥) between the current policy and the last policy that acted on prompt ğ‘¥, causing the tracker to forget faster as the policy changes more significantly. The hyperparameter ğ·half controls this forgetting rate ğœŒ [ğœŒmin, ğœŒmax]. Initialization. To initialize, we collect ğ‘›0 samples to compute an initial value estimate Ë†ğ‘£0(ğ‘¥). To avoid transient instability, we set the initial effective sample size to its expected equilibrium, ğ‘0 = 1/(1 ğœŒmin), where ğœŒmin is the minimum allowed forgetting factor. The initial parameters are then: ğ›¼0(ğ‘¥) = ğ‘0 Ë†ğ‘£0(ğ‘¥), ğ›½0(ğ‘¥) = ğ‘0 (1 Ë†ğ‘£0(ğ‘¥)). (6) This Bayesian update is equivalent to an adaptive Exponential Moving Average (EMA) on the value estimate: (7) Ë†ğ‘£(ğ‘¥) = Ë†ğ‘£1(ğ‘¥) + ğœ‚(ğ‘¥)(ğ‘Ÿ(ğ‘¥, ğ‘¦) Ë†ğ‘£1(ğ‘¥)), where the learning rate ğœ‚(ğ‘¥) = (ğœŒ(ğ‘¥)ğ‘eff,1(ğ‘¥) + 1)1 naturally adapts to both policy shifts (via ğœŒ(ğ‘¥)) and statistical confidence (via ğ‘eff = ğ›¼(ğ‘¥) + ğ›½(ğ‘¥) + 1). This formulation highlights how our tracker balances new evidence against accumulated knowledge. For general rewards beyond binary ones, we can just use the same EMA formulation to directly track Ë†ğ‘£, rather than relying on ğ›¼ and ğ›½ in the binary cases. 4.2. Advantage Estimation and Policy Optimization SPO uses the trackers estimate Ë†ğ‘£ as baseline for advantage calculation in policy gradient algorithm. At iteration ğ‘–, for single reward ğ‘Ÿ(ğ‘¥, ğ‘¦) obtained with policy ğœ‹ğœƒğ‘– , the advantage is computed using the pre-update baseline (denoted with subscript 1): ğ´(ğ‘¥, ğ‘¦) = ğ‘Ÿ(ğ‘¥, ğ‘¦) Ë†ğ‘£1(ğ‘¥). Using the baseline from the previous step ensures that it is independent of the action taken at step ğ‘–, preserving the unbiasedness of the policy gradient estimate. Since ğ‘£1(ğ‘¥) is independent of ğ‘¦ ğœ‹ğœƒğ‘–(ğ‘¥), E[(ğ‘Ÿ ğ‘£ğ‘–1(ğ‘¥)) ğœƒ log ğœ‹] = ğ½(ğœƒ) [37]. Instead of normalizing advantages on per-prompt basis in group [27, 31], SPO normalizes them across an entire batch of prompts â„¬ [3, 17, 20, 25]. The normalized advantage ğ´(ğ‘¥, ğ‘¦) is computed as: (8) ğ´(ğ‘¥, ğ‘¦) = ğ´(ğ‘¥, ğ‘¦) ğœ‡â„¬ ğœâ„¬ , (9) where ğœ‡â„¬ and ğœâ„¬ are the mean and standard deviation of advantages in the batch {ğ´(ğ‘¥, ğ‘¦)}ğ‘¥â„¬. We then apply the advantage ğ´(ğ‘¥, ğ‘¦) to each token in the response sequence ğ‘¦ and update the policy parameters using standard PPO-Clip policy loss [25]3: ğ¿CLIP(ğœƒ) = Eğ‘ ,ğ‘¡ [ ( min ğœ‹ğœƒ(ğ‘ğ‘¡ ğ‘ ğ‘¡) ğœ‹ğœƒold(ğ‘ğ‘¡ ğ‘ ğ‘¡) ğ´ğ‘¡, clip ( ğœ‹ğœƒ(ğ‘ğ‘¡ ğ‘ ğ‘¡) ğœ‹ğœƒold(ğ‘ğ‘¡ ğ‘ ğ‘¡) , 1 ğœ€, 1 + ğœ€ )] . ) ğ´ğ‘¡ (10) Methods like Clip-Higher [40], Clip-Cov [9] and KL-Cov [9] to retain policy entropy are applicable here. Other policy optimization algorithms like CISPO [33] (similar to vtrace [10, 38]) and GSPO [42] (use sequence-level likelihood instead of token-level) are compatible with our advantage estimator. Advanced methods to control policy behaviors like ASPO [19] can be utilized to modulate the advantage values. We note that if we use no baseline (i.e., Ë†ğ‘£ = 0), it is an extremely simple and valid algorithm but may suffer from high policy gradient variance. 3The term PPO is frequently used with ambiguity. It may denote the entire algorithm suite (e.g., clipped policy and value losses), refer narrowly to just the clipped policy objective, or describe the broader training framework, including mechanisms like mini-batch updates. Work in Progress 5 Single-stream Policy Optimization Algorithm 1 Single-stream Policy Optimization 1: for iteration ğ‘– = 1, 2, . . . , ğ‘‡ do 2: For each ğ‘¥ ğ’³ , compute sampling weight ğ‘¤ğ‘–(ğ‘¥) according to Eqn. (11). Sample batch of ğµ prompts â„¬ğ‘– ğ’³ according to weights {ğ‘¤ğ‘–(ğ‘¥)}. ğ’Ÿ for each prompt ğ‘¥ â„¬ğ‘– do Sample action ğ‘¦ ğœ‹ğœƒğ‘–1( ğ‘¥) and observe reward ğ‘Ÿ(ğ‘¥, ğ‘¦) {0, 1}. Compute raw advantage ğ´(ğ‘¥, ğ‘¦) ğ‘Ÿ(ğ‘¥, ğ‘¦) Ë†ğ‘£1(ğ‘¥). Store (ğ‘¥, ğ‘¦, ğ´(ğ‘¥, ğ‘¦)) in ğ’Ÿ. Update tracker Ë†ğ‘£(ğ‘¥). 3: 4: 5: 6: 7: 8: 9: 10: 11: Normalize advantages: ğ´(ğ‘¥, ğ‘¦) (ğ´(ğ‘¥, ğ‘¦) ğœ‡â„¬ğ‘– Update ğœƒğ‘–1 to ğœƒğ‘– using mini-batches with policy gradient algorithm (e.g., PPO-Clip). )/ğœâ„¬ğ‘– . 4.3. Prioritized Prompt Sampling To further enhance data efficiency, SPO employs curriculum learning strategy by prioritizing prompts with the highest learning potential [24, 29]. At each iteration, we sample batch of prompts based on score that emphasizes prompts with high uncertainty, while ensuring minimum level of exploration. The sampling weight ğ‘¤ğ‘–(ğ‘¥) for prompt ğ‘¥ is defined as: ğ‘¤ğ‘–(ğ‘¥) Ë†ğ‘£1(ğ‘¥)(1 Ë†ğ‘£1(ğ‘¥)) + ğœ–. (11) The first term corresponds to the estimated standard deviation of Bernoulli outcome, which naturally allocates more weight to prompts that are neither almost always solved (Ë†ğ‘£ 1) nor almost always failed (Ë†ğ‘£ 0). The exploration bonus ğœ–, set to 0.05 by default, prevents curriculum collapse by ensuring that every prompt retains non-zero probability of being sampled, thereby maintaining broad coverage of the data distribution. The complete SPO training procedure is outlined in Algorithm 1. Figure 1: Illustrations of GRPO and the proposed SPO. Work in Progress Single-stream Policy Optimization 4.4. Advantages over GRPO Group-Free for Scalable Infrastructure. SPOs design is inherently group-free, significant advantage in distributed training frameworks for LLMs. Each sample, consisting of single stream of (prompt, response) pair, is self-contained data point for the policy update. GRPO, however, requires the generation and evaluation of an entire group of ğº samples for single prompt before any training signal can be computed. We provide our illustrations in Figure 1. In distributed setting, this introduces synchronization barrier: the processing of given prompt is not complete until all ğº responses have been generated. This is particularly problematic in the presence of long-tail generation times, where single slow response generation can stall the processing for its entire group. For constructing training batch, SPO only needs to collect ğµ independent (prompt, response) pairs, which is far more flexible and efficient than waiting for ğµ entire groups to complete. This makes SPOs architecture significantly more infrastructure-friendly and scalable. The advantage is amplified in agentic training, especially in settings that require multi-turn interactions with tools [8, 13] or long-horizon agent rollouts [39, 41]. The scale of these interactions can be substantial: state-of-the-art open-source models (gpt-oss-120b) may average 20 search turns per task [8], with other agentic sessions reaching over 40 tool calls and generating up to 150,000 tokens of context [13]. Adaptive Curriculum. To further enhance training efficiency, SPO integrates prioritized sampling scheme. This mechanism naturally creates an adaptive curriculum by focusing computational resources on prompts with the highest learning potential. This ensures that the models training is concentrated on the most informative examples at any given point in time. GRPO, in its standard formulation, typically relies on uniform sampling of prompts. This may waste computation on prompts that are already mastered or are currently too difficult to yield useful learning signals. While dynamic sampling [40] and repeat strategies [2] have been proposed to mitigate this issue, they often discard samples after generation, wasting computation. SPOs prioritized sampling addresses the scheduling problem before response generation, leading to more natural and efficient training process. More discussions on the inefficiency of dynamic sampling and the variance reduction of policy gradient are outlined in Appendix C, where we provide detailed analysis. 5. Experiments 5.1. Experimental Setup The SPO algorithm is broadly applicable in LLM reasoning tasks [31] and Agentic training. We evaluate Tool-Integrated Reasoning (TIR) [11, 19] scenarios, where the LLMs can utilize external Python interpreter to help solve hard problems. We conduct experiments using moderately sized LLM, Qwen38B [34]. For training data, we use the English subset from the DAPO dataset [40]. Only outcome reward is applied for RLVR, without the format rewards. We evaluate performance on the challenging math competition benchmarks, i.e., AIME 24, AIME 25, BeyondAIME [30], BRUMO 25 [4], and HMMT 25 [4]. See Appendix for training and evaluation details. We distinguish our goal from that of hill-climbing on benchmark leaderboards. The latter often necessitates resource-intensive and highly specialized techniques, including SFT from frontier models [21], mid-training [36], multi-stage RL pipelines [7, 16, 23], curated hard datasets with intricate processing [2, 26], test-time scaling techniques [12] and extremely large generation group sizes [41]. Our work, instead, Work in Progress 7 Single-stream Policy Optimization concentrates on the fundamental efficiency and scalability of the RL algorithm itself. 5.2. Empirical Comparison with GRPO Table 1: Comparison of GRPO, and SPO on five benchmarks using maj@32 and avg@32. Averages are shown in the last column. Bold indicates the better-performing method for each metric. Method AIME 24 AIME BeyondAIME BRUMO 25 HMMT 25 Average maj@32 avg@32 maj@32 avg@32 maj@32 avg@32 maj@32 avg@32 maj@32 avg@32 maj@32 avg@32 Qwen3-8B GRPO SPO 77.8 83.3 84.0 64.4 77.6 74.9 70.5 72.1 76.5 58.4 64.2 65.0 45.2 45.6 46.9 38.0 39.0 40. 55.1 56.7 64.0 49.4 56.9 59.0 36.8 44.2 47.5 30.3 40.9 40.6 57.1 60.4 63.8 48.1 55.7 56. (a) AIME 24 (b) AIME 25 (c) BeyondAIME (d) BRUMO 25 (e) HMMT 25 (f) Average Figure 2: Pass@ğ‘˜ plots comparing GRPO and SPO across five math competition benchmarks. Our experiments demonstrate that SPO outperforms the GRPO baseline on aggregate metrics when training the Qwen-8B model. As shown in Table 1, SPO achieves superior weighted average scores on both primary metrics. It obtains maj@32 of 63.8 compared to GRPOs 60.4, significant improvement of +3.4 percentage points (pp). This aggregate strength is driven by remarkable consistency, as SPO outperforms GRPO on the maj@32 metric across all five benchmarks. The performance gap is most pronounced on BRUMO 25, where SPO achieves substantial +7.3 pp (64.0 vs. 56.7). Further significant gains are seen on AIME 25 (+4.4 pp) and HMMT 25 (+3.3 pp points), underscoring the robustness of SPOs improvements. While GRPO remains competitive on the avg@32 metric in some cases, SPOs consistent and significant advantage in maj@32 suggests it learns more robust and repeatable solutions, key goal for reliable reasoning models. These findings are mirrored in the pass@ğ‘˜ performance shown in Figure 2. The weighted average curve (Figure 2f) shows clear and consistent advantage for SPO across all values of ğ‘˜, translating to an average improvement of approximately 2.4 pp. While the performance on avg@32 is more competitive on per-benchmark basis, SPOs strong overall performance underscores the stability and effectiveness of its learning signal. Work in Progress 8 Single-stream Policy Optimization 5.3. Analysis of Signal Efficiency and Stability (a) Ineffective Gradient Ratios (b) Advantage Variance Comparison Figure 3: Signal Efficiency and Stability Analysis of SPO vs. GRPO. (a) The majority of GRPO samples fall into degenerate groups (blue line), yielding zero advantage and no learning signal. In contrast, SPOs rate of near-zero advantages (red/green lines) increases as its value tracker learns, indicating successful learning, not wasted computation. (b) Comparison of unnormalized advantage variance. SPOs baseline (red) provides stable, low-variance signal, significantly reducing the variance of the raw reward (denoted as SPO No Baseline as green). GRPOs effective advantage (blue), calculated only on its non-degenerate samples, is highly volatile, even more so than the raw reward variance (green line, labeled SPO No Baseline), revealing the instability of its on-the-fly approach. To empirically assess the architectural advantages of SPO, we conduct two-part analysis of the unnormalized advantage signals produced by SPO and GRPO (Figure 3). First, we quantify complete signal loss arising from degenerate groups. Second, we measure the variance of the remaining learning signals. Together, these metrics characterize each methods efficiency and stability. Signal Efficiency and Information Loss. Figure 3a reports the fraction of ineffective samples. For GRPO (blue), the share of samples in degenerate groups rises from roughly 60% to over 80%, yielding zero advantage and no gradient. For SPO, we instead track the proportion of near-zero advantages under two diagnostic tolerances: exact zero (threshold=0.0; red) and small band ğ´ < 0.02 (threshold=0.02; green). Exact zeros remain rare throughout training (red near zero), while the ğ´ < 0.02 share (green) gradually increases as the value tracker Ë†ğ‘£ becomes more accurate and residuals shrink on mastered prompts. This trend is expected and desirable: it reflects accurate prediction rather than signal loss. Unlike GRPOs degenerate groups, these SPO samples are not discardedthey still produce well-defined gradients and contribute to learning. Notably, even under the stricter 0.02 tolerance, SPOs near-zero ratio remains far below GRPOs degenerate rate, underscoring SPOs efficient use of compute. Signal Stability and Advantage Variance. Figure 3b compares advantage variance across methods. As reference, the green line (SPO No Baseline) corresponds to raw rewards, i.e., the high-variance signal faced by vanilla policy gradient. SPOs history-informed baseline (red) delivers substantial, stable variance reduction of nearly 50%. For GRPO, computing variance only over non-degenerate samples (GRPO Effective, blue) reveals highly volatile signal with the largest variance among all conditions, exceeding even SPO No Baseline. We conclude that SPOs baseline is effective, yielding stable, lowvariance gradients, whereas GRPOs on-the-fly baseline is noisy and destabilizing when it produces signal. The apparent stability of GRPOs overall variance is driven by the prevalence of zero-variance Work in Progress 9 Single-stream Policy Optimization degenerate samples and thus reflects inefficiency rather than robustness. 5.4. Agentic Training Demonstrations We perform simulations to demonstrate the practical implications of SPOs group-free design in agentic training scenarios, where interaction times can be highly variable. Group-based methods like GRPO suffer from critical scalability bottleneck due to their inherent synchronization barrier, problem that is particularly acute in agentic tasks involving multi-turn tool use or long-horizon reasoning. Figure 4 illustrates this fundamental issue. In an idealized low-variance setting (Figure 4a), where all agentic trajectories complete in similar times, the group-based approach is efficient. However, in more realistic high-variance setting (Figure 4b) characterized by long-tail latencies, single slow-running trajectory (a straggler) can stall the entire group. In our simulation, while most samples finish in under 133 seconds, the group must wait 508 seconds for its slowest member. This bottleneck effect forces faster samples to remain idle, severely hindering training throughput and wasting computational resources. (a) Low-variance Group (b) High-variance Group Figure 4: The Bottleneck Effect in Group-Based Sampling. (a) In low-variance environment, sample completion times are predictable, and the group synchronization cost is minimal. (b) In realistic highvariance agentic environment, three slow trajectories (444ğ‘ , 508ğ‘ , and 409ğ‘ ) create severe bottleneck, forcing the entire group to wait and wasting the compute used for the six faster samples. SPOs group-free architecture directly resolves this inefficiency. Figure 5 compares the time required to assemble training batch of 24 samples using both strategies. The group-based approach (left), even when optimized by running 6 groups in parallel and selecting the 3 fastest, is still constrained by the slowest trajectory within those selected groups, taking 486ğ‘  to complete. In contrast, the group-free approach (middle) leverages asynchrony by starting 48 independent samples and simply collecting the first 24 to finish. In our simulated scenario, this process takes only 112ğ‘ , as it naturally filters out the slow outliers. As shown on the right, this architectural difference results in significant 4.35 speedup in this realistic agentic simulation. Simulations show that SPOs architecture can lead to significant throughput gains, making it more scalable and robust foundation for training on complex, long-horizon agentic tasks. Work in Progress 10 Single-stream Policy Optimization (a) Group-base (b) Group-free (c) Strategy Comparison Figure 5: Throughput Comparison: Group-Based vs. Group-Free. (a) group-based strategy, even when parallelized, is bottlenecked by its slowest group, taking 486ğ‘  to collect batch of 3 groups (24 samples). (b) group-free strategy collects the 24 fastest samples from larger pool of 48, completing the batch in just 112ğ‘  by avoiding stragglers. (c) The group-free approach achieves 4.35 speedup, demonstrating its superior efficiency for agentic training. 6. Conclusions We identified critical inefficiencies in group-based policy optimization methods for LLMs, namely computational waste from degenerate groups and scalability bottlenecks from synchronization. To address these, we proposed Single-stream Policy Optimization (SPO), principled return to the classic single-stream paradigm. SPO replaces the noisy, per-group baseline with persistent KL-adaptive value tracker and global advantage normalization, creating more stable and efficient learning signal. Our empirical results demonstrate that SPOs design is not merely simpler, but superior. It consistently outperformed GRPO on complex reasoning tasks while eliminating the systemic flaws of its group-based counterpart. By demonstrating that well-designed single-stream approach can surpass more complex methods, our work challenges the prevailing trend of adding incidental complexity to RL algorithms for LLMs. SPO provides robust, scalable, and efficient foundation for future research in agentic and reasoning model training, highlighting the enduring power of foundational reinforcement learning principles. Future work can focus on refining the best practices for applying SPO and exploring its limits, pushing its effectiveness to power the next generation of reasoning and agentic LLMs."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias GallÃ©, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet ÃœstÃ¼n, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in LLMs. arXiv preprint arXiv:2402.14740, 2024. [2] Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. POLARIS: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github. io/blog/2025/Polaris. Work in Progress 11 Single-stream Policy Optimization [3] Marcin Andrychowicz, Anton Raichuk, Piotr StaÅ„czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, LÃ©onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in onpolicy reinforcement learning? large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020. [4] Mislav BalunoviÄ‡, Jasper Dekoninck, Ivo Petrov, Nikola JovanoviÄ‡, and Martin Vechev. Matharena: Evaluating LLMs on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. [5] KiantÃ© Brantley, Mingyu Chen, Zhaolin Gao, Jason Lee, Wen Sun, Wenhao Zhan, and Xuezhou Zhang. Accelerating RL for LLM reasoning with optimal advantage regression. arXiv preprint arXiv:2505.20686, 2025. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-Nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. [8] Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, et al. BrowseComp-Plus: more fair and transparent evaluation benchmark of deep-research agent. arXiv preprint arXiv:2508.06600, 2025. [9] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. [10] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International conference on machine learning, pages 1407 1416. PMLR, 2018. [11] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. ReTool: Reinforcement learning for strategic tool use in LLMs. arXiv preprint arXiv:2504.11536, 2025. [12] Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. [13] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous RL. arXiv preprint arXiv:2508.07976, 2025. [14] Evan Greensmith, Peter Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):14711530, 2004. [15] Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, and Furu Wei. On-policy RL with optimal reward baseline. arXiv preprint arXiv:2505.23585, 2025. Work in Progress 12 Single-stream Policy Optimization [16] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. SkyWork Open Reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. [17] Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. REINFORCE++: An efficient RLHF algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025. [18] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [19] Heng Lin and Zhongwen Xu. Understanding Tool-Integrated Reasoning. arXiv preprint arXiv:2508.19201, 2025. [20] Zichen Liu, Anya Sims, Keyu Duan, Changyu Chen, Diyi Yang, Wee Sun Lee, and Min Lin. GEM: gym for generalist LLMs, 2025. URL https://axon-rl.notion.site/gem. [21] Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. AceReason-Nemotron 1.1: Advancing math and code reasoning through SFT and RL synergy. arXiv preprint arXiv:2506.13284, 2025. [22] Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. Part I: Tricks or traps? deep dive into RL for LLM reasoning. arXiv preprint arXiv:2508.08221, 2025. [23] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing o1-Preview with 1.5B model by scaling RL. https://pretty-radio-b75.notion.site/DeepScaleR-Sur passing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8c a303013a4e2, 2025. Notion Blog. [24] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [26] Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rStar2-Agent: Agentic Reasoning Technical Report. arXiv preprint arXiv:2508.20722, 2025. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. HybridFlow: flexible and efficient RLHF framework. arXiv preprint arXiv: 2409.19256, 2024. [29] Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. Work in Progress 13 Single-stream Policy Optimization [30] Bytedance Seed Team. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [31] DeepSeek Team. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [32] Kimi Team. Kimi K1.5: Scaling reinforcement learning with LLMs. arXiv preprint arXiv:2501.12599, 2025. [33] MiniMax Team. MiniMax-M1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. [34] Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=ySyClPaTKAq. [36] Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. OctoThinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025. [37] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. [38] Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, E1yk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, et al. LlamaRL: distributed asynchronous reinforcement learning framework for efficient large-scale LLM training. arXiv preprint arXiv:2505.24034, 2025. [39] Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, and Wei Yang. Agents play thousands of 3D video games. arXiv preprint arXiv:2503.13356, 2025. [40] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [41] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. GLM-4.5: Agentic, reasoning, and coding (ARC) foundation models. arXiv preprint arXiv:2508.06471, 2025. [42] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. [43] Haizhong Zheng, Yang Zhou, Brian Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for LLM reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025. Work in Progress 14 Single-stream Policy Optimization A. SPO Initialization We show the SPO initialization procedure in Algorithm 2. In the experiments, we use ğ‘›0 = 8 to have good estimation of initial baseline tracker. We ablate the setting where we use no offline estimation and rely on the online moving estimator. Our early-stage experiments show that without offline estimation the training is unstable in the first steps. Algorithm 2 SPO Initialization 1: Set initial effective sample size ğ‘0 = 1/(1 ğœŒmin). 2: for each prompt ğ‘¥ ğ’³ do 3: Collect ğ‘›0 outcomes {ğ‘Ÿ(ğ‘˜)}ğ‘›0 ğ‘˜=1 Compute initial value estimate Ë†ğ‘£0(ğ‘¥) = 1 ğ‘˜=1 ğ‘Ÿ(ğ‘˜). ğ‘›0 Set ğ›¼0(ğ‘¥) = ğ‘0 Ë†ğ‘£0(ğ‘¥) and ğ›½0(ğ‘¥) = ğ‘0 (1 Ë†ğ‘£0(ğ‘¥)). with an initial policy ğœ‹0. ğ‘›0 4: 5: Practically, one may concern about the extra cost during the offline estimation of Ë†ğ‘£0. We note that we share the offline estimation for our experiments4 so that people could skip this process and directly load our datasets, and there are datasets like Polaris [2] that pre-compute accuracy for Deepseek-R1-DistillQwen-7B [31]. The cost can be amortized across the experiments people run themselves, and we will share more (dataset, base_model) combinations to facilitate experiment efficiency. B. Batch Extensions We could adapt Single-stream Policy Optimization (SPO) into prompt-repetition scheme5, processing each prompt ğº times per batch with shared baseline estimator Ë†ğ‘£ to better handle sparse rewards. Our methods primary advantage over GRPO lies in its asynchronous nature, achieved by removing the group synchronization barrier. Treating repeated prompts as independent trajectories unlocks two key efficiency improvements. First, it enables robust handling of long-tail generation issues, as slow or problematic trajectories can be terminated early, discarded, or managed via partial rollouts [32] without delaying the entire batch. Second, it facilitates more flexible batching strategy. By over-sampling the number of initial prompts (e.g., by 50%), full training batch can be assembled from the first-finishing trajectories, allowing the optimization step to proceed immediately without waiting for stragglers. This design significantly reduces training latency compared to the rigid group synchronization required by GRPO. When tackling hard prompts, the batch extensions may help obtain learning signals more quickly. C. Comparisons against GRPO C.1. Inefficiency of Dynamic Sampling To address the information loss from degenerate sample groups (where all rewards are identical), methods like DAPO [40] employ dynamic sampling. This strategy continues generating responses for prompt until the collected set contains at least one success and one failure, guaranteeing non-zero advantage. While effective at ensuring learning signal, this approach can be extremely dataand time-inefficient. 4https://huggingface.co/datasets/dingzihan737/SPO_Qwen3-8B_DAPO_16k_Tool 5Batch SPO or BSPO Work in Progress 15 Single-stream Policy Optimization Note that when people report performance with dynamic sampling, the steps indicate the learning steps rather than the sampling steps, where the latter is normally multiple of the former (e.g., 5). We can formalize the expected computational cost. For prompt ğ‘¥ with true success probability ğ‘ = ğ‘‰ğœ‹(ğ‘¥), let ğ‘ be the number of samples required to obtain non-degenerate set. We have: E[ğ‘ ğ‘] = ğ‘ ( 1 + 1 1ğ‘ ) ( + (1 ğ‘) ) = 1 + 1 ğ‘ 1 ğ‘(1 ğ‘) 1. This cost grows hyperbolically as the policy becomes either proficient (ğ‘ 1) or incompetent (ğ‘ 0). For example, if policy has 10% success rate (ğ‘ = 0.1), the expected number of generations needed to collect both success and failure is E[ğ‘ ] 10.11. In contrast, SPO requires exactly one sample per prompt and uses its adaptive curriculum to actively de-prioritize these inefficient prompts, allocating resources to where learning is most effective. This makes SPO fundamentally more scalable and computationally efficient. C.2. Variance Reduction for Policy Gradient The per-sample policy gradient is ğ‘” = ğ´(ğ‘¥, ğ‘¦)ğœƒ log ğœ‹ğœƒ(ğ‘¦ğ‘¥), where the advantage ğ´ is an estimate of the expected return over baseline. The variance of this gradient, Var[ğ‘”], is key driver of training efficiency. We analyze how the construction of the advantage ğ´ leads to significant variance differences between GRPO and SPO. GRPOs High-Variance Group-Based Advantage: GRPO computes advantages by comparing outcomes within small group of ğº (ğº = 8, 16, ...) samples generated for the same prompt. The normalized advantage for sample ğ‘¥ with binary reward ğ‘Ÿ {0, 1} is ğ´GRPO = ğ‘Ÿğœ‡ğ’¢ , where both the baseline ğœ‡ğ’¢ (e.g., ğœğ’¢ +ğœ– the group mean 1 ğ‘— ğ‘Ÿğ‘—) and the standard deviation ğœğ’¢ are estimated from the same small group of ğº ğº samples. This coupled, small-sample estimation introduces three fundamental sources of variance: Noisy Baseline (Numerator): The baseline ğœ‡ğ’¢, estimated from only ğº samples, where ğº is small, is high-variance quantity. This inflates the variance of the unnormalized advantage (ğ‘Ÿ ğœ‡ğ’¢) by factor of (1 + 1 ğº ) compared to using an optimal baseline. Noisy Scaling (Denominator): The standard deviation ğœğ’¢, estimated from only ğº samples, is also highly variable. Scaling the gradient by this noisy random variable further increases total variance. Information Loss (Degeneracy): When all rewards in the group are identical (e.g., all 0s or all 1s), the advantage for every sample becomes zero, providing no gradient signal. This event, which occurs with probability ğ‘ğº(ğ‘) = ğ‘ğº + (1 ğ‘)ğº where ğ‘ = ğ‘‰ ğœ‹(ğ‘¥), effectively reduces the batch size and inflates variance by factor of 1/(1 ğ‘ğº(ğ‘)), an issue that is especially severe for easy (ğ‘ 1) or hard (ğ‘ 0) prompts. SPOs Low-Variance Decoupled Advantage: In contrast, SPO is designed to minimize these variance sources by decoupling the advantage calculation from the current group of samples. It uses an actionindependent baseline ğ‘ = Ë†ğ‘£(ğ‘¥) from historical tracker, which provides stable, low-variance estimate of the true success probability ğ‘. The advantage is simply ğ´SPO = batch_norm(ğ‘Ÿ(ğ‘¥, ğ‘¦) Ë†ğ‘£(ğ‘¥)). Crucially, SPO then applies global normalization [3, 22, 25], scaling all advantages in large batch of size ğµ ğº by single, stable standard deviation ğœbatch. This design avoids GRPOs pitfalls: the baseline ğ‘ is near-optimal, the normalization scaler ğœ is stable, and there is no systematic information loss from group-outcome degeneracy. Work in Progress 16 Single-stream Policy Optimization Quantitative Comparison: simplified ratio of the reward-term variance quantifies the difference: Var[ğ‘”]GRPO Var[ğ‘”]SPO 1 + 1 ğº 1 + 1 ğ‘eff+1 Baseline Noise 1 1 ğ‘ğº(ğ‘) Information Loss 1 + ğœ“ğº 1 + ğœ“â„¬ Normalization Noise . (12) Here, ğ‘eff is the effective sample count for SPOs tracker, and ğœ“ğº > 0 captures the excess variance from per-group, ğœ“â„¬ represents the excess variance introduced by estimating the normalization statistics (mean and standard deviation) from large global batch of size ğ‘â„¬ (ğœ“â„¬ 0). For moderately difficult prompt (ğ‘ = 0.5) with ğº = 8, the normalization noise dominates. However, for an easy/hard prompt (ğ‘ = 0.9/ğ‘ = 0.1), the information loss term dominates, and the ratio swells to 1.97. While increasing ğº in GRPO mitigates information loss, it does so at multiple generation cost and cannot fix the inherent noise from its small-sample baseline and scaling. SPO achieves lower variance more efficiently by design. D. Training and Evaluation Details All experiments in this paper are implemented on top of verl [28] and ReTool [11] for the tool-integrated reasoning setup. During training, we set the maximum response length to 16,384 tokens. The policy learning rate is fixed at 1 106. Following DAPO [40], we adopt the Clip-Higher mechanism, with clipping parameters ğœ€low = 0.2 and ğœ€high = 0.28, to balance exploration and exploitation. The sampling parameters are set to temperature 1.0, top-ğ‘ = 1.0, and top-ğ‘˜ = 1. The forgetting rate thresholds are chosen as ğœŒmin = 0.875 and ğœŒmax = 0.96, yielding window sizes ğ‘Šmin = 1 1 ğœŒmin GRPO rollouts are collected with multiple responses per prompt, and training mini-batch sizes are chosen such that 8 gradient updates are performed per rollout step. For fair comparison, the prompt batch size in SPO is set equal to the total number of responses in GRPO, as SPO generates only single response for each prompt. Specifically, GRPO uses prompt batch size of 256 with 8 responses per prompt and training mini-batch size of 256, while SPO operates on 2, 048 = 256 8 prompts. Both algorithms are set with maximum of 8 Python interpreter interaction turns. = 8 and ğ‘Šmax = 25. For evaluation on hard math competition benchmarks, i.e., AIME 24, AIME 25, BeyondAIME [30], BRUMO 25 [4] and HMMT 25 [4], we set sampling parameters to temperature 0.6, top-ğ‘ 0.95, and top-ğ‘˜ 20, as officially recommended6. We define binary reward function ğ‘Ÿğ‘–,ğ‘— such that response receives ğ‘Ÿğ‘–,ğ‘— = 1 if the final answer is correct, and ğ‘Ÿğ‘–,ğ‘— = 0 otherwise. The same reward function is consistently used during training for policy optimization and during evaluation. We set the maximum response token to 32,768. Given test set with ğ‘€ problems, and for each problem ğ‘– we independently sample ğ‘˜ responses with rewards {ğ‘Ÿğ‘–,1, ğ‘Ÿğ‘–,2, . . . , ğ‘Ÿğ‘–,ğ‘˜}, we define: avg@ğ‘˜: the expected correctness of an individual response: avg@ğ‘˜ = 1 ğ‘€ ğ‘€ ğ‘–=1 1 ğ‘˜ ğ‘˜ ğ‘—=1 ğ‘Ÿğ‘–,ğ‘—. pass@ğ‘˜: the probability of solving problem within ğ‘˜ attempts. Directly computing 1(max1ğ‘—ğ‘˜ ğ‘Ÿğ‘–,ğ‘— = 1) can lead to high variance. Following [6], we instead generate ğ‘› ğ‘˜ responses per problem, 6https://huggingface.co/Qwen/Qwen3-8B Work in Progress 17 Single-stream Policy Optimization count the number of correct ones ğ‘ ğ‘›, and use the unbiased estimator: pass@ğ‘˜ = 1 ğ‘€ [ ğ‘€ ğ‘–=1 1 ] ) , (ğ‘›ğ‘ğ‘– ğ‘˜ ) (ğ‘› ğ‘˜ where ğ‘ğ‘– denotes the number of correct responses for problem ğ‘–. maj@ğ‘˜: the correctness of the majority-voted answer [35]. This metric first identifies the most frequent answer among ğ‘˜ responses for each problem. The score is 1 if that modal answer is correct, and 0 otherwise. Let ğ‘ğ‘–,ğ‘— be the final answer string for the ğ‘—-th response to problem ğ‘–, and let ğ‘Ÿ() be the reward function for given answer string. The metric is defined as: maj@ğ‘˜ = 1 ğ‘€ ğ‘€ ğ‘–=1 ( ğ‘Ÿ mode{ğ‘ğ‘–,ğ‘—}ğ‘˜ ğ‘—=1 ) . Work in Progress"
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}