{
    "paper_title": "Attention Prompting on Image for Large Vision-Language Models",
    "authors": [
        "Runpeng Yu",
        "Weihao Yu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image, which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, Attention Prompting on Image improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 3 4 1 7 1 . 9 0 4 2 : r Attention Prompting on Image for Large Vision-Language Models Runpeng Yu , Weihao Yu , and Xinchao Wang National University of Singapore {r.yu,weihaoyu}@u.nus.edu, xinchao@nus.edu.sg Website https://yu-rp.github.io/api-prompting/ Code Space https://github.com/yu-rp/apiprompting https://huggingface.co/spaces/rp-yu/apiprompting Fig. 1: Comparison of the proposed Attention Prompting on Image (API) with the naive VQA.API provides hints for LVLM by simply overlying heatmap on the image. Corresponding author. 2 Yu et al. Abstract. Compared with Large Language Models (LLMs), Large VisionLanguage Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose new prompting technique named Attention Prompting on Image (API), which just simply overlays text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, API improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively. Keywords: Visual Prompting Large Vision-Language Model Large Multimodal Model"
        },
        {
            "title": "Introduction",
            "content": "Benefiting from the great progress of Large Language Models (LLMs) [1, 53, 54], Large Vision-Language Models (LVLMs) [2,4,9,18,26,32,66,67,81] also advances rapidly, represented by the seminal works GPT-4V [66] and LLaVA [32].1 They have been widely applied in tasks that involve understanding both visual and linguistic information, such as referring segmentation [72, 73], localization [72], captioning [55], open world 2D/3D understanding [52,57,66,82], and image editing [63, 66]. To enhance the performance of LVLMs, an economical method is to develop prompting techniques to elicit the models potential. Similar to textual prompting [24, 61], visual prompting2 [64, 65] is technique that enhances models understanding of images by directly adding annotations such as masks, circles, and marks to the image. This technique provides clear hints for visual perception by highlighting areas relevant to solving the problem, guiding the models attention to specific parts of the image, thus mitigating issues arising from complex scenes with distractions. It has been demonstrated that even simple visual cues like circles [48], arrows [66], or image tiling [30] can improve LVLMs ability to extract the required information correctly. Unlike methods that improve LVLM performance through adaptation or fine-tuning, visual prompting does not require the training process, thereby reducing the risks of overfitting and 1 Although also referred to as Multimodal Large Language Model (MLLM) or Large Multimodal Model (LMM) [32, 66], we use Large Vision-Language Model (LVLM) to refer to the models discussed in this paper, as we primarily utilizes the models vision and language capabilities. 2 In this work, we specifically use visual prompts to refer to masks, circles, marks, and other annotations added to images and use visual prompting to refer to technologies that employ visual prompts to assist in VQA tasks. Attention Prompting on Image for Large Vision-Language Models knowledge forgetting. Moreover, compared to textual prompts, visual prompting is more direct and precise in guiding the models focus to specific areas. Textual descriptions cannot succinctly describe an irregular area in an image or accurately indicate the location of specific object, and they also face issues with aligning textual coordinates with actual image pixels [66]. However, compared to research on textual prompts and LVLM fine-tuning, visual prompting is still underexplored. Previous visual prompting techniques focused on designing appropriate finegrained annotations to the image, aiming to highlight important local areas without impairing the models overall understanding of the image. Remarkably, FGVP [65] and SoM [64] are both based on segmentation masks [64]: The former blurs the image outside the segmentation mask while the latter overlays the image with set of including alphanumerics, masks, and boxes. However, all these methods sorely process the input images without considering the text query content. In other words, whatever the text query is, an images visual prompting results are the same. This can easily lead to mismatch between the prompted image and the text query, as different text queries for the same image require focus on different areas and necessitate different annotations. This mismatch may thereby limit the models ability to follow instructions accurately. To address this issue, in this paper, we propose novel prompting technique named Attention Prompting on Image (API), which just simply overlays textquery-guided attention heatmap on the original input image. Specifically, to generate text-guided attention heatmap for an image, we utilize an auxiliary LVLM that can accept both image and text as input. For image-text matching type (like CLIP [40]) as auxiliary model, we devised heatmap generation technique based on the decomposition of cls token similarity score. For the vision-languageinput text generation model (like LLaVA [32]), we generate the heatmap based on attention weights. Extensive experiments on various commonly used visionlanguage (VL) datasets verify the effectiveness of API in enhancing the VLMs perception of visual information. For example, API improves LLaVA-1.5 by 3.8%, 2.9%, and 2.3% on MM-Vet, LLaVA-Bench and MMMU benchmarks Our contributions can be summarized as follows: 1. We find that current visual prompting techniques sorely modify input images without considering the text query, limiting the models capability to follow instructions accurately. 2. To fill the gap, we propose the API method, exploring how to derive valuable attribution maps from various types of VLM models and utilize them as visual prompts to offer hints for visual perception, thereby boosting performance. 3. Our experiments demonstrate the effectiveness of our method across wide range of VLM models on various datasets. Moreover, our approach has also proven effective in addressing the issue of hallucination."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Visual Prompting for LVLM Originating from language models [33,34,44], the concept of prompting has been widely applied in vision models and vision language models to enhance the transfer learning and adaptation for various tasks (e.g., classification [21,41,77,79,80], 4 Yu et al. detection [13, 20, 58], segmentation [42] and generation [62]) and under various learning settings (e.g., few-shot learning [49], continual learning [59, 60, 78], domain adaptation/generalization [14, 37], unlearning [74], and long-tailed learning [11]). It is crucial to distinguish our work from soft prompts generated through gradient optimization and related prompt-tuning efforts. These prompts, concatenated in the form of continuous vectors to the token sequence of the VL models transformer layer input [8, 19, 50], or added to the input image as optimizable pixel patches and paddings [22, 45], depend on an additional learning process. Thus, they are strongly coupled with the model and dataset, lack generalizability, and are not intuitively interpretable. Moreover, since (part of) these prompts are incorporated at shallow layer, their optimization process involves gradient propagation throughout the entire branch, which is costly. Unlike these methods, the visual prompting studied in this paper is manually designed and automatically generated by extra LVLMs. It is interpretable and generalizable across different models and tasks. Visual prompting is specialized technique in vision models, especially for segmentation tasks [23,27,39]. Based on an additionally trained prompt encoder, manually annotated points, strokes, boxes, and irregular masks can provide these models with extra instructions to assist in controlling segmentation granularity or in facilitating instance selection. Recently, LVLMs have also been shown to understand manually added circles and color masks in images in zero-shot manner, focusing attention on highlighted areas without relying on additional encoder components [48, 68]. Unlike these works that explore the LVLMs ability to understand visual prompts, our method discusses how to use pretrained LVLMs to automatically generate visual prompts to enhance image readability. The two methods most related to ours are [64] and [65], which modify masks generated by segmentation models to construct visual prompts to improve LVLMs performance in segmentation and grounding tasks. Our method differs fundamentally from theirs in that we use LVLMs to construct visual prompts. This leads to two main differences in functionality and applicability. 1) For single image, the visual prompts generated by [64, 65] are invariant, as these models rely on fixed segmentation models. In contrast, with different text queries, our method can adapt and generate distinct visual prompts to emphasize different areas as required. 2) The visual prompts generated by [64, 65] are essentially instance-specific proposals for segmentation and grounding tasks, focusing on enhancing the LVLMs grounding capability. Conversely, our visual prompts aim to highlight important areas needed to address text queries, thereby improving the LVLMs performance in general Visual Question Answering tasks. 2.2 Self-Reflection and Ensemble Our method involves LVLM at two stages: once for generating visual prompts and once for performing inference. When the same LVLM is used at both stages, our approach can be seen as method to enhance LVLM performance using selfreflection technology. The concept of Self-Reflection originated from LLMs [38, 47] but can be directly transferred to LVLMs. Self-Reflection scheme improves model performance by repeatedly answering query and iteratively updating the answer. The Self-Reflection process involves using self-evaluation [3], selfchecking [36], self-feedback [35], feedback from the external environment [7, 46], and even previous answers themselves [76] as hints to input into the model for Attention Prompting on Image for Large Vision-Language Models 5 it to answer the question again. Unlike these works, where the medium of selfreflection is text, our method employs visual prompting to achieve Self-Reflection in the pixel space. When different LVLMs are used at two stages, our method can be considered form of model ensemble, where the knowledge of the first VLM is ensembled into the second VLM in the form of visual prompts. In tasks with standard outputs, deep learning model ensemble involves aggregating outputs from multiple models [16]. However, output aggregation is invalid in generation tasks. In LLMs and LVLMs, model ensemble is achieved in the form of sequential or stage-wise use between auxiliary and inference models. The final inference model can enhance its performance by incorporating outputs from other auxiliary models into its input. The auxiliary model outputs used as inputs for the inference model can be responses from another language model [12] or textualized outputs from vision models or vision-language models (image captions, category names) [28,71]. Unlike these works, our method uses visual rather than textual signals for ensembling. Furthermore, our approach does not ensemble the final hard outputs of auxiliary models but their visual cues used during the inference process. This soft knowledge ensemble provides valuable auxiliary information and mitigates error accumulation introduced by mistakes in auxiliary model inference."
        },
        {
            "title": "3 Method",
            "content": "Large Vision Language Model takes an image RHW 3 and text query as inputs, generating an output text = (I, i). During the inference process using API, instead of being directly fed into , the original image undergoes an additional annotation operation A, resulting in an image = A(I, i) that has been overlaid with heatmap Φ. Subsequently, the annotated image and the original query are input into the LVLM model , producing the output = (I a, i). The overall framework of the method is shown in Fig. 1. In our method, the annotation process comprises two steps. The first step involves using an auxiliary LVLM model to establish an initial attribution map Ψ between the text query and each patch of the image. This attribution map indicates which patches in the image are more relevant to or which patches should be paid more attention to for answering i. In our method, there are no additional constraints on the LVLM g; if the inference LVLM is accessible and capable of performing the annotation operation A, then the LVLM used to generate the attribution map can be the same as , i.e., = . Alternatively, could be different LVLM to introduce knowledge from other models to enhance functionality, i.e., = . Moreover, due to the diversity of LVLM models, we do not necessarily use the attention map as our attribution map. For example, for the image-text matching model, experiments have shown that using the attention map as the attribution map has suboptimal results. After obtaining the attribution map Ψ , the second step in the annotation process is to convert it into suitable Φ and apply it to the original image using alpha blending. Various LVLM models can be utilized to generate attribution maps. We discuss two prevalent and representative LVLM models: CLIP [40], exemplifying image-text matching models, and LLaVA [32], representing vision-languageinput text generation models. 6 Yu et al."
        },
        {
            "title": "3.1 Obtaining Attribution Map from CLIP",
            "content": "The CLIP model, gclip, consists of an image encoder and text encoder, calculating the similarity between an image and text query in the image-language latent space, sim( ˆI, ˆT ), where ˆI = gimg clip (T ). This similarity measure evaluates the correlation between the entire image and the query. To obtain the attribution map value from the text query to each image patch, we decompose the output image-level similarity ˆI and then calculate the similarity of each patchs output with the ˆT . clip (I) and ˆT = gtext The decomposition process is as follows. Due to the presence of residual connections, the final output of the vision tower, ˆI, actually includes influences from each layer. Consequently, ˆI can be expressed as linear combination of the values at the class token positions from each layer ˆI = L((cid:2)Z 0 cls (cid:3)) + (cid:88) l=1 (cid:105) (cid:104) MSAl(Z l1) L( ) + cls (cid:88) l=1 (cid:104) L( (cid:105) MLPl( ˆZ l) ), cls (1) where denotes the number of transformer layers within the vision encoder, with MSA and MLP representing the Multihead Self-Attention structure and the Multi-Layer Perceptron structure within the transformer, respectively; represents the linear transformation that includes the fully-connected layer and the normalization operations performed after the transformer structure, before calculating the similarity score; signifies the input token sequence for the l-th transformer layer; and [Z]cls indicates the value of the cls token within the token sequence Z. These output cls tokens are aggregated through residual connections to form the output of the vision encoder. As evidenced in [17, 32], among these summation terms, the outputs of the last few layers of MSA play decisive role, while the contributions from the outputs of the shallow MSA layers, the outputs of MLP, and the 0 cls term, which is independent of the input image, can be considered negligible to the final measurement of similarity. Therefore, the similarity sim( ˆI, ˆT ) can effectively be approximated by calculating the similarity between ˆT and the aggregated outputs of MSAs in the deeper layers : sim( ˆI, ˆT ) sim( (cid:88) (cid:104) L( l=L MSAl(Z l1) (cid:105) cls ), ˆT ), (2) where represents predefined starting layer index. To further calculate the attribution of the text query to each patch, inspired by [17], we unfold the operations of the Multihead Self-Attention, obtaining (cid:104) MSAl(Z l1) (cid:105) cls = = (cid:88) (cid:104) A(l,h)V (l,h)W (l,h)(cid:105) + Bl cls T (cid:88) (cid:34) (cid:88) t=1 (cid:124) cls,t (l,h) A(l,h) t,: (l,h) + (cid:35) Bl 1 HT (cid:88) t=1 ηl t, (cid:123)(cid:122) The MSA output corresponding to (cid:125) (3) (4) where A(l,h), (l,h) are the attention map and the value matrix in the l-th layer corresponding to the h-th head, respectively; (l,h) is the weight matrix in the the t-th patch(token) Attention Prompting on Image for Large Vision-Language Models 7 l-th layer used to merge the multiple attention heads and corresponds to the h-th head; B(l) is the bias matrix in the l-th layer used to merge the multiple attention heads; A(l,h) cls,t denotes the attention value of the class token towards the t-th token in A(l,h), and (l,h) represents the t-th row of (l,h); and are the number of attention heads and the number of tokens, respectively; and the value equals the number of patches plus one. t,: Consequently, by summing across layers and incorporating the final linear transformation, we obtain ψt (cid:80)L l=L L(ηl t), which is the direct influence of the t-th patch to the similarity in Eq. (2), allowing us to calculate the similarity between text query and the t-th image patch. Accordingly, the attribution map Ψ cls RP is defined as Ψ cls i,j sim(ψt, ˆT ), where = 1 + + (i 1). (5) By decomposing the cls token, we can identify which patches are more relevant to the query. This approach is particularly effective when the query contains specific entities, allowing for accurate grounding. However, in complex Visual Question Answering (VQA) tasks, there are often no explicit entities mentioned in the query, or the logic and analysis process involved in answering the question may rely on entities that are not explicitly mentioned in the query. To address this issue, we also define another complementary attribution map Ψ comp using the CLIP model. This map is designed to capture patches that have potential or implicit relevance to the query. We experimentally observe that, in the vision transformer of CLIP, the similarity score of the query feature ˆT and tokens other than the cls token in the final layer can (inversely) select the important regions. Patches corresponding to the image background or large monochrome areas have significantly higher similarity score with ˆT than those tokens representing specific entities (which may not necessarily appear in the query). This phenomenon is similar to observations made in [10]. Drawing on analyses of the transformers mechanism in [10, 43], potential explanation is that these blank tokens, lacking valuable information themselves, are treated by the transformer as registers. The transformer initially utilizes them to store information from other informative tokens, subsequently filtering and aggregating this stored information to the class token via the attention mechanism to formulate the final prediction. Therefore, tokens other than the class token, with high similarity score to ˆT , represent patches with low information content that can be disregarded. We define the complementary attribution map as follows Ψ comp i,j 1 sim(L(Z ), ˆT ), where = 1 + + (i 1), (6) where is the t-th output token from the last transformer layer. The comt plementary attribution map is inversely related to similarity, suggesting that patches lacking information are ignored, retaining only those with potential relevance. Thus, we obtain two attribution maps that complement each other: Ψ cls explicitly identifies patches directly related to entities in the query but may miss some potentially relevant patches. Ψ comp equally identifies all patches with potential relevance but lacks specificity and cannot highlight those directly related to entities in the query. 8 Yu et al. By integrating the two attribution maps through the following operation, we obtain the final attribution map for CLIP: Ψi,j Ψ cls i,j + Ψ comp i,j Ψ comp i,j Ψ cls i,j . (7) This integration can be considered as soft OR operation (a detailed mathematical explanation is provided in the Appendix). This ensures that the final attribution map highlights patches directly related to entities within the query while retaining those with potential or implicit relevance, merely reducing the weights of patches that do not contain important information for the query. If the function of the final attribution map were described as an algorithm, then this attribution map would, in the first step, apply mask to all non-informative patches, making them less considered in subsequent VQA processes while leaving other patches unaffected; and, in the second step, for patches not masked, if patch is directly related to the entities in the query, it further highlights this patch. 3.2 Obtaining Attribution Map from LLaVA The LLaVA model is an auto-regressive vision-language-input text generation model that utilizes Multihead Self-Attention to extract information from text queries and image patches, predicting the following tokens. Given text token }N sequence of length , text = {Z text t=1, and an image token sequence of length , img = {Z img }P t=1 , LLaVA generates new token sequence of length t=1. We directly use the attention weight between token out , out = {Z out }M and each image token as out attribution to that image patch. Similar to the strategy for the CLIP model, we select attention maps from the deeper layer to extract attention weights. The final attribution map is averaged over the entire generated token sequence and all attention heads. Formally, the attribution map Ψ is defined as t Ψi,j 1 (cid:88) (cid:88) m=1 h=1 A( L,h) m,t , where = + (i 1). (8) In the definition, A( L,h) is again the attention map in the L-th layer corresponding to the h-th head, where is set to be hyper-parameter; for notation simplicity, A( L,h) here is submatrix of the entire attention map and only includes cross attention between out and img; A( L,h) still denotes the attention value from the m-th token to the t-th token. m,t 3.3 From Token Space to Pixel Space The attribution map Ψ RP is generated in the token space. We first resize it back to the pixel space to obtain the raw heatmap ˆΦ Resize(Ψ ). Due to the square shape of the patches, the mask pattern in ˆΦ also appears rectangular. To mitigate the issue that the rectangular mask pattern does not align with the objects irregular shape, we apply mean filter to obtain the final heatmap Φ Meank( ˆΦ), where is the kernel size of the filter. The final heatmap Φ is then overlaid on the original image by using it as the alpha channel, resulting in the final image after annotation a. Attention Prompting on Image for Large Vision-Language Models 9 Table 1: Comparison of our method with previous textual and visual prompting methods for various LVLMs. The best result are marked for each model-dataset pair. Inference Model Prompting Method LLaVA CogVLM GPT-4V (1106) Gemini w/o prompt +Step-by-Step FGVP (Mask) FGVP (RBM) SoM Ours (CLIP) Ours (LLaVA) w/o prompt +Step-by-Step FGVP (Mask) FGVP (RBM) SoM Ours (CLIP) Ours (LLaVA) w/o prompt +Step-by-Step FGVP (Mask) FGVP (RBM) SoM Ours (CLIP) Ours (LLaVA) w/o prompt +Step-by-Step FGVP (Mask) FGVP (RBM) SoM Ours (CLIP) Ours (LLaVA) VisWiz TextVQA MMMU MM-Vet MME LLaVA-Bench Dataset 60.93 35.15 71.9 48.32 73.5 (+1.6) 60.98 (+0.1) 48.22 (0.1) 35.40 (+0.3) 57.4 (<- 5) 56.89 (4.0) 39.38 (<- 5) 36.14 (+1.0) 57.4 (<- 5) 61.22 (+0.3) 33.91 (<- 5) 35.00 (0.2) 54.16 (<- 5) 56.1 (<- 5) 18.81 (<- 5) 35.57 (+0.4) 61.26 (+0.3) 48.78 (+0.5) 37.52 (+2.4) 35.3 (+2.5) 87.2 (+1.7) 74.1 (+2.2) 61.35 (+0.4) 48.79 (+0.5) 36.95 (+1.8) 36.6 (+3.8) 86.3 (+0.8) 74.8 (+2.9) 33.7 (+0.9) 84.2 (1.3) 31.0 (1.8) 75.8 (<- 5) 25.0 (<- 5) 81.4 (4.1) 26.4 (<- 5) 75.4 (<- 5) 32.8 85. 36.43 78.41 50.8 53.54 40.7 (<- 5) 28.86 (<- 5) 42.53 (<- 5) 29.19 (<- 5) 49.1 (1.7) 53.55 (+0.0) 63.69 (<- 5) 35.34 (1.1) 48.1 (2.7) 53.68 (+0.1) 65.51 (<- 5) 36.55 (+0.1) 51.00 (2.5) 36.64 (<- 5) 35.55 (0.9) 38.9 (<- 5) 54.01 (+0.5) 78.99 (+0.6) 37.05 (+0.6) 52.5 (+2.9) 82.3 (+0.5) 53.3 (+2.5) 52.0 (+2.4) 82.7 (+0.9) 52.4 (+1.6) 54.34 (+0.8) 78.85 (+0.4) 36.95 (+0.5) 48.0 (1.6) 63.0 (<- 5) 44.1 (<- 5) 80.4 (1.4) 48.2 (1.4) 82.0 (+0.2) 31.2 (<- 5) 78.0 (3.8) 49.6 81. 59.40 50.60 50.55 67.00 84.3 102. 55.75 (3.6) 49.85 (0.7) 48.33 (2.2) 62.50 (4.5) 82.0 (2.3) 102.6 (+0.6) 59.2 (<- 5) 69.30 (+9.9) 45.95 (4.6) 43.88 (<5) 61.00 (<5) 65.0 (<- 5) 92.5 (<- 5) 69.40 (+10.0) 46.15 (4.4) 52.50 (+1.9) 60.20 (<5) 79.6 (4.7) 56.1 (<- 5) 65.30 (+5.9) 45.00 (<5) 48.33 (2.22) 58.90 (<5) 65.8 (<- 5) 69.50 (+10.1) 51.50 (+0.9) 50.96 (+0.4) 67.70 (+0.7) 85.3 (+1.0) 103.3 (+1.3) 71.01 (+11.6) 50.80 (+0.2) 51.38 (+0.8) 67.10 (+0.1) 84.7 (+0.3) 103.6 (+1.6) 35.11 56.68 81.5 50.28 40.5 (<- 5) 22.82 (<- 5) 21.51 (<- 5) 36.37 (+1.3) 64.2 (<- 5) 52.88 (+2.6) 40.81 (<- 5) 34.88 (0.2) 82.3 (+0.8) 53.01 (+2.7) 45.67 (<- 5) 34.08 (1.0) 51.25 (+1.0) 27.29 (<- 5) 34.77 (0.3) 64.5 (<- 5) 58.58 (+8.3) 59.07 (+2.4) 37.71 (+2.6) 60.5 (+1.5) 80.2 (+1.6) 85.2 (+3.7) 82.3 (+0.8) 58.17 (+7.9) 58.35 (+1.7) 38.16 (+3.1) 60.1 (+1.1) 80.0 (+1.4) 30.6 (<- 5) 29.8 (<- 5) 45.8 (<- 5) 71.0 (<- 5) 52.0 (<- 5) 77.4 (1.2) 34.4 (<- 5) 69.8 (<- 5) 59. 78."
        },
        {
            "title": "4 Experiments",
            "content": "We show the main experimental results in this section. More experiments and implementation details are in the appendix. 4.1 Comprehensive VQA Tasks Datasets. Experiments are conducted on 6 datasets: VisWiz [5], TextVQA [51], MMMU [70], MME [15], MM-Vet [69], and LLaVA-Bench [32]. The performance on the first four datasets is evaluated using matching accuracy with the ground truth response. The performance of the latter two datasets is measured using the GPT-based evaluation scores. LVLMs. Experiments are conducted using two open-source models: CogVLM [56] and LLaVA [31], and two commercial models: GPT-4V [66] and Gemini [52]. Due to GPT-4Vs token limit, following the experiment protocol in the previous work [64] when conducting experiments with GPT-4V, for VisWiz, TextVQA, and MMMU, we randomly selected 200 images from the dataset to verify our method. Because, about 50 questions on MM-Vet are categorised as related to personal identification or brand evaluation due to GPT-4Vs safety policy and are refused to answers. Therefore, we evaluated our methods performance only on the remaining questions. Comparison. We compare with the following methods: (1) naively feeding the query and image to the model without any prompt; (2) using Lets think step Yu et al. Table 2: Ablation study on the auxiliary VLM Scale. The best result are marked for each auxiliary model-dataset pair."
        },
        {
            "title": "MMMU",
            "content": "w/o prompt 35."
        },
        {
            "title": "MME",
            "content": "85.50 CLIP-ViT-B 36.03 (+0.88) 83.50 (2.00) 36.21 (+1.09) 83.50 (2.00) CLIP-ViT-L CLIP-ViT-L-336 37.52 (+2.37) 87.16 (+1.66) 35.86 (+0.71) 85.66 (+0.16) LLaVA-7B LLaVA-13B 36.95 (+1.80) 86.34 (+0.84) by step as prompt to trigger the models chain-of-thought process, method that has been proven to significantly improve zero-shot reasoning performance for LLMs [25]; and (3) two visual prompting methods designed for LVLMs, FGVP [65] and SoM [64]. FGVP is designed to generate diverse visual prompts. We compared the most straightforward method of using mask as visual prompt and the best-performing method of using Reverse Blur Mask (RBM) as visual prompt. Performance improvements/decrements listed in the table are calculated relative to the w/o prompt method. The main observations from the experimental results are as follows: (1) Our method consistently achieves the best performance across all datasets-LVLM pairs in Tab. 1. Regardless of whether the CLIP or LLaVA is used as the auxiliary model, our method leads to performance improvements. For LLaVA, CogVLM, GPT-4V, and Gemini, the average improvements relative to w/o prompt are 1.94%, 1.38%, 1.76%, and 3.42%, respectively. Our method performs particularly well on Gemini+VisWiz, with an average improvement of 8.1%. Excluding it, our method appears more effective for open-ended questions, with an average improvement of 2.20% on MM-Vet and LLaVA-Bench, while the average accuracy increase on multiple-choice and true-false datasets is 1.18%. (2) The lets think step by step approach, which is significantly effective in LLMs, does not perform well in VQA tasks. We suspect this is because this method cannot enhance the LVLMs visual perception capabilities and may even exacerbate LVLMs hallucination due to its language-oriented prompt nature. (3) Previous visual prompting methods, lacking the ability to adapt to different queries, do not perform well on VQA tasks. Our method is clearly superior to them. This indicates that indiscriminately annotating objects in an image does not effectively assist the model in performing VQA tasks. Visual prompting methods need the ability to adapt to queries. 4.2 Ablation Studies We identify three important factors affecting the performance of our method and conduct ablation studies on them. The Power of the Auxiliary Model. On the MMMU and MME datasets, we used CLIP models and LLaVA models of different scales to generate heatmaps, with LLaVA serving as the inference model, to compare performance. The results are shown in Tab. 2. As the scale of the auxiliary model increased, the performance of our method also improved. Both increasing the depth of the auxiliary Attention Prompting on Image for Large Vision-Language Models 11 Table 3: Ablation study on the mean filter kernel size."
        },
        {
            "title": "MME",
            "content": "w/o filter (kernel size = 1) 3 7 36.09 (+0.94) 83.70 (1.80) 36.95 (+1.80) 86.20 (+0.70) 36.32 (+1.17) 87.14 (+1.74) w/o prompt (kernel size 2W= 2H) 35.15 85. Table 4: Ablation study on the Transformer layer for attribution map extraction. The best result are marked for each auxiliary model-dataset pair. Mask Model Layer Index MMMU w/o prompt n/a 35.15 MME 85.50 CLIP LLaVA 23 22 20 15 23 22 20 15 36.32 (+1.17) 87.16 (+1.66) 37.52 (+2.37) 84.80 (0.70) 37.12 (+1.97) 83.20 (1.30) 36.14 (+0.99) 83.16 (1.34) 36.15 (+1.00) 83.10 (1.40) 36.49 (+1.35) 83.00 (1.50) 36.95 (+1.80) 86.34 (+0.84) 36.32 (+1.17) 83.16 (2.34) model or reducing the patch size to generate attribution map with finer granularity prove to be effective for improving the performance of our method. When the capability of the auxiliary model is insufficient, the masks generated by it could even be detrimental. The Kernel Size of the Mean Filter. To mitigate the limitations of rectangular mask patterns when highlighting irregularly shaped objects, we incorporated mean filter into our method. We conducted ablation studies on different kernel sizes on the MMMU and MME datasets with LLaVA as the inference model. The results are shown in Tab. 3. Without the mean filter, heatmaps with rectangular patterns could potentially harm the final tasks performance. The optimal kernel size varied across datasets, due to the different image complexity and question complexity. The Layer for Attribution Map Extraction. Another factor affecting our methods performance was the layer used for extracting the attribution map. Although we knew that deeper layers, which contain higher-level semantic information, should be used, the specific choice of layer also impacted our methods performance. We conduct ablation on MMMU and MMe datasets using LLaVA as inference model. The results are shown in Tab. 4. For the CLIP model, the last two layers are more effective. However, for LLaVA, directly using the attention maps from the last two layers do not yield good results; the best performance occurred when mid-to-late layer was used, such as 20-th layer for LLaVA-13B. 12 Yu et al. Table 5: The comparison between our method and textual self-reflection method and their combination."
        },
        {
            "title": "Prompt Method",
            "content": "LLaVA-Bench w/o prompt textual self reflection ours (LLaVA) 71.90 72.90 (+1.00) 74.80 (+2.90) + reflection via re-emphasize 72.70 (+0.80) 76.10 (+4.20) + reflection via evaluation"
        },
        {
            "title": "4.3 Self-Reflection",
            "content": "When the auxiliary LVLM and the inference LVLM are the same, our method can be seen as having two-round chat with the LVLM. The first round generates an annotated image, where the highlighted areas represent what the LVLM considers important, embedding the LVLMs process of extracting visual information. The second round conducts inference based on the generated annotated image, allowing the LVLM to perform Self-Reflection and refine its previous process of visual information extraction. Unlike previous Self-Reflection methods using text as medium in LLMs, under the API framework, all information related to the first answer is stored in the annotated image, and the text response from the first round is not provided to the model in the second round. As new perspective of Self-Reflection, we explore two questions: (1) Can visual mediums also achieve effective Self-Reflection? To answer this, we compared text-based Self-Reflection and our method using LLaVA as the inference model on the LLaVA-Bench dataset. The results in Tab. 5 show that our method achieves better performance than text-based Self-Reflection, proving that visual mediums can effectively facilitate Self-Reflection. The second question is: (2) Can we more effectively utilize visual mediums for Self-Reflection? Generally, Self-Reflection techniques involve two steps int the second round: first, evaluating the previous answer, and second, combining the evaluation to re-answer the question. However, in our framework, the evaluation process is not included, and the model directly proceeds to inference. Therefore, we designed new inference process. We input the annotated image and the question into the VLM, prompting it to judge whether the highlighted areas in the image support the answer to the question. If yes, the answer is generated using the annotated image; if not, the answer is generated using the original image. The result (the last row of Tab. 5) shows that this strategy further improves our method. Conversely, when we do not allow the model to perform evaluation and emphasize that the answer lies within the highlighted areas of the annotated image, performance decreases (second to last row in Tab. 5). This also proves the importance and effectiveness of the evaluation process when using visual mediums for Self-Reflection. 4.4 Other Discussion Hallucination. We also explore our methods ability to assist LVLM in overcoming hallucinations. We conduct two experiments. First, on VisWiz, we calculated the accuracy with which our method and the baseline identify the unanswerable Attention Prompting on Image for Large Vision-Language Models 13 Table 6: The performance of our method on hallucination datasets. Prompt Method VisWiz-Unanswerable"
        },
        {
            "title": "POPE",
            "content": "w/o prompt Ours (CLIP) Ours (LLaVA) 81.41 83.83 (+2.42) 85.26 (+3.85) 81.00 82.81 (+0.81) 83.52 (+2.52) questions. These questions often involve information that does not exist in the image, thus the responses to these questions are based on hallucination. Second, we conduct experiments on subset of commonly used LVLM hallucination dataset POPE [29]. The experimental results presented in Tab. 6 demonstrate that our method also has the ability to mitigate hallucination."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce novel visual prompting technique called Attention Prompting on Image (API), which incorporates an auxiliary LVLM to generate an attention heatmap on the image dependent on text query. Our extensive experiments demonstrate the advantages of our prompting method for different LVLMs on various benchmarks. Additionally, our approach offers new insights into using visual signals for LVLM ensembling and LVLM self-reflection."
        },
        {
            "title": "Acknowledgement",
            "content": "This project is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006). 14 Yu et al."
        },
        {
            "title": "References",
            "content": "1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 2. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716 23736 (2022) 3. Asai, A., Wu, Z., Wang, Y., Sil, A., Hajishirzi, H.: Self-rag: Learning to retrieve, generate, and critique through self-reflection. CoRR (2023) 4. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al.: Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 (2023) 5. Bigham, J.P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R.C., Miller, R., Tatarowicz, A., White, B., White, S., Yeh, T.: Vizwiz: nearly real-time answers to visual questions. In: Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology. p. 333342 (2010) 6. Burns, C., Izmailov, P., Kirchner, J.H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., Wu, J.: Weak-to-strong generalization: Eliciting strong capabilities with weak supervision (2023) 7. Chen, X., Lin, M., Schärli, N., Zhou, D.: Teaching large language models to selfdebug. CoRR (2023) 8. Chowdhury, S., Nag, S., Manocha, D.: Apollo : Unified adapter and prompt learning for vision language models. In: Conference on Empirical Methods in Natural Language Processing, EMNLP (2023) 9. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=vvoWPYqZJA 10. Darcet, T., Oquab, M., Mairal, J., Bojanowski, P.: Vision transformers need registers. CoRR (2023) 11. Dong, B., Zhou, P., Yan, S., Zuo, W.: LPT: long-tailed prompt tuning for image classification. CoRR (2022) 12. Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.: Improving factuality and reasoning in language models through multiagent debate. CoRR (2023) 13. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for open-vocabulary object detection with vision-language model. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) (2022) 14. Fahes, M., Vu, T., Bursuc, A., Pérez, P., de Charette, R.: Pøda: Prompt-driven zero-shot domain adaptation. CoRR (2022) 15. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., Li, K., Sun, X., Ji, R.: MME: comprehensive evaluation benchmark for multimodal large language models. CoRR abs/2306.13394 (2023) 16. Ganaie, M.A., Hu, M., Malik, A.K., Tanveer, M., Suganthan, P.N.: Ensemble deep learning: review. Eng. Appl. Artif. Intell. 115, 105151 (2022) 17. Gandelsman, Y., Efros, A.A., Steinhardt, J.: Interpreting CLIPs image representation via text-based decomposition. In: International Conference on Learning Representations (ICLR) (2024) 18. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., et al.: Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010 (2023) 19. Gao, T., Fisch, A., Chen, D.: Making pre-trained language models better fewshot learners. In: Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP (2021) Attention Prompting on Image for Large Vision-Language Models 15 20. Guo, Z., Dong, B., Ji, Z., Bai, J., Guo, Y., Zuo, W.: Texts as images in prompt tuning for multi-label image recognition. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) (2023) 21. Jia, M., Tang, L., Chen, B., Cardie, C., Belongie, S.J., Hariharan, B., Lim, S.: Visual prompt tuning. In: European Conference on Computer Vision (ECCV) (2022) 22. Jia, M., Tang, L., Chen, B., Cardie, C., Belongie, S.J., Hariharan, B., Lim, S.: Visual prompt tuning. In: European Conference on Computer Vision (ECCV) (2022) 23. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. In: arXiv (2023) 24. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. Advances in neural information processing systems 35, 2219922213 (2022) 25. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. Advances in neural information processing systems 35, 2219922213 (2022) 26. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023) 27. Li, F., Jiang, Q., Zhang, H., Ren, T., Liu, S., Zou, X., Xu, H., Li, H., Li, C., Yang, J., Zhang, L., Gao, J.: Visual in-context prompting. CoRR (2023) 28. Li, S., Du, Y., Tenenbaum, J.B., Torralba, A., Mordatch, I.: Composing ensembles of pre-trained models via iterative consensus. In: International Conference on Learning Representations (ICLR) (2023) 29. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hallucination in large vision-language models. In: The 2023 Conference on Empirical Methods in Natural Language Processing (2023), https://openreview.net/ forum?id=xozJw0kZXF 30. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., Han, J., Huang, S., Zhang, Y., He, X., Li, H., Qiao, Y.: SPHINX: the joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. CoRR abs/2311.07575 (2023) 31. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023) 32. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Conference on Neural Information Processing Systems (NeurlPS) (2023) 33. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Comput. Surv. 55(9), 195:1195:35 (2023) 34. Ma, X., Fang, G., Wang, X.: Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36, 2170221720 (2023) 35. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.P., Hermann, K., Welleck, S., Yazdanbakhsh, A., Clark, P.: Self-refine: Iterative refinement with self-feedback. In: Conference on Neural Information Processing Systems (NeurlPS) (2023) 36. Miao, N., Teh, Y.W., Rainforth, T.: Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. CoRR (2023) 37. Niu, H., Li, H., Zhao, F., Li, B.: Domain-unified prompt representations for sourcefree domain generalization. CoRR (2022) 38. Pan, L., Saxon, M., Xu, W., Nathani, D., Wang, X., Wang, W.Y.: Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188 (2023) 39. Pan, T., Tang, L., Wang, X., Shan, S.: Tokenize anything via prompting. CoRR (2023) 16 Yu et al. 40. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (ICML) (2021) 41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.) International Conference on Machine Learning (ICML) (2021) 42. Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., Lu, J.: Denseclip: Language-guided dense prediction with context-aware prompting. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) (2022) 43. Reddy, G.: The mechanistic basis of data dependence and abrupt learning in an incontext classification task. International Conference on Learning Representations (ICLR) (2023) 44. Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A.: systematic survey of prompt engineering in large language models: Techniques and applications. CoRR (2024) 45. Shen, S., Yang, S., Zhang, T., Zhai, B., Gonzalez, J.E., Keutzer, K., Darrell, T.: Multitask vision-language prompt tuning. CoRR (2022) 46. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: language agents with verbal reinforcement learning. In: Conference on Neural Information Processing Systems (NeurlPS) (2023) 47. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024) 48. Shtedritski, A., Rupprecht, C., Vedaldi, A.: What does CLIP know about red circle? visual prompt engineering for vlms. In: International Conference on Computer Vision (ICCV) (2023) 49. Shu, M., Nie, W., Huang, D., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.: Test-time prompt tuning for zero-shot generalization in vision-language models. CoRR (2022) 50. Shu, M., Nie, W., Huang, D., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.: Test-time prompt tuning for zero-shot generalization in vision-language models. In: Conference on Neural Information Processing Systems 2022, NeurIPS (2022) 51. Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 83178326 (2019) 52. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023) 53. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) 54. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 55. Wang, T., Zhang, J., Fei, J., Zheng, H., Tang, Y., Li, Z., Gao, M., Zhao, S.: Caption anything: Interactive image description with diverse multimodal controls (2023) 56. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual expert for pretrained language models (2023) 57. Wang, W., Ren, Y., Luo, H., Li, T., Yan, C., Chen, Z., Wang, W., Li, Q., Lu, L., Zhu, X., Qiao, Y., Dai, J.: The all-seeing project v2: Towards general relation comprehension of the open world (2024) Attention Prompting on Image for Large Vision-Language Models 58. Wang, W., Cao, Y., Zhang, J., Tao, D.: FP-DETR: detection transformer advanced by fully pre-training. In: International Conference on Learning Representations (ICLR) (2022) 59. Wang, Y., Huang, Z., Hong, X.: S-prompts learning with pre-trained transformers: An occams razor for domain incremental learning. CoRR (2022) 60. Wang, Z., Zhang, Z., Lee, C., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy, J.G., Pfister, T.: Learning to prompt for continual learning. In: CVPR (2022) 61. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 2482424837 (2022) 62. Wu, C.H., Motamed, S., Srivastava, S., la Torre, F.D.: Generative visual prompt: Unifying distributional control of pre-trained generative models. CoRR (2022) 63. Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking, drawing and editing with visual foundation models. CoRR (2023) 64. Yang, J., Zhang, H., Li, F., Zou, X., Li, C., Gao, J.: Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V. CoRR (2023) 65. Yang, L., Wang, Y., Li, X., Wang, X., Yang, J.: Fine-grained visual prompting. In: Conference on Neural Information Processing Systems (NeurlPS) (2023) 66. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v(ision) (2023) 67. Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., Wang, L.: Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023) 68. Yao, Y., Zhang, A., Zhang, Z., Liu, Z., Chua, T., Sun, M.: CPT: colorful prompt tuning for pre-trained vision-language models. CoRR (2021) 69. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities (2023) 70. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., Chen, W.: Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023) 71. Zeng, A., Attarian, M., Ichter, B., Choromanski, K.M., Wong, A., Welker, S., Tombari, F., Purohit, A., Ryoo, M.S., Sindhwani, V., Lee, J., Vanhoucke, V., Florence, P.: Socratic models: Composing zero-shot multimodal reasoning with language. In: International Conference on Learning Representations (ICLR) (2023) 72. Zhang, A., Ji, W., Chua, T.: Next-chat: An LMM for chat, detection and segmentation. CoRR (2023) 73. Zhang, Y., Ma, Z., Gao, X., Shakiah, S., Gao, Q., Chai, J.: Groundhog: Grounding large language models to holistic segmentation (2024) 74. Zhang, Z., Zhou, Y., Zhao, X., Che, T., Lyu, L.: Prompt certified machine unlearning with randomized gradient smoothing and quantization. In: Conference on Neural Information Processing Systems (NeurlPS) (2022) 75. Zhao, X., Yang, X., Pang, T., Du, C., Li, L., Wang, Y.X., Wang, W.Y.: Weak-tostrong jailbreaking on large language models (2024) 76. Zheng, C., Liu, Z., Xie, E., Li, Z., Li, Y.: Progressive-hint prompting improves reasoning in large language models. CoRR (2023) 77. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for visionlanguage models. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) (2022) 78. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for visionlanguage models. In: IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) (2022) 79. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. Int. J. Comput. Vis. (2022) 18 Yu et al. 80. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. Int. J. Comput. Vis. (2022) 81. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023) 82. Zhu, X., Zhang, R., He, B., Zeng, Z., Zhang, S., Gao, P.: Pointclip V2: adapting CLIP for powerful 3d open-world learning. CoRR abs/2211.11682 (2022) Attention Prompting on Image for Large Vision-Language Models 19 Attention Prompting on Image for Large Vision-Language Models - Supplementary Material -"
        },
        {
            "title": "6 Examples",
            "content": "Fig. 2: In complex images including multiple objects, our method accurately highlights the fruits and masks the other objects, thereby simplifying the scene and facilitating the LVLMs inference of spatial relationships. empty 20 Yu et al. Fig. 3: Our method identifies regions related to the objects, thereby assisting the LVLM in spatial reasoning. empty Attention Prompting on Image for Large Vision-Language Models 21 Fig. 4: Our method assists LVLMs recognition process by highlighting the corresponding steps in the flowchart. Fig. 5: In this example, our method enhances LVLMs OCR capability by masking background areas and highlighting the regions that require OCR. 22 Yu et al. Fig. 6: In this example, our method highlights related regions and enables the LVLM to generate more detailed and accurate response. empty Attention Prompting on Image for Large Vision-Language Models 23 Fig. 7: In this example, where the question asks to determine whether the trash can is full, our method accurately highlights the area around the trash cans opening, thereby guiding the LVLM to make correct judgment. empty 24 Yu et al. Fig. 8: In this example, where the question is related to books, our method accurately highlights the area where the books are located in the image. empty Attention Prompting on Image for Large Vision-Language Models Fig. 9: In this example, the largest measurement number 50 on the ruler is not fully displayed, leading to error in the baseline method. In contrast, as seen through the heatmap, our method emphasizes the bottom right corner of the image where the end of the ruler is located, thereby guiding the LVLM to provide the correct answer. empty 26 Yu et al. Fig. 10: Our method accurately emphasizes the baby and dog in the image, thereby facilitating the inference of their spatial relationship. empty Attention Prompting on Image for Large Vision-Language Models 27 Fig. 11: In this example, the question is related to the shoes, which are small objects and are difficult to recognize for the model. Our method accurately located the shoes in the image, leading the LVLM to the correct answer."
        },
        {
            "title": "7 Notation Table",
            "content": "Although the definitions of all symbols are included within the main text, we provide comprehensive notation table in Tabs. 7 and 8 to facilitate easy reference and macro-level understanding of the concepts involved in each part of the method. 28 Yu et al. Table 7: The notations used in the manuscript."
        },
        {
            "title": "Mainly used in",
            "content": "Entire Sec. 3 I Ψ Φ o Auxiliary LVLM used for attribution map extraction Entire Sec. 3 Annotation function, which is the proposed method Entire Sec. Original image Image with annotations, which is obained by visual prompting method Attribution map in the token space, which is extracted from the auxiliary LVLM and is used to generate the heatmap Entire Sec. 3 Entire Sec. 3 Entire Sec. Heatmap in the pixel space, which will be overlied on the original image Entire Sec. 3 Input text query Output text response Entire Sec. 3 Entire Sec. A(l,h) Attention map in the l-th transformer layer correEntire Sec. 3 sponding to the h-th head gclip CLIP model ˆI ˆT Image feature generated by CLIP, which is able to calculate the similarity Text feature generated by CLIP, which is able to calculate the similarity Number of transformer layers within the CLIP vision encoder MSA Multihead Self-Attention structure MLP Multi-Layer Perceptron structure Input token sequence for the l-th transformer layer [Z]cls Value of the cls token within the token sequence Z. Sec. 3.1 Sec. 3.1 Sec. 3. Sec. 3.1 Sec. 3.1 Sec. 3.1 Sec. 3.1 Sec. 3.1 Attention Prompting on Image for Large Vision-Language Models 29 Table 8: The notations used in the manuscript."
        },
        {
            "title": "Mainly used in",
            "content": "L Linear transformation in the CLIP model, which is performed after the transformer structure, before calculating the similarity score In the similarity decomposition of the CLIP model, only the MSA output of last layers are considered. is the starting layer index. Sec. 3.1 Sec. 3. (l,h) Value matrix in the l-th layer corresponding to the Sec. 3.1 h-th head (l,h) Weight matrix in the l-th layer used to merge the multiple attention heads and corresponds to the hth head. For each head, after the the multiplication between the attention map and the value matrix, we have matrix with the size of D. To aggregate the matrices from all heads, weight matrix with the size of (H D) is used. (l,h) is obtained from splitting this large weight matrix. Sec. 3.1 B(l) Bias matrix in the l-th layer used to merge the mulSec. 3.1 tiple attention heads A(l,h) cls,t Attention value of the class token towards the t-th token in A(l,h) (l,h) t,: t-th row of (l,h) Number of attention heads Number of tokens ηl MSA output of the l-th layer corresponding to the t-th patch(token)} ηl summing over the layer index ψt Ψ cls Attribution map generated from the CLS token Ψ comp Complementary attribution map generated using the non-CLS token text tokens corresponding to the text query img tokens corresponding to the image patches out tokens generated by the LLaVA model A( L,h) m,t Attention value in A( L,h) from the m-th token to the t-th token ˆΦ Raw heatmap, which is generated by resizing the attribution map Sec. 3. Sec. 3.1 Sec. 3.1 Sec. 3.1 Sec. 3.1 Sec. 3.1 Sec. 3. Sec. 3.1 Sec. 3.2 Sec. 3.2 Sec. 3.2 Sec. 3.2 Sec. 3. 30 Yu et al. Fig. 12: Comparison between the functionality of CLS token similarity and the NonCLS token similarity."
        },
        {
            "title": "8 Observation and Discussion of API Method",
            "content": "8.1 CLS Token Similarity and Non-CLS Token Similarity To extract heatmaps from the CLIP model, we designed two complementary types of attribution maps: one based on the decomposition of similarity between the feature of the CLS token and text feature, and the other measuring the similarity between the feature of the Non-CLS tokens and text feature. Fig. 12 compares the differences in functionality between these two types of attribution maps. The third row in the image shows the heatmap generated solely based on Ψ cls and its resulting annotated image. The fourth row shows the heatmap obtained solely from Ψ comp. Firstly, we can observe that when the query changes, Ψ cls can highlight different parts of the image corresponding to different queries. It selects the areas where the blanket and computer are located based on the query. However, Ψ comp does not show significant differences in response patterns to different queries. On the other hand, Ψ comp can filter out the background of the image, leaving the objects, which potentially can be used in the process of VQA. For instance, when the query explicitly mentions computer, Ψ cls completely ignores the chair and blanket in the lower left corner, but Ψ comp still assigns high values to these areas. Therefore, we combine Ψ cls and Ψ comp to form complete attribution map. Attention Prompting on Image for Large Vision-Language Models"
        },
        {
            "title": "8.2 Attribution Map Aggregation for CLIP Model",
            "content": "First, Eq. (7) in the maintext can be rewritten as 1(1Ψ cls)(1Ψ comp), where since Ψ cls and Ψ comp are cosine similarities, both (1 Ψ cls) and (1 Ψ comp) range between 0 and 1. Thus, the final mask is related to the product of the two parts, (1 Ψ cls) and (1 Ψ comp). If Ψ cls and Ψ comp are considered binary, then (1Ψ cls)(1Ψ comp) can be approximated as an OR operation between (1Ψ cls) and (1 Ψ comp). That is, when either (1 Ψ cls) or (1 Ψ comp) is 0, the equation will be 1, and only when both are 1, the equation will be 0. This means that for patch i, as long as either attribution map Ψ cls or Ψ comp highlights this patch, the final attribution map Ψ will also highlight this patch. Only when both Ψ cls and Ψ comp consider patch unimportant, the final attribution map will ignore this patch. Experimental findings, as shown in Fig. 12, indicate that, on one hand, Ψ comp can indiscriminately choose all entities, whereas Ψ cls selects entities explicitly mentioned in the query. The highlighted area in Ψ cls can be understood as subset of the highlighted area in Ψ comp. On the other hand, both Ψ cls and Ψ comp will ignore non-informative parts of the image. Therefore, in actual nonbinary cases, the computation of Eq. (7) can be described as an algorithm: first, apply mask to non-informative areas (i.e., instruct the LVLM to ignore these patches) because these patches will not be selected by either Ψ cls or Ψ comp. For the remaining areas, which are patches with objects directly mentioned in the query or other entities potentially related to the query, multiplication of Ψ cls and Ψ comp further highlights the patches with objects appearing in the query because they have greater weight in Ψ cls."
        },
        {
            "title": "Details",
            "content": "9.1 Ensemble Table 9: Ensemble of visual prompts generated from different LVLM. LLaVA-Bench 102.00 w/o prompt 103.30 (+1.30) Ours (CLIP) Ours (LLaVA) 103.60 (+1.60) Ours (CLIP+LLaVA) 104.80 (+2.80) When the auxiliary LVLM and the LVLM used for inference are different, our approach can be seen as ensembling the knowledge of the auxiliary LVLM into the LVLM used for inference through visual prompts. Under this definition, baseline methods like FGVP and SoM can also be considered form of ensemble, not between LVLMs but between vision model (segmentation model) and an LVLM. From the experimental results, our method is the first effective ensemble method that is based on visual prompting in VQA context. Yu et al. In traditional ensemble methods that are based on output aggregation, the number of models to be ensembled can be more than 2. However, in our method, we ensemble only two models, namely, an auxiliary LVLM and an LVLM for inference. To achieve an ensemble of more than two models, we conduct the following experiment. We use GPT-4V as the inference model and experiment on the LLaVA-Bench (in-the-wild) dataset, Instead of using single annotated image. We input the annotated images generated by both API +CLIP and CLIP+LLaVA simultaneously into GPT-4V, while keep using the original question without additional prompts as the textual query. The experimental results in Tab. 9, show that the ensemble of API +CLIP and CLIP+LLaVA can further improve performance. 9."
        },
        {
            "title": "Influence on Different VQA Abilities",
            "content": "To thoroughly understand the impact of our method on various capabilities of LVLMs, we report the performance changes across different specific abilities on the MM-Vet dataset using the CogVLM model as the inference model and CLIP as the mask model. The results are shown in Tab. 10. It is observed that our method enhances all categories of capabilities in the MM-Vet dataset. Notably, our method is particularly beneficial for OCR and Math abilities. The significant improvement in OCR capability is attributed to our methods highlighting of relevant areas, allowing the model to focus only on regions related to answering the question. This narrows down the scope of the OCR task, thereby enhancing OCR performance. Consequently, the improvement in mathematical ability is closely linked to the enhancement in OCR capability. Since addressing math-related questions in images first requires performing OCR tasks, the improvement in OCR also contributes to the enhancement of mathematical abilities. Table 10: The influence of our method on various categories of LVLM capabilities. Capability Recognition OCR Knowledge Generation Spatial Relationship w/o prompt Ours 54.9 55.3 42 48.3 43.9 45.6 42.6 46 50.1 51.2 Math 3.5 14.6 9.3 Implementation Details Pre-trained weight and API. During the mask generation phase, we used the CLIP-ViT-L-336 model [40] released by OpenAI and the LLaVA-1.5-13B model [31]. In the inference process, we utilized the released weight of LLaVA1.5-13B model [31] and cogvlm-chat-v1.1 model [56]. We use the gpt-4-1106vision-preview and gemini-pro-vision models for GPT-4V [66] and Gemini [52] API, respectively. All local experiments were deployed on single A100 GPU. Query GPT-4V and Gemini. For GPT-4V and Gemini, we used python APIs for batch querying. When encountering errors due to server or network issues, we Attention Prompting on Image for Large Vision-Language Models 33 paused for while and retried the query once. If the error persisted, we recorded the response as an empty string. If query was detected against security policy, such as person identification, we did not retry and directly recorded the responses from GPT-4V and Gemini as empty strings. Baselines. The w/o prompt baseline is implemented by directly querying the LVLM with the question together with the original image. Following [25], the Step-by-Step baseline is implemented by inputting the original image and query in the format of [Question] Lets think step by step. For the experiments with FGVP [65] and SoM [64], we query the LVLM with the corresponding annotated image and the original question, which is also the same when we implement our method. The only difference among the experiments with FGVP, SoM and our method is the annotated image. For the FGVP method, the annotation process is aligned with the default of the released code. For the SoM method, we choose SAM [23] as the segmentation model and keep all other parameters aligned with the default setting in the released code. Implementation on each dataset. Our implementation on various datasets adopts the approach from LLaVA [32]. The evaluation process of each dataset adheres to its official usage protocols or its official template, when it is accessible. (1) LLaVA-Bench (in-the-Wild) [32] is dataset comprising real-world scenes, drawings, memes, and other types of images, along with open-ended questions. It focuses on testing LVLMs capabilities in QA, detailed description, and complex reasoning. In our implementation, the textual prompt is directly the question from the dataset. We record the LVLMs complete answer and use the GPT-based evaluation tool officially released by LLaVA-Bench (in-the-Wild) to score the answers. (2) MM-Vet [69] is comprehensive dataset containing various types of images, including real-world scenes, artworks, statistical graphs, memes, etc., along with open-ended questions. Each question involves multiple aspects of visual and language abilities, such as recognition + spatial awareness or OCR + Math. In our implementation, the textual prompt is directly the question from the dataset. We record the LVLMs complete answer and use the GPT-based evaluation tool officially released by MM-VET to score the answers. (3) MME [15] is dataset that includes images of real-world scenes, artworks, logos, etc., along with True-False questions. This dataset involves abilities in commonsense reasoning, numerical calculation, and text translation, among others. Given its binary response format (yes or no), we add Please answer yes or no as an additional textual prompt to the original question. We evaluate the performance by the matching accuracy between LVLMs answers and the ground truth. (4) The MMMU [70] dataset encompasses multi-discipline questions requiring college-level expertise for responses. The questions are either multiple-choice or can be answered with simple data or phrases. For multiplechoice questions, we guide the LVLM to directly answer the corresponding option by adding Answer with the options letter from the given choices directly after the original question and options. For other questions, we add Answer the question using single word or phrase. to the original question. Our experiment is conducted using the validation set of MMMU. Evaluation is based on the matching accuracy between LVLMs answers and the ground truth. (5) The TextVQA [51] dataset contains real-world images with text, where the questions can be answered with simple words or phrases, mainly testing the LVLMs 34 Yu et al. OCR and reasoning abilities. We add Answer the question using single word or phrase after the original question to guide the LVLM to directly respond to the query without providing additional explanations. Our experiment is conducted using the validation set of TextVQA. The evaluation score is the matching accuracy between LVLMs answers and the ground truth. (6) The VisWiz [5] dataset is collected from questions about real-world images asked by blind people and manually annotated answers. The questions can be answered with simple words or phrases. However, since the questions are from blind individuals, some questions are unanswerable based on the image alone and thus are marked as unanswerable. To address this, we concatenate the following prompt after the original question: When the provided information is insufficient, respond with Unanswerable. Answer the question using single word or phrase Our experiment is conducted using the validation set of VisWiz. Evaluation is based on the matching accuracy between LVLMs answers and the ground truth. Prompts used in the Self-Reflection experiment. For the textual selfreflection experiment, we use two-round chat. In the first round, we directly ask the LVLM to answer the query and record the answer. In the second round, we use prompt in the format of For the Question [Question], Your previous answer is [Answer in the Round 1]. Evaluate the quality of the answer and provide new answer. We record the response of the second round and extract the answer by manually delete the sentences related to the quality evaluation of previous answer. The extracted answer is stored as the final answer. For the API + reflection via re-emphasize setup, we input the annotated image together with the prompt in the format of [Question] (Hint: The answer is related to the unmasked visible regions). For the API + reflection via evaluation setup, we input the annotated image together with the prompt in the format of For this image, the question is [Question]. Evaluate whether the unmasked visible regions of the image alone can provide an answer to the question. If they suffice to answer the question, respond with letter T. If they do not support an answer to the question, reply with the letter F. If the LVLM responses with F, we query it again using the original image and the question, and then use the response as the final answer. If the LVLM responses with T, we query it again using the annotated image and the question, and then use the response as the final answer."
        },
        {
            "title": "10 Limitation, Future Direction, and Potential Impact",
            "content": "Limitation and future direction. An essential component of this work is the extraction of attribution maps based on an auxiliary LVLM. The introduction of an auxiliary LVLM enhances the performance of visual prompting methods but also introduces some limitations and new research opportunities. First, generating visual prompts based on an LVLM incurs additional computational costs, Attention Prompting on Image for Large Vision-Language Models 35 either from an extra execution of the same LVLM or forward pass through another LVLM. Note that this is limitation, exploring ways to reduce this additional overhead, such as using lightweight LVLMs to generate visual prompts to achieve weak-to-strong effect [6, 75], is worthwhile research direction. Secondly, our current selection of auxiliary LVLMs is not adaptive; we cannot automatically choose more suitable auxiliary LVLM for different image-query pairs. This is another limitation of our method and potential research direction with promise. Potential impact. The potential social impacts of this work mainly include two aspects. The first aspect is the potential accumulation of bias and unfairness due to the introduction of an extra LVLM. The bias and unfairness of the auxiliary LVLM may accumulate through our visual prompts into the final inference process. The other aspect is the creation of new possibility for attacks, namely, by attacking the auxiliary LVLM to generate harmful visual prompts, thereby attacking the LVLM. Because the attack is based on the visual prompts in the pixel space, such attacks might be more covert and difficult to detect."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}