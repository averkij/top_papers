{
    "paper_title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
    "authors": [
        "Jiageng Mao",
        "Siheng Zhao",
        "Siqi Song",
        "Tianheng Shi",
        "Junjie Ye",
        "Mingtong Zhang",
        "Haoran Geng",
        "Jitendra Malik",
        "Vitor Guizilini",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 2 7 1 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
            "content": "Jiageng Mao1 Siheng Zhao1 Haoran Geng2 Siqi Song1 Tianheng Shi1 Jitendra Malik2 Vitor Guizilini3 Yue Wang1 Junjie Ye1 Mingtong Zhang1 1University of Southern California 2UC Berkeley 3Toyota Research Institute https://usc-gvl.github.io/UH-1 Figure 1. Overview. We introduce Humanoid-X, large-scale dataset to facilitate humanoid robot learning from massive human videos. On top of Humanoid-X, we introduce UH-1, large humanoid model for universal language-conditioned pose control of humanoid robots."
        },
        {
            "title": "Abstract",
            "content": "Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control humanoid robot. Extensive simulated and real-world experiments validate that our *equal contribution (in alphabetical order). work done while at USC. scalable training approach leads to superior generalization in text-based humanoid control, marking significant step toward adaptable, real-world-ready humanoid robots. 1. Introduction Scalability is crucial in deep learning. Recent advances in computer vision have demonstrated that scaling up training data leads to more powerful foundation models for visual recognition [26, 41, 44] and generation [3, 51]. In robotics, researchers follow similar paradigm and build foundation models for robotic manipulation [4, 5, 24, 40] by collecting massive robotic demonstrations. Nevertheless, in contrast to images and videos that are abundant and easily accessible, collecting large-scale robotic demonstrations is expensive and time-consuming, which limits the scalability of current robot learning methods. This raises the question: Can we use videos as demonstrations to improve the scalability of robot learning? To address this challenge, many efforts have been made, such as learning affordances [2, 15, 28], flows [67, 69], and world models [68] from natural videos, which enable more generalizable robotic manipulation. However, when it comes to humanoid robots, learning such action representations from videos remains an open problem. Unlike robotic arms, humanoid robots have distinct kinematic structures and more degrees of freedom (DoFs), making them harder to control. Existing works [8, 9, 16, 17, 30, 47, 48] leverage large-scale reinforcement learning to learn robust humanoid control policies, but they only focus on limited robotic skills such as locomotion or jumping, making them less generalizable for handling everyday tasks. Other works [13, 19, 20, 53] control humanoid robots through teleoperation, but they require human labor to collect robotic data, which is less scalable. In contrast to these previous works, learning universal action representation from massive videos will greatly improve the scalability of humanoid robot learning and enable more generalizable humanoid pose control. To bridge this gap in humanoid robot learning, we introduce Humanoid-X, large-scale dataset curated from massive and diverse collection of videos for universal humanoid pose control. Humanoid-X utilizes natural language as an interface to connect human commands and humanoid actions, so humans can talk to their humanoid robots to control their actions. The natural language representations are extracted from videos via captioning tools and are used to describe the actions of humanoid robots. For action representations, Humanoid-X leverages both robotic keypoints for high-level control and robotic target DoF positions for direct position control. To extract humanoid actions from human videos, we first reconstruct 3D humans and their motions from videos. Then, we leverage motion retargeting to transfer motions from 3D humans to humanoid robots, resulting in robotic keypoints for high-level humanoid pose control. Finally, we learn universal RL-based control policy that maps keypoints to low-level humanoid target DoF positions that can be deployed in real robots. We collect over 160,000 human-centric videos from academic datasets and the Internet, covering diverse action categories. We further transform these videos into text-action pairs, resulting in over 20 million humanoid actions with corresponding text descriptions. Humanoid-X paves the way for developing more generalizable and scalable humanoid robotic control guided by natural language. On top of the Humanoid-X dataset, we further investigate how to learn universal humanoid pose control model using large-scale text-action pairs. We introduce Universal Humanoid-1 (UH-1), large humanoid model for universal language-conditioned humanoid pose control. UH-1 leverages the scalability of the Transformer architecture to handle vast amounts of data efficiently. We begin by discretizing 20 million humanoid actions into action tokens, creating vocabulary of motion primitives. Then, given text command as input, the Transformer model auto-regressively decodes sequence of these tokenized humanoid robotic actions. For cases where the action representation involves robotic keypoints, we transform these into robotic DoF positions using an additional action decoder. Finally, we utilize proportional-derivative (PD) controller to convert the DoF positions into motor torques, enabling us to control humanoid robots and deploy them in the real-world. To validate the effectiveness of the Humanoid-X dataset and the UH-1 model, we conducted extensive experiments across both simulated and real humanoid platforms. Our results reveal that leveraging vast amounts of video data enables our model to seamlessly translate textual commands into diverse and contextually accurate humanoid actions. Notably, the UH-1 model demonstrates strong robustness, proving reliable in real-world deployment. To summarize, our key contributions are as follows: We introduce Humanoid-X, pioneering large-scale dataset tailored for learning universal humanoid control from massive Internet video data. We introduce UH-1, powerful, scalable model for language-conditioned control of humanoid poses. Our approach supports two flexible control modes that are interchangeable, depending on task requirements. We also provide extensive ablation study for our design choices. Our experiments confirm that training on massive video data enables level of generalizability in humanoid control that was previously unattainable. 2. Related Works Robot Learning from Internet Data. Many endeavors have been made to learn scalable robot learning policies from non-robotic data, especially Internet videos. The key idea is to learn valuable representations from massive visual data and transfer them to robotic tasks. The learned representations include pre-trained visual features from videos [36, 39, 46, 65] and transferable action representations such as affordances [1, 2] and object-centric flows [67, 69]. Other works [12, 38, 68] attempt to learn world models from Internet videos. However, most of these works focus on robotic manipulation. Since robot arms have totally different kinematic structures from humanoid robots, the learned visual and action representations for robotic manipulation are not transferable to humanoid robot control. In contrast, we investigate how to learn universal pose control for humanoid robots from massive videos. Humanoid Robot Learning. Extensive work has been dedicated to learning policies that enable robust control of humanoid robots. Some works focus on humanoid locomotion using large-scale reinforcement learning [8, 16, 17, 30, 48] or imitation learning [49, 57]. Other works learn humanoid manipulation via imitation learning [29, 71]. Notably, some works [9, 13, 1921] learn humanoid teleoperation by transferring motions from 3D humans to humanoid robots. However, these works rely on well-calibrated motion capture data, limiting their generalization ability to unFigure 2. Learning Humanoid Pose Control from Massive Videos. We mine massive human-centric video clips from the Internet. We then extract text-based action descriptions and 3D human poses Phuman from the video clips. Next, we retarget the motions from humans to humanoid robots, resulting in humanoid keypoints Probot for high-level control. Finally, we employ reinforcement learning to generate physically deployable humanoid actions Arobot. In this manner, we collect 163,800 pairs of motion samples V, , Phuman, Probot, Arobot from Internet videos, which are leveraged to distill universal humanoid pose control policy. seen motions. In contrast, our method operates as fully autonomous agent that learns from massive Internet videos and performs generalizable humanoid pose control based on arbitrary text commands. 3D Human Motion Generation. Many works are attempting to generate diverse 3D human motions via Transformers [22, 72] or diffusion models [31, 54, 60, 66, 74]. Also, some works [14, 34, 35, 42, 43, 58, 64, 70] are trying to generate realistic motions to animate physics-based virtual characters. However, humanoid robots are essentially different from digital humans in many aspects: (1) they have different joint structures and degrees of freedom; (2) humanoid robots cannot access privileged information like linear velocities, which is readily available when controlling virtual humans; (3) humanoid robots have physical constraints such as motor torque limits, whereas 3D virtual humans do not have these limitations. An alternative solution for generalizable humanoid pose control is to first generate 3D human motions and then retarget them to humanoid robots [19, 23]. Compared to these approaches, our UH-1 model offers more streamlined solution by directly mapping text commands into executable humanoid actions without intermediate steps. Furthermore, unlike human motion generation models trained on expensive motion capture data, learning from massive videos significantly enhances the generalization ability of our method. 3. Humanoid-X Dataset 3.1. Overview To scale up humanoid robot learning using massive human videos, we introduce Humanoid-X, the largest humanoid robot dataset to date compiled from vast and diverse collection of videos for universal humanoid pose control. Humanoid-X consists of 163,800 motion samples covering comprehensive set of action categories. Each motion sample in the dataset contains 5 data modalities: an original video clip V, text description of the action in the video, sequence of SMPL [33]-based human poses Phuman estimated from the video, sequence of humanoid keypoints Probot for high-level robotic control, and sequence of humanoid actions Arobot representing target DoF positions for low-level robotic position control. Humanoid-X encompasses over 20 million frames, totaling approximately 240 hours of data. Beyond its extensive scale across multiple data modalities, which is essential for scalable humanoid policy training, Humanoid-X also features large and diverse text-based action vocabulary, as shown in Fig. 3 (c). This diversity supports universal and text-conditioned humanoid pose control. In the next section, we will discuss how to obtain these motion samples V, , Phuman, Probot, Arobot from massive videos. 3.2. Learning from Massive Videos To process large-scale, in-the-wild raw video data, we developed fully automated data annotation pipeline comprising five modules, as illustrated in Fig. 2. The pipeline includes (1) video processing module that mines and extracts video clips from noisy Internet videos, (2) video captioning model that generates text description of human actions , (3) human pose detection module that estimates parametric 3D human poses Phuman from video clips, (4) motion retargeting module to generate humanoid robotic keypoints Probot by transferring motions from humans to humanoid robots, and (5) goal-conditioned reinforcement learning policy to learn physically-deployable humanoid actions Arobot by imitating humanoid keypoints. Video Mining and Processing. The first step of our approach is to collect large number of human-centric videos that encompass wide variety of action types. To this end, we mine massive informative video clips from 3 sources: academic datasets for digital human research [6, 11, 18, 32, 56, 61, 75], datasets for video action understanding [7, 55], and Internet videos from YouTube. To collect Internet Figure 3. Dataset Statistics. Humanoid-X features extensive scale, diverse sources, rich action vocabulary, and multiple data modalities. videos, we designed over 400 unique search terms covering range of human activities from daily tasks to professional sports, and then utilized the Google Cloud API* to retrieve the top 20 videos for each specified search term. Original videos are often noisy, including segments with no humans, multiple humans, or stationary individual, which makes them unsuitable for humanoid control. To obtain meaningful video clips, we begin by downsampling each video to standardized 20 frames per second (FPS) to ensure consistency across the dataset. Next, we employ an object detector [50] for single-human detection, selecting frames with precisely one visible person. Following detection, we apply motion detection by calculating the pixelwise grayscale difference between consecutive frames to keep frames showing significant movement. We then compile sequences of at least 64 consecutive frames that satisfy the above single-human motion criterion into video clips, resulting in 163,800 video clips in total. Video captioning. Language bridges human commands and humanoid actions. To associate humanoid actions with semantic meaning and enable language-conditioned humanoid control, we employ video captioning model [10] to generate fine-grained action descriptions from videos: = Fcaption(V), (1) where Fcaption is the video captioning model. To avoid irrelevant text descriptions, we carefully design prompts to guide the model to describe human actions instead of physical appearance, resulting in action-centric text descriptions. 3D Human Pose Estimation. Humanoid robots inherently share similar skeleton with humans, which allows for learning control policies for humanoid robots based on human motion data. To this end, we first need to extract human poses from videos. To accurately track and estimate human poses in video clips, we adopt video-based 3D human parametric model estimator [27], which estimates SMPL [33]-based humans and camera parameters for each frame. We further extract global human motions, i.e., root *YouTube Data API v3 translations, using the estimated camera parameters. The process can be formulated as: Phuman(β, θ, troot) = Fpose(V), (2) where Fpose is the human pose estimation model. Finally, we obtain per-frame 3D human pose: Phuman(β, θ, troot), where β controls the human shapes, θ controls the joint rotations, and troot controls the global root translations. Motion Retargeting from Humans to Humanoid Robots. Since humans and humanoid robots have similar skeletons, we can track the human joint positions across frames and map them to the corresponding joints in humanoid robot, resulting in humanoid keypoints Probot for high-level conIn particular, we chose 12 joints that exist in both trol. humans and humanoid robots: left and right hips, knees, ankles, shoulders, elbows, and wrists. The joint positions Pjoints can be obtained via forward kinematics Ff k: Pjoints = Ff k(Phuman(β, θ, troot)). (3) Since humans have different shapes from humanoid robots, following [20], we first optimize the human shape parameters β to ensure that resized human shapes closely resemble those of humanoid robot. Specifically, we first obtain joint positions in the humanoid robot under standard T-shaped pose: robot. Then, under the same T-shaped pose, we optimize β to make human joint positions joints the same as the corresponding humanoid joint positions robot: min β joints robot2, s.t. joints = Ff k(Phuman(β, θT , troot)), (4) (5) where θT denotes the standard pose. For each frame of human pose, we replace the original β with the optimal β in Phuman, and following Eq. 3 we can obtain the adjusted joint positions joints. Finally, we directly set humanoid robotic keypoints as the adjusted human joint positions: Probot := joints. (6) To effectively control humanoid robots, we also extract the motor DoF positions qrobot in the humanoid robot via inverse kinematics Fik: qrobot = Fik(Probot). (7) We use the Adam optimizer [25] to solve the inverse kinematics problem. smoothing term is added to the optimization to regularize changes in qrobot across frames. Goal-conditioned Humanoid Control Policy. The retargeted humanoid keypoints Probot and DoF positions qrobot accurately reflect humanoid motions, but they cannot be directly deployed to the real robot. This is because they lack the necessary safety guarantees and robustness needed to handle real-world variability and constraints effectively. To address this, we develop goal-conditioned control policy π that adapts these motions while ensuring safe and reliable deployment on the physical robot: π : (cid:55) Arobot. (8) The inputs to the policy π include two parts: the goal space and the observation space O. The goal space contains humanoid keypoints Probot, DoF positions qrobot, and root movement goals derived from troot. The observation space contains robot proprioception information such as root orientation, angular velocity, and current motor DoF positions. The output action space Arobot are target DoF positions of each joint for controling the humanoid robot, which can be further transformed into motor torque signals through proportional-derivative (PD) controller. We train the control policy, π, using large-scale reinforcement learning with PPO [52] for policy optimization. The reward function includes multiple terms: motion rewards to encourage imitation of the retargeted humanoid keypoints Probot and DoF positions qrobot; root tracking rewards to follow target root orientations and linear velocities from troot; and stability rewards to help the robot maintain balance and prevent falls during movement. The resulting policy π and robotic actions Arobot enable the humanoid robot to operate safely in the physical world while maintaining the desired motions. Finally, we collect large number of motion samples V, , Phuman, Probot, Arobot from massive videos. In the next section, we investigate how to train universal humanoid pose control policy using massive motion samples. 4. UH-1 for Universal Humanoid Pose Control Learning from massive videos enables us to distill universal humanoid pose control policy from large-scale motion samples V, , Phuman, Probot, Arobot. We introduce UH-1, large language-conditioned humanoid model that takes natural language commands and generates corresponding humanoid robotic actions {Probot, Arobot}: πUH-1 : (cid:55) {Probot, Arobot}, (9) Figure 4. UH-1 Model Architecture. UH-1 leverages the Transformer for scalable learning. Humanoid actions are first tokenized into discrete action tokens. Then, we train the UH-1 Transformer that takes text commands as inputs and auto-regressively generates the corresponding humanoid action tokens. Figure 5. Text-to-keypoint and text-to-action control modes. UH-1 can either generate high-level humanoid keypoints (textto-keypoint) for the goal-conditioned policy π to control the humanoid robot in closed-loop, or generate robotic actions Arobot for direct open-loop control (text-to-action). where πUH-1 denotes the UH-1 model. Notably, as illustrated in Fig. 5, our model can either generate high-level humanoid keypoints Probot, which are then fed into the goal-conditioned policy π to control the humanoid robot in closed-loop, or generate robotic actions Arobot for direct open-loop control. Our model bridges the gap between semantic language commands and physically deployable robotic actions, enabling more generalizable humanoid robotic control using text instructions. For simplicity, in the following section, we use Arobot as an example to illustrate our method; Probot can be generated in the same manner. We adopt the Transformer [63] as our main model architecture due to its scalability to large-scale data. As shown in Fig. 4, to enable efficient learning, we first train an action tokenizer using [62] to discretize humanoid motions into vocabulary of action tokens. Then, we train the Transformer to auto-regressively decode action tokens, resulting in executable humanoid actions. UH-1 Action Tokenizer. We follow [62] and map frames of actions Arobot = [a1, . . . , aT ] into sequence of discrete action tokens Ztoken = [z1, . . . , zT /K] via an encoder Fencode and quantization Fquant: Ztoken = Fquant(Fencode(Arobot)), (10) where Fencode and Fquant are standard operations in [62]. The action tokens Ztoken come from shared action vocabulary, and each token can be viewed as motion primitive that is learned and shared across all data samples. Notably, different from language tokenization, humanoid actions wont change much in adjacent frames. To maintain the temporal smoothness in humanoid actions, we encode short clip with frames of actions [aiK, . . . a(i+1)K] into single action token zi, rather than encoding each frame individually. This approach not only preserves smooth transitions but also eases the learning process. The decoder of VQ-VAE Fdecode tries to reconstruct the original action sequence with the latent embeddings associated with the action tokens: robot = Fdecode(Ztoken). (11) We denote the reconstructed action sequence as [a ]. The reconstruction loss is formulated as 1, . . . , robot = Lrecon = (cid:88) (a ai+(a i+1 i)(ai+1 ai)), (12) where the first term is the L1 reconstruction loss in [62] and the second term encourages the first-order similarity of original and reconstructed action sequences. Additionally, we add regularization terms on latent embeddings as in [62]. UH-1 Transformer. We formulate the task of languageconditioned humanoid pose control as auto-regressively decoding action tokens Ztoken conditioning on text commands . Formally, let Ztoken = [z1, . . . zT /K] denote the target action token sequence, where zi is the current step to predict, and z1:i1 represent the preceding context of action tokens, and denote the text embedding by encoding the text command with the CLIP [45] encoder. The UH-1 Transformer is then trained to model the conditional probability distribution (ziz1:i1, l). special [End] token is incorporated into the vocabulary to signal the termination of sequence generation. During training, we first tokenize each Arobot into Ztoken using Eq. 3. Then, we feed the language embedding into the UH-1 transformer, and the transformer auto-regressively decodes action tokens. The learning objective is to minimize the negative log-likelihood over the whole training dataset D: Llearn = (cid:88) log ZD Π i=1 p(ziz1:i1, l). (13) During inference, using Eq. 11, the generated action tokens are decoded into Arobot for controlling the humanoid robot. Methods Oracle FID MM Dist Diversity Precision 0.005.001 3.140.010 9.846.062 MDM [59] T2M-GPT [73] 0.582.051 0.667. 5.921.034 3.401.017 UH-1 (ours) 0.445.078 3.249.016 10.122.078 10.328.099 10.157.106 0.780. 0.617.007 0.734.004 0.761.003 Table 1. Comparisons of model performances on the HumanoidML3D benchmark. We calculate standard metrics following [18], repeating each evaluation 20 times and reporting the average along with the 95% confidence interval. The results indicate that UH-1 attains the highest performance across most metrics and achieves comparable performance on the Diversity metric. The Transformer architecture and auto-regressive modeling ensure scalable learning of humanoid robot pose control. 5. Experiments In this section, we conduct extensive experiments to investigate the following research questions: (1) Universal Pose Control with UH-1: Does our UH-1 model enable universal humanoid robot pose control based on text commands? (2) Scalability and Generalization with Humanoid-X: Does the large-scale Humanoid-X dataset facilitate scalable training and improve the generalization ability of our UH-1 model? (3) Real-World Deployment of UH-1: Can our UH-1 model be deployed on real humanoid robots to enable reliable robotic control in real-world environments? 5.1. Universal Humanoid Pose Control with UH-1 We conduct extensive experiments to validate the generalization ability of the UH-1 model. An alternative solution to text-to-humanoid action generation is two-stage pipeline: generating 3D human motions first and then retargeting the human motions to humanoid robots. To this end, we compare our method with two important baselines for text-to-human motion generation: Motion Diffusion Model (MDM) [59] and Text-to-Motion GPT (T2M-GPT) [73]. For fair comparisons, We choose the commonly used HumanML3D [18] benchmarks and transform the humans in this dataset into humanoid robots, resulting in new benchmark called HumanoidML3D. Similarly, we adopt the same motion retargeting method as in this paper to transform the human motions generated by the baselines into humanoid actions. We adopt the metrics in [18] to evaluate the humanoid motions from different aspects: (1) Quality: The Frechet Inception Distance (FID) evaluates the dissimilarity between feature distributions of generated and ground truth humanoid poses. (2) Diversity: The Diversity metric evaluates the variability within the generated humanoid pose distribution, calculated as the average Euclidean distance between 300 randomly sampled pairs of humanoid poses. (3) Reliability: The Multi-modal Distance (MM Dist) measures the Euclidean distance between motions and corresponding texts, and the Precision assesses the accuracy of text and Figure 6. Real robot experiment. UH-1 model can be reliably deployed on the real humanoid robot with nearly 100% success rate. Dataset Oracle FID MM Dist Diversity Precision 0.005.001 3.140.010 9.846. 0.780.003 HumanoidML3D Humanoid-X 0.445.078 0.379.046 3.249.016 3.232.008 10.157.106 10.221.100 0.760.003 0.761. Table 2. Dataset quality evaluation.. Training on the HumanoidX dataset greatly improves the quality and reliability of humanoid actions, compared to training on the HumanoidML3D dataset. humanoid pose matches in the Top 3 rankings. Tab. 1 shows the results of our UH-1 model compared against the baselines. The results indicate that UH-1 attains the highest performance across nearly all metrics, showing an over 23% reduction in the critical FID metric, while also maintaining comparable performance on the Diversity metric. The first-order similarity loss proposed in this paper greatly enhances the quality and reliability of the generated outputs. The results suggest that UH-1 is streamlined model and performs better than the two-stage methods. 5.2. Scalable Learning with Humanoid-X In this section, we investigate whether scaling up training data with the large-scale Humanoid-X dataset can improve the generalization ability of our model. To explore this, we first pre-trained our UH-1 model on the Humanoid-X dataset and then finetuned and evaluated the performance on the HumanoidML3D benchmark. Tab. 2 shows the performance comparison with training only on HumanoidML3D. We found that pre-training on the Humanoid-X dataset greatly improves the quality, reliability, and diversity of humanoid actions, with an FID improvement from 0.445 to 0.379, MM Dist score improvement from 3.249 to 3.232, and Diversity improvement from 10.157 to 10.221. In addition, we also study how scaling up training data affects the model performance. To this end, we train our UH-1 model on varying proportions of the Humanoid-X dataset, specifically 1%, 10%, 25%, 50%, 75%, and 100%. The results shown in Fig. 7 indicate that scaling up training data from 1% to 100% leads to significant performance Figure 7. Effectiveness of scaling up training data. Points indicate the mean values, and error bars indicate the 95% confidence interval. Increasing the dataset size from 1% to 100% leads to significant improvements in both FID and Diversity metric. improvement in all metrics (FID from 0.689 to 0.463 and Diversity from 5.900 to 6.149). This suggests that by learning from massive videos, we successfully scale up the training data of humanoid robots and attain better performance. 5.3. Real-World Deployment of UH-1 To investigate whether our UH-1 model, trained on the Humanoid-X dataset, can generate reliable humanoid actions that are physically deployable on humanoid robots, we designed 12 distinct language commands, as shown in Tab. 3, and evaluated them on real humanoid robot. We use UNITREE H1-2 as our test embodiment. For the experiments, we evaluated each language command 10 times and controlled the robot in different places. Notably, for textto-humanoid actions, we found that open-loop control can only work for upper-body control, so in this control mode, we use pre-trained locomotion policy for controlling the lower-body of the humanoid robot. Fig. 6 shows the demos of real-robot experiments. Tab. 3 measures the task success rate for each language command. Our experimental results demonstrate that our UH-1 model can be reliably deployed on the real humanoid robot, achieving success rate of nearly 100% across all evaluated language instructions. https://www.unitree.com/h1 Instruction Text-to-Keypoint Text-to-Action Boxing Clapping Cross Arms Embrace Golf Putt Open Bottle & Drink Play Guitar Play Violin Pray Left Hand Punch Right Hand Punch Wave to Friend 90% 100% 80% 100% 90% 100% 100% 100% 100% 100% 100% 100% 70% 100% 80% 100% 100% 100% 100% 80% 100% 100% 90% 100% Table 3. Task success rate on real humanoid robot. Both Textto-Keypoint and Text-to-Action modes can reach success rate of nearly 100% across all evaluated language instructions. 5.4. Empirical Studies Analysis of two control modes. UH-1 can either produce high-level humanoid keypoints for goal-conditioned, closed-loop policy or directly generate robotic actions for open-loop control. To investigate the effectiveness of these two control modes, we randomly generate 100 keypoint sequences and 100 action sequences for each task, as illustrated in Fig. 8, and apply them in simulated robot control. The findings indicate that both modes can achieve an average success rate exceeding 89%, suggesting that text-toaction open-loop control with separate locomotion policy is sufficient for most tasks. Moreover, the text-to-keypoint control mode, benefiting from the whole-body control policy, demonstrates slightly better robustness. Ablation study on the action tokenizer. We conduct an ablation study to investigate the impact of different vocabulary sizes of the UH-1 action tokenizer on model training. We selected the vocabulary sizes of 512, 1024, and 2048, and reported the model performances on the Humanoid-X dataset. As illustrated in Fig. 9, increasing the vocabulary size up to 2048 leads to an improvement in FID metric from 0.539 to 0.463 and brings an improvement in Diversity metric from 6.050 to 6.149. This indicates that increasing the number of motion primitives learned in the action tokenizer results in more diverse humanoid motion generation. Due to the limited computational resources, we didnt try larger vocabulary. We will leave this for future works. Ablation study on the model architecture. key consideration for generation tasks is selecting the appropriate model architecture, such as the Transformer or diffusion model. To explore this, we trained text-controlled humanoid motion diffusion model on the Humanoid-X dataset and compared its performance with the original Transformer-based UH-1 model. The results in Tab. 4 show that the Transformer architecture used in UH-1 is more scalable to large-scale training data and achieves better performance, with lower FID and MM Dist score compared to Figure 8. Simulated experiments on the UH-1 control modes. Bars indicate success rates for specific commands and dash lines show the mean success rate on 12 different text instructions. While Text-to-Action mode with separate locomotion policy is sufficient for most tasks, Text-to-Keypoint mode shows greater robustness. Methods Oracle FID MM Dist Diversity Precision 0.005.001 3.140.010 9.846.062 0.780. Diffusion model Transformer 0.624.074 0.379.046 5.536.029 3.232.008 10.281.096 10.221.100 0.630.007 0.761.003 Table 4. Diffusion model vs. Transformer as the UH-1 model. We found that the Transformer architecture is more scalable to large-scale training data and exhibits better performance. Figure 9. Ablation on the vocabulary sizes of the UH-1 action tokenizer. Increasing the vocabulary size of the action tokenizer provides more motion primitives for humanoid robots and thus leads to an improvement in both FID and Diversity metric. the diffusion-based model. 6. Conclusion We introduce Humanoid-X, large-scale dataset that facilitates scalable humanoid robot learning from massive videos. On top of Humanoid-X, we trained large humanoid model, UH-1, for generalizable humanoid pose control based on language commands. Extensive experiments demonstrate that scalable training enables UH-1 to generate generalizable and reliable humanoid actions following language commands, and the UH-1 model can be effectively deployed on the real humanoid robot. Limitations. In this paper, we only study the humanoid pose control. Humanoid manipulation is not in the scope of this paper. In future works, we plan to investigate learning humanoid loco-manipulation from Internet videos."
        },
        {
            "title": "Acknowledgement",
            "content": "The USC Geometry, Vision, and Learning Lab acknowledges generous supports from Toyota Research Institute, Dolby, and Google DeepMind. Yue Wang is also supported by Powell Research Award."
        },
        {
            "title": "References",
            "content": "[1] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Humanto-robot imitation in the wild. In RSS, 2022. 2 [2] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as versatile representation for robotics. In CVPR, 2023. 1, 2 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. 1 [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. In RSS, 2023. [6] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4d human dataset for versatile sensing and modeling. In ECCV. Springer, 2022. 3 [7] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. 3 [8] Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao, Yanjie Ze, Zhongyu Li, Shankar Sastry, Jiajun Wu, Koushil Sreenath, Saurabh Gupta, et al. Learning smooth humanoid arXiv locomotion through lipschitz-constrained policies. preprint arXiv:2410.11825, 2024. 2 [9] Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive whole-body control for humanoid robots. arXiv preprint arXiv:2402.16796, 2024. 2, 9 [10] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 4, 3 [11] Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos. In ICCV, 2021. 3 [12] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. In ICLR, 2024. 2 [13] Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid shadowing and imitation from humans. arXiv preprint arXiv:2406.10454, 2024. [14] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In ICCV, 2023. 3 [15] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In CVPR, 2024. 1 [16] Xinyang Gu, Yen-Jen Wang, and Jianyu Chen. Humanoidgym: Reinforcement learning for humanoid robot with zeroshot sim2real transfer. arXiv preprint arXiv:2404.05695, 2024. 2 [17] Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing humanoid locomotion: Mastering challenging terrains arXiv preprint with denoising world model arXiv:2408.14472, 2024. 2 learning. [18] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022. 3, 6 [19] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning. arXiv preprint arXiv:2406.08858, 2024. 2, 3, [20] Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning humanarXiv to-humanoid real-time whole-body teleoperation. preprint arXiv:2403.04436, 2024. 2, 4 [21] Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, et al. Hover: Versatile neural whole-body controller for humanoid robots. arXiv preprint arXiv:2410.21229, 2024. 2 [22] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. In NeurIPS, 2023. 3 [23] Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, and Yuke Zhu. Harmon: Whole-body motion generation of humanoid robots from language descriptions. arXiv preprint arXiv:2410.12773, 2024. 3 [24] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1 [25] Diederik Kingma. Adam: method for stochastic optimization. In ICLR, 2014. 5, 4 [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 1 [27] Muhammed Kocabas, Nikos Athanasiou, and Michael Black. Vibe: Video inference for human body pose and shape estimation. In CVPR, 2020. 4, 3 [28] Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, and Yue Wang. Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. arXiv preprint arXiv:2407.04689, 2024. 1 [29] Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, and Yuke Zhu. Okami: Teaching humanoid robots manipulation skills through single video imitation. arXiv preprint arXiv:2410.11792, 2024. 2 [30] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Robust and versatile bipedal jumping control through reinforcement learning. In RSS, 2023. [31] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. IJCV, 2024. 3 [32] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: largescale 3d expressive whole-body human motion dataset. In NeurIPS, 2024. 3 [33] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. TOG, 34(6), 2015. 3, 4 [34] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In ICCV, 2023. 3 [35] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. In ICLR, 2024. 3 [36] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In ICLR, 2023. [37] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes. In ICCV, 2019. 4 [38] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In RSS, 2023. 2 [39] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. In CoRL, 2022. 2 [40] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. In ICRA, 2023. 1 [41] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1 [42] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. TOG, 40(4), 2021. 3 [43] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. TOG, 41 (4), 2022. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML. PMLR, 2021. 1 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML. PMLR, 2021. 6, 8 [46] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with sensorimotor pre-training. In CoRL. PMLR, 2023. 2 [47] Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, and Jitendra Malik. Learning humanoid locomotion over challenging terrain. arXiv preprint arXiv:2410.03654, 2024. 2 [48] Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, and Koushil Sreenath. Real-world humanoid locomotion with reinforcement learning. Science Robotics, 9(89), 2024. 2 [49] Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, and Jitendra Malik. Humanoid locomotion as next token prediction. arXiv preprint arXiv:2402.19469, 2024. [50] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. Real-time flying object detection with yolov8. arXiv preprint arXiv:2305.09972, 2023. 4, 2 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1 [52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5 [53] Mingyo Seo, Steve Han, Kyutae Sim, Seung Hyeon Bang, Carlos Gonzalez, Luis Sentis, and Yuke Zhu. Deep imitation learning for humanoid loco-manipulation through human teleoperation. In Humanoids. IEEE, 2023. 2 [54] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. Human motion diffusion as generative prior. In ICLR, 2024. 3 [55] Gunnar A. Sigurdsson, Gul Varol, Xiaolong Wang, Ivan Laptev, Ali Farhadi, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In ECCV, 2016. Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, 2023. 6 [74] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. PAMI, 2024. 3 [75] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In ECCV. Springer, 2022. 3 [56] Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. Grab: dataset of whole-body human grasping of objects. In ECCV. Springer, 2020. 3 [57] Annan Tang, Takuma Hiraoka, Naoki Hiraoka, Fan Shi, Kento Kawaharazuka, Kunio Kojima, Kei Okada, and Masayuki Inaba. Humanmimic: Learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation. In ICRA. IEEE, 2024. 2 [58] Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, and Xue Bin Peng. Calm: Conditional adversarial latent models for directable virtual characters. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. [59] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023. 6 [60] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano. Human motion diffusion model. In ICLR, 2023. 3 [61] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In ISMIR, 2019. 3 [62] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 5, 6 [63] Vaswani. Attention is all you need. In NeurIPS, 2017. 5 [64] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. scalable approach to control diverse behaviors for physically simulated characters. TOG, 39(4), 2020. [65] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. 2 [66] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. In ICLR, 2023. 3 [67] Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, and Shuran Song. Flow as the cross-domain manipulation interface. arXiv preprint arXiv:2407.15208, 2024. 1, 2 [68] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In ICLR, 2024. 1, 2 [69] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024. 1, 2 [70] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In ICCV, 2023. 3 [71] Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, and Jiajun Wu. Generalizable humanoid manipulation with improved 3d diffusion policies. arXiv preprint arXiv:2410.10803, 2024. 2 [72] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, 2023. 3, [73] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying"
        },
        {
            "title": "Appendix",
            "content": "A. Ethics Statement B. Details on Humanoid-X Data Collection . . . . . . . B.1. Data Source Distribution . . B.2. Video Mining and Processing . . . B.3. Video Captioning . . . . B.4. 3D Human Pose Estimation . . . . . B.5. Motion Retargeting . B.6. Goal-conditioned Control Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Details on Humanoid-X Dataset . . . . C.1. Data Format and Structure . . . . . C.2. Data Statistics . C.3. Data Preparation and Release . . . . C.4. Data examples from Humanoid-X Dataset . . . . . . . . . . . . . . . . . . . . . . D. Detailed UH-1 Model Architecture . . . D.1. UH-1 Action Tokenizer D.2. UH-1 Transformer . . D.3. Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Experiment Details E.1. Real Robot Experiment . . . . . E.2. Ablation on Goal-conditioned Control Policy . . . . . . . 1 1 1 3 3 4 4 5 5 6 6 6 7 7 8 9 9 9 9 A. Ethics Statement This paper presents Humanoid-X, large-scale dataset that facilitates scalable humanoid robot learning from massive videos, and UH-1, large humanoid model for generalizable humanoid pose control based on language commands. The Internet videos that Humanoid-X and UH-1 involve in the dataset and the pipeline are strictly for academic research and are not intended for commercial use. On the privacy protection side, we apply face anonymization to all human subjects in the Internet videos involved in Humanoid-X and UH-1, making sure that the videos do not include any personal information. In addition, we will not release the original Internet videos to protect copyright. In summary, we believe that Humanoid-X and UH-1 do not raise ethical concerns. B. Details on Humanoid-X Data Collection In this section, we will introduce more details on the whole data collection pipeline of the Humanoid-X dataset, including data source distribution, video mining and processing, video captioning, 3D human pose estimation, motion retargeting from humans to humanoid robots, and the goalconditioned humanoid control policy. B.1. Data Source Distribution Humanoid-X consists of massive motion samples with diverse sources, and the detailed source of the data in our Humanoid-X dataset is shown in Tab. 1. Humanoid-X consists of 163.8K motion samples, spanning 240.3 hours of video footage, containing 20.7M frames of human and robotic motion data, with vocabulary size of 3206 words. Each motion video sample is expanded to the 5 data modalities V, , Phuman, Probot, Arobot of the motion sample in our Humanoid-X dataset. The subsections below introduce details on the dataset building and data processing pipeline. B.2. Video Mining and Processing To collect dataset of videos featuring single-person movements, we first designed specific motion categories and then generated search prompts based on these categories. Using the phrase single person in searches often produced irrelevant results since the majority of the video titles would not specify whether the video is single person using the exact word single person. So, activity-based terms were created to ensure relevant data retrieval. These categories included martial arts tutorials, fitness and exercise drills, sports techniques, dance practice, music performance tutorials, everyday movement patterns, animal-inspired movements, and rehabilitation exercises. Martial arts tutorials included search terms for techniques, drills, and demonstrations across disciplines like Wushu, Taekwondo, Karate, and Kung Fu. Examples of Figure 1. Video Processing Pipeline. generated terms are karate front kick training, taekwondo spinning hook kick demonstration, and wushu staff spin practice. Fitness and exercise drills focused on isolated movements like yoga handstand practice, and calisthenics planche progression tutorial,. Sports techniques targeted individual actions in activities like baseball, tennis, archery, running, and parkour, with examples including tennis serve technique tutorial and running stride form analysis. Dance practice emphasized solo routines in styles such as salsa, hip hop, ballet, modern dance, and improvisation, using terms like salsa basic turn solo and ballet arabesque demonstration. Music performance tutorials captured movements involved in playing instruments such as guitar, violin, piano, and drums, with terms like guitar strumming while standing solo and violin bowing technique while standing demonstration. Everyday movement patterns focus on practical motions during daily activities, using terms like picking up an object while balancing, loading dishwasher with proper form, and squatting to tie shoelaces,. Animal-inspired movements were included to capture dynamic motion patterns with terms like bear crawl coordination movement, frog jump exercise, and flamingo balance on one leg. Rehabilitation and mobility exercises targeted balance, flexibility, and strength, focusing on slow and deliberate movements such as dynamic torso twist warm-up and hip flexor stretch technique breakdown. By designing categories and generating search terms from these, we ensured the collected videos focused on single-person movements while covering wide range of activities. After collection of videos from the designed searching prompts, we designed pipeline for detecting and extracting video segments featuring single-person movements. The process begins with the YOLOv8 model [50], which detects objects in each frame and identifies detected humans based on the class label corresponding to person. Frames containing exactly one detected person are selected, ensuring the focus remains solely on single-person actions. Once Data Source # of Clips # of Frames # of Hours Vocab. Size AIST AMASS Charades EgoBody GRAB HAA500 HuMMan IDEA400 Kinetics700 MotionX Video Online Video 1.5K 13.4K 9.3K 1.0K 1.3K 5.2K 0.7K 12.5K 68.6K 40.6K 17.8K 0.3M 2.0M 1.0M 0.4M 0.4M 0.3M 0.1M 2.6M 5.2M 7.9M 2.3M 3.2 27.4 1.0 4.0 3.8 2.9 1.0 24.0 72.4 72.9 32.6 590 3942 813 367 565 1754 980 1715 3360 4021 2040 Total 163.8K 20.7M 240.3 11897 Table 1. Dataset statistics. Compiled from diverse data sources, Humanoid-X possesses an extensive scale of data modalities and massive action vocabulary. single-person frame is identified, region of interest (ROI) is extracted using the bounding box of the detected individual from YOLOv8 detection result. To determine existence of motion, the pipeline calculates frame-to-frame differences in the grayscale ROI, assessing movement levels using predefined thresholds. This ensures only frames with significant motion are retained, while static or irrelevant segments are excluded. To further refine the selection regarding motion, the pipeline employs batch-based filtering process, analyzing sequences of frames to identify consistent motion patterns over time. Small movement threshold is applied to frameto-frame and larger threshold is applied to the frame batch, enabling the detection of subtle and significant activities by allowing relative small motions for several frames as long as large motion is detected in frames batch. Such design would benefit continuity of the clips by keeping frames in between large motions. Frames that meet these criteria are grouped into chunks representing continuous motion, and only chunks exceeding minimum duration are considered for clip generation. The output clips are processed to maintain consistent quality and playback speed. Frames within each chunk are Figure 2. Video captioning example by using Video-LLaMA. down-sampled for efficiency, interpolated for smooth transitions, and standardized to 20 FPS. The resulting clips focus exclusively on single-person actions, discarding distractions such as multiple individuals or irrelevant frames. This approach ensures precise and diverse dataset of singleperson motion segments, suitable for applications in motion analysis, action recognition, and training of computer vision models. By integrating object detection, motion analysis, and sequence processing, the pipeline achieves high accuracy and relevance in isolating meaningful single-person movements. B.3. Video Captioning Video-LLaMA Prompt There is human doing something in the user provided video. Describe what the human is doing briefly. You must follow the following rules: 1. Do not describe the appearance of the human. 2. You must at least answer man/woman doing something [adverb] 3. If applicable, you should describe the [item] the human is interacting with, the [body part] the human is using, or the [location] the human is in. 4. Your answer must be within one sentence, and do not begin with in the video. Please describe what the human is doing in the video in one sentence. For video captioning, we implemented video captioning pipeline using Video LLaMA [10], with video processing framework which extracts visual information from input videos by sampling fixed number of eight frames at regular intervals. The prompts used for video captioning are designed to produce concise and action-focused descriptions. The main prompt directs the model to describe the actions of person in the video in single sentence, explicitly avoiding mentions of the persons appearance. We used the query Please describe what the human is doing in the video in one sentence. with guidance of rules shown above. Such query would guarantee concise description of motion without any irrelevant information being collected. An example of such interaction with Video-LLaMA is shown in Fig. 2. B.4. 3D Human Pose Estimation The SMPL generation pipeline is designed to estimate 3D human pose and shape parameters from video frames. This process involves several key steps, including detecting the subject in video frames, estimating pose and shape parameters, and generating 3D mesh representation. VIBE model [27] is used to infer SMPL parameters, such as body pose, global orientation, and shape coefficients, from video sequences. Bounding boxes are first detected for the subject, and these are used to crop and process the frames for subsequent steps. The final output includes SMPL parameters, root translations, and optional visualizations of the 3D mesh overlaid on the video frames. The VIBE-based mesh regression model is used as video-based inference, which benefits from temporal consistency across frames. For the detected person in video, the pipeline extracts bounding boxes and sequences of features from the video frames. VIBE processes these sequences to estimate the SMPL parameters, including pose rotations, shape coefficients, and camera parameters. The extracted parameters are then stored for further use in 3D visualization or downstream tasks. An example of SMPL visualization is shown in Fig. 3. To compute the root translation of the subject in 3D space, the bounding boxes and camera parameters from the mesh regression step are combined. The bounding box coordinates are converted to the original image coordinate system, accounting for resolution and aspect ratio. Using the weak-perspective camera parameters, including scale and 2D translation = (tx, ty), the depth tz is estimated based on predefined focal length . The depth is computed as: tz = 0.5 Wimg , (1) Figure 3. SMPL 3D human model estimation example. where Wimg represents the width of the input image. The root translation vector Troot is then formed as: Troot = tx ty tz , (2) where = (tx, ty) corresponds to the 2D translations from the camera parameters, and tz is the computed depth. B.5. Motion Retargeting Our motion retargeting process mainly consists of two tasks: the optimization of human shape parameters β to fit human shapes to those of humanoid robot, and solve the humanoid motor DoF positions qrobot from adjusted human joint positions with inverse kinematics. Optimization of human shape parameters β. Given the forward kinematics of human body models in Eq. (3), we optimize the human shape parameters β with the Adam optimizer [25], using the loss L(β): L(β) = s.t. joints robot2, joints = Ff k(Phuman(β, θT , troot)). (3) (4) To avoid overfitting on robot which leads to too much deformation on the human model T-shaped pose, we set limit to the human shape parameters β: {1, 2, . . . , n}, β = (β1, β2, . . . , βn), βi < 5, (5) where denotes the size of the human shape parameters β. Solving humanoid motor DoF positions qrobot. With the optimal β and Eq. (6), we need to extract the motor DoF positions qrobot through inverse kinematics in Eq. (7). The inverse kinematics problem is solved by optimimzation with the loss Lik: Lik = Lr + λLs. (6) In Eq. (6), the retarget loss Lr: Lr(qrobot, sroot) = Frk(qrobot, sroot) Probot1, (7) Figure 4. Motion Retargeting, including optimization of human shape parameters and solving humanoid motor DoF positions. where sroot denotes robot root states including root translation and root orientation, Frk denotes robot forward kinematics which maps from qrobot, sroot to humanoid robot keypoint positions. Also in Eq. (6), the smoothing term Ls: Ls(qrobot) = n2 (cid:88) (2qrobot[i] qrobot[i 1] qrobot[i + 1]), i=1 (8) where is the number of frames of one motion sample trajectory, with the index ranging from 0 to 1. We use the Adam optimizer [25] to solve the inverse kinematics problem, where the weight of smoothing term λ = 0.05. B.6. Goal-conditioned Control Policy We use massively parallel simulation to train our goalconditioned humanoid RL control policy with Isaac Gym. In this subsection, we will introduce our training data, our policy, our training rewards and training parameters. Training Data. We selectively used portion of the CMU MoCap dataset in AMASS [37], in the form of SMPL models. We exclude motions that involve physical interactions with others, heavy objects, or rough terrain. We retarget from the training data to humanoid robot motion with the method introduced above, including humanoid keypoint"
        },
        {
            "title": "Hyperparameter",
            "content": "DoF Position Keypoint Position Root Linear Velocity Root Roll & Pitch Root Yaw exp(0.7qtar q) exp(ttar t) exp(4.0vtar v) exp(Ωϕθ tar Ωϕθ) exp(y) 3.0 2.0 6.0 1.0 1.0 Table 2. Imitation Rewards. Term Reward Expression Weight Height Time in Air Drag Contact Force Stumble DoF Acceleration Action Rate Energy Collision DoF Limit Violation DoF Deviation Vertical Linear Velocity Horizontal Angular Velocity Projected Gravity 1{F Fth) } 1new contact max(hfeet 0.2, 0) (cid:80) tair 1new contact (cid:80) vfoot Fth} (F 1{i, Fxy > 4F q2 at1 at q2 1collision 1qi>qmaxqi<qmin default qlow2 qlow v2 ωxy2 gxy2 2.0 10.0 -0.1 -3e-3 -2.0 -3e-7 -0.1 -1e-3 -10.0 -0.1 -10.0 -1.0 -0.4 -2.0 Table 3. Regularization Rewards. joint positions Probot, humanoid robot DoF positions qrobot and humanoid robot root states sroot. We can estimate the corresponding linear or angular velocities of humanoid DoFs and humanoid root joint from the humanoid motion data across frames. RL Control Policy. Our goal is to track the root movement goal for the whole body and the target expression goal for upper body, and our training data is introduced above. The humanoid control policy is defined with Eq. (8). The goal space can be formulated as = GeGm, where Ge includes joint angles and keypoint translations from the retargeting process above and the goal space for robot movement control Gm = v, rpy, where R3 is the linear velocity, rpy R3 is the robot pose in terms of row/pitch/yaw and is the body height. The observation includes robot proprieoception information ot = [ωt, rt, pt, y, qt, qt, at1]T where ωt is robot root angular velocity, rt, pt is roll and pitch, = yt is the difference between current and desired yaw angle, qt and qt is the joint position and angular velocity and at R27 is the target position of the joint proportional-derivative (PD) controllers. Training Rewards. In each step, the reward from the environment consists of motion rewards, root tracking rewards and regularization terms. To protect the fragile ankle roll joints on the robot hardware, we set the actions of the two joints to zero every simulation step. Motion rewards include DoF position reward and keypoint position reward, and root Discount Factor GAE parameter Timesteps per Rollout Epochs per Rollout Minibatches per Epoch Extropy Bonus (α2) Value Loss Coefficient (α1) Clip Range Reward Normalization Learning Rate # Environments Optimizer"
        },
        {
            "title": "Value",
            "content": "0.99 0.95 21 5 4 0.01 1.0 0.2 Yes 1e-3 6192 Adam Table 4. Training Parameters. tracking rewards include root linear velocity reward, root roll & pitch reward and root yaw reward. The imitation rewards, including motion rewards and root tracking rewards, are listed in Tab. 2, where qtar, R9 are the target and actual upper body DoF positions, ttar, R18 are the target and actual upper body keypoint positions, vtar, are the target and actual root velocity, Ωϕθ tar , Ωϕθ are the target and actual body roll and pitch. The regularization rewards are listed in Tab. 3, where hfeet is feet height, tair denotes the duration for which each foot remains in the air, 1new contact means new foot contact with the ground, Fxy , , Fth are foot contact force in horizontal plane and along the z-axis, and the contact force threshold respectively, q, are joint velocity and acceleration, at is action at timestep t, 1collision denotes selfcollision, qmax, qmin are limits for joint positions, and gxy is gravity vector projected on horizontal plane. Training Parameters. We use PPO with hyperparameters listed in Tab. 4 to train the policy. C. Details on Humanoid-X Dataset In this section, we will introduce the Humanoid-X dataset. We will introduce the data format and structure and show several examples of the dataset. C.1. Data Format and Structure For each motion sample in Humanoid-X, we expand them to the 5 data modalities introduced in Sec. 3.1, where they are described with V, , Phuman, Probot, Arobot. Visualization of part of the data samples in the dataset will be shown in Appendix C.4. Motion Video Clip V. The video clips are collected in MP4 format at frame rate of 20 frames per second (fps). Text Description . The text descriptions are stored in plain text (.txt) format. Human Poses Phuman. The human poses are sequences of SMPL model parameters with frame rate of 20 fps. We stored the collected data for each motion sample in NumPy (.npy) file. Humanoid Keypoints Probot. The humanoid keypoints include humanoid robot DoFs qrobot and humanoid robot root states sroot. Each frame of the data contains 27 DoFs of the robot configuration and 7-dimensional root state vector, consisting of 3-DoF root translation and 4-DoF quaternion representation for root orientation. The humanoid keypoints are recorded with frame rate of 20 fps. We stored the collected data (27 robot DoFs and 7-DoF root state) for each motion sample in NumPy (.npy) file for efficient data management and processing. Humanoid Actions Arobot. The humanoid actions are sequences of target DoF positions. The data is collected and stored at 50 fps, with each frame containing 27 robot DoFs that correspond to the robots physical configuration. We stored the collected data for each motion sample in NumPy (.npy) file. C.2. Data Statistics Sequence Length Analysis. We conduct comprehensive statistical analysis on both video sequence durations and their corresponding caption lengths, as illustrated in Fig. 5a and Fig. 5b. The analysis reveals that the majority of video clips are relatively short, with durations less than 10 seconds. This distribution pattern stems from our video segmentation strategy, where clips are specifically extracted when significant or meaningful motion patterns are detected within the continuous recordings. This approach naturally results in shorter, more focused segments, making longer clips relatively rare in our dataset. Regarding the textual descriptions, the distribution of caption lengths shows that most sentences contain fewer than 20 words. This concise nature of captions aligns with our guidelines, which emphasized brevity while maintaining descriptive accuracy. Vocabulary Analysis. To gain deeper insights into the linguistic composition of video captions, we conduct comprehensive analysis of different parts of speech, focusing on nouns, verbs, adjectives, and adverbs. This grammatical categorization helps understand how motions and actions are described in our dataset. Tab. 5 presents the vocabulary size distribution across these grammatical categories, providing quantitative view of the linguistic diversity in our annotations. The analysis reveals the richness of descriptive elements used in capturing robot motions and their contextual information. For verbs, the word cloud and the top-40 frequent words are shown in Fig. 6. It can be seen that verbs like doing, standing, playing, holding and performing occur with relatively high frequency. This implicitly matched the expectation since these words are heavily used as the"
        },
        {
            "title": "Total",
            "content": "3206 6048 1526 590 527 11897 Table 5. Vocabulary Sizes for Each Part of Speech. prompt for video collection. For nouns, the word cloud and the top-40 frequent words are shown in Fig. 7. The top 3 frequent words occurred are man, person and woman. This is also expected given the prompt for caption since it is specifically mentioned that the description should indicate what the man or woman is doing. For adjectives and adverbs, their word cloud and top-40 frequent words are shown in Fig. 8 and Fig. 9. It can been seen that most frequent adverbs are mostly about direction of motions and most frequent adjectives are mostly above color. This is cause by the fact that we explicitly instruct the Video-LLaMA to be concise so that there would not be redundant words for non-motion-related contents. (a) Video Length (b) Captioning Sentence Length Figure 5. Distribution of video length (in seconds) and captioning sentence length (in words), with the dotted line representing the average length. C.3. Data Preparation and Release We will fully release our data and code in the future, without violating the ethics concerns stated in Appendix A. C.4. Data examples from Humanoid-X Dataset We show visualized data examples from Humanoid-X in Fig. 11, Fig. 12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20, Fig. 21, Fig. 22, Fig. 23, Fig. 24, Fig. 25, Fig. 26, Fig. 27, Fig. 28, Fig. 29, Fig. 30, Fig. 31, Fig. 32. For motion video clips, we sample 5 frames from each video clip shown. For text, we directly present the text descriptions of the motions shown. For the human pose, we Figure 6. Verbs Word Cloud and Top-40 Frequent Verbs. Figure 7. Nouns Word Cloud and Top-40 Frequent Nouns. sample the SMPL visualization of the corresponding frames in the video clip. For the humanoid pose, we set the humanoid keypoints in MuJoCo and collected the MuJoCorendered images of the corresponding frames in the video clip. For humanoid actions, we render the humanoid control policy rollout in IsaacGym and collect the rendered images of the corresponding frames in the video clip. D. Detailed UH-1 Model Architecture D.1. UH-1 Action Tokenizer For given input sequence = [x1, x2, . . . , xT ] with xt Rd1 (representing either humanoid keypoint or an action), UH-1 Action Tokenizer is designed to reconstruct this sequence using learnable codebook = {c1, c2, . . . , cN } with cn Rd2 and learnable autoencoder with an encoder and decoder D. In this context, denotes the number of input frames, the codebook size, d1 the dimension of input tokens, and d2 the dimension of the codes. For sequence reconstruction, the encoder maps the input sequence into latent representations = E(X) = [z1, z2, . . . , zT /k] with zi Rd2 , where represents the temporal downsampling rate of the encoder. Each zi is subsequently quantized through the codebook into ˆz by selecting the nearest code cn C, which can be formally expressed as: ˆz = arg cnC min zi cn2. (9) Finally, the decoder reconstructs the original input sequence as Xre = D( ˆZ). The temporal downsampling enables each code in the codebook to represent input tokens, encoding primitive humanoid motion skills and facilitating the generation of smooth actions. In general, UH-1 Action Tokenizer is optimized by minimizing standard objective function: Lvqvae = Lrecon + Lembed + αLcommit, Lrecon = L1(X, Xre), Lembed = sg[Z] ˆZ2, Lcommit = sg[ ˆZ]2. (10) (11) (12) Figure 8. Adjectives Word Cloud and Top-40 Frequent Adjectives. Figure 9. Adverbs Word Cloud and Top-40 Frequent Adverbs. In this formulation, α is hyperparameter that regulates the relative influence of each loss term, and sg[] denotes the stop gradient operator. The embedding loss Lembed promotes the quantized codebook embeddings to move closer to the continuous output of the encoder, while Lcommit encourages the encoder to commit to particular codebook entries. Given the unique properties of humanoid keypoints and actions, we propose an adjusted reconstruction loss, Lrecon, which integrates forward difference loss and root regularization term: L1(X, Xre) + βL1([X], [Xre]) + γL1(X root re , 0), (13) where β and γ are hyperparameters for balancing the additional loss components, and [] represents the forward difference operator. D.2. UH-1 Transformer We formulate the language-conditioned humanoid keypoint or action generation tasks as auto-regressive prediction of the next codebook index. Formally, let si {1, 2, , } {End} denote the current index to predict, s1:i1 represent the preceding context of indices, and the language instruction embedding encoded by CLIP [45]. The UH-1 Transformer is then trained to model the conditional probability distribution (sis1:i1, l). special [End] token is incorporated into the indices set to signal the termination of sequence generation. For an input sequence = [x1, x2, , xT ], the encoder and codebook of the UH-1 Action Tokenizer map this sequence into the codebook indices as = [s1, s2, , sT /k, End]; given this sequence of indices S, it can also be mapped back to ˆZ = [cs1 , cs2 , , csT /k ], which is subsequently projected into the output space by the decoder as Xre = D( ˆZ). To train this transformer model, we minimize the negative log-likelihood over the training dataset D: Ltrans = (cid:88) SD log Π i=1 p(sis1:i1, l). (14) This objective encourages accurate predictions of the next index in the context of previous indices and language instructions. D.3. Implementation Details The implementation of our model architecture follows previous work [72]. For the UH-1 Action Tokenizer, we employ straightforward convolutional architecture consisting of 1D convolutions, residual blocks, and ReLU activation functions. Temporal downsampling and upsampling are achieved using convolutions with stride of 2 and nearestneighbor interpolation, respectively. The codebook size is configured as 2048 512, with downsampling rate = 4. During training, action sequences are cropped to temporal length of = 64. For the UH-1 Transformer, it is based on an 18-layer transformer model featuring 16 attention heads and dimensionality of 1, 024. Training the UH-1 Action Tokenizer and the UH1 Transformer on HumanoidML3D (a selected set of Humanoid-X) requires approximately 8 hours and 30 hours, respectively, on single NVIDIA RTXTM 6000 Ada GPU, while training on the full set of Humanoid-X requires approximately 40 hours and 400 hours, respectively. E. Experiment Details E.1. Real Robot Experiment Success Rate. The success rate of real robot pose control is evaluated using two criteria: (1) Stability: the humanoid robot must maintain stability while performing actions; any instance of falling or failing to maintain balance results in the humanoid robot (2) Accuracy: an unsuccessful trial. must accurately perform the desired actions based on text instructions. This is assessed by five human evaluators, and if the majority agree that the robot does not perform the actions correctly, the trial is considered unsuccessful. PD Controller. The output actions of our model are the target DoF positions for controlling the humanoid robot. We use the PD control to transform actions into motor torques τ , which can be represented as τ = Kp(a q) Kddq, (15) where Kp and Kd are the proportional coefficients of the motor position and speed errors respectively, is the current angle position of the motor rotor, and dq is the current rotor angular velocity of the motor rotor. We use the standard Kp and Kd provided in the official robot documents in our experiments. Real Robot Experiments. We demonstrate the real humanoid robot pose control with text instructions in Fig. 33, Fig. 34, Fig. 35, Fig. 36, Fig. 37, Fig. 38, Fig. 39. We also demonstrate human-humanoid interactions in Fig. 40 and Fig. 41. From these figures, we show that our method generates accurate and diverse poses to control the real humanoid robot with text instructions. Figure 10. Ablation on different RL policies, measured by task cumulative reward value. The solid line represents the mean return value, while the shaded regions correspond to the standard deviation, both calculated across five different random seeds. Our retargeted training data enhances the performance of the RL policy in tracking the imitation of body keypoints, joint positions, root orientation and root linear velocity. E.2. Ablation on Goal-conditioned Control Policy To investigate the impact of humanoid keypoints on the goal-conditioned RL policy, we compare our motion retargeting approach, originated from [19], with another approach in [9]. We evaluate the quality of the humanoid keypoints generated by different motion retargeting methods by measuring the tracking rewards in the subsequent reinforcement learning step, maintaining other factors as the same. As illustrated in Fig. 10, we launch experiments in five random seeds for both methods. We empirically found that our motion retargeting method improves the performance of the RL policy on the evaluation metrics in [9] tracking the imitation of body keypoints, joint positions, root orientation and root linear velocity in the form of training rewards. The results show that our retargeted data enhances the performance of the RL policy, thus suggesting that our retargeting method can generate humanoid pose data more executable for humanoid robots. Figure 11. Data samples in Humanoid-X. Figure 12. Data examples in Humanoid-X. Figure 13. Data examples in Humanoid-X. Figure 14. Data examples in Humanoid-X. Figure 15. Data examples in Humanoid-X. Figure 16. Data examples in Humanoid-X. Figure 17. Data examples in Humanoid-X. Figure 18. Data examples in Humanoid-X. Figure 19. Data examples in Humanoid-X. Figure 20. Data examples in Humanoid-X. Figure 21. Data examples in Humanoid-X. Figure 22. Data examples in Humanoid-X. Figure 23. Data examples in Humanoid-X. Figure 24. Data examples in Humanoid-X. Figure 25. Data examples in Humanoid-X. Figure 26. Data examples in Humanoid-X. Figure 27. Data examples in Humanoid-X. Figure 28. Data examples in Humanoid-X. Figure 29. Data examples in Humanoid-X. Figure 30. Data examples in Humanoid-X. Figure 31. Data examples in Humanoid-X. Figure 32. Data examples in Humanoid-X. Figure 33. Real robot demonstrations. Text instruction: Shooting Ball to the Basket. Figure 34. Real robot demonstrations. Text instruction: Playing Guitar. Figure 35. Real robot demonstrations. Text instruction: Putting in Golf Tournament. Figure 36. Real robot demonstrations. Text instruction: Waving to Friend. Figure 37. Real robot demonstrations. Text instruction: Playing Violin. Figure 38. Real robot demonstrations. Text instruction: Punching with the Right Hand. Figure 39. Real robot demonstrations. Text instruction: Playing Drums. Figure 40. Demonstration of human-humanoid interactions. Text instruction: High-Five. Figure 41. Demonstration of human-humanoid interactions. Text instruction: Embrace."
        }
    ],
    "affiliations": [
        "Toyota Research Institute",
        "UC Berkeley",
        "University of Southern California"
    ]
}