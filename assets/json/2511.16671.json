{
    "paper_title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "authors": [
        "Ziyu Guo",
        "Renrui Zhang",
        "Hongyu Li",
        "Manyuan Zhang",
        "Xinyan Chen",
        "Sifan Wang",
        "Yan Feng",
        "Peng Pei",
        "Pheng-Ann Heng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 7 6 6 1 . 1 1 5 2 : r Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation Ziyu Guo1, Renrui Zhang2, Hongyu Li3, Manyuan Zhang3, Xinyan Chen2 Sifan Wang, Yan Feng3, Peng Pei3, Pheng-Ann Heng1 CUHK 1IMIXR & 2MMLab 3Meituan Project Page: https://think-while-gen.github.io"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TWIG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TWIG-50K dataset, and reinforcement learning (RL) via customized TWIG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/ Thinking-while-Generating. 1. Introduction Visual generation have developed rapidly with diffusion [37, 39, 40] and autoregressive [7, 45, 52] models, enabling high-fidelity synthesis across diverse domains [32, 35, 56]. Despite impressive visual quality, todays generators often struggle with long-horizon composition, multi-entity relations, and adherence to nuanced textual instructions. Starting from Generation with CoT [17, 46], growing line of work Equal Contribution Project Lead Figure 1. Interleaving Textual Reasoning throughout Visual Generation. Inspired by the image-interleaved reasoning in textual responses [8, 33, 43, 59], we reverse the modality flow and weave textual thoughts into the unfolding canvas, delivering on-the-fly guidance and reflection throughout synthesis. explores reasoning as remedy, typically injecting chain-ofthoughts in the language modality to assist visual synthesis. Existing CoT-based approaches can be grouped by where the textual reasoning is applied, as compared in Figure 2: Think before generation as pre-planning aid. Methods [11, 23, 27] first produce structured or free-form plan, e.g., detailed captions, scene layouts, or object attributes and relations, and then condition the image generator on this plan. This improves global coherence and entity placement, but the plan is fixed once generation begins, limiting nuanced guidance and mid-course correction. 1 Figure 2. Comparison of Where the Textual Reasoning is Applied in Visual Generation: (a) Think-before-Generation [11, 23, 27] injects pre-planning thought prior to the synthesis, limiting fine-grained control and later correction; (b) Think-after-Generation [17, 26, 36] verifies and revise the image once it is complete, lacking nuanced, timely adjustment with extra inference cost; (c) Our Thinking-while-Generating interleaves thoughts and reflections throughout the synthesis, providing on-the-fly. co-evolving guidance. Think after generation as post-refinement stage. Methods [17, 26, 61] synthesize the entire image first, and then elicit textual feedback via self-critique or external verifiers, iteratively revising the visual errors. These approaches help with local fixes and attribute binding, but reasoning is only loosely coupled to the synthesis trajectory without fine-grained, timely revision and, importantly, incur additional, costly extra inference rounds. Given these limitations in visual generation, we note complementary trend in visual understanding: recent large multimodal models (LMMs) [9, 13, 25, 57, 59] perform image-text interleaved reasoning, adaptively weaving intermediate visual evidence (e.g., detected objects, zoomed-in regions, or tagged images) into textual CoTs to improve interpretation and analysis. Inspired by this paradigm, we pose natural question: as illustrated in Figure 1, Can we invert the flow and interleave text into the intermediate visual generation process, providing on-the-fly, co-evolving reasoning that guides synthesis as it unfolds? In this preliminary study, we present the first interleaved framework for visual generation that keeps textual reasoning in the loop, termed as Thinking-while-Generating (TWIG), as compared in Figure 2 (c). As our approach is compatible with multiple models and task settings, for clarity and future extensibility, we adopt the unified understanding-generation LMM (ULM) [7, 50, 52, 60] with autoregressive generation paradigms, e.g., Janus-Pro [7], and experiment on text-toimage scenarios in our study. Given text prompt, the model first interprets the instruction and plans an optimal interleave schedule, i.e., how many steps to use and how to partition the canvas into local regions for progressive synthesis. While generating each region, the model conducts on-the-fly textual reasoning and grounds its thoughts in the current partial visual state. This interleaved think step serves two roles: (i) it produces nuanced guidance for the upcoming synthesis, and (ii) it critiques and reflects on the previously generated content. In this way, textual reasoning co-evolves with the visual modality, providing detailed, step-by-step directives. The image can be dynamically revised and precisely steered as it unfolds within single generative trajectory. We consider three candidate routes for Thinking-whileGenerating, and investigate which, if any, proves effective: Can zero-shot prompting alone achieve the goal? We craft interleave-aware prompts to directly elicit global plans and reasoning thoughts. This route reveals the latent capacity of ULMs to self-organize interleaved reasoning without parameter updates, but can suffer instability. Does supervised fine-tuning (SFT) benefit the performance? We categorize the understanding and generation process into nine subtasks, and curate dataset, TWIG2 50K, for fine-tuning ULM, aiming to improve instruction adherence and reduce visual hallucination. Will reinforcement learning (RL) further unlock its potential? We optimize the interleaved reasoning policy of ULM via customized GRPO [42] algorithm, TWIGGRPO, to push the performance boundary, investigating different RL approaches and reward designs. Our experiments indicate that the ULM itself exhibits strong zero-shot capability for Thinking-while-Generating. With carefully designed prompts, it substantially improves Janus-Pro on T2I-CompBench(++) [19, 20] without additional training. Building on this, SFT with TWIG-50K provides further modest yet consistent gains, leading to more stable behavior compared with the zero-shot baseline. Finally, optimization with our TWIG-GRPO algorithm yields considerable improvements, underscoring the value of RL for deciding when to think, what to say, and how to refine. Taken together, these findings, though preliminary, are informative: they demonstrate the feasibility of interleaving textual reasoning during generation, and highlight this direction as promising avenue for advancing visual synthesis. It is worth noting that two relevant concurrent works, IRG [21] and Uni-CoT [36], attempt to interleave reasoning with generation, but still treat the visual systhesis process as monolithic block, like combination of thinkbefore-generation and think-after-generation. They are wellperformed with unique insights, but not truly interleaving reasoning within the generative process itself, limiting the granularity and controllability. 2. Thinking-while-Generating In Section 2.1, we first introduce the design scope and applicability of Thinking-while-Generating. Then, in Section 2.2, we present its overall pipeline and core components of the framework in detail. 2.1. Scope and Applicability Aiming for generalization and extensibility, Thinking-whileGenerating is conceptually compatible with diverse settings along the following three axes: System Architecture. The framework can be instantiated either as (i) pipeline that couples text-to-image model [2, 10, 37] with an LMM [1, 29, 54], where the LMM specializes in producing interleaved reasoning for the text-to-image outputs; or (ii) ULM [7, 52, 60] that performs textual reasoning and visual generation within single backbone. Generation Paradigm. The framework is applicable for visual generation with diffusion [12, 30, 38], discrete diffusion [4, 52, 53], and autoregressive models [7, 44, 45]. For continuous diffusion models, textual thoughts are interleaved at selected denoising steps; for discrete diffusion and autoregressive models, thoughts are inserted between segments of visual tokens to guide upcoming spans. Task Scenarios. The framework applies beyond T2I, e.g., image-to-image [3, 18, 56], text-to-video [14, 16, 47], textto-3D [15, 28, 35], and related generative tasks: as long as an LMM (or ULM) can provide reasoning thoughts for the target modality, they can be interleaved to steer generation. As preliminary study, we adopt single ULM with an autoregressive generation paradigm (e.g., Janus-Pro [7]) for clarity of exposition, promising headroom, and end-to-end training efficiency. We denote its understanding forward pass by ULMu and the generation forward pass by ULMg. 2.2. Framework Overview Figure 3 presents the overall Thinking-while-Generating (TWIG) framework, which interleaves textual reasoning with visual generation through three schemes: when to think, what to say, and how to refine. When to Think (Scheduling). Given an input prompt , ULMu first determines an interleaved reasoning schedule, denoted as = {Vk}K k=1, according to: = ULMu(T ), where each Vk denotes target visual region at which reasoning is applied (e.g., token spans in autoregressive and discrete diffusion models, or timestep windows in continuous diffusion models). This decouples the generation process into smaller, more controllable sub-tasks guided by the interleaved textual reasoning. Scheduling can be static (fixed K, uniform spacing) or adaptive (variable K, content-dependent Vk). In Section 3.1, we investigate different schedules and find that static schedule with = 3 performs the best, based on the heuristic that most images consist of three semantic components: upper background, central content, and lower background. Additionally, current capabilities of ULMu are limited in reliably generating well-structured adaptive schedules, which remains future work. What to Say (Reasoning Content). At each scheduled reasoning point, ULMu provides textual thought τk intended to guide the generation of the visual region Vk. This thought serves as localized sub-prompt exclusively targeted at Vk, offering finer-grained guidance and alignment than prior think-before-generation approaches. The generation of τk is conditioned on three elements, i.e., the input prompt , the previous thoughts {τj}j<k, and the visual content generated for prior regions {Vj}j<k, formulated as: τk = ULMu(T, {τj}j<k, {Vj}j<k). 3 Figure 3. Overall Pipeline of Thinking-while-Generating. The framework comprises three components: When to Think for globally determining the interleaved generation schedule; What to Say for producing the step-by-step textual thought as fine-grained guidance; and How to Refine for region-level reflection on the current canvas with optional corrective updates. ULMu and ULMg denote to apply single ULM for understanding and generation, respectively. This allows τk to incorporate accumulated contextual information and to plan appropriately for the next visual segment. Subsequently, ULMg synthesizes the target region Vk, conditioned on all reasoning thoughts and the visual content produced up to by: Vk = ULMg({τj}jk, {Vj}j<k). It is important to note that ULMg is only required to possess text-to-image capabilities, no need for image-to-image functionality. This is because the visual context {Vj}j<k is not provided as image input to the model. Instead, we directly extend the textual pre-context from {τj}j<k to {τj}jk at the beginning of the token sequence, while preserving the generated visual content {Vj}j<k unchanged at the end of the sequence. This modification preserves the autoregressive generation process within single trajectory, without introducing discontinuities or new generation rounds, as illustrated in Figure 4 (a). How to Refine (Reflection). After generating each visual region Vk, we allow ULMu to perform an immediate, regionlevel revision step that couples visual critique and an optional correction process. This enables finer-grained corrections while significantly reducing computational cost compared to prior think-after-generation approaches that conduct global post-revision. Before producing the next reasoning thought τk+1, ULMu first generates reflection tuple ck = (rk, ˆτk), given all the generated textual and visual contents as: ck = ULMu(T, {τj}jk, {Vj}jk), where rk [0, 100] is an integer representing the critic score assigned to the current region Vk, and ˆτk is revised sub-caption intended for potential correction. The score rk evaluates the semantic alignment and visual coherence of Vk with respect to its guiding prompt τk. If rk exceeds predefined threshold θ, the model proceeds directly to generate the next reasoning thought without revision. Otherwise, local 4 Table 1. Zero-shot Experiments of Thinking-while-Generating on T2I-CompBench [19]. We denote our zero-shot model as TWIGZS, and mark the improvement over the baseline, Janus-Pro-7B [7]. Panels (a), (b), (c), and (d) present four ablation studies. Setting Attribute Binding Object Relationship Complex Color Shape Texture Spatial Non-Spatial v.s. Baseline Janus-Pro-7B [7] TWIG-ZS Improve 63.59 73.11 +9.52 35.28 41.55 +6.27 49.36 64.77 +15.41 20.61 21.98 +1. Think-before-Gen. Think-after-Gen. Thinking-while-Gen. = 2 = 3 = 4 (a) Where the Textual Reasoning is Applied 65.12 64.72 73.11 72.79 73.11 72.95 36.20 37.95 41. 51.05 50.62 64.77 20.88 21.05 21.98 (b) Interleaved Reasoning Step 42.26 41.55 41.90 64.64 64.77 64.70 21.97 21.98 22. (c) How to Partition Vk in Space Uniform Spacing Adaptive Spacing 73.11 72.43 41.55 40.88 64.77 63.92 21.98 21. (d) Whether to Perform Reflection w/o Reflection 1-round Reflection 2-round Reflection 73.11 73.90 73.68 41.55 46.02 45.72 64.77 66.10 66.02 21.98 24.50 24. 30.85 30.90 +0.05 30.82 30.87 30.90 30.89 30.90 31.10 30.90 30.88 30.90 30.81 30.88 35.59 48.16 +12. 41.75 42.28 48.16 49.71 48.16 48.90 48.16 47.39 48.16 51.97 51.65 3.1. Zero-shot Prompting Prompt Customization. To elicit satisfactory zero-shot Thinking-while-Generating, we meticulously design series of interleave-aware prompts for ULM, corresponding to the three components described in Section 2.2. Please refer to the final prompt templates in the Supplementary Material. For when to think, we prompt the model to adopt global view, sketching the images high-level semantics and structure step by step from the input prompt. For an adaptive schedule, we additionally prompt the model to output the relative ratios of visual parts across the canvas. For what to say, we guide the model to focus strictly on the local region currently being generated while maintaining coherence with previously generated visual and textual context. We discourage any spatial-anchor tokens; the model should produce only the descriptive content. For how to refine, we prompt the model to provide critic score evaluating along five criteria (color accuracy, object completeness, detail richness, spatial relationships, and visual coherence), ensuring consistent standard across cases. The template enforces that any revision is local and does not contradict validated prior regions. Figure 4. Illustration of Interleaved Token Sequence: (a) In What to Say, the textual pre-context extends from {τj}j<k to {τj}jk (K = 2), guiding the generation of the next Vk while leaving the earlier {Vj}j<k untouched; (b) In How to Refine, the thought τk is revised to ˆτk, and only the local region ˆVk is re-generated to replace Vk. Neither operation requires the ULM to possess image-to-image capabilities, and both preserve single text-to-image generation trajectory without launching fresh pass or full re-generation. reflection is triggered to refine only the current sub-region, guided by ˆτk, as defined by: ˆVk = ULMg({τj}j<k, ˆτk, {Vj}j<k). This localized corrective mechanism mitigates the accumulation of visual misalignments with timely revision. Likewise, as presented in Figure 4 (b), we directly update the textual pre-context from τk to the revised ˆτk, and re-generate only the local part ˆVk to replace Vk at the end of the token sequence, which also preserves single trajectory without requiring the costly full re-generation. In sum, Thinking-while-Generating (i) first schedules number of interleaved reasoning points (when); then for each = 1, . . . , K, (ii) produces textual thought that locally steers the next visual update (what); and (iii) performs region-level reflection with optional correction (how). The loop of (ii) and (iii) preserves single generative trajectory, enabling on-the-fly guidance and precise local revision. 3. Implementation Exploration In this section, we implement three candidate approaches for Thinking-while-Generating: zero-shot prompting (3.1), supervised fine-tuning (3.2), and reinforcement learning (3.3). We present experimental results that highlight their respective strengths. Please refer to detailed experimental settings and visualizations in the Supplementary Material. Experiments and Analysis. In Table 1 (top), we present the performance of our zero-shot model, TWIG-ZS. We observe that our carefully designed prompts yield surprisingly strong improvements over the baseline, significantly surpassing Janus-Pro-7B [7] across multiple dimensions. This highlights the potential of our framework and its natural applicability within current ULMs, making the zero-shot variant strong foundation for subsequent SFT and RL. By 5 Table 2. SFT Experiments of Thinking-while-Generating on T2ICompBench [19]. We denote our fine-tuned model as TWIG-SFT, and mark the improvement over TWIG-ZS. Panel (a) ablates the varying proportions of thinking (T), generation (G), and reflection (R) data in TWIG-50K. Panel (b) reports the standard deviation (Std) across random seeds to assess stability. Model / Setting Data Attribute Binding Object Relationship / / Color Shape Texture Spatial Non-Spatial Complex Janus-Pro-7B [7] TWIG-ZS TWIG-SFT Improve Think-heavy Gen-heavy Think-Gen-equal Reflect-lite Reflect-heavy TWIG-ZS Std TWIG-SFT Std v.s. Baseline 63.59 73.11 74.58 +1.47 35.28 41.55 52.42 +10. 49.36 64.77 67.95 +3.18 20.61 21.98 27.02 +5.04 (a) Effect of Training Data Composition 73.38 74.12 74.58 72.76 71.88 50.92 51.77 52.42 49.75 48.98 66.47 67.28 67.95 65.93 65. 26.08 26.58 27.02 26.36 25.62 (b) Stability across 5 Random Seeds 0.82 0.65 0.70 0.59 0.76 0.61 0.45 0. 30.85 30.90 31.24 +0.34 30.97 31.09 31.24 30.92 30.84 0.38 0.36 35.59 48.16 53.41 +5.25 51.86 52.83 53.41 51.17 50.27 0.91 0. Table 3. RL Experiments of Thinking-while-Generating on T2ICompBench [19]. We denote our reinforced model with GRPO [41] as TWIG-RL, and mark the improvement over the TWIG-SFT. Panels (a) and (b) present the results of two ablation studies. Setting Attribute Binding Object Relationship Complex Color Shape Texture Spatial Non-Spatial Janus-Pro-7B [7] TWIG-ZS TWIG-SFT TWIG-RL Improve ULMg-GRPO ULMu-GRPO TWIG-GRPO Human Preference + Object Grounding ++ VQA Consistency +++ LMM Alignment v.s. Baseline 35.28 41.55 52.42 61.28 +8.86 49.36 64.77 67.95 73.19 +5.24 20.61 21.98 27.02 34.06 +7.04 (a) TWIG-GRPO Strategy 59.87 57.94 61.28 72.01 70.68 73. 32.47 30.93 34.06 (b) Reward Model Ensemble 60.97 60.01 59.29 61.28 71.35 73.79 74.26 73.19 20.68 25.84 30.05 34.06 63.59 73.11 74.58 82.49 +7. 80.12 78.36 82.49 79.83 80.44 80.87 82.49 30.85 30.90 31.24 31.99 +0.75 31.30 31.27 31.99 30.53 31.15 31.41 31.99 35.59 48.16 53.41 54.45 +1. 54.02 53.76 54.45 52.87 54.03 53.64 54.45 default, we adopt an interleaved schedule with = 3 and uniform spacing, and permit at most one round of reflection. We conduct four ablations: Ablation (a): Thinking-while-Generating versus Thinkbefore/after-Generation under identical zero-shot settings. Interleaving provides nuanced, on-the-fly guidance rather than only pre-planning or post-refinement, and consistently outperforms the alternatives. Ablation (b): Number of interleaved reasoning steps under uniform schedule. We find = 3 is optimal, aligning with the heuristic that many images decompose into three semantic components: upper background, central content, and lower background. Ablation (c): Adaptive scheduling of interleaved spacing. Despite exploring multiple prompting strategies, current ULMs struggle to reliably follow such instructions, leading to unstable or poorly structured adaptive schedules. Ablation (d): Effectiveness of reflection during reasoning. single reflection round corrects misalignments and improves performance across aspects; however, conducting two rounds brings no further gains, likely limited by the critiqueandrevision capacity of zero-shot ULMs. 3.2. Supervised Fine-tuning SFT Task Formulation. Building on the zero-shot baseline, we investigate whether SFT can enhance the capabilities. We decompose the Thinking-while-Generating process into nine supervised tasks that mirror the inference loop, using fixed number of three reasoning steps. These comprise three thinking targets for ULMu (upper/central/lower thoughts), three reflection targets for ULMu (three scores with revised thoughts), and three generation targets for ULMg (three visual regions). This enables the model to learn structured reasoning, localized reflection, and regionwise generation in an interleaved, context-aware manner. TWIG-50K Dataset. To support the task formulation, we curate high-quality dataset termed TWIG-50K. The construction process comprises multiple stages of synthetic supervision using advanced commercial models. For what to say (17K, three tasks), we source 5.5K text prompts from the training split of T2I-CompBench [19], and adopt GPT-4o [22] to generate stepwise sub-captions that segment the image into three coherent parts (uplower background). per background, central content, These sub-captions are concatenated and fed to GPT-4oImage [22] to synthesize images that are semantically consistent with the specified divisions. We then filter lowquality instances and organize them into interleaved formats aligned with the Thinking-while-Generating protocol. Note that, since the reasoning step count is fixed to three, we do not collect supervision data for when to think. For how to refine (17K, three tasks), building on the interleaved samples above, we construct three visual understanding tasks focused on critique and revision. GPT-4o is prompted to evaluate each region by assigning critic score along five criteria (the same as zero-shot settings) and to provide revised sub-caption that addresses deficiencies identified by the critique. If the original image attains high score, the revised thought simply repeats, case that may not trigger re-generation during inference. To enhance the generation capability of ULMg (16K, three tasks), we construct interleaved visual generation data from the imagesub-caption pairs obtained in the when/what stage. Each training instance conditions the generation of region Vk on cumulative reasoning thoughts {τj}jk and previously generated visual contents {Vj}j<k. Note that this remains text-to-image su6 Table 4. Performance Comparison on T2I-CompBench++ [20]. The best and the second-best scores are highlighted. Model Attribute Binding Object Relationship Color Shape Texture 2D-Spatial 3D-Spatial Non-Spatial Numeracy Complex Show-o [52] SD-XL-base-1.0 [34] Attend-and-Excite [5] PixArt-α [6] GoT [11] Show-o + PARM [17] FLUX.1 [24] Emu3 [49] T2I-R1 [23] Janus-Pro-7B [7] (Baseline) TWIG-ZS TWIG-SFT TWIG-RL 56 58.79 64.00 66.90 65.51 75 74.07 75.44 81.30 63.59 73.11 74.58 82.49 41 46.87 45.17 49.27 50.08 56 57.18 57.06 58.52 35.28 41.55 52.42 61.28 Current Generative Models 46 52.99 59.63 64.77 58.36 66 69.22 71.64 72. 20 21.31 14.55 20.64 24.57 29 28.63 - 33.78 Thinking-while-Generating 49.36 64.77 67.95 73.19 20.61 21.98 27.02 34.06 - 35.66 32.22 - - 38.66 - - 32.94 33.68 35.57 38.87 30 31.19 31.09 31.97 31.13 31 31.27 - 30.90 30.85 30.90 31.24 31.99 - 49.91 47.73 - - - 61.85 - 60.97 41.32 36.58 51.70 61.93 29 32.37 34.01 34.33 37.54 37 37.03 - 39. 35.59 48.16 53.41 53.56 pervision to preserve single generation trajectory (not image-to-image), augmented with visual pre-context. Experiments and Analysis. In Table 2 (top), we present the performance of our fine-tuned model, TWIG-SFT. Relative to the zero-shot baseline (TWIG-ZS), SFT delivers modest and reliable gains across benchmarks, with the largest improvements on Shape and Spatial categories. This demonstrates the effectiveness of our fine-tuning recipe and the curated TWIG-50K dataset. By default, we inherit the optimal model settings from TWIG-ZS, and adopt balanced data mixture with equal thinking and generation tasks. We further provide two analyses: Ablation (a): Effect of data composition from TWIG-50K. Balancing thinking (T) and generation (G) provides the best trade-off and strengthens Thinking-while-Generating from both sides. However, adding reflection data (R) degrades the results, where the thoughts become longer and over-corrections appear more frequently. This suggests that, TWIG-ZS already exposes most of the models reflection proficiency, and oversupplying diverts capacity away from learning stable and behaviors. Although the reflection subset cannot contribute here, we hope it will facilitate future research on critique-and-revise training. Comparison (b): Inference stability across five random seeds. We report the standard deviation (Std) over different runs, and observe that SFT notably tightens dispersion compared to TWIG-ZS, indicating more predictable behavior. Qualitatively, SFT shortens verbose thoughts, curbs hallucinations, improves attribute persistence across adjacent regions, and reduces spurious reflection triggers near the decision threshold. 3.3. Reinforcement Learning TWIG-GRPO Strategy. To further advance performance, we employ RL to enhance the interleaved reasoning. Specifically, we adopt the GRPO algorithm [41] with the training prompts from T2I-CompBench, and tailor it to our Thinkingwhile-Generating framework. Within this setup, the ULM performs multiple forward passes within single rollout during GRPO training. key design question is which components should be reinforced through the reward mechanism: all stages, or only the understanding or generation phases? We propose to reinforce all of them simultaneously through our TWIG-GRPO strategy. Concretely, we compute single reward based on the final generated image and the input prompt, and utilize it as shared reward to optimize the policies of every thinking, generation, and reflection pass jointly. This approach not only simplifies implementation (no need to compute rewards for each local visual subtask), but also enables consistent reinforcement across ULMu and ULMg, allowing global information to flow across different paths and thereby enhancing the overall synergy of the TWIG framework. Reward Model Design. Since high-quality image must satisfy multiple aspects (overall aesthetics, object attributes and relationships), we explore to combine complementary reward models for joint optimization and mitigating reward hacking [23]: (i) human preference scores (HPS v2 [51]), (ii) object grounding scores (GroundingDINO [31]), (iii) VQA consistency scores (GIT [48]), and (iv) LMM alignment scores (the fine-tuned ORM [17]). We utilize an unweighted average of the four reward model, and this simple strategy effectively leverages our frameworks generality for RL gains. 7 Figure 5. Qualitative Comparison of TWIG Variants: the baseline (Janus-Pro-7B [7]), TWIG-ZS, -SFT, and -RL. Our method demonstrates progressive improvements in compositional fidelity, object counting, and visual realism. Figure 6. The Reflection Capacity of TWIG-RL. The reflection within our Thinking-while-Generating refines both semantic and visual consistency, e.g., improving spatial alignment, shadow coherence, and overall realism across diverse prompts. Experiments and Analysis. In Table 3 (top), we present the performance of our reinforced model, TWIG-RL. Compared with TWIG-SFT, the initialization point, RL delivers substantial gains, e.g., exceeding +5%, across the three Attribute Binding categories and the Spatial category. This highlights the remaining headroom of the Think-whileGenerating paradigm once policy is guided in right direction with an appropriate GRPO strategy and reward ensemble designs. In Table 4, we report the three TWIG approaches in comparison with current generative models on T2I-CompBench++ [20]. Our method offers flexible trade-off between implementation efficiency (ZS) and competitive performance (RL), allowing practitioners to balance the cost and quality according to deployment needs. Furthermore, in Figures 5, 6, and 7, we present three visualizations, i.e., illustrating the improvements across different variants, the reflection capability, and the image-text interleaved reasoning process, respectively, which highlight the qualitative effectiveness of our methods. Ablation (a): Different strategies for GRPO algorithms. Our TWIG-GRPO jointly reinforces all (up to nine) local visual subtasks within single rollout. We investigate to separately optimize the understanding-related tasks (thinking and reflection) and the generation-related tasks, each using the shared reward to update ULMu and ULMg, respectively. As compared, the separate enhancements fail to surpass the joint strategy, highlighting their complementary nature and mutual reinforcement. Only when combined under the full TWIG-GRPO strategy can the RL potential of the interleaved reasoning be fully realized. Ablation (b): Ensemble of multiple reward models. We begin with single HPS v2, and progressively incorporate other three rewards. HPS v2 primarily improves global aesthetics and stylistic coherence; GroundingDINO tightens entity presence and localization; GIT curbs instruction violations and strengthens attribute consistency; the fine-tuned ORM improves holistic textimage alignment. Adding components steadily improves performance, and the ensemble of four achieves the best overall balance. 8 Figure 7. Thinking-while-Generating Process of TWIG-RL. Each example showcases how the model iteratively interleaves its textual reasoning and visual outputs, progressively improving compositional accuracy, spatial alignment, and scene coherence. 9 4. Conclusion In this paper, we introduce the Thinking-while-Generating (TWIG) paradigm, an interleaved framework that keeps textual reasoning in the loop during visual generation. Starting from carefully designed zero-shot prompts, then enhancing with SFT, and finally optimizing policy via RL, our TWIG model learns to think, generate, and reflect within single visual generation trajectory. We hope this paradigm may inspire future research to fully investigate the potential of interleaved visual generation schemes. Limitations. Given the incapacity of current ULMs, our when to think utilizes fixed three-step schedule, which is general but not optimal. As more capable models emerge, learning fully adaptive schedules is promising next step. Second, our RL setup employs the original GRPO, already strong, but may be further enhanced by recent variants [55, 58]. Finally, extending TWIG to video, 3D, or image-toimage tasks presents another compelling avenue."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 3 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 3 [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 7 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 7 [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1, 2, 3, 5, 6, 7, 8 [8] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. arXiv preprint arXiv:2506.05331, 2025. [9] Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, et al. Codeplot-cot: Mathematical visual reasoning by thinking with code-driven images. arXiv preprint arXiv:2510.11718, 2025. 2 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [11] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 1, 2, 7 [12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 3 [13] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. InterleavedIn Proceedings of the Computer modal chain-of-thought. Vision and Pattern Recognition Conference, pages 19520 19529, 2025. 2 [14] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, 2025. [15] Ziyu Guo*, Renrui Zhang*, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023. 3 [16] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. 3 [17] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. 1, 2, 7 [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3 [19] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for openworld compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 3, 5, 6 [20] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 3, 7, 8 [21] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-to-image generation. arXiv preprint arXiv:2509.06945, 2025. 3 [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [23] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. 1, 2, 7 [24] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [25] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 2 [26] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025. 2 [27] Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin, Yu Cheng, and Lijuan Wang. Imagegen-cot: Enhancing textto-image in-context learning with chain-of-thought reasoning. arXiv preprint arXiv:2503.19312, 2025. 1, 2 [28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution textIn Proceedings of the IEEE/CVF to-3d content creation. conference on computer vision and pattern recognition, pages 300309, 2023. 3 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [30] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European conference on computer vision, pages 423439. Springer, 2022. 3 [31] Siyi Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chun yue Li, Jianwei Yang, Hang Su, Jun-Juan Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. ArXiv, abs/2303.05499, 2023. 7 [32] OpenAI. Sora 2 system card. Technical report, OpenAI, 2025. 1 [33] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 7 [35] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 1, 3 [36] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. 2, 3 [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 3 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1 [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Jie Hou, Alexander Kolesnikov, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [41] Zhenming Shao, Jiayi Gu, Ziyang Wang, Liang Ding, Yi Wang, Yao Zhang, Shuming Tang, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Introduces Group Relative Policy Optimization (GRPO) used for RL training. 6, 7 [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3 [43] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 1 [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 3 [45] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 3 [46] Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [47] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3 11 [61] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inferencetime optimization for text-to-image diffusion models via reflection tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1532915339, 2025. 2 [48] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 7 [49] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 7 [50] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2 [51] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [52] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2, 3, 7 [53] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowImproved native unified multimodal models. arXiv o2: preprint arXiv:2506.15564, 2025. 3 [54] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3 [55] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 10 [56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 1, 3 [57] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 2 [58] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. [59] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 1, 2 [60] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2,"
        }
    ],
    "affiliations": [
        "CUHK",
        "IMIXR",
        "MMLab",
        "Meituan"
    ]
}