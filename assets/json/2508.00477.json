{
    "paper_title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer",
    "authors": [
        "Yuzhuo Chen",
        "Zehua Ma",
        "Jianhua Wang",
        "Kai Kang",
        "Shunyu Yao",
        "Weiming Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 7 4 0 0 . 8 0 5 2 : r LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer Yuzhuo Chen1 Zehua Ma1 Jianhua Wang2 Kai Kang3 Shunyu Yao2 Weiming Zhang1 1University of Science and Technology of China 2Onestory Team 3East China Normal University http://github.com/Suchenl/LAMIC"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in trainingfree manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BGS) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BGS, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMICs superior abilities in identity keeping, background preservation, layout control, and promptfollowing, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMICs performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC Project Leader Creating consistent and controllable visual content is core challenge in digital filmmaking, storyboarding, and narrative illustration [31]. In these domains, artists often need to construct scenes that involve multiple entitiessuch as characters and environmentswhile maintaining visual and stylistic consistency across varying perspectives and story beats. With the rapid progress of diffusion-based generative models, there is growing interest in leveraging such models to automate and accelerate the generation of entityconsistent, layout-controllable images guided by both textual and visual inputs [27]. Recent advances in image generation have introduced impressive capabilities in text-to-image (T2I) and imageto-image (I2I) synthesis [1, 19]. More recently, multimodal text-and-image-to-image (T&I2I) models, especially FLUX.1-Kontext released on May 29, 2025 [11], have demonstrated the great potential of combining textual descriptions with visual references to generate semantically grounded and identity-consistent images. However, as single-reference model, it still remains significantly limited in handling multiple reference images. To enable multiimage reference generation, some methods have introduced trainable extension modules into the fundamental T2I models [2], while others have retrained slightly modified T2I architecture [24, 28]. However, these training-based methods face challenges in generalization performance when combining more images, as large-scale datasets with multiple image references are difficult to collect [2]. In addition, many of these methods lack spatial layout capabilities, which limits their application in real scenarios. These limitations become particularly problematic in creative production workflows. For instance, in storyboard generation for films or animations, it is crucial to consistently generate the same multiple characters, objects and scenes which conform to explicit layout plans-such as character positioning, scene framing, or camera perspective determined by directors or artists. Previous studies have investigated layout control in T2I systems, which can be mainly divided into training-based [21, 30] and trainingfree [3, 29]. However, the former requires the introduction of additional modules for specific tasks or fine-tuning with LoRA to force the generated image to conform to the layout input, which is still constrained by the dataset. The latter is mainly achieved by manipulating the area where the prompt word is injected or using the models local predicted noise to replace the corresponding area in the global noise predicted by the model. However, such methods are prone to cross-image interference and semantic leakage, especially in the case of similar appearance of entities (for example, multiple humans or animals), resulting in reduced subject consistency. To address the above limitations, we propose LAMIC (Layout-Aware Multi-Image Composition), trainingfree framework built upon pretrained single-image reference Multimodal Diffusion Transformer (MMDiT) [6] model. Leveraging our proposed attention mechanism and the strong scalability of the MMDiT architecture, LAMIC enables users to incorporate an arbitrary number of reference images along with region-level layout priors (e.g., masks or bounding boxes). Our framework achieves superior overall performance compared to prior approaches in multi-image composition, particularly excelling in tasks involving fine-grained layout control. The main contributions of this paper are as follows: We propose LAMIC, novel framework for highquality layout-aware multi-image composition, supporting flexible spatial layout control and seamless multireference integration. LAMIC is the first to extend single-reference diffusion models to multi-reference scenarios in training-free manner. It inherits the consistency-preserving editing capabilities of the underlying model, while circumventing the generalization issues caused by the scarcity of largescale multi-reference training datasets. To reduce semantic entanglement across entities, we introduce Group Isolation Attention (GIA), which enforces localized attention within aligned visual-textualspatial (VTS) triplets. Building on GIA, we further propose Region-Modulated Attention (RMA), which defers inter-region fusion and cross-entity interaction (CEI) instruction injection to enhance layout controllability and prevent early-stage semantic leakage. 2. Related Work Reference-Guided Image Generation. Recent advancements in multimodal image generation enable synthesizing images guided by both textual and visual references. UniAdapter [22] introduced early adapter-based methods for reference adaptation, though limited to single references without spatial disentanglement. EasyRef [32] proposed leveraging multiple reference images but relies on complex multimodal large language models (MLLMs), hindering practical usability. FLUX.1-Kontext [11], built on MMDiT architecture, demonstrates significant improvements in identity consistency using single reference image. However, these methods remain inadequate for handling multiple image references effectively. layout Layout-Aware Generation. Spatial control has been explored via supervised segmentation maps, bounding-box conditioning [3], and region-aware prompts. However, most approaches rely on either fine-tuning [20], training-time supervision [7] prompt heuristics [3], making them less flexible or repeated inference for open-domain generation. In contrast, our method avoids parameter tuning, extra inference, and complex prompt engineering, offering more practical solution for open-domain scenarios. [29], Multi-Image Composition. Effective compositional generation involves integrating multiple visual references into coherent images. MS-Diffusion [23] pioneered multimodal inference with layout control but exhibits limitations in identity preservation and spatial accuracy. Methods like OmniGen [28] and OmniGen2 [24] enhance identity disentanglement but generally require extensive retraining, restricting scalability. UNO [25] and DreamO [12] provide consistent cross-reference synthesis but lack explicit layout control. Although XVerse [2] achieves fine-grained identity control, like most training-based methods, it relies on large-scale multi-reference datasets that are difficult to collect, leading to generalization limitations in practical scenarios. 3. Method 3.1. Preliminaries and Key Insights Multimodal Diffusion Transformer. MMDiT [6], introduced in Stable Diffusion 3, extends DiT [14] by concatenating text and latent image tokens for multimodal conditioning within the LDM framework [18]. This design has been adopted in models such as FLUX.1 [10] and FLUX.1Kontext [11], with the latter demonstrating strong identity preservation in single-reference generation. Notably, both Kontext and recent control frameworks (e.g., EasyControl [30], OmniControl2 [21]) introduce external control signalslike reference imagesvia token concatenation. This design paradigm reveals key insight: it is possible to introduce multiple-reference images into unified representation space using only pretrained singlereference network. Figure 1. Framework of our proposed LAMIC. We illustrate the layout-aware multi-image composition process with 5 reference groups (n=5) provided as input. Attention Mechanism. In MMDiT, both self-attention and cross-attention layers are applied at each denoising step to model complex dependencies. The attention computation is defined as: Att(Q, K, ) = softmax (1) (cid:19) (cid:18) QK where (query), (key), and (value) are projections of either latent tokens or conditioning embeddings. While standard attention enables global interaction across all tokens, this becomes problematic in the multi-reference setting: cross-token mixing can lead to interference between unrelated entitieswhether in textual descriptions, visual references, or layout controls. 3.2. Overview of LAMIC As shown in Figure 1, LAMIC enables layout-aware multientity generation through the following three stages: 1) Structured Input Definition, where each reference is organized into visual-textual-spatial (VTS) triplet, complemented by cross-entity interaction (CEI) instructions and uncontrolled regions; 2) Unified Token Representation, where all componentsVTS triplets, CEI, and uncontrolled regionsare encoded into unified token sequence for representation in MMDiT; 3) Multi-VTS Guided Generation, where image synthesis is guided by all VTS tokens. Two attention mechanisms are introduced to support this stage: Group Isolation Attention (GIA) restricts cross-group interaction among textual, spatial, and visual tokens to prevent semantic entanglement; Region-Modulated Attention (RMA) defers inter-region fusion and CEI injection to enhance layout controllability and avoid early-stage semantic leakage. joint 3.3. Structured Input Definition We structure each reference as visual-textual-spatial (VTS) triplet group Gi = (Vi, Ti, Si), where Vi denotes the visual reference (image), Ti represents the textual conditionreferred to as the self-attribute description (SAD), and Si specifies the target spatial region (e.g., bounding box or mask). Each SAD consists of an identifier describing the entity (e.g., dragon, car), and description specifying appearance behavior (e.g., keep the same appearance, change the pose). We further introduce Cross-Entity Interaction (CEI) instruction C, which governs spatial or semantic relationships between entities (e.g., rides B), and an uncontrolled region , covering areas not assigned to any specific entity. Together, these components{(Vi, Ti, Si)}N i=1, C, and form structured input that aligns visual, textual, and spatial guidance, and denotes the number of references. This design supports multi-entity composition without relying on external reasoning modules such as MLLMs [29]. 3.4. Unified Token Representation We encode all components{(Vi, Ti, Si)}N i=1, C, and into unified token sequence. Specifically, we use the pretrained VAE or AE from MMDiT to convert each Vi into latent tokens Li RB(HiWi/4)4C, where B, C, Hi, and Wi denote the batch size, channel count, and spatial dimensions. Textual inputs (Ti, C) are embedded via pretrained T5 [17] or CLIP [16], and projected to match the latent token space. Spatial regions Si are downsampled (typically 8) and reshaped to match the image token format. We then concatenate all tokens along the sequence dimension and record their positions for subsequent attention masking. 3.5. Multi-VTS Guided Generation We design two attention mechanisms to support layoutaware generation guided by multi-VTS tokens: Group Isolation Attention (GIA) and Region-Modulated Attention (RMA), as illustrated in Figure 2. and attention with and is disabled: RM A(QSi, KU , VU ) = RM A(QU , KSi, VSi) = 0 (6) RM A(QSi, KC, VC) = RM A(QC, KSi, VSi) = 0 (7) RM A(QU , KC, VC) = RM A(QC, KU , VU ) = 0 (8) In practice, we implement these rules via attention masks for efficiency. The total denoising process is divided into two sub-stages: RMA is applied during the first stage, covering predefined ratio of the total steps, followed by GIA in the remaining steps. 4. Experiments 4.1. Experimental Setting Implementation Details. We implement LAMIC based on the open-source single-image reference MMDiT-based model (Flux.1 Kontext-dev). The inference process is configured with 20 denoising steps, guidance scale of 2.5, and first-stage step ratio of 0.05. To reduce memory consumption, both the Transformer and T5 modules are quantized to INT8 during inference. All experiments are conducted on machine equipped with single NVIDIA RTX 4090 and dual NVIDIA A6000 GPUs. Benchmark Dataset. As benchmark for multi-image composition, XVerseBench [2] originally includes 74 objects, 20 human faces, and 45 animals. However, we observed its limitations in subject diversity and visual quality. To address this, we augmented the dataset with 20 additional scenes, 17 clothing items, and 1 object sourced from DreamBench++ [15], MS-Bench [23], and GPT-4o generations. Moreover, due to the low resolution and noise present in some original samples, we regenerated 20 highresolution human faces and replaced several degraded images using high-quality generation tools. Beyond improving image quality and type diversity, we constructed structured multi-image inputs with associated bounding boxes for each subject, enabling precise layout-aware generation and evaluation. Specifically, we created 60 inputs with two reference images, 40 inputs with three reference images, and 20 inputs with four reference images. Evaluation Metrics. Following prior work [2], we adopt several established metrics to assess generation quality. To better evaluate background consistency and layout controllability, we further propose three novel metrics: BG-S, INR, and FI-R, which provide finer-grained analysis in multireference, layout-aware synthesis settings. DPG Score [8], measuring the text consistency editing ability of the model; Face ID Similarity (ID-S) [4], evaluating human identity preservation; (a) Group Isolation Attention (b) Region-Modulated Attention Figure 2. Our proposed attention mechanisms. Group Isolation Attention (GIA). GIA suppresses interference across VTS groups by restricting attention computations within each group: GIA(QGi, KGj , VGj ) = (cid:40) Att(), 0, = = (2) where i, {1, . . . , }. For brevity, in multi-case equations, we use Att() to denote the attention operation with the same (Q, K, ) inputs as those on the left-hand side of the equation. To ensure structural coherence, we retain unrestricted attention between spatial regions on the condition of Eq. (2): GIA(QSi, KSj , VSj ) = Att(QSi, KSj , VSj ), i, (3) We also define cross-group interactions with CEI (C) and uncontrolled region (U ): is treated as global prompt and interacts fully with all groups, while follows the spatial attention pattern: GIA(QGi, KC, VC) = Att(QGi, KC, VC) GIA(QC, KGi, VGi) = Att(QC, KGi, VGi) GIA(QGy , KU , VU ) = GIA(QU , KGy , VGy ) = (cid:40) Att(), 0, (cid:40) Att(), 0, = = y = = (4a) (4b) (4c) (4d) where {V, T, S}, . Region-Modulated Attention (RMA). To promote precise spatial control and prevent early-stage semantic leakage, based on GIA, RMA further limits the inter-region cross-attention and CEI injection in the early denoising step: RM A(QSi, KSj , VSj ) = (cid:40) Att(), 0, = = (5) Figure 3. Visual comparison of different methods under different multi-reference images. DINOv2 Similarity (IP-S) [13], capturing object appearance consistency; Aesthetic Score (AES) [5], judging overall aesthetic appeal; Background Similarity (BG-S), weighted combination of DINOv2, CLIP [16], SSIM, and color histogram (CH): BG-S = 0.4 DINO + 0.25 CLIP + 0.2 SSIM + 0.15 CH (9) We report the unweighted average of the above five metrics as AVG for overall generation quality. For layout evaluation, we employ the Grounded SAM 2 pipeline, where Florence-2 [26] handles object detection and grounding, generating bounding boxes for target entities, which are then processed by SAM-2 to produce precise segmentation masks Mgen, which are compared to the ground-truth target region mask Mtrg. Specifically, IN-R (Inclusion Ratio) measures how much of the generated entity lies within the target region, while FI-R (Fill Ratio) evaluates how well the target region is covered by the generated entity. These two ratios jointly reflect the precision and completeness of layout control. In order to unify the value range with other metrics, we multiplied both ratios by 100. (10) 100 IN-R = (cid:80)(Mgen Mtrg) (cid:80) Mgen (cid:80)(Mgen Mtrg) (cid:80) Mtrg To avoid artificially inflated layout scores caused by large targets (e.g., full-image regions), we discard samples where the target mask occupies more than 75% of the image area. FI-R = 100 (11) Baseline Methods. We first evaluate the performance of our method on multi-image composition tasks, comparing it against several state-of-the-art multi-reference generation approaches, including MS-Diffusion [23], MIPAdapter [9], OmniGen [28], UNO [25], OmniGen2 [24], DreamO [12], and XVerse [2]. These models are evaluated across various subject composition and background integration scenarios under three settings. After that, we further evaluate the layout control performance of the above methods. 4.2. Multi-Image Composition Performance Quantitative Comparison. Table quantitatively demonstrates that LAMIC consistently achieves the best 1 Table 1. Quantitative results of multi-image combination. Bold indicates the best result, single underline indicates the second-best, and double underline indicates the third-best. Method Three-Reference DPG ID-S IP-S BG-S AES AVG DPG ID-S IP-S BG-S AES AVG DPG ID-S IP-S BG-S AES AVG Four-Reference Two-Reference 75.01 12.70 47.05 71.48 52.88 51.82 86.67 3.46 43.13 72.49 57.85 52.72 75.24 3.78 41.25 73.24 55.41 49.78 MS-Diffusion 82.64 22.28 59.59 75.15 55.77 59.09 84.97 20.40 61.58 78.93 60.61 61.30 83.49 13.31 57.73 77.82 58.60 58.19 MIP-Adapter 82.06 69.32 66.63 73.25 56.54 69.56 72.79 61.24 64.52 78.30 59.60 67.29 74.60 54.88 61.60 77.21 59.39 65.54 OmniGen 89.42 38.71 72.92 72.32 59.52 66.58 91.00 43.52 69.03 78.72 62.45 68.94 90.16 39.98 73.73 78.34 61.75 68.79 UNO 85.53 59.89 70.60 80.59 54.61 70.24 81.00 55.53 65.88 83.58 59.32 69.06 81.09 50.66 62.90 81.56 56.49 66.54 OmniGen2 88.54 58.71 73.45 73.99 55.28 69.99 90.60 63.74 71.49 80.73 57.01 72.71 90.04 57.29 70.19 80.08 55.95 70.71 DreamO 87.90 56.49 70.72 74.62 59.38 69.82 90.23 63.70 73.19 79.53 59.06 73.14 84.00 61.84 70.69 78.72 56.71 70.39 XVerse LAMIC(Ours) 85.61 78.04 72.33 83.14 53.59 74.54 91.95 65.63 67.54 86.06 59.24 73.92 90.16 70.25 66.67 87.02 58.10 74.44 Furthermore, overall performance across all settings. Notably, it obtains the highest ID-S, BG-S, and AVG scores in each reference configuration, indicating strong identity and background preservation, as well as balanced generation quality. In the threeand four-reference settings, LAMIC also achieves the highest DPG score, demonstrating its superior editing capability and prompt consistency. (1) Specifically, in the two-reference setting, LAMIC achieves an ID-S of 78.04, surpassing the second-best OmniGen by nearly 9 points; BG-S of 83.14, exceeding the second-best OmniGen2 by 2.55; and an AVG of 74.54, outperforming the runner-up OmniGen2 by 4.3. the IP-S of 72.33 is only 1.12 below the best-performing model, demonstrating excellent (2) In the three-reference setting, object preservation. LAMIC achieves DPG of 91.95, an ID-S of 65.63, and an AVG of 73.92, all outperforming the respective second-best models by approximately 1 point. Notably, it attains BG-S of 86.06, which is 2.5 points higher second-best OmniGen2, than the further validating LAMICs superior capability in background consistency. (3) In the more challenging four-reference setting, LAMIC still leads with DPG of 90.16 (tied with UNO), an ID-S of 70.25 (exceeding the second-best XVerse by 8.41), BG-S of 87.02, and an AVG of 74.44, surpassing all competitors by large margin, indicating LAMIC maintains strong performance as the reference number increases. These improvements are achieved without any finetuning or model re-training, highlighting the zero-shot generalization capability of our method. It is also worth noting that UNO consistently achieves the best AES, reflecting its strong aesthetic appeal, while OmniGen2 performs secondbest on BG-S, demonstrating good background synthesis ability. DreamO and XVerse also achieve solid overall performance, closely following LAMIC in terms of AVG. Qualitative Comparison. Figure 3 presents qualitative comparisons under diverse multi-reference scenarios. LAMIC excels in preserving subject identity and structural fidelity, generating visually coherent and high-quality results. For example, in the old man-pixelated warrior composition (Row 2), LAMIC successfully maintains the subjects stylized structure and realistic blending, while other methods exhibit over-smoothing or distortions. In the sea turtlejellyfishmanforest composition (Row 5), LAMIC respects spatial arrangements and visual semantics, accurately merging all referenced elements, whereas most baselines suffer from object mismatching or semantic drifting. XVerse consistently delivers visually pleasing generations with relatively high ID-S, particularly in cases with prominent human references (Rows 2 and 4), but tends to oversimplify background compositions. DreamO achieves smoother image transitions and realistic style rendering, as observed in Row 3 (anime girl etc.) and Row 4 (teapot and beach etc.), but occasionally struggles with precise identity preservation and text following, especially in more complex scenes. In contrast, methods like MIP-Adapter and MS-Diffusion exhibit limitations in balancing layout, identity, and appearance, often leading to incomplete or mismatched object integration. Overall, both quantitative and qualitative results validate that LAMIC achieves superior multi-image composition, maintaining generation quality and consistency. More examples are illustrated in Appendix A. 4.3. Layout-Controlled Multi-Image Composition"
        },
        {
            "title": "Performance",
            "content": "Although MS-Diffusion is currently the only baseline that explicitly supports layout-aware multi-image composition, we still compare our method against all aforementioned approaches to provide comprehensive evaluation of our proposed layout control metrics. The results are presented in Table 3. LAMIC achieves Table 2. Ablation results for different attentions and ratios of first-stage steps under layout-aware multi-image composition. Settings Three-Reference DPG ID-S IP-S BG-S AES IN-R FI-R DPG ID-S IP-S BG-S AES IN-R FI-R DPG ID-S IP-S BG-S AES IN-R FI-R Four-Reference Two-Reference A 0.05 85.61 78.04 72.33 83.14 53.59 92.39 32.75 91.95 65.63 67.54 86.06 59.24 91.90 24.26 90.16 70.25 66.67 87.02 58.10 89.81 20.81 0.10 86.76 79.73 72.99 84.74 52.73 94.34 31.98 89.56 66.37 66.76 86.33 59.24 93.92 26.31 92.71 66.85 65.59 86.14 58.36 89.27 20.23 0.15 86.26 81.21 72.90 85.17 52.39 95.34 32.94 88.84 66.97 67.23 86.52 59.09 94.67 27.33 89.50 66.81 65.57 86.10 58.17 90.25 20.54 0.20 85.47 81.37 72.93 84.48 51.90 95.26 33.43 86.48 65.53 67.15 86.58 59.17 94.65 27.49 89.14 66.80 65.59 86.13 58.41 90.72 20.71 w/o RMA 84.18 67.96 67.25 82.95 54.91 81.77 28.93 90.92 64.58 64.51 85.79 59.31 87.45 23.81 91.25 65.81 64.84 86.30 58.83 88.88 19.39 w/o GIA 86.20 39.15 61.66 79.95 53.30 66.12 24.47 87.30 42.19 55.60 84.16 57.63 69.37 20.70 84.74 32.07 51.62 85.08 54.85 66.95 16.57 Table 3. Comparative results of our LAMIC and other methods under layout-aware multi-image composition. Method Two-Ref Four-Ref Three-Ref IN-R FI-R IN-R FI-R IN-R FI-R 56.83 23.17 72.84 19.06 58.55 20.32 MS-Diffusion MIP-Adapter 59.25 21.73 70.43 20.34 63.16 19.40 58.60 16.40 66.43 20.87 58.99 14.96 OmniGen 63.46 15.74 70.37 18.54 70.13 17.85 UNO 67.49 27.87 70.72 27.27 58.50 22.30 OmniGen2 69.25 24.37 73.84 23.57 72.95 23.71 DreamO 62.65 20.30 75.42 22.83 62.24 19.62 XVerse LAMIC(Ours) 92.39 32.75 91.90 24.26 89.81 20.81 an IN-R of approximately 90 across all settings, significantly outperforming all other methods. This is expected, as most baselines lack explicit layout control capabilities. While MS-Diffusion claims to support layout-aware generation, its performance on both IN-R and FI-R is relatively subpar. We attribute this to the nature of our proposed IN-R and FI-R metrics, which evaluate the consistency of entity placement and spatial accuracy. These metrics rely on models ability to preserve entities and maintain compositional alignmentareas where MS-Diffusion underperforms (Sec. 4.2), thus leading to lower scores. It is also worth noting that although LAMIC consistently achieves the best FI-R among all methods, the margin over layout-unaware baselines is not large. This suggests that while LAMIC demonstrates superior layout control, there is still considerable room for improvement in fully capturing and preserving target spatial configurations. 4.4. Ablation Study In our ablation study, we first analyze the impact of removing each proposed module, and then investigate the effect of varying the ratio of first-stage steps on generation quality. Impact of Proposed Modules. Table 2 quantitatively demonstrates the impact of individually removing the proposed RMA and GIA components. The full model configuFigure 4. Visual comparison of different settings of LAMIC under layout-aware multi-image composition. ration (LAMIC) achieves the best overall performance. Using RMA results in slight drop in aesthetic quality, which is reasonable given the substantial improvement in layout controlIN-R increases by up to 10.62 points in the tworeference setting and 7.22 points in the three-reference setting compared to the version without RMA. In contrast, removing GIA causes significant degradation across nearly all metrics, highlighting its critical role in ensuring highquality multi-image composition. Figure 4 presents qualitative comparisons across different settings. It clearly shows that removing RMA weakens layout control performance (e.g., in the TV-donut and bear-maple leaf-cactus-desert cases), and leads to partial fusion or entanglement of entities within target regions (e.g., in panda-cat and eagle-shark). The degradation becomes more pronounced when GIA is removed: layout control capabilities nearly vanish (consistent with the scores in Table 2), which are comparable to layout-unaware baselines Table 3, and multiple reference entities collapse into single blended form in most situations or just keep single entity. Impact of First Stage Steps. Table 2 also reveals the impact of varying the ratio of first-stage steps. As expected, increasing this ratio generally improves layout control: in both two-, three-, and four-reference settings, ratio of 0.15 or 0.20 consistently achieves the best or second-best scores in IN-R and FI-R. An exception occurs in the four-reference case, where lower ratio of 0.05 yields the highest FI-R. sively quantify models capabilities, we further introduce three metricsIN-R, FI-R, and BG-Sthe first two assess layout control, while the last measures background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most of the major metrics. It consistently ranks first in ID-S, BG-S, IN-R, and AVG across 2-, 3-, and 4-reference settings, and leads in DPG under 3and 4-reference scenarios. These results highlight its superior identity fidelity, spatial controllability, and prompt-following. In the future, we will focus on refined attention designs to reduce such confusion while preserving boundary smoothness. We also plan to explore prompt-to-reference binding for earlier Cross-Entity Interaction (CEI), enhancing entity interplay and language controllability in early stages. Exploring more effective methods to extend pre-trained single-reference foundation models to multi-reference settings is also promising future direction."
        },
        {
            "title": "License",
            "content": "This preprint is made available under an arXiv.org nonexclusive license. The copyright remains with the authors."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 1 [2] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. 1, 2, 4, 5 [3] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. 2 [4] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 4 [5] discus0434. Aesthetic Predictor v2.5: SIGLIP-Based Aesthetic Score Predictor [Source code], 2024. GitHub repository. Accessed: 2025-07-01. 5 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [7] Jieru He, Xinyu Yang, Ailing Zeng, et al. Eligen: Entitylevel controlled image generation with regional attention, 2025. 2 Figure 5. Visual comparison of different first-stage step ratios in LAMIC under layout-aware multi-image composition. For the left group, the target regions are the upper and lower halves (0.5 each); for the right group, the target regions correspond to those shown in Figure 4. However, this improvement in layout control comes at the cost of other aspects of generation quality. As the ratio increases, the AES tends to decline, with the best scores observed at lower ratios (0.05 or 0.00, the latter corresponding to the w/o RMA case). similar trend is observed for DPG, suggesting that excessive first-stage emphasis may impair prompt-following fidelity and regional continuity. To visualize these effects, we present two groups of image samples in Figure 5, where each sample in single group shares identical inputs but uses different random seeds. These examples highlight the trade-off between layout precision and global coherence when adjusting the firststage ratio. We further observe that when the ratio reaches 0.10 or higher, distinct boundaries begin to appear between adjacent regions, and visual consistency within individual regions degrades noticeably. In contrast, setting the ratio to 0.05 preserves better global coherence and intra-region consistency. Although the lower ratio may occasionally result in attribute blending between closely placed entities, its quantitative scores remain comparable and consistently yield superior visual quality in practice. Therefore, we fix the first-stage ratio to 0.05 in the main experiments. 5. Conclusion In this work, we propose LAMIC, zero-shot framework for layout-aware multi-image composition. LAMIC is the first to extend the capabilities of consistent single-reference generative models to multi-reference generation and further introduces layout controllabilityboth without fine-tuning. To address semantic entanglement across references and enable precise region-wise control, we introduce two plugand-play mechanisms: Group Isolation Attention (GIA) and Region-Modulated Attention (RMA). To more comprehen- [8] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 4 Ella: [9] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion for finetuning-free personalized image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 37073714, 2025. 5 [10] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [11] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 1, 2 [12] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 2, 5 [13] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5 [14] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2 [15] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. 4 [16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, [17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [18] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [19] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1 [20] Nataniel Ruiz, Yuanzhen Li, et al. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2 [21] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 2 [22] Xiaoyu Wang, Sixiao Hu, et al. Uniadapter: Parameterefficient tuning for text-to-image diffusion models, 2023. 2 [23] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. MS-diffusion: Multi-subject zero-shot image personalization with layout guidance. In The Thirteenth International Conference on Learning Representations, 2025. 2, 4, [24] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 2, 5 [25] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 2, 5 [26] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, et al. Florence-2: Advancing unified representation for variety of vision tasks. In CVPR, 2024. 5 [27] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinarXiv preprint ema: Towards short movie generation. arXiv:2507.18634, 2025. 1 [28] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 1, 2, 5 [29] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In International Conference on Machine Learning, 2024. 2, [30] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 2 [31] Xingang Zhou, Jimei Yang, et al. Storydiffusion: Storyaware visual synthesis with character consistency, 2024. 1 [32] Mengjiao Zong, Ziqi Wu, Chunyuan Li, et al. Easyref: reference-tuning-free framework for multi-image composition, 2024."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Onestory Team",
        "University of Science and Technology of China"
    ]
}