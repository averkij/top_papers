{
    "paper_title": "ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks",
    "authors": [
        "Yan Yang",
        "Dongxu Li",
        "Haoning Wu",
        "Bei Chen",
        "Liu Liu",
        "Liyuan Pan",
        "Junnan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts."
        },
        {
            "title": "Start",
            "content": "ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks Yan Yang1 Dongxu Li1 (cid:12) Haoning Wu2 Bei Chen Liu Liu3 Liyuan Pan4 (cid:12) Junnan Li5 1ANU 2NTU 3KooMap, Huawei 4BITSZ & School of CSAT, BIT 5Salesforce AI Research dongxuli1005@gmail.com liyuan.pan@bit.edu.cn Project Page: https://yan98.github.io/ProBench/ 5 2 0 2 0 1 ] . [ 1 5 8 8 6 0 . 3 0 5 2 : r Figure 1: Examples of ProBench. Our ProBench spans 10 task fields and 56 sub-fields, supports 17 languages, and supports conversations with up to 13 conversation turns. We show the task and image fields in the header of each sample. We use Engineering Drawings for Engineering and Technical Drawings in the first plot of the second row due to space constraints. More diverse and longer samples are provided in the supplementary material."
        },
        {
            "title": "Abstract",
            "content": "Solving expert-level multimodal tasks is key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, benchmark of openended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts."
        },
        {
            "title": "Introduction",
            "content": "Solving expert-level multimodal tasks with multimodal large language models (MLLMs) represents an important milestone toward achieving humanlevel general intelligence. However, these tasks require accurate user query understanding, in-depth domain-specific knowledge, and advanced reasoning abilities, which present significant challenges for frontier models as of today. Measuring such progress requires rigorous evaluations. To this end, we introduce ProBench, challenging and automatic evaluation benchmark leveraging MLLM-asa-Judge. ProBench consists of 4,000 queries submitted independently by professional users, coverFigure 2: Comparison with WildVision (Lu et al., 2024) on challenge levels of (a) text, (b) image, and (c) reasoning for user instruction queries. To ensure fair comparison, we follow WildVision by selecting the top 500 highest-quality queries from the single-round conversations. It can be seen that ProBench contains significantly more hard samples than WildVision. ing diverse productivity demands and expert knowledge to assess MLLM capabilities in open-ended scenarios  (Fig. 1)  . One common benchmark to evaluate MLLM performance with expert knowledge is MMMU (Yue et al., 2024a). While effective for automatic evaluation using predefined answer choices, such benchmarks fail to capture MLLM capabilities in open-ended user interactions. Specifically, they do not adequately assess MLLM ability to follow user instructions or align with human preferences. Both are fundamental aspects for real-world applications (Lu et al., 2024; Luo et al., 2024; Chen et al., 2024b). Similar limitations apply to other benchmarks, such as MMMU-pro (Yue et al., 2024b), MMBench (Liu et al., 2025), among others (Lu et al., 2023; Masry et al., 2022; Singh et al., 2019; Wu et al., 2024). Alternatively, MLLM-as-a-Judge is usually employed to automatically evaluate model performance in open-ended scenarios. However, existing open-ended multimodal benchmarks require limited expert-level or professional knowledge. Among them, some (Chen et al., 2024b) are constructed by few experts, limiting their domain coverage, while remaining ones (Luo et al., 2024; Lu et al., 2024), such as WildVision, are mostly set in general chat environments and require much less domain knowledge to solve. To fill this gap, in this paper, we aim to design an open-ended benchmark that requires expert-level knowledge for multimodal tasks. Our ProBench is created from high-quality interactions within 100K real-world, professionally crowdsourced multimodal conversations for productivity scenarios. Specifically, samples are collected by encouragFigure 3: ProBench overview. Distributions of (a) task fields on the single-round track, (b) languages on the multi-linguistic track, and (c) conversation rounds on the multi-round tracks. ing professionals to ask questions related to their daily professional work, which usually require significant expert-level knowledge. This distinction sets our benchmark apart from prior works like WildVision (Lu et al., 2024)  (Fig. 2)  . For comprehensive evaluation, ProBench includes three tracks: single-round, multi-round, and multi-linguistic conversations. They respectively span 10 task fields and 56 sub-fields, support 17 languages, and support conversations with up to 13 conversation turns. An overview of ProBench is presented in Fig. 3. Leveraging MLLM-as-a-Judge (e.g., gpt-4o), we assess 24 leading MLLMs on ProBench. Our evaluation reveals several key limitations in stateof-the-art MLLMs: i) current MLLMs struggle in visual perception, textual understanding, domain knowledge, and advanced reasoning, suffering from tasks like mathematics and planning; ii) multi-linguistic understanding and long-context reasoning during multi-round interaction remain challenging for most existing MLLMs. Our main contributions are summarized as follows: we introduce ProBench, an open-ended multimodal benchmark tailored for professional work scenarios requiring expert-level knowledge, featuring 4,000 samples across 10 task fields over 56 sub-fields. The benchmark also features multi-round conversations up to 13 turns and multi-linguistic tracks in 17 languages; we design an automatic pairwise evaluation pipeline using MLLM-as-a-Judge, achieving 79.9% agreement with human experts. The evaluation is robust to different comparison baseline and judge model choices. We also Figure 4: Framework of ProBench. Starting with 100K crowdsourced conversations, we identify high-quality user queries to curate single-round, multi-linguistic, and multi-round tracks. Using MLLM-as-a-Judge, we benchmark and rank 24 state-of-theart MLLMs with ELO ratings. To ensure fairness, the ELO ratings are de-biased to remove confounder effects (e.g., MLLM response formats), resulting in the final ProBench leaderboard. Icons in the figure are sourced from (Freepik et al., 2025). provide distilled version of Llama-vision to support cost-effective local evaluations; we conduct comprehensive evaluations on 24 leading MLLMs, showing that ProBench presents significant challenges for existing MLLMs, in visual perception, advanced reasoning, and domain knowledge. This signifies the need for more advanced multimodal models for high-value practical scenarios."
        },
        {
            "title": "2 ProBench",
            "content": "Preliminary. The ProBench dynamically ranks MLLMs by employing the ELO rating system, implemented through statistical modeling based on direct pairwise model comparisons. In the following, we provide an overview. For further details, please refer to (Elo, 1966; Hunter, 2004). Given MLLMs, an online ELO rating system compares model with rating ri and model with rating rj using the probability (yi,j = 1). Here, yi,j denotes the binary outcome, where yi,j = 1 indicates that model wins, and yi,j = 0 indicates that model wins. The probability is calculated by (yi,j = 1) = 1 1 + 10(ri rj )/α , where α is hyperparameter that serves as scaling factor, typically set to α = 400. The ELO rating is dynamically updated after each model comparison. Taking model as an example, the rating is updated according to the following rule: rupt = ri + (si,j (yi,j = 1)) . = 32. The term si,j is scalar representing the actual outcome: 0 for loss, 0.5 for tie, and 1 for win. This updating rule encourages that higher-rated model gains fewer points for win, and loses more points for defeat, while lowerrated model experiences the opposite effect. However, when using MLLM-as-a-Judge, the comparison results can be sensitive to model presentation order and confounded by response style variations (Li et al., 2024c). To address these challenges, the ProBench incorporates the BradleyTerry model (Hunter, 2004) as an additional layer atop the ELO system. For MLLMs and pairwise comparisons, each round 1 comm RN pares model and model j. We have Xwin to indicate which model is presented first1, while Xsty RS captures stylistic differences between the outputs of models and (e.g., word counts, and use of markdown). The Bradley-Terry model then refines the rating of model as = + ˆβi , rref (cid:88) ˆβ, ˆγ = arg min β,γ m,i,j ℓbce(βXwin + γXsty , si,j) , where ℓbce(, ) is the binary cross-entropy loss, is baseline rating constant, β RN and γ RS are respectively known as the model strength and style coefficients, and ˆβi is scaler indicating strength of model i. This refinement known as style control in the literature (Li et al.) compensates for stylistic biases, ensuring fair model performance evaluation. Similarly, is constant determining the magnitude of rating adjustments, commonly set to 1This bias can be easily mitigated by evaluating twice while swapping the comparison order. Overview. We aim to establish comprehensive and challenging benchmark for evaluating MLLMs. The resulting ProBench is built on two primary components: i) curating high-quality conversations from crowdsourced data, categorized into singleround, multi-linguistic, and multi-round tracks; ii) employing MLLM-as-a-Judge to compare and rank MLLMs. In total, 3000, 500, and 500 conversations are selected for the single-round, multi-linguistic, and multi-round tracks, respectively, from an initial pool of 100K crowdsourced user-MLLM conversations. An overview is presented in Fig. 4. 2.1 Benchmark establishment The benchmark is curated based on three guiding principles: i) diversity, selected user instruction queries target to avoid redundancies while extensively covering MLLM-based tasks; ii) MLLMdriven, the chosen queries of conversations are tailored to evaluate the unique capabilities of MLLMs in the multimodal domain; iii) coherence, the benchmark enables targeted evaluations for specific MLLM tasks, rather than providing undifferentiated evaluations. We first describe the common steps involved in curating the three tracks, followed by discussion of the track-specific methodologies. Common step. We filter out short user instruction queries that contain excessive stop words, and apply MinHash-based text deduplication (Lee et al., 2021) to retain pool of non-redundant queries. To address potential redundancy or irrelevance between the instructions and images within user query, we perform image-instruction deduplication. This step removes queries that can be sufficiently answered using only the textual instructions, leveraging an MLLM-based filter. Single-round track. language detector is employed to filter out non-English user instruction queries. Starting with pool of MLLM task and sub-task fields derived from (Chen et al., 2024b), we use an MLLM-based annotator to assign user instruction queries to existing fields or propose new ones where necessary. Additionally, the annotator assesses the challenge level of each query. To ensure diversity, domain balancing is performed, and overrepresented task fields are downsampled, resulting in 3000 user instruction queries. Multi-linguistic track. User instruction queries are categorized by their languages, excluding all English-based conversations. Based on frequency, the queries are grouped into Portuguese (PT), French (FR), Spanish (ES), German (DE), and an Other category (e.g., Chinese, Vietnamese, and more). An MLLM-based annotator is then used to assess the challenges of the queries, with the 100 most difficult queries retained for each group. Multi-round track. Similar to the single-round track, we focus on user instruction queries in English for this track. Multi-round conversations are required to feature interconnected queries across rounds, demonstrating progressive nature. To achieve this, we identify the reasoning challenges and interdependencies between queries within the conversations, applying an MLLM annotator. Ultimately, the 100 most challenging independent queries and 400 interconnected multi-round user instruction queries are preserved. Detailed prompts used for the above steps are provided in the supplementary material. With the ProBench, we are readily to assess and rank the MLLMs. 2.2 MLLM-as-a-Judge and ranking We evaluate MLLM performance in addressing user instruction queries using 5-point Likert scale (Likert, 1932), by conducting pairwise comparisons against baseline model (e.g., GPT-4o). While evaluations by domain-specific human experts are considered as the gold standard, they are resource-intensive, time-consuming, and challenging to scale for large-scale benchmarks. As an alternative, we employ MLLM-as-a-Judge as an approximation of human expertise (Li et al., 2024c; Zheng et al., 2023; Chen et al., 2024a). The MLLM-as-aJudge is guided by the following principles. Correctness: ensures the accuracy of information, absence of factual errors, and alignments with known and visual knowledge. (For the multi-linguistic track, response language consistency is emphasized). Helpfulness: provides clear, practical, and actionable guidance to address the user instruction query. Relevance: focuses on the prompt requirements, avoiding extraneous or tangential information. Conciseness: avoids unnecessary verbosity while maintaining clarity and direct language. Completeness: covers all essential aspects of the user instruction query, providing sufficient information to address it. Figure 5: Example queries from ProBench. As shown, significant domain knowledge and reasoning capabilities are needed to solve ProBench queries. For brevity, we only show examples with relatively shorter text queries, with the remark that longer queries are common in ProBench. More examples can be found in the appendix. Table 1: Comparisons of state-of-the-art MLLMs on the single-round track are presented using the following abbreviations: Sci. (Science), Cd. (Coding), CW. (Creative Writing), IE. (Information Extraction), Perc. (Perception), Knowl. (Knowledge), Arts (Arts), Plan. (Planning), Math (Mathematics), and Mt. (Metrics). We provide ELO ratings for each task, followed by an overview that includes the average number of output tokens (#Token), 95% confidence interval (95% CI), win rate (WR), and overall ELO rating. The MLLMs are sorted by the overal ELo rating in each group of model size. Task-Specific ELO Ratings Overview Sci. Cd. CW. IE. Perc. Knowl. Arts Plan. Math. Mt. #Token 95% CI WR Elo Model Proprietary MLLMs claude-3-5-sonnet-20241022 gemini-1.5-pro-002 gpt-4o-2024-05-13 gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gemini-1.5-flash70B+ Open-source MLLMs Pixtral-Large-Instruct-2411 InternVL2_5-78B Qwen2-VL-72B-Instruct Molmo-72B-0924 NVLM-D-72B Llama-3.2-90B-Vision-Instruct llava-onevision-qwen2-72b-ov 10B+ Open-source MLLMs (cid:181) 1228 1252 1259 1211 1213 (cid:181) 1151 1145 1105 1100 1110 (cid:181) 1114 1114 1114 1114 1114 (cid:181) 1049 1074 1165 1094 1096 (cid:181) 1096 1112 1050 1097 995 (cid:181) 1025 1092 1007 1022 877 124B 1230 1194 1280 1242 1224 78B 1083 1018 1051 1091 1031 986 72B 1009 903 72B 828 849 72B 780 806 90B 830 767 72B 991 859 810 754 726 914 733 877 751 735 965 953 991 624 762 Pixtral-12B-2409 Aria-Chat InternVL2_5-38B InternVL2_5-26B Llama-3.2-11B-Vision-Instruct 12B 1028 3.9/25.3B 990 38B 1000 26B 890 11B 671 7B+ Open-source MLLMs InternVL2_5-8B Qwen2-VL-7B-Instruct MiniCPM-V-2_6 llava-onevision-qwen2-7b-ov Molmo-7B-D-0924 Molmo-7B-O-0924 8B 824 7B 803 8B 644 7B 605 7B 536 7B 457 965 982 979 816 541 806 689 599 570 304 134 1099 1031 1024 998 937 985 1021 987 1028 944 894 1008 766 702 681 983 827 767 807 720 880 877 659 683 631 483 914 861 812 809 638 681 1272 1067 1114 1101 1080 1011 1250 1084 960 881 835 842 689 1057 1034 904 876 761 840 816 676 681 655 1236 1192 1107 1095 1114 1114 1130 1102 1032 1058 946 993 1245 1221 1042 1073 921 962 817 862 881 767 769 626 679 663 1047 1083 974 1019 1041 932 964 864 524 624 915 736 673 715 681 606 895 680 667 608 531 380 1197 1134 1114 1037 1175 1175 1065 998 871 838 940 853 996 973 1026 880 744 835 858 656 573 613 428 1251 1147 1114 1159 1015 1087 1266 1023 970 852 725 662 620 1063 1016 933 896 868 833 681 724 603 528 405 500 491 526 374 493 715 558 557 301 561 448 360 659 675 521 490 531 644 787 646 575 310 296 (-7, 8) (-8, 10) (0, 0) (-8, 10) (-7, 7) (-8, 9) 65.84 1228 50.58 1118 50.00 1114 47.12 1094 44.98 1079 35.33 1009 65.97 1229 (-8, 8) 42.85 1064 (-7, 10) 978 31.37 (-9, 9) 856 18.46 (-12, 8) 834 (-10, 10) 16.63 782 (-11, 10) 12.89 734 (-11, 12) 10.09 (-5, 8) (-7, 8) (-9, 9) (-10, 8) (-13, 16) (-11, 8) (-9, 10) (-12, 10) (-13, 10) (-14, 12) (-18, 19) 39.1 32.88 32.5 22.59 7.93 20.45 15.40 7.97 7.93 5.41 3. 1037 990 987 900 688 878 818 689 688 617 540 Details of the prompts used to guide MLLM-as-aJudge are provided in the supplementary material. Subsequently, we apply the ELO rating system, as described in the preliminary section, to compute the de-biased ratings of each MLLM. These ratings are used for leaderboard comparisons, ensuring fair and consistent evaluation across models."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experimental setup Implementation detail. All MLLMs are benchmarked using the vllm (Kwon et al., 2023) and HuggingFace (Wolf, 2019) codebases, with greedy sampling employed for response generation. For MLLMs with limited context lengths (e.g., 4096 token context in Molmo-7B-D-0924), sliding window generation is applied to handle longer inputs. Our MLLM judge utilizes gpt-4o-2024-08-06 with greedy sampling for consistent and reproducible evaluation. For pairwise comparisons in Elo rating calculations, we set gpt-4o-2024-05-13 as the baseline, evaluate each model twice by swapping the presentation order for each user query, and de-bias the ELO ratings by following the methodology of (Li et al., 2024c). MLLM. We evaluate 24 leading MLLMs: gpt4o-mini-2024-07-18 (Hurst et al., 2024), gpt-4o2024-08-06 (Hurst et al., 2024), gpt-4o-2024-05-13 (Hurst et al., 2024), claude-3-5-sonnet-20241022 (Anthropic, 2024), gemini-1.5-pro-002 (Team et al., 2023), gemini-1.5-flash-002 (Team et al., 2023), Aria-Chat (Li et al., 2024b), InternVL2_5-8B (Wang et al., 2024b), InternVL2_5-26B (Wang et al., 2024b), InternVL2_5-38B (Wang et al., 2024b), InternVL2_5-78B (Wang et al., 2024b), Pixtral-12B-2409 (Agrawal et al., 2024), PixtralLarge-Instruct-2411 (Agrawal et al., 2024), Qwen2VL-7B-Instruct (Wang et al., 2024a), Qwen2-VL72B-Instruct (Wang et al., 2024a), MiniCPM-V2_6 (Yao et al., 2024), Llama-3.2-11B-VisionInstruct (Dubey et al., 2024), Llama-3.2-90BVision-Instruct (Dubey et al., 2024), Molmo-7BO-0924 (Deitke et al., 2024), Molmo-7B-D-0924 (Deitke et al., 2024), Molmo-72B-0924 (Deitke et al., 2024), NVLM-D-72B (Dai et al., 2024), llava-onevision-qwen2-7b-ov (Li et al., 2024a), and llava-onevision-qwen2-72b-ov (Li et al., 2024a)."
        },
        {
            "title": "3.2 Experimental result",
            "content": "Tab. 1 and Tab. 2 present the evaluation results. Our key observations are summarized into the following Table 2: Comparisons of state-of-the-art MLLMs on the multi-linguistic and multi-round tracks. We provide an overview that shows the average number of output tokens (#Token), 95% confidence interval (95% CI), win rate (WR), and overall ELO rating for each of the track. Refer to our supplementary material for comparison details on different languages and rounds. The MLLMs are sorted by the overall ELo rating on the multi-linguistic track in each group of model size. Model Proprietary MLLMs claude-3-5-sonnet-20241022 gpt-4o-2024-05-13 gemini-1.5-pro-002 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gemini-1.5-flash-002 70B+ Open-source MLLMs Pixtral-Large-Instruct-2411 Qwen2-VL-72B-Instruct InternVL2_5-78B NVLM-D-72B Llama-3.2-90B-Vision-Instruct Molmo-72B-0924 llava-onevision-qwen2-72b-ov 10B+ Open-source MLLMs (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) 124B 72B 78B 72B 90B 72B 72B InternVL2_5-38B Pixtral-12B-2409 Aria-Chat InternVL2_5-26B Llama-3.2-11B-Vision-Instruct 38B 12B 3.9/25.3B 26B 11B 7B+ Open-source MLLMs Qwen2-VL-7B-Instruct InternVL2_5-8B MiniCPM-V-2_6 Molmo-7B-D-0924 llava-onevision-qwen2-7b-ov Molmo-7B-O-0924 7B 8B 8B 7B 7B 7B Overview on multi-linguistic track Overview on multi-round track #Token 95% CI WR 485 585 629 480 657 567 966 834 841 907 968 426 534 868 1199 1014 814 2027 1216 1021 890 406 686 512 (-21, 29) (0, 0) (-20, 20) (-17, 26) (-21, 16) (-25, 19) (-23, 22) (-18, 21) (-14, 20) (-17, 25) (-29, 21) (-27, 19) (-27, 24) (-20, 18) (-14, 22) (-23, 17) (-28, 19) (-29, 21) (-24, 22) (-22, 20) (-36, 35) (-52, 33) (-68, 37) (-73, 51) 74.58 50.00 59.11 60.35 45.84 28.47 73.81 47.56 42.71 21.99 20.92 18.90 11.95 43.98 35.73 35.33 17.70 8. 12.25 11.95 4.44 4.32 3.07 1.95 Elo 1301 1114 1178 1187 1085 954 1294 1097 1063 894 883 861 767 1072 1012 1009 847 699 772 767 581 576 514 #Token 95% CI WR 1477 1563 1425 1052 1749 1388 2593 1608 2015 1371 1350 967 1176 1734 2264 2321 554 2004 1835 1861 923 1743 925 (-20, 18) (0, 0) (-26, 19) (-22, 18) (-17, 24) (-16, 19) (-23, 19) (-21, 19) (-21, 20) (-35, 33) (-36, 24) (-28, 25) (-31, 26) (-18, 21) (-19, 20) (-27, 12) (-27, 28) (-38, 32) (-34, 25) (-25, 22) (-33, 37) (-34, 26) (-30, 30) (-49, 37) 70.82 50.00 53.88 45.41 55.16 38. 69.73 32.24 44.84 8.49 9.88 18.64 10.30 34.68 40.48 23.92 15.77 6.03 9.48 11.77 5.35 5.04 6.58 3.43 Elo 1268 1114 1141 1082 1150 1030 1259 985 1078 701 730 858 1004 1047 913 823 637 722 764 615 604 653 534 five folds: i) best open-source models rival the best proprietary MLLMs. claude-3-5-sonnet-20241022 and Pixtral-Large-Instruct-2411 respectively belonging to proprietary and open-source MLLMs consistently achieve leading ELO scores across all three tracks. Both models significantly outperform the baseline gpt-4o-2024-05-13; ii) training recipes make difference. Though scaling parameters can generally improve performance, it is not the sole determining factor. By comparing different models, it shows that training recipes and data quality are also important. For example, Pixtral with 12B parameters and Aria-Chat with 3.9B activated parameters consistently demonstrate toptier performance; iii) reasoning tasks remain the hardest. On the single-round track, most MLLMs generally perform well on writing-based tasks (e.g., creative writing). However, their performance on logic-intensive tasks is notably poor, similar to findings in prior LLM studies (Ahn et al., 2024; Quan et al., 2025). The two tasks separately exhibit the lowest Spearman correlation with overall ELO ratings and receive the lowest scores among task fields. Similarly, among all open-source models, performance also suffers significantly in planning tasks, which have the lowest average score (excluding coding); iv) multi-linguistic tasks present challenges. MLLMs face significant challenges in multi-linguistic tasks, with 11 out of 24 MLLMs showing an overall ELO decrease compared to their performance on the single-round track. Notably, llava-onevision-qwen2-7b-ov experienced the most substantial decline; v) multi-round evaluations show larger gaps. Multi-round tasks usually demand long-context reasoning across turns, amplifying performance gaps among MLLMs. MLLMs that underperform in single-round tasks exhibit significantly lower ELO scores. This trend is particularly evident in open-source MLLMs with 7B+ and 10B+ parameters (excluding Pixtral-12B-2409)."
        },
        {
            "title": "3.3 Ablation and discussion",
            "content": "Performance declining with difficulty. We evaluate the ELO rating variances of MLLMs by categorizing user queries into easy and hard groups. The results are presented in Fig. 6. Existing MLLMs tend to exhibit noticeable performance decline compared to the baseline gpt-4o-202405-13 as the reasoning challenge level increased from easy to hard, while MLLM with poor performance typically deteriorates further on the harder queries. This observation aligns with human intuition that more challenging tasks inherently provide better separability when evaluating the Figure 6: Ablation study of reasoning challenge. We show the ELO ratings of MLLMs on two levels: easy and hard. Figure 7: Error analysis. We study cases where MLLM underperforms compared to the baseline. (a) The distribution of losing cases of the MLLM across five evaluation aspects: completeness (Compl.), conciseness (Concis.), correctness (Corre.), helpfulness (Helpf.), and relevance (Relve.). (b) The distribution of error types in losses of the MLLM, categorized into five types: textual understanding error (Text.), visual perceptual error (Perc.), reasoning error (Reas.), lack of domain knowledge error (Know.), and refusal to answer (Reje.). (c) Color bar of the heatmap. MLLM performance, highlighting the limitations of most MLLMs in effectively handling complex user queries. Error analysis. We analyze scenarios in which the state-of-the-art MLLM underperforms relative to the baseline. Fig. 7 (a) illustrates the shortcomings of the MLLM compared to the baseline across five evaluation aspects, highlighting completeness and correctness as the primary issues. Fig. 7 (b) categorizes the error types in the MLLM losses relative to the baseline. Overall, the analysis underscores the need of state-of-the-art MLLM to improve their visual perception, textual understanding, domain knowledge, and reasoning capability. Robustness of ProBench. We study the setting of our evaluation protocol on the 500 most challenging queries from the single-round track. Specifically, Fig. 8 considers two set of experiments: i) comparisons of using three top-performing MLLM as the judge (i. e., gpt-4o-2024-08-06, claude3-5-sonnet-20241022, and Pixtral-Large-Instruct2411); ii) explorations of three baseline models (i. e., gpt-4o-2024-05-13, claude-3-5-sonnet20241022, and Pixtral-12B-2409) in comparisons, representing different model scales. The results reveal high degree of agreement within our evaluation process, with an average Spearman correlation coefficient of 0.979 among the different MLLM judges and 0.983 among the baseline models, highlighting our robustness and consistency. Judge alignment with human expert. To validate the effectiveness of MLLM-as-a-Judge, human annotators are tasked with rating the comparisons using 5-point Likert scale. Our evaluation protocol achieves an agreement of 79.9% with human experts, indicating strong ability of MLLMas-a-Judge to simulate human preferences accurately. These findings demonstrate the viability of Figure 8: Ablation study of MLLM-as-the-Judge. (a-c) Pairwise comparisons of Elo scores for MLLMs evaluated using different MLLM judges. They are gpt-4o-2024-08-06, claude-3-5-sonnet-20241022 (claude-3-5-sonnet), and Pixtral-Large-Instruct-2411 (Pixtral-Large), respectively. (d-f) Comparison of using gpt-4o-2024-05-13, claude-3-5-sonnet-20241022 (claude-3-5-sonnet), and Pixtral-12B-2409 (Pixtral) as baselines. The red line in each plot indicates the best-fit curve for visualization. ProBench as an automatic, large-scale, and challenging benchmark for evaluating the assistance capabilities of MLLMs in professional productivity scenarios. By effectively aligning with human judgments, ProBench provides reliable automatic framework for advancing MLLM development and assessment. Future work and limitation. Although our ProBench has provided valuable insights into the performance and capabilities of MLLMs, several limitations remain that warrant further exploration. One key limitation is potential bias in the benchmark tasks, which may not fully capture the diversity of real-world productivity scenarios for MLLMs. Future work could focus on expanding the benchmark to include broader range of challenging tasks, potentially through the data synthesis (e.g., diffusion models and MLLMs), to improve the diversity. By addressing these challenges, ProBench can continue to evolve as robust and comprehensive tool for advancing the development and evaluation of MLLMs."
        },
        {
            "title": "3.4 Distilled local evaluator",
            "content": "Considering the high API cost of using gpt-4o2024-08-06 as the judge, we fine-tune local evaluator to enable cost-effective and GPU-friendly evaluations for future MLLMs. We use the widely spread Llama-3.2-11B-Vision-Instruct as our backbone model. The Qwen and Pixtral MLLM families are reserved for testing, with the remaining data allocated for training. Our network is trained to distill both the reasoning and decisions of using gpt-4o-2024-08-06 as the judge. The network achieves an average root mean squared error of 32.58 in Elo ratings."
        },
        {
            "title": "4 Related work",
            "content": "The evolution of MLLM-as-a-Judge is largely inspired by the concept of LLM-as-a-Judge (Li et al., 2024c; Dubois et al., 2024; Zheng et al., 2023), which aims to automatically measure the alignment between MLLMs and human preferences. While pairwise comparison (Li et al., 2024c; Chen et al., 2024a) is considered as most preferred, it suffers from biases introduced by factors such as the presentation order of MLLM outputs, verbosity, and markdown styles. To mitigate these issues, style control has been proposed (Li et al.), using statistical modeling to de-bias these confounding effects, thereby improving the MLLM judges. Other approaches, such as few-shot judging, have also been explored, but they face challenges such as reliance on the few-shot example selection and increased evaluation costs (Zheng et al., 2023). Existing MLLM-as-a-Judge leaderboards can be specified to (Luo et al., 2024; Lu et al., 2024; Chen et al., 2024a). However, these often focus on narrow scope of MLLM capability dimensions (Luo et al., 2024; Lu et al., 2024), or rely on artificially posed evaluations by limited number of human experts (Chen et al., 2024b), making them inadequate for assessing MLLMs on professional tasks. Consequently, they fail to capture the dynamic nature of real-world human and MLLM interactions for comprehensive assessment of MLLM capabilities. In contrast, this work introduces challenging benchmark, ProBench, curated from large-scale crowdsourced datasets reflecting real-world professional productivity scenarios. It features three distinct evaluation tracks: single-round, multi-round, and multi-linguistic conversations, across various task fields, offering robust framework for evaluating MLLM performance in real-world scenarios."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces the ProBench, which features single-round, multi-round, and multi-linguistic tracks to enable comprehensive and challenging assessment of the alignment between MLLMs and human preferences across diverse professional productivity demands. By employing MLLM-asa-Judge, the benchmark evaluates MLLM pairwisely, achieving 79.9% agreement with human expert judgments, and underscoring its reliability. Through benchmarking 24 leading MLLMs, our results reveal significant shortcomings of existing MLLMs, particularly in visual perception and reasoning. Furthermore, models often struggle with multi-linguistic and multi-round tracks, highlighting the challenges of diverse language requirement and complex interactions. It reveals valuable insights for future MLLM developments. We hope it inspires successors."
        },
        {
            "title": "References",
            "content": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. 2024. Pixtral 12b. arXiv preprint arXiv:2410.07073. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157. AI Anthropic. 2024. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. 2024b. Mega-bench: Scaling multimodal arXiv evaluation to over 500 real-world tasks. preprint arXiv:2410.10563. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2024. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36. Arpad Elo. 1966. The USCF Rating System: Its Development, Theory, and Applications. United States Chess Federation. Freepik, Eucalyp, Three Musketeers, Dewi Sari, Fantasyou, Jk Icon, and Flat Icons. 2025. Various icons. David Hunter. 2004. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):384406. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. 2024b. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993. Tianle Li, Anastasios Angelopoulos, and Wei-Lin Chiang. Does style matter? disentangling style and substance in chatbot arena, august 2024a. URL https://blog. lmarena. ai/blog/2024/style-control. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024c. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Rensis Likert. 1932. technique for the measurement of attitudes. Archives of Psychology. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around In European conference on computer viplayer? sion, pages 216233. Springer. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints, pages arXiv2310. Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069. Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li. 2024. Videoautoarena: An automated arena for evaluating large multimodal models in video analysis through user simulation. arXiv preprint arXiv:2411.13281. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. 2025. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. 2024b. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442. Wolf. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. 2024. Longvideobench: benchmark for longcontext interleaved video-language understanding. arXiv preprint arXiv:2407.15754. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. 2024. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024a. Mmmu: massive multi-discipline multimodal understanding In Proand reasoning benchmark for expert agi. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. 2024b. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. are then evaluated by six human annotators, each tasked with comparing the outputs of two MLLMs for addressing the user instruction queries. On average, each comparison took approximately 90.6 seconds. In contrast, the MLLM-asa-Judge method completes the task in just few seconds via an API call, highlighting the superior speed and efficiency of model-based evaluation. The annotation interface used for this task is shown in Fig. 9. Overall, we observe 79.9% agreement between human annotators and the MLLM-as-aJudge. Refer to Fig. 10 that illustrates the distribution of human annotator preferences, MLLM preferences, and human annotation time cost."
        },
        {
            "title": "D Analysis",
            "content": "In Fig. 11, we further present the distributions of image distribution, textual challenges, image challenges, and reasoning challenges across the user instruction queries. Tab. 13 provides examples of MLLM-as-a-Judge evaluations, with key information highlighted in red to indicate correctness or errors."
        },
        {
            "title": "A Experimental detail",
            "content": "We respectively present detailed comparisons of multi-linguistic and multi-round tracks in Tab. 3 and Tab. 4. The optimization details for tuning local evaluator based on Llama-3.2-11B-Vision-Instruct are 5 provided below. We use learning rate of 1 10 for both the projector and the LLM, while setting 6 for the vision enlower learning rate of 2 10 coder. The context length is set to 128K. cosine annealing strategy with 3% warm-up of the total optimization steps is employed. The AdamW optimizer is used with β1 = 0.9 and β1 = 0.95, along with weight decay of 0.03. We train with batch size of 16 for 20K optimization steps. The model is trained using 16 H100 GPUs, with the training process taking approximately 2 days. For evaluation with MLLM-as-the-Judge, the largest models require around two days for response generation on 8 GPUs, while evaluation with the local evaluator takes about one day using 2 GPUs."
        },
        {
            "title": "All data from ProBench has been collected with",
            "content": "explicit user consent."
        },
        {
            "title": "B Prompt template",
            "content": "We present the prompts for curating the singleround, multi-linguistic, and multi-round tracks, as well as for utilizing MLLM-as-a-Judge across the three tracks: Tab. 5, Tab. 6, Tab. 7, and Tab. 4 provide prompts for categorizing task and sub-task fields related to user instruction queries; Tab. 5 and Tab. 6 present prompts for evaluating challenges within user instruction queries; Tab. 7 and Tab. 8 are prompts for deduplications between visual and textual content in user instruction queries (i. e., image-instruction deduplication); Tab. 9 offers prompts for assessing interdependencies among multi-round user instruction queries; Tab. 10, Tab. 11, and Tab. 12 respectively give the prompts of MLLM-as-a-Judge for the three tracks."
        },
        {
            "title": "C Human preference evaluation",
            "content": "To assess the agreements and reliability of MLLMas-a-Judge, we evaluate the alignment between human annotators and gpt-4o-2024-08-06 as judge. All participants are volunteers who have been informed about the purpose of the study and have provided consent to share their data. In this experiment, random sample of 300 responses is drawn from the ProBench dataset. These responses Table 3: Comparisons of state-of-the-art MLLMs on the multi-linguistic track are presented using the following abbreviations: PT (Portuguese), FR (French), ES (Spanish), DE (German), and an Other category (e.g., Chinese, Vietnamese, and more). We provide ELO ratings for each language, followed by an overview that includes the average number of output tokens (#Token), 95% confidence interval (95% CI), win rate (WR), and overall ELO rating. The MLLMs are sorted by the overall ELo rating in each group. Model Proprietary MLLMs claude-3-5-sonnet-20241022 gpt-4o-2024-05-13 gemini-1.5-pro-002 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gemini-1.5-flash-002 70B+ Open-source MLLMs Pixtral-Large-Instruct-2411 Qwen2-VL-72B-Instruct InternVL2_5-78B NVLM-D-72B Llama-3.2-90B-Vision-Instruct Molmo-72B-0924 llava-onevision-qwen2-72b-ov 10B+ Open-source MLLMs (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) 124B 72B 78B 72B 90B 72B 72B InternVL2_5-38B Pixtral-12B-2409 Aria-Chat InternVL2_5-26B Llama-3.2-11B-Vision-Instruct 38B 12B 3.9/25.3B 26B 11B 7B+ Open-source MLLMs Qwen2-VL-7B-Instruct InternVL2_5-8B MiniCPM-V-2_6 Molmo-7B-D-0924 llava-onevision-qwen2-7b-ov Molmo-7B-O-0924 7B 8B 8B 7B 7B 7B PT 1248 1114 1273 1159 1038 1031 1229 1067 948 900 905 834 782 1038 935 964 779 701 760 522 445 579 383 Languge-Specific ELO Ratings Overview FR 1319 1114 1168 1224 1079 990 1496 1199 1125 863 860 835 1092 1096 1042 858 663 875 776 559 495 386 256 ES 1335 1114 1131 1226 1071 845 1216 944 1035 850 824 852 609 1070 998 983 782 673 765 603 577 144 536 DE 1389 1114 1168 1259 1151 1015 1324 1241 1123 898 863 853 800 1100 1077 1041 880 627 865 821 634 613 403 Other #Token 95% CI WR Elo 1309 1114 1139 1114 1099 815 1286 999 1084 918 864 878 1044 929 999 839 665 678 602 455 505 588 429 485 585 629 480 657 567 966 834 841 907 968 426 534 868 1199 1014 814 2027 1216 1021 890 406 686 50.0 (0, 0) (-21, 29) 74.58 1301 1114 (-20, 20) 59.11 1178 (-17, 26) 60.35 1187 (-21, 16) 45.84 1085 954 (-25, 19) 28.47 (-23, 22) 73.81 1294 (-18, 21) 47.56 1097 (-14, 20) 42.71 1063 894 (-17, 25) 21.99 883 (-29, 21) 20.92 861 (-27, 19) 18.9 767 (-27, 24) 11.95 (-20, 18) 43.98 1072 (-14, 22) 35.73 1012 (-23, 17) 35.33 1009 847 (-28, 19) 699 (-29, 21) 17.7 8. (-24, 22) 12.25 (-22, 20) 11.95 4.44 (-36, 35) 4.32 (-52, 33) 3.07 (-68, 37) 1.95 (-73, 51) 772 767 581 576 514 433 Table 4: Comparisons of state-of-the-art MLLMs on the multiround track are presented. We provide ELO ratings for rounds with lengths of 2, 3, 4, 5, and more than 6 (6+), followed by an overview that includes the average number of output tokens (#Token), 95% confidence interval (95% CI), win rate (WR), and overall ELO rating. N/A indicates cases where the model did not apply, as it lost to gpt-4o-2024-05-13 across all samples. The MLLMs are sorted by the overal ELo rating in each group Model Proprietary MLLMs claude-3-5-sonnet-20241022 gpt-4o-2024-05-13 gemini-1.5-pro-002 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gemini-1.5-flash70B+ Open-source MLLMs Pixtral-Large-Instruct-2411 Qwen2-VL-72B-Instruct InternVL2_5-78B NVLM-D-72B Llama-3.2-90B-Vision-Instruct Molmo-72B-0924 llava-onevision-qwen2-72b-ov 10B+ Open-source MLLMs (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) 124B 72B 78B 72B 90B 72B 72B InternVL2_5-38B Pixtral-12B-2409 Aria-Chat InternVL2_5-26B Llama-3.2-11B-Vision-Instruct 38B 12B 3.9/25.3B 26B 11B 7B+ Open-source MLLMs Qwen2-VL-7B-Instruct InternVL2_5-8B MiniCPM-V-2_6 Molmo-7B-D-0924 llava-onevision-qwen2-7b-ov Molmo-7B-O-0924 7B 8B 8B 7B 7B 7B 2 1260 1114 1136 1146 1147 1015 1233 1023 1135 770 754 886 1003 1054 937 881 741 808 814 664 672 737 589 Round-Specific ELO Ratings Overview 3 1249 1114 1140 1050 1143 1273 972 1040 557 757 817 721 1037 1008 913 811 380 622 724 575 470 591 413 4 1356 1114 1107 1138 1142 1015 1304 1033 1148 602 784 787 1036 1160 946 805 487 637 775 628 523 649 490 5 1248 1114 1207 1023 1200 1119 1376 936 1015 641 426 920 525 913 1013 887 753 557 686 530 409 N/A N/A 6+ #Token 95% CI WR 1321 1114 1145 965 1151 1253 875 992 682 605 808 692 902 1035 812 638 490 495 559 389 618 512 402 1477 1563 1425 1052 1749 1388 2593 1608 2015 1371 1350 967 1176 1734 2264 2321 1554 2004 1835 1861 923 1743 925 (-20, 18) (0, 0) (-26, 19) (-22, 18) (-17, 24) (-16, 19) (-23, 19) (-21, 19) (-21, 20) (-35, 33) (-36, 24) (-28, 25) (-31, 26) (-18, 21) (-19, 20) (-27, 12) (-27, 28) (-38, 32) (-34, 25) (-25, 22) (-33, 37) (-34, 26) (-30, 30) (-49, 37) 70.82 50.0 53.88 45.41 55.16 38. 69.73 32.24 44.84 8.49 9.88 18.64 10.3 34.68 40.48 23.92 15.77 6.03 9.48 11.77 5.35 5.04 6.58 3.43 Elo 1268 1114 1141 1082 1150 1030 1259 985 1078 701 730 858 1004 1047 913 823 637 722 764 615 604 653 534 Table 5: The prompt for identifying user instruction query task fields. [System] You are an AI assistant tasked with classifying user - provided question and image into predefined categories . The question should be classified based on both the text of the question and the image provided , while the image classification should be based solely on the visual content of the image . Your responsibilities are : 1. Analyze the question and classify it under one category from the following list : - Coding : Focuses on code - related tasks such as debugging , generating , translating , and understanding programming logic . - Information Extraction : Involves tasks like extracting and analyzing details from data , structured parsing , summarization , and multimodal & A. - Knowledge : Covers arts , culture , fact - checking , and understanding diverse global and historical knowledge . - Mathematics : Includes problem - solving in algebra , calculus , geometry , number theory , graph theory , and numeric reasoning . - Metrics : Evaluates quality and performance in images , videos , papers , and other models or generated content . - Perception : Encompasses tasks like 3D understanding , image segmentation , multimodal captioning , and object or scene understanding . - Planning : Deals with creating strategies for agents , solving puzzles , reordering tasks , and planning complex processes . - Science : Applies to specialized domains like chemistry , physics , life sciences , and STEM - related problem - solving . - Creative Writing : Covers character development , storytelling , poetry , dialogue , scriptwriting , and worldbuilding across genres . - Arts and Humanities : Involves creative and cultural exploration , metaphorical thinking , narrative techniques , and genre - specific expression . 2. Classify the image into one of the main categories : - Document and Text - based Images : Includes scanned documents , forms , tables , and charts , used for record - keeping , data presentation , or analysis . - Medical Images : Diagnostic visuals like MRIs , - rays , and pathology slides , used in healthcare and medical research . - Photographs : Everyday pictures , portraits , and landscapes captured with cameras , often for personal or professional use . - Scientific and Analytical Images : Specialized visuals like microscopic , astronomical , or spectrogram images for research and technical analysis . - Graphics and Artistic Images : Includes infographics , logos , cartoons , and illustrations for creative , branding , or informative purposes . - Screenshots and UI Elements : Captures of websites , apps , or software interfaces for documentation or demonstration . - Remote Sensing and Satellite Images : Aerial and satellite photos for mapping , monitoring , or geographic analysis . - Security and Surveillance : CCTV footage and thermal imaging for safety , monitoring , or investigative purposes . - Engineering and Technical Drawings : CAD designs , blueprints , and 3D models for architectural or engineering applications . - Specialized Formats : Includes barcodes , QR codes , fingerprints , and AR / VR visuals for unique or advanced use cases . 3. If the question or image does not fit existing categories , propose new category with justification . 4. Do not generate the answer for the user question . Your response should be in JSON format : { \" thinking_image \": \" Reasoning for your classification of image .\" , \" image_category \": \" The category of the image .\" \" thinking_question \": \" Reasoning for your classification of question .\" , \" question_category \": \" The category of the user question .\" , } [System] Table 6: The prompt for identifying user instruction query sub-task fields. You are an AI assistant tasked with further classifying user - provided question and image into sub - categories . The question should be classified based on both the text of the question and the image provided , while the image classification should be based solely on the visual content of the image . Your responsibilities are : 1. ** Question Classification **: - Analyze the question and assign it to the most relevant sub - category based on its content . - The question belongs to the main category \"{ question_category }\" and should be classified into one of the following sub - categories : { question_subcats_formatted } 2. ** Image Classification **: - Analyze the image and assign it to the most relevant sub - category based solely on its visual content . - The image belongs to the main category \"{ image_category }\" and should be classified into one of the following sub - categories : { image_subcats_formatted } 3. If the question or image does not fit any of the above sub - categories , propose new sub - category and provide justification . 4. Do not generate the answer for the user question . Your response must be structured in the following JSON format : {{ \" thinking_image \": \" Reasoning for the image sub - category classification .\" , \" image_subcategory \": \" The sub - category for the image .\" \" thinking_question \": \" Reasoning for the question sub - category classification .\" , \" question_subcategory \": \" The sub - category for the question .\" , }} Table 7: The task and sub-task fields for user instruction queries (e.g., questions). For consistency, the naming convention aligns with Tab. 6. question_category represents the task field, while question_subcats_formatted denotes the task sub-field. question_category question_subcats_formatted Information Extraction"
        },
        {
            "title": "Science",
            "content": "* App Function Understanding : Analyzing and interpreting the purpose , features , and functionality of an application . * Summarization : Condensing detailed information into concise form while preserving key points and context . * Entity Recognition : Identifying and categorizing specific elements such as names , dates , locations , or organizations . * Relationship Mapping : Identifying and visualizing the connections or associations between different entities . * Contextual Analysis : Understanding the meaning , intent , or relevance of data within its specific context . * Storytelling : Developing compelling and engaging narratives for readers or audiences . * Scriptwriting : Creating scripts for various media formats , including films , television , and plays . * Worldbuilding : Designing intricate and immersive fictional settings , universes , or environments . * Character Development : Creating , evolving , and deepening the personalities and arcs of fictional characters . * Plot Structuring : Organizing the sequence of events and narrative flow to build tension , conflict , and resolution . * Physics : The exploration of forces , motion , energy , and the fundamental nature of the universe . * Biology : The study of living organisms , their functions , and interactions within ecosystems . * Astronomy : The observation and study of celestial objects , space , and the physical universe as whole . * Life Science / Medical : The study of biological and medical sciences , including anatomy , physiology , and healthcare - related topics . * STEM Problem - Solving : Using interdisciplinary approaches to tackle technical and scientific challenges . Continued on next page... question_category question_subcats_formatted Knowledge"
        },
        {
            "title": "Coding",
            "content": "* Human and Culture : Insights into human behavior , societal structures , traditions , and cultural practices . * Scientific Knowledge : Understanding and explaining scientific concepts , theories , and principles across disciplines . * World Knowledge : General information about global geography , politics , economies , and cultures . * Fact - Checking : Verifying the accuracy of information and identifying misinformation or inaccuracies . * Philosophical Inquiry : Exploring existential , ethical , and metaphysical questions to gain deeper understanding . * Model Performance : Assessing the accuracy , efficiency , and reliability of algorithms or machine learning models . * Paper Review : Critiquing and analyzing research papers for quality , relevance , and scientific rigor . * Content Evaluation : Judging the quality , coherence , and relevance of generated or provided content . * Quality Assessment : Measuring and determining the overall standard or quality of various outputs or systems . * Reward Models : Designing and evaluating models that provide feedback or incentives for optimizing performance in systems . * Code Generation : Creating new code based on given requirements , templates , or problem - solving scenarios . * Code Translation : Converting code from one programming language or framework to another . * Code Optimization : Enhancing the efficiency , readability , and performance of existing code . * Code Understanding : Interpreting and explaining the purpose , logic , or functionality of code . Continued on next page... question_category question_subcats_formatted Perception * Counting : Identifying and quantifying the number of objects or elements in an image or scene . * Multimodal Captioning : Generating descriptive captions by combining visual and textual data for an enriched understanding . * Object Understanding : Recognizing , categorizing , and interpreting the attributes and roles of objects in visual content . * Scene Understanding : Comprehending the arrangement , context , and interactions within visual scene . * Diagram and Document Understanding : Interpreting and extracting information from diagrams , charts , or text - based documents . Arts and Humanities * Cultural Analysis : Examining societal norms and values . * Narrative Techniques : Exploring storytelling methods . * Genre - Specific Writing : Crafting work within specific literary or artistic genres ."
        },
        {
            "title": "Planning",
            "content": "* Calculus : Analyzing rates of change and accumulation using derivatives and integrals . * Function : Studying relationships between inputs and outputs , represented mathematically . * Geometry : Exploring shapes , sizes , dimensions , and the properties of space . * Graph Theory : Analyzing the relationships between nodes and edges in network or graph . * Number Theory : Investigating the properties , patterns , and relationships of numbers , especially integers . * Statistics / Numerical Reasoning : Interpreting , analyzing , and presenting data to draw logical inferences and conclusions . * Reordering : Resequencing tasks or events to optimize efficiency and effectiveness . * Puzzle Solving : Finding logical or creative solutions to abstract , conceptual , or practical challenges . * Game Strategy : Developing tactics , plans , and approaches to achieve success in game environments . * Complex Workflow Design : Designing and managing intricate , multi - step processes to accomplish complex tasks or objectives . Continued on next page... question_category question_subcats_formatted Other Unspecified or generic category. Table 4: The field and sub-field for images in user instruction queries. For consistency, the naming convention aligns with Tab. 6. image_category represents the image field, while image_subcats_formatted denotes the image sub-field. image_category image_subcats_formatted Screenshots and UI Elements * Mobile App UI : User interfaces for mobile applications . * Desktop Applications : Screenshots of software interfaces . * Game Interfaces : Displays from video games . * Interactive Tools : Screenshots of tools requiring user input . Document and Text-based Images"
        },
        {
            "title": "Scientific and\nAnalytical\nImages",
            "content": "* Tables : Data systematically organized in rows and columns for easy analysis and interpretation . * Scanned Documents : Digital copies of physical documents , often used for record - keeping or archival purposes . * Charts and Graphs : Visual tools to represent data trends , comparisons , or distributions , such as bar charts , pie charts , or line graphs . * Handwritten Notes : Freehand textual or graphical information , often informal or personal in nature . * Diagrams : Illustrations that depict relationships , processes , systems , or concepts using symbols , shapes , and connections , such as flowcharts , mind maps , or organizational charts . * Astronomical Images : Visuals of celestial objects or phenomena . * Spectrograms : Graphs displaying signal frequencies over time . * Graphs : Plots representing relationships between variables . * Experimental Results : Visual data from scientific experiments . Continued on next page... image_category image_subcats_formatted Engineering and Technical Drawings * Blueprints : Detailed architectural or engineering drawings . * 3D Models : Digital representations of three - dimensional objects . * Schematics : Diagrams showing systems or circuits . * Flow Diagrams : Graphs representing processes or workflows . Medical Images"
        },
        {
            "title": "Photographs",
            "content": "* MRIs : High - resolution imaging using magnetic resonance technology to capture detailed views of organs and tissues . * Pathology Slides : Microscopic images of tissues or cells used for diagnosing diseases . * Ultrasound : Images produced using sound waves to visualize internal body structures , commonly used in prenatal and organ assessments . * Microscopic Images : Magnified visuals of biological specimens , such as cells or microorganisms , for medical analysis . * CT Scans : Cross - sectional images of the body generated using computed tomography to provide detailed anatomical views . * Landscapes : Scenic views showcasing natural environments or urban settings , often highlighting beauty or scale . * Wildlife : Images capturing animals in their natural habitats , emphasizing behavior and environment . * Street Photography : Candid shots portraying urban life , capturing everyday moments and street scenes . * Event Photography : Documenting significant occasions such as weddings , conferences , or celebrations . * Daily Photos : Casual and informal photographs capturing everyday moments , activities , or surroundings . Continued on next page... image_category image_subcats_formatted Graphics and Artistic Images * Logos : Graphic symbols or emblems used to identify brands , companies , or organizations . * Cartoons : Illustrations with humorous , exaggerated , or narrative style , often used in storytelling or entertainment . * Illustrations : Artistic visuals created to complement text or communicate creative ideas . * Posters : Artistic layouts designed for advertisements , events , or promotions . * Abstract Art : Creative visuals emphasizing color , shape , and form without specific subjects . * Typography Art : Designs focusing on stylized text and fonts to create visual impact . Remote Sensing and Satellite Images * Thermal Images : Heat - map visuals for temperature analysis . * Multispectral Images : Images across various light wavelengths . * Topographic Maps : Maps showing elevation and terrain features ."
        },
        {
            "title": "Specialized\nFormats",
            "content": "* QR Codes : Two - dimensional codes for quick scanning . * Fingerprints : Unique ridged patterns for identification . * AR / VR Visuals : Content designed for augmented or virtual reality ."
        },
        {
            "title": "Other",
            "content": "Unspecified or generic category. Table 5: The prompt for identifying user instruction challenge in the single-round track and multi-linguistic track. Scores below 6 are considered easy, while scores of 6 or higher are classified as hard. [System] You are an AI assistant tasked with assessing the challenges of answering user - provided question that combines textual instructions and visual images . reference answer will be provided to guide your assessment . ### Input Format : The input consists of three components in the following order : 1. Visual Images : One or more images relevant to the question . 2. Textual Instruction : Enclosed in < inst /> tags . 3. Reference Answer : Enclosed in < answer /> tags . { images } Textual Instruction : < inst /> { instruction text } </ inst /> Reference Answer : < answer /> { reference answer } </ answer /> ### Scoring Criteria Evaluate the difficulty across three dimensions using scale of 1 -10 , where higher scores indicate greater difficulty : 1. Textual Complexity ( How complex is the instruction ?) : - (1.1) Score 0: The instruction is redundantly presented in both visual and textual content . - (1.2) Score 1 -3: Simple , straightforward instructions with minimal requirements and no domain knowledge needed . - (1.3) Score 4 -6: Moderately complex instructions with some context and basic domain knowledge required . - (1.4) Score 7 -9: Complex instructions with multiple requirements and specialized domain knowledge needed . - (1.5) Score 10: Highly complex instructions requiring significant expertise and precise understanding . 2. Visual Complexity ( How complex are the images ?) - (2.1) Score 0: The visual content merely duplicates the textual instruction . - (2.2) Score 1 -3: Simple images with clear , distinct elements requiring minimal interpretation . - (2.3) Score 4 -6: Moderately complex images with multiple elements requiring basic interpretation . - (2.4) Score 7 -9: Complex images with multiple interrelated elements requiring domain knowledge . - (2.5) Score 10: Highly complex images requiring specialized expertise to interpret . 3. Reasoning Complexity ( How complex is the integration of text and image ?) - (3.1) Score 0: Question can be answered using text alone , images are unnecessary . - (3.2) Score 1 -3: Simple reasoning requiring basic observation of text and images . - (3.3) Score 4 -6: Moderate reasoning requiring integration of text and images with basic domain knowledge . - (3.4) Score 7 -9: Complex reasoning requiring careful integration of text and images with specialized knowledge . - (3.5) Score 10: Advanced multi - step reasoning requiring expert knowledge to integrate complex text and images . ### Important Notes : - Focus only on difficulty assessment - do not attempt to answer the question . - Provide specific examples from the input when explaining scores . - Consider the reference answer 's approach when evaluating complexity . - Each dimension must be scored independently . ### Response Format : Provide your assessment in the following JSON structure : { \" challenge_textual \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (1.1 -1.5) and examples from the input \", \" score \": Integer value between 0 -10 }, \" challenge_image \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (2.1 -2.5) and examples from the input \", \" score \": Integer value between 0 -10 }, \" challenge_reasoning \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (3.1 -3.5) and examples from the input \", \" score \": Integer value between 0 - } } Table 6: The prompt for identifying user instruction challenge in the multi-round track. Scores below 6 are considered easy, while scores of 6 or higher are classified as hard. [System] You are an AI assistant tasked with assessing the challenges of answering user - provided question that combines textual instructions and visual images . reference answer will be provided to guide your assessment . ### Input Format : The input consists of two primary components : 1. Visual Images : One or more images relevant to the question . 2. Each turn which is Enclosed by < turn { number }> contains : - Textual Instruction : Enclosed in < inst / > tags - Reference Answer : Enclosed in < ans / > tags { images } < turn { number }/ > Textual Instruction : < inst /> { instruction text } </ inst > Reference Answer : < ans /> { reference answer } </ ans > </ turn { number }> ### Scoring Criteria Evaluate the difficulty across three dimensions using scale of 1 -10 , where higher scores indicate greater difficulty : 1. Textual Complexity ( How complex is the instruction ?) : - (1.1) Score 0: The instruction is redundantly presented in both visual and textual content . - (1.2) Score 1 -3: Simple , straightforward instructions with minimal requirements and no domain knowledge needed . - (1.3) Score 4 -6: Moderately complex instructions with some context and basic domain knowledge required . - (1.4) Score 7 -9: Complex instructions with multiple requirements and specialized domain knowledge needed . - (1.5) Score 10: Highly complex instructions requiring significant expertise and precise understanding . 2. Visual Complexity ( How complex are the images ?) - (2.1) Score 0: The visual content merely duplicates the textual instruction . - (2.2) Score 1 -3: Simple images with clear , distinct elements requiring minimal interpretation . - (2.3) Score 4 -6: Moderately complex images with multiple elements requiring basic interpretation . - (2.4) Score 7 -9: Complex images with multiple interrelated elements requiring domain knowledge . - (2.5) Score 10: Highly complex images requiring specialized expertise to interpret . 3. Reasoning Complexity ( How complex is the integration of text and image ?) - (3.1) Score 0: Question can be answered using text alone , images are unnecessary . - (3.2) Score 1 -3: Simple reasoning requiring basic observation of text and images . - (3.3) Score 4 -6: Moderate reasoning requiring integration of text and images with basic domain knowledge . - (3.4) Score 7 -9: Complex reasoning requiring careful integration of text and images with specialized knowledge . - (3.5) Score 10: Advanced multi - step reasoning requiring expert knowledge to integrate complex text and images . ### Important Notes : - Focus only on difficulty assessment - do not attempt to answer the question . - Provide specific examples from the input when explaining scores . - Consider the reference answer 's approach when evaluating complexity . - Each dimension must be scored independently . ### Response Format : Provide your assessment in the following JSON structure : { \" challenge_textual \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (1.1 -1.5) and examples from the input \", \" score \": Integer value between 0 -10 }, \" challenge_image \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (2.1 -2.5) and examples from the input \", \" score \": Integer value between 0 - }, \" challenge_reasoning \": { \" explanation \": \" Detailed explanation referencing specific scoring criteria (3.1 -3.5) and examples from the input \", \" score \": Integer value between 0 -10 } } Table 7: The prompt for image-instruction deduplication in the single-round track and multi-linguistic track. [System] You are an AI assistant tasked with determining whether user question can be answered solely by the textual instruction , when user provides both visual images and textual instruction . ### Input Format : The input consists of two primary components : 1. Visual Images : One or more images relevant to the question 2. Textual Instruction : Enclosed in < inst /> tags { images } Textual Instruction : < inst / > { instruction text } < inst / > ### Evaluation Criteria : - Carefully analyze the textual instruction and the associated question . - Assess whether the ENTIRE question can be comprehensively answered using ONLY the text provided . ### Decision Guidelines : - YES : If the textual instruction provides comprehensive , unambiguous information to answer the question - NO : If any critical piece of information is missing or requires visual interpretation to answer the question ### Response Format : Provide your assessment in the following JSON structure : { \" reasoning \": \" Clearly outline your analysis and explain the logic behind your conclusion .\" , \" decision \": \" YES or NO \" } [System] Table 8: The prompt for image-instruction deduplication in the multi-round track. You are an AI assistant tasked with evaluating the dependency of textual instructions on visual information across multi - turn conversation . ### Input Format : The input consists of two primary components : 1. Visual Images : Provided at the beginning of the conversation 2. Each turn which is Enclosed by < turn { number }> contains : - Textual Instruction : Enclosed in < inst / > tags - Answers : Enclosed in < ans /> tags { images } < turn { number }/ > Textual Instruction : < inst /> { instruction text } < inst /> Answers : < ans / > { answer text } < ans / > </ turn { number }> { More continuing conversation turns ...} ### Evaluation Criteria : - Carefully analyze the textual instruction from ALL conversation turns - Assess whether the ENTIRE set of instructions can be comprehensively answered without using the visual / image information - Consider the cumulative context and details from all turns . ### Decision Guidelines : - YES : If textual instructions across all turns can be fully understood and addressed without relying on the visual / image information - NO : If any critical piece of information is missing or requires visual interpretation to answer the question ### Response Format : Provide your assessment in the following JSON structure : { \" reasoning \": \" Clearly outline your analysis and explain the logic behind your conclusion .\" , \" decision \": \" YES or NO \" } Table 9: The prompt for assessing interdependency among user instruction queries in the multi-round track. [System] You are an AI assistant tasked with determining whether the turns in multi - turn conversation are independent or interconnected . ### Input Format : The input consists of two primary components : 1. Visual Images : Provided at the beginning of the conversation 2. Each turn which is Enclosed by < turn { number }> contains : - Textual Instruction : Enclosed in < inst / > tags - Answers : Enclosed in < ans /> tags { images } < turn { number }/ > Textual Instruction : < inst / > { instruction text } < inst / > Answers : < ans /> { answer text } < ans /> </ turn { number }> { More continuing conversation turns ...} ### Independence Criteria : Independent Turns : - Each turn can be understood and are answered in isolation - No contextual dependency between turns - No clear progression or building upon previous turns Interconnected Turns : - Turns have logical progression , . e., later turns depend on context from earlier turns - Conversation follows coherent narrative or problem - solving flow ### Decision Guidelines : - YES : If turns are completely independent - NO : If turns are interconnected and cannot be meaningfully separated ### Response Format : Provide your assessment in the following JSON structure : { \" reasoning \": \" Clearly outline your analysis and explain the logic behind your conclusion .\" , \" decision \": \" YES or NO \" } Table 10: The prompt for MLLM-as-a-Judge for the single-round track. [System] You are an impartial judge tasked with evaluating two AI assistants ' responses to given prompt involving textual instructions and visual images . ### Evaluation Framework #### Generate Your Own Answer 1. Generate an independent , original prompt 2. Serves as benchmark for comparison 3. Demonstrates the ideal response approach high - quality answer to the #### Evaluation Dimensions Assess the assistants ' answers based on the following dimensions : 1. Correctness - Accuracy of information - Absence of factual and demonstrable errors - Alignment with known knowledge and visual evidence 2. Helpfulness - Directly addresses the user ' instructions - Provides clear and practical guidance - Anticipates and resolves potential user questions 3. Relevance - Stringent focus on the prompt requirements - Eliminates extraneous or tangential information - Maintains precise topical alignment 4. Conciseness - Delivers information efficiently - Avoids unnecessary verbosity - Uses clear , direct language 5. Completeness - Covers all essential aspects of the prompt - Provides sufficient information to fully address the user 's needs #### Comparative Analysis - Directly compare Assistant and Assistant 's responses - Nuanced evaluation of relative strengths and weaknesses - Evidence - based assessment with specific textual references #### Judgment Guidelines 1. Avoid any position biases and ensure that the order in which the assistants ' responses were presented does not influence your decision 2. When the prompt contains ambiguity : - Prioritize requesting clarification over making assumptions - Evaluate how well each assistant handles potential uncertainties ### Input Format 1. Visual Images : Relevant images 2. Textual Instruction : Enclosed in < inst /> tags 3. Assistant ' Answer : Enclosed in <a/ > tags 4. Assistant ' Answer : Enclosed in <b/ > tags { images } Textual Instruction : < inst /> { instruction text } </ inst > Assistant 's Answer : <a/> { Answers from Assistant } </a > Assistant 's Answer : <b/> { Answers from Assistant } </b > ### Response Format Answer : [ Your comprehensive answer to the prompt ] Detailed Explanation : [ Thorough , point -by - point comparison of Assistant and ' responses ] Specific Observations : - Correctness assessment - Helpfulness evaluation - Relevance analysis - Conciseness review - Completeness check Final Verdict : Select ONE of the following : - [[A >>B ]]: Assistant is significantly better - [[A >B ]]: Assistant is slightly better - [[ =B ]]: Tie , relatively the same - [[B >A ]]: Assistant is slightly better - [[B >>A ]]: Assistant is significantly better Table 11: The prompt for MLLM-as-a-Judge for the multi-linguistic track. [System] You are an impartial judge tasked with evaluating two AI assistants ' responses to given prompt involving textual instructions and visual images . ### Evaluation Framework #### Generate Your Own Answer 1. Generate an independent , original prompt 2. Serves as benchmark for comparison 3. Demonstrates the ideal response approach high - quality answer to the #### Evaluation Dimensions Assess the assistants ' answers based on the following dimensions : 1. Correctness - Accuracy of information - Absence of factual and demonstrable errors - Alignment with known knowledge and visual evidence - Response must be in the same language as the textual instruction ( unless explicitly specified otherwise ) 2. Helpfulness - Directly addresses the user ' instructions - Provides clear and practical guidance - Anticipates and resolves potential user questions 3. Relevance - Stringent focus on the prompt requirements - Eliminates extraneous or tangential information - Maintains precise topical alignment 4. Conciseness - Delivers information efficiently - Avoids unnecessary verbosity - Uses clear , direct language 5. Completeness - Covers all essential aspects of the prompt - Provides sufficient information to fully address the user 's needs #### Comparative Analysis - Directly compare Assistant and Assistant ' responses - Nuanced evaluation of relative strengths and weaknesses - Evidence - based assessment with specific textual references #### Judgment Guidelines 1. Avoid any position biases and ensure that the order in which the assistants ' responses were presented does not influence your decision 2. When the prompt contains ambiguity : - Prioritize requesting clarification over making assumptions - Evaluate how well each assistant handles potential uncertainties ### Input Format 1. Visual Images : Relevant images 2. Textual Instruction : Enclosed in < inst /> tags 3. Assistant ' Answer : Enclosed in <a /> tags 4. Assistant ' Answer : Enclosed in <b /> tags { images } Textual Instruction : < inst /> { instruction text } </ inst > Assistant 's Answer : <a/> { Answers from Assistant } </a > Assistant 's Answer : <b/> { Answers from Assistant } </b > ### Response Format Answer : [ Your comprehensive answer to the prompt ] Detailed Explanation : [ Thorough , point -by - point comparison of Assistant and ' responses ] Specific Observations : - Correctness assessment - Helpfulness evaluation - Relevance analysis - Conciseness review - Completeness check Final Verdict : Select ONE of the following : - [[A >>B ]]: Assistant is significantly better - [[A >B ]]: Assistant is slightly better - [[ =B ]]: Tie , relatively the same - [[B >A ]]: Assistant is slightly better - [[B >>A ]]: Assistant is significantly better Table 12: The prompt for MLLM-as-a-Judge for the multi-round track. [System] You are an impartial judge tasked with evaluating two AI assistants ' responses to given prompts involving textual instructions and visual images . ### Evaluation Framework #### Generate Your Own Answer 1. Generate an independent , high - quality answer to the original prompt 2. Serves as benchmark for comparison 3. Demonstrates the ideal response approach #### Evaluation Dimensions Assess the assistants ' answers based on the following dimensions : 1. Correctness - Accuracy of information - Absence of factual and demonstrable errors - Alignment with known knowledge and visual evidence 2. Helpfulness - Directly addresses the user ' instructions - Provides clear and practical guidance - Anticipates and resolves potential user questions 3. Relevance - Stringent focus on the prompt requirements - Eliminates extraneous or tangential information - Maintains precise topical alignment 4. Conciseness - Delivers information efficiently - Avoids unnecessary verbosity - Uses clear , direct language 5. Completeness - Covers all essential aspects of the prompt - Provides sufficient information to fully address the user 's needs #### Comparative Analysis - Directly compare Assistant and Assistant ' responses - Nuanced evaluation of relative strengths and weaknesses - Evidence - based assessment with specific textual references #### Judgment Guidelines 1. Avoid any position biases and ensure that the order in which the assistants ' responses were presented does not influence your decision 2. When the prompt contains ambiguity : - Prioritize requesting clarification over making assumptions - Evaluate how well each assistant handles potential uncertainties ### Input Format 1. Visual Images : Relevant images 2. Textual Instruction : Enclosed in < inst /> tags 3. Turn Structure : Each turn is enclosed with < turn { number }> tags 4. Assistant ' Answer : Enclosed in <a /> tags 5. Assistant ' Answer : Enclosed in <b /> tags { images } < turn { number }/ > Textual Instruction : < inst /> { instruction text } </ inst > Assistant 's Answer : <a /> { Answers from Assistant } </a > Assistant 's Answer : <b /> { Answers from Assistant } </b > </ turn { number }> ### Response Format Answer : [ Your comprehensive answer to the prompt ] Detailed Explanation : [ Thorough , point -by - point comparison of Assistant and ' responses ] Specific Observations : - Correctness assessment - Helpfulness evaluation - Relevance analysis - Conciseness review - Completeness check Final Verdict : Select ONE of the following : - [[A >>B ]]: Assistant is significantly better - [[A >B ]]: Assistant is slightly better - [[ A=B ]]: Tie , relatively the same - [[B >A ]]: Assistant is slightly better - [[B >>A ]]: Assistant is significantly better Figure 9: Interface used for human annotator to compare the outputs of multiple MLLMs. Figure 10: Statistic of human expert annotation experiment. (a) Score distribution of human expert judgment. (b) Score distribution of MLLM-as-a-Judge. (c) Time distribution of human expert annotation. Figure 11: Distribution of (a) image fields, and challenge levels on (b) textual, (c) image, (d) reasoning parts of user queries across the three tracks. (b)-(c), we show the single-round, multi-linguistic, and multi-round tracks from top to bottom. Table 13: Examples of MLLM-as-a-Judge with key information highlighted in red to indicate the correctness or errors. Image: Question: what does this say about triplet loss model for person re identification task, is the model performing well or not? go into full detail and explain gpt-4o-05-13: The image displays set of results from triplet loss model used for person re-identification tasks. Each column in the image represents separate example, where three images are shown: one anchor image, positive example (Pos), and negative example (Neg). The numerical values next to the positive and negative labels are likely the computed distances or similarity scores from the anchor image. ### Analysis of Model Performance: 1. **Interpretation of Scores:** - **Positive (Pos) Score:** This score {measures the similarity or closeness between the anchor image and the positive image (images of the same person ideally). lower score indicates higher similarity. - **Negative (Neg) Score:** This score measures the similarity between the anchor image and the negative image (images of different persons). higher score indicates greater dissimilarity, which is desired. 2. **Expected Model Behavior:** - Ideally, for the model to perform well, the positive score should be significantly lower than the negative score. This indicates that the model can effectively differentiate between images of the same person and images of different people. 3. **Observations from the Image:** - In most cases, the negative scores are higher than the positive scores, which suggests that the model can distinguish between different individuals to some extent. For example, in Anchor 0, the positive score is 0.83, and the negative score is 2.64, which is good separation. - However, some examples show closer scores or inappropriate scoring. For example, in Anchor 1, the positive score is 0.17, and the negative score is 1.35. Although the negative is still higher, the difference is not as pronounced, which might indicate potential issues in cases with subtle differences or challenging scenarios. 4. **Performance Consistency:** - The performance seems varied across different examples. This variation could depend on factors like the pose of the person, the lighting conditions, and the background. Models trained on more diverse datasets tend to handle such variations better. 5. **Further Evaluation:** - Quantitative metrics such as accuracy, precision, recall, and F1-score on labeled test set would provide more concrete evidence of model performance. - Analyzing cases where the model fails or performs poorly could help in understanding its limitations and areas for improvement, such as fine-tuning the model, augmenting the training dataset, or tweaking the loss function parameters. ### Conclusion: The model shows reasonable ability to differentiate between the same and different individuals in most displayed examples, suggesting level of effectiveness in the person re-identification task. However, the variation in score differences across different examples indicates that there could be room for improvement, particularly in handling challenging scenarios more consistently. Further testing and refinement are recommended based on the observed performance and the specific requirements of the application. Llama-3.2-90B-Vision-Instruct: The image presents visual representation of the performance of triplet loss model for person re-identification tasks. The model's effectiveness is evaluated based on its ability to correctly identify individuals across different images. **Model Performance:** **Positive Pairs:** The model successfully identifies positive pairs, * where the same person appears in two different images. * two different people appear in two different images. **Negative Pairs:** The model correctly identifies negative pairs, where **Key Observations:** **Anchor Images:** The anchor images are used as references to compare * with other images. **Positive Images:** The positive images are those that match the anchor * image, indicating the same person. * anchor image, indicating different people. **Negative Images:** The negative images are those that do not match the **Conclusion:** Based on the provided information, it appears that the model is performing well in identifying positive and negative pairs. However, without additional context or metrics such as accuracy, precision, recall, or F1-score, it is challenging to make definitive conclusion about the model's overall performance. Further evaluation and analysis would be necessary to determine the model's effectiveness in person re-identification tasks. Judge (Model A: gpt-4o-2024-05-13; Model B: Llama-3.2-90B-Vision-Instruct): Answer: The image shows results from triplet loss model used for person re-identification. The model's performance is evaluated by comparing distances between anchor, positive, and negative images. The goal is for positive pairs (same person) to have lower distances than negative pairs (different people). Detailed Explanation: - **Correctness:** - **Assistant A** correctly explains the purpose of the triplet loss model and interprets the scores, noting that lower positive scores and higher negative scores indicate better performance. - **Assistant B** provides general overview but lacks detailed analysis of the scores and their implications. - **Helpfulness:** - **Assistant A** offers thorough explanation, including expected model behavior and observations from the image, which helps in understanding the model's performance. - **Assistant B** gives basic explanation without delving into specifics, making it less helpful for detailed understanding. - **Relevance:** - **Assistant A** stays focused on the task, providing relevant insights into the model's performance and potential improvements. - **Assistant B** remains relevant but lacks depth in addressing the model's performance. - **Conciseness:** - **Assistant A** is detailed yet concise, covering all necessary aspects without unnecessary information. - **Assistant B** is concise but overly brief, missing critical details. - **Completeness:** - **Assistant A** covers all essential aspects, including interpretation of scores, expected behavior, observations, and suggestions for further evaluation. - **Assistant B** lacks completeness, missing detailed analysis and suggestions for improvement. Final Verdict: [[A>>B]]: Assistant is significantly better due to its comprehensive and detailed analysis of the model's performance. Judge (Model A: Llama-3.2-90B-Vision-Instruct; Model B: gpt-4o-2024-05-13): Answer: The image shows results from triplet loss model used for person re-identification. The model's performance is evaluated by comparing distances between anchor, positive, and negative images. The model seems to perform reasonably well, as positive scores are generally lower than negative scores, indicating it can distinguish between the same and different individuals. However, some variations in score differences suggest potential areas for improvement. Detailed Explanation: - **Correctness**: Both assistants correctly interpret the image as results from triplet loss model for person re-identification. They accurately describe the role of positive and negative scores in evaluating model performance. - **Helpfulness**: Assistant provides more detailed analysis, explaining the significance of score differences and potential issues with close scores. Assistant gives general overview but lacks depth in discussing specific examples or potential improvements. - **Relevance**: Both responses are relevant, focusing on the model's performance. Assistant B's response is more aligned with the prompt's request for detailed explanation. - **Conciseness**: Assistant is more concise but at the expense of depth. Assistant provides thorough explanation without unnecessary verbosity. - **Completeness**: Assistant offers more complete analysis, discussing expected model behavior, observations, and suggestions for further evaluation. Assistant A's response is less comprehensive. Specific Observations: - Assistant mentions the need for additional metrics but does not delve into specific examples or potential improvements. - Assistant provides detailed breakdown of scores, expected behavior, and suggestions for further evaluation, making it more informative. Final Verdict: [[B>A]]: Assistant is slightly better due to its detailed analysis and comprehensive explanation. Image: Question: This is architcture diagram of Densnet121. want you to understand the architecture of it by looking at it and give me detailed compionnt diagram of it. this picture is simply for infrnce prediction. However are transfer learning on it using our own dataset on this pre trained model and we are modifying the last layer of model by changing the layers to 4 from 15 of th pretrained model. We are testing the results aftr training the model with our own test dataset and predicting the inference result gpt-4o-05-13: The image you've provided appears to be simplified architectural diagram of DenseNet-121 (Dense Convolutional Network), which is type of convolutional neural network known for its densely connected layers. Here's detailed breakdown of the components typically found in DenseNet architecture: 1. **Input Image**: The network takes an input image, which in this case seems to be an X-ray image, as suggested by the image in the input section of the diagram. 2. **Convolutional Layer**: The first layer after the input is convolutional layer that applies set of filters to the input image to create feature map. This layer is responsible for capturing the low-level features such as edges and textures. 3. **Dense Blocks**: The architecture consists of multiple dense blocks. Each dense block contains several convolutional layers. The key characteristic of dense block is that each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. This creates highly dense connectivity pattern. 4. **Transition Layers**: Between the dense blocks are transition layers, which typically consist of convolutional layer and pooling layer to reduce the spatial dimensions of the feature maps and to help the network generalize better. 5. **Classification Layer**: After the last dense block, the feature maps are passed through global average pooling layer, followed by fully connected layer that outputs the predictions. In the original DenseNet-121, this fully connected layer would have as many units as the number of classes in the dataset it was trained on (for example, 1000 for ImageNet). For your case, since you are performing transfer learning and modifying the last layer of the model, you would replace the original classification layer with new fully connected layer that has 4 units instead of 15, corresponding to the number of classes in your dataset. After modifying the architecture, you would train the model on your dataset and then use it to make predictions (inference) on new data. The architecture diagram you have provided specifically marks the prediction as \"COVID-19,\" implying that the model is being used for the classification of X-ray images to detect whether they show signs of COVID-19. It's important to note that DenseNet architectures are quite flexible, and while DenseNet-121 is specific configuration, the principles of dense connectivity apply to all DenseNet variants. The \"121\" in DenseNet-121 refers to the total number of layers. Llama-3.2-11B-Vision-Instruct: The image you provided is diagram of the DenseNet-121 architecture, which is type of convolutional neural network (CNN) used for image classification tasks. The diagram shows the input layer, followed by four dense blocks, and finally, the output layer. Here is detailed component diagram of the DenseNet-121 architecture: **Input Layer** * Input size: 224 224 3 (RGB image) * Input type: Image **Dense Block 1** * Number of layers: 6 * Layer type: Convolutional * Filter size: 7 7 * Stride: 2 * Padding: Same * Activation function: ReLU **Dense Block 2** * Number of layers: 12 * Layer type: Convolutional * Filter size: 7 7 * Stride: 2 * Padding: Same * Activation function: ReLU **Dense Block 3** * Number of layers: 24 * Layer type: Convolutional * Filter size: 7 7 * Stride: 2 * Padding: Same * Activation function: ReLU **Dense Block 4** * Number of layers: 16 * Layer type: Convolutional * Filter size: 7 7 * Stride: 2 * Padding: Same * Activation function: ReLU **Output Layer** * Number of units: 1000 (number of classes in the ImageNet dataset) * Layer type: Fully connected * Activation function: Softmax In this diagram, the input layer takes an RGB image of size 224 224 3 as input. The image is then passed through four dense blocks, each consisting of multiple convolutional layers with filter size of 7 7 and stride of 2. The convolutional layers are followed by ReLU activation functions. The output of the last dense block is then passed through fully connected layer with 1000 units, which represents the number of classes in the ImageNet dataset. The output of the fully connected layer is then passed through softmax activation function to produce probability distribution over the 1000 classes. In your case, you are using the DenseNet-121 model as pre-trained model and modifying the last layer to have 4 units instead of 1000 units. This is because you have different number of classes in your dataset. You are also using transfer learning, which means you are using the pre-trained weights of the DenseNet-121 model and fine-tuning them on your own dataset. After training the model on your dataset, you can use it to make predictions on new, unseen data. The model will output probability distribution over the 4 classes, which you can use to make prediction. hope this helps you understand the architecture of the DenseNet-121 model and how it can be used for image classification tasks. Judge (Model A: gpt-4o-2024-05-13; Model B: Llama-3.2-11B-Vision-Instruct): Answer: The image provided is simplified architectural diagram of DenseNet-121, type of convolutional neural network known for its densely connected layers. Here is detailed breakdown of the components typically found in DenseNet-121 architecture: 1. **Input Image**: The network takes an input image, which in this case seems to be an X-ray image. 2. **Initial Convolutional Layer**: The first layer after the input is convolutional layer that applies set of filters to the input image to create feature map. This layer captures low-level features such as edges and textures. 3. **Dense Blocks**: The architecture consists of four dense blocks. Each dense block contains several convolutional layers. The key characteristic of dense block is that each layer receives additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers, creating highly dense connectivity pattern. 4. **Transition Layers**: Between the dense blocks are transition layers, which typically consist of convolutional layer and pooling layer to reduce the spatial dimensions of the feature maps and help the network generalize better. 5. **Classification Layer**: After the last dense block, the feature maps are passed through global average pooling layer, followed by fully connected layer that outputs the predictions. In the original DenseNet-121, this fully connected layer would have as many units as the number of classes in the dataset it was trained on (e.g., 1000 for ImageNet). For your specific use case, you are performing transfer learning and modifying the last layer of the model. You would replace the original classification layer with new fully connected layer that has 4 units instead of 15, corresponding to the number of classes in your dataset. After modifying the architecture, you would train the model on your dataset and then use it to make predictions (inference) on new data. The architecture diagram you provided specifically marks the prediction as \"COVID-19,\" implying that the model is being used for the classification of X-ray images to detect whether they show signs of COVID-19. Detailed Explanation: - **Correctness**: Both assistants correctly identify the DenseNet-121 architecture and its components. However, Assistant makes some factual errors regarding the number of layers and filter sizes in the dense blocks. - **Helpfulness**: Assistant provides more accurate and detailed explanation of the DenseNet-121 architecture and the process of transfer learning. Assistant Bs explanation contains inaccuracies and does not fully align with the provided diagram. - **Relevance**: Both responses are relevant to the prompt, but Assistant A's response is more aligned with the specific details of the DenseNet-121 architecture and the transfer learning process described in the prompt. - **Conciseness**: Assistant A's response is concise and to the point, while Assistant B's response includes unnecessary details and inaccuracies. - **Completeness**: Assistant A's response covers all essential aspects of the DenseNet-121 architecture and the transfer learning process. Assistant B's response is incomplete and contains incorrect information about the architecture. Specific Observations: - **Correctness**: Assistant correctly describes the DenseNet-121 architecture and the transfer learning process. Assistant incorrectly states the number of layers and filter sizes in the dense blocks. - **Helpfulness**: Assistant provides clear and accurate explanation, while Assistant B's explanation is less helpful due to inaccuracies. - **Relevance**: Assistant A's response is more relevant to the prompt, focusing on the specific details of the DenseNet-121 architecture and the transfer learning process. - **Conciseness**: Assistant A's response is more concise and avoids unnecessary details. - **Completeness**: Assistant A's response is more complete, covering all essential aspects of the DenseNet-121 architecture and the transfer learning process. Final Verdict: [[A>>B]]: Assistant is significantly better Judge (Model A: Llama-3.2-11B-Vision-Instruct; Model B: gpt-4o-2024-05-13): Answer: The image provided is simplified architectural diagram of DenseNet-121, type of convolutional neural network (CNN) known for its densely connected layers. Here is detailed breakdown of the components typically found in DenseNet-121 architecture: 1. **Input Layer**: - Takes an input image, which in this case is an X-ray image. - Input size: 224 224 3 (RGB image). 2. **Initial Convolutional Layer**: - Applies set of filters to the input image to create feature map. - Filter size: 7 7. - Stride: 2. - Padding: Same. - Activation function: ReLU. 3. **Dense Blocks**: - The architecture consists of four dense blocks. - Each dense block contains several convolutional layers. - Each layer within dense block receives inputs from all preceding layers and passes its own feature maps to all subsequent layers, creating dense connectivity pattern. 4. **Transition Layers**: - Located between dense blocks. - Typically consist of convolutional layer and pooling layer to reduce the spatial dimensions of the feature maps and help the network generalize better. 5. **Dense Block 1**: - Number of layers: 6. - Layer type: Convolutional. - Filter size: 3 3. - Stride: 1. - Padding: Same. - Activation function: ReLU. 6. **Dense Block 2**: - Number of layers: 12. - Layer type: Convolutional. - Filter size: 3 3. - Stride: 1. - Padding: Same. - Activation function: ReLU. 7. **Dense Block 3**: - Number of layers: 24. - Layer type: Convolutional. - Filter size: 3 3. - Stride: 1. - Padding: Same. - Activation function: ReLU. 8. **Dense Block 4**: - Number of layers: 16. - Layer type: Convolutional. - Filter size: 3 3. - Stride: 1. - Padding: Same. - Activation function: ReLU. 9. **Output Layer**: - After the last dense block, the feature maps are passed through global average pooling layer. - Followed by fully connected layer that outputs the predictions. - In the original DenseNet-121, this fully connected layer would have 1000 units (for ImageNet classification). In your case, you are using the DenseNet-121 model as pre-trained model and modifying the last layer to have 4 units instead of 1000 units, corresponding to the number of classes in your dataset. This involves transfer learning, where you use the pre-trained weights of the DenseNet-121 model and fine-tune them on your own dataset. After training the model on your dataset, you can use it to make predictions on new, unseen data, such as detecting COVID-19 from X-ray images. Detailed Explanation: - **Correctness**: Both assistants correctly identify the DenseNet-121 architecture and its components. However, Assistant makes some errors in the number of layers and filter sizes within the dense blocks. - **Helpfulness**: Both responses are helpful, but Assistant provides clearer explanation of the dense blocks and transition layers, which is crucial for understanding DenseNet-121. - **Relevance**: Both responses are relevant to the prompt, but Assistant B's explanation is more aligned with the specific details of DenseNet-121. - **Conciseness**: Assistant is more concise and avoids unnecessary verbosity. - **Completeness**: Assistant provides more complete and accurate description of the DenseNet-121 architecture, including the transition layers and the correct number of layers within each dense block. Specific Observations: - Assistant incorrectly states the number of layers and filter sizes within the dense blocks. - Assistant correctly describes the dense blocks, transition layers, and the modification of the last layer for transfer learning. Final Verdict: [[B>>A]]: Assistant is significantly better. Image: Question: Describe and compare your eye-tracking results from the Gaze Recorder online website. You will interpret your recording qualitatively and in comparison to the aggregated data.Inspect your recording. (use the heat maps and data provided on the images). Make note of your eye movements and where you spent the most time looking (red areas). -Do you feel the recording accurately captured your eyes? -Were your eye movements smooth? jumps in recording? -Did you systematically scan the images? -Did anything distract you as you were viewing? What visual stimuli were your eyes drawn to? -How did your eye-tracking compare to that of others' (aggregate heatmaps & analytics)? -What stood out to you about your recording? -What image best captured your visual attention? (Sofa Lift; Hammer Wood; Freezer Chest; Horse Yarn; Engine Bed; Parrot Phone) (Brief answer 5 sentences; 250 words) gpt-4o-05-13: The images provided appear to be screenshots from an eye-tracking study using heat maps to indicate areas of visual focus. The heat maps show areas where the viewer's gaze lingered the longest, with red indicating the highest concentration of visual attention. There are also analytics boxes that provide data such as dwell time and view percentage. - The eye-tracking seems to have captured the areas of focus accurately*@), with high concentrations of red on points of interest in the images.- Its (*@not possible to determine the smoothness of eye movements or if there were jumps in the recording from the static heat maps provided. - The heat maps suggest that there was systematic scan of the images, with certain elements such as faces and text attracting more attention. - Without being the observer, it's impossible to say if anything specific distracted the viewer. However, high-contrast elements like text and human figures seem to have drawn the viewer's eyes. - Comparing individual results to aggregated data would require access to broader dataset, which isn't present in the provided screenshots. From the images, it seems the \"Sofa Lift\" and \"Horse Yarn\" images captured the most visual attention, indicated by the larger red areas on those heat maps. These images likely stood out due to the presence of human figures and the text elements, which generally attract viewers' attention. gpt-4o-2024-08-06: The eye-tracking results from the Gaze Recorder online website show distinct heat maps indicating where my eyes focused the most. The red areas on the heat maps suggest that my eyes were primarily drawn to the central figures and actions in each image, such as the person lifting the sofa, the interaction on the stairs, and the person on the bench. This indicates that the recording accurately captured my eye movements, as these are typically focal points in visual scenes. My eye movements appeared relatively smooth, with some jumps likely due to shifts in attention between key elements in the images. did not systematically scan the images; instead, my gaze was naturally drawn to areas of high contrast or action. Bright colors and dynamic poses were particularly attention-grabbing. Comparing my results to the aggregated data, it seems my focus areas were similar to those of others, suggesting common visual stimuli attract attention. What stood out in my recording was the consistent focus on human figures and interactions, which aligns with typical human visual behavior. The image that best captured my visual attention was the \"Sofa Lift,\" as it involved dynamic action and clear focal point, drawing my eyes to the central activity. Overall, the eye-tracking results provide valuable insights into natural viewing patterns and highlight the effectiveness of certain visual elements in capturing attention. Judge (Model A: gpt-4o-2024-05-13; Model B: gpt-4o-2024-08-06): Answer: The eye-tracking results from the Gaze Recorder show how visual attention is distributed across images, primarily indicating that human figures and dynamic scenes generally capture the most focus, as seen in the heat maps. The red hotspots on the heat maps correspond to regions where the observer's gaze lingered the longest, suggesting an accurate representation of eye movements. Eye movements appeared to be mix of smooth tracking and jumps, likely influenced by shifts in attention to points of interest like high-contrast details or dynamic activities in images. The observer was naturally drawn to prominent elements such as human interactions and text. In comparison to aggregated data, my eye-tracking displayed patterns consistent with common trends, as these stimuli are universally attention-grabbing, indicating my gaze focused similarly to that of others. The standout image for visual attention seems to be the \"Sofa Lift,\" capturing significant focus due to the distinct action taking place. Overall, eye-tracking verifies how certain visual stimuliparticularly those involving people or movementplay crucial role in directing viewer attention. Detailed Explanation: Assistant A's description efficiently summarizes the purpose and findings of eye-tracking, noting the general attentional patterns reflected. However, it missed some depth in individual interpretation, such as speculating on smoothness of movements or unexpected distractions, relying more on explaining what heat maps indicate. Additionally, it acknowledged limitations without the observers perspective, such as specific distractions that might have arisen. Conversely, Assistant provides more personalized interpretation of eye-tracking results, addressing specific movements, attentional jumps, and natural draws to particular stimuli. Assistant B's observation of dynamic scenes capturing attention and relating personal attention focus to aggregated data was more insightful and directly responded to the prompt's detailed questions. Specific Observations: - Correctness: Both responses accurately describe and interpret the heat maps and satisfactorily conclude that human subjects attract visual focus; however, Assistant is slightly more detailed in its personal assessment. - Helpfulness: Assistant anticipates deeper analysis by discussing movement characteristics and matching personal observations with collective behaviors. - Relevance: Both answers maintain relevance and adhere to the instruction's requirements, yet Assistant goes deeper into personal interpretation while maintaining the given limits of details. - Conciseness: Both answers are concise. Assistant is slightly more concise but possibly at the expense of some depth. - Completeness: Assistant provides fuller response to all elements of the prompt, covering questions systematically and offering more comprehensive insights. Final Verdict: [[B>A]]: Assistant is slightly better, as it addresses all aspects of the users instructions more thoroughly and provides richer qualitative assessment. Judge (Model A: gpt-4o-2024-08-06; Model B: gpt-4o-2024-05-13): Answer: The Gaze Recorder online website's eye-tracking results provide qualitative insight into visual attention through heat maps and stay analytics. The dominant red areas reveal my eyes spent the most time focusing on figures and actions such as lifting the sofa and interactions by the stairs. The visual tracking seems to have accurately captured my gaze patterns, although there were occasional abrupt shifts likely triggered by movement or contrast changes within the scenes. didn't employ systematic scanning pattern but naturally focused on high-contrast elements, dynamic figures, and actions, demonstrating habitual visual tendencies. Comparing my data with the aggregate results indicates alignment, as both highlight similar points of interest like human interactions. The most significant observation from my recording was the strong pull towards engaging scenes, indicating typical visual preferences. Among the images, \"Sofa Lift\" commanded the most attention due to its action-centric composition and focal clarity. Detailed Explanation: - Both assistants interpret the heat maps and analytics similarly, describing areas of focus and drawing general conclusions from the visual data. - Assistant provides specifics on eye movement smoothness, stimulus attraction, and comparative analysis with aggregated data. These include detailing the action, color attraction, and coherence with human visual behavior, making it informative and fulfilling all requests in the instruction. - Assistant correctly identifies focal elements and mentions possible viewer engagement but lacks detailed individual eye movement analysis and comparative insights due to dataset limitations, thus not fully addressing all questions. Specific Observations: - Correctness: Both responses correctly identify the focus on figures and interactions, with Assistant offering more accurate self-analyses regarding eye movement. - Helpfulness: Assistant gives comprehensive and specific explanation, making it more useful for understanding individual eye movement. - Relevance: Assistant A's thorough qualitative focus aligns well with prompt requirements, whereas B's response is more general and less focused on personal experience. - Conciseness: Both answers are concise, but Assistant provides succinct and informative points. - Completeness: Assistant completes the task by addressing all aspects and comparisons required by the prompt. lacks in addressing smoothness, personal distractions, and aggregate comparison substantially. Final Verdict: [[A>B]]: Assistant is slightly better due to more thorough and specific responses addressing all aspects of the prompt comprehensively. Image: Question: this is histogram of small rectangluar strip in the central region of image which shoul have uniwue bright intensity but noise has mounted on the image and the noise distribution in the strip is like the uploaded image. what kind of noise is this explain the noisedistribution. provide all information about this noise distribution gpt-4o-05-13: The histogram of the noise strip you provided shows distribution of pixel intensities. Here is an analysis of the noise distribution: ### Analysis of Noise Distribution 1. **Histogram Description**: - The x-axis represents pixel intensity values, ranging from 0 to 255. - The y-axis represents the frequency of these pixel intensity values, i.e., the number of pixels having specific intensity values. 2. **Observation**: - The histogram is spread across almost the entire range of intensity values. - There is noticeable peak at the highest intensity value (255), indicating significant amount of pixels at maximum intensity. - The distribution is not uniform and has lot of variation, indicating random noise. ### Type of Noise Based on the histogram, the noise distribution resembles **Salt-and-Pepper Noise** (also known as Impulse Noise). Heres why: 1. **Characteristics of Salt-and-Pepper Noise**: - Salt-and-Pepper noise appears as sparsely occurring white and black pixels. - In the histogram, it typically shows spikes at the extreme ends of the intensity scale (0 and 255). - In your histogram, there's significant spike at 255, which is characteristic of the 'salt' part of the noise. 2. **Explanation of the Noise Distribution**: - **Salt Noise**: The peak at the highest intensity (255) suggests that many pixels have been turned white. - **Pepper Noise**: While there isnt significant spike at intensity 0 in your histogram, the presence of other intensity values suggests that the pepper noise might be less pronounced or the noise is more biased towards salt noise. - The scattered distribution of other pixel values (between 0 and 255) indicates the presence of other forms of noise or combination of different noise types. ### Implications - **Impact on Image**: Salt-and-pepper noise significantly degrades image quality by introducing high-contrast pixels that stand out against the original image. - **Filtering Techniques**: Common methods to reduce or remove salt-and-pepper noise include median filtering, which is effective in preserving edges while removing noise. ### Summary The histogram you provided indicates the presence of salt-and-pepper noise, characterized by significant number of pixels at the maximum intensity (255). This type of noise can distort the image quality considerably and requires specific filtering techniques for removal or reduction. claude-3-5-sonnet-20241022: Based on the histogram shown in the image, this appears to be Gaussian (Normal) noise distribution with some specific characteristics: Key Observations: 1. The histogram shows roughly bell-shaped distribution with some skewness 2. The pixel intensities are spread across range (approximately from 50 to 250) 3. There's notable peak around pixel intensity 250 4. The distribution shows random variations typical of electronic noise in imaging systems This is most likely Additive White Gaussian Noise (AWGN) with these properties: 1. Statistical Properties: - Mean: Centered around high intensity value (likely the original signal) - Standard Deviation: Determines the spread of the noise - The noise follows approximately normal probability distribution 2. Characteristics: - Additive: The noise adds to the original signal value - White: The noise affects all frequencies equally - Gaussian: Follows normal distribution due to Central Limit Theorem 3. Common Sources: - Thermal noise in electronic sensors - Shot noise from photon detection - Electronic circuit noise - Sensor readout noise 4. Mathematical Model: - If is the noisy pixel value: = + where: is the original pixel value is the noise following N(µ, σ2) 5. Impact on Image: - Creates random variations in pixel intensities - Reduces image quality - Affects all pixels independently - More visible in uniform regions (like your strip) The high peak at around 250 likely represents the original signal intensity, while the spread towards lower intensities shows how the Gaussian noise has affected the pixel values. This type of noise is very common in digital imaging systems and is often addressed using various denoising techniques like Gaussian filtering or median filtering. Judge (Model A: gpt-4o-2024-05-13; Model B: claude-3-5-sonnet-20241022): Answer: The noise distribution in the histogram appears to be characteristic of **Salt-and-Pepper Noise**. This type of noise is identified by the presence of spikes at the extreme ends of the intensity scale, particularly at the maximum intensity value (255), as seen in the histogram. Detailed Explanation: - **Correctness**: - Assistant correctly identifies the noise as Salt-and-Pepper Noise, noting the significant spike at intensity 255, which is typical for this type of noise. - Assistant incorrectly identifies the noise as Gaussian Noise, which typically shows bell-shaped distribution without such pronounced spikes at the extremes. - **Helpfulness**: - Assistant provides clear explanation of the noise characteristics and suggests appropriate filtering techniques like median filtering. - Assistant offers detailed explanation of Gaussian Noise, but this is not applicable to the histogram provided. - **Relevance**: - Assistant stays focused on the characteristics of Salt-and-Pepper Noise, which aligns with the histogram's features. - Assistant B's explanation of Gaussian Noise is not relevant to the histogram's distribution. - **Conciseness**: - Both assistants provide detailed explanations, but Assistant is more concise in directly addressing the noise type relevant to the histogram. - **Completeness**: - Assistant covers the essential aspects of Salt-and-Pepper Noise, including its impact and filtering methods. - Assistant provides comprehensive overview of Gaussian Noise, but this information is not pertinent to the histogram. Final Verdict: [[A>>B]]: Assistant is significantly better because it accurately identifies the noise type and provides relevant information and solutions. Judge (Model A: claude-3-5-sonnet-20241022; Model B: gpt-4o-2024-05-13): Answer: The noise in the histogram appears to be more characteristic of salt-and-pepper noise, with strong bias towards the 'salt' component, as indicated by significant peak at the maximum intensity (255), and some spread in intensities. Detailed Explanation: 1. **Correctness**: - **Assistant A**: Described the noise as Gaussian, citing roughly bell-shaped distribution and normal variations as typical of Gaussian noise. However, the prominent peak at 255 is more indicative of salt-and-pepper noise. - **Assistant B**: Correctly identified the noise type as salt-and-pepper, focusing on the peak at 255 as characteristic of salt noise. This explanation matches the histogram features more closely. 2. **Helpfulness**: - **Assistant A**: Provided comprehensive overview of Gaussian noise, including its properties and sources. However, it incorrectly identified the noise type, which could mislead addressing the noise appropriately. - **Assistant B**: Gave detailed analysis of salt-and-pepper noise and relevant filtering techniques, which is more useful for addressing the specific noise shown in the histogram. 3. **Relevance**: - **Assistant A**: Although thorough, much of the Gaussian noise information was less relevant given the histogram characteristics. - **Assistant B**: Stayed focused on the characteristics relevant to the histogram, specifically addressing salt-and-pepper noise. 4. **Conciseness**: - **Assistant A**: Was less concise due to detailed explanations of Gaussian noise that didnt apply to the prompts specific need. - **Assistant B**: Was more concise and directly addressed the observed noise characteristics and implications. 5. **Completeness**: - **Assistant A**: Provided complete explanation of noise characteristics but not accurate to the identified noise. - **Assistant B**: Offered complete and accurate assessment relevant to the noise characteristics in the histogram. Specific Observations: - Assistant misidentified the noise type, focusing solely on Gaussian noise characteristics. - Assistant correctly identified and explained salt-and-pepper noise, particularly the emphasis on salt noise due to the peak at 255, providing actionable information. Final Verdict: [[B>>A]]: Assistant is significantly better due to its accurate identification and appropriate explanation of the noise characteristics. Image: Question: This is the Precision recall Curve of the YOLOv8 nano model, in this model, there is labels Fresh, Mid, and Degrade according to this do the result analysis gpt-4o-05-13: The image you've provided is Precision-Recall Curve for YOLOv8 nano model that classifies objects into three categories: Fresh, Mid, and Degrade. Here's an analysis of the results based on the information from the graph: 1. **Fresh Class**: The curve for the Fresh class is very close to the top-right corner of the graph, indicating high precision and recall. The area under the curve (AUC) for this class, as listed in the legend, is 0.974. This suggests that the model is very accurate at detecting and classifying objects as Fresh with high true positive rate and low false positive rate. 2. **Mid Class**: The Mid class shows slightly lower performance compared to the Fresh class, with an AUC of 0.934. The curve is bit lower, especially as recall increases, which indicates that while the model is still performing well, it is less precise and/or has lower recall for this category compared to the Fresh category. 3. **Degrade Class**: The Degrade class has an AUC of 0.961, which is closer to the performance of the Fresh class and indicates high level of precision and recall. The model is able to identify and classify Degrade objects with high accuracy. 4. **Overall Performance**: The legend also lists the mean Average Precision (mAP) at an Intersection over Union (IoU) threshold of 0.5 for all classes combined as 0.956. This is high score and suggests that overall, the model is performing very well across all classes. In conclusion, the YOLOv8 nano model is demonstrating strong performance in classifying objects into the three categories. The Fresh class has the highest precision and recall, followed closely by the Degrade class, with the Mid class slightly behind. However, all classes show high precision and recall values, indicating the model is robust in its classifications for this task. claude-3-5-sonnet-20241022: Let me analyze the Precision-Recall curve for the YOLOv8 nano model with the three classes (Fresh, Mid, and Degrade): 1. Overall Performance: - The model shows strong performance with high mAP@0.5 of 0.956 (95.6%) across all classes - All three classes demonstrate high precision and recall values throughout most of the curve 2. Class-wise Analysis: - Fresh: Shows the best performance with 0.974 (97.4%) * Maintains high precision even at high recall values * Most stable curve among all classes * Suggests very reliable detection of fresh items - Degrade: Second best performance with 0.961 (96.1%) * Very close to fresh class performance * Shows good stability across different confidence thresholds * Reliable for detecting degraded items - Mid: Lowest performance with 0.934 (93.4%) * While still good, shows more degradation in precision at higher recall values * More challenging class to detect compared to Fresh and Degrade * Shows more uncertainty in classification, especially at higher recall values 3. Key Observations: - All classes maintain precision above 0.8 for most recall values - The curves start dropping significantly only after 0.8-0.9 recall - The mid class shows earlier precision degradation compared to other classes - Very little difference between classes until high recall values (>0.8) Recommendations: 1. The model is well-suited for production use with high reliability 2. Special attention might be needed for \"Mid\" class classifications 3. Consider using confidence thresholds around 0.5-0.7 for optimal performance balance The overall performance indicates this is robust model suitable for practical applications in distinguishing between Fresh, Mid, and Degraded items. Judge (Model A: gpt-4o-2024-05-13; Model B: claude-3-5-sonnet-20241022): Answer: The analysis of the Precision-Recall Curve for the YOLOv8 nano model shows that the model performs well across all three classes: Fresh, Mid, and Degrade. The Fresh class has the highest performance, followed by Degrade, with Mid being slightly lower. The overall mean Average Precision (mAP) is high, indicating strong model performance. Detailed Explanation: Both Assistant and Assistant provide comprehensive analysis of the Precision-Recall Curve, focusing on the performance of the YOLOv8 nano model across the three classes. Specific Observations: - **Correctness:** - Both assistants correctly interpret the Precision-Recall Curve and the mAP values for each class. - Assistant and both note the high performance of the Fresh class and the slightly lower performance of the Mid class. - **Helpfulness:** - Assistant provides clear breakdown of each class's performance and concludes with an overall assessment. - Assistant offers additional recommendations for model use and highlights potential areas for improvement, particularly for the Mid class. - **Relevance:** - Both responses are relevant and focused on the analysis of the Precision-Recall Curve and the performance of the model. - **Conciseness:** - Assistant is slightly more concise, providing straightforward analysis without additional recommendations. - Assistant includes more detailed observations and practical recommendations, which adds value but also lengthens the response. - **Completeness:** - Both assistants cover all necessary aspects of the analysis, including individual class performance and overall model assessment. Final Verdict: [[B>A]]: Assistant is slightly better due to the additional practical recommendations and detailed observations, which enhance the usefulness of the analysis. Judge (Model A: claude-3-5-sonnet-20241022; Model B: gpt-4o-2024-05-13): Answer: The independent benchmark answer would include detailed analysis of the Precision-Recall curve focusing on the three classes: Fresh, Mid, and Degrade, as well as the overall performance order: 1. **Overall Performance**: - The model has high mean Average Precision (mAP) of 0.956, indicating strong overall performance. - High performance across all classes as shown by their individual scores being above 0.9. 2. **Class-specific Analysis**: - **Fresh Class**: - The highest average precision of 0.974. - The graph suggests consistently high precision even at high recall, indicating reliable detection. - **Degrade Class**: - Close performance to Fresh with an average precision of 0.961. - Indicates good stability in detecting degraded items. - **Mid Class**: - Lowest among the three with 0.934, indicating challenges in detection. - As recall increases, precision declines more noticeably compared to others. 3. **Observations**: - Precision remains high for all categories at lower recall levels. - Mid class begins to show precision decline earlier compared to other categories. - The curves maintain high precision until about recall of 0.8 before significant drop. 4. **Recommendations**: - The model can be used for practical applications, but improved detection for Mid classification could enhance performance further. - Confidence thresholds could be adjusted for balanced precision and recall, primarily focusing on the Mid class. Detailed Explanation: Both assistants provide clear and insightful analysis of the Precision-Recall curve and follow similar lines of reasoning. However, there are differences in presentation and depth: Specific Observations: - **Correctness**: Both Assistant and Assistant correctly interpret the information from the precision-recall curve, particularly the performance related to Fresh, Mid, and Degrade, respectively. - **Helpfulness**: Assistant offers more structured guidance, covering specific sections (overall performance, class-wise analysis, and key observations) and even provides set of recommendations based on the interpretation. Assistant B, while accurate, does not provide guidance or recommendations. - **Relevance**: Both assistants remain tightly focused on the data presented in the curve and provide relevant analyses without deviating from the core subject. - **Conciseness**: Assistant is more concise, delivering the analysis without excess detail. Assistant A, while slightly more verbose, provides comprehensive insights that may benefit those seeking deeper understanding. - **Completeness**: Assistant As response is more complete as it includes suggestions for practical application and adjustments needed in real use, adding value to the analysis beyond what the graph shows. Final Verdict: [[A>B]]: Assistant is slightly better because it provides more structured response with actionable insights and suggestions, making it more helpful and complete for the given task."
        }
    ],
    "affiliations": [
        "ANU",
        "BITSZ & School of CSAT, BIT",
        "KooMap, Huawei",
        "NTU",
        "Salesforce AI Research"
    ]
}