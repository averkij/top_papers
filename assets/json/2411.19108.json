{
    "paper_title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "authors": [
        "Feng Liu",
        "Shiwei Zhang",
        "Xiaofeng Wang",
        "Yujie Wei",
        "Haonan Qiu",
        "Yuzhong Zhao",
        "Yingya Zhang",
        "Qixiang Ye",
        "Fang Wan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 8 0 1 9 1 . 1 1 4 2 : r Timestep Embedding Tells: Its Time to Cache for Video Diffusion Model Feng Liu1* Shiwei Zhang2 Xiaofeng Wang1,3 Yujie Wei4 Haonan Qiu5 Yuzhong Zhao1 Yingya Zhang2 Qixiang Ye1 Fang Wan1 2Alibaba Group 1University of Chinese Academy of Sciences 3Institute of Automation, Chinese Academy of Sciences 4Fudan University 5Nanyang Technological University Project Page: https://liewfeng.github.io/TeaCache Figure 1. Quality-latency comparison of video diffusion models. Visual quality versus latency curves of the proposed TeaCache approach and PAB [59] using Latte [29]. TeaCache significantly outperforms PAB in both visual quality and efficiency. Latency is evaluated on single A800 GPU for 16-frame video generation under 512 512 resolution."
        },
        {
            "title": "Abstract",
            "content": "As fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces rescaling strategy to refine the estimated differences and utilizes them to indicate *Work was done during internship at Alibaba Group . Corresponding author. output caching. Experiments show that TeaCache achieves up to 4.41 acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality. 1. Introduction Recent years have witnessed the emergence of diffusion models [13, 16, 44, 46], as fundamental backbone for visual generation. The model architecture has evolved from U-Net [4, 33, 35] to diffusion transformers (DiT) [32], which greatly increased model capacities. Empowered by DiT, video generation models [1, 2, 21, 29, 56, 60] have reached groundbreaking level. Despite of the substantial efficacy of these powerful models, their inference speed remains pivotal impediment to wider adoption [24]. This core limitation arises from the sequential denoising procedure inherent to their reverse phase, which inhibits parallel decoding [40]. Moreover, as model parameters scale up and the requirements for higher resolution and longer durations of videos escalate [9, 56], the inference process experiences further decline in speed. To accelerate the visual generation procedure, distillation [30, 37, 51] and post-training [10, 28] are employed. However, these methods typically require extra training, 1 Figure 2. Comparison of the proposed TeaCache and the conventional uniform caching strategy for DiT models during inference. TeaCache is capable of selectively caching informative intermediate model outputs during the inference process, and therefore accelerates the DiT models while maintaining its performance. and respectively denote the model inputs of noisy input and timestep embedding. L1rel and are difference estimation functions of model inputs. δ is an indicator threshold of whether to cache model output or not. which implies substantial computational cost and data resources. An alternative technological pathway is to leverage the caching mechanism [3, 14, 41], which does not require additional training to maintain the performance of diffusion models. These methods [11, 38, 55, 59] find that the model outputs are similar between the consecutive timesteps when denoising and propose to reduce redundancy by caching model outputs in uniform way, Fig. 2(upper). Nevertheless, when the output difference between consecutive timesteps varies, the uniform caching strategy lacks flexibility to maximize the cache utilization. In this study, we aim to develop an novel caching approach by fully utilizing the fluctuating differences among outputs of the diffusion model across timesteps. The primary challenge is: when can we reuse cached output to substitute the current timesteps output? Intuitively, this is possible when the current output is similar with the cached output, Fig. 2(upper). Unfortunately, such difference is not predictable before the current output is computed. Consequently, without the guidance of difference, the uniformly cached outputs becomes redundant and the inference efficiency remains low. To conquer this challenge, we propose Timestep Embedding Aware Cache (TeaCache), training-free caching strategy. TeaCache leverages the following prior: There exists strong correlation between models inputs and outputs. If transformation relationship can be established between the input and output difference, one can utilizes the difference among inputs as an indicator of whether the corresponding outputs need to be cached, Fig. 2(lower). Since inputs are readily accessible, this approach would significantly reduce computation cost. We then delve into the inputs of diffusion models: noisy input, timestep embedding, and text embedding. The text embedding remains constant throughout the denoising process and cannot be used to measure the difference of input across timesteps. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the input information. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To accurately describe the model inputs and ensure their strong correlation with the outputs, TeaCache follows the inference process of diffusion and employ the timestep-embedding modulated noisy input as the final input embeddings, among which the difference are then used to estimated the output difference. It is noteworthy that the input difference estimated above still exhibits scaling bias relative to the output difference, which has been observed through empirical studies. That is because this strategy only captures the correlation trend between input difference and output difference. Considering that both input and output differences are already scalars, TeaCache further introduces simple polynomial fitting procedure to estimate the scaling factors between them. With the correction of the scaling factors, the input difference can accurately reflect the output difference and is ultimately used as an indicator of whether the outputs need to be cached, Fig. 2(lower). The contributions of this paper include: We propose TeaCache, training-free approach which is completely compatible with DiT diffusion models, to estimate the difference of model outputs, selectively cache model outputs and speed up the inference process. We propose simple-yet-effective two-stage strategy to estimate the difference of model output through model input. The proposed strategy uses timestep-embedding 2 modulated noisy input to perform coarse estimation and polynomial fitting procedure for refinement. TeaCache speeds up SOTA generation models, OpenSora [60], Open-Sora-Plan [21], and Latte [29], (PAB [59]) with large margins at negligible quality cost, Fig. 1. DiT [11] adapts this mechanism to DiT by caching residuals between attention layers. PAB [59] caches and broadcasts intermediate features at various timestep intervals based on different attention block characteristics for video synthesis. While these methods have improved Diffusion efficiency, enhancements for DiT in visual synthesis remain limited. 2. Related Work 2.1. Diffusion Model 3. Methodology 3.1. Preliminaries In the realm of generative models, diffusion models [16, 44] have become foundational due to their exceptional abilInitially ity to produce high-quality and diverse outputs. developed with the U-Net architecture, these models have demonstrated impressive performance in image and video generation [6, 7, 17, 3335, 50, 52, 53]. However, the scalability of U-Net-based diffusion models is inherently constrained, posing challenges for applications requiring larger model capacities for enhanced performance. To address this limitation, Diffusion transformers (DiT) [32] represent significant advancement. By utilizing the scalable architecture of transformers [48], DiT provides an effective means to increase model capacity. notable achievement in this field is the advancement in generating long videos through the large-scale training of Sora [31], which employs transformer-based Diffusion architecture for comprehensive simulations of the physical world. This underscores the considerable impact of scaling transformerbased Diffusion models. An increasing number of studies have adopted the Diffusion transformer as the noise estimation network [8, 9, 21, 29, 56, 60]. 2.2. Diffusion Model Acceleration Despite the notable performance of Diffusion models in image and video synthesis, their significant inference costs hinder practical applications. Efforts to accelerate Diffusion model inference fall into two primary categories. First, techniques such as DDIM [45] allow for fewer sampling steps without sacrificing quality. Additional research has focused on efficient ODE or SDE solvers [19, 20, 26, 27, 46], using pseudo numerical methods for faster sampling. Second, approaches include distillation [36, 51], quantization [15, 25, 39, 43], and distributed inference [22] are employed to reduce the workload and inference time. However, these methods often demand additional resources for fine-tuning or optimization. Some training-free approaches [5, 49] streamline the sampling process by reducing input tokens, thereby eliminating redundancy in image synthesis. Other methods reuse intermediate features between successive timesteps to avoid redundant computations [42, 54, 58]. DeepCache [55] and Faster Diffusion [23] utilize feature caching to modify the UNet Diffusion, thus enhancing acceleration. FORA [38] and - Denoising Diffusion Models. Diffusion models simulate visual generation through sequence of iterative denoising steps. The core idea is to start with random noise and progressively refine it until it approximates sample from the target distribution. During the forward diffusion process, Gaussian noise is incrementally added over steps to data point x0 sampled from the real distribution q(x): xt = αtxt1 + 1 αtzt for = 1, . . . , (1) where αt [0, 1] governs the noise level, and zt (0, I) represents Gaussian noise. As increases, xt becomes progressively noisier, ultimately resembling normal distribution (0, I) when = . The reverse diffusion process is designed to reconstruct the original data from its noisy counterpart: pθ(xt1 xt) = (xt1; µθ(xt, t), Σθ(xt, t)), (2) where µθ and Σθ are learned parameters defining the mean and covariance. Timestep Embedding in Diffusion Models. The diffusion procedures are usually splitted to one thousand timesteps during training phase and dozens of timesteps during inference phase. Timestep defines the strength of noise to be added or removed in the diffusion procedures, which is an important input of the diffusion model. Specifically, the scalar timestep is firstly transformed to timestep embedding through sinusoidal embedding and multilayer perception module: Tt = LP (sinusoidal(t)) for = 1, . . . , T. (3) Timestep embedding then modulates the input and output of the Self Attention Layer and Feed Forward Network (FFN) in each Transformer block, as shown in Fig.4. Thus, timestep embedding can significantly affect the magnitude of the model output. 3.2. Analysis To investigate the correlation between model output and input, we perform an in-depth analysis of their behaviors during the diffusion process. Model outputs: Ideally, if we could obtain the model outputs in advance, we could directly measure the difference between outputs at adjacent timesteps and decide 3 (a) Open Sora (b) Latte (c) OpenSora-Plan Figure 3. Visualization of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. Timestep embedding and Timestep embedding modulated noisy input have strong correlation with model output. Model inputs: We consider the inputs of diffusion model: text embedding, timestep embedding, and noisy input, as shown in Fig. 4. Since the text embedding remains constant throughout the diffusion process, it cannot be used to measure the difference of inputs across timestep. Therefore, text embedding is excluded from analysis. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the information of the input. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To comprehensively represent the model inputs and ensure their correlation with the outputs, we ultimately utilized the timestep embedding modulated noisy input at the Transformers input stage as the final input embedding, as illustrated in the Fig. 4. Experimental analysis: To derive robust conclusion, we make analysis using the metric defined in Eq. 4 to compute the difference of model inputs and outputs on three distinct video generation models: Open Sora [60], Latte [29], and OpenSora Plan [21]. As illustrated in Fig. 3, the difference of outputs exhibit distinct patterns across various models. In Open Sora, the pattern forms shape, whereas in Latte and OpenSora-Plan, it resembles horizontally flipped L. Additionally, OpenSora-Plan features multiple peaks because its scheduler samples certain timesteps twice. The noisy input across consecutive timesteps changes minimally and shows little correlation with the model output. In contrast, both the timestep embedding and the timestep embedding modulated noisy input demonstrate strong correlation with the model output. Given that the timestep embedding modulated noisy inputs exhibits superior generation capabilities (e.g., in Open Sora) and effectively leverages the dynamics of input, we select it as the indicator to determine whether the model output at the current step is similar to that of the previous timestep. Figure 4. Diffusion module of the visual generation model with transformer. Normalization layer is omitted for simplicity. Timestep embedding modulates the magnitude of block input and output thus has the potential to indicate the variation of output. whether to cache the outputs based on their difference. Following [54], we use the relative L1 distance as our metric. For instance, the relative L1 distance L1rel(O, t) for output embedding Ot at timestep is calculated as follows: L1rel(O, t) = Ot Ot+11 Ot+11 (4) where large L1rel(O, t) indicates that Ot is informative relative to Ot+1 and should be cached; otherwise, small L1rel(O, t) indicates that Ot+1 and Ot are similar to each other and therefore Ot+1 could be reused to replace Ot. Therefore, Eq. 4 can be used to define criterion for determining whether the model outputs should be cached. However, in most cases, the model outputs cannot be obtained in advance, making the above approach infeasible. To address this issue, an intuitive idea is that if we can efficiently estimate the difference of the model outputs, we can leverage it to design caching strategy. Fortunately, it is well-known that the model inputs and outputs are strongly correlated. Based on this insight, we analyzed the model inputs and conducted detailed experiments to investigate their correlation with the model outputs. 4 (a) Open Sora (b) Latte (c) OpenSora-Plan Figure 5. Visualization of corelation of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. The original data points deviate lot from the linear corelation. Polynomial fitting reduces the gap. set the accumulated relative L1 distance to zero. smaller threshold δ results in more frequent refreshing of cached outputs, while larger threshold speeds up visual generation but may adversely affect image appearance. The threshold δ should be chosen to enhance inference speed without compromising visual quality. Rescaled Caching Strategy. Although timestep embedding modulated noisy inputs exhibit strong correlation with model outputs, the differences in consecutive timesteps are inconsistent. Directly using the difference of timestep embedding-modulated noisy input to estimate model output difference leads to scaling bias. Such bias may cause suboptimal timestep selection. Considering that these differences are scalars, we apply simple polynomial fitting to rescale them to reduce the bias. The polynomial fitting is then performed between model inputs (timestep embedding modulated noisy inputs) and outputs, which is formulated as = (x) = a0 + a1x + a2x2 + + anxn, (6) where represents estimated difference of model output and signifies the difference of timestep embeddingmodulated noisy inputs. This can be efficiently solved using the poly1d function from the numpy package. With polynomial fitting, the rescaled difference in timestep embeddingmodulated noisy inputs better estimates model output difference, as shown in Fig. 5. The final caching indicator is formulated as tb1 (cid:88) t=ta (L1rel(F, t)) δ < tb(cid:88) t=ta (L1rel(F, t)) (7) 3.4. Discussion Caching Mechanism v.s. Reducing Timesteps. Assume that both of the caching mechanism and reducing timesteps stregegies reduces half of the timesteps. The differences between them can be concluded in three aspects: Figure 6. Caching Mechanism v.s. Reducing Timesteps. Reducing inference timesteps suffers from deteriorated visual quality while TeaCache maintains visual quality. 3.3. TeaCache As illustrated in Fig. 3, adjacent timesteps conduct redundant computations where model outputs exhibit minimal change. To minimize these redundancies and accelerate inference, we propose the Timestep Embedding Aware Cache (TeaCache). Rather than computing new outputs at each timestep, we reuse cached outputs from previous timesteps. Our caching technique can be applied to nearly all recent diffusion models based on Transformers. Naive Caching Strategy. To determine whether to reuse the cached model output from previous timestep, we employ the accumulated relative L1 distance as an indicator. tb1 (cid:88) t=ta L1rel(F, t) δ < tb(cid:88) t=ta L1rel(F, t) (5) where L1rel is defined in Eq. 4. can be timestep embedding or timestep embedding modulated noisy inputs and δ is the caching threshold. Specifically, after computing the model output at timestep ta and caching it, we accumulate the relative L1 distance (cid:80)tb1 L1rel(F, t) for subsequent t=ta timesteps. If, at timestep tb (> ta), (cid:80)tb1 L1rel(F, t) is less t=ta than the caching threshold δ, we reuse the cached model output; otherwise, we compute the new model output and Table 1. Quantitative evaluation of inference efficiency and visual quality in video generation models. TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths. Method FLOPs (P) Efficiency Speedup Latency (s) VBench LPIPS Latte (16 frames, 512512) Visual Quality SSIM PSNR Latte (T = 50) -DiT [11] T-GATE [58] PAB-slow [59] PAB-fast [59] TeaCache-slow TeaCache-fast Open-Sora 1.2 (T = 30) -DiT [11] T-GATE [58] PAB-slow [59] PAB-fast [59] TeaCache-slow TeaCache-fast OpenSora-Plan (T = 150) -DiT [11] T-GATE [58] PAB-slow [59] PAB-fast [59] TeaCache-slow TeaCache-fast 3.36 3.36 2.99 2.70 2.52 1.86 1.12 3.15 3.09 2.75 2.55 2.50 2.40 1.64 11.75 11.74 2.75 8.69 8.35 3.13 2. 1 1.02 1.13 1.21 1.34 1.86 3.28 26.90 - - 22.16 19.98 14.46 8.20 Open-Sora 1.2 (51 frames, 480P) 1 1.03 1.19 1.33 1.40 1.55 2.25 44.56 - - 33.40 31.85 28.78 19.84 77.40% 52.00% 75.42% 76.32% 73.13% 77.40% 76.69% 79.22% 78.21% 77.61% 77.64% 76.95% 79.28% 78.48% OpenSora-Plan (65 frames, 512512) 1 1.01 1.18 1.36 1.56 4.41 6.83 99.65 - - 73.41 65.38 22.62 14.60 80.39% 77.55% 80.15% 80.30% 71.81% 80.32% 79.72% - 0.8513 0.2612 0.2669 0.3903 0.1901 0. - 0.5692 0.3495 0.1471 0.1743 0.1316 0.2511 - 0.5388 0.3066 0.3059 0.5499 0.2145 0.3155 - 0.1078 0.6927 0.7014 0.6421 0.7786 0.6678 - 0.4811 0.6760 0.8405 0.8220 0.8415 0.7477 - 0.3736 0.6219 0.6550 0.4717 0.7414 0.6589 - 8.65 19.55 19.71 17.16 22.09 18. - 11.91 15.50 24.50 23.58 23.62 19.10 - 13.85 18.32 18.80 15.47 21.02 18.95 (1) Timesteps. Our caching strategy dynamically selects timesteps with large difference for caching and reusing in the following timesteps, whereas the strategy of reducing timesteps is conducted uniformly, lacking awareness of dynamic differences among different timesteps. (2) Model output. In our caching strategy, only the residual signal (i.e., Output minus Input) in the diffusion transformer is cached, therefore the model output at the next timestep is updated. In contrast, reducing timesteps can be considered as keeping the output constant in the next timestep. (3) Parameter αt. Reducing timesteps results in coarser-grained αt, which suffers from deteriorated visual quality. In comparison, TeaCache is able to maintain the visual quality, as illustrated in Fig. 6. 4. Experiment 4.1. Settings Base Models and Compared Methods. To demonstrate the effectiveness of our method, we apply our acceleration technique to various video, such as Open-Sora 1.2 [60], Open-Sora-Plan [21] and Latte [29]. We compare our base models with recent efficient video synthesis techniques, including PAB [59], T-GATE [58] and -DiT [11], to highlight the advantages of our approach. Notably, -DiT and T-GATE are originally designed as an acceleration method for image synthesis; PAB adapted them for video synthesis to facilitate comparison. Evaluation Metrics and Datasets. To assess the performance of video synthesis acceleration methods, we focus on two primary aspects: inference efficiency and visual quality. For evaluating inference efficiency, we use Floating Point Operations (FLOPs) and inference latency as metrics. For visual quality evaluation, we employ VBench [18], LPIPS [57], PSNR, and SSIM. VBench serves as comprehensive benchmark suite for video generative models, aligning well with human perceptions and offering valuable insights from multiple perspectives. LPIPS, PSNR, and SSIM evaluate the similarity between videos produced by the accelerated sampling method and the original model. PSNR assesses pixel-level fidelity, LPIPS measures perceptual consistency, and SSIM evaluates structural similarity. 6 Figure 7. Comparison of visual quality and efficiency (denoted by latency) with the competing method. TeaCache outperforms PAB [59] in both visual quality and efficiency. Latency is evaluated on single A800 GPU. Video generation specifications: Open-Sora [60] (51 frames, 480p), Latte [29] (16 frames, 512512), Open-Sora-Plan [21] (65 frames , 512512). Best-viewed with zoom-in. Generally, higher similarity scores imply better fidelity and visual quality. The details of evaluation metrics are presented in Appendix. Implementation Detail All experiments are carried out on the NVIDIA A800 80GB GPUs with Pytorch. We enable FlashAttention [12] by default for all experiments. To obtain robust polynomial fitting, we sample 70 texts from T2V-CompBench [47] to generate videos, assessing seven desired attributes of generated videos. 10 prompts are sampled for each attributes. δ is 0.1 for TeaCache-slow and 0.2 for TeaCache-fast. speedup of 2.25 as compared to the previous 1.40, and the highest overall quality with speedup of 1.55. Additionally, using Open-Sora-Plan [21], TeaCache achieves the highest speedup of 6.83, surpassing the previously best 1.49 offered by PAB, while also delivering the highest quality at speedup of 4.41. Visualization. Fig. 7 compares the videos generated by TeaCache against those by the original model and PAB. The results demonstrate that TeaCache outperforms PAB in visual quality with lower latency. More visual results can be found in the Appendix. 4.2. Main Results 4.3. Ablation Studies Quantitative Comparison. Tab. 1 presents quantitative evaluation of efficiency and visual quality using the VBench benchmark [18]. We examine two variants of TeaCache: slow variant and fast variant with greater speedup. Compared to other training-free acceleration methods, TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths. In evaluating the Latte [29] baseline, the TeaCache-slow model demonstrates superior performance across all visual quality metrics, achieving 1.86 speedup compared to PAB [59], which provides 1.34 speedup. TeaCache-fast achieves the highest acceleration at 3.28, albeit with slight reduction in visual quality. With the OpenSora [60] baseline, we obtain the optimal Scaling to multiple GPUs. Aligned with previous research employing Dynamic Sequence Parallelism (DSP) [59] for supporting high-resolution long-video generation across multiple GPUs, we assess the performance of TeaCache in these scenarios. The results of this study are presented in Tab. 4. We utilize Open-Sora [60] (480p - 192 frames at 30 timesteps) and Open-Sora-Plan [21] (512512 - 221 frames at 150 timesteps) as baselines and compare them against the prior method PAB [59] regarding latency measurements on A800 GPUs. As the number of GPUs increases, TeaCache consistently improves inference speed across various base models and outperforms PAB. Performance at different Length and Resolution. To assess the effectiveness of our method in accelerating sam7 (a) 480P, 48 frames (b) 480P, 192 frames (c) 360P, 240 frames (d) 720P, 48 frames Figure 8. Inference efficiency and visual quality of TeaCache at different video lengths and resolutions. pling for videos with varying sizes, we perform tests across different video lengths and resolutions. The results, presented in Fig. 8, demonstrate that our method sustains consistent acceleration performance, even with increases in video resolution and frame count. This consistency highlights the methods potential to accelerate sampling processes for longer and higher-resolution videos, meeting practical demands. Quality-Efficiency trade-off. In Fig. 1, we compare the quality-latency trade-off of TeaCache with PAB [59]. Our analysis reveals that TeaCache achieves significantly higher reduction rates, indicated by lower absolute latency, compared to PAB. Additionally, across wide range of latency configurations, TeaCache consistently outperforms PAB on all quality metrics. This is particularly evident in the reference-free metric VBench score [18], which aligns more closely with human preferences. Although there is decline in reference-based scores such as PSNR and SSIM at extreme reduction rates, qualitative results suggest that the outputs remain satisfactory, despite not perfectly matching the reference. Choice of Indicator. When determining the caching schedule, we evaluate various indicators to estimate the differences in model outputs across consecutive timesteps. These indicators include timestep embedding and timestep embedding-modulated noisy input. As illustrated in Fig. 3, the timestep embedding-modulated noisy input demonstrates stronger correlation with model output compared to the timestep embedding, particularly in the OpenSora. Moreover, the selection of timesteps by the timestep embedding-modulated noisy input adapts dynamically to different prompts, whereas the timestep embedding selects the same timesteps for all prompts. This observation is validated by the results presented in Tab. 2, where the timestep embedding-modulated noisy input consistently surpasses the timestep embedding across various models, especially in OpenSora. Effect of Rescaling. Tab.3 illustrates the impact of rescaling. first-order polynomial fitting outperforms the original data by 0.24% under Vbench score metric, as well as in LPIPS, SSIM, and PSNR metrics. Performance gains Table 2. Ablation study of caching indicator. Timestep: timestep embedding. Input: timestep embedding-modulated noisy input. Indicator OpenSora Timestep Input Latte Timestep Input VBench 79.22% 77.01% 78.21% 77.40% 77.05% 77.17% LPIPS - 0.3425 0.2549 - 0.2653 0.2558 SSIM - 0.6934 0.7457 - 0.7073 0.7164 PSNR - 15.86 19.05 - 19.76 20. Table 3. Ablation study of polynomial fitting. Rescaling with polynomial fitting outperforms original data. Higher-order fitting obtains better performance and saturates in 4-order fitting. Order OpenSora Original 1-order 2-order 4-order VBench 79.22% 78.21% 78.45% 78.48% 78.48% LPIPS - 0.2549 0.2517 0.2513 0.2511 SSIM - 0.7457 0.7478 0.7477 0.7477 PSNR - 19.05 19.10 19.09 19. Table 4. Inference efficiency and visual quality when scaling to multiple GPUs with Dynamic Sequence Parallelism (DSP). Method 1 A800 2 A800 4 A800 8 Open-Sora (192 frames, 480P) Baseline PAB TeaCache Baseline PAB TeaCache 188.87(1) 142.23(1.33) 114.01(1.66) 72.86(2.59) 53.74(3.51) 47.03(4.02) 39.26(4.81) 29.19(6.47) 24.64(7.67) 22.18(8.52) 16.88(11.19) 14.41(13.10) Open-Sora-Plan (221 frames, 512512) 324.41(1) 207.70(1.56) 48.22(6.73) 166.94(1.94) 110.06(2.95) 26.99(12.02) 88.18(3.68) 58.07(5.59) 15.91(20.39) 47.79(6.79) 31.92(10.16) 10.13 (32.02) tend to saturate with fourth-order polynomial fitting. 5. Conclusion In this study, we introduce TeaCache, novel, training-free approach designed to significantly accelerate video synthesis inference while maintaining high-quality output. We analyze the correlation between model input and output, observing that similarity of timsetep embedding modulated noisy input in consecutive timesteps shows strong correlation with similarity of model output. We propose to utilize similarity of timsetep embedding modulated noisy input as an indicator of output similarity, allowing for dynamic caching of model outputs. Further, we propose rescaling strategy to refine the estimation of model output similarity, 8 optimizing the selection process for timestep caching. Extensive experiments demonstrate TeaCaches robust performance in terms of both efficiency and visual quality across diverse video generation models and image generation models, sampling schedules, video lengths, and resolutions, underscoring its potential for real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Mochi 1, 2024. https://www.genmo.ai. 1 [2] Vchitect. for https://github.com/Vchitect/Vchitect-2.0. 1 vchitect-2.0: video Parallel diffusion models, scaling up transformer 2024. [3] David Albonesi. Selective cache ways: On-demand cache resource allocation. In MICRO-32. Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture, pages 248259. IEEE, 1999. 2 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [5] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45994603, 2023. 3 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 3 [7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 3 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [9] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 1, 3 [10] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. arXiv preprint arXiv:2406.17343, 2024. 1 [11] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. Delta dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. 2, 3, 6 [12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact at9 tention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 [14] James Goodman. Using cache memory to reduce processor-memory traffic. In Proceedings of the 10th annual international symposium on Computer architecture, pages 124131, 1983. 2 [15] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 3 [17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [18] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 7, 8 [19] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. 3 [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [21] PKU-Yuan Lab and Tuzhan AI etc. Open-sora: Democratizing efficient video production for all, 2024. https://doi.org/10. 5281/zenodo.10948109. 1, 3, 4, 6, 7 [22] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183 7193, 2024. 3 [23] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. arXiv e-prints, pages arXiv2312, 2023. 3 [24] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. [25] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in Neural Information Processing Systems, 36, 2024. 3 [26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 3 [27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 3 [28] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024. 1 [29] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 1, 3, 4, 6, 7 [30] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 1 [31] OpenAI. Sora, 2024. https://openai.com/index/sora/. 3 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1, 3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1, 3 [36] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [37] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. [38] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 2, 3 [39] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 3 [40] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 1 [41] Alan Jay Smith. Cache memories. ACM Computing Surveys (CSUR), 14(3):473530, 1982. 2 [42] Junhyuk So, Jungwon Lee, and Eunhyeok Park. Frdiff: Feature reuse for universal training-free acceleration of diffusion models. arXiv preprint arXiv:2312.03517, 2023. [43] Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3 [44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 1, 3 [45] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [46] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1, [47] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 7 [48] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [49] Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj Jha, and Yuchen Liu. Attention-driven training-free efficiency enhancement of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1608016089, 2024. 3 [50] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [51] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023. 1, 3 [52] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. [53] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 3 [54] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceed10 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 3, 4 [55] Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xuanzhe Liu. Deepcache: Principled cache for moIn Proceedings of the 24th annual interbile deep vision. national conference on mobile computing and networking, pages 129144, 2018. 2, 3 [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, [57] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [58] Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jurgen Schmidhuber. Crossattention makes inference cumbersome in text-to-image diffusion models. arXiv preprint arXiv:2404.02747, 2024. 3, 6 [59] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. 1, 2, 3, 6, 7, 8 [60] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. https://github.com/hpcaitech/Open-Sora. 1, 3, 4, 6,"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Fudan University",
        "Institute of Automation, Chinese Academy of Sciences",
        "Nanyang Technological University",
        "University of Chinese Academy of Sciences"
    ]
}