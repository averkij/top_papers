{
    "paper_title": "Towards Automated Kernel Generation in the Era of LLMs",
    "authors": [
        "Yang Yu",
        "Peiyu Zang",
        "Chi Hsu Tsai",
        "Haiming Wu",
        "Yixin Shen",
        "Jialing Zhang",
        "Haoyu Wang",
        "Zhiyou Xiao",
        "Jingze Shi",
        "Yuyu Luo",
        "Wentao Zhang",
        "Chunlei Men",
        "Guang Liu",
        "Yonghua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 7 2 7 5 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Towards Automated Kernel Generation in the Era of LLMs",
            "content": "Yang Yu1 , Peiyu Zang1,2 , Chi Hsu Tsai1,3 , Haiming Wu1,4 , Yixin Shen1,5 , Jialing Zhang1,6 , Haoyu Wang1,7 , Zhiyou Xiao1,3 , Jingze Shi8 , Yuyu Luo8 , Wentao Zhang3 , Chunlei Men1 , Guang Liu1 and Yonghua Lin1 1Beijing Academy of Artificial Intelligence 2Beijing Normal University 3Peking University 4Beijing Institute of Technology 5Cornell University 6Beijing Jiaotong University 7Renmin University of China 8Hong Kong University of Science and Technology (Guangzhou)"
        },
        {
            "title": "Abstract",
            "content": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are wellsuited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedbackdriven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagosai/awesome-LLM-driven-kernel-generation."
        },
        {
            "title": "1 Introduction\nThe rapid scaling of large language models (LLMs) has\nplaced efficient hardware utilization at the core of modern AI",
            "content": "systems [Kaplan et al., 2020]. To meet these demands, specialized accelerators such as GPUs and NPUs have become the backbone of large-scale training and inference [Choquette et al., 2021; Liao et al., 2021]. At the core of these platforms are kernels that implement fundamental operations, including matrix multiplication and attention, which dominate execution time in LLM workloads. As result, the end-to-end performance, efficiency, and cost of LLM systems are largely determined by kernel efficiency rather than hardware peak capability. Despite their foundational role, the development of efficient kernels remains formidable engineering challenge. Achieving near-peak hardware utilization requires deep expertise in both algorithmic design and hardware-specific intricacies. Furthermore, kernel optimization is inherently nonscalable: implementations are often tightly coupled to particular hardware architectures and workload characteristics, which hinders their reuse and generalization across different GPU generations or hardware vendors [Wu, 2023]. In response to these challenges, LLMs and LLM-based agents offer transformative paradigm for kernel generation. By training on vast repositories of code and documentation, LLMs effectively compress expert-level world knowledge regarding hardware specifications, enabling them to bridge the semantic gap between high-level algorithms and lowlevel implementation details. Beyond static code generation, LLM-based agents excel in navigating the irregular optimization landscape through iterative refinement. This closed-loop approach not only drastically reduces the engineering but also generalizes across workloads and hardware configurations, toward future of scalable, automated kernel discovery. As result, LLMs and LLM-based agents are emerging as compelling foundations for the next generation of kernel generation and optimization frameworks. Although the integration of LLMs and LLM-based agents into kernel generation marks rapidly advancing frontier in AI systems research, the absence of systematic survey has resulted in fragmented research landscape. This survey adFigure 1: Illustration of the growth trend in the field of LLM-driven kernel generation. We organize these research works chronologically and categorically based on their publication dates and the domains they belong to. dresses this gap by presenting unified overview of the field, clarifying foundational concepts, and highlighting emergent methodologies and trends. key contribution is our consolidated resource infrastructure, featuring structured compilation of training-ready kernel datasets and literature collection tailored for retrieval-augmented generation (RAG), designed to facilitate data-driven research in this specialized kernel-generation domain. Moving beyond synthesis of existing methodologies, we also spotlight critical open challenges and propose promising research directions, aiming to establish foundational reference for the next generation of innovation in LLM-driven kernel generation."
        },
        {
            "title": "2 Background\nLLM and LLM-based Autonomous Agents. The foun-\ndation of modern LLMs is the Transformer architec-\nture [Vaswani et al., 2017], which functions as the probabilis-\ntic predictor trained via the next token prediction objective.\nGiven a sequence of tokens x = (x1, . . . , xT ), the model\nmaximizes the joint probability:",
            "content": "P (x) = (cid:89) t=1 (xt x1, . . . , xt1; θ). This objective enables the model to internalize world knowledge and reasoning patterns implicitly during pretraining. While LLMs serve as the cognitive engine for reasoning and decision-making, autonomous agents extend this capability by integrating additional system components such as planning, memory, and tool-use mechanisms [Wang et al., 2024]. These components enable agents to decompose complex tasks, retain and retrieve long-term context, and interact with external environments. In this framework, the LLM functions as the brain, orchestrating actions through reasoning strategies. And agents utilize tools such as compilers or interpreters to perform actions beyond the models internal knowledge. Kernel Programming and Code Generation. kernel is the fundamental unit of GPU execution that concretizes highlevel algorithmic semantics into hardware-level parallel operations. Since CUDA made kernels explicitly programmable, GPUs have evolved into general-purpose computing platforms, supported by highly optimized libraries such as CUTLASS [Thakkar et al., 2017]. Nevertheless, writing highperformance custom kernels remains challenging, as it requires expert knowledge of hardware-specific optimization strategies. While higher-level abstractionssuch as Triton and tile-based compiler frameworkssignificantly improve programmability, achieving competitive performance still demands substantial domain expertise and often fails to transfer across heterogeneous accelerator platforms, underscoring the persistent gap between programmability and performance portability. In parallel, large language models have significantly advanced code generation, evolving from simple completion to managing complex software engineering workflows. However, kernel generation fundamentally differs from generalpurpose code synthesis. While conventional code generation focuses on functional correctness, kernel generation must satisfy strict efficiency constraints and adapt to hardware execution characteristics. As result, kernel generation aligns more closely with performance-oriented program synthesis and compiler optimization than with standard software development, necessitating specialized generation methods beyond generic LLM-based code generation."
        },
        {
            "title": "4 LLM Agent for Kernels Generation\nRelying on foundational LLMs alone typically reduces kernel\ndevelopment to a static, one-pass inference process. In con-",
            "content": "trast, LLM-based agents introduce autonomy and feedback into the optimization loop by enabling planning, tool use, and evaluation of intermediate results. This closed-loop, selfimproving paradigm allows agent-based approaches to scale kernel optimization across diverse workloads and hardware platforms, while sustaining long-horizon, fatigue-free exploration. Concretely, we categorized recent agent-driven advancements into four structural dimensions: learning mechanisms, external memory management, hardware profiling integration, and multi-agent orchestration."
        },
        {
            "title": "4.1 Learning Mechanisms\nThe first dimension of advancement concerns search strate-\ngies.\nInitial approaches view kernel generation as iterative\nrefinement. Caesar (in [Ouyang et al., 2025]) utilizes sim-\nple feedback loops to refine kernels, while Inference-Time\nScaling [Chen et al., 2025b] demonstrates that scaling test-\ntime compute and reflection significantly boost kernel quality.\nTo manage complexity, PEAK [Tariq et al., 2025] employs a\nstepwise, modular iterative refinement strategy, and “Mini-\nmal Executable Programs” is proposed [Chu et al., 2025] to\nenable efficient, isolated iteration without building costly full-\nscale applications. DiffAgent [Zhu et al., 2026] adopts itera-\ntive refinement to accelerate diffusion models, TritonX [Ham-\nmond et al., 2025] uses iterative refinement within a state ma-\nchine to cover kernels of complete PyTorch ATen backends,\nand KernelGen [BAAI, 2025] leverages test-time scaling and\nreflection techniques to enable kernel generation for multi-\nchip backends. MaxCode [Ou et al., 2026] further unifies\nexisting iterative search methods under a max-reward rein-\nforcement learning framework, combined with a natural lan-\nguage critique model converting raw execution feedback into\ndiagnostic insights.",
            "content": "recent To escape local optima, frameworks adopt population-based evolution. Lange et al. [Lange et al., 2025b] optimize translation CUDA via mutation and crossover. FM Agent [Li et al., 2025a] includes an evolutionary stage with the principles of diversity preservation, adaptive evolution, and multi-population dynamics. Advanced population dynamics are also introduced in EvoEngineer [Guo et al., 2025], which decouples traversal techniques from population man- [Andrews and Witteveen, agement. GPU Kernel Scientist 2025] employs multi-stage evolutionary workflow to address the challenge of optimizing HIP kernels for the AMD accelerators. And cuPilot [Chen et al., 2025a] guides evolution through high-level semantic strategies."
        },
        {
            "title": "4.2 External Memory Management\nComplex kernel optimization often requires domain-specific\nknowledge, such as CUDA APIs and hardware instruction\nsets that may be hallucinated or forgotten by the LLM. Agents\nin this category augment generation with external memory.\nThe AI CUDA Engineer [Lange et al., 2025a] leverages a\nvector database of high-quality kernel examples to ground\nthe LLM’s generation, ensuring syntactic correctness and ad-\nherence to best practices in low-level programming. Ker-\nnelEvolve [Liao et al., 2025] further advances the external\nknowledge management paradigm by integrating a sophisti-\ncated hardware-specific knowledge base specifically tailored",
            "content": "for heterogeneous AI accelerators. Beyond retrieving unstructured textual context, recent work has explored utilizing structured representations as external memory to guide model inference. Work such as ReGraphT [Gong et al., 2025] proposes novel framework that treats reasoning graph as domain-specific external memory for CUDA code optimizaIn this approach, the logical transitions between option. timization states of large language models are externalized into static, navigable graph structure for the small language model to retrieve."
        },
        {
            "title": "4.3 Hardware Profiling Integration\nThe third dimension addresses the hardware-agnostic nature\nof standard LLMs by configuring the agent’s persona profile\nwith hardware specifications, and iteratively reasoning over\nperformance profiling feedback.",
            "content": "QiMent-TensorOp [Zhang et al., 2025a] triggers LLMs to analyze and distill low-level hardware documentation according to user input into the generation prompt, while QiMeng-GEMM [Zhou et al., 2025b] generates General Matrix Multiplication (GEMM) with the meta-prompt, which offers universal templates for various general optimization techniques and platform-specific optimization details. QiMengAttention [Zhou et al., 2025a] considers target GPU architecture and instruction set to convert the high-level thinking language into low-level CUDA code, and implements the highperformance FlashAttention on different GPUs. SwizzlePerf [Lei et al., 2025] explicitly tackles the swizzling problem, which explicitly injects precise architectural specifications into the prompt context and restricts the search space specificaslly to swizzling patterns that focuses solely on maximizing the L2 cache hit rate. Complementing this, agents leverage dynamic feedback. CUDA-LLM [Chen et al., 2025c] incorporates detailed target GPU specifications (e.g., warp size, cache size) into the agents prompt. Simultaneously, compilation logs and runtime performance metrics are also aggregated to guide the optimization process. TritonForge [Li et al., 2025b] utilizes profiling-guided feedback loops to iteratively analyze and identify performance bottlenecks. PRAGMA [Lei et al., 2025] uses specialized profiling module to parse low-level quantitative metrics into the interpretable natural language suggestion. KERNELBAND [Ran et al., 2025] clusters runtime behavior of potential kernels to reduce the exploration space and utilizes profiling data as context to guide the optimization strategies selection."
        },
        {
            "title": "4.4 Multi-Agent Orchestration\nRecognizing that kernel development inherently involves het-\nerogeneous skills ranging from algorithmic planning to low-\nlevel coding and debugging, recent works increasingly adopt\nmulti-agent designs that explicitly decompose these respon-\nsibilities into coordinated roles.",
            "content": "STARK [Dong et al., 2025] structures generation into Plan-Code-Debug phases to emulate human workflows, while AKG [Du et al., 2025] leverages similar modularity to achieve cross-platform synthesis. Astra [Wei et al., 2025] specializes this approach for production-grade SGLang kernels, focusing on tuning-focused agents. CudaForge [Zhang et al., 2025b] employs Coder-Judge loop driven by hardware-level feedback, whereas KForge [Sereda et al., 2025] adapts this dual-agent model to new platforms using only single-shot example supervision. Addressing scale, KernelFalcon [Team and Contributors, 2024] employs multiagent system to tackle the challenge of GPU kernel generation of full machine learning architectures, where the system specifically addresses hierarchical task decomposition and delegation through coordinated manager and worker agents. Conversely, GEAK [Wang et al., 2025] targets AMD GPUs, integrating generation and reflection within Triton-based workflow."
        },
        {
            "title": "5 Datasets for LLM-Based Kernel Generation\nThe efficacy of Large Language Models (LLMs) in high-\nperformance kernel generation relies critically on the avail-\nability of domain-specific data. Unlike general software en-\ngineering, kernel generation requires models to internalize\nhardware intrinsics, parallel execution semantics, and mem-\nory hierarchy constraints. In this section, we survey the data\n(1)\nlandscape and organize resources into two categories:\nTraining Corpora, covering both structured datasets and raw\nkernel repositories; (2) Knowledge Bases, which we identify\nas essential for grounding RAG systems.",
            "content": "The training data consists of targeted, structure-aware curation and unstructured repositories. Structured datasets represent the highest-value signal for instruction tuning, as they explicitly pair intent with optimization. Open-source repositories contain the vast majority of domain knowledge, where optimized kernel code can be extracted and cleaned from open-source operator and kernel libraries, integrated framework or system, and domain-specific languages tutorials and reference implementations. Beyond executable code, domain knowledge base also plays critical role in LLM-driven kernel generation. Such knowledge can be distilled into pretraining corpora to enrich model understanding, or integrated as external knowledge bases to support agent-based systems, where the corpora is always provided in authoritative documentation and guides, as well as in community indices or tutorials. comprehensive index is provided in Table 1. Note that the dates listed in the table correspond to the initial release, where these libraries are under active development."
        },
        {
            "title": "6.1 Metrics\nSeveral factors should be considered when evaluating the per-\nformance of the operator implementation: correctness, effi-\nciency, and so on. To build a comprehensive evaluation, ex-\nisting benchmarks generally adopt execution-based unit tests,\nwhere the generated kernels will be compared with the stan-\ndard implementations of CUDA/PyTorch. Given the instabil-\nity of operator generation, each testing task usually involves",
            "content": "Data Resource Description I. Structured Datasets (Hugging Face & Benchmarks) 02/2024 The Stack v2 [Lozhkov et al., 2024] 06/2024 HPC-Instruct [HPC-AI Tech, 2024] 05/2025 KernelBook [Paliskara and Saroufim, 2025] Torch-Triton Aligned Corpus 02/2025 KernelBench samples Unsupervised CUDA/Triton Corpus Instructions for CUDA/MPI/OpenMP"
        },
        {
            "title": "Kernel Code Snapshots and Profiling Data",
            "content": "xFormers [Lefaudeux et al., 2022] II. Code-Centric Corpora (GitHub Repositories) Layer 1: High-Performance Operator Libraries 12/2017 CUTLASS 05/2022 FlashAttention [Dao et al., 2022] 11/2023 FlagAttention [FlagOpen Team, 2023] 02/2024 AoTriton [AMD, 2024] 11/2021 08/2024 Liger-Kernel [LinkedIn, 2024] 04/2024 FlagGems [FlagOpen Team, 2024] 09/2022 Bitsandbytes [Dettmers et al., 2022] 09/2024 Gemlite [Dropbox, Inc., ] 01/2025 FlashInfer [Ye et al., 2025] 05/2021 FBGEMM [Khudia et al., 2021] 09/2022 Transformer Engine [NVIDIA, 2022] Layer 2: Framework & System Integration 10/2016 PyTorch (ATen) vLLM 06/2023 SGLang 12/2023 llama.cpp 03/2023 08/2023 TensorRT-LLM 10/2019 DeepSpeed Layer 3: Domain-Specific Languages 07/2019 Triton 04/2024 TileLang 12/2025 cuTile III. Knowledge Bases & Educational Resources Documentation & Guides 06/2007 CUDA Guide 06/2007 PTX ISA 05/2020 Tuning Guides Community Indices & Tutorials 01/2024 GPU-MODE 01/2024 Triton Index 06/2016 Awesome-CUDA 12/2023 Awesome-GPU 05/2023 LeetCUDA 01/2023 Triton-Puzzles 01/2011 Colfax Research 09/2018 Nsight Compute CUDA C++ Template Library for Matrix Ops Fast and Memory-Efficient Exact Attention Memory Efficient Attention Operators in Triton AOT-compiled Triton kernels for AMD ROCm Hackable and Optimized Transformer Blocks Efficient Triton Kernels for LLM Training Triton-based Operator Library for LLMs K-bit Quantization Kernels for LLMs Low-Bit Matrix Multiplication Triton Kernels Kernel Library for Efficient LLM Serving Low-Precision Matrix Multiplication Acceleration Library for Transformer Models Foundational Tensor Library for C++ and Python High-Efficient Serving Engine Structured Generation Language for LLMs LLM Inference in C/C++ TensorRT Toolbox for LLM Inference System for Large Scale Model Training Open-Source GPU Programming Language Tile-based Optimization Language NVIDIAs DSL for Tile-centric Programming CUDA C++ Programming Guide PTX ISA Reference NVIDIA Architecture Tuning Guides Resource Stream & KernelBook Community Index for Triton Optimization Community Curated List for CUDA Awesome GPU Engineering List CUDA Programming Exercises Puzzles for Learning Triton Technical Hub Dedicated to HPC and AI Kernel Profiling Guide Access [Data] [Data] [Data] [Data] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Code] [Link] [Docs] [Docs] [Docs] [List] [List] [List] [List] [Code] [Code] [Link] [Docs] Table 1: structured overview of training corpora and kernel knowledge bases. Note that the dates in the table correspond to the initial release; the libraries themselves continue to undergo active development. multiple evaluations across random samples among times of generation. rect implementation is generated in trials. The standard estimator is defined as: Correctness primarily includes two aspects based on difficulty: (1) successful compilation and (2) consistency with the reference in multiple input-output comparisons. Among various metrics used in code generation, pass@k is widely chosen, which calculates the probability that at least one corpass@k (cid:20) 1 (cid:18)n (cid:19) (cid:18)n / (cid:19)(cid:21) , (1) where the expectation is taken over kernel tasks and prompts, is the number of correct kernel implementations. Name ParEval Time Metrics Hardware Description 01/2024 E S"
        },
        {
            "title": "KernelBench",
            "content": "02/2025 f"
        },
        {
            "title": "TritonBench",
            "content": "02/2025 E * MultiKernel-Bench 07/2025 H TritonBench-revised & ROCm Benchmark 07/2025 Robust-kbench 09/2025 BackendBench 09/2025 CUDAEval 10/2025 FlashInfer-Bench 01/2026 N 420 expert-selected tasks across 12 algorithmic domains for benchmarking general parallel code generation. 250 PyTorch-to-CUDA kernel generation tasks, curated from popular GitHub repositories and official PyTorch operators, for evaluating AI/DL kernel generation. TritonBench evaluates Triton kernel generation via two subsets: 184 high-level kernels sourced from popular GitHub projects (TritonBench-G) and 166 fusion tasks derived from diverse PyTorch operators with different frequencies of usage (TritonBench-T). 285-task benchmark across 14 operator categories for multiplatform DL kernel synthesis. An AMD GPU-centric evaluation dataset comprising 30 expert-verified ROCm kernels and an adapted version of TritonBench-G, specifically optimized for AMD GPU performance benchmarking. robustness-focused benchmark featuring 9 specialized deep learning task categories, derived by refining and extending KernelBench. rigorous evaluation framework that enforces PyTorchs official core library standards on 271 operators. Current use cases primarily leverage NVIDIA CUDA and Triton, yet the architecture remains backend-agnostic. Leveraging 313 curated tasks from the the Stack v2 to benchmark the efficacy of reasoning transfer in CUDA code optimization. Provide unified schema describing kernel definitions, workloads, implementations, and evaluations, including eight representative kernel types used in LLM inference. * Efficiency here is defined as the ratio of the operators measured throughput to the theoretical maximum performance. Table 2: Benchmark datasets for kernel generation and optimization. Metrics: Correctness, Speedup, Efficiency, Perf, Similarity. Hardware Platforms: NVIDIA GPUs, HUAWEI NPUs, Google TPUs, AMD GPUs. astp, Efficiency is another principal goal that kernel evaluation focuses on. Speedup@k measures how much faster generated implementation is compared with the baselines by calculating speedup@k (cid:88) j=1 (cid:19) (cid:18)(cid:18)j 1 1 base (cid:19) (cid:19) (cid:18)(cid:18)n / (cid:19) Tj , (2) where Tj is the running time of the j-th generated implementation while base is the time consumed by the baseline. Note that the implementations are sorted by their performance, i.e., T1 corresponds to the slowest and Tn to the fastest. In addition, Efficiency@k refers to how effectively the generated operators utilize computation resources during execution, and Compatibility is considered when evaluating operator generation techniques across different hardware platforms or languages. Combined metrics are also used to evaluate multiple aspects of performance. For example, Perf@K measures how close the best result from generated kernels is to human expert performance. The astp jointly evaluates the functional correctness and runtime performance of generated kernels. Similarity uses 4 items (n-gram, weighted n-gram, syntax and dataflow) to measure the similarity between the generated code and the reference code."
        },
        {
            "title": "6.2 Benchmark Datesets",
            "content": "As summarized in Table 2, kernel benchmarks are evolving from simple, single-platform evaluations toward comprehensive, real-world and generalized operator evaluation. We observe the following three key trends. Metrics. Moving beyond basic correctness and raw speedup (ParEval [Nichols et al., 2024], KernelBench), recent suites adopt composite objectives. Examples include efficiency metrics in TritonBench [Li et al., 2025c] and robustness assessments in Robust-kbench. Hardware. Evaluation is expanding beyond NVIDIA exclusivity. Compared to early benchmarks such as ParEval and KernelBench exclusively targeting NVIDIA GPUs, MultiKernelBench [Wen et al., 2025] integrates HUAWEI NPUs and Google TPUs, while TritonBench-revised targets AMD GPUs. Additionally, NPUEval [Kalade and Schelle, 2025] specifically targets power-sensitive kernels of neural processing units. Content. Workloads are shifting from generic algorithms to production-grade traces. KernelBench and TritonBench emphasize real-world PyTorch-to-CUDA or Triton kernel generation curated from popular GitHub repositories and The Stack v2. FlashInfer-Bench [Xing et al., 2026] standardizes 1,600 real-world LLM serving workloads, and BackendBench [Saroufim et al., 2025] targets complex edge cases."
        },
        {
            "title": "7 Challenges and Opportunities\nWhile the integration of LLMs and agents has shown strong\npotential for automating kernel generation, the field remains\nat an early stage of development. Bridging the gap between\npromising prototypes and production-grade systems requires\naddressing a set of interrelated challenges. This section ex-\namines these challenges and highlights emerging research di-\nrections spanning data, agents, infrastructure, evaluation, and\nhuman–AI collaboration, which are likely to shape the next\ngeneration of AI-driven kernel generation and optimization\nsystems.",
            "content": "Data Scarcity and Synthetic Scaling. Progress toward production-grade performance remains fundamentally constrained by data scarcity. High-performance kernels exhibit pronounced long-tail distribution and are sparsely represented in existing code corpora, where most available datasets still lack deep, hardware-aware domain knowledge, and existing corpora predominantly capture only final optimized kernels but omit optimization trajectories. Promising directions to access these limitations include systematic kernel dataset construction, large-scale synthetic data generation, and the collection of execution-driven optimization processes. Such data can support wide range of learning paradigms, including pretraining, supervised fine-tuning, and reinforcement learning, and may be crucial for enabling meaningful scaling behavior in kernel generation systems. Agentic Reasoning and Engineering Standards . Current agent-based kernel optimization relies on predefined, workflow-driven paradigms, often failing long-horizon tasks due to redundant exploration and context exhaustion. To transcend these limitations, we propose three critical advancements: (1) enhancing autonomy by shifting from handcrafted workflows to self-directed planning and dynamic memory; (2) enabling principled reasoning by integrating dispersed heuristics across documentation and experts into structured knowledge bases; and (3) ensuring reliability through rigorous engineering standards, including formal verification and strict specifications. Collectively, addressing these challenges is critical for transforming agentic kernel optimization from exploratory automation into robust, engineering-grade capability. Scalable Infrastructure for Synthesis and Training. Scalable infrastructure remains bottleneck due to the severe latency mismatch between rapid model inference and costly kernel compilation. This disparity hinders the highthroughput feedback loops essential for reinforcement learning and synthetic data generation. Addressing this challenge calls for infrastructure that cleanly decouples model reasoning from environment execution via standardized, distributed gym-like environments, while supporting distributed and asynchronous execution at scale. Ultimately, advances in scalable infrastructure are critical for transforming kernel synthesis and data sampling from low-throughput experimentation into systematic, data-driven learning process. Evaluation Robustness and Generalization. key open challenge in AI-driven kernel generation is the lack of robust and comprehensive evaluation. critical deficit in AIdriven kernel generation is the lack of robust evaluation. Existing benchmarks are often confined to fixed input shapes and forward-pass primitives within the NVIDIA ecosystem, failing to reflect the diversity of real-world workloads. Addressing these gaps requires evaluation protocols that jointly assess robustness and generalization across shapes, operators, and ecosystems, providing more reliable foundation for measuring progress in kernel generation research. Human-AI Collaboration for Kernel Generation. Beyond fully automated approaches, humanAI collaboration represents an important and complementary paradigm for kernel generation. An open research question is how to systematically combine agentic exploration with human expertise to expand the design space and improve controllability in performance-critical settings. To operationalize this, we identify two critical requirements: (1) Explainability, where agents provide interpretable rationales for optimization decisions (e.g., tiling) to facilitate expert verification; and (2) Mixed-initiative interaction, paradigm where humans specify high-level constraints while agents execute implementation and tuning. Establishing this principled division of labor is essential to balance controllability with the scalability of automation."
        },
        {
            "title": "8 Conclusion\nThis survey highlights the transformative potential of large\nlanguage models and agentic workflows for automating high-\nperformance kernel generation, synthesizing recent advances\nin supervised fine-tuning, reinforcement learning, and multi-\nagent orchestration, together with progress in kernel-centric\ndataset and benchmark development. Looking ahead, fu-\nture work should move beyond rigid workflows toward self-\nevolving agentic reasoning with strong hardware generaliza-\ntion. Such a shift is essential not only to alleviate the burden\nof manual kernel engineering, but also to unlock substantial\nproductivity gains in the face of rapidly scaling AI infrastruc-\nture",
            "content": "References [AMD, 2024] AMD. Aotriton: Pre-compiled triton kernels for rocm. https://github.com/ROCm/aotriton, 2024. [Andrews and Witteveen, 2025] Martin Andrews and Sam Witteveen. Gpu kernel scientist: An llm-driven framearXiv preprint work for iterative kernel optimization. arXiv:2506.20807, 2025. [BAAI, 2025] BAAI. KernelGen. https://github.com/ flagos-ai/KernelGen, 2025. [Baronio et al., 2025] Carlo Baronio, Pietro Marsella, et al. Kevin: Multi-turn rl for generating cuda kernels. 2025. [Cao et al., 2026] Xinzi Cao, Jianyang Zhai, et al. Ascendkernelgen: systematic study of llm-based kernel arXiv preprint generation for neural processing units. arXiv:2601.07160, 2026. [Chen et al., 2025a] Jinwu Chen, Qidie Wu, et al. cupilot: strategy-coordinated multi-agent framework for cuda kernel evolution. arXiv preprint arXiv:2512.16465, 2025. [Chen et al., 2025b] Terry Chen, Bing Xu, et al. Automating gpu kernel generation with deepseek-r1 and inference time scaling. NVIDIA Developer Blog, 2025. [Chen et al., 2025c] Wentao Chen, Jiace Zhu, et al. Cudallm: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092, 2025. [Choquette et al., 2021] Jack Choquette, Wishwesh Gandhi, et al. Nvidia a100 tensor core gpu: Performance and innovation. IEEE Micro, 41(2):2935, 2021. [Chu et al., 2025] Ruifan Chu, Anbang Wang, et al. Gpu kernel optimization beyond full builds: An llm framework with minimal executable programs. arXiv preprint arXiv:2512.22147, 2025. [Dao et al., 2022] Tri Dao, Dan Fu, et al. Flashattention: Fast and memory-efficient exact attention with ioawareness. NeurIPS, 35:1634416359, 2022. [Dettmers et al., 2022] Tim Dettmers, Mike Lewis, et al. Llm.int8(): 8-bit matrix multiplication for transformers at scale. NeurIPS, 2022. [Dong et al., 2025] Juncheng Dong, Yang Yang, et al. Stark: arXiv Strategic team of agents for refining kernels. preprint arXiv:2510.16996, 2025. [Dropbox, Inc., ] Dropbox, Inc. Gemlite: lightweight machine learning framework for efficient model serving. https://github.com/dropbox/gemlite. [Du et al., 2025] Jinye Du, Quan Yuan, et al. Akg kernel agent: multi-agent framework for cross-platform kernel synthesis, 2025. [Fisches et al., 2025] Zacharias V. Fisches, Sahan Paliskara, et al. Kernelllm: Making kernel development more accessible, 6 2025. [FlagOpen Team, 2023] FlagOpen Team. Flagattention: collection of memory efficient attention operators implemented in the triton language. https://github.com/ flagos-ai/FlagAttention, 2023. Accessed: 2025-12-30. [FlagOpen Team, 2024] FlagOpen Team. library Flagopen/flaggems: for large language models implemented in the triton language., 2024. Flaggems is an operator [Gong et al., 2025] Junfeng Gong, Zhiyi Wei, et al. From large to small: Transferring cuda optimization expertise via reasoning graph. arXiv preprint arXiv:2510.19873, 2025. [Guo et al., 2025] Ping Guo, Chenyu Zhu, et al. Evoengineer: Mastering automated cuda kernel code evoarXiv preprint lution with large language models. arXiv:2510.03760, 2025. [Hammond et al., 2025] Alec Hammond,"
        },
        {
            "title": "Aram\nAgentic operator generation for",
            "content": "Markosyan, et al. ml asics. arXiv preprint arXiv:2512.10977, 2025. [HPC-AI Tech, 2024] HPC-AI Tech. hpc-instruct: dataset for hpc instruction tuning. https://huggingface.co/datasets/ hpcgroup/hpc-instruct, 2024. Hugging Face Dataset. [Kalade and Schelle, 2025] Sarunas Kalade and Graham Schelle. Npueval: Optimizing npu kernels with llms and open source compilers. arXiv preprint arXiv:2507.14403, 2025. [Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Khudia et al., 2021] Daya Khudia, Jianyu Huang, et al. Fbgemm: Enabling high-performance low-precision deep arXiv preprint arXiv:2101.05615, learning inference. 2021. [Kong et al., 2025] Lingcheng Kong, Jiateng Wei, et al. Concur: Conciseness makes state-of-the-art kernel generation. CoRR, abs/2510.07356, 2025. [Lange et al., 2025a] Robert Tjarko Lange, Aaditya Prasad, et al. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. Technical report, Sakana AI, 2025. [Lange et al., 2025b] Robert Tjarko Lange, Qi Sun, et al. Towards robust agentic cuda kernel benchmarking, verification, and optimization. arXiv preprint arXiv:2509.14279, 2025. [Lefaudeux et al., 2022] Benjamin Lefaudeux, Francisco xformers: modular and hackhttps://github.com/ Massa, et al. able transformer modelling library. facebookresearch/xformers, 2022. [Lei et al., 2025] Kelun Lei, Hailong Yang, et al. Pragma: profiling-reasoned multi-agent framework for automatic arXiv preprint arXiv:2511.06345, kernel optimization. 2025. [Li et al., 2025a] Annan Li, Chufan Wu, et al. The fm agent. arXiv preprint arXiv:2510.26144, 2025. [Li et al., 2025b] Haonan Li, Keyu Man, et al. Tritonforge: Profiling-guided framework for automated triton kernel optimization. arXiv preprint arXiv:2512.09196, 2025. [Li et al., 2025c] Jianling Li, ShangZhan Li, et al. Tritonbench: Benchmarking large language model capabilities In ACL, pages 23053 for generating triton operators. 23066, 2025. [Li et al., 2025d] Shangzhan Li, Zefan Wang, et al. Autotriton: Automatic triton programming with reinforcement learning in llms. arXiv preprint arXiv:2507.05687, 2025. [Liao et al., 2021] Heng Liao, Jiajin Tu, et al. Ascend: scalable and unified architecture for ubiquitous deep neuIn 2021 ral network computing: IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 789801. IEEE, 2021. Industry track paper. [Liao et al., 2025] Gang Liao, Hongsen Qin, et al. Kernelevolve: Scaling agentic kernel coding for heterogeneous ai accelerators at meta, 2025. [Wang et al., 2024] Lei Wang, Chen Ma, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [LinkedIn, 2024] LinkedIn. Liger-kernel: Efficient triton https://github.com/linkedin/ kernels for llm training. Liger-Kernel, 2024. [Lozhkov et al., 2024] Anton Lozhkov, Raymond Li, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. [Nichols et al., 2024] Daniel Nichols, Joshua H. Davis, et al. Can large language models write parallel code? In HPDC, HPDC 24, page 281294, New York, NY, USA, 2024. Association for Computing Machinery. [NVIDIA, 2022] NVIDIA. Transformer engine: An nvidia library for accelerating transformer training with fp8. https://github.com/NVIDIA/TransformerEngine, 2022. Open-source library for FP8-based Transformer training and inference. [Ou et al., 2026] Jiefu Ou, Sapana Chaudhary, et al. Maxcode: max-reward reinforcement learning framework for automated code optimization. arXiv preprint arXiv:2601.05475, 2026. [Ouyang et al., 2025] Anne Ouyang, Simon Guo, et al. Kernelbench: Can LLMs write efficient GPU kernels? In ICML, 2025. [Paliskara and Saroufim, 2025] Sahan Paliskara and Mark Saroufim. Kernelbook, 5 2025. [Ran et al., 2025] Dezhi Ran, Shuxiao Xie, et al. Kernelband: Boosting llm-based kernel optimization with hierarchical and hardware-aware multi-armed bandit. arXiv preprint arXiv:2511.18868, 2025. [Saroufim et al., 2025] Mark Saroufim, Jiannan Wang, et al. Backendbench: An evaluation suite for testing how well llms and humans can write pytorch backends, 2025. [Sereda et al., 2025] Taras Sereda, Tom St John, et al. Kforge: Program synthesis for diverse ai hardware accelerators. arXiv preprint arXiv:2511.13274, 2025. [Su et al., 2025] Songqiao Su, Xiaofei Sun, et al. Cuda-l2: Surpassing cublas performance for matrix multiplication through reinforcement learning, 2025. [Tariq et al., 2025] Muhammad Usman Tariq, Abhinav Jangda, et al. Peak: performance engineering aiassistant for gpu kernels powered by natural language transformations. arXiv preprint arXiv:2512.19018, 2025. [Team and Contributors, 2024] PyTorch Team and Contributors. Kernelfalcon: Autonomous GPU kernel generation via deep agents, 2024. Accessed: 2026-01-02. [Thakkar et al., 2017] Vijay Thakkar, Pradeep Ramani, et al. CUTLASS: CUDA templates for linear algebra subroutines, 2017. Version 3.x, accessed 2025. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, et al. Attention is all you need. NeurIPS, 30, 2017. [Wang et al., 2025] Jianghui Wang, Vinay Joshi, et al. Geak: Introducing triton kernel ai agent & evaluation benchmarks. arXiv preprint arXiv:2507.23194, 2025. [Wei et al., 2025] Anjiang Wei, Tianran Sun, et al. Astra: multi-agent system for gpu kernel performance optimization. arXiv preprint arXiv:2509.07506, 2025. [Wen et al., 2025] Zhongzhen Wen, Yinghui Zhang, et al. Multikernelbench: multi-platform benchmark for kernel generation. arXiv eprints, pp. arXiv2507, 2025. [Woo et al., 2025] Jiin Woo, Shaowei Zhu, et al. Tritonrl: Training llms to think and code triton without cheating. arXiv preprint arXiv:2510.17891, 2025. [Wu, 2023] Peng Wu. Pytorch 2.0: The journey to bringing compiler technologies to the core of pytorch (keynote). In Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization, CGO 23, page 1, New York, NY, USA, 2023. Association for Computing Machinery. [Xing et al., 2026] Shanli Xing, Yiyan Zhai, al. Flashinfer-bench: Building the virtuous cycle for aiarXiv preprint arXiv:2601.00227, driven llm systems. 2026. et [Ye et al., 2025] Zihao Ye, Lequn Chen, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. [Zhang et al., 2025a] Xuzhi Zhang, Shaohui Peng, et al. Qimeng-tensorop: highAutomatically performance tensor operators with hardware primitives. arXiv preprint arXiv:2505.06302, 2025. generating [Zhang et al., 2025b] Zijian Zhang, Rong Wang, et al. Cudaforge: An agent framework with hardware feedarXiv preprint back for cuda kernel optimization. arXiv:2511.01884, 2025. https://arxiv.org/abs/2511. 01884. [Zhou et al., 2025a] Qirui Zhou, Shaohui Peng, et al. QiMeng-attention: SOTA attention operator is generated by SOTA attention algorithm. In Findings of ACL, pages 84918505, Vienna, Austria, July 2025. Association for Computational Linguistics. [Zhou et al., 2025b] Qirui Zhou, Yuanbo Wen, generating et al. Qimeng-gemm: highAutomatically performance matrix multiplication code by exploiting In AAAI, volume 39, pages large language models. 2298222990, 2025. [Zhu et al., 2025] Xinguo Zhu, Shaohui Peng, et al. Qimengkernel: Macro-thinking micro-coding paradigm for llmarXiv based high-performance gpu kernel generation. preprint arXiv:2511.20100, 2025. [Zhu et al., 2026] Haowei Zhu, Puyuan Yang, et al. Diffbench meets diffagent: End-to-end llm-driven diffuarXiv preprint sion acceleration code generation. arXiv:2601.03178, 2026."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing Institute of Technology",
        "Beijing Jiaotong University",
        "Beijing Normal University",
        "Cornell University",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Peking University",
        "Renmin University of China"
    ]
}