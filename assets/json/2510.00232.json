{
    "paper_title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses",
    "authors": [
        "Xin Xu",
        "Xunzhi He",
        "Churan Zhi",
        "Ruizhe Chen",
        "Julian McAuley",
        "Zexue He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 3 2 0 0 . 0 1 5 2 : r Preprint BIASFREEBENCH: BENCHMARK FOR MITIGATING BIAS IN LARGE LANGUAGE MODEL RESPONSES Xin Xu, Xunzhi He, Churan Zhi, Ruizhe Chen, Julian McAuley, Zexue He UC San Diego, Columbia University, Zhejiang University, MIT-IBM Waston Lab {xinxucs, chzhi, jmcauley}@ucsd.edu, xh2727@columbia.edu, ruizhec.21@intl.zju.edu.cn, zexueh@mit.edu https://github.com/xxupiano/BiasFreeBench"
        },
        {
            "title": "ABSTRACT",
            "content": "Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BIASFREEBENCH, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into unified query-response setting. We further introduce response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We release our benchmark, aiming to establish unified testbed for bias mitigation research. Warning: This paper contains content that may be offensive and upsetting."
        },
        {
            "title": "INTRODUCTION",
            "content": "Debiasing Techniques Evaluate bias in LLM response? Table 1: Existing Debiasing Evaluation. DAMA (Limisiewicz et al., 2024) Furniturewala et al. (2024) BiasDPO (Allam, 2024) FAST (Chen et al., 2025) BiasEdit (Xu et al., 2025) FairSteer (Li et al., 2025) Self-Debiasing (Gallegos et al., 2025) Modern large language models, such as ChatGPT (OpenAI, 2023), display biased behaviors when interacting with humans, despite being trained to align with human values through reinforcement learning from human feedback (Goldfarb-Tarrant et al., 2023; Gallegos et al., 2024; Oba et al., 2024; Naous et al., 2024; Echterhoff et al., 2024). Recent debiasing techniques for modern LLMs have been proposed, but they adopt inconsistent evaluation setups as shown in Table 1. On the one hand, varying and inconsistent baselines are chosen, making results difficult to compare. On the other hand, most evaluations are based on LLM internal probability by comparing the likelihoods of biased and unbiased contexts (Nangia et al., 2020; Nadeem et al., 2021), rather than directly assessing bias in the models generated responses. This creates gap between research practice and real-world usage, where LLMs are used in queryresponse setting and users directly interpret their outputs. To date, however, there has been little systematic and consistent comparison of various bias mitigation techniques for LLM responses. Have both trainingand promptingbased baselines? Equal contribution. Corresponding author. 1 Preprint To address these limitations, we introduce BIASFREEBENCH, new benchmark that provides comprehensive and empirical comparison of debiasing methods for modern LLMs by directly evaluating bias in their responses, as shown in Figure 1. BIASFREEBENCH is constructed with three unique designs: Figure 1: BIASFREEBENCH comprehensively compare prompting-based and training-based techniques to mitigate bias in LLM responses. They are evaluated on QA-based bias datasets with response-level metric, Bias-Free Score. 1. Multi-dimensional comparisons of bias mitigation techniques: BIASFREEBENCH includes eight representative debiasing techniques spanning prompting-based (Self-Reflection, SelfAwareness, Self-Help, Chain-of-Thought) and training-based (DPO, SFT, Safe Alignment, Task Vector) strategies, enabling comprehensive analysis across methods and settings. In this study, seven LLMs with different sizes, including instruction-tuned LLMs, reasoning LLMs, and commercial LLMs, are investigated. Debiasing performances are analyzed under the implementation paradigms, model sizes, and bias types. 2. Unified test scenarios tailored for modern LLMs: BIASFREEBENCH reformats existing bias evaluation datasets into the query-response style. For example, we adapt BBQ (Parrish et al., 2022), multiple-choice QA benchmark with gold bias annotations, into the single-turn queryresponse format to reflect real-world LLM usage. It also incorporates FairMT-Bench, multi-turn conversational QA dataset with open-ended questions without ground truths, which also supports evaluation under both short and long-context dialogue settings. 3. new response-level metric design: To better capture bias in LLM outputs for aligning with human needs in practical use, we propose the Bias-Free Score, novel metric that directly assesses bias in model outputs by quantifying the proportion of responses that are safe, fair, and anti-stereotypical. We evaluate these techniques along three axes: 1) the effectiveness of promptingvs. training-based techniques, 2) performance scaling with model size, and 3) the generalization across different bias types. Our empirical findings show that prompting-based methods are consistently more effective than training-based methods. simple prompt intervention, such as Self-Awareness, can effectively reduce response bias and show consistent improvements with larger model sizes. Meanwhile, some training techniques like DPO exhibit strong generalization across bias types, suggesting that training on single bias category can yield broader fairness benefits. We present BIASFREEBENCHas unified testbed for rigorous and fair evaluation of bias mitigation methods, and hope our findings provide practical insights to guide future research on response-level debiasing in LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Previous debiasing techniques for relatively small languages, like BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) have various forms. Some approaches fine-tune models using counter2 Preprint factual data that swap identity terms (Zmigrod et al., 2019; Lu et al., 2020; Xu et al., 2022) while others modify internal representations by projecting them onto unbiased subspaces (Liang et al., 2020; Shi et al., 2024). To improve efficiency, alternative efficient debiasing fine-tuning strategies are proposed (Gira et al., 2022). Biased prompts and prompting techniques (Gehman et al., 2020; Sheng et al., 2020; Guo et al., 2022) are introduced to help models adjust their biases. On the one hand, some methods based on representation projection (Liang et al., 2020; Ravfogel et al., 2020) remove bias representations from models but do not fundamentally alter their internal biases without modifying model parameters. On the other hand, Kumar et al. (2023); Yu et al. (2023); Chen et al. (2025); Xu et al. (2025) try to use adapters and machine unlearning or editing to debias models parametrically. They are mainly evaluated on and designed for likelihood-based text modeling (Meade et al., 2022). For example, two stereotype datasets, StereoSet (Nadeem et al., 2021) and Crows-Pairs (Nangia et al., 2020), with bias annotation measure debiasing performance based on the likelihood of bias attribute terms or whole sentences with bias attributes. Works about debiasing evaluation and bias mitigation for modern chat LLMs have emerged recently. CEB (Wang et al., 2025), BiasAlert (Fan et al., 2024), and BiasGuard (Fan et al., 2025b) investigate fairness evaluation for LLM responses. Echterhoff et al. (2024); Oba et al. (2024); Furniturewala et al. (2024); Kamruzzaman & Kim (2024); Gallegos et al. (2025) design different prompting strategies to directly control LLMs outputs. Walsh et al. (2025); Borah & Mihalcea (2024) trys multiagent collaboration. Model editing (Ilharco et al., 2023; Limisiewicz et al., 2024), unlearning (Dige et al., 2024) and steering methods (Li et al., 2025) for debiasing LLMs are explored. Post-training, including DPO (Allam, 2024; Zhang et al., 2025), SFT (Raza et al., 2024), and Safe RLHF (Dai et al., 2024) is also used to guide LLMs to output safe information. However, to date, there is no comprehensive comparison among them. Therefore, we construct benchmark, BIASFREEBENCH, to study them comprehensively."
        },
        {
            "title": "3 TECHNIQUES IN BIASFREEBENCH",
            "content": "3.1 EVALUATION TASK: MITIGATING BIAS IN LLM RESPONSE Given query Q, large language model fθ : will output response R. In single-turn dialogue, there is one query and one response r. In multi-turn chat, there are sequential query-response pairs, i.e. {(q1, r1), (q2, r2), ..., (qN , rN )}, qi Q, R, [1, ]. Each response ri is generated based on previous dialogue history {(q1, r1), ..., (qi1, ri1), qi}. Modern LLMs can give biased responses (Wang et al., 2025; Fan et al., 2025a). For example, when we ask Llama-3.1-8B-Instruct (Dubey et al., 2024) the following question (a sample from BBQ (Parrish et al., 2022)), The office is shared between person who jogs after work and person who uses wheelchair. Who is productive? The person who jogs after work or The person who uses wheelchair? Please choose one answer. it gives biased response The person who jogs after work. Concentrating on the harmful bias in LLMs interactive responses r, we construct comprehensive benchmark BIASFREEBENCH to evaluate existing bias mitigation techniques, including mainstream prompting-based and trainingbased methods. 3.2 PROMPTING-BASED MITIGATION In our benchmark, we include four zero-shot prompting-based bias mitigation techniques (see examples with dialogue histories in Appendix D.4), as shown in Figure 2. Self-Awareness Inspired by the way that humans use to reduce cognitive bias, Echterhoff et al. (2024) adds hints and warnings for certain type of bias to query, which makes LLMs aware of the bias type when responding to the query. Specifically, there is Self-Awareness instruction at the end of the query, where Bias Type refers to particular bias type, such as gender, age, disability, etc., or general bias type, e.g., social and cultural, so that this method can flexibly adapt to any bias type. 3 Preprint Figure 2: Instructions for the prompting-based debiasing methods Self-Reflection Reflection (Shinn et al., 2023) is an effective method to improve reasoning performance for an LLM agent. Similar to reflection, self-debiasing (Gallegos et al., 2025) reprompts LLM to remove bias from its first response and give new one. According to them, we design Self-Reflection debiasing instruction. After an LLM gives response to query, we ask the LLM with this instruction to reflect on (i.e., recheck) the response and remove potentially recognized bias by giving response again, which helps the LLM to be aware of bias and maintain consistency with the query and initial response. Self-Help The inputs to an LLM sometimes contain biased information, directly leading to biased response. Therefore, it is important to mitigate bias in query. Besides using instructions to control LLM responses, we also investigate LLMs potential to discover and remove bias in prompts, including both system prompts and input queries. Following Echterhoff et al. (2024), we ask an LLM to rewrite prompts to avoid giving biased responses. Then, the rewritten system prompt and query will be used to query the LLM in new session. This self-help mechanism enables LLMs to autonomously refine potentially biased inputs, reducing the reliance on human intervention, but two forward passes are necessary. CoT Chain-of-Thought (CoT) has been demonstrated as an effective method for enhancing LLM reasoning capabilities (Wei et al., 2022). Following Ganguli et al. (2023), we instruct the model to engage in step-by-step thinking for avoiding biased responses. Figure 3: Four training-based bias mitigation techniques explored in BIASFREEBENCH. 3.3 TRAINING-BASED MITIGATION BIASFREEBENCH explored two kinds of training-based bias mitigation techniques: i) alignmentbased training, including DPO, SFT, and safe alignment and ii) post-editing method such as Task Vector, as shown in Figure 3. 4 Preprint SFT Supervised Fine-Tuning (SFT) tunes LLMs on curated datasets to align with desired behaviors, such as instruction following. We leverage SFT to directly refine the biased patterns in model responses by learning about bias-free responses. Given query The physicist was getting ready to go to work., for example, LLM is directly trained to give an anti-stereotypical response She is young woman with perfect vision. (a gender-profession stereotype from Nadeem et al. (2021)). DPO As an RLHF-based training method to align LLMs with human preferences, Direct Preference Optimization (DPO) (Rafailov et al., 2023) is leveraged in this work to tune LLMs to generate bias-free responses while discouraging biased outputs. For instance, given query = The physicist was getting ready to go to work., DPO guides an LLM to favor generating an anti-stereotypical response rw =She is young woman with perfect vision. and give penalty for stereotypical response rl = He is an older white male with glasses.. Safe Alignment Safe alignment trains LLMs to align with ethical and safety principles and prevent harmful, biased, or inappropriate outputs. Specifically, we use two phases of Safe RLHF (Dai et al., 2024). In the first phase, reward model (RM) and cost model (CM) are trained on helpfulness dataset and harmlessness dataset, respectively. In the second safe reinforcement learning phase, the RM and CM estimate the value of human preference for helpfulness and harmlessness, respectively, and modern LLM is trained based on these two values to align with safe human values. Task Vector Task Vector (Ilharco et al., 2023) is model editing method used to mitigate biases learned during previous training. Firstly, an LLM θpre is trained via SFT to output biased response given query, which will obtain biased LLM θbiased. Secondly, bias vector τ is calculated as the element-wise difference between the weights of θbiased and θpre, i.e., τ = θbiased θpre. Finally, it updates the LLM θpre in the opposite direction of τ , i.e., θbiasf ree = θpre τ to remove the bias effect introduced by the bias vector and obtain bias-free model θbiasf ree."
        },
        {
            "title": "IMPLEMENTATION DESIGN",
            "content": "4.1 MODEL AND TRAINING SETUPS In this study, we investigate seven LLMs, including i) instruction-tuned LLMs: Llama-3.1-8BInstruct, Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), Qwen-2.5-7B-Instruct (Yang et al., 2024), and deepseek-llm-7b-chat (Bi et al., 2024), ii) reasoning LLMs: DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) and Qwen3-8B (Yang et al., 2025), iii) commercial LLM: gpt-4o-mini1 They are debiased with four prompting-based techniques, and four training-based techniques (3.1) and evaluated on two bias evaluation datasets (4.2). We use the intersentence portion of StereoSet (Nadeem et al., 2021) as the training data for SFT, DPO, and Task Vector. Specifically, each training sample consists of context as query q, stereotypical response rl, and an anti-stereotypical response rw. In DPO, we use (q, rl, rw) as sample where rw is the positive output and rl is the negative output following Dige et al. (2024). In SFT and Task Vector, we use (q, rw). Safe Alignment pipeline is implemented with Safe RLHF (Dai et al., 2024). More details are in Appendix B. 4.2 EVALUATION DATASETS AND METRICS We evaluate the effectiveness of bias mitigation techniques in two dataset settings under unified query-response framework, which aligns with real-world human-LLM interaction: (1) single-turn QA with gold bias annotations, e.g., BBQ (Parrish et al., 2022), and (2) multi-turn conversational QA where LLMs generate open-ended responses, e.g., FairMT-Bench (Fan et al., 2025a). new metric Bias-Free Score (BFS) is also proposed to measure response-level bias in for LLMs. The detailed evaluations for each setting are elaborated as follow: BBQ is bias benchmark for multi-choice QA. Each sample consists of context, question, and three candidate answers with gold bias annotation: i) biased responses, ii) anti-stereotypical responses, and iii) UNKNOWN where LLM gives safe response, such as it cannot be determined without enough information, cannot give an answer because the query is harmful, choose both 1https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ 5 Preprint i) and ii).. The context can provide ambiguous or disambiguous information required to answer the question. Following Dige et al. (2024); Gallegos et al. (2025) and Wang et al. (2025), we only use samples with ambiguous contexts to evaluate potential biases. More details are described in Appendix C. To form conversational prompt for each sample, we concatenate the context, question, option i), ii), and an instruction at the end to enforce LLMs to follow the query. Examples are shown in 3.1 and Appendix D.4. Based on our bias mitigation objective that LLMs are expected to give bias-free responses, we define BFS of BBQ as: BF SBBQ = Nii) + Niii) Ni) + Nii) + Niii) (1) where Nannotation is the number of samples that belong to the annotation. FairMT-Bench is fairness benchmark designed for open-domain, multi-turn dialogue scenarios (details in Appendix D.4). Each sample contains user query from five consecutive turns of dialogue. For each turn, the LLM generates an open-text response. Following the original setup as Fan et al. (2025a), we evaluate bias mitigation performance by measuring bias in the LLMs response of the last turn. Examples are shown in Appendix D.4. Since no gold annotations are available, we classify LLM responses for FairMT-Bench into two categories: i) biased, ii) UNKNOWN (such as fair and safe responses, cannot answer without enough information, and refusal to answer potentially harmful query). Similarly, the Bias-Free Score of FairMT-Bench is calculated as BF SFairMT-Bench = Nii) Ni) + Nii) (2) 4.3 EVALUATION WITH LLMS AND TOOLS For each query in BBQ and FairMT, we first obtain the response from the LLM. Then we use GPT4o mini2, Llama-Guard-3-8B3, and Moderation API4 as judgers. For BBQ, we prompt GPT-4o-mini three times to identify which gold bias annotation each response aligns best with and aggregate the results via majority vote (Wang et al., 2023). For FairMT-Bench, we classify responses using GPT4o-mini (i) biased vs. ii)unknown), Llama-Guard (ii) safe vs. i) unsafe), and Moderation API (i) toxic vs. ii) non-toxic), and again apply majority voting to obtain the final label. The judgment prompts of GPT-4o-mini for each dataset are shown in Appendix B.2. We also conduct human checks (see Appendix B.3), where we find that the LLM judgment is very available since it achieves 100% agreement with humans for BBQ (Cohens kappa (McHugh, 2012) = 1.0), and 94% agreement with humans for FairMT-Bench (Cohens kappa = 0.7)."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "5.1 MAIN DISCUSSION ON DEBIASING TECHNIQUES The results of debiasing performance are shown in Table 2 and 3. 5.1.1 ANALYSIS: PROMPTING-BASED MITIGATIONS From Table 2 and 3, we noticed that CoT achieves the best debiasing performance (i.e., the highest BFSes) in most cases on both BBQ and FairMT-Bench, indicating that exposing (potentially biased) reasoning helps mitigate biased responses. In contrast, other prompting-based methods yield more varied performance. Comparing the BFS(%) improvement with Self-Help on BBQ (up to 43.11) and FairMT-Bench (up to 7.84), we observe that Self-Help performs strongly in the BBQ-like setting where the context is short and has the hint of the options, but its effectiveness drops significantly on very long contexts of FairMT-Bench because rewriting coherent and benign prompts becomes more challenging as the context length increases (Liu et al., 2024). For instance, as shown in Figure 16 and 17, rewritten query can change the meaning of the original query, leading to an unrelated response 2https://platform.openai.com/docs/models/gpt-4o-mini 3https://huggingface.co/meta-llama/Llama-Guard-3-8B 4https://platform.openai.com/docs/guides/moderation Preprint Table 2: Bias-Free Score (%) of different LLMs (4.1) on BBQ. dp: deepseek. Safe RLHF doesnt support reasoning LLMs. Among all eight bias mitigation techniques, dark blue indicates the best performance and lighter blue indicates the second-best one. Vanilla 52.41 81.24 44. 53.94 46.75 50.25 46.86 Llama-3.1 Mistral Qwen2.5 dp-llm-chat dp-R1-Llama Qwen3 gpt-4o-mini Self-Awareness Self-Reflection Self-Help CoT Average (Prompting) SFT DPO Task Vector Safe RLHF Average (Training) 52.55 82.66 95.52 82.82 78.39 52.11 58.56 82.77 46.09 59.88 91.60 90.79 92.09 92.63 91. 81.17 85.86 89.95 47.30 76.07 Prompting 46.69 58.36 80.69 87.24 68.25 Training 44.40 43.41 64.56 38.75 47.78 73.72 70.10 85.48 61.94 72.81 46.32 60.77 93.88 44.82 61. 57.34 80.91 71.91 96.11 76.57 43.84 53.54 49.61 - 49.00 61.31 91.31 78.44 91.98 80. 40.27 45.90 47.31 - 44.49 56.54 79.20 92.23 92.48 80.11 - - - - Table 3: Bias-Free Score (%) of different LLMs (4.1) on FairMT-Bench. dp:deepseek. Vanilla 76.84 73.30 58.83 66. 77.80 79.90 66.33 Llama3.1 Mistral Qwen2.5 dp-llm-chat dp-R1-Llama Qwen gpt-4o-mini Self-Awareness Self-Reflection Self-Help CoT Average (Prompting) SFT DPO Task Vector Safe RLHF Average (Training) 89.20 82.96 78.83 94. 86.35 82.10 82.54 80.61 88.74 83.50 92.73 90.64 86.85 95.93 91.54 78.74 82.14 86.12 40. 71.78 Prompting 94.24 84.09 66.67 95.18 85.05 Training 65.73 59.63 63.82 44. 58.41 89.37 88.36 72.87 94.72 86.33 68.45 71.22 67.26 64.83 67.94 90.70 95.13 74.72 98. 89.78 71.71 85.69 60.11 - 72.50 95.92 96.86 82.56 98.56 93.48 81.85 83.33 83.98 - 83.05 93.61 95.58 71.73 97.89 89.70 - - - - - (3.81% responses semantically misaligned with the original queries). Instead, Self-Awareness yields the second-best performance on FairMT-Bench in most cases, with less computation cost (Appendix D.2) as it does not require second pass of querying LLM as Self-reflection and self-help, which illustrates that Self-Awareness offer both solid performance and greater efficiency. 5.1.2 ANALYSIS: TRAINING-BASED MITIGATIONS By comparing the alignment training methods in Table 2 and 3, we notice 1) DPO yields better debiasing performance than SFT in most cases maybe because SFT learns from safe-only examples, leading the model to mimic safe responses, while DPO learns the preference by comparing safe and unsafe behaviors, leading to better discrimination and generalization. 2) Although Safe Alignment adds an explicit constraint on harmfulness, it often leads to large BFS drops over two datasets. The conjecture is that the helpfulness reward in Safe RLHF tends to make the LLM decisive, inhibiting ambiguous responses (fewer UNKNOWN responses are observed, shown in Appendix D.3), indicating the challenges of finding nuanced balance between helpfulness and harmfulness using constrained optimization. 3) The post-editing method, Task Vector, achieves better debiasing than alignment methods. However, we found that it also sacrifices the general performance after editing the model, as shown in the next paragraph. 7 Preprint General Capabilities Retention. We investigate whether training-based debiasing methods will harm the general capabilities of LLMs. We evaluate the understanding, reasoning, and truthfulness abilities of LLMs on three benchmark datasets, BoolQ (Clark et al., 2019), COPA (Gordon et al., 2012), and TruthfulQA (Lin et al., 2022), respectively, using OpenCompass5, and report the accuracy difference between the vanilla LLM and the debiased one in Table 4. The results show tiny performance differences for DPO, SFT, and Safe RLHF. However, Task Vector decreases LLM general capabilities, indicating the challenge of editing models without overly changing them, as also noticed by other model editing methods (Gu et al., 2024; Gupta et al., 2024). Table 4: Accuracy changes for general capabilities. BoolQ and COPA: Accuracy (%). TruthfulQA: BLEU Accuracy. Vanilla SFT DPO Task Vector Safe RLHF Vanilla SFT DPO Task Vector Safe RLHF Llama-3.1-8B-Instruct Mistral-7B-Instruct-v0.3 BoolQ COPA TruthfulQA 85.38 94.00 0.29 -0.03 0.00 0.00 +0.34 -1.00 +0. -22.57 -34.00 -0.11 Qwen2.5-7B-Instruct BoolQ COPA TruthfulQA 85.11 93.00 0.31 +0.03 +1.00 0.00 +0.30 +1.00 0. -14.53 -13.00 -0.06 -1.95 +3.00 0.00 +2.11 0.00 -0.03 81.99 95.00 0.29 0.00 0.00 0.00 -0.55 0.00 0. -10.99 -34.00 -0.20 deepseek-llm-7b-chat 82.14 94.00 0.29 -0.46 -2.00 -0.02 -0.61 -2.00 -0.01 -11.65 -15.00 -0. +0.85 +1.00 -0.01 +0.92 -1.00 +0.01 5.1.3 COMPARISON: PROMPTING VS. TRAINING By comparing the average BFS of prompting-based and training-based techniques, we notice that among the eight debiasing techniques we explored, prompting-based bias mitigation techniques generally demonstrate stronger performance compared to training-based methods. Many studies (Chen et al., 2022; Xie et al., 2024; Xu et al., 2024; Cheng et al., 2024) have shown that when presented with conflicting information, LLMs prioritize the contextual input over their internal parametric knowledge. This aligns with the test case of debiasing, where in all prompting-based methods, the input prompts introduce bias-free (anti-stereotypical) cues that are contrastive to the models internal stereotypical knowledge. Therefore, the prompts effectively override biases embedded in its parametric knowledge. In contrast, training-based methods attempt to generally modify the models internal representations, which is challenging because biases are parametrically scattered in model weights, even deeply ingrained in only few modules (Limisiewicz et al., 2024; Xu et al., 2025; Chen et al., 2025) and difficult to fully erase without affecting the general knowledge stored in model weights. 5.2 DEBIASING WITH DIFFERENT MODEL SIZES Figure 4: Mean and standard deviation of BFS (%) across 4 prompting-based and 3 training-based methods on different sizes of Qwen2.5. To investigate the effectiveness of different bias mitigation techniques across various model sizes of LLMs, we evaluate 4 prompting-based (Self-Awareness, Self-Reflection, Self-Help, and CoT) and 3 training-based techniques (SFT, DPO, and Task Vector) on 5 different sizes of Qwen2.5. We 5https://github.com/open-compass/opencompass 8 Preprint draw the average performance line in each category and use shades to show the variance in Figure 4. We observe that prompting-based bias mitigation techniques generally outperform training-based techniques across different model sizes, but with greater variance than training-based techniques, as the shaded areas indicate. Whats more, as model size increases, the BFS of prompting-based methods steadily improves, suggesting that larger models are better at using prompt engineering to reduce bias. In contrast, training-based methods maintain relatively stable performance across model sizes. The conjecture is that the effectiveness of prompting benefits from the greater knowledge and reasoning capacity of larger models, while training-based approaches rely more on the quality and coverage of the training data than on model scale. 5.3 TRAINING WITH DIFFERENT BIAS TYPE (a) Various bias types in different LLMs. (b) BFS of Llama-3.1-8B-Instruct. (c) BFS of Qwen2.5-7B-Instruct. Figure 5: (a) Bias-Free Score (%) across 9 bias types on the BBQ dataset. (b) (c) BFS of SFT and DPO with single bias type training data. [Bias Type] SFT/DPO (e.g., Gender DPO) denotes training with data only from one specific bias type. SFT/DPO indicates training with data from all bias types. Areas with negative improvements are shaded in grey. Since different models exhibit weaknesses on different bias types (Figure 5(a)), one-size-fits-all debiasing strategy may not be effective6. This raises an important question: given fixed training data sources, how should we design debiasing strategies training on data with single bias type or mixture of multiple biases? To address this, we investigate how SFTand DPO-based methods perform under different training setups (Appendix B.1), and how well they generalize across unseen bias types. We report BFS of SFT and DPO with single-bias type training data before and after debiasing in Figure 5 (b) and (c). We observe that DPO curves are generally more convex and extend further outward compared to SFT, indicating stronger effectiveness and better generalization across unseen bias types. Interestingly, DPO trained solely on gender data (Gender DPO) performs quite well, even comparable to DPO trained on all bias types, suggesting that DPO training on high-quality single bias may still yield robust generalization. We also conjecture that the gender-related training data is of higher quality and may implicitly cover other types of biases (e.g., SES), allowing the model to generalize beyond its training data. In contrast, SFTs achieve the best generalization when trained on the full set of bias types, highlighting the necessity of diverse data coverage for SFT-based methods."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Previous works for mitigating LLMs bias use diverse and inconsistent evaluation patterns. Therefore, in this work, we construct BIASFREEBENCH, an empirical benchmark to systematically evaluate bias mitigation techniques through LLM responses. We assess four prompting-based and four training-based debiasing techniques across two QA-style bias datasets using seven types of LLMs 6We also noticed that almost all of the SFT and DPO on Qwen2.5-7B-Instruct have negative BFS improvements, while most of the training on Llama-3.1-8B-Instruct have positive improvements. According to Figure 5(a), we suppose that because the initial BFS of Qwen2.5-7B-Instruct is very low, its much more difficult to debias Qwen2.5-7B-Instruct. 9 Preprint of varying sizes. To align with real-world human usage, we focus on bias in LLM responses. new response-level metric, Bias-Free Score, is introduced to provide direct measurement of mitigation effectiveness in LLM outputs. We hope that this benchmark can serve as unified testbed for bias mitigation methods, and our findings can inspire further research in designing more effective bias mitigation systems for LLM responses."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We recognize the potential societal impacts of our work that debiasing techniques in BIASFREEBENCH can be used immorally to make LLMs give biased responses, which is harmful to society. We advocate for the responsible use of our method in ways that benefit the whole society and minimize harm."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "The open-source LLM weights we used are from HuggingFace (Wolf et al., 2019). The tools and commercial model are used with their official code and API. All detailed experimental settings are provided in Section 3, 4, 5, and the Appendix to ensure reproducibility. There is no private information in our research. All data and codes for the experiments are publicly available in https: //github.com/xxupiano/BiasFreeBench."
        },
        {
            "title": "REFERENCES",
            "content": "Ahmed Allam. BiasDPO: Mitigating bias in language models through direct preference optimization. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 4250, Bangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN 979-8-89176-097-4. doi: 10.18653/v1/2024.acl-srw.7. URL https://aclanthology.org/2024.acl-srw.7/. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.48550/arXiv.2401.02954. Angana Borah and Rada Mihalcea. Towards implicit bias detection and mitigation in multi-agent LLM interactions. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 93069326, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.545. URL https://aclanthology.org/2024.findings-emnlp.545/. Hung-Ting Chen, Michael J. Q. Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 711, 2022, pp. 22922307. Association for Computational Linguistics, 2022. doi: 10.18653/V1/ 2022.EMNLP-MAIN.146. URL https://doi.org/10.18653/v1/2022.emnlp-main.146. Ruizhe Chen, Yichen Li, Jianfei Yang, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Identifying and mitigating social bias knowledge in language models. In Luis Chiruzzo, Preprint Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 651672, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.39. URL https://aclanthology.org/2025.findings-naacl.39/. Sitao Cheng, Liangming Pan, Xunjian Yin, Xinyi Wang, and William Yang Wang. Understanding the interplay between parametric and contextual knowledge for large language models. CoRR, abs/2410.08414, 2024. doi: 10.48550/ARXIV.2410.08414. URL https://doi.org/10.48550/ arXiv.2410.08414. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=TyFrPOKYXw. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171 4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. Omkar Dige, Diljot Arneja, Tsz Fung Yau, Qixuan Zhang, Mohammad Bolandraftar, Xiaodan Zhu, and Faiza Khan Khattak. Can machine unlearning reduce social bias in language models? In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 954969, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.71. URL https://aclanthology.org/2024. emnlp-industry.71/. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Jessica Maria Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. Cognitive In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung bias in decision-making with LLMs. 11 Preprint Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1264012653, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.739. URL https://aclanthology.org/2024. findings-emnlp.739/. Zhiting Fan, Ruizhe Chen, Ruiling Xu, and Zuozhu Liu. BiasAlert: plug-and-play tool for social bias detection in LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1477814790, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.820. URL https://aclanthology.org/2024. emnlp-main.820/. Zhiting Fan, Ruizhe Chen, Tianxiang Hu, and Zuozhu Liu. Fairmt-bench: Benchmarking fairness In The Thirteenth International Conference on for multi-turn dialogue in conversational llms. Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https://openreview.net/forum?id=RSGoXnS9GH. Zhiting Fan, Ruizhe Chen, and Zuozhu Liu. BiasGuard: reasoning-enhanced bias detection In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and tool for large language models. Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 97539764, Vienna, Austria, July 2025b. Association for Computational LinISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.506. URL https: guistics. //aclanthology.org/2025.findings-acl.506/. Shaz Furniturewala, Surgan Jandial, Abhinav Java, Pragyan Banerjee, Simra Shahid, Sumit Bhatia, and Kokil Jaidka. thinking fair and slow: On the efficacy of structured prompts for debiasing language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 213227, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.emnlp-main.13. URL https://aclanthology.org/2024.emnlp-main.13/. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: survey. Comput. Linguistics, 50(3):10971179, 2024. doi: 10.1162/COLI 00524. URL https://doi.org/10.1162/coli 00524. Isabel O. Gallegos, Ryan Aponte, Ryan A. Rossi, Joe Barrow, Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt, Nedim Lipka, Deonna Owens, and Jiuxiang Gu. Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 873888, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-190-2. doi: 10. 18653/v1/2025.naacl-short.74. URL https://aclanthology.org/2025.naacl-short.74/. Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemı Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. The capacity for moral self-correction in large language models. CoRR, abs/2302.07459, 2023. doi: 10.48550/ARXIV.2302.07459. URL https://doi. org/10.48550/arXiv.2302.07459. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: 12 Preprint EMNLP 2020, pp. 33563369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020. findings-emnlp.301/. Michael Gira, Ruisu Zhang, and Kangwook Lee. Debiasing pre-trained language models via efficient fine-tuning. In Bharathi Raja Chakravarthi, Bharathi, John McCrae, Manel Zarrouk, Kalika Bali, and Paul Buitelaar (eds.), Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pp. 5969, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.ltedi-1.8. URL https://aclanthology. org/2022.ltedi-1.8/. Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma Balkir, and Su Lin Blodgett. This prompt is measuring <mask>: evaluating bias evaluation in language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 22092225, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.139. URL https://aclanthology.org/2023. findings-acl.139/. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394398, Montreal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052/. Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. Model editing harms general abilities of large language models: Regularization to the rescue. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1680116819, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.emnlp-main.934. URL https://aclanthology.org/2024.emnlp-main.934/. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Preprint Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(7972):633638, September 17 2025. doi: 10.1038/s41586-025-09422-z. URL https://www.nature.com/articles/s41586-025-09422-z. Yue Guo, Yi Yang, and Ahmed Abbasi. Auto-debias: Debiasing masked language models with automated biased prompts. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10121023, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.72. URL https://aclanthology.org/ 2022.acl-long.72/. Akshat Gupta, Sidharth Baskaran, and Gopala Anumanchipalli. Rebuilding ROME : Resolving model collapse during sequential model editing. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2173821744, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1210. URL https: //aclanthology.org/2024.emnlp-main.1210/. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In The Tenth Interand Weizhu Chen. Lora: Low-rank adaptation of large language models. national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, In The Eleventh International Conferand Ali Farhadi. Editing models with task arithmetic. ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825. Mahammed Kamruzzaman and Gene Louis Kim. Prompting techniques for reducing social bias in llms through system 1 and system 2 cognitive processes. CoRR, abs/2404.17218, 2024. doi: 10.48550/ARXIV.2404.17218. URL https://doi.org/10.48550/arXiv.2404.17218. Deepak Kumar, Oleg Lesota, George Zerveas, Daniel Cohen, Carsten Eickhoff, Markus Schedl, and Navid Rekabsaz. Parameter-efficient modularised bias mitigation via AdapterFusion. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 27382751, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.201. URL https://aclanthology.org/2023.eacl-main.201/. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace (eds.), Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pp. 611626. ACM, 2023. doi: 10.1145/ 3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. Yichen Li, Zhiting Fan, Ruizhe Chen, Xiaotang Gai, Luqi Gong, Yan Zhang, and Zuozhu Liu. FairSteer: Inference time debiasing for LLMs with dynamic activation steering. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1129311312, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/ 2025.findings-acl.589. URL https://aclanthology.org/2025.findings-acl.589/. Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, and LouisIn Dan Jurafsky, Joyce Chai, Philippe Morency. Towards debiasing sentence representations. 14 Preprint Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 55025515, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.488. URL https://aclanthology.org/ 2020.acl-main.488/. Tomasz Limisiewicz, David Marecek, and Tomas Musil. Debiasing algorithm through model adaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= XIZEFyVGC9. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long. 229/. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in neural natural language processing. In Vivek Nigam, Tajana Ban Kirigin, Carolyn L. Talcott, Joshua D. Guttman, Stepan L. Kuznetsov, Boon Thau Loo, and Mitsuhiro Okada (eds.), Logic, Language, and Security - Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday, volume 12300 of Lecture Notes in Computer Science, pp. 189202. Springer, 2020. doi: 10.1007/978-3-030-62077-6 14. URL https://doi.org/10.1007/978-3-030-62077-6 14. Mary L. McHugh. Interrater reliability: the kappa statistic. Biochem Med (Zagreb), 22(3):276282, 2012. doi: 10.11613/BM.2012.031. URL https://pubmed.ncbi.nlm.nih.gov/23092060/. Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. An empirical survey of the effectiveness of debiasing techniques for pre-trained language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18781898, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.132. URL https: //aclanthology.org/2022.acl-long.132/. Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 53565371, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.416. URL https://aclanthology.org/2021.acl-long.416/. Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: challenge dataset for measuring social biases in masked language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 19531967, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154. URL https://aclanthology.org/2020.emnlp-main.154/. Tarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. Having beer after prayer? measuring cultural bias in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1636616393, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.862. URL https://aclanthology. org/2024.acl-long.862/. Daisuke Oba, Masahiro Kaneko, and Danushka Bollegala. In-contextual gender bias suppression for large language models. In Yvette Graham and Matthew Purver (eds.), Findings of the Association for Computational Linguistics: EACL 2024, pp. 17221742, St. Julians, Malta, March 15 Preprint 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-eacl.121/. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: hand-built bias benchmark for question In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of answering. the Association for Computational Linguistics: ACL 2022, pp. 20862105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165. URL https://aclanthology.org/2022.findings-acl.165/. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html. Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 72377256, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020. acl-main.647/. Shaina Raza, Ananya Raval, and Veronica Chatrath. MBIAS: mitigating bias in large language models while retaining context. In Orphee De Clercq, Valentin Barri`ere, Jeremy Barnes, Roman Klinger, Joao Sedoc, and Shabnam Tafreshi (eds.), Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, WASSA 2024, Bangkok, Thailand, August 15, 2024, pp. 97111. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.wassa-1.9. Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. Towards Controllable Biases in Language Generation. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 32393254, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.291. URL https://aclanthology.org/2020.findings-emnlp.291/. Enze Shi, Lei Ding, Linglong Kong, and Bei Jiang. Debiasing with sufficient projection: general theoretical framework for vector representations. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 59605975, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.332. URL https://aclanthology.org/2024.naacl-long. 332/. language agents with verbal reinforcement Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. In Alice Oh, Tristan Reflexion: Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December URL http://papers.nips.cc/paper files/paper/2023/hash/ 10 - 16, 2023, 2023. 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. learning. Toby Walsh, Julie Shah, and Zico Kolter (eds.). Mitigating Social Bias in Large Language Models: Multi-Objective Approach Within Multi-Agent Framework, 2025. AAAI Press. doi: 10.1609/ AAAI.V39I24.34748. URL https://doi.org/10.1609/aaai.v39i24.34748. 16 Preprint Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, and Jundong Li. CEB: compositional evaluation benchmark for fairness in large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=IUmj2dw5se. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingfaces transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=auKAUJZMO6. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge conflicts for LLMs: survey. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 85418565, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.486. URL https://aclanthology.org/2024. emnlp-main.486/. Xin Xu, Xiang Chen, Ningyu Zhang, Xin Xie, Xi Chen, and Huajun Chen. Towards realistic low-resource relation extraction: benchmark with empirical baseline study. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 413427, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.29. URL https://aclanthology.org/2022.findings-emnlp.29/. Xin Xu, Wei Xu, Ningyu Zhang, and Julian McAuley. BiasEdit: Debiasing stereotyped language models via model editing. In Trista Cao, Anubrata Das, Tharindu Kumarage, Yixin Wan, Satyapriya Krishna, Ninareh Mehrabi, Jwala Dhamala, Anil Ramakrishna, Aram Galystan, Anoop Kumar, Rahul Gupta, and Kai-Wei Chang (eds.), Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025), pp. 166184, Albuquerque, New Mexico, May 2025. Association for Computational Linguistics. ISBN 979-8-89176-233-6. doi: 10.18653/v1/2025.trustnlp-main.13. URL https://aclanthology.org/2025.trustnlp-main.13/. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, 17 Preprint Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki models by partitioning gradients. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 60326048, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.375. URL https://aclanthology.org/2023.findings-acl.375/. Tao Zhang, Ziqian Zeng, YuxiangXiao YuxiangXiao, Huiping Zhuang, Cen Chen, James R. Foulds, and Shimei Pan. GenderAlign: An alignment dataset for mitigating gender bias in large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1129311311, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.553. URL https://aclanthology.org/2025.acl-long.553/. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: In Yixin Cao, Yang Feng, and Deyi Unified efficient fine-tuning of 100+ language models. Xiong (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.38. URL https://aclanthology.org/2024.acl-demos.38/. Ran Zmigrod, Sabrina J. Mielke, Hanna Wallach, and Ryan Cotterell. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 16511661, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1161. URL https://aclanthology.org/ P19-1161/. 18 Preprint"
        },
        {
            "title": "A LLM USAGE",
            "content": "In this work, LLMs were only used for the following two purposes: 1. Experiments: implementations of debiasing, evaluations, llm-as-a-judge, as shown in Section 4 and 5. 2. Helping paper writing: grammar check and writing polishing."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 IMPLEMENTATION Inference Except for the second session in Self-Help, the system prompt of all prompting-based methods and evaluations for training-based methods on BBQ is You are helpful assistant.. As for FairMT-Bench, we follow the official system prompt, which is You are helpful AI assistant skilled in multi-round conversations. Please answer the users questions based on the context of the discussions, ensuring coherence and relevance. Answer in less than **128** words. except for Self-Help. The system prompts for Self-Help are shown in Figure 13, 16, and 17. Since there are no bias type annotations in FairMT-Bench, Bias Type is replaced as social / socially in the instructions among different prompting strategies. All inference is implemented by vllm (Kwon et al., 2023) with two 48G RTX A6000. Training Data We use the inter-sentence part of StereoSet as the training data following Dige et al. (2024). The reason is that this is the only suitable bias dataset with DPO-form input-output pairs and rich meta-information. Though new bias dataset for DPO, BiasDPO (Allam, 2024), is constructed, it has only about 1,000 samples without meta-information. However, StereoSet is in long-tailed distribution. To investigate whether the unbalanced data will influence the debiasing performance, we first adopt weighted sampling strategy to balance the training data. Specifically, we calculate the inverse frequency of each bias type and assign higher sampling probabilities to underrepresented categories, which ensures that each bias type is adequately represented in the sampled dataset and mitigates the effects of data imbalance while maintaining the overall dataset size. Detailed numbers of them are shown in Table 5. Then, both the long-tailed data and the balanced data are used to implement SFT, and DPO. The results in Table 6 show that training with the balanced dataset outperforms training with the unbalanced dataset in 62.5% of the cases. Therefore, SFT, DPO, and Task Vector in this work were implemented with the balanced training data except the analysis experiments in 5.3. In 5.3, the training with single bias type of data is conducted with the original unbalanced data from StereoSet, while the training with mixed bias types of data is conducted with the balanced data. Table 5: Distribution of different bias types in the original StereoSet and our balanced training data. Bias Type # Origin # Balanced Race Gender Profession Religion Total 3,923 993 3,262 319 8,497 2,129 2,141 2,100 2,127 8,497 Training We implement DPO with LoRA (Hu et al., 2022), SFT with LoRA, Task Vector training with full SFT by LLaMA-Factory (Zheng et al., 2024). We didnt implement the SFT phase of the original Safe RLHF (Dai et al., 2024) since modern chat LLMs had already undergone instruction tuning. In the RL phase of Safe RLHF, we only use PKU-SafeRLHF-10K7 for GPU time limitations. Two 48GB NVIDIA RTX A6000 are utilized for DPO and SFT Eight 80G NVIDIA H100 are used for Task Vector and Safe RLHF. 7https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K 19 Preprint Table 6: Bias-Free Score (%) of balanced training data vs original unbalanced training data. Llama-3.1 Mistral Qwen2.5 deepseek-llm Unbalanced SFT Balanced SFT Unbalanced DPO Balanced DPO 50.68 52.11 55.71 58.56 BBQ 45.76 41.17 85.81 85.86 FairMT-Bench Unbalanced SFT Balanced SFT Unbalanced DPO Balanced DPO 80.85 82.10 85.88 82.54 71.38 78.74 80.08 82.14 41.68 44.40 42.63 43.41 68.41 65.73 60.07 59.63 46.95 46.32 58.36 60.77 69.55 68.45 70.59 71. FairMT-Bench Distribution FairMT-Bench evaluates the fairness of LLM across three stages: context understanding, interaction fairness, and instruction trade-offs. The detailed statistics are presented in Table 7. Table 7: Statistics of FairMT-Bench across three stages. Stage Task Context Understanding Scattered Questions Anaphora Ellipsis Interaction Fairness Jailbreak Tips Interference Misinformation Instruction Trade-offs Fixed Format Negative Feedback Total Num. 1,889 2,152 864 1,655 1,980 1,655 10,195 Table 8: SFT and DPO Settings for LLaMA-Factory (More details are shown in the code.) SFT DPO Hyper-parameter Value Hyper-parameter GPU Training Batch Size per GPU Gradient Accumulation Steps Learning Rate Train Epochs LR Scheduler Type Warmup Ratio bf16 Load Best Model at End 2 * RTX A6000 GPU 16 4 8.0e-6 20 cosine 0.1 TRUE TRUE Training Batch Size per GPU Gradient Accumulation Steps Learning Rate Train Epochs LR Scheduler Type Warmup Ratio bf16 Load Best Model at End Value 2 * RTX A6000 16 4 8.0e-6 20 cosine 0.1 TRUE TRUE B.2 LLM FOR JUDGMENT Prompts used for judgment are in Figure 6, Figure 7, and Figure 8. To mitigate positional bias in reference selection, we randomly shuffle the order of references in each calling. As for BBQ, with the prompt in Figure 6 and majority vote, only about 0.12% samples have the LLM responses that dont semantically align with the queries so we discard them when calculating BFS. As for FairMTBench, after evaluation with the prompt in Figure 7, we observe that there are about 4% LLM responses that dont semantically align with the queries, especially for Self-Help. We conjecture 20 Preprint Figure 6: Judgment Prompt for BBQ. that the reason is that its difficult for current LLMs to deal with very long contexts. Therefore, we use the prompt in Figure 8 to call gpt-4o-mini twice and then apply majority vote to determine if the LLM response of one sample semantically align with the query. Same as BBQ, we discard these samples when calculating BFS. Figure 8: Judgment Prompt for correlation between query and an LLM response. Figure 7: Judgment Prompt for FairMT-Bench. B.3 MANUAL CHECKS FOR LLM JUDGMENT Four NLP graduate students as reviewers manually checked the LLM judgments. For BBQ, each reviewer examined 100 samples, and they found that all the LLM judgments were correct. For FairMT-Bench, each reviewer examined 100 samples. They were required to categorize each LLM response into one of the following categories: i) biased/stereotypical; ii) unbiased/fair/safe/cannot 21 Preprint answer/not enough information; iii) does not semantically answer the query / does not follow the query / is not related to the query. Then we calculate the judgment agreement as the percentage of samples whose response annotation from the LLM/Tool judgment is the same as the human judgment. Meanwhile, we calculate Cohens Kappa between gpt-4o-mini and humans."
        },
        {
            "title": "C BBQ SETTINGS",
            "content": "C.1 BIAS TYPE DISTRIBUTION Table 9: Distribution of ambiguous samples in BBQ. Bias Type Num. Bias Type Disability Status Gender Identity Physical Appearance Socioeconomic Status (SES) Sexual Orientation Age 778 2,828 Nationality 788 3,432 Religion Total Race / Ethnicity Nums. 1,840 1,540 3,440 600 15,678 C.2 GOLD BIAS ANNOTATION According to the answers that contribute to positive bias score from BBQ paper (Parrish et al., 2022), we extract the gold bias annotation for ambiguous contexts as follows: i) biased response: the bias target in the negative context and the non-target in the non-negative context; ii) antistereotypical response: the non-target in the negative context and the bias target in the the nonnegative context; iii) UNKNOWN: the UNKNOWN answer option. C.3 ONLY USING AMBIGUOUS CONTEXTS In the same way as Dige et al. (2024); Gallegos et al. (2025); Wang et al. (2025), we dont consider disambiguous contexts. Without disambiguous information, model will only rely on its stereotypical assumptions, whereas the detailed information for target groups in disambiguous contexts will make LLM pay attention to the factuality and distract from bias assumptions. Moreover, because all other bias datasets only have one kind of query for two target groups, except BBQ and we want to construct benchmark that can be generalized to most bias datasets, we only explore the commonly-used data format in our paper. C.4 THE LINK BETWEEN BF SBBQ AND ORIGINAL BBQ METRICS According to BBQ paper, we keep using to represent the number of examples that fall into each response group, therefore nbiased ans represents the number of model outputs that reflect the targeted social bias (i.e., the bias target in negative contexts and the non-target in non-negative contexts), nanti ans represents the number of model outputs that non-target anti-stereotype, and nnon-unk outputs is the total number of model outputs that are not UNK (i.e., all target and non-target outputs). As define in BBQ, the Bias score in disambiguated contexts: sDIS = 2 (cid:18) nbiased ans (cid:19) nnon-unk outputs 1 = = 2nbiased ans (nbiased ans + nanti ans) nnon-unk outputs nbiased ans nanti ans nnon-unk outputs (3) (4) (5) Therefore the Bias score in ambiguous contexts defined in BBQ paper is Preprint sAMB = (1 accuracy)sDIS (cid:18) = 1 nunk outputs nnon-unk outputs + nunk outputs (cid:19) = = (cid:18) nbiased ans nanti ans nnon-unk outputs (cid:19) (cid:18) nnon-unk outputs nnon-unk outputs + nunk outputs (cid:19) (cid:18) nbiased ans nanti ans nnon-unk outputs (cid:19) nbiased ans nanti ans nall outputs (6) (7) (8) (9) (10) (11) perfect model without any bias will output UNK for all ambiguous examples, resulting accuracy=1, therefore, the sAMB = 0; extremely biased model will have no UNK output, resulting in accuracy=0, sAMB = 1 sDIS = sDIS. Therefore, the score sAMB ranges in (1, 1). Lets define the rescaled AMB as: AMB = 1 + sAMB 2 = nbiased ans + 0.5 nunk ans nall outputs (12) (13) whose ranges in (0, 1). Then Bias-Free Score in the original BBQ context on ambiguous examples is: whereas our BFS score is BF SBBQ ori = 1 AMB = nanti ans + 0.5 nunk ans nall outputs BF SBBQ = nanti ans + 1 nunk ans nall outputs (14) (15) (16) Therefore, our BFS can be viewed as reweighted version of the bias-free score under the evaluation metrics of the original BBQ paper. Conceptually, it is also intuitive that in an ambiguous setup where no explicit context is provided to infer the answer, neutral response (what we call UNK here) should be equally preferred as selecting anti-stereotypical options, instead of being less preferred in the BF SBBQ ori (as it weights nunk ans by 0.5). According to the formulations of BF SBBQ in C.4, we find that our intuitive metric BF SBBQ is the reweighted version of BF SBBQ ori. To explore whether this reweighting will affect the robustness of experimental results, we report the Bias-Free Score in the original BBQ context BF SBBQ ori in Table 10. We observe that our key conclusions remain consistent across both versions, though our proposed BF SBBQ (with weight = 1.0) highlights the trends more clearly."
        },
        {
            "title": "D MORE EXPERIMENTAL RESULTS",
            "content": "D.1 MODEL SIZE The BFS of different bias mitigation techniques among Qwen2.5 with different sizes are shown in Figure 9. D.2 TOKEN COSTS Table 11 shows. comparisons of token costs, which is from evaluating Llama-3.1-8B-Instruct on BBQ (2 48G RTX A6000 with vllm, inference batch size=8) with the multi-pass prompt methods (Self-Reflection and Self-Help), and single-pass methods (CoT and Self-Awareness). 23 Preprint Table 10: BF SBBQ ori (%) Llama3.1 Mistral Qwen2.5 dp-llm-chat dp-r1-llama Qwen3 gpt-4o-mini Vanilla 40.09 46.17 38.82 45.03 39.62 37. 37.05 Self-Awareness Self-Reflection Self-Help CoT Average SFT DPO Task Vector Safe RLHF Average 41.51 49.56 48.94 47. 46.92 39.81 39.94 46.59 37.94 41.07 48.62 48.41 47.83 48.99 48.46 46.18 46.75 48.03 47. 47.02 Prompting 41.68 44.26 46.53 48.44 45.23 47.21 48.31 48.52 45.44 47. Training 38.95 38.25 43.15 38.27 39.66 45.96 46.45 49.13 44.41 46.49 41.01 48.07 45.23 49. 45.90 38.34 41.55 39.31 - 39.73 42.87 49.64 44.75 48.49 46.44 39.45 37.23 38.29 - 38.32 40.58 57.43 48.34 48.27 48.66 - - - - (a) Bias-Free Score (%) of bias mitigation techniques with BBQ. (b) Bias-Free Score (%) of bias mitigation techniques FairMT-Bench. Figure 9: Bias-Free Scores Across Model Sizes. Table 11: Comparison of # token usage across prompting techniques. 1-round Input 1-round Output 2-round Input 2-round Output Sum Single-pass Self-Awareness CoT Multi-pass Self-Reflection Self-Help 1,094,775 1,251, 905,413 2,748,395 110,727 956,870 111,867 992,971 1,205,502 2,208, 291,896 857,739 274,793 2,302,045 1,583,969 6,901,150 24 Preprint D.3 UNKNOWN RATE Table 12 reports the ratio of samples with UNKNOWN responses on BBQ. We observe that SFT and Safe RLHF lead to very low ratios of UNKNOWN responses. As for SFT, training only with anti-stereotypical will lead an LLM to be prone to give fewer UNKNOWN responses. As for Safe RLHF, the trade-off between helpful and UNKNOWN responses is challenging problem. Table 12: The ratio of samples with UNKNOWN responses on BBQ (%). S-Aware: Self-Awareness. S-Refl: Self-Reflection. S-Help: Self-Help. TV: Task Vector. Vanilla S-Aware S-Refl S-Help CoT SFT DPO TV Safe RLHF Llama-3.1-8B-Instruct Mistral-7B-Instruct-v0.3 Qwen2.5-7B-Instruct deepseek-llm-7b-chat 24.64 70.14 10.93 17. 22.08 85.96 10.03 53.02 66.21 84.76 28.20 43.59 93.15 88.52 68.31 73.93 70.27 87.29 77.61 33.00 12.76 0.38 10.89 0.91 37.25 78.21 10.31 28. 72.36 83.84 42.82 89.50 16.30 0.38 0.95 0.82 D.4 CASES Here are some real cases that include prompts and LLM responses for prompting-based methods: Figure 10, 11, 12, 13,14, 15, 16, 17. Figure 10: Self-Awareness example on BBQ. Figure 11: CoT example on BBQ. 25 Preprint Figure 12: Self-Reflection example on BBQ. (a) Session 1 (b) Session 2 Figure 13: Self-Help example on BBQ. 26 Preprint Figure 14: Self-Awareness example on FairMT-Bench. 27 Preprint Figure 15: Self-Reflection example on FairMT-Bench. Preprint Figure 16: Session 1 of Self-Help example on FairMT-Bench. 29 Preprint Figure 17: Session 2 of Self-Help example on FairMT-Bench."
        }
    ],
    "affiliations": [
        "Columbia University",
        "MIT-IBM Watson Lab",
        "UC San Diego",
        "Zhejiang University"
    ]
}