{
    "paper_title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
    "authors": [
        "Suhyeok Jang",
        "Dongyoung Kim",
        "Changyeon Kim",
        "Youngsuk Kim",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations."
        },
        {
            "title": "Start",
            "content": "VERIFIER-FREE TEST-TIME SAMPLING FOR VISION LANGUAGE ACTION MODELS Suhyeok Jang1 Dongyoung Kim1,3 Changyeon Kim1 Youngsuk Kim2 1KAIST 2Seoul National University 3RLWRLD Jinwoo Shin1,3 5 2 0 O 7 ] . [ 1 1 8 6 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), novel test-time scaling framework for VLAs that leverages the models internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from reference action token distribution as confidence metric for selecting the optimal action from multiple candidates. We introduce reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action models (VLAs; Zitkovich et al. 2023; Kim et al. 2024; Black et al. 2025; Bjorck et al. 2025), trained on large-scale robotic datasets (ONeill et al., 2024; Bu et al., 2025), have demonstrated remarkable performance in robot control. Among these, autoregressive VLAs represent one of the predominant VLAs (Driess et al., 2023; Kim et al., 2024; Pertsch et al., 2025), leveraging the same autoregressive objective used in training vision and foundation models without requiring architectural modifications, yet achieving comparable performance to more sophisticated architectures. Despite their success, VLAs remain fundamentally limited in tasks that demand high precision; even after extensive pre-training, they often fail on fine-grained manipulation tasks such as grasping or object placement (Nakamoto et al., 2024; Kwok et al., 2025; Gu et al., 2025; Yang et al., 2025). This precision gap is particularly problematic for real-world robotic applications where millimeter-level accuracy can determine task success or failure. Previous work (Nakamoto et al., 2024; Kwok et al., 2025) shows that while VLAs can achieve high precision with adequate training, their greedy decoding (always choosing the highest-probability action) becomes bottleneck. To address this limitation, inspired by the substantial gains observed in LLM reasoning with Test Time Scaling (TTS) (Wang et al., 2023; Wan et al., 2025; Kang et al., 2025), they use repeated sampling paired with an external verifier, i.e., value function trained on robotic data. However, these approaches have significant drawbacks: First, they require additional training to obtain verifiers with reinforcement learning objectives before inference, which adds substantial computational overhead and complexity to the deployment pipeline. Second, these external verifiers fail to generalize to unseen input conditions (Nakamoto et al., 2024), such as novel task prompts or objects, and their reward modeling is tailored to specific datasets, severely limiting their broader applicability (Kwok et al., 2025)."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of MG-Select. (1) Autoregressive VLA πθ samples action tokens in parallel from the predicted distribution, while simultaneously computing token-wise KL divergence from the condition-masking distribution to the predicted distribution. (2) Best-of-N selection is then performed using an action confidence score Ca obtained by aggregating these token-wise scores. Our approach. To tackle this problem, our research goal is to develop test-time scaling framework for VLAs that leverages the models internal properties without requiring additional training or external modules. Inspired by verifier-free approaches for TTS (Zheng et al., 2024), we begin with the most straightforward approach: selecting the action with the highest likelihood from multiple sampled actions. We observe that this simple technique alone can improve VLA performance by producing more precise actions in some cases (see Table 5 (a)). However, this approach is not effective in general, as VLAs fine-tuned on target tasks for next action token prediction often memorize expert trajectories, causing the probability distribution over action tokens to become overly concentrated, which leads to multiple sampling converging to the same result. These insights motivate us to propose Masking Distribution Guided Selection (MG-Select), novel TTS framework that leverages the KL divergence from reference action token distribution as confidence metric for selecting the optimal action from multiple candidates. Inspired by recent advances in LLM literature that use self-certainty measures (Kang et al., 2025), we adapt this principle to the VLA setting. Specifically, we introduce reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs. This design ensures the reference distribution represents maximum uncertainty while remaining aligned with the target task distribution, providing more meaningful baseline for confidence measurement. By selecting actions with the highest KL divergence from this uncertainty-aware reference, MG-Select effectively identifies the most confident action sequences while avoiding the limitations of likelihood-based selection, achieving significant performance improvements in practice. Additionally, we propose joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. In our experiments, we have validated the effectiveness of our test-time scaling framework on both simulated (Nasiriany et al., 2024; Li et al., 2024; Liu et al., 2023) and real-world benchmarks (Khazatsky et al., 2024). Our results show that MG-Select consistently improves state-of-the-art In VLAs (Pertsch et al., 2025) across diverse pick-and-place tasks and various environments. particular, MG-Select achieves 28% improvement in real-world in-distribution tasks and 35% in out-of-distribution tasks, along with 168% relative gain on RoboCasa (Nasiriany et al., 2024) pick-and-place tasks trained with 30 demonstrations."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Problem formulation. We train the policy using the Imitation Learning (IL) framework. Specifically, IL formulates the robot control problem as Markov Decision Process (MDP) (Sutton et al., 1998) the action space, and without rewards, (s , given action s, a) [0, 1) represents the discount factor, and ρ0 denotes the initial state distribution. Given policy γ i πθ and an expert demonstration dataset t=1 , P, γ, ρ0), where [0, 1] is the transition probability from state denotes the state space, (st, at) to i = = ( ND i=1, where each trajectory } {T = S { } ,"
        },
        {
            "title": "Preprint",
            "content": "i at timestep and an instruction , the policy is optimized such that πθ(st) closely matches consists of stateaction pairs of length the expert action at for each demonstration pair. Autoregressive VLA. Given state st , we assume denotes the space of language-conditioned VLA parameterized by θ, πθ : ) denotes the set of probability distributions over actions. The possible language instructions and ( ND i, i=1 and outputs distribution πθ(a policy is trained on demonstration dataset st, I) } . We further decompose the state into visual observation ot and proprioceptive state qt, over denote the observation and proprioceptive state st = (ot, qt) with ot spaces, respectively. Therefore, the policys action distribution can be expressed as πθ(a st, I) = ot, qt, I). In our test-time scaling framework, we utilize this distribution through repeated πθ(a sampling to generate multiple candidate actions. In an autoregressive VLA, continuous action and tokenized into an action chunk at:t+H with time horizon is extracted from trajectory sequence = (a1, . . . , aT ) of variable length . The probability of an action sequence factorizes as , qt ), where , where and ( {T = L πθ(a ot, qt, I) = (cid:89) k=1 πθ(ak ot, qt, I, a<k), where a<k = (a1, . . . , ak1) is the prefix up to step action tokens. At each step k, the model produces logit vector ℓk the softmax function yields the next-token distribution πθ( categorical distribution over possible tokens and sums to one. 1. Let ot, qt, I, a<k) denote the vocabulary of discrete . Applying [0, 1]V, which is RV over V"
        },
        {
            "title": "3 METHOD",
            "content": "We present Masking Distribution Guided Selection (MG-Select), novel test-time scaling framework In that selects actions based on confidence scores from reference action token distribution. Section 3.1, we first introduce our overall test-time scaling framework. In Section 3.2, we introduce the confidence metric and its reference distribution used in our framework. In Section 3.3, we propose joint training strategy for further improving the quality of the reference distribution in parallel with fine-tuning on the target dataset. We provide the overview of MG-Select in Figure 1. For additional details, please refer to Appendix A. 3.1 TEST-TIME SCALING FRAMEWORK While VLAs demonstrate strong performance in robot control tasks, the single-inference paradigm becomes bottleneck: the model always selects the most probable action from its predicted distribution (greedy decoding), even when this action may be suboptimal. This limitation is particularly problematic for tasks requiring high precision, such as fine-grained manipulation. To resolve this, we propose test-time scaling framework that leverages only the models internal signals, without relying on external verifiers. It consists of two stages: (1) parallel stochastic sampling to generate candidates, and (2) Best-of-N selection using specific criterion . 1. Sampling candidate actions. At timestep t, the autoregressive VLA πθ samples actions ot, qt, I). To obtain diverse candidates in parallel (batch-inference), we sample from πθ(a with temperature τ > 0: a(n) πθ( ot, qt, I, a(n) <j ; τ ), = 1, . . . , N, = 1, . . . , Tn, where Tn denotes each action candidates sequence length; πθ( distribution sharpness and sample diversity (close to greedy as τ set = n=1 with a(n) = (a(n) } 1 , . . . , a(n) Tn ). 2. Best-of-N selection. Among the candidate actions, we select the final action according to pre-defined criterion . This criterion is metric for selecting the best candidate, and the selected action is given by: ; τ ) = softmax(ℓ/τ ) controls 0). This yields the candidate a(n) { = arg max a(n) Ma(n) ."
        },
        {
            "title": "3.2 CONDITION-MASKING DISTRIBUTIONAL CONFIDENCE FOR TEST-TIME SAMPLING",
            "content": "For test-time scaling, choosing proper metric for selecting the best candidate is crucial for effectiveness. When multiple candidate actions are generated, we need reliable way to identify the most promising one. Intuitively, using the models likelihood for action selection would be the simplest choice. However, this approach is not effective in general because VLAs fine-tuned on target tasks often produce overly concentrated probability distributions over action tokens, causing multiple sampling to converge to the same result. Instead, we propose confidence metric based on the KL divergence between predicted distribution and reference distribution that represents uncertainty. This approach is motivated by the insight that actions that deviate most from an uncertainty-aware reference are likely to be the most confident and precise. as probability distribution (ai) where ai Confidence over action token distributions. We first define the action token distribution over the represents the i-th action action vocabulary token. While the VLA πθ produces conditional distributions πθ( ot, qt, I, a<i) given observations, states, and instruction sequences, reference distributions can be constructed independently of such conditioning. These reference distributions can take various forms, such as uniform distributions over the action vocabulary, task-specific priors or other types of policy distributions. For computing the confidence over the action sequence, we first compute token-level distributional confidence at the i-th step token ai by measuring the distance between the predicted distribution Pi = πθ( ot, qt, I, a<i) Pi), where we use KullbackLeibler (KL) divergence and reference distribution Qi as Ci = KL(Qi as our distributional confidence measure. We then aggregate these token-level confidences across the entire action sequence to obtain the final action-level confidence score for ranking candidate actions. Formally, for an action sequence = (a1, a2, . . . , aT ) of length , we compute the action-level confidence as Ca = (cid:80) Pi), where represents the set of iI KL(Qi depends on the action tokenizing scheme: for full token indices to be aggregated. The choice of sequence aggregation, we use , while for partial aggregation, we select specific 1, 2, . . . , } token ranges based on the tokenization structure. iI Ci = (cid:80) 1, 2, . . . , { = } { Condition-masking distribution. To construct reference distribution Q, our hypothesis is that reference distribution that is uncertain yet not too distant from the target action token distribution will provide meaningful confidence signals. To this end, we mask specific information (Text, State, or both Text & State) from the input modalities given to the VLA πθ, creating condition-masking distributions that approximate failure modes where essential conditions for task solving are ignored. Formally, we compute the scoring metric as follows: (Text-masking) KLtext = KL(cid:0)πθ( (State-masking) KLstate = KL(cid:0)πθ( ot, (Text&State-masking) KLboth = KL(cid:0)πθ( ot, qt, I, a<i)(cid:1), ot, qt, I, a<i)(cid:1), ot, qt, I, a<i)(cid:1), , I, a<i) , a<i) πθ( ot, qt, , a<i) πθ( πθ( ot, , For each task environment, the optimal confidence variant can vary. For example, in the SIMPLERWidowX benchmark (Li et al., 2024), which consists solely of pick-and-place tasks, state-masking confidence works best because the model already memorizes how to pick and place objects without task instructions. In contrast, RoboCasa benchmark (Nasiriany et al., 2024), which has multiple task types, text-masking or text&state-masking are more effective, since the model cannot determine the correct action without instructions. 3.3 JOINT TRAINING STRATEGY Although our method can be seamlessly integrated with any autoregressive VLA, existing VLAs are not trained under condition-masking settings, and directly masking inputs often leads to unintended actions. To address this, we propose new fine-tuning strategy that enables the model to generate condition-masking distributions while maintaining the performance gains from standard fine-tuning on the target dataset. Specifically, we train the VLA with both all-condition and condition-masking data, randomly dropping certain conditions during fine-tuning to the target dataset, thereby increasing awareness of condition-masking distributions. Given the dataset , we augment it using four different masking variants applied to the proprioceptive state qt and the instruction I: )(cid:9), corresponding to (i) all-condition, (ii) text-masking, (iii) = (cid:8)(qt, I), (qt, , I), ( , ), ("
        },
        {
            "title": "Preprint",
            "content": "state-masking, and (iv) both-masking cases. We then train the VLA with the augmented dataset augmented where augmented = ( i, i, q(m) { E((ot,qt),at:t+H ,I)D , (m)) (cid:34) (q(m) , (m)) (q(m) ,I (m))M (cid:104) M} log πθ(at as follows: ot, q(m) , (m)) (cid:35) (cid:105) , Joint-IL(θ; ) = where at denotes the action sequence of length generated at timestep t. As result, this finetuning strategy enables the VLA to maintain performance comparable to standard fine-tuning while gaining awareness of condition-masking distributions. When combined with our proposed confidence measure, this enhanced model (denoted as MG-Select*) demonstrates improved performance over the original Masking Distribution Guided Selection framework."
        },
        {
            "title": "4.1 SIMULATION EXPERIMENTS",
            "content": "To validate the effectiveness of MG-Select, we conduct experiments across diverse robotic simulation environments including RoboCasa, SIMPLER-WidowX, and LIBERO. We fine-tune the pretrained π0-FAST model for evaluation on all simulation environments, and additionally fine-tune OpenVLA for evaluation on LIBERO to demonstrate that our method improves performance regardless of the underlying model architecture. 4.1.1 SETUP RoboCasa (Nasiriany et al., 2024). RoboCasa provides 24 atomic tasks set in household kitchen environments. We focus on 8 pick-and-place tasks, which are particularly challenging since they require high-precision actions (i.e., grasping objects) and are well-suited for evaluating improvements in precision. Following Bjorck et al. (2025), we train the base model with 30, 100, and 300 demonstrations for each task. For comparison, we also report results for GR00T N1 (Bjorck et al., 2025), taken from the original paper. SIMPLER-WidowX (Li et al., 2024). This benchmark evaluates whether our method improves precision in real-to-sim setting. Because it does not provide simulated training data, we train the base model on BridgeData V2 (Walke et al., 2023) and evaluate it on 4 pick-and-place tasks. For comparison, we also report results for RT-1-X (ONeill et al., 2024), Octo (Team et al., 2024), RoboVLM (Liu et al., 2025), and SpatialVLA (Qu et al., 2025), as reported in the SIMPLER paper (Li et al., 2024) and the respective original papers. LIBERO (Liu et al., 2023). This benchmark evaluates multiple axes of generalization, including variations in layout, objects, and goals, as well as long-horizon tasks (LIBERO-Long) that require sustained high-precision actions. 4.1.2 EXPERIMENTAL RESULTS RoboCasa (Nasiriany et al., 2024). Table 1 presents the performance of MG-Select with π0-FAST (Pertsch et al., 2025) on RoboCasa. MG-Select consistently improves the base model across all tasks, including the 8 pick-and-place tasks, and under all demonstration scales. Notably, improvements appear even without joint training, showing that our test-time scaling alone can reliably select higherprecision actions. When combined with joint training, the gains are further amplified, since learning the condition-masking distribution during training provides more reliable confidence signal for testtime scaling. We also observe particularly strong improvements in the low-data regime. For instance, with only 30 demonstrations, MG-Select with our joint training achieves 168% relative improvement on pick-and-place tasks over the base model, highlighting that our method effectively compensates for limited performance under scarce data. We provide the detailed results in Appendix B. SIMPLER-WidowX (Li et al., 2024). Table 2 shows the performance of MG-Select with π0FAST (Pertsch et al., 2025) on SIMPLER-WidowX. MG-Select clearly improves the base model across all tasks, demonstrating the robustness of our approach in enhancing action precision. We note that the base model performs relatively poorly on the put eggplant in basket task, since its background differs substantially from the other three tasks, making it sensitive to model-specific"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison on RoboCasa (Nasiriany et al., 2024). We report the average success rate (%) over 50 trials on 24 tasks, including 8 pick-and-place tasks, trained with varying numbers of demonstrations. Results for our methods are averaged over 3 random seeds, while baseline results are taken as reported in the original paper. indicates results with additional joint training before applying our test-time scaling framework. indicates reproduced performance, and Model 30 Demos 100 Demos 300 Demos Pick and Place All Pick and Place All Pick and Place All GR00T N1 π0-FAST + MG-Select (Ours) + MG-Select* (Ours) 0.4 5.3 7.2 14.2 17.4 30.9 32.0 34.6 2.2 17.0 22.6 31. 32.1 40.2 43.7 48.1 22.6 43.2 46.5 46.9 49.6 61.2 61.3 62. Table 2: Performance comparison on SIMPLER-WidowX (Li et al., 2024). We report the average success rate (%) over 24 trials on 4 pick-and-place tasks. Results for our methods are averaged over 3 random seeds, while baseline results are taken as reported in SIMPLER paper (Li et al., 2024) and the respective original papers. indicates results with additional joint training before applying our test-time scaling framework. indicates reproduced performance, and Model Spoon on Towel Carrot on Plate Stack Cubes Eggplant in Basket Average RT-1-X Octo RoboVLM SpatialVLA π0-FAST + MG-Select* (Ours) 0.0 12.5 29.2 16.7 66.7 69.4 4.2 8.3 25.0 25.0 70.8 75.0 0.0 0.0 12.5 29. 41.7 43.1 0.0 43.1 58.3 100.0 8.3 13.9 1.1 16.0 31.3 42.7 46.9 50.3 training configurations. For instance, SpatialVLA (Qu et al., 2025) achieves 100% success on the eggplant task but performs poorly on the remaining tasks. Despite this challenge, MG-Select still provides consistent improvements on the eggplant task, indicating that our approach remains effective even when the base model struggles. For detailed results, please refer to Appendix B. LIBERO (Liu et al., 2023). Table 6 presents the performance of MG-Select with π0-FAST (Pertsch et al., 2025) on LIBERO. In this benchmark, we further extend our evaluation by applying MG-Select to OpenVLA (Kim et al., 2024), showing that our approach is compatible with different architectures. The results show that MG-Select achieves superior average performance over both base models, demonstrating its general effectiveness. Notably, LIBERO-Object and LIBERO-Long are the most challenging task suites (lowest base model performance), and the gains observed on these benchmarks highlight the effectiveness of our test-time scaling framework in improving precision. 4.2 REAL WORLD EXPERIMENTS To further validate our methods generalization beyond simulation environments, we conduct realrobot experiments on 7-DoF Franka Research 3 robot arm. We use the publicly released π0-FASTDROID as the base model for evaluation, which fine-tunes the pre-trained π0-FAST on the DROID dataset (Khazatsky et al., 2024). 4.2.1 SETUP In-distribution tasks. We design in-distribution (ID) tasks to evaluate the effectiveness of our method in enhancing base model performance under limited data, as task-specific real-world data is costly to collect. The ID tasks are pick-and-place tasks defined by start and goal location, focusing on whether our method can generate high-precision actions for objects with different geometries, e.g., teddy bear, cube, rigid cup, and sponge. For these tasks, we fine-tune the π0-FAST-DROID model on 60 demonstrations per task, consisting of 15 demonstrations for each of the four objects. Out-of-distribution tasks. We design out-of-distribution (OOD) tasks to evaluate whether our method improves the zero-shot generalization of the policy. We construct 2 OOD tasks involving unseen objects, e.g., lighter cup and roll of tape. These OOD tasks are pick-and-place tasks similar to the ID tasks, but the policy must generalize to unseen real-world scenes and objects. The gains on these tasks reflect the effectiveness of our method in improving policy robustness."
        },
        {
            "title": "Preprint",
            "content": "(a) Grasping sponge from the box (b) Releasing sponge to the bowl Figure 2: Qualitative results of MG-Select in real-world pick-and-place tasks. We visualize one of our real-world experiments in the Box to Bowl task: (a) grasping an object from the box and (b) releasing it into the bowl. The rollout shows that MG-Select can generate high-precision actions at critical moments for task success, whereas the base policy (π0-FAST-DROID) often struggles at these steps. Table 4: Real-world performance on in-distribution tasks with Franka Research 3. We evaluate our method on seen tasks after multi-task training with 60 demonstrations per task. Each task is defined by start and goal location with 4 different target objects. We report the average success rate (%) over 24 trials (4 objects indicates results with additional joint training before applying our test-time scaling framework. 6 trials) for each task. Model Box to Bowl Box to Plate Basket to Bowl Plate to Basket Pick and Place π0-FAST-DROID + MG-Select* (Ours) 41.7 58.3 37.5 54.2 45.8 50.0 25.0 29.2 Average 37.5 47. 4.2.2 EXPERIMENTAL RESULTS Table 3: Real-world performance on out-of-distribution tasks with Franka Research 3. We report the average success rate (%) over 16 trials for each task. In-distribution tasks. Table 4 presents the performance of MG-Select on π0FAST-DROID (Pertsch et al., 2025) in in-distribution tasks. MG-Select outperforms the base model across all tasks, achieving 28% relative gain. This demonstrates that our test-time scaling framework remains effective beyond simulation, enabling high-precision actions to complete pickand-place tasks with diverse objects. We also provide qualitative results about real-world experiments in Figure 2, which show that MG-Select improves the precision of policy at critical moments of pick-and-place tasks, i.e., grasping and releasing, where the base model often fails. π0-FAST-DROID + MG-Select (Ours) Take Cup out of Bowl Average Pick up Tape 56.3 68.8 53.1 71.9 50.0 75.0 Model Out-of-distribution tasks. Table 3 presents the performance of MG-Select on π0-FAST-DROID in out-of-distribution tasks. The results demonstrate that MG-Select can be directly applied to generalizable policy, enhancing its robustness and precision on novel objects, with 35% improvement. Notably, MG-Select shows clear gains on objects that are more difficult to grasp and lift than in-distribution ones, e.g., roll of tape. 4.3 ABLATION STUDIES AND ANALYSES We investigate the effectiveness of the proposed components on RoboCasa and conduct the inference latency analysis on LIBERO-Object. For RoboCasa, we use models trained with 100 demonstrations, while the LIBERO-Object evaluation is performed using π0-FAST fine-tuned on LIBERO. Inference strategy. Table 5 (a) shows that low-temperature sampling (e.g., τ = 0.5) already improves over greedy decoding on the jointly trained model. Even simple Best-of-N strategies, such as selecting actions by likelihood or KL divergence against uniform reference distribution (Kang et al., 2025),"
        },
        {
            "title": "Preprint",
            "content": "M PnP All 1 Greedy Sampling 1 Uniform KL 4 4 Likelihood 4 MG-Select 28.5 42.7 27.6 43.8 30.0 46.5 30.5 46.8 31.0 48.1 1 2 4 8 PnP 27.6 30.0 31.0 30.0 30.7 All 43.8 46.2 48.1 46.9 46.1 Text State PnP All 31.0 48.1 30.1 46.7 29.7 46.3 (a) Inference strategy (b) Number of candidates (c) Condition-masking variants Joint-IL MG-Select PnP All 17.0 40.2 22.6 43.7 28.5 42.7 31.0 48.1 τ 0.5 1.0 2.0 4.0 8.0 PnP 27.5 28.8 25.4 31.0 30.0 All 43.9 44.3 43.8 48.1 45.5 PnP All 26.1 44.5 Sum 24.7 44.7 Avg 31.0 48.1 First 5 First 10 26.6 45. (d) Effect of joint training (e) Regularization temperature (f) Aggregation strategy Table 5: MG-Select ablation experiments. We present component-wise analysis of our proposed test-time scaling framework on RoboCasa (Nasiriany et al., 2024), trained with 100 demonstrations. We report the average success rate (%) over 50 trials and 3 random seeds. Temperature (τ ) for stochastic sampling is fixed to 0.5 across all experiments. PnP denotes the 8 pick-and-place tasks, and All denotes the full set of 24 tasks. Gray rows indicate the main results reported in Table 1. yield further gains. Building on this, MG-Select achieves the strongest improvements, confirming that condition-masking distributional confidence provides more effective uncertainty signal. Number of candidates. Table 5 (b) shows that performance improves as increases from 1 to 4, indicating that even small number of samples is sufficient to generate diverse candidates and to yield meaningful precision gains. Condition-masking variants. Table 5 (c) presents the results of different masking variants after joint training. Text-masking achieves the best performance, while other variants remain competitive and outperform the uniform baseline (Kang et al., 2025). Effect of joint training. Table 5 (d) shows the effect of combining our joint training strategy with MG-Select. Joint training alone already outperforms vanilla imitation learning, likely because condition-masking prevents the model from overfitting. Coupling it with MG-Select yields further gains over using MG-Select alone, confirming the effectiveness of the proposed strategy. Regularization temperature. We empirically find that naively using the condition-masking distribution (τ = 1.0) as reference does not work well, as shown in Table 5 (e), compared to uniform-based KL divergence (Kang et al., 2025). It is possibly because condition-masking distribution may be \"peaked\" around certain action tokens, which undermines the purpose of distributional confidence by failing to consider the entire probability distribution. To address this issue, we apply an appropriate high temperature (e.g., τ = 4.0) to the condition-masking distribution, which regularizes its concentration and results in superior performance. Figure 3: Inference latency comparison on LIBERO-Object (Liu et al., 2023). We compare vanilla MG-Select with its efficient deployment variant using single prefill, based on π0-FAST (Pertsch et al., 2025). Aggregation strategy. Table 5 (f) shows that the aggregation strategy for action-level confidence is crucial for selecting high-precision actions. Intriguingly, the naive summation of token-level confidence performs the worst, while truncating to the first 5 tokens works best. We hypothesize that the results may be correlated with the nature of the FAST tokenizer (Pertsch et al., 2025), i.e., each action sequence is composed of variable number of action tokens, which are aligned from lowto high-frequency."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Performance comparison on LIBERO (Liu et al., 2023). We report the average success rate (%) over 4 task suites, each consisting of 10 tasks with 50 trials per task. Results for our methods are averaged over 3 random seeds. indicates results with additional joint training before applying our test-time scaling framework. indicates reproduced performance, and Model OpenVLA + MG-Select* (Ours) π0-FAST + MG-Select* (Ours) LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average 85.2 81. 97.4 97.2 63.7 72.5 95.4 98.0 75.5 73.6 95.6 94.5 52.5 55. 79.6 82.7 69.2 70.8 92.0 93.1 Effect of single prefill deployment. Since MG-Select generates multiple candidate actions in parallel, it inevitably introduces additional latency, as the prefill step must be repeated times. This issue is particularly critical for VLAs, which require prefilling at every step when generating action sequences conditioned on the current observation. To address this, we design single-prefill deployment strategy that shares one prefill across all candidates before decoding. This significantly reduces the computational overhead, as shown in Figure 3: with = 4, our deployment achieves 45% reduction in latency compared to vanilla MG-Select. As result, the inference time of MG-Select remains comparable to that of single-action inference across different candidate sizes. We provide the detailed results in Appendix C."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Vision-Language-Action models. Developing generalist robot policies has long been central objective in robotics. Recently, Vision-Language-Action models (VLAs) have emerged as prominent approach, showing strong performance across diverse downstream tasks through large-scale pretraining on robotic datasets (Driess et al., 2023; Zitkovich et al., 2023; Black et al., 2025; Pertsch et al., 2025; Bjorck et al., 2025). Two common design paradigms have been explored: augmenting vision-language model (VLM) with diffusion-based action expert (Black et al., 2025; Bjorck et al., 2025), or converting the VLM into VLA in an autoregressive manner (Kim et al., 2024; Pertsch et al., 2025). However, despite these advances, they fundamentally rely on single-inference paradigm to generate actions, which increases the risk of errors in high-precision tasks. Test-time computing. Applying additional computation at test time is widely recognized as an effective approach to generate more accurate outputs for challenging tasks across domains. In large language models (LLMs), numerous methods have demonstrated its effectiveness in improving reasoning capabilities, e.g., mathematics, coding, and problem-solving (Chen et al., 2024; Brown et al., 2024; Ehrlich et al., 2025; Song et al., 2024). In robotics, test-time scaling has recently emerged as promising paradigm, which involves repeated sampling combined with external value functions. For example, Nakamoto et al. (2024) ranks candidate actions using value function trained via offline reinforcement learning on diverse robotic datasets, while Kwok et al. (2025) introduces VLM-based action verifiers obtained through reward modeling with synthetic preference datasets. Unlike these approaches, MG-Select requires no external modules. It performs Best-of-N sampling using only the models intrinsic signals, i.e., condition-masking distributional confidence. MG-Select consistently improves performance across diverse pick-and-place tasks. Moreover, it offers an efficient framework by eliminating the need for external model loading or interaction and by introducing optimized parallel sampling that reduces inference latency."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose MG-Select, novel test-time scaling framework for Vision-LanguageAction models (VLAs). Our approach leverages condition-masking distributional confidence as self-generated signal for Best-of-N sampling, enabling precise action selection without external verifiers. This framework mitigates the precision issues inherent in single-inference paradigms and consistently improves policy performance across wide range of simulation and real-world benchmarks. In addition, we introduce joint training strategy and optimized implementation to further enhance both effectiveness and efficiency. We believe MG-Select contributes to establishing general test-time scaling paradigm for improving robustness and precision in VLAs."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide implementation details about training and deployment in Appendix A."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This paper was supported by RLWRLD."
        },
        {
            "title": "REFERENCES",
            "content": "Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. Robotics: Science and Systems, 2025. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. Advances in Neural Information Processing Systems, 37:2768927724, 2024. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, pp. 84698488. PMLR, 2023. Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, and Azalia Mirhoseini. Codemonkeys: Scaling test-time compute for software engineering. arXiv preprint arXiv:2501.14723, 2025. Qiao Gu, Yuanliang Ju, Shengxiang Sun, Igor Gilitschenski, Haruki Nishimura, Masha Itkina, and Florian Shkurti. Safe: Multitask failure detection for vision-language-action models. arXiv preprint arXiv:2506.09937, 2025. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. arXiv preprint arXiv:2502.18581, 2025. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. Conference on Robot Learning, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jacky Kwok, Christopher Agia, Rohan Sinha, Matt Foutter, Shulu Li, Ion Stoica, Azalia Mirhoseini, and Marco Pavone. Robomonkey: Scaling test-time sampling and verification for vision-languageaction models. arXiv preprint arXiv:2506.17811, 2025. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. Conference on Robot Learning, 2024. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action models. CoRR, 2025. Mitsuhiko Nakamoto, Oier Mees, Aviral Kumar, and Sergey Levine. Steering your generalists: Improving robotic foundation models via value guidance. Conference on Robot Learning, 2024. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. Robotics: Science and Systems, 2024. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In IEEE International Conference on Robotics and Automation, pp. 68926903. IEEE, 2024. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-languageaction model. Robotics: Science and Systems, 2025. Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen Lin. The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism. arXiv preprint arXiv:2407.10457, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction. MIT press Cambridge, 1998. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. Robotics: Science and Systems, 2024. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023. Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Reasoning aware self-consistency: Leveraging reasoning paths for efficient llm sampling. Conference of the North American Chapter of the Association for Computational Linguistics, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. International Conference on Learning Representations, 2023. Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, et al. Fpc-vla: vision-language-action framework with supervisor for failure prediction and correction. arXiv preprint arXiv:2509.04018, 2025."
        },
        {
            "title": "Preprint",
            "content": "Danna Zheng, Danyang Liu, Mirella Lapata, and Jeff Pan. Trustscore: Reference-free evaluation of llm response trustworthiness. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 TRAINING ON SIMULATION DATA Imitation learning. We use two representative autoregressive VLA policies as base models: π0-FAST (Pertsch et al., 2025): It uses Paligemma-3B VLM (Beyer et al., 2024) as the backbone and is trained on 2 NVIDIA A100 GPUs with full fine-tuning from the pre-trained checkpoint. Common training configurations are fixed with the AdamW optimizer and cosine decay schedule, with warmup_steps = 1,000, peak_lr = 2.5e-5, decay_lr = 2.5e-6, and decay_steps = 30,000. Training steps, global batch size, and action chunk horizon vary by dataset as shown in Table 7. Table 7: Training setups of π0-FAST for different simulation benchmarks. Configuration RoboCasa 30 demos 100 demos 300 demos SIMPLER-WidowX LIBERO Training steps Global batch size Action chunk horizon 3k 64 5k 64 16 20k 64 16 10k 64 5 10k 32 10 OpenVLA (Kim et al., 2024): It uses Prismatic-7B VLM (Karamcheti et al., 2024) as the backbone and is trained on 2 NVIDIA A100 GPUs with LoRA fine-tuning (r = 32) from the pre-trained checkpoint. We use global batch size of 32 for LIBERO (Liu et al., 2023), while other training configurations follow the official OpenVLA implementation. Note that, consistent with the OpenVLA configuration, we train the model separately on each LIBERO benchmark rather than performing multi-task fine-tuning. Joint imitation learning. Joint imitation learning strictly follows the training configuration of the aforementioned imitation learning, differing only in the data configuration, as it incorporates condition-masking data. We consider 3 variants of dropout data, (1) text-masking, (2) state-masking, and (3) both text&state-masking. For π0-FAST, we randomly dropout 10%/10%/10% (text / state / both text&state) in RoboCasa and LIBERO, and only dropout 10% of state data in SIMPLERWidowX. For OpenVLA, we apply 10% dropout only to text condition since OpenVLA does not receive state input. A.2 TRAINING ON REAL WORLD DATA Imitation learning. We use the official π0-FAST-DROID (Pertsch et al., 2025) checkpoint as the base model for real-world experiments. We fine-tune it on our manually collected dataset using 2 NVIDIA A100 GPUs with full fine-tuning for 10k steps. Training is performed with global batch size of 64 and an action chunk horizon of 10. Other configurations are fixed with the AdamW optimizer and cosine decay schedule, with warmup_steps = 300, peak_lr = 1e-5, decay_lr = 1e-6, and decay_steps = 30,000. Joint imitation learning. We follow the same training configuration as the above imitation learning setup, while additionally applying random dropout of 10%/10%/10% (text / state / both text&state). A.3 DEPLOYMENT MG-Selects main hyperparameters are: (1) the sampling temperature τ , (2) the number of candidate actions , (3) the variant of condition-masking, and (4) the regularization temperature for the condition-masking distribution. We search for the optimal configuration on each dataset within the text, state, text&state following ranges: τ , and regularization temperature , and report the best result for each } policy. , } 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16. 0.1, 0.3, 0.5, 0.7, 1.0 , variants { { { { 4, 8 } } For aggregating token-level confidence scores, we use the summation of the first 5 tokens by default in π0-FAST. In contrast, for OpenVLA, we use the average score across the entire token sequence, since its output sequence length is fixed to the action dimension of the training data. Additionally, only the text-masking variant is applied in OpenVLA, as it does not take state input."
        },
        {
            "title": "B DETAIL RESULTS ON SIMULATION EXPERIMENTS",
            "content": "Table 8: Detailed performance comparison on RoboCasa (Nasiriany et al., 2024). We report the average success rate (%) over 50 trials, trained with varying numbers of demonstrations. For clarity, the 24 tasks are grouped into three categories: pick-and-place, open-and-close, and others. Results for our methods are averaged over 3 random seeds, while baseline results are taken from the original paper (Bjorck et al., 2025). indicates results with additional joint training before applying our test-time scaling framework. indicates reproduced performance, and Model 30 Demos 100 Demos 300 Demos Pick and Place Open and Close Others All Pick and Place Open and Close Others All Pick and Place Open and Close Others All GR00T N1 π0-FAST + MG-Select (Ours) + MG-Select* (Ours) 0.4 5.3 7.2 14.2 26. 51.3 53.7 53.2 26.0 17.4 39.2 30.9 38.9 32.0 39.7 34.6 2.2 17.0 22.6 31.0 52. 60.7 63.2 67.3 43.5 32.1 46.6 40.2 48.9 43.7 50.1 48.1 22.6 43.2 46.5 46.9 68. 74.7 76.1 81.0 60.0 49.6 67.4 61.2 64.3 61.3 64.9 62.9 Table 9: Detailed performance comparison on SIMPLER-WidowX (Li et al., 2024). We report both the task success rate and the grasp success rate (%) over 24 trials on 4 pick-and-place tasks. Results for our methods are averaged over 3 random seeds, while baseline results are taken from the SIMPLER paper (Li et al., 2024) and the respective original papers (Liu et al., 2025; Qu et al., 2025). indicates results with additional joint training indicates reproduced performance, and before applying our test-time scaling framework. Model Spoon on Towel Carrot on Plate Stack Cubes Eggplant in Basket Average Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success RT-1-X Octo RoboVLM SpatialVLA π0-FAST + MG-Select* (Ours) 16.7 34.7 54.2 20.8 83.3 87.5 0.0 12.5 29.2 16.7 66.7 69. 20.8 52.8 25.0 29.2 83.3 83.3 4.2 8.3 25.0 25.0 70.8 75.0 8.3 31.9 45.8 62.5 91.7 79. 0.0 0.0 12.5 29.2 41.7 43.1 0.0 66.7 58.3 100.0 8.3 26.4 0.0 43.1 58.3 100.0 8.3 13. 11.5 46.5 45.8 53.1 66.7 69.1 1.1 16.0 31.3 42.7 46.9 50."
        },
        {
            "title": "C DETAIL RESULTS ON EFFICIENT DEPLOYMENT STRATEGY",
            "content": "Table 10: Detailed inference latency comparison on LIBERO-Object (Liu et al., 2023). This table presents the detailed results corresponding to Figure 3, comparing vanilla MG-Select and MG-Select with the single prefill strategy. We report the average inference latency over 10 episodes for each of 5 random seeds, across different numbers of candidates. Latency (s, ) MG-Select MG-Select + Single Prefill 20.2 30.4 43.4 62.0 76. 20.2 22.7 23.7 24.3 30.4 1 2 4"
        },
        {
            "title": "D LLM USAGE DISCLOSURE",
            "content": "We acknowledge the use of large language models (LLMs) in preparing this manuscript. LLMs were employed solely to refine writing quality, including grammar correction, vocabulary suggestions, and typographical checks. All substantive ideas, analyses, and conclusions in this paper are entirely the work of the authors."
        }
    ],
    "affiliations": [
        "KAIST",
        "RLWRLD",
        "Seoul National University"
    ]
}