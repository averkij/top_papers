{
    "paper_title": "Fast-SAM3D: 3Dfy Anything in Images but Faster",
    "authors": [
        "Weilun Feng",
        "Mingqiang Wu",
        "Zhiliang Chen",
        "Chuanguang Yang",
        "Haotong Qin",
        "Yuqi Li",
        "Xiaokun Liu",
        "Guoxin Fan",
        "Zhulin An",
        "Libo Huang",
        "Yulun Zhang",
        "Michele Magno",
        "Yongjun Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \\textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \\textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \\textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \\textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \\textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \\textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \\textbf{2.67$\\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D."
        },
        {
            "title": "Start",
            "content": "Fast-SAM3D: 3Dfy Anything in Images but Faster Weilun Feng * 1 2 Mingqiang Wu * 1 2 Zhiliang Chen 3 Chuanguang Yang(cid:66) 1 Haotong Qin 4 Yuqi Li 5 Xiaokun Liu 1 2 Guoxin Fan 1 2 Zhulin An(cid:66) 1 Libo Huang 1 Yulun Zhang 6 Michele Magno 4 Yongjun Xu 1 6 2 0 2 5 ] . [ 1 3 9 2 5 0 . 2 0 6 2 : r Figure 1. Fast-SAM3D accelerates the state-of-the-art single-view reconstruction model SAM3D (Chen et al., 2025) by up to 2.67, while maintaining the geometric fidelity and semantic consistency."
        },
        {
            "title": "Abstract",
            "content": "SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the first systematic investigation into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipelines inherent multi-level heterogeneity: the kinematic *Equal contribution 1Institute of Computing Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3School of Artificial Intelligence, China University of Mining & Technology, Beijing 4ETH Zurich 5City College of New York, City Univeristy of New York, USA 6Shanghai Jiao Tong University. Correspondence to: (cid:66)Zhulin An <anzhulin@ict.ac.cn>, (cid:66)Chuanguang Yang <yangchuanguang@ict.ac.cn>. Preprint. February 6, 2026. 1 distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present Fast-SAM3D, training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) Modality-Aware Step Caching to decouple structural evolution from sensitive layout updates; (2) Joint Spatiotemporal Token Carving to concentrate refinement on high-entropy regions; and (3) Spectral-Aware Token Aggregation to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to 2.67 end-to-end speedup with negligible fidelity loss, establishing new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/ wlfeng0509/Fast-SAM3D. Fast-SAM3D: 3Dfy Anything in Images but Faster 1. Introduction Unified 3D reconstruction models (Hunyuan3D et al., 2025; Xiang et al., 2025b; Wu et al., 2024) that recover highquality object-centric 3D assets from minimal user input are emerging as key foundation for scalable 3D perception and content creation. Among them, SAM3D (Chen et al., 2025) is distinctive in that it performs mask-conditioned, openworld multi-object reconstruction directly from single scene image, enabling practical reconstruction of arbitrary objects without category-specific training. However, this strong reconstruction quality and generalization comes with substantial computation overhead and severely hinders realworld deployment. In this work, we conduct the first systematic investigation into the inference characteristics of SAM3D. Our profiling reveals that latency is not uniformly distributed but dominated by three coupled components: the dual-stage iterative denoising (structure and texture) and the combinatorial complexity of decoding long token sequences. Crucially, we find that straightforward applications of generic acceleration techniques like uniform step skipping (Liu et al., 2025; Zhou et al., 2025) or random token pruning (Yang et al., 2025b; Bolya & Hoffman, 2023) are brittle for SAM3D. This arises from the multi-level heterogeneity inherent to the pipeline: (i) the kinematic distinctiveness between stable shape evolution and sensitive layout updates, where uniform skipping induces pose drift; (ii) the intrinsic sparsity of texture refinement, where uniform compute wastes resources on low-entropy surfaces; and (iii) the spectral variance across geometries, where instance-agnostic downsampling erases high-frequency details on complex shapes. These observations imply that accelerating SAM3D requires departure from isolated optimizations toward model-aware design. To this end, we present Fast-SAM3D, training-free, end-to-end acceleration framework derived from unified principle: allocate computation non-uniformly, matching stage-specific difficulty and instance-specific complexity. Fast-SAM3D instantiates this principle via three plug-andplay modules that seamlessly integrate into the inference pipeline: (1) Modality-Aware Step Caching for the structure generator, which disentangles caching rules to accelerate shape evolution while anchoring sensitive layout attributes; (2) Joint Spatiotemporal Token Carving for the latent generator, which eliminates redundancy by concentrating refinement compute solely on dynamically selected active regions; and (3) Spectral-Aware Token Aggregation for mesh decoding, which utilizes geometric spectral entropy to aggressively compress simple shapes while preserving details for complex geometries. Our contributions are summarized as follows: Systematic Profiling. We provide the first modulewise characterization of the SAM3D pipeline, identifying key latency sources and revealing why generic acceleration strategies fail due to kinematic and spectral heterogeneity. Holistic Framework. We propose Fast-SAM3D, unified, training-free framework that systematically accelerates the geometry, texture, and decoding stages by exploiting their specific redundancies. Adaptive Components. We design three lightweight modules: modality-aware caching, spatiotemporal token carving, and spectral-aware aggregation. Together, deliver substantial latency reduction while preserving reconstruction quality. Strong empirical results. Extensive experiments demonstrate significant end-to-end speedups across diverse objects and scenes with negligible degradation in reconstruction fidelity. 2. Related Works 3D Reconstruction and Generation. Early reconstruction methods focused on deterministically regressing representations like voxels (Xu et al., 2019; Wang et al., 2025c), point clouds (Mildenhall et al., 2021), or meshes (Worchel et al., 2022). While recent feed-forward transformers (Yang et al., 2025a; Wang et al., 2025b; 2024a) achieve rapid inference, but often struggles to high-fidelity generation. The paradigm has notably shifted toward diffusion models (Wang et al., 2025a), where explicit representations (Wu et al., 2024; Xiang et al., 2025b;a) offer superior topological control over implicit counterparts (Li et al., 2024; Yang et al., 2024). However, in the challenging single-view reconstruction, prior works (Lambert et al., 2025; Hunyuan3D et al., 2025; Geng et al., 2025) frequently fail under severe occlusions. SAM3D (Chen et al., 2025) overcomes this via mask-conditioned geometric priors for robust open-world reconstruction, yet its prohibitive inference latency remains significant barrier to interactive deployment. Efficient Generative Models. Acceleration strategies typically fall into training-based methods, such as distillation (Yin et al., 2024; Feng et al., 2025d;b; Dao et al., 2024) and quantization (Li et al., 2023; Feng et al., 2025c;e), or training-free approaches like step optimization (Lu et al., 2025; Ma et al., 2024; Liu et al., 2025) and token pruning (Bolya & Hoffman, 2023; Zou et al., 2024a). However, these techniques are primarily tailored for 2D domains, exploiting spatial smoothness while neglecting intrinsic 3D structural sparsity and geometric sensitivity. Existing 3D accelerators are either restricted to implicit fields (Yang et al., 2025c) or regression transformers (Feng et al., 2025a; 2 Fast-SAM3D: 3Dfy Anything in Images but Faster Figure 2. Overview of the proposed Fast-SAM3D framework. Our approach integrates three heterogeneity-aware modules designed to align computation with the specific dynamics of each stage: (Stage 1) Modality-Aware Step Caching disentangles the smooth evolution of shape tokens from the sensitive trajectory of layout tokens; (Stage 2) Joint Spatiotemporal Token Carving dynamically eliminates redundancy by concentrating refinement compute solely on high-entropy regions; and (Stage 3) Spectral-Aware Token Aggregation adapts the decoding grid density based on the instance-specific geometric complexity. Shen et al., 2025). Notably, Fast3DCache (Yang et al., 2025b) relies on multi-view redundancy, which is unavailable in single-view tasks. This highlights critical gap for training-free, system-level framework that accelerates singleview 3D diffusion by simultaneously leveraging temporal consistency and structural sparsity. 3. Preliminaries Table 1. Module-wise inference characteristics of SAM3D. SS Generator SLaT Generator Mesh Decoder Others Param (M) Token Length Inference Steps Inference Time (ms) FLOPs (T) 1033.63 5000 25 4090 95.757 600.43 26892 25 9720 219. 90.93 26335 13820 324.043 3370 SAM3D. SAM3D (Chen et al., 2025) takes an image and an object mask as input, and reconstructs the objects 3D shape S, texture , and layout parameters (R, t, s). Pipeline and components. As illustrated in Fig. 3a, SAM3D follows two-stage coarse-to-fine pipeline: it first predicts coarse structure and global layout, and then refines geometric details and synthesizes texture. Condition embedding encodes (I, ) into visual tokens using pretrained vision encoders (e.g., DINOv2 (Oquab et al., 2023)). Sparse Structure (SS) generator predicts coarse structural latent (voxel-like representation) and global layout (R, t, s) via iterative denoising. Sparse Latent (SLaT) generator conditions on (I, M, O), and iteratively refines appearance-related signals (e.g., texture/color) and fine-grained geometry. 3D decoders transform refined latents into explicit 3D outputs, including mesh decoder and 3D Gaussian splatting (GS) decoder. Computation characteristics and inference burden. Despite strong reconstruction quality and generalization, SAM3D incurs substantial inference overhead. Our profiling results in Tab. 1 and Fig. 3b show that the end-to-end latency is dominated by three components: (i) SS generator and (ii) SLaT generator, whose costs mainly arise from the iterative denoising steps; and (iii) mesh decoding path, which becomes expensive when decoding long structured 3D token sequences (e.g., 3D convolution over dense grids). These observations motivate Fast-SAM3D: holistic acceleration framework that jointly reduces the sampling cost in both diffusion stages and the decoding cost in 3D heads, while preserving reconstruction fidelity. 3 Fast-SAM3D: 3Dfy Anything in Images but Faster range, near-linear increments; (ii) vlayout is significantly more volatile, because they directly control the global coordinate frame and thus small errors can induce systematic drift. As result, applying uniform caching/prediction policy to all tokens is either unstable (for layout) or overly lagged (for shape). We therefore propose Modality-Aware Step Caching, which disentangles the update rules for vshape and vlayout. Finite-difference prediction for structural tokens. Leveraging the smooth evolution of vshape, we approximate the local trend using finite difference from two anchor evaluations: vshape = vshape vshape t+k , (1) where is the cache stride. When skipping backbone execution at step i, we extrapolate structural tokens using 1-order Taylor expansion (Kanwal & Liu, 1989): ˆvshape ti = vshape + (i) vshape . (2) Momentum-anchored smoothing for layout tokens. For layout tokens, naive extrapolation is brittle. We introduce an anchor-corrected prediction that blends linear trend with stable anchor from the most recent full backbone evaluation. Specifically, we first form linear extrapolation vlayout lin (t i) = vlayout + (i) vlayout , (3) and then apply momentum-anchored smoothing: ˆvlayout ti = β vlayout lin (t i) + (1 β) vlayout anchor, (4) where vlayout anchor is the last full compute layout token computed by fθ, and β [0, 1) is momentum coefficient. This anchoring suppresses high-frequency jitter and mitigates pose drift, improving temporal consistency of global 3D layout under caching. 4.2. Joint Spatiotemporal Token Carving and Adaptive Step Caching Motivation: intrinsic refinement sparsity. In the second stage, the Sparse Latent (SLaT) generator conditions on the coarse structure and performs iterative denoising to refine fine-grained geometry and appearance (e.g., color/texture). As shown in Fig. 5a, we observe systematic efficiency bottleneck, termed intrinsic refinement sparsity: token updates are strongly non-uniform across the latent field. Semantically simple regions (low-frequency, smooth surface areas) change slowly across denoising steps, while high-frequency regions (edges, seams, thin structures) exhibit persistent, larger-magnitude updates. Since geometry (a) SAM3D pipeline. (b) Key inference bottlenecks. Figure 3. Pipeline characterization and bottleneck analysis. (a) The standard two-stage coarse-to-fine architecture of SAM3D. (b) Latency scaling analysis revealing the dominant computational costs: the linear scaling of iterative denoising steps in the generators and the combinatorial complexity of processing dense voxel tokens in the mesh decoder. 4. Methods 4.1. Modality-Aware Step Caching for Sparse Structure Generator Notation. We follow the standard diffusion sampling convention (Peebles & Xie, 2023; Liu et al., 2025) where timesteps decrease from = (high noise) to = 0 (clean). We denote the current latent by xt, the diffusion backbone prediction by vt = fθ(xt, t), and skip of steps means reusing/predicting the model output at xt. Figure 4. Illustration of modality heterogeneity. comparison of update trajectories for shape tokens versus layout tokens. While shape tokens evolve along smooth path amenable to extrapolation, layout tokens exhibit high-frequency volatility. More analysis in Appendix Sec. B. Problem setting. In the first stage, the Sparse Structure Generator (SSG) synthesizes coarse 3D shape tokens vshape and layout tokens vlayout (encoding pose/translation/scale) through iterative denoising. We observe pronounced modality heterogeneity along the denoising trajectory as shown in Fig. 4: (i) vshape evolves relatively smoothly with short4 Fast-SAM3D: 3Dfy Anything in Images but Faster 4.2.2. DYNAMIC ADAPTIVE STEP CACHING Curvature-aware trajectory approximation. As shown in Fig. 5b, beyond spatial sparsity, the diffusion trajectory often contains quasi-linear regimes where the mapping changes smoothly. We estimate local nonlinearity via curvature proxy (Federer, 1959): κt = vt vt12 xt xt1 , (7) where xt is the input token and smaller κt indicates more stable regime in which the trajectory is well approximated by its tangent. Tangent update reuse. Let denote the most recent anchor step where the backbone is fully evaluated. We cache the tangent-like offset := vi xi, and, when skipping at step t, approximate ˆvt = xt + i. (8) (9) Error-bounded switching. To prevent uncontrolled drift, we define an instantaneous relative-change proxy εt = vt vt12 vt1 κi xt xt12 vt12 , (10) and accumulate it since the last anchor: Et = (cid:88) εn. n=i+1 (11) We trigger full backbone evaluation (refreshing the anchor) when Et E; otherwise we use the cached tangent update. Formally, vt = (cid:40) (cid:0)xt mt, t(cid:1), fθ xt + i, if Et , otherwise, (12) where mt = I(Jt Top-K) is the binary mask indicating the carved active tokens. (a) Saliency heatmap and corresponding real voxel change. (b) Feature difference trajectory visualization. Figure 5. Visualization of intrinsic refinement sparsity. (a) Real change map demonstrates that significant updates are spatially sparse, and our unified saliency map accurately predicts this pattern. (b) Temporal feature difference plots confirm the diffusion trajectory is non-uniform, validating our dynamic reusing strategy. More analysis in Appendix Sec. C. is largely established by O, much of the remaining computation is spent on correcting local high-entropy details, making uniform per-token computation redundant. This motivates adaptive computation in both space (which tokens to update) and time (which steps require full evaluation). 4.2.1. SPATIOTEMPORAL TOKEN CARVING Temporal saliency. Following the notation used in Sec. 4.1, let vt,i be the i-th output token at step t. We quantify per-token temporal activity using firstand secondorder variations: Mi(t) = vt,i2, Ai(t) = vt,i vt+1,i2, (5) where Mi(t) measures update magnitude and Ai(t) highlights abrupt changes. Spatial saliency. To prioritize tokens that contribute to geometric/texture details, we compute lightweight frequencybased complexity score Sfreq(i) using Fast Fourier Transform (FFT) (Nussbaumer, 1981) statistics, emphasizing tokens with stronger high-frequency structural skeleton. Unified importance and carving. We combine temporal and spatial cues into unified importance potential: Ji(t) = 1 2 (cid:0)Mi(t) + γ Ai(t)(cid:1) + 1 Sfreq(i), (6) and keep only the top-K tokens under Ji(t) as the active set at step t. This token carving aligns computation with instantaneous detail complexity, substantially reducing redundant updates on low-entropy regions. Figure 6. Visualization of instance-level spectral heterogeneity. Simple objects (left) exhibit sparse activations concentrated primarily along boundaries and mostly low frequency, whereas complex objects (right) display dense high-frequency components throughout the surface texture. More analysis in Appendix Sec. D. 5 Fast-SAM3D: 3Dfy Anything in Images but Faster Table 2. Quantitative comparison on SAM3D (Chen et al., 2025) benchmark. We evaluate reconstruction quality (Visual, Geometric, Layout) and inference speed (Scene Time for all objects in an image and Object Time for single object) against state-of-the-art acceleration baselines. Bold: the best result. Method SAM-3D Random Drop Uniform Merge Fast3Dcache TaylorSeer EasyCache Fast-SAM3D Visual Alignment Geometric Accuracy Layout Accuracy Uni3D CD F1@0.05 vIoU 3D-IoU ICP-rot 0.369 0.264 0.329 0.348 0.344 0.342 0.350 0.022 0.030 0.023 0.022 0.028 0.028 0.022 92. 83.52 91.48 91.31 90.95 87.06 92.59 0.543 0.327 0.540 0.505 0.504 0.432 0.552 0.403 0.094 0.367 0.051 0.374 0.186 0.375 19. 32.38 18.44 18.87 18.43 18.44 17.71 Scene Time(s) 462.3 402.2 366.8 443.3 265.6 244.9 229.7 Speed 1.00 1.15 1.26 1.04 1.74 1.89 2.01 Acceleration Object Time(s) Speed FLOPs(T) 31. 15.93 15.43 30.14 22.93 23.11 11.60 1.00 1.95 2.01 1.03 1.35 1.34 2.67 639.59 450.42 251.07 532.98 448.69 459.62 201.78 4.3. Spectral-Aware Dynamic Token Aggregation for Mesh Decoding Composite complexity and adaptive scheduling. We fuse 2D and 3D complexity into robust indicator: Motivation. In the final stage, the mesh decoder transforms the refined sparse latent manifold into mesh primitives, which is runtime-bottlenecked by the initial processing over large number of sparse 3D tokens. While uniform downsampling (Bolya & Hoffman, 2023) is straightforward remedy, it is instance-agnostic and often destroys fine-grained details for complex shapes. We thus seek an adaptive aggregation policy that conditions token reduction strength on the objects geometric complexity. Hjoint = H(M2D) + (1 w) H(V3D), (15) where [0, 1] balances boundary and volumetric cues. We then select an instance-adaptive downsampling factor via discrete schedule: = 1.25, Hjoint > τhigh, 1.50, 2.00, Hjoint < τlow, τlow Hjoint τhigh, (16) 4.3.1. SPECTRAL COMPLEXITY ANALYSIS where τlow and τhigh are thresholds. Key insight. We find that shapes geometric complexity correlates with the distribution of spectral energy as shown in Fig. 6: Simple objects are dominated by low-frequency components, while complex structures exhibit substantial high-frequency energy. This motivates spectral-aware policy that adaptively schedules token aggregation based on an instance-level complexity score. Dual-domain spectral metric. To capture both (i) boundary sharpness from the 2D silhouette and (ii) volumetric topology from the coarse 3D voxel grid, we compute joint spectral metric from the input mask and coarse structure O. Let M2D denote the 2D binary mask (from ), and let V3D denote the coarse voxel grid (from O). We transform both to the frequency domain: F2D = FFT2(M2D), F3D = FFT(V3D). (13) High-frequency energy ratio. For signal X, we define the high-frequency energy ratio (HFER) as 4.3.2. TOKEN AGGREGATION Coordinate quantization. Given 3D token centered at pi = (xi, yi, zi), we quantize its coordinates with factor S: ˆpi = (cid:107) . (cid:106) pi (17) Tokens mapped to the same quantized coordinate are grouped into voxel-aligned bin, yielding reduced token set whose size adapts to S. Feature aggregation. For each bin, we aggregate token features using permutation-invariant operator. To preserve the most prominent features of within each bin, we use max pooling: ˆz(ˆp) = maxpoolzii: ˆpi=ˆp zi, where zi is the latent feature of token and ˆz(ˆp) is the aggregated feature for bin ˆp. This reduces the token count by approximately 3 while preserving salient local features. (18) (cid:80) ωΩhigh H(X) = (cid:80) F(X)[ω]2 2 F(X)[ω]2 ωΩtotal where Ωhigh denotes frequencies above cutoff threshold, and Ωtotal denotes the full spectrum. Larger H(X) indicates richer high-frequency content and thus higher geometric complexity. , (14) 5. Experiments 5.1. Experimental Settings Datasets and Metrics. We adopt SAM3D (Chen et al., 2025) as our base framework. To comprehensively evaluate geometric accuracy and scene layout alignment, we conduct 6 Fast-SAM3D: 3Dfy Anything in Images but Faster Figure 7. Qualitative comparison of our proposed Fast-SAM3D and other methods. experiments on the Toys4K (Stojanov et al., 2021) and Aria Digital Twin (ADT) (Pan et al., 2023) datasets. We report standard metrics including Chamfer Distance (CD), F-Score, and Volumetric IoU (vIoU) following protocols in (Liu et al., 2023; Wang et al., 2024b), with meshes aligned via Iterative Closest Point (ICP). Additionally, we assess perceptual fidelity on the ISO3D (Ebert, 2025) dataset using the Uni3D score (Zhou et al., 2023) to measure cross-modal semantic consistency. Baselines. We benchmark our method against state-ofthe-art diffusion acceleration techniques, categorizing them into: (i) Token Caching: Fast3Dcache (Yang et al., 2025b); (ii) Step Caching: TaylorSeer (Liu et al., 2025) and EasyCache (Zhou et al., 2025). To further validate the efficacy of our proposed saliency-aware mechanisms, we also compare with naive variants, including Random Drop for token carving and Uniform Merge for mesh decoding. (1.35) and EasyCache (1.34), confirming that exploiting spatial redundancy via token carving is essential alongside temporal caching. Remarkably, Fast-SAM3D maintains or even exceeds the base models geometric fidelity (e.g., FScore: 92.59 vs. 92.34). We attribute this to the denoising effect of our saliency mechanism, which effectively prunes high-frequency noise inherent in the original full-token generation. Baseline Analysis. Comparisons highlight the limitations of existing methods in single-view settings. Fast3Dcache brings minimal acceleration (1.03) due to the lack of multi-view redundancy, while Random Drop suffers catastrophic structural collapse (3D-IoU drops to 0.094), proving that 3D structural information is non-uniform and requires our saliency-aware preservation. Fast-SAM3D successfully bridges these gaps, ensuring robust geometry with maximal end-to-end speedup. See Appendix Sec. for more detailed descriptions. 5.3. Visual Comparison 5.2. Main Results Results in Tab. 2 demonstrate that Fast-SAM3D achieves the optimal trade-off between efficiency and quality. Performance and Efficiency. Our method delivers significant acceleration, achieving 2.01 and 2.67 speedups for scene and object generation, respectively. This substantially outperforms pure step-skipping baselines like TaylorSeer Fig. 7 presents qualitative comparison across diverse categories. Fast-SAM3D (2nd Col.) produces results that are perceptually indistinguishable from the original model SAM3D (Chen et al., 2025), faithfully preserving both highfrequency textures (e.g., the wooden bird) and global topology. In contrast, generic strategies prove brittle. Random Drop (6th Col.) suffers from catastrophic structural collapse, validating that 3D sparsity requires intelligent saliencyaware retention rather than stochastic pruning. Furthermore, 7 Fast-SAM3D: 3Dfy Anything in Images but Faster step-skipping baselines like TaylorSeer (Liu et al., 2025) exhibit severe semantic drift, which completely alters object attributes (e.g., the blue shark turning orange in 5th Col.) due to the unstable evolution of layout tokens. Our method effectively circumvents these pitfalls by anchoring sensitive layout updates, ensuring consistent alignment with the input condition. We provide more visual comparisons in Appendix Sec. F. 5.4. Ablation Study We conduct comprehensive ablation study to validate the individual contributions of our acceleration modules and analyze the sensitivity of key hyperparameters. Table 3. Ablation study on different component acceleration. Acceleration SLaT Mesh CD F1@0.05 vIoU 0.022 0.022 0.022 0.022 0.021 0.022 0.022 0. 92.34 92.34 92.50 92.43 92.88 92.58 92.43 92.59 0.543 0.543 0.540 0.557 0.534 0.553 0.554 0.552 SS Scene Time(s) 462.33 408.64 365.86 320.41 310.54 289.92 301.34 229.68 Impact of Acceleration Components. Tab. 3 disentangles the effects of our three acceleration stages: ModalityAware Step Caching (SS), Spatiotemporal Token Carving (SLaT), and Spectral-Aware Token Aggregation (Mesh). The standalone application of Mesh acceleration yields significant latency reduction (462s 320s). This identifies the high-resolution mesh decoding process as the computational bottleneck in the original SAM-3D pipeline, justifying our aggressive spectral-aware aggregation strategy. Interestingly, applying SLaT not only reduces inference time (366s) but also improves geometric fidelity (F1 increases from 92.34 to 92.50). This corroborates our hypothesis that saliency-based carving acts as spatial filter, removing lowconfidence, noisy tokens that would otherwise degrade the final surface. Combining all three modules yields the optimal performance (230s), achieving 2.0 speedup without compromising the volumetric intersection over union (vIoU: 0.552 vs. 0.543). Table 4. Ablation study on momentum factor β in Eq. 4. β 1.0 0.9 0.7 0.5 0.3 CD F1@0.05 vIoU 3D-IoU ICP-rot 0.022 0.022 0.022 0.022 0.022 92.51 92.58 92.57 92.59 92.58 0. 0.551 0.550 0.552 0.551 0.375 0.375 0.375 0.375 0.375 17.87 17.97 17.83 17.77 17.71 Momentum Factor β (Layout Stability). Tab. 4 analyzes the layout update coefficient β. Unlike shape tokens, layout tokens exhibit high volatility. Pure linear extrapolation (β = 1.0) leads to suboptimal geometric metrics (F1: 92.51) due to cumulative drift. Decreasing β increases the influence of the anchor step. We find that β = 0.5 yields the peak performance in both fine-grained geometry (F1: 92.59) and volumetric completeness (vIoU: 0.552). This suggests that balanced trade-off that equally weighs the instantaneous linear trend and the stable anchor is crucial for suppressing high-frequency jitter in the global coordinate frame. Table 5. Ablation study on merging threshold τ in Eq. 16. {τlow, τhigh} CD F1@0.05 vIoU {0.0, 0.0} 0.0208 {0.5, 0.7} {0.5, 0.9} {0.3, 0.7} {0.3, 0.5} 0.0215 0.0216 0.0214 0.0213 92.884 92.585 92.590 92.527 92.497 0. 0.5521 0.5498 0.5500 0.5485 Scene Time(s) 250.98 229.68 226.64 239.02 237.26 Merging Thresholds τ (Speed-Fidelity Trade-off). Tab. 5 explores the adaptive grid settings {τlow, τhigh}. While the conservative setting {0.0, 0.0} without merging provides high upper bound for accuracy, it suffers from high latency (250.98s). Our selected configuration {0.5, 0.7} identifies the optimal Pareto point: it reduces inference time by 8.5% while remarkably achieving the highest vIoU (0.5521). This indicates that aggressive downsampling in spectrally simple regions effectively removes redundancy without compromising the objects topological integrity. We provide more analysis and ablation studies of cache stride in Appendix Sec. B, switching threshold and carving factor γ in Appendix Sec. C, and complexity weight in Appendix Sec. D. 6. Conclusion In this work, we identified and addressed the critical latency bottleneck hindering the interactive deployment of open-world 3D reconstruction frameworks. Through the first systematic analysis of SAM3Ds inference dynamics, we revealed that the inefficiency of generic accelerators stems from their inability to accommodate the intrinsic multi-level heterogeneity of 3D generation. In response, we presented Fast-SAM3D, training-free acceleration framework that dynamically harmonizes computational resources with instantaneous generation complexity. By synergizing modality-aware step caching, spatiotemporal token carving, and spectral-aware aggregation, our method successfully decouples structural evolution from redundant computation. Extensive experiments demonstrate that Fast-SAM3D achieves remarkable 2.67 speedup while preserving, and 8 Fast-SAM3D: 3Dfy Anything in Images but Faster in some cases enhancing, geometric fidelity. We hope this work establishes new baseline for efficient single-view 3D generation and inspires future research into heterogeneityaware optimization for complex diffusion pipelines."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bolya, D. and Hoffman, J. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 45994603, 2023. 2, 6 Chen, X., Chu, F.-J., Gleize, P., Liang, K. J., Sax, A., Tang, H., Wang, W., Guo, M., Hardin, T., Li, X., et al. arXiv preprint Sam 3d: 3dfy anything in images. arXiv:2511.16624, 2025. 1, 2, 3, 6, 7, 18 Dao, T., Nguyen, T. H., Le, T., Vu, D., Nguyen, K., Pham, C., and Tran, A. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In European Conference on Computer Vision, pp. 176192. Springer, 2024. 2 Ebert, D. 3d arena: An open platform for generative 3d evaluation. arXiv preprint arXiv:2506.18787, 2025. 7, 12 Federer, H. Curvature measures. Transactions of the American Mathematical Society, 93(3):418491, 1959. Feng, W., Qin, H., Wu, M., Yang, C., Li, Y., Li, X., An, Z., Huang, L., Zhang, Y., Magno, M., et al. Quantized visual geometry grounded transformer. arXiv preprint arXiv:2509.21302, 2025a. 2 Feng, W., Qin, H., Yang, C., An, Z., Huang, L., Diao, B., Wang, F., Tao, R., Xu, Y., and Magno, M. Mpqdm: Mixed precision quantization for extremely low bit diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1659516603, 2025b. 2 Feng, W., Qin, H., Yang, C., Li, X., Yang, H., Li, Y., An, Z., Huang, L., Magno, M., and Xu, Y. 2 qvdit: Accurate quantized video diffusion transformer with salient data and sparse token distillation. arXiv preprint arXiv:2508.04016, 2025c. 2 Feng, W., Yang, C., Qin, H., Li, X., Wang, Y., An, Z., Huang, L., Diao, B., Zhao, Z., Xu, Y., et al. Q-vdit: Towards accurate quantization and distillation of video-generation diffusion transformers. arXiv preprint arXiv:2505.22167, 2025d. 2 Feng, W., Yang, C., Qin, H., Li, Y., Li, X., An, Z., Huang, L., Diao, B., Zhuang, F., Magno, M., et al. Mpq-dmv2: Flexible residual mixed precision quantization for lowbit diffusion models with temporal distillation. arXiv preprint arXiv:2507.04290, 2025e. Geng, Z., Wang, N., Xu, S., Ye, C., Li, B., Chen, Z., Peng, S., and Zhao, H. One view, many worlds: Singleimage to 3d object meets generative domain randomization for one-shot 6d pose estimation. arXiv preprint arXiv:2509.07978, 2025. 2 Hunyuan3D, T., Yang, S., Yang, M., Feng, Y., Huang, X., Zhang, S., He, Z., Luo, D., Liu, H., Zhao, Y., et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. 2 Kanwal, R. and Liu, K. taylor expansion approach for solving integral equations. International Journal of Mathematical Education in Science and Technology, 20(3): 411414, 1989. 4 Lambert, N., Pyatkin, V., Morrison, J., Miranda, L. J. V., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. Rewardbench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 1755 1797, 2025. 2 Li, W., Liu, J., Yan, H., Chen, R., Liang, Y., Chen, X., Tan, P., and Long, X. Craftsman3d: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2 Li, X., Liu, Y., Lian, L., Yang, H., Dong, Z., Kang, D., Zhang, S., and Keutzer, K. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1753517545, 2023. Liu, J., Zou, C., Lyu, Y., Chen, J., and Zhang, L. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 2, 4, 7, 8, 13 Liu, M., Xu, C., Jin, H., Chen, L., Varma T, M., Xu, Z., and Su, H. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36:22226 22246, 2023. 7 9 Fast-SAM3D: 3Dfy Anything in Images but Faster Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pp. 122, 2025. 2 Ma, X., Fang, G., and Wang, X. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1576215772, 2024. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 Nussbaumer, H. J. The fast fourier transform. In Fast Fourier transform and convolution algorithms, pp. 80 111. Springer, 1981. 5 Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 Pan, X., Charron, N., Yang, Y., Peters, S., Whelan, T., Kong, C., Parkhi, O., Newcombe, R., and Ren, Y. C. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20133 20143, 2023. 7, 12 Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Shen, Y., Zhang, Z., Qu, Y., and Cao, L. Fastvggt: Trainingfree acceleration of visual geometry transformer. arXiv preprint arXiv:2509.02560, 2025. 3 Stojanov, S., Thai, A., and Rehg, J. M. Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17981808, 2021. 7, 12 Wang, C., Peng, H.-Y., Liu, Y.-T., Gu, J., and Hu, S.-M. Diffusion models for 3d generation: survey. Computational Visual Media, 11(1):128, 2025a. 2 Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., and Novotny, D. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025b. 2 Wang, R., Xu, S., Dai, C., Xiang, J., Deng, Y., Tong, X., and Yang, J. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52615271, 2025c. 2 Wang, S., Leroy, V., Cabon, Y., Chidlovskii, B., and Revaud, J. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024a. Wang, Z., Wang, Y., Chen, Y., Xiang, C., Chen, S., Yu, D., Li, C., Su, H., and Zhu, J. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In European conference on computer vision, pp. 5774. Springer, 2024b. 7, 12 Worchel, M., Diaz, R., Hu, W., Schreer, O., Feldmann, I., and Eisert, P. Multi-view mesh reconstruction with neural deferred shading. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 61876197, 2022. 2 Wu, S., Lin, Y., Zhang, F., Zeng, Y., Xu, J., Torr, P., Cao, X., and Yao, Y. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. Advances in Neural Information Processing Systems, 37:121859121881, 2024. 2 Xiang, J., Chen, X., Xu, S., Wang, R., Lv, Z., Deng, Y., Zhu, H., Dong, Y., Zhao, H., Yuan, N. J., et al. Native and compact structured latents for 3d generation. arXiv preprint arXiv:2512.14692, 2025a. 2 Xiang, J., Lv, Z., Xu, S., Deng, Y., Wang, R., Zhang, B., Chen, D., Tong, X., and Yang, J. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025b. 2 Xu, Q., Wang, W., Ceylan, D., Mech, R., and Neumann, U. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. Advances in neural information processing systems, 32, 2019. Yang, J., Sax, A., Liang, K. J., Henaff, M., Tang, H., Cao, A., Chai, J., Meier, F., and Feiszli, M. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2192421935, 2025a. 2 Yang, M., Yang, Y., Xu, C., Song, C., Zuo, Y., Zhao, T., Li, R., and Zhang, C. Fast3dcache: Training-free arXiv preprint 3d geometry synthesis acceleration. arXiv:2511.22533, 2025b. 2, 3, 7, 13 10 Fast-SAM3D: 3Dfy Anything in Images but Faster Yang, X., Shi, H., Zhang, B., Yang, F., Wang, J., Zhao, H., Liu, X., Wang, X., Lin, Q., Yu, J., et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 2 Yang, X., Liu, S., and Wang, X. Hash3d: Training-free In Proceedings of the acceleration for 3d generation. Computer Vision and Pattern Recognition Conference, pp. 2148121491, 2025c. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024. 2 Zhou, J., Wang, J., Ma, B., Liu, Y.-S., Huang, T., and Wang, X. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. 7, 12, 13 Zhou, X., Liang, D., Chen, K., Feng, T., Chen, X., Lin, H., Ding, Y., Tan, F., Zhao, H., and Bai, X. Less is enough: Training-free video diffusion acceleration via runtimeadaptive caching. arXiv preprint arXiv:2507.02860, 2025. 2, 7, 13 Zou, C., Liu, X., Liu, T., Huang, S., and Zhang, L. Accelerating diffusion transformers with token-wise feature caching. arXiv preprint arXiv:2410.05317, 2024a. 2 Zou, C., Zhang, E., Guo, R., Xu, H., He, C., Hu, X., and Zhang, L. Accelerating diffusion transformers with dual feature caching. arXiv preprint arXiv:2412.18911, 2024b. 13 Fast-SAM3D: 3Dfy Anything in Images but Faster A. Detailed Experimental Settings A.1. Data Preparation and Evaluation Protocols Geometry Evaluation (Toys4K). For fine-grained geometry assessment, we utilize the Toys4K dataset (Stojanov et al., 2021). We preprocess the data by rendering ground-truth meshes from randomly sampled viewpoints, applying background removal, and strictly filtering out low-quality samples. This results in curated test set of 600 unique views (one view per object). We evaluate the reconstruction quality using Chamfer Distance (CD), F-Score (F1@0.05), and Volumetric IoU (vIoU). Note that all generated meshes are rigidly aligned to the ground truth using Iterative Closest Point (ICP) before metric calculation to disentangle pose errors from shape reconstruction quality. Scene Layout Alignment (ADT). To evaluate the models ability to handle complex scene layouts, we select the Aria Digital Twin (ADT) dataset (Pan et al., 2023). We filter highly similar video frames and select four sequences that represent distinct scene types. From each sequence, we sample 4 key views, yielding 16 evaluation views in total. We report 3D IoU and rotation error (ICP-rot) to measure the alignment accuracy between the reconstructed scene and the ground truth. Perceptual Fidelity (ISO3D). For wild objects lacking 3D ground truth, we employ the ISO3D dataset (Ebert, 2025), consisting of 101 synthetic objects with 101 unique views. Since standard geometric metrics are inapplicable, we use the multi-modal Uni3D (Zhou et al., 2023) model to compute the perceptual similarity score. Specifically, we sample 8,192 points uniformly from the surface of each generated mesh to construct point cloud. We then calculate the cosine similarity between the point cloud embeddings and the input image embeddings, serving as proxy for shape-appearance consistency. A.2. Details of Evaluation Metrics We define the specific metrics used to assess geometric fidelity, structural alignment, and perceptual quality. In the following definitions, let denote the ground-truth 3D representation and denote the generated 3D representation. Chamfer Distance (CD, ). Chamfer Distance measures the bi-directional geometric discrepancy between the generated mesh and the ground truth. We sample = 10, 000 points from the surface of both meshes to obtain point clouds SP and SG. The metric is defined as: CD(SP , SG) = 1 SP (cid:88) xSP min ySG y2 2 + 1 SG (cid:88) ySG min xSP x2 2. (19) Lower CD indicates higher geometric fidelity and closer surface alignment. F-Score (F1@τ , ). While CD is sensitive to outliers, F-Score provides robust evaluation of reconstruction completeness and precision. It is the harmonic mean of precision and recall at specific distance threshold τ : F1(τ ) = 2 Precision(τ ) Recall(τ ) Precision(τ ) + Recall(τ ) , (20) where Precision(τ ) is the percentage of points in SP within distance τ of any point in SG, and vice versa for Recall(τ ). We set τ = 0.05 following (Wang et al., 2024b). Higher F-Score implies better surface coverage and fewer artifacts. Volumetric IoU (vIoU, ). To evaluate the holistic 3D structure and occupancy, we voxelize both the ground truth and generated meshes into resolution of 323. Volumetric IoU is calculated as the intersection over union of the occupied voxels: vIoU = , (21) VP VG VP VG where denotes the set of occupied voxels. This metric is particularly useful for assessing whether the model captures the correct global topology and volume. ICP Rotation Error (ICP-rot, ). For the ADT dataset, we focus on the alignment of the object/scene within the global coordinate system. We perform Iterative Closest Point (ICP) alignment and record the magnitude of the rotation component of the transformation matrix required to align the generated shape to the ground truth. lower ICP-rot error indicates that the model has successfully preserved the correct canonical pose and scene layout without significant drift. 12 Fast-SAM3D: 3Dfy Anything in Images but Faster Uni3D Score (Perceptual Fidelity, ). For the ISO3D dataset where ground-truth 3D models are unavailable, we utilize Uni3D (Zhou et al., 2023), scalable 3D foundation model aligned with CLIP image embeddings, to measure semantic consistency. We compute the cosine similarity between the embedding of the input image and the embedding of the generated point cloud P: ScoreUni3D = EI (I) EP (P) EI (I)2EP (P)2 , (22) where EI and EP are the image and point cloud encoders, respectively. higher score reflects better preservation of semantic attributes and visual appearance consistent with the input view. A.3. Implementation Details For fair comparison, all baselines are implemented on the same SAM3D backbone and all the experiments are conducted on single NVIDIA-A800. Fast-SAM3D: We adopt our proposed Fast-SAM3D by comprehensively accelerating SS Generator, SLaT Generator, and Mesh Decoder. In the SS Generator configuration, we set the cache stride to = 3 and the momentum factor to β = 0.5, with warmup period of 2 steps (common setup in diffusion step caching (Zou et al., 2024b; Liu et al., 2025; Zhou et al., 2025)). For the SLaT Generator, the switching threshold is set to = 1.5 with 2 warmup steps; additionally, the temporal carving factor is configured as γ = 0.7, and we cache top 0.1 tokens for spatiotemporal carving. Finally, regarding the merging thresholds, we use τlow = 0.5 and τhigh = 0.7, with = 0.9. Fast3Dcache (Yang et al., 2025b): Adapted for single-view inputs by removing its multi-view dependency while retaining its token caching mechanism. As Fast3Dcache targets reducing shape generation tokens, we adopt Fast3Dcache to the SS Generator. We follow the official settings (Yang et al., 2025b) for other parameters. TaylorSeer (Liu et al., 2025): TaylorSeer reduces diffusion sample steps using feature cache. We adopt TaylorSeer to both SS and SLaT Generator (following the same step caching protocol as ours). In both the SS and SLaT Generator configuration, we also adopt warmup period of 2 steps for fair comparison. EasyCache (Zhou et al., 2025): Similar to TaylorSeer, we also adopt EasyCache to both SS and SLaT Generator. Due to the collapsed performance, we use warmup period of 3 steps for the SS Generator and 2 warmup steps for SLaT Generator. We follow the official settings (Zhou et al., 2025) and maintain similar acceleration ratio as our step caching setting. Naive Baselines: Random Drop replaces our saliency-based carving (SLaT Generator) with random selection at the same sparsity ratio; Uniform Merge replaces our spectral-aware aggregation (Mesh Decoder) with fixed uniform grid downsampling = 2. B. More Analysis of Modality-Aware Step Caching In this section, we provide deeper investigation into the motivation behind our Modality-Aware Step Caching strategy and analyze the sensitivity of the caching stride k. B.1. Visualizing Trajectory Heterogeneity To empirically validate the modality heterogeneity assumption discussed in Sec. 4.1, we visualize the denoising trajectories of different token types in the latent space. Fig. 8 plots the evolution of representative Shape Tokens (vshape) and Layout Tokens (vlayout) across timesteps. Observation & Insight. As shown in the left column of Fig. 8, Shape Tokens exhibit highly smooth, quasi-linear evolution trajectory. The gradients between adjacent steps change gradually, forming predictable arc. This high temporal coherence justifies our use of Linear Extrapolation for structural tokens, as the local trend at step is reliable predictor for step k. In stark contrast, the right column reveals that Layout Tokens undergo volatile, non-monotonic fluctuations. The trajectory is characterized by high-frequency zig-zag oscillations. Applying naive linear extrapolation here would amplify these instantaneous fluctuations, leading to severe divergence. This observation strongly supports our design of Momentum-Anchored Smoothing, which utilizes the stable anchor from the backbone to dampen these high-frequency jitters while tracking the global pose evolution. 13 Fast-SAM3D: 3Dfy Anything in Images but Faster Figure 8. Visualization of latent trajectories. We visualize the evolution of randomly selected Shape Tokens (Left) and Layout Tokens (Right) during the denoising process. Shape tokens show smooth, predictable evolution, while layout tokens exhibit erratic, high-frequency oscillations, necessitating our Modality-Aware caching strategy. B.2. Sensitivity to Cache Stride We further analyze the impact of the cache stride (i.e., the interval between anchor updates) on generation quality and speed. The results are summarized in Tab. 6. Table 6. Ablation study on cache stride in Eq. 1. 2 3 4 5 CD F1@0.05 vIoU 3D-IoU ICP-rot Object Time(s) 0.0215 0.0215 0.0225 0.0236 92.587 92.585 91.862 90.994 0.5499 0.5521 0.5302 0. 0.3750 0.3750 0.2408 0.2331 17.958 17.715 16.630 16.559 11.85 11.60 11.39 11.17 Trade-off Analysis. Increasing naturally reduces the number of expensive backbone evaluations, linearly decreasing the object inference time. Short Stride (k = 2): While offering high fidelity (F1: 92.587), the speed gain is conservative. Optimal Stride (k = 3): We observe that = 3 achieves the sweet spot. It yields the highest volumetric completeness (vIoU: 0.5521) and optimal scene layout (3D-IoU: 0.3750) while reducing inference time to 11.60s. This suggests that within window of 3 steps, the local linearity assumption holds sufficiently well for our extrapolation method. Failure at Large Stride (k 4): When increases to 4, we observe catastrophic drop in layout accuracy (3D-IoU plummets from 0.3750 to 0.2408). This indicates that the interval has exceeded the valid radius of the first-order Taylor expansion approximation. The error in layout prediction accumulates rapidly over 4 steps, causing the object to drift significantly from the ground-truth pose. Consequently, we fix = 3 as the default setting for Fast-SAM3D to maximize efficiency without compromising structural stability. C. More Analysis of Joint Spatiotemporal Token Carving In this section, we provide visual verification of our saliency mechanisms predictive accuracy and analyze the sensitivity of the carving percentage K, adaptive switching threshold, and carving factor γ. C.1. Visualizing Saliency Effectiveness To validate the reliability of our token carving strategy, we compare our estimated importance score against the oracle ground truth. In Fig. 9, the Real Change (Left) represents the ground truth magnitude of the actual token updates at the next step (vt vt1), indicating where computation is truly needed. The Unified Saliency (Right) is our predicted 14 Fast-SAM3D: 3Dfy Anything in Images but Faster importance map (Jt) derived from spatiotemporal history. As observed in the visualization, our Unified Saliency map exhibits remarkable correlation with the Ground Truth change map: Accurate Localization: Our method correctly highlights the regions that subsequently undergo significant updates (e.g., the edges and complex topological structures), effectively capturing the active set of the diffusion process. Correctness of Prediction: The strong visual alignment confirms that our proposed metric, which combines first/second-order temporal dynamics with spatial frequency is highly accurate proxy for future variation. This ensures that we are not randomly pruning, but selectively preserving tokens that are mathematically destined to change, thereby guaranteeing geometric fidelity even under high sparsity. Figure 9. Validation of Saliency Prediction. We compare the Ground Truth update magnitude (Left) against our Predicted Unified Saliency map (Right). The high consistency between our prediction and the actual future change demonstrates the correctness of our saliency estimation, ensuring that critical computation regions are accurately identified. Table 7. Ablation study on token carving percentage in Eq. 12. top-K CD F1@0.05 vIoU 3D-IoU ICP-rot Object Time(s) top-5% 0.0215 top-10% 0.0215 top-20% 0.0256 92.603 92.585 90.254 0.5589 0.5521 0.5311 0.375 0.375 0.364 17.710 17.713 18. 12.15 11.60 10.91 C.2. Sensitivity to Carving Percentage Table 7 investigates the impact of the carving percentage (the ratio of tokens pruned per step) on the trade-off between reconstruction quality and inference speed. Benefit of Moderate Pruning (K 10%): Pruning the top-5% to 10% of least salient tokens yields sweet spot. At = 10%, the method maintains high geometric fidelity (F1: 92.585) comparable to the conservative = 5% setting, while achieving decent inference speed (11.60s). This confirms that moderate portion of the diffusion updates corresponds to redundant computations that can be safely removed without affecting the structural outcome. Unfavorable Trade-off (K = 20%): As we increase the carving ratio further, the cost-benefit ratio becomes unfavorable. While increasing from 10% to 30% does shorten the inference time (11.60s 10.91s), this marginal speedup comes at an inevitable cost to quality, where the F1-Score plummets from 92.58 to 90.25. This indicates that too aggressive pruning discards essential structural information, making the slight efficiency gain not may worth the degradation in fidelity. Consequently, we set = 10% as the default to maximize efficiency within the safe redundancy zone. C.3. Sensitivity to Switching Threshold The switching threshold in Eq. 12 governs the aggressiveness of our Adaptive Step Caching. It determines the tolerance for error accumulation before triggering full backbone refresh. Tab. 8 presents the ablation results. 15 Fast-SAM3D: 3Dfy Anything in Images but Faster Table 8. Ablation study on switching threshold in Eq. 12. 1.0 1.5 2. CD F1@0.05 vIoU 3D-IoU ICP-rot Object Time(s) 0.0216 0.0215 0. 92.575 92.585 92.526 0.5493 0.5521 0.5502 0.3750 0.3750 0.3750 17.610 17.713 17.982 11.97 11.60 11.46 Balance between Stability and Denoising. Conservative Threshold (E = 1.0): While stable, tight threshold triggers frequent anchor updates, limiting the speedup (11.97s). Interestingly, its F-Score (92.575) is slightly lower than the optimal setting, suggesting that strictly following every fluctuation of the original model may include unnecessary noise. Optimal Threshold (E = 1.5): This setting yields the best performance across geometry (F1: 92.585) and volume (vIoU: 0.5521). It indicates that allowing moderate degree of deviation effectively smooths out the diffusion trajectory (the denoising effect) while maintaining structural integrity. Aggressive Threshold (E = 2.0): Relaxing the threshold further improves speed (11.46s) but leads to noticeable drop in geometric accuracy (F1 drops to 92.526). This suggests that the accumulated error has exceeded the linear regime, causing the cached tangent approximation to drift from the true manifold. Therefore, we adopt = 1.5 as the default to maximize the quality-efficiency trade-off. Table 9. Ablation study on γ in Eq. 6. γ 0.0 0.5 0.7 1.0 Uni3D CD F1@0.05 3D-IoU 0.3373 0.3474 0.3503 0. 0.0215 0.0216 0.0215 0.0217 92.061 92.064 92.585 91.759 0.375 0.375 0.375 0.375 C.4. Sensitivity to Carving Factor γ Tab. 9 assesses the balance between update magnitude and abruptness in our saliency potential (Eq. 6). Setting γ = 0.7 achieves the best balance. Neglecting abrupt changes (γ = 0.0) misses critical turning points in the diffusion trajectory, lowering F1. However, over-emphasizing second-order variations (γ = 1.0) introduces noise sensitivity. The optimal value γ = 0.7 confirms that while the magnitude of evolution is the primary signal, incorporating derivative changes is essential for capturing transient high-frequency details. D. More Analysis of Spectral-Aware Dynamic Token Aggregation In this section, we provide the empirical motivation for our adaptive aggregation strategy and analyze the contribution of different spectral cues. D.1. Visualizing Spectral Heterogeneity The core premise of our mesh decoding acceleration is that geometric information is non-uniformly distributed across different object instances. To validate this, Fig. 10 visualizes the frequency spectrum distributions of Simple Object (e.g., smooth cylinder) and Complex Object (e.g., dragon). Spectrum-Information Gap. As shown in the visualization: Simple Objects (Left): The spectral energy is highly concentrated in the low-frequency band and decays rapidly. This indicates that the objects geometry is dominated by smooth surfaces with minimal high-frequency detail. Consequently, applying aggressive token aggregation (e.g., coarse grid) incurs negligible information loss while maximizing speed. Fast-SAM3D: 3Dfy Anything in Images but Faster Complex Objects (Right): In contrast, the spectrum exhibits long-tail distribution with significant energy persisting in higher frequencies. This signifies the presence of intricate topological details and sharp edges. uniform downsampling strategy would severely alias these high-frequency components, leading to the loss of fine geometry. This distinct spectral heterogeneity strongly justifies our Instance-Adaptive design: explicitly measuring the High-Frequency Energy Ratio (HFER) allows us to dynamically allocate computational budget (grid resolution) where it is structurally needed. Figure 10. Spectral Analysis of Geometric Complexity. We compare the Fourier spectrum of simple object (left) versus complex object (right). Simple objects show rapid energy decay, supporting aggressive aggregation, whereas complex objects retain significant high-frequency energy, necessitating finer token grid. D.2. Sensitivity to Complexity Weight Our complexity metric Hjoint fuses cues from the 2D silhouette and the coarse 3D voxel grid, balanced by weight w. Tab. 10 investigates the impact of this fusion. Table 10. Ablation study on in Eq. 15. 0.0 0.5 0.9 1.0 Uni3D CD F1@0.05 3D-IoU 0.3194 0.3163 0.3503 0.3474 0.0213 0.0215 0.0215 0.0211 92.557 92.552 92.585 92.103 0.375 0.375 0.375 0.375 The Role of 2D vs. 3D Cues. Failure of Pure 3D (w = 0.0): Relying solely on the coarse voxel grid results in the lowest perceptual score (Uni3D: 0.3194). This is expected because the input voxel from Stage 1 is inherently coarse and lacks the high-frequency surface details required for fine complexity estimation. Risks of Pure 2D (w = 1.0): While the 2D mask captures sharp boundaries, relying on it entirely (w = 1.0) neglects internal volumetric topology, leading to drop in geometric precision (F1 decreases to 92.103). Optimal Fusion (w = 0.9): The results show clear preference for 2D dominance (w = 0.9), achieving the best balance (Uni3D: 0.3503, F1: 92.585). This suggests that the high-resolution 2D input provides the primary signal for detail density, while the 3D voxel acts as necessary topological regularizer to prevent misjudging voluminous but smooth shapes. 17 E. Reproducibility Statement Fast-SAM3D: 3Dfy Anything in Images but Faster To enhance reproducibility, we have attached our necessary code and the generated raw mesh files in the supplementary material. F. More visual comparison Here, we provide more visual comparisons to demonstrate the effectiveness of our proposed Fast-SAM3D. The results are shown in Fig. 11 and Fig. 12. Figure 11. More visual comparison between Fast-SAM3D and existing methods. Figure 12. More visual comparison between Fast-SAM3D and original SAM3D (Chen et al., 2025)."
        }
    ],
    "affiliations": [
        "City College of New York, City Univeristy of New York, USA",
        "ETH Zurich",
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "School of Artificial Intelligence, China University of Mining & Technology, Beijing",
        "Shanghai Jiao Tong University",
        "University of Chinese Academy of Sciences"
    ]
}