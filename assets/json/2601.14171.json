{
    "paper_title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
    "authors": [
        "Qianli Ma",
        "Chang Guo",
        "Zhiheng Tian",
        "Siyu Wang",
        "Jipeng Xiao",
        "Yuanhao Yue",
        "Zhipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released."
        },
        {
            "title": "Start",
            "content": "Paper2Rebuttal: Multi-Agent Framework for Transparent Author Response Assistance Qianli Ma* Chang Guo* Zhiheng Tian*"
        },
        {
            "title": "Jipeng Xiao",
            "content": "Yuanhao Yue Zhipeng Zhang AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University {mqlqianli,zhipengzhang}@sjtu.edu.cn Project Page: https://Paper2Rebuttal.github.io HF Demo: https://huggingface.co/spaces/RebuttalAgent 6 2 0 2 0 2 ] . [ 1 1 7 1 4 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Writing effective rebuttals is high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as direct-to-text generation problem, suffering from hallucination, overlooked critiques, and lack of verifiable grounding. To address these limitations, we introduce REBUTTALAGENT, the first multiagents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, REBUTTALAGENT ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed REBUTTALBENCH and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering transparent and controllable assistant for the peer review process. Code will be released."
        },
        {
            "title": "Introduction",
            "content": "The rebuttal phase represents decisive juncture in the peer review lifecycle where authors must address critiques through evidence-backed clarifications and actionable manuscript revisions. This undertaking extends far beyond simple textual composition. It requires rigorous synthesis process in which authors must accurately decipher reviewer intent while ensuring every response is firmly anchored in verifiable manuscript details. The inherent difficulty of this multi-step reasoning is amplified by the strict turnaround windows typical of *Equal Contribution. Corresponding Author. 1 Figure 1: Overview of our work. Given manuscript and reviews, (a) direct text generation (SFT on peerreview corpora) often fabricates experiment results (b) Interactive promptand prone to hallucination. ing with chat-LLMs depends on manual concern feeding and many iterations. (c) RebuttalAgent reframes rebuttal writing as decision-and-evidence organization problem, performing concern breakdown, queryconditioned internal and external evidence construction, and strategy-level plan verification with human-in-theloop checkpoints before drafting the final response. top-tier venues. Authors are frequently forced to reconcile the need for meticulous verification with urgent deadlines, leaving little room for hallucination or ambiguity. In response to these intense cognitive and temporal demands, Large Language Models (LLMs) have emerged as promising assistants for scientific writing (Wang et al., 2024b) and peer-review communication (Gao et al., 2024; Zhu et al., 2025; Lu et al.). Current approaches generally fall into two paradigms. The direct-to-text generation paradigm typically involves models that are supervised finetuned (SFT) on paper-response pairs (Fig. 1a). While straightforward, this approach is fundamentally flawed because it trains models to memorize specific, non-transferable experimental outcomes rather than the underlying logic of formulating strategic response. Consequently, these models are prone to hallucination, frequently fabricating experimental results or over-commit to unverified claims instead of reasoning about the actual content of the manuscript. The second paradigm relies on interactive sessions with proprietary chat-LLMs such as GPT or Gemini (Fig. 1b). While these high-capability models can offer superior reasoning, the workflow is notoriously inefficient and opaque. Authors are forced to engage in lengthy, multi-turn prompting to guide the model, which consumes valuable time that could be spent on verification. Furthermore, critical intermediate steps like concern parsing and evidence retrieval remain concealed behind the chat interface. This lack of transparency makes the process difficult to audit and renders the output quality heavily dependent on the prompting expertise of the user. In this paper, we reframe rebuttal assistance as decision and evidence organization problem with explicit constraints, rather than the free-form text generation tasks. Specifically, reliable system must satisfy four critical requirements: (i) Comprehensive Coverage, tracking every reviewers concern without omission; (ii) Strict Faithfulness, adhering to the submitted manuscript without hallucinating technical details; (iii) Verifiable Grounding, linking major statements to specific internal passages or external references; and (iv) Global Consistency, maintaining unified stance and avoiding conflicting commitments across different responses. To instantiate this view, we propose REBUTTALAGENT, multi-agent system that enforces novel \"verify-then-write\" workflow to overcome the opacity of previous two paradigms, shown in Fig. 1c. Instead of rushing to generation, our architecture explicitly decouples reasoning from drafting by producing verifiable intermediate artifacts. The process begins by atomizing unstructured reviews into discrete concerns to guarantee comprehensive coverage, followed by dual-source evidence construction phase that synthesizes high-fidelity manuscript passages and citation-ready external briefs to strictly ground every claim. Crucially, we introduce strategic planning stage that audits the response logic for global consistency and commitment safety before any text is drafted, ensuring that concessions made to one reviewer do not contradict the overall stance. By exposing these structured artifacts through human-in-the-loop checkpoints, REBUTTALAGENT transforms rebuttal writing from black-box generation task into transparent, authorcontrolled collaboration. We evaluate REBUTTALAGENT through an author-centric lens, prioritizing practical usability and reliability over mere text fluency. Specifically, we assess performance across four rigorous dimensions: coverage of reviewer concerns, traceability of evidence sources, global coherence of the argumentative stance, and overall argumentation quality. Experimental results on our proposed benchmark demonstrate that our pipeline consistently outperforms previous \"direct-to-text\" baselines and chat-LLMs on these critical metrics. By delivering structured, verifiable assistance, REBUTTALAGENT significantly reduces the cognitive burden of rebuttal writing while ensuring authors remain the ultimate arbiters of their scientific defense. Our contributions are: We formulate rebuttal assistance as decision-and-evidence organization problem and propose REBUTTALGENT, multiagent system with explicit verification and humanin-the-loop checkpoints. We introduce concernconditioned context construction and on-demand evidence synthesis to produce point-specific, verifiable support under realistic context limits. We construct benchmark and establish an authorcentric evaluation protocol, demonstrating that our approach outperforms baselines in coverage, traceability, and coherence."
        },
        {
            "title": "2 Related Works",
            "content": "LLM Agents. LLMs (OpenAI, 2025; Team et al., 2023) were initially valued for fluent generation, but real deployments revealed mismatch between writing well and completing complex tasks reliably. When goals require multi-step planning, fresh evidence, and interaction with external systems, purely parametric generation can accumulate errors and hallucinations, motivation shift toward intelligent agents embedded in dynamic, goaldirected frameworks that plan and act with external tools and environments. Recent work (Yao et al., 2023b,a) shows that combining reasoning traces with concrete actions (e.g., search tool) improves robustness in long-horizon tasks and reduces hallucinations. Modern agents often incorporate deliberation and search (Wei et al., 2023; Wang et al., 2024b), learned tool-use policies (Schick et al., 2023; Patil et al., 2023), and memory or reflection from execution feedback (Shinn et al., 2023; Zhang et al., 2024). Multi-agent frameworks further enable role specialization and structured collabora2 Figure 2: Overview of RebuttalAgent. Given manuscript (PDF) and reviewer comments, the system (1) structures inputs by parsing and compressing the paper with fidelity checks and extracting atomic reviewer concerns with coverage checks; (2) builds concern-conditioned evidence by constructing query-specific hybrid manuscript context and, when needed, retrieving and summarizing external literature into citation-ready briefs; and (3) generates an inspectable, evidence-linked response plan that is checked for consistency and commitment safety, incorporates optional author feedback, and is then realized into formal rebuttal draft. tion (Wang et al., 2024a; Wu et al., 2023; Ma et al., 2025b; Lu et al.; DArcy et al., 2024), while benchmarks such as AgentBench (Liu et al., 2025b), WebArena (Zhou et al., 2024), and GAIA (Mialon et al., 2023) evaluate real-world tool use and end-toend task success. These advances motivate extending agentic systems from conducting research to communicating it, e.g., retrieving evidence, orgnizing words and iteratively refining rebuttals. AI Assisted Peer Review. Peer review stands as the cornerstone of research quality yet faces significant strain from the exponential growth in conference submissions. This pressure has catalyzed the adoption of LLMs to maintain efficiency and decision reliability across the review pipeline (Gao et al., 2024; Lu et al.; Zhu et al., 2025; Zhang et al., 2025). Within this process, the author rebuttal phase holds unique value for rectifying misunderstandings and influencing borderline decisions (Gao et al., 2019). To operationalize this complex interaction, researchers have developed datasets like DISAPERE (Kennard et al., 2021) and APE (Cheng et al., 2020) for argument alignment alongside comprehensive corpora like Re2 (Zhang et al., 2025). While recent efforts employ argumentative strategies (Purkayastha et al., 2023) or multiagent simulations (Yu et al., 2025; Jin et al., 2024) to model this workflow, they predominantly treat rebuttal generation as single-step prompt-to-text task. As illustrated in Fig. 1a, these methods overlook the critical need for explicitly decomposing concerns and planning evidence-based responses."
        },
        {
            "title": "3 RebuttalAgent",
            "content": "REBUTTALAGENT operates as multi-agent framework that transforms the rebuttal process into structured and inspectable workflow. By generating evidence-linked intermediate artifacts before drafting the final text, the system ensures that the output remains grounded and controllable. Fig. 2 illustrates how the architecture decomposes complex reasoning into specialized agents paired with lightweight checkers. This design exposes critical decision points and allows authors to retain full responsibility for the strategic stance and final wording. The pipeline initiates by distilling the manuscript into structured summary and extracting atomic reviewer concerns to enable stable long-context reasoning (Sec. 3.1). Guided by these atomic concerns, the system constructs evidence bundles by retrieving relevant high-fidelity excerpts from the original manuscript and augmenting them with verifiable external literature via web search (Sec. 3.2). The workflow concludes by synthesizing an explicit response plan that outlines the arguments and evidence links. Authors refine this plan through human-in-the-loop mechanism before the system produces formal rebuttal letter compliant with academic conventions (Sec. 3.3). 3 3.1 Manuscript and Review Structuring The pipeline commences by distilling the raw manuscript and reviews into condensed representations optimized for downstream reasoning. This approach addresses the dual challenges of efficiency and controllability as effective rebuttals demand repeated access to fine-grained evidence scattered throughout the paper. Since processing the full manuscript directly is often costly and brittle due to context limitations, our compact format minimizes token overhead while improving retrieval precision. It serves as navigational anchor that allows subsequent modules to selectively access high-fidelity excerpts from the original text only when precise evidence is necessary. Dense Manuscript to Compact Representation. The transformation begins as parser agent converts the manuscript PDF into paragraph-indexed format to preserve structural integrity and facilitate targeted lookups. compressor agent subsequently distills these paragraphs into concise representation that retains essential technical statements and experimental results. This compact view functions as the primary retrieval surface and enables the system to match reviewer concerns to relevant sections with minimal token usage. To safeguard against silent information loss, consistency checker verifies each condensed unit against its source and automatically triggers reprocessing if it detects missing claims or semantic drift. Complex Reviews to Actionable Atomic Concerns. Operating in parallel with manuscript processing, an extractor agent parses raw feedback into discrete and addressable atomic concerns. This component organizes the critiques by grouping related sub-questions and assigning preliminary categories. coverage checker subsequently validates the output for intent preservation and appropriate granularity to guarantee that substantive points remain distinct without being over-split or incorrectly merged. The resulting structured list forms the foundational unit for the subsequent evidence gathering and response planning stages."
        },
        {
            "title": "3.2 Evidence Construction",
            "content": "With the atomic concerns established, the system generates targeted evidence bundles to ensure that every argument remains traceable to specific facts. This strategy contrasts sharply with the direct generation approaches depicted in Fig. 1a that bypass explicit grounding. By prioritizing evidence construction over immediate text generation, our pipeline anchors each concern in verifiable sources and ensures that the downstream planning and drafting stages operate on validated information. Atomic Concern Conditioned Hybrid Context. The system identifies the most pertinent sections by searching within the compressed manuscript representation (Sec.3.1) for each atomic concern. It then selectively expands these focal points by retrieving the corresponding raw text to replace the specific condensed units while retaining the rest of the document in its summarized form. This approach yields an atomic concern conditioned hybrid context that integrates the efficiency of the compressed view with the precision of the original text. Such structure enables the system to support its reasoning with exact quotations and detailed evidence without overwhelming the context window. On-Demand External Evidence. While the hybrid context effectively grounds responses in the authors own work, certain reviewer critiques necessitate evidence beyond the manuscript boundaries. To address scenarios such as novelty disputes or requests for broader positioning where internal data is insufficient, the system augments the evidence bundle with external support. search planner initiates this expansion by formulating targeted search strategy, while subsequent retrieval step gathers candidate papers via scholarly search tools1. screening agent then filters these candidates for relevance and utility to ensure high-quality input. The pipeline concludes this phase by parsing the selected works into structured evidence brief that highlights key claims and experimental comparisons to provide citation-ready material for the subsequent planning and drafting stages."
        },
        {
            "title": "3.3 Planning and Drafting",
            "content": "A critical failure of the direct-to-text pipeline is its tendency to hallucinate experimental results when addressing empirical critiques. Our system overcomes this by implementing bifurcated reasoning strategy that strictly distinguishes between interpretative defense and necessary intervention. For concerns resolvable through existing data, the Strategist Agent synthesizes arguments directly from the hybrid context and anchors them in the manuscript text. In contrast, when the system detects demand for new experiments or baselines, it explicitly inhibits result generation and instead produces 1https://export.arxiv.org/api/query 4 Figure 3: RebuttalBench statistics and rubric design. (a) Word-cloud and top-word histogram of reviews in REBUTTALBENCH-CORPUS, highlighting recurring reviewer emphases (e.g., clarity, novelty, reproducibility). (b) Motivated by these signals, REBUTTALBENCH evaluates rebuttals with rubric that mirrors these concerns, scoring Relevance, Argumentation Quality, and Communication Quality rather than fluency alone. concrete Action Items framed as recommendations (see cases in App. E). This design prevents the fabrication of outcomes by forcing structural pause where authors must verify or perform the suggested tasks. The resulting plan serves as an interactive human-in-the-loop checkpoint that allows authors to actively refine the strategic logic rather than merely accepting or rejecting proposals. Users can modify the scope of action items or correct the reasoning path to ensure the strategy aligns perfectly with their capabilities and intent. Only after the author validates these strategic decisions does the Drafter Agent convert the plan into final response to ensure that every claim remains grounded in reality. Optionally, the drafter can also produce submission-style rebuttal draft from the validated plan, but it renders any yet-tobe-conducted experiments as explicit placeholders (e.g., [TBD]). Authors can then fill in these placeholders after completing the recommended action items, keeping the draft faithful."
        },
        {
            "title": "4 RebuttalBench",
            "content": "Standard evaluation metrics for text generation fail to capture the strategic nuance and factual precision required in peer review rebuttals. Therefore, we introduce REBUTTALBENCH as specialized benchmark derived from real-world OpenReview interactions. This dataset moves beyond simple text-to-text pairs by curating high-quality reviewresponse dyads to ensure technical density and argumentative complexity. We complement the data with multidimensional evaluation framework that prioritizes content coverage and evidence traceability over surface-level fluency. Unlike generic instruction-following benchmarks, our protocol specifically measures how well system identifies atomic concerns and grounds its counter-arguments in verifiable facts. This allows us to quantify the gap between the hallucination-prone outputs of standard models and the structured reasoning produced by our pipeline."
        },
        {
            "title": "4.1 Evaluation Dataset",
            "content": "Data source. To evaluate rebuttal assistance with an observable post-rebuttal signal, we curated dataset of peer-review discussion threads from the publicly available ICLR OpenReview forum. Each instance in our benchmark pairs an initial reviewer critique with the corresponding author rebuttal and crucially includes the reviewers follow-up response. We leverage the subsequent reviewer reaction as decisive classification signal to partition the dataset into positive and negative samples for evaluation purposes. Positive instances are identified by follow-up comments confirming that all concerns were resolved while negative samples consist of cases where the reviewer indicated that the rebuttal failed to address the core issues. Filtering and corpus construction. Starting from the raw peer-review discussion threads, we apply automatic filtering to retain instances with sufficiently explicit follow-up signals and remove ambiguous cases. To obtain broad and reliable evaluation pool, we apply automatic filtering to retain instances with sufficiently explicit followup signals and discard ambiguous cases. This yields REBUTTALBENCH-CORPUS, broad pool of 9.3K review-rebuttal pairs used for analysis and evaluation setup. (see Appendix A). To form focused and challenging benchmark for standardized comparison, we construct REBUTTALBENCHCHALLENGE by ranking papers according to the number of instances that exhibit both positive and 5 negative follow-up signals, and selecting the top 20 papers with over 100 reviewers. This strategy maximizes within-paper diversity of resolved and unresolved concerns, producing compact test suite with realistic interaction patterns. Data statistics. Fig. 3 summarizes corpus-level characteristics of REBUTTALBENCH-CORPUS. Beyond basic length and interaction statistics, we visualize reviewer language with word cloud and top-words histogram, shown in Fig. 3a. Frequent terms such as clarity, quality, correct(ness), reproducibility, novelty, and experiments indicate that reviewers repeatedly emphasize exposition, claim support, and scientific rigor; these axes are also explicitly reflected in standard review forms used in OpenReview venues. Accordingly, our rubricbased evaluation is designed to align with these recurring concerns by scoring relevance/coverage to reviewer points, strength of evidence-backed argumentation, and communication quality (e.g., clarity and professionalism), demonstrated in Fig. 3b."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "To systematically measure rebuttal response quality beyond surface fluency, we use an LLM-asjudge (Zheng et al., 2023; Lin and Chen, 2023) rubric with fine-grained 0-5 scale. The evaluation framework covers three complementary dimensions: Relevance (R-Score), Argumentation Quality (A-Score), and Communication Quality (C-Score). Each dimension contains three components (9 total). We calculate the average component scores within each dimension and then compute the final score. Full component rubrics and judge prompts are provided in Appendix B. R-Score evaluates the extent to which the response addresses reviewer concerns with point-specific precision. It rewards outputs that cover all major points without omission and demonstrate correct interpretation of the critique while favoring concrete actions over generic assurances A-Score measures the strength of the justification behind each claim. It requires arguments to be logically consistent and supported by appropriate evidence from the manuscript or external sources. The metric prioritizes substantive rebuttals that engage with the underlying critique rather than offering superficial restatements. C-Score captures the quality of communication and professional conduct. It assesses whether the response maintains respectful tone and presents information with clear structure and unambiguous language. The metric ensures the text remains constructive to facilitate productive discussion between the reviewer and the author. In addition to scalar scores, the evaluator outputs brief structured diagnosis (strengths, weaknesses, and suggested improvements) for qualitative analysis. Detailed scoring standards (0-5 anchors) and implementation are provided in Appendix B."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We assess the efficacy of REBUTTALAGENT by comparing it with strong closed-source LLM baselines and by ablating key components of the system. For scalable and controlled benchmarking, all experiments in the main paper run REBUTTALAGENT in fully automated mode without human intervention. While human-in-the-loop checkpoints can further improve reliability and author control, they are impractical for batch evaluation at scale. Accordingly, the reported results should be viewed as conservative lower bound on the systems performance under real-world usage. Baselines. We consider four SOTA LLMs as baselines: GPT-5-mini (OpenAI, 2025), Grok-4.1fast (xGr), Gemini-3-Flash (Team et al., 2023), and DeepSeekV3.2 (Liu et al., 2025a). For each baseline model, we evaluate direct-to-text generation setting where the model produces rebuttal conditioned on the manuscript and reviewer comments. To ensure fair comparison, we also instantiate REBUTTALAGENT with the same model as its foundation backbone, keeping inputs and outputs identical across conditions; differences therefore reflect the contribution of our structured pipeline rather than the underlying model choice. Implementation Details. To ensure controlled and fair comparisons, we evaluate REBUTTALAGENT and each closed-source baseline under matched model backbones. For every baseline LLM (e.g., GPT-5-mini (OpenAI, 2025)), we instantiate REBUTTALAGENT with the same LLM as its backbone, so that both the baseline and REBUTTALAGENT consume the same manuscript and reviewer comments and produce responses in an identical point-by-point format. Differences therefore reflect the contribution of the structured workflow rather than model capacity. All experiments in the main paper run RebuttalAgent in fully automated mode, and we keep decoding settings consistent across conditions for each backbone. Finally, we 6 Table 1: Main evaluation results across our full suite of RebuttalBench. Results demonstrate promising improvements of our method against the baseline LLM. Method DeepSeekV3.2 RebuttalAgent-DeepSeekV3. Grok4.1-fast RebuttalAgent-Grok-4.1-fast Gemini3-Flash RebuttalAgent-Gemini3-Flash GPT5-mini RebuttalAgent-GPT5-mini Coverage 3.65 4.43 (+0.78) 3.98 4.66 (+0.68) 4.00 4.51 (+0.51) 3.61 4.34 (+0.73) Relevance Semantic Alignment 4.44 4.82 (+0.38) 4.58 4.92 (+0.34) 4.71 4.88 (+0.17) 4.22 4.84 (+0.62) Specificity 3.28 4.39 (+1.11) 3.72 4.65 (+0.93) 3.77 4.49 (+0.72) 2.96 4.29 (+1.33) Argumentation Quality Communication Quality Logic Consistency Evidence Support Response Engagement Professional Tone Statement Clarity Suggestion Constructiveness 3.44 3.86 (+0.42) 3.73 4.13 (+0.40) 3.71 4.11 (+0.40) 3.37 3.78 (+0.41) 3.01 3.23 (+0.22) 3.32 3.42 (+0.10) 3.30 3.39 (+0.09) 2.92 3.31 (+0.39) 3.16 3.79 (+0.63) 3.60 4.15 (+0.55) 3.56 4.07 (+0.51) 3.07 3.70 (+0.63) 3.37 3.60 (+0.23) 3.48 3.68 (+0.20) 3.51 3.78 (+0.27) 3.35 3.60 (+0.25) 3.96 4.18 (+0.22) 4.05 4.23 (+0.18) 4.08 4.28 (+0.20) 3.95 4.21 (+0.26) 3.81 4.06 (+0.25) 3.92 4.24 (+0.32) 3.95 4.09 (+0.14) 3.91 4.24 (+0.33) Average 3.57 4.08 (+0.51) 3.82 4.25 (+0.43) 3.85 4.23 (+0.38) 3.48 4.05 (+0.57) adopt Gemini-3-Flash (Team et al., 2023) as unified LLM judge for all systems and ablations. Full prompt templates and evaluation prompts are provided in Appendix and Appendix D. 5.2 Main Results Obs. 1: RebuttalAgent consistently outperforms strong closed-source LLMs. As shown in Tab. 1, under fair comparisons where REBUTTALAGENT and LLM baselines share the same base models, REBUTTALAGENT achieves consistent improvements across all evaluation dimensions on REBUTTALBENCH. The largest gains are observed in Relevance and Argumentation Quality. Across matched base models, REBUTTALAGENT improves coverage by up to +0.78 for DeepSeekV3.2 and specificity by up to +1.33 for GPT5-mini, and strengthens argumentation with up to +0.63 higher rebuttal quality. Improvements in Communication Quality are smaller but consistent, suggesting that the gains mainly come from structured decision making and evidence organization rather than surface-level fluency. Notably, these gains are achieved without changing the language model, indicating that performance improvements stem from task decomposition and structured intermediate reasoning rather than stronger generative capacity. This suggests that rebuttal quality is bottlenecked less by surface fluency and more by systematic concern tracking, evidence grounding, and response planning. These factors that are poorly handled by direct-to-text prompting even with SOTA LLMs. Obs. 2: The benefit of RebuttalAgent is larger for weaker base models. Tab. 1 also suggests that the weaker the base model, the larger the improvement obtained from our agent pipeline. While all advanced LLMs benefit from our RebuttalAgent, the margin over direct-to-text prompting is more pronounced for smaller or less capable backbones (e.g., GPT5-mini) than for stronger ones (e.g., Gemini-3-Flash). Using the mean score averaged over all nine components as summary, the weakest backbone GPT5-mini gains about +0.55 on average, whereas stronger proprietary backbones (e.g., Gemini-3-Flash) gain smaller margin (+0.33). The same pattern is particularly clear on Relevance. GPT5-mini improves by roughly +0.89 on the relevance sub-scores (coverage, semantic alignment, and specificity), compared to about +0.47 for Gemini-3-Flash. This indicates that explicit concern structuring, evidence construction, and response planning can partially compensate for limited base-model capability, shifting performance bottlenecks from raw generation to decision and evidence organization. Obs. 3: RebuttalAgent yields balanced improvements across the full rebuttal pipeline. Beyond isolated metric gains, Tab. 1 shows that REBUTTALAGENT improves all three dimensions in coordinated way across matched base models. For example, under Gemini-3-Flash, REBUTTALAGENT raises Relevance (coverage from 4.00 to 4.51; specificity from 3.77 to 4.49), strengthens Argumentation Quality (logic consistency from 3.71 to 4.11; rebuttal quality from 3.56 to 4.07), and also improves Communication Quality (professional tone from 3.51 to 3.78; statement clarity from 4.08 to 4.28). similar across-the-board improvement pattern holds for other backbones, suggesting that the benefits are not localized to single stage, such as evidence insertion or phrasing. Instead, structuring concerns and grounding claims early supports downstream planning and drafting, leading to more coherent and constructive final responses."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Ablation setting. To understand the contribution of each intermediate artifact, we perform controlled ablations by removing one module at time from the full ebuttalAgent pipeline while keeping the base model, prompts, and evaluation protocol fixed. Specifically, we consider three variants: (i) w/o Input Structuring, where reviewer concerns are not explicitly decomposed and merged but handled in raw form; (ii) w/o Evidence Construction, where external literature retrieval and citation-ready evi7 Table 2: Ablation study on key components. We remove each module from the full system: Input Structuring, Evidence Construction, and Checker. Metric Relevance Coverage Semantic Alignment Specificity Argumentation Quality Logic Consistency Evidence Support Response Engagement Communication Quality Professional Tone Statement Clarity Suggestion Constructiveness RebuttalAgent Structing Evidence Checker w/o Component 4.51 4.88 4.49 4.11 3.39 4.07 3.78 4.28 4.09 4.49 (-0.02) 4.71 (-0.17) 4.46 (-0.03) 4.06 (-0.05) 3.23 (-0.16) 4.04 (-0.03) 3.69 (-0.09) 4.33 (+0.05) 4.06 (-0.03) 4.26 (-0.25) 4.73 (-0.15) 4.19 (-0.30) 4.05 (-0.06) 3.32 (-0.07) 3.97 (-0.10) 3.74 (-0.04) 4.22 (-0.06) 3.82 (-0.27) 4.54 (+0.03) 4.89 (+0.01) 4.47 (-0.02) 4.13 (+0.02) 3.39 (+0.00) 4.01 (-0.06) 3.73 (-0.05) 4.29 (+0.01) 4.05 (-0.04) dence briefs are disabled; and (iii) w/o Checkers, where plan-level verification for coverage, evidence linkage, and cross-point consistency is removed. All variants still produce complete rebuttal drafts, allowing us to isolate how each module affects response quality rather than system completeness. Obs. 4: External evidence briefs are the most Critical Artifact, while structuring and checkers provide more targeted benefits. Tab. 2 shows that Evidence Construction is the most critical intermediate artifact. Removing external evidence briefs leads to the largest and most consistent degradation across dimensions, with clear drops in Relevance and Communication Quality. In particular, Coverage decreases from 4.51 to 4.26 and constructiveness falls from 4.09 to 3.82, indicating that citation-ready evidence plays central role in enabling specific, actionable, and convincing responses rather than generic assurances. These degradations indicate that citation-ready evidence briefs are central to producing point-specific and constructive responses. Although the effects are smaller, Input Structuring and Checkers also contribute measurably to overall quality. Without structuring, multiple metrics decline, including semantic alignment (4.88 to 4.71) and evidence support (3.39 to 3.23), suggesting that explicit concern decomposition and stable manuscript representations help preserve intent understanding and evidence linkage. Without checkers, we observe degradations in key quality dimensions such as evidence support (3.39 to 3.33) and rebuttal quality (4.07 to 4.01), indicating that lightweight verification remains beneficial even when base responses are fluent. Overall, the ablation results indicate that the gains of REBUTTALAGENT arise from the combination of complementary modules. Evidence-centered artifacts act as the primary driver of quality improvements, while explicit structuring and verification provide guardrails that reduce error accumulation. 5.4 Case Study We also provide cases that directly compare REBUTTALAGENT with strong LLM baselines on representative reviewer concerns in Appendix E. Rather than emphasizing the final rebuttal prose, these examples highlight the intermediate artifacts that RebuttalAgent surfaces to authors: an explicit response strategy, evidence-linked clarification points, and concrete action items (e.g., targeted edits, and suggested experiments or additional) that can be verified before any claims are finalized. Obs. 5: Action items reduce hallucination and over-commitment. In the shown cases, reviewers either question potential contradiction in key proposition or criticize the clarity and rigor of the theoretical presentation. RebuttalAgent first produces an inspectable plan that separates interpretative defense (what can be clarified using manuscript content) from necessary intervention (what requires additional evidence). Crucially, when new experiments or analyses are implicated, RebuttalAgent does not generate results; instead, it outputs concrete deliverables (e.g., revised exposition, new proof sketch) and scoped to-do list, as described in Sec. 3.3. By contrast, baseline outputs tend to respond with short narrative that may be overly confident or implicitly commit to empirical claims without exposing the underlying reasoning and verification steps. Overall, these cases illustrate how REBUTTALAGENT supports author decision-making by making the reasoning path and required work explicit before drafting, enabling authors to validate or edit the plan and keep final commitments grounded."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed RESPONSEAGENT, multi-agent framework for rebuttal assistance that constructs structured, evidence-linked intermediate artifacts before drafting text. By decomposing rebuttal writing into concern structuring, query-conditioned context building, on-demand external evidence synthesis, and response planning, the system improves traceability and cross-point coherence while keeping authors responsible for final decisions and wording. We also introduced an author-centric benchmark and rubric-based evaluation that measures relevance, global coherence, and argumentation quality beyond text fluency. Experimental results on our benchmark show that REBUTTALAGENT improves the key requirements of reliable 8 rebuttal assistance, highlighting the benefits of transparent verify-then-write workflow that reduces cognitive burden while keeping authors in control of the final wording."
        },
        {
            "title": "References",
            "content": "Gemini Deep Research - your personal research https://gemini. assistant gemini.google. google/overview/deep-research. [Accessed 2204-2025]. Grok 4 xAI x.ai. https://x.ai/news/grok-4. [Accessed 15-10-2025]. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738. Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, and Luo Si. 2020. Ape: Argument pair extraction from peer review and rebuttal via multi-task learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 70007011. Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. 2024. Marg: Multi-agent review arXiv preprint generation for scientific papers. arXiv:2401.04259. Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, and Yusuke Miyao. 2019. Does my rebuttal matter? insights from major nlp conference. arXiv preprint arXiv:1903.11367. Zhaolin Gao, Kianté Brantley, and Thorsten Joachims. Reviewer2: Optimizing review generaarXiv preprint 2024. tion through prompt generation. arXiv:2402.10886. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, and 1 others. 2025. Pasa: An llm agent for comprehensive academic paper search. arXiv preprint arXiv:2501.10120. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. 2024. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. 2024. Agentreview: Exploring peer review dynamics with llm agents. In EMNLP. Neha Kennard, Tim OGorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, and Andrew McCallum. 2021. Disapere: dataset for discourse structure in peer review discussions. arXiv preprint arXiv:2110.08520. Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, and Ang Chen. 2025. Curie: Toward rigorous and automated scientific experimentation with ai agents. arXiv preprint arXiv:2502.16069. Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, and Chenhao Tan. 2024. Literature meets data: synergistic approach to hypothesis generation. arXiv preprint arXiv:2410.17309. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, and 3 others. 2025b. Agentbench: Evaluating llms as agents. Preprint, arXiv:2308.03688. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Kai Lu, Shixiong Xu, Jinqiu Li, Kun Ding, and Gaofeng Meng. Agent reviewers: Domain-specific multimodal agents with shared memory for paper review. In Forty-second International Conference on Machine Learning. Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Led-merging: Mitiand Jing Shao. 2025a. gating safety-utility conflicts in model merging arXiv preprint with location-election-disjoint. arXiv:2502.16770. Qianli Ma, Siyu Wang, Yilin Chen, Yinhao Tang, Yixiang Yang, Chang Guo, Bingjie Gao, Zhening Xing, Yanan Sun, and Zhipeng Zhang. 2025b. Humanagent collaborative paper-to-page crafting for under $0.1. Preprint, arXiv:2510.19600. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. Preprint, arXiv:2311.12983. OpenAI. 2025. Gpt-5 system card. Technical report. Available at: https://cdn.openai.com/gpt-5-systemcard.pdf. 9 Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. Preprint, arXiv:2305.15334. Sukannya Purkayastha, Anne Lauscher, and Iryna Gurevych. 2023. Exploring jiu-jitsu argumentation for writing peer review rebuttals. arXiv preprint arXiv:2311.03998. Chandan Reddy and Parshin Shojaee. 2025. Towards scientific discovery with generative ai: Progress, In Proceedings of opportunities, and challenges. the AAAI Conference on Artificial Intelligence, volume 39, pages 2860128609. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Preprint, arXiv:2302.04761. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Jamshid Sourati and James Evans. 2023. Accelerating science with human-aware artificial intelligence. Nature human behaviour, 7(10):16821696. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Henrik Voigt, Kai Lawonn, and Sina Zarrieß. 2024. Plots made quickly: An efficient approach for generating visualizations from natural language queries. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1278712793, Torino, Italia. ELRA and ICCL. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024a. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6). Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, and 1 others. 2024b. Autosurvey: Large language models can automatically write surveys. Advances in Neural Information Processing Systems, 37:115119115145. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation. Preprint, arXiv:2308.08155. Yang Wu, Yao Wan, Hongyu Zhang, Yulei Sui, Wucai Wei, Wei Zhao, Guandong Xu, and Hai Jin. 2024. Automated data visualization from natural language via large language models: An exploratory study. Proceedings of the ACM on Management of Data, 2(3):128. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, and Jiaxuan You. 2025. Researchtown: Simulator of human research community. Preprint, arXiv:2412.17767. Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, and Yang Yang. 2025. Re2: consistency-ensured dataset for fullstage peer review and multi-turn rebuttal discussions. arXiv preprint arXiv:2505.07920. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024. survey on the memory mechanism of large language model based agents. Preprint, arXiv:2404.13501. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: realistic web environment for building autonomous agents. Preprint, arXiv:2307.13854. Minjun Zhu, Yixuan Weng, Linyi Yang, and Yue Zhang. 2025. Deepreview: Improving llm-based paper review with human-like deep thinking process. arXiv preprint arXiv:2503.08569."
        },
        {
            "title": "A Evaluation Dataset",
            "content": "To construct robust benchmark for evaluating rebuttal effectiveness, we derive our data from the RE2 dataset (Zhang et al., 2025), focusing on the ICLR 2023 subset (approximately 9,310 entries). We process this corpus through four-stage pipeline: 1. Outcome-based Classification: We first categorize entries into Improved (review score or acceptance status increased) and Unimproved groups based on the final decision. 2. Reliability-based Stratification: To ensure data quality, we subdivide these groups into three tiers based on evidence objectivity and LLM confidence: Tier 1 (Gold Standard) comprises cases with objective score increases (initial = final) or explicit revision statements; Tier 2 (High Confidence) includes instances without score changes but where an LLM identifies sentiment with high certainty ( 0.7); and Tier 3 (Medium Confidence) covers more ambiguous cases with moderate confidence (0.4 conf < 0.7). 3. Ground Truth Curation: From this stratified data, we curate balanced test set of 20 representative papers, prioritizing those with high review volumes to ensure diverse coverage of both positive and negative review samples across tiers. 4. Baseline Generation Protocol: For each paper, the baseline runs multi-round rebuttal generation following the author-reviewer dialogue. Each round uses fixed prompt (including intent, required format, and guardrails), concatenating the paper text, the current review, and an optional prior-round abstract. The rebuttal is then summarized into factual abstract with fewer than 200 words to seed the next round, and all outputs and token usage are logged."
        },
        {
            "title": "B Evaluation Metric",
            "content": "This section describes our rubric-based scoring protocol and how scores are aggregated. We adopt fine-grained 0-5 rating scheme, allowing for halfpoint increments to capture nuanced differences in response quality beyond prior binary judgments. Dimensions and weights. Our final score is weighted combination of three dimensions: RScore, A-Score, and C-Score, as mentioned in Sec. 4.2. Each dimension is decomposed into three components (9 components in total): R1 Coverage, R2 Semantic Alignment, R3 Specificity; A1 Logic Consistency, A2 Evidence Support, A3 Response Engagement; C1 Professional Tone, C2 Clarity, C3 Constructiveness. Relevance (R-Score). This dimension measures whether and how well the author addresses the reviewers concerns. R1 Coverage: Evaluates whether the response addresses all major points raised by the reviewer. R2 Semantic Alignment: Checks if the response directly answers the specific type of question asked (e.g., how vs. what). R3 Specificity: Measures the precision and granularity of the response (e.g., explicitly referencing specific equations or table rows vs. generic statements). Argumentation (A-Score). This dimension measures whether the author provides logically sound and substantively supported arguments. A1 Logic Consistency: Evaluates whether the logical chain is sound, coherent, and free from fallacies. A2 Evidence Support: Assesses the strength and verifiability of the backing proofs (e.g., new experimental data or rigorous derivations vs. vague promises). A3 Response Engagement: Evaluates whether the author demonstrates genuine understanding of the reviewers underlying concerns. Communication (C-Score). This dimension measures how effectively and professionally the author communicates their response. C1 Professional Tone: Evaluates whether the author maintains respectful and non-defensive tone. C2 Clarity: Measures writing quality and logical organization to ensure the response is easy to parse. C3 Constructiveness: Evaluates the commitment to improvement, specifically looking for actionable steps rather than vague commitments. Scoring protocol. For each review-response instance, we query an LLM judge to assign 05 score to every component and return brief justification for each score, as well as short overall 11 diagnosis (e.g., strengths, weaknesses, suggested improvements). The exact judge prompt, output schema, and the full 0-5 anchored criteria for each dimension/component are provided in Appendix D. Aggregation. Let Ri, Ai, Ci {0, . . . , 5}, {1, 2, 3} be the component scores. We compute dimension scores by averaging the three components, for example: = R1 + R2 + R3 3 , where means R-Score. The final weighted score is: Score = + + 3 . In the main paper, we report the overall weighted score and provide per-dimension and percomponent breakdowns for analysis, detailed in Sec. 5."
        },
        {
            "title": "C Related Works",
            "content": "Automatic Scientific Research. growing line of work studies how agentic LLM systems can automate substantial portions of the scientific workflow (Lu et al., 2024; Yamada et al., 2025; Ma et al., 2025a; goo). These systems have been used to streamline literature review and survey writing (Wang et al., 2024b; He et al., 2025), propose hypotheses from prior evidence (Liu et al., 2024; Sourati and Evans, 2023), and support research ideation and framing (Hu et al., 2024; Baek et al., 2024; Reddy and Shojaee, 2025). They are also expanding toward execution-facing stages, including experiment planning (Kon et al., 2025) and automatic generation of scientific visualizations and figures (Voigt et al., 2024; Wu et al., 2024), with early efforts extending to peer-review workflows (Zhu et al., 2025; Gao et al., 2024; Jin et al., 2024; Lu et al.). Sakana AIs AI Scientist (Lu et al., 2024; Yamada et al., 2025) further illustrates the trajectory toward closed-loop, end-to-end research automation. Building on this trajectory, we focus on more high-stakes stage of the research lifecycle, the rebuttal phase, where responses must precisely track reviewer intent while remaining verifiably grounded in manuscript evidence."
        },
        {
            "title": "D Prompt Templates",
            "content": "In this section, we present the prompt templates used by each component of our system, including those used for LLM-as-Judge evaluation. 12 Prompt: Rebuttal Strategist You are the Lead Rebuttal Strategist. Your goal is to dissect reviews for paper (based on the [compressed paper]) and create structured list of actionable tasks (Issues) for the authors. INPUT DATA: [compressed paper]: The summary of the authors work. [review original text]: Comments from multiple reviewers (R1, R2, R3...). MULTI-ROUND CONTEXT (if present): The input may include \"Previous Discussion Context\" showing earlier rounds of author rebuttals and reviewer responses. For follow-up rounds, focus on extracting NEW issues or unresolved concerns raised by the reviewer in the current round. Do NOT re-extract issues that have already been addressed in previous rebuttals unless the reviewer explicitly states dissatisfaction. If the reviewers current comment acknowledges previous responses positively (e.g., am satisfied with the response), there may be few or no new issues to extract. CORE TASKS: 1. Deconstruct: Break down long, complex paragraphs into atomic technical points. 2. Filter: Discard generic praise or non-actionable comments (see Blacklist). 3. Consolidate: Merge issues that represent the same core objection and can be addressed with the same response logic. 4. Format: Output strictly according to the traceability requirements. CRITICAL RULES FOR MERGING & SPLITTING (The \"Granularity\" Logic) Do NOT Merge (Split them): Different Evidence Needed: If R1 asks for \"Comparison with Baseline X\" and R2 asks for \"Comparison with Baseline Y\", these are two separate issues. Why? Because you need to run two different experiments. Different Aspects: If R1 criticizes \"Novelty\" and R2 criticizes \"Clarity of writing\", do NOT merge them just because they are generic complaints. Compound Questions: If single sentence says \"The method is slow AND the accuracy is low\", split this into two points: (1) Efficiency/Speed, (2) Performance. Do Merge: Same Question, Different Phrasing: R1: \"Why did you use L1 loss?\" vs R2: \"Justification for the loss function is needed.\" Merge. Same Missing Reference: R1 and R3 both ask to cite \"Smith et al. 2023\". Merge. General Confusion: R1: \"Section 3 is hard to follow\" and R2: \"I dont understand the methodology workflow\". Merge into \"Clarity of Section 3/Methodology\". NOISE FILTERING (BLACKLIST) Ignore: \"Ethics\", \"Confidence\", \"Summary\", \"Soundness\" (unless specific flaws are listed). Ignore: Generic praise (\"Good paper\", \"Interesting idea\"). Ignore: Empty templates (\"No ethical concerns\"). MANDATORY TRACEABILITY & FORMAT For each distinct issue, output block wrapped in tags [qN] and [qN] (where is the index). Structure within each block: (1) Issue: concise, professional summary of the problem. CRITICAL: If reviewers mentioned specific papers/links, you MUST include the full titles/links here. 13 (2) Sources: Verbatim quotes proving this issue exists. Format: ReviewerID-Type (Line/Para): \"Quote\". Use semicolons to separate multiple reviewers. (3) Paper hooks: Specific Sections, Equations, Figures, or Tables in the original paper related to this issue (e.g., Sec. 3.2, Eq. 5). Use \"Global\" for general issues. (4) Priority: P1 (Critical): Fatal flaws, missing baselines, wrong math, rejection reasons. P2 (Important): Clarity issues, missing citations, minor experiments. P3 (Minor): Typos, formatting, optional suggestions. OUTPUT EXAMPLE (Strictly Follow This) [q1] (1) Issue: Lack of comparison with state-of-the-art method [LoRA]. (2) Sources: R1-W2 (line 23): \"no comparison with parameter-efficient methods like LoRA\"; R3Q1 (para 2): \"how does this compare to LoRA?\" (3) Paper hooks: Sec.4.2, Tab.2 (4) Priority: P1 [q1] [q2] (1) Issue: The motivation for using Mutual Information (MI) in Eq. 3 is unclear. (2) Sources: R2-Q3 (line 47): \"why choose MI for layer mapping?\"; R1-W3 (para 5): \"mapping details not explained\" (3) Paper hooks: Sec.3.2, Eq.(3) (4) Priority: P2 [q2] Strictly follow the example format; do not include any other content! Prompt: Rebuttal Strategist Checker You are the Lead Rebuttal Strategist. Your goal is to dissect reviews for paper (based on the [compressed paper]) and create structured list of actionable tasks (Issues) for the authors. INPUT DATA: [compressed paper]: The summary of the authors work. [review original text]: Comments from multiple reviewers (R1, R2, R3...). MULTI-ROUND CONTEXT (if present): The input may include \"Previous Discussion Context\" showing earlier rounds of author rebuttals and reviewer responses. For follow-up rounds, focus on extracting NEW issues or unresolved concerns raised by the reviewer in the current round. Do NOT re-extract issues that have already been addressed in previous rebuttals unless the reviewer explicitly states dissatisfaction. If the reviewers current comment acknowledges previous responses positively (e.g., am satisfied with the response), there may be few or no new issues to extract. CORE TASKS: 1. Deconstruct: Break down long, complex paragraphs into atomic technical points. 2. Filter: Discard generic praise or non-actionable comments (see Blacklist). 3. Consolidate: Merge issues that represent the same core objection and can be addressed with the same response logic. 4. Format: Output strictly according to the traceability requirements. 14 CRITICAL RULES FOR MERGING & SPLITTING (The \"Granularity\" Logic) Do NOT Merge (Split them): Different Evidence Needed: If R1 asks for \"Comparison with Baseline X\" and R2 asks for \"Comparison with Baseline Y\", these are two separate issues. Why? Because you need to run two different experiments. Different Aspects: If R1 criticizes \"Novelty\" and R2 criticizes \"Clarity of writing\", do NOT merge them just because they are generic complaints. Compound Questions: If single sentence says \"The method is slow AND the accuracy is low\", split this into two points: (1) Efficiency/Speed, (2) Performance. Do Merge: Same Question, Different Phrasing: R1: \"Why did you use L1 loss?\" vs R2: \"Justification for the loss function is needed.\" Merge. Same Missing Reference: R1 and R3 both ask to cite \"Smith et al. 2023\". Merge. General Confusion: R1: \"Section 3 is hard to follow\" and R2: \"I dont understand the methodology workflow\". Merge into \"Clarity of Section 3/Methodology\". NOISE FILTERING (BLACKLIST) Ignore: \"Ethics\", \"Confidence\", \"Summary\", \"Soundness\" (unless specific flaws are listed). Ignore: Generic praise (\"Good paper\", \"Interesting idea\"). Ignore: Empty templates (\"No ethical concerns\"). MANDATORY TRACEABILITY & FORMAT For each distinct issue, output block wrapped in tags [qN] and [qN] (where is the index). Structure within each block: (1) Issue: concise, professional summary of the problem. CRITICAL: If reviewers mentioned specific papers/links, you MUST include the full titles/links here. (2) Sources: Verbatim quotes proving this issue exists. Format: ReviewerID-Type (Line/Para): \"Quote\". Use semicolons to separate multiple reviewers. (3) Paper hooks: Specific Sections, Equations, Figures, or Tables in the original paper related to this issue (e.g., Sec. 3.2, Eq. 5). Use \"Global\" for general issues. (4) Priority: P1 (Critical): Fatal flaws, missing baselines, wrong math, rejection reasons. P2 (Important): Clarity issues, missing citations, minor experiments. P3 (Minor): Typos, formatting, optional suggestions. OUTPUT EXAMPLE (Strictly Follow This) [q1] (1) Issue: Lack of comparison with state-of-the-art method [LoRA]. (2) Sources: R1-W2 (line 23): \"no comparison with parameter-efficient methods like LoRA\"; R3Q1 (para 2): \"how does this compare to LoRA?\" (3) Paper hooks: Sec.4.2, Tab.2 (4) Priority: P1 [q1] [q2] (1) Issue: The motivation for using Mutual Information (MI) in Eq. 3 is unclear. (2) Sources: R2-Q3 (line 47): \"why choose MI for layer mapping?\"; R1-W3 (para 5): \"mapping details not explained\" (3) Paper hooks: Sec.3.2, Eq.(3) (4) Priority: P2 [q2]"
        },
        {
            "title": "Revision Task",
            "content": "15 Your students have already carried out the initial extraction of questions based on the review comments as per the above requirements, as shown in [students output]. His extraction is very likely to have some omissions. Please carefully check for any omissions and make necessary revisions to improve the quality, and output the final version. Do not include any comments on the students in your final output! You only need to output the final version! Strictly follow the example format; do not include any other content! Prompt: Literature Retrieval Assistant You are literature-retrieval assistant for the rebuttal stage of an academic paper. Your task is to decide, based on the [compressed paper] and the [review_question], whether external reference papers need to be searched, and to generate appropriate search queries. When Search Is Required You must generate search queries when any of the following conditions occur: 1. The reviewer explicitly mentions reference papers. 2. The review_question contains specific method names or dataset names that are not from the current paper. 3. The reviewer requests compare with / ablation on / baseline Z. 4. The content of the paper is insufficient to answer the question. When Search Is NOT Required If the paper_summary already contains evidence that can directly answer the reviewers question (e.g., existing experiments, tables, section explanations), or the question concerns only minor formatting issues, then no search is needed. Search Query Generation Rules Generate less than 5 queries, keeping the number as small as possible. But if the reviewers provide the title of the reference article or links, then you should keep them all. Use topic phrases; never fabricate paper titles or authors. If reviewers provided the reference paper names or links directly, you can directly use them. If reviewers provided both title and link for an article, it is only necessary to provide the link. That is to say, either the link or the title can only appear once, and the link has higher priority. Please note that the links can only be obtained from the reviewers comments and must not be fabricated. Queries for comparative experiments must contain method names or dataset names. query contains one main query point. If there are different query points, please separate them and do not mix them together. Reference Output Format (strict JSON) ```json { \"need_search\": true, \"queries\": [ \"domain adaptation segmentation Cityscapes\", \"unsupervised domain adaptation transformer baseline\" ], \"links\": [ \"https://arxiv.org/abs/2409.13074v1\", \"https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_VisionLanguage-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf\" ], \"reason\": \"Reviewer requests additional comparisons related to domain adaptation on 16 Cityscapes and transformer baselines.\" } ``` ```json { \"need_search\": false, \"queries\": [], \"links\": [], \"reason\": \"there is no need to search, because... \" } ``` Strictly follow the example format; do not include any other content! Prompt: Rebuttal Expert You are rebuttal expert. You need to complete high-quality rebuttal for paper. You need to understand the papers information and the reviewers question from [compressed paper] and [review_question]. Now your less-than-intelligent assistant has retrieved some relevant papers using keywords, and their reasoning is shown in [query reason]. You need to carefully examine the abstracts of these papers, filter out irrelevant papers and those that are not very helpful for the rebuttal, and identify papers that are highly relevant to [compressed paper] and [review_question] and are extremely useful for the rebuttal. Your standards are very high. You should only keep these references if they are of great help to the rebuttal of the current problem. Papers that are merely related and not particularly significant must be rejected. You cannot allow insignificant papers to interfere with the overall rebuttal. Strict Rules: Generally no more than 6 papers (fewer is better; if no paper is of significant help, select none, unless the reviewers comments explicitly mention specific papers to reference, in which case you must include all of them. Please note that the links to the references provided by the reviewers in the review comments will be checked by dedicated person. You dont need to pay attention to the articles that have the links; only the papers that only have the titles need your attention.) For every candidate paper in your reasoning field, you must provide: ID Title and brief description of the abstract 1. How it helps the rebuttal of the current problem (brief description in one paragraph) Anti-Redundancy (with explanation): If multiple papers come from the same source or use the same method, only keep the most relevant one. You must output your result in the following JSON format: { } \"selected_papers\": [1,3,6], \"reason\": \"...\" The selected_papers array should contain the paper IDs. If no paper is useful, output: { \"selected_papers\": [], \"reason\": \"...\" } Ensure that the papers you return are objectively highly relevant to the original paper and significantly helpful for the rebuttal! Be rigorous! Ensure that you only output valid JSON, without any additional text before or after. Prompt: Reference Extractor You are an expert in responding to reviewer comments. You need to produce high-quality paper rebuttal. You must understand the paper information from the [compressed paper] and the questions raised by reviewers in the [review_question]. Your assistant has now retrieved relevant reference paper [reference paper]. You must carefully read this reference paper and extract the most relevant and useful information for the current reviewer comments, including content that can be safely cited in the rebuttal. Important: Your task is to extract information from the reference paper, not from our paper. You are analyzing the reference paper (not our submitted paper). Any information you extract must come from the reference paper and will be used by subsequent agents. Subsequent agents must clearly know that this information is from an external source, not from our paper. This avoids mixing the two papers and prevents hallucinations. Fixed structure (no more than 600 words, as concise as possible): Your output must follow this structure: (1) paper title (2) one-paragraph summary of the reference paper (3) Direct relevance to the current reviewer comment [review_question]: (Explain how the reference paper helps shape the rebuttal and how it aids in responding to the reviewers question.) (4) Content we can safely cite in the rebuttal (5) Limitations or mismatches: (12 points explaining differences or inapplicable aspects between the reference paper and our paper.) (6) Reference paper URL: [reference paper URL] If you dont get the reference paper, output: \"This reference is blank. Please skip it\". Value assessment: If the reference paper objectively provides little help to the rebuttal, you must explicitly state that its value is limited or its relevance is low. Be honest and rigorous. If the reference paper is empty, state so directly. If only an abstract is provided due to an error, you must still try to extract information from the abstract and complete the taskbut you must never fabricate information or data, and you must avoid all hallucinations. Your output must contain concrete, justifiable evidence. You must follow rebuttal principles: the paper is already completed and cannot undergo major modifications, only minor adjustments. Therefore, your analysis must be based on the existing content. If the reference paper is objectively not closely related to our paper, state this clearly. Absolutely no fabricated content or hallucinations. 18 Prompt: Human-In-The-Loop Strategy Revisor You are Senior Computer Science Researcher and Rebuttal Expert. Your role is to incorporate human feedback to refine the rebuttal strategy while maintaining strategic balance. Input Context: [original paper]: The submitted manuscript. [review_question]: Extracted and merged reviewer concerns. [reference papers summary]: Potential supporting literature. [current rebuttal strategy and to-do list]: The current version to be revised. [humans feedback]: Feedback from the paper authors on the current strategy. YOUR ROLE: Human-Guided Refinement The human author knows their paper best and has practical constraints. Your job is to: 1. Incorporate the humans specific requests and preferences 2. Maintain the balance between action and acknowledgment Task: Based on the [humans feedback], revise the [current rebuttal strategy and to-do list]. Preserve balance, incorporate human preferences, and output the final revised version. Do not include commentary on the previous version in the outputonly the clean revised strategy. Do not provide specific time arrangements such as < 5 Days, day1, day2 in your output. In the to-do list, only the items to be done are elaborated in points. Do not include time-related descriptions such as \"strictly less than 5 days\" in the title of the to-do list. Prompt: Rebuttal Letter Writer Role You are senior researcher and an expert in academic writing, specifically for top-tier conferences like ICLR (International Conference on Learning Representations). You are currently in the \"Rebuttal/Author Response\" phase. Task Your team already provide detailed rebuttal ideas. Your task is to write formal, persuasive, and polite rebuttal letter based on them. Inputs Provided by User 1. [original paper]: Original submitted paper. 2. [review original text]: The actual text from Reviewers. 3. [review_question]: Merged questions extracted by your team. 4. [rebuttal_idea and to_do_list]: Prepared by your team for each merged question. You should take these as your rebuttal strategy. Note that your output should be specifically answered in combination with each reviewers question. Guidelines & Constraints 1. You should precisely identify each reviewers questions from [review original text], and then, following the order provided, find the corresponding response ideas in [rebuttal_idea and to_do_list] and generate the responses. Do not make any mistakes regarding the reviewers questions, or confuse the questions of the first reviewer with those of the second reviewer. You must strictly follow the rebuttal approach for each small problem in [rebuttal_idea and to_do_list]. 2. Tone: Professional, respectful, objective, and grateful. Even if the reviewer is harsh, your 19 response must be diplomatic (e.g., \"We thank the reviewer for the insightful comment...\"). Respect every reviewer. Do not generate statements that require particular reviewer to read the response to another reviewer. 3. Format: Use standard ICLR rebuttal formatting. Structure it clearly: \"Common Response\" (if applicable) followed by \"Response to Reviewer X\". Strictly follow this format! Use Q1/A1 or Comment/Response structure for clarity. Be sure to respond to each reviewer. Do not ignore specific reviewers and directly list all the issues your team has listed in [rebuttal_idea and to_do_list]! 4. LaTeX: Use LaTeX syntax for all mathematical notations (e.g., α, Lnorm). 5. Handling Missing Experiments (CRITICAL): Since you are an AI and cannot perform actual experiments, but the rebuttal might require empirical evidence (e.g., ablation studies, baseline comparisons), you must invent plausible, realistic values/results that support the users argument. MANDATORY RULE: Any time you generate specific number, metric, or experimental result that is not present in the input, you MUST append distinct asterisk symbol (*) right after it. Example: \"Our method achieves an accuracy of 85.4%* on ImageNet, outperforming the baseline.\" This indicates to the human user that this number is placeholder and needs to be manually verified or replaced with real data. 6. Although the supplementary experimental data in your final output is speculative (marked with an asterisk), you still need to ensure that your output is very formal, just like real rebuttal. Except for the asterisk, it should not be immediately recognizable as an AI-written rebuttal, but should be as close as possible to real person. Your output should not contain any other content. It should consist of the breakdown to each reviewers questions and corresponding detailed response. 7. The responses to each split question can include tables to visually present the experimental result numerical data to improve readability. But dont use tables to specifically present text! Dont put q1, response to q1, q2, response to q2 in large table. Instead, list them separately. Prompt: Unified Rebuttal Evaluation You are an EXPERIENCED and DISCERNING senior Area Chair evaluating rebuttal response. Your goal is to assess whether the author addressed the reviewers concerns with SUBSTANCE."
        },
        {
            "title": "Scoring Principle",
            "content": "Base Scores: Assign integer scores (0-5) first based on the rubric below. Upgrade (+0.5): Check the \"Upgrade Criteria\" section. If conditions are met, add 0.5 to the base score (e.g., 3 3.5). I. Relevance (R-Score) R1 Coverage: Are ALL aspects addressed with substance?"
        },
        {
            "title": "5 Covers ALL aspects comprehensively with specific details (numbers, examples, explanations)",
            "content": "for each."
        },
        {
            "title": "4 Covers ALL aspects, most with good specificity, a few with moderate detail.\n3 Covers ALL aspects but with varying specificity, some aspects addressed only briefly.\n2 Covers SOME aspects, misses or glosses over important points.\n1 Covers only 1-2 minor aspects, ignores most major concerns.",
            "content": "20 0 Does not address any of the reviewers points. R2 Semantic Alignment: Does response DIRECTLY address what was asked? 5 Perfectly matches question type with direct, concrete answers (if asked HOW explains HOW with details). 4 Matches question type well with substantive engagement, minor tangential points. 3 Acknowledges the right question and provides relevant response, but some drift or indirectness. 2 Partially addresses question but significant mismatch (asked HOW only says WHAT). 1 Off-topic or deflects, barely connects to the actual question. 0 Completely misunderstands or ignores the question. R3 Specificity: Does the response reference specific details rather than generalities? 5 Explicitly references specific paper components (e.g., \"Eq. 2\", \"Table 5 row 3\", \"the attention head in Layer 4\") or specific reviewer constraints. No vague language. 4 Uses concrete terminology and context-specific descriptions. Avoids generic phrases like \"our method\" without qualification. 3 Answers the question but uses broad terms (e.g., \"the loss function\" instead of \"the KLdivergence term\"). 2 Mostly relies on high-level summaries or generic templates applicable to any paper. 1 Purely abstract, avoiding any concrete details of the work. 0 Content-free filler. II. Argumentation (A-Score) A1 Logic Consistency: Is the logical chain sound?"
        },
        {
            "title": "5 Exceptionally clear logical chain with rigorous reasoning, each step well-justified.\n4 Clear logical chain with sound reasoning, well-structured argument.\n3 Adequate logic with reasonable support, generally coherent.\n2 Weak logic with some circular reasoning or unsupported leaps.\n1 Poor logic, circular reasoning, or pseudo-logic throughout.\n0 No logical structure or completely incoherent.",
            "content": "A2 Evidence Support: Is the argument backed by strong proof?"
        },
        {
            "title": "5 Backed by new quantitative results, specific comparative data, or rigorous mathematical",
            "content": "derivations presented directly in the rebuttal."
        },
        {
            "title": "4 Backed by existing concrete data (citing specific numbers from the paper) or detailed, verifiable",
            "content": "logical deduction."
        },
        {
            "title": "3 Claims are supported by qualitative reasoning or citations to external literature, but lack direct",
            "content": "quantitative verification."
        },
        {
            "title": "2 Relies on \"promises to fix\" or assertions without proof (e.g., \"we believe it will work\").\n1 Purely opinion-based statements (\"we think our method is novel\") with no backing.\n0 Claims made without any basis.",
            "content": "A3 Response Engagement: Does response show genuine engagement?"
        },
        {
            "title": "5 Exceptional engagement with deep understanding, addresses nuances and implications.\n4 Genuine engagement with specific improvements, demonstrates clear understanding of the",
            "content": "concern."
        },
        {
            "title": "3 Adequate response showing understanding, not just template language.\n2 Generic response with excessive hedging or template-like language.\n1 Minimal engagement, mostly boilerplate text.\n0 No genuine engagement.",
            "content": "21 III. Communication (C-Score) C1 Professional Tone: Is the tone authentic and professional? 5 Exceptionally professional and AUTHENTIC tone with gracious acknowledgment and genuine respect. 4 Professional and authentic tone with genuine engagement, appropriately courteous. 3 Adequate professional tone with standard academic courtesy. 2 Somewhat defensive OR excessively polite while masking weak content (artificial politeness). 1 Defensive tone or insincere language, reads as \"academic speak\" without substance. 0 Rude, hostile, or completely inappropriate. C2 Clarity: Is the response clear and well-organized? 5 Exceptionally clear and well-structured WITH REAL SUBSTANCE (clear writing + concrete details). 4 Clear and well-organized with substantive content, easy to follow. 3 Adequate clarity, generally well-organized, understandable. 2 Somewhat unclear OR superficial clarity (sounds good but vague). 1 Confusing, poorly organized, or misleading presentation. 0 Incomprehensible or no coherent structure. C3 Constructiveness: Does author show willingness to improve?"
        },
        {
            "title": "5 Multiple concrete improvements detailed IN the rebuttal text itself with specific changes",
            "content": "described."
        },
        {
            "title": "4 Detailed improvements (3+ items) with clear explanations, OR good mix of in-text details +",
            "content": "external references with content previews."
        },
        {
            "title": "3 Specific improvements with good detail, OR specific actions mentioned with some concrete",
            "content": "description."
        },
        {
            "title": "2 Vague promises without specifics, or only external references without content.\n1 Defensive or dismissive, minimal constructive response.\n0 Refuses to improve or no constructive response.",
            "content": "IV. Upgrade Criteria & Critical Considerations Upgrade Check (Apply +0.5 to Base Score): Note: This upgrade applies ONLY to Base Scores of 3 and 4. Scores 02 indicate fundamental flaws (e.g., irrelevance, logic errors) that cannot be redeemed by these details, and 5 is already the ceiling. From 3 to 3.5: Must meet AT LEAST 2 conditions: (1) Content preview provided with specific details; (2) Detailed improvement list (3+ items); (3) Mixed evidence chain (concrete content + external reference). From 4 to 4.5: Must meet AT LEAST 2 conditions: (1) Perfect content-reference match; (2) Multi-dimensional evidence (code/results/theory); (3) Exceeds expectations (provides additional value). Critical Considerations: Relevance Check: Watch for excessive repetition, vague qualifiers (e.g., \"somewhat\", \"to some extent\"), or drifting off-topic to avoid hard questions. Logical Scrutiny: Identify circular reasoning, unfulfilled promises (e.g., \"we will add\" without content), or citations listed without explaining their specific relevance. Tone Analysis: Be wary of over-polished, artificial politeness that masks weak substance, or mismatch between confident tone and shaky evidence. Output Format: { } \"R_scores\": {\"R1_coverage\": 4.5, \"R2_semantic_alignment\": 4, \"R3_specificity\": 3.5}, \"A_scores\": {\"A1_logic_consistency\": 4, \"A2_evidence_support\": 3, \"A3_response_engagement\": 4}, \"C_scores\": {\"C1_professional_tone\": 5, \"C2_clarity\": 4, \"C3_constructiveness\": 3.5}, \"quality_warnings\": [\"Vague Language\", \"Over-Polished Tone\"], \"explanation\": \"...\" Output Format: { } \"R_scores\": {\"R1_coverage\": 4.5, \"R2_semantic_alignment\": 4, \"R3_specificity\": 3.5}, \"A_scores\": {\"A1_logic_consistency\": 4, \"A2_evidence_support\": 3, \"A3_response_engagement\": 4}, \"C_scores\": {\"C1_professional_tone\": 5, \"C2_clarity\": 4, \"C3_constructiveness\": 3.5}, \"red_flags\": [\"Vague Language\"], \"explanation\": \"...\""
        },
        {
            "title": "E Case Study",
            "content": "Rigorous formalization & verification v.s. High-level intuitive explanation"
        },
        {
            "title": "Question",
            "content": "Lack of clarity and rigor in the theoretical contribution (Section 3 and Proposition 1). RebuttalAgent(Ours) We thank the reviewer for highlighting clarity issues around Proposition 1 (Sec. 3) and its proof (App. A.1). Section 3 and Appendix A.1/A.8 already contain the core statements and empirical hints. We will (a) rewrite the theoretical exposition for line-by-line readability, and (b) add focused empirical sanity checks that make each assumption transparent. Rebuttal strategy what we will deliver: Clear, self-contained theory section edits: Replace the informal statement with numbered, fully specified Proposition in Sec. 3 (explicitly naming L, K, ψ, δ, k, model class Vcnn). Add 23 paragraph proof sketch giving the intuitive chain: PD gap early-layer separability larger V-usable information model prefers shortcuts. Reformat Appendix A.1 into stepwise proof with numbered lemmas. Per-assumption empirical sanity checks: (i) Function-class separation: show per-layer kNN error vs shallow-linear head vs full-model error. (ii) Controlled confidence growth: plot fknn 0.5 vs layer index for easy vs hard subsets. (iii) PD-separation: report empirical fractions (PD and PD > K) for key datasets. Quantify PD V-information evidence: Expand Appendix A.8 with Pearson/Spearman correlations between per-example PD and HVcnn. Add controlled synthetic pair (Ds/Di) demonstrating mean PD(Ds) < mean PD(Di) and IVcnn(Ds) > IVcnn(Di). Robustness checks: Show correlations across {5, 15, 29}, δ {0.05, 0.1, 0.2}, and 23 architectures (ResNet18, VGG16, DenseNet121). Didactic aids: Add one-page flow diagram and short lemma stating the minimal sufficient condition linking PD gap to V-information gap. How these actions address concerns: Readability & Rigor: Explicit notation and numbered lemmas directly improve accessibility and logical flow. Assumptions: Each assumption is now explicitly listed, interpreted, and backed by empirical data. Intuition: The new didactic flow diagram and lemma provide low-barrier entry point to the theory. Key deliverables for the revision: Revised Text: Sec. 3 (Prop. 1 + proof sketch) and rewritten App. A.1 (stepwise lemmas). New Tables: Mean PD gap vs. IVcnn and PD correlation statistics (with p-values). New Plots: Per-layer k-NN error/confidence curves, synthetic PD histograms, and robustness sweeps (k, δ). Artifacts: Didactic flow diagram and reproduction scripts committed to the repo. Concrete to-do list: 1. Draft numbered Proposition 1 with explicit notation and enumerated assumptions. 2. Write proof sketch for Sec. 3; reformat Appendix A.1 with numbered lemmas. 3. Compute per-layer k-NN error, shallow head error, and final model error plots. 4. Compute fknn 0.5 vs layer curves; tabulate PD-separation fractions. 5. Compute Pearson/Spearman correlations for PD vs HVcnn. 6. Create controlled synthetic validation and robustness tables. 7. Produce flow diagram and commit scripts to repository. Feasibility: All planned items use existing model checkpoints, intermediate embeddings, k-NN heads, small auxiliary classifiers, or lightweight synthetic datasets. No retraining of large backbones from scratch or new large-scale dataset collection is required. The planned experiments are deliberately scoped to be implementable using the papers existing code and checkpoints. Summary: We will replace the current informal and hard-to-follow presentation with compact, fully-specified Proposition and annotated proof plus focused empirical sanity checks (including controlled synthetic demonstration and robustness analyses) so the connection between Prediction Depth and V-usable information is transparent, reproducible, and intuitively accessible. LLM output(baselines) We apologize for the density of the theoretical section. The goal of Proposition 1 is to bridge structural metric (PDhow many layers model needs) with an information-theoretic metric (V -informationhow much predictive power model family can extract). 24 Intuition: If dataset Ds has lower mean PD than Di, it means the model family can solve the task using fewer layers (less computation). Proposition 1 proves that under mild assumptions, this computational efficiency implies that the \"usable\" information in Ds is strictly higher than in Di. Improvement: In the final version, we will move the formal definitions of -information to the main text and add \"Key Takeaway\" box for Proposition 1 to ensure the link between \"layer depth\" and \"information usability\" is explicit for readers less familiar with -information. Actionable theoretical expansion v.s. Passive logical defense"
        },
        {
            "title": "Question",
            "content": "I dont understand Proposition 1. My understanding was that when the energy increases, the euclidean distance of features also increases. But Proposition 1 states the opposite. Is this mistake? RebuttalAgent(Ours) We thank the reviewer for spotting this confusion. There is no mathematical mistake, but the exposition was misleading. Proposition 1 is one-sided upper bound (Ez2 AIE), which allows large feature changes when energy is high but does not force them. The monotonic relationship (AIE = ) requires stronger assumptions (NTK/Linear regime), which we will now make explicit. Rebuttal strategy what we will deliver: Clarify Scope (Sec 4.1): Explicitly label Proposition 1 as an upper bound derived via CauchySchwarz. Add an \"Interpretation Box\" explaining that while the bound limits the maximum change, the expected change scales with energy under the conditions of Proposition 2. New Theoretical Lemma (Appendix D): Add formal \"Monotonicity Lemma\" with selfcontained proof. It will state: Under overparameterized linear/NTK assumptions, the mapping AIE (cid:55) Ez2 is strictly monotonic. Empirical Validation: We will not just argue; we will show the data. We will add scatter plots of AIE vs. Feature Change (EzT z02) for representative tasks (Toy MLP, ResNet50STL10), reporting Pearson/Spearman correlations to prove the positive trend holds in practice. Bound Diagnostics: Overlay the empirical upper bound line (c AIE) on the scatter plots to visualize where the bound is tight vs. loose. Key deliverables for the revision: Revised Text: Rewritten Sec 4.1 (Prop 1 interpretation) and Annotated Appendix (step-bystep proof with constants C1, C2). New Math: formal Monotonicity Lemma in Appendix D. New Figures: Multi-panel scatter plots (AIE vs. z) with regression lines and correlation stats. Concrete to-do list: 1. Rewrite Prop 1 exposition to emphasize \"Upper Bound\" and list assumptions (linear head, bounded NTK). 25 2. Formalize and write the proof for the Monotonicity Lemma in Appendix D. 3. Run diagnostics on 4 settings (Toy MLP, Sup-IN, SimCLR, Graph): Compute per-τ AIE and feature distance. 4. Generate scatter plots, calculate Pearson/Spearman ρ, and fit linear regression (α AIE + β). 5. Estimate constants (C1, C2) on subset to plot the theoretical bound line AIE. Feasibility: The theoretical work involves derivations based on existing Appendix material. The empirical diagnostics use existing checkpoints and require only lightweight forward passes to compute feature norms and correlations. No training is required. Summary: We resolve the apparent contradiction by distinguishing the general upper bound (Prop 1) from the specific monotonic trend (Prop 2/New Lemma). We back this clarification with new formal proof and empirical scatter plots showing strong positive correlation (r > 0.9). LLM output(baselines) No there is no contradiction. Proposition 1 provides an upper bound: EzT z02 AIE. This means that larger AIE (Average Initial Energy) permits larger bound on the feature change; equivalently, if AIE is small the change must be small. Intuitively: low energy guarantees small adaptation; higher energy does not guarantee large change but allows it. Our empirical and analytic statements say that, in the settings we study, larger AIE tends to produce larger feature changes (the bound is consistent with and does not contradict this observed trend). We clarified the text around Proposition 1 to emphasize it is an upper bound and to explain the direction of implication more carefully. Methodological triangulation v.s. Linear request fulfillment"
        },
        {
            "title": "Question",
            "content": "The proposed topographic similarity metric (based on Hausdorff distance) fails to conclusively demonstrate compositionality. The analysis is visually suggestive but not systematic.It should be validated on real-world ideograms/pictograms to establish trustworthiness. RebuttalAgent(Ours) We acknowledge that the topographic maps (Fig. 6, 7) and score ρ are visually suggestive but not definitive proof of compositionality. As noted in literature (e.g., \"Measuring non-trivial compositionality\"), distance-based metrics on raw signals can be agnostic to functional meaning. We will reframe the Hausdorff analysis as geometric visualization tool rather than primary proof, and supplement it with systematic decodability tests. Rebuttal strategy what we will deliver: Systematic Decodability (New Analysis): Directly address R1-W3 by testing if composite symbols u(rij) can be functionally decoded into constituents and using simple classifier (k-NN) trained on single-feature utterances. Real-World Validation (Calibration): Apply our Hausdorff metric to curated set of realworld ideograms (e.g., \"man\" + \"walking\") to calibrate the ρ score and establish trustworthiness. Latent Space Linearity: Quantify compositionality in the embedding space (where the energy model operates) by measuring the reconstruction error of composite embeddings as linear combinations of constituent embeddings. Reframing Text: Revise Section 4 to clarify the topographic metrics role as descriptive, citing relevant literature on metric limitations. Key deliverables for the revision: New Table: \"Constituent Identification Accuracy\" (k-NN classification results). New Supplementary Figure: Real-world ideograms with their computed topographic scores. Revised Section 4: Updated discussion distinguishing geometric similarity from functional compositionality. Concrete to-do list: 1. Decodability: Implement k-NN to classify constituents of u(rij) using u(ri) library; report accuracy. 2. Validation: Curate 10-15 public ideograms, manually decompose them, and compute ρ using our pipeline. 3. Linearity: Calculate MSE for z(rij) wiz(ri) + wjz(rj) in the latent space. 4. Ablation: Compare decodability/linearity between descriptive vs. discriminative generation strategies. Feasibility: High. The decodability and latent space analyses use existing trained models and require only lightweight inference scripts (k-NN, linear regression). The real-world validation requires small manual dataset collection but utilizes the existing metric code without modification. Summary: We will pivot from claiming the topographic metric as \"proof\" to using it as \"visualization,\" while providing robust quantitative evidence via new symbol decodability tests and latent space linearity analysis. LLM output(baselines) This is valuable suggestion for establishing the metrics external validity. In revision, we will apply our topographic score (ρ) to curated set of real-world symbols (e.g., simple ideograms, traffic signs, emojis) where compositionality is intuitively judged. For example, we would expect the symbol for \"restaurant\" (a fork and knife) to score highly against the symbols for \"fork\" and \"knife,\" while non-compositional symbol would not. This analysis will be included in the supplementary material, providing benchmark for interpreting the scores obtained from our emergent language."
        }
    ],
    "affiliations": [
        "AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
    ]
}