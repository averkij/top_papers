{
    "paper_title": "C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models",
    "authors": [
        "Kairong Han",
        "Nuanqiao Shan",
        "Ziyu Zhao",
        "Zijing Hu",
        "Xinpeng Dong",
        "Junjian Ye",
        "Lujia Pan",
        "Fei Wu",
        "Kun Kuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\underline{\\textbf{C}}ausal \\underline{\\textbf{C}}oncept-Guided \\underline{\\textbf{D}}iffusion \\underline{\\textbf{L}}anguage \\underline{\\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~\\href{https://github.com/Kairong-Han/C-2-DLM}{here}."
        },
        {
            "title": "Start",
            "content": "C2DLM: Causal Concept-Guided Diffusion Large Language Models Kairong Han1, Nuanqiao Shan1, Ziyu Zhao1, Zijing Hu1, Xinpeng Dong1, Junjian Ye2, Lujia Pan2, Fei Wu1, Kun Kuang1 1College of Computer Science and Technology, Zhejiang University, 2Noahs Ark Lab, Huawei Technologies, 5 2 0 2 7 2 ] . [ 1 6 4 1 2 2 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose Causal Concept-Guided Diffusion Language Model (C2DLM). Starting from DLMs fully connected attention, C2DLM first obtains concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C2DLM improves 12% with about 3.2 training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31% across six downstream reasoning tasks. More details in the repository here."
        },
        {
            "title": "Introduction",
            "content": "In recent years, the development of large language models (LLMs) (Zhao et al., 2023; Liu et al., 2024; Team et al., 2023) has led to two dominant paradigms: autoregressive (AR) LLMs and diffusion language models (DLMs) (Li et al., 2025a; Nie et al., 2025). In the AR paradigm, causal mask (Vaswani et al., 2017) constrains the model with lower-triangular matrix to predict the next token based on preceding tokens. In contrast, the DLM paradigm employs fully connected attention to guide the model in globally modeling data from coarse to fine (Ye et al., 2024). 1 Figure 1: Difference between AR, DLM, and C2DLM. AR models struggle to capture global information, and linguistic flexibility is not bound to strict left-to-right, token-by-token causal order. DLMs discard causal priors entirely. The C2DLM explicitly guides the model to learn causal relations between concepts, capturing the underlying causal priors of natural language generation. However, both AR and DLMs suffer from insufficient reasoning capabilities, such as frequent hallucinations (Huang et al., 2025) and unreliable reasoning chains (Lanham et al., 2023; Yehudai et al., 2025), imposing fundamental limitations on tasks that require reasoning and posing significant challenges in real-world deployment (Pan et al., 2025; Acharya et al., 2025; Wu et al., 2024a). Specifically, AR models exhibit limitations in complex reasoning, long-term planning, and maintaining global coherence (Ye et al., 2024; Bubeck et al., 2023; Kambhampati et al., 2024; Zeˇcevic et al., 2023). DLMs, as strong competitors to AR models, reduce training efficiency and hinder the effective scaling of reasoning depth, thereby limiting their potential for complex tasks. Human reasoning inherently relies on causal knowledge and thought. From natural language perspective, it is inherently flexible rather than strictly left-to-right and token-by-token causal structures. However, AR generation enforces unidirectional information flow, resulting in local greediness and limited understanding of global objectives and long-term structure. In contrast, DLMs discard the causal order between tokens, often producing final answers before the intermediate reasoning steps that CoT methods would generate (Wang et al., 2025a). Their training further involves numerous difficult sub-tasks (e.g., predicting masked cause variables from outcomes under random masking) (Kim et al., 2025), which reduces training efficiency and constrains the scalable development of reasoning depth. To address the above fundamental problems, we hypothesize that these limitations stem from misalignment between the attention mechanisms modeling priors on natural language and the causal priors underlying natural language. Therefore, we aim to guide the model to capture the underlying causal priors of the natural language generation process, rather than superficial correlations. Inspired by this, we propose Causal Concept-Guided Diffusion Language Model (C2DLM) paradigm, as shown in Figure 1. The C2DLM extends DLMs by two key steps: (1) concept-level causal meta-knowledge extraction, and (2) causal alignment via the V-aware Reattention mechanism. In the first step, to obtain concept-level causal graphs at low cost, an automated workflow leverages the in-context learning (ICL) (Dong et al., 2022) capabilities of teacher LLMs to extract concept-level meta-knowledge. In the second step, we propose the V-aware Reattention mechanism to align the attention map weighted by the L2-norm of the Value matrix with the underlying causal priors of the natural language generation process generated by step one. To compare AR, DLM, and C2DLM systematically, we design the COT-OrderPerturb dataset to quantify the impact of priors. AR models are sensitive to concept order, whereas DLMs are more robust but limited by efficiency and performance bottlenecks. Building on DLMs, C2DLM achieves 12% higher performance and 3.2 faster training. On downstream tasks with explicit causal priors, C2DLM yields average improvements of 7.43% on STG (Han et al., 2025) and 10.84% on Sudoku (training set size 200). Across six reasoning-related datasets, it delivers an average gain of 1.31%, while causal prior extraction with GLM-4.5 costs only $0.46 per million tokens. Our contributions can be summarized as follows: We propose C2DLM, new paradigm distinct from AR and DLM. It enhances reasoning ability by guiding attention through causal knowledge between concepts to achieve causal alignment. The C2DLM achieves 12% improvement and 3.2 acceleration of training efficiency in the COT-OrderPerturb tasks, 7.43% on the STG dataset, and 1.31% across six reasoningrelated downstream datasets on average. We reveal the risk of misalignment between attention mechanisms and the causal priors underlying natural language, which shows the potential of combining causality into language models."
        },
        {
            "title": "2 Preliminaries and Related Works",
            "content": "2.1 Diffusion Large Language Model Recently, researchers have adapted the diffusion paradigm (Yang et al., 2023; Cao et al., 2024) to discrete text data, proposing DLM (Nie et al., 2025; Ye et al., 2025, 2024; Austin et al., 2021), which achieve competitive performance compared to AR models. DLMs employ bidirectional attention mechanism and leverage the Negative Evidence Lower Bound to provide an upper bound on the negative log-likelihood of the training data, thereby modeling the distribution of language. LLaDA (Nie et al., 2025) first demonstrated the effectiveness of diffusion at the 8B parameters, using the following supervised fine-tuning (SFT) loss LDLM: Et,p0,r0,rt (cid:34) (cid:88) i=1 1[rt = ] log pθ(r0 (cid:35) p0, rt) , where rt denotes the noised sequence appended to the prompt p0. Recent work on DLMs has mainly focused on improving DLM by reinforcement learning (RL) (Kaelbling et al., 1996), such as d1 (Zhao et al., 2025a),wd1 (Tang et al., 2025), BranchGRPO (Li et al., 2025b), and et al. (Zhao et al., 2025b; Zhu et al., 2025). Another line of work focuses on speeding up DLM inference time, such as Fast-dllm (Wu et al., 2025), SlowFast (Wei et al., 2025), and et al. (Wang et al., 2025b; Hu et al., 2025). However, C2DLM focuses on causal alignment of the attention mechanism in the SFT stage. 2 2.2 Combining Causality and Attention 3.1 Concept-level Causal Meta-knowledge Mechanism Extraction Transformer (Vaswani et al., 2017) proposes the multi-head attention mechanism, which models dependencies between tokens: Aattn = softmax (cid:19) (cid:18) Qi i dk Vi, where for each head in multi-head attention, Qi, Ki Rndk ,Vi Rndv . Although attention and causal (Pearl, 2009) graphs are correlated in the covariance structure (Rohekar et al., 2023), the attention sink (Sun et al., 2024; Xiao et al., 2023; Gu et al., 2024) phenomenon reveals outliers in the attention distribution, reducing interpretability (Kobayashi et al., 2020). To correct and denoise attention, some studies leverage causal backdoor mechanisms for debiasing in text (Wu et al., 2024b) and vision (Yang et al., 2021). Recent work proposed the Reattention mechanism (Han et al., 2025) to inject causal knowledge into student model. Our C2DLM is the new paradigm to investigate attention mechanisms in DLMs, interpreting the importance of guiding the model to align with data generation priors. 2.3 The Limitations of Attention in AR and DLM The limitations of AR and DLM models stem from the structural priors imposed by their attention mechanisms. The AR model, with its lowertriangular attention matrix and modeling objective (x) = (cid:81) pθ(xix<i), is unable to handle situations in natural language where the outcome precedes the cause. On the other hand, DLM can be regarded as an any-order AR model (Arriola et al., 2025). DLM adopts fully connected structure that can model arbitrary dependencies: (xi) = pθ(xi x=i). The absence of causal constraints causes key causal signals in each step of the COT process to be diluted by redundant information from both past and future contexts. This makes it difficult for the model to perform stable and effective reasoning over COT."
        },
        {
            "title": "3 Method",
            "content": "Humans, when confronted with downstream tasks, analyze relationships among conceptual entities, perform reasoning, and integrate contextual information to verbalize their thought process in natural language. Inspired by this, we extract and construct concept-level reasoning graphs for tasks described in natural language. Each concept encapsulates the core information necessary for reasoning and reflects human causal logic, thereby encoding the true and flexible priors underlying the data. Therefore, the generating function of language should be consistent with humans prior understanding of causal concepts. To reduce the cost of generating such priors, we design an automated workflow. The teacher model first extracts set of concepts = {c1, c2, . . . , cn} from the reasoning steps, and denotes the remaining text as context . Each concept represents semantically complete entity or sentence. The teacher then constructs reasoning graph over C, capturing causal dependencies between concepts. Unlike prior work (Han et al., 2025), the graph is not restricted to pairwise causal forms such as cA causes cB. For given concept cA, information from cB that is unnecessary for generation is pruned, preventing reverse dependencies. For example, as shown in Figure 2(a), the teacher decomposes the problem into four steps and identifies causal meta-knowledge within each step. Conditions like 2, 3, 5, 7, 11, 13, 17, 19 and each prime factor of 20! must be assigned entirely to or together imply 256 valid (a, b) pairs, though this fact is not required when generating those conditions. For certain tasks, we further introduce rule-based, semi-autoregressive supervisory signal that decomposes the reasoning chain into coarsergrained steps based on inter-rule inference. This mechanism prunes irrelevant information from earlier steps, improving the efficiency of subsequent reasoning. Prompt details are provided in Appendix A. Based on the above constraints, we define the prior supervision mask as: As shown in Figure 2, C2DLM consists of two main steps: (1) concept-level causal meta-knowledge extraction, and (2) causal alignment via the V-aware Re-attention mechanism. Mi,j = 1, 0, 1, ci, cj C, ci cj, ci or cj , ci, cj C, cj ci or si > sj. Figure 2: (a) Leveraging the contextual learning capability of strong model, the causal teacher model uses prompts to automatically extract concept-level information from CoTs and generates causal meta-knowledge links between concepts as supervisory signals. (b) During training, for the internal attention map obtained from CoTs, the V-aware Re-attention mechanism weights the attention maps by the norms of the corresponding Value matrix. (c) The tokenizer maps the textual supervisory signals from step (a) to the weighted attention maps, and loss-based intervention is applied to guide the C2DLMs decision-making process. where and are concepts and steps indexed by token and separately. 3.2 Causal Alignment via the V-aware Re-attention Mechanism To align the models decision dependencies with the underlying causal mechanisms of natural language and eliminate instability caused by outliers in the attention map, we propose the V-aware Reattention mechanism. With respect to the supervisory mask introduced in the previous section, we define the index sets as Ik = {j Mi,j = k}, {1, 0, 1}. Because the attention map can be distorted by the attention sink phenomenon, its raw values may fail to accurately reflect token-level interactions. To address this, we incorporate the L2-norm of the Value matrix as weighting information. Let A(h) RTqTk denote the attention map of the h-th head, and (h) RTqdh the corresponding Value matrix. We use the L2 norm of the Value matrix as weighted information (Kobayashi et al., 2020): (h) 2 = (cid:118) (cid:117) (cid:117) (cid:116) dh(cid:88) d=1 (cid:0)V (h) i,d (cid:1)2, = 1, . . . , Tq. The weighted attention map is then (cid:101)A(h) i,j = A(h) i,j (h) 2, [1, Tq], [1, Tk], and averaging across nh heads yields (cid:101)Ai,j = 1 nh nh(cid:88) h=1 (cid:101)A(h) i,j . Based on (cid:101)A, we compute the average attention values for encouraged and neutral sets as A1 = 1 I1 (cid:88) jI1 (cid:101)Ai,j, A0 = 1 I0 (cid:88) jI0 (cid:101)Ai,j. The ratio loss for the i-th row is Lratio(i) = 0, A1 A1 + A0 , A1 A0 < α, otherwise, where α > 0 enforces minimum ratio between encouraged and neutral attentions. In addition, for masked entries with Mi,j = 1, we penalize the squared weighted attention values: Lneg(i) = λ (cid:88) (cid:101)A2 i,j, jI1 4 with λ > 0 controlling the penalty strength. The total loss for the i-th row is then Lrow(i) = Lneg(i) + Lratio(i). For the Jr valid rows that have supervisory signals, we apply weighting and combine them with the DLM downstream SFT loss (as described in Section 2.1) to obtain the final training loss: Ltotal = LDLM + γ Jr (cid:88) iJr Lrow(i), where γ > 0 is balancing coefficient controlling the relative strength of the proposed constraint loss. Because we directly intervene on the weights of the attention matrix, we introduce smoothing mechanism to stabilize training. Inspired by learning rate scheduling, we define γ-parameter scheduler Sγ that modulates γ over training steps. Specifically, for the initial set of steps, γ increases linearly from γmin to γmax, and for the subsequent steps, it decreases linearly back to γmin. Formally, this can be expressed as: (cid:40) γt = γmin + T1 γmax tT1 T2T1 (γmax γmin), (γmax γmin), [0, T1] [T1, T2], where denotes the current training step, T1 is the number of warm-up steps, and T2 T1 is the number of cool-down steps."
        },
        {
            "title": "4 Experimental Results",
            "content": "4.1 Experimental Setup Datasets. We first constructed synthetic dataset, COT-OrderPerturb, to examine how the ordering of concepts within COT influences AR and DLM. We then employed the Sudoku1 and STG (Han et al., 2025) datasets to assess how models benefit when downstream tasks exhibit explicit causal structures. Finally, we evaluated broader reasoning-related downstream tasks, including MATH500 (Lightman et al., 2023), GSM8K (Cobbe et al., 2021), GPQA (Rein et al., 2024), ARC_C (Clark et al., 2018), SAT (Zhong et al., 2023), and MMLU_STEM (Hendrycks et al., 2021b,a). Baselines and hyperparameters. We adopt LLaDA-8B-Instruct2 as the primary experimental model and apply LoRA (Hu et al., 2022) for 1https://github.com/Black-Phoenix/4x4-Sudoku-Dataset 2https://github.com/ML-GSAI/LLaDA/ 5 fine-tuning, with SFT serving as the DLM main baseline. During training and evaluation, all hyperparameters are consistent except for the loss introduced by C2DLM. In addition, we include the following commonly used AR models for comparison: Llama-3.1-8B, Llama-3.2-1B (Dubey et al., 2024), Qwen-2.5-1.5B, and Qwen3-8B (Yang et al., 2025). For the γ-parameter scheduler, we set T1 = 0.1 T2. For the λ-parameter, we set 100 for the COT-OrderPerturb and 10 for all other tasks. The learning rate is uniformly fixed at 2 e5, and the LoRA rank is set to 128. For STG and Sudoku, we set α as 5, and for other tasks, α is 3. Unless otherwise specified, the block length during the test defaults to 32. More detailed hyperparameter configurations are provided in Appendix B. 4.2 Quantifying the Impact of Priors To verify the impact of AR and DLM attention limitations, we propose the COT-OrderPerturb synthetic dataset, thereby quantifying the impact of misalignment between the attention mechanisms modeling priors on natural language and the causal priors underlying natural language. We first generate CoT simulation data based on given prior causal graph, as shown in Figure 3, including both CoTs that follow the standard causal order and CoTs with permuted concept sequences, where outcomes may precede their causes. To systematically explore different types of perturbations, we apply the following shuffling strategies: DFS, local reverse (LR), output first (OF), reverse (RE), and three random shuffles R1, R2, and R3 (details in the Appendix C), along with control condition named No COT in which answers are generated directly without CoT reasoning. Results are summarized in Table 1. Structural Bias in AR Models. We observe that AR models exhibit declining performance consistency when data is perturbed such that outcomes precede their causes. However, linguistic flexibility is not bound to strict left-to-right, token-by-token causal order, e.g., \"The ground is slippery today because it rained\" or \"Lung cancer is caused by smoking\". The outcome may precede the cause. The misalignment between AR priors and underlying causal priors of natural language introduces structural risks that cannot be resolved by simply scaling the training data. Robustness from Order-Independence in DLMs. DLM, trained with fully connected attention and order-independent masking, demonstrates Model Normal COT DFS LR OF RE Shuffle COT R1 R3 avgstd No COT AR Llama-3.2-1B Qwen3-8B Llama-3.1-8B DLM LLaDA-8B-Instruct (SFT) 22.40% 60.60% 47.60% 20.60% 25.80% 31.40% 22.60% 25.00% 24.00% 24.20% 24.80%3.37% 15.60% 0.20% 23.00% 32.40% 33.20% 19.37%18.32% 36.40% 2.41% 44.20% 0.20% 18.20% 44.00% 14.00% 21.40% 4.00% 32.60% 29.80% 23.43%13.20% 44.60% 38.60% 38.20% 36.60% 42.40% 41.80% 33.80% 35.00% 40.60% 38.34%3.38% 57.60% Table 1: Performance of AR and DLM under different setting in COT-OrderPerturb dataset. Figure 4: Accuracy curve as training progresses in the COT-OrderPerturb task. Model Normal COT LLaDA-8B-Instruct (SFT) C2DLM (ours) 38.60% 50.60% +12.00% Table 2: Performance comparison under the Normal COT setting. Notation denotes the performance gain relative to direct SFT on LLaDA-8B-Instruct. nism, and suppresses attention on element interactions that violate the causal structure. As shown in Table 2, aligning with causal priors leads to significant performance improvement of 12%, surpassing that of Llama-3.1-8B. Moreover, as illustrated in Figure 4, the training efficiency of C2DLM is 3.2 times that of DLM. 4.3 Downstream Task Experiments with Explicit Causal Structures Some downstream tasks contain explicit causal structures as priors in their data generation processes. We selected Sudoku and STG as representative benchmarks to evaluate C2DLM. 4.3.1 Sudoku dataset For the Sudoku task, we focus on 4 4 grid, which aligns the setting the same as (Zhao et al., In Sudoku, each number in Sudoku is 2025a). determined by the values in its row, column, and corresponding subgrid. Experimental results in Figure 3 indicate that AR approaches are constrained Figure 3: Normal COT follows the causal topological order of the data-generating process to construct reasoning steps, whereas the Shuffle setting simulates cases where COT exhibits causal misordering. In the shuffled CoT setting, greater robustness. DLM achieves both better mean and standard deviation of accuracy. Performance Bottlenecks of DLMs. Interestingly, DLMs perform notably worse than AR models under the Normal CoT setting. While AR models consistently benefit from CoT supervision, DLMs achieve substantially higher performance in the No-CoT condition than when CoT is included. This discrepancy arises because, by discarding causal order, DLM training effectively becomes form of multi-objective learning across all reasoning steps. With limited data, this hinders the acquisition of deeper reasoning capabilities. Moreover, the longer CoT sequences further exacerbate inefficiency: DLMs typically require nearly 80 epochs to converge, whereas AR models converge within only 4 epochs. Performance Gain Using C2DLM. C2DLM explicitly incorporates step-wise conceptual causal relationships, guiding the model to learn the data generation process via V-aware Re-attention mecha6 best AR baselines in the OOD setting of STG_S, STG_M, and STG_L. We further examined the training efficiency of C2DLM in STG_H. As shown in Figure 5. The training efficiency of C2DLM is 2 times that of DLM. Setting AR Llama-3.2-1B Qwen3-8B Llama-3.1-8B Sudoku n=200 n=500 n=5000 3.00% 22.40% 80.60% 67.40% 76.40% 83.00% 29.20% 80.80% 8.60% DLM LLaDA-8B-Instruct (SFT) C2DLM (ours) 77.05% 90.23% 92.14% 87.89% 91.21% 92.97% +10.84% +0.98% +0.83% Table 3: Performance on Sudoku task. And represents the size of the training data. Notation denotes the performance gain relative to direct SFT on LLaDA-8BInstruct. Figure 5: Performance change curve during different epochs of training on the STG_H dataset. by the unidirectional flow of information, causing misalignment between attentions prior and the data generation process, which leads to low performance. By contrast, C2DLM effectively leverages the causal priors, which allows the model to better fit the data and avoid learning spurious and unrelated correlations. This advantage is particularly obvious in small-data scenarios (n = 200), where LLaDA-8B-Instruct lacks clear supervisory guidance and performs worse than C2DLM. Figure 6: Attention visualization and weight distribution bar charts, where the x-axis represents different attributes in the STG task. Purple indicates causal factors, green denotes spurious correlations, and yellow represents unrelated factors. To better interpret the impact of C2DLM on the attention mechanism, we conducted attention visualizations weighted by the value matrix. As shown in Figure 6, C2DLM effectively learns causal relationships, whereas direct fine-tuning of LLaDA fails to distinguish spurious correlations and irrelevant factors from causal factors. By mechanistically guiding the model to fit the data generation process, C2DLM yields more robust predictions and enhances model reliability. Further analysis is provided in Appendix E. 4.3.2 STG dataset 4.4 Evaluate on Broader Math and Reasoning Similarly, the STG dataset is also generated from explicit causal graphs and provides both IID and OOD testing scenarios, which facilitate more systematic evaluation of robustness in the presence of spurious correlations. As shown in 4, C2DLM significantly outperforms direct SFT across different STG subsets, with an average improvement of 7.43% over IID and OOD settings. Compared to AR, introducing the causal prior via C2DLM markedly narrows their gap and even surpasses the Downstream Tasks For broader downstream tasks without explicit causal structure, we leverage the automated workflow introduced in Section 3.1. Specifically, we adopt the latest GLM-4.5(Zeng et al., 2025), which provides balance between scalability and costeffectiveness while addressing the challenges of extracting causal relationships from long CoT sequences. Aligning with previous work(Zhao et al., 2025a; Tang et al., 2025), we start from the s1k Setting AR Llama-3.2-1B Qwen2.5-1.5B Llama-3.1-8B DLM LLaDA-8B-Instruct (SFT) C2DLM (ours) STG_S STG_M STG_L STG_H IID OOD IID OOD IID OOD IID OOD 83.50% 67.25% 95.75% 61.50% 97.25% 87.00% 37.40% 27.90% 81.50% 78.50% 93.25% 82.00% 95.75% 82.00% 51.50% 53.40% 90.50% 86.25% 93.25% 64.50% 96.00% 88.25% 57.80% 49.60% 86.50% 81.25% 85.75% 83.00% 88.25% 83.25% 24.80% 25.60% 88.00% 88.50% 93.00% 95.00% 93.50% 92.25% 35.20% 32.40% +1.50% +7.25% +7.25% +12.00% +5.25% +9.00% +10.40% +6.80% Table 4: Performance on STG task. Notation \"\" means results from (Han et al., 2025). Notation denotes the performance gain relative to direct SFT on LLaDA-8B-Instruct. Setting GSM8K MATH500 GPQA MMLU_STEM ARC_C SAT Avg LLaDA-8B-Instruct LLaDA-8B-Instruct (SFT) C2DLM (ours) 28.79% 36.60% 80.36% 80.74% 29.24% 36.20% 81.96% 37.20% 29.46% +1.22% +1.00% +0.22% 58.74% 59.21% 60.42% +1.21% 85.75% 71.36% 60.27% 85.67% 75.00% 61.01% 86.26% 78.64% 62.32% +0.59% +3.64% +1.31% Table 5: Performance on diverse math and reasoning downstream datasets. Notation denotes the performance gain relative to direct SFT on LLaDA-8B-Instruct. dataset and construct training dataset containing 686 instances annotated by the GLM-4.5. We conducted manual random sampling of 50 instances to evaluate the causal graphs generated by the teacher model. Two instances (4% of the sampled data) failed to produce causal graphs due to decoding errors. Among the successfully decoded instances, the accuracy was 93.42% 1.41%. Detailed experimental procedures are provided in Appendix F. Both SFT and C2DLM are trained on this dataset, with the only difference being that C2DLM incorporates the causal prior loss. The resulting models are then evaluated on six downstream tasks. As shown in Table 5, C2DLM achieves consistent improvements across six test datasets, with an average gain of 1.31%. Notably, these gains are obtained using only 686 causally annotated examples. As performance improvements from scaling next-token prediction alone are approaching bottleneck, our pipeline highlights the potential of leveraging twodimensional supervision signals based on token interactions as promising direction for future scaling, with cost as low as 0.46$ per million tokens (see Appendix for details). 4.5 Ablation Study To analyze the effects of different components and hyperparameters, we conducted ablation studies on the parameters α and γ scheduler. α represents Setting α = 2 α = 3 w/o Sγ w/o V-aware α = 4 α = GSM8K MATH500 SAT Avg 81.43% 81.96% 81.65% 81.35% 81.65% 81.50% 37.60% 77.73% 65.58% 78.64% 65.93% 37.20% 74.09% 62.98% 33.20% 68.64% 61.33% 34.00% 76.82% 64.16% 34.00% 78.18% 65.49% 36.80% Table 6: Ablation study under different α and γ scheduler. Gray line (α = 3) is the default setting. And w/o Sγ means dont use γ scheduler. the degree of emphasis on causal relationships, and an appropriate level of emphasis contributes to improved model performance. Additionally, we evaluated an ablation of the V-aware strategy, in which the model performance was assessed without the weighting provided by the Value matrix (denoted as w/o V-aware). As shown in Table 6, within certain range, model performance first improves and then declines as α increases. We further examined the impact of the γ scheduler. Without γ scheduler, the performance of C2DLM declines on all datasets. When the V-aware strategy is not employed, and causal knowledge is directly injected, performance decreases. This is due to the direct manipulation of attention scores. Ignoring the influence of the Value matrix introduces substantial instability during training, which in turn leads to performance degradation. These results demonstrate the effectiveness of the V-aware strategy."
        },
        {
            "title": "5 Conclusion",
            "content": "To address the limitations of language models in reasoning, we propose C2DLM, new paradigm distinct from AR and DLM. C2DLM leverages automated pipelines to extract causal meta-knowledge and employs the V-aware Reattention mechanism to align attention. We propose the COT-OrderPerturb task to quantify the influence of language modeling priors, and we validate the effectiveness of C2DLM on Sudoku, STG, and six broad downstream tasks. C2DLM improves both the models reasoning ability and training efficiency. Furthermore, we reveal the risk of misalignment between attention mechanisms and the causal priors underlying natural language, which shows the potential of combining causality into language models."
        },
        {
            "title": "Limitations",
            "content": "Our experiments focus on the LLaDA-8B-Instruct model, but due to constraints in training resources and the base model, C2DLMs performance still lags behind the SOTA AR models. Furthermore, larger-scale DLMs remain underexplored, so the effectiveness of C2DLM on such models is still unknown. Limited by computational resources, we are unable to inject causal knowledge at scale during pretraining; the pretraining stage remains largely unexamined, and our current work focuses solely on the SFT phase. Causal knowledge in the real world is highly complex, and thus extracting causal structures and benefiting from them in more intricate causal graphs or ultra-long CoT remains significant challenge."
        },
        {
            "title": "Ethical considerations",
            "content": "A potential risk of this work lies in the possibility that the proposed method could be misused to inject illegal or unethical information into models. Therefore, we strongly urge users to ensure that all training datasets comply with relevant laws and ethical guidelines when applying this approach."
        },
        {
            "title": "References",
            "content": "Deepak Bhaskar Acharya, Karthigeyan Kuppan, and Divya. 2025. Agentic ai: Autonomous intelligence for complex goalsa comprehensive survey. IEEe Access. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025. Block Interpolating between autoregressive diffusion: arXiv preprint and diffusion language models. arXiv:2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, and 1 others. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Li. 2024. survey on generative diffusion models. IEEE transactions on knowledge and data engineering, 36(7):28142830. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, and 1 others. 2022. survey on incontext learning. arXiv preprint arXiv:2301.00234. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. 2024. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781. Kairong Han, Wenshuo Zhao, Ziyu Zhao, Ye Jun Jian, Lujia Pan, and Kun Kuang. 2025. CAT: Causal attention tuning for injecting fine-grained causal knowledge into large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 99159932, Suzhou, China. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR). 9 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. 2025. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. 1996. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. 2024. Llms cant plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. 2025. Train for the worst, plan for the best: Understanding token arXiv preprint ordering in masked diffusions. arXiv:2502.06768. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only weight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 70577075, Online. Association for Computational Linguistics. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, and 1 others. 2023. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. 2025a. survey on diffusion language models. arXiv preprint arXiv:2508.10875. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Large language diffusion models. arXiv preprint arXiv:2502.09992. Melissa Pan, Mert Cemri, Lakshya Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Kannan Ramchandran, Dan Klein, and 1 others. 2025. Why do multiagent systems fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications. Judea Pearl. 2009. Causality. Cambridge university press. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Raanan Rohekar, Yaniv Gurwicz, and Shami Nisimov. 2023. Causal interpretation of self-attention in pre-trained transformers. Advances in Neural Information Processing Systems, 36:3145031465. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. 2024. Massive activations in large language models. arXiv preprint arXiv:2402.17762. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. 2025. wd1: Weighted policy optimization for reasoning in diffusion language models. arXiv preprint arXiv:2507.08838. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, and Shanghang Zhang. 2025b. Branchgrpo: Stable and efficient grpo with structured branching in diffusion models. arXiv preprint arXiv:2509.06040. Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. 2025a. Time is feature: Exploiting temporal dynamics in diffusion language models. arXiv preprint arXiv:2508.09138. 10 Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. 2025b. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192. Matej Zeˇcevic, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. 2023. Causal parrots: Large language models may talk causality but are not causal. arXiv preprint arXiv:2308.13067. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. 2025. Accelerating diffusion large language models with slowfast: The three golden principles. arXiv preprint arXiv:2506.10848. Anpeng Wu, Kun Kuang, Minqin Zhu, Yingrong Wang, Yujia Zheng, Kairong Han, Baohong Li, Guangyi Chen, Fei Wu, and Kun Zhang. 2024a. Causality for large language models. Preprint, arXiv:2410.15319. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. 2025. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618. Yiquan Wu, Yifei Liu, Ziyu Zhao, Weiming Lu, Yating Zhang, Changlong Sun, Fei Wu, and Kun Kuang. 2024b. De-biased attention supervision for text classification with causality. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19279 19287. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: comprehensive survey of methods and applications. ACM computing surveys, 56(4):139. Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. 2021. Causal attention for vision-language tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9847 9857. Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2024. Beyond autoregression: Discrete diffusion for comarXiv preprint plex reasoning and planning. arXiv:2410.14157. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487. Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. 2025. Survey on evaluation of llmbased agents. arXiv preprint arXiv:2503.16416. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. 2025a. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216. Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, and 1 others. Inpainting-guided policy optimization for 2025b. diffusion large language models. arXiv preprint arXiv:2509.10396. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: humancentric benchmark for evaluating foundation models. Preprint, arXiv:2304.06364. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and 1 others. 2025. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223."
        },
        {
            "title": "A Details of Prompt",
            "content": "This appendix provides the prompt templates used for concept-level causal meta-knowledge extraction with Teacher LLM. By decomposing questions and extracting concept-level causal relations, we derive reasoning graphs, which serve as surrogate for the underlying causal priors of natural language. Teacher LLMs System Prompt ##Command: You are causality expert. Your task is to identify causal relationships in given problem and its solution, in order to generate two-dimensional supervisory signal between tokens. Specifically, you need to extract concepts in the chain of thought. These concepts should: 1.Reflect the essential reasoning process. 2.Carry richer information content. 11 3.Have stronger impact on predicting the final answer. 4.Be unique within the text, to avoid ambiguity during matching. 5.The extracted content must come strictly from the original text provided. The extraction format should be dictionary: { \"Effect1\": [Cause11, Cause12, ...], \"Effect2\": [Cause21, Cause22, ...], ... }.The dictionary key concept represents the effect, while the value list represents the causes. Therefore, in each segment of text, you need to carefully determine which concept is the cause and which is the effect that is, without the cause, the effect cannot be naturally obtained. It is best if the concepts can be linked end-to-end through causal relationships, forming complete reasoning map. Teacher LLMs In-context Demo Question: Given rational number, write it as fraction in lowest terms and calculate the product of the resulting numerator and denominator. For how many rational numbers between 0 and 1 will 20! be the resulting product? Answer: Given rational number between 0 and 1, we need to write it as fraction in lowest terms and calculate the product of the resulting numerator and denominator. We need to determine how many such rational numbers exist where the product = 20!. 1. Prime Factorization of 20!: - The prime factors of 20! are 2, 3, 5, 7, 11, 13, 17, and 19. These are 8 distinct primes. 2. Coprime Pairs: - For = 20! and gcd(a, b) = 1, each prime factor of 20! must be assigned entirely to either or b. This results in 28 = 256 coprime pairs (a, b). 3. Counting Pairs (a < b): - Since and are coprime and = 20!, each pair (a, b) and (b, a) are distinct. To find pairs where < b, we divide the total number of pairs by 2, excluding the case = (which is impossible since 20! is not 12 square). 4. Result: - The number of coprime pairs (a, b) with < is 256 2 = 128. Thus, the number of rational numbers between 0 and 1 for which the product of the numerator and denominator in lowest terms is 20! is 128 . ## Demo Output 1: { 2, 3, 5, 7, 11, 13, 17, and 19\": [\"The prime factors of 20!\"], \"This results in 28 = 256 coprime pairs (a, b)\": [\"2, 3, 5, 7, 11, 13, 17, and 19\", \"each prime factor of 20! must be assigned entirely to either (a) or (b)\"], \"each pair (a, b) and (b, a) are distinct\": [\"Since (a) and (b) are coprime and = 20!\"], \"The number of coprime pairs (a, b) with < is 256 2 = 128\" : [\"This results in 28 = 256 coprime pairs (a, b)\", \"we divide the total number of pairs by 2, excluding the case (a = b)\"] } ## Demo Input 2: Question: Square AIM has sides of length 10 units. Isosceles triangle GEM has base EM , and the area common to triangle GEM and square AIM is 80 square units. Find the length of the altitude to EM in GEM . Answer: Square (AIM E) has sides of length 10 units. Isosceles triangle (GEM ) has base (EM ), and the area common to triangle (GEM ) and square (AIM E) is 80 square units. We need to find the length of the altitude to (EM ) in GEM . 1. Vertices of the Square: - Place the square on coordinate system with vertices (A(0, 0)), (I(0, 10)), (M (10, 10)), and (E(10, 0)). 2. Isosceles Triangle (GEM ): - Base (EM ) is vertical from (E(10, 0)) to (M (10, 10)). - The apex of the triangle is to the left of (EM ) (outside the square for larger altitudes). \"the length of the altitude to (EM ) in GEM is 25 .\" : [\"Setting the area equal to 80: 100 500 } = 80 = = 25\"] 3. Coordinates of G: - Let the altitude from to (EM ) be h. The coordinates of are (10 h, 5) because the triangle is isosceles with (GE = GM ). 4. Equations of Lines: - Line (GE) has the equation = 5 - = (GM ) Line + 10 50 . has the x+ 50 . equation 5. Intersection with the Square: - The lines (GE) and (GM ) intersect the left edge of the square (x=0) at points (0, 50 ). ) and (0, 10 6. Area Calculation: - The overlap area is: Area = (cid:90) 10 0 (cid:32) (cid:0) 5 + 10 (cid:1) (cid:0) 5 + 50 (cid:33) dx (cid:1) Simplifying: (cid:0) 10 + 10 100 - (cid:82) 10 0 - Setting Area = 80: 100 500 = 25. Area (cid:1) dx = 100 500 . = 80 = = Thus, the length of the altitude to (EM ) in GEM is 25 . ## Demo Output 2: { \"The coordinates of (G) are (10 h, 5)\": [\"the triangle is isosceles with (GE = GM )\", \"Let the altitude from (G) to (EM ) be (h)\", \"Place the square on coordinate system with vertices (A(0, 0)), (I(0, 10)), (M (10, 10)), and (E(10, 0))\"], \"The lines (GE) and (GM ) intersect the left edge of the square (x=0) at points (0, 50 ) and (0, 10 50 )\": [\"Line (GE) has the equation = 5 + 50 \", \"Line (GM ) has the equation = 5 + 10 50 \"],"
        },
        {
            "title": "B Details of Hyperparameters",
            "content": "Since different tasks vary in sequence length and convergence speed, we assign task-specific training epochs and generation lengths at evaluation. The detailed configurations are as follows: For the COT-OrderPerturb task, we train for 10 epochs with generation length of 512. To fully examine the impact of the DLM generation paradigm without any AR strategy, we additionally set the block length to 512. For the Sudoku task, we train for 10 epochs with generation length of 256 at evaluation. For the STG task, the STG_E subset is binary classification problem, and we therefore train for 10 epochs. The more challenging STG_H subset is trained for 40 epochs. Since these tasks do not involve chain-of-thought reasoning, the generation length is fixed at 8. For the six downstream tasks, we follow the setup of (Zhao et al., 2025a; Tang et al., 2025) and train on the s1k-1.1 subset3, with the training context length set to 1600. At evaluation, the generation length is set to 512 for GSM8K, MATH500, and SAT, while all other choice tasks use generation length of 32. For the autoregressive model baselines, we uniformly adopt LoRA learning rate of 2 e4. Sudoku is trained for 8 epochs, while all other tasks are trained for 4 epochs. All training is conducted on 2 NVIDIA A100 40GB GPUs, with the random seed fixed at 42 across all experiments. For testing, GSM8K, MATH500, and SAT are run on 4 A100 40GB GPUs, while all other tasks are evaluated on 2 A100 40GB GPUs. Detailed implementation examples can be found in our code repository."
        },
        {
            "title": "C Generation Details of",
            "content": "COT-OrderPerturb Dataset The goal of this process is to generate chain-ofthought reasoning trajectories with controlled order perturbations while preserving the underlying 3https://huggingface.co/datasets/simplescaling/s1K-1.1 13 causal structure. Answer: C.1 Graph Construction and Chain-of-Thought Generation We define directed acyclic graph (DAG) template in which nodes represent abstract variables (e.g., Quasar, Flux, Radiant, Nova), and edges encode functional dependencies. Each non-source variable is associated with deterministic transformation rule, typically linear or nonlinear combination of its parent variables. Two source nodes (Zorin and Vortex) are sampled uniformly from the integer range [0,100], while all other variables are computed sequentially via topological ordering. The target variable Stardust is uniquely determined by this process, ensuring consistency across samples. Given the DAG and computed values, we construct step-wise reasoning traces. Each reasoning step includes: the functional rule applied (e.g., Quasar = (Zorin + Vortex) * 0.5 + 10), the input variables, and the evaluated output. When arranged in strict topological order, these steps form reasoning trajectory that faithfully reflects the data-generating process. Finally, the data generation process is shown in Figure 7. One example is as following: Figure 7: DAG used in data generation process. Yellow nodes are input variables. The Stardust variable is the final answer. C.2 Order Perturbations To examine robustness to reasoning irregularities, we apply controlled perturbations to the canonical reasoning sequence. The perturbation modes are: COT-OrderPerturb Example reverse (RE): complete reversal of all steps. Question: Please infer the value of the Stardust variable based on the variables below. The input variables are Zorin (value: 80) and Vortex (value: 79). COT: Quasar = (Zorin + Vortex) * 0.5 + 10 = 90 Flux = (Zorin - Vortex) * 0.6 + 20 = 21 Radiant = (Quasar + 2 * Flux) / 3 = 44 Nova = (Quasar - Flux + Zorin) / 3 + 5 = 55 Gravity = (Radiant * Quasar) / 120 + 8 = 41 Pulse = Radiant * 0.4 + Flux * 0.9 = 36 Helix = (Gravity + Pulse + Radiant) / 3 = 40 Echo = (Pulse - Flux) * 0.8 = 12 Comet = (Pulse + Gravity) * 0.6 + 2 = 48 Aether = (Echo + Gravity) * 0.5 = 26 Nebula = (Helix + Comet) / 2 + 3 = 47 Celestia = (Nebula + Aether + Echo) * 1.1 + 6 = 100 Stardust = int(Celestia * 0.7) = 70 Therefore, the final answer is 70. local reverse (LR): pairwise reversal within local consecutive steps. output first (OF): moving the final output computation to the beginning. DFS: depth-firststyle ordering based on node depth. random 3 (R1, R2, R3): three fixed random permutations of reasoning steps. no cot: omission of reasoning; only the final answer is given. These perturbations preserve the correctness of the final answer (Stardust) while only shuffling the intermediate reasoning trajectory. In total, 2000 unique base samples are generated, yielding multiple perturbed training subsets. We generate another 500 different samples as unique test dataset. The pseudo-code of the data generation process is shown in Algorithm 1. 14 Algorithm 1 COT-OrderPerturb Data Generation Require: Number of base samples , DAG template G, Perturbation modes Ensure: Test set , Training sets {Dm}mM 1: Initialize , Dm for all 2: Initialize seen signatures 3: Define perturbation modes: RE, LR, OF, DFS, R1 . . .4, no cot 4: while < do 5: Sample source nodes (Zorin, ortex) Uniform(0, 100) Evaluate all other nodes in via topological order Compute final target variable Stardust signature Create (Zorin, ortex, Stardust) if σ then continue = σ end if Add σ to Construct canonical chain-of-thought steps π Store canonical sample (Q, π, Stardust) into test set for all do Apply perturbation to steps π πm perturbed Store (Q, πm, Stardust) into Dm sample 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: 19: end while 20: return , {Dm}mM Cost Analysis of C2DLM We also analyze the cost of using GLM-4.5 for causal annotation. Specifically, we randomly select 100 examples as representative subset to estimate annotation costs and calculate both input and output token counts. For the input, the average token length is 865.2, and with an additional prompt overhead of 1981 tokens. For the output, the average length is 295.3 tokens. Formally, the average cost for one token is computed as: Cost = 1 865. (Tin Pin + Tout Pout), where Tin and Tout denote the total input and output token counts. So Tin = 2846.2, Tout = 295.3. And the official pricing of GLM-4.5 is Pin=0.8 RMB/M tokens, Pout=2.0 RMB/M tokens. 15 Substituting the empirical statistics: Cost = 1 865. (2846.20.8+295.32.0) = 3.31. Converting to USD (1 RMB 0.14 USD)4, the total annotation cost is about 0.46$ per million tokens. Attention Visualization of C2DLM This section provides supplementary analysis of the attention visualizations presented in the main text. Specifically, the attention map illustrates how tokens interact within the model. However, examining the attention map alone is insufficient. For instance, in the phenomenon of attention sink, disproportionate amount of attention is assigned to semantically insignificant tokens such as punctuation or prepositions. As compensation, the Value matrix of these tokens exhibit substantially lower norms compared to normal tokens. Consequently, using value-weighted attention offers more accurate visualization target. Our visualization results are shown in Figure 8 and 9. Based on the visualization, the following conclusions can be drawn: Regardless of whether value weighting is applied, direct fine-tuning of LLaDA fails to effectively differentiate among the three types of factors, leading to substantial decline in OOD performance. This occurs because the model primarily captures token correlations; when such correlations are disrupted in OOD settings, performance deteriorates significantly. In contrast, C2DLM effectively learns the underlying causal mechanisms of the model. As illustrated, the causal factors consistently exhibit greater importance than other factors, irrespective of whether value weighting is applied. Notably, although the Exercise attribute receives lower attention scores than Hormones in the raw attention map, its importance surpasses Hormones and ranks third once value weighting is considered. This highlights that weighted attention better captures the influence of value matrix norms across tokens, thereby providing more faithful basis for analyzing token interactions. 4The pricing unit of the GLM API is in CNY, approximately 1 CNY per million tokens, which is equivalent to about 0.14 USD based on the exchange rate as of October 2, 2025. Figure 8: Mask is the position which will be decoded as high or low (directly determines the final answer). Visualization of the attention matrices, presented from top to bottom: direct visualization of LLaDAs attention map; LLaDAs attention map weighted by the Value norm; direct visualization of C2DLMs attention map; and C2DLMs attention map weighted by the Value norm. Acci = 1 Ei (cid:88) score(ci,j, ei) (ci,j ,ei)Ei The overall causal accuracy across all evaluated instances is: Overall Accuracy = 1 (cid:88) i=1 Acci The experimental results are shown in Table 7, and can be summarized as follows: 1. Two instances failed to generate causal graphs due to decoding errors, accounting for 4% of the sampled data. 2. For instances with correctly decoded causal graphs, the accuracy is 93.42% 1.41%."
        },
        {
            "title": "G Use Of AI Assistants",
            "content": "We used generative AI, ChatGPT, to check for syntactic and grammatical errors in the manuscript. We carefully verified the correctness of the revised content."
        },
        {
            "title": "Generated Causal Graphs",
            "content": "Ideally, human experts should be employed for annotation. In this work, however, we use teacher model to reduce costs and facilitate potential scaling. Accordingly, we conducted human evaluation to verify the accuracy of the generated causal graphs. The evaluation procedure is detailed as follows: We randomly sampled 50 annotated instances from the dataset, representing 7.3% of the total data. Two human experts with undergraduate degrees in science and engineering independently conducted the evaluation. During the evaluation, the experts were allowed to use any online resources to search and cross-check concepts to ensure the accuracy of their assessments. For the causal graphs, given the absence of preestablished ground truth, we focused on the causal logical consistency of each edge. Specifically, for each effect, we checked whether the list of causes extracted by the LLM could logically account for it. If the causal relationship was correct, it was scored as 1; if the cause list was partially correct or incomplete, it was scored as 0.5; if incorrect, it was scored as 0. The evaluation metric is computed as follows. Let Ei = {(ci,j, ei)} denote the set of causal pairs, where ci,j is cause and ei is the corresponding effect for instance i, and let score(ci,j, ei) be the score assigned to each pair as described above. Then the accuracy for instance is: 16 Figure 9: Visualization of the weight distribution bar charts, presented from top to bottom: direct visualization of LLaDAs attention weights bar chart; LLaDAs attention weights weighted by the Value norm; direct visualization of C2DLMs attention weights bar chart; and C2DLMs attention weights weighted by the Value norm. 17 Evaluator Total Pairs Human 1 Human 2 311 311 #Score=1 280 271 #Score=0.5 28 29 #Score=0 Average Accuracy 3 11 94.84% 92.02% Table 7: Evaluation results for human assessment of causal graphs."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Zhejiang University",
        "Noahs Ark Lab, Huawei Technologies"
    ]
}