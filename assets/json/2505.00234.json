{
    "paper_title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks",
    "authors": [
        "Vishnu Sarukkai",
        "Zhiqiang Xie",
        "Kayvon Fatahalian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 3 2 0 0 . 5 0 5 2 : r Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian Stanford University"
        },
        {
            "title": "Abstract",
            "content": "Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineeringsuch as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining database of self-generated examples. We demonstrate that even naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorldmatching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers compelling alternative to labor-intensive knowledge engineering."
        },
        {
            "title": "Introduction",
            "content": "When creating LLM agents for sequential decision-making tasks, practitioners often improve agent performance by investing in task-specific knowledge engineering (through tedious prompt tuning (Wei et al., 2022), human-crafted in-context examples (Brown et al., 2020; Wei et al., 2023) or custom observation and action spaces (Chen et al., 2024; Yang et al., 2024)). Using these techniques, scaling agent performance comes from scaling human effort. In this paper, we investigate an alternative path: enabling LLM agents to autonomously bootstrap their own performance by leveraging their own successful experiences via incontext learning. The efficacy of in-context learning depends critically on both the quality of the examples (Brown et al., 2020; Wei et al., 2023) and their relevance to the current decision point (Aky urek et al., 2022; Von Oswald et al., 2023; Agarwal et al., 2024). This insight provides natural direction for automated agent self-improvement: accumulating successful self-generated trajectories and estimating the most relevant and effective prior experiences to use as in-context examples at each step of the decision-making process. Our work assumes ReAct-style agent (Yao et al., 2023) that retrieves different examples for each decision point based on their relevance to the current situation (Kagaya et al., 2024; Zhou et al., 2024). We build on this foundation by focusing specifically on how to construct and refine the underlying database of self-generated examples. How can we identify which trajectories enhance performance on new tasks versus those that hinder performance? This database construction problem requires addressing both the collection 1 of high-quality trajectories and the strategic selection of the most valuable ones for future retrieval at each decision point in the agents reasoning and acting loop. We demonstrate that naive database accumulation improves test-set performance from 73% to 89% on ALFWorld, 55% to 64% on Wordcraft, and 75% to 79% on InterCode-SQL. On all benchmarks, the improved agent matches the performance the base agent would achieve if it were allowed two to three attempts per task. Building upon these results, we propose two database construction enhancements: (1) database-level selection through an approach that identifies and propagates high-performing example databases, and (2) exemplar-level selection that identifies helpful in-context example trajectories based on their empirical utility as in-context examples for future tasks. These approaches boost success on ALFWorld to 91%matching more complex approaches like AutoManual (Chen et al., 2024) that employ per-benchmark hand-crafted promptsand further boost Wordcraft and InterCode-SQL to 81% and 68% respectively. Our approach achieves competitive performance while avoiding task-specific prompt engineering (Fu et al., 2024; Chen et al., 2024) or observation/action spaces (Zhou et al., 2024; Chen et al., 2024)."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Sequential Decision-Making Tasks We focus on multi-step sequential decision-making tasks where agents must produce series of actions over time based on observations of the environment. The sequential nature of these tasks introduces unique challenges for LLM agents, as they must interpret intermediate environmental feedback, maintain coherent reasoning across multiple steps, and adapt their strategy based on the evolving task state. This contrasts with one-shot generation tasks (e.g., solving math problems (Hendrycks et al., 2021), one-shot code generation (Jimenez et al., 2023)) where feedback is only available after the complete solution is provided. Our example-driven learning strategy is potentially also suitable to single-step decision-making tasks, but we focus on the multi-step setting due to its applicability to number of agentic tasks in real-world settings (embodied agents (Song et al., 2023), browser-based tasks (He et al., 2024), etc.). Formally, these tasks can be modeled as Partially Observable Markov Decision Processes (POMDPs), represented by the tuple (S, O, A, , R, γ), where denotes the underlying state space, the observation space, the action space, : defines the deterministic transition function, : is the reward function, and γ [0, 1] is the discount factor. The partial observability reflects that agents dont have direct access to the full environment state but rather receive observations that provide limited information. Given task goal g, an episode consists of the agent interacting with the environment for maximum of timesteps. At each timestep t, the agent receives an observation ot of the current state, takes an action at A, and the environment transitions to the next state according to the transition function . In our setting, we specifically consider sparse-reward environments where success is only determined at the end of an episodethe agent receives = 1 for successful task completion and = 0 otherwise. This is standard setting in prior agentic work (Yao et al., 2023; Zhao et al., 2024; Fu et al., 2024; Chen et al., 2024). 2.2 ReAct-style Agent Loop Our work assumes ReAct-style (Yao et al., 2023) agent architecture that employs recent best practices for in-context retrieval (Kagaya et al., 2024; Zhou et al., 2024). The agent operates through three-phase approach (planning, reasoning, and acting) as formalized in Algorithm 1. Two key components differentiate our implementation from basic ReAct: To support complex, long-horizon tasks, we incorporate an initial planning step where the agent generates high-level plan for the entire task before execution begins ( 1, line 3). Full-task planning has been shown to boost agent performance in prior work (Kagaya et al., 2024; Song et al., 2023). This modification is fairly standardeither built directly into the algorithm (Kagaya et al., 2024), or included 2 3: 4: 5: 6: 7: 8: 9: Algorithm 1 ReAct-style Agent Loop 1: function AGENT(g, D, , T) 2: Cp Retrieve(D, keys = [g]) LLMplan(g, Cp) Initialize τ (g, p, {}, ) o1 .obs() C1 Retrieve(D, keys = [g, p, o1]) for = 1 to do rt LLMreason(τ, ot, Ct) Ct+1 Retrieve(D, keys = [g, p, rt]) at LLMact(τ, ot, rt, Ct+1) ot+1, done, .step(at) τ τ (ot, rt, at) if done then return (g, p, {(oi, ri, ai)}t i=1, s) end for return (g, p, {(oi, ri, ai)}T i=1, 0) 10: 11: 12: 13: 14: 15: 16: 17: 18: end function end if Retrieve for plan Generate initial plan Retrieve for current observation Generate reasoning Retrieve for current reasoning Decide action Execute action in environment Failed due to timeout in the first reasoning step of human-crafted in-context examples (Zhao et al., 2024; Chen et al., 2024). Rather than using the same per-task examples throughout an episode (Yao et al., 2023; Zhao et al., 2024; Fu et al., 2024), we follow retrieve different trajectory segments for each decision point, ensuring the agent has access to the most relevant information at each step (Kagaya et al., 2024; Zhou et al., 2024). See Appendix for details. The agent operates through three key LLM-based functions: 1. LLMplan generates high-level plan for achieving the goal 2. LLMreason processes the current observation ot to produce reasoning rt 3. LLMact determines the appropriate action at based on the reasoning The Retrieve() function selects the most relevant examples from database based on the average cosine distance from the provided lookup keys to the corresponding examples in Dsee Appendix and Algorithm 4 for details. The environment provides observations and processes actions, returning the next observation, termination signal, and the success indicator when the episode ends. This dynamic retrieval approach is critical for sequential decision-making tasks, as it allows the agent to access specialized knowledge relevant to each unique situation encountered during task execution. Our contribution builds upon this architecture by focusing specifically on how to construct and refine the underlying trajectory database that powers this retrieval mechanism. Note that Algorithm 1 avoids strategies that employ task-specific prompting, observation spaces (Zhou et al., 2024) or action spaces (Yang et al., 2024; Chen et al., 2024). The only task-specific knowledge is encapsulated in the content of the trajectory database D. For simplicity, we eschew other techniques, like hierarchical learning (Zhao et al., 2024; Fu et al., 2024; Chen et al., 2024), that are also task-agnostic, but add additional agent complexity. We view the benefits of hierarchical learning as orthogonal and complimentary to our database construction focus."
        },
        {
            "title": "3 Problem Statement",
            "content": "Given the ReAct-style agent described in Section 2, our goal is to construct trajectory database that maximizes LLM agent performance across sequential decision-making tasks. We focus specifically on how to build and refine the database of examples that the agent retrieves from at each decision point. Formally, we aim to construct database of trajectories, where each trajectory τ captures complete task attempt: τ = (g, p, {(ot, rt, at)}T t=1, s) Our objective is to maximize the agents expected performance across distribution of tasks : = arg max E gT [Success(Agent(g, D, , T))] where Success() returns the binary outcome of the agents execution. We assume that we are given: (1) initialized with small number of human-generated trajectories, (2) descriptor of the action space, and (3) access to set of training tasks drawn from that the agent can attempt. All three assumptions are typical in ReAct-based agentic setups (Yao et al., 2023; Kagaya et al., 2024; Zhao et al., 2024; Fu et al., 2024; Chen et al., 2024). Given this setup, our focus is on the agent self-generating trajectories, then choosing the trajectories that should be added to to maximize the agents expected performance on novel tasks. This problem presents two key questions: Scaling Trajectory Collection: As we gather more successful trajectories, does larger database provide better guidance for future tasks? Intelligent Trajectory Selection: How can we determine which subset of collected trajectories will most effectively support the agent in solving new tasks?"
        },
        {
            "title": "4 Related Work",
            "content": "Our work intersects with research in prompt optimization, in-context learning, and agent self-improvement. We focus on in-context approaches, and leave exploration of selfimprovement approaches involving weight updates to future work. Alternatives to an In-Context Approach Despite the current popularity of reinforcement learning-based approaches for improving agent capabilities (Bai et al., 2022; Rafailov et al., 2023; Jaech et al., 2024; Guo et al., 2025), in-context learning offers distinct scientific and practical advantages. First, in-context methods provide portability across different LLMs without requiring model-specific optimization. Second, they demonstrate particular efficiency in low-sample regimes (Wei et al., 2023; Bertsch et al., 2024). In contrast to in-context methods, weight-based adaptation approaches like fine-tuning face substantial implementation barriers. Using RL to train an LLM demands extensive computational resources (Bai et al., 2022; Guo et al., 2025), while supervised fine-tuning requires training data. While it is of course possible to fine-tune on the trajectories we collect, we focus on in-context methods that extract maximum value from limited examples without modifying model weights, and leave exploration of complementary fine-tuning or distillation approaches to future work. Automatic In-Context Examples Recent work has demonstrated the effectiveness of optimizing both instructional content and example selection in prompts. DSPy (Khattab et al., 2023) introduced framework for optimizing multi-stage reasoning pipelines through 4 instruction tuning and strategic example selection. Bootstrapped examples containing intermediate reasoning traces can eliminate the need for human-written examples, and bootstrap examples often contribute more to performance than optimized instructions alone (OpsahlOng et al., 2024). These approaches typically require supervised optimization procedures to identify effective examples, whereas our method enables autonomous example selection through empirical performance tracking. Scalable In-Context Learning Empirical and theoretical work has established the scaling properties of in-context learning. Previous work has shown that in-context performance continues to improve with additional examples up to context window limits (Bertsch et al., 2024), while large-scale in-context learning can outperform fine-tuning for many tasks (Agarwal et al., 2024). Theoretical analyses have connected in-context learning to classical machine learning methods like linear regression (Aky urek et al., 2022; Von Oswald et al., 2023), providing foundation for understanding its scaling behavior. These findings suggest that example accumulation should lead to performance improvementsa hypothesis our work confirms through empirical evaluation across diverse agent tasks. We additionally hypothesize that database quality, not just quantity, influences performance scaling. In-Context Self-Improvement of LLM Agents Self-improvement methods for LLM agents operate either aim to solve one task (essentially performing search/optimization to solve the task) or use knowledge from prior, different tasks to inform how to solve novel task (generalization). See Appendix B.2 for an in-depth discussion. Approaches to solve single task search the state space by scaling the number of sampled solutions (Brown et al., 2024; Wang et al., 2024a;b), or incorporate feedback from failed attempts into subsequent attempts (Shinn et al., 2023). These methods do not address knowledge transfer across distinct tasks. Approaches to use knowledge from prior, different tasks to solve novel tasks include abstraction-based methods that transform trajectories into higher-level representations: ExpeL (Zhao et al., 2024) extracts voting-based rules, and AutoGuide (Fu et al., 2024) generates contextual guidelines. Multiple approaches solving this problem employ taskspecific information in their agent designRAP (Kagaya et al., 2024) uses task-specific prompts for retrieving in-context examples, and AutoManual (Chen et al., 2024) constructs task-specific state and action spaces to simplify agent learning (see Appendix B.1 for details). Other dimensions of self-improvement include hierarchical execution (Wang et al., 2023) and optimization techniques for multi-stage agentic systems (Saad-Falcon et al., 2024; Hu et al., 2024; Zhang et al., 2024)techniques complementary to our approach. Rather than developing complex agent architectures or transformation mechanisms, we focus on identifying which trajectories most consistently contribute to successful outcomes when used as retrieved examples."
        },
        {
            "title": "5 Methods",
            "content": "In this section, we discuss three algorithms for constructing database using continual collection approach. 5.1 Traj-Bootstrap: Constructing Database of Previously-Solved Tasks Our trajectory-bootstrapping algorithm Traj-Bootstrap constructs trajectory database by collecting successful agent experiences. As outlined in Section 3, we start with minimal set of human-provided exemplars (which could be empty), then grow the database as the agent successfully completes training tasks. This process creates positive feedback loop where successful examples help the agent solve new tasks, generating more successful examples. 5 for = 1, 2, ..., in parallel do Initialize performance metrics {m1, m2, ..., mN} for each database for = 1, 2, ..., Ttrain do Algorithm 2 Database Selection Logic for +DB-Selection 1: procedure OPTIMIZEDATABASES({D1, D2, ..., DN}, interval) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end procedure Execute task using database Di If successful, add trajectory to Di Update rolling performance metric mi on recent tasks Sort databases by rolling performance on recent tasks Replace worst database with copies of best end for if = 10 2j for any then end for end if Traj-Bootstrap operates on principles similar to reward-weighted regression in reinforcement learning (Peters & Schaal, 2007), where only successful trajectories (s = 1) are stored in the database. This filtering mechanism ensures the agent learns from positive examples while avoiding potentially misleading failed attempts. Successful trajectories can be leveraged by asking the agent to imitate the successful patterns in these trajectories. However, failed trajectories are more challenging to operationalize due to the credit attribution problem: it is necessary to identify the good vs bad parts of the trajectory, before we can even guide the agent to imitate the good parts and avoid the mistakes made in the bad parts. Failed trajectories do offer the opportunity to guide exploration (Shinn et al., 2023); we leave this direction to future work. 5.2 +DB-Selection: Database-Level Data Selection The Traj-Bootstrap algorithm displays unpredictable performance variation across training trials, even when following identical collection procedures. Figure 1 illustrates this variation across five trials on the InterCodeSQL benchmark (a benchmark we use for evaluation in Section 6). The variance arises from two factors: (1) the stochasticity of LLM outputs creating different initial trajectories, and (2) an amplification effect where early differences in collected examples lead to wide performance variation. This observation motivates data-selection strategy inspired by population-based training in reinforcement learning (Jaderberg et al., 2017). Figure 1 shows that some databases lead to better task performance than othersso we identify the underperforming databases periodically during training and remove them, continuing growth from top-performing ones. We introduce +DB-Selection, population-based training algorithm (Algorithm 2) to identify and propagate the most effective databases during the bootstrapping process. Figure 1: Traj-Bootstrap leads to variance in test-time success rate. Individual trials (5) shown as dashed lines, results on Intercode-SQL benchmark. There is noticeable variability in performance across trials. We maintain database instances initialized with identical human-provided exemplars. Each instance is used by separate agent, accumulating successful trajectories independently. We trigger selection events when the number of tasks attempted reaches size thresholds (starting at size 10 and doubling thereafter: 10, 20, 40, 80, etc.). At each threshold, we evaluate database performance based on the agents success rate on all training tasks since the last threshold, and we replace the worst-performing database with copy of the topperforming database. 6 Algorithm 3 Database Construction from Top Exemplars for +Exemplar-Selection 1: procedure SELECTEXEMPLARS({D1, D2, ..., DN}, Ttrain) 2: Dcomposite Compute quality metric Q(τ) for each trajectory τ (cid:83)N for each task Ttrain do i=1 Di Tt {successful trajectories for task across all databases} if Tt is not empty then Select top-1 trajectory from Tt by quality metric Add selected trajectory to Dcomposite 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end procedure end if end for return Dcomposite The key insight of this approach is that database quality emerges from collective propertieslike coverage, diversity, and complementarity across examplesnot just individual trajectory quality. Moreover, single trajectory collected early in training can influence many future trajectories by guiding the agent toward particular solution strategies, creating cascading database-level effects. By selecting and propagating entire databases, we preserve these beneficial emergent properties while using simple, computationally efficient evaluation metric based on recent performance. 5.3 +Exemplar-Selection: Exemplar-Level Data Selection While the database-level selection performed by +DB-Selection identifies entire sets of complementary trajectories, discarding whole databases can eliminate valuable trajectories. We find that even poor-performing databases contain individual high-quality trajectories that yield better outcomes when used as examples. Conversely, some trajectories marked as successful lead to failures when retrievedbecause some trajectories lead to success in spite of some bad decisions that would be bad to repeat. Explicit identification of high-performing trajectoriesthose that exemplify generalizable reasoning patterns, as opposed to trajectories containing incorrect reasoning or actions (see Appendix for further analysis)can improve efficiency compared to wholesale database removal. This observation motivates Exemplar-Selection: identifying and selecting individual highquality exemplars across multiple database instances based on their empirical utility as in-context examples. This approach parallels value-function learning in reinforcement learning (Barto, 2021), where we estimate the value of each trajectory based on its contribution to successful outcomes. We introduce retrieval-weighted quality metric analogous to value function to quantify each trajectorys contribution to successful outcomes: Q(τ) = iR(τ) oi fi(τ) iR(τ) fi(τ) (1) where R(τ) is the set of tasks for which trajectory τ was retrieved, oi is the binary outcome of task i, and fi(τ) is the retrieval frequency during task i. This value metric measures how often trajectory is associated with successful outcomes when retrieved as an in-context example. It weights outcomes by retrieval frequency, prioritizing trajectories frequently retrieved during successful completions while penalizing those associated with failures. Algorithm 3 outlines our exemplar-level selection approach. For each task in the training set, we identify all successful trajectories across database instances and select only the exemplar with the highest value according to the metric. This approach constructs composite 7 database containing only the most effective exemplars as measured by their empirical contribution to successful outcomes on subsequent tasks. 5.4 Train-Time vs Test-Time LLM Costs The selection methods (+DB-Selection and +Exemplar-Selection) require times more LLM inference calls during training compared to Traj-Bootstrap, as they maintain parallel database instances. However, they do not require additional training tasks - all methods use the same task distribution and quantity. At test time, all three methods have identical computational costsAlgorithm 1 is simply provided with different database for each method. This contrasts with approaches that scale the number of LLM calls per test-time task in order to improve performance (Brown et al., 2024; Wang et al., 2024a;b; Shinn et al., 2023). Our methods shift computational burden to training while maintaining efficient inference, property our in-context methods share with fine-tuning methods."
        },
        {
            "title": "6 Experiments",
            "content": "We evaluate our trajectory database construction methods through experiments addressing three key questions: Database scaling: How does task success rate scale with increasing database size? Improving database construction: How much do population-based training and exemplar-level selection improve task success rate? Overall effectiveness: How do our approaches compare to alternative approaches? 6.1 Experimental Setup 6.1.1 Benchmark Tasks We evaluate our methods on three benchmarks: ALFWorld (Shridhar et al., 2020): text-based environment for navigation and object manipulation through textual commands, requiring exploration and sequential reasoning. InterCode-SQL (Yang et al., 2023): An interactive coding environment where agents generate SQL queries to answer user question, requiring understanding of database structures and query semantics. Wordcraft (Jiang et al., 2020): simplified adaptation of Little Alchemy, where agents combine elements to create new ones through multi-step processes, requiring compositional reasoning. These benchmarks were selected because they: (1) provide large enough task pools to support meaningful train/test splits, (2) represent diverse reasoning challenges relevant to sequential decision-making, and (3) have been used in prior work, enabling direct comparisons with existing methods. 6.1.2 Methods Compared Our methods include: Fixed-DB: The baseline agent as described in Section 2, with fixed database of human-provided initial examples and no database growth. Traj-Bootstrap: simple database construction approach that progressively accumulates successful trajectories into growing database. Each task in the training set of given benchmark is attempted once, generating single trajectory that is added to the database if successful. Method ALFWorld InterCode-SQL Wordcraft Fixed-DB Traj-Bootstrap Traj-Bootstrap+DB-Selection Traj-Bootstrap+Exemplar-Selection 0.73 0.89 0.91 0.90 0.75 0.79 0.78 0.81 0.55 0.64 0.64 0. Table 1: Average success rate of our methods: self-collected trajectories provide the largest boosts in task success rate. Traj-Bootstrap outperforms Fixed-DB across all three benchmarks. +Exemplar-Selection provides an additional boost on InterCode-SQL and Wordcraft, while +DB-Selection boosts ALFWorld. Figure 2: Traj-Bootstrap results: success rate improves with increasing training tasks across all three benchmarks. Individual trials (5) shown as dashed lines. All benchmarks exhibit diminishing returns as the database size increases. Trials show substantial performance variability, both within individual trials and across different trials. Traj-Bootstrap+DB-Selection: Our database-level trajectory selection approach that implements Algorithm 2 to maintain multiple trajectory databases and selectively propagate the ones with the highest success rates. Traj-Bootstrap+Exemplar-Selection: Our exemplar-level trajectory selection approach that implements Algorithm 3 to identify and preserve individual highquality trajectories based on their empirical contribution to successful outcomes. We compare these methods to two more advanced hierarchical designs. Autoguide (Fu et al., 2024) converts successful trajectories into explicit rules and retrieves the most contextually relevant rules, alongside low-level trajectories, at inference time. AutoManual (Chen et al., 2024) leverages hand-crafted task-specific observation and action spacessee Appendix B.1 for details. 6.1.3 Implementation Details Unless otherwise specified, we use GPT-4o-mini as our base LLM (temperature 0.1). For Fixed-DB and all Traj-Bootstrap agents, we retrieve the top-k most similar trajectories at each decision step (k = 6 for ALFWorld and InterCode-SQL, 10 for Wordcraft). We initialize each database with small human-provided example set (18 for ALFWorld, 10 for InterCode-SQL, 4 for Wordcraft). With +DB-Selection, we maintain five database instances with selection every time the database size is doubled, starting with minimum size of ten trajectories. We report success rates averaged over five random seeds. By default, we report success rate given the database at the end of the training process. 6.2 Traj-Bootstrap Results Traj-Bootstrap Performance Improves with More Training Tasks The performance of Traj-Bootstrap generally improves with increases in the number of training tasks attempted (Figure 2) On ALFWorld, success rate increases from 0.73 to 0.89 as the database grows from 18 initial examples to 3500 accumulated trajectories. InterCode-SQL improves from 0.75 to 9 Method LLM(s) Num Training Tasks ALFWorld Autoguide Automanual Automanual Fixed-DB Fixed-DB Traj-Bootstrap +DB-Selection +Exemplar-Selection gpt-3.5-turbo + gpt-4-turbo gpt-4o-mini gpt-4-turbo + gpt-4o-mini gpt-4o-mini gpt-4o gpt-4o-mini gpt-4o-mini gpt-4o-mini gpt-4o-mini gpt-4o-mini gpt-4o-mini 100 0.79* 36 36 0 0 100 3500 100 3500 100 0.72 0.91 0.73 0.88 0.84 0.89 0.86 0.91 0.86 0.90 Table 2: Comparison of agent success rates on ALFWorld: contextualizing the performance of Traj-Boostrap. The task success rate from database construction via Traj-Bootstrap (0.89) compares favorably to the boost from upgrading Fixed-DB from gpt-4o-mini to gpt-4o (0.88). The performance of Traj-Bootstrap+DB-Selection matches Automanual with gpt-4turbo+gpt-4o-mini, matching an approach both with hand-designed observation and action spaces and with better LLM. * indicates results reported from original papers. Figure 3: Task success rate comparison between Traj-Bootstrap, Traj-Bootstrap+DBSelection, and Traj-Bootstrap+Exemplar-Selection. +DB-Selection enhances final success rate only on ALFWorld, but improves early-stage success rate (smaller DB sizes) across all benchmarks. +Exemplar-Selection delivers success rate gains on both Intercode-SQL and Wordcraft. 0.79 and Wordcraft from 0.55 to 0.64. Table 1 presents the final success rate metrics for our database construction methods. Performance continues to improve with more training tasks across all benchmarks, but exhibits diminishing returnsmost gains occur within the first 25% of added training tasks. This efficiency decline occurs because each new example is retrieved less frequently as the database grows, influencing fewer generations, pattern consistent with findings from Bertsch et al. (2024) and Agarwal et al. (2024). As mentioned in Section 5, we observe performance variability across trials and within individual trials. Cross-trial variance indicates that some trials produce higher-performing databases than others when solving identical tasks. Within-trial fluctuations show that certain added trajectories can degrade overall database performance. These patterns highlight the potential value of our +DB-Selection and +Exemplar-Selection algorithms. +DB-Selection boosts performance on ALFWorld Figure 3 illustrates how +DB-Selection can improve upon Traj-Bootstraps performance, despite exhibiting occasional performance 10 Figure 4: The best bootstrapped trajectories compared to the worst. Databases constructed from the highest-quality successful trajectory per task, as measured by our metric from Section 5.3, outperform databases built from the lowest-quality successful trajectories on both ALFWorld and Wordcraft. The best curve is identical to +Exemplar-Selection, while the worst curve selects the bottom-1 trajectory instead of top-1 in Algorithm 3, line 7. dips at smaller database sizes. These dips result from inaccurate estimates (due to low sample count) of database quality early in the processintroducing noise into the selection process. As discussed in Section 5.4, +DB-Selection incurs additional train-time LLM costs to construct multiple independent databasesbut requires neither extra training tasks nor additional test-time costs. +Exemplar-Selection boosts performance on InterCode-SQL and Wordcraft As seen in Table 1, +Exemplar-Selection yields improvements in final task success rates on InterCodeSQL (0.79 to 0.81) and Wordcraft (0.64 to 0.68), with more minor imporvements on ALFWorld (0.89 to 0.90). +Exemplar-Selection also boosts success rate at intermediate database sizes for both InterCode-SQL and Wordcraft (Figure3). To further highlight the impact of our exemplar-level selection metric, Figure 4 compares databases built from the best trajectories that are the most empirically effective in-context examples versus the least effective trajectories, as determined by Equation 1 in Section 5.3. The best curve is identical to +Exemplar-Selection, while the worst curve selects the bottom-1 trajectory instead of top-1 in Algorithm 3, line 7. Using the database of highquality examples yields higher success rate across all database sizes for ALFWorld and Wordcraft, and for smaller database sizes for InterCode-SQL. 6.3 Contextualizing performance boosts from Traj-Bootstrap To contextualize the improvements gained through our trajectory bootstrapping approaches, we compare Traj-Bootstrap with several alternative strategies: test-time sampling, using better LLM, using hierarchical strategies, and task-specific strategies. Comparison with Test-Time Scaling To contextualize the improvements gained through our trajectory bootstrapping approaches, we compare them against an alternative strategy: simply making multiple attempts at each test task with the Fixed-DB baseline and selecting the best outcomean approach that only applies when its permissible to attempt tasks multiple times at test time, and the agent has access to verifier of success. Table 3 reports the pass@k metrics for Fixed-DB across all three benchmarks, representing the probability of at least one successful attempt when making independent attempts at each task. Our basic Traj-Bootstrap approach achieves success rate comparable to Fixed-DB pass@2-pass@3 on all three benchmarksbut with only single attempt per task. Our enhanced methods (with +DB-Selection or +Exemplar-Selection) perform nearly on the level of Fixed-DB pass@3 on ALFWorld, pass@4 on InterCode-SQL, and Fixed-DB pass@5 for Wordcraft. This comparison highlights that our trajectory bootstrapping approaches achieve success rate improvements equivalent to making multiple task attemptsan advantage in scenarios 11 Number of attempts (k) ALFWorld Intercode-SQL Wordcraft 1 2 3 4 5 0.73 0.87 0.92 0.94 0. 0.75 0.78 0.80 0.81 0.82 0.55 0.62 0.64 0.66 0.68 Table 3: Pass@k of Fixed-DB on all benchmarks. With only single attempt per task in the test set, our Traj-Bootstrap achieves performance between the performance achieved by Fixed-DB at pass@2 and at pass@3 on all three benchmarks. Therefore, the boost from our database construction procedure is comparable to having 2-3 attempts per novel task, paired with ground-truth solution verifier. where multiple attempts are impractical or when success verification is unavailable at test time. Furthermore, our approaches provide these benefits without requiring any modifications to the test-time inference process. Comparsion with Model Upgrades On ALFWorld, Traj-Bootstrap yields slightly greater improvement than upgrading to more powerful LLM. Traj-Bootstrap with GPT-4o-mini reaches 0.89 success rate after 3500 training tasks, outperforming Fixed-DB with GPT-4o, which achieves 0.88  (Table 2)  . For context, Fixed-DB with GPT-4o-mini only achieves 0.73, highlighting the boost provided by either database construction or model scaling. Comparison with Hierarchical Algorithms Given 100 training tasks, Autoguide, hierarchical rule-learning approach, achieves 0.79 success rate when using combination of gpt-3.5-turbo + gpt-4-turbo, while given the same number of training tasks our best approach achieves 0.86 success rate with gpt-4o-mini  (Table 2)  . While this comparison is across different algorithms using different LLMs, the performance of Traj-Bootstrap suggests that self-constructed databases of low-level trajectories can be competitive with hierarchical approaches. Comparison with Task-Specific Strategies Table 2 shows that Traj-Bootstrap+DB-Selection achieves 0.91 success rate, surpassing AutoManual when using combination of GPT-4turbo and GPT-4o-mini (0.91). Our database construction strategy matches the performance of an algorithm using more powerful LLMs with customized observation and action spaces."
        },
        {
            "title": "7 Discussion",
            "content": "Our work demonstrates that improvements in LLM agent performance can be achieved through principled construction of trajectory databases, without requiring task-specific human engineering. Our selection mechanisms identify trajectories that improve success rate on novel tasks. The success of our approach reveals performance gains that stem primarily from accumulating successful examples, establishing foundation for agent self-improvement where the quantity and quality of accessible data rivals the importance of architectural complexity. This parallels trends in traditional deep learning, where data curation often yields substantial improvements. Our findings point to promising research directions that approach LLM agent enhancement from data-centric perspectiveadvancing both strategic data collection methods (balancing exploration versus exploitation across diverse tasks) and refined filtering techniques to maximize performance. Acknowledgments Thank you to Brennan Shacklett, Purvi Goel, Zander Majercik, William Wang, Bradley Brown, Jon Saad-Falcon, and William Mark for valuable discussions and feedback. Support for this project was provided by Roblox and Meta, and API credits were provided by OpenAI and together.ai."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, et al. Many-shot in-context learning. Advances in Neural Information Processing Systems, 37:7693076966, 2024. Ekin Aky urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Andrew Barto. Reinforcement learning: An introduction. by richards sutton. SIAM Rev, 6(2):423, 2021. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, and Xiaofei He. Automanual: Generating instruction manuals by llm agents via interactive environmental learning. arXiv preprint arXiv:2405.16247, 2024. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of context-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 13 Minqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip HS Torr, Shimon Whiteson, and Tim Rocktaschel. Wordcraft: An environment for benchmarking commonsense agents. arXiv preprint arXiv:2007.09185, 2020. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610, 2024. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. Krista Opsahl-Ong, Michael Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs. arXiv preprint arXiv:2406.11695, 2024. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745750, 2007. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/ abs/1908.10084. Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Re, et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre ˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language In Proceedings of the IEEE/CVF international conference on computer vision, pp. models. 29983009, 2023. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 3515135174. PMLR, 2023. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024a. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 14 Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023. Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: simple yet strong baseline for llm-based web agents. arXiv preprint arXiv:2410.13825, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762, 2024. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1963219642, 2024. Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, and Weinan Zhang. Trad: Enhancing llm agents with step-wise thought retrieval and aligned decision. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 313, 2024."
        },
        {
            "title": "A Key agent details",
            "content": "In Section 2, we establish an agent design that enables it to learn in-context from its own self-collected experiences. Here, we elaborate on few key design decisions in our agent design: Standardized prompts: we use the same simple prompt templates for all tasks, rather than writing new prompts per task. These prompts are in Appendix D. Two-level retrieval: We retrieve trajectories at both trajectory level (for planning) and state level (for reasoning and action selection), enabling the agent to leverage both strategic patterns and situation-specific techniques. Database contains selfcollected trajectories, and retrieval is performed at the trajectory level for the initial plan p, and in the state-level observation-reasoning-action loop for both rt and at. Multi-key retrieval: All retrieval is performed by KNN, with similarity metric defined as the average of cosine similarities across the specified key variables. For instance, in Line 3 of Algorithm 1, we retrieve from using two keys: goal and plan p. We return similar trajectories based off the average of the cosine similarities of goals and plans when comparing each trajectory to the current trajectory. When doing state-level retrieval (Lines 7 and 10), we additionally find the most similar states within the selected trajectories via state-level key ot or rt, then return window of states around the most similar state. This is similar to the retrieval scheme in (Kagaya et al., 2024). See detailed pseudocode for retrieval in Algorithm 4. 15 Average similarity across trajectory keys State-level retrieval with window Algorithm 4 Multi-key Retrieval sim sim + CosineSimilarity(query[key], τ[key]) sim 0 for each key in traj keys do similarities [] for each trajectory τ in do end for sim sim/traj keys similarities.append(sim, τ) end for similar trajectories TopK(similarities, k) if state key is not None then windowed results [] for each trajectory τ in similar trajectories do 1: procedure MULTIKEYRETRIEVAL(D, traj keys, state key, query, k, window size) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end if 28: 29: end procedure end for , most similar state, idx Max(state similarities) window start max(0, idx window size/2) window end min(τ.states, idx + window size/2) windowed results.append(τ.states[window start : window end]) state sim CosineSimilarity(query[state key], s[state key]) state similarities.append(state sim, s, index(s)) state similarities [] for each state in τ.states do end for return windowed results return similar trajectories else Thought-based retrieval: For the first step of trajectory, we retrieve using the trajectory-level keys (g,p) as well as the current observation o1 (Algorithm 1, line 6)but for all subsequent steps we use reasoning rt as key instead of observation ot (Algorithm 1, line 9). This approach, inspired by Zhou et al. (2024), enables generalization across trajectories with similar reasoning, and similarity across natural-language rt can be handled by generic embedding functions more easily than potentially bespoke observations ot. By retrieving at every step, we aim to retrieve the most relevant trajectories for each decision. Generic embedding mechanism: Since g, p, and rt are all natural-language strings, we employ standard embeddings (all-MiniLM-L6-v2 (Reimers & Gurevych, 2019)) that generalize across domains without task-specific engineering."
        },
        {
            "title": "B Relevant nuances of existing agentic approaches",
            "content": "B.1 Hand-crafted information in Automanual Beyond implementing both hierarchical learning system and code-based action spaces, Automanual (Chen et al., 2024) incorporates domain knowledge about the ALFWorld task into multiple components of the algorithm. In this section we include some code from the official Automanual GitHub to illustrate. Observation spaces : Automanual uses modified observation space that enhances the ALFWorld string by adding two critical pieces of information: 1) The current location of the agent, 2) What the agent is currently holding. Both of these pieces of information 16 typically have to be deduced from the trajectory of previous observations and actions, but Automanual tracks them explicitly: 1 if \"Nothing happens\" not in observation: self.last_obs = observation if \"go to\" in script: self.cur_loc = re.search(r'go to (S+)', script).group(1) self.cur_loc_info = observation if \"open\" in script or \"close\" in script: self.cur_loc_info = observation if \"take\" in script: self.holding = re.search(r\"(?<=takes)(.*?)(?=sfrom)\", script).group(1) self.cur_loc_info = \"\" if \"put\" in script: self.holding = \"nothing\" self.cur_loc_info = \"\" 13 14 elif \"go to\" in script: 15 loc = re.search(r'go to (S+)', script).group(1) if loc == self.cur_loc: 17 18 observation += f\" You are at {self.cur_loc} and holding {self.holding}.\" observation = self.cur_loc_info Action spaces : in ALFWorld, any task typically involves three main components: 1) Searching for an object, 2) Performing an action with the object (heating, cooling, cleaning, etc.), 3) Placing the object somewhere. Automanual significantly simplifies both the search and placement operations by providing multi-action helper functions within its code-based action space: 1 # Define helper method to find object that is needed 2 def find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation = agent.go_to(receptacle) # Check if we need to open the receptacle. If we do, open it. if 'closed' in observation: observation = agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: object_ids = get_object_with_id(observation, object_name) return object_ids, receptacle return None, None 13 14 # Define helper method to put object in/on the target receptacle 15 def go_to_put_object(agent, target_receptacle, object_id): observation = agent.go_to(target_receptacle) # check if target_receptacle is closed. If so, open it. if 'closed' in observation: observation = agent.open(target_receptacle) observation = agent.put_in_or_on(object_id, target_receptacle) return observation 3 4 6 7 8 9 10 12 16 4 5 6 8 9 10 11 12 18 19 20 21 B.2 note on training and test sets The distinction between how different techniques leverage data is crucial in understanding the generalization capabilities of LLM agents. We can categorize existing approaches based on how they separate (or fail to separate) training and testing data: Single-Task Optimization : Some approaches focus exclusively on improving performance on single task instance without concern for generalization. For example, Shinn et al. (2023) leverages feedback from failed attempts to incrementally improve performance on the same task. Similarly, search methods (Brown et al., 2024; Wang et al., 2024b) expand the solution space for specific problem instance. While these approaches can solve individual 17 Figure 5: Ablating the value of initial human-provided examples, Wordcraft. TrajBootstrap, initialized by default with database of 5 human-provided trajectories for Wordcraft, achieves better performance with these starting examples than when initialized from an empty database (-Human-Examples). Performance still scales with database size for -Human-Examplesbut in this case fails to reach the performance achieved via 5 humanprovided examples, even after self-collecting trajectories on 4000 training tasks. tasks, they dont transfer knowledge across different problems, essentially overfitting to single instance. Mixed Train-Test Evaluation : Some recent work blurs training and test boundaries. For instance, RAP (Kagaya et al., 2024) makes multiple passes over the same dataset, allowing the system to learn from some test examples before evaluating on others within the same set. This approach makes it difficult to assess true generalization capability, as the model has indirect exposure to the test distribution during its learning phase. Proper Train-Test Separation : The most rigorous approaches maintain clear separation between training and test data: 1) ExpeL (Zhao et al., 2024) extracts general rules from training set of trajectories and applies them to entirely separate test tasks, 2) AutoGuide (Fu et al., 2024) generates contextual guidelines from training experiences that are evaluated on distinct test scenarios. 3) AutoManual (Chen et al., 2024) constructs hierarchical manuals from training interactions that are then applied to novel test tasks. Our approach maintains this same rigor by ensuring that trajectories used for database construction come exclusively from designated training tasks, with evaluation conducted on separate set of test tasks never seen during the database construction phase. This proper separation is essential for validating that the knowledge captured by the agent generalizes to new problems rather than memorizing specific solutions. Ablating the value of human-provided initial examples Providing small number of hand-crafted in-context examples is standard practice in the LLM agent literature (Yao et al., 2023; Zhao et al., 2024; Fu et al., 2024; Chen et al., 2024; Kagaya et al., 2024). However, what if we initialized Traj-Bootstrap with an empty database? In order to understand the value of the initial human-provided examples, we test Traj-Boostrap with and without the initial human-provided examples on Wordcraft. We refer to the variant initialized with an empty database as -Human-Examples. TrajBootstrap, initialized by default with database of 5 human-provided trajectories for Wordcraft, achieves better performance with these starting examples than when initialized from an empty database (-Human-Examples). Performance still scales with database size for -Human-Examplesbut in this case fails to reach the performance achieved via 5 humanprovided examples, even after self-collecting trajectories on 4000 training tasks. On at least this one task, the initial human-provided trajectories shaped the reasoning and action patterns of the agent in way that boosted the continual database construction process. We leave exploration of hand-crafting these in-context examples to future work."
        },
        {
            "title": "D Additional Implementation Details",
            "content": "D.1 Prompt Templates Across all benchmarks, we used the following prompt templates for the core components of our Step-by-Step Retrieval framework: Plan: 1 system_prompt: 'You are an expert at generating high-level plans of actions to (cid:44) (cid:44) (cid:44) achieve goal.n Here is your action space: {action_space}.n Here are some examples of goal,plan from episodes that successfully achieved similar goals: {examples}' 2 user_prompt: 'goal: {goal}n plan: ' Reason: 1 system_prompt: 'You are an expert at reasoning about the most appropriate action to (cid:44) (cid:44) (cid:44) take towards achieving goal.n Here is your action space: {action_space}.n Here are some examples of goal,plan,observation,reasoning,action from episodes that successfully achieved similar goals: {examples}' 2 user_prompt: 'goal: {goal}n plan: {plan}n trajectory: {trajectory}n reasoning: ' Act: 1 system_prompt: 'You are an agent in an environment. Given the current observation, (cid:44) (cid:44) (cid:44) (cid:44) you must select an action to take towards achieving the goal: {self.goal}.n Here is your action space: {action_space}.n Here are some examples of goal,plan,observation,reasoning,action from episodes that successfully achieved similar goals: {examples}' 2 user_prompt: 'goal: {goal}n plan: {plan}n trajectory: {trajectory}n action: ' Across all templates, the in-context examples follow the format specified in the prompt itself (for plan, the in-context examples are of form goal,plan, etc). These templates are intentionally minimalist, focusing on providing the necessary context and retrieved examples while avoiding task-specific prompt engineering. The same templates were used across all benchmarks with no task-specific modifications. D.2 Retrieval Implementation For all retrieval steps, we implement hybrid search across all the desired retrieval keysex. goal, plan, observation, reasoning. We return the top-k examples by averaged distance across each of the keys. We implement sliding window approach for state-level retrieval to enhance contextual relevancewe include the surrounding context (preceding and following states) up to window of 5 steps to provide coherent episode fragments. The retrieval mechanism is implemented using FAISS (Douze et al., 2024) for efficient similarity search as the database grows. We use exact nearest neighbor search. D.3 Population-Based Training Details Our database-level selection approach maintains population of 5 database instances. Each instance is initialized with the same set of human-provided exemplars. The population undergoes selection every time the database size doubles, and performance is evaluated on the tasks attempted since the previous doubling. The replacement strategy follows standard population-based training practices: the bottom 20% of databases (based on validation performance) are replaced with copies of the top 20%. 19 D.4 Quality Metric Computation For exemplar-level selection, we track the retrieval patterns of each trajectory throughout the training process. For each task, we record: 1. Which trajectories were retrieved 2. How many times each trajectory was retrieved during the solution process 3. Whether the task was successfully completed After completing all training tasks, we compute the quality metric Q(τ) for each trajectory τ as: Q(τ) = iR(τ) oi fi(τ) iR(τ) fi(τ) (2) where R(τ) is the set of tasks for which trajectory τ was retrieved, oi {0, 1} is the outcome of task i, and fi(τ) is the retrieval frequency of trajectory τ during task i. To ensure statistical significance, we only compute the quality metric for trajectories that were retrieved for at least 3 different tasks. For trajectories with insufficient retrieval data, we assign neutral quality score equal to the average success rate across all tasks. D.5 Note on planning step Following the convention from RAP (Kagaya et al., 2024), we omit the planning step on benchmarks with short trajectory length (Intercode-SQL, Wordcraft). This planning step is valuable for maintaining long-horizon coherence on the ALFWorld benchmarks (30 steps), and is standard in prior ReAct-based agentic work (Kagaya et al., 2024; Zhao et al., 2024; Fu et al., 2024), whether the planning step is explicitly separate from reasoning, or incorporated into the first reasoning step."
        },
        {
            "title": "E Benchmark Details",
            "content": "E.1 ALFWorld ALFWorld (Shridhar et al., 2020) is text-based environment that aligns with embodied tasks, allowing agents to navigate and manipulate objects through textual commands. We use the standard ALFWorld benchmark consisting of 3500 training tasks and 134 out-ofdistribution test tasks across 6 task categories: Pick & Place: Find and move objects to specified locations Clean & Place: Find, clean, and place objects Heat & Place: Find, heat, and place objects Cool & Place: Find, cool, and place objects Pick Two & Place: Find and move two objects to specified location Look at Object: Find an object and examine it under light Following (Kagaya et al., 2024), for the ALFWorld benchmark we perform similarity search over task categories in addition to the other retrieval keys (goal, plan, observation, action). We do this to follow the convention in this prior work. For our initial human-provided exemplars, we used the 18 successful trajectories (3 per task category) provided by Zhao et al. (2024). These trajectories were used to initialize all database instances. The success criteria for ALFWorld tasks are defined by the environment and require the agent to satisfy all conditions specified in the goal. For example, in Heat & Place task, the agent must find the target object, place it in the microwave, turn on the microwave, and finally place the heated object at the specified destination. Both Autoguide and Automanual allow 50 actions for task completionbut choosing to employ reasoning counts as an 20 action. Since we force our agent to reason at every step, we allow our agents (Fixed-DB, TrajBootstrap and variants) only 30 steps for task completion (on Autoguide and Automanual, the agent does not reason in practice at most steps ex. in search procedure). For this benchmark, we do not provide an action space string to the LLM, relying purely on the in-context examples to communicate the action space. E.2 InterCode-SQL InterCode-SQL (Yang et al., 2023) is an interactive coding environment for evaluating language agents SQL programming abilities. We use subset of the InterCode benchmark focusing on SQL query generation, built upon the Spider SQL dataset. Of the 1034 tasks in the dataset, we randomly assign 800 tasks to train and the remaining 234 tasks to test. Each task provides natural language query request. The agent must generate syntactically correct SQL query that retrieves the requested information. The agent must first execute queries to understand the database schema. The environment provides feedback on syntax errors and execution results, but the agent is only allowed to submit solution once. The success criteria for InterCode-SQL tasks require the agent to submit solution query within 10 steps. The environment executes the query and compares the results against ground-truth reference. For our initial human-provided exemplars, we collected 10 human-created trajectories for 10 randomly-selected training tasks. These trajectories were used to initialize all database instances. For all solved trajectories, we append the solution query to the goal stringsince the goal of the task is to discover this query through interacting with the SQL database. We used the following action space string for InterCode-SQL: Your action space is outputting valid mysql commands to solve the sql task. You will be evaluated on the Latest Standard Output. If you believe the latest observation is the final answer, you can complete the task by running 'submit' by itself. You have 10 iterations to solve the task. Follow the syntax and logical flow from the provided examples exactly. E.3 Wordcraft Wordcraft (Jiang et al., 2020) is simplified adaptation of the game Little Alchemy, where agents must combine elements to create new elements through multi-step processes. We randomly select 4000 training tasks and 500 test tasks from the subset of tasks requiring up to 2 steps to solve, with the train-test split separating the tasks into disjoint sets of goal elements. The agent starts with set of elements and must discover combinations that creates particular target element specified in the goal. The environment provides feedback on successful combinations and updates the available elements accordingly. The success criteria for Wordcraft tasks require the agent to create the target element within 4 steps, while the minimum solution length is up to 2 steps. For our initial human-provided exemplars, we collected 4 human-annotated trajectories from randomly-selecting training tasks. These trajectories were used to initialize all database instances. We collected fewer initial trajectories for Wordcraft than for InterCode-SQL (4 vs 10) since Wordcraft is slightly simpler task, requiring up to 4 steps for task completion while InterCode-SQL requires up to 10. We used the following action space string for Wordcraft: Output strings with the names of the two entities we would like to combine in this step."
        },
        {
            "title": "F Computational Resources",
            "content": "All experiments were conducted using the following computational resources: 1 NVIDIA A5000 GPU (24GB memory) for embedding computation 48 CPU cores (AMD EPYC 7543 32-Core Processor) 64GB RAM The majority of computation was spent on OpenAI API calls for the LLM-based decisionmaking. Database operations including embedding computation, storage, and retrieval accounted for less than 5% of the total computation time. For embedding computations, we used all-MiniLM-L6-v2 (Reimers & Gurevych, 2019). For LLM inference, we used the OpenAI API for GPT-4o-mini, which required approximately: 2,000,000 API calls for ALFWorld 200,000 API calls for InterCode-SQL 500,000 API calls for Wordcraft The total cost of API usage was approximately $3,000 USD."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}