{
    "paper_title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
    "authors": [
        "Xuyang Liu",
        "Zichen Wen",
        "Shaobo Wang",
        "Junjie Chen",
        "Zhishan Tao",
        "Yubo Wang",
        "Xiangqi Jin",
        "Chang Zou",
        "Yiyu Wang",
        "Chenfei Liao",
        "Xu Zheng",
        "Honggang Chen",
        "Weijia Li",
        "Xuming Hu",
        "Conghui He",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression}. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 4 1 9 1 . 5 0 5 2 : r Shifting AI Efficiency From Model-Centric to Data-Centric Compression Xuyang Liu1,2 Zichen Wen1,3,4 Shaobo Wang1 Junjie Chen1 Zhishan Tao1 Yubo Wang1 Xiangqi Jin1,3 Chang Zou1,3 Yiyu Wang1 Chenfei Liao6 Xu Zheng6 Honggang Chen2 Weijia Li4,5 Xuming Hu6 Conghui He4 Linfeng Zhang1 1EPIC Lab, Shanghai Jiao Tong University 2Sichuan University 3University of Electronic Science & Technology of China 4Shanghai AI Laboratory 5Sun Yat-sen University 6Hong Kong University of Science and Technology (Guangzhou) Project: Awesome-Token-level-Model-Compression"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, we argue that the focus of research for efficient AI is shifting from model-centric compression to datacentric compression. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI communitys advancement."
        },
        {
            "title": "Introduction",
            "content": "The explosive growth of large language models (LLMs) [111, 131, 5, 52, 40, 152, 151, 95, 55] and their multi-modal extensions (MLLMs) [98, 96, 26, 25, 177, 138, 7, 29] over the past few years has driven remarkable gains in AI capabilities. Notably, this unprecedented progress has been largely achieved through increasing model scale across the field, with larger models consistently demonstrating superior performance in reasoning, knowledge acquisition, and task generalization. Indeed, the evolution from early language models with modest parameter counts, such as BERT (117M) [38], to todays state-of-the-art LLMs like Llama 4 [110], DeepSeek-R1 [55], and Qwen3 [151] (100B+), demonstrates how each successive model iteration has delivered disproportionate performance improvements through sheer scale. Nevertheless, this relentless pursuit of enhanced Equal contribution. liuxuyang@stu.scu.edu.cn Corresponding author: zhanglinfeng@sjtu.edu.cn Preprint. Figure 1: The evolution of AI efficiency: from model-centric to data-centric compression. From 2022 to 2024, AI model performance gains mainly came from scaling model size, directing efficiency research towards model-centric compression. By mid-2024, with model sizes approaching 1000B parameters, their growth has slowed down. Consequently, the focus has shifted to expanding context length to further enhance model capabilities. This paradigm shift necessitates transition to data-centric compression, emphasizing context length reduction for model efficiency. performance through increased model size comes at an ever-increasing computational cost. As result, by early 2024, the dominant source of computational overhead was primarily attributed to the linear growth in parameter count and associated memory requirements. In response to this scaling trend, the research community has developed numerous model-centric compression techniques. These include model quantization [153, 123], network pruning [58, 27], knowledge distillation [62, 50] and low-rank decomposition [159, 67]. These methods directly reduce computational overhead by decreasing model size, and were natural response to the 2022 to 2024 era, when scaling up model size was the primary driver of performance gains. As model sizes approach hardware limits, the pace of parameter growth is flattening. Meanwhile, new computational challenge has emerged: the exponential growth in token sequence lengths. Figure 1 (left) clearly shows that while from 2022 to 2024, model size primarily drove computational costs, reaching around 1000B parameters before stagnating, from 2024 onward, the dominant factor has dramatically shifted to the staggering number of processed tokens, which continues to grow exponentially. This trend is evident across multiple domains: language models now process context lengths orders of magnitude longer than their predecessors [110, 151, 95], particularly with emerging technologies like long chain-of-thought reasoning [55] and multi-agent systems [57], vision models must handle increasingly high-resolution images [82, 177, 7] and longer videos [22, 101], and generative models are tasked with creating higher-resolution images [18, 78] and hour-long videos [12], requiring substantially more tokens and leading to overwhelming computational overhead. Consequently, by late 2024, the primary computational bottleneck has clearly shifted from model size to the quadratic cost of attention mechanism over these extremely long token sequences. This unprecedented growth in sequence lengths has fundamentally shifted the computational bottleneck from model size to the quadratic cost of attention over long context sequences. Based on this observation, as illustrated in Figure 1 (right), we propose critical position: the AI community should shift its efficiency optimization paradigm from model-centric to data-centric compression. Specifically, we advocate for token compression - data-centric compression approach that directly reduces token redundancy in model inputs [51, 70, 88, 10, 11, 19]. Token compression methods address computational overhead by identifying and removing low-information tokens during processing, generally without modifying model architectures or even requiring retraining. Our detailed analysis in Section 3.3 demonstrates that token compression offers compelling advantages in terms of universality, efficiency, and compatibility, positioning it as promising solution for enabling efficient next-generation LLMs and MLLMs. Building upon these analyses, we make four key contributions in this position paper: Evolution of AI Efficiency: We analyze recent developments in long-context AI across various domains, revealing critical transition from parameter-centric to context-centric computational bottlenecks that necessitates paradigm shift in efficiency optimization. 2 Unified Formulation of Model Efficiency: We establish comprehensive mathematical formulation that unifies different perspectives on model efficiency, bridging architectural design, model-centric compression, and data-centric compression approaches through theoretical analysis. Systematic Review of Token Compression: We present thorough investigation of token compression methods, constructing unified framework to categorize diverse approaches while analyzing their benefits and trade-offs across different scenarios and tasks. Challenges and Future Directions: We provide an in-depth analysis of current challenges in token compression research and propose promising future directions, aiming to catalyze research efforts toward more efficient and effective compression methods."
        },
        {
            "title": "2 Background",
            "content": "2.1 Token Overhead aross Various Domains The field of artificial intelligence has witnessed remarkable advancements across multiple domains, including natural language processing, computer vision, and content generation. These developments have been largely driven by the introduction of the Transformer architecture [132], which has spawned wide variety of models. As these domains evolve, we observe significant increase in token sequence lengths across three main areas: Longer Context Length in Language Models Large language models (LLMs) [38, 131, 52, 95, 5, 152, 151, 111, 15] have demonstrated remarkable capabilities in natural language understanding and generation [174, 56]. The context length these models can handle has expanded dramatically from 2,048 tokens in early models like Llama 1 [131] to 10M tokens in recent iterations like Llama 4 Scout [110]. This expansion has led to the emergence of large reasoning models [55, 66, 64], which focus on complex multi-step problem solving through techniques like long chain-of-thought reasoning [94, 151, 21] and multi-agent collaboration [129, 24, 57]. Higher Resolution and Longer Video Understanding Building on the success of LLMs, multimodal large language models (MLLMs) [85, 96, 97, 82, 6, 26, 25, 177] extend these capabilities by integrating vision and text processing [148]. The visual inputs these models process have evolved significantly from basic 224 224 resolution images in early models like LLaVA [98] to 4K ultrahigh-resolution images in InternVL3 [177] and 8K-frame videos in Video-XL-Pro [101], achieving remarkable performance in tasks involving images [7], videos [22], and multi-modal reasoning [140]. More Complex Content in Generation Tasks The field of content generation has seen dramatic advancements, particularly with the application of Transformers to generative domains [116, 12, 89, 23]. Early diffusion models like Stable Diffusion [124] were limited to generating 512512 resolution images. With Transformers being successfully applied to generative domains [116, 47, 12, 89], DiT-based models have dramatically advanced the field, producing high-quality 4K images in PixArtΣ [18] and even hour-long videos in Sora [12]. These models capture complex dependencies across space and time, achieving remarkable results in high-fidelity content generation [78, 156, 133, 73]. While these advancements across domains have demonstrated outstanding performance, they now face significant efficiency challenges due to the quadratic cost of attention mechanisms over extremely long token sequences. This growing trend toward longer contextswhether processing complex reasoning chains in language tasks, high-resolution images and longer videos in understanding tasks, or high-fidelity content in generation tasksnecessitates prioritizing research into model efficiency, particularly in addressing the computational overhead associated with increasing context lengths. Detailed statistical analysis of this trend is presented in the Appendix A. 2.2 Model Efficiency from Different Perspectives Improving model efficiency has been key goal in deep learning research. Given input data and network parameters W, neural network produces output through the transformation: (cid:124)(cid:123)(cid:122)(cid:125) output = (cid:124)(cid:123)(cid:122)(cid:125) network ( (cid:124)(cid:123)(cid:122)(cid:125) weights , (cid:124)(cid:123)(cid:122)(cid:125) input ) (1) where model efficiency can be optimized from three perspectives: (I) Efficient Computation Architecture aims to design efficient neural architectures [127, 119, 53], (II) Model-centric Compression 3 focuses on model weights [62, 153, 83, 159], and (III) Data-centric Compression targets token sequences from input data [122, 70, 10, 179, 19]. (I) Efficient Computation Architecture (F) Since the computational efficiency of neural networks is determined by their architectural design, optimizing architectures represents fundamental approach to enhance efficiency. Unlike traditional Transformer architectures with quadratic computational complexity O(n2) in attention [132], where is sequence length, recent innovations typically achieve linear or sub-quadratic scaling: (i) linear attention reformulates the attention mechanism to achieve linear complexity O(n), enabling efficient processing while maintaining model capacity [74, 127]; (ii) RWKV architecture integrates RNN-like linear scaling O(n) with transformer-like parallelism for efficient sequence processing [119, 42]; (iii) State Space Models like Mamba leverage structured state space modeling to achieve linear complexity O(n) while maintaining performance [53, 178].While these architectural innovations improve efficiency significantly, they require complete model retraining, motivating the exploration of alternative approaches. (II) Model-centric Compression (W) As model parameters directly contribute to computational costs and memory usage, reducing parameter complexity serves as another essential strategy. Model compression can be regarded as model-centric compression, which transforms the original parameter set to reduced set W: = Γ(W), where < (2) where Γ represents the compression operator and denotes parameter size. Several key approaches have emerged: (i) network pruning removes redundant weights, reducing parameters and computation [106, 27]; (ii) model quantization reduces parameter precision from high-bit to low-bit representations [153, 123]; (iii) knowledge distillation transfers information from large models to compact ones [62, 50]; (iv) low-rank decomposition approximates weight matrices with lower-rank representations [159, 67]. With model sizes plateauing and context lengths growing, research focus has begun shifting from model-level to token-level compression strategies. (III) Data-centric Compression (X) Different from model-centric compression, token compression represents data-centric approach that directly reduces input complexity. Given input data represented as token set X, it produces reduced representation X: = Φ(X), where < (3) where Φ is the token compression operator and denotes token length. This approach complements model compression and has demonstrated significant effectiveness across computer vision [122, 10] and natural language processing [75, 70]."
        },
        {
            "title": "3 How Token Compression Drives Efficient and Effective Models",
            "content": "In this section, we begin with the research roadmap of data-centric compression (i.e., token compression) in Section 3.1. Then, we comprehensively analyze the benefits of token compression during both training and inference stages in Section 3.2. Finally, we summarize five compelling advantages shared by existing token compression approaches in Section 3.3. 3.1 Research Roadmap - What Makes Token Compression Work? Existing token compression methods fundamentally operate through two-stage process: first, identifying tokens eligible for compression within the existing token sequence = [x1, x2, . . . , xT ] using carefully designed compression criteria through scoring function : {st}T t=1, and then determining the precise handling of these tokens through specific compression strategies t=1) that transform the original sequence into compressed one where < : (X, {st}T X. Given that existing research primarily revolves around these two key components, we next systematically analyze their designs and review representative approaches across various scenarios. Compression Criteria (E) To determine which tokens should be compressed in sequence = [x1, x2, . . . , xT ], compression criteria employ scoring functions to evaluate each tokens importance 4 or redundancy. Based on whether additional parameters are introduced into original models, these criteria can be categorized into two main approaches: (I) Parametric Methods employ auxiliary networks as scoring functions Eθ : {st}T t=1, introducing additional parameters θ beyond the original model parameters θ. These methods include: (i) training-aware approaches [122, 157, 87, 76] that optimize θ through training to learn scoring function Sθ : {st}T t=1, and (ii) training-free approaches [108, 172] that directly employ pre-trained networks as scoring function Sfixed : {st}T (II) Non-parametric Methods utilize parameter-free heuristics for token scoring without introducing extra parameters. These approaches can be categorized into: (i) inherent computation methods [90, 179, 19, 149, 48] that leverage models internal calculations for token scoring Sin : {st}T t=1, such as using attention weights (st = (cid:80)T , where aj represents attention score between tokens), and (ii) external computation methods [9, 161, 39, 141, 102] that design additional metrics Sex : RT to evaluate token relationships. For external methods, an additional function : is introduced to compute intermediate features, where = g(X). The scoring function then operates on these features: si,j = (zi, zj), where is custom pairwise scoring function. typical example is using cosine similarity, where is an identity function and si,j = xi,xj t=1 without updating θ. j=1 aj . xi2xj 2 Compression Strategies (P) To reduce sequence length while preserving critical information, compression strategies transform the original sequence based on token scores {st}T t=1. These strategies can be primarily categorized into two distinct approaches: (I) Token Pruning directly discards less important tokens from the sequence based on their scores. These methods [122, 51, 70, 19] typically remove tokens with scores below threshold, producing compressed sequence: = {xt st < τ } (4) where τ is threshold determining token removal. Token pruning reduces computation through direct elimination but risks information loss, particularly for fine-grained tasks. (II) Token Merging preserves information by combining semantically similar tokens [9, 90, 168, 11]. Given an input sequence = {x1, . . . , xT } and mapping π : {1, . . . , } {1, . . . , } that assigns tokens to merge groups based on their semantic relationships, this approach generates compressed sequence = {x } through weighted aggregation: 1, . . . , (cid:88) = t:π(t)=m wtxt, wt = st t:π(t)=m st (cid:80) (5) where wt represents importance weights. Token merging preserves information through weighted combinations of tokens, offering more nuanced approach than direct elimination. 3.2 Training and Inference Targets - How Token Compression Benefits? Training Stage Token compression techniques contribute to improving both the quality and efficiency of model training. These benefits can be broadly categorized into two aspects: enhancing training quality and increasing training efficiency. (I) Enhancing Training Quality Improvement in training quality can be achieved through methods such as data augmentation and token selection, which serve to increase data diversity and emphasize the most informative content, respectively. Data augmentation techniques have been widely adopted to enrich training datasets by introducing variability that enhances robustness and informativeness [35]. In computer vision, mixing or combining image tokens creates novel representations that elevate training effectiveness [162, 160]. This strategy has also been extended to synthetic datasets, where adaptive augmentation controls the informativeness of generated images [171, 81, 139]. Analogously, in natural language processing, augmenting text tokens through synonym replacement [142], contraction expansion [33], back-translation [17], and reformulation [60], supporting better generalization. Token selection focuses on filtering out low-quality tokens to refine training data quality [93, 80, 118, 145, 46, 84]. Common approaches include rule-based heuristics [121, 117], deduplication methods [80, 118, 1], and scoring strategies leveraging large language models [145, 46, 84, 146, 125]. Formally, consider training batch = {Xi}N i=1, where each Xi = [xi,1, xi,2, . . . , xi,T ] is token sequence of length . quality scoring function : assigns each token xi,j score reflecting its informativeness or relevance. Using threshold τ , tokens with scores below τ are . The filtered batch consists of sequences: filtered out via mask mi: mi,j = q(xi,j) τ (cid:26)1, 0, otherwise Xi = {xi,j mi,j = 1, = 1, . . . , }. (6) Training on these curated, high-quality tokens enables the model to concentrate on the most relevant information, reducing noise and redundancy, which enhances generalization and learning efficiency. (II) Increasing Training Efficiency Token compression directly reduces the token length processed during training, addressing critical challenges associated with scaling large models [10, 28, 126, 150]. For Transformer architectures with sequence length reduced from to (m < n), the computational and memory benefits can be quantified as: Ω(X) Ω(X) = O(m2d) O(n2d) = (cid:18) m2 n2 (cid:19) , M(X) M(X) md nd = , (7) where is the embedding dimension, Ω() represents the computational measure, and M() denotes the memory measure. This quadratic reduction in computation and linear reduction in memory enable faster training iterations and larger batch sizes on fixed hardware resources. Inference Stage Token compression methods can also enhance model inference efficiency through two key aspects: decreasing computational complexity and reducing memory usage. (I) Decreasing Computational Complexity Following patterns established in training, token compression achieves quadratic speedup in inference computations. Notably, many non-parametric compression methods [10, 72, 19] can be directly integrated into inference without additional training or architectural modifications, enabling immediate benefits across various domains [168, 144]. (II) Reducing Memory Usage Token compression optimizes memory efficiency through two mechanisms: (i) computing memory reduction following the linear scaling pattern shown in training, and (ii) KV cache optimization for large language models [88, 14, 135, 134]. During autoregressive generation, each layer caches key and value states for attention computation, with memory growing with sequence length. For sequence of length compressed to length m, with layers and hidden dimension d, the KV cache memory reduction is: MKV(X) MKV(X) = 2Lmd 2Lnd = , (8) where factor 2 accounts for both key and value states per layer. These benefits are particularly crucial for real-time interactive systems, including UI agents [130], autonomous driving [45], and embodied AI [41], where efficient processing of continuous inputs under resource constraints is essential. 3.3 Compelling Advantages - Why Token Compression Matters? Based on our comprehensive analysis of token compression techniques, we identify five compelling advantages that make them particularly promising: Universal Applicability: The redundancy of tokens exists consistently across modalities and tasks, making token compression possible in all kinds of settings. Dual-phase Efficiency: Token compression is capable of accelerating both model training and inference phases with minimal accuracy loss. Architectural Compatibility: Token compression is orthogonal to existing model compression and compression methods, making it is possible to be integrated seamlessly with existing compression techniques. Besides, it is friendly to the hardware and computer systems. Low Implementation Costs: Modern neural networks, such as transformers, is able to process tokens of different lengths. As result, token compression can be done without introducing any training costs and data utilization. 6 Quadratic Gains: The O(n2) computation complexity of widely used self-attention indicates token compression can bring significant benefits in computation.. As AI development enters new phase where context length becomes the primary bottleneck, the research focus of AI efficiency should shift towards data-centric compression through token compression, enabling more efficient and scalable AI systems."
        },
        {
            "title": "4 Current Challenges",
            "content": "4.1 Performance Degradation Methodological Bottlenecks. Attention scores play crucial role across existing token compression approaches. For example, [CLS] token attention scores are used to select key visual tokens [120, 61, 167, 136, 59, 154, 103], while cross-modal guidance [69, 16, 128, 20, 150, 168] relies on text-vision attention scores. But are attention scores truly reliable for deciding which tokens to keep? Recent work [167, 143] reveals that attention scores can suffer from position bias. For instance, when using text-vision scores to retain visual tokens, those near the sequence end often get higher weights. In 2D image space, this biases retention toward the lower half or bottom-right corner. Clearly, its unrealistic to assume the lower half of all images is more important. Such bias can significantly hurt compression performance. As shown in Figure 2, comprehensive review and comparison of key studies [71, 144, 143, 169, 100, 102] confirms our hypothesis: even well-crafted attention-based methods can underperform simple random pruning or pooling. Detailed analysis is in Appendix B. Inherent Limitations of Token Compression. Beyond performance degradation from methodological design, does token compression face inherent limitations? Is it universally applicable across various tasks? For multi-modal large language models, [143] shows most existing compression methods underperform on visual grounding tasks, with significant drops on benchmarks like RefCOCO [158]. In OCR-related parsing [155, 112], documents with dense layouts yield highly information-rich visual tokens. Compressing these risks severe information loss and degraded performance. Beyond vision, current methods also face inherent limits in other modalities. In Automatic Speech Recognition (ASR) and Automatic Speech Translation (AST) [3, 32], audio is encoded and then decoded into text using an MLLM [2, 30]. Audio tokens in ASR and AST are dense and temporally continuous. Pruning or merging them disrupts this continuity, leading to fragmented recognition or translation. Similarly, translation across languages in the text modality may suffer significant degradation under high compression ratios. 4.2 Suboptimal Token Representation Most existing token compression methods fall into two categories: redundancy-based approaches that maximize information preservation between original (X) and compressed tokens (X) via maxC I(X; X), and importance-based methods that ensure predictive sufficiency through I(X; Y) I(X; Y) ϵ and ϵ is the bearable information loss. While effective in their respective objectives, we argue that these paradigms share critical limitation: neither guarantees that the compressed tokens form an optimal representation for downstream modeling. The redundancybased framework, despite preserving maximal mutual information with X, often retains tokens with reconstructive but low discriminative value. The importance-based framework, on the other hand, prioritizes maintaining predictive performance with respect to the target variable Y, but often at the cost of introducing task-specific biases. By focusing solely on the preservation of information relevant to predefined label, these methods may overlook the need to maintain stable structural and semantic patterns across the token sequence in that could enhance generalization across diverse downstream tasks. Consequently, both approaches risk producing token representations that are misaligned with the ultimate goal of effective and generalizable downstream modeling. 4.3 Fair Comparison Rethinking FLOPs and Compression Ratios as Efficiency Metrics. Many token compression methods report speedup by estimating FLOPs reductions or directly using token compression ratios. But do FLOPs or compression ratios truly reflect real acceleration? Our analysis shows that, even with similar compression ratios or FLOPs, methods often vary significantly in runtime latency. 7 Figure 2: Empirical comparison of carefully designed token compression methods and random token dropping. Results demonstrate that in multiple scenarios, some meticulously engineered token compression methods surprisingly underperform compared to random token pruning. Investigating further, we find: (i) Importance-based compression often uses attention scores [19, 168], but this can limit compatibility with efficient attention mechanisms like Flash Attention [37, 36], potentially contributing to the substantial discrepancy between FLOPs and actual runtime latency observed in some approaches. (ii) Some methods pursue high compression via progressive compression across layers [150], adding overhead that offsets the gains from token reduction. Thus, we argue that runtime latency should be prioritized in evaluations, as FLOP or token count reductions dont always yield real-world speedup. Token Compression Evaluation: The Benchmarking Gap. Current token compression methods are primarily evaluated using general-purpose benchmarks, which are not tailored to capture the unique challenges of token compression. As result, some benchmarkssuch as ScienceQA [107] and VizWiz [8]paradoxically show improved performance under certain compression settings, or minimal degradation across varying compression ratios. These observations defy intuition and suggest that existing benchmarks may fail to meaningfully reflect the trade-offs introduced by token compression. This discrepancy raises concerns about the validity of current evaluation practices. In particular, benchmarks that fail to penalize information loss introduced by compression may obscure meaningful differences between methods. Additionally, the lack of task diversity and compression-sensitive metrics further limits our understanding of how these methods behave in realistic scenarios. Without dedicated benchmarks, it remains unclear whether observed gains are due to genuine improvements in compression quality or artifacts of misaligned evaluation settings."
        },
        {
            "title": "5 Future Works",
            "content": "5.1 Data-Model Centric Compression Co-Development As AI systems continue to scale in both model complexity and context length, promising direction for future research lies in the co-development of data-centric and model-centric compression strategies. Instead of treating these approaches independently, integrating them can yield synergistic benefitsenhancing overall efficiency while maintaining, or even improving, model performance. The most straightforward form of integration adopts staged approach, where model-centric compression is applied first, followed by data-centric methods. For example, token compression techniques can be employed on models that have already undergone quantization, pruning, or distillation. More advanced approaches aim for mutual reinforcement between the two paradigms. From data-centric perspective, analyzing the layer-wise evolution of token representations may reveal that certain layers contribute minimal changes. This insight can inform model-centric compression by identifying layers suitable for removal or more aggressive quantization. Conversely, gradient information or attention scores associated with the critical neurons retained after model pruning can also guide token selection in data-centric compression, helping to preserve only the most informative tokens. 5.2 Dedicated Benchmarks for Token Compression Given the current limitations in evaluating data-centric token compression methods using generalpurpose benchmarks, we envision the development of dedicated benchmark specifically designed to evaluate them. Such benchmark should comprehensively span diverse domainsincluding natural language processing, computer vision, and multi-modal tasksand incorporate task-specific challenges particularly relevant to token compression, such as optical character recognition (OCR) parsing [112, 163] and automatic speech recognition (ASR) [32, 114]. Furthermore, it is essential 8 that this benchmark jointly considers both task performance and latency, as both are critical for real-world deployment. well-rounded benchmark of this nature would enable more rigorous, fair, and holistic evaluation of token compression techniques, ultimately driving progress in this area."
        },
        {
            "title": "6 Alternative Positions",
            "content": "While this paper promotes data-centric compression as key strategy for advancing Efficient AI, it is equally important to recognize and engage with alternative viewpoints that challenge the feasibility, necessity, or overall effectiveness of this approach. 6.1 Model-Centric Compression as Superior Alternative Model-centric compression methods, such as pruning [106, 63, 58, 122, 68, 113], quantization [123, 137, 176, 153, 92, 43], and knowledge distillation [62, 166, 165, 115], have long been established as effective techniques for reducing model size and computational cost. Proponents argue that this paradigm is reliable for deployment in resource-constrained environments and maintains performance consistency. For example, pruning techniques such as DynamicViT [122] dynamically remove uninformative tokens during inference, reducing the computational load by up to 3040% with minimal impact on accuracy. Proponents of this view claim that this approach achieves substantial speedups without discarding any original data. In contrast, data-centric methods that prune input tokens risk removing critical contextual information, which may degrade performance. Counterargument. Although model-centric compression is effective, it faces scalability issues as models and datasets grow, requiring costly full retraining and processing of entire inputs. In contrast, data-centric compression reduces input complexity upfront, easing computational burdens. Some data-centric methods update only small parameter subset [173, 99, 79], while others enable training-free deployment [9, 19, 88]. Combining both approaches can improve efficiency without sacrificing accuracy [4, 77], making data-centric methods complement to model-centric techniques. 6.2 Advanced Model Architectures as more Promising Direction Another argument against data-centric compression is the continued advancement of model architectures that can inherently handle large datasets and long sequences more efficiently [54, 49, 119]. The development of transformer-based architectures, such as Vision Transformers [34], Swin Transformers [105], and large language models like GPT-3 [13], has shown significant improvements in both accuracy and scalability. These architectures integrate advanced techniques, such as hierarchical processing, self-attention mechanisms, and dynamic sparsity, enabling them to process large amounts of data efficiently. For example, Swin Transformers [105] utilize window-based self-attention mechanism, which reduces the computational complexity of the standard attention mechanism, making it feasible to scale models to much larger datasets and sequences. Proponents of this view argue that as these advanced models continue to evolve, there may be less need for aggressive input compression, as these models are inherently better equipped to handle large-scale data directly. Counterargument. Advanced model architectures offer strong performance but demand substantial computational resources, especially during training [110, 151, 111]. Data-centric compression reduces computational load early by simplifying input data, enabling more efficient training and inference without sacrificing accuracy. Techniques like token pruning and augmentation preserve or improve performance by focusing on informative data. Combined with advanced architectures, data-centric methods enhance efficiency and maintain high performance [46, 146], making them complementary rather than competitive."
        },
        {
            "title": "7 Conclusion",
            "content": "In this position paper, we propose repositioning AI efficiency research by advocating shift from model-centric to data-centric compression strategies, focusing on token compression to address longcontext processing challenges. We first examine recent developments in long-context capabilities across various downstream scenarios, demonstrating how performance scaling has shifted from model size to context length, emphasizing the need for token compression to mitigate the impact of increasing context lengths. We then review approaches for improving model efficiency, with 9 emphasis on the research roadmap of data-centric compression (i.e., token compression) and its potential benefits. After analyzing current challenges in token compression research, we propose promising future directions to inspire innovation in this emerging field. Our work aims to advance AI efficiency by providing fresh perspective and catalyzing new research directions."
        },
        {
            "title": "References",
            "content": "[1] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. [2] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [3] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. [4] Abdul Hameed Azeemi, Ihsan Qazi, and Agha Ali Raza. Data pruning for efficient model pruning in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 236246, 2023. [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [8] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. [9] Daniel Bolya, Xingyu Dai, Tianyu Dai, Yinpeng Zhou, and Vladlen Koltun. Token merging for fast and accurate attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [10] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In Proceedings of the International Conference on Learning Representations, 2023. [11] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In CVPRW, 2023. [12] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901, 2020. 10 [14] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv caching compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. [15] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [16] Qingqing Cao, Bhargavi Paranjape, and Hannaneh Hajishirzi. Pumer: Pruning and merging tokens for efficient vision language models. arXiv preprint arXiv:2305.17530, 2023. [17] Jiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 21472157, 2020. [18] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [19] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In Proceedings of the European Conference on Computer Vision, 2024. [20] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. [21] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [22] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [23] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842, 2024. [24] Zhaorun Chen, Zhuokai Zhao, Wenjie Qu, Zichen Wen, Zhiguang Han, Zhihong Zhu, Jiaheng Zhang, and Huaxiu Yao. Pandora: Detailed llm jailbreaking via collaborated phishing agents In ICLR 2024 Workshop on Secure and Trustworthy Large with decomposed reasoning. Language Models, 2024. [25] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [26] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [27] Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [28] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris Kitani, and László Jeni. Dont look twice: Faster video transformers with run-length tokenization. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, pages 2812728149, 2024. [29] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [30] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [31] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [32] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. IEEE, 2023. [33] Claude Coulombe. Text data augmentation made simple by leveraging nlp cloud apis. arXiv preprint arXiv:1812.04718, 2018. [34] Ian Connick Covert, Chanwoo Kim, and Su-In Lee. Learning to estimate shapley values with vision transformers. In The Eleventh International Conference on Learning Representations, 2022. [35] Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. [36] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [37] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [39] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. simple and effective l_2 norm-based strategy for kv caching compression. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1847618499, Miami, Florida, USA, nov 2024. Association for Computational Linguistics. [40] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. Advances in Neural Information Processing Systems, 37:4256642592, 2024. [41] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230244, 2022. [42] Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, and Wenhai Wang. Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint arXiv:2403.02308, 2024. 12 [43] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate posttraining quantization for generative pre-trained transformers. In Proceedings of the International Conference on Learning Representations, 2022. [44] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [45] Cong Gao, Geng Wang, Weisong Shi, Zhongmin Wang, and Yanping Chen. Autonomous driving security: State of the art and challenges. IEEE Internet of Things Journal, 9(10):7572 7595, 2021. [46] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [47] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2316423173, 2023. [48] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv caching compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [49] Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, and Eugene Cheah. Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-caching compression. arXiv preprint arXiv:2407.12077, 2024. [50] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International Journal of Computer Vision, 129(6):17891819, 2021. [51] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference via progressive wordvector elimination. In International Conference on Machine Learning, pages 36903699. PMLR, 2020. [52] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [53] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [54] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [55] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [56] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. survey on large language models: Applications, challenges, limitations, and practical usage. Authorea Preprints, 2023. [57] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. Llm multi-agent systems: Challenges and open problems. arXiv preprint arXiv:2402.03578, 2024. [58] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural In Proceedings of the network with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2016. 13 [59] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. Rethinking token reduction in mllms: Towards unified paradigm for training-free acceleration. arXiv preprint arXiv:2411.17686, 2024. [60] Xintong Hao, Ke Shen, and Chenggang Li. Maga: Massive genre-audience reformulation to pretraining corpus expansion. arXiv preprint arXiv:2502.04235, 2025. [61] Joakim Bruslund Haurum, Sergio Escalera, Graham Taylor, and Thomas Moeslund. Which tokens to use? investigating token reduction in vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 773783, 2023. [62] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [63] Junzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with structured sparsity. Journal of Machine Learning Research, 12(103):33713412, 2011. [64] Shuo Huang, Zhenyao Zhu, Jiejun Xu, and Xin Wang. Towards robust tokenization for speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12321236, 2022. [65] Drew Hudson and Christopher Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [66] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [67] Yerlan Idelbayev and Miguel Carreira-Perpinán. Low-rank compression of neural nets: Learning the rank of each layer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80498059, 2020. [68] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In Proceedings of the International Conference on Learning Representations, 2017. [69] Chaoya Jiang, Haiyang Xu, Chenliang Li, Ming Yan, Wei Ye, Shikun Zhang, Bin Bi, and Songfang Huang. Trips: Efficient vision-and-language pre-training with text-relevant image patch selection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 40844096, 2022. [70] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1335813376, 2023. [71] Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. What kind of visual tokens do we need? training-free visual token pruning for multi-modal large language models from the perspective of graph. arXiv preprint arXiv:2501.02268, 2025. [72] Chen Ju, Haicheng Wang, Zeqian Li, Xu Chen, Zhonghua Zhai, Weilin Huang, and Shuai Xiao. Turbo: Informativity-driven acceleration plug-in for vision-language models. In Proceedings of the European Conference on Computer Vision, 2024. [73] Hengrui Kang, Siwei Wen, Zichen Wen, Junyan Ye, Weijia Li, Peilin Feng, Baichuan Zhou, Bin Wang, Dahua Lin, Linfeng Zhang, et al. Legion: Learning to ground and explain for synthetic image detection. arXiv preprint arXiv:2503.15264, 2025. [74] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 14 [75] Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer: Train once with length drop, use anytime with search. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 65016511, 2021. [76] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 784794, 2022. [77] Humaira Kousar, Hasnain Irshad Bhatti, and Jaekyun Moon. Pruning-based data selection and network fusion for efficient deep learning. arXiv preprint arXiv:2501.01118, 2025. [78] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [79] Dong Hoon Lee and Seunghoon Hong. Learning to merge tokens via decoupled embedding for efficient vision transformers. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, pages 5407954104, 2024. [80] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445, 2022. [81] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation with contrastive signals. In International Conference on Machine Learning, pages 1235212364. PMLR, 2022. [82] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [83] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations (ICLR), 2017. [84] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. [85] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [86] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [87] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. [88] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [89] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multiresolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 15 [90] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. In Proceedings of the International Conference on Learning Representations, 2022. [91] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [92] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware weight quantization for on-device LLM compression and acceleration. In Proceedings of the Annual Conference on Machine Learning and Systems, pages 87100, 2024. [93] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. [94] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [95] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [96] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2628626296, 2024. [97] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. [98] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the Advances in Neural Information Processing Systems, 2023. [99] Ting Liu, Xuyang Liu, Siteng Huang, Liangtao Shi, Zunnan Xu, Yi Xin, Quanjun Yin, and Xiaohong Liu. Sparse-tuning: Adapting vision transformers with efficient fine-tuning and inference. arXiv preprint arXiv:2405.14700, 2024. [100] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model. arXiv preprint arXiv:2411.10803, 2024. [101] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. [102] Xuyang Liu, Yiyu Wang, Junpeng Ma, and Linfeng Zhang. Video compression commander: Plug-and-play inference acceleration for video large language models. arXiv preprint arXiv:2505.14454, 2025. [103] Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, Siteng Huang, and Honggang Chen. Compression with global guidance: Towards training-free high-resolution mllms acceleration. arXiv preprint arXiv:2501.05179, 2025. [104] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. [105] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 16 [106] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the In Proceedings of the International Conference on Learning value of network pruning. Representations, 2019. [107] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [108] Tanvir Mahmud, Burhaneddin Yaman, Chun-Hao Liu, and Diana Marculescu. Papr: Trainingfree one-step patch pruning with lightweight convnets for faster inference. In Proceedings of the European Conference on Computer Vision, pages 110128. Springer, 2024. [109] Maxwell-Jia. Aime2024. https://huggingface.co/datasets/Maxwell-Jia/AIME_ 2024, 2024. [110] Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. [111] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [112] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. arXiv preprint arXiv:2412.07626, 2024. [113] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude Oliva. Ia-red2: Interpretability-aware redundancy reduction for vision transformers. In Proceedings of the Advances in Neural Information Processing Systems, volume 34, pages 2489824911, 2021. [114] Se Jin Park, Julian Salazar, Aren Jansen, Keisuke Kinoshita, Yong Man Ro, and RJ SkerryarXiv preprint Ryan. Long-form speech generation with spoken language models. arXiv:2412.18603, 2024. [115] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39673976, 2019. [116] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [117] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. [118] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36:7915579172, 2023. [119] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1404814077, 2023. [120] Yao Qiang, Deng Pan, Chengyin Li, Xin Li, Rhongho Jang, and Dongxiao Zhu. Attcat: Explaining transformers via attentive class activation tokens. In Proceedings of the Advances in Neural Information Processing Systems, volume 35, pages 50525064, 2022. [121] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [122] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision transformers with dynamic token sparsification. In Proceedings of the Advances in Neural Information Processing Systems, pages 1393713949, 2021. [123] Babak Rokh, Ali Azarpeyvand, and Alireza Khanteymoori. comprehensive survey on model quantization for deep neural networks in image classification. ACM Transactions on Intelligent Systems and Technology, 14(6):150, 2023. [124] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [125] Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed Chi, James Caverlee, Julian McAuley, and Derek Zhiyuan Cheng. arXiv preprint arXiv:2402.09668, 2024. [126] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [127] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 35313539, 2021. [128] Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang. Crossget: Cross-guided ensemble of tokens for accelerating vision-language transformers. arXiv preprint arXiv:2305.17455, 2023. [129] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023. [130] Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, et al. survey on (m) llm-based gui agents. arXiv preprint arXiv:2504.13865, 2025. [131] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [132] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, and Aidan Gomez. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems, volume 30, pages 59986008, 2017. [133] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [134] Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, and Mi Zhang. Meda: Dynamic kv cache allocation for efficient multimodal long-context inference. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 24852497, 2025. [135] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 40654078, 2024. [136] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. [cls] token tells everything needed for training-free efficient mllms. arXiv preprint arXiv:2412.05819, 2024. 18 [137] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86128620, 2019. [138] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [139] Shaobo Wang, Yicun Yang, Zhiyuan Liu, Chenghao Sun, Xuming Hu, Conghui He, and Linfeng Zhang. Dataset distillation with neural characteristic function: minmax perspective. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2025. [140] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805, 2024. [141] Zheng Wang, Boxiao Jin, Zhongzhi Yu, and Minjia Zhang. Model tells you where to merge: Adaptive kv caching merging for llms on long-context tasks. arXiv preprint arXiv:2407.08454, 2024. [142] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196, 2019. [143] Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, and Linfeng Zhang. Token pruning in arXiv preprint multimodal large language models: Are we solving the right problem? arXiv:2502.11501, 2025. [144] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025. [145] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019. [146] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. arXiv preprint arXiv:2402.09739, 2024. [147] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, pages 2882828857, 2024. [148] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip Yu. Multimodal large language models: survey. In 2023 IEEE International Conference on Big Data (BigData), pages 22472256. IEEE, 2023. [149] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [150] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large visionlanguage models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. [151] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [152] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [153] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73087316, 2019. [154] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024. [155] Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Yuliang Liu, et al. Cc-ocr: comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy. arXiv preprint arXiv:2412.02210, 2024. [156] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [157] Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, et al. Layer-and timestep-adaptive differentiable token compression ratios for efficient diffusion transformers. arXiv preprint arXiv:2412.16822, 2024. [158] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [159] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 73707379, 2017. [160] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. [161] Evelyn Zhang, Jiayi Tang, Xuefei Ning, and Linfeng Zhang. Training-free and hardwarefriendly acceleration for diffusion models via similarity-based token pruning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 98789886, 2025. [162] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. [163] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and Wentao Zhang. Ocr hinders rag: Evaluating the cascading impact of ocr on retrieval-augmented generation. arXiv preprint arXiv:2412.02592, 2024. [164] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. [165] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):43884403, 2021. 20 [166] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 37133722, 2019. [167] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster. arXiv preprint arXiv:2412.01818, 2024. [168] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. [169] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. SparseVLM: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. [170] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [171] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1267412685. PMLR, 1824 Jul 2021. [172] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, and Yang You. stitch in time saves nine: Small vlm is precise guidance for accelerating large vlms. arXiv preprint arXiv:2412.03324, 2024. [173] Wangbo Zhao, Jiasheng Tang, Yizeng Han, Yibing Song, Kai Wang, Gao Huang, Fan Wang, and Yang You. Dynamic tuning towards parameter and inference efficiency for vit adaptation. In Proceedings of the Advances in Neural Information Processing Systems, 2024. [174] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. [175] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [176] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quantization for deep neural network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [177] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [178] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In Proceedings of the International Conference on Machine Learning. [179] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with token-wise feature caching. In Proceedings of the International Conference on Learning Representations, 2025. Trends in LLM Scaling: Parameters vs. Context Length In this section, we provide comprehensive analysis of the temporal progression of mainstream LLMs, documenting the growth trends in both parameter counts and context lengths. This analysis provides empirical support for our central thesis regarding the shift in computational bottlenecks from model parameters to context processing. As shown in Tables 1, 2, 3, 4, 5, 6, 7, and 8, both in text and vision domains, model size growth has significantly slowed, while context length continues to increase. This trend indicates that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression."
        },
        {
            "title": "B Comparison of Token Compression Methods and Random Token Dropping",
            "content": "In this section, we conduct detailed analysis comparing carefully designed token compression methods with random token dropping (the simplest baseline for token compression). This analysis aims to support the arguments presented in Section 4.1, demonstrating that existing token compression techniques have certain performance limitations. Our experiments span across multiple domains, including: complex reasoning in the language domain, image and video understanding in the vision domain, and text-to-image generation in the AI content generation domain. This comprehensive approach allows us to evaluate the effectiveness of token compression methods across diverse AI tasks and modalities. Our findings underscore the need for more robust and effective approaches in token compression for LLMs, MLLMs, VideoLLMs, and DiTs, highlighting the importance of developing universally applicable compression strategies. LLMs: Complex Reasoning We evaluated DeepSeek-R1-Distill-Llama-8B [55] on suite of complex reasoning tasks, including MATH-500 [91], AIME24 [109], and GSM8K [31]. During the LLMs decoding phase, we enforced fixed token budget (e.g., 1024 tokens) for the KV cache and applied existing KV cache token dropping strategies such as H2O [170], SnapKV [88], KNorm [39], along with random dropping at regular intervals (e.g., every 512 tokens). Figure 2 (a) reveals counterintuitive finding: existing KV cache token dropping strategiesincluding H2O, SnapKV, and KNormconsistently underperform compared to simple random token dropping across complex math reasoning tasks. Most strikingly, on AIME24, random dropping even surpasses the second-best method (SnapKV) by significant margin of 10% accuracy. Our findings yield two critical insights for the field: (i) We strongly suggest that random dropping should be included as fundamental baseline in KV cache dropping studies, as it is frequently overlooked in current research despite its competitive performance; (ii) We hypothesize that random droppings unexpected effectiveness may stem from its inherent property of preserving token distribution uniformity during auto-regressive decoding, thereby better maintaining semantic coherence and information integrity compared to deterministic dropping strategies. Our findings challenge the conventional wisdom that complex token dropping policies are inherently superior, while revealing fundamental gaps in current token importance modeling paradigms for KV cache management. MLLMs: Image Understanding We conducted experiments on multiple widely used image understanding benchmarks (e.g., GQA [65] and MMB [104]) using LLaVA-1.5-7B [98]. For all experiments, we uniformly retained 25% of the visual tokens, and benchmark evaluations were performed in accordance with the official evaluation scripts of LLaVA2. In our experiments, we compared representative token compression methods, including FastV [19] and SparseVLM [168], along with two simple baselines: random token dropping and token-wise pooling. As shown in Figure 2 (b), models employing random token dropping and token-wise pooling surprisingly outperform even some carefully designed methods. We hypothesize that the underlying reason lies in key shared characteristic of random token dropping and token-wise pooling: spatial uniformity. This property effectively mitigates the issue of position bias (Sec. 4) inherent in attentionbased token compression methods such as FastV. It also indirectly highlights the negative impact that position bias in attention scores can have on model performance. Therefore, we advocate for incorporating spatial uniformity as key consideration in the design of token compression strategies. 2https://github.com/haotian-liu/LLaVA 22 VideoLLMs: Video Understanding We conducted comprehensive comparative experiments on LLaVA-OneVision-7B [82] across multiple video large language models (VideoLLMs) benchmarks, including MVBench [86], LongVideoBench [147], MLVU [175], and VideoMME [44]. Our study compares various token compression methods, specifically FastV [19], SparseVLM [168], and PDrop [150], with the token retention ratio set to = 15%. All evaluations were performed using the LMMs-Eval framework [164], ensuring consistency and reproducibility in our experimental setup3. Figure 2 (c) reveals an unexpected result: even when retaining only 15% of visual tokens, random token dropping outperforms carefully designed token compression methods (e.g., FastV [19], SparseVLM [168], and PDrop [150]). This finding has two important implications: (i) Future token compression studies should include random token dropping as baseline for comparative experiments; (ii) The design of token compression methods for VideoLLMs should prioritize achieving uniform spatial and temporal distribution of tokens, potentially ensuring more comprehensive representation of video information. We hypothesize that the success of random dropping may be attributed to its inherent ability to maintain this uniformity across the video sequence. DiTs: Image Generation We conducted experiments on the widely-used DiT-based image generation model FLUX.1-dev [78], using the classic ToCa [179] method as an example. We set the cache cycle length to = 4 and the cache ratio to = 90%, meaning that only 10% of the tokens were computed at each cache step. We compared several classic token selection strategies, such as those based on maximum Attention, Key Norm (i.e., Knorm), Value Norm (i.e., Vnorm), and random selection. Surprisingly, all of these characteristic-based selection strategies yielded lower Image Reward scores than random selection. Figure 2 (d) shows that random selection performs well in token compression for image generation tasks. To investigate the reasons behind this phenomenon, we designed similarity-based token selection strategy: we first randomly selected 1% of the tokens as base tokens, and then selected the remaining 9% based on their highest similarity to the base tokens. This selection process resulted in the chosen tokens being clustered into one or few similar types, leading to the worst generation quality. This suggests that redundancy exists among similar tokens, whereas random selection benefits from diversity, allowing the selected subset of tokens to carry richer and more varied information. 3https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md 23 Table 1: Qwen series model specifications. Includes release dates, parameter counts, maximum context lengths, and Hugging Face links. Model Name Qwen-1.8B Qwen-7B Qwen-14B Qwen-72B Qwen1.5-0.5B Qwen1.5-1.8B Qwen1.5-4B Qwen1.5-7B Qwen1.5-14B Qwen1.5-32B Qwen1.5-72B Qwen1.5-110B Qwen1.5-MoE-A2.7B Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct-1M Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-30B-A3B Qwen3-235B-A22B Release Date Nov 30, 2023 Aug 3, 2023 Sep 25, 2023 Nov 30, 2023 Early 2024 Early 2024 Early 2024 Early 2024 Early 2024 Early 2024 Early 2024 Early 2024 Mar 28, 2024 Jun 6, 2024 Jun 6, 2024 Jun 6, 2024 Jun 6, 2024 Jun 6, 2024 Sep 19, 2024 Sep 19, 2024 Sep 19, 2024 Sep 19, 2024 Sep 19, 2024 Sep 19, 2024 Sep 19, 2024 Jan 2025 Jan 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Apr 29, 2025 Parameters Maximum Context Length Model Link 32K 2K (Original), 8K (Updated) 8K 32K 32K 32K 32K 32K 32K 32K 32K 32K 32K 32K 32K 32K (Base), 131K (Instruct) 32K (Base), 64K (Instruct) 32K (Base), 131K (Instruct) 32K 32K 32K 128K 128K 128K 128K 1M 1M 32K 32K 32K 131K 131K 131K 131K 131K 1.8B 7B 14B 72B 0.5B 1.8B 4B 7B 14B 32B 72B 110B 14B 0.5B 1.5B 7B 57B 72B 0.5B 1.5B 3B 7B 14B 32B 72B 7B 14B 0.6B 1.7B 4B 8B 14B 32B 30B 235B link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link link Table 2: DeepSeek series model specifications. Includes release dates, parameter counts, and maximum context lengths. Model Name DeepSeek-Coder DeepSeek-LLM DeepSeek-LLM DeepSeekMoE DeepSeek-Math DeepSeek-V2 DeepSeek-V2-Lite DeepSeek-Coder-V2 DeepSeek-Coder-V2-Lite DeepSeek-V2.5 DeepSeek-V3 DeepSeek-R1-Zero DeepSeek-R1 DeepSeek-R1-Distill DeepSeek-V3-0324 Release Date November 2, 2023 November 29, 2023 November 29, 2023 January 11, 2024 April 2024 May 6, 2024 May 16, 2024 June 17, 2024 June 17, 2024 September 2024 December 26, 2024 January 20, 2025 January 20, 2025 January 20, 2025 March 2025 Parameters 1.3B/6.7B/33B 7B 67B 16B total, 2.7B activated 7B 236B total, 21B activated 16B total, 2.4B activated 236B total, 21B activated 16B total, 2.4B activated 236B total, 21B activated 671B total, 37B activated 671B total, 37B activated 671B total, 37B activated 1.5B, 7B, 8B, 14B, 32B, 70B 671B total, 37B activated Context Length Model Link 16K tokens 4096 tokens 4096 tokens 4096 tokens 4096 tokens 128K tokens 32K tokens 128K tokens 128K tokens 128K tokens 128K tokens 128K tokens 128K tokens 32K tokens 128K tokens link link link link link link link link link link link link link link link 24 Table 3: Llama series model specifications. Details include release date, parameter count, context length, and Hugging Face model link. Model Name Llama 1 7B Llama 1 13B Llama 1 33B Llama 1 65B Llama 2 7B Llama 2 13B Llama 2 70B Llama 3 8B Llama 3 70B Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B Llama 4 Scout Llama 4 Maverick Release Date February 24, 2023 February 24, 2023 February 24, 2023 February 24, 2023 July 18, 2023 July 18, 2023 July 18, 2023 April 18, 2024 April 18, 2024 July 23, 2024 July 23, 2024 July 23, 2024 April 5, 2025 April 5, Parameters 7B 13B 33B 65B 7B 13B 70B 8B 70B 8B 70B 405B 109B total / 17B active 400B total / 17B active Context Length Model Link 2,048 tokens 2,048 tokens 2,048 tokens 2,048 tokens 4,096 tokens 4,096 tokens 4,096 tokens 8,192 tokens 8,192 tokens 128,000 tokens 128,000 tokens 128,000 tokens 10M tokens 1M tokens link link link link link link link link link link link link link link Table 4: GLM series model specifications. Includes release dates, parameter counts, maximum context lengths, and Hugging Face links. Model Name GLM-130B ChatGLM-6B ChatGLM2-6B ChatGLM2-6B-32K ChatGLM3-6B ChatGLM3-6B-32K ChatGLM3-6B-128K GLM-4-9B GLM-4-9B-Chat GLM-4-9B-Chat-1M Release Date August 2022 March 14, 2023 June 25, 2023 July 2023 October 2023 October 2023 November 2023 May 2024 May 2024 May 2024 Parameters 130B 6.2B 6.2B 6.2B 6.2B 6.2B 6.2B 9B 9B 9B Context Length 2,048 tokens 2,048 tokens 32,768 tokens 32,768 tokens 8,192 tokens 32,768 tokens 131,072 tokens 8,192 tokens 131,072 tokens 1,048,576 tokens Model Link link link link link link link link link link link Table 5: InternLM series model specifications. Includes release dates, parameter counts, maximum context lengths, and Hugging Face links. Model Name InternLM-7B InternLM-7B-Chat v1.1 InternLM-20B InternLM-20B-Chat InternLM2-7B InternLM2-20B InternLM2.5-7B InternLM2.5-7B-Chat-1M InternLM2.5-1.8B InternLM2.5-20B InternLM3-8B-Instruct Release Date July 2023 August 22, 2023 September 20, 2023 September 20, 2023 January 17, 2024 January 17, 2024 July 3, 2024 July 2024 August 1, 2024 August 1, 2024 January 15, 2025 Parameters 7B 7B 20B 20B 7B 20B 7B 7B 1.8B 20B 8B Context Length 8,000 tokens 8,000 tokens 16,000 tokens 16,000 tokens 200,000 tokens 200,000 tokens 200,000 tokens 1,000,000 tokens 200,000 tokens 200,000 tokens 32768 tokens Model Link link link link link link link link link link link link 25 Table 6: LLaVA series model specifications. Includes release dates, backbone models, context lengths, and multimodal capabilities. si: single image; mi: multiple images; vid: video. LLM Backbone Max Context Release Date Vicuna-7B April 2023 Vicuna-13B April 2023 October 2023 Vicuna-7B-v1.5 October 2023 Vicuna-13B-v1.5 January 2024 January 2024 January 2024 January 2024 Nous-Hermes-2-Yi-34B Mistral-7B Vicuna-7B-v1.5 Vicuna-13B-v1.5 Model Name LLaVA-7B LLaVA-13B LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-NeXT-7B LLaVA-NeXT-7B LLaVA-NeXT-13B LLaVA-NeXT-34B LLaVA-OneVision-0.5B August 2024 LLaVA-OneVision-7B August 2024 LLaVA-OneVision-72B August 2024 Qwen2-0.5B Qwen2-7B Qwen2-72B Image Resolution 224224 224224 336336 336336 336x{2x2,1x{2,3,4}, {2,3,4}x1} 336x{2x2,1x{2,3,4}, {2,3,4}x1} 336x{2x2,1x{2,3,4}, {2,3,4}x1} 336x{2x2,1x{2,3,4}, {2,3,4}x1} 336336[6,6] 336336[6,6] 336336[6,6] Max Tokens 256 256 576 576 2880 2880 2880 2880 7290(si), 8748(mi), 6272(vid) 7290(si), 8748(mi), 6272(vid) 7290(si), 8748(mi), 6272(vid) Model Link link link link link link link link link link link link 2K 2K 4K 4K 8K 4K 4K 4K 32K 32K 32K Table 7: InternVL series model specifications. Includes release dates, backbone architectures, and multimodal capabilities. LLM Backbone Max Context Vicuna-7B Vicuna-13B InternLM2-20B Release Date Model Name Dec 2023 InternVL-21B InternVL-27B Dec 2023 InternVL1.5-26B Apr 2024 Dec 2024 Qwen2.5-0.5B-Instruct InternVL2.5-1B Dec 2024 Internlm2.5-1.8B-chat InternVL2.5-2B Dec 2024 Qwen2.5-3B-Instruct InternVL2.5-4B Internlm2.5-7B-chat InternVL2.5-8B Dec 2024 InternVL2.5-26B Dec 2024 Internlm2.5-20B-chat InternVL2.5-38B Dec 2024 Qwen2.5-32B-Instruct InternVL2.5-78B Dec 2024 Qwen2.5-72B-Instruct InternVL3-1B InternVL3-2B InternVL3-8B InternVL3-9B InternVL3-14B InternVL3-38B InternVL3-78B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-7B InternLM3-8B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B Apr 2025 Apr 2025 Apr 2025 Apr 2025 Apr 2025 Apr 2025 Apr 2025 2K 2K 200K 32K 200K 32K 200K 200K 128K 128K 32K 32K 128K 32K 128K 128K 128K Image Resolution 224224, 336336, 448448 224224, 336336, 448448 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 26882688 Max Tokens Model Link 1,024 1,024 8,192 8,192 8,192 8,192 8,192 8,192 8,192 8,192 32K 32K 32K 32K 32K 32K 32K link link link link link link link link link link link link link link link link link Table 8: Qwen-VL series model specifications. Includes release dates, backbone architectures, and multimodal capabilities. Model Name Aug 2023 Qwen-VL-9.6B Sep 2024 Qwen2-VL-2B Sep 2024 Qwen2-VL-7B Sep 2024 Qwen2-VL-72B Feb 2025 Qwen2.5-VL-3B Qwen2.5-VL-7B Feb 2025 Qwen2.5-VL-72B Feb 2025 Release Date LLM Backbone Max Context Qwen-7B Qwen2-1.5B Qwen2-7B Qwen2-72B Qwen2.5-3B Qwen2.5-7B Qwen2.5-72B 2K 32K 32K 32K 32K 128K 128K Image Resolution 448448 native resolution(max=20482048) native resolution(max=20482048) native resolution(max=20482048) native resolution(max=20482048) native resolution(max=20482048) native resolution(max=20482048) Max Tokens Model Link 1,024 16,384 16,384 16,384 24,576 24,576 24,576 link link link link link link link"
        }
    ],
    "affiliations": [
        "EPIC Lab, Shanghai Jiao Tong University",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Shanghai AI Laboratory",
        "Sichuan University",
        "Sun Yat-sen University",
        "University of Electronic Science & Technology of China"
    ]
}