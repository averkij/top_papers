{
    "paper_title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
    "authors": [
        "Shaojie Zhang",
        "Pei Fu",
        "Ruoceng Zhang",
        "Jiahui Yang",
        "Anan Du",
        "Xiuwen Xi",
        "Shaokang Wang",
        "Ying Huang",
        "Bin Qin",
        "Zhenbo Luo",
        "Jian Luan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing a misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, a novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces a dual reward mechanism, combining a binary reward for correct actions with a truncated Gaussian-based spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 6 6 2 7 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "HYPERCLICK: ADVANCING RELIABLE GUI GROUNDING VIA UNCERTAINTY CALIBRATION Shaojie Zhang Pei Fu Ruoceng Zhang Jiahui Yang Anan Du Xiuwen Xi Shaokang Wang Ying Huang Bin Qin Zhenbo Luo Jian Luan MiLM Plus, Xiaomi Inc {zhangshaojie5, fupei1, luozhenbo, luanjian}@xiaomi.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces dual reward mechanism, combining binary reward for correct actions with truncated Gaussianbased spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation. Figure 1: Overview of accuracy and confidence evaluation on ScreenSpot-Pro. (a): Illustration of probabilistic and verbalized confidence. Probabilistic confidence represents the probability of the model generating the next token corresponding to the target coordinates, while verbalized confidence indicates the models self-reported certainty about its output in natural language. (b): Comparisons of accuracy, probabilistic confidence, and verbalized confidence for several general-purpose and GUI-specific models on the ScreenSpot-Pro benchmark. The models exhibit higher confidence in their answers than in the accuracy that they actually achieve. Equal contribution; Corresponding author. https://github.com/xiaomi-research/hyperclick"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "The revolution of autonomous Graphical User Interface (GUI) agents is transforming human-computer interaction, enabling users to control mobile applications, web platforms, and complex desktop software directly through natural language instructions (Wang et al., 2024b; Nguyen et al., 2024). At the heart of these agents lies the GUI grounding, the ability to accurately map textual commands to precise pixel coordinates on user interface elements (Cheng et al., 2024; Tang et al., 2025a). This fundamental task determines whether an agent can successfully execute user commands, making it the cornerstone of reliable GUI automation. Recent progress in GUI grounding has been driven by supervised fine-tuning (SFT) with curated large-scale datasets (Wu et al., 2024; Gou et al., 2025; Xu et al., 2024) and reinforcement fine-tuning (RFT) with verifiable GUI-specific rewards (Lu et al., 2025; Luo et al., 2025; Liu et al., 2025b). Although these techniques yield strong performance, they share critical weakness: the lack of self-awareness of their capability boundary, making it difficult to judge when predictions are reliable. reliable GUI agent should be aware of its limitations and accurately distinguish between what it can and cannot do (Ding et al., 2025). While this ability has been extensively studied in large language models (LLMs) (Xiong et al., 2023; Tian et al., 2023), it remains underexplored in GUI agents. The reliability level of an agent is assessed by the alignment between its confidence and actual performance (Ding et al., 2025). In this paper, we first evaluate probabilistic and verbalized confidence for several general models (OpenAI, 2024; Bai et al., 2025; Guo et al., 2025b; Team et al., 2025; Xiaomi, 2025) and GUI-specific models (Qin et al., 2025) on the ScreenSpot-Pro benchmark (Li et al., 2025), which emphasizes high-resolution displays, smaller target sizes, and complex environments. Specifically, probabilistic confidence reflects token-level likelihoods for predicted coordinates (Guo et al., 2017; Desai & Durrett, 2020), while verbalized confidence captures self-reported certainty in natural language (Lin et al., 2022; Yang et al., 2024b). As shown in Figure 1, the models exhibit higher confidence in their answers than in the accuracy that they actually achieve. In other words, even on challenging tasks, these agents remain overconfident in their predictions both probabilistically and from self-assessed perspective. We argue that this is analogous to the hallucination problem commonly observed in LLMs and vision-language models (VLMs), where the model produces fluent, yet factually erroneous outputs while maintaining high confidence (Ji et al., 2023a;b; Kalai et al., 2025). This limitation is particularly critical in realworld GUI tasks, where their dynamic and continuous nature means that even single error in an intermediate step can result in overall task failure. To address this limitation, we propose HyperClick, novel framework that enhances reliable GUI grounding through uncertainty calibration. Unlike prior approaches that treat grounding as pure hit-or-miss classification problem, HyperClick explicitly integrates verbalized confidence estimation into the grounding process. Each prediction consists not only of selected UI element, but also of natural-language confidence statement, providing self-assessment of reliability. Specifically, we introduce two complementary rule-based reward mechanisms that optimize both action accuracy and uncertainty calibration. binary reward enforces correct grounding actions, while truncated Gaussianbased distribution models spatial confidence over the entire screenshot. The predicted confidence is then calibrated against this distribution using the Brier score (Glenn et al., 1950; Damani et al., 2025). This dual mechanism enables HyperClick to achieve two intertwined goals: accurate GUI grounding and well-calibrated confidence. More importantly, it fosters form of introspectiveness, where the model not only acts but also critiques its own reliability. This selfcriticism capacity reduces overconfidence, supports safer decision-making, and gradually expands the agents boundaries of reliable operation. Our contributions are summarized as follows: We systematically reveal that existing GUI grounding models are prone to overconfidence, analogous to hallucinations in LLMs and VLMs, and highlight their critical implications for reliable GUI automation. We propose HyperClick, the first GUI grounding framework that explicitly integrates uncertainty calibration, introducing dual reward mechanism that jointly optimizes grounding"
        },
        {
            "title": "Preprint",
            "content": "accuracy and confidence reliability via binary correctness and truncated Gaussianbased confidence modeling. Through extensive evaluations on challenging GUI grounding benchmarks, HyperClick not only achieves state-of-the-art (SOTA) accuracy but also establishes well-calibrated confidence, enabling introspective self-criticism and more reliable GUI agents."
        },
        {
            "title": "2.1 GUI AGENTS AND GROUNDING",
            "content": "GUI agents, as autonomous intelligent systems specialized in interacting with graphical user interfaces, have emerged as key technology to automate complex desktop and mobile tasks (Wang et al., 2024b; Nguyen et al., 2024; Zhang et al., 2024). Recently, VLM-based GUI agents (Cheng et al., 2024; Wu et al., 2024; Qin et al., 2025) have demonstrated strong GUI comprehension by integrating visual perception with language reasoning, allowing them to handle diverse interface styles across applications. At the heart of VLM-based GUI agents lies the task of GUI grounding, which bridges natural language instructions with precise interface elements, thereby underpinning reliable GUI automation. Early works (Cheng et al., 2024; Lin et al., 2025; Yang et al., 2024a) primarily focused on acquiring GUI-specific capabilities by collecting large-scale GUI corpora for SFT, thereby developing models customized for GUI tasks. SeeClick (Cheng et al., 2024) first introduced VLM to complete GUI tasks with only visual inputs. OS-Atlas (Wu et al., 2024), UGround (Gou et al., 2025), and Aguvis (Xu et al., 2024) aim to enhance perception by fine-tuning pre-trained models on dataset constructed from diverse environments. UI-TARS (Qin et al., 2025) develops native end-to-end GUI agent through large-scale GUI screenshots to enhance perception and reasoning for unified action modeling across platforms. With the success of DeepSeek-R1-Zero (Guo et al., 2025a), RFT has drawn increased attention in the GUI-specific domain. UI-R1 (Lu et al., 2025), GUI-R1 (Luo et al., 2025), InfiGUI-R1 (Liu et al., 2025b), and BTL-UI (Zhang et al., 2025b) naively replicate techniques from DeepSeek-R1, prompting the model to think before generating an answer and optimizing the policy model with Verifiable GUI-specific reward functions. However, these native R1-based GUI agents overlook an important insight: Chain-of-Thought (CoT) reasoning degrades performance in GUI grounding, where precise spatial perception matters more than deep reasoning. Subsequently, GUI-G1 (Zhou et al., 2025) revisits the limitations of current R1-based GUI agents by introducing controllable box-size rewards for grounding tasks. SE-GUI (Yuan et al., 2025) proposes self-evolution approaches and continuous rewards to guide model learning. GUI-G2 (Tang et al., 2025a) further introduced Gaussian reward modeling for GUI grounding. However, existing GUI grounding approaches primarily focus on improving grounding accuracy, while largely overlooking the importance of confidence calibration. 2.2 UNCERTAINTY CALIBRATION The concept of uncertainty originates from the error analysis theory, where it quantifies the degree of confidence associated with measurement (Oberkampf et al., 2002). This notion has been widely adopted in computer vision tasks such as object detection (Ren et al., 2015; Redmon et al., 2016) and semantic segmentation (Long et al., 2015; He et al., 2017), helping to assess the reliability of model predictions. With the rise of large language models (LLMs) and vision-language models (VLMs), several representative types of confidence signals have been proposed to capture the uncertainty of generated natural language: (1) Probabilistic confidence (Guo et al., 2017; Desai & Durrett, 2020), which uses token generation probabilities as measure of uncertainty; (2) Answer consistency confidence (Zhang et al., 2023; Manakul et al., 2023; Fu et al., 2025), which quantifies uncertainty based on semantic consistency between multiple model outputs rather than token-level probabilities; and (3) Verbalized confidence (Lin et al., 2022; Yang et al., 2024b), where the model explicitly reports its confidence in natural language, providing an intuitive model-agnostic signal without requiring repeated sampling. Building on these advances, uncertainty estimation has been shown to improve the robustness and reliability of neural network systems by providing calibrated confidence for downstream decision-making."
        },
        {
            "title": "3 METHOD",
            "content": "Figure 2: Framework of the proposed HyperClick, optimized with Group Relative Policy Optimization (GRPO). Given screenshot and an instruction, the policy generates predictions, which are evaluated by verifiable reward mechanism. The correctness reward measures grounding precision, while the calibration reward assesses uncertainty. For clarity, the reference model is omitted. 3.1 PROBLEM FORMULATION GUI grounding can be formalized as the problem of mapping natural language instruction to spatial coordinates corresponding to the target UI element on given screenspot. From the perspective of policy optimization, this task can be instantiated in two ways: location formulation (Wu et al., 2024; Tang et al., 2025a) and click formulation (Xu et al., 2024; Luo et al., 2025; Yuan et al., 2025). Location formulation: Given screenshot and an instruction q, the policy model is optimized by predicting the bounding box ˆb = (ˆx1, ˆy1, ˆx2, ˆy2), where (ˆx1, ˆy1) and (ˆx2, ˆy2) denote the top-left and bottom-right corners of the UI element referred to by q. Click formulation: Alternatively, the policy model predicts single point ˆp = (ˆx, ˆy), corresponding to the center of the target element, which directly simulates clicking action. In this work, we adopt the click formulation as our primary paradigm, as it naturally aligns with executable actions in GUI interaction, simplifies the action space compared to bounding-box prediction, and provides direct objective for reinforcement learning. 3.2 CONFIDENCE MODELING Building on the introduction of the Gaussian distribution in error analysis theory (Gauss, 1809; 1877; MacKenzie, 1988) and recent advances in GUI-G2 (Tang et al., 2025a), we model the confidence distribution in GUI grounding using Gaussian formulation. Furthermore, as shown in Figure 2, since most UI element annotations are represented as bounding boxes, and to jointly account for correctness and confidence, we adopt truncated Gaussian distribution (Galli et al., 1994) to model confidence. Truncated Gaussian Representation. For each UI element with bounding box = (x1, y1, x2, y2), the 2D Gaussian distribution on the screenshot interface can be denoted as: (x; µ, Σ) = 1 2πΣ 1 exp( 1 2 (x µ)TΣ1(x µ)), (1) where means any point on the 2D interface, µ = (µx, µy) = ( x1+x2 , y1+y2 ) represents the 2 center point of the UI element, and Σ = is the covariance matrix. The diagonal (cid:18)σ2 0 (cid:19) 0 σ"
        },
        {
            "title": "Preprint",
            "content": "structure assumes independence between the dimensions and y, simplifying the computation while maintaining expressiveness. We preliminarily formulate the uncertainty distribution based on the constructed 2D Gaussian distribution on the interface. For each prediction point ˆp on the screenshot, the confidence value can be computed: C(ˆp) = 2πΣ 1 2 (ˆp; µ; Σ) = exp( 1 [ (ˆx µx)2 σ2 + (ˆy µy)2 σ2 ]). (2) For the center point (µx, µy) of the grounding truth bounding box b, the constructed value naturally reaches its maximum value of 1. This means that when the policy model predicts the point (µx, µy), the model should have the highest confidence in its response. Furthermore, we truncate the constructed confidence distribution by restricting it to the region defined by the bounding box b, which aligns with the discriminative nature of the task. Specifically, confidence is assigned only when the predicted point ˆp is within b; otherwise, the confidence is set to zero. In summary, the confidence distribution is modeled as truncated Gaussian: C(ˆp) = (cid:26)C(ˆp), 0, (x1 < ˆx < x2) (y1 < ˆy < y2), otherwise. (3) Adaptive Variance. Previous approaches (Zhou et al., 2025; Tang et al., 2025a) have highlighted the difficulty bias in GUI grounding, where target elements with smaller relative box size on the screenshot are more challenging. To handle UI elements with wide range of sizes, we adopt the adaptive variance mechanism to control the confidence distribution on various platforms and screenshots: (4) which α is scaling factor that controls the relative influence of the element size on the standard deviations. σx = α (x2 x1), σy = α (y2 y1), 3.3 TRAINING OBJECTIVE Correctness Reward. As shown in Figure 2, we adopt the binary reward mechanism to guide the prediction point of the policy model ˆp within the bounding box b. This discrete supervision directly aligns the policy objective with the success or failure of the grounding. Therefore, the correctness reward is expressed as follows: Rcorrectness = 1ˆpb = (cid:26)1, 0, (x1 < ˆx < x2) (y1 < ˆy < y2), otherwise. (5) Confidence Reward. The purpose of the confidence reward is to encourage the policy model to evaluate and criticize the prediction generated ˆp, making the confidence in the model output more precise. Thus, the confidence ˆc of the model output should be aligned with the confidence distribution constructed in section 3.2. To achieve this, we introduce the Brier score (Glenn et al., 1950) to build the reward function, which can be thought of as measure of the calibration of set of probabilistic forecasts. The confidence reward can be formulated as follows. Rconfidence = 1 (ˆc C(ˆx, ˆy))2. (6) This formulation provides several key properties. First, the closer the prediction confidence of the policy model ˆc to the value corresponding to the constructed confidence distribution, the model will receive more reward. Second, when the models prediction is incorrect and has low confidence value for its generation, the policy model can still obtain high confidence reward, which aligns with the models motivation to self-criticize through confidence. In summary, the final reward signal for the policy model combines format reward Rformat with the correctness reward Rcorrectness and the confidence reward Rconfidence. The total reward is thus: = Rformat + Rcorrectness + Rconfidence. (7) We optimize HyperClick with Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which extends the idea of relative advantage estimation to group of predictions. Unlike standard policy"
        },
        {
            "title": "Preprint",
            "content": "gradient methods that rely on single sampled return, GRPO leverages multiple candidate outputs to construct relative reward signal, leading to more stable and informative optimization. Given generations {oi}N i=1, each is evaluated by the reward function R. GRPO normalizes these rewards within the group to obtain relative advantages: Ai = R(oi) mean({R(oj)}N std({R(oj)}N j=1) j=1) (8) The training objective of GRPO is then defined as (θ) = {oi}N i=1πθold (s,q)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:26) min i=1 (cid:20) πθ(ois, q) πθold (ois, q) Ai, clip (cid:18) πθ(ois, q) πθold(ois, q) (cid:19) (cid:21) (cid:27) , 1 ϵ, 1 + ϵ Ai β KL(πθπref) , (9) where πθ denotes the policy model parameterized by θ, ϵ is hyperparameter that controls clip(, 1 ϵ, 1 + ϵ) and β weights the KL regularization (Schulman et al., 2017; Shao et al., 2024) to stabilize training."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS We implement HyperClick on top of Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct. Training data is sampled from multiple public GUI datasets, including OS-Atlas (Wu et al., 2024), Widget Caption (Li et al., 2020), UI-Refexp (Bai et al., 2021), and OmniAct (Kapoor et al., 2024), resulting in approximately 30K samples. Model training is conducted within the VLM-R1 (Shen et al., 2025) codebase. We train for one epoch on 16 NVIDIA H100 GPUs, using learning rate linearly decayed from 1e-6 to 0 with cosine scheduler, global batch size of 16, 8 generations per instance, and KL constraint coefficient of β = 0.04. To improve efficiency, we leverage FlashAttention-2 (Dao, 2023), adopt bfloat16 precision, and enable gradient checkpointing. During inference, the temperature is fixed to 0 to ensure reproducibility. 4.2 EVALUATION BENCHMARKS We comprehensively evaluate the GUI grounding capability of HyperClick on ScreenSpot (Cheng et al., 2024) (SS), ScreenSpot-V2 (Wu et al., 2024) (SS2), ScreenSpot-Pro (Li et al., 2025) (SSP), MMBench-GUI (Wang et al., 2025) (MMG), UI-I2E-Bench (Liu et al., 2025a) (I2E), CAGUI (Zhang et al., 2025c) (CAG) and UI-Vision (Nayak et al., 2025) (UIV). More details about each evaluation benchmark are described in the Appendix. 4.3 MAIN RESULTS Comparisons with Baselines. The main experimental results of HyperClick and comparisons with general models and GUI-specific models are shown in Table 4. HyperClick achieves consistently strong performance across all benchmarks. In particular, HyperClick-7B reaches new SOTA results in SS2 (93.7), SSP (48.2), MMG (79.6), I2E (76.5), CAG (82.9), and UIV (25.7), surpassing previous RFT-based approaches such as GUI-G2 (Tang et al., 2025a) and SE-GUI (Yuan et al., 2025). In ScreenSpot (SS), HyperClick-7B obtains 91.5, which is highly competitive and comparable to the best results (92.0) of GUI-G2. Moreover, HyperClick also demonstrates strong performance, outperforming much larger GUI-specific models, such as UI-TARS-72B (Wu et al., 2024) and Aguvis-72B (Xu et al., 2024). key source of HyperClicks improvement lies in the introduction of uncertainty calibration, which equips the model with self-criticism mechanism. Unlike GUI grounding models that rely solely on sparse binary (Lu et al., 2025; Luo et al., 2025) or continuous (Yuan et al., 2025; Tang et al., 2025a) correctness rewards, HyperClick leverages calibrated confidence distribution to explicitly distinguish between reliable and uncertain predictions. This enables the policy to penalize overconfident errors while reinforcing well-calibrated clicks. As shown in Table 4, such self-criticism translates into"
        },
        {
            "title": "Preprint",
            "content": "Table 1: GUI grounding accuracy on seven benchmarks including ScreenSpot (Cheng et al., 2024) (SS), ScreenSpot-V2 (Wu et al., 2024) (SS2), ScreenSpot-Pro (Li et al., 2025) (SSP), MMBenchGUI (Wang et al., 2025) (MMG), UI-I2E-Bench (Liu et al., 2025a) (I2E), CAGUI (Zhang et al., 2025c) (CAG) and UI-Vision (Nayak et al., 2025) (UIV). Bold and underline indicate the best and second-best results. The detailed experimental results on each benchmark are in the appendix. Model Size SS SS2 SSP MMG I2E CAG UIV General Models GPT-4o (OpenAI, 2024) Claude (Anthropic, 2024) Qwen2-VL (Wang et al., 2024a) Qwen2.5-VL (Bai et al., 2025) Intern3VL (Zhu et al., 2025) MiMo-VL (Xiaomi, 2025) GUI-specific Models (SFT) CogAgent (Hong et al., 2024) SeeClick (Cheng et al., 2024) Aria-UI (Yang et al., 2024a) ShowUI (Lin et al., 2025) UGround (Gou et al., 2025) UGround-V1 (Gou et al., 2025) OS-Atlas (Wu et al., 2024) Aguvis (Xu et al., 2024) UI-TARS (Qin et al., 2025) TongUI (Zhang et al., 2025a) GUI-Actor (Wu et al., 2025) JEDI (Xie et al., 2025) GUI-specific Models (RFT) UI-R1 (Lu et al., 2025) UI-R1-E (Lu et al., 2025) GUI-R1 (Luo et al., 2025) InfiGUI-R1 (Liu et al., 2025b) GUI-G1 (Zhou et al., 2025) SE-GUI (Yuan et al., 2025) LPO (Tang et al., 2025b) GUI-G2 (Tang et al., 2025a) Ours HyperClick - - 7B 3B 7B 8B 38B 7B 18.8 83.0 42.9 55.5 84.7 79.5 85.6 87.2 47.4 18B 9.6B 53.4 25.3B 82.4 75.1 73.3 77.7 86.3 70.1 82.5 84.4 89.2 82.3 89.5 88.4 83.6 86.0 86.5 88.3 - - 2B 7B 2B 7B 4B 7B 7B 72B 2B 7B 72B 3B 7B 2B 7B 3B 7B 83.3 89.2 - - 87.5 90.3 88.2 - 92.0 3B 3B 3B 7B 3B 3B 7B 8B 7B 3B 7B 20.1 - - 80.9 88.8 81.4 88.3 90.5 - 55.1 - 77.3 - - - 71.9 84.1 - - 84.7 91.6 90.3 85.5 88.7 88.6 89.5 88.6 91.7 85.4 89.5 - - - - 90.3 90.5 93.3 0.8 17.1 - 16.1 26.8 - - 41.9 7.7 1.1 11.3 7.7 16.5 - 31.1 3.7 18.9 - - 27.7 35.7 38.1 18.0 24.7 42.2 44.6 36.1 39.5 17.8 33.5 28.6 31.3 35.7 37.1 47.3 - 47. 2.9 4.7 - - 33.9 - - - - - - 16.0 - - 65.7 - 41.4 45.7 - - - 74.3 - - - - - - - - - - - - - - - - - 48.7 41.7 53.8 - - - - 26.4 - 41.5 16.5 57.4 70.3 44.3 58.6 53.2 - 27.7 61.4 73.7 - - - - - - 58.5 - - - 69.7 - - - - 21.0 - - - 59.6 - - - - - - - - - - - 57.2 68.7 - - 61.8 - - - - - - - - - - - - - - - - 1.4 8.3 2.7 - 0.9 - - - 8.9 5.4 10.1 5.9 8.8 12.9 23.2 - 9.0 13.7 - - 17.6 25.5 15.4 18.0 - - 19.0 25.2 - - - - - - - - - 88.5 91.5 90.6 93.7 41.3 48.2 71.4 79.6 71.8 76.5 81.0 82. 19.6 25.7 consistent gains across benchmarks, highlighting that calibrated confidence improves the models ability to generalize across diverse UI environments. These results confirm that confidence-aware grounding not only enhances accuracy but also makes the model more robust to task difficulty and annotation variability. The confidence of HyperClick is reliable. To evaluate whether HyperClick is truly reliable, we introduce the average precision (AP) of object detection (Lin et al., 2014), which adopts"
        },
        {
            "title": "Preprint",
            "content": "Table 2: The evaluation of HyperClick on ScreenSpot-Pro is conducted under reliable and reproducible settings. The Original accuracy refers to the results reported in the corresponding papers or reproduced by subsequent studies, while the Replicated accuracy denotes our reproduction using the vllm-project (Kwon et al., 2023) with the official model weights. The observed performance gaps may stem from differences in prompt design or in whether unparsed outputs are included during evaluation. Model GPT-4o (OpenAI, 2024) Doubao (Guo et al., 2025b) Qwen2.5-VL (Bai et al., 2025) KiMi-VL (Team et al., 2025) MiMo-VL (Xiaomi, 2025) UI-TARS (Qin et al., 2025) UI-TARS-1.5 (Qin et al., 2025) HyperClick Size - - 7B 16B 7B 7B 7B 3B 7B Accuracy Original Replicated APconf=50 APconf=75 APconf=90 ARconf=95 0.8 - 26.8 34.5 39.9 35.7 - - - 0.8 13.0 22.5 35.4 38.3 37.6 37.2 41.3 48.2 0.9 13.6 24.9 34.8 29.5 37.5 37.6 70.6 61.3 0.9 15.8 24.9 34.8 28.9 37.5 37.5 76.0 64. 1.2 21.2 24.8 25.8 28.8 37.4 37.5 78.0 71.2 1.0 21.5 24.7 40.6 30.0 39.3 40.4 78.0 78.7 Table 3: Ablation study of reward configurations. Rformat Rcorectness Rconfidence Acc(%) Table 4: Ablation study of confidence. α Acc(%) Table 5: Ablation of baseline. Model Acc(%) 47.5 47.7 48.0 48.2 0 1/2 1/4 1/6 47.7 48.0 48.2 45. Qwen2.5-VL HyperClick MiMo-VL HyperClick 26.8 48.2 39.9 49.5 confidence {0.5, 0.75, 0.9, 0.95} as the boundary positive for counting positive and negative samples. As shown in Table 2, HyperClick consistently maintains high AP across all thresholds, and as the confidence threshold increases, the AP also gradually increases, which indicates that the model not only makes accurate predictions but also assigns well-calibrated confidence scores, rather than overestimating or underestimating its certainty. Furthermore, compared to baseline models, HyperClick shows clear margin of improvement, particularly in the high-confidence regime (APconf=90 and APconf=95). This suggests that HyperClick is capable of self-criticizing its predictions: when the model outputs high confidence score, the prediction is highly reliable; when the score is low, it effectively signals uncertainty. Such behavior is crucial for practical deployment in GUI automation, where wrong but overconfident predictions may lead to catastrophic task failures. 4.4 ABLATION STUDY We conducted an ablation study on ScreenSpot-Pro to verify the effectiveness of key components of HyperClick. Reward Mechanism. The results in Table 3 demonstrate the importance of combining correctness and confidence rewards. Using only the format or correctness reward yields relatively limited improvements (47.5% and 47.7%, respectively). Introducing the confidence reward alone already achieves stronger performance (48.0%), while the combination of correctness and confidence rewards further increases the precision to 48.2%. This validates our motivation that confidence calibration acts as self-critical signal, discouraging overconfident errors and reinforcing reliable predictions. Confidence Modeling. Table 4, investigates the effect of the adaptive variance factor α, Without confidence modeling based on the truncated Gaussian distribution (α=0), which means only binary confidence is used for uncertainty calibration. Therefore, when α=0, the confidence reward is represented as: Rconfidence = 1 (ˆc 1 ˆpb)2, the policy model reaches 47.7%, which is weaker than the truncated Gaussian variants. Moreover, we set α according to the principle 3σ of the Gaussian distribution. Take the direction as an example, σx = 1 6 }. As shown in the results, while too large (α = 1 6 ) variances lead to suboptimal performance. 2 (x2 x1), where {1, 2, 3} and subtract the scaling factor α { 1 2 ) or too small (α = 1 2 , 4 , 1 (10)"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Visualization of the confidence distribution output by HyperClick. We inject the coordinates on the interface into the assistants generation and enforce it to continue to output the confidence for the click position. The darker the color, the higher the confidence value. Specifically, when α = 1 2 , the variance is too large and the Gaussian distribution is excessively truncated within the bounding box. As result, the confidence mass is overly concentrated near the center, which weakens the models sensitivity to the boundary regions of the element. In contrast, when α = 1 6 , the variance is too small, leading to distribution that is too truncated. Consequently, the confidence at the edge of the bounding box is nearly zero, making the calibration too strict and reducing the tolerance to minor prediction deviations. For comparison, α = 1 4 provides balanced trade-off between concentration and spread, providing the most effective uncertainty modeling and the highest precision (48.2%). Extension to other baselines. As shown in Table 5, we further extend HyperClick to MiMo-VL (Xiaomi, 2025), strong general-purpose VLM. With our training framework, MiMo-VL improves from 39.9% to 49.5%, demonstrating that HyperClick serves as plug-and-play training paradigm for GUI grounding. Similarly, applying HyperClick to Qwen2.5-VL also brings substantial improvement (from 26.8% to 48.2%), confirming the generality and scalability of our approach across different foundation models. 4.5 VISUALIZATION To better understand the effect of uncertainty calibration, we visualize the confidence distributions predicted by HyperClick in Figure 3. For each instruction, we inject the coordinates on the interface into the assistants generation and enforce the policy model, continuing to output the click position. Thus, the heatmap represents its confidence in the possible click positions on the interface. We observe that the confidence is sharply concentrated around the ground-truth elements, while irrelevant regions exhibit low or near-zero confidence. This aligns with our design of truncated Gaussian modeling, where confidence only exists inside valid bounding boxes. Moreover, the adaptive variance mechanism adjusts the spread of the confidence distribution according to the element size: smaller UI elements yield tighter confidence peaks, whereas larger ones result in more diffuse heatmaps. These visualizations intuitively demonstrate how HyperClick achieves reliable and robust GUI grounding by avoiding overconfident but incorrect clicks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we address the critical issue of overconfidence in GUI grounding models, which undermines the reliability of autonomous GUI agents. We introduce HyperClick, novel framework that augments grounding with explicit uncertainty calibration. By combining binary correctness rewards with truncated Gaussianbased spatial confidence modeling, HyperClick enhances grounding"
        },
        {
            "title": "Preprint",
            "content": "accuracy while producing well-calibrated confidence estimates, enabling the agent to assess its own reliability introspectively. Extensive experiments on challenging benchmarks demonstrate that HyperClick achieves SOTA performance in both accuracy and calibration, substantially enhancing the reliability of GUI agents. Looking ahead, this framework can be extended to broader multimodal agentic settings, where reliable confidence estimation is essential for safe and reliable human-AI interaction."
        },
        {
            "title": "ETHICS AND REPRODUCIBILITY STATEMENT",
            "content": "This research focuses on building policy model for reliable GUI grounding. The data used are obtained by synthesizing or reprocessing previously released datasets, with all datasets or benchmarks properly cited. In this paper, there are no discrimination, bias, or fairness issues that need to be addressed. In addition, our models are not expected to generate potentially harmful content. To ensure reproducibility, we provide all experimental and data details in Section 4 and the corresponding appendices. We will release the source code and model checkpoints to support reproducibility."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024. Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 93139332, 2024. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892, 2020. Zhikai Ding, Shiyu Ni, and Keping Bi. Do lvlms know what they know? systematic study of knowledge boundary perception in lvlms. arXiv preprint arXiv:2508.19111, 2025. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Galli, Beucher, Le Loch, Doligez, and Heresim Group. The pros and cons of the truncated gaussian method. In Geostatistical Simulations: Proceedings of the Geostatistical Simulation Workshop, Fontainebleau, France, 2728 May 1993, pp. 217233. Springer, 1994."
        },
        {
            "title": "Preprint",
            "content": "Carl Friedrich Gauss. Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore Carolo Friderico Gauss. sumtibus Frid. Perthes et IH Besser, 1809. Carl Friedrich Gauss. Theoria motus corporum coelestium in sectionibus conicis solem ambientium, volume 7. FA Perthes, 1877. Brier Glenn et al. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In Proceedings of the International Conference on Learning Representations, 2025. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In Proceedings of the IEEE International Conference on Machine Learning, pp. 1321 1330. PMLR, 2017. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pp. 29612969, 2017. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138, 2023a. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating llm hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 18271843, 2023b. Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang. Why language models hallucinate. arXiv preprint arXiv:2509.04664, 2025. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In Proceedings of the European Conference on Computer Vision, pp. 161178. Springer, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020."
        },
        {
            "title": "Preprint",
            "content": "Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1949819508, 2025. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European conference on computer vision, pp. 740755. Springer, 2014. Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, and Yan Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis. arXiv preprint arXiv:2504.11257, 2025a. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025b. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 34313440, 2015. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Donald MacKenzie. The history of statistics: the measurement of uncertainty before 1900 by stephen m. stigler. Technology and Culture, 29(2):299300, 1988. Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023. Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, Tamer Ozsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: desktopcentric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. William Oberkampf, Sharon DeLand, Brian Rutherford, Kathleen Diegert, and Kenneth Alvin. Error and uncertainty in modeling and simulation. Reliability Engineering & System Safety, 75(3):333357, 2002. OpenAI. Hello gpt-4o, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779788, 2016. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. volume 28, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025a. Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, et al. Lpo: Towards accurate gui agent interaction via location preference optimization. arXiv preprint arXiv:2506.09373, 2025b. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023. Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1564115653, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024b. Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. 2024. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/ 2506.03569. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025."
        },
        {
            "title": "Preprint",
            "content": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024a. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty. volume 37, pp. 6356563598, 2024b. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, SongChun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025a. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Malin, and Sricharan Kumar. Sac3: reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. arXiv preprint arXiv:2311.01740, 2023. Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, et al. Btl-ui: Blink-think-link reasoning model for gui agent. arXiv preprint arXiv:2509.15566, 2025b. Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025c. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 USE OF LLMS In this paper, LLMs are employed solely as auxiliary tools for text refinement. Specifically, they are used to edit, polish and improve the clarity and readability of the manuscript, without contributing to the design of methods, the execution of experiments, or the analysis of results. All conceptual development, technical implementation, and empirical evaluation were independently conducted by the authors. The use of LLMs is therefore limited to linguistic enhancement, ensuring that the works presentation is more precise and accessible to readers. A.2 LIMITATION Although the effect of the uncertainty calibration mechanism proposed in this work has been verified, it has not been extended to GUI planning tasks. We believe that the reliability of planning is even more critical for the overall success of GUI automation, since inaccurate or overconfident planning decisions can propagate errors across multiple steps and ultimately lead to task failure. In future work, we plan to investigate how uncertainty calibration can be incorporated into planning modules, enabling agents to not only ground actions reliably but also make trustworthy high-level decisions throughout complex multi-step interactions. A.3 PROMPT In this section, we detail the prompt for the replicated evaluation in ScreenSpot-Pro (Li et al., 2025). We follow the instructions they originally provided to reproduce and analyze the experimental results. The prompts are shown as follows: GPT-4os Prompt Locate the UI element most related to the instruction {problem} on the screenshot. Output only JSON in the format [{point 2d: [...]}]. Doubaos Prompt Locate the UI element most related to the instruction {problem} on the screenshot. Output only JSON in the format [{point 2d: [...]}]. Qwen2.5-VLs Prompt Locate the UI element most related to the instruction {problem} on the screenshot. Output only JSON in the format [{point 2d: [...], label: ... }]. KiMi-VLs Prompt Point to the UI element most related to the instruction {problem} on the screenshot. MiMo-VLs Prompt Locate the UI element most related to the instruction {problem} on the screenshot. Output JSON format [{bbox 2d: [...], label: ...}]./no think UI-TARS and UI-TARS-1.5s Prompt Point to the element related to the instruction {problem} on the screenshot."
        },
        {
            "title": "Preprint",
            "content": "Due to UI-TARS (Qin et al., 2025) and UI-TARS-1.5 (Qin et al., 2025) being trained with large amount of GUI-specific data, the ability to follow instructions is relatively poor. To prompt such models to generate verbalized confidence in their predictions, we adopt multi-round conversation to output confidence for their answer. Specifically, policy models use the above prompts for GUI grounding in the first round and in the second round, generate the verbalized confidence of the prediction according to the prompt below:"
        },
        {
            "title": "Confidence Prompt",
            "content": "Output only float number ranging from 0 to 1, representing your confidence with your provided answer, without any format. A.4 STABILITY OF CONFIDENCE To evaluate the reliability of HyperClicks confidence, we verified that the models confidence in the same answer remains stable. We first let HyperClick predict the coordinates without doing sample. Then, we inject the coordinates into the assistants generation and instruct it to continue outputting confidence at temperature of 1.0 for 8 times. As shown in Table 6, we report the mean variance for different sample sizes. The results indicate that both HyperClick-3B and HyperClick-7B maintain very low variance across different sampling scales, with the larger 7B model showing slightly more stable outputs. This suggests that the confidence estimation of HyperClick is well-calibrated, ensuring consistent reliability even under repeated sampling. Table 6: Stability evaluation of the model for the same prediction. Model Variance 10 50 100 500 1000 HyperClick-3B 0.020 HyperClick-7B 0.014 0.028 0.020 0.023 0.020 0.020 0.019 0.020 0.019 0.020 0. A.5 DATA DETAILS To provide comprehensive grounding resource across diverse platforms, we construct dataset containing 30K samples distributed across three representative domains: Mobile, Web, and Desktop. Each domain contains balanced set of grounding instances that pair natural language commands with corresponding UI elements. The number of samples collected from each dataset is shown below. Table 7: Statistics and sources of the grounding dataset adopted in HyperClick. Source OmniAct ShowUI-Web UI-Refexp Widgent-Caption OS-Atlas In-House Size 119 19172 280 3672 1664 To construct high-quality samples for RFT, we first employ Qwen2.5-VL-7B (Bai et al., 2025) to generate raw data with the temperature set to 0, and identify cases where the model produces incorrect predictions. For each of these error cases, we then perform eight additional inferences with temperature 0.9 and extract the correctly predicted results as the final training data. In addition, prior to RFT, we incorporate an equal number of correctly predicted samples from Stage 1 to provide cold start. This initialization not only stabilizes the training but also helps the model adhere to the target output format: <point>[x,y]</point><confidence>conf</confidence>. A.6 EVALUATION BENCHMARKS AND DETAILED EXPERIMENTAL RESULTS In this section, we detail the benchmarks adopted in this work."
        },
        {
            "title": "Preprint",
            "content": "ScreenSpot evaluates GUI grounding across mobile, desktop, and web platforms. Provides diverse set of interface types, enabling the comparison of model robustness across common user scenarios. Detailed experimental results and comparisons with baselines are shown in Table 8. Table 8: GUI grounding accuracy on the ScreenSpot (Cheng et al., 2024) benchmarks over the Mobile, Desktop, and Web sub-tasks. Bold and underline indicate the best and second-best results. Model Size Mobile Desktop Web ScreenSpot v1 Text (273) Icon (229) Text (194) Icon (140) Text (230) Icon (206) General Models GPT-4o (OpenAI, 2024) Claude (Anthropic, 2024) Qwen2-VL (Wang et al., 2024a) Qwen2.5-VL (Bai et al., 2025) InternVL3 (Zhu et al., 2025) GUI-specific Models (SFT) CogAgent (Hong et al., 2024) SeeClick (Cheng et al., 2024) Aria-UI (Yang et al., 2024a) ShowUI (Lin et al., 2025) UGround (Gou et al., 2025) UGround-V1 (Gou et al., 2025) OS-Atlas (Wu et al., 2024) Aguvis (Xu et al., 2024) UI-TARS (Qin et al., 2025) TongUI (Zhang et al., 2025a) GUI-Actor (Wu et al., 2025) GUI-specific Models (RFT) UI-R1 (Lu et al., 2025) UI-R1-E (Lu et al., 2025) GUI-R1 (Luo et al., 2025) InfiGUI-R1 (Liu et al., 2025b) GUI-G1 (Zhou et al., 2025) SE-GUI (Yuan et al., 2025) GUI-G2 (Tang et al., 2025a) Ours HyperClick - - 7B 3B 7B 8B 38B 18B 9.6B 25.3B 2B 7B 2B 7B 4B 7B 7B 72B 2B 7B 72B 3B 7B 2B 7B 3B 3B 3B 7B 3B 3B 7B 7B 3B 7B 30.5 - 61.3 - - - - 67.0 78.0 92.3 92.3 82.8 89.4 93.0 85.7 93.0 95.6 94.5 93.0 94.5 94.9 92.6 91.9 93.0 94.9 95.6 97.1 - - 97.1 98.6 - 96. 96.7 95.6 23.2 - 39.3 - - - - 24.0 52.0 73.8 75.5 60.3 72.0 79.9 58.5 72.9 77.7 85.2 75.5 85.2 82.5 77.7 79.5 79.9 82.1 84.7 83.0 - - 81.2 85.8 - 90.8 83.9 91.7 20.6 - 52.0 - - - - 74.2 72.2 93.3 76.3 82.5 88.7 93.8 72.2 91.8 93.8 95.4 90.7 95.9 89.7 92.3 93.8 88.1 91.8 90.2 95.4 93.8 91.8 94.3 96.4 - 95.9 92.8 93.8 19.4 - 45.0 - - - - 20.0 30.0 64.3 61.1 63.6 65.7 76.4 45.7 62.9 67.1 77.9 68.6 85.7 88.6 77.1 80.0 78.6 80.0 59.3 77.9 64.8 73.6 77.1 80.7 - 88. 80.7 82.9 11.1 - 33.0 - - - - 70.4 55.7 86.5 81.7 80.4 81.3 90.9 82.6 90.89 88.3 91.3 84.3 90.0 88.7 87.8 89.1 90.9 91.3 85.2 91.7 89.6 91.3 91.7 91.4 - 90.9 88.7 92.2 7.8 - 21.8 - - - - 28.6 32.5 76.2 63.6 70.4 68.9 84.0 63.1 74.3 75.2 85.9 74.8 83.5 85.0 74.8 81.6 84.0 85.4 73.3 85.0 72.1 75.7 77.6 82.3 - 86.9 83.5 88.4 SSv1 Avg. 18.8 83.0 42.9 55.5 84.7 79.5 85.6 47.4 53.4 82.4 75.1 73.3 77.7 86.3 70.1 82.5 84.4 89.2 82.3 89.5 88.4 83.6 86.0 86.5 88. 83.3 89.2 - - 87.5 90.3 88.2 92.0 88.5 91.5 ScreenSpot-V2 extends ScreenSpot with more challenging tasks and refined annotations. Additionally, it tests grounding accuracy in various real-world environments. Detailed experimental results and comparisons with baselines are shown in Table 9. ScreenSpot-Pro focuses on high-resolution professional settings with expert-annotated tasks. Covers 23 applications, five industries, and three operating systems, making it one of the most comprehensive"
        },
        {
            "title": "Preprint",
            "content": "Table 9: GUI grounding accuracy on the ScreenSpot (Cheng et al., 2024) and ScreenSpot-V2 benchmarks over the Mobile, Desktop, and Web sub-tasks. Bold and underline indicate the best and second-best results. ScreenSpot V2 Model Size Mobile Desktop Web General Models GPT-4o (OpenAI, 2024) Qwen2.5-VL (Bai et al., 2025) GUI-specific Models (SFT) SeeClick (Cheng et al., 2024) UGround (Gou et al., 2025) OS-Atlas (Wu et al., 2024) UI-TARS (Qin et al., 2025) TongUI (Zhang et al., 2025a) GUI-Actor (Wu et al., 2025) JEDI (Xie et al., 2025) GUI-specific Models (RFT) UI-R1 (Lu et al., 2025) UI-R1-E (Lu et al., 2025) SE-GUI (Yuan et al., 2025) LPO (Tang et al., 2025b) GUI-G2 (Tang et al., 2025a) Ours HyperClick Text (290) Icon (211) Text (194) Icon (140) Text (234) Icon (203) - 3B 7B 9.6B 7B 4B 7B 2B 7B 72B 3B 7B 2B 7B 3B 7B 3B 3B 7B 8B 7B 3B 7B 26.6 93.4 97.6 78.4 75.1 87.2 95.2 95.2 96.9 94.8 94.4 93.1 95.0 96.5 96.6 96. 96.2 98.2 - 97.9 98.3 98.6 98.3 24.2 73.5 87.2 50.7 84.5 59.7 75.8 79.1 89.1 86.3 79.6 81.5 82.2 84.3 81.5 87.2 84.3 83.9 - 82.9 91.9 86.3 93. 24.2 88.1 90.2 70.1 85.1 72.7 90.7 90.7 95.4 91.2 92.8 96.4 92.2 91.7 96.9 95.9 92.3 94.8 - 95.9 95.4 95.4 96.9 19.3 58.6 74.2 29.3 61.4 46.4 63.6 68.6 85.0 87.9 75.0 82.9 81.8 84.1 78.6 87. 63.6 75.0 - 86.4 89.3 90.6 85.7 12.8 88.0 93.2 55.2 84.6 85.9 90.6 87.2 93.6 91.5 87.6 90.2 92.9 93.9 88.5 94.4 89.2 83.7 - 95.6 94.0 82.2 96. 11.8 71.4 81.3 32.5 71.9 63.1 77.3 78.3 85.2 87.7 77.8 84.7 82.7 82.3 83.7 84.2 75.4 93.2 - 84.2 87.7 84.7 86.7 SSv2 Avg. 20.1 80.9 88. 55.1 76.3 71.9 84.1 84.7 91.6 90.3 85.5 88.7 88.6 89.5 88.6 91.7 85.4 89.5 90.3 90.5 93.3 90.6 93."
        },
        {
            "title": "Preprint",
            "content": "GUI grounding benchmarks. Detailed experimental results and comparisons with baselines are shown in Table 10. Table 10: GUI grounding accuracy on the ScreenSpot-Pro (Li et al., 2025) benchmark over the CAD, Development, Creative, Scientific, Office, and OS sub-tasks. Bold and underline indicate the best and second-best results. CAD Development Creative Scientific Office OS Text (197) Icon (64) Text (154) Icon (145) Text (198) Icon (143) Text (144) Icon (110) Text (177) Icon (53) Text (107) Icon (89) Model General Models GPT-4o (OpenAI, 2024) Claude (Anthropic, 2024) Qwen2.5-VL (Bai et al., 2025) GUI-specific Models (SFT) CogAgent (Hong et al., 2024) SeeClick (Cheng et al., 2024) ShowUI (Lin et al., 2025) Aria-UI (Yang et al., 2024a) UGround (Gou et al., 2025) UGround-V1 (Gou et al., 2025) OS-Atlas (Wu et al., 2024) UI-TARS (Qin et al., 2025) TongUI (Zhang et al., 2025a) GUI-Actor (Wu et al., 2025) JEDI (Xie et al., 2025) GUI-specific Models (RFT) UI-R1 (Lu et al., 2025) UI-R1-E (Lu et al., 2025) GUI-R1 (Luo et al., 2025) InfiGUI-R1 (Liu et al., 2025b) GUI-G1 (Zhou et al., 2025) SE-GUI (Yuan et al., 2025) GUI-G2 (Tang et al., 2025a) Ours HyperClick Size - - 3B 7B 18B 9.6B 2B 25.3B 7B 7B 4B 7B 2B 7B 72B 3B 7B 2B 7B 3B 7B 3B 3B 3B 7B 3B 3B 3B 7B 7B 3B 7B 2.0 14.5 9.1 16.8 7.1 2.5 2.5 7.6 14.2 15.8 2.0 12.2 17.8 20.8 18.8 11.7 17.3 - - 27.4 38. 11.2 37.1 26.4 23.9 33.0 39.6 38.1 51.3 55.8 43.7 51.3 0.0 3.7 7.3 1.6 3.1 0.0 0.0 1.6 1.6 1.2 0.0 4.7 4.7 9.4 12.5 1.6 9.4 - - 9.4 14.1 6.3 12.5 7.8 6.3 14.1 9.4 12.5 42.2 12.5 23.5 20. 1.3 22.0 22.1 46.8 14.9 0.6 16.9 16.2 26.6 51.9 7.1 33.1 47.4 58.4 62.9 32.5 40.9 - - 61.0 42.9 22.7 46.1 33.8 49.4 51.3 50.7 55.8 68.2 68.8 62.4 70.2 0.0 3.9 1.4 4.1 0.7 0.0 1.4 0.0 2.1 2.8 0.0 1.4 4.1 12.4 17.2 0.7 3.5 - - 13.8 11. 4.1 6.9 4.8 4.8 12.4 10.3 7.6 19.3 17.2 20.0 22.1 1.0 25.9 26.8 35.9 9.6 1.0 9.1 23.7 27.3 47.5 3.0 28.8 42.9 50.0 57.1 24.8 31.3 - - 53.5 50.0 27.3 41.9 40.9 38.9 44.9 36.6 47.0 57.6 57.1 50.5 57. 0.0 3.4 2.1 7.7 0.0 0.0 0.0 2.1 2.8 9.7 1.4 2.8 6.3 9.1 15.4 2.8 7.0 - - 8.4 11.9 3.5 4.2 5.6 8.4 7.0 11.9 4.9 9.1 15.4 12.6 20.3 2.1 33.9 38.2 49.3 22.2 3.5 13.2 27.1 31.9 57.6 9.0 37.5 56.9 63.9 64.6 43.1 50.7 - - 54.2 72. 42.4 56.9 61.8 55.6 58.3 61.8 61.8 75.0 77.1 55.6 76.4 0.0 15.8 7.3 7.3 1.8 0.0 7.3 6.4 2.7 14.5 5.5 7.3 17.3 31.8 20.9 12.7 12.7 - - 18.2 25.5 11.8 21.8 17.3 11.8 20.0 30.0 16.4 28.2 24.5 30.0 30. 1.1 30.1 33.9 52.5 13.0 1.1 15.3 20.3 31.6 60.5 5.1 33.9 50.3 63.3 63.3 32.8 45.8 - - 64.4 75.1 32.2 65.0 53.6 58.7 65.5 67.2 59.9 78.5 74.0 63.9 70.1 0.0 16.3 15.1 20.8 0.0 0.0 7.5 1.9 11.3 13.2 3.8 5.7 17.0 20.8 26.4 7.6 13.2 - - 32.1 47. 11.3 26.4 17.0 26.4 28.3 32.1 24.5 43.4 32.7 37.8 30.2 0.0 11.0 10.3 37.4 5.6 2.8 10.3 4.7 17.8 38.3 5.6 27.1 21.5 30.8 42.1 15.0 28.0 - - 38.3 33.6 13.1 32.7 28.1 42.1 43.9 23.5 40.2 49.5 57.9 41.1 56. Avg. 0.8 17.1 16.1 26.8 7.7 1.1 7.7 11.3 16.5 45.2 3.7 18.9 27.7 35.7 38.1 18.0 24.7 36.7 40.7 36.1 39.5 17.8 33.5 28.6 31.3 35.7 37.1 35.9 47.3 47.5 0.0 4.5 1.1 6.7 0.0 0.0 2.2 0.0 0.0 7.9 0.0 4.5 5.6 16.9 15.7 1.1 6.7 - - 9.0 16. 4.5 10.1 5.6 16.9 12.4 10.6 12.4 25.8 21.3 20.2 22.5 41.3 48.2 MMBench-GUI organizes tasks into hierarchical structure of basic and advanced instructions. This design enables the systematic evaluation of model performance across varying levels of instruction complexity. Detailed experimental results and comparisons with baselines are shown in Table 11. Table 11: GUI grounding accuracy on the MMBench-GUI (Wang et al., 2025) benchmark over the Windows, MacOS, Linux, iOS, Android, and Web sub-stasks. Bold and underline indicate the best and second-best results. Windows MacOS Linux iOS Android Web Model General Models GPT-4o (OpenAI, 2024) Claude (Anthropic, 2024) Qwen-Max-VL (Bai et al., 2023) Qwen2.5-VL (Bai et al., 2025) InternVL3 (Zhu et al., 2025) GUI-specific Models (SFT) ShowUI (Lin et al., 2025) OS-Atlas (Wu et al., 2024) Aguvis (Xu et al., 2024) UGround-V1 (Gou et al., 2025) UI-TARS (Qin et al., 2025) Ours HyperClick Size - - - 7B 72B 72B 2B 7B 7B 7B 72B 3B 7B Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. (308) (271) (345) (272) (346) (196) (314) (330) (191) (310) (355) (356) 1.5 1.5 43.9 31.4 55.7 70.1 9.2 36.9 37.3 66.8 78.6 73.8 82. 1.1 0.7 36.8 16.5 33.8 42.6 4.4 18.8 21.7 39.0 51.8 45.6 61.4 8.7 12.5 58.8 31.3 49.9 75.7 24.1 44.4 48.1 71.3 80.3 80.3 82. 4.3 7.5 56.1 22.0 30.1 52.3 10.4 21.7 33.3 48.6 62.7 52.9 67.1 1.1 1.1 53.9 21.5 40.3 59.2 25.1 31.4 33.5 56.5 68.6 66.5 66. 1.0 0.0 30.1 12.2 20.9 41.3 11.7 13.3 25.0 31.1 51.5 35.7 48.0 5.1 13.7 77.4 66.6 56.1 93.6 29.0 74.8 67.5 92.7 90.8 91.4 94. 3.3 10.6 59.1 55.2 28.2 80.6 19.7 48.8 65.2 70.9 81.2 72.7 82.1 2.5 1.4 79.5 35.1 55.6 92.7 17.4 69.6 61.0 93.5 93.0 92.4 95. 1.4 1.4 70.1 35.2 25.4 78.6 8.7 46.8 51.0 71.0 80.0 74.9 85.1 3.2 3.2 74.8 40.3 68.4 90.7 22.9 61.3 61.6 88.7 88.1 89.1 93. 2.9 2.3 58.8 32.5 45.8 65.9 12.7 35.4 45.5 64.6 68.5 60.1 85.1 19 Avg. 2.9 4.7 58.0 33.9 41.8 72. 16.0 41.4 45.7 65.7 74.3 71.4 79."
        },
        {
            "title": "Preprint",
            "content": "UI-I2E-Bench introduces implicit instructions that require both semantic understanding and spatial reasoning. Highlights the limitations of direct grounding and encourages models to adopt more sophisticated reasoning. Detailed experimental results and comparisons with baselines are shown in Table 12. Table 12: GUI grounding accuracy on the UI-I2E-Bench (Liu et al., 2025a) benchmark over the platforms of mobile, desktop, and web with various implicitness. Bold and underline indicate the best and second-best results. Model General Models Qwen2.5-VL (Bai et al., 2025) GUI-specific Models (SFT) ShowUI (Lin et al., 2025) SeeClick (Cheng et al., 2024) Aguvis (Xu et al., 2024) OmniParser (Wan et al., 2024) OmniParser (Yu et al., 2025) OS-Atlas (Wu et al., 2024) UGround-V1 (Gou et al., 2025) UI-TARS (Qin et al., 2025) UI-I2E-VLM (Liu et al., 2025a) GUI-specific Models (RFT) UI-R1 (Lu et al., 2025) Ours HyperClick Size 3B 7B 72B 2B 9.6B 7B - - 4B 7B 2B 7B 72B 2B 7B 72B 4B 7B 3B 3B 7B Platform Implicitness Mobile Desktop Web (253) (519) (705) Explicit (917) Implicit (560) 44.5 61.7 55. 53.9 37.2 60.3 67.6 69.4 58.6 68.1 59.9 73.5 78.2 66.7 65.7 75.5 61.4 76.2 38.7 41.6 47.2 30.4 15.8 47.6 45.5 42.4 19.9 48.9 49.5 65.7 74.6 54.0 58.0 69.8 38.9 64.0 39.9 56.9 49.0 29.6 18.2 45.1 30.8 40.7 54.6 52.2 66.4 70.8 74.7 62.2 56.5 77.1 60.9 62.1 51.4 58.4 49. 51.3 37.1 61.1 54.3 57.0 51.5 63.2 72.9 81.3 84.5 74.1 71.4 80.9 61.9 72.0 35.8 51.0 52.5 35.6 19.9 48.4 52.4 53.5 39.9 55.8 47.9 63.6 71.3 54.5 55.3 69.4 48.3 67.9 Avg. 41.7 53.8 51.4 41.5 26.4 53.2 53.1 54.8 44.3 58.6 57.4 70.3 76.3 62.0 61.4 73.7 53.4 69. 67.8 46.2 58.1 67.9 52.8 58. 77.9 80.4 59.0 67.5 81.0 84.2 81.1 84.8 66.1 71.4 71.8 76. CAGUI is Chinese benchmark for mobile GUI grounding. It emphasizes the grounding of textual elements and functional operations within Chinese-language applications. Detailed experimental results and comparisons with baselines are shown in Table 13. UI-Vision evaluates the generalization of cross-applications in diverse desktop environments. By incorporating previously unseen applications, it tests the models robustness and adaptability. Detailed experimental results and comparisons with baselines are shown in Table 14."
        },
        {
            "title": "Preprint",
            "content": "Table 13: GUI grounding accuracy on the CAGUI (Zhang et al., 2025c) benchmark over the Fun2Point, Text2Point, and Bbox2Text sub-tasks. Bold and underline indicate the best and second-best results. Model General Models GPT-4o (OpenAI, 2024) Qwen2.5-VL (Bai et al., 2025) InternVL2.5 (Chen et al., 2024) GUI-specific Models (SFT) OS-Genesis (Sun et al., 2024) OS-Altas (Wu et al., 2024) Aguvis (Xu et al., 2024) UI-TARS (Qin et al., 2025) GUI-specific Models (RFT) AgentCPM-GUI (Zhang et al., 2025c) Ours HyperClick Size - 7B 8B 26B 7B 7B 7B 7B 8B 3B 7B Fun2Point Text2Point (1500) (1500) Avg. 22.1 59.8 17.2 14. 8.3 53.6 60.8 56.8 79.1 80.9 82.7 19.9 59.3 24.2 16.6 5.8 60.7 76.5 66.7 21.0 59.6 20.7 15. 7.1 57.2 68.7 61.8 76.5 77.8 81.2 83.1 81.0 82.9 Table 14: GUI grounding accuracy on the UI-Vision (Nayak et al., 2025) benchmark over the Education (Ed.), Browsers (Br.), Development (De.), Productivity (Pr.), Creativity (Cr.), and Entertainment (En.) subtasks. Bold and underline indicate the best and second-best results. Model General Models GPT-4o (OpenAI, 2024) Gemini-1.5-pro (Team et al., 2024) Claude (Anthropic, 2024) Qwen2.5-VL (Wang et al., 2024a) InternVL2.5 (Chen et al., 2024) MiniCPM-V (Yao et al., 2024) GUI-specific Models (SFT) CogAgent (Hong et al., 2024) SeeClick (Cheng et al., 2024) AriaUI (Yang et al., 2024a) ShowUI (Lin et al., 2025) OS-Atlas (Wu et al., 2024) UGround-V1 (Nayak et al., 2025) Aguvis (Xu et al., 2024) UI-TARS (Qin et al., 2025) Ours HyperClick Size - - - 7B 8B 8B 9B 9.6B 25.3B 2B 7B 7B 72B 7B 7B 72B 3B 7B Setting Category Basic (1772) Functional (1772) Spatial (1935) Ed. (642) Br. (143) De. (1090) Pr. (1950) Cr. (1462) En. (192) 1.6 0.8 9.5 1.2 2.5 7. 12.0 9.4 12.2 8.1 12.2 15.4 27.9 17.8 20.1 31.4 28.7 35.3 1.5 0.3 7.7 0.8 2.8 5.3 12.2 4.7 14.0 7.7 11.2 17.1 26.7 18.3 24.3 30.5 24.4 32.1 1.0 0.6 7.6 0.5 1.0 1. 2.6 2.1 4.0 2.1 3.7 6.3 14.9 5.1 8.4 14.7 6.8 11.0 1.5 0.5 6.1 0.5 1.1 3.0 8.7 4.2 9.0 3.7 8.7 10.4 22.4 13.1 14.2 24.8 19.6 24.3 0.0 0.6 9.8 0.0 7.0 16. 11.2 13.3 18.9 13.3 16.8 28.7 35.7 30.8 35.0 40.5 30.8 47.6 2.2 0.9 8.0 1.2 3.0 5.4 8.6 7.3 11.2 7.5 10.3 17.5 27.6 17.1 19.7 27.9 20.6 26.5 1.1 0.5 9.4 0.9 1.8 3. 10.3 4.3 10.4 6.5 9.2 12.2 21.6 12.1 18.3 26.8 21.1 27.1 0.8 0.4 7.7 0.5 1.2 2.1 5.6 4.0 6.5 2.5 5.6 8.6 18.3 9.6 11.1 26.8 12.7 18.3 4.2 0.0 8.3 1.0 5.2 13. 15.6 11.0 19.3 15.6 16.2 18.2 38.0 24.0 38.5 17.8 40.6 50.0 Avg. 1.4 0.6 8.3 0.9 2.1 4.3 8.9 5.4 10.1 5.9 9.0 12.9 23.2 13.7 17.6 25.5 19.6 25."
        }
    ],
    "affiliations": [
        "Xiaomi Inc"
    ]
}