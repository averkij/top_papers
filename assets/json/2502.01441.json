{
    "paper_title": "Improved Training Technique for Latent Consistency Models",
    "authors": [
        "Quan Dao",
        "Khanh Doan",
        "Di Liu",
        "Trung Le",
        "Dimitris Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 4 4 1 0 . 2 0 5 2 : r Published as conference paper at ICLR IMPROVED TRAINING TECHNIQUE FOR LATENT CONSISTENCY MODELS Quan Dao Rutgers University quan.dao@rutgers.edu Khanh Doan VinAI Research dnkhanh.k63.bk@gmail.com Di Liu Rutgers University di.liu@rutgers.edu Trung Le Monash University trunglm@monash.edu Dimitris Metaxas Rutgers University dnm@cs.rutgers.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Consistency models are new family of generative models capable of producing high-quality samples in either single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/"
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, generative models have gained significant prominence, with models like ChatGPT excelling in language generation and Stable Diffusion (Rombach et al., 2021) advancing image and video generation. In computer vision, the diffusion model (Song et al., 2020; Song & Ermon, 2019; Ho et al., 2020; Sohl-Dickstein et al., 2015) has quickly popularized and dominated the Adversarial Generative Model (GAN) (Goodfellow et al., 2014). It is capable of generating high-quality diverse images that beat SoTA GAN models (Dhariwal & Nichol, 2021). Additionally, diffusion models are easier to train, as they avoid the common pitfalls of training instability and the need for meticulous hyperparameter tuning associated with GANs. The application of diffusion spans the entire computer vision field, including text-to-image generation (Rombach et al., 2021; Gu et al., 2022), image editing (Meng et al., 2021; Wu & la Torre, 2023; Huberman-Spiegelglas et al., 2024; He et al., 2024), text-to-3D generation (Poole et al., 2022; Wang et al., 2024), personalization (Ruiz et al., 2022; Van Le et al., 2023; Kumari et al., 2023) and control generation (Zhang et al., 2023b; Brooks et al., 2023). Despite their powerful capabilities, they require thousands of function evaluations for sampling, which is computationally expensive and hinders their application in the real world. Numerous efforts have been made to address this sampling challenge, either by proposing new training frameworks (Xiao et al., 2021; Rombach et al., 2021) or through distillation techniques (Meng et al., Equal contributions. Project Lead & Corresponding Author. 1 Published as conference paper at ICLR 2023; Yin et al., 2024; Sauer et al., 2023; Dao et al., 2024). However, methods like (Xiao et al., 2021) suffer from low recall due to the inherent challenges of GAN training, while (Rombach et al., 2021) still requires multi-step sampling. Distillation-based approaches, on the other hand, rely heavily on pretrained diffusion models and demand additional training. Recently, (Song et al., 2023) introduced new family of generative models called the consistency model. Compared to the diffusion model (Song & Ermon, 2019; Song et al., 2020; Ho et al., 2020), the consistency model could both generate high-quality samples in single step and multi-steps. The consistency model could be obtained by either consistency distillation (CD) or consistency training (CT). In previous work (Song et al., 2023), CD significantly outperforms CT. However, the CD requires additional training budget for using pretrained diffusion, and its generation quality is inherently limited by the pretrained diffusion. Subsequent research (Song & Dhariwal, 2023) improves the consistency training procedure, resulting in performance that not only surpasses consistency distillation but also approaches SoTA performance of diffusion models. Additionally, several works (Kim et al., 2023; Geng et al., 2024) have further enhanced the efficiency and performance of CT, achieving significant results. However, all of these efforts have focused exclusively on pixel space, where data is perfectly bounded. In contrast, most large-scale applications of diffusion models, such as text-to-image or video generation, operate in latent space (Rombach et al., 2021; Gu et al., 2022), as training on pixel space for large-scale datasets is impractical. Therefore, to scale consistency models for large datasets, the consistency must perform effectively in latent space. This work addresses the key question: How well can consistency models perform in latent space? To explore this, we first directly applied the SoTA pixel consistency training method, iCT (Song & Dhariwal, 2023), to latent space. The preliminary results were extremely poor, as illustrated in fig. 6, motivating deeper investigation into the underlying causes of this suboptimal performance. We aim to improve CT in latent space, narrowing the gap between the performance of latent consistency and diffusion. We first conducted statistical analysis of both latent and pixel spaces. Our analysis revealed that the latent space contains impulsive outliers, which, while accounting for very small proportion, exhibit extremely high values akin to salt-and-pepper noise. We also drew parallel between Deep Q-Networks (DQN) and the Consistency Model, as both employ temporal difference (TD) loss. This could lead to training instability compared to the Kullback-Leibler (KL) loss used in diffusion models. Even in bounded pixel space, the TD loss still contains impulsive outliers, which (Song & Dhariwal, 2023) addressed by proposing the use of Pseudo-Huber loss to reduce training instability. As shown in fig. 1, the latent input contains extremely high impulsive outliers, leading to very large TD values. Consequently, the Pseudo-Huber loss fails to sufficiently mitigate these outliers, resulting in poor performance as demonstrated in fig. 6. To overcome this challenge, we adopt Cauchy loss, which heavily penalizes extremely impulsive outliers. Additionally, we introduce diffusion loss at early timesteps along with optimal transport (OT) matching, both of which significantly enhance the models performance. Finally, we propose an adaptive scaling schedule to effectively control the robustness of the model, and we incorporate Non-scaling LayerNorm into the architecture. With these techniques, we significantly boost the performance of latent consistency model compared to the baseline iCT framework and bridge the gap between the latent diffusion and consistency training."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Consistency model (Song et al., 2023; Song & Dhariwal, 2023) proposes new type of generative model based on PF-ODE, which allows 1, 2 or multi-step sampling. The consistency model could be obtained by either training from scratch using an unbiased score estimator or distilling from pretrained diffusion model. Several works improve the training of the consistency model. ACT (Kong et al., 2023), CTM (Kim et al., 2023) propose to use additional GAN along with consistency objective. While these methods could improve the performance of consistency training, they require an additional discriminator, which could need to tune the hyperparameters carefully. MCM (Heek et al., 2024) introduces multistep consistency training, which is combination of TRACT (Berthelot et al., 2023) and CM (Song et al., 2023). MCM increases the sampling budget to 2-8 steps to tradeoff with efficient training and high-quality image generation. ECM (Geng et al., 2024) initializes the consistency model by pretrained diffusion model and fine-tuning it using the consistency training objective. ECM vastly achieves improved training times while maintaining good generation performance. However, ECM requires pretrained diffusion model, which must use the same architecture as the pretrained diffusion architecture. Although these works successfully improve the 2 Published as conference paper at ICLR 2025 performance and efficiency of consistency training, they only investigate consistency training on pixel space. As in the diffusion model, where most applications are now based on latent space, scaling the consistency training (Song et al., 2023; Song & Dhariwal, 2023) to text-to-image or higher resolution generation requires latent space training. Otherwise, with pretrained diffusion model, we could either finetune consistency training (Geng et al., 2024) or distill from diffusion model (Song et al., 2023; Luo et al., 2023). CM (Song et al., 2023) is the first work proposing consistency distillation (CD) on pixel space. LCM (Luo et al., 2023) later applies consistency technique on latent space and can generate high-quality images within few steps. However, LCMs generated images using 1-2 steps are still blurry (Luo et al., 2023). Recent works, such as Hyper-SD Ren et al. (2024) and TCD Zheng et al. (2024), have introduced notable improvements to latent consistency distillation. TCD Zheng et al. (2024) employed CTM Kim et al. (2023) instead of CD Song et al. (2023), significantly enhancing the performance of the distilled student model. Building on this, Hyper-SD Ren et al. (2024) divided the Probability Flow ODE (PF-ODE) into multiple components inspired by Multistep Consistency Models (MCM) Heek et al. (2024), and applied TCD Zheng et al. (2024) to each segment. Subsequently, Hyper-SD Ren et al. (2024) merged these segments progressively into final model, integrating human feedback learning and score distillation Yin et al. (2024) to optimize one-step generation performance."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Denote pdata(x0) as the data distribution, the forward diffusion process gradually adds Gaussian noise with monotonically increasing standard deviation σ(t) for {0, 1, . . . , } such that pt(xtx0) = (x0, σ2(t)I) and σ(t) is handcrafted such that σ(0) = σmin and σ(T ) = σmax. By setting σ(t) = t, the probability flow ODE (PF-ODE) from (Karras et al., 2022) is defined as: dxt dt = txt log pt(xt) = (xt (xt, t)) , (1) where : (xt, t) x0 is the denoising function which directly predicts clean data x0 from given perturbed data xt. (Song et al., 2023) defines consistency model based on PF-ODE in eq. (1), which builds bijective mapping between noisy distribution p(xt) and data distribution pdata(x0). The bijective mapping : (xt, t) x0 is termed the consistency function. consistency model fθ(xt, t) is trained to approximate this consistency function (xt, t). The previous works (Song et al., 2023; Song & Dhariwal, 2023; Karras et al., 2022) impose the boundary condition by parameterizing the consistency model as: fθ(xt, t) = cskip(t)xt + cout(t)Fθ(xt, t), (2) where Fθ(xt, t) is neural network to train. Note that, since σ(t) = t, we hereafter use and σ interchangeably. cskip(t) and cout(t) are time-dependent functions such that cskip(σmin) = 1 and cout(σmax) = 0. To train or distill consistency model, (Song et al., 2023; Song & Dhariwal, 2023; Karras et al., 2022) firstly discretize the PF-ODE using sequence of noise levels σmin = tmin = t1 < t2 < < tN = tmax = σmax, where ti = and ρ = 7. t1/ρ min + i1 1 (t1/ρ max t1/ρ min) (cid:17)ρ (cid:16) Consistency Distillation Given the pretrained diffusion model sϕ(xt, t) xt log pt(xt), the consistency model could be distilled from the pretrained diffusion model using the following CD loss: LCD(θ, θ) = (cid:2)λ(ti)d(fθ(xti+1, ti+1), fθ (xti, ti))(cid:3) , (3) where xti+1 = x0 + ti+1z with the x0 pdata(x0) and (0, I) and xti = xti+1 (ti ti+1)ti+1xti+1 Consistency Training The consistency model is trained by minimizing the following CT loss: log pti+1 (xti+1) = xti+1 (ti ti+1)ti+1sϕ(xti+1, ti+1). LCT(θ, θ) = (cid:2)λ(ti)d(fθ(xti+1, ti+1), fθ (xti, ti))(cid:3) , (4) where xti = x0 + tiz and xti+1 = x0 + ti+1z with the same x0 pdata(x0) and (0, I) In eq. (3) and eq. (4), fθ and fθ are referred to as the online network and the target network, respectively. The targets parameter θ is obtained by applying the Exponential Moving Average (EMA) to the students parameter θ during the training and distillation as follows: θ stopgrad(µθ + (1 µ)θ), (5) 3 Published as conference paper at ICLR with 0 µ < 1 as the EMA decay rate, weighting function λ(ti) for each timestep ti, and d(, ) is predefined metric function. In CM (Song et al., 2023), the consistency training still lags behind the consistency distillation and iCT (Song & Dhariwal, 2023) later propose several improvements that signifidiffusion models. cantly boost the training performance and efficiency. First, the EMA decay rate µ is set to 0 for better training convergence. Second, the Fourier scaling factor of noise embedding and the dropout rate are carefully examined. Third, iCT introduces Pseudo-Huber losses to replace L2 and LPIPS since LPIPS introduces the undesirable bias in generative modeling (Song & Dhariwal, 2023). Furthermore, the Pseudo-Huber is more robust to outliers since it imposes smaller penalty for larger errors than the L2 metric. Fourth, iCT proposes an exp curriculum for total discretization steps N, which doubles after predefined number of training iterations. Moreover, uniform weighting λ(ti) = 1 is replaced by λ(ti) = 1/(ti+1 ti). Finally, iCT adopts discrete Lognormal distribution for timestep sampling as EDM (Karras et al., 2022). With all these improvements, CT is now better than CD and performs on par with the diffusion models in pixel space."
        },
        {
            "title": "4 METHOD",
            "content": "In this paper, we first investigate the underlying reason behind the performance discrepancy between latent and pixel space using the same training framework in section 4.1. Based on the analysis, we find out the root of unsatisfied performance on latent space could be attributed to two factors: the impulsive outlier and the unstable temporal difference (TD) for computing consistency loss. To deal with impulsive outliers of TD on pixel space, (Song & Dhariwal, 2023) proposes the PseudoHuber function as training loss. For the latent space, the impulsive outlier is even more severe, making Pseudo-Huber loss not enough to resist the outlier. Therefore, section 4.2 introduces Cauchy loss, which is more effective with extreme outliers. In the next section 4.3 and section 4.4, we propose to use diffusion loss at early timesteps and OT matching for regularizing the overkill effect of consistency at the early step and training variance reduction, respectively. Section 4.5 designs an adaptive scheduler of scaling to control the robustness of the proposed loss function more carefully, leading to better performance. Finally, in section 4.6, we investigate the normalization layers of architecture and introduce Non-scaling LayerNorm to both capture feature statistic better and reduce the sensitivity to outliers. 4.1 ANALYSIS OF LATENT SPACE We first reimplement the iCT model (Song & Dhariwal, 2023) on the latent dataset CelebA-HQ 32 32 4 and pixel dataset Cifar-10 32 32 3. Hereafter, we refer to the latent iCT model as iLCT. We find that iCT framework works well on pixel datasets as claim (Song & Dhariwal, 2023). However, it produces worse results on latent datasets as in fig. 6 and table 1. The iLCT gets very high FID above 30 for both datasets, and the generative images are not usable in the real world. This observation raises concern about the sensitivity of CT algorithm with training data, and we should carefully examine the training dataset. In addition, we notice that the DQN and CM use the same TD loss, which update the current state using the future state. Furthermore, they also possess the training instability. This motivates to carefully examine the behavior of TD loss with different training data. While the pixel data lies within the range [1, 1] after being normalized, the range of latent data varies depending on the encoder model, which is blackbox and unbound. After normalizing latent data using mean and variance, we observe that the latent data contains high-magnitude values. We call them the impulsive outliers since they account for small probability but are usually very large values. In the bottom left of fig. 1, the impulsive outlier of latent data is red, spanning from 9 to 7, while the first and third quartiles are just around 1.4 and 1.4, respectively. We evaluate how the iCT will be affected by data outliers by analyzing the temporal difference TD = fθ(xti+1, ti+1) fθ (xti, ti). In the top right of fig. 1, the impulsive outliers of pixel TD range from -1.5 to 1.7, which are not too far from the interquartile range compared to latent TD. The impulsive outliers of latent TD range is much wider from -3.2 to 5. iCT uses Pseudo-Huber loss instead of L2 loss since the Huber is less sensitive to outliers, see fig. 2. However, for latent data, the Hubers reduction in sensitivity to outliers is not enough. This indicates that even using Pseudo-Huber loss, the iLCT training on latent space could still be unstable and lead to worse performance, which matches our 4 Published as conference paper at ICLR Figure 1: Box and Whisker Plot: Impulsive noise comparison between pixel and latent spaces. The right column shows the statistics of TD values at 21 discretization steps. Other discretization steps exhibit same behavior, where impulsive outliers are consistently present regardless of the total discretization steps. The blue boxes represent interquartile ranges of the data, while the green and orange dashed lines indicate inner and outer fences, respectively. Outliers are marked with red dots. experiment results on iLCT. Based on the above analysis, we hypothesize that the TD value statistic highly depends on the training data statistic. To mitigate the impact of impulsive outliers, we could use more stable target updates like Polyak or periodic in TD loss Lee & He (2019), but they lead to very slow convergence, as shown in (Song et al., 2023). Even though CM is initialized by pretrained diffusion model, the Polyak update still takes long time to converge. Therefore, using Polyak or periodic updates is computationally expensive, and we keep the standard target update as in (Song & Dhariwal, 2023). Another direction is using special metric for latent like LPIPS on pixel space (Song et al., 2023). (Kang et al., 2024) proposes the E-LatentLPIPS as metric for distillation and performs well on distillation tasks. However, this requires training network as metric and using this metric during the training process will also increase the training budget. To avoid the overhead of the training, we seek simple loss function like Pseudo-Huber but be more effective with outliers. We find that the Cauchy loss function (Black & Anandan, 1996; Barron, 2019) could be promising candidate in place of Pseudo-Huber for latent space. 4.2 CAUCHY LOSS AGAINST IMPULSIVE OUTLIER In this section, we introduce the Cauchy loss (Black & Anandan, 1996; Barron, 2019) function to deal with extreme impulsive outliers. The Cauchy loss function has the following form: dCauchy(x, y) = log 1 + (cid:18) y2 2 2c2 (cid:19) , (6) and we also consider two additional robust losses, which are Pseudo-Huber (Song & Dhariwal, 2023; Barron, 2019) and Geman-McClure (Geman & Geman, 1986; Barron, 2019) dPseudo-Huber(x, y) = (cid:113) y2 2 + c2 c, dGeman-McClure(x, y) = 2x y2 2 y2 2 + 4c2 , (7) (8) where is the scaling parameter to control how robust the loss is to the outlier. We analyze their robustness behavior against outliers. As shown in fig. 2a, the Pseudo-Huber loss linearly increases like L1 loss for the large residuals y. In contrast, the Cauchy loss only grows logarithmically, and the Geman-McClure suppresses the loss value to 1 for the outliers. The Pseudo-Huber loss works well if the residual value does not grow too high and, therefore, has good performance on the pixel space. However, for the latent space, as shown in the bottom right of fig. 1, the TD suffers from extremely high values coming from the impulsive outlier in the latent dataset, the Cauchy loss could be more suitable since it significantly dampens the influence of extreme outliers. Otherwise, even Geman-McClure is very highly effective for removing outlier effects than two previous losses; it gives gradient 0 for high TD value and completely ignores the impulsive outliers as fig. 2b. This is unexpected behavior because even though we call the highvalue latent impulsive outlier, they actually could encode important information from original data. 5 Published as conference paper at ICLR 2025 Completely ignoring them could significantly hurt the performance of training model. Based on this analysis, we choose Cauchy loss as the default loss for latent CM for the rest of the paper. The loss ablation is provided in table 2c. (a) Robust Loss (b) Derivative of Robust Loss Figure 2: Analysis of robust loss: Pseudo-Huber, Cauchy, and Geman-McClure 4.3 DIFFUSION LOSS AT SMALL TIMESTEP For small noise level σ, the ground truth of (xσ, σ) can be well approximated by x0, but this does not hold for large noise levels. Therefore, for low-level noise, the consistency objective seems to be overkill and harms the models performance since instead of optimizing fθ(xσ, σ) to approximated ground truth x0, the consistency objective optimizes through proxy estimator fθ (x<σ, < σ) leading to error accumulation over timestep. To regularize this overkill, we propose to apply an additional diffusion loss on small noise level as follows: Ldif = fθ(xti, ti) x02 2 int(N r), (9) where is the number of training discretization steps and [0; 1] is the diffusion threshold, and we heuristicly choose = 0.25. We do not apply diffusion loss for large noise levels since (xσ, σ) will differ greatly from the target x0, leading to very high L2 diffusion loss. This could harm the training consistency process, misleading to the wrong solution. We provide the ablation study in table 2b. Furthermore, CTM (Kim et al., 2023) also proposes to use diffusion loss, but they use them on both high and low-level noise, which is different from us. 4.4 OT MATCHING REDUCES THE VARIANCE In this section, we adopt the OT matching technique from previous works (Pooladian et al., 2023; Lee et al., 2023). (Pooladian et al., 2023) proposes to use OT to match noise and data in the training batch, such as the moving L2 cost is optimal. On the other hand, (Lee et al., 2023) introduces βVAE for creating noise corresponding to data and train flow matching on the defined data-noise pairs. By reassigning noise-data pairs, these works significantly reduce the variance during the diffusion/flow matching training process, leading to faster and more stable training process. According to (Zhang et al., 2023a), the consistency training and diffusion models produce highly similar images given the same noise input. Therefore, the final output solution of the consistency and diffusion models should be close to each other. Since OT matching helps reduce the variance during training diffusion, it could be useful to reduce the variance of consistency training. In our implementation, we follow (Pooladian et al., 2023; Tong et al., 2023) using the POT library to map from noise to data in the training batch fig. 3. The overhead caused by minibatch OT is relatively small, only around 0.93% training time, but gains significant performance improvement as shown in table 2a. 4.5 ADAPTIVE SCHEDULER In this section, we examine the choice of scaling parameter in robust loss functions. The scaling parameter controls the robustness level, which is very important for model performance. The 6 Published as conference paper at ICLR 2025 Figure 3: Each iteration, we use optimal transport to produce the optimal coupling. This helps reduce the variance during training, leading to better performance. Figure 4: Model convergence plot on different schedule. (Left) Our proposed values. Performance on FID (Middle) and Recall (Right) of our proposed in comparison with different choices. previous work (Song & Dhariwal, 2023) proposes to use fixed constant c0 = 0.00054 d, where is the dimension of data. We find that using this simple fixed is not yet optimal for the training consistency model. Especially in this paper, we follow the Exp curriculum specified by eq. (10) in (Song & Dhariwal, 2023), which doubles the total discretization step after defined number of training iterations. NFE(k) = min (cid:16) s02 , s1 (cid:17) + 1, = (cid:22) log2 s1/s0 + 1 (cid:23) , (10) where is current training iteration, is total training iteration and s0 = 10, s1 = 640. During training, we notice that the variance of TD is significantly reduced as doubling total discretization steps using eq. (10). Since the more discretization steps, the closer distance of xti and xti+1, the TD values range between them should be smaller. However, the impulsive outlier still exists regardless of the number of discretization steps. Intuitively, we propose heuristic adaptive scheduler where the is scaled down proportional to the reduction rate of TD variance as the number of discretization steps increases. We plot our scheduler versus discretization steps in fig. 4 and we fit the scheduler to get the scheduler equation as following: = exp(1.18 log(NFE(k) 1) 0.72) (11) 4.6 NON-SCALING LAYERNORM As mentioned in section 4.1, the statistic of training data could play an important role in the success of consistency training. Furthermore, in architecture design, the normalization layer specifically handles the statistics of input, output, and hidden features. In this section, we investigate the normalization layer choice for consistency training, which is sensitive to training data statistics. Currently, both (Song & Dhariwal, 2023; Song et al., 2023) use the UNet architecture from (Dhariwal & Nichol, 2021). In UNet (Dhariwal & Nichol, 2021), GroupNorm is used in every layer by default. The GroupNorm only captures the statistics over groups of local channels, while the LayerNorm further captures the statistics overall features. Therefore, LayerNorm is better at capturing Published as conference paper at ICLR 2025 Model NFE FID Recall Epochs Total Bs Pixel Diffusion Model WaveDiff (Phung et al., 2023) Score SDE (Song et al., 2020) DDGAN (Xiao et al., 2021) RDUOT (Dao et al., 2023b) RDM (Teng et al., 2023) UNCSN++ (Kim et al., 2021) 2 4000 2 2 270 2000 5.94 7.23 7.64 5.60 3.15 7.16 Latent Diffusion Model LFM-8 (Dao et al., 2023a) LDM-4 (Rombach et al., 2021) LSGM (Vahdat et al., 2021) DDMI (Park et al., 2024) DIMSUM (Phung et al., 2024) LDM-8 85 200 23 1000 73 250 5.82 5.11 7.22 7.25 3.76 8. Latent Consistency Model iLCT (Song & Dhariwal, 2023) iLCT (Song & Dhariwal, 2023) Ours Ours 1 2 1 2 37.15 16.84 7.27 6.93 0.37 - 0.36 0.38 0.55 - 0.41 0.49 - - 0.56 - 0.12 0.24 0.50 0.52 500 6.2K 800 600 4K - 500 600 1K - 395 1.4K 1.4K 1.4K 1.4K 1.4K 64 - 32 24 - - 112 48 - - 32 128 128 128 128 Model NFE FID Recall Epochs Total Bs Pixel Diffusion Model WaveDiff (Phung et al., 2023) Score SDE (Song et al., 2020) DDGAN (Xiao et al., 2021) 2 4000 2 5.94 7.23 5.25 Latent Diffusion Model LFM-8 (Dao et al., 2023a) LDM-8 (Rombach et al., 2021) LDM-8 90 400 250 7.70 4.02 10. Latent Consistency Model iLCT (Song & Dhariwal, 2023) iLCT (Song & Dhariwal, 2023) Ours Ours 1 2 1 2 52.45 24.67 8.87 7.71 0.37 - 0.36 0.39 0.52 - 0.11 0.17 0.47 0.48 500 6.2K 500 90 400 1.8K 1.8K 1.8K 1.8K 1.8K 64 - 32 112 96 256 256 256 256 (b) LSUN Church Model NFE FID Recall Epochs Total Bs Latent Diffusion Model LFM-8 (Dao et al., 2023a) LDM-4 (Rombach et al., 2021) LDM-8 84 200 250 8.07 4.98 10.23 Latent Consistency Model iLCT (Song & Dhariwal, 2023) iLCT (Song & Dhariwal, 2023) Ours Ours 1 2 1 48.82 21.15 8.72 8.29 0.40 0.50 - 0.15 0.19 0.42 0.43 700 400 1.4K 1.4K 1.4K 1.4K 1.4K 128 42 128 128 128 128 (a) CelebA-HQ (c) FFHQ Table 1: Our performance on CelebA-HQ, LSUN Church, FFHQ datasets at resolution 256 256. () means training on our machine with the same diffusion forward and equivalent architecture. fine-grained statistics over the entire feature. We further carry out the experiments for other types of normalization, such as LayerNorm, InstanceNorm, RMSNorm in table 2d and observe that the GroupNorm and InstanceNorm perform relatively well compared to others, especially LayerNorm. This could be due to that they are less sensitive to the outliers since they only capture the statistic over groups of channels. Therefore, the impulsive features only affect the normalization of group containing them. For the LayerNorm, the impulsive features could negatively impact the overall featuress normalization. We further look into the LayerNorm implementation and suspect that the scaling term could significantly amplify the outliers across features by serving as shared parameter. This observation is also mentioned in (Wei et al., 2022) for LLM quantization. In implementation, we set the scaling term of LayerNorm to 1 and disabled the gradient update for it 12. We refer to it as Non-scaling LayerNorm (NsLN) as (Wei et al., 2022). By using NsLN, our model achieves significant performance as shown in table 2d, validating our normalization choice. LNγ,β(x) = u(x) (cid:112)σ2(x) + ϵ γ + β, NsLNβ(x) = u(x) (cid:112)σ2(x) + ϵ + β, (12) where u(x) and σ2(x) are mean and variance of x."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 PERFORMANCE OF OUR TRAINING TECHNIQUE Experiment Setting: We measure the performance of our proposed technique on three datasets: CelebA-HQ (Huang et al., 2018), FFHQ (Karras et al., 2019), and LSUN Church (Yu et al., 2015), at the same resolution of 256256. Following LDM (Rombach et al., 2021), we use pretrained VAE to obtain latent data with the dimensionality of 32 32 4. We adopt the OpenAI UNet arKL-8 chitecture (Dhariwal & Nichol, 2021) as the default architecture throughout the paper. Furthermore, we use the variance exploding (VE) forward process for all the consistency and diffusion experiments following (Song et al., 2023; Song & Dhariwal, 2023). The baseline iCT is self-implemented based on official implementation CM (Song et al., 2023) and iCT (Song & Dhariwal, 2023). We refer to this baseline as iLCT. Furthermore, we also train the latent diffusion model for each dataset using the same VE forward noise process for fair comparisons with our technique. This LDM model https://huggingface.co/stabilityai/sd-vae-ft-ema Published as conference paper at ICLR 2025 is referred to as LDM-8 in table 1. All three frameworks, including ours, iLCT, and LDM-8, use the same architecture. Evaluation: During the evaluation, we first generate 50K latent samples and then pass them through VAEs decoder to obtain the pixel images. We use two well-known metrics, Frechet Inception Distance (FID) (Naeem et al., 2020) and Recall (Kynkaanniemi et al., 2019), for measuring the performance of the model given the training data and 50K generated images. Model Performance: We report the performance of our model across all three datasets in table 1, primarily to compare it with the baseline iLCT (Song & Dhariwal, 2023) and LDM (Rombach et al., 2021). For both 1 and 2 NFE sampling, we observe that the FIDs of iLCT for all datasets are notably high (over 30 for 1-NFE sampling and over 16 for 2-NFE sampling), consistent with the qualitative results shown in fig. 6, where the generated image is unrealistic and contain many artifacts. This poor performance of iLCT in latent space is expected, as the Pseudo-Huber training losses are insufficient in mitigating extreme impulsive outliers, as discussed in section 4.1 and section 4.2. In contrast, our proposed framework demonstrates significantly better FID and Recall than iLCT. Specifically, we achieve 1-NFE sampling FIDs of 7.27, 8.87, and 8.29 for CelebA-HQ, LSUN Church, and FFHQ, respectively. For 2-NFE sampling, our FID scores improve across all three datasets. Notably, our 1-NFE sampling outperforms LDM-8, using the same noise scheduler and architecture. However, our models still exhibit higher FIDs compared to LDM (Rombach et al., 2021) and LFM (Dao et al., 2023a). In contrast, we only need 1 or 2 timestep sampling, whereas they require multiple timesteps for high-fidelity generation. Its important to note that we employ the VE forward process, whereas these other methods use VP and flow-matching forward processes. Furthermore, the qualitative results of our framework, as shown in fig. 5, highlight our ability to generate high-quality images. Both quantitative and qualitative results confirm the superior performance of our method over iLCT, bridging the gap between latent consistency and diffusion models. (a) CelebA-HQ (b) LSUN Church (c) FFHQ Figure 5: Our qualitative results using 1-NFE at resolution 256 256 (a) CelebA-HQ (b) LSUN Church (c) FFHQ Figure 6: iLCT qualitative results using 1-NFE at resolution 256 9 Published as conference paper at ICLR"
        },
        {
            "title": "5.2 ABLATION OF PROPOSED FRAMEWORK",
            "content": "We ablate our proposed techniques on the CelebA-HQ 256 256 dataset, with all FID and Recall metrics measured using 1-NFE sampling. All models are trained for 1,400 epochs with the same hyperparameters. As shown in table 2a, replacing Pseudo-Huber losses with Cauchy losses makes our models training less sensitive to impulsive outliers, resulting in significant FID reduction from 37.15 to 13.02. This demonstrates the effectiveness of Cauchy losses in handling extremely highvalue outliers, as discussed in section 4.2. Additionally, applying diffusion loss at small timesteps further reduces FID by approximately 4 points to 9.11, as this loss term stabilizes the training process at small timesteps, as described in section 4.3. Introducing OT coupling during minibatch training reduces training variance, improving the FID to 8.89. Notably, by replacing the fixed scaling term = c0, (Song & Dhariwal, 2023) with an adaptive scaling schedule, our model achieves an additional FID reduction of more than 1 point, reaching 7.76, highlighting the importance of the scaling term in robustness control. Finally, we propose using Non-scale LayerNorm (NsLN), which removes the scaling term from LayerNorm to handle outliers more effectively. NsLN captures feature statistics while mitigating the negative impact of outliers, resulting in our best FID of 7.27. Robustness Loss To analyze the impact of different robust loss functions, we conduct an ablation study using our best settings but replace the Cauchy loss with alternatives such as L2, E-LatentLPIPS Kang et al. (2024), the Huber and the Geman-McClure loss. The results, shown in table 2c, indicate that both Huber and Geman-McClure underperform compared to the Cauchy loss when applied in the latent space. This is because the Huber loss remains too sensitive to extremely impulsive outliers, while the Geman-McClure loss tends to ignore such outliers entirely, leading to loss of important information. This behavior is also discussed in section 4.2. Framework FID Recall iLCT Cauchy + Diff + OT + Scaled + NsLN 37.15 13.02 9.11 8.89 7.76 7.27 0.12 0.36 0.41 0.42 0.47 0.50 (a) Components of proposed framework 1.0 0.6 0.25 FID Recall 7.47 7.33 7.27 0.49 0.49 0.50 (b) Threshold using Diffusion loss Loss FID Recall L2 E-LatentLPIPS Huber Geman McClure Cauchy 50.40 11.49 9.97 11.28 7.27 0.04 0. 0.44 0.44 0.50 (c) Robust losses. Norm layer FID Recall GN IN LN RMS NsLN 7.76 8.47 9.05 8.96 7.27 0.47 0.43 0.46 0.46 0.50 (d) Norm Layer Table 2: Ablation Studies on CelebA-HQ 256 256 dataset at epoch Diffusion Threshold In this section, we explore the impact of varying the threshold for applying the diffusion loss function in combination with the consistency loss. We observe that using the diffusion loss at every timestep improves consistency training; however, it underperforms compared to applying the diffusion loss selectively at smaller timesteps such as = 0.25 as shown in table 2b. This suggests that applying diffusion losses primarily at small noise levels improves performance as discussed section 4.3. At larger timesteps, the diffusion loss may conflict with the consistency loss, potentially guiding the model toward incorrect solutions, thereby reducing overall performance. Scaling term scheduler In this section, we compare the performance of our adaptive scaling scheduler with the fixed scaling scheduler proposed in (Song & Dhariwal, 2023). Our model demonstrates better convergence with the proposed adaptive scheduler. The rationale behind this improvement lies in the fact that, as the discretization steps increases using the exponential curriculum, the value of the TD scales down. Despite the reduced TD value, impulsive outliers still persist. 10 Published as conference paper at ICLR 2025 fixed large scaling is not effective in handling these outliers. To address this, we scale down as discretization steps increases, which leads to better performance, as shown in fig. 4. Normalizing Layer We denote GN, IN, LN, RMS, and NsLN as GroupNorm, InstanceNorm, LayerNorm, RMSNorm, and Non-scaling LayerNorm, respectively. The baseline UNet architecture from (Song et al., 2023; Dhariwal & Nichol, 2021) uses GroupNorm by default. We replace the normalization layers in the baseline with each of these types and train the model on CelebA-HQ using the best settings. The results are reported in table 2d. GN and IN only capture local statistics, making them more robust to outliers, as outliers in one region do not affect others. In contrast, LN captures statistics from all features, making it more vulnerable to outliers because an outlier affects all features through shared scaling term. By removing the scaling term in LN, we obtain NsLN, which is both effective in capturing feature statistics and resistant to outliers. As shown in table 2d, NsLN outperforms the second-best GN by 0.5 FID and significantly outperforms LN, indicating that removing the scaling term reduces sensitivity to outliers and improves overall performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "CT is highly sensitive to the statistical properties of the training data. In particular, when the data contains impulsive noise, such as latent data, CT becomes unstable, leading to poor performance. In this work, we propose using the Cauchy loss, which is more robust to outliers, along with several improved training strategies to enhance model performance. As result, we can generate high-fidelity images from latent CT, effectively bridging the gap between latent diffusion models and consistency models. Future work could explore further improvements to the architecture, specifically by investigating normalization methods that reduce the impact of outliers. For example, removing the scaling term from group normalization or instance normalization may help mitigate outlier effects. Another promising future direction is the integration of this technique with Consistency Trajectory Models (CTM) Kim et al. (2023), as CTM has demonstrated improved performance compared to traditional Consistency Models (CM) Song et al. (2023)."
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan Barron. general and adaptive robust loss function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 43314339, 2019. David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. Michael Black and Paul Anandan. The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer vision and image understanding, 63(1):75104, 1996. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint arXiv:2307.08698, 2023a. Quan Dao, Binh Ta, Tung Pham, and Anh Tran. Robust diffusion gan using semi-unbalanced optimal transport. arXiv preprint arXiv:2311.17101, 2023b. Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, and Anh Tran. Self-corrected flow distillation for consistent one-step and few-step text-to-image generation. arXiv preprint arXiv:2412.16906, 2024. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Donald Geman and Stuart Geman. Bayesian image analysis. In Disordered systems and biological organization, pp. 301319. Springer, 1986. Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Published as conference paper at ICLR 2025 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. Xiaoxiao He, Ligong Han, Quan Dao, Song Wen, Minhao Bai, Di Liu, Han Zhang, Martin Renqiang Min, Felix Juefei-Xu, Chaowei Tan, et al. Dice: Discrete inversion enabling controllable editing for multinomial diffusion and masked generative models. arXiv preprint arXiv:2410.08207, 2024. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Huaibo Huang, zhihang li, Ran He, Zhenan Sun, InIntrovae: and Tieniu Tan. trospective variational autoencoders for photographic image synthesis. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2018/ 2018. file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf. Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1246912478, 2024. Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling Diffusion Models into Conditional GANs. In European Conference on Computer Vision (ECCV), 2024. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In Proc. NeurIPS, 2022. Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: universal training technique of score-based diffusion model for high precision score estimation. arXiv preprint arXiv:2106.05527, 2021. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, arXiv preprint Xiaoshuang Shi, and Kaidi Xu. Act: Adversarial consistency models. arXiv:2311.14097, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19311941, 2023. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. Donghwan Lee and Niao He. Target-based temporal-difference learning. In International Conference on Machine Learning, pp. 37133722. PMLR, 2019. 12 Published as conference paper at ICLR 2025 Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based generative models. arXiv preprint arXiv:2301.12003, 2023. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and In Proceedings of the IEEE/CVF Tim Salimans. On distillation of guided diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 1429714306, 2023. Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. ArXiv, abs/2002.09797, 2020. URL https: //api.semanticscholar.org/CorpusID:211259260. Dogyun Park, Sihyeon Kim, Sojin Lee, and Hyunwoo Kim. Ddmi: Domain-agnostic latent diffusion models for synthesizing high-quality implicit neural representations. arXiv preprint arXiv:2401.12517, 2024. Hao Phung, Quan Dao, and Anh Tran. Wavelet diffusion models are fast and scalable image generators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1019910208, June 2023. Hao Phung, Quan Dao, Trung Dao, Hoang Phan, Dimitris Metaxas, and Anh Tran. Dimsum: Diffusion mambaa scalable and unified spatial-frequency method for image generation. arXiv preprint arXiv:2411.04168, 2024. Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Published as conference paper at ICLR 2025 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in neural information processing systems, 34:1128711302, 2021. Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, and Anh Tran. Antidreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 21162127, 2023. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:1740217414, 2022. Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zeroshot image editing and guidance. In ICCV, 2023. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of largescale image dataset using deep learning with humans in the loop. ArXiv, abs/1506.03365, 2015. URL https://api.semanticscholar.org/CorpusID:8317437. Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, and Qing Qu. The In Forty-first International emergence of reproducibility and consistency in diffusion models. Conference on Machine Learning, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023b. Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 14 Published as conference paper at ICLR"
        },
        {
            "title": "A APPENDIX",
            "content": "We provide additional uncurated samples of our models for three datasets: CelebaA-HQ (7, 8), LSUN Church (9, 10), and FFHQ (11, 12). We also provide additional uncurated samples of our models on CelebaA-HQ trained with L2 loss (13) and E-LatentLPIPS loss (14). Figure 7: One-step samples on CelebA-HQ 256 256 Figure 8: Two-step samples on CelebA-HQ 256 256 15 Published as conference paper at ICLR 2025 Figure 9: One-step samples on LSUN Church 256 Figure 10: Two-step samples on LSUN Church 256 256 16 Published as conference paper at ICLR 2025 Figure 11: One-step samples on FFHQ 256 256 Figure 12: Two-step samples on FFHQ 256 256 Published as conference paper at ICLR 2025 Figure 13: One-step samples on CelebA-HQ 256 256 (L2 loss) Figure 14: One-step samples on CelebA-HQ 256 256 (E-LatentLPIPS loss)"
        }
    ],
    "affiliations": [
        "Monash University",
        "Rutgers University",
        "VinAI Research"
    ]
}