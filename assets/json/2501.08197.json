{
    "paper_title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training",
    "authors": [
        "Yijiong Yu",
        "Ziyun Dai",
        "Zekun Wang",
        "Wei Wang",
        "Ran Chen",
        "Ji Pei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 9 1 8 0 . 1 0 5 2 : r OPENCSG CHINESE CORPUS: SERIES OF HIGHQUALITY CHINESE DATASETS FOR LLM TRAINING Yijiong Yua,b, Ziyun Daib, Zekun Wangb, Wei Wangb, Ran Chenb, and Ji Peib aTsinghua University bOpenCSG"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopediachinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs. The datasets are publicly available on huggingface."
        },
        {
            "title": "INTRODUCTION",
            "content": "LLM training typically requires vast amounts of data, yet obtaining sufficiently large and diverse corpora for Chinese poses multiple hurdles. For example, annotation can be prohibitively expensive and time-consuming, especially when domain experts are required to label specialized content (Wang et al., 2022a). Robust infrastructure is also needed to store and process massive corpora, In addition, licensing which demands substantial computational resources (Brown et al., 2020). and copyright concerns complicate the open distribution of textual data (Scao et al., 2022), particularly for non-English material such as Chinese. While prominent Chinese LLMsincluding Qwen2 (Yang et al., 2024), GLM (Du et al., 2022), and Deepseek (DeepSeek-AI et al., 2024)have demonstrated strong performance, most rely on proprietary data, limiting community access and slowing advances in Chinese NLP. Drawing inspiration from SmolLM and SmolLM2 (Allal et al., 2024), which utilize partially automated pipelines and crowd-sourced content to reduce collection costs, we seek to adopt similarly efficient strategies for assembling large-scale Chinese datasets. At the same time, the FineWeb framework (Penedo et al., 2024b) offers proven filtering methods that transform raw corpora into clean, context-rich training data. By merging these ideascost-effectiveness from SmolLM/SmolLM2 and high-quality filtering from FineWebour approach balances practical scalability with rigorous curation to produce openly accessible resources for Chinese LLMs. Based on these insights, we introduce four datasets, each addressing different aspects of data quality and utility: Fineweb-edu-chinese and Fineweb-edu-chinese-v2 provide large corpora. Both sets rely on Qwen2-based (Yang et al., 2024) scorer to emphasize educational value, ensuring 1 valuable and coherent text. The v1 variant contains about 90 million samples with about 200 billion tokens. The v2 variant doubles the dataset size to over 180 million samples with 420 billion tokens, employing tighter filtering to improve clarity and completeness. Cosmopedia-chinese is fully synthetic dataset that mainly comprises textbook-like chapters generated by glm4-longwriter. Smoltalk-chinese captures multi-turn conversations created through system prompts and advanced Chinese LLMs (e.g., Deepseek-V2.5, Qwen2.5-72b-Instruct). This pipeline forgoes manual data gathering of dialogues but incorporates automated scoring, classification, and de-duplication to mitigate potential style biases or repetitive content. Our method integrates automated scoring modules, synthetic text generation, and domain-focused curation, these Chinese datasets stand apart for their scalability, diversity, and openness. By refining techniques introduced by SmolLM, SmolLM2, and FineWeb, we strive to address recurring obstacles in data collectionsuch as cost, infrastructure demands, and licensing restrictionswhile still ensuring the rich quality and broad coverage essential for training robust Chinese LLMs."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Pretraining large language models (LLMs) requires vast amounts of text data, yet raw datasets such as CommonCrawl (Team, 2024a) are often noisy and unstructured, making direct training inefficient and less effective. To address this, refined corpora such as C4 (Dodge et al., 2021), RedPajama (Computer, 2023), SlimPajama (Soboleva et al., 2023), and DCLM-baseline (Li et al., 2024) have been developed. These datasets leverage strict filtering techniques, including heuristic rule-based filtering, trained scorer models, and deduplication methods like MinHash, transforming raw corpora into clean, structured datasets suitable for pretraining. As data processing techniques have advanced, datasets like RefinedWeb (Penedo et al., 2023), FineWeb (Penedo et al., 2024a), and FineWeb-2 (Penedo et al., 2024b) have set new standards for data quality, improving both the efficiency and effectiveness of pretraining. While these advances have significantly improved English datasets, high-quality Chinese corpora remain limited in both scale and refinement. Existing Chinese datasets, such as Wudao-200GB (BAAI, 2023), CCI data (Wang et al., 2024a), and Skypile-150B (Wei et al., 2023), are relatively coarse, relying on rudimentary processing methods that do not ensure high-quality filtering. This disparity highlights the need for more robust and sophisticated approaches to Chinese dataset curation. Synthetic data has emerged as powerful tool for enriching training corpora. Initially demonstrated in instruction tuning through methods like self-instruct (Wang et al., 2022b) and evol-instruct (Luo et al., 2023), synthetic data generation uses powerful LLMs, such as ChatGPT, to create instructions and responses from seed examples, significantly increasing the size of fine-tuning datasets. Magpie (Xu et al., 2024) further streamlined this process by enabling LLMs to directly generate data from system prompts, eliminating the need for seed data and simplifying dataset creation. Beyond fine-tuning, synthetic data has proven effective in pretraining, as evidenced by the Phi series models (Gunasekar et al., 2023), which demonstrated its potential to create comprehensive pretraining datasets, though their data remains proprietary. To address this limitation, Cosmopedia (Ben Allal et al., 2024), the largest open-source synthetic pretraining dataset to date, offers 25 billion tokens of synthetic textbooks, blog posts, stories, and instructional articles generated by Mixtral-8x7BInstruct-v0.1 (Jiang et al., 2023). This highlights the scalability and quality achievable with advanced synthetic data generation techniques. While synthetic data holds great promise, its application to Chinese datasets is still underexplored, largely due to the computational resources required for generation and the challenge of ensuring the datas utility. Existing Chinese fine-tuning datasets also face limitations. For example, llama2chinese (FlagAlpha, 2023) provides only 10k samples, and Firefly-Train-1.1M (JianXin, 2024) covers 23 tasks but is restricted to single-turn conversations with short text lengths. ShareGPT-ChineseEnglish-90k (shareAI, 2023) extends to multi-turn conversations but lacks sufficient diversity and quality control. Infinity-Instruction (of Artificial Intelligence , BAAI) iteratively builds datasets using instruction selection and evolution, yet only 10% of its millions of samples are in Chinese, limiting its suitability for bilingual or exclusively Chinese training. 2 The progression from raw corpora to refined datasets like FineWeb underscores the importance of quality control in pretraining data, while advances in synthetic data generation, such as Cosmopedia and Magpie, demonstrate the potential for efficient dataset expansion. However, Chinese datasets still lag behind their English counterparts in scale, quality, and sophistication. Bridging this gap requires combining robust filtering techniques with advanced synthetic data generation methods tailored to the specific challenges of Chinese text. Our work builds on these advancements, integrating the strengths of existing approaches while addressing their limitations, to provide scalable, diverse, and high-quality resources for Chinese LLM research."
        },
        {
            "title": "3 DATASET CONSTRUCTION PROCESS",
            "content": "Figure 1: The diagram illustrates the construction pipelines for three Chinese datasets: FineWebedu-Chinese, COSMOPEDIA-Chinese, and Smoltalk-Chinese. The FineWeb-edu-Chinese pipeline begins with various Chinese corpora, followed by random sampling, data pooling, annotation, and scoring. fine-tuned BERT-based model is trained to filter and generate the final dataset. The COSMOPEDIA-Chinese pipeline starts with seed data collection, proceeds through prompt design and data generation, and results in database of curated knowledge. Lastly, the Smoltalk-Chinese pipeline leverages powerful Chinese LLMs with task-specific system prompts to generate conversational data. This chapter introduces the construction process of three datasets designed to meet the demand for high-quality and domain-relevant Chinese data. Inspired by previous works such as Fineweb-edu (Penedo et al., 2024a) and Cosmopedia-v2 (Ben Allal et al., 2024), we tailored and expanded upon their methodologies to address the challenges of limited Chinese data availability and to ensure the datasets meet diverse task requirements. 3.1 FINEWEB-EDU-CHINESE DATASET The Fineweb-Edu-Chinese dataset construction pipeline largely follows the methodology of Fineweb-edu. However, unlike Fineweb-edu, which is filtered from the 15-terabyte Fineweb corpus, large-scale Chinese data is relatively scarce. To address this, we assembled multiple opensource Chinese corpora, including Wudao (BAAI, 2023), Telechat (Wang et al., 2024b), Map-CC (Du et al., 2024), CCI2 (Wang et al., 2024a), and Skypile (Wei et al., 2023), to form the original data pool. These datasets were selected for their diversity and their educational and technical relevance. 3 From the CCI2 dataset, we randomly sampled one million entries and used Qwen2-7b-instruct (Yang et al., 2024) to evaluate the educational value of each sample on scale from 0 to 5, using the prompt shown in Appendix A.1. This scoring data was then used to fine-tune bge-rerank-zh model (Zhang et al., 2023), enhanced with linear regression layer. The model was trained for three epochs using cosine decay learning rate of 1e-4, balancing computational efficiency and model performance. Using this scorer, the dataset was filtered to exclude samples with scores below 3, retaining highquality entries. To remove redundancy, we applied Min-Hash with an overlap threshold of 0.7. This technique ensured computational efficiency while maintaining diversity in the dataset. The resulting FinewebEdu-Chinese dataset consists of 89 million high-quality samples, offering rich resource for educational and technical applications. 3.2 FINEWEB-EDU-CHINESE-V2 DATASET In the V2 version of Fineweb-edu-chinese, we expand the data pool, adding Michao (Liu et al., 2023b), CCI3 (Wang et al., 2024a), IndustryCorpus2 (Shi et al., 2024) and ChineseWebText (Chen et al., 2023). And in the first step, we replace Qwen2-7b-instruct with Qwen2.5-14b-instruct (Team, 2024b), and one million samples is sample from the entire pool. The other steps are almost the same. 3.3 COSMOPEDIA-CHINESE Inspired by Cosmopedia-v2 (Ben Allal et al., 2024), we extended the dataset construction process by generating synthetic textbook-like data from seed samples. Seed data plays critical role in guiding the generation of high-quality samples, which is expected to contain valuable knowledge. While web texts from the Chinese data pool were initially considered, their quality was not high enough, as they often consisted of advertisements or lacked valuable knowledge. To overcome this, we collected seed samples from high-quality sources, including 5.6 million entries from BaiduBaike, 1 million samples from Zhihu Q&A, and 2 million entries from technical blogs. These sources were selected for their rich domain knowledge and high informational density. Since the experiments of Cosmopedia-v2 have found that using larger models, such as 70B level LLMs, to generate data did not provide no significant improvements, we decided to just choose 7B level models to accelerate the synthetic process. However, after trying qwen2-7b-instruct Yang et al. (2024) or yi-1.5-9b-chat AI et al. (2024), we find usual chat models always tend to output concise and generalizable content like an abstract or outline, rather than sufficiently detailed and specific content like the main content of the textbook, even though we have prompted the model to answer as detailed as possible. Thus, we resort to glm4-9b-longwriter Bai et al. (2024), which can generate long answers with sufficiently detailed content. Using glm4-9b-longwriter (Bai et al., 2024), we generated synthetic samples in various genres, such as textbook units, narrative stories, and detailed how-to guides. The prompts we used for generating each genre are in Appendix A.2. temperature setting of 0.8 ensured diverse outputs. From 20 million generated samples, deduplication via Min-Hash retained 15 million high-quality entries, preserving diversity without compromising data quality. 3.4 SMOLTALK-CHINESE DATASET The Smoltalk-Chinese dataset builds on the methodologies of Magpie-ultra-1M (Xu et al., 2024) and Smoltalk (Allal et al., 2024), addressing gaps in task diversity and conversational depth. Since Smoltalk includes some extra tasks besides Magpie-ultra-1M which focusing on 11 task categories, we also introduces 7 additional task categories in Smoltalk-Chinese: format constraints, summarization, rewriting, document-QA, safe-QA, translation, and everyday-talk. This expansion ensures broader coverage of tasks relevant to natural language understanding and generation. Using advanced LLMs such as Deepseek-V2.5 (DeepSeek-AI et al., 2024) and Qwen2.5-72BInstruct (Team, 2024b), we generate 3-turn conversations for the 11 task categories used in Magpieultra-1M, 1-turn conversations for the new categories except everyday-talk, and 5-turn conversations for everyday-talk. The system prompt and detailed description of each task category is shown Figure 2: Score distribution of all the unfiltered source data scored by the Fineweb-Edu-Chinesev2 scorer. High-quality samples (score > 3) form only small fraction, indicating the scarcity of valuable data in open-source Chinese corpora. in Appendix A.3. To ensure diversity, which is highly important for instruction-tuning, we set the temperature to 1.2. To enhance quality, Qwen2.5-7b-instruct was used to score the first user query of each generated conversation, based on its clarity and coherence, and we only keep those with the score over 3. The prompts used to instruct Qwen2.5-7b-instruct is shown in Appendix A.3. Deduplication was performed using embeddings encoded by gte-zh-large (Zhang et al., 2024) and filtered based on cosine similarity thresholds of 0.8 for multi-turn samples and 0.7 for single-turn samples. After filtering, the Smoltalk-Chinese dataset contains about 70,000 high-quality samples, offering robust resource for tasks requiring diverse conversational and task-oriented data."
        },
        {
            "title": "4 EXPERIMENTS AND ANALYSIS",
            "content": "This chapter presents the experiments conducted to validate the effectiveness of our three proposed datasetsFineweb-Edu-Chinese, Cosmopedia-Chinese, and Smoltalk-Chinesein pretraining and fine-tuning language models (LMs). We focus on systematically assessing each datasets contribution to model performance, comparing to other open-source datasets, and drawing insights into potential areas of improvement. 4.1 FINEWEB-EDU-CHINESE (a) Text length (b) Source Figure 3: (a) The text length distribution of Fineweb-Edu-Chinese shows most samples lengths are in the interval 0.2k-1k and 1k-2k. (b) The source where the samples are from in Fineweb-EduChinese. We show the score distribution of the the unfiltered data scored by the Fineweb-Edu-Chinese-v2 scorer in Figure 2. It is clear that most samples from the original Chinese corpora were scored 5 (a) Text length (b) Source Figure 4: (a) The text length distribution of Fineweb-Edu-Chinese-v2 shows most samples lengths are iin the interval 0.2k-1k. (b) The source where the samples are from in Fineweb-Edu-Chinese-v2. in the 1-2 range, confirming that truly high-quality data is comparatively rare. Despite this low proportion of top-tier samples, careful filtering and rigorous scoring resulted in the Fineweb-EduChinese dataset, which is expected to contain more meaningful and instructional text than typical open-source alternatives. The statistical information on text length and data sources of Fineweb-Edu-Chinese v1 and v2 is shown in Figure 3 and Figure 4. To prove the datasets advantage, we pretrained 2B-level language model based on the Llama architecture from scratch using Fineweb-Edu-Chinese v1. baseline dataset, randomly sampled from the raw source of Fineweb-Edu-Chinese and matched in size, served as our control condition. Both models were trained for over 50k steps using sequence length of 2048, global batch size of 512, and constant learning rate of 1e3. We chose large sequence length (2048) to exploit LLMs capacity for capturing extended dependencies, and relatively high learning rate to accelerate earlystage convergence under limited compute resources. (a) Evaluation on CMMLU (b) Evaluation on CEval Figure 5: The score in CMMLU and CEval of each checkpoint when training with different datasets. We assessed model performance on the C-Eval (Huang et al., 2023) and CMMLU (Li et al., 2023) benchmarks, two authoritative suites for evaluating Chinese NLP models world-knowledge and understanding abilities. During evaluation, we set the temperature to 0 (for deterministic outputs) and used 5-shot prompting strategy to provide limited contextual examples. As illustrated in Figure 5, the Fineweb-Edu-Chinese model experienced sharp accuracy increase around 45k steps, surpassing the baseline by significant margin. This sudden rise likely reflects the models acquisition of domain-specific patterns present in the filtered dataset, confirming that Fineweb-Edu-Chineses focus on higher-scoring educational content enhances pretraining efficiency and downstream performance. Due to the limitation of computational resource, we have not yet run pretraining experiments on Fineweb-Edu-Chinese v2. But due to its larger scale and refined construction pipeline, we are sure it will provide much better effect. 4.2 COSMOPEDIA-CHINESE Cosmopedia-Chinese, in contrast, is synthesized in its entirety from high-quality seed data sources, including BaiduBaike entries, Zhihu Q&A discussions, and technical blog articles. And each piece of seed data can be used to generate multiple diffirent genres. As illustrated in Figure 6, this approach yields seemingly balanced mix of content genres, suggesting coverage of diverse topics such as science, literature, and practical know-how. And the average text length is relatively longer than normal datasets, with over half samples are longer than 2k. To rigorously test its effectiveness, we pretrained 2B-level Llama-based model on Cosmopedia-Chinese with the same hyper-parameters used for Fineweb-Edu-Chinese, ensuring fair comparison. Despite the datasets breadth of topics, benchmark results indicated only little accuracy gains over the baseline after 2 epochs of training. We attribute this lack of improvement primarily to two factors. First, the homogeneity of synthesized dataeven though the topics were varied, the limited genres may have led to repetitive rhetorical structures or stylistic patterns that reduced the overall diversity LLMs rely on to achieve robust generalization. Second, the overuse of markdown formatting throughout many samples could divert the models attention toward parsing formatting tokens rather than absorbing substantive content, potentially diminishing its ability to capture key semantic and contextual signals. (a) Genre (b) Seed Data (c) Text Length Figure 6: Statistic information about Cosmopedia-Chinese. Nonetheless, human evaluators noted that the Cosmopedia-Chinese model produced consistently well-structured and knowledge-rich responses, indicating it effectively inherited some valuable patterns from the seed data. This outcome highlights the datasets potential utility in use cases where coherent, polished text is prioritizedsuch as educational materials, guided tutorials, or domainspecific reference resources. To further improve generalization, we recommend blending real-world data with the synthesized corpus and reducing or removing markdown tags during preprocessing, allowing the model to concentrate more fully on semantic richness and contextual variety. 4.3 SMOLTALK-CHINESE Smoltalk-Chinese is specifically designed for instruction fine-tuning and contains diverse tasks and conversation lengths (Figure 7). To assess its effectiveness, we took the 2B model pretrained on Fineweb-Edu-Chinese as our backbone, then fine-tuned it on Smoltalk-Chinese, InfinityInstruct (of Artificial Intelligence , BAAI) (using 100k Chinese-only samples), and Magpie-Qwen2Pro-200K-Chinese (Xu et al., 2024). We conducted training over 2 epochs, with starting learning rate of 3e4 and cosine decay. We then evaluated each fine-tuned model on Alignbench (Liu et al., 2023a), benchmark focused on multi-dimensional alignment criteria for Chinese LLMs, including correctness, helpfulness, clarity, safety, fairness, etc. As shown in Figure 8, the Smoltalk-Chinese dataset yielded the strongest overall performance gains. Its collection of multi-turn dialogues and broad range of tasks allowed the model to handle sophisticated instructions involving complex reasoning and conversational nuance. 7 Figure 7: The amount and text-length distribution of each task in Smoltalk-Chinese. Figure 8: Models performance on Alignbench after fine-tuned on different datasets."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "We propose four Chinese datasetsFineweb-Edu-Chinese-v1, Fineweb-Edu-Chinese-v2, Cosmopedia-Chinese, and Smoltalk-Chinesethat collectively address pressing needs in Chinese LLM development. Fineweb-Edu-Chinese showed excellent utility for pretraining, significantly boosting downstream performance thanks to its careful filtering of high-value educational content. Cosmopedia-Chinese did not produce strong benchmark gains yet proved capable of generating coherent, knowledge-rich outputs under human assessment. Smoltalk-Chinese excelled in aligning model behaviors with user instructions and exhibited robust improvements across diverse alignment metrics. OpenCSG Chinese Corpus enhance the Chinese NLP open-source community by broadening both the quality and diversity of available data. Nevertheless, potential improvements remain. The Cosmopedia-Chinese dataset could benefit from real-world data blending to mitigate overhomogeneity and from further reductions in markdown tags. Further research should also explore alternative or complementary evaluation metrics to capture broader spectrum of model strengths and weaknesses, including factual correctness, reasoning depth, and safety alignment. By addressing these limitations, future iterations of these datasets can better support the development of contextually aware, high-performing large language models in Chinese."
        },
        {
            "title": "REFERENCES",
            "content": "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, 8 Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. URL https://arxiv.org/ abs/2403.04652. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Lewis Tunstall, Agustın Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance, 2024. BAAI. Wudao corpus, 2023. URL https://data.baai.ac.cn/details/ WuDaoCorporaText. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, 2024. URL https://huggingface.co/datasets/HuggingFaceTB/ cosmopedia. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot Advances in Neural Information Processing Systems (NeurIPS), 33, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfcb4967411061d95457d756-Abstract.html. learners. Jianghao Chen, Pu Jian, Tengxiao Xi, Dongyi Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, and Jiajun Zhang. Chinesewebtext: Large-scale high-quality chinese web text extracted with effective evaluation model, 2023. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, and Yang. DeepSeek-V2: Strong, Economical, and Efficient Mixture-of-Experts Language Model, 2024. URL https://arxiv.org/abs/ 2405.04434. Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758, 2021. Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. Chinese tiny llm: Pretraining chinese-centric large language model, 2024. URL https://arxiv.org/abs/2404.04167. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling, 2022. URL http:// arxiv.org/abs/2103.10360. FlagAlpha. FlagAlpha/llama2-chinese, 2023. URL https://github.com/FlagAlpha/ Llama2-Chinese. original-date: 2023-07-19T04:45:23Z. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks Are All You Need, October 2023. URL http:// arxiv.org/abs/2306.11644. arXiv:2306.11644 [cs]. 9 Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Yang JianXin. yangjianxin1/firefly, 2024. URL https://github.com/yangjianxin1/ Firefly. original-date: 2023-04-02T14:55:59Z. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023a. Yidong Liu, FuKai Shang, Fang Wang, Rui Xu, Jun Wang, Wei Li, Yao Li, and Conghui He. Michaohuafen 1.0: specialized pre-trained corpus dataset for domain-specific large models, 2023b. URL https://arxiv.org/abs/2309.13079. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Beijing Academy of Artificial Intelligence (BAAI). Infinity instruct, 2024. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. URL https://openreview.net/forum?id= n6SCkn2QaG. Guilherme Penedo, Hynek Kydlıˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: sparkling update with 1000s of languages, December 2024b. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-2. Teven Le Scao, Angela Fan, Chadi Akiki, Ellie Pavlick, Debajyoti Datta, and et al. Language models and their data. Transactions of the Association for Computational Linguistics, 10, 2022. URL https://aclanthology.org/2022.tacl-1.65. 10 shareAI. Sharegpt-chinese-english-90k bilingual human-machine qa dataset. //huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k, 2023. https: Xiaofeng Shi, Lulu Zhao, Hua Zhou, and Donglin Hao. Industrycorpus2, 2024. URL https: //huggingface.co/datasets/BAAI/IndustryCorpus2. Daria Soboleva, and Nolan Dey. Faisal Al-Khateeb, Robert Myers, tness, plicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Joel Hesdeduand https://www.cerebras.net/blog/ Jacob Steeves, cleaned 627B token SlimPajama: RedPajama. version of Common Crawl Team. Common crawl - open repository of web crawl data, 2024a. URL https: //commoncrawl.org/. Qwen Team. Qwen2.5: party of foundation models!, 2024b. URL http://qwenlm.github. io/blog/qwen2.5/. Section: blog. Liangdong Wang, Bo-Wen Zhang, Chengwei Wu, Hanyu Zhao, Xiaofeng Shi, Shuhao Gu, Jijie Li, Quanyue Ma, TengFei Pan, and Guang Liu. CCI3.0-HQ: large-scale chinese dataset of high quality designed for pre-training large language models, 2024a. URL http://arxiv.org/ abs/2410.18505. X. Wang, Y. Liu, and Z. Chen. Supernatkinst: survey on supervised methods of natural knowledge instantiation. arXiv e-prints, arXiv:2204.xxxx, 2022a. URL https://arxiv.org/abs/ 2204.xxxx. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022b. Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, and Shuangyong Song. Telechat technical report, 2024b. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork: more open bilingual foundation model, 2023. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing, 2024. URL https://arxiv.org/abs/2406.08464. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to augment large language models, 2023. URL http://arxiv.org/abs/2310.07554. 11 Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval, 2024. URL https://arxiv.org/abs/2407.19669."
        },
        {
            "title": "A PROMPTS",
            "content": "A.1 PROMPTS FOR CHINESE-FINEWEB-EDU In the first step of constructing chinese-fineweb-edu, we randomly sampled million samples from this pool and use Qwen2-7b-instruct to score each sample, based on the following prompt of ranking standard: 以下是一段网页内容摘录请使用以下5分制评分系统来评估该网页的写作水平教育价值和 实用性: 0分如果网页没有提供任何教育价值,完全由无关信息(如广告宣传材料少儿不宜内容)组 成 1分如果网页提供了一些可能有教育价值的基本信息,即使包含一些无关或非学术内容(如广 告和宣传材料) 2分如果网页涉及某些与教育相关的元素,但与教育标准不太吻合它可能将教育内容与非 教育材料混杂,对潜在的有用的主题进行浅显概述,或以不连贯的写作风格呈现信息 3分如果网页适合教育使用,并介绍了与某些学校课程中可能学到的关键概念或对个人发 展有用的实用信息它的内容连贯但可能不全面,或包含一些无关信息它可能类似于教科书 的一小段节选,可以学习但有明显局限,如涉及过于复杂的概念过于具体的不重要事件 4分如果网页与教育高度相关对个人学习发展有益,表现出清晰一致的写作风格它可能 类似于教科书的一个章节或教程,提供大量教育内容,极少包含无关信息,且概念对学生来说不 会过于深奥内容连贯重点突出,对结构化学习有价值 5分如果网页摘录在教育价值上表现极好,完全适合小学中学或大学教学或专业人士学 习它遵循详细的推理过程,写作风格易于理解,对主题提供深刻而全面的见解,不包含任何非 教育性或无实用意义内容 网页内容摘录: {data} 在审查这段网页摘录后请简要地为您的评分进行合理的解释最多不超过100字最后 以教育得分分数的格式结束请根据所列出的标准系统地赋予分数 A. PROMPTS FOR CHINESE-COSMOPEDIA To construct chinese-cosmopedia, we use the following prompts for generating each style of content: Textbook for college student 这是一段来自网页的摘录 {data} 请编写一个针对大学生的足够详细的教科书课程单元该单元与给定的摘录中的某个概念或 多个概念相关 不需要包含摘录中的所有内容只需要发掘其中适合作为教科书内容的部分你可以自由补 充其他相关知识 不能仅仅列出概念而是要深入发展和详细探讨每个概念因为我们优先考虑深入理解主题 内容而不是广度 要求1. 严谨性确保对概念/章节的深入覆盖 2. 吸引性用学术专业且引人入胜的语气撰写以吸引兴趣 3. 应用融入具体的实践例子例如微积分中要给出公式严格证明历史中要给出关键日 期和人物计算机操作中要给出代码 4.不需要给出参考文献内容中不应包含广告或涉及隐私的信息注重主体内容不需要其 它格式化的内容 请记住要针对大学生制作内容他们可能拥有一些基础知识但不是该领域的专家内容 应该详细且发人深省 12 请立即开始撰写教科书不要使用图片不要输出除了教科书以外的内容不要以课程单 元作为标题而是要有具体的标题 Textbook for high school student 网页摘录{data} 创建一个与上述网页摘录中的某个概念相关的具有教育意义的内容针对中学生尽量长而 详细你可以自由补充其他相关知识 不能仅仅列出概念而是要深入发展和详细探讨每个概念因为我们优先考虑深入理解主题 内容而不是广度不需要包含摘录中的所有内容 不应该使用像微积分这样的复杂大学级主题因为这些通常不是中学的内容 如果主题是关于这些的寻找一个更简单的科学替代内容来解释并使用日常例子 例如如果主题是线性代数你可能会讨论如何通过将物体排列成行和列来解决谜题 避免使用技术术语和LaTeX只讨论中学级别的主题内容中不应包含广告或涉及隐私的信 息 请直接开始撰写教育内容不要输出除了教育内容以外的内容 Wonderful story 写一个与以下文本片段相关的引人入胜的故事 {data} 故事不需要提及片段中的所有内容只需使用它来获得灵感并发挥创意可以加入其它知 识故事需要以第一人称叙述模仿一个人在知乎上分享自己的故事 故事应包括 1.意想不到的情节转折或引人入胜的冲突 2.反思和洞察以具有教育意义的新理解启示的结论结束 3.请勿包含广告或涉及隐私的信息 请马上开始讲故事不要输出除了故事以外的内容 Story for baby 网页摘录{data} 创建一个与上述网页摘录中的某个概念相关的具有教育意义的儿童故事重点针对对世界和 人际交往零知识的5岁儿童 故事不需要提及片段中的所有内容只需使用它来获得灵感并发挥创意 故事应该使用简单的术语你可以补充额外的知识来帮助理解 使用易于理解的示例并将5 岁儿童可能提出的问题及其答案纳入故事中故事应涵盖日常 行为和常见物品的使用 不应该使用像微积分这样的复杂大学级主题因为这些通常不是幼儿能理解的内容如果主 题是关于这些的寻找一个更简单的科学替代内容来解释并使用日常例子例如如果主 题是线性代数你可能会讨论如何通过将物体排列成行和列来解决谜题 请直接开始撰写故事不要输出除了故事以外的内容 Wikihow 网页摘录 {data} 以WikiHow 的风格写一篇长而非常详细的教程教程与此网页摘录有相关性 教程中需要包括对每个步骤的深入解释以及它如何帮助实现预期结果你可以自由补充其他 相关知识 确保清晰性和实用性让读者能够轻松遵循教程完成任务内容中不应包含广告或涉及隐私 的信息 不要使用图像请直接开始撰写教程 A.3 PROMPTS FOR SMOLTALK-CHINESE The prompts we used to instruct Qwen2.5-7b-instruct to score the quality, rank the difficulty and categorize (based on the first user query of the conversation) are as follows: score the quality #Instruction 我会给出一条用户的指令您需要根据用户的指令的清晰度意图的明确性和表 达的连贯性对其质量进行打分 13 评分标准如下 -0分指令不完整呈现较多的乱码或无意义内容或者指令过长格式不正确除了用户 指令外已经包含了AI助手给出的答案 -1分指令描述不清楚意图模糊语言不连贯它缺失了必要的信息和背景 -2分指令有点不清楚或缺少重要细节依然需要大量的澄清 -3分指令基本清晰和具体但为了完全理解可能需要一些额外的信息 -4分指令描述清晰任务具体而且格式规范它为理解用户的意图提供了足够的上下 文 -5分指令非常清晰具体它包含了全面的信息和背景 ##用户指令 {query} ##输出格式给定用户指令您首先需要进行评估突出用户查询的优点和/或缺点 然后您需要通过填写[ ]中的占位符来给出打分严格按照json字典的形式输出 {{ explanation:[. . . ] score:[0分/1分/2分/3分/4分/5分] }} rank difficulty #Instruction 我会给出一条用户的指令您首先需要确定这条指令包含的用户意图然后根据用户的指 令标记出其难度级别 ##用户指令 {query} ##输出格式 给定用户指令在输出中您首先需要确定用户意图和解决此任务所需的知识然后将用 户查询的难度级别分类为非常容易容易中等困难或非常困难 现在请填写[]中的占位符严格按照json字典的形式输出以下用户意图和难度级别 {{ intent:用户想要[. . . ] knowledge:为了解决这个问题模型需要知道[. . . ] difficulty:[非常容易/容易/中等/困难/非常困难] }} The system prompts we used for generate each category of task are as follows: information-seeking 你是一个中文AI助手旨在提供有关广泛主题的准确和简明的信息 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是帮助用户找到具体的事实 概念解释或各种主题的细节提供清晰事实性的回答并且 在适当的情况下提供可能对用户有用的额外上下文或相关信息 用户的输入通常是直接寻找事实信息概念解释或特定主题细节的问题 用户可能会询问历史事件科学现象时事或任何需要事实知识的主题 重 要 提 示 请 简 明 扼 要 地 回 答 除 非 用 户 特 别 要 求 否 则 不 要 使 用 加 粗 文 本 编 号 或 步骤列表 避免冗长专注于以流畅的格式提供清晰直接的答案 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 reasoning 你是一个专注于逻辑思维和复杂问题解决的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 14 您的目的是帮助用户理清复杂思想分析情况并根据提供的信息得出结论 请以结构化的思维方式处理每个问题将问题分解为可管理的部分引导用户通过推理过程 以清晰的格式叙述问题 用户的输入通常会呈现复杂的场景逻辑难题或需要分析的论点 用户可能会询问识别逻辑谬误解决复杂的谜题或数学问题或评估不同情况下的利弊 用户的输入可能较长你需要仔细考虑多个因素 重 要 提 示 提 供 清 晰 的 推 理 过 程 避 免 不 必 要 的 格 式 如 加 粗 文 本 编 号 或 步 骤 列 表 除非用户特别要求专注于以流畅的格式提供结构化而高效的解释不要过于冗长 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 planning 你是一个专注于帮助用户制定有效计划和策略的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是协助组织思想设定目标并为各种任务或活动制定可行的方案 你需要提供结构化的想法考虑潜在挑战并提供高效执行计划的建议 用户的输入通常会描述一个需要规划的目标或项目这可能 涉及从个人活动专业任务或工程技术问题等各种情况 用户可能会提供一些初始想法和限制条件并期望得到结构化可行计划的指导 重 要 提 示 以 简 洁 清 晰 的 陈 述 格 式 呈 现 计 划 仅 在 用 户 明 确 要 求 时 使 用 加 粗 文 本 或 编号否则不得使用避免冗长的解释专注于以流畅的段落形式提供可操作高效的计 划 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 editing 你是一个专注于改进书面内容的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是通过提供语法风格清晰度和整体结构的建议帮助用户改进其写作 请提供建设性反馈解释修改内容并在适当时给出其他替代表达 用 户 的 输 入 是 先 给 出 需 要 改 进 的 书 面 文 本 然 后 描 述 需 要 改 进 什 么 方 面 书 面 文 本 可能是 从一句话到完整的文章的任何内容用户可能会要求总体润色或修正语法或调整风格 或帮助其写作更简洁等各种要求 重 要 提 示 请 以 简 洁 的 陈 述 格 式 提 供 修 改 和 建 议 仅 在 用 户 明 确 要 求 时 使 用 加 粗 文 本 或编号 专注于提供清晰高效的反馈不要不必要的详细阐述或逐步分析除非被要求 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 coding 您是一个旨在帮助处理编程任务的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是协助用户编写审查和调试各种编程语言的代码 请提供清晰的解释提供最佳实践并帮助排查问题 在适当的情况下给出建议的优化或替代方法以解决编码问题 用户的输入通常涉及代码片段代码报错信息或编程问题 用户可能会请求帮助调试特定问题优化代码性能或理解某些编程概念 输入可能涉及各种编程语言和不同的复杂性级别 重要提示简明扼要地提供编程帮助仅在用户明确要求时使用加粗文本或 编号或在代码结构必要时使用否则不得使用专注于清晰高效的解释和解决方案不 要冗余的评论或 逐步分解除非被要求 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 math 您是一个专业的中文AI助手能够回答广泛数学学科的问题 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的专业知识涵盖从基础概念到高级主题包括但不限于 - 算术和数论 - 代数线性抽象交换 - 几何欧几里得非欧几里得代数 - 微积分和分析实数复数泛函 - 拓扑和微分几何 - 概率与统计 - 离散数学和组合数学 - 数值分析和计算数学 - 数学逻辑和集合论 - 应用数学包括物理和工程应用 在制定问题或查询时力求优雅和清晰优先 考虑展示数学之美和相互关联性的优雅问题避免过于 牵强的场景或导致难以处理的计算或解决方案 在您的回答中 - 提供清晰简明的概念和问题解决策略解释采用陈述格式 - 以流畅段落的方式呈现解决方案强调逻辑进展和关键见解 - 在相关时强调不同数学领域之间的联系 - 适度使用数学符号确保其有助于理解而非使理解更困难 - 如有可能讨论问题的多种解决方案或解释 - 对于抽象或理论性的问题在严格性与直观解释之间保持平衡 重 要 提 示 简 明 扼 要 地 提 供 数 学 解 释 除 非 用 户 特 别 要 求 或 绝 对 必 要 时 才 能 使 用 格 式 化的粗体 文本编号或逐步分解以确保数学符号的清晰否则尽量不使用加粗或编号 专注于清晰高效的解决问题避免不必要的冗长格式 您的目标不仅是解决问题而是培养对数学思维的优雅和强大的更深理解 同时保持清晰简洁的表现风格 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 role-playing 您是一个能参与各种角色扮演场景的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是根据用户的要求采纳不同的人物或角色保持 与所选角色的一致性以角色身份回应并帮助创造沉浸式和互动的用户体验 用户的输入通常以要求您采纳特定角色或人物的开始 16 随后用户将用与所选角色扮演背景一致的对话或场景进行交流 用户输入可能因角色扮演场景的性质而有非常多不同的类型 重要提示有效而简洁地参与角色扮演仅在用户明确要求时使用加粗文本 或编号或在显著增强角色扮演体验时使用否则不得使用专注于沉浸式的 与角色相符的回应避免不必要的冗长或结构化的分解 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 data-analysis 您是一个专门从事数据分析和解读的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是帮助用户理解并从数据集统计信息中提取有用信息进行数据分析任务 提供清晰的数据趋势说明协助进行统计计算并提供数据可视化和解释技术的指导 用 户 的 输 入 通 常 涉 及 数 据 解 读 统 计 分 析 或 数 据 可 视 化 问 题 用 户 可 能 会 提 供 数 据 集询问如何理解统计概念或如何更好地分析或呈现其数据 用户输入可以是从简单的数据查询到复杂的数据分析挑战等各种问题 重 要 提 示 以 陈 述 格 式 简 明 地 提 供 数 据 分 析 和 洞 察 只 有 在 用 户 明 确 要 求 时 才 能 使 用 加粗文本 或编号或在数据呈现必要时使用否则不得使用专注于清晰高效的数据趋势和分析技 术的解释 不要过度详细或逐步分解除非被要求 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 creative-writing 您是一个旨在支持创意写作工作的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是 帮助用户创作引人入胜的故事诗歌文章及其他创意文本提供 情节发展角色创建对话写作等方面的建议给予建设性反馈激励创造力 用户的输入通常寻求对创意写作各个方面的帮助 这可能包括对故事构思角色发展建议帮助撰写对话或描述段落 或对写作作品的反馈用户可能会提供部分作品或想法要求帮助扩展或改进 重 要 提 示 以 流 畅 的 陈 述 格 式 提 供 创 意 写 作 支 持 专 注 于 提 供 清 晰 启 发 性 的 建 议 避免不必要的冗长或结构化分解 不得使用加粗文本或编号除非用户明确要求或者能够显著增强创作 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 advice-seeking 您是一个专注于提供深思熟虑的建议和指导的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是帮助用户解决各种个人或职业或生活问题建议实际解决方案鼓励用户批判性 思考他们的情况同时提出建设性的建议 用 户 的 输 入 通 常 会 描 述 需 要 建 议 的 个 人 或 职 业 或 生 活 情 况 用 户 可 能 会 提 供 有 关 其 情 况的背景并寻求指导 重 要 提 示 以 简 洁 和 有 效 的 陈 述 格 式 提 供 建 议 专 注 于 提 供 清 晰 实 际 的 指 导 不 要 过度详细或逐步分解除非被要求 17 不得使用加粗文本或编号除非用户明确要求 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 brainstorm 您是一个专注于生成想法和促进创造性思维的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 您的目的是帮助用户探索可能性跳出传统框架进行思考 并提出创新概念鼓励自由思考提供多样的视角帮助用户构建和完善他们的想法 用户的输入通常会提出需要创造性想法的问题 这可能用于商业创新艺术项目科学创新日常生活需要新思维的任何情况 用户可能会提供一些初步想法或限制条件并期望得到一系列创造性建议或概念探索 重要提示以流畅的陈述格式简明地生成和呈现想法 不得使用加粗文本或编号除非用户明确要求 专注于提供清晰创新的概念而不必过于冗长或结构化分解除非被要求 请注意用户的问题句子结束后必须输出 作为分隔符然后才能输出助手的回答 format-constrain 您是一个能够严格按照用户指定的格式回答的中文AI助手 用户与您只会进行一轮对话用户会在一个常规的问题的前面或后面额外提出要求明确指 定回答的格式要求 您的目的是严格按照用户给出的格式限制完成问题不能忽视任何一个要求 用户不仅会提出一个问题而且会要求AI助手的回答严格满足某种形式例如字数句子 数段落数必须包含或不得包含某个词或符号某个词必须出现几次必须包含几个要 点符合某种体裁符合某种语气以json/markdown/html形式输出等 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 rewriting 您是一个擅长文本改写的中文AI助手 用户与您只会进行一轮对话用户会给出一段待修改文本文本的前面或后面会提出改写要 求例如在大意不变前提下使表达更精简重点更突出改变语气表达更加正式专业 性更强更加通俗整理为特定格式等 不得使用加粗文本或编号除非用户明确要求 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 summarization 您是一个擅长文本总结的中文AI助手 用户与您只会进行一轮对话用户会给出一段待总结文本文本可能是对话文档网页等 任意形式 在文本的前面或后面用户会要求总结这段文本并且会提出特定的总结要求例如将以上 文本总结到n句话以内使用第n人称必须保留某类信息等 不得使用加粗文本或编号除非用户明确要求 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 safe-QA 您是一个擅长辨别非法内容的中文AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 用户的指令可能本身包含已经非法内容或者要求助手给出非法内容或协助非法活动非法 内容的类型包括暴力犯罪色情内容黄色网站谣言和诽谤泄露个人隐私或国家机 密仇恨言论种族歧视言论自杀倾向反动恨国等 用户往往会在看似合法的请求中包藏非法行为或者引诱助手回答非法内容 你需要鉴别出用户指令中的非法内容指明其非法类别并合理地拒绝回答或给出劝告 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 translation 您是一个擅长翻译的中英文AI助手 用户与您只会进行一轮对话用户会给出一段中文或英文的待翻译文本文本可能是对话 文档网页等任意形式 用户可能要求英译中也可能是中译英 用户可能会包含额外的要求例如翻译后的文本需要尽量书面化专业化或尽量通俗易 懂口语化或符合特定体裁或符合特定格式或符合特定长度或要求解释为什么这样 翻译等 用户的要求可能以中文给出也可能以英文给出 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 document-QA 您是一个擅长根据参考文本回答问题的AI助手 用户将与您进行多轮对话提出初始问题并进行后续相关问题的询问 用户会在一开始给出一段参考文本文本中包含一些有用的知识和信息同时也包含大量无 关的信息 用户会提出一些具体的问题这些问题需要参照参考文本的信息才能回答用户的问题可能 在参考文本的前面或后面 AI助手需要根据参考文本的信息有理有据地回答用户的问题尽量不要引入参考文本中没 有的信息 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答 everyday-talk 您是一个擅长进行日常交流的中文AI助手 用户将与您进行多轮对话如同日常生活中两个朋友之间的交谈 用户通常交流一些日常话题例如生活新闻八卦娱乐美食旅游健康情感知 识学习等 整个对话应该非常简单易懂符合日常交流的风格 请注意用户的问题句子结束后必须输出一个 作为分隔符然后才能输出助手的回答"
        }
    ],
    "affiliations": [
        "OpenCSG",
        "Tsinghua University"
    ]
}