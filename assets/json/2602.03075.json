{
    "paper_title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
    "authors": [
        "Junjie Huang",
        "Jiarui Qin",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Xing Sun",
        "Weinan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs."
        },
        {
            "title": "Start",
            "content": "ReMiT ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution Junjie Huang1, Jiarui Qin2, Di Yin2, Weiwen Liu1, Yong Yu1, Xing Sun2, Weinan Zhang1 1Shanghai Jiao Tong University 2Tencent Youtu Lab Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for bidirectional processwhere insights from post-training retroactively improve the pre-trained foundationremains unexplored. We aim to establish self-reinforcing flywheel: cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement LearningGuided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs. 6 2 0 F 3 ] . [ 1 5 7 0 3 0 . 2 0 6 2 : r Figure 1. ReMiT on OLMo-1B substantially outperforms baselines trained with the standard mid-training method. (a) At the mid-training stage, ReMiT improves average accuracy across 10 widely-used benchmark tasks by 5.2% and reaches the baseline performance 6 faster. (b) The improvements can carry over to post-training: during RL, ReMiT maintains higher verifiable correct rate than the baseline and achieves better performance. Equal contribution. huangjunjie2019@sjtu.edu.cn, jaredqin@tencent.com Corresponding author. ReMiT Figure 2. We identify the mid-training stage as critical turning point, as it rapidly shifts the base models token distribution toward that of more capable RL model. ReMiT enhances this stage by dynamically reweighting tokens in the mid-training corpus."
        },
        {
            "title": "1 Introduction",
            "content": "Training large language models (LLMs) is multi-stage process comprising pre-training and post-training [Achiam et al., 2023, Touvron et al., 2023, Yang et al., 2025]. Pre-training of LLMs typically optimizes next-token prediction objective that applies uniform weight to every token in the loss function. In contrast, reinforcement learning (RL) follows different training paradigm: RL methods assign non-uniform token weights based on reward signals from the environment [Schulman et al., 2017, Shao et al., 2024], yielding substantial improvements in the reasoning capabilities. strong pre-trained base model serves as the foundation for effective post-training. However, little research examines how to leverage these two stages synergistically, specifically, how post-trained models can provide guidance for pre-training, thereby improving final performance. Much of the literature on improving pre-training focuses on data selection, which can be viewed as documentlevel reweighting. Methods in this line of work [Wenzek et al., 2019, Xie et al., 2023, Albalak et al., 2024] primarily perform data cleaning to quickly filter noisy documents. Other studies investigate sample-level reweighting [Gu et al., 2023, Zhang et al., 2025], but these approaches often rely on heuristic rules and remain relatively coarse-grained. Recent research suggests that only small subset of tokens is pivotal for model capability [Arbuzov et al., 2025, Cui et al., 2025], indicating that sentence-level upsampling or downsampling may be suboptimal and motivating finer-grained token-level reweighting. By analyzing training dynamics during the pre-training phase, we identify the mid-training stage as critical turning point (see Appendix for details). This stage typically trains on the highest-quality corpora under rapidly decaying learning rate to accelerate the acquisition of high-order capabilities such as graduate-level STEM knowledge or mathematical/code reasoning [Touvron et al., 2023, Yang et al., 2025]. Our analysis indicates that mid-training induces significant shift in the probability distribution of the base model. As shown in Figure 2, we compare the token-level log probability distributions over sequences produced by the final RL model with those of the base model before and after mid-training. It can be observed that the RL model is much closer to the mid-trained model, while it differs significantly from the pre-trained model. The observation suggests that mid-training plays pivotal role in the qualitative transformation of the base model. Since the RL model and the mid-trained model exhibit similar token-level probability distributions, it is natural to consider whether the RL model can be used to refine the token loss distribution during mid-training. By doing so, we aim to produce model that is more closely aligned with the RL model, thereby yielding stronger base. This improved base could be more compatible with the subsequent RL phase and ultimately lead to better overall performance. Building on these observations, we propose ReMiT (Reinforcement Learning-Guided Mid-Training). ReMiT bridges pre-training and post-training by reusing the in-pipeline RL-tuned model as reference, thereby 2 ReMiT eliminating the need for separately trained teacher model as in Lin et al. [2024]. Specifically, we compute the per-token loss discrepancy between the RL reference and the base model to derive dynamic importance weights. Rather than discarding tokens via hard selection, ReMiT dynamically modulates their weights, thereby preserving the semantic coherence of the training data. Consequently, the mid-training stage effectively mimics the RL training paradigm by prioritizing tokens that contribute most to downstream performance, while largely retaining the efficiency of standard next-token prediction within the existing pipeline. In summary, the contributions of this paper are as follows: Novel Mid-Training Framework: We propose ReMiT, the first framework to leverage an RL-tuned model to guide token-level weight assignment during the mid-training phase. By dynamically prioritizing tokens pivotal for reasoningin contrast to standard uniform weightingReMiT effectively transfers advanced capabilities retroactively to the base model. Bidirectional Synergy without External Teachers: We establish bidirectional synergy between pretraining and post-training by reusing in-pipeline models. Notably, ReMiT eliminates the need for specially trained or external reference models, ensuring practicality and scalability. Empirical Validation and Flywheel Effect: Extensive experiments across diverse model families demonstrate the efficacy of ReMiT, yielding an average improvement of 3% on 10 pre-training benchmarks. Crucially, we verify that these gains persist through post-training, sustaining an improvement of over 2% throughout the pipeline, thereby establishing co-improving flywheel between the base and RL models for continuous evolution."
        },
        {
            "title": "2.1 Standard Mid-Training Formulation",
            "content": "Standard training pipelines for LLMs typically conclude the pre-training phase with specialized stage known as mid-training (see Appendix A). This stage focuses on refining the model using high-quality corpora (e.g., mathematical reasoning or code) under rapidly decaying learning rate. Despite the shift in data distribution, the fundamental objective remains the standard Next-Token Prediction (NTP). Consider sequence of tokens, x1:T = [x1, . . . , xT] from vocabulary V. An autoregressive model defines the probability of this sequence as the product of conditional probabilities, as shown in Equation (1). Given }N mid-training dataset Dmid comprising sequences Dmid = {x i=1, the model parameters θ are optimized by minimizing the average negative log-likelihood in Equation (2): (i) 1:Ti pθ(x1:T) = t=1 pθ(xt x<t) . LNTP(θ) = 1 Ti i=1 Ti t=1 log pθ (cid:16) (i) (i) <t (cid:17) . (1) (2) This standard objective applies uniform weighting to every token position t. Such indiscriminate treatment fails to distinguish between low-entropy tokens and pivotal tokens that drive complex reasoning steps, limitation that motivates our proposed framework. 3 ReMiT Figure 3. Visualization of the log-probability divergence between the pre-trained base model and the RL model. Background intensity reflects the margin log = log pRL log pbase, where deeper red highlights pivotal tokens on which the RL model demonstrates significantly higher confidence than the base model."
        },
        {
            "title": "2.2 Analyzing Distributional Discrepancies Between Base and RL Models",
            "content": "We visualize the distributional shift by comparing the token-level log-probability gap between the base and RL models in Figure 2. Specifically, we compute probability distributions over sequences drawn from the mid-training corpus. We observe that the gap narrows substantially after mid-training, confirming that this stage acts as critical turning point, driving qualitative transformation in the base model. To pinpoint exactly where the RL model outperforms the pre-trained base model before mid-training, Figure 3 visualizes the per-token prediction difference ( log p). The background color encodes this margin: deeper red indicates that the RL model assigns significantly higher likelihood to the ground-truth token than the base model. Visually, we observe that the advantage of RL model is highly localized. While the majority of the text retains light background, reflecting modest margins, distinct pivotal tokens emerge with deep red highlighting. This pattern suggests that the superior reasoning of the RL model is driven by small set of pivotal tokens, with the rest largely following the distribution of the base model. Notably, these high-margin tokens often correspond to discourse connectives (e.g., \"Therefore\", \"So\"), structural markers (e.g., \"boxed\"), or key logical verbs (e.g., \"follow\", \"Adding\"), consistent with the enhanced reasoning capabilities of RL models. See Appendix for more examples. Building on these observations, we propose to dynamically upweight these pivotal tokens. We identify the mid-training phase as the optimal window for this intervention. Unlike general pre-training, mid-training utilizes high-quality corporaenriched with instruction and reasoning-intensive datathat are inherently aligned with post-training objectives. Furthermore, due to the rapidly decaying learning rate, this phase serves as the critical stage for acquiring high-order capabilities."
        },
        {
            "title": "3.1 Framework Overview",
            "content": "As illustrated in Figure 4, ReMiT introduces paradigm shift from standard unidirectional training pipelines to bidirectional, self-reinforcing flywheel. By retroactively transferring reasoning priors from the posttraining stage back to the pre-training foundation, ReMiT creates closed-loop system: utilizing more capable RL reference for mid-training yields strengthened base model, which in turn unlocks potential for subsequent post-training stages. ReMiT is designed with two core principles: Efficient Reuse of In-Pipeline Models: ReMiT directly repurposes the RL-tuned model inherently 4 ReMiT Figure 4. Overview of the proposed ReMiT framework. The pipeline connects pre-training and post-training, establishing self-reinforcing flywheel: improvements from the RL stage are retroactively transferred to strengthen the base model foundation, which in turn amplifies performance in subsequent post-training stages. produced by the previous training cycle as the reference. This eliminates the need for training separate teacher models on manually curated clean datasetsa process that is often costly and ill-defined. Strategic Intervention at Mid-Training: We identify mid-training as the optimal phase due to its reasoningoriented corpora and rapid learning rate decay. This data-efficient environment enables the base model to rapidly assimilate the reasoning priors from the RL reference."
        },
        {
            "title": "3.2 Dynamic Token Reweighting Mechanism",
            "content": "ReMiT introduces dynamic weighting scheme designed to prioritize tokens where the base model significantly underperforms relative to the RL reference. This approach effectively steers the optimization focus toward pivotal tokens that embody the RL models reasoning priors. Quantifying Discrepancy. We focus on tokens where the RL model exhibits higher confidence than the current base model. We quantify this discrepancy as the delta loss L(xt), defined as the difference in log-likelihoods: L(xt) = Lθ(xt) LRL(xt) = log pθ(xt x<t) + log pRL(xt x<t). (3) Sequence-Level Centering. Raw log-probability gaps can vary significantly across different sequences due to intrinsic sentence difficulty. Sequence-level centering is used as normalization mechanism to control per-sequence scale variation, ensuring stable and comparable token-level weights: µ = 1 t=1 L(xt), (cid:98)L(xt) = L(xt) µ. (4) 5 ReMiT Non-Linear Modulation with Clipping. To map these centered deltas into scalar weights, we employ scaled sigmoid function. This mapping is constructed to satisfy neutrality constraint: when the two models are equally confident ((cid:98)L(xt) 0), the objective should reduce to the standard NTP loss. Since σ(0) = 0.5, scaling factor of 2 is therefore required to ensure wt 1, thereby preserving the default learning signal. To maintain training stability, we additionally enforce clipping mechanism bounded by hyperparameter ϵ: (cid:16) wt = clip 2 σ(cid:0) (cid:98)L(xt)(cid:1), 1 ϵ, 1 + ϵ (cid:17) , σ(z) = 1 1 + ez . (5) This clipping mechanism serves as regularization term or safety rail, ensuring that the objective remains anchored to the fundamental NTP paradigm. In particular, it prevents over-fitting to individual tokens and avoids degradation of basic syntactic coherence due to vanishing weights. Together, this bounded monotonic mapping converts log-probability gaps into token weights in controlled manner, enabling stable optimization. While other monotonic functions could be used, the scaled sigmoid offers desirable properties of smoothness and boundedness. We provide an ablation study on ϵ in Section 5.3. Algorithm 1 ReMiT: RL-Guided Mid-Training Require: Params θ, frozen RL ref, corpus Dmid, clip ϵ 1: for x1:T Dmid do 2: Lθ(xt) log pθ(xt x<t) LRL(xt) log pRL(xt x<t) (cid:98)L(xt) Lθ(xt) LRL(xt) µ Centered log-prob gap (cid:16) wt clip 2σ((cid:98)L), 1 ϵ, 1 + ϵ (cid:17) 3: 4: 5: Dynamic reweighting LReMiT 1 θ θ ηθLReMiT wt Lθ(xt) 6: 7: 8: end for The ReMiT Objective. Finally, we integrate these dynamic weights into the standard objective. By performing soft reweighting rather than hard token selection, ReMiT preserves the semantic coherence of the sequence while amplifying the gradient signal for pivotal tokens: LReMiT(θ) = 1 Ti i=1 Ti t=1 (i) log pθ (cid:16) (i) (i) <t (cid:17) . (6) The procedure is summarized in Algorithm 1, which can be seamlessly integrated into existing pre-training pipelines."
        },
        {
            "title": "4 Theoretical Motivation",
            "content": "Having introduced ReMiT in Section 3, we now analyze its theoretical foundations. key question arises: why does reweighting tokens based on an RL model effectively improve the base model? We address this by demonstrating that each ReMiT update actively steers the base model in direction that locally reduces the Kullback-Leibler (KL) divergence to the KL-regularized optimal policy."
        },
        {
            "title": "4.1 ReMiT as Optimization towards an Implicit Target Distribution",
            "content": "We first analyze the ReMiT objective for fixed context x<t. By viewing the weighted negative log-likelihood in Equation (6) through distributional lens, we reveal that it is equivalent to minimizing the divergence 6 ReMiT towards constructed implicit target distribution. Let w(x x<t) denote the token-level weighting function derived from the log-probability gap. We define the implicit target distribution qw as the re-weighted data distribution: qw(xt x<t) = pdata(xt x<t)w(xt x<t), (7) 1 Zw(x<t) where Zw(x<t) = xV pdata(x x<t)w(x x<t) is the partition function ensuring normalization. Intuitively, qw can be interpreted as an enhanced data distribution where pivotal tokens are assigned higher probability mass. As derived in Appendix B.1, the gradient of the ReMiT objective aligns with the gradient of the KL divergence between this implicit target and the current model: θLReMiT(θ) θ x<tD [DKL(qw( x<t) πθ( x<t))] . (8) This decomposition reveals that ReMiT updates effectively drive the model distribution πθ towards qw. Through this process, the RL model influences optimization by reshaping the implicit target, thereby acting as soft navigator for the base model."
        },
        {
            "title": "4.2 Directional Consistency with the Optimal Policy",
            "content": "A critical question remains: does aligning with qw actually steer the model toward the true optimal policy π? To answer this, we reframe the mid-training process within the framework of KL-regularized reinforcement learning. We model next-token generation as decision step in Markov decision process, where the context x<t serves as the state and the next token xt constitutes the action. We assume the existence of an underlying reward function r(xt, x<t) that captures the desired reasoning quality. The optimization objective is to maximize the expected reward while maintaining proximity to reference policy πref (typically the pre-trained base model) to prevent reward hacking: max π x<tD,xtπ (cid:20) r(xt, x<t) β log π(xt x<t) πref(xt x<t) (cid:21) , (9) where β > 0 is the regularization coefficient. Local Geometry Analysis. It is well-established in optimal control literature [Peters et al., 2010, Rafailov et al., 2023] that the closed-form solution to this objective, π, follows Gibbs distribution: π(xt x<t) πref(xt x<t) exp (cid:18) r(xt, x<t) β (cid:19) . (10) With the explicit form of π defined, we analyze the optimization landscape. Consider local update of the current model πθ in the direction of the implicit target qw derived in Section 4.1. Let the updated policy be πθ = (1 τ)πθ + τqw for sufficiently small step size τ > 0. first-order expansion of the KL divergence DKL(π πθ) (derivation in Appendix B.2) demonstrates that the divergence from the optimal policy decreases provided that: π(x x<t) (cid:18) qw(x x<t) πθ(x x<t) πθ(x x<t) (cid:19) > 0. (11) This condition admits clear interpretation: if the implicit target qw assigns higher probability mass to tokens favored by π, the update is directionally consistent. 7 ReMiT Crucially, we leverage the RL model πRL as specialized detector for reasoning-critical tokens. Optimized for reasoning rewards, πRL exhibits significantly higher confidence than the base model specifically at pivotal decision points. Consequently, the log-probability gap functions as reliable signal for identifying these tokens. By assigning wt > 1 (and thus qw > πθ) primarily when this gap is large, ReMiT actively concentrates optimization on tokens favored by π. This approach satisfies the consistency condition in Equation (11) by utilizing πRL to scale the gradient magnitude on informative tokens, thereby effectively transferring reasoning priors while maintaining the base models broader generative capabilities."
        },
        {
            "title": "4.3 Comparisons with Knowledge Distillation",
            "content": "While ReMiT shares the high-level aspiration of transferring capabilities from the RL model to the base model, it fundamentally diverges from knowledge distillation (KD) Hinton et al. [2015]. We characterize ReMiT as importance-aware discriminative distillation, contrasting it with the mimicry inherent in standard KD. Gradient Analysis. To clarify the underlying difference, we analyze the gradients at single token position rather than the aggregate loss in Equation (6). Standard KD Gou et al. [2021], Yang et al. [2025] minimizes the forward KL divergence (equivalent to cross-entropy with soft targets) across the entire vocabulary V: LKD = xV π(x x<t) log πθ(x x<t). (12) Differentiating with respect to θ yields gradient that compels the student to match the teachers full probability distribution. Crucially, this formulation exerts gradient pressure even on low-probability tokens (often termed dark knowledge Fan et al. [2024], Yu et al. [2024]). θLKD = xV π(x x<t) (cid:123)(cid:122) (cid:125) (cid:124) Soft Target θ log πθ(x x<t). (13) In contrast, ReMiT operates on hard targets but dynamically modulates the learning intensity via reweighted negative log-likelihood. The per-token gradient is: θ LReMiT = wt θ log πθ(xt x<t), (14) where wt denotes the weight defined in Equation (5). Unlike KD, which alters the optimization direction to match potentially imperfect teacher πRL, ReMiT preserves the ground-truth direction while utilizing πRL solely to scale the gradient magnitude on informative tokens. We empirically compare ReMiT with KD in Section 5.3 and Appendix E."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Base Models. We evaluate ReMiT across three representative open-source model families that provide publicly available checkpoints suitable for mid-training: OLMo-1B [OLMo et al., 2024], SmolLM3-3B [SmolLM et al., 2025], and Youtu-LLM-2B Lu et al. [2025]. In our experimental setup, the base model is defined as the 8 ReMiT Table 1. Few-shot accuracy across 10 widely used downstream tasks. All improvements of ReMiT over baselines are statistically significant. Extended results and the num_shots are provided in Appendix D. The best scores of each model family are boldfaced. Method GSM8K MATH GPQA BBH IFE HE MBPP TQA ARC-C MMLUP Avg. Pre-Trained Vanilla NTP MiniPLM RHO-1 ReMiT Pre-Trained Vanilla NTP MiniPLM RHO-1 ReMiT Pre-Trained Vanilla NTP MiniPLM RHO-1 ReMiT 3.03 48.14 48.45 50.42 61.64 31.61 64.22 60.65 62.93 63.76 34.87 49.51 51.86 42.46 52.69 2.94 10.26 9.60 10.32 14.50 14.52 31.64 30.84 28.70 31. 15.30 25.00 22.76 16.44 24.50 Mid-Training on OLMo-1B 28.43 30.87 30.38 29.33 32.07 22.66 16.19 16.79 19.06 28.54 6.71 8.54 7.32 6.71 12.80 4.80 4.60 6.80 6.20 9. Mid-Training on SmolLM3-3B 43.13 56.50 57.10 55.32 58.27 22.06 43.29 41.49 38.73 45.68 25.61 28.05 29.88 31.10 37.20 37.40 47.60 48.00 43.80 49.60 Mid-Training on Youtu-LLM-2B 43.39 44.37 44.20 40.45 47.21 23.26 32.61 33.09 32.01 36.93 25.00 37.20 35.98 27.44 39.94 38.80 46.60 47.00 39.20 47.00 20.31 22.54 23.21 25.45 24.55 27.68 26.34 26.34 26.56 26. 22.99 27.90 26.34 26.79 29.69 21.30 22.40 23.13 23.38 25.58 30.23 29.74 29.13 28.52 31.95 27.54 26.56 26.56 26.68 27.42 44.71 46.67 45.31 46.42 49.23 56.31 53.24 53.58 54.10 54. 52.30 53.33 53.75 49.23 54.61 9.54 13.31 13.15 13.68 17.44 23.24 30.68 29.52 28.19 30.73 19.96 24.93 25.34 21.69 25.85 16.44 22.35 22.41 23.10 27.56 31.18 41.13 40.65 39.80 42. 30.34 36.80 36.69 32.24 38.58 standard pre-trained checkpoint (the state immediately prior to mid-training), while the corresponding publicly released RL-tuned version serves as the reference policy. Baselines. We compare ReMiT with four baselines: Pre-Trained. The standard pre-trained base model, which serves as the initialization point for mid-training. Vanilla NTP. The conventional next-token prediction objective that assigns uniform weights to all tokens during mid-training, as formulated in Equation (2). MiniPLM [Gu et al., 2024]. An offline sample selection framework that uses difference sampling to prioritize hard samples based on teacher probabilities. We adopt their offline selection strategy with retention ratio of 0.8. RHO-1 [Lin et al., 2024]. token-level selection method that only retains tokens with high RHO loss. Following their method, we mid-train the reference model on curated data and filter tokens with top-k retention ratio of 0.8. Implementation Details. We conduct mid-training on 50B tokens using the official high-quality corpora associated with each model family. For ReMiT, we set the clipping threshold ϵ = 0.2 in Equation (5). To ensure rigorous comparison, we align all optimization hyperparameters (e.g., learning rates, batch sizes, sequence lengths) with the models original pre-training configurations. Detailed hyperparameters, corpus compositions, and training settings are strictly provided in Appendix D. Evaluation. To comprehensively evaluate the effectiveness of ReMiT, we assess the few-shot performance of the resulting mid-trained models across multiple widely-used downstream benchmarks using the lmeval-harness framework [Gao et al., 2021]. Our evaluation suite covers both reasoning-intensive domains, such as MATH and MBPP, and general capability benchmarks, including MMLU-Pro, TruthfulQA, and ARC-C. To further demonstrate the downstream benefits of ReMiT, we also conduct subsequent post-training 9 ReMiT Figure 5. Performance gains of ReMiT on OLMo-1B acquired during mid-training transfer consistently to post-training, regardless of the post-training process (SFT, DPO, or RLVR). Figure (a): applying RL directly to the mid-trained base model. Figure (b): applying the complete post-training procedure to the mid-trained base model. alignmentspecifically SFT, DPO, and RLVRand evaluate the zero-shot accuracy of the resulting models. Further evaluation protocols are detailed in Appendix D, with extended results provided in Appendix E."
        },
        {
            "title": "5.2 Main Results",
            "content": "ReMiT Enhances Mid-Training Performance. Table 1 reports few-shot accuracies on downstream tasks, leading to three key observations. First, mid-training proves to be critical stage: all mid-trained variants significantly outperform the pre-trained base models. Second, ReMiT achieves the best performance by dynamically prioritizing tokens pivotal to the advanced reasoning capabilities of the RL model. Finally, ReMiT outperforms both RHO-1 and MiniPLM. Unlike RHO-1, which incurs costly auxiliary reference model training with uncertain gains and disrupts semantic coherence by discarding tokens, ReMiT employs cost-free, soft reweighting strategy. Moreover, ReMiT offers finer granularity than MiniPLMs sample selection, ensuring critical informative tokens are not obscured. Mid-Training Gains Transfer to Post-Training. stronger base model does not necessarily guarantee superior final RL model after post-training, as further analyzed in Figure 7. To address this, we show that ReMiT enhances performance not only during the mid-training stage but also provides superior foundation for subsequent post-training. As illustrated in Figure 5, we apply identical post-training procedures to both the vanilla NTP model and ReMiT on OLMo-1B. We observe that the performance gains achieved during mid-training transfer consistently to post-training, regardless of the specific technique employed. Figure 5 (a) presents the results of applying RL directly to the mid-trained models, while Figure 5 (b) shows the results after applying the full post-training pipeline, including SFT, DPO, and RL. Additional results are provided in Appendix E. Co-Improving Feedback Loop Between Pre-Training and Post-Training. We have shown that ReMiT enhances performance during both pre-training and post-training. We further demonstrate that stronger RL reference amplifies mid-training gains (Figure 6 (a)). By employing the initial ReMiT-derived RL model to guide second round of mid-training, we produce ReMiT2. This creates cascading effect: an improved base yields superior RL reference, which subsequently refines the base further. The performance superiority of ReMiT2 over ReMiT confirms the frameworks capability to drive continuous self-improvement. SFT: supervised fine-tuning; DPO: direct preference optimization; RLVR: reinforcement learning with verifiable rewards. 10 ReMiT Figure 6. Robust generalization via ReMiT. (a) Coimproving Loop: Performance gains consistently transfer and amplify through iterative mid-training cycles (Vanilla NTP ReMiT ReMiT2). (b) KL Analysis: While KD forces the model to strictly mirror the RL policy, ReMiT allows for moderate KL divergence, ensuring better transferability during post-training."
        },
        {
            "title": "5.3 Analysis and Discussion",
            "content": "Figure 7. Sustained advantages of ReMiT across training stages. ReMiT achieves better generalization with pretraining gains that consistently carry over to post-training, whereas KDs early advantages fade in later stages. ReMiT vs. KD: Robust Generalization via Soft Alignment. Our experiments reveal critical distinction in how ReMiT and KD influence the models long-term evolution. As illustrated in Figure 7, while KD achieves impressive performance gains immediately after mid-training, these advantages fail to sustain after subsequent post-training. This diminishing return suggests that KD compromises the models capacity for further adaptation. We attribute this to the over-alignment phenomenon. In Figure 6 (b), we analyze the KL divergence distributions relative to the teacher model. The KD model exhibits an extremely low KL divergence, indicating it has collapsed onto the teachers specific distribution and over-fitted its output patterns. In contrast, ReMiT maintains moderate distributional gap. This soft alignment ensures the base model assimilates the teachers high-level reasoning capabilities without strictly memorizing its probability distribution, thereby preserving the diversity and potential required for continuous self-improvement during post-training. Extended results are provided in Appendix E. Choosing the Reference: Why RL over SFT. To investigate the influence of reference policy selection, we conduct an ablation study comparing the RL model against an SFT baseline; for detailed visualization and analysis of their distinct token-level preferences, please refer to Appendix C. Regarding downstream performance, Figure 12 demonstrates that ReMiT consistently outperforms the SFT baseline throughout the mid-training process. Impact of the Clipping Mechanism. As analyzed in Figure 8, the clipping mechanism serves as critical safety rail that prevents the gradient collapse observed in the no clip variant. By enforcing baseline weight, clipping preserves general modeling capabilities while prioritizing reasoning-critical tokens. This balance ensures healthy gradient norms and faster convergence, contrasting sharply with RHO-1, where aggressive token discarding results in diminished gradients and impedes efficient optimization. Figure 8. Clipping mechanism in ReMiT on Youtu-LLM. By bounding token-level weights, clipping keeps gradient norms within normal range and accelerates loss reduction. ReMiT"
        },
        {
            "title": "6 RELATED WORK",
            "content": "Data Selection Strategies in Pre-Training. Optimizing data quality is fundamental to pre-training [Xie et al., 2023, Albalak et al., 2024, Liu et al., 2024, Gu et al., 2024, Zhu et al., 2025]. Traditional strategies employ lightweight filters, including heuristic-based, classifier-based [Mann et al., 2020], and perplexity or loss-based approaches [Qin et al., 2023, Wenzek et al., 2019], while recent approaches utilize reference models for hard data selection based on loss signals [Ankner et al., 2024, Lin et al., 2024]. In contrast, ReMiT introduces token-level reweighting mechanism that leverages the existing RL model as cost-free reference, avoiding the overhead of training auxiliary models. Token-Level Analysis. Recent studies in RLVR highlight the critical role of token-level entropy. Yang et al. [2025] argue that low-probability tokens disproportionately influence model updates due to their large gradient magnitudes, which hinders effective learning of LMs. Other works observe that meaningful updates are concentrated at sparse key tokens representing pivotal decision junctions [Wang et al., 2025, Arbuzov et al., 2025]. Wang et al. [2025] further leverage these entropy patterns to design adaptive weighting strategies that prioritize uncertain transitions during post-training."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work investigates the coupling of pre-training and post-training, identifying the mid-training phase as critical turning point. We introduce ReMiT, method that leverages an RL reference to assign dynamic, token-wise weights during mid-training, guiding the base model toward pivotal reasoning tokens. Unlike methods that discard data, ReMiT employs soft reweighting to preserve semantic coherence and training efficiency. Evaluations across multiple model families demonstrate that ReMiT consistently enhances both mid-training and post-training, establishing an iterative feedback loop between the base and the RL model."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [5] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [6] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019. 12 ReMiT [7] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023. [8] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. [9] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. [10] Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, and Dianhui Chu. survey on data selection for llm instruction tuning. Journal of Artificial Intelligence Research, 83, 2025. [11] Mikhail Arbuzov, Alexey Shvets, and Sisong Beir. Beyond exponential decay: Rethinking error accumulation in large language models. arXiv preprint arXiv:2505.24187, 2025. [12] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. [13] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen, et al. Not all tokens are what you need for pretraining. Advances in Neural Information Processing Systems, 37:2902929063, 2024. [14] Jan Peters, Katharina Mülling, and Yasemin Altun. Relative entropy policy search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 24, pages 16071612, 2010. [15] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, 2023. [16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [17] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International journal of computer vision, 129(6):17891819, 2021. [18] Chuanpeng Yang, Yao Zhu, Wang Lu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. Survey on knowledge distillation for large language models: methods, evaluation, and application. ACM Transactions on Intelligent Systems and Technology, 16(6):127, 2025. [19] Wen-Shu Fan, Xin-Chun Li, and De-Chuan Zhan. Exploring dark knowledge under various teacher capacities and addressing capacity mismatch. arXiv preprint arXiv:2405.13078, 2024. [20] Chengting Yu, Fengzhao Zhang, Ruizhe Chen, Aili Wang, Zuozhu Liu, Shurun Tan, and Er-Ping Li. Decoupling dark knowledge via block-wise logit distillation for feature-level alignment. IEEE Transactions on Artificial Intelligence, 2024. [21] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. [22] Team SmolLM et al. Smollm3: smol, multilingual, long-context reasoner. https://huggingface.co/ blog/smollm3, 2025. [23] Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, et al. Youtu-llm: Unlocking the native agentic potential for lightweight large language models. arXiv preprint arXiv:2512.24618, 2025. ReMiT [24] Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Miniplm: Knowledge distillation for pre-training language models. arXiv preprint arXiv:2410.17215, 2024. [25] Leo Gao, Jonathan Tow, Stella Biderman, Shawn Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jasmine Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Version v0. 0.1. Sept, 10:89, 2021. [26] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023. [27] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: comprehensive survey. arXiv preprint arXiv:2402.18041, 2024. [28] Xiaoxuan Zhu, Zhouhong Gu, Baiqian Wu, Suhang Zheng, Tao Wang, Tianyu Li, Hongwei Feng, and Yanghua Xiao. Toremi: Topic-aware data reweighting for dynamic pre-training data selection. arXiv preprint arXiv:2504.00695, 2025. [29] Ben Mann, Nick Ryder, Melanie Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1(3): 3, 2020. [30] Ziheng Qin, K. Wang, Zangwei Zheng, Jianyang Gu, Xiang Peng, Daquan Zhou, and Yang You. Infobatch: Lossless training speed up by unbiased dynamic data pruning. ArXiv, abs/2303.04947, 2023. URL https://api.semanticscholar.org/CorpusID:257427625. [31] Zachary Ankner, Cody Blakeney, Kartik K. Sreenivasan, Max Marion, Matthew L. Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. ArXiv, abs/2405.20541, 2024. URL https://api.semanticscholar.org/CorpusID:270199394. [32] Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, and Yunjian Xu. Do not let low-probability tokens over-dominate in rl for llms. ArXiv, abs/2505.12929, 2025. URL https://api.semanticscholar.org/CorpusID:278740466. [33] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. ArXiv, abs/2506.01939, 2025. URL https://api.semanticscholar.org/ CorpusID:279119146. [34] Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, and Wei Wang. Entropy-based adaptive weighting for self-training. ArXiv, abs/2503.23913, 2025. URL https://api.semanticscholar.org/CorpusID: 277451642. [35] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [36] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:3843 3857, 2022. [37] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [38] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 14 ReMiT [39] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [40] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 56735684, 2023. [41] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [42] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [43] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [44] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [46] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [47] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [48] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [49] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [50] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, 2019. [51] Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. ArXiv, abs/2310.16049, 2023. URL https://api. semanticscholar.org/CorpusID:264439655. [52] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= 1qvx610Cu7. 15 ReMiT Background: The Mid-Training Phase Standard large language model (LLM) training typically comprises two stages: pre-training on large-scale corpora and post-training for alignment. Recent work highlights an intermediate stageoften termed mid-training or annealingthat bridges broad knowledge acquisition and the refinement of specialized capabilities. Mid-training is commonly associated with two shifts: High-quality data shift. Whereas stable pre-training relies primarily on large volumes of general web text to build broad coverage, mid-training emphasizes data quality over quantity. As illustrated in the top panel of Figure 9, this phase increases the proportion of STEM, code, and reasoning-focused data, which can encourage the model to develop stronger competence in complex problem solving. Learning-rate decay. To incorporate this higher-quality data effectively, training typically applies an accelerated learning-rate decay. As shown in the bottom panel of Figure 9, the learning rate decreases from the pre-training plateau to near-zero value. This process can stabilize optimization and consolidate acquired behaviors, making mid-training natural window for injecting or reinforcing reasoning-oriented capabilities before post-training. Figure 9. Overview of the pre-training pipeline. Top: During mid-training, the data distribution shifts from general web text to higher-quality mixture enriched with code, STEM, and reasoning content. Bottom: The learning rate schedule is synchronized with this data shift, exhibiting rapid decay throughout the mid-training phase."
        },
        {
            "title": "B Mathematical Derivations",
            "content": "In this section, we provide the rigorous derivations supporting the theoretical motivation in Section 4. B.1 Decomposition of the ReMiT Objective We show that minimizing the ReMiT objective is equivalent to minimizing the KL divergence between the implicit target qw and the model πθ. Recall the ReMiT loss for single step (ignoring the expectation over 16 contexts for brevity): LReMiT = pdata(x)w(x) log πθ(x). ReMiT (15) Using the definition of the implicit target qw(x) = 1 Zw Zwqw(x): pdata(x)w(x), we can substitute pdata(x)w(x) = LReMiT = (cid:32) = Zw Zwqw(x) log πθ(x) (cid:33) qw(x) log πθ(x) . (16) (17) Now, consider the KL divergence DKL(qw πθ): DKL(qw πθ) = qw(x) log qw(x) πθ(x) = (cid:124) qw(x) log qw(x) (cid:123)(cid:122) H(qw) (cid:125) qw(x) log πθ(x). (18) Substituting the cross-entropy term back into the loss equation: Since the partition function Zw and the entropy H(qw) depend only on the fixed reference model and data distribution (independent of θ), we have: LReMiT = Zw (DKL(qw πθ) + H(qw)) . (19) Note that while Zw acts as scaling factor varying per context, the gradient direction aligns with minimizing the divergence to qw. θLReMiT = Zwθ DKL(qw πθ). (20) B.2 First-Order Expansion of KL Divergence We verify the condition under which an update towards qw reduces the divergence to the optimal policy π. Let J(π) = DKL(π π). We consider perturbation of the policy π in the direction of target distribution q: π = π + ϵ(q π), where ϵ is small learning rate. The functional derivative (Fréchet derivative) of the KL divergence J(π) with respect to π(x) is: δJ δπ(x) = π(x) π(x) . (21) Using first-order Taylor expansion, the change in divergence is: (π(x) π(x)) J(π) J(π) = = ϵ For the divergence to decrease (i.e., J(π) < J(π)), we require the change to be negative, which implies: (cid:18) q(x) π(x) π(x) ϵ(q(x) π(x)) π(x) π(x) π(x) (cid:19) (cid:19) . δJ δπ(x) (cid:18) (22) (23) (24) π(x) q(x) π(x) π(x) > 0. (25) This confirms Equation (11) in the main text. 17 ReMiT Figure 10. Token distribution divergence between the Base and RL models. Background intensity encodes the per-token log-probability margin ( log = log pRL log pbase), with deeper red indicating higher confidence in the ground-truth token from the RL model. B.3 Relation Between ReMiT Updates and Distributional Motion In standard gradient descent, the parameter update θnew θ ηθL induces change in the distribution space. For log-linear models (like the softmax output layer of LLMs), the natural gradient update direction in the probability simplex is proportional to the difference between the target and the current distribution: π(x) qw(x) πθ(x). (26) 18 ReMiT Figure 11. comparison of token-level preferences between the RL and SFT models. The background color of each ground-truth token indicates the log-probability margin log = log pRL log pSFT. Deeper red signifies stronger preference from the RL model, deeper blue from the SFT model, and white denotes negligible difference ( log < 0.1). This justifies our assumption in Appendix A.2 that minimizing the ReMiT objective effectively moves the policy distribution πθ in the direction of qw πθ. Thus, if the alignment condition (Equation (11)) holds, the parameter update on θ translates to functional improvement towards π. Visualization of Log-Probability Discrepancies Between Different"
        },
        {
            "title": "Models",
            "content": "C.1 Base Model versus RL Model To identify where the RL model surpasses the base model on given sentence, Section 2.2 presents an example visualizing log-probability discrepancies. Figure 10 provides additional examples for further analysis. For each token, the background color encodes the RL-to-Base log-probability margin ( log p): lighter shades indicate smaller margins, whereas deeper red indicates that the RL model assigns higher likelihood to the ground-truth token. Most tokens show modest margins, while small subset exhibits large positive margins (deep red). This pattern suggests that the RL models superior reasoning is driven by limited set of pivotal tokens, with the remainder largely following the base models distribution. 19 ReMiT Figure 12. Comparison of downstream performance between SFT-Guided and ReMiT during mid-training on OLMo1B. The results demonstrate that our RL-guided approach ReMiT consistently outperforms the SFT-guided baseline throughout the training trajectory. C.2 SFT Model versus RL Model We now analyze the rationale for selecting the RL model over the SFT model as reference during the midtraining process. key justification lies in their distinct token-level preferences, which we now examine in greater detail. As our examples illustrate, the RL model assigns significantly higher likelihood to discourse connectives (e.g., First, But, So, Let). This preference, visualized by deeper red background for the corresponding tokens, is consistent with the enhanced reasoning capabilities cultivated during reinforcement learning, confirming the RL model as the more suitable reference for ReMiT."
        },
        {
            "title": "D Experimental Configuration",
            "content": "D.1 Mid-Training Corpus For the data setup, we utilize the official mid-training mixes associated with each model family. These corpora share similar high-quality composition, blending general web text with domain-specific data such as code, mathematics, and STEM reasoning. For all three models, the mid-training phase is conducted on total of 50B tokens. The sequence length is set to 4,096 for OLMo and 8,192 for both SmolLM3 and Youtu-LLM, consistent with their original configurations. 20 D.2 Optimization Hyperparameters We employ cosine decay schedule down to minimum learning rate of 1e-7. The specific configurations are: ReMiT OLMo-1B: Maximum learning rate 3e-4, Global batch size 2M tokens. SmolLM3-3B: Maximum learning rate 2e-4, Global batch size 2M tokens. Youtu-LLM-2B: Maximum learning rate 2e-4, Global batch size 64M tokens. These settings are consistent with the configurations used during the respective stable pre-training phases of each model. D.3 Evaluation of Pre-trained Base LMs We comprehensively evaluate the pre-trained base models by measuring their few-shot performance across suite of widely used downstream benchmarks, including: GSM8K (Cobbe et al. [35]; 8-shot), Minerva Math (Lewkowycz et al. [36]; 4-shot), GPQA (Rein et al. [37]; 5-shot), BigBenchHard (BBH; Suzgun et al. [38]; 3-shot CoT), IFEval (IFE; Zhou et al. [39] 3-shot), HumanEval (HE; Zheng et al. [40]; 0-shot), MBPP (Austin et al. [41]; 3-shot), TruthfulQA (TQA; Lin et al. [42]; 3-shot), ARC-Challenge (Clark et al. [43]; 25-shot), and MMLUPro ([44]; 5-shot). Unless otherwise specified, we report Pass@1 for both MBPP and HumanEval. In extended experiments, we also evaluate additional benchmarks such as HellaSwag [45], PIQA [46], Social IQA [47], WinoGrande [48], OpenBookQA [49], CommensenseQA [50] and MuSR [51]. D.4 Evaluation of Post-trained LMs Given that the post-trained models already exhibit instruction-following ability, we evaluate their zero-shot accuracy. We report results on 10 widely used benchmarks: GSM8K [35], Minerva Math [36], MBPP [41], MBPP+ [52], HumanEval (HE; Zheng et al. [40]), HumanEval+ (HE+; Liu et al. [52]), ARC-Challenge [43], GPQADiamond (GPQADM; Rein et al. [37]), MMLUPro [44], IFEval (IFE; Zhou et al. [39])."
        },
        {
            "title": "E Extended Analysis and Results",
            "content": "E.1 Pre-Training Results In this section, we provide extended experimental results comparing different mid-training methodologies, with particular focus on the ablation variants discussed in Section 5.3. Reference Policy Ablation: RL vs. SFT. We first present detailed comparison between utilizing the RL model (ReMiT) versus the SFT model (SFT-Guided) as the reference policy. Figure 12 illustrates the downstream performance trajectories on OLMo-1B throughout the mid-training process. It is evident that ReMiT consistently outperforms SFT-Guided across the training steps. This superiority is further 21 ReMiT Table 2. Extended evaluation of pre-training models across 10 widely used downstream tasks. The best scores within each model family are boldfaced. Method GSM8K MATH GPQA BBH IFE HE MBPP TQA ARC-C MMLUP Avg. Vanilla NTP SFT-Guided KD ReMiT Vanilla NTP SFT-Guided KD ReMiT 48.14 50.34 61.26 61.64 49.51 47.54 60.42 52.69 10.26 12.16 15.88 14.50 25.00 19.84 23.86 24. Mid-Training on OLMo-1B 30.87 31.15 32.11 32.07 16.19 13.07 45.56 28.54 8.54 10.98 13.41 12.80 4.60 9.00 13.20 9.20 Mid-Training on Youtu-LLM-2B 44.37 44.60 50.71 47.21 32.61 32.73 38.61 36.93 37.20 32.93 32.93 39.94 46.60 43.40 45.20 47.00 22.54 23.44 23.88 24.55 27.90 30.13 25.67 29. 22.40 24.11 25.83 25.58 26.56 25.46 29.38 27.42 46.67 47.70 49.23 49.23 53.33 52.65 52.13 54.61 13.31 15.23 17.45 17.44 24.93 24.42 27.77 25. 22.35 23.72 29.78 27.56 36.80 35.37 38.67 38.58 Table 3. Zero-shot accuracy of post-trained models on OLMo-1B across 10 widely used downstream tasks. The best scores are boldfaced. Method GSM8K MATH MBPP MBPP+ HE HE+ ARC-C GPQADM MMLUP IFE Avg. Vanilla NTP KD ReMiT Vanilla NTP KD ReMiT Vanilla NTP KD ReMiT 42.61 41.55 47.00 57.70 49.66 55.04 66.72 62.77 65.73 12.28 15.22 16.54 15.10 15.96 17. 18.60 16.84 19.66 16.20 21.20 19.60 9.20 12.80 12.40 8.80 11.50 11.60 Stage: SFT 21.95 30.49 26. 18.90 26.22 21.95 Stage: DPO 24.39 22.56 25.61 21.95 20.73 22.56 Stage: RL 19.51 24.39 28. 17.07 22.56 24.39 25.66 33.33 29.10 18.78 22.75 21.69 17.33 21.30 22.09 40.96 38.91 41.38 43.43 39.25 45. 43.00 41.38 44.80 24.75 25.25 25.76 21.72 26.34 23.23 20.20 24.75 25.25 14.34 15.41 13.56 14.61 15.80 15. 14.58 15.24 16.73 56.83 58.03 59.11 27.45 30.56 30.02 67.39 67.87 69.18 29.43 29.37 30.87 69.90 69.30 70. 29.57 31.00 32.92 Table 4. Zero-shot accuracy of post-trained models on Youtu-LLM-2B across 10 widely used downstream tasks. The best scores are boldfaced. Method GSM8K MATH MBPP MBPP+ HE HE+ ARC-C GPQADM MMLUP IFE Avg. Vanilla NTP KD ReMiT Vanilla NTP KD ReMiT 57.32 63.38 64. 64.75 72.48 71.34 22.46 27.28 22.84 26.36 30.76 28.62 46.60 45.00 49.40 47.60 47.00 47.00 Stage: SFT 56.71 57.32 57.32 50.61 51.22 53.66 Stage: DPO 60.37 60.98 64.02 54.88 56.10 59.15 62.17 61.90 65. 62.70 65.08 67.46 43.77 43.00 45.22 46.50 45.22 47.10 24.75 27.27 29.80 27.78 25.76 30.30 23.66 27.02 25. 27.32 30.40 29.40 65.83 66.55 65.59 45.39 46.99 47.86 74.82 74.58 75.54 49.31 50.84 51.99 corroborated by the comprehensive benchmark results in Table 2, empirically validating the choice of the RL model as the optimal reference for guiding the base model. For analysis investigating why these models yield different results, we provide visualizations of their distinct token-level preferences in Appendix C. Effectiveness of Knowledge Distillation. Echoing the results in Figure 7(a) of the main text, we observe that knowledge distillation (KD) achieves 22 ReMiT Figure 13. Loss trajectories during SFT and DPO on OLMo-1B. Figure 14. Performance comparison at the reinforcement learning stage on OLMo-1B. impressive performance gains immediately in the mid-training phase. This demonstrates the effectiveness of strict distribution matching for transferring capabilities in the short term. However, critical limitation emerges upon further evaluation: these advantages fail to sustain after subsequent post-training. This phenomenon suggests that while KD effectively clones the teachers current distribution, it may compromise the models plasticity for future adaptation, which we will show in the following section regarding posttraining results. E.2 Post-Training Results To further demonstrate that ReMiT enhances performance beyond the mid-training stage, we conduct additional post-training on the mid-trained base models. As illustrated in Figure 13, the loss trajectories for both SFT and DPO stages show that ReMiT consistently achieves lower loss values compared to the standard approach. Furthermore, Figure 14 presents the training dynamics during the RLVR stage, indicating that ReMiT achieves higher verifiable correct rate while also maintaining longer sequence lengths. Table 3 details the post-trained performance across 10 downstream benchmarks. The results confirm that the gains acquired during mid-training are effectively preserved throughout the entire post-training pipeline. 23 ReMiT Figure 15. Impact of RL reference quality on mid-training performance across 10 downstream tasks. We compare ReMiT (using fully converged RL model) against ReMiT (Intermediate) which uses checkpoint trained for only 10% of RL steps. Notably, compared to KD, whose early advantages tend to diminish during subsequent alignment steps due to over-fitting the teachers distribution, ReMiT demonstrates sustained superiority. This observation aligns with the conclusions drawn from Figure 6 (b) and 7(b) in the main text, highlighting ReMiTs robust generalization capability."
        },
        {
            "title": "F Discussion",
            "content": "F."
        },
        {
            "title": "Impact of RL Reference Quality",
            "content": "We study how the quality of the RL reference affects mid-training by using an intermediate RL checkpoint (trained for only 10% of the total RL steps) as weaker reference, denoted ReMiT (Intermediate). We compare this variant with standard ReMiT (using the fully converged RL model) and the Vanilla NTP baseline. Results are shown in Figure 15. Sensitivity to signal quality. We observe clear performance ordering: ReMiT > ReMiT (Intermediate) > Vanilla NTP. The observed positive correlation between reference quality and downstream performance confirms that ReMiT effectively leverages the specific reasoning priors embedded in the reference signal. Robustness and safety boundaries. Despite being trained for only 10% of the RL steps, ReMiT (Intermediate) consistently outperforms Vanilla NTP, indicating robustness to imperfect guidance. Moreover, the clipping mechanism in Equation (5) provides an explicit safeguard by limiting extreme token weights, thereby reducing the risk of overfitting to noise or bias in weaker reference. Implications for the in-pipeline strategy. These findings further support the in-pipeline design (Figure 4): the observed gains can be obtained using an RL reference produced within the same pipeline and of the same parameter scale, yielding reliable and consistent signal without relying on larger, external teachers. This reinforces our central contributionan iterative evolution cycle between pre-training and post-training. By feeding post-training improvements back into mid-training, ReMiT forms self-reinforcing flywheel that 24 ReMiT Figure 16. Training efficiency analysis of OLMo-1B on the same 50B mid-training corpus. (a) Wall-clock Convergence: Despite the computational overhead of the reference model, ReMiT demonstrates superior efficiency, converging 3.3x faster than the baseline in terms of GPU hours. (b) Throughput Comparison: While ReMiT incurs reduction in training throughput (steps/sec) due to the additional forward pass, this cost is effectively amortized by its rapid convergence rate. enables continual progress while avoiding the additional computational cost of training separate, external reference models. F.2 Computational Efficiency and Overhead Analysis potential concern with reference-guided approaches is the computational overhead introduced by the additional forward pass of the reference model. To address this, we analyze the trade-off between training throughput and convergence speed, demonstrating that ReMiT offers superior compute-performance ratio compared to standard next token prediction. Time-to-Convergence Efficiency. While Figure 1 establishes ReMiTs sample efficiencyshowing it reaches baseline performance using significantly fewer tokens (6 faster in token count) Figure 15 provides critical wall-clock analysis that accounts for real-world computational costs. As shown in Figure 15(a), ReMiT achieves 3.5x speedup in time-to-convergence compared to the Vanilla NTP baseline, measured in GPU hours. This result is significant because it confirms that the gains in sample efficiency vastly outweigh the reduction in training throughput. Overhead vs. Gain. Figure 15(b) explicitly quantifies the computational cost: ReMiT incurs 43% reduction in throughput (steps/sec) due to the inference overhead of the frozen RL reference. However, this cost is effectively amortized by the methods rapid learning trajectory. Crucially, the performance ceiling achieved by ReMiT in Figure 15(a) is not merely result of accelerated convergence; the curve shows that ReMiT reaches accuracy levels that the Vanilla NTP baseline struggles to attain even with extended training. This indicates that ReMiT provides qualitative improvement in reasoning capabilities that cannot be replicated simply by training standard model for longer periods or with more tokens. Moreover, given that the mid-training stage typically occupies short window at the end of pre-training, this temporary overhead does not pose barrier for scaling to larger models. Future research could investigate strategies to mitigate this overhead, for instance, through offline weight computation or asynchronous scoring mechanisms."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tencent Youtu Lab"
    ]
}