{
    "paper_title": "Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs",
    "authors": [
        "Paiheng Xu",
        "Gang Wu",
        "Xiang Chen",
        "Tong Yu",
        "Chang Xiao",
        "Franck Dernoncourt",
        "Tianyi Zhou",
        "Wei Ai",
        "Viswanathan Swaminathan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains."
        },
        {
            "title": "Start",
            "content": "Paiheng Xu1*, Gang Wu2, Xiang Chen2, Tong Yu2, Chang Xiao2, Franck Dernoncourt2, Tianyi Zhou1, Wei Ai1, Viswanathan Swaminathan2 1University of Maryland, College Park, 2Adobe Research 5 2 0 2 9 2 ] . [ 1 6 0 4 0 2 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate software-specific skillseta collection of verified scriptsby exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillsets diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as testbed for LLMbased systems, highlighting the advantages of leveraging execution feedback in controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains."
        },
        {
            "title": "Scripting interfaces in software applications play a\npivotal role in extending the capabilities of software",
            "content": "*Work done during internship at Adobe Research Corresponding authors: Gang Wu (gawu@adobe.com) and Xiang Chen (xiangche@adobe.com) beyond their standard functionalities. They enable users to automate repetitive tasks, customize workflows, and integrate applications with other systems (Ousterhout, 1998). Prominent software like Adobe Illustrator and Adobe Photoshop support scripting through ExtendScript, which is Adobes extended version of JavaScript tailored for their applications. 1 Similarly, Microsoft Office applications provide scripting interfaces based on JavaScript, allowing users to automate tasks within Excel, Word, and other Office programs. 2 These scripting interfaces expose Application Programming Interfaces (APIs) that allow scripts to interact with the softwares internal functions and data structures. Traditionally, creating scripts using these interfaces requires programming expertise and familiarity with the specific APIs of the software, posing barrier for many users. With the strong code generation capacity of Large Language Models (LLMs) (Chen et al., 2021; Bubeck et al., 2023), some solutions generate code based on user query during runtime (Gandhi et al., 2023; Zhao et al., 2024a). However, such runtime generation approaches have notable limitations: (1) the generated code is unverified when presented to the users, leading to low-quality code that may not align with users intentions and can introduce security risks through unintended behaviors; (2) they impose considerable runtime burden, including increased response times and token generation costs, particularly for applications with large user base. In this work, we propose using offline simulation to curate software-specific skillset set of scripts that automate tasks within the software. Then they can be retrieved during runtime to solve user queries. We use publicly available scripting 1Illustrator: https://ai-scripting.docsforadobe. dev/. Photoshop: https://helpx.adobe.com/photoshop/ using/scripting.html. 2https://learn.microsoft.com/en-us/office/dev/ add-ins/reference/javascript-api-for-office guides and LLMs knowledge about the software to create the skillset. The offline simulation consists of two LLM-based components: (1) task creation, which generates useful tasks within the software, and (2) skill generation with trials, translating the generated tasks into skills with execution feedback from previous trials. For task creation, we introduce two simulation strategies that use the softwares functionality information (top-down) and API information (bottom-up) from the publicly available scripting guide. To more efficiently explore the vast number of APIs supported in the software, we define synergistic API pairs as APIs that can work together in existing skills. We construct synergistic API graph and train link prediction model using Graph Neural Network (GNN) to capture both the semantic and structural patterns of existing synergistic API pairs. This enables the model to generalize to unseen API pairs and assess their compatibility. The synergy of APIs is further used to prompt LLMs to generate tasks that better elicit the softwares internal functions and data structures by using more long-tailed APIs. We conduct comprehensive experiments using Adobe Illustrator as testbed to evaluate our approach. Our findings demonstrate that our offline simulation framework significantly improves success rates and efficiency of automation task compared to traditional runtime code generation methods. Our main contributions are: We propose novel offline simulation framework for curating software-specific skillset, leveraging LLMs and publicly available scripting guides. Our framework employs two simulation strategiestop-down functional guidance and bottom-up API synergy explorationto generate tasks and scripts that cover wide range of software functionalities. We introduce new setup that leverages the softwares API information to explore the capacity of the software. We propose to use GNNbased link prediction model to capture the synergy between APIs, which encourages generating skills involving underutilized or long-tailed APIs, thereby expanding the diversity and utility of the skillset. To the best of our knowledge, this is the first attempt to use software scripting interfaces as testbed for LLM-based systems. This approach highlights the advantages of obtaining direct execution feedback in controlled environment and offers valuable insights into aligning AI capabilities with user needs in specialized software domains."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Skill Discovery with LLMs LLM-based skill discovery is gaining attention across various domains, including embodied agents (Zhao et al., 2024b), sandbox environment (Wang et al., 2024b), and LLM tool usage (Qian et al., 2023; Cai et al., 2024; Yuan et al., 2024; Nguyen et al., 2024). Similarly, skills in these scenarios are represented by code and designed to interact with corresponding environments. However, existing methods either do not consider the coverage of these skills within their environments (Zhao et al., 2024b; Wang et al., 2024b; Qian et al., 2023; Cai et al., 2024) or rely on rich dataset pairing task descriptions with desired outcomes in question-answering format (Qian et al., 2023; Cai et al., 2024; Yuan et al., 2024; Nguyen et al., 2024). Therefore, these approaches are limited in applicability to practical scenarios such as software scripting automation, where outcomes are more complex (sometimes involving multiple modalities) and cannot be easily encapsulated in simple text string. Curating dataset with such diverse and complex outcomes is challenging, especially when the goal is to explore softwares full automation potential. Our study addresses this gap by investigating novel skill discovery strategies that leverage softwares publicly available functionalities and API information."
        },
        {
            "title": "2.2 Program Synthesis with LLMs",
            "content": "Program synthesis aims to generate code given natural language description (Zhang et al., 2023), where LLMs (Austin et al., 2021; Chen et al., 2021; Nijkamp et al., 2023) have demonstrated impressive performance. Modern code generation models are typically evaluated on functional correctness (Liu et al., 2024b), often requiring predefined unit tests (Chen et al., 2021; Li et al., 2022). In the context of software scripting automation, execution results from the software environment provide immediate feedback on generated code. Unlike the setting of code generation with given task descriptions, this study emphasizes deciding what tasks to generate in specific software, leveraging LLMs strong code generation capacity that can be further improved with execution feedback from the software environment during offline simulation (Kim Figure 1: Overview of our offline simulation framework for skill discovery in software scripting automation. The framework consists of two components: (1) Task Creation (Section 3.1), utilizing two simulation strategies: top-down functionality guidance and bottom-up API synergy exploration to generate wide range of tasks; and (2) Skill Generation with Trials (Section 3.2), where LLMs iteratively refine scripts based on execution feedback to produce verified skills ready for runtime retrieval. et al., 2024; Pan et al., 2024)."
        },
        {
            "title": "2.3 Automation with LLM Agents",
            "content": "With the rapid advancement of Large Language Models (LLMs), researchers have developed systems and benchmarks to automate complex tasks requiring multiple applications in computer environments (Xie et al., 2024; Cao et al., 2024). However, existing LLM-based agents face significant challenges in reliably automating these tasks, achieving only about 15% success rate across several hundred tasks. Efforts to improve LLM capabilities have also focused on domain-specific tasks, such as spreadsheet manipulation (Li et al., 2024; Ma et al., 2024b), Graphical User Interface (GUI) automation (Gao et al., 2024), and web browsing (Yao et al., 2022; Ma et al., 2024a; Deng et al., 2024). In these specialized domains, LLM-based agents demonstrate higher capacity. Our paper aligns with this body of work by leveraging LLMs to automate domain-specific computer tasks. However, we present the first attempt to (1) automate tasks through the softwares internal scripting environment, and (2) use LLM-based agents to systematically explore and identify which tasks can be automated in the software. In this section, we detail our framework for skill discovery with offline simulations, which can be divided into two modules, task creation and skill generation with trials. We formulate the problem as follows: our goal is to develop set of tasks (in natural language), = {t1, t2, ..., tn}, which can be automated via the scripting interface in software application. The corresponding scripts or code are defined as skills, = {s1, s2, ..., sn}. After developing during offline simulation, user can input some operation they want to automate as query to the system during runtime, where relevant skill, si, is retrieved to solve q. Figure 1 shows an overview of the proposed framework and we now introduce our method in detail."
        },
        {
            "title": "3.1 Task Creation",
            "content": "We rely on an LLM to generate the tasks. To fully exploit the functionalities supported by the software, we adopted two strategies to guide LLMs to search for possible tasks, i.e., top-down approach that starts from high-level functionalities of the software, followed by bottom-up approach that considers low-level APIs supported by the software. Top-down We curate list of high-level functionalities for the software. Taking Adobe Illustrator as an example, the high-level functionalities can be drawing, arranging objects, and so on. This list can be obtained from the content categorization from publicly available scripting guides. For each functionality, we prompt the LLMs to generate related tasks. Following previous works that adopt longterm memory (Wang et al., 2024b,a), we generate tasks in multiple rounds and integrate the feedback obtained from the previous round. We introduce what feedback our framework provides in Section 3.2. The prompts are in Appendix B. We note that this step can also be considered as warmup for the following bottom-up search. Bottom-up unique opportunity and challenge in the Software scripting interface setting is that we have list of all APIs supported by the software, denoted as = {a1, a2, ..., an}, and the scripts largely rely on the collaboration of multiple APIs. Therefore, we prompt LLMs with appropriate API combinations to spark LLMs knowledge about the software. Given the large number of APIs and the potential for multiple tasks to be accomplished using the same set of APIs, we introduce an API synergy graph, G, to model the likelihood of APIs working together. Graph-based representations are widely employed to model relationships in related applications such as API recommendations (Qi et al., 2022; Huang et al., 2022) and LLM-based reasoning (Anokhin et al., 2024; Liu et al., 2024a). Specifically, we define APIs, A, as the nodes in and the edges, E, represent whether the two APIs have appeared in verified script. The node features are the semantic embeddings of the corresponding API descriptions, denoted as X. The two nodes with link are defined as synergistic API pair that can work together. We then train link prediction model by randomly masking existing links in and predicting the likelihood of their existence. We use Graph Convolutional Network (GCN) model (Kipf and Welling, 2017) to achieve this. The model aggregates information from neighboring nodes and their features, H(l) = GCN(l)(H(l1), E) where H(0) = are the initial node embeddings and H(l) are the node representations after layers of message passing. The likelihood of an edge between two nodes and (cid:16) , where is modeled as: ˆyuv = σ , h(L) h(L) (cid:17)(cid:17) (cid:16) and h(L) h(L) are the L-layer representations of nodes and (final layer), is scoring function, such as the inner product (hu, hv) = hv, and σ is the sigmoid function. In short, the model learns to predict the likelihood of two API nodes working together by aggregating information from neighboring nodes and their features. The model is trained with binary cross-entropy loss: = 1 (cid:88) (cid:104) yuv log(ˆyuv) (u,v)D + (1 yuv) log(1 ˆyuv) (cid:105) , where is the set of sampled edges (positive pairs from and negative pairs not in E), and yuv {0, 1} is the ground truth label for whether the edge (u, v) exists. After training, the GCN model captures both the semantic and structural patterns of synergistic API pairs, enabling it to generalize to unseen API pairs and assess their compatibility. Then for each API ai, we prompt LLMs to generate tasks related to ai and its top-k synergistic APIs. We show full prompts in Appendix B."
        },
        {
            "title": "3.2 Skill Generation with Trials",
            "content": "One advantage of generating skills offline is that we can generate the code with multiple trials offline without adding any burden to the users during runtime, providing better user experience. For each generated ti, we use strategy similar to the ideas of , where the LLM learns from the execution feedback in the software to refine its own outputs. Additionally, we use another LLM or Large Vision Language Model (LVLM) as validator to judge the script by looking at the generated code, execution output, and (visual) outcome in the software. The validator comments on whether the skill accomplishes the task and provides feedback for improvement for the next trial. skill is added to the skillset if it passes the validator."
        },
        {
            "title": "4 Experiments",
            "content": "Adobe Illustrator as the Testbed Adobe Illustrator is leading vector graphics software used by professionals worldwide for tasks such as creating logos, illustrations, and complex design elements. It supports multiple categories of high-level functionalities, including drawing, arranging objects, and applying effects. full list of the high-level functionalities used in the top-down search in Section 3.1 is provided in Appendix A, which can be obtained through Illustrators official scripting guide. 3 Modern software platforms that support scripting interfaces offer tasks with varying levels of complexity. This study focuses on generating atomic skills: fundamental, modular tasks that involve minimal design choices and serve as building blocks for more complex operations. For instance, in the context of Illustrator, instead of creating an entire flower design, an atomic skill would arrange pre-designed petals into circular pattern, emphasizing precision and modularity. Illustrators scripting interface supports 1818 API endpoints, where 378 are the methods and the remaining are attributes of the object, enabling extensive programmatic control over the software. Desc Code arrange selected objects in circle function arrangeInCircle (cX , cY , radius ) { var sel = app . selection if ( sel . length === 0) { throw new Error ( 'No selection ' ); } var angleStep = 360 / sel . length ; for ( var = 0; < sel . length ; ++) { var angle = angleStep *i *( Math . PI /180); var = cX + radius * Math . cos ( angle ); var = cY + radius * Math . sin ( angle ); sel [i ]. position = [x , ]; } return ' Objects arranged in circle '; } Effect Table 1: An example skill, arrangeInCircle. Desc is the natural language description. Effect shows the layout before and after running the skill. Simulation Setup Adobe Illustrator, as vector design tool, focuses on creating and manipulating graphical objects. In practical applications, typical Illustrator project often contains numerous objects, and scripts are usually expected to operate on specific subsets of these objects. The common approach is to manually select the desired objects before running the script, leveraging the selection attribute available for each object. Therefore, for each skill, we prompt the LLM to generate initialization scripts that set up the document with necessary elements and adjust their selection attributes according to the task description. Following previous work with other virtual environments (Wang et al., 2024b), we prompt the LLM to make the task code generic and reusable, as well as other Illustrator-specific instructions. The full prompt is 3https://helpx.adobe.com/pdf/illustrator_reference.pdf shown in Appendix B. Table 1 presents an example skill, including the corresponding task description, code implementation, the layout after running the initialization script, and the layout after running the skill. We now detail how we operationalize our framework in Adobe Illustrator. We first execute the top-down simulation using the functionality categories listed in Appendix A, where the LLM explores tasks in each subcategory over three rounds. The validated skills, combined with sample scripts provided by the software, are then used to construct the API synergy graph. To train the GCN model for link prediction, we randomly split the sampled edges (both positive and negative) into training, development, and test sets with ratio of 0.85/0.05/0.1. Next, for each of the approximately 500 method APIs, we retrieve its top-k (k = 5) synergistic APIs and prompt the LLM to generate related tasks for one round. In each round of the task creation stage (both top-down and bottom-up approaches), the LLM generates ten tasks. For each task, the LLM is allowed up to three trials with previous feedback to generate the code of skills. We note that these hyperparameters may be subject to change based on cost considerations. We chose these values to ensure that the cost and size of the generated skillset remain manageable. Evaluation Setup To curate test set that represents the needs of real users, we iteratively prompt the LLM to generate useful tasks in Illustrator. We manually verify the tasks and correct any issues in the corresponding initialization scripts, if any, to enable automatic evaluation, resulting in 94 test tasks. We use the all-mpnet-base-v2 model from sentence BERT (Reimers and Gurevych, 2019) to encode the task descriptions for both test set and the skillset built during offline simulation. We use the test task description as the query and retrieve the most relevant skill from the skillset with semantic matching through cosine similarity. Baseline The baseline method for this task is runtime generation where an LLM generates code to solve the query during runtime. We use gpt-4o 2024-08-06 as the LLM for baseline, as well as the ones in the proposed framework for fair comparisons. Evaluation Metrics We evaluate the generated skillset by assessing its ability to solve the given tasks. Specifically, we report the Success Rate, which represents the proportion of tasks where an LVLM determines that the outcomes satisfy the provided task descriptions. This follows similar procedure outlined in Section 3.2. We further evaluate how reliable the LVLMs judgment is by comparing it with human judgment, the results are discussed in Section 5.3. We note that judging the actual outcome after execution is challenging. Previous work on LLM tool usage adopts simpler setting, e.g., only focus on tasks in question-answering format where answers can be easily verified (Mialon et al., 2024; Yuan et al., 2024) or only checking the correctness of tool calling (without checking the execution results) (Wang et al., 2024a). Additionally, because automating scripting interfaces requires consideration for user experiences and runtime cost, we report Response Time as the averaged seconds per task for the system to output script (either through retrieval or generation), along with averaged Token Cost during runtime."
        },
        {
            "title": "5.1 Effectiveness in Real-World Scenarios",
            "content": "Our proposed method demonstrates substantial improvement over the baseline in automating tasks within Adobe Illustrators scripting environment. As shown in Table 2, our approach achieves success rate of 44.7%, outperforming the baselines success rate of 28.7%, on the held-out test set. This evaluation setup effectively measures the end-toend performance of automation systems when deployed in real-world software contexts, with the natural language task descriptions in the test set simulating real user queries. key advantage of our approach lies in its markedly lower runtime costs, which is critical consideration given the potential volume and repetitive nature of user queries. Our approach demonstrates significantly lower response time by avoiding runtime code generation; instead, it retrieves solutions from pre-built skillset. Additionally, the proposed method requires zero runtime token cost, as the cost during offline simulation is one-off."
        },
        {
            "title": "These results highlight",
            "content": "the benefits of our method in terms of efficiency and resource utilization. By pre-generating repository of verified skills, we enable rapid retrieval and execution of automation tasks without the latency and cost associated with real-time LLM interactions. Success Rate Response Time Token Cost Baseline Ours 28.7% 44.7% 4.0 0.1 666 0* Table 2: Evaluation of the baseline and the proposed approach. * The token cost for offline simulation in our approach is one-off and not included in the runtime cost. Top-down Bottom-up Total Successful Skills Unsuccessful Skills Total 35.1% 40.4% 75.5% 9.8% 44.7% 14.9% 55.9% 24.4% 100% Table 3: Distribution of retrieved skills across op-down and bottom-up simulation strategies in the test set. 5.2 Effectiveness of Two Simulation Strategies Contribution to solving the test set. During offline simulation, we employed both top-down and bottom-up simulation strategies to capture wide range of functionalities within the softwares scripting environment. Table 3 presents the contributions of each strategy and their combined impact when tackling tasks in the test set. Top-down simulation accounts for approximately 75% of the skills retrieved. While the top-down strategy has higher overall contribution, the bottom-up strategy still plays significant role. Its important to note that the test set curation and the top-down simulation both involve prompting the LLM to generate useful tasks at high level of abstraction based on the softwares functionalities. This similarity potentially gives the top-down approach an advantage in matching test tasks. On the other hand, the bottom-up simulation, which prompts the LLM with low-level API information, is designed to cover long-tailed, less obvious skills. Despite this, the bottom-up simulation still achieves comparable success rates relative to their contributions (top-down: 35.1% 75.5% = 46.5%, bottom-up: 9.8% 24.4% = 40.2%) to the test set that predominantly contains head-tail tasks the most helpful tasks as determined by the LLM. Bottom-up simulation uses more APIs but coverage remains incomplete. To further demonstrate that the bottom-up approach is effective in exploring the long-tail distribution of skills within the softwares capabilities, Table 4 shows skills from bottom-up simulation cover 151 unique APIs, significantly more than the 49 APIs covered by the top-down approach and the 48 from sample scripts Sample Scripts Top-down Bottom-up Semantic matching Link Prediction # APIs 48 49 151 Hit@ 16.7% 37.3% Table 4: Number of API endpoints from Illustrators inherit sample scripts and skills built from two simulation strategies. Table 5: Comparison between the link prediction model and semantic matching method for identifying synergistic APIs. provided by the software. This broader API coverage indicates that the bottom-up approach is successful in exploring wide range of functionalities beyond the more commonly used APIs. However, as mentioned in Section 3.1, the bottom-up simulation iterated through approximately 378 method APIs. The fact that only 151 of these APIs resulted in successfully validated scripts suggests that many APIs did not have any corresponding successful tasks or scripts. We posit that the main reason for this limitation is the employed general-purpose LLMs limited knowledge of Adobe Illustrators extensive API library, resulting in LLMs failure to generate relevant tasks or to produce code with correct API usage. This outcome is expected, given the complexity and long-tail nature of software scripting API design. The long tail represents numerous specialized APIs that are infrequently used and therefore less likely to be well-represented in the LLMs training data. This highlights room for future improvement, which we discuss in Section 6. Qualitative Analysis To better understand the differences in skills between the two simulation strategies, we can use skills related to the functionality of arrange objects as an example. The top-down simulation produces skills such as arranging selected objects in circle  (Table 1)  , as well as other common patterns like zigzag or starburst formations. hese skills align with typical design tasks and provide straightforward implementations of the \"arrange objects\" functionality. Meanwhile, the bottom-up simulation produces skills that utilize more specialized APIs and offer advanced functionalities. For example, it might generate skill that arranges selected objects based on custom tags assigned to them. This allows for dynamic arrangements driven by metadata, which is expected to be rarer in practical usage but provides greater flexibility for advanced users seeking more control over their design elements. Synergy Modeling of APIs in Bottom-up Simulation During bottom-up simulation, we use # Tasks %Success@1 %Success@3 Top-down Bottom-up 1721 3256 16.7% 23.1% 34.9% 46.6% Table 6: Success rate at the first and third trials during simulations. GNN-based link prediction model to model the synergy of APIs. We show that this model can find synergistic APIs more effectively, compared to semantically matching similar APIs. We evaluate the performance of the GCN model in retrieving synergistic APIs for task creation using the Hit@k metric. In this context, Hit@k evaluates, for each API, how often the correct items are within the top-k predicted items where correct items refer to APIs in existing skills (not in the training set). We compare the link prediction model with baseline method-semantic matching. For each API, we retrieve top-k similar APIs based on the semantic representation of their descriptions. Similarly, we use all-mpnet-base-v2 for semantic representation and cosine similarity as the similarity measure. Table 5 shows our link prediction model significantly outperforms the semantic matching method in identifying synergistic APIs, demonstrating its effectiveness in enhancing task creation during the bottom-up simulation. Generation with trials is key driver of the improved success rate for both simulation strategies. One key advantage of our offline simulation is the ability to perform multiple trials during code generation without incurring runtime penalties. We analyzed the impact of allowing up to three trials for code generation on the overall success rate. Table 6 demonstrates that the success rate nearly doubles after three trials for both top-down and bottom-up strategies. Specifically, the success rate for top-down skills increases from 16.7% after the first trial to 34.9% after three trials. For bottom-up skills, the success rate rises from 23.1% to 46.6%. These results underscore the importance of iterative code generation in enhancing the quality and reliability of the generated scripts. Success (LVLM) Fail (LVLM) Success (Human) Fail (Human) 40 (TP) 4 (FP) 10 (FN) 68 (TN) Table 7: Confusion matrix between human and LVLM judgments. 5.3 Reliability of LVLMs judgment Assessing the success of generated scripts requires reliable verification. We employed an LVLM to evaluate whether the execution outcomes satisfy the task descriptions. To estimate the reliability of the LVLMs judgments, we compared its assessments with human evaluations on sample of 122 tasks. The confusion matrix in Table 7 shows that the LVLM has precision of 90.9% and recall of 80.0%. Specifically, out of 50 tasks deemed successful by humans, the LVLM correctly identified 40 as successful (true positives) and 10 as failures (false negatives). Meanwhile, the LVLM only incorrectly labeled 4 tasks as successful when they were not (false positives). These results indicate that the LVLM is generally reliable, albeit slightly conservative in its judgments."
        },
        {
            "title": "6 Future Work",
            "content": "This study is one of the first to utilize software scripting interfaces as testbed for LLM-based systems. This setup offers the advantage of reliable and immediate execution feedback in controlled environment, which is often challenging to obtain in contexts where API interactions involve multiple external services. Despite the promising results, there is considerable scope for enhancing our methods effectiveness. One direction for future work is to enrich LLMs knowledge of specific software. Our current skillset does not fully exploit the entire range of available APIs, especially those that are less frequently used or more complex. Training domainspecific LLM tailored to the software through iterative trial and error could generate more accurate and comprehensive skills, thereby enhancing the systems overall performance. Another avenue for future research is leveraging real-world usage data. Our skillset is currently derived from static documentation rather than live usage patterns. Mining API interactions from actual scripting projects could provide valuable insights into common user behaviors and task patterns, making the generated skills more applicable in practice. However, obtaining such data is challenging and raises interesting questions about what kinds of behavioral data are useful for mining such patterns and how to collect them responsibly. Additionally, integrating user feedback loops would allow for continuous refinement of skill discovery and adaptation, ensuring that the system evolves in alignment with user needs. While our method is tailored for software scripting automation, it has the potential to be applied to other API orchestration scenarios. Extending this approach to different software environments or domains could uncover new challenges and opportunities. Applying our offline simulation framework to other applications could help validate its effectiveness and adaptability, potentially bridging the gap between AI capabilities and user needs in specialized domains."
        },
        {
            "title": "7 Conclusions",
            "content": "In this work, we proposed an offline simulation framework for skill discovery in software scripting automation using LLMs. Unlike prior approaches that rely on runtime code generation, our method pre-build diverse and verified skillset in two phases: (1) task creation, guided by both top-down functionality categories and bottom-up API synergy exploration, and (2) skill generation with trials, where LLM-generated scripts are iteratively refined based on execution feedback. Our experiments using Adobe Illustrator as testbed demonstrated the effectiveness of this approach, achieving significantly higher success rate while largely reducing response time and token cost by avoiding runtime generation. The topdown task creation strategy encourages broad functional coverage, while the bottom-up API synergy modeling explores the of discovered skills, particularly for underutilized API endpoints. Furthermore, our GCN-based API link prediction model outperformed naive semantic matching in identifying synergistic API pairs, enhancing task creation during the bottom-up simulation. To the best of our knowledge, this is the first attempt to use software scripting interfaces as testbed for LLM-based systems, highlighting the advantage of obtaining direct execution feedback in controlled environment. We identified several future directions, including enhancing LLMs knowledge of specific software, leveraging realworld usage data, and generalizing the approach to other API orchestration scenarios. We hope that this work not only provides insights into improving software scripting interfaces but also contributes to bridging the gap between AI capabilities and user needs in specialized domains. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712."
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results of our offline simulation framework for skill discovery in software scripting automation, there are several limitations to consider beyond those discussed in Section 6. First, our method relies heavily on publicly available scripting guides and the existing knowledge of LLMs about the software. In cases where such documentation is incomplete, outdated, or unavailable, or when LLMs have limited knowledge of the software, the effectiveness of our approach may be reduced. This dependence may restrict the applicability of our framework to well-documented software systems and those familiar to the LLMs. Second, our evaluation primarily focuses on automated success rates and performance metrics within controlled environment. While these metrics provide valuable insights into technical performance, they do not capture user experience or satisfaction. Future studies could include user evaluations to assess the usability and usefulness of the generated skills with real users. Third, we expect that queries from real users may be more complex and not always match the granularity of the skills in our skillset. Real-world user queries might be ambiguous or involve higherlevel tasks that require combining multiple skills or adapting existing ones. The skillset generated by our framework does not account for individual user preferences or specific workflow requirements. As result, while our skillset covers broad range of tasks, it may not fully address the unique needs of all users, limiting its adaptability and personalization."
        },
        {
            "title": "References",
            "content": "Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. arXiv preprint arXiv:2407.04363. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2024. Large language models as tool makers. In The Twelfth International Conference on Learning Representations. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, et al. 2024. Spider2-v: How far are multimodal agents from automating data science and engineering workflows? arXiv preprint arXiv:2407.10956. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36. Apurva Gandhi, Thong Nguyen, Huitian Jiao, Robert Steen, and Ameya Bhatawdekar. 2023. Natural language commanding via program synthesis. arXiv preprint arXiv:2306.03460. Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. 2024. Assistgui: Task-oriented pc graphical user interface automation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13289 13298. Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Zhengkang Zuo, Changjing Wang, and Xin Xia. 2022. 1+ 1> 2: Programming know-what and know-how knowledge fusion, semantic enrichment and coherent application. IEEE Transactions on Services Computing, 16(3):15401554. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36. Thomas Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations (ICLR). Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and ZHAO-XIANG ZHANG. 2024. Sheetcopilot: Bringing software productivity to the next level through large language models. Advances in Neural Information Processing Systems, 36. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2024a. Explore then determine: gnn-llm synergy framework for reasoning over knowledge graph. arXiv preprint arXiv:2406.01145. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024b. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024a. Agentboard: An analytical evaluation board of multi-turn llm agents. arXiv preprint arXiv:2401.13178. Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, and Jie Tang. 2024b. Spreadsheetbench: Towards challenging real world spreadsheet manipulation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. Creator: Tool creation for disentangling abstract and concrete reasoning of large In Findings of the Association language models. for Computational Linguistics: EMNLP 2023, pages 69226939. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. 2024a. LLMs in the imaginarium: Tool learning through simulated trial and error. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1058310604, Bangkok, Thailand. Association for Computational Linguistics. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2024b. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. arXiv preprint arXiv:2404.07972. Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, et al. 2024. Dynasaur: Large language agents beyond predefined actions. arXiv preprint arXiv:2411.01747. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations. John Ousterhout. 1998. Scripting: Higher level programming for the 21st century. Computer, 31(3):23 30. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2024. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506. Lianyong Qi, Wenmin Lin, Xuyun Zhang, Wanchun Dou, Xiaolong Xu, and Jinjun Chen. 2022. correlation graph based approach for personalized and compatible web apis recommendation in mobile app development. IEEE Transactions on Knowledge and Data Engineering, 35(6):54445457. Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. 2024. Craft: Customizing llms by creating and retrieving from specialized toolsets. In The Twelfth International Conference on Learning Representations. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the perspectives of nlp and software engineering: survey on language models for code. arXiv preprint arXiv:2311.07989. Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao Wan, Hongyu Zhang, Yulei Sui, and Haidong Zhang. 2024a. NL2Formula: Generating spreadsheet formulas from natural language queries. In Findings of the Association for Computational Linguistics: EACL 2024, pages 23772388, St. Julians, Malta. Association for Computational Linguistics. Xufeng Zhao, Cornelius Weber, and Stefan Wermter. 2024b. Agentic skill discovery. arXiv preprint arXiv:2405.15019."
        },
        {
            "title": "A Adobe Illustrator Functionalities",
            "content": "Part of the functionality categories in Illustrator are shown in Table 8."
        },
        {
            "title": "B Prompts",
            "content": "In this section, we show system prompts and user prompts for task creation, skill generation with trials, and LVLM-based validation in Table 9, Table 10, and Table 11, respectively. Category Subcategories Drawing Color Painting Select and arrange objects Reshape objects Import, export, and save Type Create special effects Web graphics Printing Drawing basics, Draw pixel-perfect art, Edit paths, . . . Adjust Colors, Select Colors, Use the Adobe Color Themes panel, . . . Gradients, Paint with fills and strokes, Brushes, . . . Select objects, Move and align objects, Layers, . . . Crop images, Transform objects, Puppet Warp, . . . Save artwork, Export artwork, Package files, . . . Create text, Using fonts in Illustrator, Format paragraphs, . . . Work with effects, Graphic styles, Drop shadows, . . . Best practices, Create animations, SVG, . . . Set up documents, Print with color management, Overprint, . . . Table 8: Illustrator Functionality Overview System Prompt for Task Creation in Illustrator You are an expert user for Adobe Illustrator. Your goal is to generate as many tasks as possible that are helpful and represent common needs for Illustrator users. The generated tasks should follow the following criteria: 1. Describe these tasks so that they can be coded into script. 2. The tasks should not be already implemented in Illustrator. 3. The tasks should be minimally dependent on the content. Generate the tasks in plain text. Each task takes one line. Do no generate any other information such as numbering. User Prompt for Task Creation with Top-down Simulation Give me 10 most useful tasks related to {subcategory} under the category of {category}, in Adobe Illustrator. Examples of successful tasks in the previous rounds include: {a list of successful task descriptions from previous round under the same category} User Prompt for Task Creation with Bottom-up Simulation Give me 10 most useful Adobe Illustrator tasks related to {api} whose description is: {Names and descriptions of top synergistic APIs} Take inspiration from the following APIs and their descriptions by considering the possibility of using {api} with at least one of the following APIs. Its okay to not use them as long as the tasks related to {api} are useful. {top_nodes_info} The generated tasks should follow the following guidelines: - Make sure the tasks are reusable. - The tasks should be logical and reasonable to use two or more APIs together. - The generated task should not simply be concatenation of two API nodes, i.e., do task and do separate task that doesnt closely depend on task A. - Prioritize the usefulness of the tasks over generating exactly 10 tasksfewer, high-quality tasks are acceptable. Table 9: Prompts for Task Creation in Illustrator System Prompt for Code Generation You are an assistant generating ExtendScript code for Adobe Illustrator. You will be provided query that attempts to perform an action in Illustrator. Return only the ExtendScript code snippet without additional messages, formatting, or markdown. Initialize document to simulate this code. Generate the initialization code and task code separately in the following JSON format: {\"init_code\": INITIALIZATION_CODE, \"code\": CODE, \"code_name\": \"[brief description]\"} The code must follow these rules: 1. Do not use alert. Return messages for stdout. 2. If the task is not feasible, return {\"code\": \"\"}. 3. Start error messages with \"Error: \". 4. Include necessary initialization for selecting objects. 5. Ensure reusability of task code. 6. Call the function you create to execute the task. 7. Keep the initial layout minimal for clear visual results. 8. Do not crash Illustrator. Table 10: System Prompt for ExtendScript Code Generation System Prompt for Skill Validation You will be given task description, piece of initialization code, layout after running the initialization code, piece of task code, and layout after running the task code. The context for the task is Adobe Illustrator. Your job is to judge whether the task was performed correctly or not, given the task description and the two layout figures. The difference between the two layout figures should reflect the result of running the task code. Sometimes, the failure reason can be that the initialization code does not generate necessary elements for the task code to run correctly, or the task code does not perform the task correctly. The output should be in JSON format: {\"valid\": true/false, \"reason\": [brief reason for the judgment], \"suggestion\": [brief suggestion for improvement]} Ensure the output contains no additional messages, formatting, or markdown, so that it can be directly parsed by json.loads(). User Prompt for Validation with LVLM Task description: {task_description} Initialization code: {init_code} Task code: {code} {init_layout.png} {outcome_layout.png} Table 11: Prompts for Skill Validation"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Maryland, College Park"
    ]
}