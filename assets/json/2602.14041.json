{
    "paper_title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
    "authors": [
        "Yuang Ai",
        "Jiaming Han",
        "Shaobin Zhuang",
        "Weijia Mao",
        "Xuefeng Hu",
        "Ziyan Yang",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance."
        },
        {
            "title": "Start",
            "content": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens Yuang Ai1,2,4, Jiaming Han1,2, Shaobin Zhuang1,3, Weijia Mao1,5, Xuefeng Hu1 Ziyan Yang1, Zhenheng Yang1, Huaibo Huang4, Xiangyu Yue2, Hao Chen1 1ByteDance, 2MMLab, The Chinese University of Hong Kong, 3Shanghai Jiao Tong University 4Institute of Automation, Chinese Academy of Sciences, 5National University of Singapore Equal contribution, Corresponding Author, Project lead"
        },
        {
            "title": "Abstract",
            "content": "We present BitDance, scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2256 states, yielding compact yet highly expressive discrete representation. Sampling from such huge token space is difficult with standard classification. To resolve this, BitDance uses binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4 fewer parameters (260M) and achieving 8.7 speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 10241024 images, BitDance achieves speedup of over 30 compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code: https://github.com/shallowdream204/BitDance (cid:128) Project Page: https://bitdance.csuhan.com 6 2 0 2 5 1 ] . [ 1 1 4 0 4 1 . 2 0 6 2 : r Figure 1 Performance vs. efficiency compared with SOTA diffusion models and autoregressive models. 1 Figure 2 High-resolution samples generated by the 14B BitDance model, showcasing its capabilities in prompt adherence, spatial reasoning, and text rendering across various aspect ratios and artistic styles."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have shown that autoregressive pre-training via next-token prediction can distill large-scale textual knowledge into single model with strong generalization [1, 14, 27]. This success has motivated extending AR modeling to visual generation [43, 61, 66], yielding unified paradigm with language modeling and enabling progress on multimodal applications [21, 63, 82]. However, applying the AR paradigm to vision still faces persistent challenges. primary issue is token design: visual tokens must be expressive enough to capture rich image content while remaining sufficiently regularized to limit error accumulation over long sequences. Existing discrete AR models typically leverage Vector Quantization (VQ) [19, 67] for tokenization, but often suffer from degraded reconstruction due to difficulties in scaling visual vocabularies [61, 66, 68]. In contrast, continuous AR models use VAEs to achieve superior reconstruction, the unconstrained nature of their latent spaces often leads to severe error accumulation [36, 56, 63]. Furthermore, the sequential nature of token-by-token generation imposes significant bottleneck on inference efficiency, particularly as image resolutions scale upward. In this paper, we present BitDance, simple yet scalable autoregressive framework that achieves state-of-the-art performance in image generation. BitDance is built upon three key components: (i) large-vocabulary binary tokenizer, (ii) binary diffusion head for sampling in extremely large discrete spaces, and (iii) next-patch diffusion paradigm that enables efficient multi-token prediction. Inspired by recent advances in binary quantization [79, 83], we scale the entropy of binary representations, expanding the vocabulary size up to 2256. This scale is orders of magnitude larger than that of existing discrete tokenizers [29, 61], enabling our discrete representation to surpass continuous VAEs in reconstruction fidelity (see Tab. 1). By encoding images into compact yet expressive binary latent space, BitDance preserves finegrained visual details while introducing beneficial discreteness that helps regularize long-sequence generation and mitigate error accumulation. However, such an expansive vocabulary poses severe challenges for conventional sampling. As shown in Fig. 3, index-based classification heads suffer from an inherent trade-off between parameter efficiency and sampling accuracy. To address this issue, we propose binary diffusion head. Rather than mapping binary tokens to discrete indices, we embed binary tokens as vertices of hypercube in continuous space and model their distribution using diffusion objective [33]. By jointly modeling all binary channels, the proposed head enables accurate sampling while maintaining controllable parameter footprint. To further accelerate vision token prediction, we propose next-patch diffusion approach for visual autoregressive modeling. Considering the spatial dependency in images, we posit that tokens within local patch are highly correlated, making them easier to predict jointly. As shown in Fig. 5, unlike prior parallel AR generation methods [41, 52, 69] that use factorized samplingsampling each token independently via classification headBitDance uses binary diffusion head to explicitly model the joint distribution of tokens generated in parallel. This design enables high-fidelity parallel token prediction and substantially improves efficiency. BitDance demonstrates superior performance in both class-conditional and text-to-image generation. On ImageNet 256256, the 1B BitDance model achieves an FID of 1.24, outperforming previous AR models. As shown in Fig. 1, BitDance outperforms 1.4B state-of-the-art parallel AR generative model using only 260M parameters, while achieving an 8.7 speedup. For text-to-image generation, we scale BitDance to 14B parameters and train on large-scale multi-modal tokens. After pre-training, continued training, and supervised fine-tuning, BitDance exhibits strong text-to-image synthesis capabilities, generating high-resolution images with fine-grained details. Specifically, BitDance achieves scores of 0.86 on GenEval [25], 88.28 on DPG-Bench [34], 0.532 on OneIG-EN [7], and 0.512 on OneIG-ZH [7], setting new state-of-the-art among existing autoregressive models. As shown in Fig. 1, our model achieves speedup of over 30 when generating 10241024 images, compared to standard next-token prediction AR models (NextStep-1 [63] and Emu3.5 [15]). Our main contributions can be summarized as follows: We present BitDance, simple and scalable autoregressive model. BitDance demonstrates the viability of scaling token entropy for high-fidelity visual generation, offering new insights into the design space of visual autoregressive modeling. 3 Figure 3 Comparison of binary token sampling paradigms. Scaling up binary token entropy yields reconstruction performance on par with continuous VAEs, but it simultaneously creates bottleneck during sampling. For d-channel binary token: (a) Directly modeling p(b1, b2, . . . , bd) requires 2d parameters, which suffers from an exponential explosion as scales. (b) Bit-wise classification [29] reduces the parameter count to h2d by assuming bit independence, i.e., (cid:81)d i=1p(bi), but this restrictive assumption compromises sampling fidelity. (c) We embed binary tokens as vertices of d-dimensional hypercube in continuous space. By modeling the joint distribution of all bits via diffusion objective, we achieve controllable parameters as scales up and high-fidelity sampling. We propose binary diffusion head to resolve the sampling bottlenecks inherent in expansive visual vocabularies. By seamlessly extending it to multi-token sampling, BitDance facilitates precise and efficient parallel prediction through next-patch diffusion paradigm. Extensive experiments on class-conditional and text-to-image generation show that BitDance offers exceptional scaling, achieving superior generative quality with high inference speed."
        },
        {
            "title": "2.1 Visual Tokenizers",
            "content": "To mitigate the prohibitive costs of pixel-space training [11, 33], Variational Autoencoders (VAEs) [37] are widely used to project visual content into continuous latent spaces. Due to their high-fidelity reconstruction and smooth gradient flow, VAEs have become the standard for leading diffusion models [40, 53, 57, 74]. Conversely, discrete tokenizers using Vector Quantization (VQ) [19, 67] often struggle with quantization errors and instability of codebook utilization. Recent works have increasingly investigated binary quantization as solution. MAGVIT-v2 [79] introduces Lookup-Free Quantization (LFQ) to scale vocabularies to 218, but its entropy loss incurs linear memory costs that hinder further scaling. Subsequent works like BSQ [83] and WeTok [86] attempt to resolve this via independence assumptions or grouping strategies, respectively. Building on these advances, we aim to further explore scaling up the vocabulary size to achieve higher token entropy."
        },
        {
            "title": "2.2 Autoregressive Viusal Generation",
            "content": "Standard autoregressive (AR) visual generation typically quantizes images into discrete tokens and models their distribution via next-token prediction in raster-scan order [15, 61, 68]. Recently, several paradigm shifts have emerged, exploring continuous token spaces [43, 55], randomized ordering [41, 52, 80], and alternative modeling primitives [56, 66]. For instance, MAR [43] introduces token-level diffusion head to facilitate continuous token sampling, while NextStep-1 [63] scales up the continuous AR framework for high-fidelity text-to-image synthesis. However, continuous tokens often lack sufficient regularization, leading to severe error accumulation and representation drift [56, 62] during long-sequence generationfactors that significantly degrade the quality of high-resolution images. SphereAR [36] employs hyperspherical constraints to regularize the latent features of VAEs. In this work, we focus on discrete binary representations to explore the potential 4 of this more radical constraint in autoregressive visual generation."
        },
        {
            "title": "2.3 Parallel Prediction in AR Models",
            "content": "Accelerating AR generation has become pivotal research direction in visual generation. Mask-GIT [6] and MAR [43] adopt MAE-style masking strategies for modeling. VAR [66] utilizes next-scale prediction to predict tokens within unified scale in parallel. PAR [69] employs grouping strategy to generate weakly dependent tokens. RandAR [52] and ARPG [41] leverage random-order modeling to enable the prediction of tokens at arbitrary positions. While showing promise, these methods often struggle to model the joint distribution of tokens predicted in parallel, lacking sufficient multi-token constraints during the final sampling stage (see Fig. 5). Our BitDance seamlessly extends the binary diffusion head to model the joint distribution of multi-token, achieving efficient and reliable parallel prediction through next-patch diffusion."
        },
        {
            "title": "3.1 Binary Visual Tokenizer",
            "content": "For discrete visual tokenizers, scaling up the vocabulary size to increase token entropy is critical for enhancing both reconstruction fidelity and downstream generation quality. However, traditional Vector Quantization (VQ) often encounters codebook collapse as the vocabulary expands. To ensure efficient utilization of the codebook space, we adopt binary quantization via Lookup-Free Quantization (LFQ) [79]. Given an encoded latent token Rd, LFQ employs an implicit, learning-free codebook CLF = {1, 1}d. The binary quantization process is performed independently across each channel of x: xq = sign(x). (1) To prevent codebook collapse and maximize information capacity, an entropy loss [35] is typically employed: Lentropy = E[H(q(x)]) H[E(q(x))], (2) where H() denotes the entropy calculation. In standard LFQ, computing the distribution q(x) requires calculating similarities between and the entire codebook space. As the vocabulary size grows exponentially (2d), this calculation becomes computationally prohibitive due to its significant memory footprint. To address this bottleneck, we adopt group-wise LFQ strategy [86], which partitions channels into distinct groups for entropy calculation. This strategy strikes balance between computational efficiency and optimization accuracy, enabling the training of models with an exceptionally large vocabulary. Specifically, we scale the codebook size up to 2256. The vast representation space allows our discrete tokenizer to achieve reconstruction fidelity comparable to continuous VAEs (see Tab. 1)."
        },
        {
            "title": "3.2 Binary Diffusion Head",
            "content": "While large vocabulary increases token entropy and reconstruction fidelity, it introduces challenges for sampling. For binary token with channels, the number of possible discrete indices grows exponentially to 2d. As illustrated in Fig. 3, conventional index-based classifiers face an inherent trade-off between sampling precision and parameter efficiency. The Sampling Bottleneck. To model the joint probability p(b1, b2, . . . , bd) of d-bit token, standard classification head must characterize categorical distribution across all 2d possible indices. This approach incurs prohibitive parameter overhead of 2d, where denotes the hidden dimension. As scales, this becomes computationally intractable; for instance, with = 1024 and = 32, the parameter count reaches approximately 4.4 trillion, far exceeding the capacity of modern hardware. To address this, [29] adopts bit-wise independence assumption, decomposing the joint distribution into separate binary classifications. By modeling the distribution as product of independent probabilities, (cid:81)d i=1p(bi), the approach effectively reduces the parameter overhead to 2d. However, this assumption is overly restrictive, as it fails to capture the intricate inter-bit correlations in binary tokens, leading to degraded sampling precision and inferior generative quality (see Tab. 13). 5 Figure 4 Architecture of BitDance, an autoregressive model trained on multi-modal tokens. An input image is first encoded into binary latents and then flattened into 1D sequence following patch-wise raster scan order with patch size p. Vision tokens are modeled using our proposed next-patch diffusion, utilizing binary diffusion head to achieve efficient and precise parallel prediction. Diffusion-based Binary Prediction. We propose novel binary diffusion head to address these sampling difficulties. Unlike conventional applications of diffusion models that primarily focus on modeling continuous data distributions, we are the first to leverage such models for discrete binary tokens. Rather than mapping these tokens to discrete indices, we represent them as d-dimensional vectors within continuous space. Specifically, let Rd denote the ground-truth binary token to be predicted and Rh represent the condition (i.e., the hidden state from the AR transformer). To model the conditional probability distribution p(xz), we adopt the Rectified Flow [45] formulation and optimize the head using x-prediction with velocity-matching loss [42]: L(z, x) = Et,x,ϵ vθ(xt, t, z) vt2 , where xt = tx + (1 t)ϵ is the noisy token at time interpolated with Gaussian noise ϵ (0, I), and vt = ϵ is the target velocity. The velocity vθ is parameterized by x-prediction network fθ such that vθ(xt, t, z) = (fθ(xt, t, z) xt)/(1 t). During inference, we initialize x0 (0, I) and integrate the learned velocity field using an Euler solver with uniform steps = 1/N : (3) xt+t = xt + vθ(xt, t, z)t. After steps, we apply hard binarization constraint: x1 = sign(x1). This step effectively projects the continuous prediction back onto the binary hypercube, exploiting the structural priors of the token space. This mechanism circumvents the compounded error accumulation typically found in AR models that operate in unconstrained continuous spaces [36, 63]. (4) Why Binary Diffusion Head is Effective. Geometrically, binary tokens in continuous space form finite set of vertices on hypercube, characterized by uniform magnitudes but distinct orientations. Unlike standard VAE latent spaces which are often unconstrained, the binary target space provides strong structural regularization. This restriction to finite, well-defined set of target vectors significantly reduces the optimization complexity for the diffusion head, leading to faster convergence and superior sampling stability (see Tab. 12). Fig. 9 illustrates the output distribution of the binary diffusion head. It reveals that the model effectively learns the discrete nature of binary tokens without any manual constraints. Figure 5 Comparison of different sampling heads for parallel prediction in autoregressive models. (a) The standard classification head is limited to independent token sampling, which violates the inherent dependencies required for parallel prediction. (b) Our proposed binary diffusion head models the joint distribution of tokens generated simultaneously, enabling coherent sampling."
        },
        {
            "title": "3.3 Next-Patch Diffusion",
            "content": "Standard autoregressive (AR) generation formulates image synthesis as sequence of next-token predictions. Formally, given flattened 1D sequence of tokens = [x1, x2, . . . , xN ], the generative process is decomposed into chain of conditional probabilities: p(x) = (cid:89) n=1 p(xnx1, x2, . . . , xn1). (5) While effective, this token-by-token paradigm imposes significant inference bottleneck, particularly as image resolutions and sequence lengths increase. From Next-Token to Next-Patch. To alleviate this, we exploit the inherent spatial dependency in visual data. We posit that tokens within local patch exhibit strong statistical dependencies and can thus be predicted in parallel without compromising generative quality. In our next-patch prediction scheme, the sequence is m, . . . , xp2 partitioned into disjoint groups (patches), = [X1, X2, . . . , XM ], where each group Xm = {x1 } contains tokens. The parallel AR generation can be expressed as: m, x2 p(x) = (cid:89) m= p(Xm X1, . . . , Xm1). (6) As illustrated in Fig. 4, we adhere to patch-wise raster-scan order. To implement this within the AR Transformer, we employ block-wise causal attention mask. Unlike standard causal masks that enforce strict 1D dependency, our block-wise configuration allows tokens within the same patch to be mutually visible. This enables the model to explicitly capture intra-patch spatial interactions while maintaining the autoregressive dependency across patches. Besides, to facilitate the simultaneous prediction of all tokens in the initial patch, we introduce p2 1 learnable prefix tokens as placeholders before vision tokens. Precise Multi-token Sampling. While recent parallel AR frameworks [41, 52, 69] have introduced various modeling trajectories and specialized attention mechanisms, we contend that their efficacy is fundamentally constrained by training-inference discrepancy in the parallel AR objective. Specifically, these methods still adhere to the token-wise training objectives defined in Eq. (5). As shown in Fig. 5, this creates critical mismatch during inference: while the model is tasked with generating group of tokens simultaneously, the tokens within that group are sampled independently through standard classification head. Such an implicit independence assumption directly violates the joint distribution modeling necessitated by Eq. (6), inevitably compromising structural coherence and introducing artifacts within the generated groups. To bridge this gap, we extend the binary diffusion head introduced in Sec. 3.2 to support joint multi-token prediction. By leveraging the flexibility of the diffusion formulation, we adapt the training objective in Eq. (3) to multi-token. Let Rp2d represent the patch of ground-truth tokens and Rp2h denote the corresponding hidden states. The optimization objective becomes: Lparallel = Et,X,ϵ vθ(Xt, t, Z) vt 2. (7) 7 Table 1 Reconstruction on ImageNet 256256 validation set. All models are trained on general domain datasets. By scaling token entropy, our discrete tokenizer outperforms continuous VAEs, even at higher compression ratios. Method SD-VAE [57] Cosmos [2] Show-o [75] LlamaGen [61] Open-MAGVIT2 [47] Infinity [29] BitDance-Tok (Ours) WeTok [86] DC-AE [10] DC-AE-SANA [73] BitDance-Tok (Ours) BitDance-Tok (Ours) Tokenizer Type Continuous Discrete Discrete Discrete Discrete Discrete Discrete"
        },
        {
            "title": "Discrete\nContinuous\nContinuous\nDiscrete\nDiscrete",
            "content": "Downsample Ratio 8 16 16 16 16 16 16 Codebook Size - 65536 8192 16384 218 232 232 Compression Ratio 24 384 473 439 341 192 192 32 32 32 32 32 232 - - 2128 2256 768 48 48 192 PSNR SSIM 23.54 19.93 21.34 20.65 22.70 22.70 24.90 20.77 24.81 24.72 23.26 25.29 0.68 0.49 0.59 0.54 0.64 - 0.72 0.55 0.69 0.69 0. 0.74 Figure 6 Generative performance across different vocabulary sizes and Transformer sizes. For large vocabularies, small Transformers struggle to converge. Scaling the vocabulary size requires concurrent expansion of the Transformer size. To effectively model the p2 tokens within the head, we design the architecture of prediction network fθ as lightweight DiT [53]. By aligning the training objective with the diffusion heads capacity for joint distribution modeling, BitDance achieves efficient and accurate parallel prediction. For class-conditional generation, BitDance is optimized solely using Eq. (7). For text-to-image synthesis, where the Transformer is initialized from an LLM, we additionally incorporate standard cross-entropy loss on text tokens to preserve the models text-understanding capabilities."
        },
        {
            "title": "4.1 Scaling up Token Entropy",
            "content": "We first investigate the impact of scaling token entropy on the reconstruction performance of our visual tokenizer. To this end, we evaluate three distinct configurations: (i) 16 downsampling ratio with codebook size of 232, (ii) 32 downsampling ratio with codebook size of 2128, and (iii) 32 downsampling ratio with codebook size of 2256. We utilize DataComp-1B [22] as our main training set, supplemented with high-quality face and text datasets to improve domain-specific reconstruction. Training is conducted at 256256 resolution for 400K steps using batch size of 1024. Following previous works [47, 86], the tokenizer adopts pure CNN architecture, enabling generalization on various resolutions. Scaling Token Entropy for Reconstruction. Tab. 1 compares the performance of tokenizers under diverse downsampling and compression settings. For continuous tokenizers, we assume that latent features are stored in the commonly used bfloat16 format to calculate the compression ratio. Token entropy scaling markedly improves discrete reconstruction accuracy, bridging the gap with continuous models. The results show that with 232 codebook, our 16 tokenizer outperforms the continuous SD-VAE [57]. Similarly, for 32 downsampling, scaling the codebook from 2128 to 2256 allows our discrete approach to exceed the 8 Table 2 Evaluation of class-conditional image generation on ImageNet 256256. We mark models using additional self-supervised models (e.g., DINO) in gray. BitDance achieves superior performance while employing standard AR modeling with raster-order. Model Type Order Tokenizer #Params FID IS Pre. Rec. Continuous Tokens DiT-XL/2 [53] SiT-XL/2 [48] DiCo-XL [3] MDTv2 [23] REPA [81] RAE [84] MAR-B [43] MAR-L [43] MAR-H [43] SphereAR-B [36] SphereAR-L [36] SphereAR-H [36] xAR-B [56] xAR-L [56] xAR-H [56]"
        },
        {
            "title": "Discrete Tokens",
            "content": "Diff. Diff. Diff. Mask.+Diff. Diff. Diff. Mask. Mask. Mask. AR AR AR AR+Diff. AR+Diff. AR+Diff. AR LlamaGen-L [61] AR LlamaGen-XL [61] AR LlamaGen-XXL [61] AR RandAR-L [52] AR RandAR-XL [52] AR RandAR-XXL [52] AR RAR-L [80] AR RAR-XL [80] AR RAR-XXL [80] OpenMAGVIT2-XL [47] AR MAGVIT-v2 [79] VAR-d20 [66] VAR-d30 [66] BitDance-B-1x BitDance-L-1x BitDance-H-1x Mask. VAR VAR AR AR AR - - - - - - random random random raster raster raster raster raster raster raster raster raster random random random hybrid hybrid hybrid raster random - - raster raster raster"
        },
        {
            "title": "VQ\nVQ\nVQ\nVQ\nVQ\nVQ\nVQ\nVQ\nVQ\nLFQ\nLFQ\nVQ\nVQ\nLFQ\nLFQ\nLFQ",
            "content": "675M 2.27 675M 2.06 701M 2.05 675M 1.58 675M 1.42 675M 1.13 208M 2.31 479M 1.78 943M 1.55 208M 1.92 479M 1.54 943M 1.34 172M 1.72 608M 1.28 1.1B 1.24 343M 3.07 775M 2.62 1.4B 2.34 343M 2.55 775M 2.22 1.4B 2.15 461M 1.70 955M 1.50 1.5B 1.48 804M 2.51 307M 1.78 600M 2.57 2B 1.92 242M 1.68 527M 1.31 1.0B 1.24 278.2 277.5 282.2 314.7 305.7 262.6 281.7 296.0 303.7 277.8 295.9 300.0 280.4 292.5 301.6 256.1 244.1 253.9 288.8 314.2 322.0 299.5 306.9 326.0 271.7 319.4 302.6 323.1 297.1 300.2 304.4 0.83 0.83 0.83 0.79 0.80 0.78 0.82 0.81 0.81 0.81 0.80 0.80 0.82 0.82 0.83 0.83 0.80 0.80 0.81 0.80 0.79 0.81 0.80 0.80 0.84 - 0.83 0.82 0.80 0.80 0. 0.57 0.59 0.59 0.65 0.65 0.67 0.57 0.60 0.62 0.61 0.63 0.64 0.59 0.62 0.64 0.52 0.57 0.59 0.58 0.60 0.62 0.60 0.62 0.63 0.54 - 0.56 0.59 0.62 0.64 0.64 reconstruction quality of the continuous DC-AE [10]. Scaling Token Entropy and Autoregressive Model Size. We investigate how vocabulary size influences downstream generative performance on ImageNet. To maintain uniform computational overhead across different downsampling rates, we employ class token replication to align FLOPs during training. Fig. 6 reveals that while small-scale Transformers struggle with the convergence of large vocabularies, larger models effectively leverage them to achieve superior generative quality. These findings suggest that scaling up the vocabulary is most effective when accompanied by corresponding increase in Transformer scale."
        },
        {
            "title": "4.2.1 Experimental Setup",
            "content": "Datasets and Metrics. We follow prior studies [43, 61, 66] and evaluate our models on the class-conditional ImageNet-1K [17] generation benchmark at resolution of 256256. The main evaluation metric is the Fréchet Inception Distance (FID) [31]. Additionally, we report Inception Score (IS) [58] as well as Precision and Recall [39] as complementary measures of generative quality. All evaluation metrics are computed using 9 Table 3 Overall comparison with parallel image generation methods on ImageNet 256256. Throughput is measured on one A100 with batch size of 64 at bfloat16 precision. The suffixes \"-4x\" and \"-16x\" indicate that 4 and 16 tokens are generated in parallel per step. With only 260M parameters, our model outperforms the 1.4B SOTA model in parallel prediction, while achieving significantly faster generation speeds."
        },
        {
            "title": "Type",
            "content": "Order #Params Steps Throughput FID IS Pre. Rec. DiT-XL/2 [53] DiCo-XL [3] MaskGIT [6] MAR-B [43] MAR-L [43] VAR-d20 [66] VAR-d24 [66] Diff. Diff. Mask. Mask. Mask."
        },
        {
            "title": "VAR\nVAR",
            "content": "PAR-L [69] AR PAR-XL [69] AR PAR-XXL [69] AR NAR-L [30] AR NAR-XL [30] AR NAR-XXL [30] AR RandAR-L [52] AR AR RandAR-XL [52] RandAR-XXL [52] AR AR BitDance-B-4x AR BitDance-B-16x - - random random random - - hybrid hybrid hybrid hybrid hybrid hybrid random random random raster raster 675M 701M 227M 208M 479M 600M 1.0B 343M 775M 1.4B 372M 816M 1.5B 343M 775M 1.4B 260M 260M 250 250 8 256 256 10 147 147 147 31 31 31 88 88 88 64 16 1.06 img/s 2.62 img/s 50.73 img/s 1.83 img/s 1.39 img/s 71.31 img/s 47.22 img/s 15.01 img/s 8.09 img/s 5.17 img/s 40.03 img/s 23.12 img/s 15.37 img/s 25.12 img/s 16.01 img/s 10.39 img/s 24.18 img/s 90.26 img/s 2.27 2. 6.18 2.31 1.78 2.57 2.09 3.76 2.61 2.35 3.06 2.70 2.58 2.55 2.25 2.15 1.69 1.91 278.2 282.2 182.1 281.7 296.0 302.6 312. 218.9 259.2 263.2 263.9 277.5 293.5 288.8 317.8 322.0 291.2 283.8 0.83 0.83 0.80 0.82 0.81 0.83 0.82 0.81 0.80 0.80 0.81 0.81 0.82 0.81 0.80 0.79 0.79 0.78 0.57 0. 0.51 0.57 0.60 0.56 0.59 0.60 0.62 0.62 0.53 0.58 0.57 0.58 0.60 0.62 0.63 0.62 OpenAIs TensorFlow-based evaluation toolkit [18]. Implementation Details. Following MAR [43], we explore the scaling capabilities of three BitDance models of varying sizes. BitDance-B, -L, and -H feature 24, 32, and 40 Transformer blocks with widths of 768, 1024, and 1280, respectively. The binary diffusion head maintains consistent width, while the number of blocks is set at 6, 8, and 12. Furthermore, our model nomenclature incorporates the patch size used in next-patch diffusion modeling. For instance, BitDance-B-4x denotes patch size of = 2, where each AR step generates p2 = 4 tokens. Following previous works [36, 43, 66], we conduct our experiments using 16 downsampling binary tokenizer trained on ImageNet. The autoregressive model is trained for 800 epochs using the AdamW [46] optimizer with β = (0.9, 0.95) and weight decay of 0.05. We employ batch size of 1024 and cosine learning rate schedule with 20K warmup steps. Additionally, an Exponential Moving Average (EMA) decay rate of 0.9999 is applied."
        },
        {
            "title": "4.2.2 Main Results",
            "content": "We begin by exploring the scalability of BitDance when = 1. Tab. 2 indicates consistent performance gain as the model size grows. Our largest variant, BitDance-H-1x (1B parameters), reaches an FID of 1.24, outperforming prior AR baselines even under standard raster-scan causal framework. Such results underscore the superior modeling proficiency inherent in our approach. We also explore parallel generation using BitDance-B as the base model for = 2 and 4. Tab. 3 reveals that our BitDance-B-4x outperforms the 1.4B-parameter state-of-the-art parallel AR baseline RandAR-XXL [52] by roughly 0.5 FID, despite having only 260M parameters. These results provide strong evidence that the binary diffusion head effectively captures the joint distribution of parallel tokens, enabling both parameter efficiency and high generative quality."
        },
        {
            "title": "4.3 Text-to-image Generation",
            "content": "10 Table 4 Training recipe of BitDance for text-to-image generation. In the distillation phase, which requires only few training steps, we transition model with 16-token parallel prediction (p = 4) into model with 64-token parallel prediction (p = 8) to achieve faster inference speeds. Stage PT CT SFT Distill (optional) Learning rate LR scheduler Weight decay Gradient norm clip Optimizer Loss weight (text : vision) Warm-up steps Training steps Global batch size # Training samples Image resolution (256px : 512px : 1024px) Text token drop prob 1.0 104 Constant 0.0 1.0 1.0 104 Constant 0.0 1.0 2.0 105 Constant 0.0 1.0 2.0 105 Constant 0.0 1. AdamW (β1 = 0.9, β2 = 0.95, ϵ = 1.0 1015) 0.01 : 1 500 20K 1536 30.7M 0 : 0 : 1 0.1 0.01 : 1 1000 40K 2480 99.2M 0 : 1 : 1 0.1 0.01 : 1 500 40K 2320 92.8M 0 : 1 : 1 0.1 0.01 : 1 2000 100K 2560 256M 2 : 7 : 1 0.1 Table 5 Evaluation of text-to-image generation on DPG-Bench [34]. We mark the best and second-best performance among autoregressive models in bold and underline, respectively."
        },
        {
            "title": "Model",
            "content": "Global Entity Attribute Relation Other Overall Proprietary Models GPT Image 1 [51] Seedream 3.0 [24] Diffusion Models PixArt-α [9] FLUX.1-Dev [40] SD3 Medium [20] Z-Image-Turbo [65] BAGEL [16] HiDream-I1-Full [5] Lumina-Image-2.0 [54] Z-Image [65] Qwen-Image [64]"
        },
        {
            "title": "Autoregressive Models",
            "content": "Emu3-Gen [68] Infinity [29] Janus-Pro [12] Tar [28] NextStep-1 [63] GLM-Image [82] BitDance 88.89 94.31 88.94 92.65 86.89 74.35 87.90 91.29 - 76.44 - 93.39 91.32 85.21 93.11 86.90 83.98 - 87.74 89.53 82.89 90.00 91.01 89.59 - 90.22 91.97 91.22 91. 86.68 - 88.90 88.62 - 90.25 93.76 89.84 91.36 88.94 88.96 88.83 90.14 - 89.48 90.20 93.16 92.02 86.84 - 89.40 88.05 - 89.08 92.47 92.63 92.78 86.59 90.87 80.70 92.16 - 93.74 94.85 92.22 94. 90.22 90.76 89.32 93.98 - 92.15 91.81 90.96 88.24 87.68 88.33 88.68 88.68 - 91.83 - 91.52 92.73 83.15 - 89.48 84.86 - 90.17 90.26 85.15 88.27 80.54 83.84 84.08 84.86 85.07 85.89 87.20 88.14 88. 80.60 83.46 84.19 84.19 85.28 84.78 88."
        },
        {
            "title": "4.3.1 Training Details",
            "content": "Model Design. To scale up BitDance on the text-to-image generation task, we choose the pretrained Qwen314B [77] as the base AR model. Following previous works [63, 68], the AR model serves as both the text encoder and image generator. Our primary goal is to enhance it with image generation capability using high-quality image-text pairs. To balance reconstruction quality and generative convergence speed, we choose the tokenizer with 16x downsampling ratio for text-to-image generation. To achieve more efficient high-resolution generation, we set the patch size to 4 and enable parallel prediction of p2 = 16 tokens per step. For positional encoding, in addition to the 1D RoPE [60] used in LLM, we incorporate 2D sinusoidal 11 Table 6 Evaluation of text-to-image generation on GenEval [25]. We mark the best and second-best performance among autoregressive models in bold and underline, respectively. Model Single Obj. Two Obj. Count Colors Pos. Color Attri. Overall Proprietary Models GPT Image 1 [51] Seedream 3.0 [24] Diffusion Models PixArt-α [9] SD3 Medium [20] JanusFlow [49] FLUX.1-Dev [40] SD3.5-Large [20] Lumina-Image-2.0 [54] Show-o2 [76] Z-Image-Turbo [65] HiDream-I1-Full [5] Z-Image [65] Qwen-Image [64] BAGEL [16] Autoregressive Models Emu3-Gen [68] Infinity [29] Janus-Pro [12] Tar [28] NextStep-1 [63] BitDance 0.99 0.99 0.98 0.98 0.97 0.98 0.98 - 1.00 1.00 1.00 1.00 0.99 0.98 0.98 - 0.99 0.98 - 1. 0.92 0.96 0.50 0.74 0.59 0.81 0.89 0.87 0.87 0.95 0.98 0.94 0.92 0.95 0.71 0.85 0.89 0.92 - 0.96 0.85 0.91 0.44 0.63 0.45 0.74 0.73 0.67 0.58 0.77 0.79 0.78 0.89 0.84 0.34 - 0.59 0.83 - 0. 0.92 0.93 0.80 0.67 0.83 0.79 0.83 - 0.92 0.89 0.91 0.93 0.88 0.95 0.81 - 0.90 0.85 - 0.95 0.75 0.47 0.08 0.34 0.53 0.22 0.34 - 0.52 0.65 0.60 0.62 0.76 0.78 0.17 0.49 0.79 0.80 - 0. 0.61 0.80 0.07 0.36 0.42 0.45 0.47 0.62 0.62 0.68 0.72 0.77 0.77 0.77 0.21 0.57 0.66 0.65 - 0.83 0.84 0.84 0.48 0.62 0.63 0.66 0.71 0.73 0.76 0.82 0.83 0.84 0.87 0.88 0.54 0.73 0.80 0.84 0.73 0. positional embeddings to enhance the models spatial awareness of images. Notably, to support varying image resolution and aspect ratios, we add two resolution tokens [res_i] and [res_j] at the beginning of visual tokens: [bos], {text tokens}, [boi], [res_i], [res_j], {visual tokens}, [eoi], [eos], where and indicate the number of visual tokens on the height and width dimensions. In this way, we can decide how many tokens (i.e., j) to be decoded during inference. Training Recipe. We adopt three-stage training pipeline, including pre-training (PT), continued training (CT) and supervised fine-tuning (SFT). Our PT and CT data is collected from open-sourced datasets such as LAION [59]. The SFT data is collected from both open-source datasets and small number of images generated by other text-to-image models such as Seedream [24] and Z-Image-Turbo [65], which help our model to quickly adapt to the distribution of high-quality images and achieve strong visual fidelity with substantially fewer training samples. Besides, we also preserve small amount of image-to-text data to maintain the text understanding ability of LLM. To further accelerate model inference, we introduce an additional distillation stage that transitions the SFT model from 16-token parallel prediction to 64-token parallel prediction. Specifically, we inherit the weights from the SFT model and fine-tune it using small amount of high-quality data. We find that the model can adapt to predicting more tokens in parallel with only few training steps. The training parameters are listed in Tab. 4. Different from prior works, we adopt mixed-resolution training strategy during pre-training, which we find crucial for training stability. Specifically, we use 512px as the primary training resolution, while jointly incorporating 256px images to improve training throughput and 1024px images to enhance stability and robustness at higher resolutions. This mixed-resolution setup enables stable optimization while maintaining both efficiency and high-resolution generalization. During the continued training stage, we increase the proportion of 1024px images to enhance the models performance in high-resolution image generation. 12 Table 7 Evaluation of text-to-image generation on OneIG-EN [7]. We mark the best and second-best performance among autoregressive models in bold and underline, respectively."
        },
        {
            "title": "Alignment",
            "content": "Text Reasoning Style Diversity Overall"
        },
        {
            "title": "Proprietary Models",
            "content": "Imagen 4 [26] Seedream 3.0 [24] GPT Image 1 [51] Diffusion Models Show-o2 [76] SANA-1.5 [74] BAGEL [16] FLUX.1-Dev [40] OmniGen2 [71] HiDream-I1-Full [5] Z-Image-Turbo [65] Qwen-Image [64] Z-Image [65]"
        },
        {
            "title": "Autoregressive Models",
            "content": "Janus-Pro [12] NextStep-1 [63] GLM-Image [82] BitDance 0.857 0.818 0.851 0.817 0.765 0.769 0.786 0.804 0.829 0.840 0.882 0.881 0.553 0.826 0.805 0.853 0.805 0.865 0.857 0.002 0.069 0.244 0.523 0.680 0.707 0.994 0.891 0. 0.001 0.507 0.969 0.937 0.338 0.275 0.345 0.226 0.217 0.173 0.253 0.271 0.317 0.298 0.306 0.280 0.139 0.224 0.298 0.297 0.377 0.413 0.462 0.317 0.401 0.367 0.368 0.377 0.347 0.368 0.418 0. 0.276 0.332 0.353 0.395 0.199 0.277 0.151 0.177 0.216 0.251 0.238 0.242 0.186 0.139 0.197 0.194 0.365 0.199 0.213 0.177 0.515 0.530 0.533 0.308 0.334 0.361 0.434 0.475 0.477 0.528 0.539 0. 0.267 0.418 0.528 0.532 Table 8 Evaluation of text-to-image generation on OneIG-ZH [7]. We mark the best and second-best performance among autoregressive models in bold and underline, respectively."
        },
        {
            "title": "Alignment Text Reasoning",
            "content": "Style Diversity Overall Proprietary Models Kolors 2.0 [38] GPT Image 1 [51] Seedream 3.0 [24]"
        },
        {
            "title": "Diffusion Models",
            "content": "HiDream-I1-Full [5] CogView4 [85] BAGEL [16] Z-Image-Turbo [65] Qwen-Image [64] Z-Image [65]"
        },
        {
            "title": "Autoregressive Models",
            "content": "Janus-Pro [12] GLM-Image [82] BitDance 0.738 0.812 0.793 0.620 0.700 0.672 0.782 0.825 0.793 0.324 0.738 0.786 0.502 0.650 0.928 0.205 0.193 0.365 0.982 0.963 0. 0.148 0.976 0.961 0.226 0.300 0.281 0.256 0.236 0.186 0.276 0.267 0.266 0.104 0.284 0.276 0.331 0.449 0.397 0.304 0.348 0.357 0.361 0.405 0. 0.264 0.335 0.376 0.333 0.159 0.243 0.300 0.214 0.268 0.134 0.279 0.243 0.358 0.221 0.159 0.426 0.474 0.528 0.337 0.338 0.370 0.507 0.548 0. 0.240 0.511 0.512 For all training stages, we use the AdamW [46] optimizer with β = (0.9, 0.95) and constant learning rate. The visual tokenizer is frozen, while all other model parameters remain trainable. The training loss consists of two parts: vision loss from the binary diffusion head and text loss (cross-entropy loss) from the AR model. We set the relative weighting of vision and text loss as 1 : 0.01, which allows the AR model to maintain fundamental text encoding capabilities while keeping the primary focus on image generation. To enable classifier-free guidance [32] and improve model robustness, we randomly drop text tokens with predefined token dropout probability of 0.1 during training. This encourages the model to rely on both text and image cues and prevents overfitting to the textual input. 13 Table 9 Evaluation of text-to-image generation on TIIF Bench testmini [70]. We mark the best and second-best performance among autoregressive models in bold and underline, respectively. Model Overall short long short long short long short long short long short long short long short long short long short long short long short Avg Attribute Relation Reasoning Avg Attr.+Rela. Attr.+Reas. Rela.+Reas. Style Text Real World long Basic Following Advanced Following Designer Proprietary Models Midjourney V7 [50] 68.74 65.69 77.41 76.00 77.58 81.83 82.07 76.82 72.57 69.32 64.66 60.53 67.20 62.70 81.22 71.59 60.72 64.59 83.33 80.00 24.83 20.83 68.83 63.61 74.96 70.81 78.72 78.50 79.50 79.83 80.82 78.82 75.82 76.82 73.39 67.27 73.45 67.20 72.01 71.34 63.59 60.72 89.66 86.67 66.83 54.83 72.93 60.99 DALL-E 3 [4] 86.02 84.31 87.07 84.93 90.50 90.00 89.85 85.94 80.86 78.86 79.16 80.60 79.76 81.82 77.23 78.85 75.64 78.64 100.0 93.33 97.17 87.78 83.21 83.58 Seedream 3.0 [24] GPT Image 1 [51] 89.15 88.29 90.75 89.66 91.33 87.08 84.57 84.57 96.32 97.32 88.55 88.35 87.07 89.44 87.22 83.96 85.59 83.21 90.00 93.33 89.83 86.83 89.73 93.46 Diffusion Models 0.83 47.56 49.05 50.93 52.46 64.58 66.08 56.83 59.33 67.57 71.82 69.32 67.07 44.75 45.63 51.44 43.20 51.09 59.72 44.72 54.46 70.00 66.67 Lumina-Next [87] Hunyuan-DiT [44] 0.83 40.10 44.20 51.38 53.28 69.33 69.00 65.83 69.83 78.07 73.82 64.07 63.32 42.62 45.45 50.20 41.57 59.22 61.84 47.84 51.09 56.67 73.33 62.00 58.12 70.66 75.25 69.33 78.83 75.07 77.32 67.57 69.57 57.65 49.50 65.20 56.57 66.96 61.72 66.59 54.59 83.33 70.00 1.83 62.11 52.41 PixArt-Σ [8] 67.15 65.73 79.66 77.08 79.83 77.83 85.57 83.57 73.57 69.82 61.50 60.67 65.32 56.57 69.96 73.09 62.96 65.84 80.00 80.00 17.83 15.83 71.07 68.83 SANA 1.5 [74] 67.46 66.09 78.32 77.75 83.33 79.83 82.07 78.82 71.07 74.07 61.46 59.56 61.07 64.07 68.84 70.34 50.96 57.84 66.67 76.67 59.83 20.83 63.23 67.34 SD 3 [20] FLUX.1-dev [40] 71.09 71.78 83.12 78.65 87.05 83.17 87.25 80.39 75.01 72.39 65.79 68.54 67.07 73.69 73.84 73.34 69.09 71.59 66.67 66.67 43.83 52.83 70.72 71.47 Z-Image-Turbo [65] 77.73 80.05 81.85 81.59 86.50 87.00 82.88 79.99 76.17 77.77 68.32 74.69 72.04 75.24 60.22 73.33 68.90 71.92 83.33 93.33 83.71 84.62 85.82 77.24 80.20 83.04 78.36 82.79 79.50 86.50 80.45 79.94 75.13 81.94 72.89 77.02 72.91 77.56 66.99 73.82 73.89 75.62 90.00 93.33 94.84 93.21 88.06 85.45 Z-Image [65] 86.14 86.83 90.18 87.22 90.50 91.50 88.22 90.78 79.81 79.38 79.30 80.88 79.21 78.94 78.85 81.69 75.57 78.59 100.0 100.0 92.76 89.14 90.30 91.42 Qwen-Image [64] 0.00 0.00 1.83 Autoregressive Models LightGen [72] Infinity [29] Janus-Pro [13] GLM-Image [82] BitDance 53.22 43.41 66.58 47.91 55.83 47.33 74.82 45.82 69.07 50.57 46.74 41.53 62.44 40.82 61.71 50.47 50.34 45.34 53.33 53.33 6.83 50.92 50.55 62.07 62.32 73.08 75.41 74.33 76.83 72.82 77.57 72.07 71.82 56.64 54.98 60.44 55.57 74.22 64.71 60.22 59.71 80.00 73.33 10.83 23.83 54.28 56.89 66.50 65.02 79.33 78.25 79.33 82.33 78.32 73.32 80.32 79.07 59.71 58.82 66.07 56.20 70.46 70.84 67.22 59.97 60.00 70.00 28.83 33.83 65.84 60.25 81.01 81.02 79.64 78.12 78.79 80.44 83.50 87.50 77.22 77.62 75.64 76.21 72.19 71.66 72.89 78.88 68.06 66.10 70.63 67.21 96.67 90.00 87.78 75.57 84.33 83.96 0.00 - - - - - - - - - - - - - - - - - - - - - - Table 10 Quantitative results of the SFT and distilled models. #Tokens denotes the number of predicted tokens in one decoding step. Model #Tokens DPG-Bench GenEval SFT Distilled 16 64 88.28 88. 0.86 0.85 Figure 7 Images generated by the SFT and distilled models. Table 11 Efficiency comparison of text-to-image generation at 10241024 resolution. Latency is measured using single H100 GPU with bfloat16 precision. Type Model Diffusion BAGEL [16] Diffusion Qwen-Image [64] Diffusion Z-Image [65] NextStep-1 [63] Autoregressive GLM-Image [82] Autoregressive Autoregressive BitDance #Params (B) 7 20 6 14 16 Latency-1024 (s) 23.1 20.3 21.1 402 53.2 12."
        },
        {
            "title": "4.3.2 Main Results",
            "content": "We conduct comprehensive evaluation of BitDance on text-to-image generation, covering capabilities such as prompt following, text rendering, and reasoning. We compare it against state-of-the-art proprietary models (e.g., Seeddream [24], GPT Image 1 [51]), diffusion models (e.g., Qwen-Image [64], Z-Image [65]), and autoregressive models (e.g., GLM-Image [82], NextStep-1 [63]). The results demonstrate that BitDance achieves state-of-the-art performance among autoregressive models and is comparable to leading proprietary and diffusion models. As shown in Tab. 5, Tab. 6 and Tab. 9, BitDance demonstrates strong prompt-following capabilities. On GenEval [25] and DPG-Bench [34], BitDance outperforms the majority of existing open-source models across 14 Table 12 Different visual tokenizers for AR generation (p = 1). Table 13 Comparison of different sampling heads (p = 1). Table 14 Ablation study on design of next-patch diffusion (p = 4). Tokenizer FID IS Sampling Head FID IS Model MARs VAE [43] VA-VAE [78] BitDance-Tok 3.16 4.84 1.79 289.9 273.7 290.5 Token Cls Head Bit-wise Cls Head Binary Diff Head OOM OOM 174.5 8.37 290.5 1. Next-Patch Diffusion Block-wiseFull PatchToken Raster FID IS 1.98 2.07 2.15 276.7 271.8 270.0 Figure 8 Sampling steps in binary diffusion head (p = 4). High-quality generation can be achieved with small number of sampling steps. (e.g., 10-20 steps). most evaluation dimensions. BitDance achieves an overall GenEval score of 0.86 and DPG-Bench score of 88.28, ranking among the top-performing methods. On the TIIF benchmark (testmini) [70], BitDance achieves the second-best overall performance among autoregressive models, demonstrating its strong capability in executing complex and diverse user instructions. Importantly, these competitive results are achieved with significantly fewer training data. BitDance is trained on fewer than 450M imagetext pairs, which is orders of magnitude smaller than the billionto multi-billion-scale datasets typically used by current leading commercial models. Despite this substantial data disadvantage, BitDance is able to match or closely approach the performance of such proprietary systems on multiple benchmarks. Results on OneIG Bench [7] further validate the data efficiency and generalization ability of BitDance. As shown in Tab.7 and Tab. 8, on both OneIG-EN and OneIG-ZH, BitDance achieves competitive average scores, narrowing the gap with state-of-the-art commercial models, particularly in text fidelity and alignment. Taken together, these results highlight that BitDance effectively bridges the performance gap between open-source and proprietary models while relying on dramatically less training data, underscoring the effectiveness and scalability of our method. Furthermore, we provide qualitative and quantitative results for both the SFT and distilled models in Fig. 7 and Tab. 10. These results demonstrate that the distilled model maintains excellent generation quality while achieving faster inference speeds. In Tab. 11, we present an efficiency comparison of various text-to-image models generating 10241024 images. Compared to both diffusion and autoregressive models, our distilled model achieves faster inference speeds."
        },
        {
            "title": "4.4 Ablation Study\nWe conduct ablation studies on ImageNet 256×256 to analyze the components within BitDance. We use\nBitDance-B as the AR generation backbone. All models are trained for 400 epochs and evaluated using their\nbest classifier-free guidance [32] scale.",
            "content": "Continuous VAE vs. Binary Tokenizer. Comparing continuous VAEs [43, 78] against our binary tokenizer for AR generation, Tab. 12 reveals notably inferior performance for the former. This suggests that unconstrained continuous tokens lead to significant error accumulation during generation. Sampling Head. We evaluate different sampling heads discussed in Sec. 3.2 in Tab. 13. The token classification head suffers from OOM issues caused by high volume of parameters, while the bitwise classification head performs poorly due to the restrictive assumption of bit independence. Next-Patch Diffusion. Two critical designs in next-patch diffusion modeling are investigated: the patch-wise raster scan order for token generation and the use of block causal masks for intra-patch token visibility. As shown in Tab. 14, both designs effectively improve generation performance. 15 Figure 9 Output distribution of binary diffusion head at different timesteps t. As the timestep increases from 0 to 1 (i.e., as noise decreases), the predictions of the binary diffusion head become progressively more distinct, converging towards the binary values of -1 and 1. Diffusion Sampling Steps. Fig. 8 illustrates how the number of sampling steps in the binary diffusion head affects performance. Notably, good results are attained with as few as 10 steps. This suggests that the discrete nature of binary tokens simplifies the sampling task relative to continuous tokens, enabling rapid and robust generation. Prediction of Binary Diffusion Head. As described in Sec. 3.2, our binary diffusion head adopts the xprediction [42] formulation, meaning the network directly predicts the clean data. In our specific case, the prediction target is the binary latents. Fig. 9 illustrates the output distribution of the head across different timesteps. When is small (indicating high noise levels), the prediction task is challenging, causing many predicted values to cluster around 0. As increases, the networks predictions become progressively more distinct, converging towards the binary values of -1 and 1. This demonstrates that our binary diffusion head implicitly learns the characteristics of binary discrete distribution, even without explicit, hand-crafted constraints in network design or training objective."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce BitDance, simple yet scalable autoregressive model that achieves high-quality image generation through the efficient prediction of binary visual tokens. BitDance marks the first time visual tokenizers vocabulary has been expanded to 2256, attaining reconstruction fidelity comparable to that of continuous VAEs. By leveraging the proposed binary diffusion head and next-patch diffusion modeling, BitDance effectively captures the joint distribution of multiple binary tokens, enabling efficient and precise parallel prediction. Extensive evaluations across both class-conditional and text-to-image generation benchmarks validate the effectiveness and efficiency of BitDance. We aim to further scale up the data and model size, exploring BitDances potential in wider range of multi-modal tasks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, and Huaibo Huang. Dico: Revitalizing convnets for scalable and efficient diffusion modeling. In NeurIPS, 2025. [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn.openai.com/papers/dall-e-3.pdf, 2023. [5] Qi Cai, Yehao Li, Yingwei Pan, Ting Yao, and Tao Mei. Hidream-i1: An open-source high-efficient image generative foundation model. In ACM MM, pages 1363613639, 2025. [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. [7] Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. In NeurIPS, 2025. [8] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. [9] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [10] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [11] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. [12] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [13] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [14] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [15] Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. [16] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [21] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In ICLR, 2025. [22] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2023. [23] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [24] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [25] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. [26] Google. Imagen 4 model card. Imagen-4-Model-Card.pdf, 2025. https://storage.googleapis.com/deepmind-media/Model-Cards/ [27] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [28] Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as dialect: Unifying visual understanding and generation via text-aligned representations. In NeurIPS, 2025. [29] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In CVPR, 2025. [30] Yefei He, Yuanyu He, Shaoxuan He, Feng Chen, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Neighboring autoregressive modeling for efficient visual generation. arXiv preprint arXiv:2503.10696, 2025. [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [34] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [35] Aren Jansen, Daniel P. W. Ellis, Shawn Hershey, R. Channing Moore, Manoj Plakal, Ashok Popat, and Rif A. Saurous. Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision. In ICASSP, 2019. [36] Guolin Ke and Hui Xue. Hyperspherical latents improve continuous-token autoregressive generation. In ICLR, 2026. [37] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [38] Kuaishou Kolors Team. Kolors 2.0. https://app.klingai.com/cn/, 2025. [39] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In NeurIPS, 2019. [40] Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. 18 [41] Haopeng Li, Jinyue Yang, Guoqi Li, and Huan Wang. Autoregressive image generation with randomized parallel decoding. arXiv preprint arXiv:2503.10568, 2025. [42] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. [43] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [44] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [45] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [47] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [48] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [49] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [50] Midjourney. Midjourney v7. https://www.midjourney.com/home, 2025. [51] OpenAI. Gpt-image-1. https://openai.com/zh-Hans-CN/index/introducing-4o-image-generation/, 2025. [52] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [54] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. [55] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. FlowAR: Scale-wise autoregressive image generation meets flow matching. In ICML, 2025. [56] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. In ICCV, 2025. [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [58] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [60] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [61] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [62] Yutao Sun and et al. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024. 19 [63] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. In ICLR, 2026. [64] Qwen-Image Team. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [65] Z-Image Team. Z-image: An efficient image generation foundation model with single-stream diffusion transformer. arXiv preprint arXiv:2511.22699, 2025. [66] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. [67] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [68] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [69] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. In CVPR, 2025. [70] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. [71] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [72] Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025. [73] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: efficient high-resolution text-to-image synthesis with linear diffusion transformers. In ICLR, 2025. [74] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [75] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025. [76] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. [77] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [78] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. [79] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In ICLR, 2024. [80] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. In ICCV, 2025. [81] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [82] Z.ai. Glm-image: Auto-regressive for dense-knowledge and high-fidelity image generation. https://z.ai/blog/ glm-image, 2026. 20 [83] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. In ICLR, 2025. [84] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. [85] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv:2403.05121, 2024. [86] Shaobin Zhuang, Yiwei Guo, Canmiao Fu, Zhipeng Huang, Zeyue Tian, Fangyikang Wang, Ying Zhang, Chen Li, and Yali Wang. Wetok: Powerful discrete tokenization for high-fidelity visual reconstruction. In ICLR, 2026. [87] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. In NeurIPS, 2024."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Institute of Automation, Chinese Academy of Sciences",
        "MMLab, The Chinese University of Hong Kong",
        "National University of Singapore",
        "Shanghai Jiao Tong University"
    ]
}