{
    "paper_title": "Weight Decay Improves Language Model Plasticity",
    "authors": [
        "Tessa Han",
        "Sebastian Bordt",
        "Hanlin Zhang",
        "Sham Kakade"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 7 3 1 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Weight Decay Improves Language Model Plasticity",
            "content": "Tessa Han1, Sebastian Bordt2, Hanlin Zhang3, and Sham Kakade3 1 Broad Institute, Schmidt Center 2 University of Tübingen, Tübingen AI Center 3 Harvard University Abstract The prevailing paradigm in large language model (LLM) development is to pretrain base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base models validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decays mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that single optimization hyperparameter plays in shaping model behavior."
        },
        {
            "title": "1 Introduction",
            "content": "Weight decay is canonical hyperparameter in deep learning whose role has evolved alongside changes in training regimes. In classical multi-epoch training, weight decay was understood primarily as regularizer that improves generalization by shrinking weights and controlling model capacity (Hardt et al., 2016; Sun et al., 2025; Zhang et al., 2017). In contemporary large-scale pretraining, which often uses single epoch over massive datasets (Brown et al., 2020; Kaplan et al., 2020), weight decay no longer primarily serves the purpose of generalization but plays decisive role in optimization stability and convergence (DAngelo et al., 2024; Wang and Aitchison, 2024; Zhang et al., 2025). Moreover, modern large language models are typically developed in two distinct stages: large-scale pretraining phase followed by post-training phase involving supervised fine-tuning, alignment, and reinforcement learning (Bi et al., 2024; Brown et al., 2020; Lambert et al., 2024; Ouyang et al., 2022). While pretraining and post-training are functionally linked, current practices often treat them as decoupled. Specifically, pretraining hyperparameters and scaling laws are predominantly studied through the lens of the base models validation loss, under the assumption that lower pretraining validation loss also yields more capable downstream model (Bi et al., 2024; Hoffmann et al., 2022). Correspondence to Tessa Han (than@broadinstitute.org). 1 This decoupling is especially pronounced when the two stages are carried out by different teams or at different times: best pretrained model is selected in isolation and only later adapted for downstream use. However, to what extent does optimizing pretraining hyperparameters for pretraining performance also optimize the final, post-trained models performance? In this work, we study the relationship between pretraining and post-training from the perspective of model plasticity. Model plasticity is the ability of trained model to effectively adapt to new data upon further training, modifying its parameters and internal representations in response to the new data and enabling effective learning of new tasks without reinitialization (Berariu et al., 2021; Dohare et al., 2024). While the literature on model plasticity and on language models have evolved largely independently, the notion of plasticity naturally bridges pretraining and post-training: while pretraining loss measures how well model learned the training distribution, plasticity captures how readily that model can be reshaped for downstream tasks. As we will show in this work, two models with similar pretraining loss may differ in their plasticity, meaning that optimizing hyperparameters for pretraining loss alone may not yield the best post-trained model. In our experiments, we vary the weight decay parameter during pretraining and subsequently evaluate the pretrained models ability to learn various tasks during fine-tuning. Pretraining is performed for two model families (Llama-2 and OLMo-2), multiple model sizes (up to 4B parameters), and in both the compute-optimal (20 tokens-per-parameter, TTP hereafter) and overtrained (140 TPP) regimes. Fine-tuning is performed across six Chain-of-Thought (CoT) tasks, and model performance is evaluated using comprehensive suite of metrics that cover both solution correctness and quality. Our experimental design takes an end-to-end perspective (Mayilvahanan et al., 2025; Qi et al., 2025), aligning pretraining hyperparameter selection with the ultimate objective of maximizing performance after further training. Our contributions are as follows: We show that weight decay is key factor in shaping model plasticity, facilitating adaptation to new tasks during subsequent fine-tuning. In our experiments across range of model families, scales, and training regimes, the evidence points toward an optimal pretraining weight decay value larger than the standard default of 0.1. This highlights the potential for re-evaluating standard hyperparameter choices to better account for models downstream adaptability. We provide one of the first examples showing that optimizing hyperparameters to minimize pretraining validation loss does not necessarily yield the best downstream model performance. Specifically, we show that there is training regime where larger weight decay values lead to higher pretraining validation loss and better downstream performance after fine-tuning. We provide mechanistic perspective on the effect of weight decay on model training dynamics, showing that weight decay encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. These effects provide potential explanation for how weight decay preserves the models ability to flex and learn during subsequent adaptation, thereby sustaining plasticity."
        },
        {
            "title": "2 Related Work",
            "content": "Here, we discuss related work on weight decay and model plasticity and how our work contributes new insights. 2 Weight decay in language model training. Weight decay is standard hyperparameter in language model training and is commonly implemented in conjunction with adaptive optimizers such as AdamW (Brown et al., 2020; Grattafiori et al., 2024; Liu et al., 2024; Loshchilov and Hutter, 2019; OLMo Team et al., 2024). Beyond its classical role in regularization and generalization (Krogh and Hertz, 1991; Loshchilov and Hutter, 2019; Zhang et al., 2018; Zhou et al., 2024), weight decay has also been shown to play other roles in language model training, such as improving optimization and training stability (DAngelo et al., 2024), shaping the learning rate (Kosson et al., 2024, 2025; Li et al., 2020), controlling the effective step size (Wen et al., 2025), inducing low-rank attention layers (Kobayashi et al., 2024), and increasing forgetting of contaminated benchmark questions (Bordt et al., 2025). Wang and Aitchison (2024) show that the weights of AdamW can be understood as an exponential moving average, and that the weight decay hyperparameter plays critical role in controlling its time scale. Bergsma et al. (2025) study how to set weight decay to minimize the pretraining loss of language models, finding that lower weight decay improves pretraining loss in the overtrained (high TPP ratio) regime. Kim et al. (2025) show that larger weight decay improves pretraining loss in the multi-epoch setting. In contrast to previous work which primarily focuses on weight decays effects on the pretrained model, this paper examines how weight decay during pretraining shapes model plasticity. Plasticity of deep learning models. Model plasticity has previously been studied in the contexts of continual learning, transfer learning, and reinforcement learning, settings in which models often undergo multiple rounds of training (Coetzer et al., 2025; Dohare et al., 2024; Klein et al., 2024). Prior works have demonstrated that image models lose plasticity when subjected to additional rounds of training on new data, leading to decreased ability to learn this new data (Dohare et al., 2024; Klein et al., 2024; Lyle et al., 2023). Various approaches have been developed to improve model plasticity, including shrinking and perturbing model weights at the start of each training round (Ash and Adams, 2020), identifying and re-initializing less-useful model weights during training (Dohare et al., 2024), pushing weights towards initialization weights during training (Kumar et al., 2023), and learning per-connection plasticity strengths among neuron pairs (Miconi et al., 2018). While previous studies have examined how active forgetting and tokenization (Abagyan et al., 2025; Chen et al., 2023) affect language model plasticity, research on language model plasticity remains underdeveloped. In contrast to these works, this paper investigates the role of weight decay, standard hyperparameter for language model training, on language model plasticity."
        },
        {
            "title": "3 Background and Methods",
            "content": "In this section, we provide further background, define the research question, and describe the experimental setup. Weight decay in AdamW. Motivated by prior findings that regularization helps vision models maintain plasticity (Dohare et al., 2024), this paper investigates weight decays role in language model plasticity. We focus on the weight decay hyperparameter λ in the AdamW optimizer which, for each optimizer step 1, performs two decoupled updates: gradient update given by θt = ˆθt γt ˆmt (cid:112) (cid:14)( ˆvt + ϵ) followed by weight decay update given by ˆθt = θt1 γtλθt1 3 (1) (2) based on model parameters θt, learning rate γt, and firstand second-order moment estimates of the gradient ˆmt and ˆvt (Loshchilov and Hutter, 2019). For language model pretraining, the choice λ = 0.1 has emerged as kind of default, used in many pretraining runs where the optimization hyperparameters are known (Brown et al., 2020; OLMo Team et al., 2024; Touvron et al., 2023). Language model plasticity. To assess the plasticity of pretrained model, we fine-tune the model on task and then measure its performance on this task. The better the performance on this downstream task, the better the pretrained model was able to learn new data during fine-tuning, thus the higher the plasticity of the pretrained model. This approach to measuring model plasticity is consistent with prior literature (Berariu et al., 2021; Dohare et al., 2024). In this context, we now specify the research question: Research Question. How does weight decay during language model pretraining affect model plasticity, i.e., the pretrained models ability to learn new knowledge during subsequent training? We investigate this research question empirically. We perform experiments that systematically vary weight decay during pretraining, then fine-tune and evaluate the models performance on various downstream tasks. Our experiments span various model families, model sizes, training regimes (TPP ratios), and fine-tuning tasks. The setup is as follows. Pretraining. We train Llama-2 models on the FineWeb-Edu dataset (Penedo et al., 2024) and OLMo-2 models on the OLMo-Mix-1124 dataset. We vary model size and TPP ratio, training models at the 20 TPP Chinchilla-optimal ratio (Hoffmann et al., 2022) and at the 140 TPP overtrained ratio. This setup yields five models: Llama-2-0.5B-20x, Llama-2-1B-20x, Llama-2-4B-20x, OLMo-2-1B-20x, and OLMo-2-1B-140x. For each model, we pretrain variants with different weight decay. Fine-tuning. We perform supervised fine-tuning (SFT) of the pretrained models across six CoT tasks spanning various domains: MetaMathQA (math reasoning), MedMCQA (medical reasoning), PubMedQA (biomedical research), MMLUProCoT (general knowledge and reasoning), RACE (reading comprehension), and SimpleScaling (math, science, and logical reasoning) (Jin et al., 2019; Lai et al., 2017; Muennighoff et al., 2025; Pal et al., 2022; Wang et al., 2024; Yu et al., 2023). Evaluation of model performance. We evaluate the fine-tuned models in zero-shot manner, prompting them to generate solutions to test set questions, and assess both the correctness and quality of the solutions using six evaluation metrics. Greedy (i.e., Pass@1): single deterministic response is generated (temperature = 0). The question is marked correct if this response is correct. Maj@16, RM@16, and Pass@16: Sixteen responses are sampled (temperature = 1). The question is marked correct if the majority answer is correct (Maj@16), if the response with the highest outcome reward model (ORM; Skywork-Reward-Llama-3.1-8B-v0.2) score is correct (RM@16), or if any of the responses are correct (Pass@16). Correct Ratio: Sixteen responses are sampled (temperature = 1). Among questions with at least one correct response, we compute the proportion of correct responses out of the sixteen sampled responses. ORM Score: In addition solution correctness, we also measure solution quality. Sixteen responses are sampled (temperature = 1). Each response is assigned score using an ORM (SkyworkReward-Llama-3.1-8B-v0.2) and the average score is computed. Since these experiments span pretraining and fine-tuning, we adopt the end-to-end analysis framework from Qi et al. (2025). Additional setup details are in Appendix and B."
        },
        {
            "title": "4 Weight decay Improves Language Model Plasticity",
            "content": "We present the main experimental results. We begin by identifying the optimal pretraining weight decay based on pretraining performance (Section 4.1), common way to select pretraining hyperparameters (Hoffmann et al., 2022). Next, we investigate how weight decay shapes the plasticity of the pretrained model and identify its optimal pretraining value based on downstream performance (Section 4.2). Then, we examine whether models pretraining performance is fully predictive its downstream performance (Section 4.3)."
        },
        {
            "title": "4.1 The optimal pretraining weight decay based on pretraining validation loss",
            "content": "We first identify the weight decay value that leads to the lowest cross-entropy validation loss after pretraining. This is the value considered optimal by current approaches in hyperparameter optimization for LLM pretraining (Bergsma et al., 2025). Following Bergsma et al. (2025), we pretrain various models by sweeping over different weight decay values and fixing all other hyperparameters. Figure 1 shows the validation cross-entropy loss of these pretrained models. Extremely small weight decay values during pretraining do not have significant effect on pretraining loss (Figure 1a). On the other hand, extremely large weight decay values can result in very high pretraining loss, significantly degrading pretraining performance (Figure 1a). At 20 TPP, for both Llama-2 and OLMo-2 models, we find that the optimal weight decay parameter is larger than the default of 0.1. In particular, among the weight decay values examined, the optimal weight decay is 0.5 for Llama-2-{0.5B and 1B}-20x (Figure 1a), 0.6 for OLMo-2-1B-20x (Figure 1b), and 1.0 for Llama-2-4B-20x (Figure 1c). However, this relationship changes as training time increases. At 140 TPP, for the OLMo-2-1B-140x model, the default value of 0.1 outperforms (leads to lower validation loss than) larger values of 0.3 and 1.0 (Figure 1c). This result that overtrained models have lower optimal weight decay is consistent with previous analyses on weight decay scaling laws which recommend decreasing the value of the weight decay hyperparameter as training time (TPP) increases to optimize for pretraining validation loss (Bergsma et al., 2025)."
        },
        {
            "title": "4.2 The optimal pretraining weight decay based on downstream performance",
            "content": "Next, we investigate how weight decay during pretraining affects model plasticity and downstream model performance. We fine-tune the pretrained models from Section 4.1 (which were trained with varying weight decay) on six CoT tasks and evaluate the final models performance on these tasks. Figure 2 shows the average downstream accuracy of the models across the six tasks based on four metrics. The downstream accuracy for individual tasks and based on all six metrics is in Appendix C. 5 (a) Llama-2 at 20 TPP (b) OLMo-2 at 20 TPP (c) Llama-2 4B and OLMo-2 at 140 TPP Figure 1: Pretraining validation cross-entropy loss of models pretrained with varying weight decay. The weight decay value that minimizes pretraining loss may be equal to or larger than the standard default value of 0.1 depending on the training regime. Figure 2: Weight decay during pretraining improves language model plasticity and downstream performance. This figure plots the average accuracy after fine-tuning for models pretrained with varying weight decay. The results indicate that weight decay leads to better downstream model performance, suggesting it enables the pretrained model to learn better during fine-tuning and improves model plasticity. In these experiments, the optimal weight decay for downstream performance is larger than the standard default of 0.1. In addition, the optimal weight decay based on pretraining loss (Figure 1) and that based on fine-tuning accuracy (this figure) are different, suggesting that optimizing hyperparameters based solely on pretraining loss does not always produce models with the best downstream performance. Among models that achieved reasonable pretraining validation losses in Section 4.1 (i.e., models that are suitable candidates for subsequent training), higher weight decay during pretraining confers higher degree of model plasticity, enabling the pretrained model to learn better during finetuning and perform better on the fine-tuning task. The results show that models pretrained with weight decay higher than the default 0.1 value perform better on downstream tasks. This finding is consistent across model families (Llama-2 and OLMo-2), model sizes (up to 4B parameters), training regimes (20 TPP and 140 TPP), fine-tuning tasks (six tasks spanning various domains), and evaluation metrics (six metrics measuring both solution correctness and quality). Among the weight decay values examined, in the compute-optimal 20 TPP regime, the optimal pretraining weight decay is 1.0 (Llama-2-0.5B-20x, Llama-2-1B-20x, Llama-2-4B-20x, and OLMo-2-1B-20x). In the overtrained 140 TPP regime, the optimal pretraining weight decay is 0.3 (OLMo-2-1B-140x). It is 6 Figure 3: models performance after pretraining is not perfectly predictive of its performance downstream. Models with similar pretraining losses can perform differently downstream, and models with lower validation cross-entropy loss after pretraining can perform better or worse downstream (i.e., after fine-tuning) than models with higher pretraining losses. possible that as models are trained for even longer (i.e., beyond 140 TPP), the optimal pretraining weight decay that leads to the best downstream model performance may continue to decrease (this is an extrapolation that would need to be validated). We also compare the weight decay value that minimizes pretraining validation loss (Figure 1 in Section 4.1) with the value that maximizes task accuracy after fine-tuning (Figure 2 in this section). We find that these two weight decay values differ for each model. This shows that the optimal weight decay during pretraining is not absolute it depends on the intended objective, such as optimizing for pretraining performance or downstream performance. Finding 1. Pretraining weight decay can improve model plasticity and lead to better downstream performance. The optimal pretraining weight decay value for plasticity is larger than the default of 0.1."
        },
        {
            "title": "4.3 The relationship between pretraining loss and downstream performance",
            "content": "Following the findings from the previous sections, we now investigate whether models pretraining performance is predictive of its downstream performance. We examine the pretraining validation cross-entropy loss of the pretrained models (from Section 4.1) and their accuracy on tasks after fine-tuning (measured in Section 4.2). The relationship between these two variables is plotted in Figure 3. We compare models with the same training setup (i.e., same model family, size, and TPP) that differ only in the pretraining weight decay hyperparameter. Although the Pearson correlation coefficient between pretraining and downstream performance tends to be negative for models trained at 20 TPP (Llama-2-0.5, Llama-2-1B, Llama-2-4B, and OLMo-2-1B) and positive for models trained at 140 TPP (OLMo-2-1B), this correlation should be interpreted cautiously because the relationship is not visually apparent. Moreover, re-computing the correlation coefficient by removing one observation at time can change the magnitude and even the sign of the coefficient, suggesting this relationship is 7 not very stable (Appendix Figure 9). By examining pairs of points, the results show that models with similar pretraining performance can perform differently downstream (such observations exist for Llama-2-0.5B-20x, Llama-2-1B-20x, and OLMo-2-1B-20x). In addition, models with better pretraining performance (lower loss after pretraining) can perform better downstream (such observations exist for all five models) or worse downstream (such observations exist for Llama-2-0.5B-20x, Llama-21B-20x, and OLMo-2-1B-140x). For example, OLMo-2-1B-140x pretrained with weight decay 0.3 or 1.0 performs slightly worse after pretraining (achieving pretraining cross-entropy validation losses of 2.6208 and 2.7064, respectively) than the same model pretrained with weight decay 0.1 (which achieves pretraining cross-entropy validation loss of 2.6088), but the former two pretrained models perform noticeably better after fine-tuning (Figure 2, purple line). Altogether, these results indicate that pretraining performance is not necessarily predictive of downstream performance. Finding 2. The pretraining weight decay value that minimizes the pretraining cross-entropy validation loss does not necessarily lead to the best downstream performance."
        },
        {
            "title": "5 A Mechanistic Perspective on Weight Decay and Model Behavior",
            "content": "Prior work has shown that various factors can influence model plasticity, including the initialization state of model weights at the start of subsequent training, data representation (e.g., tokenization and categorical output representations), and model architecture (e.g., normalization layers) (Abagyan et al., 2025; Ash and Adams, 2020; Lyle et al., 2023). In Section 4, we find that weight decay also shapes model plasticity. In this section, we explore three mechanisms through which weight decay shapes model behavior: how weight decay shapes the pretrained models internal representations, attention matrices, and the extent to which it overfits the pretraining data. We also discuss how each mechanism might explain why weight decay improves language model plasticity."
        },
        {
            "title": "5.1 Weight decay encourages linearly separated representations",
            "content": "Inspired by previous findings that weight decay leads to more structured representations in vision models (Jacot et al., 2024), we investigate the effect of weight decay on the representations learned by pretrained language models. We pretrain models with varying weight decay, obtain the last-token embeddings for different types of text at given model layer, and train linear probe to classify these embeddings. We examine two tasks: classifying text based on sentiment (positive or negative movie reviews from the Stanford Sentiment Treebank dataset; Socher et al. (2013)) or topic (four types of news articles from the AG News dataset; Zhang et al. (2015)). The average accuracy of these linear probes over the two tasks is shown in Figure 4. Accuracy for individual tasks are in Appendix D.1. We observe that when given model is pretrained with higher weight decay, the accuracy of the linear probe trained on the models representations tends to be higher at every layer of the model. While this relationship is not perfectly monotonic (in some instances, slightly higher weight decay can lead to similar or slightly lower probing accuracy), it is generally consistent across weight decay values and model layers. In addition, we observe this relationship across model families, sizes, and training regimes (i.e., for all five model setups). Thus, through these linear probing experiments, we find that representations from models pretrained with higher weight decay result in higher 8 (a) Llama-2-0.5B-20x (b) Llama-2-4B-20x (c) OLMo-2-1B-20x (d) OLMo-2-1B-140x Figure 4: Weight decay encourages linearly separated representations. This figure depicts the accuracy of linear probes for sentiment and topic for models pretrained with different weight decay values. We observe that linear probing achieves better accuracy when models are pretrained with weight decay greater than the default 0.1. (a) Query-Key (b) Value-Projection (c) Query-Key (d) Value-Projection Figure 5: Weight decay reduces the rank of attention matrices. This figure depicts the average pseudo-rank (Appendix D.2.1) of the query-key (WQK) and value projection (WV ) matrices in layers 5 and 15 during the training of OLMo-2-1B models at 20 TPP. probing accuracies, indicating that these representations are more linearly separated and suggesting that models pretrained with higher weight decay form more structured internal representations. The finding that weight decay shapes the representations of pretrained language models points to potential explanation for why weight decay improves model plasticity (Section 4.2). Pretraining models with higher weight decay produces models with more structured representations, i.e., representations in which information is encoded in more linearly accessible form. As result, fine-tuning may focus on refining and aligning existing representations to the fine-tuning task rather than continuing to learn representations, effectively starting at better initialization and leading to improved downstream performance. This hypothesis is consistent with previous findings that weight decay produces representations that are more transferable to downstream tasks in computer vision (Lee et al., 2023). It is further supported by the observation that the linear separability of model representations (probing accuracy) is strongly positively correlated with downstream model performance (Appendix Figure 15)."
        },
        {
            "title": "5.2 Weight decay reduces the rank of attention matrices",
            "content": "Previous work by Kobayashi et al. (2024) provides theoretical argument that weight decay should reduce the rank of attention matrices. Recall that attention scores can be understood as bilinear KWQ Rnembednembed is the product of the query and key matrices, form WQKX where WQK = and RnembedT is the matrix of token embeddings (or hidden representations) for sequence of length . Now, the matrix WQK is naturally low-rank since its rank is at most dhead, which is usually significantly smaller than nembed. Kobayashi et al. (2024) argue that weight decay should further reduce the rank of WQK, as well as of the value-projection matrix WV = WP WV Rnembednembed. Concretely, they show that L2 regularization applied to the factored matrices WK and WQ becomes equivalent to nuclear norm regularization on their product WQK, which is known to induce low rank by promoting sparsity in the singular values. While Kobayashi et al. (2024) also provide empirical evidence on the Pile, their experiments were relatively small-scale from todays perspective. We now revisit the impact of weight decay on the rank of attention in our more modern setup. Weight decay reduces the rank of attention, but default weight decay yields near full-rank matrices. Figure 5 depicts the evolution of the pseudo-rank (Appendix D.2.1) of the attention matrices during the training of the OLMo-2-1B-20x models. From Figure 5, we observe that there is monotonic relationship between the weight decay parameter and the rank of the attention matrices, where larger weight decay values reduce the rank of both WQK and WV . However, unlike what is observed in Kobayashi et al. (2024), we see that the default weight decay parameter of 0.1 yields near full-rank matrices. This observation is further confirmed by Figure 18, which shows that the attention matrices in the fully trained OLMo-2-1B model are nearly full-rank. Attention matrices are differentially affected by weight decay. Another important observation from our experiments is that the rank of the matrix WQK seems to be significantly more sensitive to weight decay than WV . In our experiments, weight decay of λ = 1.0 reduces the rank of WQK by roughly factor of 2, which is common rank reduction observed in the literature on low-rank matrices. In contrast, the matrix WV is still close to full-rank even for weight decay value of 1.0. These results are especially pronounced for Llama-2 models depicted in Figure 16, where the rank of WV remains essentially stable up to weight decay value of 1.0, after which the rank collapsesa transition that correlates with significant drop in performance. Low-rank structure as driver of adaptability. The observation that increased weight decay leads to lower-rank attention matrices provides potential explanation for why weight decay improves In machine learning literature, low-rank constraints are canonical form of model plasticity. regularization that is often believed to encourage simpler, more robust hypotheses (Cai et al., 2010; Hu et al., 2022; Oymak et al., 2019). We conjecture that by encouraging WQK toward lower-rank configuration, weight decay may prevent the model from overfitting to high-dimensional noise in the pretraining distribution."
        },
        {
            "title": "5.3 Weight decay reduces overfitting on training data",
            "content": "Lastly, we explore how weight decay influences the extent to which the pretrained model overfits the pretraining data. Previous work has shown that weight decay can cause the forgetting of individual benchmark questions seen during pretraining (Bordt et al., 2025). In the context of model plasticity, the ability to learn new information tends to be associated with the forgetting of prior data, trade-off commonly referred to as the stability-plasticity dilemma (Elsayed and Mahmood, 2024; 10 Figure 6: Weight decay reduces overfitting on training data. The figure depicts the train-val gap (Equation 3) for OLMo-2-1B models trained at 20 TPP. Ibrahim et al., 2024; Kirkpatrick et al., 2017; Riemer et al., 2018). Building on these insights, we investigate how weight decay influences overfitting, which is closely related to the forgetting of training data, in pretrained models. To measure the degree to which pretrained model overfits the training data, we compute the difference between the loss on the validation data and that on the training data: Train-Val Gap = Validation Loss Training Loss (3) Here, the training loss is the average loss that the fully trained model encounters on the training data, which is distinct from the training loss curve or the final training loss value. model that does not overfit the training data would theoretically have train-val gap of zero. In practice, larger train-val gap indicates higher degree of overfitting on the training data, thus less forgetting of the training data. Figure 6 depicts the train-val gap for the OLMo-2 models trained at 20 TPP. We observe that the train-val gap decreases monotonically as the weight decay parameter is increased. This provides empirical evidence that models trained with larger weight decay values do indeed overfit the training data less. Finding 3. The pretraining weight decay hyperparameter has diverse mechanistic effects on model behavior. It encourages linearly separated representations, regularizes attention matrices, and reduces overfitting on the training data."
        },
        {
            "title": "6 Discussion and Concluding Remarks",
            "content": "This work provides multidimensional characterization of the effects of the weight decay hyperparameter within the modern language-model training lifecycle. While traditional perspectives have primarily viewed weight decay through the lenses of capacity control in over-parameterized regimes or optimization stability in single-epoch pretraining, our findings suggest that weight decay plays far more nuanced role in shaping model behavior. In particular, we showed that models with smaller weight decay achieve lower validation loss after pretraining (especially in the over-trained regime), but that models with larger weight decay benefit from improved plasticity, enabling them to perform best when fine-tuned on downstream tasks. Weight decay may shape model plasticity through several mechanisms, including promoting linearly separable representations, regularizing 11 attention matrix ranks, and reducing overfitting on the training data. Together, these findings reveal fundamental trade-offs in hyperparameter optimization. They also provide one of the first rigorous empirical demonstrations that selecting pretraining hyperparameters based solely on minimal pretraining validation loss can fail to yield the model with the highest performance on downstream tasks. The trade-offs we show mean that, in practice, the benefits of increased plasticity must be weighed against other effects that may depend on model size, training duration, and other parameters of the training setup. In heavily overtrained scenarios or for very large models trained for many steps Anthropic (2025); Comanici et al. (2025); Singh et al. (2025), the benefits of markedly lower pretraining validation loss may outweigh those of plasticity. In addition, weight decays diverse roles in training dynamics from plasticity (shown in this work) to optimization, training stability, convergence rate, and overfitting DAngelo et al. (2024); Hoffmann et al. (2022); Kosson et al. (2025) adds further complexity to model training decisions. Its optimal value for one objective can conflict with that for another, as observed when optimizing for pretraining versus downstream performance. single weight decay value may not satisfy multiple objectives, requiring weighing trade-offs and prioritizing objectives. Future work may investigate in more detail the trade-offs between stability and plasticity, and the extent to which our results hold in large-model and heavily overtrained scenarios. They may also investigate the role of weight decay in model plasticity for foundation models beyond language (e.g., multimodal foundation models) and for other downstream desiderata (e.g., safety alignment). Taken together, the findings in this work cast light on the complexity of hyperparameter tuning throughout the training process of modern language models and the multifaceted role that single optimization hyperparameter plays in shaping model behavior."
        },
        {
            "title": "References",
            "content": "Diana Abagyan, Alejandro R. Salamanca, Andres Felipe Cruz-Salinas, Kris Cao, Hangyu Lin, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. One tokenizer to rule them all: Emergent language plasticity via multilingual tokenizers. arXiv preprint arXiv:2506.10766, 2025. Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4, 2025. Jordan Ash and Ryan Adams. On warm-starting neural network training. NeurIPS, 2020. Tudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, and Claudia Clopath. study on the plasticity of neural networks. arXiv preprint arXiv:2106.00042, 2021. Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, and Joel Hestness. Power lines: Scaling laws for weight decay and batch size in LLM pre-training. arXiv preprint arXiv:2505.13738, 2025. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek LLM: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Sebastian Bordt and Martin Pawelczyk. Train once, answer all: Many pretraining experiments for the cost of one. arXiv preprint arXiv:2509.23383, 2025. 12 Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, and Ulrike von Luxburg. How much can we forget about data contamination? ICML, 2025. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, et al. Language models are few-shot learners. NeurIPS, 2020. Jian-Feng Cai, Emmanuel Candès, and Zuowei Shen. singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 2010. Albert Catalan-Tatjer, Niccolò Ajroldi, and Jonas Geiping. Training dynamics impact post-training quantization robustness. arXiv preprint arXiv:2510.06213, 2025. Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Adelani, Pontus Lars Erik Saito Stenetorp, Sebastian Riedel, and Mikel Artetxe. Improving language plasticity via pretraining with active forgetting. NeurIPS, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Xander Coetzer, Arné Schreuder, and Anna Sergeevna Bosman. Restoring neural network plasticity for faster transfer learning. Southern African Conference for Artificial Intelligence Research, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Francesco DAngelo, Maksym Andriushchenko, Aditya Vardhan Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? NeurIPS, 2024. Shibhansh Dohare, Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Rupam Mahmood, and Richard Sutton. Loss of plasticity in deep continual learning. Nature, 2024. Mohamed Elsayed and Rupam Mahmood. Addressing loss of plasticity and catastrophic forgetting in continual learning. ICLR, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Alex Hägele, Elie Bakouch, Atli Kosson, Leandro Von Werra, Martin Jaggi, et al. Scaling laws and compute-optimal training beyond fixed training durations. NeurIPS, 2024. Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. ICML, 2016. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, et al. Training compute-optimal large language models. NeurIPS, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. ICLR, 2022. Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models. arXiv preprint arXiv:2403.08763, 2024. Arthur Jacot, Peter Súkeník, Zihan Wang, and Marco Mondelli. Wide neural networks trained with weight decay provably exhibit neural collapse. arXiv preprint arXiv:2410.04887, 2024. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: dataset for biomedical research question answering. EMNLP, 2019. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Konwoo Kim, Suhas Kotha, Percy Liang, and Tatsunori Hashimoto. Pre-training under infinite compute. arXiv preprint arXiv:2509.14786, 2025. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 2017. Timo Klein, Lukas Miklautz, Kevin Sidak, Claudia Plant, and Sebastian Tschiatschek. Plasticity loss in deep reinforcement learning: survey. arXiv preprint arXiv:2411.04832, 2024. Seijin Kobayashi, Yassir Akram, and Johannes Von Oswald. Weight decay induces low-rank attention layers. NeurIPS, 2024. Atli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay balances learning across neural networks. ICML, 2024. Atli Kosson, Jeremy Welborn, Yang Liu, Martin Jaggi, and Xi Chen. Weight decay may matter more than muP for learning rate transfer in practice. arXiv preprint arXiv:2510.19093, 2025. Anders Krogh and John Hertz. simple weight decay can improve generalization. NeurIPS, 1991. Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continual learning via regenerative regularization. arXiv preprint arXiv:2308.11958, 2023. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale reading comprehension dataset from examinations. EMNLP, 2017. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Jae-Hun Lee, Doyoung Yoon, ByeongMoon Ji, Kyungyul Kim, and Sangheum Hwang. Rethinking evaluation protocols of visual representations learned via self-supervised learning. arXiv preprint arXiv:2304.03456, 2023. Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate. NeurIPS, 2020. 14 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. ICML, 2023. Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, and Wieland Brendel. LLMs on the line: Data determines loss-to-loss scaling laws. ICML, 2025. Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: Training plastic neural networks with backpropagation. ICML, 2018. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. EMNLP, 2025. Team OLMo Team, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 OLMo 2 Furious. arXiv preprint arXiv:2501.00656, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. NeurIPS, 2022. Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the Jacobian. arXiv preprint arXiv:1906.05392, 2019. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. MedMCQA: large-scale multi-subject multi-choice dataset for medical domain question answering. Conference on Health, Inference, and Learning, 2022. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Colin Mitchell, Margaret an Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. NeurIPS, 2024. Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, and Hanlin Zhang. EvoLM: In search of lost language model training dynamics. NeurIPS, 2025. Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. OpenAI GPT-5 system card. arXiv preprint arXiv:2601.03267, 2025. 15 Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. EMNLP, 2013. Tao Sun, Yuhao Huang, Li Shen, Kele Xu, and Bao Wang. Investigating the role of weight decay in enhancing nonconvex sgd. CVPR, 2025. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do large language model benchmarks test reliability? arXiv preprint arXiv:2502.03461, 2025. Xi Wang and Laurence Aitchison. How to set AdamWs weight decay as you scale model and dataset size. arXiv preprint arXiv:2405.13698, 2024. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-Pro: more robust and challenging multi-task language understanding benchmark. NeurIPS, 2024. Kaiyue Wen, Xingyu Dang, Kaifeng Lyu, Tengyu Ma, and Percy Liang. Fantastic Pretraining Optimizers and Where to Find Them II: From Weight Decay to Hyperball Optimization, 2025. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017. Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay regularization. arXiv preprint arXiv:1810.12281, 2018. Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean Foster, and Sham Kakade. How does critical batch size scale in pre-training? ICLR, 2025. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. NeurIPS, 2015. Pan Zhou, Xingyu Xie, Zhouchen Lin, and Shuicheng Yan. Towards understanding convergence and generalization of AdamW. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024."
        },
        {
            "title": "Appendices",
            "content": "A Pre-training A.1 Model architectures and training regimes . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Fine-tuning B.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Weight Decays Mechanistic Effects on Model Behavior D.1 Model representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Attention matrix rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 18 20 20 21 21 24 24 26 Pre-training A.1 Model architectures and training regimes We pretrain models from different families (Llama-2 and OLMo-2), of different scales (up to 4B), and under different training regimes (20 TPP and 140 TPP), yielding five model setups. Details are in Table 1. For each model setup, we pretrain variants with varying weight decay values. Model size Hidden size Intermediate size Vocab size Context length # Heads # Layers # Query groups Llama-2-0.5B Llama-2-1B Llama-2-4B OLMo-2-1B 0.5B 1536 3216 32000 2048 32 20 4 1.5B 2048 16384 100278 4096 16 16 4B 4096 7792 32000 2048 32 28 4 1B 2048 4896 32000 2048 32 22 4 Table 1: Model architectures. We use Llama-2 model architectures from Qi et al. (2025) and OLMo-2 model architecture from OLMo Team et al. (2024). Llama-2 models are trained at 20 TPP and OLMo-2 models are trained at 20 TPP and 140 TPP. A.2 Training details The training data size (measured in tokens) for each model is determined by the TPP ratio. Model Llama-2-0.5B-20x Llama-2-1B-20x Llama-2-4B-20x OLMo-2-1B-20x OLMo-2-1B-140x Model Size TPP Ratio Training Data Size 0.5B 1B 4B 1.5B 1.5B 10 BT 20 BT 80 BT 30 BT 210 BT 20 20 20 20 140 Table 2: Model configurations and training data sizes. To pre-train Llama-2 models, we use up to 8 A100 GPUs or 16 H100 GPUs. To pre-train OLMo-2 models, we use 8xH100 GPUs. The OLMo-2-1B-20x models are each trained for 2 days on single H100 node. The OLMo-2-1B-140x models are trained for 2 weeks on single H100 node. For all models, we use the AdamW optimizer and standard warmup-cosine learning rate schedule. The only exception is the OLMo-2-1B-140x models, which follow warmup-stable-decay schedule Hägele et al. (2024). Llama-2 models are pretrained using the repository from Qi et al. (2025). OLMo-2-1B models are pretrained using the official AllenAI repository. For each model, we train variants with various weight decay values specified in Table 3. Additional hyperparameters are in Table 4 and 5. Model Llama-2-0.5B-20x Llama-2-1B-20x Llama-2-4B-20x OLMo-2-1B-20x OLMo-2-1B-140x Weight Decay 9 values: {0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 3.0, 10.0} 9 values: {0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 3.0, 10.0} 2 values: {0.1, 1.0} 10 values: {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} 3 values: {0.1, 0.3, 1.0} Table 3: Weight decay values for each model. For Llama-2-4B-20x, we use the weight decay 0.1 model from Qi et al. (2025) and pretrain the weight decay 1.0 model. For OLMo-2-1B-140x, we use the weight decay 0.1 model from Bordt and Pawelczyk (2025) and pretrain the weight decay 0.3 and 1.0 models. Hyperparameter precision global_batch_size max_seq_length lr_warmup_ratio max_norm lr min_lr weight_decay beta1 beta2 epoch Llama-2-0.5B-20x Llama-2-1B-20x Llama-2-4B-20x bf16-mixed 512 2048 0.1 1 0.00025 0.000025 varies 0.9 0.95 bf16-mixed 512 2048 0.1 1 0.0002 0.00002 varies 0.9 0.95 1 bf16-mixed 1024 2048 0.1 1 0.00015 0.000015 varies 0.9 0.95 1 Table 4: Hyperparameters for Llama-2 model pretraining. For Llama-2 models, hyperparameter values are chosen following those in Qi et al. (2025), except for weight decay, which is varied as the independent variable in our experiments. 19 Hyperparameter OLMo-2-1B-20x OLMo-2-1B-140x precision global_batch_size max_seq_length lr_warmup_ratio max_norm lr min_lr weight_decay beta1 beta2 epoch bf16-mixed 512 4096 0.1 1 0.0004 0 varies 0.9 0.95 bf16-mixed 512 4096 0.1 1 0.0004 0.00004 varies 0.9 0.95 1 Table 5: Hyperparameters for OLMo-2 model pretraining. For OLMo-2 models, hyperparameter values follow the OLMo-2 defaults (OLMo Team et al., 2024), except for weight decay, which is varied as the independent variable in our experiments. Fine-tuning B.1 Datasets We fine-tune the pretrained models on datasets spanning various domains. We clean the fine-tuning training datasets, removing incoherent or questions that exceed the maximum input sequence length of the models. The size of the fine-tuning training set for each task and the test set used to subsequently evaluate model performance are shown in Table 6. Training set Test set = 395, 000 GSM8KPlatinum (n = 1, 209) + MATH (n = 5, 000) = 182, 555 MedMCQA (n = 4183) = 211, 168 PubMedQA (n = 1000) Task MetaMathQA MedMCQA PubMedQA MMLUProCoT = 123, 836 MMLUProCoT (n = 567) RACE SimpleScaling RACE (n = 4934) GSM8KPlatinum (n = 1, 209) + MATH (n = 5, 000) = 92, 737 = 54, 484 Table 6: Fine-tuning and evaluation datasets. MetaMathQA and SimpleScaling are evaluated on test sets of the GSM8KPlatinum Cobbe et al. (2021); Vendrow et al. (2025) and MATH Hendrycks et al. (2021) datasets because MetaMathQA and SimpleScaling contain questions that are augmented from the training sets of these two datasets. 20 B.2 Training details Hyperparameters cutoff_len batch_size learning_rate lr_scheduler_type warmup_ratio n_epochs 1B and under 2048 64 0.00001 cosine 0.1 3 4B 2048 64 0.0000075 cosine 0.1 3 Table 7: Hyperparameters for supervised fine-tuning. We set hyperparameters for fine-tuning following Qi et al. (2025). We set n_epochs = 3 based on results from Qi et al. (2025) indicating that this setting leads to the best downstream performance. We use smaller batch size (batch_size = 64) than Qi et al. (2025) due to computational constraints. B.3 Template We use the following template for supervised fine-tuning. Human: {question} Assistant: {response}"
        },
        {
            "title": "C Evaluation",
            "content": "21 (a) MetaMathQA (b) MedMCQA (c) PubMedQA (d) MMLUProCoT (e) RACE (f) SimpleScaling Figure 7: Model performance on each fine-tuning task evaluated using six metrics. Weight decay during pretraining improves model plasticity. 22 (g) Average accuracy over tasks (a) MetaMathQA (b) MedMCQA (c) PubMedQA (d) MMLUProCoT (e) RACE (f) SimpleScaling (g) Average accuracy over tasks Figure 8: Pretraining validation cross-entropy loss vs. fine-tuning accuracy for individual tasks. 23 Pretraining validation cross-entropy loss (pretraining performance) is not fully predictive of model performance after fine-tuning (downstream performance). Figure 9: Stability analysis for Pearson correlation coefficient. Pearson correlation is computed for each leave-one-out (LOO) subset in Figure 8g. The LOO correlation can change noticeably in magnitude and sign, suggesting that the correlation for the full set of data points in Figure 8g is rather unstable, which further supports the finding in that pretraining validation cross-entropy loss (pretraining performance) is not perfectly predictive of fine-tuning accuracy (downstream performance). Weight Decays Mechanistic Effects on Model Behavior D.1 Model representations (a) AG News (b) SST (c) Averaged over tasks Figure 10: Linear probing experiments for Llama-2-0.5B-20x. The train and test accuracies of the linear probes for the SST and AG News datasets and the average train and test accuracy over the two datasets. (a) AG News (b) SST (c) Averaged over tasks Figure 11: Linear probing experiments for Llama-2-1B-20x. The train and test accuracies of the linear probes for the SST and AG News datasets and the average train and test accuracy over the two datasets. 24 (a) AG News (b) SST (c) Averaged over tasks Figure 12: Linear probing experiments for Llama-2-4B-20x. The train and test accuracies of the linear probes for the SST and AG News datasets and the average train and test accuracy over the two datasets. (a) AG News (b) SST (c) Averaged over tasks Figure 13: Linear probing experiments for OLMo-2-1B-20x. The train and test accuracies of the linear probes for the SST and AG News datasets and the average train and test accuracy over the two datasets. (a) AG News (b) SST (c) Averaged over tasks Figure 14: Linear probing experiments for OLMo-2-1B-140x. The train and test accuracies of the linear probes for the SST and AG News datasets and the average train and test accuracy over the two datasets. Figure 15: Probing accuracy is highly predictive of downstream model performance. The x-axis is the best average probing accuracy of the model (highest probing accuracy out of all model layers). The y-axis the average accuracy of the model over all tasks after fine-tuning. Pretrained models with higher probing accuracies from the linear probing experiments tend to perform better downstream after fine-tuning. 25 D.2 Attention matrix rank D.2.1 Attention Pseudo-Rank Computation To quantify the effective dimensionality of weight matrices, we follow Kobayashi et al. (2024) and compute the pseudo-rank of the matrices. For matrix with singular values σ1 σ2 σn, the pseudo-rank is defined as the ratio k/n, where is the smallest integer satisfying: (cid:80)k (cid:80)n i=1 σi i=1 σi 0.95 (4) This metric represents the fraction of the largest singular values required to capture at least 95% of the sum of all singular values. In our analysis, we apply this computation to the product of the key-query matrices (WQK = KWQ) and the value-projection matrices (WV = WP WV ) to monitor the emergence of low-rank structures during training. D.2.2 Additional analyses on attention matrix rank (a) Query-Key (b) Value-Projection (c) Query-Key (d) Value-Projection Figure 16: Weight decay reduces the rank of attention matrices. The figure depicts the average pseudo-rank (Appendix D.2.1) of the query-key (WQK) and value projection (WV ) matrices in layers 5 and 15 of the fully-trained Llama-2 models at 20 TPP. 26 (a) Query-Key (b) Value-Projection Figure 17: Weight decay reduces the rank of attention matrices. This is for the OLMo models trained at 140 TPP. We observe that the rank of attention for weight decay 0.1 is generally smaller than that for both the 20 TPP and the fully trained OLMo-2-1B-0425 model. Hence, we conjecture that this is because the 140 TPP models were trained with warmup-stable-decay learning rate schedule, whereas the 1x and 144x models were trained with cosine learning rate schedule. While it has been shown that WSD leads to similar validation loss to cosine decay (Hägele et al., 2024), there is emerging evidence that there are important differences between the training dynamics of the two learning rate schedules (Catalan-Tatjer et al., 2025). (a) Query-Key (b) Value-Projection Figure 18: Training time does not reduce the rank of attention matrices. 27 (a) Input Layer (b) Intermediate Layer (c) Output Layer Figure 19: Weight decay reduces the norm of the weights of the model. The effect does not occur for the input layer, where the weights are not being decayed. This is for OLMo-2-1B models trained at 20 TPP."
        }
    ],
    "affiliations": [
        "Broad Institute",
        "Harvard University",
        "University of Tübingen"
    ]
}