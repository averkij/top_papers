{
    "paper_title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos",
    "authors": [
        "Kejian Zhu",
        "Zhuoran Jin",
        "Hongbang Yuan",
        "Jiachun Li",
        "Shangqing Tu",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as \"question frame\") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 4 1 4 0 . 6 0 5 2 : r MMR-V: Whats Left Unsaid? Benchmark for"
        },
        {
            "title": "Multimodal Deep Reasoning in Videos",
            "content": "Kejian Zhu1,2, Zhuoran Jin1,2, Hongbang Yuan1,2, Jiachun Li1,2, Shangqing Tu3 Pengfei Cao1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3Tsinghua University zhukejian2025@ia.ac.cn {zhuoran.jin, hongbang.yuan} @nlpr.ia.ac.cn {pengfei.cao, yubo.chen, kliu, jzhao} @nlpr.ia.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "The sequential structure of videos poses challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as question frame) and perceive few adjacent frames. To address this gap, we propose MMR-V: Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities. Project https://mmr-v.github.io/"
        },
        {
            "title": "Introduction",
            "content": "Recent models like OpenAIs o1 [1] and Deepseek-R1 [2] have significantly improved text reasoning ability through reinforcement learning. This has sparked growing interest in multimodal reasoning [3]. Models like o3 and o4-mini [4] have achieved impressive results on image reasoning tasks through tool use, integrating visual information into the reasoning process to enable deep reflection and evidence mining. However, most of these studies focus on images, with limited exploration of more challenging video reasoning tasks. Video naturally involves sequential and richer multimodal information, requiring models to perform reasoning and mine evidence over long-range, multi-frame. Since this capability is essential for real-world applications such as embodied intelligence and intelligent security monitoring [5; 6], it naturally raises an important question: can current MLLMs perform deep multimodal reasoning and mine evidence on complex videos like o3 on image tasks? Preprint. Under review. Figure 1: Examples showing the MMR-V tasks and the difference from previous video benchmarks. However, existing video benchmarks primarily focus on perception and understanding tasks [7; 8]. These tasks often only require locating frames mentioned in the question and understanding adjacent frames. For example, at the bottom of Figure 1, noticing the boy being hit by the metal frame is enough to understand why he ran into the girl. Such tasks fall short in evaluating multimodal reasoning abilities. We summarize their limitations as follows: (1) Limited frame context: Even for long videos, existing tasks often rely on just few adjacent frames, failing to exploit the long-range sequential structure of the video. (2) Lack of reasoning: Many questions can be answered through direct perception. (3) Unrealistic task: Simple perception and adjacent-frame understanding tasks do not meet the real-world demands for AI system strong capabilities. To address these shortcomings, we propose MMR-V Bench: Benchmark for Multi-modal Deep Reasoning in Videos. We present two examples to illustrate the key differences with previous video understanding benchmarks in Figure 1. MMR-V offers the following features: (1) Long-range, multi-frame reasoning: tasks involve multimodal reasoning over non-adjacent video frames to locate and analyze multiple evidences; (2) Beyond perception: questions cannot be answered by direct perception of question frame directly, requiring reasoning and the extraction of implications; (3) Reliability: All tasks are annotated manually, and potential subjective bias is reduced by crossreferencing the most popular video comments. (4) Confusability: We employ carefully designed annotation strategies to craft model-aligned distractor options, thereby ensuring confusability. Inspired by cognitive and psychological theories [9; 10; 11], such as Kahnemans Dual Process Theory [12], we categorize the tasks in MMR-V into implicit reasoning and explicit reasoning. The key distinction lies in whether the question requires reasoning beyond surface-level information to infer underlying implications. Explicit reasoning is defined as questions that can be solved using perceivable information from the video. For example, the task shown in Figure 1 requires noticing the two lighters hidden in the hand. Implicit reasoning requires extracting and interpreting the underlying subtext behind visual information. For example, in the implicit reasoning case shown in Figure 1, it requires inferring the underlying implication that the girls room number 7 symbolizes good luck. This is more of an assessment of EQ, testing whether the model can use its deep understanding of the world knowledge to make implicit and subconscious reasoning paths like humans. MMR-V comprises 317 videos and 1257 tasks. The videos span six major categories, with lengths ranging from 7 to 3771 seconds, with an average of 277 seconds. Tasks are further divided into 10 categories and subcategories. Each task is in multiple-choice format with approximately ten options on average. Tasks typically require reasoning over average 12 video frames, covering about 60% of video duration. All questions and correct answers are human-annotated and reviewed. Distractors are generated using carefully designed annotation strategy (Details in Section 3.2). We evaluated 9 proprietary models and 11 open-source models on MMR-V. The results reveal that even the best-performing model, o4-mini, achieved only 52.5% accuracy, highlighting the significant challenge MMR-V poses to current multimodal large language models. Our key findings are as follows. (1) Multimodal reasoning challenge: Our findings in Section 4.2 show that reasoning enhancement strategies (e.g., CoT and scaling test-time compute) yield limited improvements, indicating that MMR-V presents greater challenge to current multimodal reasoning models. Further error analysis in Section 4.5 shows that the CoT demanded in multimodal reasoning differs from those in textual reasoning. Current models tend to rely on textual reasoning based on visual information from the question frame and few adjacent frames, lacking the multimodal reasoning needed to locate and analyze evidence from long-range frames. This limitation hinders the overall reasoning performance. (2) More modality will benefit: We found that for models that support all modalities, adding additional audio modalities will improve the performance (Accuracy improved by 1.4%, 1.0%, and 1.0% for Gemini 2.0-Flash, Gemini 2.0-Flash-Thinking, and Phi-4-Multimodal-Instruct, respectively). (3) Human-model gap: In human experiments, we found that although models exhibit human-level performance on text reasoning tasks, there is still significant gap between model and human on multimodal, especially video, reasoning tasks. We hope MMR-V will inspire further research into enhancing multimodal reasoning capabilities in AI systems."
        },
        {
            "title": "2 Task Overview",
            "content": "The tasks in MMR-V require deeper multimodal reasoning. Unlike previous tasks such as math and puzzle problems [13; 14; 15], we argue that the scope of multimodal reasoning should be more broadly defined. Previous work focuses more on text-oriented reasoning based on perceived visual information. In contrast, our task requires integrating the various forms of visual evidences, such as artistic style, lighting, and depth, into the reasoning process. Even more challenging, it involves reasoning over long-range, multi-frame visual evidence. Videos have temporal dimension, which puts greater challenge on the ability to find clues in different frames through multimodal reasoning. 2.1 Definition for Implicit and Explicit Reasoning. We categorize reasoning tasks in MMR-V into Implicit Reasoning and Explicit Reasoning, inspired by Kahnemans Dual Process Theory [12] and other cognitive theories [9; 10; 11]. The most obvious difference is whether or not one needs to understand the subtext beneath the surface information. Secondly, implicit reasoning for human is often achieved by experience based on world knowledge, thus consuming little attention resources. Tasks are further divided into 10 categories and 33 subcategories. Six categories are shown in Figure 2, with the first row belonging to implicit and the second row is explicit. Further explanations and examples can be found in Appendix D. Implicit Reasoning focuses on incorporating hidden meanings behind visual information into reasoning. In these tasks, surface-level visual cues often conceal deeper layers of meaning, such as metaphor. Besides, for human, (implicit) operates automatically and quickly, with little or no effort and no sense of voluntary control. - Dual Process Theory. 3 Figure 2: Overview of six tasks in MMR-V Bench. Explicit Reasoning evaluates whether model can perform reasoning based on multimodal details explicitly presented across long-range, multi-frame of video. However, solving these tasks demands fine-grained perception and rigorous logical reasoning. (explicit) allocates attention to the effortful mental activities that demand it, including complex computations. - Dual Process Theory. 2.2 Implicit Reasoning Tasks Metaphor Understanding (MU): MU tasks evaluate the ability to reason about metaphors for entities or environment. For example, the case in Figure 2 interprets the metaphor of the brown coat. Theme Understanding (TU): TU assesses the ability to infer the main idea and attitude of the author through the full video. For example, the case in Figure 2 II asks what social issue the video reveals. Emotion Recognition (ER): ER tasks evaluate the ability to analyze character emotional states, as well as higher-level emotions such as the authors attitude and the audiences emotional response. For example, the case in Figure 2 III involves inferring whether the character feels happy at the end. Comment Matching (CM): CM task is to predict the most fitting audience comments for video based on criteria. For example, selecting which comment would be the most humorous after watching the video. Detailed example can be found in Appendix D.1. Implicit Symbol (IS): IS task is to interpret implicit symbols in the video, such as cultural elements. For example, inferring the ethnicity of the filming location. Details can be found in Appendix D.1. 2.3 Explicit Reasoning Tasks Causal Reasoning (CAR): CAR assesses the ability to reason about causal relationships in the video. For example, in Figure 2 IV, it involves inferring the reason why the girl is making card. Sequential Structure Reasoning (SSR): SSR tasks assess reasoning about temporal structure in video editing and storytelling. In the example from Figure 2 V, the task is to infer if the video is reversed. However, the creator of this video explains the video is played normally. Counterintuitive Reasoning (CIR): CIR tasks evaluate the ability to analyze information that contradicts common sense, requiring detailed cross-frame analysis. In the example from Figure 2 VI, the task is to reason the principle behind the counterintuitive magic trick. 4 Cross-modal Transfer Reasoning (CTR): To reason and match information out of the video that shares similar meaning. For example, find the quote with same theme of the video. Video Type and Intent (VTI): VTI tasks test the ability to infer key meta-level information such as the genre and communicative intent of the video from global perspective. For example, the case in Appendix D.2 infers the release time by reasoning the video is set during COVID-19."
        },
        {
            "title": "3 MMR-V Bench",
            "content": "To ensure that MMR-V effectively evaluate multimodal reasoning abilities, we follow three principles during construction: P1. Multi-frame: Questions require reference to long-range, multi-frame information, prompting the model to reason across multiple visual cues. P2. Deep reasoning: Answers should not be directly perceivable from the video; instead, they should demand understanding of the subtext or multimodal reasoning, reflecting deep comprehension of the content. P3. Realistic: Tasks should align with real-world question-answering needs, ensuring answers are consistent with common user understanding and free from individual cognitive biases or prejudices. 3.1 Video Collection We manually curated diverse original videos from Youtube with following checklist: (1) Avoidance of linear, descriptive content: We excluded videos with straightforward structures, such as daily recordings or sports broadcasts, in order to ensure that the tasks require deep reasoning over multiframes (For Principle P1). (2) Creative and thematically rich videos: We selected videos that are intentional designed and edited by creators, often conveying well-crafted themes. This ensures that the questions require interpretation beyond surface-level visual content (For Principle P2). (3)Alignment with real-world: Highly Popular Videos were preferred, which are indicated by active comment sections and audience engagement. This helps avoid biases introduced by niche content and ensures alignment with general user cognition (For Principle P3). (4) Diverse coverage: To further promote generalizability, we ensured broad coverage across video types, topics, and durations, allowing MMR-V to reflect the diversity of real-world video content (For Principle P3). As result, our final benchmark comprises 317 videos spanning six major categories: Animation, Film, Philosophy, TV, Life, and Art. The specific categories are shown in the Appendix C. Furthermore, for problems where audio might be helpful, we ensure that the videos include audio. 3.2 Data Annotation & Quality Assurance All tasks in MMR-V Bench are designed in multiple-choice format. There is one correct option and several wrong options. Make sure there are carefully crafted distractors among the wrong options. To ensure the quality and plausibility of these distractors, we designed three distinct distractors annotation strategies. (1) Str. 1: We prompt strong model GPT-4o [16] to directly answer the manually annotated question. If the model generate an incorrect answer (as verified by human annotators), that answer is retained as high-quality distractor. If correct, we combine human-written distractors with incorrect options generated by GPT-4o as distractors. (2) Str. 2: Given the question and correct answer annotated manually, GPT-4o is prompted to generate distractors. (3) Human annotators construct distractors manually. Models We conducted test using 100 questions, using three strategies to form three test-set with 100 multiple-choice tasks. As shown in Table 1, distractors generated by strategy 1 are more confusing, significantly increasing the difficulty and quality of our tasks. It is worth noting that in the above test process, when GPT-4o directly answered 100 tasks, the accuracy rate verified by humans was only 17%. This reflects the limitations of the current model in multimodal reasoning capabilities. Table 1: Performance on 100 questions annotated with different strategies (str.). GPT-4o Qwen-VL-7B 59% 37% 62% 42% 70% 51% Str. 3 Str. 2 Str. 1 To ensure high quality, we also developed an checklist based on the construction principles and invited human annotators to verify the accuracy and difficulty of the tasks using this checklist. We invited five annotators with at least bachelors degree to participate in the annotation and review 5 process. The checklist of MMR-V is shown in the Appendix B. The overall annotation process and the annotation platform can be found in Figure 7 and Figure 8 in the Appendix B. 3.3 Data Statistics MMR-V comprises total of 317 videos spanning wide range of content types, and includes 1,257 multiple-choice reasoning tasks. Each question is annotated with 7 to 11 candidate answers, with only one correct answer guaranteed. As illustrated in Figure 9a, the videos are categorized into six major domains, each encompassing fine-grained subcategories to ensure diversity in content, style, and semantics. The reasoning tasks in our benchmark are organized across three levels of granularity, reflecting different dimensions of reasoning complexity and modality. The distribution of task types across these levels is shown in Figure 9b. More information is shown in Table 2. Table 2: Dataset Statistic of MMR-V. Dataset Statistic Task Question Count Average Option Count Average Question Words Average Option Words Video Video Count Minimum Length (s) Maximum Length (s) Average Length (s) 1257 10 14 10 317 7"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings We conducted extensive evaluations on 9 proprietary and 11 open-source models as detailed in the Appendix E.1. Our main experiments were conducted under two settings: zero-shot and zero-shot + CoT [17], in order to examine whether reasoning enhances performance. For further analysis, we introduced the following categories of comparative models: (1) Models with different scales. (2) Thinking model and its base version. (e.g., Gemini-2.0-Flash and Gemini-2.0-Flash-Thinking). Multimodal Inputs: For models supporting full-modal inputs (e.g., Gemini-2.0-flash), we further compare their performance with and without audio input to evaluate its influence on reasoning results. Frame Selection: Since some models only support multiple images or short video clips, we standardized the number of input frames. Details of frame sampling are provided in Appendix E. Human Experiment: To provide meaningful upper bound for MMR-V and to examine the humanmodel gap, we invited participants with at least bachelor degree to conduct human experiment. We sampled 100 tasks GPT-4o answered incorrectly and 100 tasks it answered correctly for experiment. 4.2 Main Results We report the evaluation results in Table 3. Results indicate that the MMR-V Bench poses significant challenge to current multimodal large models. Even the best-performing model, o4-mini, achieves only 52.5% accuracy. Among open-source models, Gemma-3-27b-it performs the best, demonstrating relatively strong performance. However, there remains gap compared to proprietary models. Current reasoning enhancements have limitations on MMR-V. Results in Table 3 show that current reasoning enhancement strategies, which are relatively effective in textual domains, such as CoT prompt reasoning and scaling test-time compute (i.e., \"Thinking\" models), offer only limited gains on MMR-V. CoT brings only 0.57% average gain, and \"Thinking\" model improves just 2.4%. This indicates that MMR-V presents significant challenge to the multimodal reasoning capabilities of existing models. Analysis of sampled model responses shows that visual analysis accounts for only about 10% of the CoTs. This reveals that reasoning process of current model is mostly text-based (reasoning on questions and options), relying on visual perception of question frame, instead of integrating visual reasoning and evidence mining into CoTs. Several examples are provided in Appendix H, and further analysis in Section 4.5 supports similar findings. Model performance on MMR-V Bench exhibits clear scaling law effect. Smaller models under the same architecture perform poorly on tasks that require complex reasoning. For instance, larger models like Qwen2.5-VL-72B (39.1%) and GPT-4o (44%) outperform their smaller versions Qwen2.5VL-7B (30.1%) and GPT-4o-mini (34.8%), showing relative gains of 9% and 9.2%, respectively. Model performance across different tasks on MMR-V Bench. 6 Model Overall Implicit Explicit Art Life TV Film Ani. Phi. Tasks Video Categories LLaVA-Onevision LLaVA-Video NVILA-8B-Video Phi-4-multimodal-instruct Cogvlm2-video-llama3 Qwen2.5-VL-7B Intern3-8B Gemma-3-12b-it InternVL2.5-38B Qwen2.5-VL-72B Gemma-3-27b-it GPT-4o-mini-2024-07-18 Gemini-2.0-Flash (16 frames) Claude-3.5-Sonnet-20241022 GPT-4o-2024-11-20 Gemini-2.0-Flash-thinking GPT-4.1-2025-04-14 Gemini-2.0-Flash (512 frames) Gemini-2.5-Flash o4-mini-2025-04-16 6.5 18.4 25.5 26.7 25.6 30.1 33.6 34.0 39.9 39.1 42.0 34.8 42.6 43.3 44.0 45.0 46.6 48.0 51.2 52.5 8.8 17.6 25.3 27.6 26.1 32.4 32.9 34.2 39.7 40.4 41.1 35.2 44.3 44.2 46.1 43.5 48.9 49.9 50.5 52. Open-source models 7.0 19.1 26.2 29.4 25.4 33.7 35.5 37.8 43.8 41.3 46.5 9.6 18.1 24.2 31.2 26.2 36.2 33.4 37.6 43.7 42.8 44.7 5.4 15.4 23.9 19.4 26.1 20.8 28.6 24.0 29.9 33.4 30.3 Proprietary models 38.6 38.0 45.9 44.3 46.1 45.0 46.9 46.6 46.0 46.6 51.7 49.1 52.6 50.5 52.3 52.9 54.5 54.6 26.3 38.3 38.9 37.6 40.6 40.3 41.6 46.9 47.1 6.6 16.3 25.9 18.1 25.7 22.5 31.4 25.4 29.4 34.3 32. 26.3 40.0 39.1 44.0 37.1 41.7 42.9 45.7 46.0 6.5 14.4 17.3 19.4 15.5 20.9 23.0 19.4 30.4 28.9 31.7 29.5 30.9 33.8 38.1 34.5 43.2 36.7 45.3 48.2 3.4 11.2 21.3 19.2 18.3 18.1 22.6 24.9 28.8 28.2 32.2 25.4 32.2 31.1 37.3 31.6 35.6 36.7 39.5 40.1 9.5 13.2 23.5 25.9 24.7 29.6 31.7 25.9 30.4 29.1 35. 29.6 40.7 41.3 34.9 38.6 43.9 39.7 50.3 54.0 3.8 17.4 21.6 26.4 19.1 21.2 24.3 31.3 37.2 36.5 41.3 33.0 40.6 41.3 41.0 48.3 46.5 46.2 47.9 51.7 9.8 21.4 38.0 33.9 43.2 48.4 52.9 51.9 57.4 55.6 56.1 48.7 58.5 55.8 61.6 60.1 57.1 66.7 65.6 65.3 1.2 12.8 21.8 24.4 20.8 19.8 23.2 24.4 29.1 37.2 33. 18.6 24.4 44.4 32.6 25.6 34.9 31.4 34.9 27.9 Best Performance of Models Human 52.5 86.0 Baseline 54.6 80.6 47.1 91. 48.2 57.7 40.1 92.3 54.0 90.6 51.7 92.3 65.6 90.7 44.4 70. Table 3: Evaluation results (%) on MMR-V. Results under CoT prompting are highlighted in gray. The random accuracy on MMR-V Bench is approximately 10%. Bold and underlined values indicate the best performance among proprietary and open-source models, respectively. Firstly, the models performed better on implicit tasks than on explicit tasks (with an average gain of +7.9%). Through analysis of tasks and model responses, we found that in implicit tasks, video creators often embed implicit meanings throughout the entire video, resulting in abundant visual cues that can support reasoning. This reduces the requirements for multi-modal reasoning and clue localization. In contrast, explicit tasks demand finer-grained reasoning and the ability to identify specific evidence. For example, in the implicit task at the bottom of Figure 1, many frames provide clues suggesting that the girl symbolizes good luck (e.g., room number, flowers, lighting, weather, etc.). In contrast, the explicit task at the top contains only few key frames where the hidden lighter in magicians hand can be seen. Figure 3: Performance on different tasks. Secondly, the models performed particularly poorly on Counterintuitive Reasoning (CIR), Sequential Structure Reasoning (SSR), and Comment Matching (CM) tasks. For CIR and SSR tasks, poor performance mainly stems from the limited ability of current models to perform multi-frame reasoning. These two tasks require the model to reason on long-range videos, rather than relying on internal knowledge. However, instead of analyzing to locate evidences in other frames, models often rely on surface-level visual perception of the question frame, followed by textual reasoning over question and options. For CM tasks, the results highlight significant gap between model and human capabilities in implicit reasoning. While humans can infer underlying information such as humor and emotion with minimal cognitive effort [18], current models consistently fail to capture such subtleties. 7 Tasks Categories Overall Imp. Exp. Art Life TV Film Ani. Phi. Gemini-2.0 +audio Gemini-2.0-thinking +audio Phi-4-multimodal-instruct +audio 44. 45.0 30.9 38.3 42.6 32.2 44.01.4 46.21.9 38.30.0 31.00.1 31.60.6 42.31.6 41.00.4 61.12.6 29.14.7 31.6 46.01.0 48.41.8 39.70.9 31.72.8 33.92.3 44.45.8 42.75.6 62.42.3 32.67.0 19.2 27.71.0 31.31.9 18.11.3 15.43.0 19.70.5 24.51.4 27.81.4 37.33.4 26.72.3 25. 29.4 26.7 46.6 34.5 24.4 40. 40.6 26.4 38.6 33.9 24.4 48. 60.1 25.6 40.7 58.5 19.4 19. Table 4: The impact of adding audio modality on the performance (accuracy %) on different tasks. Human Performance. Humans achieved an average score of 86%, which highlights significant human-model gap. Although studies suggest that models achieved human-level performance on text tasks [2; 19], models still lag behind on multimodal reasoning tasks. Humans can identify clues in videos easily, while models tend to focus on question frames rather than exploring other evidence frames. Specially, unlike models, humans perform slightly worse on implicit tasks, which is mainly due to the challenges posed by highly abstract implicit understanding in art and philosophy. 4.3 Influence of Frames Count For Gemini-2.0-Flash, which supports long video inputs, we evaluated performance changes as the number of frames increases. As shown in Figure 4, accuracy improves with more frames, but the rate of improvement gradually slows. After sampling and observing the CoTs, it is found that the initial gains come from the addition of evidence frames, while the slowdown is mainly due to limited multi-frame reasoning ability of the model. Performance on implicit tasks continues to improve in later stages, as visual cues for such tasks are often dispersed throughout the video (as discussed in Section 4.2); more frames tend to provide more clues. In contrast, explicit clues are fewer and more localized. 4. Influence of Audio Input Figure 4: Accuracy with the increase of input frame counts. For models that support full-modal input, we compared their performance before and after incorporating the audio modality. As shown in Table 4, overall performance improved with the addition of audio. Specifically, Gemini 2.0-Flash, Gemini 2.0-Flash-Thinking, and Phi-4-multimodal-instruct showed improvements of 1.4%, 1.0%, and 1.0%, respectively. This suggests that advancing research on fully multimodal models is promising direction. 4.5 Error Analysis We sampled 100 incorrect responses from GPT-4o for error analysis. The main sources of errors can be categorized as follows: (1) Lack of Visual Reasoning: the model often failed to locate the correct evidence frames and lack of long-range, multi-frame visual reasoning. (2) Implicit Misinterpretation: revealing significant understanding gap between the model and human cognition. (3) Knowledge Insufficiency: the model lacks some intrinsic knowledge (4) Reasoning Error: during the multi-step deduction process. (5) Hallucination: the model introduced fake or unsupported information. (6) Output Formatting Issue: model refusals or formatting errors prevent answer extraction. Among error cases, Lack of Visual Reasoning accounts for the largest proportion. This indicates that Figure 5: Error analysis of GPT-4o. Figure 6: CoT content across different stages. The y-axis indicates the ratio of the 500 sampled CoTs that include analysis of these four types of content at each stage. current models still lack genuine multimodal reasoning capabilities. They tend to rely on text-based reasoning after briefly perceiving frames adjacent to the question, rather than engaging in deep, long-range, multi-frame video reasoning. Most existing reasoning models remain inadequate in integrating multimodal information into the reasoning process and performing thorough analysis. In contrast, o4-mini exhibits better reasoning paradigm, as shown in Figure 11 for comparison. We further analyzed model CoTs by categorizing each step into video or text analysis (e.g. options), with video analysis divided into question frame and other frame analysis (details in Appendix F). We sampled 500 CoTs from models, split each into 10 equal-length segments, and used GPT-4.1 to label each segment. As shown in Figure 6, where models further to the right perform better on MMR-V, models with better performance on MMR-V show more video analysis, especially on other frames (red line). Notably, 4o-mini stands out with strong analysis of non-question frames, highlighting the value of enhanced visual reasoning and tool use in multi-frame video reasoning tasks."
        },
        {
            "title": "5 Related Work",
            "content": "Video Understanding Benchmark. Existing video benchmarks primarily focus on evaluating models perception and intuitive understanding of visual elements in videos, such as action recognition [20; 21; 22; 23] and video description [24; 25]. Recent notable works, such as Video-MME [8], MVBench [26] and MMBench-Video [27], have extended video understanding to multiple task types and video types, enabling more comprehensive assessment of video understanding capabilities. Additionally, benchmarks like LVBench [28] and LongVideoBench [29] have introduced long-video questionanswering tasks. However, these tasks mainly evaluate whether model can accurately extract relevant information from long videos based on the given questions, while the subsequent steps remain largely perception-oriented. MMR-V is designed to assess whether model can perform multi-frame, long-span, multimodal autonomous reasoning on videos based on the given questions. Multimodal Reasoning. Recent advancements have greatly enhanced LLM reasoning [2; 1; 30; 31]. Many top LLMs perform well on complex reasoning tasks, but their evaluation focuses on textbased reasoning [32; 33; 34; 35; 36; 37]. MLLMs still lack thorough assessment in this area. Current multimodal reasoning benchmarks mainly involve mathematical or coding tasks in image form [14; 38; 39], which primarily test visual recognition followed by text reasoning. True multimodal reasoning requires integrating details like depth, texture, and audio for complex inference. MMR-V Bench aims to evaluate multimodal sequential reasoning in video tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces MMR-V: Benchmark for Multimodal Deep Reasoning in Videos. All tasks are annotated by human experts and designed to evaluate abilities of multimodal reasoning. MMR-V presents significant challenge to current models, with the best model performance still lagging 33.5% accuracy behind human. This highlights human-model gap in interpreting and reasoning about video information. Notably, o4-mini achieves the best results on MMR-V, suggesting that integrating visual reasoning into CoT and leveraging tool use is promising direction for tackling video reasoning tasks. We hope MMR-V will serve as reliable evaluation benchmark for the development of MLLMs and offer valuable insights into advancing multimodal reasoning research."
        },
        {
            "title": "References",
            "content": "[1] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al., Openai o1 system card, arXiv preprint arXiv:2412.16720, 2024. [2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] Y. Wang, S. Wu, Y. Zhang, S. Yan, Z. Liu, J. Luo, and H. Fei, Multimodal chain-of-thought reasoning: comprehensive survey, arXiv preprint arXiv:2503.12605, 2025. [4] OpenAI, Openai: Introducing openai o3 and o4-mini, 2025. [5] J. Hou, C. Wu, Z. Yuan, J. Tan, Q. Wang, and Y. Zhou, Research of intelligent home security surveillance system based on zigbee, in 2008 International Symposium on Intelligent Information Technology Application Workshops, pp. 554557, IEEE, 2008. [6] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie, Thinking in space: How multimodal large language models see, remember, and recall spaces, arXiv preprint arXiv:2412.14171, 2024. [7] J. Zhou, Y. Shu, B. Zhao, B. Wu, S. Xiao, X. Yang, Y. Xiong, B. Zhang, T. Huang, and Z. Liu, Mlvu: comprehensive benchmark for multi-task long video understanding, arXiv preprint arXiv:2406.04264, 2024. [8] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al., Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, arXiv preprint arXiv:2405.21075, 2024. [9] J. S. B. Evans, Heuristic and analytic processes in reasoning, British Journal of Psychology, vol. 75, no. 4, pp. 451468, 1984. [10] R. Sun, The clarion cognitive architecture: Extending cognitive modeling to social simulation, Cognition and multi-agent interaction, pp. 7999, 2006. [11] M. Polanyi, Personal knowledge. Routledge, 2012. [12] D. Kahneman, Thinking, fast and slow. macmillan, 2011. [13] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv preprint arXiv:2310.02255, 2023. [14] K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li, Measuring multimodal mathematical reasoning with math-vision dataset, Advances in Neural Information Processing Systems, vol. 37, pp. 9509595169, 2024. [15] F. Zhang, L. Wu, H. Bai, G. Lin, X. Li, X. Yu, Y. Wang, B. Chen, and J. Keung, Humaneval-v: Evaluating visual understanding and reasoning abilities of large multimodal models through coding tasks, arXiv preprint arXiv:2410.12381, 2024. [16] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [17] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., Chain-ofthought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 2482424837, 2022. [18] K. Krishna, Y. Chang, J. Wieting, and M. Iyyer, RankGen: Improving text generation with large ranking models, in Proceedings of EMNLP, pp. 199232, 2022. [19] OpenAI, Gpt-4 technical report, arXiv preprint arxiv:2303.08774, 2023. 10 [20] M. U. Khattak, M. F. Naeem, J. Hassan, M. Naseer, F. Tombari, F. S. Khan, and S. Khan, How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms, arXiv preprint arXiv:2405.03690, 2024. [21] K. Mangalam, R. Akshulakov, and J. Malik, Egoschema: diagnostic benchmark for very long-form video language understanding, Advances in Neural Information Processing Systems, vol. 36, pp. 4621246244, 2023. [22] V. Patraucean, L. Smaira, A. Gupta, A. Recasens, L. Markeeva, D. Banarse, S. Koppula, M. Malinowski, Y. Yang, C. Doersch, et al., Perception test: diagnostic benchmark for multimodal video models, Advances in Neural Information Processing Systems, vol. 36, pp. 4274842761, 2023. [23] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, Next-qa: Next phase of question-answering to explaining temporal actions, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021. [24] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang, Video question answering via gradually refined attention over appearance and motion, in Proceedings of the 25th ACM international conference on Multimedia, pp. 16451653, 2017. [25] J. Xu, T. Mei, T. Yao, and Y. Rui, Msr-vtt: large video description dataset for bridging video and language, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 52885296, 2016. [26] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, et al., Mvbench: comprehensive multi-modal video understanding benchmark, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. [27] X. Fang, K. Mao, H. Duan, X. Zhao, Y. Li, D. Lin, and K. Chen, Mmbench-video: longform multi-shot benchmark for holistic video understanding, Advances in Neural Information Processing Systems, vol. 37, pp. 8909889124, 2024. [28] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al., Lvbench: An extreme long video understanding benchmark, arXiv preprint arXiv:2406.08035, 2024. [29] H. Wu, D. Li, B. Chen, and J. Li, Longvideobench: benchmark for long-context interleaved video-language understanding, Advances in Neural Information Processing Systems, vol. 37, pp. 2882828857, 2024. [30] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al., Kimi k1. 5: Scaling reinforcement learning with llms, arXiv preprint arXiv:2501.12599, 2025. [31] Y. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang, W. Luo, and K. Zhang, Marco-o1: Towards open reasoning models for open-ended solutions, arXiv preprint arXiv:2411.14405, 2024. [32] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the math dataset, arXiv preprint arXiv:2103.03874, 2021. [33] Y. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, et al., Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks, arXiv preprint arXiv:2412.15204, 2024. [34] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, Training verifiers to solve math word problems, ArXiv preprint, vol. abs/2110.14168, 2021. [35] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level google-proof q&a benchmark, in First Conference on Language Modeling, 2024. [36] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al., Mmlu-pro: more robust and challenging multi-task language understanding benchmark, in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [37] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan, Swe-bench: Can language models resolve real-world github issues?, arXiv preprint arXiv:2310.06770, 2023. [38] W. Shi, Z. Hu, Y. Bin, J. Liu, Y. Yang, S.-K. Ng, L. Bing, and R. K.-W. Lee, Math-llava: Bootstrapping mathematical reasoning for multimodal large language models, arXiv preprint arXiv:2406.17294, 2024. [39] K. Ying, F. Meng, J. Wang, Z. Li, H. Lin, Y. Yang, H. Zhang, W. Zhang, Y. Lin, S. Liu, et al., Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi, arXiv preprint arXiv:2404.16006, 2024. [40] G. Lakoff and M. Johnson, Metaphors we live by. University of Chicago press, 2008. [41] OpenAI, Openai: Hello gpt-4o, 2024. [42] OpenAI, Gpt-4o mini: advancing cost-efficient intelligence, 2024. [43] OpenAI, Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. [44] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [45] Google DeepMind, Gemini 2.5: Our most intelligent ai model, March 2025. [46] Anthropic, Anthropic: Introducing claude 3.5 sonnet, 2024. [47] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu, Qwen2.5 technical report, CoRR, vol. abs/2412.15115, 2024. [48] A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière, L. Rouillard, T. Mesnard, G. Cideron, J. Grill, S. Ramos, E. Yvinec, M. Casbon, E. Pot, I. Penchev, G. Liu, F. Visin, K. Kenealy, L. Beyer, X. Zhai, A. Tsitsulin, R. Busa-Fekete, A. Feng, N. Sachdeva, B. Coleman, Y. Gao, B. Mustafa, I. Barr, E. Parisotto, D. Tian, M. Eyal, C. Cherry, J. Peter, D. Sinopalnikov, S. Bhupatiraju, R. Agarwal, M. Kazemi, D. Malkin, R. Kumar, D. Vilar, I. Brusilovsky, J. Luo, A. Steiner, A. Friesen, A. Sharma, A. Sharma, A. M. Gilady, A. Goedeckemeyer, A. Saade, A. Kolesnikov, A. Bendebury, A. Abdagic, A. Vadi, A. György, A. S. Pinto, A. Das, A. Bapna, A. Miech, A. Yang, A. Paterson, A. Shenoy, A. Chakrabarti, B. Piot, B. Wu, B. Shahriari, B. Petrini, C. Chen, C. L. Lan, C. A. ChoquetteChoo, C. Carey, C. Brick, D. Deutsch, D. Eisenbud, D. Cattle, D. Cheng, D. Paparas, D. S. Sreepathihalli, D. Reid, D. Tran, D. Zelle, E. Noland, E. Huizenga, E. Kharitonov, F. Liu, G. Amirkhanyan, G. Cameron, H. Hashemi, H. Klimczak-Plucinska, H. Singh, H. Mehta, H. T. Lehri, H. Hazimeh, I. Ballantyne, I. Szpektor, and I. Nardini, Gemma 3 technical report, CoRR, vol. abs/2503.19786, 2025. [49] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, et al., Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479, 2025. [50] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, et al., Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [51] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan, Video-llava: Learning united visual representation by alignment before projection, arXiv preprint arXiv:2311.10122, 2023. [52] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai, V. Chaudhary, C. Chen, D. Chen, D. Chen, J. Chen, W. Chen, Y. Chen, Y. Chen, Q. Dai, X. Dai, R. Fan, M. Gao, M. Gao, A. Garg, A. Goswami, J. Hao, A. Hendy, Y. Hu, X. Jin, M. Khademi, D. Kim, Y. J. Kim, G. Lee, J. Li, Y. Li, C. Liang, X. Lin, Z. Lin, M. Liu, Y. Liu, G. Lopez, C. Luo, P. Madan, V. Mazalov, A. Mitra, A. Mousavi, A. Nguyen, J. Pan, D. PerezBecker, J. Platin, T. Portet, K. Qiu, B. Ren, L. Ren, S. Roy, N. Shang, Y. Shen, S. Singhal, S. Som, X. Song, T. Sych, P. Vaddamanu, S. Wang, Y. Wang, Z. Wang, H. Wu, H. Xu, W. Xu, Y. Yang, Z. Yang, D. Yu, I. Zabir, J. Zhang, L. L. Zhang, Y. Zhang, and X. Zhou, Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, CoRR, vol. abs/2503.01743, 2025. [53] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, et al., Cogvlm2: Visual language models for image and video understanding, arXiv preprint arXiv:2408.16500, 2024. [54] Z. Liu, L. Zhu, B. Shi, Z. Zhang, Y. Lou, S. Yang, H. Xi, S. Cao, Y. Gu, D. Li, et al., Nvila: Efficient frontier visual language models, arXiv preprint arXiv:2412.04468, 2024."
        },
        {
            "title": "A Limitations",
            "content": "Figure 7: MMR-V Construction Pipeline. Despite our efforts to improve our work, several limitations remain. (1) Scaling MMR-V is challenging due to the high cost of manual annotation and verification, as all tasks and correct answers are curated and reviewed by human annotators. (2) Although we strive to cover wide range of video and task types, certain real-world categories (such as mystery, puzzle-solving, and gaming) are still underrepresented. (3) The majority of videos in MMR-V are in English, with only small proportion in other languages such as Chinese, French, Thai, and German, which constrains its multilingual applicability. We will further study and try to solve this issue in the future. MMR-V Construction B.1 Checklist According to the MMR-V construction principles introduced in the main text Section 3 , we wrote the following annotation checklist: (1) You are expected to watch the entire video before formulating any questions or answers. (2) Each question must require long-distance, multi-frame reasoning and cannot be answered through direct perception (ensuring compliance with Principles 1 and 2). (3) To ensure consistency with real-world user perception (Principle 3), annotators are encouraged to refer to the official interpretation of the original video author and user consensus (highly praised comments in the comment section) when writing or verifying the correct answer. This helps mitigate annotator bias and ensures that the reasoning task reflects the understanding of wider audience. B.2 Construction Pipeline In this section, we present the construction process of MMR-V Bench in macro sense. The whole process is divided into three stages: video collection, data annotation, and quality assurance. For video collection, we designed checklist to ensure the quality and diversity of videos in the Bench. \"High recognition interpretation?\" ensures that the questions raised and the annotated answers based 14 Figure 8: Annotation Platform of MMR-V. (a) Video categories. (b) Proportion of different tasks. Figure 9: (a) Video categories in MMR-V Bench. (b) Proportion of different tasks in MMR-V Bench. on the video have references that are consistent with public cognition (official interpretations or highly praised comments) to alleviate the subjective bias of the annotator. \"Is it Non-straightforward?\" ensures that the video is not straightforward narrative, which is conducive to increasing the reasoning difficulty of the question. For data annotation, as described in section 3.2 of the main text, we use gpt-4o to assist in annotation with interference options. Let the model generate the correct answer based on the question, and manually review to ensure that the correct answer generated by the model is different from the manual annotation. If they are different, the answer generated by gpt-4o is used as the interference item, otherwise the interference item is manually written. For quality assurance, we designed checklist for human reviewers to check the correctness and difficulty of the tasks. The annotation platform is shown in Figure 8. Diversity of MMR-V In this section, we show the diversity of MMR-V Bench, including video diversity and task diversity. For video, we show the six categories of videos in MMR-V in Figure 9a, including Life, Animation, Film, Art, TV, and Philosophy. At the same time, for each category, we divide it into several 15 Ability Type Ability Type L2 Ability Type L3 Metaphor Understanding (MU) Implicit Reasoning Theme Understanding (TU) Emotion Recognition (ER) Comment Matching (CM) Implicit Symbol (IS) Causal Reasoning (CAR) Explicit Reasoning Sequential Structure Reasoning (SSR) Counterintuitive Reasoning (CIR) Structural Metaphor, Orientational Metaphor, Ontological Metaphor, Creative Metaphor Philosophical Concepts, Social Issues, Personal Reflection, Everyday Topics, Video Naming Explicit Emotion, Implicit Emotion, Meta-emotion, Audience Emotion Humorous, Thought-provoking, Trending Cultural Symbols, Art Symbols, Other Symbols Forward Reasoning, Backward Reasoning Narrative Structure, Core Connecting Elements, Inference on Editing Techniques, Hallucination Magic Deconstruction or Special Effects Editing, Artistic Techniques, Humor and Exaggeration Cross-modal Transfer Reasoning (CTR) Video-to-Text, Video-to-Audio, Video-to-Video Video Type and Intent (VTI) Video Type, Video Intent Table 5: Three-level classification of tasks in MMR-V. subcategories to better understand the classification of video categories. Secondly, in section 3.3 we show the diversity of video length, ranging from 7 seconds to 3771 seconds. For tasks, we divide them into two parts, ten categories and 33 subcategories, three levels. The division of the first and second levels, as well as the proportion of different types of tasks, can be seen in Figure 9b."
        },
        {
            "title": "D Task Details",
            "content": "The tasks in MMR-V can be divided into three levels. Level 1: Implicit Reasoning & Explicit Reasoning. Level 2: Contains ten task classes. Level 3: Contains 33 task subclasses. Next, we will introduce these tasks with some task examples. D.1 Implicit Reasoning Tasks I. Metaphor Understanding (MU) For the definition of subclasses of the metaphor understanding task, we mainly refer to the book Metaphors We Live [40] By by George Lakoff and Mark Johnson, which introduces metaphor-related concepts in detail. I.1. Structural Metaphor Task Description: There are structural similarities between the subject and object. For example, time can be compared to flowing water, both of which have the structure of flow and passing away. Example Question: Question: What does the brown coat in the video symbolize? Options: (A) It is said to represent the familys long - lost fortune that they are still searching for. (B) The brown coat symbolizes the lost hope of the family because it was worn during difficult time. (C) It refers to coat that has been washed and taken out to dry, likely worn by the father.\", (D) It symbolizes the father in family, who protects his family in times of difficulty. (E) It represents the fear of the outside world. (F) The unfulfilled dreams of the children in the family as they always saw it as sign of something unattainable. (G) The brown coat in the video represents raincoat, used to protect the clothes inside from getting wet. 16 (H) The bad luck that has been following the family for generations. CorrectAnswer: (D) Video: father - 1 minute emotional award winning - video_url I.2. Orientational Metaphor Task Description: There are similarities in direction or composition between the subject and the metaphor, for example, walking up staircase is compared to ambition. Example Question: Question: Why does the dance, which is filled with artistry and beauty throughout, end with descent? Options: (A) There is connection between the fall and the creation at some point. (B) It represents dive to explore new depths, both literal and metaphorical. (C) It indicates the dancers exhaustion, capturing moment of fatigue. (D) It reflects the calmness of the ocean, evoking sense of tranquility. (E) It highlights the theme of rebirth, symbolizing renewal and transformation. (F) It represents the beauty of underwater life, showcasing its unique allure. (G) It symbolizes being weighed down by emotions, expressing inner turmoil. (H) It symbolizes the end of dream, marking moment of conclusion. (I) It shows the dancers connection to water, emphasizing fluidity and grace. (J) It symbolizes return to nature and surrender to lifes forces, embracing the natural flow. (K) It signifies the end of the dances energy, indicating point of culmination. CorrectAnswer: (A) Video: Falling - Underwater dance - video_url I.3. Ontological Metaphor Task Description: This metaphor involves viewing an abstract concept as concrete entity. Usually, the core concept of the entire video is turned into concrete entity to tell the story. Example Question: Question: The scene around 1:00 metaphorically represents what aspect of communities? Options: (A) Communities can build their resilience to setbacks by working together and adapting to new challenges. (B) Promoting individual success in competitive environments. (C) Building resilience through community partnerships. (D) Overcoming challenges for community progress. (E) Celebrating the individual achievements of community members. (F) Developing sustainable practices for environmental harmony. (G) Decision-making processes of community. (H) The interconnectedness of global communities. (I) Isolation of communities for self-sufficiency. (J) The role of external aid in community development. (K) Highlighting the diversity of cultures within community. CorrectAnswer: (A) Video: Resilience: Anticipate, organise, adapt - video_url I.4. Creative Metaphor Task Description: This metaphor is usually carefully designed by the author for specific video and needs to be understood in the context of the video. Example Question: Question: What is the pink fairy ball in the film? Options: 17 (A) Its toy the boy picked up on the street, having no special connection to his condition. (B) They are the microorganisms in this world, living in every corner. (C) The pink fairy ball represents the boys childhood dream of becoming fairy. (D) Its hallucination caused by lack of sleep, not related to antidepressants at all. (E) They are the boys toys, which he bought to help treat his depression. (F) It is the effect of the antidepressants the boy is taking, which helps him see many things with vitality and positive effects. (G) Its an advertisement prop for new product in the background of the scene. (H) The ball is sign of the boys wish to escape from his daily work routine. (I) The pink fairy ball is symbol of the citys upcoming festival decorations CorrectAnswer: (F) Video: Soft Rain Animated Short Film (2023) - video_url II. Theme Understanding (TU) II.1. Philosophical Concepts Task Description: The themes of the videos are usually about concepts and principles related to philosophy and psychology. Example Question: Question: What is the overall message that the animation aims to convey? Options: (A) It suggests happiness comes solely from financial achievements. (B) The animation emphasizes the need to avoid all responsibilities. (C) The animation aims to illustrate the ways to relieve stress. (D) It illustrates the mechanical process of water flow. (E) The animation encourages saving water to prevent wastage. (F) The animation conveys the importance of managing stress through self-care practices. (G) The animation highlights achieving success through hard work. (H) The animation suggests that ignoring stress leads to happiness. (I) The video underlines the significance of collective teamwork. (J) It depicts progress and growth through constant work. CorrectAnswer: (C) Video: The Stress Bucket - video_url II.2. Social Issues Task Description: The theme of the video is usually to reflect some problems existing in todays society and express strong appeal of the author. Example Question: Question: What social reality does this video satirize? Options: (A) The rise of environmental awareness in urban settings. (B) The video represents the bystander effect in society. (C) The economic disparities in urban vs. rural areas. (D) The challenges of modern relationship dynamics. (E) The impact of fashion trends on daily life. (F) The increasing complexity of urban development planning. (G) The need for infrastructure improvement and road safety. (H) The influence of social media on public behavior. (I) The rapid pace of technological advancement in transportation. (J) The shift in societal values towards individualism. CorrectAnswer: (B) Video: Stone 1 Minute Short Film Hot Shot - video_url 18 II.3. Personal Reflection Task Description: The author hopes that the video will inspire people to reflect on and resonate with things in their lives. Example Question: Question: What is the core concept that the film aims to convey? Options: (A) Romantic relationships in adolescence. (B) The importance of education institutions. (C) Overcoming supernatural challenges. (D) The dynamics of family disagreements. (E) Exploration of technological advancement. (F) Not to judge others too quickly. (G) Journey of superhero in saving the city. (H) Inter-species relations on Earth. (I) Power struggles in political leadership. (J) Historical recount of famous personality. CorrectAnswer: (F) Video: Award Winning SHORT FILMS Dont Judge BATTI Hindi Heart Touching Short Movies Content Ka Keeda - video_url II.4. Everyday Topics Task Description: The themes expressed in the videos are usually the sublimation of the insights and themes in daily life, such as praising maternal love, friendship, etc. Example Question: Question: What is implied by the contrast between the scenes around 0:47 and 1:11? Options: (A) The contrast shows that the mother is indecisive and cant make up her mind in crisis. (B) It demonstrates the fathers sense of responsibility and bravery, praising paternal love. (C) The contrast between the beginning and the end conveys sense of tragedy, criticizing the destruction of the ecological environment by humans. (D) It shows that the father wants to abandon the child when facing danger. (E) It shows the bravery of the bird in the background, facing authority head-on, and praises courage. (F) The mother still protects her child at all costs even in the face of danger, which praises maternal love. (G) It implies that the father is doing it for self - preservation rather than out of love for the child. (H) It shows that even when there are many birds, they do not appear very united, and in the face of danger, they become disorganized mess. CorrectAnswer: (F) Video: Mother 1 minute Sad Emotional Award Winning Iranian Short Film Animation Animated - video_url II.5. Video Naming Task Description: Come up with suitable title for this video or the core content of the video (dance, etc.). This tests the models control over the overall content and whether it can get the subtleties of the title like humans. Example Question: Question: \"Please come up with suitable name for this dance.\", Options: (A) The Dance of the Butterfly.\", (B) The Rhythm of the Phoenix.\", (C) The Grace of the Swan.\", 19 (D) The Spirit of the Dragon.\", (E) The Charm of the Peony.\", (F) The Step of the Tiger.\", (G) The Soul of Peacock\", (H) The Beat of the Forest.\", (I) The Leap of the Deer.\", (J) The Spin of the Star.\", (K) The Waltz of the Moon.\" CorrectAnswer: (G) Video: Yang Liping - The Soul of Peacock - Peacock Dance - Traditional Dance - video_url III. Emotion Recognition (ER) III.1. Explicit Emotion Task Description: Analyze the emotions of the characters in the video. Explicit emotions can usually be directly understood through facial expressions, body movements, etc. Example Question: Question: Summarize the boys emotional changes between 6:00 and 7:00. options: (A) Anger - Fear - Surprise and happiness (B) Sadness - Excitement - Helplessness (C) Disappointment - Let - down - Sorrow (D) Loneliness - Isolation - Solitude (E) Sadness - Grief - Mourning (F) Sadness - Shock - Surprise and happiness (G) Disappointment - Astonishment - Stupefaction (H) Loneliness - Isolation - Sorrow (I) Disappointment - Excitement - Helplessness correctAnswer: (F) Video: CGI Animated Short Film: \"Crunch\" by Gof Animation CGMeetup - video_url III.2. Implicit Emotion Task Description: Analyze the emotions of characters in the video. Implicit emotions usually need to be analyzed indirectly through the environment, style, etc. Example Question: Question: What kind of emotional atmosphere does the stage lighting create? options: (A) Solemn and sorrowful atmosphere. (B) Neutral and unemotional atmosphere. (C) Intense and dramatic atmosphere. (D) Joyful and festive atmosphere. (E) Sadness and loss. (F) Confident and empowering atmosphere. (G) Chaotic and confusing atmosphere. (H) Calm and serene atmosphere. (I) Playful and whimsical atmosphere. (J) Romantic and loving atmosphere. CorrectAnswer: (E) Video: Stages of GriefAVANTGARDE SHOW 2023 - video_url III.3. Meta-emotion Task Description: This part refers to the high-level emotions in the video, such as the emotions expressed by the author through the video, and the emotions expressed by the entire video. Example Question: Question: Summarize the meaning of this short film in one word. Options: [ (A) Creation (B) Transformation (C) Stress (D) Mutation (E) Metamorphosis (F) Growth (G) Rebirth\" (H) Destruction (I) Erosion (J) Development (K) Isolation (L) Conversion CorrectAnswer: (C) Stress - Shortfilm - video_url III.4. Audience Emotion Task Description: Analyze the emotions that viewers are most likely to feel after watching the video. This is more advanced and relatively easy for humans to sense. Including the perception of humor. Example Question: Question: What are the reasons for the high number of views on this video? Options: (A) The video features well-known celebrity who has large fan base, drawing lot of attention. (B) The dance style is extremely unique and has never been seen before, sparking curiosity. (C) People are under lot of stress and need videos that can help them unwind. (D) The background music is popular hit song that many people recognize and enjoy. (E) The video was released during major holiday season when people are more likely to watch videos. (F) The choreography is incredibly complex and impressive, showcasing the dancers skills. (G) The video has strong and inspiring message that resonates with wide audience. (H) The video was featured on popular TV show or news segment, driving more views. (I) The video was shared by large number of dance schools and communities, spreading its reach. (J) The video was part of viral challenge that encouraged people to share it. (K) The video has high-quality production values that make it stand out from other content. CorrectAnswer: \"(C) Satisfying and Relaxing Kinetic Sand ASMR shorts - video_url IV. Comment Matching (CM) IV.1. Humorous Task Description: The video will spark laughter because of certain comments, making the audience feel funny, testing whether the model can match it correctly. Example Question: Question: Based on this video, which of the following comments is likely to make people laugh? Options: (A) Did he just audition for water ballet? 21 (B) How many fish does it take to catch man? (C) Is there Walmart beneath the river? (D) The fish are holding grudge, watch out! (E) Now thats what call splash of creativity. (F) came for the fishing tips and stayed for the synchronized swimming. (G) That water has more personality than my neighbor! (H) Im starting to think hes part fish. (I) think the fish caught him instead. (J) Thats definitely land fish champion. (K) That fish will never trust humans again. CorrectAnswer: \"(C)\", He DI Lao - video_url IV.2. Thought-provoking Task Description: Some comments under the video will enhance peoples thinking and test whether the model can accurately understand. Example Question: Question: Which of the following statements can better explain the social reality expressed in this animation? Options: (A) The animation showcases an idealized view of advancement within corporate ladder. (B) The depiction highlights the dehumanization and mechanization of individuals in powerful social system. (C) It portrays the joy of discovering ones true passions through societal pressures. (D) The scenes show man achieving happiness through daily routine. (E) It represents personal ambition and the drive for success in individual careers. (F) The animation indicates the triumph of an individuals spirit in the face of adversity. (G) It reflects the disintegration of traditional family roles. (H) The animation shows the importance of family support in work-life balance. (I) It emphasizes the challenge of maintaining personal identity in urban settings. (J) We are all working for others without realizing it due to our own needs. (K) The animation illustrates the struggle with contemporary health issues. CorrectAnswer: (J) EL EMPLEO - video_url IV.3. Trending Task Description: It is relatively difficult to test whether the model can accurately infer and analyze the most popular comments under the video. Example Question: Question: Which of the following comments best summarizes the content conveyed by this film? Options: (A) Material possessions define ones value. (B) Selfless acts lead to rewards that surpass material wealth. (C) Loneliness is desirable state. (D) Personal gains are the ultimate goal of helping others. (E) Isolation is the path to personal growth. (F) True happiness is found through wealth accumulation. (G) Success comes from competitive behavior. (H) Sharing leads to financial prosperity. (I) He receives what money cant buy. (J) Adversity breeds stronger individuals. CorrectAnswer: (I) Unsung Hero - video_url 22 V. Implicit Symbol (IS) V.1. Cultural Symbols Task Description: Test whether the model can infer and analyze the cultural characteristics hidden under the surface visual elements of the video (such as nationality, festivals, customs, religion, etc.). Example Question: Question: The plaque inscribed with Dominating Three Continents that appears in the video is most likely to be found in the architecture of which of the following religions? Options: (A) Taoism (B) Shinto (C) Sikhism (D) Judaism (E) Islam (F) Christianity (G) Buddhism (H) Hinduism (I) Jainism (J) Zoroastrianism CorrectAnswer: (G) [4K] Hangzhou 2024 in the misty rain West Lake, Lingyin Temple, Night walking in Hefang Street - video_url V.2. Art Symbols Task Description: Test whether the model can infer and analyze the art-related characteristics hidden under the surface visual elements of the video (such as dance style, artistic skills, imitation, etc.). Example Question: Question: What is the shadow that appears in our view at 1:40 imitating? Options: (A) The shadow is imitating pole dancer. (B) The shadow is imitating person washing dog. (C) The shadow is imitating person brushing their hair. (D) The shadow is imitating someone playing violin. (E) The shadow is imitating two people engaged in conversation. (F) The shadow is imitating someone painting wall. (G) The shadow is imitating person feeding horse. (H) The shadow is imitating person washing their car. (I) The shadow is imitating dog barking at person. (J) The shadow is imitating someone performing magic trick. (K) The shadow is imitating person holding an umbrella. (L) The shadow is imitating someone walking large dog. CorrectAnswer: (A) LEAKED! Hilarious Shadow Puppets - AGT 2023 Early Release - video_url V.3. Other Symbols Task Description: Test whether the model can infer and analyze other special symbols (such as commercial advertisements, etc.) hidden under the surface visual elements of the video. Example Question: Question: \"What do you think the chimpanzee that appears multiple times in the film symbolizes?\", Options: (A) The chimpanzee symbolizes chaos and disruption in everyday life. (B) The chimpanzee symbolizes childhood fear. (C) The chimpanzee symbolizes technology invading personal (D) The chimpanzee symbolizes the unpredictability of fate. (E) The chimpanzee space. symbolizes glue company. (F) The chimpanzee symbolizes lost opportunities. (G) The chimpanzee symbolizes an obsession with social status. (H) The chimpanzee symbolizes environmental degradation. (I) The chimpanzee symbolizes the desire for freedom. (J) The chimpanzee symbolizes misunderstanding between people. (K) The chimpanzee symbolizes reliability and trust in friendships. CorrectAnswer: (E) All Gorilla glue ads - video_url D.2 Explicit Reasoning Tasks I. Causal Reasoning (CAR) I.1. Forward Reasoning Task Description: Forward reasoning can also be understood as the prediction of future events, including prediction of outcomes, prediction of content that has not yet appeared, etc. Example Question: Question: What is the speculated ending of the film? Options: (A) The movie concludes with an unexpected twist where the flowers reveal hidden secret. (B) The ending is cliffhanger, leaving the audience uncertain about the characters fate. (C) Her boyfriend passed away due to illness, leaving the girl devastated with grief. (D) The film wraps up with joyous family reunion. (E) The film ends with dramatic breakup as one character leaves with heavy heart. (F) The movie concludes with comedic mishap involving the flowers. (G) The ending shows tragic farewell as one character moves to new city. (H) The film ends with the revelation of long-lost sibling. (I) The story concludes with the characters embarking on spontaneous road trip. (J) The film ends on melancholic note, reflecting on lost opportunities. (K) The video closes with heartwarming reconciliation between the main characters after exchanging heartfelt notes and gestures. CorrectAnswer: (C) For Milo - AWARD WINNING 1 Minute Short film (2020) - video_url I.2. Backward Reasoning Task Description: Backward reasoning means finding the cause from the effect and inferring the reason why an event occurred. Example Question: Question: Why was the elderly black man warned by security at the beginning of the film? Options: (A) Mobile phones are not allowed for recording during magic shows. (B) He was trying to sell unauthorized merchandise. (C) He was recognized as local celebrity causing disruptions. (D) He was accused of stealing bicycle. (E) He was creating loud music disturbing the peace. (F) He was believed to have lost his entrance ticket. (G) He was inadvertently blocking the pathway. (H) He was associated with another person causing trouble nearby. (I) He was engaged in card tricks that security found suspicious. (J) He was loitering without purpose. CorrectAnswer: (A) Now You See Me Official Opening Scene (2013) - Mark Ruffalo, Morgan Freeman Movie HD - video_url 24 II. Sequential Structure Reasoning (SSR) II.1. Narrative Structure Task Description: Reasoning and analyzing the narrative order of the entire video, including the editing order, such as sequential, flashback, and interpolation. Example Question: Question: What kind of narrative sequence does the film employ? Options: (A) non-linear flashback sequence, where events are shown out of chronological order, often revealing backstory (B) parallel overlapping sequences, showing multiple storylines happening simultaneously with some overlap (C) cyclical narrative structure, repeating events or themes in circular pattern (D) linear narrative sequence, following straightforward progression from beginning to end (E) random jumps in the timeline, moving unpredictably between different points in time (F) interwoven thematic structure, weaving together different themes and ideas throughout the story (G) reverse chronological order, starting with the end and moving backwards in time (H) fragmented narrative, presenting the story in disjointed or broken segments (I) begins with flashback and then proceeds in chronological order (J) episodic progression, advancing the story through series of distinct episodes or chapters (K) multi-perspective narrative, telling the story from multiple characters points of view CorrectAnswer: (I) Identity SHORT FILM (Award Winning Inspirational Short) - video_url II.2. Core Connecting Elements Task Description: Videos with this type of question usually have key connecting element that runs through the entire video. It is carefully designed by the producer and tests the models inductive reasoning of the visual information of the entire video. Example Question: Question: What is the recurring element in the video, summarized in one word? Options: (A) Pareidolia (B) Smile (C) Alarm (D) Work (E) Mirror (F) Mundane (G) Routine (H) Suit (I) Coffee (J) Sleep (K) Bedroom (L) Portrait CorrectAnswer: (B) PAREIDOLIA - 1 Minute Short Film Award Winning - video_url II.3. Inference on Editing Techniques Task Description: These tasks evaluate the models deep analysis and multimodal reasoning about video editing strategies. Example Question: 25 Question: \"Please guess how many videos were needed to record the moment the man punched the punctured water ball at the beginning of the video?\", Options: (A) At least two separate takes would be needed. (B) At least one single take is needed. (C) Three separate takes are needed. (D) Four separate takes are needed. (E) Each scene can be captured in single continuous take. (F) Five separate takes are needed. (G) Six separate takes are needed. (H) Eight separate takes are needed. (I) Ten separate takes are required. (J) Twenty separate takes are necessary. (K) At least ten separate takes are needed. CorrectAnswer: (C) Playing With Time - video_url Note: The reasoning and analysis process of this question can refer to this disassembly video . II.4. Hallucination Task Description: Evaluate whether the model perceives various types of hallucinations when perceiving video content. Example Question: Question: How many dancers are there in the video? Options: (A) 0 (B) 1 (C) 2 (D) 3 (E) 4 (F) 5 (G) 6 (H) 7 (I) 8 (J) 9 (K) options before are all false CorrectAnswer: (B) Rat dance with falling body parts - video_url III. Counterintuitive Reasoning (CIR) III.1. Magic Deconstruction or Special Effects Editing Task Description: This type of video usually creates some impossible magical effects, but some are magic tricks, and some are editing and special effects, which require deeply reasoning. Example Question: Question: Starting at 4:35, how did the man achieve this magical effect in the magic trick? Options: (A) Sleight of hand technique with hidden ring, using dexterity to conceal and reveal the ring. (B) Utilizing mirror to confuse the audience, creating optical illusions through reflection. (C) distraction technique with smoke bomb, diverting attention with sudden burst of smoke. (D) special ring that retracts into fake thumb, using concealed mechanism to make the ring disappear. (E) Using magnet hidden in the sleeve, manipulating objects with magnetic force. (F) camera trick with video editing, altering footage to create the illusion of magic. (G) Sleight of hand technique with hidden string, using concealed thread to control objects. (H) The bottle inside the paper bag had already been altered to leave only the outer plastic skin. (I) Employing twin assistant to swap the ring, using look-alike to deceive the audience. (J) The use of an invisible thread, employing nearly undetectable line to move objects. (K) sound cue to mislead the audiences attention, using noise to distract from the real action. CorrectAnswer: (H) Level 1 to 100 Magic Tricks Anyone Can Do - video_url III.2. Artistic Techniques Task Description: This type of video usually creates some impossible scenes, but it is usually an artistic expression deliberately designed by the author. Example Question: Question: Why is the shadow on the boys face illuminated by sunlight at 1:06? Options: (A) Because the boy moves to position where strong light source is directly above him, not related to the girl. (B) Its just coincidence that the angle of the sun changes suddenly at that moment, and has nothing to do with any special meaning. (C) The sunlight illuminates the shadow because the cameraman adjusts the lighting equipment to create better visual effect. (D) The girls appearance brings good luck, and the sunlight representing good fortune clears away the gloom of bad luck in his world. (E) This is because the boy has walked into neighborhood with better weather and climate. (F) The sunlight lights up the shadow because there is hidden light - emitting device in the scene that is turned on at 1:06. (G) Its result of the special lens filter used during filming, which makes the shadow on the boys face appear to be lit by sunlight. (H) Because the boy didnt get hurt after falling and his mood improved, the sunlight is used to represent his improved mood. CorrectAnswer: (D) CGI Animated Short Film HD \"Jinxy Jenkins & Lucky Lou\" by Mike Bidinger & Michelle Kwon CGMeetup - video_url III.3. Humor and Exaggeration Task Description: common technique in humorous videos is to use exaggerated expressions that seem unreasonable, but there are some clues to understand the meaning. This type of question tests the models ability to reason about exaggerations and unusual techniques. Example Question: Question: Why does the first half of the scene look sunny but also show rain? Options: (A) It is sunshower, when rain falls while the sun is shining. (B) The character is dreaming of being both wet and warm. (C) There are rainclouds directly above while sunlight comes from the side. (D) It is snow instead of rain, reflecting the sunlight. (E) The effect is caused by morning fog and light refraction. (F) Its visual illusion caused by mist. (G) The character moved to different location quickly. (H) rainbow is forming which intensifies the sunlight. (I) Dew drops from trees reflect sunlight. (J) There are two unrelated weather animations merged together. (K) The man wet the bed, which caused the presence of water in his dream. CorrectAnswer: (K) It now makes sense - video_url 27 IV. Cross-modal Transfer Reasoning (CTR) Evaluate the ability to transfer reasoning from video to text, audio, video or image (for example, video-to-text: the theme of video may have the same meaning as famous quote) Task Description: Evaluate the ability to transfer reasoning from video to text (for example, the theme of video may have the same meaning as famous quote) Example Question: Question: Which of the following proverbs best explains the theme of this short film? Options: (A) When one door closes, another opens. (B) Opportunity knocks only once. (C) Time heals all wounds. (D) The early bird catches the worm. (E) Never judge book by its cover. (F) All that glitters is not gold. (G) The grass is always greener on the other side. (H) Actions speak louder than words. (I) stitch in time saves nine. (J) Beauty is in the eye of the beholder. (K) Absence makes the heart grow fonder. (L) penny saved is penny earned. CorrectAnswer: (E) Video: Award Winning SHORT FILMS Dont Judge BATTI Hindi Heart Touching Short Movies Content Ka Keeda - video_url V. Video Type and Intent (VTI) V.1. Video Type Task Description: Evaluate the models ability to analyze video types, such as commercials, science fiction films, comedies, etc. Example Question: Question: What type of video is this most likely to be? Options: (A) documentary about airplane technology (B) Advertisement for an ice-cream (C) drama set on an airplane (D) comedy film featuring an airline (E) An in-flight safety demonstration video (F) travel vlog featuring aerial views (G) science fiction movie on spaceship (H) This is an advertisement. (I) video tour of an airplane factory (J) virtual reality experience of flying (K) news segment on turbulence incidents CorrectAnswer: (H) Leo Messi vs Kobe Bryant - Legends on Board - Turkish Airlines - video_url V.2. Video Intent Task Description: Reasoning and analyzing the purpose and production intention of the video (e.g. what kind of product performance is promoted in commercial advertisement, etc.) Example Question: Question: Which year do you think this video was most likely released? Options: (A) 28 (B) 2017 (C) 2016 (D) 2015 (E) 2023 (F) 2019 (G) 2023 (H) 2020 (I) 2014 (J) 2013 CorrectAnswer: (H) Lockdown One Minute Short Film Challenge Film Riot - video_url"
        },
        {
            "title": "E Evaluation Details",
            "content": "E.1 Baselines The baselines include closed-source models: (1) GPT series: GPT-4o [41], GPT-4o-mini [42], and GPT-4.1 [43]; (2) Gemini series: Gemini-2.0-flash, Gemini-2.0-flash-thinking-01-21 [44], and Gemini-2.5-flash [45]; (3) Claude-3-5-Sonnet-20241022 [46]; (4) o4-mini [4]; open-source models: (1) Qwen series: Qwen2.5-VL (7B/72B-Instruct) [47]; (2) Gemma series: Gemma-3 (12B/27B) [48]; (3) InternVL series: Intern3-VL (8B/38B) [49]; (4) LLava series: LLava-Onevision-7B [50], VideoLLava-7B [51]; (5) Phi-4-multimodal-Instruct [52]; (6) Other video models: Cogvlm2-video-llama3chat [53], NVILA-8B-Video [54]. All local experiments are conducted on 4A100 80GB GPUs. E.2 Frame Selection We followed the official configurations of models that support multi-image input, as well as settings in previous works [8; 28], to define the number of input frames for each model. Specifically, we fixed the number of frames per model and sampled them evenly across the video duration. We sampled 8 frames for LLaVA-OneVision, Video-LLaVA, and NVILA-8B-Video, Phi-4-multimodal-instruct; 16 frames were sampled for Qwen2.5-VL-7B, Qwen2.5-Omni-7B, CogVLM2-Video-LLaMA3Chat, InternVL-8B, Gemma-3-it-12B and Gemini-2.0-Flash-Thinking; 32 frames were sampled for Qwen2.5-VL-72B, InternVL-38B, Gemma-3-it-27B , GPT-4o, GPT-4o-Mini, o4-mini, Gemini2.5-Flash-preview and Claude-3.5-Sonnet. Exceptionally, since Gemini-2.0-Flash supports long video and multimodal context inputs, we sampled one frame per second across each video, with maximum cap of 512 frames to ensure API stability. Additionally, to enable fair comparison with Gemini-2.0-Flash-Thinking, we also tested version of Gemini-2.0-Flash with 16 frames."
        },
        {
            "title": "F Details of CoT Analysis Experiment",
            "content": "This section presents the CoT analysis experiments discussed in Section 4.5. We use representative model CoT to illustrate 4 categories of analysis in Figure 10. Specifically, Text Analysis refers to the examination of textual information such as the question and options; Video Analysis focuses on the content of the video; Question Frame targets the specific frame referenced in the questionfor instance, the frame where the magician controls two flames; and Other Frame pertains to frames outside the scope of the question. In Figure 10, yellow, red, and blue represent text, question frame, and other frame analysis respectively. Red, blue, and green all represent video analysis. 29 Prompt for CoT Annotation You will be given models textual reply to video-based question along with the video frames. Your task is to determine four boolean labels for each chunk of the reply: 1. other frame desc: Does this chunk describe visual information from frames other than question frame? 2. question frame desc: Does this chunk correctly describe visual information from the question frame specified in the question? 3. video analysis: Does this chunk perform analysis of the video content? 4. text analysis: Does this chunk perform analysis of the text (e.g., question text, options) rather than visual content - The question frame refers to the specific frame(s) referenced by the question prompt. - Other-frame descriptions are visual details not present in the question frame but from other frames. - Video analysis includes describing trends, motions, or visual inference beyond plain description. - Text analysis includes reasoning over question text, options, or external text context. Respond strictly in JSON: { \"other frame desc\": true or false, \"question frame desc\": true or false, \"video analysis\": true or false, \"text analysis\": true or false } Question: {question} Reply Chunk: {chunk} Whole CoT Reply: {CoT} Table 6: CoT analysis prompt. 30 Figure 10: CoT example of experiments in Section 4.5. Yellow, red, and blue represent text, question frame, and other frame analysis respectively. Red, blue, and green all represent video analysis."
        },
        {
            "title": "G Evaluation Prompt",
            "content": "We evaluated two settings in the main experiment, zero shot and zero shot + CoT. The prompts used are as follows. Prompt for Zero-Shot Setting [[INSTRUCTION]] Please select the best answer to the following multiple-choice question based on the video. Only one option is the most accurate answer in relation to the question and the video. What is the correct answer to this question {Question} Options: {Options} [[END OF INSTRUCTION]] [[OUTPUT FORMAT]] Format your answer as follows: Please directly output the answer letter without thinking and explanation. If the correct option letters (A, B, C, D... ) for the multiple-choice question is X, give the final correct option number in the following format: \"[[X]]\" [[END OF OUTPUT FORMAT]] Table 7: Evaluation prompt for the Zero-Shot Setting. Prompt for Zero-Shot + CoT Setting [[INSTRUCTION]] Please select the best answer to the following multiple-choice question based on the video. Only one option is the most accurate answer in relation to the question and the video. What is the correct answer to this Question: {Question} Options: {Options} Lets think step by step. [[END OF INSTRUCTION]] [[OUTPUT FORMAT]] Format your answer as follows: Your thinking process. If the correct option letters (A, B, C, D... ) for the multiple-choice question is X, give the final correct option number in the following format: \"[[X]]\" [[END OF OUTPUT FORMAT]] Table 8: Evaluation prompt for the Zero-Shot + CoT Setting."
        },
        {
            "title": "H Case Study",
            "content": "In this section, we present reasoning processes and results from selected models on the MMR-V benchmark. Through these case studies, we aim to better illustrate the current shortcomings of models in multimodal reasoning tasks and provide insights that may inspire future research and advancements in this area. Firstly, there is comparison between good CoT and poor CoT in 11. Yellow highlights indicate text-based analysis, while green highlights denote video-based analysis. As shown, the upper CoT engages in deep multimodal reasoning grounded in the video content, ultimately leading to the correct answer. In contrast, the lower CoT performs only shallow perception of the video and relies heavily on deep reasoning over the textual content, resulting in text-dominant reasoning process that yields an incorrect answer. Note: The reasoning and analysis process of example Figure 15 can refer to this disassembly video . 32 Figure 11: comparison of CoTs from two models on the same task. Yellow and green indicate text and video analysis, respectively. As shown, o4-minis reasoning paradigm demonstrates deeper analysis of the video content. Figure 12: Error Case: Lack of Visual Reasoning. 34 Figure 13: Error Case: Lack of Visual Reasoning. 35 Figure 14: Error Case: Implicit Misinterpretation. Figure 15: Error Case: Reasoning Error. 37 Figure 16: MMR-V Construction Pipeline."
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
        "Tsinghua University"
    ]
}