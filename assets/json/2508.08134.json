{
    "paper_title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control",
    "authors": [
        "Zeqian Long",
        "Mingzhe Zheng",
        "Kunyu Feng",
        "Xinhua Zhang",
        "Hongyu Liu",
        "Harry Yang",
        "Linfeng Zhang",
        "Qifeng Chen",
        "Yue Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 3 1 8 0 . 8 0 5 2 : r Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided"
        },
        {
            "title": "Region Control",
            "content": "Zeqian Long2, Mingzhe Zheng1, Kunyu Feng1, Xinhua Zhang, Hongyu Liu1, Harry Yang1, Linfeng Zhang3, Qifeng Chen1(cid:66), Yue Ma1(cid:66) 1 HKUST 2 University of Illinois at Urbana-Champaign 3 Shanghai Jiao Tong University https://follow-your-shape.github.io/ Figure 1. We propose Follow-Your-Shape, trainingand mask-free image editing framework that excels at prompt-driven shape transformation. Our method enables flexible modification of arbitrary object shapes while strictly maintaining non-target content. The examples demonstrate both single-object and multi-object cases involving significant shape transformation."
        },
        {
            "title": "Abstract",
            "content": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded Equal contribution. (cid:66) Corresponding author. background quality. We propose Follow-Your-Shape, training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute Trajectory Divergence Map (TDM) by comparing tokenwise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate rigorous evaluation, we introduce ReShapeBench, 1 new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement. 1. Introduction Image editing methods based on diffusion [5, 10, 45] and flow models [1720, 32] have demonstrated considerable success in general tasks, yet they often fail when faced with complex, large-scale shape transformations. These models can struggle to modify an objects structure as intended or may inadvertently alter background regions, which degrades the overall image quality. This limitation indicates critical gap in their ability to perform precise structural edits while maintaining the integrity of unedited content. Meanwhile, the rise of AIGC foundation models has significantly accelerated progress across various downstream applications, including visual generation and editing [6, 11, 28 31, 33, 35, 50], animation and colorization [34, 57]. The primary cause for this limitation lies in the inadequacy of existing region control strategies [51, 61]. Methods that rely on external binary masks are often too rigid and struggle with the fine details of object boundaries. Alternatively, strategies that use cross-attention maps to infer editable regions are frequently unreliable, as these maps can be noisy and inconsistent. While unconditional Key-Value (KV) injection can preserve background structure, it lacks selectivity and tends to suppress the intended edits [2, 16]. We argue that breakthrough requires new approach: one that derives the editable region dynamically from the editing process itself by analyzing how the models behavior shifts between the source and target conditions. To address these challenges, we propose Follow-YourShape, training-free framework for precise and controllable shape editing. As illustrated in Figure 2, the core innovation of our pipeline is the Trajectory Divergence Map (TDM). The TDM is generated by computing the tokenwise difference between the denoising velocity fields of the source and target prompts. This map accurately localizes the regions intended for editing, which in turn guides selective KV injection mechanism to ensure that modifications are applied precisely where needed while preserving the background. However, directly applying TDM-guided injection across all denoising timesteps is suboptimal because the TDM can be unstable in the early, high-noise stages of the process. We therefore introduce Scheduled KV Injection strategy that adapts its guidance throughout the denoising process. As visualized in Figure 2, this staged approach first performs unconditional KV injection to stabilize the initial trajectory, and only then applies TDM-guided editing once coherent latent structure has formed. This scheduling ensures more robust and faithful editing outcome compared to direct application. To validate our approach, we introduce ReShapeBench, new benchmark for the rigorous evaluation of large-scale shape modifications. On this benchmark, Follow-YourShape demonstrates state-of-the-art performance. Quantitative results show that our method achieves superior background preservation, with PSNR [14] of 35.79 and an LPIPS [56] score of 8.23. Furthermore, our method attains the highest scores for both text-to-image alignment (CLIPSim [40]: 33.71) and overall aesthetic quality, measured using the LAION aesthetic predictor [43] (Aesthetic Score: 6.57), confirming its effectiveness. Our primary contributions are summarized as follows: novel and training-free editing framework, FollowYour-Shape, that uses Trajectory Divergence Map (TDM) to achieve precise, large-scale shape transformations while preserving background content. trajectory-guided scheduled injection strategy that improves editing stability by adapting the guidance mechanism throughout the denoising process. new benchmark, ReShapeBench, designed for the systematic evaluation of shape-aware image editing methods. 2. Related Work 2.1. Region-Specific Image Editing central challenge in image editing is localizing modifications to specific regions [4, 13, 21, 32, 58, 60]. Early methods often relied on explicit user-provided masks to delineate editable areas [1, 7, 24, 49, 53]. While effective for certain tasks, this approach requires manual annotation, limiting its applicability. To address this, subsequent work explored techniques to infer editable regions directly from text prompts. Methods such as Prompt-to-Prompt [12] and Plug-and-Play [45] manipulate cross-attention maps to associate textual tokens with spatial areas, enabling localized edits without explicit masks. Other approaches, such as DiffEdit [9], generate mask by computing differences between diffusion model predictions conditioned on source and target prompts. However, attention-based localization can be imprecise and unstable, especially during large-scale shape transformations where object boundaries change significantly [5, 37]. In contrast, Follow-Your-Shape provides training-free and mask-free method for identifying editable regions directly from the models behavior, avoiding the need for external masks or noisy attention maps. 2 2.2. Structure Preservation via Inversion and Feature Reuse Preserving non-target regions is equally critical for highfidelity editing, and this is closely tied to the quality of the models inversion process. For diffusion models, significant research has focused on improving DDIM inversion [44] to better reconstruct source image from noise. Previous works like null-text inversion [36] and optimization-based methods [47] aim to reduce the discrepancy between the reconstruction and editing trajectories. With the shift toward flow-based models, inversion fidelity has become even more important due to their deterministic nature. Methods like RF-Solver [51] achieve more accurate reconstructions by incorporating higher-order derivative information. Beyond improving inversion, another line of work focuses on explicitly reusing modules or features from the source images generation process [27, 54, 59]. Techniques based on Key-Value (KV) caching [2, 61] or feature injection [10, 51] enforce structural consistency by propagating source-image features into the new generation process. In contrast to prior methods that rely on simple heuristics, Follow-Your-Shape employs trajectory-guided scheduled injection strategy to achieve more precise, content-aware control. 3. Methodology Our goal is to enable precise object shape-aware editing while strictly preserving the background. Motivated by the limitations of existing region control strategies and the need for more adaptive mechanism, we introduce Trajectory Divergence Map (TDM) that quantifies token-wise semantic deviation between inversion and editing trajectories, as shown in Figure 2. The overall pipeline of Follow-YourShape is shown in Figure 3. 3.1. Motivation Effective image editing requires precise balance between introducing new content and preserving the original structure. As illustrated in Figure 2 (Top), traditional structurepreserving editing approaches often produce unstable denoising trajectories that deviate significantly from the stable reconstruction path, leading to severe structural degradation and undesired artifacts. Moreover, prior methods for localizing edits have notable drawbacks: Binary Segmentation Masks: Rely on external tools, introducing overhead and dependency on mask quality. Their rigid boundaries hinder large-scale shape changes and often produce artifacts. Cross-Attention Masks: Inferred from model attention, these maps are often noisy and inconsistent, proving unreliable for localizing edits, especially during significant shape transformations. Unconditional Feature Injection: This strategy preserves structure by globally injecting source features, but its lack of selectivity suppresses intentional edits, creating conflict between editability and consistency. To address these limitations, we propose new approach from dynamical systems perspective. We posit that the semantic difference between the source and target concepts can be measured by the divergence between their respective denoising trajectories. Based on this, we achieved precise and mask-free method (shown in Figure 3) to stabilize the editing trajectory and perform targeted, shape-aware modifications without relying on external masks or rigid heuristics. Figure 2. Motivation for Trajectory Divergence Map (TDM) Guided Editing. Top: Standard editing methods (red) often produce unstable trajectories compared to the stable reconstruction path (orange), leading to distorted outputs. Middle: Our staged approach first stabilizes the editing trajectory before using the TDM to guide it toward the target concept. This method supports diverse shape modifications (dashed lines). Bottom: The TDM visualizes the dynamically localized editing region across different timesteps, with different border colors correspond to different stages. 3 Figure 3. Overview of our proposed pipeline. Given source image and corresponding prompt, we first perform inversion to obtain the initial noisy latent code xT . The editing process is then divided into three stages. In Stage 1, we stabilize the initial denoising trajectory. This is achieved by injecting key-value (KV) features from the inversion path into the denoising model during its initial steps. In Stage 2, we compute Trajectory Divergence Map (TDM) by comparing the denoising trajectories generated from the source and edit prompts. This map is then processed to precisely identify the regions intended for editing. In Stage 3, we perform the final edit. Guided by the TDM, blended KV features are injected into the final attention blocks of the denoising model to introduce the new semantics. Simultaneously, ControlNet conditions are supplied to ensure the edited regions conform to the original structure. 3.2. Follow-Your-Shape predicted under the two prompts: Our objective is to perform shape-aware edits by selectively preserving background and non-target regions. We achieve this through staged editing process that combines scheduled Key-Value (KV) injection with structural guidance, where the edit is localized by the Trajectory Divergence Map (TDM). 3.2.1. Trajectory Divergence Map Our approach is grounded in the perspective of flow trajectories within the latent space, extending concepts from flowmatching frameworks to the inference setting. As illustrated in Figure 2 (Top), standard reconstruction follows stable denoising trajectory guided by the source prompt csrc. In an editing task, conditioning on target prompt ctgt alters the velocity field, causing the denoising trajectory to deviate from this initial path. We posit that the magnitude of this deviation spatially localizes the semantic difference between the two prompts. Regions intended for modification will exhibit significant divergence, while background areas will follow nearly identical trajectories. To formalize this, let {xt}T t=0 be the latent sequence from the source image inversion, and let {zt}T t=0 be the corresponding sequence during the editing (denoising) process. We define the tokenwise Trajectory Divergence Map (TDM) δt at timestep as the L2 norm of the difference between the velocity vectors δ(i) = (cid:13) (cid:13)vθ(z(i) (cid:13) , t, ctgt) vθ(x(i) , t, csrc) (cid:13) (cid:13) (cid:13)2 , (1) where the velocity fields are evaluated at their respective trajectory latents, zt and xt. To enhance interpretability and prepare the map for temporal aggregation, we apply min-max normalization across all spatial tokens at each timestep: δ(i) = δ(i) minj δ(j) maxj δ(j) minj δ(j) . (2) As shown in the bottom panel of Figure 2, this produces normalized TDM, {δ(i) }, which quantifies the localized editing strength on scale of [0, 1]. 3.2.2. Scheduled KV Injection and Mask Generation Directly applying TDM-guided injection across all timesteps is suboptimal due to the instability of the TDM in early, high-noise regimes. To address this, we introduce scheduled injection strategy that partitions the denoising steps into three distinct phases, adapting the guidance mechanism to the state of the latent variable. Stage 1: Initial Trajectory Stabilization. For an initial set of kfront timesteps, we perform unconditional KV injection from the source inversion path across all spatial tokens. This operation enforces global reconstruction objective, equivalent to setting the edit mask MS = 0, which stabilizes the trajectory and prevents semantic drift while the latent representation zt is still dominated by noise. Stage 2: TDM-Guided Injection. Once stable latent structure has emerged, for the subsequent timesteps ahead of ktail, we activate TDM-guided injection. To form temporally consistent and spatially coherent edit mask, we first aggregate the normalized per-step TDMs {δt} from Eq. 2 over an editing window . This is achieved via softmaxweighted temporal fusion for each token i: ˆδ(i) = (cid:88) tT α(i) δ(i) , where α(i) = exp(δ(i) ) tT exp(δ(i) ) . (cid:80) (3) The resulting map ˆδ is further refined via convolution with Gaussian kernel Gσ to produce the final edit mask MS [0, 1]HW : MS = Gσ ˆδ. (4) The smoothed attention map is then binarized with threshold of τ = 0.35 to produce the final mask, MS. This binary mask enables selective fusion of Key-Value (KV) features. For regions where the mask is active (MS = 1), we utilize the target features (Ktgt, Vtgt) computed in the current denoising step. Conversely, for inactive regions (MS = 0), we inject the corresponding source features (K inv, inv) that were stored during the inversion process. This featureblending operation is formulated as: MS Vtgt + (1 MS) inv. (5) An analogous operation is performed for the key feature as demonstrated in Algorithm 1. Stage3: Structural and Semantic Conformance. Our framework ensures editing conformance by combining explicit structural guidance via ControlNet with semantic preservation through our TDM-guided feature injection, which extends the principles of RF-Edit. To enforce strong geometric constraints, ControlNet conditions the process on structural information ccond by injecting residual stream into each block of the denoising model vθ. For latent representation zt at given block, the output is computed as: Algorithm 1 Region-Controlled Editing Input: Inference ), vtgt(z(i) {vsrc(x(i) }N {K inv , inv t=0, durations {kfront, ktail} target prompt ctgt, steps , )}N"
        },
        {
            "title": "Source",
            "content": "i=1,"
        },
        {
            "title": "Predicted",
            "content": "inversion velocities features schedule phase 1: for = down to 0 do 2: if > kfront then MS 0 3: 4: 5: 6: 7: 8: 9: 10: 11: minj δ(j) else if > kfront ktail then (cid:13) , t) vsrc(x(i) (cid:13)vtgt(z(i) δ(i) (cid:13) δ(i) δ(i) ˆδ(i) (cid:80) tT MS Gσ ˆδ MS 1[MS > τ ] minj δ(j) exp(δ(i) ) tT exp(δ(i) ) maxj δ(j) (cid:80) , t, csrc) (cid:13) (cid:13) (cid:13) δ(i) else MS 1 end if MS Ktgt + (1 MS) inv MS Vtgt + (1 MS) inv 12: 13: 14: 15: end for 16: return , out is computed using the blended key-value pairs from Eq. 5: (7) out = Attention(Qtgt, , ). By modulating the injection with our soft mask MS, we achieve fine-grained control, preserving non-target regions identified by the TDM while allowing for significant shape modifications in the edit region. This synergy between ControlNets geometric enforcement and our TDM-guided semantic preservation enables precise, high-fidelity edits. 4. Experiment 4.1. Experimental Setup In our experiment, we use the open-sourced image generation model FLUX.1-[dev] [18] as our base, and conduct all experiments in PyTorch on an NVIDIA A100 GPU with 40GB memory. For hyperparameters, we set the timestep to 28, guidance scale to 2.0, and kfront to 2. We also apply multi-ControlNet conditioning using Depth and Canny maps, with injection timing set in the normalized denoising interval [0.1, 0.7], and respective strengths of 2.5 and 3.5. = Block(zt) + β ControlNetBlock(zt, ccond), (6) 4.2. ReShapeBench Construction where β controls the guidance strength. Concurrently, our feature injection mechanism builds on RF-Edits background preservation by replacing the standard self-attention with TDM-guided variant. The modified attention output 4.2.1. Limitations While several benchmarks have been introduced to evaluate image editing methods [15, 52, 55], they present certain limitations for our use case. Notably, PIE-Bench [15], 5 Figure 4. Qualitative comparisons on various shape-aware editing cases. Follow-Your-Shape successfully performs large-scale shape transformations while preserving the background, demonstrating advantages in both editing ability and visual consistency over existing baselines. Method MasaCtrl [5] PnpInversion [45] Dit4Edit [10] Flux-Kontext [19] KV-Edit [61] RF-Solver [51] FlowEdit [17] Ours Image Quality Background Preservation Text Align Aesthetic Score PSNR LPIPS 103 CLIP Sim 5.83 6.11 6.14 6.53 6.51 6.52 6.42 6. 23.54 24.77 24.36 32.91 34.73 33.28 32.46 35.79 125.36 102.91 83.75 18.35 16.42 17.53 18.92 8. 20.84 19.23 22.66 28.53 26.97 30.41 28.94 33.71 Table 1. Comparison with the state-of-the-art methods on ReShapeBench. We evaluate the assessment on the image quality, background preservation, and text align, all of them implemented using official open-source code. 6 Image Quality Background Preservation Text Align Aesthetic Score PSNR LPIPS 103 CLIP Sim tual consistency. Further details are available in the Supplementary Materials. kfront 0 1 2 3 4 6.51 6.55 6.57 6.52 6.48 32.79 34.38 35.79 31.25 30.41 10.04 9.88 8.23 10.52 12.37 31.05 32.56 33.71 29.41 27. Table 2. Ablation study on different kfront. The results show that with the increase of kfront, the editing results exhibit trend of first improving and then degrading, and the kfront = 2 is the best. ControlNet modules were disabled in this experiment to isolate the effect of kfront. prominent benchmark for prompt-driven editing with 700 images, has two key shortcomings. First, its concise prompts lack the detail required for fine-grained, shapeaware edits. Second, its broad scopecovering object replacement, stylization, and background changesmakes it less focused for the specific evaluation of shape transformation capabilities. To overcome these issues, we introduce ReShapeBench. 4.2.2. Image Collection ReShapeBench comprises 120 newly collected images organized into two primary subsets and one evaluation set. The first subset contains 70 images from online sources, each featuring single, prominent object with well-defined boundaries suitable for precise shape editing. The second subset consists of 50 images with multiple objects, designed to test models ability to perform targeted, mask-free edits. Finally, general evaluation set of 50 images was created by curating selection from the first two subsets and adding high-quality examples from PIE-Bench. This set is designed to assess model generalization across diverse shape-editing scenarios. All images are standardized to resolution of 512 512. 4.2.3. Refined Text Prompts To address the issue of overly simple prompts in existing benchmarks, every image in ReShapeBench is paired with detailed source and target prompts. Both prompt types adhere to consistent four-sentence structure: the first sentence provides general summary, the second describes the foreground object(s), the third details the background, and the fourth describes the overall scene. When creating edit prompts, only the relevant attributes (such as object identity or features) are modified. These prompts are manually crafted to specify significant shape transformations, such as changes to contours or proportions, that result in low spatial alignment with the source object. This level of detail enables accurate, mask-free object recognition. All prompts were initially generated using Qwen-2.5-VL [3] and subsequently validated by hand to ensure precision and contex4.3. Comparison with Baselines 4.3.1. Qualitative Comparison. We compare Follow-Your-Shape with two categories of image editing methods: Diffusion-based and Flow-based methods. Diffusion-based baselines include PnPInversion [15], MasaCtrl [5], and Dit4Edit [10], which perform editing by modulating attention mechanisms and conditions throughout the diffusion process. Flow-based baselines include RF-Edit [51], FlowEdit [17], KV-Edit [61], and FluxKontext [19], all of which build upon the Rectified Flow framework for controllable generation. Specifically, FluxKontext is newly proposed model that leverages context-token concatenation, enabling strong in-context editing capabilities. Fig. 4 presents qualitative results showcasing the superior shape-aware editing ability and background preservation of our method compared to these baselines. Diffusion-based methods usually struggle to preserve the background under structural edits (e.g., MasaCtrl and Dit4Edit for the lion-shaped latten) and sometimes fail to perform high-magnitude shape transformations (e.g., PnPInversion for the car leaps through the air). On the other hand, flow-based methods generally produce higher-quality images with better background preservation, but still suffer from issues such as detail jitter (e.g., Flux-Kontext for the car leaps through the air, KV-Edit for the lion-shaped latten), ghosting artifacts (e.g., KV-Edit for the dragonflies), and failure to complete large-scale shape transformations in challenging cases (e.g., all baselines for the hat). In contrast, Follow-Your-Shape excels at performing largescale shape transformations while faithfully preserving nontarget regions. 4.3.2. Quantitative comparison. Following the same categorization, we conduct the quantitative comparison on ReShapeBench to evaluate the effectiveness of our method against both diffusion-based and flow-based baselines. To ensure fairness, we use identical source and target prompts and apply the same number of denoising timesteps across all methods. Specifically, as were following the RF-Solver implementation and utilizing its second-order solver, we double the number of timesteps for methods that do not employ second-order scheme to ensure the same NFE (Number of Function Evaluations). As shown in Tab. 1, we select the PNSR [14] and LPIPS [56] to evaluate the background consistency, Aesthetic Scores computed via the LAION aesthetic predictor [43] for the image quality, and the CLIP [40] for the text alignment. The results demonstrate that our model outperforms the baseline models across all metrics. The proposed region-controlled editing strategy achieves better performance for the shapeaware editing tasks, while our designed edit mask MS preserves the background well. ing range [0, 1]. Figure 6 (a) shows that injecting at relatively early stages yields the best, as the latent features are less noisy and more receptive to structural guidance. We also vary the conditioning strength on both Canny and Depth guidance. As shown in Figure 6 (b), moderate scale (e.g., (2.5, 3.5)) achieves the best balance between structure preservation and editability, while overly weak or strong signals lead to underor over-constrained outputs. Figure 5. The qualitative ablation study on kfront. As the number of stabilization steps increases, background is better preserved, but shape transformation becomes less effective. 5. Conclusion We introduce Follow-Your-Shape, framework that enables large-scale object shape transformation by using novel trajectory-based region control mechanism. Our method achieves precise, mask-free edits while preserving background integrity by dynamically localizing modifications through Trajectory Divergence Map with scheduled injection. To properly evaluate this task, we developed ReShapeBench, new benchmark tailored for complex shape-aware editing. To the best of our knowledge, Follow-Your-Shape is the first work to systematically address prompt-driven shape editing. Extensive qualitative and quantitative experiments validate its state-of-the-art performance on the proposed benchmark. Our work thus opens promising new avenues for controllable generation."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42 (4):111, 2023. 2 [2] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78777888, 2025. 2, 3, 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. 7, 2 [4] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009. 2 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 2, 6, 7 [6] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video Figure 6. Ablation on ControlNet conditioning timestep and condition strength. (a) Conditioning is applied within five subranges of the normalized denoising interval [0, 1]. Injection at relatively early stages (e.g., [0.1, 0.3]) yields the most stable results. (b) Condition strength of the depth and canny branches, denoted as (depth, canny), is varied across different settings. Excessive strength can overly constrain the edit, while insufficient strength leads to weak guidance. 4.4. Ablation Study We conduct ablation studies to assess two key components in our editing framework: Initial trajectory stabilization steps and the timing and strength of ControlNet conditioning. 4.4.1. Effectiveness of Initial Trajectory Stabilization To assess the role of initial trajectory stabilization, we vary the number of stabilization steps kfront from 0 to 4. As shown in Figure 5, small kfront leads to noticeable drift and structural deviation from the source image, while large kfront overly restricts the intended shape transformation. To ensure controlled quantitative comparison, ControlNet modules were disabled to isolate the effect of kfront. Table 2 shows that as kfront increases, background preservation improves but CLIP similarity consistently drops, indicating trade-off where trajectory stabilization reduces editing flexibility. Setting kfront to 2 provides the best performance for preserving layout structure and allowing flexible edits. 4.4.2. Effectiveness of ControlNet conditioning timestep and strength To explore the effect of ControlNet conditioning timestep, we vary the injection interval within the normalized denois8 outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. 2 [7] Siran Chen, Qinglin Xu, Yue Ma, Yu Qiao, and Yali Wang. Attentive snippet prompting for video retrieval. IEEE Transactions on Multimedia, 26:43484359, 2023. 2 [8] Siran Chen, Yuxiao Luo, Yue Ma, Yu Qiao, and Yali Wang. H-mba: Hierarchical mamba adaptation for multi-modal In Proceedvideo understanding in autonomous driving. ings of the AAAI Conference on Artificial Intelligence, pages 22122220, 2025. [9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 2 [10] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: DifIn Proceedings of fusion transformer for image editing. the AAAI Conference on Artificial Intelligence, pages 2969 2977, 2025. 2, 3, 6, 7 [11] Kunyu Feng, Yue Ma, Xinhua Zhang, Boshi Liu, Yikuang Yuluo, Yinhan Zhang, Runtao Liu, Hongyu Liu, Zhiyuan Qin, Shanhui Mo, Qifeng Chen, and Zeyu Wang. Followyour-instruction: comprehensive mllm agent for world data synthesis. arXiv preprint arXiv:2508.05580, 2025. 2 [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2 [13] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 [14] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800801, 2008. 2, [15] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. 5, 7 [16] Jeongsol Kim, Yeobin Hong, and Jong Chul Ye. Flowalign: inversion-free flow-based image Trajectory-regularized, editing. arXiv preprint arXiv:2505.23145, 2025. 2 [17] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free arXiv text-based editing using pre-trained flow models. preprint arXiv:2412.08629, 2024. 2, 6, 7 [18] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5 [19] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 6, 7 [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 1 [21] Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, and Qifeng Chen. Avatarartist: Open-domain 4d avatarization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1075810769, 2025. 2 [22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1 [23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:5775 5787, 2022. [24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 2 [25] Li Lun, Kunyu Feng, Qinglong Ni, Ling Liang, Yuan Wang, Ying Li, Dunshan Yu, and Xiaoxin Cui. Towards effective and sparse adversarial attack on spiking neural networks via breaking invisible surrogate gradients. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 35403551, 2025. 3 [26] Xiang Lv, Mingwen Shao, Yecong Wan, Yue Ma, Yuanshuo Cheng, and Lingzhuang Meng. Bm-edit: Background retention and motion consistency for zero-shot video editing. Knowledge-Based Systems, page 113784, 2025. 1 [27] Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, and Harry Yang. Model reveals what to cache: Profiling-based feature reuse for video diffusion models. arXiv preprint arXiv:2504.03140, 2025. 3 [28] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, and Yu Qiao. Visual knowledge graph for human action reasoning in videos. In Proceedings of the 30th ACM International Conference on Multimedia, pages 41324141, 2022. 2 [29] Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Magicstick: Controllable video editing via control handle transformations. arXiv preprint arXiv:2312.03047, 2023. [30] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. [31] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2 [32] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video 9 generation: survey. 2025. 2 arXiv preprint arXiv:2507.16869, [44] Jiaming Song, Chenlin Meng, [33] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. 2 [34] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 60186026, 2025. 2 [35] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [36] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 3 [37] Yatian Pang, Bin Zhu, Bin Lin, Mingzhe Zheng, Francis EH Tay, Ser-Nam Lim, Harry Yang, and Li Yuan. Dreamdance: Animating human images by enriching 3d geometry cues from 2d poses. arXiv preprint arXiv:2412.00397, 2024. 2 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [39] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of machine learning and systems, 5: 606624, 2023. 1 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2, 7 [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 1 [42] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 1 [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 2, 7 Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [45] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19211930, 2023. 2, 6 [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1 [47] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2253222541, 2023. 3 [48] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 3 [49] Zhen Wan, Yue Ma, Chenyang Qi, Zhiheng Liu, and Tao Gui. Unipaint: Unified space-time video inpainting via mixture-of-experts. arXiv preprint arXiv:2412.06340, 2024. 2 [50] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. [51] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 2, 3, 6, 7, 1 [52] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi PontTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Imagen editor Laszlo, David Fleet, Radu Soricut, et al. and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18359 18369, 2023. 5 [53] Zhen Xiong, Yuqi Li, Chuanguang Yang, Tiao Tan, Zhihong Zhu, Siyuan Li, and Yue Ma. Enhancing image generation fidelity via progressive prompts. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2 [54] Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial 10 and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270, 2025. [55] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 5 [56] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2, 7 [57] Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, and Zeyu Wang. Magiccolor: Multi-instance sketch colorization. arXiv preprint arXiv:2503.16948, 2025. 2 [58] Zhongping Zhang, Jian Zheng, Zhiyuan Fang, and Bryan Plummer. Text-to-image editing by image information removal. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 52325241, 2024. 2 [59] Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, et al. Videogen-of-thought: collaborative framework for multi-shot video generation. arXiv eprints, pages arXiv2412, 2024. 3 [60] Chenyang Zhu, Kai Li, Yue Ma, Chunming He, and Xiu Li. Multibooth: Towards generating all your concepts in an imIn Proceedings of the AAAI Conference on age from text. Artificial Intelligence, pages 1092310931, 2025. [61] Tianrui Zhu, Shiyi Zhang, Jiawei Shao, and Yansong Tang. Kv-edit: Training-free image editing for precise background preservation. arXiv preprint arXiv:2502.17363, 2025. 2, 3, 6, 7 11 Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Preliminaries 6.1. Rectified Flow (RF) Let p0 and p1 denote the source and target distributions, respectively. Flow Matching [20] models the transport between them by learning time-dependent velocity field v(t, x) that defines continuous transformation ψt(x) via the ordinary differential equation: dψt(x) dt = v(t, ψt(x)), ψ0(x) p0, ψ1(x) p1. (8) Rectified Flow (RF) [22] simplifies this by assuming linear trajectory between X0 p0 and X1 p1: Xt = (1 t)X0 + tX1, [0, 1], (9) with the associated velocity field becomes: v(Xt, t) = X1 X0. (10) The model is trained by minimizing the conditional flow matching loss: LCFM = EX0,X1,t (cid:2)v(Xt, t) (X1 X0)2(cid:3) . (11) During inference, the learned velocity field is used to generate new samples by solving the reverse-time ODE: dXt dt = v(Xt, t), (12) starting from sample X1 (0, I). Since the ODE does not admit closed-form solution in general, sampling is performed via numerical integration over discretized set of timesteps {ti}N i=0. common approach is to use first-order solvers such as Euler or Heuns method to approximate the trajectory: where tv(Xti, ti) is the time derivative of the learned velocity field. This correction term reduces local integration error and leads to more accurate inversion and sampling trajectories, which is particularly important for downstream editing tasks that require high structural fidelity. 6.2. KV Injection Key-Value (KV) injection is adapted from the KV caching mechanism originally used in Transformers [46] to accelerate autoregressive inference [39]. In large language models, cached key and value tensors allow reuse of past attention computations, enabling efficient decoding without recomputing earlier tokens. When adapted to vision models, KV injection serves as mechanism to enforce structural consistency by injecting key and value features from the inverted source image into the self-attention layers of the generative model. This guides attention toward the original spatial layout, helping to constrain edits to the desired region. In architectures like U-Net [41] and DiT [38], KV injection is typically applied during the denoising phase, where reused attention features act as anchors that preserve the overall scene structure. To reduce memory cost and avoid limiting foreground flexibility, recent work such as StableFlow [2] explores the vital layers within DiT crucial for image formation. Therefore, by only reusing KV pairs in subset of layers, it can balance structural fidelity and editability while effectively reduce the memory usage. In our work, we demonstrate that KV injection provides modular and interpretable mechanism for controllable image editing and particularly effective in shapeaware tasks where edits must stay localized without affecting the broader scene. 7. Additional Details on ReShapeBench ConXti1 = Xti v(Xti , ti), (13) struction where = ti ti1 is the integration step size. However, these first-order solvers can suffer from numerical instability and truncation error, especially in highdimensional generation tasks. Several recent works [8, 23, 26, 42, 51] have explored higher-order integration strategies or adaptive solvers to improve generation fidelity. Specifically, RF-Solver [51] introduces second-order update derived from Taylor expansion of the velocity field: Xti1 = Xti v(Xti, ti) + 1 2 h2 tv(Xti, ti), (14) 7.1. Shape Transformation While recent generative models exhibit strong generalpurpose editing capabilities, the concept of shape transformation remains ill-defined in the literature. Existing approaches typically perform localized editssuch as detail refinement, scaling, or texture changesoften guided by external signals like masks or ControlNet images. Although these methods succeed in achieving visible modifications, they are not explicitly framed as shape transformations and lack clear taxonomy for distinguishing what qualifies as 1 true change in shape. When constructing ReShapeBench, we need clear definition and categorization of shape transformation to guide data curation and enable meaningful evaluation. By analyzing wide range of prompt editing pairs, we identify representative class of transformations in which the objects contour, geometry, and even semantic identity undergo significant changes, yet its role in the scene remains intact. As shown in Figure 7, typical examples include swan transformed into small wooden boat, flamingo, etc., where the objects outline or category changes entirely, but the spatial location, visual salience, and narrative function of the object are preserved. Based on these observations, we define shape transformation of two objects as structural change that satisfies four key criteria below: Cross-contour: The objects overall silhouette or boundary changes significantly, beyond local warping or resizing. Cross-semantic: The transformation changes the object into one from different semantic class, while preserving overall scene consistency. Structural transition: The transformation reflects global reconfiguration of the objects visual form, requiring changes to multiple parts more than attributes (e.g., colors, textures). Subject continuity: Despite changes in shape and semantics, the object should maintain its spatial role and salience within the scene, ensuring visual coherence and contextual consistency. The most challenging part in shape transformation is it requires the model to reinterpret object geometry while maintaining consistency in background, perspective, and composition. Unlike existing editing benchmarks that encompass on diverse editing tasks, our benchmark emphasizes large-scale shape transformation under prompt guidance, without relying on masks or external conditioning. 7.2. Image and Prompt Examples Following the criteria of shape transformation outlined in the previous section, each prompt pair in ReShapeBench is carefully constructed to reflect meaningful transformations. For each selected source image, we first brainstormed plausible shape transformation targets that satisfy the criteria, then selected the most representative transformation for each image to ensure diversity and task difficulty across the dataset. Prior benchmarks such as PIEBench often rely on singlesentence prompts, which are concise but frequently underspecify spatial or structural details. To generate high-quality prompts for our benchmark, we utilized the Qwen-VLChat [3] model. After specifying the intended transformation for given image, we provided both the source prompt and the editing instruction to Qwen-VL-Chat. Leveraging its multi-turn dialogue memory, the model was able to maintain consistent formatting and descriptive style across all prompt pairs, producing aligned edit prompt. Examples are illustrated in Figure 9. Complete image data are included in the data appendix. 7.3. Evaluation Metrics imWe use four metrics grouped under three aspects: age quality, background preservation, and text-image alignment. Aesthetic Score (AS) is used to assess the perceptual quality of the generated image, reflecting how well the visual content adheres to natural image statistics. This metric is computed using the LAION aesthetic predictor [43], which applies linear estimator on top of CLIP embeddings to predict aesthetic quality1. This is especially relevant in shape transformation tasks where the output may differ substantially from the source. In particular, AS serves as useful proxy for detecting unnatural boundaries or blending artifacts that may arise during large-scale shape transitions, helping to evaluate whether the new shape is integrated smoothly into the scene. For background preservation, we adopt two widely used metrics that capture different aspects of visual similarity. Peak Signal-to-Noise Ratio (PSNR) measures low-level pixel fidelity, while Learned Perceptual Image Patch Similarity (LPIPS) evaluates perceptual similarity based on deep feature representations. Since we do not assume access to ground-truth masks, and the edited shape can vary across models, we follow the principle of subject continuity to localize the edited region. We apply fixed-size box centered on the subject to occlude the foreground and compute similarity metrics over the remaining background area. This heuristic enables consistent and fair evaluation of how well the unedited context is preserved across different editing results. Finally, to assess text-image alignment, we compute CLIP similarity between the generated image and the target prompt. This metric offers widely adopted embeddingbased measure of semantic consistency. Together, these metrics provide comprehensive evaluation of structural realism, background integrity, and semantic alignment in shape-aware image editing. 8. Limitations and Future Work While our method demonstrates strong performance in shape-aware image editing, it also comes with certain limitations that suggest directions for future work. 1https : / / github . com / LAION - AI / aesthetic - predictor 2 Figure 7. Visualization of Shape Transformation. The objects contour, semantic, structure changed while ensuring its subject continuity. In particular, the spatial editing regions indicated by TDM often fluctuate across frames, leading to inconsistent or incomplete transformations in the resulting video. Since well-defined and temporally consistent TDM is crucial for successful editing, future work may consider strategies such as temporally-aware TDM construction, or explicit disentanglement of spatial and temporal components in the denoising trajectory. Figure 8. TDM of Wan2.1 video editing at single timestep across different frames. 9. More Editing Results We present additional editing results in Figure 10, all conducted on images in PIEBench and ReShapeBench. 8.1. Robustness to Prompt and Hyperparameters First, the framework exhibits high sensitivity to prompt description. Since the models editing behavior is driven entirely by prompt-guided inversion and denoising trajectories, slight variations in wording or structure can lead to noticeably different editing outcomes. This makes it difficult to guarantee consistent performance across prompts, especially in open-ended or ambiguous settings. Second, as training-free method, our approach relies on carefully tuned hyperparameters (e.g., injection timing, control strength). However, no single configuration generalizes optimally across all tasks and image categories. The absence of learnable mechanism makes it challenging to adaptively adjust parameters for different editing scenarios, which may limit flexibility in real-world applications. However, in practical applications, the robustness of the framework is required to be concerned [25]. Future work could explore developing prompt-robust techniques that maintain semantic fidelity across broader range of user inputs. 8.2. Extending to Video Editing We also explore extending our shape-aware editing framework to the video domain using Wan 2.1 [48], an opensource video generation model utilized Rectified Flow. While our method can in principle be applied to all frames, we find that the temporal dimension introduces major challenge, where the TDM becomes much less stable and effective when extended across time, as shown in Figure 8. 3 Figure 9. Comparisons between original prompts and our refined prompts 4 Figure 10. Additional Shape-Aware Editing Results"
        }
    ],
    "affiliations": [
        "HKUST",
        "Shanghai Jiao Tong University",
        "University of Illinois at Urbana-Champaign"
    ]
}