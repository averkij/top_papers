{
    "paper_title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "authors": [
        "Haoyuan Wu",
        "Hai Wang",
        "Jiajia Wu",
        "Jinxiang Ou",
        "Keyao Wang",
        "Weile Chen",
        "Zihao Zheng",
        "Bei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 5 9 2 2 . 2 1 5 2 : r 2025-12-30 Diversity or Precision? Deep Dive into Next Token Prediction Haoyuan Wu1,2, Hai Wang1,, Jiajia Wu1, Jinxiang Ou1, Keyao Wang1, Weile Chen1, Zihao Zheng1, Bei Yu2 1LLM Department, Tencent 2The Chinese University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained models token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as specific instance of policy gradient optimization applied within single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as stochastic decision process, we introduce reward-shaping strategy that explicitly balances diversity and precision. Our method employs positive reward scaling factor to control probability concentration on ground-truth tokens and rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing precision-oriented prior yields superior exploration space for RL."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements have demonstrated that reinforcement learning (RL) (Bai et al., 2022; Guo et al., 2025) can significantly enhance the reasoning capabilities of large language models (LLMs) (Google DeepMind, 2025; Guo et al., 2025; Anthropic, 2025; Kimi et al., 2025). By utilizing verifiable rewards, such as passing unit tests or deriving correct mathematical solutions, LLMs evolve from merely mimicking human data to actively searching for optimal reasoning paths (Guo et al., 2025). On-policy training paradigms have proven effective in unlocking the potential of pre-trained LLMs, prompting researchers to investigate how token output distributions influence RL. Recent studies (Wang et al., 2025; Zhu et al., 2025b; Cui et al., 2025; Gandhi et al., 2025) indicate that uncertainty in chain-of-thought reasoning is concentrated within small subset of high-entropy forking tokens that govern pivotal decisions, while the majority of tokens exhibit low entropy. This observation underscores the critical impact of the pre-trained models output distribution on subsequent RL outcomes. Concurrently, researchers have explored next-token and next-segment reasoning objectives to derive self-supervised signals from massive unlabeled pre-training corpora (Zelikman et al., 2024; Dong et al., 2025; Li et al., 2025; Xing et al., 2025). Applying RL to the pre-training corpus suggests theoretical bridge connecting pre-training and RL. Specifically, next-token prediction can be reformulated as reasoning task optimized via RL algorithms, where the model receives verifiable rewards for accurately predicting the subsequent token according to given context. Notably, if the intermediate reasoning process is omitted, resulting in the direct generation of the answer, this procedure becomes analogous to standard pre-training. From the perspective of policy optimization, next-token prediction serves foundational role by defining the initial policy distribution for subsequent RL. This distribution establishes the models behavioral trajectory and implicitly constrains its exploration space, thereby determining which reasoning paths the model prioritizes during RL. Motivated by this connection, we revisit the cross-entropy loss for next token prediction. Although traditionally viewed as supervised metric, cross-entropy can be interpreted as specific instance of policy gradient optimization within single-step episode (Wu et al., 2025; Ming et al., 2025). This interpretation suggests that next-token prediction inherently permits an on-policy perspective, even though standard teacher forcing utilizes off-policy samples drawn directly from the training corpus distribution. From an entropy perspective, cross-entropy implicitly assigns maximal reward to the single ground-truth token while uniformly suppressing all negative tokens. Building on this insight, we aim to establish unified pre-training objective that subsumes cross-entropy as special case, enabling systematic study of how reward configurations during pre-training influence subsequent RL dynamics. In this paper, we propose generalized objective that integrates on-policy training principles into supervised learning. By formulating next-token prediction as stochastic decision process, we expose the intrinsic reward mechanism of cross-entropy and introduce reward-shaping strategy. This approach explicitly regulates the trade-off between diversity and precision during pre-training, rather than deferring this balance to subsequent RL stages. Specifically, Project Lead. we introduce positive reward scaling factor to control the concentration of probability mass on ground-truth tokens, and we differentiate between high-ranking and low-ranking negative tokens to modulate suppression asymmetrically. This strategy allows us to reshape the token output distribution and systematically analyze the relationship between pre-training objectives and RL exploration. Contrary to the conventional intuition that higher distribution entropy facilitates effective exploration, our findings reveal that imposing precision-oriented prior yields superior exploration space for RL, ultimately enhancing end-to-end reasoning performance. Our main contributions are summarized as follows: We propose generalized pre-training objective for next-token prediction that incorporates reward-shaping strategy, utilizing positive reward scaling factor and rank-aware negative suppression. We investigate how reshaping the token output distribution during pre-training modulates the exploration space for subsequent RL, thereby impacting end-to-end reasoning performance. We demonstrate that precision-oriented pre-training prior provides more effective initialization for RL than high-entropy distributions, leading to improved reasoning capabilities."
        },
        {
            "title": "2.1 Next Token Prediction",
            "content": "Autoregressive LLMs are typically trained using next-token prediction objective. This process can be formulated as sequential decision-making problem where the LLM functions as stochastic policy πθ. Let = {x1, x2, , xn} denote sequence of tokens. At step t, the state st is defined by the prefix X<t = {x1, x2, , xt1}. The action at corresponds to the next token, sampled from the vocabulary according to the policy πθ( st). The training objective optimizes the parameters θ to maximize the expected cumulative reward: J(θ) = Eτπθ r(st, at) (cid:105) , (cid:104) t= (1) where τ = (s1, a1, s2, a2, ) represents trajectory sampled from πθ, and r(st, at) is the scalar reward received for taking action at in state st. The policy gradient can be derived as: θ J(θ) = Eτπθ (cid:104) t=1 R(τ)θ log πθ(at st) (cid:105) , (2) where R(τ) = replaced by the return-to-go Gt = t=1 r(st , at ). To reduce variance without introducing bias, the total return R(τ) is typically t=t r(st , at ), often incorporating baseline b(st) for variance reduction: θ J(θ) = Eτπθ (cid:104) t=1 (Gt b(st))θ log πθ(at st) (cid:105) . (3) Building upon Equation (3), we treat the generation of single token as complete episode (Ming et al., 2025). The objective for fixed state st simplifies to: yielding the gradient: θ Jt(θ st) = atπθ (st) (cid:2)r(st, at)θ log πθ(at st)(cid:3). Jt(θ st) = atπθ (st)[r(st, at)], (4) (5) Crucially, for Equation (5) to remain consistent with the cumulative reward structure of Equation (3), the reward r(st, at) must depend solely on the immediate state-action pair."
        },
        {
            "title": "2.2 Revisiting Cross-Entropy",
            "content": "LLM pre-training is generally cast as supervised learning process designed to maximize the log-likelihood of the ground-truth token xt given the context st = X<t: JCE(θ) = log πθ(xt st). (6) The gradient of this objective explicitly maximizes the probability of the ground-truth token: θ JCE(θ) = θ log πθ(xt st). We can express this gradient as an expectation over the full policy distribution πθ( st), encompassing both positive (at = xt) and negative (at = xt) tokens. By invoking the log-derivative identity θ log πθ(x) = θ πθ (x) πθ (x) (7) 2 and introducing the indicator function 1(at = xt), we expand the gradient into summation over the vocabulary V: θ JCE(θ) = = 1 πθ(xt st) 1 πθ(xt st) θπθ(xt st) 1(at = xt)θπθ(at st). atV (8) We recover the probability density using the substitution θπθ(at st) = πθ(at st)θ log πθ(at st), and then form an expectation: θ JCE(θ) = atV πθ(at st) (cid:20) 1(at = xt) πθ(at st) atπθ (st) [rCE(st, at)θ log πθ(at st)] . θ log πθ(at st) (cid:21) (9) = In supervised training, the ground-truth token xt is deterministically defined by the dataset. Consequently, the indicator 1(at = xt) evaluates the action at against static property of st, ensuring that the derived intrinsic reward depends exclusively on information available at step t. Comparing Equation (9) with Equation (5) reveals the intrinsic reward function of cross-entropy: rCE(st, at) = sg( 1(at = xt) πθ(at st) ), (10) where sg() denotes the stop-gradient operator. Equation (10) demonstrates that when the sampled action matches the ground truth (at = xt), the reward is scaled by the inverse probability πθ (xtst) . On the contrary, for all negative tokens, the intrinsic reward is exactly 0. Unlike RL scenarios where negative actions are often explicitly penalized, cross-entropy achieves suppression of negative tokens implicitly through the Softmax normalization constraint πθ(at st) = 1. By increasing the probability of the positive tokens via positive rewards, the probabilities of atV competing tokens are forced to decrease."
        },
        {
            "title": "2.3 Diversity or Precision",
            "content": "As derived in Equation (10), the intrinsic reward of the cross-entropy objective implicitly balances diversity and precision. To explicitly regulate the trade-off between these two objectives, we propose generalized reward function designed to independently control the influence of positive and negative tokens. First, we introduce modulating factor to scale the reward associated with the ground-truth token. Let at denote the generated token and xt the ground truth, we define the modified positive reward as: rpos(st, at) = sg(( 1 πθ(at st) where (1 πθ(at st))β serves as positive reward scaling factor. Equation (11) facilitates the control of global entropy. Specifically, when β < 0, the reward is amplified relative to the baseline (β = 0). This produces large gradient updates that aggressively concentrate probability mass onto the ground truth, collapsing the distribution and minimizing global entropy. Conversely, β > 0 attenuates the reward signal. In this regime, the model is less penalized for assigning lower probability to the ground truth, allowing the policy to maintain flatter distribution with higher entropy. )(1πθ (atst))β ), (11) Second, while standard cross-entropy assigns zero reward to all negative tokens, we propose shaping the negative distribution to control local entropy. Let Kt = TopK(πθ( st), k) denote the set of the top-k predicted tokens, we define the negative reward as: rneg(st, at) = λ 1(at Kt at = xt) + ˆλ 1(at / Kt at = xt). (12) As shown in Equation (12), we assign reward λ to high-ranking negative tokens to prevent the model from becoming overly confident in the ground truth alone, thereby reserving probability mass for plausible alternatives. Meanwhile, to suppress low-probability tail tokens, we apply reward ˆλ to tokens falling outside Kt, forcing the distribution to concentrate on the head. Finally, the generalized reward function for the single-step objective is defined as: r(st, at) = rpos(st, at) 1(at = xt) + rneg(st, at) 1(at = xt). (13) Notably, the setting β = 0, λ = 0, ˆλ = 0 recovers standard cross-entropy."
        },
        {
            "title": "3.1 Training Settings",
            "content": "The training pipeline proceeds in three stages: pre-training, mid-training, and RLVR. Adhering to the Qwen3 (Yang et al., 2025), we develop LLMs using both dense and MoE architectures. Specifically, we develop series of LLMs, which include 1B and 4B dense models, as well as 5B-A0.3B and 10B-A0.5B MoE models. Moreover, we conduct the complete training pipeline on the 4B and 10B-A0.5B models, while the 1B and 5B-A0.3B models undergo the pre-training stage only. More training details are provided in Section and Section B. Training Data. For pre-training, we curate corpus of 500B tokens primarily focused on general knowledge. This is followed by mid-training stage comprising 100B tokens, which incorporates approximately 5% synthetic data and significantly increases the proportion of reasoning-oriented content. Crucially, we deliberately exclude the synthetic long-reasoning data from all training stages to accurately observe the activation trends of the models long-CoT reasoning capabilities. The RL stage prioritizes mathematical reasoning tasks, as the emergence of long-reasoning capabilities is typically associated with these domains. Hyperparameters. Hyperparameters are maintained across the pre-training and mid-training stages. Our goal is to investigate how different reward shaping strategies influence end-to-end performance. Consequently, we perform specific reward configurations for positive tokens (β = 0.25 and β = 0.5) and negative tokens ( ˆλ = 0.1, λ = 0, = 100 and ˆλ = 0, λ = 0.1, = 100). Employing these distinct hyperparameter configurations allows us to isolate the specific effects of positive and negative reward signals."
        },
        {
            "title": "3.2 Evaluation Settings",
            "content": "Evaluation of Base Models. Our comprehensive evaluation of base models assesses five core capabilities: general knowledge, logic reasoning, commonsense reasoning, mathematics, and coding. The evaluation is conducted using 19 distinct benchmarks: General Knowledge: MMLU (Hendrycks et al., 2020)(4-shot, CoT), MMLU-Pro (Wang et al., 2024)(5shot, CoT), TriviaQA (Joshi et al., 2017)(5-shot), and NaturalQuestions (Kwiatkowski et al., 2019)(5-shot). Commonsense Reasoning: Hellaswag (Zellers et al., 2019)(0-shot), SIQA (Sap et al., 2019)(0-shot), PIQA (Bisk et al., 2020)(0-shot), WinoGrande (Sakaguchi et al., 2021)(0-shot), OpenBookQA (Mihaylov et al., 2018)(5-shot), and CommonsenseQA (Talmor et al., 2018)(5-shot) Logic Reasoning: ARC-Easy (Clark et al., 2018)(0-shot), ARC-Challenge (Clark et al., 2018)(0-shot), and BBH (Suzgun et al., 2022)(3-shot, CoT) Mathematics: GSM8K (Cobbe et al., 2021)(4-shot, CoT), MATH-500 (Lightman et al., 2023)(4-shot, CoT), Minerva (Lewkowycz et al., 2022)(4-shot, CoT), and OlympiadBench (He et al., 2024)(0-shot). Coding: HumanEval+ (Liu et al., 2023)(0-shot) and MBPP+ (Liu et al., 2023)(3-shot). Specifically, general knowledge and commonsense reasoning evaluate the models knowledge-base capabilities, whereas logical reasoning, mathematics, and coding probe its reasoning-base capabilities. Moreover, we employ the Pass@k metric to evaluate the models upper-bound capability for tasks requiring mathematical reasoning and code generation. Pass@k measures the probability that at least one correct solution is present within independent attempts. We utilize the unbiased estimator of Pass@k (Chen, 2021), which is defined as: Pass@k = 1 (mc ) (m ) , (14) where represents the total number of sampled responses generated per prompt, and denotes the count of correct responses among those samples. We sample = 128 responses with temperature 0.7 and top-p 0.95 and report Pass@64 metric. Notably, we configure the maximum output length to 4K for pre-trained models and 16K for mid-trained models. Evaluation of RL Models. For RL models evaluation, we employ various mathematics benchmarks, including AMC23 (MAA, b), AIME (MAA, a), MATH-500 (Lightman et al., 2023), Minerva (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). We sample 128 responses per problem and report Avg@128, Cons@128, and Pass@64 metrics. Specifically, Avg@128 represents the average accuracy across all 128 samples, while Cons@128 refers to the majority voting accuracy. Similarly, we configure the maximum output length to 16K for RL models."
        },
        {
            "title": "3.3 Pre-Training",
            "content": "Our analysis of the proposed generalized training objective reveals that it effectively regulates the trade-off between diversity and precision by strategically varying reward configurations. As illustrated in Figure 1 and Figure 2, perplexity (PPL) consistently converges to comparable low values across both dense (1B, 4B) and MoE (5B-A0.3B, 10B-A0.5B) architectures. This demonstrates that, within specific range, modifying the reward function modulates 4 Figure 1: Changes of PPL and entropy during pre-training across 1B and 4B dense models, developed based on different configurations. Figure 2: Changes of PPL and entropy during pre-training across 5B-A0.3B and 10B-A0.5B MoE models, developed based on different configurations. training dynamics without compromising final predictive accuracy. The parameter β serves as potent global entropy regulator. Specifically, setting β < 0 significantly reduces entropy, resulting in more peaked and confident token distribution by amplifying rewards for ground turth tokens. Conversely, β > 0 maintains higher entropy and flatter distribution, thereby promoting diversity in the generated output. Meanwhile, the parameters ˆλ and λ facilitate local entropy fine-tuning. These parameters shape the token distribution by either rewarding ( ˆλ = 0, λ = 0.1, = 100) or penalizing ( ˆλ = 0.1, λ = 0, = 100) negative tokens, enabling granular control over the training process. Furthermore, we analyze the evolution of model performance during pre-training to investigate the dynamics and specific impact of the proposed reward function. As depicted in Figure 3, larger models consistently achieve substantially higher final performance than smaller models after processing an equivalent number of training tokens. This confirms that explicitly regulating the diversity-precision trade-off is an orthogonal mechanism that does not interfere with the fundamental scaling properties of language models. Crucially, configurations that prioritize lowering global entropy (β < 0) or maintaining high local entropy ( ˆλ = 0.1, λ = 0, = 100) demonstrate superior performance and scaling behavior. Although these settings may not yield optimal initial performance in smaller models, they exhibit enhanced growth potential as model size increases. This suggests that with greater model capacity, strategies that promote precision, either globally via generously rewarding positive tokens or locally by aggressively penalizing tail negative tokens, lead to better performance growth compared to the baseline."
        },
        {
            "title": "3.4 Mid-Training",
            "content": "Subsequently, we evaluate the evolution of model performance during the mid-training stage, spanning from 0B to 100B tokens. As depicted in Figure 4, the choice of β significantly influences training dynamics. We observe consistent trend where negative value, specifically β = 0.25, yields the best results. This configuration consistently outperforms the baseline (β = 0) across both dense and MoE models in knowledge and reasoning 5 Figure 3: Changes of performance during pre-training across models with various model parameters, developed based on dense and MoE architectures under different configurations. Figure 4: Changes of performance during mid-training across 4B dense and 10B-A0.5B MoE models, developed based on different configurations. tasks. Conversely, positive setting (β = 0.50) does not demonstrate consistent superior performance comparing to the baseline. Similar to the observations with β, slight negative adjustment appears beneficial. The configuration ˆλ = 0.1, λ = 0, = 100 generally matches or slightly surpasses the performance of the standard CE baseline. However, when shifting to ˆλ = 0.1, λ = 0, = 100, performance exhibited uncertainty in knowledge-intensive scenarios. In reasoning tasks, the performance remained comparable to the standard CE baseline."
        },
        {
            "title": "3.5 Reinforcement Learning",
            "content": "Finally, we investigate the performance dynamics during the RL training stage across various actor models, as illustrated in Figure 5 and Figure 6. Pre-trained models derived from different reward configurations exhibit distinct output distributions, leading to significant variations in subsequent RL and end-to-end reasoning performance. Regarding the global entropy regulator β, we observe consistent and robust trend across both the 4B dense and 10B-A0.5B MoE models. Specifically, the global low entropy setting (β = 0.25) yields superior performance trajectories. This configuration consistently outperforms the global high entropy setting across all evaluated metrics, including Avg@128, Cons@128, and Pass@64. Furthermore, the configuration ˆλ = 0.1, λ = 0, = 100 demonstrates significant advantage, consistently achieving the highest performance and notably surpassing the baseline on the 10B-A0.5B MoE model. For the 4B dense model, maintaining local high entropy exhibits superior scaling trend compared to the baseline. In conclusion, strategies that promote precision, either globally via generously rewarding positive tokens or locally by aggressively penalizing tail negative tokens, enables the model to converge to higher-quality solutions, potentially providing better exploration space for RL. To better understand the performance divergence observed during RL, we analyze the evolution of policy entropy and response length throughout the training process, as illustrated in Figure 7. Contrary to the expectation that higher entropy maintains diversity, setting higher β leads to rapid entropy collapse during the early stages of training. Coinciding with this collapse, the response length decreases drastically, indicating suppression of the reasoning capability. In contrast, local high-entropy configurations exhibit greater stability. These settings effectively prevent entropy collapse, maintaining robust policy distribution from the onset. They demonstrate smooth and continuous 6 Figure 5: Changes of performance during RL training across various actor models, developed based on 4B dense architecture under different configurations. Figure 6: Changes of performance during RL training across various actor models, developed based on 10B-A0.5B MoE architecture under different configurations. Figure 7: Changes of entropy and response length during RL training across various actor models, developed based on 4B dense and 10B-A0.5B MoE architectures under different configurations. activation of long reasoning capabilities, allowing for steady increase in generation length and reasoning depth without the recovery lag observed in global high entropy settings."
        },
        {
            "title": "3.6 Pass@k Analysis of Base Models",
            "content": "Moreover, we analyze the Pass@k curves as increases to estimate the upper bound of the capability of base models. This metric relies on delicate equilibrium between solution precision and diversity. As shown in Figure 8, maximizing global diversity (high entropy) does not inherently yield higher Pass@k curves. Instead, superior 7 Figure 8: Pass@k curve of base models on mathematics reasoning and code generation tasks, developed based on 4B dense and 10B-A0.5B MoE models under different configurations. Pass@k scores in mathematics and coding tasks are achieved by prioritizing precision. Crucially, we observe that this low-entropy setting does not lead to collapse in output diversity. Rather, it maintains sufficient variation to cover the solution space. Furthermore, the data indicate that promoting local diversity also yields better results. This suggests that while models benefit from high precision, they simultaneously benefit from targeted local exploration."
        },
        {
            "title": "4.1 Weighted Cross-Entropy Loss",
            "content": "The standard cross-entropy objective can be generalized within policy-gradient framework, where it is equivalent to optimizing sparse reward defined as rCE(st, at) = 1(at = xt)πθ(at st)1. Existing modifications to this objective include smooth loss (label smoothing), which encourages diversity by allocating uniform probability mass to all positive tokens, and focal loss (Lin et al., 2018), which down-weights easy examples via wt = (1 πθ(xt st))γ. Our proposed generalized training objective can also formulate these established variations. In this paper, we specifically explore two different reward configurations within this framework. Firstly, we introduce modified positive reward, which is equivalent to applying state-dependent weight wt = πθ(xt st)1(1πθ (xtst))β to the standard cross-entropy. In addition, we incorporate TopK-based negative shaping, which explicitly controls local entropy by assigning non-zero rewards to selected actions with at = xt."
        },
        {
            "title": "4.2 Next Token Reasoning",
            "content": "Treating each token emission as distinct episode ensures that the reward depends only on the immediate stateaction pair (st, at), thereby preserving unbiased credit assignment. The framework is naturally compatible with architectures that perform iterative internal computation prior to token emission, including latent-reasoning models (Zelikman et al., 2024) and loop transformers (Dehghani et al., 2019; Zhu et al., 2025a). Although each episode terminates at token emission, the state st may encode the outcome of internal refinement cycles. Our reward design can serve as an uncertainty-aware learning signal that can be combined with adaptive computation policies to allocate additional internal processing steps in uncertain contexts. By explicitly shaping positive and negative token-level rewards within single-step policy-gradient framework, we provide general and controllable mechanism that natively supports reasoning-oriented architectures through principled reward design."
        },
        {
            "title": "5 Conclusion",
            "content": "This study establishes theoretical bridge between next-token prediction and RL by interpreting cross-entropy loss as specific instance of policy gradient optimization. To exploit this connection, we introduce generalized pre-training objective that utilizes reward-shaping strategy with positive scaling and rank-aware negative rewards. Our experiments across multiple architectures and scales reveal that regulating the diversity-precision trade-off during pre-training modulates token entropy. Our findings indicate that precision-focused strategies (e.g., global entropy reduction or tail-token suppression) yield superior scaling for the subsequent RL stage. These insights provide novel perspective on optimizing pre-training for long CoT reasoning, suggesting new directions for sophisticated reward shaping in LLM development."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. arXiv preprint arXiv:2204.05862, 2022. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about Physical Commonsense in Natural Language. In AAAI Conference on Artificial Intelligence (AAAI), volume 34, pp. 74327439, 2020. Mark Chen. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models. arXiv preprint arXiv:2505.22617, 2025. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal Transformers. arXiv preprint arXiv:1807.03819, 2019. Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement Pre-Training. arXiv preprint arXiv:2506.08007, 2025. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs. arXiv preprint arXiv:2503.01307, 2025. Google DeepMind. Gemini3 Pro. https://deepmind.google/technologies/gemini/pro/, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. DeepSeek-R1: Incentivizes Reasoning in LLMs through Reinforcement Learning. Nature, 645(8081):633638, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300, 2020. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv preprint arXiv:1705.03551, 2017. Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi K2: Open Agentic Intelligence. arXiv preprint arXiv:2507.20534, 2025. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving Quantitative Reasoning Problems with Language Models. In Annual Conference on Neural Information Processing Systems (NIPS), volume 35, pp. 38433857, 2022. Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, et al. Reinforcement Learning on Pre-Training Data. arXiv preprint arXiv:2509.19249, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. In International Conference on Learning Representations (ICLR), 2023. 9 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal Loss for Dense Object Detection. arXiv preprint arXiv:1708.02002, 2018. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 Technical Report. arXiv preprint arXiv:2412.19437, 2024. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Annual Conference on Neural Information Processing Systems (NIPS), volume 36, pp. 2155821572, 2023. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101, 2017. MAA. American Invitational Mathematics Examination (AIME), Mathematics Competition Series, a. URL https://maa.org/math-competitions/aime. MAA. American Mathematics Competitions (AMC 10/12), Mathematics Competition Series, b. URL https: //maa.org/math-competitions/amc. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. arXiv preprint arXiv:1809.02789, 2018. Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, and Bei Yu. One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient. arXiv preprint arXiv:2509.26313, 2025. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An Adversarial Winograd Schema Challenge at Scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. SocialiQA: Commonsense Reasoning about Social Interactions. arXiv preprint arXiv:1904.09728, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced Transformer with Rotary Position Embedding. Neurocomputing, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging Big-Bench Tasks and Whether Chain-ofThought Can Solve Them. arXiv preprint arXiv:2210.09261, 2022. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: Question Answering Challenge Targeting Commonsense Knowledge. arXiv preprint arXiv:1811.00937, 2018. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning. arXiv preprint arXiv:2506.01939, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. In Annual Conference on Neural Information Processing Systems (NIPS), 2024. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, MingHsuan Yang, and Xu Yang. On the Generalization of SFT: Reinforcement Learning Perspective with Reward Rectification. arXiv preprint arXiv:2508.05629, 2025. Xingrun Xing, Zhiyuan Fan, Jie Lou, Guoqi Li, Jiajun Zhang, and Debing Zhang. PretrainZero: Reinforcement Active Pretraining. arXiv preprint arXiv:2512.03442, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. arXiv preprint arXiv:2503.14476, 2025. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arXiv preprint arXiv:2403.09629, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can Machine Really Finish Your Sentence? arXiv preprint arXiv:1905.07830, 2019. Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, Yoshua Bengio, and Jason Eshraghian. Scaling Latent Reasoning via Looped Language Models. arXiv preprint arXiv:2510.25741, 2025a. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning. arXiv preprint arXiv:2506.01347, 2025b. Experiment Details for Pre-Training and Mid-Training A.1 Implementation Details For both the pre-training and mid-training phases, we employ the AdamW (Loshchilov & Hutter, 2017) optimizer, implementing weight decay of 0.1 and applying gradient clipping at 1.0. Throughout these stages, we utilize warmup-stable-decay learning rate schedule with global batch size of 16M. During the stable pre-training stage, which encompasses 500B tokens, the learning rate warms up over 2000 steps before stabilizing at 3 104. Subsequently, we perform mid-training on an additional 100B tokens, gradually decaying the learning rate from 3 104 to 3 105. We set the maximum sequence length to 4096 during pre-training and extend it to 16384 for the mid-training stage. To support long-context modeling during mid-training, we increase the base frequency of RoPE (Su et al., 2024) from 1e4 to 1e6. A.2 Model Architecture Building upon the Qwen3 (Yang et al., 2025) architectures, we perform our experiments utilizing both dense and MoE architectures. Notably, we adopt an auxiliary loss free approach (Liu et al., 2024) for the training of the MoE models. Detailed architecture settings are provided in Table 1, where denotes the total number of experts and Ea denotes the number of active experts. Table 1: Detailed architectures settings of dense and MoE models."
        },
        {
            "title": "Model",
            "content": "nlayer dmodel 1B Dense 4B Dense 5B-A0.3B MoE 10B-A0.5B MoE 28 36 12 16 1536 2560 1024 1536 dffn 4608 9728 - - dexpert nhead nkvhead - - 320 320 16 32 32 4 8 4 4 - - 384 384 Ea - - 12 12 A.3 Experiment Results We report comprehensive evaluation results to demonstrate performance progression throughout the training process. Tables 2 to 9 present the pre-training results across various models and different training tokens. Similarly, Tables 10 to 13 summarize the performance metrics for the mid-training stage."
        },
        {
            "title": "B Experiment Details for RL",
            "content": "B.1 Implementation Details For RLVR on mathematical reasoning tasks, we employ the on-policy GRPO algorithm (Shao et al., 2024) without KL regularization. Following Yu et al. (2025), we incorporate clip-higher and dynamic sampling strategies to stabilize training. The process is conducted in two stages: an initial 700 steps with sequence length of 8K, followed by continued training at sequence length of 16K. We maintain batch size of 128 and constant learning rate of 1 106 for two stages. During training, we sample 16 outputs per prompt at temperature of 1.0. B.2 Experiment Results We provide detailed evaluation results to illustrate performance trajectories during the RL process. Tables 14 to 23 display the RL results across different models and training steps. 12 Table 2: Pre-Training performance comparison across different β based on 1B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = 0 β = 0.50; λ = 0; ˆλ = 0 # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 20.47 8.01 3.02 8.45 9.99 38.24 38.43 67.36 51.70 31.40 19.66 41.13 23.65 9.25 4.34 12.32 12. 44.17 42.22 69.53 49.96 32.40 19.00 42.88 24.06 9.54 4.88 14.75 13.31 46.17 40.53 69.70 52.09 31.80 21.13 43.57 25.12 10.06 5.35 16.03 14.14 47.49 39.15 71.27 51.62 32.20 20.80 43.76 20.85 7.63 2.74 8.65 9. 38.96 40.02 67.90 52.49 29.80 19.57 41.46 23.39 8.23 4.07 12.63 12.08 44.42 40.33 69.75 52.17 32.20 21.79 43.44 24.04 7.98 4.52 13.53 12.52 46.26 41.50 71.00 52.80 33.60 19.82 44.16 26.62 9.02 5.79 16.25 14. 48.33 42.32 71.11 53.83 32.80 20.07 44.74 21.71 7.85 2.69 8.35 10.15 38.88 39.36 67.57 53.43 30.00 20.15 41.57 24.26 8.55 3.77 12.39 12.24 43.95 40.84 69.59 54.54 31.60 19.41 43.32 24.37 9.44 4.18 14.24 13. 46.83 42.02 70.62 54.70 31.80 20.72 44.45 25.51 9.59 5.21 16.49 14.20 48.06 42.32 70.02 53.28 33.40 20.56 44.61 Knowledge Average 25.56 27. 28.44 28.95 25.71 27.76 28.34 29. 25.86 27.78 28.75 29.40 Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 52.74 25.34 23.47 33.85 40.06 34.16 14.62 20.39 27. 8.06 21.39 14.73 55.30 27.47 23.41 35.39 43.01 37.09 16.19 22.72 29.75 13.03 33.69 23.36 59.43 30.03 26.46 38.64 46.15 39.79 17.07 23.50 31. 15.46 40.36 27.91 59.26 30.97 26.68 38.97 49.52 38.15 15.76 22.35 31.45 15.81 41.70 28.76 51.64 27.30 22.13 33.69 41.09 33.41 16.44 21.06 28. 8.71 20.63 14.67 55.68 29.44 22.56 35.89 48.78 35.43 16.71 21.01 30.48 12.15 34.37 23.26 56.70 29.52 25.68 37.30 48.77 37.10 14.80 21.85 30. 12.72 37.53 25.13 58.80 29.01 27.34 38.38 48.76 38.49 16.43 21.83 31.38 15.95 41.67 28.81 49.24 25.51 25.13 33.29 43.52 30.41 14.94 20.83 27. 7.68 16.58 12.13 55.64 27.05 22.49 35.06 46.65 39.64 15.32 22.91 31.13 11.92 31.37 21.65 55.51 27.30 24.99 35.93 46.58 40.80 16.54 23.00 31. 13.67 38.18 25.93 58.88 27.90 26.37 37.72 49.98 38.97 15.99 22.91 31.96 14.55 39.26 26.91 Reasoning Average 25. 29.50 32.73 33.06 25.45 29.88 31. 32.86 24.28 29.28 31.20 32.19 Average 25.40 28.76 31.01 31.41 25.56 29. 29.95 31.55 24.91 28.68 30.22 31. Table 3: Pre-Training performance comparison across different β based on 4B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = 0 β = 0.50; λ = 0; ˆλ = 0 # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 24.98 8.81 6.87 17.26 14.48 50.36 42.99 71.98 52.72 33.00 21.13 45. 30.94 11.11 9.11 25.14 19.08 57.81 44.27 74.65 56.12 36.00 29.48 49.72 34.38 11.99 10.89 30.39 21.91 60.80 44.32 75.52 57.22 36.00 37.43 51.88 36.38 13.47 12.58 33.39 23.96 63.01 45.44 75.57 59.04 36.80 49.63 54. 25.76 9.25 6.65 17.91 14.89 50.22 42.37 72.42 53.67 33.40 20.39 45.41 29.91 10.90 9.75 25.94 19.13 57.00 43.35 74.70 55.80 36.40 28.26 49.25 33.61 11.71 10.89 27.80 21.00 60.50 43.65 75.35 56.67 37.00 47.91 53. 36.17 12.23 12.22 33.83 23.61 62.91 44.73 76.28 58.72 36.00 52.91 55.26 24.45 8.48 6.32 17.19 14.11 39.15 41.40 71.76 53.67 34.00 18.43 42.85 30.14 9.93 8.92 24.26 18.31 57.49 41.71 74.43 55.80 36.00 29.32 48. 32.84 10.96 10.28 30.10 21.05 60.98 43.24 74.32 56.67 37.80 37.10 51.91 36.55 12.63 11.69 32.90 23.44 62.87 44.78 75.46 58.72 39.40 48.98 55."
        },
        {
            "title": "Knowledge Average",
            "content": "29.92 34.40 36.90 39.44 30.15 34. 37.26 39.44 28.48 33.59 36.48 39."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 61.15 31.83 26.14 39.71 49.66 39.07 15.48 22.24 31.61 17.11 44.07 30.59 66.84 35.49 26.32 42. 62.66 47.73 20.73 24.87 39.00 22.94 59.66 41.30 69.44 36.77 30.36 45.52 67.23 48.62 19.37 24.08 39.83 27.79 65.54 46.67 70.29 37.80 31.65 46. 71.19 51.14 20.07 24.18 41.65 31.29 65.63 48.46 60.27 30.55 26.54 39.12 48.43 38.19 15.09 21.79 30.88 17.32 40.66 28.99 63.05 33.96 27.42 41. 60.24 48.15 19.22 25.53 38.29 23.22 56.64 39.93 67.97 36.77 29.53 44.76 68.75 48.88 19.68 23.70 40.25 30.08 65.69 47.89 67.55 37.54 28.29 44. 71.26 51.54 20.63 24.84 42.07 29.13 66.29 47.71 58.96 31.91 25.02 38.63 50.39 40.86 16.33 22.23 32.45 16.73 43.90 30.32 64.98 33.61 26.95 41. 61.71 46.72 18.45 23.81 37.67 24.36 59.74 42.05 65.82 37.12 28.17 43.70 69.88 48.23 17.64 25.33 40.27 27.72 65.07 46.40 66.75 37.12 29.78 44. 71.98 51.67 19.85 24.87 42.09 28.52 65.65 47."
        },
        {
            "title": "Reasoning Average",
            "content": "33.97 41.06 44.00 45.56 33.00 39. 44.30 44.75 33.80 40.52 43.46 44."
        },
        {
            "title": "Average",
            "content": "32.35 38.40 41.16 43.11 31.86 37. 41.48 42.62 31.67 37.75 40.66 42. 13 Table 4: Pre-Training performance comparison across different β based on 5B-A0.3B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = 0 β = 0.50; λ = 0; ˆλ = # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 22.70 8.80 5.21 15.05 12. 47.89 40.53 71.49 52.88 30.80 20.64 44.04 27.21 8.56 7.45 22.23 16.36 54.08 41.25 72.63 53.35 35.60 27.27 47.36 29.36 9.42 8.45 25.51 18.19 56.70 42.68 73.67 57.54 34.00 33.25 49.64 29.77 11.16 10.06 28.08 19. 57.78 43.14 75.24 57.38 36.60 38.57 51.45 23.10 9.47 5.15 14.93 13.16 48.54 41.25 71.27 50.43 31.20 18.67 43.56 25.12 8.89 7.51 22.03 15.89 54.63 42.27 73.18 53.20 32.80 22.93 46.50 29.17 10.06 8.73 25.27 18. 56.88 43.76 74.43 56.51 34.40 28.50 49.08 31.19 11.87 10.42 28.25 20.43 57.41 42.68 74.76 56.20 33.60 35.38 50.01 24.13 8.62 5.76 13.75 13.07 48.58 40.28 71.60 52.01 33.80 19.41 44.28 26.99 9.83 7.48 21.27 16. 54.31 42.43 73.45 54.22 34.00 22.69 46.85 28.40 9.15 8.78 25.96 18.07 56.81 42.01 74.59 55.09 35.80 25.88 48.36 30.34 10.36 10.17 28.00 19.72 58.13 43.19 74.59 56.43 35.60 31.37 49.89 Knowledge Average 28.49 31.86 33.91 35.61 28.36 31. 33.69 35.22 28.67 31.62 33.22 34. Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 57.79 27.47 23.30 36.19 52.99 39.15 17.91 22.79 33.21 18.11 37.25 27.68 60.98 32.25 26.80 40.01 59.43 43.06 18.98 23.65 36.28 23.31 55.06 39. 62.12 32.94 27.12 40.73 64.26 48.46 18.79 24.29 38.95 26.20 58.57 42.39 62.08 34.39 27.85 41.44 66.78 52.00 21.79 25.62 41.55 28.44 60.06 44. 58.54 30.80 23.97 37.77 51.48 37.57 15.47 23.50 32.01 17.52 39.81 28.67 62.92 34.81 25.79 41.17 59.15 43.69 16.93 24.14 35.98 22.79 54.29 38. 64.48 35.67 27.71 42.62 61.81 46.91 18.00 23.40 37.53 26.15 55.93 41.04 63.34 35.15 27.03 41.84 65.65 51.19 17.92 24.42 39.80 29.32 57.77 43. 58.12 28.67 25.94 37.58 51.36 38.99 14.81 23.73 32.22 17.14 38.75 27.95 64.02 33.36 26.69 41.36 56.21 42.89 18.87 23.10 35.27 22.20 52.50 37. 63.55 34.13 27.00 41.56 62.12 47.80 17.61 25.18 38.18 27.53 56.45 41.99 63.38 34.56 27.49 41.81 65.04 48.26 19.36 26.00 39.67 27.95 59.97 43. Reasoning Average 32.36 38.49 40.69 42.41 32. 38.56 40.40 41.73 32.58 37.99 40. 41.81 Average 30.81 35.84 37.98 39. 31.03 35.62 37.72 39.12 31.02 35. 37.63 39.01 Table 5: Pre-Training performance comparison across different β based on 10B-A0.5B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = β = 0.50; λ = 0; ˆλ = 0 # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 26.13 9.44 6.79 20.69 15.76 52.86 41.61 73.45 54.06 34.40 20.07 46.08 32.74 10.21 10.36 29.78 20.77 59.47 43.45 74.32 56.59 35.00 34.64 50.58 35.09 11.38 12.47 35.25 23.55 61.65 45.14 74.76 56.27 35.40 44.80 53. 36.60 12.26 13.99 37.95 25.20 63.26 45.60 75.68 58.64 37.40 51.60 55.36 25.35 8.24 7.06 19.94 15.15 53.00 42.02 72.74 52.88 32.80 22.52 45.99 29.14 10.01 10.47 29.86 19.87 59.27 42.73 74.59 56.12 35.60 33.74 50. 33.50 11.64 11.41 33.89 22.61 62.00 43.45 75.52 59.04 35.60 35.54 51.86 35.12 12.15 11.96 37.31 24.14 63.71 45.09 76.71 58.88 38.20 43.90 54.42 26.05 8.81 7.84 20.73 15.86 53.09 41.91 74.10 53.83 33.20 21.54 46. 30.71 11.32 10.58 29.64 20.56 59.17 42.84 74.92 56.04 37.60 32.02 50.43 34.03 10.97 11.36 35.31 22.92 61.96 44.63 76.55 58.80 37.20 37.51 52.78 35.48 11.90 13.77 39.13 25.07 63.48 43.76 76.88 59.04 39.40 43.16 54."
        },
        {
            "title": "Knowledge Average",
            "content": "30.92 35.68 38.28 40.28 30.57 35. 37.23 39.28 31.07 35.50 37.85 39."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 60.94 31.91 27.34 40.06 57.59 42.95 16.68 22.26 34.87 20.72 52.05 36.39 65.91 37.20 28.24 43. 66.86 52.32 18.55 24.79 40.63 29.90 65.95 47.93 66.84 35.67 29.35 43.95 66.89 54.27 20.30 24.54 41.50 34.53 70.15 52.34 70.29 37.46 29.30 45. 73.64 57.82 21.75 24.58 44.45 34.19 73.34 53.77 63.01 32.68 25.21 40.30 59.40 41.29 17.23 20.53 34.61 19.12 51.01 35.07 67.09 35.24 25.37 42. 70.02 50.96 18.24 22.27 40.37 28.36 63.64 46.00 67.63 36.52 28.15 44.10 75.25 53.70 21.93 22.77 43.41 30.82 66.74 48.78 67.00 37.29 29.44 44. 76.63 56.76 22.41 24.31 45.03 34.66 70.16 52.41 62.37 32.51 24.51 39.80 56.16 41.69 17.37 22.04 34.32 19.07 51.40 35.24 66.37 36.09 27.37 43. 66.38 48.35 19.38 25.70 39.95 26.91 62.94 44.93 68.77 37.20 27.74 44.57 72.31 50.79 19.95 24.86 41.98 32.81 68.06 50.44 69.32 38.82 28.29 45. 74.60 56.46 21.71 26.52 44.82 33.38 72.47 52."
        },
        {
            "title": "Reasoning Average",
            "content": "37.11 44.11 45.93 47.97 36.66 42. 45.43 47.34 36.45 42.72 45.66 47."
        },
        {
            "title": "Average",
            "content": "34.63 40.74 42.87 44.89 34.22 39. 42.15 44.11 34.30 39.83 42.54 44. 14 Table 6: Pre-Training performance comparison across different λ and ˆλ based on 1B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = 0 β = 0; λ = 0.1; ˆλ = # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 21.61 8.34 2.66 8.61 10. 38.61 39.51 66.70 50.20 29.40 19.49 40.65 23.71 9.32 4.71 12.70 12.61 44.06 38.84 69.64 50.36 31.60 19.08 42.26 24.04 8.42 4.90 14.95 13.08 46.40 40.63 70.84 51.54 32.20 20.39 43.67 26.06 8.67 5.21 15.59 13. 48.17 40.48 70.95 52.33 32.40 18.92 43.88 20.85 7.63 2.74 8.65 9.97 38.96 40.02 67.90 52.49 29.80 19.57 41.46 23.39 8.23 4.07 12.63 12.08 44.42 40.33 69.75 52.17 32.20 21.79 43.44 24.04 7.98 4.52 13.53 12. 46.26 41.50 71.00 52.80 33.60 19.82 44.16 26.62 9.02 5.79 16.25 14.42 48.33 42.32 71.11 53.83 32.80 20.07 44.74 23.59 9.04 3.05 8.76 11.11 38.87 38.79 67.30 48.93 30.80 20.64 40.89 22.92 8.69 4.79 11.18 11. 43.72 40.53 69.48 51.38 30.20 19.00 42.39 24.63 8.50 4.74 14.08 12.99 46.67 41.30 71.60 54.78 32.20 18.84 44.23 25.89 9.23 5.15 15.50 13.94 48.26 42.37 71.11 52.64 31.40 23.34 44.85 Knowledge Average 25.48 27.44 28.37 28.88 25.71 27. 28.34 29.58 26.00 27.14 28.61 29. Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 52.36 27.65 24.48 34.83 38.32 34.17 14.35 20.44 26.82 8.13 19.08 13.61 57.28 29.86 24.33 37.16 44.09 36.42 16.68 22.29 29.87 13.04 30.08 21. 59.09 28.84 25.19 37.71 47.21 38.42 16.20 21.95 30.95 15.12 38.37 26.75 58.46 29.78 23.42 37.22 47.38 39.69 17.07 23.76 31.98 14.69 43.02 28. 51.64 27.30 22.13 33.69 41.09 33.41 16.44 21.06 28.00 8.71 20.63 14.67 55.68 29.44 22.56 35.89 48.78 35.43 16.71 21.01 30.48 12.15 34.37 23. 56.70 29.52 25.68 37.30 48.77 37.10 14.80 21.85 30.63 12.72 37.53 25.13 58.80 29.01 27.34 38.38 48.76 38.49 16.43 21.83 31.38 15.95 41.67 28. 50.21 26.02 23.32 33.18 41.85 33.52 15.11 21.12 27.90 8.84 19.25 14.05 55.01 28.16 24.42 35.86 43.30 38.01 16.89 21.24 29.86 12.34 32.66 22. 58.38 30.46 24.05 37.63 45.25 37.35 17.21 20.77 30.15 16.91 40.90 28.91 60.35 30.20 25.59 38.71 50.04 37.29 16.87 21.62 31.46 16.39 41.62 29. Reasoning Average 25.09 29.53 31.80 32.68 25. 29.88 31.02 32.86 25.04 29.41 32. 33.06 Average 25.24 28.69 30.43 31. 25.56 29.03 29.95 31.55 25.43 28. 30.78 31.59 Table 7: Pre-Training performance comparison across different λ and ˆλ based on 4B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = β = 0; λ = 0.1; ˆλ = 0 # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 23.79 9.57 6.15 17.19 14.18 50.56 42.17 71.27 54.62 34.40 19.41 45.41 30.57 10.37 9.92 26.53 19.35 57.22 42.48 74.37 56.43 38.60 30.06 49.86 33.71 11.50 11.41 31.13 21.94 60.25 44.11 74.16 59.04 38.00 39.80 52. 34.80 11.83 11.77 33.59 23.00 62.27 46.98 75.19 58.88 37.80 46.76 54.65 25.76 9.25 5.26 17.91 14.89 50.22 42.37 72.42 53.67 33.40 20.39 45.41 29.91 10.90 9.34 25.94 19.13 57.00 43.35 74.70 55.80 36.40 28.26 49. 33.61 11.71 10.86 27.80 21.00 60.50 43.65 75.35 56.67 37.00 47.91 53.51 36.17 12.23 12.35 33.83 23.61 62.91 44.73 76.28 58.72 36.00 52.91 55.26 23.02 8.44 6.65 16.93 13.41 50.21 41.40 71.16 53.91 32.80 20.56 45. 31.66 11.37 9.75 25.99 19.59 58.09 44.52 73.88 57.54 35.80 30.06 49.98 33.80 11.87 10.89 30.57 21.78 61.30 45.39 73.56 58.25 36.40 41.52 52.74 35.43 12.20 12.22 33.29 23.32 62.09 45.29 76.12 59.91 37.40 46.93 54."
        },
        {
            "title": "Knowledge Average",
            "content": "29.79 34.60 37.25 38.82 30.15 34. 37.26 39.44 29.21 34.79 37.26 38."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 59.51 31.57 27.38 39.49 46.40 38.86 16.60 21.80 30.92 16.71 42.24 29.48 65.32 35.15 28.08 42. 59.61 47.09 17.99 23.30 37.00 23.77 56.78 40.28 68.31 36.95 29.15 44.80 62.93 48.23 17.90 24.40 38.37 27.27 63.88 45.58 68.14 37.20 29.15 44. 70.92 52.47 21.27 24.19 42.21 29.41 66.01 47.71 60.27 30.55 26.54 39.12 48.43 38.19 15.09 21.79 30.88 17.32 40.66 28.99 63.05 33.96 27.42 41. 60.24 48.15 19.22 25.53 38.29 23.22 56.64 39.93 67.97 36.77 29.53 44.76 68.75 48.88 19.68 23.70 40.25 30.08 65.69 47.89 67.55 37.54 28.29 44. 71.26 51.54 20.63 24.84 42.07 29.13 66.29 47.71 60.19 31.66 26.22 39.36 48.09 38.16 16.67 22.92 31.46 15.42 38.19 26.81 67.05 35.75 27.15 43. 58.17 43.83 18.06 25.74 36.45 20.41 57.32 38.87 68.48 38.23 30.32 45.68 61.84 46.77 19.47 25.16 38.31 29.56 64.74 47.15 67.63 37.20 28.80 44. 70.94 50.45 20.54 25.62 41.89 31.08 65.65 48."
        },
        {
            "title": "Reasoning Average",
            "content": "33.29 40.04 42.91 44.92 33.00 39. 44.30 44.75 32.54 39.54 43.71 44."
        },
        {
            "title": "Average",
            "content": "31.89 37.87 40.65 42.48 31.86 37. 41.48 42.62 29.21 37.64 41.13 42. 15 Table 8: Pre-Training performance comparison across different λ and ˆλ based on 5B-A0.3B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = 0 β = 0; λ = 0.1; ˆλ = # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 22.24 7.55 5.60 14.54 12. 48.44 40.48 69.48 52.80 32.00 19.74 43.82 26.11 8.74 7.34 21.20 15.85 53.64 43.55 72.96 54.85 31.80 23.67 46.75 26.41 8.82 7.37 25.70 17.08 56.72 43.65 73.39 55.01 33.40 20.15 47.05 29.99 10.36 8.84 27.26 19. 57.58 42.84 74.37 55.56 34.20 30.30 49.14 23.10 9.47 5.15 14.93 13.16 48.54 41.25 71.27 50.43 31.20 18.67 43.56 25.12 8.89 7.51 22.03 15.89 54.63 42.27 73.18 53.20 32.80 22.93 46.50 29.17 10.06 8.73 25.27 18. 56.88 43.76 74.43 56.51 34.40 28.50 49.08 31.19 11.87 10.42 28.25 20.43 57.41 42.68 74.76 56.20 33.60 35.38 50.01 22.20 8.18 5.21 15.09 12.67 48.51 38.84 71.55 52.88 32.20 20.07 44.01 26.45 9.63 7.95 21.84 16. 53.89 41.50 74.43 54.22 34.40 24.65 47.18 29.44 10.11 8.31 25.85 18.43 56.42 41.50 74.43 56.91 34.40 27.76 48.57 31.37 11.02 9.81 28.91 20.28 57.33 41.81 74.92 55.96 36.40 31.86 49.71 Knowledge Average 28.15 31.30 32.06 34.13 28.36 31. 33.69 35.22 28.34 31.82 33.50 35. Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 57.15 30.20 24.56 37.30 50.99 39.91 16.36 22.16 32.36 14.01 38.32 26.17 61.49 33.28 24.60 39.79 59.44 46.65 15.98 23.01 36.27 22.49 52.88 37. 63.43 34.73 28.29 42.15 62.10 48.92 18.30 25.23 38.64 27.53 57.44 42.49 63.01 33.62 26.45 41.03 64.61 50.65 18.83 25.54 39.91 27.00 60.50 43. 58.54 30.80 23.97 37.77 51.48 37.57 15.47 23.50 32.01 17.52 39.81 28.67 62.92 34.81 25.79 41.17 59.15 43.69 16.93 24.14 35.98 22.79 54.29 38. 64.48 35.67 27.71 42.62 61.81 46.91 18.00 23.40 37.53 26.15 55.93 41.04 63.34 35.15 27.03 41.84 65.65 51.19 17.92 24.42 39.80 29.32 57.77 43. 58.63 31.06 22.33 37.34 48.16 38.71 17.31 22.62 31.70 15.27 40.39 27.83 62.25 34.73 26.28 41.09 58.15 42.31 18.01 24.02 35.62 21.36 49.00 35. 62.42 34.04 28.09 41.52 60.69 46.49 17.78 24.57 37.38 24.62 56.04 40.33 62.08 34.98 27.74 41.60 59.90 48.60 18.02 24.19 37.68 27.88 60.17 44. Reasoning Average 31.94 37.92 41.09 41.56 32. 38.56 40.40 41.73 32.29 37.30 39. 41.10 Average 30.43 35.27 37.48 38. 31.03 35.62 37.72 39.12 30.71 35. 37.25 38.66 Table 9: Pre-Training performance comparison across different λ and ˆλ based on 10B-A0.5B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = β = 0; λ = 0.1; ˆλ = 0 # Pre-Trained Tokens 125B 250B 375B 500B 125B 250B 375B 500B 125B 250B 375B 500B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 25.99 9.40 7.51 20.55 15.86 52.70 40.63 73.23 52.72 34.20 21.38 45.81 31.87 11.10 11.22 29.22 20.85 59.23 43.60 74.92 57.54 35.60 37.92 51.47 35.24 12.17 11.94 33.57 23.23 63.30 44.37 75.90 58.56 37.00 44.64 53. 36.13 13.19 12.60 37.30 24.81 63.68 44.17 76.22 59.98 36.80 48.89 54.96 25.35 8.24 7.06 19.94 15.15 53.00 42.02 72.74 52.88 32.80 22.52 45.99 29.14 10.01 10.47 29.86 19.87 59.27 42.73 74.59 56.12 35.60 33.74 50. 33.50 11.64 11.41 33.89 22.61 62.00 43.45 75.52 59.04 35.60 35.54 51.86 35.12 12.15 11.96 37.31 24.14 63.71 45.09 76.71 58.88 38.20 43.90 54.42 25.38 8.96 7.06 21.12 15.63 53.06 41.04 72.42 53.83 33.60 19.25 45. 32.48 11.19 10.44 29.48 20.90 59.75 44.17 74.70 57.14 35.40 34.81 51.00 34.47 12.64 11.25 33.93 23.07 63.67 45.34 75.52 59.12 36.20 41.03 53.48 36.75 12.34 13.52 37.61 25.06 63.38 45.50 76.50 59.98 35.60 45.29 54."
        },
        {
            "title": "Knowledge Average",
            "content": "30.84 36.16 38.60 39.88 30.57 35. 37.23 39.28 30.58 35.95 38.28 39."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 60.69 33.87 25.63 40.06 55.13 43.56 17.23 22.90 34.71 21.82 48.31 35.07 66.25 37.63 26.34 43. 69.42 49.61 19.23 22.84 40.28 29.67 63.41 46.54 69.61 38.73 29.06 45.80 71.80 54.32 20.27 24.30 42.67 32.92 68.98 50.95 68.43 37.71 29.10 45. 77.63 56.39 22.60 26.67 45.82 35.18 71.01 53.10 63.01 32.68 25.21 40.30 59.40 41.29 17.23 20.53 34.61 19.12 51.01 35.07 67.09 35.24 25.37 42. 70.02 50.96 18.24 22.27 40.37 28.36 63.64 46.00 67.63 36.52 28.15 44.10 75.25 53.70 21.93 22.77 43.41 30.82 66.74 48.78 67.00 37.29 29.44 44. 76.63 56.76 22.41 24.31 45.03 34.66 70.16 52.41 64.06 34.13 25.20 41.13 55.89 42.58 16.07 23.23 34.44 19.54 50.24 34.89 65.99 37.71 29.46 44. 65.29 49.50 19.55 24.28 39.66 25.32 60.62 42.97 70.03 37.88 29.72 45.88 71.44 52.46 21.59 25.25 42.69 30.97 68.22 49.60 67.59 39.42 29.70 45. 75.11 54.75 20.38 26.15 44.10 32.96 71.02 51."
        },
        {
            "title": "Reasoning Average",
            "content": "36.61 43.41 46.47 48.00 36.66 42. 45.43 47.34 36.82 42.34 46.05 47."
        },
        {
            "title": "Average",
            "content": "34.30 40.51 43.32 44.75 34.22 39. 42.15 44.11 34.33 39.78 42.94 44. 16 Table 10: Mid-Training performance comparison across different β based on 4B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = 0 β = 0.50; λ = 0; ˆλ = # Mid-Trained Tokens 25B 50B 75B 100B 25B 50B 75B 100B 25B 50B 75B 100B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 37.99 16.51 11.69 32.60 24. 61.59 44.78 75.24 60.69 35.60 52.09 55.00 39.51 18.44 11.75 32.75 25.61 61.70 43.71 76.01 59.67 38.20 52.42 55.29 39.69 19.69 12.16 34.05 26.40 61.95 44.42 75.19 60.22 37.20 54.38 55.56 39.92 19.70 12.33 34.50 26. 62.41 44.78 75.84 60.62 37.40 55.77 56.14 37.27 15.50 12.16 31.90 24.21 61.25 45.44 75.24 58.96 36.60 54.13 55.27 39.34 17.27 11.91 32.54 25.27 61.72 45.29 75.24 59.98 37.40 52.99 55.44 39.23 17.77 12.33 33.50 25. 62.14 45.75 75.24 60.46 38.00 53.97 55.93 40.47 18.68 12.55 34.17 26.47 62.09 45.85 75.73 60.85 37.40 54.87 56.13 36.53 14.97 11.75 32.55 23.95 61.68 44.32 74.54 58.88 39.20 49.14 54.63 38.45 16.44 11.14 32.78 24. 62.01 44.78 74.92 59.35 40.40 50.04 55.25 39.00 17.99 12.08 34.09 25.79 62.54 44.83 74.81 59.51 40.60 52.58 55.81 39.48 18.53 11.88 34.67 26.14 62.51 44.88 74.92 59.67 40.00 53.07 55.84 Knowledge Average 39.85 40.45 40.98 41.37 39.74 40. 40.82 41.30 39.29 39.98 40.80 40. Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 66.84 39.33 33.90 46.69 84.66 63.69 21.88 28.96 49.80 52.37 79.91 66.74 68.94 41.04 38.58 49.52 88.94 66.70 23.67 30.15 52.37 57.74 82.27 70. 70.45 40.78 39.66 50.30 91.17 70.73 26.82 32.15 55.22 64.85 85.48 75.17 69.82 41.89 39.83 50.51 91.25 70.41 25.99 32.21 54.97 64.33 86.79 75. 68.81 40.61 33.47 47.63 85.73 62.42 24.70 28.44 50.32 51.64 77.50 64.57 68.94 41.13 36.95 49.01 89.60 66.14 24.38 33.15 53.32 60.52 82.32 71. 69.82 41.64 36.45 49.30 92.23 69.48 28.22 32.96 55.72 64.95 84.81 74.88 69.99 41.89 37.28 49.72 92.70 71.35 26.72 32.65 55.86 65.89 85.28 75. 66.84 38.14 31.90 45.63 83.30 62.42 22.86 29.21 49.45 48.07 77.83 62.95 69.19 41.38 34.88 48.48 88.82 65.49 23.90 29.80 52.00 60.60 82.51 71. 69.91 40.87 36.11 48.96 91.75 68.17 23.95 31.51 53.85 63.31 83.77 73.54 71.17 41.72 35.92 49.60 92.08 68.97 24.99 32.85 54.72 66.30 83.41 74. Reasoning Average 54.21 57.30 60.23 60.35 54. 57.91 59.97 60.39 52.67 57.35 58. 59.73 Average 48.46 50.56 52.53 52. 48.40 50.89 52.31 52.75 47.32 50. 51.59 52.23 Table 11: Mid-Training performance comparison across different β based on 10B-A0.5B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0.25; λ = 0; ˆλ = 0 β = 0; λ = 0; ˆλ = β = 0.50; λ = 0; ˆλ = 0 # Mid-Trained Tokens 25B 50B 75B 100B 25B 50B 75B 100B 25B 50B 75B 100B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 36.39 12.31 12.80 36.61 24.53 62.25 43.86 75.46 58.80 37.40 50.53 54.72 37.23 12.88 13.77 37.37 25.31 62.39 44.63 74.92 59.98 36.80 50.45 54.86 37.75 13.69 14.18 38.29 25.98 62.94 44.68 75.57 60.54 37.60 50.61 55. 38.11 13.91 14.35 38.76 26.28 63.25 45.19 75.95 60.46 38.00 50.61 55.58 36.15 12.98 11.30 35.84 24.07 62.47 45.55 76.06 58.96 36.60 43.90 53.92 37.27 13.91 12.11 37.15 25.11 62.82 43.04 75.63 58.88 37.20 46.52 54. 37.54 14.41 12.44 37.81 25.55 63.50 43.70 75.73 59.59 37.00 49.06 54.76 37.82 14.71 12.85 38.38 25.94 63.74 44.22 76.55 59.83 37.20 49.06 55.10 35.04 12.03 11.30 37.57 24.40 62.51 44.63 75.90 58.64 40.00 40.46 53. 36.49 12.82 12.11 38.84 25.41 62.67 44.01 76.06 59.51 40.40 42.75 54.23 36.45 13.32 12.44 39.62 25.87 63.29 44.73 76.50 60.77 40.20 43.73 54.87 37.10 13.46 12.85 40.21 26.18 63.43 44.01 76.82 60.30 40.60 43.90 54."
        },
        {
            "title": "Knowledge Average",
            "content": "39.62 40.09 40.65 40.93 39.00 39. 40.16 40.52 39.04 39.82 40.37 40."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 68.35 38.65 30.70 45.90 85.06 65.20 25.33 29.73 51.33 48.79 77.31 63.05 68.90 40.10 31.64 46. 87.67 68.85 25.02 30.76 53.08 55.88 80.74 68.31 69.40 41.30 31.84 47.51 90.35 70.19 26.26 32.84 54.91 56.24 84.55 70.40 70.12 40.53 32.05 47. 90.53 71.73 26.53 33.21 55.50 58.05 83.41 70.73 68.35 39.59 30.64 46.40 87.16 64.39 24.51 28.91 51.24 48.00 76.28 62.14 68.90 40.44 33.68 48. 90.68 68.35 23.83 30.05 53.23 53.35 80.29 66.82 69.40 42.49 33.59 48.91 90.83 70.77 24.77 33.11 54.87 56.98 81.82 69.40 70.12 42.83 34.14 49. 92.03 70.97 27.03 32.63 55.67 55.18 81.38 68.28 68.52 39.93 30.69 46.38 84.62 64.27 23.51 28.57 50.24 49.33 76.03 62.68 70.03 40.87 32.42 47. 88.39 68.59 26.76 29.86 53.40 53.14 78.12 65.63 69.87 41.64 33.25 48.25 89.84 67.46 27.11 32.82 54.31 55.06 80.15 67.61 70.24 41.98 33.44 48. 90.30 70.00 25.90 33.39 54.90 56.21 80.80 68."
        },
        {
            "title": "Reasoning Average",
            "content": "53.43 56.09 57.61 57.93 53.26 56. 57.73 57.74 53.10 55.60 56.72 57."
        },
        {
            "title": "Average",
            "content": "47.90 49.69 50.82 51.13 47.56 49. 50.70 50.85 47.48 49.29 50.18 50. 17 Table 12: Mid-Training performance comparison across different λ and ˆλ based on 4B dense models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = 0 β = 0; λ = 0.1; ˆλ = # Mid-Trained Tokens 25B 50B 75B 100B 25B 50B 75B 100B 25B 50B 75B 100B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 37.33 14.17 11.25 33.29 24. 61.11 45.09 75.46 60.22 39.80 46.93 54.77 38.73 16.61 11.99 33.89 25.31 61.25 44.37 76.01 60.22 38.20 51.84 55.32 38.83 17.57 12.30 34.80 25.88 61.76 44.06 76.22 61.48 38.20 53.73 55.91 39.13 18.23 12.66 35.32 26. 62.29 45.24 76.12 60.46 38.00 53.81 55.99 37.27 15.50 12.16 31.90 24.21 61.25 45.44 75.24 58.96 36.60 54.13 55.27 39.34 17.27 11.91 32.54 25.27 61.72 45.29 75.24 59.98 37.40 52.99 55.44 39.23 17.77 12.33 33.50 25. 62.14 45.75 75.24 60.46 38.00 53.97 55.93 40.47 18.68 12.55 34.17 26.47 62.09 45.85 75.73 60.85 37.40 54.87 56.13 37.15 14.17 11.14 32.73 23.80 61.59 46.21 74.59 60.85 39.80 45.86 54.82 38.06 15.58 12.49 33.50 24. 61.46 46.21 74.76 60.14 38.60 47.67 54.81 38.76 16.41 12.58 34.17 25.48 61.98 46.37 75.41 59.51 39.60 49.80 55.45 39.43 17.50 12.74 34.59 26.07 62.16 46.62 75.41 59.98 40.20 50.20 55.76 Knowledge Average 39.39 40.31 40.89 41.16 39.74 40. 40.82 41.30 39.31 39.86 40.46 40. Logic Reasoning ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average Mathematics Coding GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 68.52 39.93 32.78 47.08 84.38 64.40 21.64 30.09 50.13 50.62 78.24 64.43 68.56 40.96 37.15 48.89 89.90 70.24 23.81 32.29 54.06 58.51 83.29 70. 70.75 41.81 38.49 50.35 90.43 72.50 25.65 33.52 55.53 64.58 85.36 74.97 71.34 42.66 39.30 51.10 91.69 72.78 26.86 34.13 56.37 65.23 85.22 75. 68.81 40.61 33.47 47.63 85.73 62.42 24.70 28.44 50.32 51.64 77.50 64.57 68.94 41.13 36.95 49.01 89.60 66.14 24.38 33.15 53.32 60.52 82.32 71. 69.82 41.64 36.45 49.30 92.23 69.48 28.22 32.96 55.72 64.95 84.81 74.88 69.99 41.89 37.28 49.72 92.70 71.35 26.72 32.65 55.86 65.89 85.28 75. 70.33 41.30 33.62 48.42 85.99 63.31 23.28 26.32 49.73 51.68 79.31 65.50 70.29 42.15 35.40 49.28 87.51 67.35 22.60 28.90 51.59 58.17 81.59 69. 71.04 42.24 36.58 49.95 90.68 70.36 25.53 30.55 54.28 63.94 85.17 74.56 71.25 43.34 37.74 50.78 90.11 70.31 25.43 34.11 54.99 65.31 85.23 75. Reasoning Average 53.88 57.95 60.28 60.90 54. 57.91 59.97 60.39 54.55 56.92 59. 60.35 Average 48.08 50.89 52.53 53. 48.40 50.89 52.31 52.75 48.45 50. 51.94 52.57 Table 13: Mid-Training performance comparison across different λ and ˆλ based on 10B-A0.5B MoE models. The highest scores at the final checkpoint across the different configurations are shown in bold. Hyperparameters β = 0; λ = 0; ˆλ = 0.1 β = 0; λ = 0; ˆλ = β = 0; λ = 0.1; ˆλ = 0 # Mid-Trained Tokens 25B 50B 75B 100B 25B 50B 75B 100B 25B 50B 75B 100B General Knowledge Commonsense Reasoning MMLU (Acc.) MMLU-Pro (Acc.) NaturalQuestions (EM) TriviaQA (EM) Average Hellaswag (Acc.) SIQA (Acc.) PIQA (Acc.) WinoGrande (Acc.) OpenBookQA (Acc.) CommonsenseQA (Acc.) Average 36.86 14.10 12.35 35.51 24.71 62.67 43.86 76.01 60.06 37.60 45.62 54.30 38.22 15.20 13.05 37.14 25.90 62.96 43.96 75.41 61.09 35.80 48.89 54.69 38.98 15.93 13.63 37.49 26.51 63.12 43.71 75.95 60.77 36.00 45.86 54. 39.07 16.39 13.60 38.25 26.83 63.50 43.71 76.22 58.88 35.40 48.48 54.37 36.15 12.98 11.30 35.84 24.07 62.47 45.55 76.06 60.06 36.60 43.90 53.92 37.27 13.91 12.11 37.15 25.11 62.82 43.04 75.63 61.09 37.20 46.52 54. 37.54 14.41 12.44 37.81 25.55 63.50 43.70 75.73 60.77 37.00 49.06 54.76 37.82 14.71 12.85 38.38 25.94 63.74 44.22 76.55 58.88 37.20 49.06 55.10 36.90 13.02 12.52 36.25 24.67 62.93 45.34 76.01 58.56 37.20 45.45 54. 37.96 15.05 12.63 36.98 25.66 63.07 45.50 75.73 59.59 35.80 47.83 54.59 38.29 14.67 13.30 38.01 26.07 63.42 45.91 76.17 60.46 37.00 47.91 55.15 39.30 14.72 13.49 38.44 26.49 63.73 45.45 76.12 61.01 37.80 49.63 55."
        },
        {
            "title": "Knowledge Average",
            "content": "39.50 40.29 40.37 40.60 39.00 39. 40.16 40.52 39.46 40.12 40.61 41."
        },
        {
            "title": "Logic\nReasoning",
            "content": "ARC-e (Acc.) ARC-c (Acc.) BBH (Acc.) Average"
        },
        {
            "title": "Coding",
            "content": "GSM8K (Pass@64) MATH-500 (Pass@64) Minerva (Pass@64) OlympiadBench (Pass@64) Average HumanEval+ (Pass@64) MBPP+ (Pass@64) Average 69.78 40.10 31.18 47.02 87.23 64.74 24.50 28.84 51.33 47.08 78.67 62.88 69.78 41.21 33.21 48. 90.34 69.10 25.23 30.41 53.77 50.89 79.80 65.35 70.58 41.81 33.71 48.70 91.10 71.01 25.50 31.34 54.74 52.11 81.25 66.68 71.09 41.30 34.05 48. 91.45 70.77 26.67 31.20 55.02 55.14 84.00 69.57 68.98 39.59 30.64 46.40 87.16 64.39 24.51 28.91 51.24 48.00 76.28 62.14 70.79 40.44 33.68 48. 90.68 68.35 23.83 30.05 53.23 53.35 80.29 66.82 70.66 42.49 33.59 48.91 90.83 70.77 24.77 33.11 54.87 56.98 81.82 69.40 70.83 42.83 34.14 49. 92.03 70.97 27.03 32.63 55.67 55.18 81.38 68.28 68.48 41.64 30.70 46.94 84.20 63.07 23.05 31.11 50.36 47.36 76.24 61.80 69.95 42.41 33.19 48. 87.47 65.99 26.98 32.09 53.13 51.56 80.35 65.96 70.56 41.81 32.64 48.34 89.89 69.52 27.02 32.57 54.75 55.66 82.42 69.04 71.13 42.06 32.73 48. 90.41 69.96 27.41 32.83 55.15 55.62 83.16 69."
        },
        {
            "title": "Reasoning Average",
            "content": "53.74 55.73 56.71 57.80 53.26 56. 57.73 57.74 53.03 55.87 57.38 57."
        },
        {
            "title": "Average",
            "content": "48.05 49.55 50.17 50.92 47.56 49. 50.70 50.85 47.60 49.57 50.67 51. 18 Table 14: RL performance of the 4B Dense Model with β = 0; λ = 0; ˆλ = 0. AIME24 AIME25 AMC23 OlympiadBench MATH-500 Minerva Average # RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 0.89 0.00 15.34 0.57 0.00 14.90 18.46 37.50 71.39 14.53 23.41 55.45 39.85 56.80 87.63 9.11 17.65 44. 13.90 22.56 48.21 200 0.81 0.00 13.77 0.86 0.00 11.67 23.07 35.00 76.75 17.94 26.22 55. 45.48 60.60 87.16 10.52 17.28 45.07 16.45 23.18 48.24 300 1.14 0.00 21.27 0.86 0.00 10. 24.94 40.00 79.44 19.98 27.11 56.70 48.23 62.60 87.38 11.19 18.38 47.02 17.72 24.68 50.44 1.04 0.00 14.80 1.46 3.33 10.00 27.87 47.50 75.70 21.45 28.00 56.49 50.56 62.00 88.36 11.99 17.64 44. 19.06 26.41 48.37 500 1.12 3.33 11.68 1.59 0.00 16.16 28.96 45.00 74.24 22.31 28.44 55. 51.41 61.60 88.88 12.52 19.12 43.97 19.65 26.25 48.46 600 1.54 3.33 22.86 2.29 6.67 10. 28.73 45.00 72.45 22.28 29.63 55.84 51.10 62.40 88.81 12.36 19.49 45.29 19.72 27.75 49.35 1.33 3.33 19.81 1.30 3.33 12.93 27.17 45.00 67.17 22.04 29.48 55.45 49.87 61.80 88.82 12.93 18.01 48. 19.11 26.83 48.84 800 1.69 6.67 19.68 2.06 3.33 14.35 28.18 45.00 77.07 23.67 32.00 56. 51.22 62.60 89.49 13.26 20.59 48.53 20.01 28.37 50.98 900 1.64 3.33 14.78 2.21 3.33 10. 29.16 47.50 78.26 24.64 33.04 57.57 52.20 65.40 90.18 12.43 19.12 47.36 20.38 28.62 49.83 Table 15: RL performance of the 10B-A0.5B MoE Model with β = 0; λ = 0; ˆλ = 0. AIME24 AIME25 AMC"
        },
        {
            "title": "OlympiadBench",
            "content": "MATH-"
        },
        {
            "title": "Average",
            "content": "# RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 100 0.34 0.00 12.88 0.13 0.00 6.68 10.64 20.00 75. 7.13 14.81 49.58 26.71 45.60 82.96 5.66 9.93 42.39 8.44 15.06 45.05 200 0.47 0.00 12. 0.23 0.00 11.27 11.82 25.00 76.02 8.32 16.00 49.53 28.91 48.40 82.71 5.70 10.66 40.93 9.24 16.68 45. 300 0.34 0.00 11.42 0.16 0.00 8.35 11.95 17.50 70.67 9.22 17.04 52.16 32.72 49.80 83. 6.42 11.40 41.04 10.14 15.96 44.56 500 0.63 0.00 12.93 0.36 0.00 16.58 14.26 25.00 73. 11.83 21.04 52.69 36.45 54.40 85.93 7.22 11.40 43.47 11.79 18.64 47.56 600 0.57 0.00 11. 0.47 0.00 19.10 16.11 30.00 74.33 12.45 21.33 52.50 37.67 54.40 85.79 7.84 13.24 42.89 12.52 19.83 47. 700 0.55 0.00 9.59 0.60 0.00 18.67 16.21 27.50 73.13 12.60 21.19 53.48 38.47 55.20 85. 7.98 12.87 44.35 12.74 19.46 47.48 800 0.55 3.33 9.17 0.44 0.00 14.10 17.79 35.00 73. 13.50 22.52 53.34 39.60 56.20 86.01 8.28 12.50 44.50 13.36 21.59 46.72 900 0.65 0.00 10. 0.63 0.00 16.66 18.38 32.50 71.65 14.41 24.30 52.67 41.38 58.20 86.04 8.36 12.50 40.90 13.97 21.25 46. 400 0.44 0.00 11.68 0.29 0.00 14.80 13.07 27.50 71.55 10.66 20.00 52.06 34.81 52.40 83. 6.93 12.50 42.75 11.03 18.73 46.04 19 1000 1.62 3.33 18.32 2.24 3.33 14. 29.51 45.00 77.43 24.56 33.78 58.09 52.26 64.60 89.95 12.75 19.49 47.55 20.49 28.26 50.99 0.65 0.00 10.84 0.57 0.00 16.59 19.06 37.50 73.71 15.03 24.89 53.31 42.19 58.20 85.75 8.73 13.97 44. 14.37 22.43 47.52 Table 16: RL performance of the 4B Dense Model with β = 0.25; λ = 0; ˆλ = 0. AIME24 AIME25 AMC23 OlympiadBench MATH-500 Minerva Average # RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 0.76 0.00 14.17 0.83 0.00 17.46 19.55 32.50 72.21 14.44 24.89 54.46 39.36 58.00 87.37 9.23 18.39 43. 14.03 22.30 48.21 200 0.96 3.33 17.11 1.46 0.00 13.33 25.47 40.00 79.29 17.77 26.52 56. 45.23 60.40 87.68 10.71 18.38 46.44 16.93 24.77 50.09 300 1.02 3.33 13.31 2.11 3.33 21. 29.08 40.00 75.65 19.71 28.44 57.38 48.29 62.00 88.34 11.16 18.75 46.34 18.56 25.98 50.49 0.55 0.00 12.10 1.72 3.33 19.12 30.96 45.00 74.33 20.38 28.15 55.02 49.88 60.80 88.53 12.03 17.65 46. 19.25 25.82 49.24 500 1.09 0.00 17.09 2.27 3.33 19.99 31.70 45.00 72.62 21.52 28.59 58. 50.59 63.20 88.89 12.14 18.75 46.30 19.89 26.48 50.51 600 1.43 3.33 19.99 2.11 3.33 15. 31.72 50.00 72.48 21.44 28.89 55.17 50.72 62.40 89.18 12.22 18.38 45.61 19.94 27.72 49.68 1.82 3.33 20.64 1.98 3.33 14.15 32.38 50.00 70.62 22.98 31.26 57.97 52.33 62.20 89.02 12.97 20.96 47. 20.74 28.51 49.93 800 2.03 6.67 19.17 2.19 3.33 16.51 32.85 45.00 71.24 23.33 30.81 57. 52.50 61.60 88.82 12.73 18.75 45.57 20.94 27.69 49.79 900 2.47 6.67 24.12 2.40 6.67 11. 33.01 50.00 76.36 23.52 31.11 56.56 53.09 61.80 89.21 12.99 18.38 46.61 21.25 29.11 50.76 Table 17: RL performance of the 4B Dense Model with β = 0.50; λ = 0; ˆλ = 0. AIME24 AIME25 AMC"
        },
        {
            "title": "OlympiadBench",
            "content": "MATH-"
        },
        {
            "title": "Average",
            "content": "# RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 100 0.94 3.33 18.77 0.83 0.00 17.05 19.34 32.50 76. 14.37 23.56 54.84 39.40 56.00 85.72 8.50 16.91 42.52 13.90 22.05 49.18 200 1.38 3.33 23. 1.88 6.67 15.84 22.52 37.50 73.68 18.07 26.22 57.02 45.70 59.40 87.81 10.44 16.91 45.32 16.67 25.01 50. 300 1.43 3.33 16.58 1.95 3.33 17.51 23.09 37.50 75.33 20.35 28.15 57.95 48.18 61.80 88. 11.41 17.65 44.50 17.74 25.29 49.98 500 2.29 6.67 23.65 3.39 10.00 15.00 26.60 45.00 72. 22.38 30.37 57.25 50.41 61.40 88.92 11.96 18.01 44.10 19.51 28.58 50.21 600 1.95 3.33 19. 3.57 10.00 19.59 25.46 42.50 74.94 22.27 29.93 58.01 50.58 61.80 89.49 11.67 16.91 43.17 19.25 27.41 50. 700 1.74 3.33 18.98 2.76 3.33 15.84 28.28 37.50 68.79 22.24 29.33 55.62 51.53 60.20 88. 11.73 16.54 41.51 19.71 25.04 48.22 800 2.19 3.33 24.56 3.67 6.67 20.01 29.45 40.00 71. 23.32 30.81 57.67 52.32 62.20 88.58 11.71 15.44 45.40 20.44 26.41 51.37 900 2.53 3.33 23. 4.14 10.00 18.77 29.49 42.50 73.76 24.44 31.85 57.93 53.20 62.40 89.39 12.23 15.44 46.06 21.01 27.59 51. 400 1.46 3.33 20.87 2.03 3.33 13.33 26.35 40.00 71.56 21.00 28.59 56.39 49.46 60.20 88. 11.34 18.01 43.82 18.61 25.58 49.12 20 1000 2.57 6.67 20.34 2.63 3.33 15. 33.09 50.00 74.00 23.67 31.26 57.23 52.98 62.20 89.94 13.11 19.49 45.22 21.34 28.83 50.43 2.42 3.33 22.08 4.66 10.00 15.00 30.10 42.50 73.35 24.96 32.00 56.91 54.00 63.20 89.76 12.10 16.18 43. 21.37 27.87 50.09 Table 18: RL performance of the 10B-A0.5B MoE Model with β = 0.25; λ = 0; ˆλ = 0. AIME24 AIME25 AMC23 OlympiadBench MATH-500 Minerva Average # RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 0.29 0.00 9.15 0.16 0.00 8.35 11.52 30.00 75.75 7.08 13.33 47.16 24.43 45.60 81.19 5.26 10.29 38. 8.12 16.54 43.38 200 0.49 0.00 7.93 0.29 0.00 13.78 14.02 35.00 73.72 9.60 17.78 49. 30.21 48.60 83.88 6.36 13.24 41.40 10.16 19.10 44.97 300 0.86 3.33 8.13 0.36 0.00 15. 16.88 37.50 73.54 11.51 20.59 52.08 34.68 52.60 85.36 7.45 13.24 43.51 11.96 21.21 46.35 0.49 0.00 10.84 0.36 0.00 7.92 18.48 37.50 76.83 12.79 20.89 51.30 37.65 54.20 84.99 8.24 13.24 44. 13.00 20.97 46.06 500 0.49 0.00 11.68 0.63 0.00 19.81 19.47 37.50 74.15 14.47 21.93 50. 40.62 54.20 85.75 9.02 15.07 43.11 14.12 21.45 47.52 600 0.86 3.33 14.17 0.63 0.00 17. 21.05 32.50 70.35 15.71 24.59 52.86 42.61 56.20 86.98 9.89 15.07 45.72 15.13 21.95 47.87 0.86 0.00 14.59 0.89 0.00 19.20 22.44 35.00 71.18 16.85 24.15 53.15 44.71 57.80 88.10 10.29 15.81 45. 16.01 22.13 48.69 800 0.78 3.33 19.19 0.70 0.00 13.12 22.66 35.00 74.50 16.02 24.00 52. 45.35 59.40 87.39 10.48 15.81 45.16 16.00 22.92 48.69 900 0.86 0.00 15.01 0.81 0.00 14. 21.25 32.50 74.13 16.78 25.19 54.66 44.40 57.00 87.16 10.14 16.18 45.82 15.71 21.81 48.49 Table 19: RL performance of the 10B-A0.5B MoE Model with β = 0.50; λ = 0; ˆλ = 0. AIME24 AIME25 AMC"
        },
        {
            "title": "OlympiadBench",
            "content": "MATH-"
        },
        {
            "title": "Average",
            "content": "# RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 100 0.42 0.00 18.31 0.26 0.00 14.19 11.17 17.50 70. 7.14 14.81 47.23 27.15 44.40 81.18 5.86 11.40 41.33 8.67 14.69 45.47 200 0.29 0.00 8. 0.16 0.00 9.17 13.46 25.00 72.46 8.17 15.56 50.58 30.51 47.40 82.20 5.92 10.66 42.59 9.75 16.44 44. 300 0.65 3.33 16.69 0.18 0.00 10.01 13.54 22.50 71.53 9.46 16.59 51.54 33.40 47.80 82. 6.83 11.76 43.28 10.68 17.00 45.98 500 0.68 0.00 18.77 0.26 0.00 13.77 15.90 25.00 76. 11.60 20.44 52.53 36.67 52.00 84.79 7.51 12.13 44.07 12.10 18.26 48.46 600 0.78 3.33 14. 0.36 0.00 14.97 17.27 30.00 78.90 12.76 21.63 51.78 38.32 52.80 84.78 8.13 12.13 44.88 12.94 19.98 48. 700 0.68 0.00 12.51 0.29 0.00 12.11 16.15 27.50 79.26 12.15 20.44 52.20 36.86 52.20 85. 7.94 12.50 44.56 12.35 18.77 47.72 800 1.02 3.33 17.53 0.26 0.00 11.06 17.60 30.00 74. 13.38 21.78 52.60 39.42 55.00 86.00 8.38 13.97 44.13 13.34 20.68 47.68 900 0.94 3.33 20. 0.34 0.00 13.67 18.42 32.50 78.60 14.04 22.81 53.27 40.66 55.60 85.97 8.80 13.60 46.17 13.87 21.31 49. 400 0.70 3.33 19.19 0.34 0.00 14.41 14.14 25.00 78.57 10.45 19.11 51.79 34.80 50.60 83. 7.01 11.03 42.82 11.24 18.18 48.39 21 1000 1.15 3.33 21.27 1.04 0.00 17. 22.40 30.00 78.66 17.68 26.96 55.99 44.86 58.00 86.89 9.96 16.91 44.46 16.18 22.53 50.75 1.04 3.33 16.58 0.68 0.00 19.02 19.67 32.50 77.41 15.17 25.19 52.94 41.36 57.00 86.24 9.01 16.91 45. 14.49 22.49 49.59 Table 20: RL performance of the 4B Dense Model with β = 0; λ = 0; ˆλ = 0.1. AIME24 AIME25 AMC23 OlympiadBench MATH-500 Minerva Average # RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 0.70 0.00 22.51 0.76 0.00 13.97 18.50 37.50 76.21 13.79 22.22 54.56 37.92 56.20 86.56 8.07 17.28 41. 13.29 22.20 49.24 200 0.70 3.33 12.51 1.09 0.00 15.85 20.76 40.00 78.48 15.29 24.30 56. 40.85 58.20 87.15 9.22 18.01 45.66 14.65 23.97 49.29 300 0.76 3.33 15.01 1.46 0.00 17. 21.84 37.50 79.17 17.63 25.04 55.14 43.98 59.00 87.23 10.25 15.07 46.22 15.99 23.32 50.05 0.89 3.33 14.19 1.90 3.33 19.59 24.67 40.00 73.11 19.50 26.81 55.54 45.69 59.00 87.95 11.08 15.81 43. 17.29 24.71 49.04 500 1.07 3.33 13.77 1.67 3.33 18.29 21.82 40.00 69.87 18.03 27.11 56. 43.78 59.80 87.60 10.67 17.28 43.71 16.17 25.14 48.32 600 0.81 3.33 15.01 2.32 6.67 16. 25.35 37.50 77.31 19.51 27.41 56.07 46.94 59.60 87.54 11.43 16.18 44.43 17.73 25.12 49.51 0.86 3.33 20.87 2.11 3.33 15.00 27.55 37.50 72.07 20.84 27.41 55.65 48.87 61.80 87.25 12.28 16.54 44. 18.75 24.99 49.19 800 1.15 3.33 21.03 3.02 3.33 21.90 28.73 42.50 75.05 22.23 29.63 56. 50.44 63.00 88.08 12.54 18.01 43.08 19.69 26.63 51.01 Table 21: RL performance of the 4B Dense Model with β = 0; λ = 0.1; ˆλ = 0. AIME24 AIME AMC"
        },
        {
            "title": "OlympiadBench",
            "content": "MATH-"
        },
        {
            "title": "Average",
            "content": "# RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 100 0.49 0.00 9.17 0.68 0.00 21.79 18.24 32.50 75. 13.38 22.96 54.04 37.71 56.00 84.83 8.00 14.71 41.49 13.08 21.03 47.77 200 0.55 0.00 13. 0.83 0.00 22.09 20.59 32.50 71.98 16.45 25.48 55.12 43.26 58.80 87.11 9.30 16.54 43.44 15.16 22.22 48. 300 0.91 0.00 23.37 1.35 3.33 17.83 24.16 42.50 75.55 18.73 27.56 55.80 47.34 62.20 88. 10.37 16.91 45.78 17.14 25.42 51.13 500 0.68 0.00 18.97 1.28 3.33 16.61 23.89 42.50 70. 19.28 27.11 56.31 48.09 63.60 87.97 11.03 16.91 44.17 17.38 25.58 49.13 600 0.63 0.00 17. 1.25 3.33 15.81 28.38 42.50 71.66 21.67 29.48 55.42 51.15 63.80 87.80 11.39 16.91 43.23 19.08 26.00 48. 700 0.68 0.00 20.65 1.51 3.33 17.48 29.20 42.50 72.38 21.92 29.33 57.13 51.84 62.80 87. 11.47 16.54 44.97 19.44 25.75 50.04 800 0.76 0.00 17.94 1.93 3.33 15.84 29.57 37.50 72. 22.44 29.93 56.75 52.49 64.80 88.55 11.51 18.01 44.22 19.78 25.60 49.22 400 0.73 0.00 8. 1.46 3.33 20.01 25.16 35.00 77.70 19.53 27.70 56.96 48.65 63.20 88.69 10.83 18.38 44.18 17.73 24.60 49. 22 900 1.38 3.33 22.93 3.72 6.67 23.58 30.47 42.50 73.18 23.58 29.78 57. 51.11 64.00 89.22 12.24 18.38 41.96 20.42 27.44 51.46 900 0.91 0.00 16.37 2.29 3.33 16. 31.43 45.00 75.16 23.38 31.41 57.94 53.79 64.60 88.98 11.70 16.54 44.80 20.58 26.81 49.92 1.46 3.33 23.70 3.18 3.33 18.33 31.13 45.00 77.07 24.72 31.70 56.66 53.30 65.00 89.02 13.13 18.38 42. 21.25 27.79 51.18 1000 1.28 3.33 20.32 2.29 6.67 15.00 32.68 45.00 74.30 24.35 32.00 57. 54.85 65.40 88.76 11.66 15.81 45.56 21.19 28.04 50.26 Table 22: RL performance of the 10B-A0.5B MoE Model with β = 0; λ = 0; ˆλ = 0.1. AIME24 AIME AMC23 OlympiadBench MATH-500 Minerva Average # RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 100 0.37 0.00 11.45 0.39 0.00 12.50 11.91 25.00 72.28 8.08 16.00 49. 27.41 47.20 84.11 5.54 11.76 40.01 8.95 16.66 44.95 200 0.34 0.00 11.66 0.18 0.00 10. 14.30 35.00 75.78 9.71 20.00 50.50 32.00 51.20 84.77 6.92 15.44 40.64 10.58 20.27 45.70 0.50 0.00 13.77 0.31 0.00 12.83 15.12 27.50 74.15 10.69 20.89 52.65 34.17 53.60 84.79 7.44 15.81 41. 11.37 19.63 46.63 400 0.70 0.00 18.58 0.31 0.00 13.25 17.87 35.00 75.68 12.87 23.70 54. 38.08 55.00 86.88 8.61 16.91 43.33 13.07 21.77 48.65 500 0.78 0.00 26.30 0.60 0.00 19. 19.71 32.50 77.69 14.36 23.26 53.54 40.73 57.40 86.96 9.29 15.44 42.83 14.25 21.43 51.21 0.70 0.00 17.94 0.52 0.00 15.01 20.27 37.50 74.47 14.86 23.26 53.70 41.59 58.00 87.37 9.17 15.81 44. 14.52 22.43 48.80 700 0.76 0.00 19.83 0.55 0.00 23.10 21.72 35.00 75.45 15.96 24.74 54. 43.06 57.80 86.11 9.88 16.54 44.58 15.32 22.35 50.52 800 0.81 0.00 23.30 0.60 0.00 15. 22.68 37.50 70.93 16.31 25.33 55.85 42.97 58.40 87.32 9.82 16.91 44.51 15.53 23.02 49.49 0.78 0.00 26.39 0.55 0.00 14.19 22.87 35.00 80.24 16.76 24.59 54.78 44.00 59.40 87.34 9.92 15.81 44. 15.81 22.47 51.25 Table 23: RL performance of the 10B-A0.5B MoE Model with β = 0; λ = 0.1; ˆλ = 0. AIME24 AIME25 AMC"
        },
        {
            "title": "OlympiadBench",
            "content": "MATH-"
        },
        {
            "title": "Average",
            "content": "# RL Steps Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@ Avg@128 Cons@128 Pass@64 Avg@128 Cons@128 Pass@64 100 0.32 0.00 9.57 0.21 0.00 8.97 12.30 27.50 71. 8.81 15.70 49.50 28.13 48.00 83.25 5.50 8.46 40.19 9.21 16.61 43.81 200 0.50 0.00 10. 0.26 0.00 10.65 14.57 30.00 67.79 11.18 19.11 49.62 33.38 50.40 83.85 6.58 12.13 40.53 11.08 18.61 43. 300 0.65 0.00 9.17 0.34 0.00 14.62 17.58 30.00 73.48 13.36 21.78 50.93 37.19 52.00 86. 7.76 11.40 42.63 12.81 19.20 46.15 500 0.83 0.00 10.84 0.42 0.00 11.14 19.90 30.00 74. 15.89 24.59 52.84 41.51 57.00 85.14 9.08 15.44 44.28 14.61 21.17 46.39 600 0.96 0.00 13. 0.76 0.00 18.23 20.88 37.5 75.33 16.77 25.04 52.59 42.88 57.40 85.43 8.88 15.07 43.39 15.19 22.50 48. 700 0.81 0.00 16.48 0.83 0.00 18.34 20.55 35.00 77.74 17.09 24.89 53.86 43.12 57.20 85. 8.95 13.97 44.42 15.23 21.84 49.42 800 0.68 0.00 10.85 0.99 0.00 16.27 21.35 40.00 71. 16.27 24.44 51.74 43.56 58.40 85.36 7.83 11.03 42.66 15.11 22.31 46.42 900 0.83 0.00 13. 1.17 3.33 17.94 20.63 35.00 78.56 16.30 24.00 51.80 43.53 58.40 86.33 7.89 9.93 42.90 15.06 21.78 48. 400 0.86 0.00 21.66 0.44 0.00 16.81 19.16 32.50 73.13 14.83 23.85 50.98 39.12 53.20 85. 8.31 12.50 43.57 13.79 20.34 48.59 23 1000 1.22 0.00 25.86 0.76 0.00 13. 24.32 40.00 73.19 17.47 25.93 54.95 45.20 59.00 87.70 10.03 18.01 44.25 16.50 23.82 49.87 0.99 0.00 20.77 1.02 3.33 17.51 19.86 32.50 76.32 16.13 24.30 52.61 42.68 58.80 86.73 7.78 11.40 42. 14.74 21.72 49."
        }
    ],
    "affiliations": [
        "Tencent",
        "The Chinese University of Hong Kong"
    ]
}