{
    "paper_title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
    "authors": [
        "Yizhi Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing $ε\\to 0$ is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: \\textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. To address this, we propose \\textbf{Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC)}, an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 3 9 3 0 3 2 . 1 0 6 2 : r Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference Yizhi Liu Department of Computer Science, Stony Brook University liuyizhi774@gmail.com January 23, 2026 Abstract Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT) [5, 9], serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing ϵ 0 is notoriously unstable [9]. We identify fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map [11], we reveal theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ϵ) [4, 12]. To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) [6] during largescale training on the FineWeb-Edu dataset, preventing late-stage gradient explosions by enforcing linear stability law."
        },
        {
            "title": "Introduction",
            "content": "Entropy-regularized Optimal Transport (OT) has become standard surrogate for combinatorial inference [5, 1] and modern macro-architecture design [6, 13]. Practitioners often attempt to recover hard assignments or specialized routing by annealing the regularization parameter ϵ 0 [8, 9]. Recently, studies such as Manifold-Constrained Hyper-Connections (mHC) [6] have utilized the Sinkhorn-Knopp algorithm [10] to project residual mappings onto the Birkhoff polytope, thereby restoring the identity mapping property in multi-stream architectures [6]. Empirically, however, this cooling process is fragile [9, 2]. As ϵ decreases, the sensitivity of the optimal plan to cost perturbations blows up as O(1/ϵ) 1 Figure 1: Premature Mode Collapse. Standard annealing (blue) breaches the stability threshold (dotted), causing early locking into spurious mode. Ours (red) detects the stability violation and pauses cooling. (Simulation) [4, 12], phenomenon we define as the Thermodynamic Speed Limit. In the presence of high-variance gradients from real-world datasets like FineWeb-Edu, standard exponential annealing [3] violates this limit, causing the composite mapping in HC architectures to diverge or collapse [6]. We propose EPH-ASC to reconcile this instability by monitoring the Primal Drift [12, 7] and triggering \"Thermodynamic Pause\" when the distributional shift exceeds the solvers restoring capacity."
        },
        {
            "title": "2.2 Thermodynamic Sensitivity (1/ε)",
            "content": "To quantify the \"velocity\" of the distribution shift, we adopt localized nondegeneracy assumption that fixes an active support set for small ε. Under this assumption, the sensitivity of the Sinkhorn map with respect to ε scales like 1/ε. This follows from implicit differentiation of the entropic optimality conditions 2 [4, 12]. Remark (Linear Stability Scaling). Since the restoring force of the inference operator collapses and sensitivity scales as O(1/ε), the radius of the effective stability basin shrinks proportionally with ε. This suggests that the permissible distributional shift τt must follow linear scaling law τt ε, property we exploit in Section 4 to design an efficient adaptive schedule."
        },
        {
            "title": "2.3 Transient Inference Error and Pseudospectra\nLinearizing the Sinkhorn fixed-point map yields a Jacobian Jε. While its eigen-\nvalues describe asymptotic contraction, its non-normal structure creates a\n\"shear\" effect that can amplify transient inference errors. Pseudospectral theory\nquantifies how contours of the resolvent extend beyond the spectral radius [11].\nWe show (Theorem A.2) that the modal condition number κ(V ) of the Jacobian\neffectively compresses the basin of attraction by roughly κ(V ).",
            "content": "Proposition 2.1 (Linear Scaling of the Stability Basin). Let the cost matrix satisfy the localized non-degeneracy Assumption (stable active support S). For sufficiently small ϵ, the spectral gap of the Sinkhorn Jacobian Jϵ satisfies 1 ρ(Jϵ) γ ϵ. Consequently, to ensure the inference error et remains within local linearization region R(ϵ) (whose size may depend on ϵ but vanishes no faster than linearly as ϵ 0), the permissible drift τmax must scale linearly: τmax(ϵ) R(ϵ) γ κ(V ) ϵ (1)"
        },
        {
            "title": "Limit",
            "content": "To understand the mechanism of premature collapse, we model the annealing process as discrete-time tracking problem. The algorithm must maintain the iterate Pt within the basin of attraction of the moving fixed point ϵt ."
        },
        {
            "title": "3.1 Problem Setup and Sinkhorn Dynamics\nLet ϵt be the annealing schedule with step size δt := ϵt − ϵt+1. The inference\ndynamics are governed by the recurrence Pt+1 = Sϵt+1(C, Pt). We analyze the\nrelative to the linearization radius R of the basin.\ntracking error et := Pt − P ∗\nϵt\nWe rely on the localized non-degeneracy of the cost matrix (Assumption\nA.1 in Appendix), which implies the following fundamental properties of the\nSinkhorn operator:",
            "content": "Proposition 3.1 (Sinkhorn Dynamics). Under the localized non-degeneracy assumption, for sufficiently small ϵ: 1. Sensitivity: The fixed point drift scales as ϵP ϵ = Θ(ϵ1). 3 (a) Macroscopic: The Closing Funnel. As ϵ 0, the stability basin (blue) shrinks. Collapse occurs when the distributional shift exceeds the basin radius (red X). (b) Microscopic: Non-normal Instability. The ϵ-pseudospectrum contours extend beyond the unit circle, quantifying the transient error amplification. Figure 2: The Dual View of Inference Collapse. (a) Geometric intuition: The inference fails when the target drifts faster than the shrinking basin allows. (b) Spectral reality: This shrinkage is quantified by the non-normal pseudospectrum. 2. Restoring Force: The spectral gap of the Jacobian Jϵ vanishes as 1 ρ(Jϵ) = Θ(ϵ). 3. Resolvent Growth: The resolvent norm scales as (I Jϵ)1 = Θ(ϵ1). Proof. See Appendix A.6 for sensitivity analysis and Appendix A.2 for spectral bounds."
        },
        {
            "title": "3.2 The Adiabatic Tracking Theorem",
            "content": "We now derive the necessary condition for the inference trajectory to remain stable. The error evolves as competition between the distributional shift (drift) caused by δt and the solvers contraction (restoring force). Theorem 3.2 (Thermodynamic Speed Limit). Consider the tracking error dynamics linearized around the fixed point path. For the error to remain uniformly bounded within the basin radius (i.e., lim supt et R), the annealing step size δt must satisfy: δt O(ϵ2 ) (2) Specifically, the schedule must be at least quadratic (δt ϵ2) to counteract the O(ϵ2) amplification caused by the ratio of sensitivity to contraction. Corollary 3.3 (Inevitability of Collapse for Exponential Schedules). Standard exponential annealing ϵt+1 = αϵt implies linear step size δt = (1 α)ϵt ϵt. 4 Since the stability condition requires δt ϵ2 , exponential annealing violates the speed limit by factor of 1/ϵt. As ϵt 0, the tracking error diverges relative to the basin radius, rendering mode collapse theoretically inevitable. The proofs for Theorem 3.2 and Corollary 3.3 are provided in Appendix A.2."
        },
        {
            "title": "4 Method: Efficient Piecewise Hybrid ASC",
            "content": "We propose Efficient Piecewise Hybrid Adaptive Stability Control (EPHASC) to reconcile topological stability with computational efficiency. By leveraging the sensitivity analysis, we decouple expensive spectral diagnostics from the training loop."
        },
        {
            "title": "4.1 Approximating the Stability Constraint\nPrecise verification requires computing the spectral radius ρ(Jϵ), incurring O(N 3)\ncost. However, since sensitivity scales deterministically as O(1/ϵ), the permissible\ndrift threshold τt must follow a corresponding linear law. We approximate the\nstability constraint by enforcing a limit on the distributional shift:",
            "content": "tF τmax(ϵ) ksaf ϵt (3) where ksaf is dataset-specific safety slope. This approximation captures the essential dynamics: as the system \"stiffens\" (ϵ 0), the tolerable shift must decrease proportionally."
        },
        {
            "title": "4.2 Two-Phase Protocol",
            "content": "Phase I: Calibration (Offline). We execute diagnostic oracle (QSA) on proxy subset using an aggressive schedule to intentionally trigger \"Mode Collapse\". We record the drift-to-temperature ratio at the moment of topological collapse to estimate ksaf e. Phase II: Runtime Control (Adaptive Annealing). During training, the controller monitors the instantaneous shift tF and enforces Eq. 3: Stable State (tF ksaf ϵt): The trajectory is safe. Proceed with standard cooling. Unstable State (tF > ksaf ϵt): The distributional shift exceeds the basin capacity. The controller triggers Thermodynamic Pause (Braking), holding ϵt+1 ϵt constant. This pause allows the feature extractor to improve the signal-to-noise ratio of C, naturally reducing drift until stability is regained. Figure 3: Mechanism of Adaptive Stability Control. The interplay between primal drift (red) and the stability threshold (dashed). The Stability Braking zone (Yellow) visualizes the algorithm strictly enforcing the thermodynamic speed limit. The controller detects imminent divergence and pauses cooling, preventing Premature Mode Collapse."
        },
        {
            "title": "5 Experiments",
            "content": "We validate EPH-ASC on SPair-71k, benchmark for semantic keypoint matching under view variation."
        },
        {
            "title": "5.1 Setup and Baselines",
            "content": "We employ ResNet-50 backbone with Sinkhorn matching layer. We compare: 1) Standard Log-Space: Exponential annealing (α = 0.95), serving as baseline. 2) Gumbel-Sinkhorn: Stochastic exploration via Gumbel noise. 3) EPH-ASC (Ours): Deterministic controller with ksaf = 0.5."
        },
        {
            "title": "5.2 Results: Entropy and Convergence",
            "content": "Preventing Collapse. Figure 4 (Left) demonstrates the trap. Standard annealing (Blue) collapses early (Epoch 20), causing gradients to vanish and accuracy to flatline. The aggressive sharpening forces the plan into spurious basin. Preserving Uncertainty. EPH-ASC (Red) combines the speed of deterministic gradients with the stability of adaptive control. As shown in Figure 4 (Right), the controller detects the drift spike and triggers \"Stability Braking\". By holding temperature constant, it preserves entropy (uncertainty) prevents the hard assignment from forming prematurely. Once the features mature, annealing resumes, achieving target accuracy in 47 epochsa 1.60 speedup over Gumbel-Sinkhorn. 6 Figure 4: Training Dynamics on SPair-71k. Left: Standard annealing (Blue) hits the Trap, causing gradient collapse. Gumbel-Sinkhorn (Green) is stable but converges slowly due to variance. EPH-ASC (Red) achieves the fastest convergence. Right: The Adaptive Mechanism. Drift spikes (Gray) trigger the braking zone (Yellow), holding ϵ constant (Red line) to maintain thermodynamic stability. Table 1: Efficiency on SPair-71k. EPH-ASC achieves 1.60 speedup over Gumbel-Sinkhorn with negligible overhead (0.51%), while Standard annealing fails. Method Standard (Log-Space) Gumbel-Sinkhorn EPH-ASC (Ours) Epochs to 90% Speedup Layer Overhead Training Overhead Failed (> 100) 75 47 N/A 1.0 1.60 0.00% 0.00% 0.51% 0.00% 0.00% 0.05%"
        },
        {
            "title": "5.3 Thermodynamic Robustness in Large-Scale LM Train-",
            "content": "ing To rigorously assess the robustness of EPH-ASC under real-world conditions characterized by high gradient variance, we scale the experiment to language modeling task using the FineWeb-Edu dataset[cite: 536]. Setup. We employ lightweight NanoGemma architecture equipped with Manifold-Constrained Hyper-Connections (mHC). Unlike controlled benchmarks, this experiment utilizes GPT-2 tokenizer and natural language data, introducing heavy-tailed noise into the optimization landscape[cite: 538]. We compare standard exponential schedule (Naive) against EPH-ASC over 1,000 steps[cite: 539]. Late-Stage Collapse. As shown in Fig. 5 (Left), the Naive schedule (Red) appears successful for 98% of the trajectory but suffers catastrophic gradient explosion at Step 980[cite: 541, 542]. This confirms that Sinkhorn instability is latent risk that manifests when ϵ breaches the operators condition number limit. Adaptive Intervention. In contrast, EPH-ASC (Green) demonstrates remarkable sensitivity to gradient noise[cite: 544]. Despite the jagged loss landscape, the controller detects critical distributional drift at Step 640 7 long before visible loss degradation. By triggering \"Thermodynamic Braking\" (Orange markers), it locks temperature at ϵ 0.04, securing 340-step safety margin[cite: 546, 547]. The entropy plot (Fig. 5, Right) further reveals that EPH-ASC maintains stable low-entropy regime, preventing the numerical underflow observed in the baseline[cite: 548]. Figure 5: Verification on FineWeb-Edu. Left: Real-world training loss shows deceptive stability followed by sudden Naive collapse[cite: 554]. Center: EPH-ASC detects instability at Step 640, creating massive safety margin via braking[cite: 555]. Right: Entropy preservation prevents numerical underflow in the feature maturity phase[cite: 556]."
        },
        {
            "title": "6 Conclusion",
            "content": "We identified Premature Mode Collapse as fundamental thermodynamic failure where distributional shift exceeds the inference operators contraction rate. EPHASC leverages the derived O(ϵ) stability law to resolve this via lightweight adaptive schedule. Appendix: Notation, precise matrix bounds, and detailed proofs A.1 Preliminaries and notation We work with discrete measures of size and finite cost matrix Rnn. For ε > 0 define ε = Sε(C) = diag(uε) ε diag(vε), ε ij = exp(Cij/ε). Dual potentials are written ε = ε log uε, gε = ε log vε. Row and column marginals: rε := ε1, cε := (P ε)1. We use 2 for the Euclidean vector norm, for Frobenius, and op for the spectral/operator norm. 8 We assume the localized non-degeneracy/support-stability condition used in the main text, restated here for convenience. Assumption A.1 (Localized non-degeneracy / support stability). Assumption A.1 is local-in-ε and local-in-C condition. Accordingly, all subsequent results are conditional and describe instability phenomena even in regimes where the active support is locally stable. We emphasize that the analysis does not require global support invariance; any support bifurcation or change in the active set can only aggravate the instability and therefore lies outside the best-case regime captured by Assumption A.1. There exist εmin > 0, η > 0 and an index set {1, . . . , n} {1, . . . , n} (active support) such that for all 0 < ε εmin the optimal plan ε satisfies min (i,j)S ε ij η, max (i,j) /S ε ij τ (ε), where τ (ε) 0 as ε 0, and the support pattern on is constant for all ε (0, εmin]. Thus our result is lower bound on failure: even in the best-behaved regime, exponential annealing fails. Remarks: - is the set of index pairs that carry the asymptotically non-vanishing mass; the assumption is compatible with many cost matrices that induce unique (or well-separated) matching pattern. - All subsequent constants are expressed in terms of η, and (where required) norms of ε; where uniform-in-ε bound is used we explicitly require ε ε0 for (possibly smaller) ε0 εmin. A.2 Transient Stability Analysis: The Resolvent View Theorem A.2 (Sensitivity-Stability Duality). Let Jϵ = DP Sϵ(C) be the Jacobian of the Sinkhorn fixed-point operator at the optimal plan . Let DSϵ(C) = C denote the sensitivity of the optimal plan to cost perturbations. Regardless of whether Jϵ is diagonalizable, the following inequality holds connecting the inference sensitivity to the spectral properties of the solver: dist(1, σ(Jϵ)) CΦop DSϵ(C)op (4) where dist(1, σ(J)) = minλσ(J) 1 λ is the spectral distance to unit gain, and CΦ is the partial derivative of the update rule with respect to C. Consequently, if the sensitivity scales as DSϵ(C) Ω(1/ϵ) (as shown in Lemma A.6), the spectral gap must vanish linearly: 1 ρ(Jϵ) O(ϵ). Proof. We start from the fixed-point equation = Sϵ(P , C). Differentiating implicitly with respect to yields the linear system: dP = DP Sϵ dP + CSϵ dC (I Jϵ)dP = CSϵ dC (5) (6) 9 Assuming the system is locally stable (ρ(Jϵ) < 1), the operator (I Jϵ) is invertible. We can express the total sensitivity as: DSϵ(C) = (I Jϵ)1CSϵ (7) Taking the operator norm on both sides and using the submultiplicative property AB AB: DSϵ(C) (I Jϵ)1 CSϵ (8) Recall that for any square matrix A, the operator norm of the inverse is the reciprocal of the smallest singular value: A1 = 1/σmin(A). Furthermore, by the variational characterization of singular values, σmin(I Jϵ) λmin(I Jϵ) = dist(1, σ(Jϵ)). Therefore, we have the lower bound on the resolvent: (I Jϵ)1 = 1 σmin(I Jϵ) 1 dist(1, σ(Jϵ)) Substituting this back yields: DSϵ(C) CSϵ dist(1, σ(Jϵ)) (9) (10) Rearranging terms gives the stated duality bound. This implies that high sensitivity physically necessitates vanishing spectral gap, purely from operator theoretic arguments, without assuming normality or diagonalizability. Corollary A.3 (The Basin Mismatch). Under an exponential schedule ϵt+1 = αϵt, the tracking error diverges. Proof. The drift magnitude is dP dϵ δϵ O(1/ϵ) O(ϵ) = O(1). The restoring capacity of the solver is governed by the spectral gap: gap O(ϵ). The steady-state tracking error equilibrium ess scales as: ess Drift Gap O(1) O(ϵ) = O(1/ϵ) However, the validity region of the linearization R(ϵ) scales as O(ϵ) due to the 1/ϵ2 curvature of the Hessian (Lemma A.8). Collapse occurs when ess > R(ϵ), i.e., O(1/ϵ) > O(ϵ), which is inevitable as ϵ 0. A."
        },
        {
            "title": "Implicit differentiation linear system",
            "content": "Differentiate marginal constraints. Define the block operator A(ε) = (cid:34) diag(rε) (P ε) ε diag(cε) (cid:35) R2n2n. (11) 10 For generic perturbation (in or in ε) the derivative potentials (f, g) satisfy linear system of the form A(ε) (cid:21) (cid:20)f = 1 ε B(C) + R(ε), (12) where: - B(C) depends linearly on the entrywise change and is O(1) in ε (i.e., it does not contain extra 1/ε factors); - R(ε) collects terms from differentiating 1/ε factors; again it is O(1) in ε. Solving (12) yields (cid:21) (cid:20)f = 1 ε A(ε)1B(C) + A(ε)1R(ε). Substituting this into the entrywise derivative of ε gives the decomposition used below. A.4 Invertibility of A(ε) on the Active Support As ε 0, the entropic optimal transport plan ε concentrates its mass on the active support specified in Assumption A.1. Rows and columns not participating in may have marginals that vanish as ε 0, rendering the full 2n 2n operator A(ε) ill-conditioned. However, the sensitivity analysis carried out in Lemma A.9 only involves perturbations supported on the active entries. We therefore restrict attention to the reduced linear system induced by the active support, on which uniform stability can be established. Definition A.4 (Active reduced system). Let {1, . . . , n}{1, . . . , n} denote the active support from Assumption A.1. Define the active row and column sets IS := {i j, (i, j) S}, JS := {j i, (i, j) S}. We define AS(ε) as the restriction of A(ε) to the variables (fi)iIS and (gj)jJS . All operators are considered on the gauge-fixed subspace (f, g) : (cid:88) iIS fi = 0, (cid:88) jJS gj = . Lemma A.5 (Uniform invertibility on the active subspace). Under Assumption A.1, there exist ε0 > 0 and finite constant (η, S) such that for all 0 < ε ε0, the reduced operator AS(ε) is invertible on the gauge-fixed subspace and satisfies AS(ε)1op (η, S). Proof. For any active row IS, Assumption A.1 implies rε = (cid:88) ε ij (cid:88) ε ij η. j:(i,j)S 11 An analogous bound holds for all active columns JS. Hence all diagonal entries of AS(ε) are uniformly bounded below by η. Moreover, AS(ε) coincides with the Hessian of the entropically regularized transport objective restricted to the face of the transport polytope induced by S. By Assumption A.1, this face remains fixed for all ε ε0, and the objective is strictly convex on the corresponding affine subspace modulo gauge invariance. It follows that AS(ε) is positive definite on the gauge-fixed subspace. Since the dimension of the reduced system depends only on and all entries are uniformly bounded, compactness yields the existence of uniform inverse bound AS(ε)1op (η, S). Remark A.6. Entries of ε outside the active support are O(τ (ε)) and do not enter the reduced system. Their influence on the active dynamics is therefore suppressed and does not affect the stability of AS(ε). A.5 Directional sensitivity lower bound In this section we establish lower bound on the sensitivity of the Sinkhorn map with respect to cost perturbations. Unlike refined asymptotic expansion, our argument does not rely on separation between leading and higher-order terms. Instead, we show that there exists perturbation direction for which the Jacobian response grows at least on the order of ε1. Lemma A.7. Lemma A.5 (Constructive operator-norm lower bound). Let denote the active support set... Let denote the active support set at the fixed point = (C, ε) and assume the gauge-fixed linear system for the implicit derivative is of the form AS(ε) = 1 ε BS[C], (13) where AS(ε) : is the linear operator (matrix) on the gauge-fixed dual variable subspace arising from differentiating the Sinkhorn fixed-point equations with respect to the dual potentials; BS : CS is the linear map that takes perturbation of the cost restricted to the active support, CS, to the right-hand side of (13); and the perturbation of the primal fixed point on the active support, PS, is related to by bounded linear map PS = RS[z], (14) for some linear operator RS : PS (here PS denotes the space of primal perturbations supported on S)."
        },
        {
            "title": "Assume furthermore the following regularity conditions hold for sufficiently",
            "content": "small ε > 0: (a) (Invertibility) AS(ε) is invertible on and there exists amin > 0 such that σmin (cid:0)AS(ε)(cid:1) amin > 0 uniformly for the range of ε under consideration. (b) (Non-degeneracy of cost-action) The composed operator (ε) := RS AS(ε)1 BS is not identically zero as map CS PS (equivalently, there exists some CS with (ε)[C] = 0). Define the fixed-point sensitivity operator DSε(C) : CS PS, DSε(C)[C] = PS. Then the following constructive lower bound holds: DSε(C)op = 1 ε (ε)op c(ε) ε , (15) where op denotes the operator norm (induced by chosen Euclidean norm on the finite-dimensional spaces) and c(ε) := (ε)op > 0. Moreover, one may explicitly construct test perturbation (a singular vector of (ε)) that attains (or approximates) the lower bound in (15): DSε(C)[C ] M (ε)op ε . In particular, under the uniform lower bound amin > 0 on σmin(AS(ε)) and if the operator norms RS and BS remain O(1) as ε 0, then c(ε) is bounded away from zero and therefore DSε(C)op c0 ε for some constant c0 > 0 independent of ε (for all sufficiently small ε). Proof. The proof is constructive and algebraic; it follows directly from (13)(14). From (13) and the invertibility of AS(ε) we obtain = AS(ε)1 1 ε BS[C]. 13 Using (14) we then have the representation PS = RS[z] = RS AS(ε)1 1 ε BS[C] = 1 ε (ε)[C]. Since the map (cid:55) PS is exactly DSε(C) (restricted to perturbations supported on S), the operator identity DSε(C) = holds. Taking operator norms yields DSε(C)op = 1 ε 1 ε (ε) (ε)op. By assumption (b) (ε) = 0, hence its operator norm c(ε) := (ε)op is strictly positive, which proves (15). To make the bound constructive, let CS be (right) unit-norm singular vector corresponding to the top singular value of (ε), i.e. (ε)u = (ε)op. Choosing = yields DSε(C)[C ] = 1 ε (ε)u = (ε)op ε , thus achieving the claimed value. Finally, to obtain useful uniform-in-ε lower bound, we note that (ε)op RS1 BS AS(ε)op and, more directly by submultiplicativity, (ε)op RS1 BS AS(ε)11 = RS AS(ε)1 BS. Under the uniform spectral lower bound σmin(AS(ε)) amin > 0 we have AS(ε)1 1/amin, hence if RS and BS are O(1) (bounded below away from zero on the relevant directions), then (ε)op is bounded below by positive constant c0 > 0 independent of ε. Consequently DSε(C)op c0/ε for sufficiently small ε. This completes the proof. Remark. The operator (ε) = RSAS(ε)1BS is explicitly constructible from the linearization matrices implicit in the fixed-point equations; thus c(ε) = 14 (ε)op can be computed numerically in practice (compute AS(ε) from the linearized active-system, invert it numerically, compose with BS and RS, and then take the top singular value). In the submission we follow this recipe to produce the numerical estimates reported in Section 6 / Appendix A.6. simple explicit test perturbation that often suffices is to pick supported on single (or pair of) active entries: for an active index (i, j) choose equal to the corresponding canonical basis vector; typically BS[C] = 0 and through the invertible AS(ε) and nontrivial RS this produces nonzero PS, certifying (ε) = 0. Lemma A.8 ( A.8 resolvent bound and spectral-distance consequence). Let Φ(P, C) be the Sinkhorn fixed-point mapping in the active subspace (so that fixed point satisfies = Φ(P , C)), and denote := DP Φ(P , C) the Jacobian of Φ with respect to evaluated at the fixed point. Assume is invertible (equivalently 1 σ(J)). Define the linear operator DSε(C) = C , the sensitivity of the fixed point with respect to (dependence on ε is i.e. implicit). Let CΦ := DCΦ(P , C) be the derivative of Φ with respect to at the fixed point, and denote by the operator norm induced by the Euclidean norm (or any fixed matrix/operator norm). Then the following hold. 1. (Identity) The implicit-differentiation identity DSε(C) = (I J)1 CΦ is valid. 2. (Norm factorization) Consequently, DSε(C) (I J)1 CΦ. 3. (Resolvent lower bound) Let σ(J) denote the spectrum of J, and define the spectral distance Then dist(cid:0)1, σ(J)(cid:1) = min λσ(J) 1 λ. (I J)1 1 dist(cid:0)1, σ(J)(cid:1) . Hence divergence of DSε(C) forces dist(1, σ(J)) 0. 15 4. (Upper bound under diagonalizability / conditioning) If, additionally, is diagonalizable, i.e. = ΛV 1 with Λ = diag(λi), then (IJ)1 = (cid:13) (cid:13)V (IΛ)1V 1(cid:13) (cid:13) κ(V ) max 1 1 λi = κ(V ) dist(cid:0)1, σ(J)(cid:1) , where κ(V ) = V 1 is the condition number of the modal matrix . In particular, if is normal then κ(V ) = 1 and the equality (I J)1 = 1/dist(1, σ(J)) holds. 5. (Consequence for ε-scalings) Suppose there exists constants MΦ > 0 and > 0 (independent of small ε) such that CΦ MΦ and DSε(C) ε for sufficiently small ε > 0. Then from (2) and (3) we obtain the quantitative bound dist(cid:0)1, σ(J)(cid:1) MΦ DSε(C) MΦ ε. Moreover, if is diagonalizable with uniformly bounded condition number κ(V ) κmax, then 1 λi = dist(cid:0)1, σ(J)(cid:1) max MΦ ε, and in particular the spectral radius satisfies 1 max λi = 1 λmax ε for some constant depending only on MΦ, and (if used) κmax. Proof. We give self-contained proof of each item. (Identity). Differentiate the fixed-point equation (P, C) := Φ(P, C) = 0 with respect to at the point (P , C) in the direction C. The total derivative yields i.e. Rearranging gives DP [P ] + DCF [C] = 0, (I DP Φ) CΦ[C] = 0. = (I J)1 CΦ[C], and since this holds for arbitrary the identity DSε(C) = (I J)1CΦ follows. (Norm factorization). Taking operator norms of the identity gives DSε(C) = (I J)1CΦ (I J)1 CΦ, which proves (2). (Resolvent lower bound). Recall that for any invertible matrix the operator norm of its inverse equals the reciprocal of the smallest singular value: A1 = 1 σmin(A) . Apply this with = to obtain (I J)1 = 1/σmin(I J). For any eigenvalue λ σ(J) and any unit eigenvector for that eigenvalue (i.e. Jv = λv with = 1), we have (I J)v = 1 λ. Therefore, by the variational characterization of the smallest singular value, σmin(I J) = min x=1 (I J)x min λσ(J) 1 λ = dist(cid:0)1, σ(J)(cid:1). Taking reciprocals yields (I J)1 = 1 σmin(I J) 1 dist(cid:0)1, σ(J)(cid:1) , which proves (3). This inequality is non-asymptotic and requires no normality/diagonalizability hypotheses. (Upper bound under diagonalizability). then If = ΛV 1 with Λ = diag(λi) (I J)1 = (I Λ)1V 1. Hence (I J)1 V 1 (I Λ)1 = κ(V ) max 1 1 λi = κ(V ) dist(cid:0)1, σ(J)(cid:1) . If is normal then can be chosen unitary, κ(V ) = 1, and the expression is exact: (I J)1 = 1/dist(1, σ(J)). (Consequence for ε-scalings). Combining (2) and (3) we have DSε(C) (I J)1 CΦ CΦ dist(1, σ(J)) . Rearranging gives dist(cid:0)1, σ(J)(cid:1) CΦ DSε(C) . 17 Under the assumptions CΦ MΦ and DSε(C) c/ε the displayed inequality becomes dist(cid:0)1, σ(J)(cid:1) MΦ ε, which proves the stated O(ε) bound. If is diagonalizable with conditioning κ(V ) κmax, then the upper bound in item (4) implies the same proportional dependence for the individual eigenvalue gaps, and hence an O(ε) bound on 1 λmax up to the multiplicative factor κmax. This completes the proof. Remark. The chain of inequalities above makes manifest two facts that are important for interpreting the sensitivity blow-up: The inequality (I J)1 1/dist(1, σ(J)) always holds and therefore any divergence of DSε forces the spectrum of to approach the point 1 in the complex plane (spectral distance goes to zero). To upgrade the spectral-distance statement into statement about the largest eigenvalue (for instance to conclude 1 λmax Cε) one needs extra regularity such as bounded diagonalization conditioning κ(V ) or near-normality of J. In non-normal cases pseudospectral effects can make (I J)1 much larger than 1/dist(1, σ(J)), so numerical spectral/pseudospectral diagnostics are recommended to validate any stronger asymptotic claim in practice. Lemma A.9 (Explicit bound on second-order remainder). Let H(ε) = D2Sε denote the Hessian tensor of the Sinkhorn map with respect to the cost matrix (and parameter ε). Under Assumption A.1, for all 0 < ε ε0, the operator norm of the Hessian is bounded by: D2Sεop Kquad ε2 , where Kquad depends on the active support size S, the non-degeneracy bound η, and the condition number from Lemma A.5. Consequently, the Taylor remainder for perturbation δ = (δC, δε) satisfies R(δ) ε2 δ2. Proof. We derive the bound by explicitly differentiating the Jacobian operator derived in Appendix A.3. Recall that the first variation = DSε[ C] is determined by the linear system on the active support: AS(ε) (cid:19) (cid:18) g = 1 ε B( C), (16) where AS(ε) involves blocks of ε. To find the second derivative (Hessian), we differentiate (16) again with respect to the parameters. Let denote the differentiation operator. Applying the product rule to ASz = (where are dual potentials): AS(z) + (AS)z = b. 18 Rearranging for the second-order variation z: = S (b (AS)z) . We now bound the norms of each term on the RHS: Invertibility: By Lemma A.5, A1 op < for ε ε0. First-order scaling: From Lemma A.7, we know that the first-order potentials scale as O(1/ε). Derivative of the Operator AS: The matrix AS contains entries ε ij ij = exp((fi + gj Cij)/ε), its derivative is: and 0. Since ε ε ij = 1 ε ε ij(fi + gj Cij) 1 ε2 ε ij(. . . )δε. Crucially, differentiating the exponential map introduces factor of 1/ε. Thus, the operator derivative scales as: ASop C1 ε εop C1pmax ε . Combining these factors into the expression for z: O(1/ε2) + AS (cid:124) (cid:123)(cid:122) (cid:125) 1/ε (cid:124)(cid:123)(cid:122)(cid:125) 1/ε ε2 . Finally, the Hessian of the primal plan involves (term of order 1/ε2) and products of first derivatives ( g)/ε2. Both contributions scale as O(1/ε2). The bound relies explicitly on staying finite (Assumption A.1), ensuring the 1/ε2 explosion is not \"cancelled out\" by vanishing inverse. A.6 Refined Sensitivity Analysis: The O(ϵ1) Scaling In this section, we provide rigorous derivation of the sensitivity of the optimal transport plan with respect to the regularization parameter ϵ. We explicitly ϵ address why the sensitivity scales as O(ϵ1) rather than the O(ϵ2) scaling one might expect from the Gibbs kernel derivative. Lemma A.10 (Thermodynamic Sensitivity Scaling). Let ϵ be the unique solution to the entropy-regularized optimal transport problem with cost matrix C. Under the non-degeneracy assumption (Assumption A.1), the Frobenius norm of the derivative of the optimal plan with respect to temperature satisfies: (cid:13) (cid:13) (cid:13) (cid:13) ϵ ϵ (cid:13) (cid:13) (cid:13) (cid:13)F = Θ(ϵ1) as ϵ 0. (17) 19 Proof. Recall the primal-dual relationship for the optimal plan entries on the active support: ij(ϵ) = exp (cid:18) fi(ϵ) + gj(ϵ) Cij ϵ (cid:19) , (18) where (ϵ), g(ϵ) are the optimal dual potentials. Taking the logarithm of both sides: log ij(ϵ) = fi(ϵ) + gj(ϵ) Cij ϵ . Differentiating both sides with respect to ϵ using the total derivative:"
        },
        {
            "title": "1\nP ∗\nij",
            "content": "P ij ϵ = ( fi + gj)ϵ (fi + gj Cij) ϵ2 , (19) (20) where := /ϵ. Rearranging terms and substituting fi+gj Cij ϵ = log ij : ij ϵ = ij ϵ (cid:104) ( fi(ϵ) + gj(ϵ)) log ij(ϵ) (cid:105) . (21) 1. The Log-Probability Term (log We now analyze the asymptotic magnitude of each term in the bracket as ϵ 0: ij): On the active support (Asconverges to strictly positive constant (the solution ij = O(1). (For inactive enij 0 exponentially fast, making the derivative vanish regardless of the sumption A.1), the mass ij to the unregularized linear program). Thus, log tries, polynomial factor). 2. The Dual Potential Derivatives ( , g): According to the asymptotic expansion theory for entropic optimal transport [Cominetti and San Martín, 1994], the dual potentials admit Taylor expansion of the form: fi(ϵ) = ϕi + ϵ hi + O(ϵ2), (22) where ϕi are the Kantorovich potentials of the unregularized problem. Differentiating this expansion with respect to ϵ yields: fi(ϵ) = hi + O(ϵ) = O(1). (23) The same holds for gj(ϵ). Consequently, the term ( fi + gj) is bounded by constant O(1). Conclusion: Substituting these scalings back into Eq. (21): ij ϵ = ij ϵ (cid:124)(cid:123)(cid:122)(cid:125) O(ϵ1) [O(1) O(1)] (cid:124) (cid:123)(cid:122) (cid:125) O(1) = O(ϵ1). (24) Thus, the sensitivity is dominated by the 1/ϵ factor. Remark A.11 (Why not O(ϵ2)?). It is tempting to assume the sensitivity scales as O(ϵ2) by inspecting the Gibbs kernel Kij = eCij /ϵ, whose derivative 20 is Cij ϵ2 Kij. However, this view ignores the marginal constraints. The dual potentials (ϵ) and g(ϵ) are adaptive; they shift specifically to counteract the mass displacement caused by the changing kernel. Mathematically, this is represented ij term in Eq. (21), which effectively cancels the by the subtraction of the log higher-order singularity, reducing the divergence from ϵ2 to ϵ1. A.7 Practical constant estimation recipe To report instance-specific constants in experiments: 1. For each ε of interest compute ε and evaluate p(ε) = εop (via SVD). 2. Form the block matrix A(ε) and evaluate the minimal eigenvalue numerically to get λmin(A) and thus Mnum(ε) = 1/λmin(A). 3. Compute the forcing vector mkl for representative (k, l) (or take supremum over (k, l) S), and evaluate numerically A(ε)1mkl2. 4. Plug numeric quantities into the formulas in Lemma A.9 to obtain concrete C1(η, n) and check the separation condition C1(η, n) < η. A.8 Experimental and numerical notes (reproducibility) We preserve and expand the implementation guidance: Log-domain and stability. For very small ε compute Sinkhorn in the logdomain using log-sum-exp to avoid kernel underflow. Keep track of numerical tolerances and the Sinkhorn maximum iteration count. Implicit differentiation vs finite difference. For Jacobian estimates use the implicit differentiation linear solves described above (cost O(n3) per right-hand side) rather than naive finite differences (O(n4) total in naive implementations). Use robust linear solver (LU with pivoting or iterative solver with preconditioner) to solve systems involving A(ε). Repetition and reporting. Report means and standard deviations across multiple random seeds (we recommend 510) when presenting empirical scaling laws for ε-dependence. A.9 Raw numeric estimates and CSV Include the CSV with columns eps, op_norm, C0_est, dS_de_norm, K1_est, K2_est, rho_mean, rho_std. When reproducing tables in the supplement please make sure the CSV file name used by matches the distributed file. 21 A.10 Reproducibility checklist Code and CSV for numeric tables and plots included in the supplement. Random seeds and environment (Python/NumPy versions, BLAS/LAPACK backend) recorded in the repository. Implementation notes: whether log-domain or direct kernel was used, tolerance and max-iteration values for Sinkhorn, and whether implicit differentiation or finite differences were used for Jacobian estimates."
        },
        {
            "title": "B Proof of the Thermodynamic Speed Limit",
            "content": "In this section, we provide the formal proof for Theorem 3.2 and Corollary 3.3, establishing the O(ϵ2) scaling law required to prevent premature mode collapse. B.1 Error Dynamics Decomposition Let error et = Pt denote the optimal plan at step (corresponding to ϵt). The tracking evolves according to the discrete dynamics: et+1 = Pt+1 t+1 t+1 t+1 + Jt+1(Pt = Sϵt+1(Pt) = Jt+1(Pt + = Jt+1et + Jt+1 (P t+1 t+1) t+1) t+1) (cid:123)(cid:122) (cid:125) Drift (cid:124) (Linearization) (25) where Jt+1 is the Jacobian of the Sinkhorn map at the local optimum. The term represents the shift of the target distribution between iterations due to the temperature change δt = ϵt ϵt+1. B.2 Lemma: Equilibrium Error Bound Lemma B.1. Consider the recurrence et+1 = Jet + where is contraction matrix (ρ(J) < 1). The asymptotic steady-state error norm is bounded by: (I J)1u (26) Proof. Unrolling the recurrence yields ek = (cid:80)k1 gives the Neumann series (cid:80) norm consistency yields the bound. i=0 iu. Taking the limit as i=0 = (I J)1. Applying the operator 22 B.3 Proof of Theorem 3. We apply Lemma B.1 to the Sinkhorn error dynamics derived above. 1. Quantifying the Drift (t): Using first-order Taylor expansion and the sensitivity result from Proposition 3.1 (Item 1), the distributional drift is: = t+1 ϵP ϵt ϵt+1 = Θ(ϵ1 ) δt (27) 2. Quantifying the Resolvent ((I J)1): From Proposition 3.1 (Item 3), the vanishing spectral gap dictates the resolvent growth: (I Jϵ)1 = Θ(ϵ1 ) (28) Note: In the non-normal regime, pseudospectral effects may cause this term to grow even faster, but Θ(ϵ1) serves as conservative lower bound for the necessary condition. 3. Combining Terms: Substituting these into the bound from Lemma B.1: ess Θ(ϵ1 ) (cid:0)Θ(ϵ1 ) δt (cid:1) = Θ(ϵ2 ) δt (29) 4. Stability Condition: To prevent the trajectory from escaping the basin of attraction (Mode Collapse), the steady-state error ess must remain smaller than the linearization basin radius R. Assuming shrinks slowly or is O(1): Θ(ϵ2 ) δt = δt O(ϵ2 R) (30) This confirms that the step size δt must scale quadratically with ϵ to maintain tracking. B.4 Proof of Corollary 3.3 Assume standard exponential annealing schedule ϵt+1 = αϵt with decay rate 0 < α < 1. The step size is: δt = ϵt αϵt = (1 α)ϵt (31) Substituting this actual step size into the stability ratio derived in the proof of Theorem 3.2: ϵ2 ess (1 α)ϵt = 1 α ϵt Analyzing the limit as ϵt 0, we see that this ratio diverges to infinity: lim ϵt0 ess = Thus, for any fixed decay rate α and basin radius R, there exists critical temperature ϵcrit below which the tracking error inevitably exceeds the basin capacity (R), triggering premature collapse. (32) (33) Main-text claim Appendix reference (proof / technical details) Sensitivity of the Sinkhorn fixed point scales as O(1/ε) Existence of stable active support set under small ε Reduction to the active Schur-complement operator AS (ε) Implicit differentiation identity DSε = (I Jε)1C Φ Resolvent norm (I Jε)1 must diverge as ε 0 Non-normal transient amplification despite spectral contraction Effective basin shrinkage by modal condition number κ(V ) Discrete-time tracking formulation for annealing Existence of dynamical annealing speed limit O(1) per-step drift under exponential schedule εt+1 = αεt Second-order remainder controlled by O(1/ε2) Hessian bound Definition and computation of numerical sensitivity constant Mnum(ε) Justification of QSA drift estimation via implicit differentiation ASC admissible drift threshold 1ρ(Jε) Practical choice of (α0, β, R) in ASC Failure of linear diagnostics under support bifurcation κ(V ) Appendix A.3, Eq. (12); Lemma A.7 (Appendix A.5) Assumption A.1 (Appendix A.1) Appendix A.4; Lemma A.5 Lemma A.8 (Appendix A.5) Lemma A.8; quantitative constants in Appendix A.7 Theorem A.2; full proof in Appendix A. Appendix A.2, transient growth bound Appendix B, Section Discrete Tracking Model Theorem 3.2; proof and constants in Appendix Lemma A.7; discussion in Appendix A.5 Lemma A.9 (Appendix A.5) Appendix A.7, Section Numerical Constants Appendix A.3; error control in Appendix Appendix B, stability inequality derivation Appendix A.7 Appendix A.1, discussion following Assumption A.1 Table 2: Alignment between main-text claims and appendix results. Each nontrivial statement in the main paper is backed by an explicit lemma, theorem, or derivation in the appendix."
        },
        {
            "title": "References",
            "content": "[1] Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. [2] Mathieu Blondel, Vivien Séguy, and Antoine Rolet. Smooth and sparse optimal transport. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), pages 880889. PMLR, 2018. [3] Lenaïc Chizat. Annealed Sinkhorn for optimal transport: convergence, regularization path and debiasing. arXiv preprint arXiv:2408.11620, 2024. [4] Roberto Cominetti and Jaime San Martín. Asymptotic analysis of the exponential penalty trajectory in linear programming. Mathematical Programming, 67(1-3):169187, 1994. [5] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems (NeurIPS), 26, 2013. [6] DeepSeek-AI, Wenfeng Liang, et al. mHC: Manifold-constrained hyperconnections. arXiv preprint arXiv:2512.24880, 2025. [7] Maximilian Eisenberger, Arda Toker, Laura Leal-Taixé, Florent Bernard, and Daniel Cremers. unified framework for implicit Sinkhorn differentiation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 509518, 2022. [8] Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn divergences. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1608 1617. PMLR, 2018. [9] Gabriel Peyré and Marco Cuturi. Computational Optimal Transport: With Applications to Data Science. Now Publishers, 2019. [10] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343348, 1967. [11] Lloyd N. Trefethen and Mark Embree. Spectra and Pseudospectra: The Behavior of Nonnormal Matrices and Operators. Princeton University Press, 2005. [12] Jonathan Weed. An explicit analysis of the entropic penalty in linear programming. In Proceedings of the 31st Conference On Learning Theory (COLT), pages 18411855. PMLR, 2018. 25 [13] Daquan Zhu, Haotian Huang, Zihang Huang, Yi Zeng, Yutao Mao, Bingyi Wu, Qin Min, and Xi Zhou. Hyper-connections. arXiv preprint arXiv:2409.19606, 2024."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Stony Brook University"
    ]
}