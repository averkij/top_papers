{
    "paper_title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models",
    "authors": [
        "Yeongtak Oh",
        "Jisoo Mok",
        "Dohyun Chung",
        "Juhyeon Shin",
        "Sangha Park",
        "Johan Barthelemy",
        "Sungroh Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 6 3 8 1 . 6 0 5 2 : r RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models Yeongtak Oh1, Jisoo Mok1, Dohyun Chung2, Juhyeon Shin3, Sangha Park1, Johan Barthelemy4, Sungroh Yoon1,3, 1Department of Electrical and Computer Engineering, Seoul National University 2Department of Future Automotive Mobility, Seoul National University 3Interdisciplinary Program in Artificial Intelligence, Seoul National University 4NVIDIA"
        },
        {
            "title": "Abstract",
            "content": "Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with largescale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multiconcept image captioning task. https://github.com/oyt9306/RePIC"
        },
        {
            "title": "Introduction",
            "content": "The emergence of Large Language Models (LLMs) has greatly propelled the advancement of AI, particularly in natural language understanding and generation [1, 44, 11, 4]. These models demonstrate impressive general knowledge and achieve strong performance across wide range of tasks [15, 45]. This progress inspired the development of Multimodal Large Language Models (MLLMs) [26, 22, 46, 3]. MLLMs integrate visual inputs using pretrained vision encoders, treating image embeddings similarly to text tokens within unified architecture. This integration [26, 25, 3] extends the utility of LLMs to vision-language tasks, enabling image-grounded dialogue [22] and captioning [22, 25, 3]. Unfortunately, despite being pre-trained on large-scale datasets, MLLMs struggle to perform personalization by recognizing and incorporating personal, user-specific concepts, typically provided in the form of reference image of the user and an associated textual description [33, 13]. Figure 1 illustrates MLLMs failure in personalized image captioning, one of the most widely-studied tasks in MLLM personalization. Given the reference image of thao and the corresponding description about her, i.e., 23-year-old woman who adores her beloved dog, Bo., an MLLM is prompted to provide personalized caption for new query image of thao that significantly differs from the reference image in lighting, background conditions, and poses; successfully personalized caption should accurately refer to thao and include faithful details about the query image. However, *Corresponding author, sryoon@snu.ac.kr Preprint. Under review. Figure 1: Visualizations of personalized image captioning results. In the first row, the zero-shot MLLM frequently fails to generate personalized captions. The used images are sourced from YoLLava [32]. The remaining rows illustrate multi-concept scenarios at inference time. Compared to other SFTbased methods, our approach consistently produces faithful and detailed captions while accurately recognizing all provided identities, even for 3 or 4 concepts. All images are sourced from MuDI [19]. Qwen-2.5 VL 7B [3], one of the most performant open-source MLLMs, fails to recognize thao and include her personal concepts in its caption. Thus, recent works [2, 32, 33, 13] have increasingly focused on the personalization of MLLMs. Existing approaches for MLLM personalization can be categorized into two groups: those that require retraining as new personal concepts are introduced [2, 32], and those that do not [33, 13]. In real-world applications, where the types of personal concepts are unpredictable and new ones continuously emerge, the former family of personalization approaches is inherently highly limited. The latter cases [33, 13], which enable MLLM personalization without the need for retraining when new concepts appear, fine-tune the LLM on large-scale label-annotated datasets composed of question-answering pairs using Supervised Fine-Tuning (SFT). As result, the post-tuned MLLM can recognize corresponding personal concepts between reference and query images and generate outputs including the given personal information at inference time. To curate training data composed of image-caption pairs, previous SFT-based approaches [33, 13] have relied on proprietary MLLMs such as GPT-4o [18] and Gemini [43] to generate large-scale personal captions. For instance, PVIT [33] used GPT-4o to create captions and manually validated them for individual human images. Similarly, RAP-MLLM [13] curated captions using Gemini. However, even after post-training with large-scale captions, we observe that existing SFT-based post-tuned MLLMs still often struggle to generate faithful personal captions for the query image. As shown in Figure 1, these difficulties become more pronounced in real-world scenarios, such as involving 3 or 4 concepts. Following our investigation, the SFT-based method [13] underperforms primarily due to the scarcity of captions in training data for multi-concept settings (i.e., only 5.4% of the total dataset). However, considering the fact that the performance of SFT is highly sensitive to the quality of the training data [5, 40, 27], obtaining large volume of high-quality personal captions for SFT is both costly and challenging. This challenge becomes particularly severe when curating captions for images containing multiple distinct identities, as each caption must accurately incorporate detailed personal information corresponding to every identity represented. 2 To overcome these difficulties, we investigate the key capabilities that MLLM should possess for personalized image captioning: (1) robust visual recognition ability: the ability to consistently identify the same object across different images, even under variations in pose, location, lighting, and background. This ability induces the MLLM to describe the query image accurately and faithfully; and (2) consistent personalized generation ability: the ability to incorporate personal information from demonstrations into its responses, including correctly referencing the provided names. MLLM equipped with this capability can perform personalized image captioning. To strengthen the abovementioned key capabilities, we propose Reinforced post-training for Personalized Image Captioing (RePIC) framework. We leverage the strengths of reinforcement learning (RL) for method composed of three key components, as outlined below: Object Consistency: To strengthen the MLLMs recognition abilities, we propose an object consistency reward that provides direct positive and negative feedback for the output. Visual Localization: To further reinforce the MLLMs visual recognition ability, we exploit the visual localization reward that predicts bounding box (BBox) coordinates based on query instruction. Identity Consistency: To enhance the MLLMs ability to generate personalized responses, we introduce an identity consistency reward that explicitly encourages the inclusion of target names in the output. In our experimental results, we reveal that SFT-based personalization methods are highly limited for visual recognition and generalization abilities. Conversely, by integrating our proposed reward templates along with curated datasets and instructions, our method achieves significant performance improvements over existing baselines, particularly in multi-concept personalized image captioning benchmarks. To the best of our knowledge, this is the first work to present an RL-based post-training framework that enables MLLMs to perform personalized image captioning effectively."
        },
        {
            "title": "2 Related Works",
            "content": "MLLM Personalization To enable general-purpose MLLMs [26, 26, 3] to perform personalized image captioning, several methods have been proposed, including those requiring retraining when new personal concepts emerge [2, 32] and those that do not [33, 13]. In the former case, MyVLM [2] uses external concept heads to identify user-specific concepts and learns embeddings for each to input into the LLM. YoLLaVA [32] encodes personal concepts as special textual tokens that serve as concept identifiers. However, these methods lack scalability to new concepts, as they require retraining the concept identifiers whenever new concept emerges and do not guarantee sufficient training data per concept. To overcome these limitations, SFT-based post-training approaches have emerged. PVIT [33] uses special prefixes to encode individual-specific information, enabling MLLMs to answer queries about new individuals. RAP-MLLM [13] presents pipeline combining retrieval for visual demonstrations and post-training for personalized generation based on the query and retrieved concept info. Compared to existing SFT-based approaches, we propose an RL-based post-training method that reduces reliance on large-scale, high-quality personal captions and demonstrates superior effectiveness, particularly in multi-concept personalized image captioning. RL-based MLLM Post-Training Methods. RL has demonstrated substantial improvements in several tasks of LLMs [12, 38, 49, 31, 9]. Recent studies have extended RL-based post-training to MLLMsapplying preference-based RL for hallucination mitigation [50] and model alignment [23], and policy-based RL for visual reasoning [29, 17, 41]. For example, regarding policy-based RL, Visual-RFT [29] proposes reward templates to improve performance on visual perception tasks such as fine-grained image classification and few-shot object detection. Vision-R1 [17] combines cold-start initialization with RL training to enhance MLLM reasoning capabilities, particularly in tasks like bounding box prediction and few-shot classification. Reason-RFT [41] introduces two-phase RL framework that integrates SFT-based and RL-based methods for visual reasoning. In contrast to these approaches, we leverage policy-based RL to enhance personalized image captioning in MLLMs."
        },
        {
            "title": "3 Method",
            "content": "In this section, we detail the GRPO [12] algorithm employed in our approach. To enhance both visual recognition and personalized generation capabilities, we introduce an RL-based post-training 3 Figure 2: Overview of our RePIC framework: (a) training phase and (b) inference phase. An abbreviated example of the prompt template is shown; complete templates are provided in the Appendix. framework for MLLMs. Specifically, we outline the design of verifiable rewards and the structuring of instruction and data templates. Figure 2 provides an overview of our proposed RePIC framework during both training and inference stages. 3.1 Preliminary: Group Relative Policy Optimization (GRPO) GRPO [38] is an improved RL algorithm of PPO [36] that exploits the rewards and introduces group-based learning based on relative preferences. Specifically, as shown in Eq. (1), it optimizes clipped surrogate objective, similar to PPO, while simultaneously minimizing the KL divergence between the current and reference policies. For each state, reference policy πθref samples group of responses {oi}G i=1, for the given question-answer pair (q, a). Then, their rewards are normalized based on relative quality within the group as follows: i=1 and their corresponding rewards {Ri}G LGRPO(θ) = 1 (cid:88) i=1 {oi}G i=1πref (cid:104) min (cid:16) t(θ) ˆAi ri t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:17) (cid:105) βKLDKL(πθπref) t(θ) = πθ(oi,tq,oi,<t) πθref (oi,tq,oi,<t) , ˆAi where ri mean and standard deviation of group-level rewards {Ri}G that controls the strength of the regularization. (1) is the normalized advantage for action ai with µ and σ being the i=1. Here, βKL represents the coefficient We leverage verifiable reward (VR) [12] with GRPO, enabling training without the need for an auxiliary reward model. In the context of personalization, the following sections discuss how the core capabilitiesvisual recognition and personalized generationcan be formulated using VRs. 3.2 Proposed Verifiable Rewards Object Consistency Tuning (OCT) To improve the visual recognition capabilities of MLLMs by providing VR, we construct positive-negative pairs. Positive pairs consist of images containing the same object, while negative pairs include images with different objects. We then use binary-response questions, such as Is <name> present in the second image?, and assign VR of 1 if the model responds yes for positive pair and no for negative pair. Incorrect responses receive VR of 0. Based on these pairs, we design reward template called OCT, as defined in Eq. (2). rOCT = (cid:26)1, 0, if correctly answer with yes or no else (2) We use real datasets such as COCO [24], Objects365 [37], and CelebA [28], from which we crop object regions to serve as reference images. However, as real data often lacks sufficient variation in attributes such as pose and lighting, we additionally incorporate high-quality, visually diverse synthetic images from Subject 200K+ [42]. These synthetic images, generated using diffusion models such as Flux [21], include variations in pose and background while preserving subject identity. Visual Localization Tuning (VLT) To enhance MLLM by strengthening its localization capability, we adopt the IoU-based accuracy reward template introduced in VLM-R1 [39]. In this setup, the Intersection over Union (IoU) score serves as the reward criterion, where VR is assigned as 1 if the predicted BBox aligns with the GT BBox at an IoU threshold greater than 0.5, as described in Eq.(3): rVLT = (cid:26)1, if IoU 0,5 0, otherwise (3) We refer to this reward template as VLT, and by reinforcing VLT, the model is enabled to localize the object within the image and can understand its relative location, such as right, left, or top. Specifically, we use Refcoco/+/g datasets [30, 48] commonly used for the general visual reasoning task of referring expression comprehension (REC). Notably, our empirical findings indicate that removing the REC task from the training set often leads to instability during RL-based post-training. Identity Consistency Tuning (ICT) To force the model to consistently utilize the provided information from visual demonstrations in its responses, we consider positive pair composed of few reference images and query image pair. For the reward template of using one reference image per query, we call Single-ICT, and for the reward template of using multiple reference images per query, we call Multi-ICT. In this setting, we assign unique name to each reference image as <name> token. In detail, we prompt the model with response questions such as Describe the query image while referencing the reference images., and assign VR of 1 is only assigned when the model accurately describes the query image using all the given names. Note, in our experiments, we used maximum of 3 reference images per query image. In detail, for single-ICT, we assign VR as follows in Eq. (4): rSingle-ICT = (cid:26)1, if <name> appears in the output 0, otherwise (4) We use the positive pair images used for OCT. Next, for multi-ICT, for given set of m( 3) names and correctly mentioned names, we assign VR for the response as described in Eq. (5): rMulti-ICT = (cid:26)n/m, 0, if <name 1>, <name 2>, , <name n> appears in the output otherwise (5) To construct multiple reference images, we use real multi-object images from COCO [24] and Objects365 [37], and manually curate high-quality examples by cropping two or three distinct objects from single query image. The data templates used for each component are provided in the Appendix. Furthermore, we incorporate descriptive prompts (e.g., describe this image in detail.) that elicit richer language generation in the training dataset. Additionally, we apply output length regularization to ensure that captions exceed minimum length, which helps avoid less preferable responses (e.g., This is <name>.)."
        },
        {
            "title": "4 Experiments",
            "content": "Baseline We compare our method with other post-training-based approaches. As baseline models, we fine-tuned PVIT-LLaVA [33] using 210K subset of the 3M dataset. We also consider pretrained RAP-LLaVA [13], which is fine-tuned using LoRA [16] from LLaVA-1.5 Vicuna 13B [25]. In addition, we fine-tune RAP-LLaVA using only 2K samples randomly selected from the full 210K dataset, matching the amount of seen data used in our method. In our study, we adopt the instruction-tuned Qwen2.5-VL 7B [3] as backbone MLLM. For fair comparison, we also fine-tune it using LoRA on the 210K instruction dataset [13], with data templates adapted for Qwen-VL compatibility. The resulting model, fine-tuned using LLaMA-Factory [52], is referred to as RAPQwen. Hyperparameter sensitivity details are provided in the Appendix. Dataset We consider both single and multi-concept datasets for evaluation. The single-concept data are sourced from YoLLaVA, MyVLM, and DreamBooth. These datasets consist of various 5 Table 1: Single-concept personal grounding performance evaluation results. Models Seen Data PVIT-LLAVA RAP-LLAVA RAP-LLAVA RAP-Qwen Qwen-2.5 VL Ours Retrieval (Top-2) RAP-LLAVA RAP-LLAVA RAP-Qwen Qwen-2.5 VL Ours 210K 210K 2K 210K 0 2K 210K 2K 210K 0 2K Pre. 17.1 100 100 100 100 97.6 95.6 79.2 95.5 91.5 95.3 1.8 92.9 49.4 98.8 56.8 97.1 MyVLM [2] Rec. Pre. F1 Skip-Retrieval Setting 20.1 3.3 100 96.3 50.6 66.1 100 99.4 100 72.4 99.7 98.5 Retrieval Setting 97.0 87.8 64.1 91.6 65.2 92.1 96.5 79.1 53.8 87.9 50.6 89.1 83.6 82.7 71.2 79.2 77.4 83.4 YoLLaVA [32] Rec. F1 2.1 95.5 48.6 99.8 33.3 95.8 82.9 79.9 52.2 75.1 42.3 75.4 3.8 97.7 49.6 99.8 50.0 97.7 83.3 81.2 64.4 76.2 55.2 79.2 DreamBooth [35] F1 Rec. Pre. 26.5 97.3 68.4 100 96.0 100 99.3 96.0 69.5 98.7 95.2 99.3 16.5 91.8 65.8 100 76.6 97.5 96.2 91.1 66.5 94.3 75.3 93.0 20.3 94.5 67.1 100 85.2 98. 97.7 93.5 68.0 96.4 84.1 96.1 Table 2: Multi-concept personal grounding performance evaluation results. Models Seen Data 2-Concepts 4-Concepts RAP-LLaVA RAP-LLaVA RAP-Qwen Qwen-2.5 VL Ours - Full 210K 2K 210K 0 2K Skip-Retrieval Rec. 93.9 90.2 82.9 75.0 98.8 F1 96.9 94.9 90.7 85.7 99.4 Pre. 100 100 100 100 100 Retrieval Rec. 89.6 81.1 73.2 64.0 92. F1 94.5 87.8 84.5 77.5 95.6 Pre. 99.3 95.7 100 98.1 98.7 Skip-Retrieval Rec. 4.3 1.9 13.6 22.9 59.5 Pre. 52.9 36.4 49.6 73.3 88.0 F1 7.9 3.6 21.3 34.8 71.0 Retrieval Rec. 3.1 0.7 2.6 6.4 15. Pre. 16.7 22.4 12.6 22.5 24.8 F1 5.2 1.4 4.3 10.0 19.2 single-concept images with variations in lighting, pose, and background conditions. For multiconcept evaluation, we use the RAP-MLLM [13] dataset, constructed by collecting YouTube videos and extracting frames that include 2 concepts. To evaluate its personalization capabilities in more challenging scenarios, we experiment on images containing 4-concept cases, never seen during training. To this end, we curate dataset by crawling images from movie teasers and award ceremonies where multiple celebrities appear together. We select those for our evaluation dataset in which at least 4 distinct concepts are clearly present. Details on the used datasets are provided in the Appendix. Evaluation We evaluate personalization capabilities under two different settings. First, in the case where GT demonstrations are directly provided at inference time, we refer to this as the skip-retrieval setting. Note that the demonstrations include several reference image-text pairs. Next, we consider the retrieval setting [13] that automates the manual selection of the demonstrations by retrieving the most relevant visual content from database. Additionally, we newly evaluate personal grounding performance in the skip-retrieval setting using reference images that do not match the query. lower score in this case implies that the MLLM does not merely duplicate the given demonstrations while performing personalized image captioning. Implementation Details For the zero-shot model, we apply detailed prompt (e.g., Output the final answer, including its name in the answer.) to guide the model to mention the target identity. Note that in multi-concept settings, in-domain (ID) refers to using 2 concepts, while out-of-domain (OOD) refers to using 4 concepts. In the retrieval setting, we retrieve the top-2 most relevant samples for single and 2-concept settings, and the top-4 for the 4-concept setting. All training experiments are conducted using 8 A40 GPUs, with inference performed on single A40 GPU. Additional details on the retrieval and experimental setup are provided in the Appendix. Figure 3: Visualization of preference evaluation scores for single and 2-concept settings, corresponding to the first and second rows, respectively. In (a), our model outperforms all other baseline models, while in (b), it surpasses all ablation variants. Figure 4: Qualitative examples of 2-concept personalized image captioning. 4.1 Personalized Image Captioning Evaluation 4.1.1 Personal Grouding Performance We evaluate model performance for all settings using the same query evaluation prompts. To quantify how frequently the target <name> or Name or name appears in the models output, we compute precision, recall, and F1-score following the evaluation protocol of [13]. In detail, these scores represent how well the model generates the response while containing the personalized information, without considering the caption quality. We will refer to these scores as the personal grounding performance of the post-tuned MLLM. Here, recall reflects the proportion of correctly mentioned target concepts out of total concept names, while precision indicates the fraction of correct mentions out of all concept names that occurred. In the skip-retrieval setting, precision can be reported as 100%, as the GT demonstrations are provided. For multi-concept settings, we assign score of n/m if the model correctly included target names in the response from given names. As shown in Table 1, in the single-concept setting, the reproduced baseline PVIT [33] shows notably low personal grounding performance, even under the skip-retrieval setting. We conjecture that this poor performance likely stems from the training data, which is largely human-centric. The reproduced RAP-Qwen, trained on 210K samples, achieves the highest scores in the skip-retrieval setting, while our method performs comparably and outperforms RAP-LLaVA. Under the retrieval setting, the top-performing model varies by dataset. Notably, RAP-LLaVA suffers significant performance drop 7 Table 3: Personal grounding with wrong visual demonstrations. Models Seen Data RAP-LLAVA RAP-Qwen Qwen-2.5 VL Ours 210K 210K 0 2K MyVLM [2] () Rec. 89.7 69.7 55.6 54.4 F1 94.6 82.1 72.4 71.5 Pre. 100 100 100 98.9 YoLLaVA [32] () F1 Rec. Pre. 98.3 97.0 99.7 89.4 82.6 98.2 60.4 75.0 99.0 76.1 93.8 64.0 DreamBooth [35] () F1 Rec. Pre. 92.4 90.8 95.2 77.7 71.5 85.0 92.7 88.6 97.2 71.7 63.3 82.6 Table 4: Ablation studies for personal grounding in 2-concept image captioning. Models Seen Data Skip-Retrieval Recall F1 Pre Ours <think> Ours <observe> Ours w/o OCT Ours w/o ICT Ours w/o VLT Ours w/o length reg. Ours w/o detail prompt Ours - Full Reasoning Template Ablations 2K 2K 90.9 80.5 100 100 95.2 89. Reward Template Ablations 99.2 100 100 73.2 17.1 53.6 2K 2K 2K Additional Component Ablations 2K 2K 2K 92.1 86.0 98.8 100 100 84.2 29.2 69.6 95.9 92.5 99.4 Retrieval Recall 84.2 67.7 67.7 14.6 50.0 91.5 74.4 92. F1 90.2 80.9 80.4 25.5 66.7 95.2 84.7 95.6 Pre 97.2 99. 99.1 100 98.9 99.3 98.4 98.7 when trained on only 2K samples, highlighting the data dependency of SFT. For reference, we also report the top-2 retrieval performance as an upper bound, accounting for retrieval noise. In the multi-concept settings, as shown in Table 2, the performance of SFT-based methods drops significantly in both 2 and 4-concept scenarios. In contrast, our proposed method consistently and substantially outperforms all SFT-based baselines under both skip-retrieval and retrieval settings. Notably, the performance gap becomes even more pronounced in the 4-concept setting. Note that the zero-shot Qwen outperforms other SFT-based models in terms of personal grounding across all cases, further highlighting the limitations of SFT-based post-training in generalizing to OOD scenarios. These results underscore the effectiveness of our RL-based post-training approach, particularly in extending it to real-world personalized image captioning tasks. 4.1.2 Preference Evaluation We conduct human-level quality evaluation with GPT-4o [18] to assess the quality of generated personalized image captions. The quality evaluation is conducted as preference-based assessment, where captions that merely duplicate the provided information or fail to accurately describe the image are considered low-preference. The evaluation template is provided in the Appendix. In Figure 3, we present the preference evaluation on the single-concept YoLLaVA dataset and on the RAP-MLLM dataset, corresponding to the first and second rows, respectively. Those results indicate the superiority of our proposed post-tuning method in generating high-quality and faithful personalized captions compared to all other methods. Our proposed method significantly outperforms (a) all other baselines, and (b) all ablation models, including those without length regularization, without detailed prompts in the dataset, and with reasoning templates. To better visualize the effectiveness of our method, we present two qualitative examples in Figure 4. In the first row, although the zero-shot model correctly references the given names, its generated captions are less accurate. SFT-based methods fail to recognize identities consistently, often missing given identities, resulting in unfaithful captions. In the second row, while the zero-shot model produces detailed captions, it fails to include the provided names. Conversely, although previous SFT-based methods mention the names correctly, but produce inaccurate descriptions due to limited visual recognition capability. However, in both cases, our method consistently generates faithful and accurate personalized captions for the query images, demonstrating superior personal grounding and visual understanding. Please refer to further qualitative results in the Appendix. 8 4.2 Ablation Studies Necessity of RL. To emphasize the necessity of RL, we examine the limitations of SFT based on our experimental results. First, we analyze the algorithmic shortcomings in visual recognition for personal grounding. Although RAP-Qwen achieves the highest personal grounding scores in the singleconcept setting  (Table 1)  , its performance significantly deteriorates when incorrect demonstrations are provided  (Table 3)  . The results suggest that SFT-based methods struggle to distinguish different objects between reference and query images, often resulting in inaccurate personalized captions. In contrast, our RL-based method consistently yields lower scores when provided with incorrect demonstrations, highlighting its robustness in differentiating objects and its ability to avoid merely duplicating the given personal information when generating personalized captions. Furthermore, we analyze why SFT-based methods struggle with personal grounding in multi-concept settings, as shown in Table 2. To this end, we examine the training data compositions that include multiple identities within single image. Interestingly, despite comparable proportion of multipleidentity training data (i.e., RAP-MLLM: 5.4% of 210K, Ours: 4.7% of 2K), SFT yields only marginal improvements in the 2-concept setting (ID) and performs poorly in the 4-concept setting (OOD). In contrast, our method achieves substantial performance gains in both the 2 and 4-concept personalized image captioning tasks. We attribute the shortcomings of SFT-based methodsparticularly their limited visual recognition and poor generalization to OOD scenariosto fundamental algorithmic differences. As data-centric approach, SFT tends to overfit to dominant patterns and struggles to learn from rare or diverse data [10]. Our experimental results further highlight why RAP-Qwen performs well in single-concept settings but fails significantly in OOD scenarios. These findings underscore the necessity of RL-based approaches to perform robust and generalizable personalized image captioning. Efficacy of Each Component. The overall ablation studies are shown in Table 4. First, we evaluate the effectiveness of reasoning templates [12], such as the <think> token used in visual reasoning tasks. As result, using reasoning templates rather degrades the personal grounding performance, regardless of the two different special reasoning tokens. For reward template ablations, removing the OCT template from the training dataset weakens the scores, underscoring the importance of reinforced visual recognition. Most notably, removing ICT causes drastic decline in performance, confirming its critical role in personal grounding. Removing the VLT also leads to performance drop, suggesting that reinforcing visual localization contributes to enhancing personal grounding. These findings emphasize that the integration of all proposed components is essential to achieve state-of-the-art performance. For the ablation results on the additional components related to preserving the captioning quality, excluding length regularization or detailed query prompts results in minor accuracy drops. For the qualitative effects of these components on image captioning quality, please refer to the Appendix. Figure 5: Visualization of training stability. Training Stability. In Figure 5, we report the mean and standard deviation of rewards across different random seeds to demonstrate the stability of our RL training, independent of the data curriculum."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose RePIC, novel RL-based post-training framework that alleviates the cost of collecting high-quality personal captions, enabling effective personalized image captioning. By leveraging verifiable rewards, curated data compositions, and tailored instruction templates, our posttuned MLLM achieves strong performance on personalized captioning tasks across both single and multi-concept settings. Furthermore, our experimental results demonstrate that RePIC equips MLLMs with more robust visual recognition abilities, enhanced personalized generation, and improved generalizability than existing SFT-based methods, highlighting the superiority of our approach in complex scenarios such as 4-concept image captioning cases not seen during post-training. Limitations. In this work, the reproduced baseline models may not have undergone exhaustive hyperparameter tuning. Further, as this study primarily focuses on evaluating the impact of RL on personalized image captioning, future work could extend this approach to other personalized 9 tasks, such as multi-turn conversations. Additionally, we evaluated caption faithfulness only using human-level preference scores due to the lack of GT captions. This would be improved by generating GT captions with larger models like GPT-4o and human refinement. Future Work Reducing retrieval noise through reasoning or stronger retrieval models, and exploring alternatives to length regularization (e.g., self-correction, test-time scaling) are potential directions for future research. While this work focuses on RL-based post-training on the image domain, future research could extend personalization to other modalities such as audio and video, aligned with the progress of models like Qwen-2.5 Omni [46]."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported by the National Research Foundation of Korea (NRF) through grant funded by the Korean government (MSIT) [No. 2022R1A3B1077720]; the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) [No. RS2021-II211343], Artificial Intelligence Graduate School Program at Seoul National University; and the BK21 FOUR program, Education and Research Program for Future ICT Pioneers at Seoul National University, in 2025. The authors also gratefully acknowledge the support from the NVIDIA Academic Grant Program."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. In European Conference on Computer Vision, pages 7391. Springer, 2024. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [5] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. [6] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [7] Jooyoung Choi, Chaehun Shin, Yeongtak Oh, Heeseung Kim, and Sungroh Yoon. Style-friendly snr sampler for style-driven generation. arXiv preprint arXiv:2411.14793, 2024. [8] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In European Conference on Computer Vision, pages 206235. Springer, 2024. [9] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [10] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023. [11] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 [13] Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, and Xiangyu Yue. Remember, retrieve and generate: Understanding infinite visual concepts as your personalized assistant. arXiv preprint arXiv:2410.13360, 2024. [14] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [17] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Sangwon Jang, Jaehyeong Jo, Kimin Lee, and Sung Ju Hwang. Identity decoupling for multi-subject personalization of text-to-image models. arXiv preprint arXiv:2404.04243, 2024. [20] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, and Yeul-Min Baek. Instantfamily: Masked attention for zero-shot multi-id image generation. arXiv preprint arXiv:2404.19427, 2024. [21] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [22] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [23] Shengzhi Li, Rongyu Lin, and Shichao Pei. Multi-modal preference alignment remedies degradation of visual instruction tuning on language models. arXiv preprint arXiv:2402.10884, 2024. [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [27] Ziche Liu, Rui Ke, Yajiao Liu, Feng Jiang, and Haizhou Li. Take the essence and discard the dross: rethinking on data selection for fine-tuning large language models. arXiv preprint arXiv:2406.14115, 2024. [28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 37303738, 2015. [29] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [30] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [31] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [32] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. arXiv preprint arXiv:2406.09400, 2024. [33] Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. Personalized visual instruction tuning. arXiv preprint arXiv:2410.07113, 2024. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 11 [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [37] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. [38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [39] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [40] Ming Shen. Rethinking data selection for supervised fine-tuning. arXiv preprint arXiv:2402.06094, 2024. [41] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [42] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [44] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [45] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. [46] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [48] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [49] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [50] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [51] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [52] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 12 Figure A.1: Visualization of more complex and diverse multi-concept image captioning result using our proposed RePIC. The generated query image is sourced from InstantFamily [20]. Figure A.2: Visualization of RePIC results on various personalized image captioning tasks."
        },
        {
            "title": "A Additional Qualitative Results",
            "content": "A.1 More Challenging Multi-Concept Setting In this section, we further present the results of our method in the 7-concept setting, as illustrated in Figure A.1. Notably, our approach generates faithful descriptions of the query image while accurately referencing the corresponding reference images and their associated information. In Figure A.2, we demonstrate that our proposed method can faithfully perform image captioning on synthetic images across various tasks, including virtual try-on [8], multi-human or subject personalization [14], and style-driven personalization [7], which is compatible with the state-of-the-art personalized image generation benchmarks. These results highlight the superiority of our approach in handling diverse personalized image captioning tasks with MLLM. A.2 Visualizations on Image Captioning Quality In Figure A.3, we present qualitative comparisons of image captioning quality in both single and multi-concept settings, highlighting the effectiveness of our proposed method. Note that the RAP13 Figure A.3: Examples of generated captions on single and 2-concept personalized image captioning tasks. Figure A.4: Visualization of qualitative results for additional components used for our methods. LLaVA often merely duplicates the retrieval information or generates visual hallucinations without considering vision perception, and RAP-Qwen severely fails to caption correctly. In contrast, only the proposed approach faithfully and concretely describes the given query image. A.3 Effects of Additional Components for Preserving Captioning Quality In Figure A.4, we present additional ablation results evaluating the impact of applying length regularization and incorporating detailed prompts in the training dataset. The results indicate that incorporating length regularization and detailed prompts effectively mitigates generating uninformative captions for the query images. 14 Figure A.5: Visualizations of output responses with and without the use of reasoning templates. Figure A.6: Examples illustrating additional limitations of RePIC in 2-concept scenario. A.4 Effect of Reasoning Templates We consider the post-tuned model trained with reasoning templates such as <think> and <observe> to verify the effectiveness of visual reasoning in personalized tasks, which has become prevalent choice [29, 17, 39] for MLLM post-training with RL. However, in Figure A.5, we observe that using reasoning templates often results in longer outputs that fail to faithfully describe the query image. In contrast, omitting templates leads to more concise yet accurate and faithful image descriptions. Furthermore, as demonstrated in our main analysis, reasoning templates negatively impact personal grounding performance in personalized image captioning tasks. A.5 Further Limitations of RePIC We illustrate the limitations of our RePIC model on the personalized image captioning task in Figure A.6. In the first row, RePIC incorrectly captions the image with blue jeans, despite no such item being present. similar issue is observed in the second row, where the model references polka-dotted dress that does not appear in the query image. These examples show limitation of RePIC in generating accurate personalized captions, primarily due to insufficient fine-grained visual perception. For instance, it struggles when objects are not visibly present (e.g., no blue jeans appear) or when the reference and query images differ significantly (e.g., back view vs. front view), making it difficult to recognize them as the same person or the same object. We expect that these limitations can be mitigated either by constructing high-quality database for each conceptavoiding the use of personal information based solely on the visual appearance of the image, and ensuring the reference image clearly shows front view of the objector by leveraging an MLLM equipped with an advanced vision encoder and more powerful backbone LLM, such as Qwen-3 [47]. Figure A.7: Datasets used for training and evaluation. Note that the Subject200K+ dataset (a) was used for training, while all real datasets (b) to (f) were used only for evaluation. Figure A.8: Visualization of the DreamBooth database constructed in this work."
        },
        {
            "title": "B Additional Experimental Configurations",
            "content": "B.1 Experimental Details Our implementation is based on the open-source codebase. To train our model, we set LoRA rank as 64, LoRA alpha as 128, and use the number of generations per prompt as 8. B.2 Used Datasets for Evaluation The data configuration used for both training and evaluation in our experiments is detailed in Figure A.7. Notably, the Subject200K+ dataset was used exclusively for post-training and was not included in the evaluation. All other real-image benchmarks were used for evaluation purposes. In Figure A.8, we present the configuration of our curated DreamBooth [35] database used for single-concept captioning evaluation in our experiment. https://github.com/om-ai-lab/VLM-R1 Figure A.9: Visualization of data templates used for MLLM post-training, including examples of OCT, VLT, single-ICT, and multi-ICT. Figure A.10: (a) Dataset composition, (b) instruction composition, and (c) the sensitivity to the proportion of identity grounding instructions within the overall training set. Table A.1: Used reasoning templates <think> Reasoning Template: First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. <observation> Reasoning Template: First, observe carefully and enclose the observation process in <observe> </observe> tags and then output the final answer in <answer> </answer> tags. B.3 Used Data Templates In Figure A.9, we illustrate the data and instructions for verifiable rewards of OCT, VLT, and ICT used for post-training. 17 Figure A.11: Distributions of mean verifiable rewards during training for each task: (a) OCT, (bc) ICT, and (d) VLT. B.4 Dataset Compositions For generating <name>, we use random name generatore to sample human or object names in an on-the-fly manner. Table A.1 shows the two reasoning templates of using <observe> and <think> tokens used for our ablation studies involving special tokens. These templates were appended directly before each captioning query. Figure A.10 illustrates how we construct high-quality dataset for personal grounding. In (a), we show that the dataset is composed of COCO, Objects365, CelebA, and Subject200K+. (b) visualizes the instruction composition, which includes OCT, VLT, single-ICT, and multi-ICT. We note that approximately 31% of the total training data is composed of single and multi-ICT samples. In (c), we highlight the dataset sensitivity for convergence. We observe that if the amount of ICT instruction in training data is too high, the RL training often fails. This demonstrates that while RL is inherently data-efficient, it is sensitive to data quality. Overall, our findings highlight the importance of well-structured instruction dataset for effective RL-based post-training in MLLM personalization. B.5 Details On Retrieval Setting Following the previous work [13], for retrieval setting, we first utilize database of image, text pairs representing user-specific concepts. Given database images, each image is first processed using pre-trained CLIP [34] encoder to obtain visual embeddings. Then, for given query image and its corresponding textual instructions, YOLO-World [6] is employed to detect regions of interest. Thus, cropped images are encoded into embeddings, and by computing Euclidean distances between these embeddings and pre-stored embeddings, the most relevant reference images are retrieved from the database."
        },
        {
            "title": "C Additional Analyses",
            "content": "C.1 How Efficiently Does RL Maximize Rewards During Post-Training? Figure A.11 illustrates how efficiently our proposed method achieves personalization of the model. To analyze this, we divide the sections with the criterion of seen data during training into bins and count the number of responses with verifiable reward of 1 within each bin. These counts are then normalized by the total number of responses that include both rewards of 0 and 1, which we call this score as normalized occurrence. The results show clear upward trend in performance across both OCT, single and multi-ICT, once the proportion of seen data exceeds 50%. Here, the total number of seen data is 2K. Notably, ICTs both begin with low occurrence rate of approximately 0.2 but show sharp emergence towards 1.0 once the seen data surpasses 50% (i.e., 1K samples). These results suggest that our method effectively guides MLLM personalization in data-efficient and effective manner, armed with our carefully designed verifiable rewards, data construction, and instruction compositions. Note, VLT shows relatively stable performance regardless of the amount of seen data. https://faker.readthedocs.io/en/master/ 18 Figure A.12: Ablation studies on output length distributions of image captioning across single and multi-concept evaluation datasets. Figure A.13: Visualization of results: (a) measured output response length (e.g. between <answer> and </answer> tokens), (b) output length measured within the reasoning template (e.g. between <think> and </think> tokens), and (c) ablation studies. C.2 Can Length Regularization Reward Guides To Prolong Output Completions? Figure A.12 presents ablation results on output completion lengths of the image captioning task across the evaluation datasets. In all cases, applying our length regularization proves as simple yet effective strategy for increasing output lengths, consistently yielding longer completions, surpassing those generated by both zero-shot and SFT (i.e., RAP-Qwen) baselines, which often generate uninformative captions such as This is <name>. C.3 Does Reasoning Template Matter for Personalization? We further conduct an experiment to investigate whether the reasoning templates have meaningful impact on personalization. Specifically, we examine the effects of using special tokens and measure the output completion length when no reasoning template is used. Interestingly, as shown in Figure A.13, our ablation results reveal the following: (a) Even without reasoning template, the model is capable of producing sufficiently long and informative answer responses. (b) In contrast, when using reasoning template, we observe similar tendency toward overthinking [31], which is decline in accuracy as the average reasoning time increases, where the model focuses primarily on the reasoning process at the expense of informative answers. Thus, in the same context as our experiments in the main paper, for post-training with RL, eliminating the reasoning template contributes to enhancing the personal grounding. C.4 Does RePIC Enhance The General Image Captioning Ability of MLLM? In this section, we compare the captioning performance of our proposed method with the zero-shot baseline on general image captioning task. The evaluation does not consider both skip-retrieval and retrieval settings, as it focuses solely on captioning single query image using general prompts without any reference images. As result, in Figure A.14, our method consistently generates more faithful and accurate descriptions for the image compared to the zero-shot model under general query settings. In Figure A.15, we further compare results using detailed query prompts. In this case, the results of both our method and the zero-shot model show nearly equivalent performance in caption generation. This suggests that RL-based post-training does not enhance models ability to perform detailed image captioning beyond what the zero-shot model can already achieve. Rather, our RL-based post-training method reinforces the frequency of more faithful and preferable captions in the output under general query prompt settings. These observations align with the results reported in concurrent studies [51, 12], and quantitative results for the preference evaluations with GPT4o across the MyVLM, YoLLaVa, and DreamBooth datasets are presented in Figure A.16. 19 Figure A.14: Visualization of image captioning results for general query prompts. Figure A.15: Visualization of image captioning results for detail query prompts. Importantly, these results also demonstrate that our RePIC does not degrade the original models general captioning capabilities after post-tuning. Unlike SFT approaches, our GRPO-based RL training maintains the models generalization ability. This is achieved by applying KL-divergence regularization between the reference and target models during training, ensuring that the target model remains close to the reference. Thus, by maximizing verifiable reward while preserving instruction-following ability through KL-divergence, RePIC generates the preferable personalized image captions without compromising the original models zero-shot capabilities. Figure A.16: Quantitative results of preference evaluations for the single-image captioning task without reference images, using (a) general query prompts and (b) detailed query prompts. Note that RePIC outperforms the zero-shot model in (a), and achieves comparable results in (b). Figure A.17: Visualization of KL-divergence and accuracy reward plots on the seen data. Table A.2: Visualization of recall scores (%) for 2-concept personal grounding. Models 2-Concept AVG Skip-Ret. Ret. Ours-2K w/o multi-ICT Ours-2K w/ multi-ICT 43.9 98.8 42.7 92.7 43.3 95.8 C.5 Why Multi-ICT is Necessary? We present the recall scores in the 2-concept settings to verify the need to contain multi-ICT in the training data. As shown in Table A.2, models trained only with single-ICT fail to perform well in multi-concept setting. This highlights the necessity of our proposed multi-ICT for improving multi-concept personal grounding performance. C.6 Analysis on Hyperparameter Sensitivity Figure A.17 presents the results of various ablation studies. In (a), we compare three settings: using only simple prompts, incorporating detailed prompts, and further applying length regularization based on verifiable reward function. This reward assigns value of 1 only when the output response length exceeds predefined cutoff. We set the cutoff length to 100, as the average length of personal information in our database is approximately 100 tokensroughly equivalent to at least one complete sentence. To encourage more informative image captions, we regularize the model to generate outputs of at least this length. In (b), we investigate how the expected reward changes with different values of the KL-divergence regularization weight βKL. We also observe that the convergence behavior is influenced by the cutoff length used for length regularization. Our results indicate that the combination of βKL = 0.04 and cutoff length of 100 yields the best performance."
        },
        {
            "title": "D Used Templates",
            "content": "D.1 Evaluation Templates In Table A.3, we present the evaluation prompts used for personalized image captioning. Table A.3: Prompts used for evaluating the personalized image captioning experiments. General Caption Template: Give personalized caption of this image. Give caption of the image. Can you provide personalized caption for this photo? Provide caption of the given image. D.2 Preference Evaluation Templates The template used for our preference evaluation is shown below. Rather than favoring captions that merely duplicate retrieved content, we instructed the model to evaluate preferred captions that convey meaningful and accurate information to satisfy the following criteria: 1. Reference Similarity: Measures how closely the generated caption matches the retrieved reference sentence. higher similarity indicates potential redundancy, and thus lower preference score is assigned. 2. Captioning Faithfulness: Assesses how accurately the generated caption describes the visual content of the input image. Preference Evaluation Template Retrieval-based Preference Evaluation: You are an evaluation expert. Your task is to determine which answer best describes the given image accurately. Carefully analyze the options and select the most appropriate one as your final choice. Input: <Image> The name of the object in this image is: {Name}. The additional information for the given image is: {Info}. The preferable caption is one that is not merely duplication of the given information but provides meaningful and accurate description. Which one is more preferable caption to the {Name}? Options: A: {string1} B: {string2} Output the final answer by choosing one of the options with single alphabet. Answer: A, D.3 Instruction Templates We further present the system prompts used for OCT and ICTs. In the following, in Tables A.4, A.5, and A.6, we present the full instruction templates used for OCT, ICT, and VLT, respectively. Note, we augment the instructions using GPT-4o. 22 Table A.4: Instruction templates used for OCT in training data. Object Consistency Tuning (OCT) Template: Please verify whether the objects in these pictures are the same. An object is considered the same if its consistency is maintained despite variations in lighting or pose. Is <name> visible in this picture? Is <name> in this image? Do you see <name> in the photo? Is <name> present in this photograph? Can you identify if <name> is captured in this picture? Is <name> depicted in this image? Does the picture feature <name>? Can you confirm if <name> appears in this photo? Is <name> included in this shot? Is <name> shown in this image? Can you tell if <name> is part of this photograph? Is there any sign of <name> in this picture? Can you detect <name> in the photo? Is <name> captured in this image? Do you recognize <name> in this picture? System Prompt for OCT As an evaluation expert, your task is to verify whether the object identified as <name> in the first image is also present in the second image. Answer with yes or no. {Question} System Prompt for Single-ICT You are captioning expert. Your task is to generate an accurate caption for the second image while referencing the first image. Both images contain the same object. The object in the first image is named <name>. {Question} System Prompt for multi-ICT You are captioning expert. Your task is to generate an accurate caption for the last query image while referencing the given reference images. The reference images each contain an object, named respectively as <name1>, <name2>. {Question} These are additional information about the given images except the last image: <name1>, <name2>, and <name3>. {Question} Each object in the images not including the last image has name: <name1>, <name2>. {Question} Below is additional information about the object all images except the last one: <name1>, <name2>. {Question} 23 Table A.5: Instruction templates used for VLT in training data. Visual Localization Tuning (VLT) Template: Please provide the bounding box coordinate of the region this sentence describes: <name>. Give <name>s bounding box in the image. Describe <name>s position in the image. Please provide the coordinates of the bounding box for <name> in the given image. Specify the rectangular boundaries of <name> in the image. Give <name>s position in the following image. Please provide <name>s bounding coordinates in the image. Indicate the bounding box for <name> in the image. Show the bounding box for <name> in the picture. Specify <name>s bounding box in the photograph. Mark <name>s bounding box within the image. 24 Table A.6: Instruction templates used for single and multi-ICT in training data. Identity Consistency Tuning (ICT) Template: Give caption of the image. Give personalized caption of this image. Provide general caption of the image. Summarize the visual content of the image. Create detail caption of the image. Offer rich and clear interpretation of the image. Describe the image in detail. Render summary of the photo. Provide caption of the given image. Can you provide personalized caption of this photo? Could you describe this image faithfully? Generate detailed and accurate description of the image. Write caption that reflects the contents and context of the image. Compose meaningful caption that truly represents the image. Describe the image in personalized and context-aware manner. Provide natural-sounding caption that accurately conveys what is in the image. Craft caption that authentically describes the scene in the image. Create caption that captures the essence of the image. Write caption that reflects whats visually happening in the photo. Generate human-like description that accurately represents the image. Describe this image as if you were explaining it to friend. Produce relevant and truthful caption based on the image. Give caption that matches the visual elements in the image. Summarize the visual content of this image in natural way. Write an image-grounded caption that remains faithful to the content. Provide descriptive sentence that corresponds closely to the image."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Seoul National University",
        "Department of Future Automotive Mobility, Seoul National University",
        "Interdisciplinary Program in Artificial Intelligence, Seoul National University",
        "NVIDIA"
    ]
}