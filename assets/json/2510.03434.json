{
    "paper_title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
    "authors": [
        "Zhiying Jiang",
        "Raihan Seraj",
        "Marcos Villagra",
        "Bidhan Roy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\\times$ less training data and 16$\\times$ less compute than the prior decentralized baseline."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 3 4 3 0 . 0 1 5 2 : r Paris: Decentralized Trained Open-Weight Diffusion Model Zhiying Jiang, Raihan Seraj, Marcos Villagra, Bidhan Roy* Bagel Labs bagel.com We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation (Section 3) confirms that Pariss decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14 less training data and 16 less compute than the prior decentralized baseline (Table 3, Figure 3). bageldotcom/paris bageldotcom/paris silver car on brick road surrounded by palm trees Sunset over calm ocean crystal-clear alpine lake reflecting the majestic mountain photograph of majestic garden beautiful landscape peaceful garden with blooming cherry trees cozy cabin in snowy forest Abstract art with vibrant colors and geometric shapes high-resolution photograph of golden retriever puppy running through meadow high-quality photograph of fresh bagels, out of the oven"
        },
        {
            "title": "Text conditioned image generation samples using Paris",
            "content": "*Corresponding author: bidhan[at]bagel[dot]com 1 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs 1. Introduction Large-scale diffusion model training requires synchronized gradient updates across thousands of GPUs connected through high-bandwidth interconnects. State-of-the-art text-to-image models like Stable Diffusion required 150,000 A100 GPU-hours (Rombach et al., 2022), while Googles Imagen required hundreds of TPU-v4 chips (Saharia et al., 2022), demanding dedicated clusters with InfiniBand or similar networking. This creates two fundamental barriers: (i) Only institutions with massive computing infrastructure can train these models. (ii) The synchronization requirement prevents leveraging geographically distributed and/or commodity hardware lacking specialized interconnects. We present Paris, demonstrating that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. The model consists of 8 independently trained expert diffusion models that exchange neither gradients, parameters, nor intermediate activations during training. This complete computational independence required implementing our Distributed Diffusion Training framework from scratch, which extends the decentralized flow matching theory of McAllister et al. (2025), the Diffusion Transformer architecture of Peebles and Xie (2022), and architectural optimizations from PixArt-𝛼 (Chen et al., 2023) to achieve practical, production-scale implementation. Key insights enabling this approach include the natural decomposition of flow matching objectives across data partitions, allowing experts to optimize locally while the ensemble approximates the global distribution, the superior scaling properties of Diffusion Transformers for distributed training, and parameter-efficient conditioning that reduces model size without quality loss. These advances enable lightweight routing mechanism learned post-hoc to orchestrate expert collaboration during inference without requiring any coordination during training. This eliminates the need for specialized interconnects and dedicated clusters. Empirical validation confirms that fully decentralized training maintains generation quality while enabling development on fragmented compute resources previously unusable for large-scale diffusion models. Contributions. Our key contributions are: (i) the first open-weight text-to-image diffusion model trained entirely through decentralized computation with zero inter-expert communication; (ii) extending DDMs framework (McAllister et al., 2025) with architectural and computational optimizations that achieve comparable generation quality while using only 1/14 of the training data (11M vs. 154M images) and 1/16 of the computational resources; and (iii) complete open-source implementation demonstrating practical deployment on heterogeneous, geographically distributed hardware. 2. Method 2.1. Overview We pretrain our model, Paris, utilizing our Distributed Diffusion Training framework, to enable fully decentralized training that doesnt require communication among experts. In order to achieve that, we partitioned the training set into 𝐾 clusters. We then train each expert on partition in isolation; and lightweight router fuses experts at inference. Concretely, we: (i) pre-encode images into latent space with pretrained VAE (Kingma and Welling, 2014) (sd-vae-ft-mse) to reduce compute, following Rombach et al. (2022); (ii) compute semantic features with DINOv2 and cluster the data; (iii) train 𝐾 expert denoisers; (iv) train small transformer router independently; and (v) perform different expert selection strategies at sampling. 2 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Figure 1 Multi-expert training pipeline of Paris. 2.2. Implementation Scale and Architecture Paris extends decentralized flow matching theory (Lipman et al., 2023; McAllister et al., 2025), the Diffusion Transformer architecture (Peebles and Xie, 2022), and architectural optimizations from PixArt-𝛼 (Chen et al., 2023). We implement decentralized training using Diffusion Transformers for their superior scaling properties compared to U-Nets (Ronneberger et al., 2015), with initialization schemes ensuring convergence without coordination. We validate Paris training recipe at two scales: DiT-B/2 (where denotes Base model size with 768 hidden dimensions, and /2 indicates patch size 2 for latent tokenization) with 129M parameters per expert and DiT-XL/2 with 605M parameters per expert, totaling 1.03B and 4.84B parameters respectively across 8 experts. The framework partitions 11M LAION-Aesthetic (Schuhmann et al., 2022) images into semantically coherent clusters using DINOv2 embeddings, enabling each expert to specialize on distinct visual domains like portraits, landscapes, or architecture. Pariss routing mechanism operates directly on partially denoised latents throughout the reverse diffusion process, requiring the router to identify appropriate experts despite observing only noisy intermediate states. This noise-aware routing, combined with flow matching objectives using velocity prediction and initialization strategies, ensures stable convergence despite the complete absence of 3 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs inter-expert coordination during training. 2.3. Decentralized Flow Matching Objective Following the works of McAllister et al. (2025) and Lipman et al. (2023), we decompose the standard flow matching objective across 𝐾 expert models, each trained on disjoint data partitions. The marginal flow 𝑢𝑡 (𝑥𝑡) transports samples from the noise distribution to the data distribution through: 𝑢𝑡 (𝑥𝑡) = 𝑥 𝑢𝑡 (𝑥𝑡 𝑥0) 𝑝𝑡 (𝑥𝑡 𝑥0)𝑞(𝑥0) 𝑝𝑡 (𝑥𝑡) 𝑑𝑥0, (1) where 𝑥𝑡 = 𝛼𝑡 𝑥0 + 𝜎𝑡𝜖 represents the noisy latent at timestep 𝑡 [0, 1], with 𝛼𝑡 = 1 𝑡 and 𝜎𝑡 = 𝑡 for linear scheduling. In the discrete case over dataset D: 𝑢𝑡 (𝑥𝑡) = 1 𝑝𝑡 (𝑥𝑡) 𝑥0 𝑢𝑡 (𝑥𝑡 𝑥0) 𝑝𝑡 (𝑥𝑡 𝑥0)𝑞(𝑥0). (2) The key insight of DDM is partitioning the data into 𝐾 disjoint clusters {𝑆1, 𝑆2, . . . , 𝑆𝐾 }, enabling decomposition: 𝑢𝑡 (𝑥𝑡) = 𝐾 𝑘=1 𝑝𝑡 (𝑘𝑥𝑡) 𝑢(𝑘) 𝑡 (𝑥𝑡), (3) where 𝑢(𝑘) (𝑥𝑡) is the flow predicted by expert 𝑘 trained only on cluster 𝑆𝑘, and 𝑝𝑡 (𝑘𝑥𝑡) is the posterior probability from the router network. This formulation allows each expert to optimize independently: 𝑡 (𝑘) expert = 𝔼𝑥0 𝑆𝑘,𝑡 (cid:2)𝑣𝜃𝑘 (𝑥𝑡, 𝑡) (𝑥0 𝑥𝑡)2(cid:3) , (4) where 𝜃𝑘 denotes the parameters of expert 𝑘, 𝑣𝜃𝑘 (𝑥𝑡, 𝑡) is the predicted velocity field at noisy latent 𝑥𝑡 and timestep 𝑡 [0, 1], (𝑥0 𝑥𝑡) represents the target velocity direction from the noisy state back to the clean data, and the expectation is taken over clean samples 𝑥0 drawn from cluster 𝑆𝑘 and uniformly sampled timesteps. 2.4. Expert Models The DiTExpert architecture represents our adaptation of the Diffusion Transformer (DiT) framework for decentralized diffusion modeling, incorporating architectural innovations from both the original DiT paper and recent advances in text-to-image generation such as PixArt-𝛼. This architecture serves as the backbone for each expert model in our decentralized ensemble, providing superior scalability compared to traditional U-Net (Ronneberger et al., 2015) architectures while maintaining computational efficiency through strategic design choices. 2.4.1. Core Architecture At its foundation, DiTExpert operates on latent representations rather than raw pixels, following the latent diffusion paradigm of Rombach et al. (2022). The model processes 32 32 latent tensors with 4 channels, obtained through pre-trained VAE encoder that performs 8 spatial downsampling from the original 256 256 images. Following the Vision Transformer paradigm of Dosovitskiy et al. (2020), we incorporate fixed sinusoidal positional embeddings (Vaswani et al., 2017) to preserve 4 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs spatial relationships between patches. Temporal information, critical for the diffusion process, is injected through specialized timestep embedder. 2.4.2. Transformer Blocks with Adaptive Layer Normalization The core of DiTExpert consists of various transformer blocks, each implementing modified architecture that integrates timestep conditioning through Adaptive Layer Normalization (AdaLN). Unlike standard transformers that use fixed layer normalization (Ba et al., 2016), AdaLN modulates the normalized activations based on the timestep embedding: AdaLN(ℎ, 𝑐) = 𝛾𝑐 LayerNorm(ℎ) + 𝛽𝑐, (5) where denotes the Hadamard product, 𝛾𝑐, 𝛽𝑐 ℝ𝐷 are scale and shift parameters predicted from the conditioning signal 𝑐 = 𝜏(𝑡) through block-specific MLPs. This mechanism allows the model to adaptively adjust its behavior across different noise levels without increasing the sequence length. Each DiT block follows the structure: ℎ = ℎ + MSA(AdaLN(ℎ, 𝑐)), ℎ = ℎ + CrossAttn(AdaLN(ℎ, 𝑐), 𝑒text) ℎout = ℎ + FFN(AdaLN(ℎ, 𝑐)), (if text-conditioned), (6) (7) (8) where MSA denotes multi-head self-attention (Vaswani et al., 2017) with multiple heads, CrossAttn represents optional cross-attention for text conditioning, and FFN is feed-forward network with expansion ratio 4.0 and GELU (Hendrycks and Gimpel, 2016) activation. For text-conditional generation, we extend the base DiT architecture with cross-attention layers positioned between self-attention and feed-forward blocks. Text embeddings from frozen CLIP (Radford et al., 2021) encoders are projected to the models hidden dimension through learned linear transformation. 2.4.3. Parameter-Efficient AdaLN-Single Variant We implement an optional AdaLN-Single variant proposed by PixArt-𝛼. This modification computes conditioning signals once globally and uses learnable per-block embeddings: [𝛾1, 𝛽1, ..., 𝛾𝐿, 𝛽𝐿] = MLPglobal(𝜏(𝑡)) + 𝐸blocks, (9) where 𝐸blocks ℝ𝐿6𝐷 are learned embeddings for 𝐿 blocks, each containing 6 modulation parameters, which helps reducing parameters substantially. 2.5. Router Networks The DiTRouter architecture represents critical innovation in our decentralized diffusion framework, employing lightweight Diffusion Transformer variant specifically designed for dynamic expert selection. Unlike traditional routing mechanisms that operate on clean inputs or require separate encoders, DiTRouter directly processes noisy latents at arbitrary timesteps, learning to identify which experts training distribution best matches the current sample. This design enables coherent expert collaboration during the reverse diffusion process while maintaining computational efficiency. 2.5.1. Core Architectural DiTRouter adopts the DiT-B/2 configuration, utilizing smaller transformer architecture compared to the expert models to minimize routing overhead. The router processes the same 32 32 4 latent 5 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Figure 2 Multi-expert inference pipeline of Paris. representations as the experts but with reduced capacity, occupying around one-third the size of each expert model. The fundamental task of the router is formulated as classification problem over noisy inputs: where 𝑘 {1, ..., 𝐾} indexes the experts, 𝑥𝑡 is the noisy latent at timestep 𝑡. This probabilistic formulation enables both hard routing (top-1 selection) and soft ensemble strategies during inference. 𝑝(𝑘𝑥𝑡, 𝑡) = Router𝜙(𝑥𝑡, 𝑡), (10) 2.5.2. Timestep-Aware Processing key insight in router design is that expert specialization may vary across noise levels, certain experts might excel at high-frequency detail recovery (low noise) while others specialize in global structure formation (high noise). DiTRouter incorporates timestep information through the same sinusoidal embedding and MLP projection as the experts: 𝜏router(𝑡) = MLProuter(SinusoidalEmbed(𝑡)). (11) This temporal conditioning is crucial for maintaining consistency with expert models, as both router and experts must interpret timesteps identically to ensure coherent collaboration. The router learns to adapt its classification strategy based on the denoising stage, potentially routing to different experts at different points in the generation process. 6 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs 2.5.3. Training Objective and Loss Function DiTRouter training employs straightforward cross-entropy loss against ground-truth cluster assignments: Lrouter = 𝔼𝑥0D,𝑡[0,1] (cid:2) log 𝑝𝜙(𝑐(𝑥0)𝑥𝑡, 𝑡)(cid:3) , (12) where 𝑐(𝑥0) {1, ..., 𝐾} denotes the cluster assignment of the clean sample 𝑥0. Critically, the router must learn to identify the correct expert despite only observing noisy versions of the input, requiring it to develop robust features invariant to diffusion noise while remaining sensitive to semantic content. During training, we sample timesteps uniformly and apply the corresponding noise schedule to create 𝑥𝑡, ensuring the router gains experience across all noise levels. This comprehensive training enables accurate routing throughout the entire reverse diffusion process. 2.6. DINOv2-Based Semantic Clustering We employ DINOv2 (Oquab et al., 2023), self-supervised vision transformer trained on 142M images, to extract semantically meaningful features that guide expert specialization through data partitioning. For each image, we extract 1024-dimensional features using DINOv2-ViT-L/14 on center-cropped 224 224 images: 𝑓𝑖 = AvgPool(DINOv2(𝑥𝑖)) ℝ1024. To handle large-scale datasets efficiently, we adopt two-stage hierarchical clustering strategy following McAllister et al. (2025): first performing fine-grained k-means (MacQueen et al., 1967) clustering to identify fine-grained initial centroids, then consolidating these into 𝐾 coarse clusters (typically 𝐾 = 8) through second round of clustering. 2.7. Inference Strategies for Decentralized Generation At inference time, the decentralized diffusion framework offers multiple strategies for combining expert predictions, each presenting distinct trade-offs between generation quality, computational cost, and sample diversity. As shown in Figure 2, these strategies leverage the routers learned expertise to dynamically orchestrate expert collaboration throughout the reverse diffusion process. 2.7.1. Top-1 Expert Selection The most computationally efficient strategy routes each denoising step to single expert based on the routers highest-confidence prediction: 𝑣𝑡 (𝑥𝑡) = 𝑣𝜃𝑘 (𝑥𝑡, 𝑡), 𝑘 = arg max 𝑘 𝑝𝜙(𝑘𝑥𝑡, 𝑡). (13) This hard routing approach maintains constant computational cost regardless of ensemble size, requiring only one expert forward pass per denoising step. Surprisingly, this simple strategy often yields the best results according to McAllister et al. (2025). While our experiments on smaller networks  (Table 2)  show different pattern, Top-1 routing achieves competitive quality and Top-2 surpasses both monolithic and full ensemble approaches. 2.7.2. Top-K Weighted Ensemble The Top-K strategy combines predictions from the 𝐾 most relevant experts, weighted by their renormalized router probabilities: 7 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs 𝑣𝑡 (𝑥𝑡) = 𝑘Top-K 𝑝𝜙(𝑘𝑥𝑡, 𝑡) (cid:205) 𝑗Top-K 𝑝𝜙( 𝑗𝑥𝑡, 𝑡) 𝑣𝜃𝑘 (𝑥𝑡, 𝑡). (14) This approach provides middle ground between computational efficiency and ensemble benefits. The renormalization ensures the weights sum to unity, maintaining proper velocity scaling in the flow matching framework. The implementation supports per-sample Top-K selection, allowing different samples in batch to utilize different expert combinations based on their individual characteristics. This flexibility proves particularly valuable when generating diverse batches where samples may benefit from different specializations. 2.7.3. Full Ensemble Integration The full ensemble strategy incorporates all experts weighted by their router probabilities: 𝑣𝑡 (𝑥𝑡) = 𝐾 𝑘=1 𝑝𝜙(𝑘𝑥𝑡, 𝑡) 𝑣𝜃𝑘 (𝑥𝑡, 𝑡). (15) While computationally expensive, requiring 𝐾 forward passes per step, this strategy theoretically provides the most accurate approximation to the true data distribution by fully utilizing the learned decomposition. However, empirical results often show diminishing returns or even degraded quality compared to selective strategies, potentially due to interference from less-relevant experts adding noise to the predictions. 2.8. Comparison with Parallelization Strategies Traditional distributed training strategies maintain mathematical equivalence to single-device training through synchronized computation. Data Parallelism requires all-reduce gradient synchronization at regular intervals (every iteration for synchronous variants and every 𝑁 iterations for approaches like local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023)). Model Parallelism (Shoeybi et al., 2019) creates sequential dependencies as layers wait for gradients from subsequent stages. Pipeline Parallelism (Huang et al., 2019; Narayanan et al., 2019) introduces idle time during pipeline fills and drains. Each strategy imposes distinct synchronization patterns that constrain hardware deployment. Pariss training recipe eliminates synchronization entirely. Once data partitions are established, experts train independently with no gradient synchronization, no parameter sharing, and no activation exchange. Each expert (129M-605M parameters) operates on consumer GPUs without coordination barriers or specialized interconnects. Experts train asynchronously across heterogeneous infrastructure (AWS, GCP, local clusters, Runpod) at different speeds without communication. Modern optimizations including communication-computation overlap, sequence parallelism, and zero-bubble scheduling reduce synchronization overhead but cannot eliminate worker coordination requirements. Hardware heterogeneity introduces stragglers that reduce throughput proportionally, while geographic distribution amplifies latency penalties multiplicatively. Pariss zero-communication training recipe eliminates these constraints, enabling training on 120 A40 GPU-days across heterogeneous infrastructure where centralized approaches require thousands of GPU-days with specialized interconnects. 8 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Table 1 Comparison of parallelization strategies: Paris uniquely eliminates all communication overhead. Training Strategy Synchronization Pattern Data Parallel Periodic all-reduce Model Parallel Pipeline Parallel Sequential layer transfers Stage-to-stage per microbatch Straggler Impact Topology Constraints Slowest worker blocks iteration Slowest layer blocks pipeline Bubble overhead from slowest stage Latency-sensitive cluster Linear pipeline Linear pipeline Paris No synchronization No blocking Arbitrary 3. Experiments and Results 3.1. Training Details We conduct experiments at two model scales to validate the scalability of our decentralized diffusion framework: DiT-B/2 (129M parameters per expert) and DiT-XL/2 (605M parameters per expert), resulting in total ensemble sizes of 1.0B and 4.84B parameters respectively when accounting for 8 experts. 3.1.1. DiT-B/2 Configuration The base-scale configuration employs DiT-B architecture with 768 hidden dimensions, 12 transformer layers, and 12 attention heads per expert. Training proceeds on pre-computed VAE (Kingma and Welling, 2014) latents at 32 32 resolution with 4 channels, corresponding to 256 256 pixel-space images. Each expert trains independently on its assigned cluster using bath size 128 with gradient accumulation over 2 steps (effective batch 256); learning rate 1 104 with AdamW (Loshchilov and Hutter, 2019) optimizer, no scheduling; Exponential moving average (Polyak and Juditsky, 1992) with 𝛽 = 0.9999 for stable inference; FP16 mixed precision training (Micikevicius et al., 2018) with automatic loss scaling The corresponding router has smaller size (DiT-S) and trains separately on the full dataset with cross-entropy loss. Router training employs smaller base batch size of 64 with 4-step gradient accumulation, yielding an effective batch of 256. We use reduced learning rate of 5105 with cosine annealing (Loshchilov and Hutter, 2017) over 25 epochs to prevent overfitting on the classification task. 3.1.2. DiT-XL/2 Configuration The large-scale configuration scales to DiT-XL/2 with 1152 hidden dimensions, 28 transformer layers, and 16 attention heads. The corresponding router uses smaller architecture (DiT-B), reducing routing overhead while maintaining classification accuracy. This asymmetric design reflects the insight that routing decisions require less capacity than generation itself. 9 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Table 2 FID-50K (lower is better) comparing monolithic training and decentralized multi-expert training with different inference strategies on Laion-art. All models use the DiT-B/2 architecture (129M parameters per expert). Inference Strategy FID-50K Monolithic (single model) Top-1 Top-2 Full Ensemble (all experts) Improvement vs. Monolithic 29.64 30.60 22.60 47.89 7. Notes. Monolithic: Single model trained on full Laion-art dataset. Top-K: Weighted combination of top-K experts by router confidence. Full: All 8 experts weighted by router probabilities. Lower FID indicates better generation quality. Table 3 Comparison of Paris with DDM baseline on LAION-Aesthetic. Paris achieves competitive quality while using 14.4 fewer training images and 16.3 less compute. Model Params Train Images Train Compute FID Paris (DiT-XL/2, Top-1) 0.6B 11M DDM (McAllister et al., 2025) 0.6B 158M 120 A40 (72 A100) 168 GPUs 7d (1176 A100) 12. 9.84 Ratio (DDM/Paris) 1.0 14.4 16.3 1.27 Note: Paris FID-50K is measured on LAION-Aesthetic subsets but not exactly the same subset as DDM. DDMs exact evaluation subset not publicly disclosed. GPU-days computed as total training time. 3.2. Results We evaluate generation quality using the Fréchet Inception Distance (FID) (Heusel et al., 2017), which measures the similarity between distributions of real and generated images in the feature space of pretrained Inception network, with lower scores indicating better quality and diversity. Table 2 compares inference strategies for our multi-expert approach against monolithic baseline, both using the DiT-B/2 architecture (129M parameters per expert). The Top-2 strategy, which uses weighted combination of the top-2 experts by router confidence, achieves the best performance with FID-50K of 22.60, representing 7.04 improvement over the monolithic baseline (29.64). Surprisingly, the Full Ensemble strategy that weights all 8 experts by router probabilities underperforms (47.89), suggesting that selective expert collaboration is more effective than naive averaging across all experts. This validates our hypothesis that targeted routing outperforms simple ensemble approaches with Paris. Table 3 and Figure 3 present direct comparison between Paris and the original DDM baseline (McAllister et al., 2025). Both models use the DiT-XL/2 architecture with 0.6B parameters and Top-1 expert selection. Paris achieves FID of 12.45 on subset of LAION-Aesthetic using only 11M training images and 120 A40 GPU-days (72 A100 equivalent), while DDM achieves FID of 9.84 using 158M training images and 1176 A100 GPU-days. This represents 14 reduction in training data and 16.3 reduction in compute at the cost of 1.27 higher FID. The results demonstrate that Paris successfully implements decentralized diffusion training with substantially improved resource efficiency, making high-quality text-to-image generation accessible with commodity hardware and limited datasets. The modest quality gap suggests promising directions for future optimization while maintaining the core Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Figure 3 Efficiency comparison of Paris and DDM. advantage of zero inter-expert communication. 4. Conclusion We demonstrate that high-quality diffusion models can be trained through fully decentralized computation, achieving competitive text-to-image synthesis without gradient synchronization. This work establishes both the feasibility and practical implementation patterns for distributed generative modeling, opening pathways toward larger-scale decentralized training. Future work includes model distillation for deployment efficiency, adaptation to video generation, and incorporation of architectural advances."
        },
        {
            "title": "References",
            "content": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Ping. Pixart-𝛼: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is 11 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Arthur Douillard, Martin Jaggi, and Thomas Parnell. Diloco: Distributed low-communication training of language models. arXiv preprint arXiv:2311.08105, 2023. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, volume 30, 2017. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in neural information processing systems, volume 32, 2019. Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281297. Oakland, CA, USA, 1967. David McAllister, Matthew Tancik, Jiaming Song, and Angjoo Kanazawa. Decentralized diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 23323 23333, 2025. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations, 2018. Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Gregory Ganger, Phillip Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 115, 2019. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Boris Polyak and Anatoli Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838855, 1992. 12 Paris: Decentralized Trained Open-Weight Diffusion Model by Bagel Labs Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5b: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, volume 35, pages 2527825294, 2022. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Sebastian Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017."
        }
    ],
    "affiliations": [
        "Bagel Labs"
    ]
}