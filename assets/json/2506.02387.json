{
    "paper_title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments",
    "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Xiangmin Yi",
        "Huining Yuan",
        "Xinlei Chen",
        "Yi Wu",
        "Chao Yu",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io."
        },
        {
            "title": "Start",
            "content": "VS-BENCH: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments Zelai Xu1, Zhexuan Xu2, Xiangmin Yi1, Huining Yuan1, Xinlei Chen1, Yi Wu1,3, Chao Yu1,4, Yu Wang1 1Tsinghua University, 2University of Science and Technology of China 3Shanghai Qi Zhi Institute, 4Beijing Zhongguancun Academy"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce VISUAL STRATEGIC BENCH (VS-BENCH), multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-BENCH comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents ability to predict others future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-BENCH as foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io. 5 2 0 2 3 ] . [ 1 7 8 3 2 0 . 6 0 5 2 : r Figure 1: Evaluation results of fourteen state-of-the-art VLMs on strategic reasoning and decisionmaking averaged over eight multi-agent environments in VS-BENCH. Equal contribution. zelai.eecs@gmail.com Corresponding authors. {yuchao,yu-wang}@tsinghua.edu.cn Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Vision Language Models (VLMs) have recently unlocked impressive capabilities in open-world perception, multimodal reasoning, and interactive problem-solving [5, 39, 89]. Driven by these advancements, evaluations of VLMs have progressed beyond static tasks such as image captioning [15] and visual reasoning [3, 85] toward dynamic agent benchmarks including software engineering [13, 82], computer use [30, 80], game environments [75, 87], and embodied control [25, 68, 83]. However, existing VLM benchmarks mainly focus on single-agent settings, where one agent reasons and acts in isolation. The real world, by contrast, is inherently multi-agent environment that involves cooperation, competition, and mixed-motive interactions between agents [20, 77]. This poses new challenges to the ability of intelligent agents. First, an agents outcome depends not only on its own action but also on other agents actions, requiring strategic reasoning to infer others intentions and predict their future moves. Second, as all agents learn and adapt concurrently, the underlying dynamics become non-stationary, demanding decision-making under uncertainty that optimizes for long-term objectives. Third, the coexistence of cooperation and competition gives rise to social dilemmas where agents must strategically balance self-interest and collective welfare. These challenges raise crucial question that current benchmarks leave underexplored: How capable are VLMs at strategic reasoning and decision-making in multi-agent environments? While prior efforts [1, 18, 79] have explored multi-agent evaluation for Large Language Models (LLMs), these benchmarks remain restricted to text-only environments, limiting their capability to assess agents in multimodal scenarios. On the one hand, many strategic domains intrinsically rely on visual observations like board grids [33, 64], card faces [7, 11], and game frames [8, 12]. Flattening these visual observations into symbolic text strings requires hand-crafted encodings and inevitably discards spatial information critical for reasoning and decision-making. On the other hand, humans naturally combine vision and language when interacting with others. Consequently, purely text-based environments diverge from real-world human-agent interactions and obscure progress toward developing human-compatible intelligent agents. These limitations underscore the need for multimodal benchmark that incorporates visual context in multi-agent environments. To bridge this gap, we introduce VISUAL STRATEGIC BENCH (VS-BENCH), multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VSBENCH comprises eight vision-grounded environments that cover three fundamental types of multiagent interactions: (1) Cooperative games, including Hanabi and Overcooked, require agents to understand teammates intentions and coordinate to achieve shared objectives. (2) Competitive games, including Breakthrough, Kuhn Poker, and Atari Pong, require agents to model their opponents and remain robust against adversaries. (3) Mixed-motive games, including Coin Dilemma, Monster Hunt, and Battle of the Colors, require agents to balance conflicting interests and sustain cooperation while avoiding exploitation. Collectively, these games provide diverse set of multi-agent environments. VS-BENCH evaluates VLMs along two complementary dimensions: offline evaluation of strategic reasoning and online evaluation of decision-making. Strategic reasoning is the theory-of-mind ability to infer other agents intentions and predict their future moves for effective cooperation and competition. We construct an offline dataset for each environment and evaluate VLM agents performance by their prediction accuracy of other agents next actions. Decision-making is the ability to optimize for long-term objectives under non-stationary dynamics. We let VLM agents engage in online self-play or interactions with conventional agents for full-length episodes and evaluate their performance by normalized episode returns. By jointly analyzing both perspectives, our benchmark provides unified and comprehensive evaluation of VLMs in multi-agent environments. We evaluate fourteen leading VLMs, including five commercial reasoning models, six commercial chat models, and three open-source models on VS-BENCH. Extensive results show that although current VLMs exhibit preliminary strategic reasoning ability by surpassing random agents, the best-performing model only attains an average prediction accuracy of 47.8%. Moreover, existing VLMs struggle with decision-making in multi-agent environments, with the best-performing model achieving 24.3% normalized episode return averaged across all environments. Notably, although commercial reasoning models in general attain the best results, some open-source models can achieve comparable performance in certain mixed-motive games with prosocial behaviors for mutual benefit. We further conduct in-depth analyses and provide insights on multimodal observations, test-time scaling, social behaviors, and failure modes of VLMs in multi-agent environments. 2 Figure 2: Overview of VS-BENCH, multimodal benchmark for evaluating VLMs in multi-agent environments. We evaluate fourteen state-of-the-art models in eight vision-grounded environments with two complementary dimensions, including offline evaluation of strategic reasoning by nextaction prediction accuracy and online evaluation of decision-making by normalized episode return. In summary, our contributions are threefold: We introduce VS-BENCH, multimodal benchmark for evaluating VLMs on strategic reasoning and decision-making in multi-agent environments, comprising eight vision-grounded games across cooperative, competitive, and mixed-motive interactions. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode returns, to provide unified and comprehensive assessment of VLM agents. We perform extensive experiments of eleven commercial VLMs and three open-source VLMs and provide in-depth analyses of multimodal observations, test-time scaling, social behaviors, and failure modes, highlighting significant performance gaps for future research."
        },
        {
            "title": "2 VS-BENCH environments",
            "content": "In this section, we first formalize the evaluation of VLMs in multi-agent environments and then introduce eight vision-grounded games comprising VS-BENCH. These games are carefully curated from game theory and multi-agent reinforcement learning (MARL), each serving as well-recognized environment in the literature. We further adapt these games to incorporate image and text observations while preserving their strategic dynamics. By covering cooperation, competition, and mixed-motive interactions, these games provide diverse set of multi-agent environments for evaluating VLMs. An additional set of simpler games named VS-BENCH MINI is described in Appendix G."
        },
        {
            "title": "2.1 Problem formulation",
            "content": "Multi-agent environments are formalized as Partially Observable Markov Games (POMG) [38, 63]. POMG is defined by tuple = (N , S, {Ai}iN , {Oi}iN , P, {Ri}iN , γ), where = {1, , n} is the set of agents; is the state space; Ai and Oi are the action space and observation space of agent i, respectively; : {Ai}iN (S) is the transition function; Ri : {Ai}iN is the reward function of agent i; and γ is the discount factor. In step t, each agent receives an observation oi,t and chooses an action ai,t according to its policy πi. Given the current state st and the joint action at = (a1,t, , an,t), the environment transitions to the next state 3 st+1 P(st, at) and each agent receive reward ri,t = Ri(st, at). The objective of agent is to maximize its expected accumulated reward Eπ1, ,πn [(cid:80) To evaluate VLMs in multi-agent environments, we consider multimodal observation space Oi = (Ii, Ti), where Ii is the image observation space and Ti is the text observation space. We also consider text action space Ai and mapping function : Ai Ai that converts each text action into the original action space. To more comprehensively evaluate the strategic ability of VLM agents, we consider three types of multi-agent interactions defined by the reward structure. γtri,t]."
        },
        {
            "title": "2.2 Cooperative games",
            "content": "In cooperative games, all agents share the same objective. Formally, the reward functions in cooperative games are identical: R1(s, a) = = Rn(s, a) for all (s, a) {Ai}iN . To achieve strong performance in cooperative games, agents must understand their teammates intentions under partial observability, divide the tasks to improve efficiency, and coordinate their actions to optimize for the shared objective. We consider two representative cooperative games in MARL literature. Hanabi [7] is partially observable card game where players can observe others cards but not their own. Each card has color and rank that can only be revealed through hint actions at the cost of an information token. To succeed, agents must coordinate to play cards in rank order for five colors. We consider the two-player full game, which is widely used for research on theory of mind, zero-shot coordination, and ad-hoc teamplay [27, 28]. Detailed descriptions can be found in Appendix H.1. Overcooked [23] is popular video game where two chefs cooperate to cook and serve dishes in kitchen. Each dish requires multiple operations like navigating, chopping, cooking, plating, and delivering, which are difficult to coordinate even for human players. Our implementation is based on Overcooked-AI [12], well-known environment for zero-shot coordination and human-AI interactions [66, 86]. Detailed descriptions can be found in Appendix H.2."
        },
        {
            "title": "2.3 Competitive games",
            "content": "In competitive games, the objective of each agent strictly contradicts those of the others. Formally, the reward functions in competitive games are zero-sum: (cid:80)n i=1 Ri(s, a) = 0 for all (s, a) {Ai}iN . To succeed in competitive games, agents must model their opponents to predict their future moves, stay robust against adversarial exploitation, and adapt to non-stationary dynamics. We consider three representative competitive games in game theory and MARL literature. Breakthrough [72] is chess-like board game with simplified rules and identical pawns. Two players compete to advance their pieces across an 8 8 grid to reach the opponents back row. The game is deceptively simple, yet it exhibits deep combinatorial complexity and sharp tempo imbalance between attack and defense, making it suitable environment for studying multi-step lookahead and adversarial decision-making [41, 59]. Detailed descriptions can be found in Appendix H.3. Kuhn Poker [32] is simplified variant of Texas Holdem [11, 48] designed to study imperfectinformation games for game-theoretic analysis. It has three-card deck and single betting round where two players take turns to either check or bet with limited stakes. The game has been used as classic environment for counterfactual reasoning and decision-making with imperfect information [33, 49]. Detailed descriptions can be found in Appendix H.4. Atari Pong [4] is classic arcade video game where two players control paddles to hit ball across the screen. With raw pixel observations and competitive dynamics, the game has become canonical environment in the Arcade Learning Environment (ALE) [8], which requires spatio-temporal reasoning and strategic gameplay [46, 47]. Detailed descriptions can be found in Appendix H.5."
        },
        {
            "title": "2.4 Mixed-motive games",
            "content": "In mixed-motive games, agents objectives are partially aligned and partially divergent. Formally, the reward functions are neither identical nor zero-sum, that is, there exists (s, a) such that Ri(s, a) = Rj(s, a) and (cid:80)n i=1 Ri(s, a) = 0. To excel in mixed-motive games, agents must anticipate the hidden intentions of others, balance self-interest and common welfare, and achieve high-payoff equilibria. We consider three mixed-motive games adapted from classic social dilemmas in game theory. 4 Coin Dilemma [35] is grid-world environment inspired by the classic Prisoners Dilemma [55] in game theory. red player and blue player move in 5 5 grid world to collect red and blue coins. player earns 1 point for collecting any coin. However, if the red player collects blue coin, the blue player is penalized 2 points, and vice versa. This setup creates tension between mutual benefit and self-interest: while both players collecting coins of their own color leads to win-win result, unilateral defection to collect all coins maximizes ones own gains at the others expense. Therefore, the game has been common environment for studying rational reasoning, opponent shaping, and social dilemma resolution [21, 43, 58]. Detailed descriptions can be found in Appendix H.6. Monster Hunt [53] is grid-world environment inspired by the classic Stag Hunt [57] in game theory. Two players move in 5 5 grid world to individually eat an apple for 2 points or jointly defeat monster for 5 points. player who confronts the monster alone, however, is penalized 2 points. This leads to multiple Nash equilibria where agents can both safely eat apples alone or take risks to cooperate for higher rewards. The game is used to investigate trust formation and risk-sensitive decision-making [34, 67]. Detailed descriptions can be found in Appendix H.7. Battle of the Colors is grid-world environment inspired by the classic Battle of the Sexes [44] in game theory. We propose and design this game in manner similar to the previous two social dilemma games. red player and blue player move in 5 5 grid world with red block and blue block. If both players move to the red block, the red player earns 2 points while the blue player earns 1 point, and vice versa. If players move to two blocks of different colors, both players earn 0 points. Therefore, while coordination is mutually beneficial, each player strictly prefers choosing the block of their own color, creating conflict of interest that produces two payoff-asymmetric Nash equilibria and mixed equilibrium. This game thus challenges agents to solve conflicting preferences while avoiding coordination failure, making it suitable for studying equilibrium selection, bargaining dynamics, and social fairness. Detailed descriptions can be found in Appendix H.8."
        },
        {
            "title": "3 Evaluating VLMs in multi-agent environments",
            "content": "To comprehensively benchmark VLMs in multi-agent environments, we consider two complementary dimensions including offline evaluation of strategic reasoning and online evaluation of decisionmaking. We further provide several insights from our evaluation, which highlight the limitations of existing VLMs and motivate our analysis in the next section. Model setup. We select fourteen state-of-the-art VLMs for evaluation. For commercial VLMs, we select five reasoning models and six chat models from OpenAI GPT [50] and o-series [51], Anthropic Claude [2], Google Gemini [17], xAI Grok [78], Qwen [69], and Doubao [61]. For open-source VLMs, we select three leading models from Llama-3.2-Vision [45], InternVL3 [89], and Qwen2.5VL [6]. We set the temperature to 1.0 and the maximum number of output tokens to 8k for all models. We also set the maximum number of reasoning tokens to 16k for reasoning models. When encountering cutoff for reaching maximum tokens, we dynamically extend the output and reasoning tokens to the models limit. Detailed descriptions of model setups can be found in Appendix B."
        },
        {
            "title": "3.1 Strategic reasoning",
            "content": "Strategic reasoning is the theory-of-mind ability to infer the hidden beliefs, desires, and intentions of other agents [31, 54]. This requires agents to think from others perspectives and answer the question: What are the next actions of other agents? Strategic reasoning is crucial in multi-agent environments because an agents reward function depends not only on its own action, but also on other participants actions. Therefore, to achieve strong performance, agents must anticipate teammates moves to coordinate in cooperative games, predict opponents actions to counter them in competitive games, and deduce whether to cooperate or compete in mixed-motive games. Detailed descriptions of strategic reasoning evaluation can be found in Appendix C. Evaluation setup. We evaluate the strategic reasoning ability of VLMs by their prediction accuracy of other agents next actions on an offline dataset for each environment. More specifically, each sample in the dataset is tuple ({imgi,τ }t τ =t and texti,t are the image observation sequence and text observation of agent at step t, respectively, and ai,t+1 is the action of other agents at the next step + 1. The VLMs are prompted with the image sequence and text observations to predict the next actions of other agents. To ensure rigorous and thorough τ =t, texti,t, ai,t+1), where {imgi,τ }t 5 Models1 Oracle Overall Cooperative Hanabi Overcooked Board 2 Comptitive Poker Mixed-Motive Pong Dilemma Hunt Battle 100. 100.0 100.0 100.0 100.0 100.0 100. 100.0 100.0 o4-mini gemini-2.5-flash claude-3-7-sonnet doubao-1-5-thinking-pro qvq-max gpt-4.1 doubao-1-5-vision-pro gemini-2.5 w/o thinking qwen-vl-max claude-3-7 w/o thinking grok-2-vision Qwen2.5-VL-72B-Ins. InternVL3-78B Llama-3.2-90B-Vision-Ins. Random 47.8 45.5 42.4 36.6 30.5 36.6 30.8 29.2 28.5 28.3 25.7 30.4 29.7 26.4 24.3 58.3 37.0 39.0 32.8 32.3 23.0 15.0 21.5 26.5 9.8 12. 26.8 25.3 20.0 8.8 31.8 21.0 26.0 26.3 19.0 27.0 22.3 19.3 26.0 16.0 17.3 26.5 20.5 16.5 16. 26.8 23.3 24.3 19.8 21.8 22.5 15.8 14.8 19.5 18.0 10.8 23.8 14.0 11.8 4.3 63.5 65.0 65.5 57.8 59.3 54.0 53.8 48.5 45.3 56.0 53. 45.2 45.5 53.3 50.0 43.5 41.3 44.8 44.3 37.8 41.5 31.3 34.0 23.5 43.3 20.8 27.0 34.8 36.3 33. 62.8 63.5 53.8 26.5 25.3 49.9 37.3 31.8 26.3 31.0 30.3 30.0 37.0 26.3 25.4 43.8 50.3 42.5 45.3 21.5 36.8 36.0 30.5 23.5 26.0 31. 27.3 30.0 25.0 29.3 52.5 62.8 43.3 40.0 27.0 38.0 34.8 33.8 37.3 26.8 29.0 36.8 30.3 18.8 26. Table 1: Strategic reasoning evaluation results. For each environment, the first , second , and third best results are highlighted in green, while the results below random are highlighted in red. evaluation, we construct the datasets according to the following three principles. (1) Predictable: the next actions can be predicted from the image and text observations, which exclude actions like reveal in Hanabi that require unobservable information. (2) Diverse: the dataset should cover diverse range of observations and actions in the environments, considering different reasoning contexts and different strategies of other agents. (3) Balanced: the distribution of the samples should be balanced to avoid bias toward certain behaviors or patterns. Guided by these principles, we collect dataset of 400 samples for each environment and benchmark fourteen VLMs for strategic reasoning ability measured by next-action prediction accuracy in eight environments. The evaluation results in Fig. 1 and Table 1 show that current VLMs exhibit preliminary strategic reasoning ability by surpassing random agents in overall prediction accuracy, yet they still lag behind the oracle results by noticeable margin. All fourteen models perform better than random guesses in at least five of the eight games, demonstrating non-trivial theory-of-mind capability in multi-agent environments. In general, reasoning models achieve better results than chat models and open-source models, with the best-performing model o4-mini attaining an overall accuracy of 47.8% and consistently ranking in the top three across all environments. Notably, the three leading open-source models achieve an average overall accuracy of 28.8%, which is comparable to commercial chat models with 29.8% average overall accuracy. However, even these most capable models attain less than 50% overall accuracy, and some even fail to outperform random. This deficit is especially pronounced in Overcooked, Kuhn Poker, Atari Pong, and Monster Hunt, three of which are adapted from video games. We further investigate this observation in the next analysis section. Finding 1: Existing VLMs exhibit preliminary strategic reasoning ability by outperforming random guesses, but they are still far from accurate prediction of others next actions."
        },
        {
            "title": "3.2 Decision-making",
            "content": "Decision-making is the ability to optimize for ones long-term objectives under uncertainty [19]. This requires agents to prioritize future accumulated returns over immediate gains, adapt to non-stationary dynamics with evolving agents, and balance cooperation and competition to navigate toward favorable equilibria. Detailed descriptions of decision-making evaluation can be found in Appendix D. Evaluation setup. We evaluate the decision-making ability of VLMs by their normalized episode returns through online self-play or interactions with conventional agents in each environment. More 1Specific model versions and links to open-source models can be found in Appendix B. 2The Board column corresponds to Breakthrough, and the remaining columns to its right correspond to Kuhn Poker, Atari Pong, Coin Dilemma, Monster Hunt, and Battle of the Colors, respectively. 6 Models Optimal Overall Cooperative Hanabi Overcooked Board Comptitive Poker Pong Dilemma Mixed-Motive Hunt 100.0 100. 100.0 100.0 100.0 100.0 100.0 100. Battle 100.0 o4-mini doubao-1-5-thinking-pro gemini-2.5-flash claude-3-7-sonnet qvq-max gemini-2.5 w/o thinking gpt-4.1 qwen-vl-max grok-2-vision claude-3-7 w/o thinking doubao-1-5-vision-pro Qwen2.5-VL-72B-Ins. InternVL3-78B 24.3 21.3 20.1 19.5 0. 3.0 2.8 1.0 1.1 1.9 4.5 1.9 1.0 Llama-3.2-90B-Vision-Ins. 4.2 30.047.0 42.930.5 17.06.8 10.021.0 56.722.8 10.14.7 20.025.75 8.55.4 27.136.0 6.721.1 10.13.5 20.039.75 2.03.4 0.00.0 5.031.5 11.216.5 4.621.4 24.98.2 69.332.9 17.211.3 0.73.2 2.92.5 68.524.3 26.25.8 10.025.5 1.61.9 34.753.2 19.93.5 4.615.4 92.922.7 0.51.0 0.74.5 0.50.0 0.02.1 8.737. 3.55.4 4.04.8 32.88.5 2.54.6 0.41.6 2.04.0 0.00.0 0.00.0 0.50.0 1.22.0 0.50.0 0.00.0 0.00.0 0.00.0 0.50.0 1.53.3 2.04.0 1.01.4 0.21. 18.230.4 7.140.4 0.00.0 0.00.0 0.00.0 20.556.9 0.31.0 0.42.8 13.220.2 0.50.0 0.00.0 11.854.2 0.11.5 5.031.5 23.650.9 0.90.3 0.00.0 40.257.4 0.90.3 2.15.2 0.45.8 0.28.2 7.88.2 0.50.0 0.74.3 17.86.7 0.78.9 11.25.6 1.17.0 1.49. 0.52.0 1.02.3 2.53.4 0.52.0 0.81.8 0.50.0 0.01.5 0.00.0 1.53.3 0.00.0 3.249.3 0.80.2 0.00.0 0.00.0 0.90.3 4.062.1 0.00.0 39.459.7 0.90.3 0.02.7 6.88.9 1.89.2 0.43.4 19.625.7 0.50.0 0.01.5 1.02. 3.64.9 Random 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 Table 2: Decision-making evaluation results. For each environment, the first , second , and third best results are highlighted in green, while the results below or equal to random are in red. specifically, for cooperative and mixed-motive games, we let multiple VLM agents of the same model interact with each other. For competitive games, we evaluate VLM agents against well-recognized conventional agents like Monte Carlo Tree Search (MCTS) [16], Counterfactual Regret Minimization (CFR) [90], and Atari built-in bot [8]. For all environments, we also evaluate the random agents and the oracle agents with maximum return to normalize the results so that the normalized return for random agents is 0 and the normalized return for oracle agents is 100. The evaluation results in Fig. 1 and Table 2 show that current VLMs exhibit limited decision-making ability in multi-agent environments, highlighting significant gap between existing models and optimal performance. As illustrated by the large swaths of red cells, six out of fourteen models overall performance is even worse than random agents, indicating their incompetence to optimize long-term return under non-stationary, interdependent multi-agent dynamics. Although reasoning models achieve relatively better results than chat models and open-source models, even the bestperforming model o4-mini only attains an overall normalized return of 24.3%, which is far behind the optimal agents. Surprisingly, we observe that some open-source models can achieve comparable results to reasoning models in certain mixed-motive games like Qwen2.5-VL-72B-Ins. in Coin Dilemma and InternVL3-78B in Monster Hunt. We also observe that the failures to outperform random agents are concentrated on video games like Overcooked, Atari Pong, and Coin Dilemma, which underscores the coupled difficulty of multimodal perception and strategic decision-making. We further investigate and analyze these observations in the next section. Finding 2: Existing VLMs struggle with decision-making in multi-agent environments, leaving significant performance gap that remains an open challenge for future research."
        },
        {
            "title": "4 Analysis",
            "content": "Motivated by the observations in the evaluation results, we further investigate several aspects of VLMs in multi-agent environments and provide in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases. More experiment results can be found in Appendix E. 3Specific model versions and links to open-source models can be found in Appendix B. 4The Board column corresponds to Breakthrough, and the remaining columns to its right correspond to Kuhn Poker, Atari Pong, Coin Dilemma, Monster Hunt, and Battle of the Colors, respectively. 7 Figure 3: Comparison of reasoning VLMs on decision-making with multimodal and text-only observations. The solid and dashed vertical lines represent the average results of two settings."
        },
        {
            "title": "4.1 Multimodal observations",
            "content": "In principle, multimodal observations with both images and texts provide richer information and are expected to give better results. However, the evaluations on reasoning and decision-making show that environments with inherent visual states, like video games, are especially challenging for VLM agents, indicating their potential incompetence with multimodal observations. We investigate this by considering text-only observations that replace images with corresponding text descriptions. We select board game, card game, and video game, and evaluate reasoning VLMs for decisionmaking with multimodal and text-only observations. The evaluation results in Fig. 3 show that VLMs fail to achieve better performance with multimodal observations. More specifically, multimodal observations lead to worse average performance than text-only input in all three environments. This demonstrates the incompetence of existing VLMs in extracting information from visual input for effective reasoning and decision-making. More evaluation results can be found in Appendix E.1. Finding 3: Existing VLMs can fail to extract visual information and improve strategic reasoning and decision-making performance with multimodal observations."
        },
        {
            "title": "4.2 Test-time scaling",
            "content": "We observe in the evaluation results that reasoning models generally achieve better performance than chat models. We further investigate the test-time scaling of VLMs in multi-agent environments by using Chain-of-Thought (CoT) [76] prompting for chat models and comparing their performance with reasoning models and chat models with simple IO prompting. The evaluation results in Fig. 4 show that CoT prompting significantly improves chat models performance in all three environments, while reasoning models still achieve the best results. More evaluation results can be found in Appendix E.2 . Finding 4: Test-time scaling like reasoning and Chain-of-Thought (CoT) prompting can substantially improve VLMs performance in multi-agent environments."
        },
        {
            "title": "4.3 Social behaviors",
            "content": "Another interesting observation in the evaluation results is that open-source models can achieve comparable results to reasoning models in some mixed-motive games. We investigate this by visualizing the behaviors of two leading reasoning models and the best-performing open-source models in each social dilemma games. As shown in Fig. 5, in Coin Dilemma, the reasoning models are better at collecting coins, as they both cooperate (collect their own coin) and defect (collect others coin) more times than the open-source model. However, they are also more self-interested, especially o4-mini, which collects others coins more times than its own, resulting in worse-than-random result. In comparison, although InterVL3-78B is not adept at collecting coins, it exhibits strong preference for cooperation that collects its own coins rather than those of others, leading to win-win situation where both agents get high returns. Similar behaviors can be found in Monster Hunt, 8 Figure 4: Comparison of reasoning VLMs and chat VLMs on decision-making with IO and CoT prompting. The solid, dashed, and dotted vertical lines represent the average results of three settings. Figure 5: Social behaviors of two reasoning models and the best-performing open-source models in mixed-motive social dilemma games. Dimensions are agents behaviors described in Section 2.4. where the two reasoning models tend to safely eat apples alone and avoid encountering the monster. By contrast, Qwen2.5-VL-72B-Ins. is more inclined to take the risk to cooperate and defeat the monster together, which gives high reward. More evaluation results can be found in Appendix E.3. Finding 5: Open-source VLMs can achieve comparable results to commercial reasoning VLMs in some social dilemma games with prosocial behaviors for mutual benefit."
        },
        {
            "title": "4.4 Failure cases",
            "content": "To understand why VLMs underperform in multi-agent environments, we conduct qualitative analysis of their failure cases. In strategic reasoning, two common failure cases are ignoring history and private information. For example, in Hanabi, players cards are observable to other agents but not to themselves. VLMs often overlook this information asymmetry and incorrectly use their private information to predict the next actions of others. In decision-making, another common failure case is focusing excessively on ones own actions while ignoring those of others. For example, in Breakthrough, VLMs tend to persistently advance their own pieces and fail to identify defensive vulnerabilities that directly result in losing the match. More failure cases can be found in Appendix F."
        },
        {
            "title": "5.1 Multi-agent environments and benchmarks",
            "content": "Early work on multi-agent reasoning and decision-making is grounded in classical game theory [22, 74], which models rational interactions among self-interested players and introduces canonical environments like board games [62, 70], card games [32, 65], and social dilemmas [44, 55, 57]. Building on these foundations, breakthroughs in multi-agent reinforcement learning (MARL) [11, 64] have expanded the field toward complex, high-dimensional environments like video games. 9 These environments cover diverse range of multi-agent dynamics like cooperation [7, 12, 60], competition [48, 73], and mixed-motive interactions [9, 42]. Despite their impressive achievements, agents developed in these environments are typically specialized for single task and lack generalpurpose abilities to perform strategic reasoning and decision-making across different domains. Recent advancements in Large Language Models (LLMs) [24, 52, 71] have catalyzed paradigm shift toward generalist agents that can perceive and act in various environments without task-specific training. growing body of text-based benchmarks has been proposed to evaluate different facets of LLM agents in multi-agent environments. LLM-Coordination [1] analyzes LLM agents ability in pure cooperative tasks. GT-Bench [18] and GAMA-Bench [29] consider non-cooperative games and evaluate LLMs through the lens of game theory. Trust Games [79] studies the trust behaviors of LLM agents in their interactions. MAgIC [81] and LLMArena [14] include both cooperative and competitive games for comprehensive evaluation. However, these benchmarks mainly focus on text-only environments, which are different from real-world decision-making that integrates both visual and linguistic observations. Our work fills this gap by introducing eight vision-grounded games to evaluate multimodal generalist agents in multi-agent environments."
        },
        {
            "title": "5.2 VLM agent benchmarks",
            "content": "The rapid evolution of Vision Language Models (VLMs) [5, 39] has driven evaluation beyond static tasks like image captioning [15] and visual reasoning [3, 85] toward interactive agent environments. Existing benchmarks can be broadly categorized into four domains: coding, GUI interaction, game environments, and embodied control. Coding benchmarks [13, 36, 82] consider software engineering and machine learning engineering with both image and text input. GUI benchmarks evaluate VLMs on graphic interface operations like web browsing [26, 30, 88], computer use [80, 10], and phone use [37, 56]. Game benchmarks [40, 75, 87] offer dynamic virtual environments with structured rewards to assess VLMs ability in perception, reasoning, and decision-making. Embodied benchmarks [25, 68, 83] evaluate VLMs in vision-driven robotics control and physical world interactions. Nevertheless, these benchmarks predominantly concentrate on single-agent tasks, which overlook the distinctive challenges of multi-agent environments including non-stationary dynamics, interdependent decisionmaking, and equilibrium selection. Our work bridges this gap by evaluating VLMs in multi-agent games with both offline evaluation of strategic reasoning and online evaluation of decision-making."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present VS-BENCH, multimodal benchmark for evaluating VLMs on strategic reasoning and decision-making in multi-agent environments. By introducing eight vision-grounded environments and two complementary evaluation dimensions, including next-action prediction accuracy and normalized episode returns, we establish unified and comprehensive multi-agent evaluation of VLMs. Extensive experiments on fourteen leading VLMs reveal significant gap between existing models and optimal performance, highlighting their limitations for future development. We further provide detailed analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By releasing VS-BENCH as an open platform, we seek to spur research on strategic multimodal agents that excel in vision-grounded multi-agent environments."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by National Natural Science Foundation of China (No.62406159, 62325405), Postdoctoral Fellowship Program of CPSF under Grant Number (GZC20240830, 2024M761676), China Postdoctoral Science Special Foundation 2024T170496."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. Llm-coordination: evaluating and analyzing multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903, 2023. [2] Anthropic. Claude 3.7 sonnet system card, 2025. [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence In Proceedings of the IEEE Zitnick, and Devi Parikh. Vqa: Visual question answering. international conference on computer vision, pages 24252433, 2015. [4] Atari. Pong. Arcade Video Game, 1972. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwenvll: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Nolan Bard, Jakob Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: new frontier for ai research. Artificial Intelligence, 280:103216, 2020. [8] Marc Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of artificial intelligence research, 47:253279, 2013. [9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. [10] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. [11] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885890, 2019. [12] Micah Carroll, Rohin Shah, Mark Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019. [13] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [14] Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, and Lijie Wen. Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments. arXiv preprint arXiv:2402.16499, 2024. [15] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. [16] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 7283. Springer, 2006. [17] Google DeepMined. Gemini 2.5: Our most intelligent ai model, 2025. [18] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. arXiv preprint arXiv:2402.12348, 2024. [19] Ward Edwards. The theory of decision making. Psychological bulletin, 51(4):380, 1954. [20] Jacques Ferber and Gerhard Weiss. Multi-agent systems: an introduction to distributed artificial intelligence, volume 1. Addison-wesley Reading, 1999. [21] Jakob Foerster, Richard Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017. [22] Drew Fudenberg and Jean Tirole. Game theory. MIT press, 1991. [23] Ghost Town Games. Overcooked, 2016. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Pranav Guruprasad, Harshvardhan Sikka, Jaewoo Song, Yangyue Wang, and Paul Pu Liang. Benchmarking vision, language, & action models on robotic learning tasks. arXiv preprint arXiv:2411.05821, 2024. [26] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [27] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. other-play for zero-shot coordination. In International Conference on Machine Learning, pages 43994410. PMLR, 2020. [28] Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordination. In International Conference on Machine Learning, pages 1358413598. PMLR, 2023. [29] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael Lyu. How far are we on the decisionmaking of llms? evaluating llms gaming ability in multi-agent environments. arXiv preprint arXiv:2403.11807, 2024. [30] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [31] Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 4:169, 2023. [32] Harold Kuhn. simplified two-person poker. Contributions to the Theory of Games, 1(97-103):2, 1950. [33] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al. Openspiel: framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019. [34] Joel Leibo, Edgar Dueñez-Guzman, Alexander Vezhnevets, John Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement learning with melting pot. In International conference on machine learning, pages 61876199. PMLR, 2021. [35] Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilemmas using deep reinforcement learning. arXiv preprint arXiv:1707.01068, 2017. [36] Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. Mmcode: Benchmarking multimodal large language models for code generation with visually rich programming problems. arXiv preprint arXiv:2404.09486, 2024. 12 [37] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. [38] Michael Littman. Markov games as framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157163. Elsevier, 1994. [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [40] Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024. [41] Richard Lorentz and Therese Horey. Programming breakthrough. In International Conference on Computers and Games, pages 4959. Springer, 2013. [42] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. [43] Christopher Lu, Timon Willi, Christian Schroeder De Witt, and Jakob Foerster. Model-free opponent shaping. In International Conference on Machine Learning, pages 1439814411. PMLR, 2022. [44] Duncan Luce and Howard Raiffa. Games and decisions: Introduction and critical survey. Wiley, 1957. [45] meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, 2024. [46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [47] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. [48] Matej Moravˇcík, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508513, 2017. [49] Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat, Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes, et al. generalized training approach for multiagent learning. arXiv preprint arXiv:1909.12823, 2019. [50] OpenAI. Introducing gpt-4.1 in the api, 2025. [51] OpenAI. Openai o3 and o4-mini system card, 2025. [52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [53] Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve generalized stag hunts better than selfish ones. arXiv preprint arXiv:1709.02865, 2017. [54] Anand Rao, Michael Georgeff, et al. Bdi agents: from theory to practice. In Icmas, volume 95, pages 312319, 1995. [55] Anatol Rapoport and Albert Chammah. Prisoners dilemma: study in conflict and cooperation, volume 165. University of Michigan press, 1965. 13 [56] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [57] Jean-Jacques Rousseau. discourse on inequality. Penguin, 1985. [58] Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Garðar Ingvarsson Juto, Timon Willi, Ravi Hammond, Akbir Khan, Christian Schroeder de Witt, et al. Jaxmarl: Multi-agent rl environments and algorithms in jax. Advances in Neural Information Processing Systems, 37:5092550951, 2024. [59] Abdallah Saffidine, Nicolas Jouandeau, and Tristan Cazenave. Solving breakthrough with race patterns and job-level proof number search. In Advances in Computer Games: 13th International Conference, ACG 2011, Tilburg, The Netherlands, November 20-22, 2011, Revised Selected Papers 13, pages 196207. Springer, 2012. [60] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019. [61] ByteDance seed. Doubao-1.5-pro, 2025. [62] Claude Shannon. Xxii. programming computer for playing chess. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 41(314):256275, 1950. [63] Lloyd Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):10951100, 1953. [64] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [65] Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes bluff: Opponent modelling in poker. arXiv preprint arXiv:1207.1411, 2012. [66] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. Advances in Neural Information Processing Systems, 34:1450214515, 2021. [67] Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. arXiv preprint arXiv:2103.04564, 2021. [68] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [69] Qwen team. Qvq-max: Think with evidence, 2025. [70] Gerald Tesauro. Td-gammon, self-teaching backgammon program, achieves master-level play. Neural computation, 6(2):215219, 1994. [71] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [72] Dan Troyka. Breakthrough. About Board Games 8x8 Game Design Competition Winner, 2000. [73] Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354, 2019. 14 [74] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior: 60th anniversary commemorative edition. In Theory of games and economic behavior. Princeton university press, 2007. [75] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? arXiv preprint arXiv:2503.02358, 2025. [76] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [77] Michael Wooldridge. An introduction to multiagent systems. John wiley & sons, 2009. [78] xAI. Grok-2 beta release, 2024. [79] Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, et al. Can large language model agents simulate human trust behavior? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [80] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [81] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. arXiv preprint arXiv:2311.08562, 2023. [82] John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024. [83] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [84] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in neural information processing systems, 35:2461124624, 2022. [85] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [86] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et al. Proagent: building proactive cooperative In Proceedings of the AAAI Conference on Artificial agents with large language models. Intelligence, volume 38, pages 1759117599, 2024. [87] Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering agents with imagination for creative tasks. arXiv preprint arXiv:2312.02519, 2023. [88] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [89] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [90] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007."
        },
        {
            "title": "C Strategic reasoning evaluation details",
            "content": "D Decision-making evaluation details"
        },
        {
            "title": "F Failure case examples",
            "content": "G VS-BENCH MINI"
        },
        {
            "title": "H Environment details",
            "content": "17 17 17 20 22 27"
        },
        {
            "title": "A Limitations",
            "content": "Number of agents. Many real-world scenarios involve more than two participants. While our current environments mainly focus on two-player games, some of our environments like Hanabi support up to five players. Moreover, our framework can be easily extended to other multi-agent environments. Human baseline. We currently use random and optimal agents as references for the performance of VLM agents. future improvement is to include human experiments with participants of varying expertise to establish meaningful human baseline for benchmarking VLMs. Evaluation metric. For strategic reasoning evaluation, we use next-action prediction accuracy. Including other metrics like recall and F1 score can more comprehensively assess different aspects of the models reasoning ability. For decision-making evaluation, we use episode return in self-play or against conventional agents. Including results against population of diverse agents can provide more thorough assessment of VLM agents generalization and adaptability."
        },
        {
            "title": "Version",
            "content": "Evaluated Reasoning Multimodal Open-Source o4-mini claude-3-7-sonnet gemini-2.5-flash doubao-1-5-thinking-pro qvq-max o4-mini-2025-04-16 claude-3-7-sonnet-20250219 gemini-2.5-flash-preview-04-17 doubao-1-5-thinking-pro-m-250415 qvq-max-2025-03-25 gpt-4.1 claude-3-7 w/o thinking gemini-2.5 w/o thinking grok-2-vision doubao-1-5-vision-pro qwen-vl-max gpt-4.1-2025-04-14 claude-3-7-sonnet-20250219 gemini-2.5-flash-preview-04-17 grok-2-vision-1212 doubao-1-5-pro-32k-250115 qwen-vl-max-2025-04-08 Llama-3.2-90B-Vision-Ins. InternVL3-78B Qwen2.5-VL-72B-Ins. huggingface link huggingface link huggingface link o3 gemini-2.5-pro grok-3 deepseek-r1 deepseek-v3 N/A N/A N/A N/A N/A Table 3: Model configurations used in the evaluation. In this work, we evaluated the state-of-the-art VLMs released before May 1, 2025. All models in our experiments are listed in Table 3. For each model, the table specifies the exact version, whether it is reasoning model, supports multimodal inputs, and is open-source. For open-source models, the corresponding Hugging Face links are recorded in the Version column. We also list several leading models that are not included in our evaluation. Because our environments provide both visual and linguistic observations, we do not evaluate models lacking multimodal input support, such as grok-3, deepseek-r1, deepseek-v3. Currently, o3 is not evaluated due to its high cost of over $1000, and gemini-2.5-pro is not evaluated due to its rate limit of 1000 requests per day. We plan to release evaluation results of o3, gemini-2.5-pro in the next version of our manuscript. <BACK> to Section 3 Evaluating VLMs in multi-agent environments."
        },
        {
            "title": "C Strategic reasoning evaluation details",
            "content": "In this section, we describe how the dataset for each environment is generated. Our dataset is available at https://huggingface.co/datasets/zelaix/VS-Bench. C.1 Hanabi We generate dataset consisting of 400 Hanabi samples to evaluate the strategic reasoning ability of VLMs. The dataset is generated in three steps. First, to include data with different gameplay levels, 17 90% of the samples are obtained through mutual prediction between two reasoning models, while the remaining 10% are generated by chat model predicting reasoning models move. We chose doubao-1-5-thinking-pro and doubao-1-5-vision-pro, both of which demonstrated top-tier decision-making performance, to represent the reasoning and chat models, respectively. Second, to balance the ratio of different actions to approximate human gameplay, we analyze the gameplay data of leading VLM agents and set the ratio of <Play>:<Discard>:<Reveal> to 2 : 3 : 4 in our dataset. Third, we balance the agent order and step index in the dataset so that the first and second agents each account for 50% samples and the step index is uniformly distributed across the entire game sequence. C.2 Overcooked We generate dataset consisting of 400 Overcooked samples to evaluate the strategic reasoning ability of VLMs. This dataset is derived from the human experiment data provided by Overcooked-AI [12], which comprises game trajectories recorded from multiple human participants. We focus on the trial-train subset of these data, with instances of invalid actions filtered out from the trajectories. We then use random sampling on these filtered trajectories to ensure comprehensive coverage of possible game states. Each sample comprises sequence of four consecutive game frames. Additionally, we apply constraints to simulate realistic game scenarios and control the distribution of target actions. Specifically, the proportion of <STAY> action among the possible actions is limited to 10%. The dataset is balanced for two chefs, each accounting for 50% samples of the dataset. C.3 Breakthrough We generate dataset consisting of 400 Breakthrough samples to evaluate the strategic reasoning ability of VLMs. All samples are generated by minimax algorithm with alphabeta pruning, widely adopted baseline in Breakthrough research [41, 59]. Since minimax search does not always reach terminal positions to determine winloss outcomes, we implement state evaluation function: upon reaching fixed search depth, we compute the difference between the maximum effective forward advancement of our deepest piece and that of the opponents deepest piece, then normalize this difference to obtain reward for the state. We configure minimax agents with maximum search depths for the first and second players as (3, 4), (3, 5), (4, 5), (4, 6), (4, 4), and (5, 5), respectively, and sample step indices uniformly to ensure coverage of diverse game states. C.4 Kuhn poker We generate dataset consisting of 400 Kuhn Poker samples to evaluate the strategic reasoning ability of VLMs. Kuhn Poker has continuum of mixed Nash equilibria parameterized by single probability α [0, 1/3], which denotes the likelihood of betting when holding Jack [32]. We consider all pairwise matches among three values of α = 0, 1/6, 1/3, resulting in nine distinct strategy combinations. For each combination, we simulate 600 head-to-head games and uniformly sample total of 400 game states to construct the final dataset. C.5 Atari Pong We generate dataset consisting of 400 Pong samples to evaluate the strategic reasoning ability of VLMs. We uniformly sample 400 state transitions from logged trajectories of two best-performing models in the decision-making process, namely o4-mini and doubao-1-5-thinking-pro, using the next actions of these VLM agents as ground truth. We then modify the prompts to ask VLMs to control the left paddle (the built-in bots paddle) and predict those actions. C.6 Coin Dilemma We generate dataset consisting of 400 Coin Dilemma samples to evaluate the strategic reasoning ability of VLMs. We consider two types of heuristic strategies for playing Coin Dilemma and generate the dataset by simulating game play with these strategies: 1. Common welfare: the agent only collects the coin of its own color. 2. Self interest: the agent always collects the closest coin, regardless of the color. Concretely, we collect samples from six settings, resulting in dataset of 400 samples: 1. Common welfare + common welfare: collect 100 samples. 2. Self interest + self interest: collect 100 samples. 3. Common welfare + self interest: collect 50 samples. 4. Self interest + common welfare: collect 50 samples. 5. Random + self interest: collect 50 samples. 6. Self interest + random: collect 50 samples. For Coin Dilemma, different actions can lead to the same transition in the environment. Therefore, the actions with the same outcome as the ground truth action are all considered correct. For example, if player is at the top-left corner of the grid map, then actions <UP> and <LEFT> are both considered correct with ground truth <STAY> as they all result in no movement of the player. C.7 Monster Hunt We generate dataset consisting of 400 Monster Hunt samples to evaluate the strategic reasoning ability of VLMs. We consider four types of heuristic strategies for playing Monster Hunt and generate the dataset by simulating game play with these strategies: 1. Common welfare 1: the agent always moves towards the monster. 2. Common welfare 2: the agent first moves to the middle block of the grid map and stays there to wait for the other agent and the monster. 3. Common welfare 3: the agent first moves to corner of the grid map and stays there to wait for the other agent and the monster. 4. Self interest: the agent always moves towards the closest apple. Concretely, we collect samples from six settings, resulting in dataset of 400 samples: 1. Common welfare 1 + common welfare 1: collect 80 samples. 2. Common welfare 2 + common welfare 2: collect 80 samples. 3. Common welfare 3 + common welfare 3: collect 80 samples. 4. Self interest + self interest: collect 80 samples. 5. Random + self interest: collect 40 samples. 6. Self interest + random: collect 40 samples. For Monster Hunt, different actions with the same outcome as the ground truth action are all considered correct, as in Coin Dilemma. C.8 Battle of the Colors We generate dataset consisting of 400 Battle of the Colors samples to evaluate the strategic reasoning ability of VLMs. We consider four types of heuristic strategies for playing Battle of the Colors and generate the dataset by simulating game play with these strategies: 1. Common welfare: the agent always moves to the closest color block (to both players) and stays there to wait for the other player. 2. Self interest: the agent always moves to the block of its own color. 3. Biased red: the agent always moves to the red block. 4. Biased blue: the agent always moves to the blue block. Concretely, we collect samples from six settings, resulting in dataset of 400 samples: 1. Common welfare + common welfare: collect 100 samples. 19 2. Self interest + self interest: collect 100 samples. 3. Common welfare + self interest: collect 50 samples. 4. Self interest + common welfare: collect 50 samples. 5. Biased red + biased red: collect 50 samples. 6. Biased blue + biased blue: collect 50 samples. For Battle of the Colors, different actions with the same outcome as the ground truth action are all considered correct, as in Coin Dilemma. <BACK> to Section 3.1 Strategic reasoning. Decision-making evaluation details D.1 Hanabi In Hanabi, we consider two kinds of return. Standard return. This is the standard return in existing Hanabi environments [7], and we use it in the main text results. If all life tokens are consumed before the fireworks are completed, the agents get reward of 0. If all fireworks stacks are built successfully, the agents get return of 25. If the above two early terminal conditions are not reached and the deck is exhausted, the game continues for one additional round, and the return is the sum of the ranks of the fireworks. Firework return. Since many VLM agents often consume all the life tokens and result in zero return, we also consider another return that relaxes the zero-out penalty upon losing all life tokens. More specifically, if all life tokens are consumed before the fireworks are completed, the return is the sum of the ranks of the firework piles. This return measures the progress of the fireworks and is reported in the raw decision-making results without normalization in Table 4. For each model, we perform 10 self-play games and report the average standard return and the firework return. These results are then normalized and compared with random baseline and an optimal policy as reported in Multi-Agent PPO (MAPPO) [84]. D.2 Overcooked In Overcooked, each episode is limited to 50 steps. Within these steps, two chefs cooperatively cook soup and deliver the cooked soup to the service desk. The two agents share common return, which includes three process rewards and delivery reward. Process rewards. To measure the progress of making soup, the agents get 2 reward for each of the three events: (1) An agent successfully adds an onion to cooking pot; (2) An agent picks up dish when pot contains onions or cooking is in progress; (3) An agent successfully plates finished soup using dish. Completing three-onion soup leads to total of 10 process rewards. Delivery reward. To align with the games goal of successfully delivering specified dishes, the agents get 10 reward upon successful delivery to the service desk. For the 3-onion soup recipe, the total return for successfully completing and delivering one soup is 20, including process reward of 10 and final reward of 10. We evaluate each VLM for 10 episodes of self-play, where both agents use the same model. We report the agents accumulated reward as the episode return. We further normalize these scores with respect to the returns of the random agents and the optimal agents. The optimal agents can complete 2 deliveries in single episode. D.3 Breakthrough In Breakthrough, the return is determined by the final outcome: the winning agent gets reward of +1, the losing agent gets reward of 1, and draws are not possible. We selected moderately strong MCTS agent as our baseline, configured with an exploration constant = 2.0, maximum of 100 simulations per move, and rollout count of 10. Each model is evaluated for 20 games against this MCTS agent, with 10 games as the first agent and 10 games as the second agent. For the optimal policy, we use minimax agent with alphabeta pruning and maximum search depth of 5, using 20 state evaluation function as described in Appendix C.3. Although minimax is not guaranteed to be optimal for Breakthrough, it achieved perfect win rate against the MCTS agent in our experiment, making it reasonable choice as the optimal policy in this study. D.4 Kuhn poker In Kuhn Poker, the return of each agent is the net chips won or lost at the end of the game. We consider Nash equilibrium (NE) strategy as described in Appendix C.4 with α = 0. Each model is evaluated for ten runs of 120 games against this NE agent, with 60 games as the first agent and 60 games as the second agent. We then calculate the mean and standard deviation of the ten runs and normalize the result. The optimal agent in Kuhn poker is the agent with the NE strategy. D.5 Atari Pong In Atari Pong, we adopt frame stacking of 4 frames to pass dynamic information to the VLM agent. We also employ sticky action probability of 0.25 and perform random number (between 1 and 30) of <STAY> steps at the beginning of an episode to achieve randomness. These settings have been common practice in related works, such as DQN [46]. We also consider two kinds of reward. Score reward. The agent scores one point when the ball passes the opponents paddle and gets score reward of +1. The game ends when an agent scores three points. Step reward. As many VLMs fail to score even one point against the built-in bot, the score reward alone becomes too sparse to distinguish the performance of different models. Therefore, we design continuous reward proportional to the number of steps that VLMs survived against the built-in bot. We evaluate each VLM for 10 episodes against the built-in bot, and the overall return is the weighted sum of the normalized score reward with 0.9 weight and the normalized step reward with 0.1 weight. The optimal agent is an RL agent that scores three points against the built-in bot. D.6 Coin Dilemma In Coin Dilemma, the agents receive rewards on different game events: 1. Red agent collects red coin: red agent gets +1 reward. 2. Red agent collects blue coin: red agent gets +1 reward, blue agent get 2 reward. 3. Blue agent collects blue coin: blue agent gets +1 reward. 4. Blue agent collects red coin: blue agent gets +1 reward, red agent get 2 reward. We evaluate the return of each VLM for 10 episodes of self-play, where the red and blue agents use the same model. We further normalize these returns with respect to the returns of the random agents and the optimal agents that always move towards the coin of their own color. D.7 Monster Hunt In Monster Hunt, the agents receive rewards on different game events: 1. Red agent eats an apple: red agent gets +2 reward. 2. Blue agent eats an apple: blue agent gets +2 reward. 3. Red agent encounters the monster alone: red agent gets 2 reward. 4. Blue agent encounters the monster alone: blue agent gets 2 reward. 5. Both agents defeat the monster together: both agents get +5 reward. We evaluate the return of each VLM for 10 episodes of self-play, where the red and blue agents use the same model. We further normalize these returns with respect to the returns of the random agents and the optimal agents that always move to the middle block and stay there to wait for the monster. 21 Models Optimal Cooperative Comptitive Mixed-Motive Hanabi standard5 firework6 Overcooked Board Poker Pong score 7 step8 Dilemma Hunt Battle 24.0 24.0 40.0 1.0 0.0 1. 398.0 14.2 92.2 29.9 o4-mini 10.37.3 13.32.9 doubao-1-5-thinking-pro 13.65.5 14.14.0 6.58.6 10.75.3 9.73.9 1.65.1 4.92.9 0.00. gemini-2.5-flash claude-3-7-sonnet qvq-max gemini-2.5 w/o thinking gpt-4.1 qwen-vl-max grok-2-vision claude-3-7 w/o thinking doubao-1-5-vision-pro 0.00.0 0.00.0 0.30.5 0.00.0 0.00.0 0.00.0 Qwen2.5-VL-72B-Ins. InternVL3-78B 0.20.4 0.00.0 Llama-3.2-90B-Vision-Ins. 0.00.0 3.81.6 3.61.4 0.30.5 1.61.0 2.90.9 4.61. 0.20.4 2.41.0 1.21.6 7.02.7 0.41.0 0.00.0 1.60.2 205.291.0 0.85.0 14.310.5 1.21.6 6.812.1 1.41.4 4.21.9 0.80.4 0.00.0 1.50.0 230.572.6 0.01.0 15.66.6 9.92.9 3.62.1 0.60.5 0.10.1 1.50.0 194.453.2 1.34.8 4.21.4 0.60.8 0.00.0 1.50.0 133.727.8 0.53.4 0.91.4 9.45.7 1.01.3 0.90.3 0.10.1 1.50.0 158.246.9 0.10.7 9.45.9 0.00.0 1.01.6 1.00.0 0.10.1 1.50.0 175.941.5 0.21.5 9.410.2 0.91.1 0.00.0 1.00.0 0.10.0 1.50.0 151.841.5 2.42.7 0.30.6 0.00.0 1.00.0 0.10.1 1.50.0 139.729.3 0.10.7 2.919.9 0.00.0 0.81.3 1.00.0 0.10.1 1.60.2 152.645.9 0.12.3 10.58.3 0.30.6 1.01.6 0.90.6 0.10.1 1.50.0 121.48.4 0.12.7 9.99.1 0.50.7 0.00.0 1.00.0 0.10.1 1.50.0 121.48.4 0.42.0 2.48.8 0.00.0 0.98.4 0.00.0 1.00.0 0.10.1 1.50.0 123.84.6 0.10.9 9.125.4 0.00.0 0.20.6 1.00.0 0.10.1 1.50.0 121.48.4 0.83.3 11.99.5 0.20.5 0.81.3 1.00.0 0.20.1 1.50.0 121.48.4 0.11.1 6.67.0 0.50.7 Random 0.0 1.2 0.2 1.0 0.1 1. 147.2 0.1 10.1 0.2 Table 4: Raw decision-making results without normalization. D.8 Battle of the Colors In Battle of the Colors, the agents receive rewards on different game events: 1. Both players on the red block: red agent gets +2 reward, blue agent gets +1 reward. 2. Both players on the blue block: blue agent gets +2 reward, red agent gets +1 reward. 3. Two players on different blocks: both agents get 0 reward. We evaluate the return of each VLM for 10 episodes of self-play, where the red and blue agents use the same model. We further normalize these returns with respect to the returns of the random agents and the optimal agents that always move to the closest block to the two players. D.9 Raw results without normalization The decision-making results in all environments without normalization are listed in Table 4. We also report the firework return for Hanabi and the score reward and step reward for Atari Pong. <BACK> to Section 3.2 Decision-making."
        },
        {
            "title": "E Additional experiment results",
            "content": "E.1 Multimodal observation results We select cooperative card game Hanabi, competitive board game Breakthrough, and mixedmotive video game Monster Hunt, and evaluate the performance of fourteen VLMs with multimodal observations and text-only observations. The evaluation results of strategic reasoning are shown in Table 5 and the evaluation results of decision-making are shown in Table 6. In general, most VLMs achieve better results with text-only observations in both strategic reasoning and decisionmaking. However, multimodal observations provide richer information and should lead to at least the same performance. These results demonstrate that VLMs can fail to extract visual information in multimodal observations and improve strategic reasoning and decision-making performance. <BACK> to Section 4.1 Multimodal observations. 5Hanabi standard return is described in Appendix D.1 6Hanabi firework return is described in Appendix D.1 7Atari Pong score reward is described in Appendix D.5 8Atari Pong step reward is described in Appendix D.5 22 Model Optimal Hanabi Text-Only Multimodal CoT Breakthrough Text-Only Multimodal CoT Monster Hunt Text-Only Multimodal CoT 100.0 100.0 100.0 100. 100.0 100.0 100.0 100.0 100.0 o4-mini doubao-1-5-thinking-pro gemini-2.5-flash claude-3-7-sonnet qvq-max gemini-2.5 w/o thinking gpt-4.1 qwen-vl-max claude-3-7 w/o thinking grok-2-vision doubao-1-5-vision-pro Qwen2.5-VL-72B-Ins. InternVL3-78B Llama-3.2-90B-Vision-Ins. Random 53.8 34.5 42.0 45.0 41.0 24.5 40.0 17.0 19.2 23.8 19.5 18.5 26.8 26. 8.8 58.2 32.8 37.0 39.0 32.2 21.5 23.0 26.5 9.8 12.8 15.0 26.8 25.2 20.0 8.8 N/A N/A N/A N/A N/A 24.0 49.8 20.0 32.8 22.5 25.2 22.2 20.5 14.8 8.8 27.5 23.5 23.5 25.0 27.5 20.5 20.5 19.0 19.2 14.0 17.2 19.2 17.5 6. 4.3 26.8 19.8 23.2 24.2 21.8 14.8 22.5 19.5 18.0 10.8 15.8 23.8 14.0 11.8 4.3 N/A N/A N/A N/A N/A 21.5 27.5 17.2 19.0 18.2 16.8 16.5 16.0 14.0 4.3 47.8 57.2 49.0 56.0 48.2 40.2 41.0 31.8 37.0 42.5 33.2 32.2 34.2 36. 29.3 43.8 45.2 50.2 42.5 21.5 30.0 36.8 23.5 26.0 31.5 36.0 27.2 30.0 26.2 29.3 N/A N/A N/A N/A N/A 45.0 34.5 35.5 43.8 33.2 39.2 37.5 37.8 27.8 29.3 Table 5: Strategic reasoning results on multimodal input and CoT prompting. Model Optimal Hanabi Text-Only Multimodal CoT Breakthrough Text-Only Multimodal CoT Monster Hunt Text-Only Multimodal CoT 100.0 100.0 100.0 100.0 100. 100.0 100.0 100.0 100.0 o4-mini 37.126.1 42.930.5 doubao-1-5-thinking-pro 37.532.9 56.722.8 40.821.9 27.136.0 33.835.8 6.721.1 0.00.0 0.00. gemini-2.5-flash claude-3-7-sonnet qvq-max N/A N/A N/A N/A N/A 30.047.0 30.047.0 15.037.0 10.021.0 30.042.3 20.025.7 45.050.0 20.039.8 5.015.8 5.015.8 N/A N/A N/A N/A N/A 23.24.2 24.98.2 27.66.1 17.211.3 15.634.6 26.25.8 19.93.5 27.34.6 0.74.5 15.15.7 N/A N/A N/A N/A N/A gemini-2.5 w/o thinking gpt-4.1 qwen-vl-max claude-3-7 w/o thinking grok-2-vision doubao-1-5-vision-pro 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 Qwen2.5-VL-72B-Ins. InternVL3-78B 6.26.6 0.00.0 Llama-3.2-90B-Vision-Ins. 0.00.0 0.00.0 0.00.0 1.22.0 0.00.0 0.00.0 0.00.0 0.81.8 0.00.0 0.00. 0.00.0 3.310.5 0.00.0 0.00.0 5.015.8 0.00.0 5.015.8 0.00.0 0.00.0 0.00.0 5.05.0 10.021.0 20.039.8 5.14.9 0.00.0 10.031.5 13.06.8 0.00.0 0.00.0 0.00.0 5.015.8 10.031.5 15.05.8 0.00.0 0.00.0 5.031.5 0.00.0 0.78.9 6.39.8 11.25.6 18.510.9 14.17.3 13.220.2 0.68.3 12.48.6 0.28.2 3.03.9 5.15.3 0.45.8 16.215.0 8.16.3 7.88.8 2.96.2 1.75.2 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. 0.00.0 0.00.0 0.00.0 16.29.6 19.625.7 23.322.9 8.27.6 4.83.9 1.89.2 3.08.8 3.64.9 5.53.7 Random 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0. Table 6: Decision-making results on multimodal input and CoT prompting. E.2 Test-time scaling results We consider the same three games as in multimodal observation experiments and evaluate the performance of six chat VLMs and three open-source VLMs with IO prompting and Chain-ofThought (CoT) prompting [76]. We do not consider CoT prompting for reasoning models because they already generate step-by-step reasoning by themselves. The evaluation results of strategic reasoning are shown in Table 5, and the evaluation results of decision-making are shown in Table 6. In general, CoT prompting leads to much better performance, which shows that test-time scaling methods like reasoning and CoT prompting can substantially improve VLMs performance. <BACK> to Section 4.2 Test-time scaling. E.3 Social behaviors results For Coin Dilemma, the social behaviors of different models are shown in Fig. 6a, 6d, and 6g. Compared to reasoning models, chat models shows inferior performance in collecting coins, resulting in fewer numbers of both cooperation and defections. The best-performing chat model, GPT-4.1, achieves more cooperation over defections, indicating behavior pattern that favors common welfare over self-interest. The open-source model InternVL3-78B shows similar behavior pattern. For Monster Hunt, the social behaviors of different models are shown in Fig. 6b, 6e, and 6h. For reasoning models, o4-mini, gemini-2.5-flash, and claude-3-7-sonnet prefer eating ap23 (a) Reasoning VLMs in Dilemma. (b) Reasoning VLMs in Hunt. (c) Reasoning VLMs in Battle. (d) Chat VLMs in Dilemma. (e) Chat VLMs in Hunt. (f) Chat VLMs in Battle. (g) Open-source VLMs in Dilemma. (h) Open-source VLMs in Hunt. (i) Open-source VLMs in Battle. Figure 6: Social behaviors of all models in mixed-motive social dilemma games. Dimensions are agents behaviors described in Sec. 2.4. ples alone; doubao-1-5-thinking-pro-m prefers cooperatively defeating the monster; qvq-max performs little better than random. For chat models, gpt-4.1 prefer eating apples alone; doubao-1-5-vision-pro and qwen-vl-max prefer cooperatively defeating the monster; the other models perform little better than random. For open-source models, Qwen2.5-VL-72B-Instruct prefers cooperatively defeating the monster; the other models perform little better than random. For Battle of the Colors, the social behaviors of different models are shown in Fig. 6c, 6f, and 6i. Only gemini-2.5-flash is able to achieve considerable numbers of game events, demonstrating strong decision-making ability. As shown in Fig. 6c, gemini-2.5-flash demonstrates strong willingness to cooperate with others, even if this leads to higher reward for the other agent. <BACK> to Section 4.3 Social behaviors. 24 Visual observation: VLM response: ... Now, Player 1's possible actions: play, discard, or reveal. Let's consider playing first. The fireworks for Red have 1, so the next needed is Red 2. Player 1 has R2 (card 3), which is the next in Red. Playing that would be valid. Green needs 1 first, but Player 1 has G2, which can't be played yet since Green's 1 isn't down. The 5s (R5, B5) can't be played until their sequences are up to 4. B3 is Blue 3, but Blue needs 1 first. So the valid play here is R2 (card 3) since Red is at 1, and R2 is the next. ... Figure 7: Failure case example of strategic reasoning in Hanabi. Visual observation: frame 1 VLM response: frame 2 frame frame 4 ... Actually, let me look more carefully at what chef_0 is doing. In frames 23, 24, and 25, chef_0 is standing in the same position and hasn't moved. In frame 26, the pot shows black strip, indicating the soup is fully cooked. According to the rules, after the soup is cooked for 5 timesteps, the next step is to plate it using dish. Since chef_0 is directly in front of the pot, the most likely action would be <INTERACT> to take the soup from the pot. However, this would require chef_0 to have dish already. Since don't clearly see chef_0 holding dish, they would first need to get one. Given this analysis, believe chef_0 will most likely choose <INTERACT> to interact with the pot. ... Figure 8: Failure case example of reasoning in Overcooked."
        },
        {
            "title": "F Failure case examples",
            "content": "F.1 Strategic reasoning We present three illustrative failure cases in strategic reasoning from different game environments. In Hanabi, VLM agents only observe the other agents hands but not their own hands, creating distinct information asymmetry. An example with visual observation and the VLMs response is shown in Fig. 7. When predicting the other agents next move, VLMs often immediately identify which visible card could yield points and assume the other agent will play it. This prediction, however, overlooks critical fact: from the other agents perspective, their information about that card is 25 Visual observation: VLM response: ... If the red player moves <UP>, it collects an apple, and gets +2 points. The red player's best move to maximize their own score is to go up. ... Figure 9: Failure case example of strategic reasoning in Monster Hunt. Visual observation: VLM response: ... Now, considering optimal play, White would want to advance their pieces towards Black's side, possibly creating threats or blocking Black's moves. The b4 piece is already advanced. Moving it to c5 (b4c5) would bring it closer to the center and maybe set up capture or block. Looking at the starting row pieces (d1 to h1), moving them straight forward (like d1d2, e1e2, f1f2, g1g2, h1h2) or diagonally. Straight moves are safe but maybe less aggressive. Diagonal moves could potentially set up captures later. ... Figure 10: Failure case example of decision-making in Breakthrough. incomplete, so they would not risk losing life token by playing it prematurely. Instead, they would wait for more definitive clues before making that play. In Overcooked, accurately predicting the next action of the other agent requires robust visual perception capabilities and thorough comprehension of image information. An example with visual observation and the VLMs response is shown in Fig. 8. In this case, the VLM should predict the blue chefs action from the green chefs perspective. Despite correctly identifying from historical frames that the soup was cooked, the VLMs visual perception is inadequate: it fails to recognize that the blue chef was already holding the soup, plated in dish. In addition, the VLM overlooks critical game rule, mistakenly believing that soup could be collected from the pot before dish was acquired. As result of these combined deficiencies in visual understanding and rule application, the VLM predicts the erroneous action <INTERACT>. In Monster Hunt, in order to predict the next action of the other player, it is important to track the positions and movements of all game elements, rather than focusing on only part of the game mechanics and make short-sighted predictions. An example with visual observation and the VLMs response is shown in Fig. 9. The VLM notices the apple is just one step above the red player, but does not pay attention to the monster on top of the apple. The VLM predicts the red player to go up to collect the apple, while the red player actually goes down to flee from the monster. F.2 Decision-making We also present three illustrative failure cases in decision-making from different game environments. Frame 1 Frame 2 Frame 3 Frame 1 Figure 11: Failure case example of decision-making in Overcooked. Frame Frame 2 Frame 3 Frame 4 Figure 12: Failure case example of decision-making in Atari Pong. In Breakthrough, strong strategy must excel at both offense and defense. However, VLMs occasionally overlook situations in which the opponents pieces are on the verge of victory and require immediate defensive intervention. Instead, they persistently advance their most distant piece by one step, thereby neglecting the critical balance between attack and defense. An example with visual observation and the VLMs response is shown in Fig. 10. In Overcooked, to correctly prepare soup with three onions, the VLM agent needs to accurately manage the number of onions. common failure mode of VLMs is that they often neglect these precise quantity requirements. As illustrated in Fig. 11, the green chef initiates cooking by performing an <INTERACT> operation after adding only the second onion for this recipe. Concurrently, the blue chef prematurely begins to retrieve plate. As consequence, the dish, even when served, fails to earn the delivery reward because the ingredient count requirement is not met. In Atari Pong, in order to successfully intercept the ball, the VLM agent needs to accurately predict the landing point of the ball. common failure mode of the VLMs is that they may over-adjust their paddle. As illustrated in Fig. 12, the VLM on the right detects that the ball is moving downward and then move the paddle down, but misses the landing point due to over-aggressive adjustment. <BACK> to Section 4.4 Failure cases. VS-BENCH MINI For preliminary evaluations of VLMs, we introduce set of simpler games named VS-BENCH MINI, which comprises one cooperative game, Tiny-Hanabi, and one competitive game, Tic-Tac-Toe. Tiny Hanabi is simplified version of the full game with only two colors, three ranks, and hand size of three cards. The players have eight information tokens to convey hints and three life tokens to be penalized for misplays. This simplified setting provides preliminary evaluation of VLMs ability in cooperative multi-agent environments. An example visual observation is shown in Fig. 13. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/Hanabi_(card_game). Tic-Tac-Toe is competitive board game played by two-players on 3 3 grid. The first player to align three of their marks horizontally, vertically, or diagonally wins the game. This game has relatively small state space and known optimal policies that always lead to draw. Therefore, we use it as preliminary evaluation of VLMs ability in competitive multi-agent environments. 27 Figure 13: Tiny Hanabi. Figure 14: Tic-Tac-Toe. An example visual observation is shown in Fig. 14. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/Tic-tac-toe. <BACK> to Section 2 VS-BENCH environments."
        },
        {
            "title": "H Environment details",
            "content": "H.1 Hanabi Game rules. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/ Hanabi_(card_game). 1. Hanabi is cooperative card game for 2 players. 2. The deck consists of 5 colors: (Red), (Yellow), (Green), (White), B(Blue), with ranks ranging from 1 to 5. Each color contains 10 cards: three of rank 1, two each of rank 2 through 4, and one of rank 5, for total of 50 cards. 3. Each player holds 5 cards in hand. 4. There are 8 Info tokens (used to give hints) and 3 Life tokens (penalties for misplays). 5. As in blind mans bluff, players can see each others cards but they cannot see their own. Play proceeds around the table; each turn, player must take one of the following actions: <PLAY>, <DISCARD i>, <REVEAL color c>, <REVEAL rank r>. 6. The game ends immediately when either all Life tokens are used up, resulting in game loss with score of 0, or when all 5s have been successfully played, resulting in game win with score of 25. Otherwise, the game continues until the deck runs out and one final round is completed. At the end of the game, the final score is calculated as the sum of the highest card played in each suit, up to maximum of 25 points. Visual observation. An example is shown in Fig. 15. The visual observation has four parts: 1. Basic Information: counts of remaining life tokens, information tokens, and deck size. 2. Historical Information: all discarded cards, together with each players two most recent actions. 3. Fireworks: current progress of each colors fireworks stack, indicating the highest played rank. 4. Players Hands: each players hand, with annotations beneath each card showing all possible colors and ranks deduced from received hints. <BACK> to Section 2.2 Cooperative games. 28 Figure 15: Hanabi. Figure 16: Overcooked. H.2 Overcooked Game rules. HumanCompatibleAI/overcooked_ai. Detailed game descriptions can be found in https://github.com/ 1. Overcooked is cooperative game where two chefs collaborate to cook and serve soups in 50 timesteps. 2. The chefs can move in the available area and cannot move to the counter. 3. The chefs can interact with the object on the tile that they are facing. 4. soup is cooked in the following steps: a. Pick up (interact) 1 onion and place (interact) it in the pot. b. After placing 3 onions in the pot, open (interact) the pot and cook for 5 timesteps. The pot will show how long the soup has been cooked. c. When the pot shows the number 5, the soup is finished. Pick up (interact) dish to plate (interact) the soup. d. Deliver the soup and put (interact) it on the serving location. Visual observation. An example is shown in Fig. 16. On the left is the current game state, showing the overall kitchen layout, the positions and orientations of both chefs, and the items they hold. On the right is legend explaining the visual representations of game elements, such as objects and chef orientations, used in the game state. <BACK> to Section 2.2 Cooperative games. H.3 Breakthrough Game rules. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/ Breakthrough_(board_game). 1. Breakthrough is two-player strategy game played on an 8x8 grid. 2. Each player controls pieces of color: White or Black. White starts at the bottom (rows 1 and 2), while Black starts at the top (rows 7 and 8). 3. If White moves piece to row 8, White wins the game. Conversely, if Black moves piece to row 1, Black wins the game. 4. Players alternate turns, moving one piece per turn, with Black going first. 5. piece may only move one space straight or diagonally forward, and only if the destination square is empty. 29 Figure 17: Breakthrough. Figure 18: Kuhn Poker. Figure 19: Atari Pong. 6. piece may only capture an opponents piece by moving one space diagonally forward into its square. In this case, the opponents piece is removed, and your piece takes its place. 7. Black moves forward by decreasing row indices (downward), while White moves forward by increasing them (upward). 8. Moves are specified by their start and end positions. For example, a2a3 indicates moving piece from a2 (column a, row 2) to a3 (column a, row 3). 9. The board is labeled with columns a-h and rows 1-8. Thus, h8 is the top-right corner, and a1 is the bottom-left corner. Visual observation. An example is shown in Fig. 17. The figure illustrates the current positions of both black and white pieces on the board. Row and column indices are annotated on the left and bottom sides of the image, respectively. <BACK> to Section 2.3 Competitive games. H.4 Kuhn Poker Game rules. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/ Kuhn_poker. 1. Kuhn poker is two-player card game. The deck includes only three cards: King (K) > Queen (Q) > Jack (J). 2. At the start of each game, both player 0 and player 1 place 1 chip into the pot as blind ante. 3. Each player is dealt card as private information, and the third card is set aside unseen. 4. The two players take turns acting, starting with player 0. player can choose to: a. <PASS>: place no additional chips into the pot. b. <BET>: place 1 additional chip into the pot. 5. If player chooses to <PASS> after the other players <BET>, the betting player wins the pot. 6. If both players choose to <PASS> or both players choose to <BET>, the player with the higher card wins the pot. Visual observation. An example is shown in Fig. 18. Each player receives visual representation of their private card and the current chips in the pot. <BACK> to Section 2.3 Competitive games. H.5 Atari Pong Game rules. Detailed game descriptions can be found in https://en.wikipedia.org/wiki/ Pong. 1. Atari Pong is zero-sum game played on 2D screen with two players (left and right) and ball. 2. Each player controls paddle and receives rewards on different events: 30 Figure 20: Coin Dilemma. Figure 21: Monster Hunt. a. If the ball passes your paddle: the opponent +1 point. b. If the ball passes the opponents paddle: you +1 point. 3. The ball bounces off the top/bottom walls and the paddles. 4. Paddles can only move vertically within the top and bottom walls. 5. First player to score 3 points wins. Visual observation. An example is shown in Fig. 19. The two players each control paddle on the side of the screen to hit ball back and forth with each other. The paddles are vertical rectangles, and the ball is white square. The players score if the ball passes their opponents paddle. The built-in bot controls the left paddle, while the VLM agent controls the right paddle. The scores of both players are displayed at the top of the screen. <BACK> to Section 2.3 Competitive games. H.6 Coins Dilemma Game rules. Detailed game descriptions can be found in [35]. 1. Coin Dilemma is general-sum game played on 5x5 grid board with two players (red and blue) and two types of coins (red and blue). 2. Players receive rewards on different events: a. player collects one coin of its own color: the player +1 point. b. player collects one coin of the other players color: the player +1 point, the other player -2 points. 3. New coins spawn randomly on the board after each collection. Visual observation. An example is shown in Fig. 20. On the left of the image is grid map showing the current positions of all game elements, including two players (red and blue) and two coins (red and blue). The players are each represented by Pac-Man icon, and the coins are each represented by coin icon. On the right of the image is table demonstrating the rewards of each event and corresponding counter tracking the number of occurrences for that event. <BACK> to Section 2.4 Mixed-motive games. H.7 Monster Hunt Game rules. Detailed game descriptions can be found in [53]. 1. Monster Hunt is general-sum game played on 5x5 grid board with two players (red and blue), one monster, and two apples. 2. The monster moves towards the closest player in each step. 3. Players move in the grid-world and receive rewards on different events: a. One player eats an apple: the player +2 points and the apple respawns at random position. b. One player encounters the monster alone: the player -2 points and respawns at random position. 31 c. Two players defeat the monster together: both players +5 points and the monster respawns at random position. Visual observation. An example is shown in Fig. 21. On the left of the image is grid map showing the current positions of all game elements, including two players (red and blue), two apples, and monster. The players are each represented by Pac-Man icon, the apples are each represented by green apple icon, and the monster is represented by black demon icon. On the right of the image is table demonstrating the rewards of each event and corresponding counter tracking the number of occurrences for that event. <BACK> to Section 2.4 Mixed-motive games. H.8 Battle of the Colors Game rules. 1. The Battle of the Colors is general-sum game played on 5x5 grid board with two players (red and blue) and two types of blocks (red and blue). 2. Players receive rewards on different events: a. When both players are on red block: red player +2 points, blue player +1 point, and the red block will be refreshed to new random position. b. When both players are on blue block: red player +1 point, blue player +2 points, and the blue block will be refreshed to new random position. c. When players are on different blocks: both players +0 points, and both blocks will be refreshed to new random positions. Visual observation. An example is shown in Fig. 22. On the left of the image is grid map showing the current positions of all game elements, including two players (red and blue) and two colored blocks (red and blue). The players are each represented by Pac-Man icon. On the right of the image is table demonstrating the rewards of each event and corresponding counter tracking the number of occurrences for that event. <BACK> to Section 2.4 Mixed-motive games. Figure 22: Battle of the Colors."
        }
    ],
    "affiliations": [
        "Beijing Zhongguancun Academy",
        "Shanghai Qi Zhi Institute",
        "Tsinghua University",
        "University of Science and Technology of China"
    ]
}