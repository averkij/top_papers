{
    "paper_title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence",
    "authors": [
        "Kun Ouyang",
        "Yuanxin Liu",
        "Linli Yao",
        "Yishuo Cai",
        "Hao Zhou",
        "Jie Zhou",
        "Fandong Meng",
        "Xu Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 7 4 0 2 . 0 1 5 2 : r Conan: Progressive Learning to Reason Like Detective over Multi-Scale Visual Evidence Kun Ouyang1, Yuanxin Liu1, Linli Yao1, Yishuo Cai1, Hao Zhou2, Jie Zhou2, Fandong Meng2, Xu Sun1* 1State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University 2WeChat AI, Tencent Inc., China kunouyang10@gmail.com, xusun@pku.edu.cn Figure 1. Top: The evidence reasoning processes of our proposed Conan. Bottom: Quantitative comparison with other models, such as the advanced GPT-4o [11], across six multi-step reasoning benchmarks"
        },
        {
            "title": "Abstract",
            "content": "Video reasoning, which requires multi-step deduc- *Corresponding Author tion across frames, remains major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on 1 text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design multi-stage progressive cold-start strategy combined with an IdentificationReasoningAction (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to longvideo understanding tasks, validating its strong scalability and robustness. Model: huggingface.co/RUBBISHLIKE/Conan-7B github.com/OuyangKun10/Conan Code: 1. Introduction There is only one truth! Edogawa Conan"
        },
        {
            "title": "Frontier multimodal",
            "content": "large language models (MLLMs) [2, 11, 22, 26] have demonstrated remarkable progress on standard video understanding tasks such as question answering [1], temporal grounding [8], and captioning [23]. However, video reasoning [5, 18] remains substantial challenge. Unlike conventional tasks, video reasoning demands active visual information accumulation across temporal spans and multi-step logical inference to reach well-grounded conclusions. Inspired by the success of reinforcement learning with verifiable rewards [9] (RLVR) in incentivizing reasoning ability of LLMs, recent works [6, 14, 19] have begun extending this paradigm to video reasoning, achieving promising gains. Nevertheless, these approaches primarily rely on puretext reasoning without explicit grounding in visual evidence, often leading to superficial or hallucinated reasoning chains that fail to reflect the actual video content. To integrate visual evidence into the reasoning process, concurrent works [10, 27, 30] introduce the frame retrieval mechanism to enable video chain-of-thought (Video-CoT) reasoning, boosting performance of long-video understanding [25, 31]. However, these approaches usually suffer from inaccurate or implicit evidence localization, yielding unreliable reasoning paths. Additionally, some methods [10, 30] partially rely on benchmark-specific training data (e.g. VideoHolmes [5] and LongVideoReason [3]), making it difficult to disentangle solid reasoning improvements from in-domain overfitting. Motivated by these challenges, we aim to equip MLLMs with multi-step, evidence-grounded video reasoning skills, analogous to how Conan as detective (Figure 1): Specifically, our framework identifies relevant frames at multiple scales (including both contextual and evidence frames) reasons over cross-frame clues to form coherent chains of deduction, and decides whether to draw the final conclusion or continue exploring the video. Achieving this goal raises two core challenges: 1) How to automatically construct high-quality evidence-based reasoning dataset that explicitly captures evidence localization, multi-step deductive reasoning, and confident action decision, and 2) How to design effective training procedure and objectives. To tackle the first challenge, we introduce Conan-91k, large-scale dataset for Conan-style evidence reasoning. Built upon the key-frame identification dataset GenS-Video-150K [28], we develop an automated pipeline to generate interleaved video-text reasoning traces using the advanced reasoning LLM Kimi K2, as illustrated in Figure 2. Each reasoning trace contains three key components: 1) Frame Identification distinguishes between evidence, contextual and irrelevant frames 2) Evidence Reasoning conducts textual reasoning over the question and accumulated visual clues 3) Action Decision decides whether to sample additional frames or reach the final conclusion. 2. Related Work 2.1. Video Reasoning Tasks To address the second challenge, we propose training procedure that combines multi-stage progressive cold-start strategy for SFT with joint IdentificationReasoningAction (AIR) optimization framework for RLVR. Specifically, the progressive cold-start strategy incrementally enhances the models multi-step reasoningstarting from textual reasoning, advancing to multimodal alignment, and culminating in vision-centric deduction. Building upon this foundation, the joint AIR RLVR framework further guides the model to perform multi-step reasoning over multi-scale visual evidence. Together, these components empower Conan to seek, deduce, and act across visual clues, achieving reliable and verifiable multi-step reasoning."
        },
        {
            "title": "Extensive",
            "content": "experiments on six challenging multi-step reasoning benchmarks (e.g. MMR- [5], VRBench [29], [32], Video-Holmes VCRBench [20], LongVideoReason [3], and Human-P&C [13]) demonstrate that Conan consistently surpasses state-of-the-art MLLMs, achieving over 10% accuracy gains over the baseline Qwen2.5-VL-7B-Instruct. Moreover, Conan generalizes effectively to long-video understanding tasks (e.g. LongVideoBench [25], MLVU [31], LVBench [24], and Video-MME [7]), validating strong scalability and robustness. In summary, our contributions are threefold: 1) We introduce Conan-91k, the first large-scale dataset for multi-scale evidence reasoning with evidence difficulty-aware sampling. 2) We propose multi-stage progressive cold-start strategy and joint IdentificationReasoningAction RLVR framework to foster gradual acquisition of evidence-based reasoning skills. 3) We conduct extensive experiments, which demonstrate that Conan achieves state-of-the-art multi-step reasoning performance among frontier MLLMs and also generalizes well to standard long-video understanding. 3 Recent advances in multimodal large language models (MLLMs) such as Qwen2.5-VL [2], KimiVL [22], MiMo-VL [26], and GPT-4o [11], have substantially improved video understanding including captioning [23], question answering [1], and temporal grounding [8]. However, these capabilities mainly reflect perceptual understanding [17], whereas video reasoning [15], which demands multi-hop deduction and causal inference across frames, remains insufficiently explored and evaluated. To address this gap, several benchmarks have been introduced to assess reasoning capabilities of MLLMs, such as Video-Holmes [5], VideoReasonBench [18], MMR-V [32], VRBench [29], and VCRBench [20]. Unlike conventional video understanding tasks focused on recognizing visual content, these benchmarks require models to actively locate, connect, and reason over multiple relevant clues, demanding deeper comprehension of temporal dependencies and causal structures in dynamic visual narratives. 2.2. Video Reasoning Models Inspired by the reasoning advancements of the DeepSeek-R1 [9], several studies [6, 14] adopt reinforcement learning with verifiable rewards [9] (RLVR) to promote video reasoning in MLLMs. While these approaches [6, 14, 19] encourage step-by-step reasoning, most are limited to textonly chain of thought, lacking explicit grounding in visual evidence, which often leads to unverified or hallucinated reasoning. To bridge this gap, concurrent works like Video-MTR [27], and FrameThinker [10] incorporate frame retrieval actions into the reasoning process, enabling dynamic evidence gathering and long-form understanding. Despite great improvement of long video underthey partially depend on standing [24, 25, 31], benchmark-specific training set and still lack reliable evidence identification mechanism, rendering the retrieval actions less reliable. Motivated by this, we aim to develop framework named Conan that incentivizes deductive-like reasoning abilities in MLLMs, combining precise evidence identification, logical multi-step reasoning, and confident action decision towards robust video reasoning. 3. Dataset Construction In this section, we introduce the construction procedure of Conan-91k dataset: (i) Data Collection & Processing, (ii) Reasoning Trace Construction, (iii) Evidence Difficulty-Aware Sampling, (iv) Dataset Statistics. 3.1. Data Collection & Processing We collect source data from the GenS-Video150K [28] dataset, which provides dense frame descriptions, multiple-choice and free-form QA pairs, as well as frame-level relevance scores. Leveraging these relevance scores, we categorize video frames into three types: 1) Evidence frames, which are directly relevant to answering the question; 2) Contextual frames, which offer auxiliary hints that may support the reasoning process; and 3) Irrelevant frames, which bear no relation to the question. This multi-scale frame categorization establishes the foundation for subsequent stepwise reasoning trace simulation. 3.2. Reasoning Trace Construction Starting from the processed data above, we apply an automatic pipeline that constructs Conan-style videotext interleaved reasoning traces with the assistance of strong reasoning LLM, Kimi K2 [21]. Figure 2 a) illustrates the pipeline. The core loop proceeds as follows: We first sample 16 frames uniformly from the raw video and retain each frames type label (evidence, contextual, or irrelevant). If all sampled frames are irrelevant, we select the action Random Frame Sampling, which randomly samples 8 new frames and continue the loop. If some sampled frames are evidence or contextual but the evidence proportion does not exceed predefined retrieval threshold, we select the action Specific Frame Retrieval, which uniformly retrieves 8 frames within specific clips that contain evidence, and continue the loop. If the proportion of evidence frames exceeds the threshold, we terminate the loop by selecting Confident Question Answering as the final action. For every loop iteration, in addition to the sampled frames and the chosen action, we prompt K2 with the frame types, the QA pair, dense frame descriptions, timestamps, and the action decision. K2 generates coherent textual reasoning trace that (a) analyzes the QA and sampled frames, and (b) reaches the chosen action. 3.3. Evidence Difficulty-Aware Sampling (cid:80)m To facilitate progressive training curriculum from simple to complex reasoning cases, we adopt an Evidence Difficulty-Aware Sampling (EDAS) stratIn particular, we define an Evidence Diffiegy. culty Index (EDI) to quantify each samples reasoning complexity based on the proportion and temporal dispersion of evidence frames. Let the evidence ratio be = m/N , where and denote the numbers of evidence and total frames, respectively. The temporal variance of evidence frames i=1 (xi-x)2, where xi is computed as Var = 1 represents the temporal position of i-th evidence frame, and is the mean position of all evidence frames. The overall difficulty is then defined as EDI = (1 ) Var, where higher EDI values indicating sparser evidences and more challenging reasoning scenarios. Utilizing this metric, we adopt curriculum-aligned sampling scheme: for SFT, we select 60k samples with 70% having EDI<0.5, while for RLVR, we select 31k samples with 70% having EDI>0.5. This progressive sampling design ensures smooth transition from low-difficulty grounding during SFT to high-difficulty multi-hop reasoning in RLVR, fostering gradual acquisition of robust evidence-based reasoning skills. 3.4. Dataset Statistics The final Conan-91k dataset comprises two subsets: 1) Conan-CoT-60k. 60k in total, including 25k one-round, 25k two-round, and 10k threeround reasoning samples for cold-start. 2) ConanRLVR-31k. 31k challenging reasoning samples for RLVR to enhance model robustness and reasoning generalization in complex scenarios. 4 Figure 2. a) Reasoning Trace Construction. b) Data Example. c) Multi-stage Progressive Cold-start, including textual, multimodal alignment, and vision-centric reasoning stages. d) The Joint Identification-Reasoning-Action RLVR. 4. Training Procedure This section details the training procedure of Conan, comprising two key phases: MultiStage Progressive Cold-Start and joint IdentificationReasoningAction (AIR) RLVR. 4.1. Multi-Stage Progressive Cold-Start To progressively activate the multi-step reasoning abilities, we propose multi-stage progressive cold-start training strategy, which consists of the following three stages: Textual Reasoning Stage. In the initial stage, the model is trained on 10k single-round samples from Conan-CoT-60k, where frames are represented solely by dense textual descriptions and timestamps. This stage focuses on learning temporal and causal reasoning across ordered frame descriptions, establishing structured reasoning foundation for subsequent multimodal learning. Multimodal Alignment Reasoning Stage. The second stage incorporates 25k single-round and 10k two-round samples combining raw visual frames with corresponding textual descriptions and timestamps. This multimodal configuration bridges the gap between text-only and vision-language reasoning, promoting effective alignment between visual and linguistic cues. Moreover, the inclusion of two-round samples introduces the frame retrieval action, allowing the model to learn how to gather supplementary evidence for complex reasoning tasks. Vision-Centric Reasoning Stage. In the final stage, the model is trained on the complete ConanCoT-60k, performing reasoning directly over visual frames with interleaved timestamps. This stage compels the model to execute multi-step reasoning across multi-scale visual clues, thereby achieving higher level of perceptual understanding and visual reasoning competence. Overall, this progressive curriculum, from textbased to multimodal and ultimately to visioncentric reasoning, enables the model to incrementally acquire robust multi-scale evidence reasoning skills, laying solid foundation for the subsequent RLVR phase. 4.2. AIR RLVR Building upon the model Conan-SFT obtained from the cold-start process, we further enhance its multi-step reasoning capabilities via the AIR RLVR framework. Given that the model has already learned to produce reasoning traces consisting of: 1) frame identification, 2) evidence reasoning, and 3) action decision, AIR RLVR aims to optimize the exploration of effective reasoning trajectories through set of carefully designed reward functions. We first introduce one format reward and two outcome rewards to ensure both structural consistency and answer accuracy. Format Reward. To enforce structural consistency in model outputs y, we define format reward Rfmt that verifies whether specific tags are correctly applied. And the model is restricted to perform only one action (e.g. random frame sampling, specific frame retrieval, confident question answering) at time. The format reward Rf mt is defined as: (cid:40) Rf mt(y) = 0.5, 0, if matches format, otherwise. (1) Multi-choice Reward. For multi-choice QA, we apply binary outcome reward Rmc based on exact match between the predicted answer and ground truth ˆy: Rmc(y, ˆy) = (cid:40) if = ˆy, 1, 0, otherwise. (2) Free-form Reward. For free-form QA, the outcome reward Rfree is computed as the average of ROUGE-1, ROUGE-2, and ROUGE-L scores [16] between the predicted answer and the reference answer ˆy: 1 3 Rfree(y, ˆy) = (cid:0)R1(y, ˆy) + R2(y, ˆy) + RL(y, ˆy)(cid:1) (3) To evaluate the joint quality of the multi-scale frame identification, frames retrieval, we design an identification reward Ride and retrieval reward Rret: 1) the identification reward Ride measures the average accuracy of identified evidence/contextual frames across steps, 2) the retrieval reward Rret evaluates the quality of retrieved frames by computing the average ratio of evidence/contextual frames among all retrieved frames. And the final joint identification-retrievaloutcome reward RIRO is formulated as: RIRO = (cid:40) Rf mt + Ro + Ride + Rret, Rf mt + Ro, if Ro > 0, otherwise, (4) where Ro is the outcome reward and {mc, ree}. This reward shaping encourages the model to generate structurally valid, evidencegrounded, and accurate reasoning traces while Finally, we improving retrieval efficiency. prompt the model generate group of response {y1, y2, , yG}, where is the number of generated responses, and adopt the GRPO [9] algorithm for reinforcement optimization to stabilize training and refine the reasoning policy. 5. Experiment 5.1. Evaluation Setups Implementation Details 1) Training Settings. We adopt Qwen-2.5-VL-7B-Instruct [2] as the base model. During the multi-stage cold start, the model is trained for up to one epoch per stage with global batch size of 32. The trained model is then used for the AIR RLVR phase, also trained for one epoch under the same batch configuration. The maximum completion length is set to 4, 000 tokens, with generation temperature of 1.0 and number of 8. Each input video contains 16 initial frames, and the model is allowed to retrieve up to 8 additional frames per reasoning step. 2) Evaluation Settings. The generation temperature is fixed at 1.0 and each sample is evaluated for three times. The maximum new tokens is set to 4, 000 when reasoning traces are included, and 128 for direct answering. The number of reasoning rounds is limited to three to maintain retrieval efficiency. videos are standardized to 16 frames for multi-step reasoning benchmarks, 32 frames for long-video understanding benchmarks during evaluation at resolution of 6 #Params Overall MMR-V Video-Holmes VRBench VCRBench LongVideoReason Human-P&C Closed-source Models GPT-4o [11] Gemini 1.5 Pro Gemini 2.0 Flash Gemini 2.5 Flash Gemini 2.5 Pro Open-source Models LLaVA-OneVision-7B [12] InternVL3-8B [4] Kimi-VL-A3B-Instruct [22] Qwen2.5-VL-72B-Instruct [2] Qwen2.5-VL-7B-Instruct [2] Text CoT Models Video-R1 [6] VideoChat-R1 [14] Video CoT Models Video-MTR [27] Rewatch-R1 [30] Conan SFT Conan - - - - - 7B 8B 3B/16B 72B 7B 7B 7B 7B 7B 7B 7B - - - - - - 44.4 55.1 46.9 44.4 49.8 49.1 50.9 44. 42.6 51.2 6.5 33.6 32.4 39.1 30.1 36.3 36.1 36.5 45.3 42.0 41.2 30.6 45. - 32.3 32.4 40.2 28.5 36.5 33.0 35.7 37.8 68.7 - - 60.5 72.7 66.4 69.5 61. 69.7 79.1 46.9 44.0 51.7 30.7 - 34.3 50.8 46.5 48.0 48.2 48.1 49.8 - 69. - - 64.6 72.3 61.8 70.3 67.9 57.3 70.5 48.4 52.6 56.1 60.0 48.5 54.2 42.0 55.7 48.2 49.8 51. 47.2 51.6 49.1 57.4 ( 10.5 ) 35.4 42.7 ( 12.6 ) 34.9 44.6 ( 16.1 ) 64.4 81.0 ( 14.6 ) 43.3 51.0 ( 4.5 ) 66.0 72.8 ( 11.0 ) 50.4 52.3 ( 4.1 ) Table 1. Evaluation results of base model Qwen2.5-VL-7B-Instruct , Conan , and other baselines on the six multistep reasoning benchmarks. means the model is partly trained on LongVideoReason-QA [3], the training set of LongVideoReason. denotes the multi-choice subset. 448 28 28. Multi-step Reasoning Benchmarks. We conduct comprehensive evaluation on six challenging multi-step reasoning benchmarks: MMR-V [32], Video-Holmes [5], VRBench [29], VCRBench [20] (multi-choice subset), LongVideoReason [3], and Human-P&C [13]. Accuracy is used as the primary evaluation metric. Compared Methods. We compare Conan with set of closed-source MLLMs, and open-source models, including non-reasoning models, text-CoT models, and video-CoT models. 5.2. Main Results The evaluation results on the six multi-step reasoning benchmarks are shown in Table 1. And we have the following key observations and analyses. Overall Analysis. Conan substantially surpasses its base model Qwen2.5-VL-7B-Instruct across all benchmarks, with average accuracy gains exceeding 10%. Remarkably, Conan also outperforms the advanced GPT-4o on most benchmarks, underscoring its superior capabilities of multi-step, evidence-grounded reasoning. Furthermore, two advanced text-CoT models (e.g. Video-R1 and VideoChat-R1) perform notably worse than Conan, demonstrating the effectiveness of the identificationreasoningaction mechanism in grounding reasoning on accurate visual evidence. In addition, compared with concurrent Video-CoT approaches (e.g. Video-MTR and Rewatch-R1), Conan exhibits stronger multi-hop reasoning abilities, highlighting the superiority of accurate evidence identification. LongVideoBench MLVU LVBench VideoMME Qwen2.5-VL-7B-Instruct [2] Video-R1 [6] VideoChat-R1 [14] Rewatch-R1 [30] Video-MTR [27] FrameThinker [10] Conan 48.9 55.6 54.3 50.5 56.4 52. 52.8 62.5 60.5 55.2 59.7 59.1 34.4 38.3 38.0 37.2 38.6 36. 55.8 58.6 56.9 58.9 59.3 - 56.6 ( 7.7 ) 63.4 ( 10.6 ) 39.2 ( 4.8 ) 60.5 ( 4.7 ) Table 2. Evaluation results on long-video understanding. Long video understanding. Beyond multi-step reasoning, Conan exhibits strong generalization to long-video understanding tasks. As presented in Table 2, Conan consistently outperforms Qwen2.5VL-7B-Instruct across LongVideoBench [25], MLVU [31], LVBench [24], and Video-MME [7], achieving state-of-the-art performance compared with both text-CoT and video-CoT models. These results indicate that the high-quality, multi-scale evidence reasoning data and progressive training strategy not only enhance stepwise reasoning but 7 Overall MMR-V Video-Holmes VRBench VCRBench LongVideoReason Human-P&C w-binary scale w/o-data sampling w/o-textual reasoning w/o-multimodal alignment reasoning w/o-vision-centric reasoning w-direct RLVR w/o-evidence reward w/o-retrieval reward w-text CoT"
        },
        {
            "title": "Conan",
            "content": "53.8 55.2 57.0 56.4 53.0 51.0 53.8 54.0 55.2 57.4 39.7 40.0 42.2 43.5 39.0 39. 39.9 38.2 41.1 42.7 39.1 40.8 43.9 44.2 36.5 36.8 40.0 42.4 41.0 44. 75.4 78.9 81.4 80.4 75.2 73.3 74.8 75.7 76.8 81.0 46.9 48.4 49.8 48.2 50.1 46. 46.1 47.8 48.8 51.0 70.2 72.3 73.1 72.5 68.5 62.3 71.7 69.2 70.5 72. 51.3 50.7 51.7 49.3 48.7 47.9 50.1 50.5 52.8 52.3 Table 3. Ablation results of Conan, where the best results are in boldface. Figure 3. Training dynamics in the AIR RLVR process of Conan. also effectively boost long-video understanding. 5.3. Ablation Study To investigate the contribution of each component in our framework, we conduct comprehensive ablation study with multiple Conan variants. For the Conan-91k dataset, we design two variants: 1) w-binary scale, which removes the contextual frame type in multi-scale frame identification by merging it into the irrelevant category, reducing frame labels to only evidence and irrelevant types; and 2) w/o-data sampling, which omits the evidence difficulty-aware sampling process and applies uniform random sampling instead. For the multi-stage progressive cold-start strategy, we develop four variants: 1) w/o-textual reasoning, which excludes the textual reasoning stage; 2) w/o-multimodal alignment reasoning, which skips the multimodal alignment reasoning stage; and 3) w/o-vision-centric reasoning, which removes the vision-centric reasoning stage. 4) wdirect RLVR, which bypasses the three cold-start stages and directly employs AIR RLVR to train the model on the Conan-RLVR-31k dataset. For the AIR RLVR framework, we design three variants: w/o-identification reward, which discards the identification reward; w/o-retrieval reward, which removes the retrieval reward; and wtext CoT, which enforces single-round text-CoT paradigm performing pure textual reasoning and answering without additional actions in training and evaluation procedures. The ablation results in Table 3 reveal several key findings: a) Dataset design. w-binary scale underperforms Conan, confirming the benefits of multi-scale frame identification in providing richer contextual cues. Moreover, Conan outperforms w/o-data sampling, which validates that evidence difficulty-aware sampling effectively guides the model to progressively acquire multi-step reasoning abilities. b) Progressive cold-start. Conan consistently surpasses w/o-textual reasoning, w/omultimodal alignment reasoning, and w/o-visioncentric reasoning across most multi-step reason8 Figure 4. qualitative example from VRBench showing the reasoning traces of Video-R1 (Text CoT), Video-MTR (Video CoT), and Conan for comparison. ing benchmarks, and shows substantial gains over w-direct RLVR. These results demonstrate that the multi-stage progressive cold-start strategy is crucial for gradually activating the models multi-hop reasoning capabilities. c) AIR RLVR. Conan outperforms w/o-evidence reward and w/o-retrieval reward, proving the effectiveness of the two rewards in enhancing accurate evidence localization, and efficient frame retrieval, respectively. 5.4. Training Dynamics To gain deeper understanding of how the models behavior evolves during end-to-end reinforcement learning, we perform fine-grained analysis of its training dynamics. As shown in Figure 3, the training process can be divided into two stages: Stage I: Accuracy-Oriented Evidence Exploration. Building upon the solid foundation established through multi-stage cold-start training, the model initially enters phase characterized by frequent yet progressively more accurate frame retrieval. During this stage, it actively queries additional clips to maximize the stepwise identificationretrievaloutcome reward across tasks. The increasing retrieval precision reflects an early learning strategy where the model compensates for incomplete internal reasoning by broadly exploring the visual context to identify useful evidence. This stage marks transitional period in which the model learns to recognize the importance of accurate evidence localization before optimizing retrieval efficiency. Stage II: Efficient Evidence Retrieval. As training progresses, the model transitions to more refined and selective retrieval policy. It significantly reduces retrieval frequency while maintaining high reward accuracy, indicating that Conan has internalized compact and efficient multi-step reasoning strategy, retrieving evidence only when necessary, much like detective who strategically gathers key clues rather than exhaustively examining all information. 5.5. Qualitative Evaluation Figure 4 illustrates qualitative comparison on VRBench [29] between Text CoT (Video-R1), Video 9 CoT (Video-MTR), and Conan. The text CoT model Video-R1 performs single-round textual reasoning without visual grounding, leading to hallucinated answer based solely on linguistic priors. The video CoT model Video-MTR incorporates frame retrieval but fails to localize relevant evidence, resulting in weak reasoning alignment between retrieved frames and the question. In contrast, Conan performs multi-round, evidencegrounded reasoning. In Round 1, it detects the absence of causal evidence and performs random In frame sampling to broaden the search scope. Round 2, guided by contextual cues, it executes specific frame retrieval around key timestamps where player interactions occur. In Round 3, Conan identifies frames depicting color-triggered game events, such as team-based activities and flag captures, and integrates these observations to infer that different colored shirts trigger different game events or interactions. This progressive process, from exploration to targeted verification to confident deduction, demonstrates Conans superior ability to accurately locate, reason over, and act upon relevant visual evidence compared with both Text CoT and Video CoT models. 6. Conclusion and Future work In this work, we present Conan, unified framework that empowers multimodal large language models to perform Conan-like visual reasoning through multi-scale frame identification, evidencebased reasoning, and confident action decision. Employing the Conan-91k dataset, constructed via multi-scale evidence categorization, Conan simulation, and evidence difficulty-aware sampling, we devise multi-stage progressive cold-start strategy alongside joint EvidenceReasoningAction RLVR framework to progressively cultivate robust multi-step reasoning abilities. Extensive experiments six multi-step reasoning across and four long-video understanding benchmarks demonstrate that Conan consistently outperforms the base model Qwen2.5-VL-7B-Instruct, achieving state-of-the-art accuracy and strong generalization over both text-CoT and video-CoT In future work, we plan to extend Comodels. nan toward chain-of-frame reasoning, enabling dynamic frame generation during reasoning to provide visual evidence beyond the video for solving more complex video reasoning tasks."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. 2, 3 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 3, 6, 7 [3] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, and Song Han. Scaling rl to long videos. In Advances in Neural Information Processing Systems, 2025. 2, 3, 7 [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 7 [5] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. 2, 3, 7 [6] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 2, 3, [7] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 3, 7 [8] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via In Proceedings of the IEEE inlanguage query. 10 ternational conference on computer vision, pages 52675275, 2017. 2, 3 [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948, 2025. 2, 3, 6 [10] Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, and Yu Cheng. Framethinker: Learning to think with long videos via multi-turn frame arXiv preprint arXiv:2509.24304, spotlighting. 2025. 2, 3, [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 2, 3, 7 [12] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan LlavaZhang, Yanwei Li, Ziwei Liu, et al. onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7 [13] Keliang Li, Hongze Shen, Hao Shi, Ruibing Hou, Hong Chang, Jie Huang, Chenghao Jia, Wen Wang, Yiling Wu, Dongmei Jiang, Shiguang Shan, and Xilin Chen. Humanpcr: Probing mllm capabilities in diverse human-centric scenes. arXiv preprint arXiv:2508.13692, 2025. 3, 7 [14] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement finetuning. arXiv preprint arXiv:2504.06958, 2025. 2, 3, 7 [15] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. 3 [16] Chin-Yew Lin. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481, 2004. 6 [17] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really unIn Findings of the Association derstand videos? for Computational Linguistics, pages 87318772, 2024. 3 [18] Yuanxin Liu, Kun Ouyang, Haoning Wu, Yi Liu, Lin Sui, Xinhao Li, Yan Zhong, Charles, Xinyu Zhou, and Xu Sun. Videoreasonbench: Can mllms perform vision-centric complex video reasoning? arXiv preprint arXiv:2505.23359, 2025. 2, 3 [19] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. 2, 3 [20] Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, and Feng Zhao. Vcrbench: comprehensive evaluation framework for video chain-of-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. 3, 7 [21] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi arXiv preprint k2: Open agentic intelligence. arXiv:2507.20534, 2025. 4 [22] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 2, 3, [23] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14941504, 2015. 2, 3 [24] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 3, 7 [25] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Longvideobench: benchmark for longLi. context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 2, 3, 7 [26] LLM-Core-Team Xiaomi. Mimo-vl technical report. arXiv preprint arXiv:2506.03569, 2025. 2, 3 [27] Yuan Xie, Tianshui Chen, Zheng Ge, and Lionel Ni. Video-mtr: Reinforced multi-turn reasoning for long video understanding. arXiv preprint arXiv:2508.20478, 2025. 2, 3, 7 11 [28] Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long In Findings of the Associavideo understanding. tion for Computational Linguistics, pages 17900 17917, 2025. 2, [29] Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, et al. Vrbench: benchmark for multi-step reasoning in long narrative videos. arXiv preprint arXiv:2506.10857, 2025. 3, 7, 9 [30] Congzhi Zhang, Zhibin Wang, Yinchao Ma, Jiawei Peng, Yihan Wang, Qiang Zhou, Jun Song, and Bo Zheng. Rewatch-r1: Boosting complex video reasoning in large vision-language models arXiv preprint through agentic data synthesis. arXiv:2509.23652, 2025. 2, 7 [31] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understandthe Computer Vision ing. and Pattern Recognition Conference, pages 13691 13701, 2025. 2, 3,"
        },
        {
            "title": "In Proceedings of",
            "content": "[32] Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Mmr-v: Whats left unsaid? benchmark for multimodal deep reasoning in videos. arXiv preprint arXiv:2506.04141, 2025. 3,"
        }
    ],
    "affiliations": [
        "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
        "WeChat AI, Tencent Inc., China"
    ]
}