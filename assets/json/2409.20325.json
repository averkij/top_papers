{
    "paper_title": "Old Optimizer, New Norm: An Anthology",
    "authors": [
        "Jeremy Bernstein",
        "Laker Newhouse"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of $\\mathbb{R}^{m\\times n}$, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 5 2 3 0 2 . 9 0 4 2 : r Preprint 1 Old Optimizer, New Norm: An Anthology Jeremy Bernstein Laker Newhouse MIT CSAIL, United States jbernstein@mit.edu lakern@mit.edu Abstract Deep learning optimizers are often motivated through mix of convex and approximate second-order theory. We select three such methodsAdam, Shampoo and Prodigyand argue that each method can instead be understood as squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under particular norm. By generalizing this observation, we chart new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of Rmˆn, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training."
        },
        {
            "title": "Prologue",
            "content": "Deep learning optimizers are often motivated from the perspectives of convex and approximate second-order theory. These theoretical frameworks have been used to inspire algorithmic ideas, as well as providing means to analyse the convergence of various optimizers. However, we believeand will attempt to demonstratethat there is wealth of untapped algorithmic opportunity in the simpler realm of exact first-order theory without convexity assumptions. To make our case, we choose three optimizers that were originally analysed under convex or approximate second-order theory: Adam, Shampoo and Prodigy. After disabling their exponential moving averages (EMA), we show that each algorithm admits parsimonious theoretical explanation as variant of steepest descent under certain norm. EMA can then be thought of as smoothing out the algorithm, or making it more robust to mini-batch noise, although nailing down the precise role of EMA is perhaps still an open problem. By steepest descent, we mean the procedure of choosing weight update to minimise 2 }w}2, local quadratic model of the loss function of the form Lpwq ` wLpwqJw ` λ visualized in Figure 1. Crucially, the sharpness parameter λ and norm }} are chosen priori, without touching an (approximate) Hessian during training. As such, we consider steepest descent to be squarely first-order method and not an (approximate) second-order method. Throughout the anthology, we rely on dual description of steepest descent: Proposition 1 (Steepest descent) For any Rn thought of as the gradient and any λ ě 0 thought of as the sharpness, and for any norm }} : Rn Ñ with dual norm }}:: arg min wPRn gJw ` λ 2 }w} 2 ȷ }g}: λ arg max }t}1 gJt. (1) J. Bernstein & L. Newhouse. Bernstein Newhouse Figure 1: Steepest descent considers the problem of minimizing linear functional under for Rn. Here we show gJw ` λ quadratic penalty: arg minwPRn how the solution varies with the sharpness λ ą 0 and the choice of norm }}. We overlay different norm balls on top of linear color gradient, and use arrows to denote the solution, meaning the member of the norm ball that minimizes the color. a) Increasing the sharpness decreases the size of the solution vector. b) Changing the norm can change the direction of the solution vector. For different ℓp norms, the solution direction changes because the gradient is not axis-aligned. In practice, we should pick the sharpness and norm to fit the geometry of our loss. 2 }w}2 Equation (1) separates the solution of the steepest descent problem into two pieces: first computing the step size as the dual norm of the gradient divided by the sharpness, and second solving for the step direction as the unit vector that maximizes the inner product with the gradient. The proof of this proposition is given in Appendix B. Of course, the art of steepest descent lies in choosing norm }} and sharpness λ suited to the optimization problem at hand. While it may be possible to turn this art into science (Large et al., 2024), that ambition is beyond the scope of this anthology. Here we point out that past methods do implicitly make decisions about norms, and in somewhat haphazard manner. In fact, they implicitly assign different induced matrix norms to the network layers: Definition 1 (Induced operator norm) Given matrix Rdoutˆdin and two normed vector spaces pRdin, }}αq and pRdout, }}βq, the α to β induced operator norm is given by: }M }αÑβ max xPRdin }M x}β }x}α . (2) Definition 1 tells us that by varying the choice of vector norms }}α and }}β, we can induce large family of matrix norms. In turn, this implies correspondingly large family of steepest descent optimizers. By foregrounding this issue, we hope that algorithm designers may develop more suitable optimizers by becoming more intentional about their choice of norm. 2 Old Optimizer, New Norm: An Anthology Story I. Adam as Steepest Descent under the Max-of-Max Norm Adam is widely used deep learning optimizer: the original paper of Kingma and Ba (2015) now has well over 100,000 citations. Adam has been motivated in various ways, including through convex analysis (Kingma and Ba, 2015) and as an approximate second-order method (Sun and Spall, 2021). However, there have been efforts to build more direct understanding of Adam: for instance, with exponential moving averages (EMA) switched off, Adam is just sign gradient descent (Balles and Hennig, 2018; Bernstein et al., 2018), which is equivalent to steepest descent under the infinity norm (Carlson et al., 2015). In this story, we connect Adam to certain max-of-max norm, showing how Adam respects the tensor structure of neural network in very particular way. To begin, we review how Adam connects to sign gradient descent. Ignoring bias corrections and numerical stabilizations, Adam is given by the following system of updates: mt β1 mt1 ` p1 β1q gt, vt β2 vt1 ` p1 β2q g2 , wt`1 wt η mt{ vt, ? (3) (4) (5) where denotes the time step, gt Rn the gradient vector and η ą 0 the step size. The EMA time scales of the first gradient moment mt and second moment vt are set by 0 ď β1, β2 ă 1. All operations are conducted entry-wise. If we switch off EMA by setting β1 β2 0, the Adam updates reduce to just sign gradient descent: wt`1 wt η gt{ g2 wt η signpgtq. (6) (7) This connection to sign descent should not be surprising since Adam, published in 2015, builds on the RMSprop optimizer that Tieleman and Hinton (2012) already called the mini-batch version of just using the sign of the gradient. And RMSprop itself built on the RPROP optimizer (Riedmiller and Braun, 1993), which also uses gradient signs. Still, why should using the sign of the gradient be good idea in deep learning? In search of motivation, it is interesting to consider that sign descent solves the problem of steepest : maxi vi (Carlson et al., 2015, 2016; Fan, 2017): descent under the vector ℓ8 norm, }v}8 Proposition 2 (Sign descent as steepest descent under the infinity norm) For any gradient vector Rn and sharpness λ ą 0, it holds that: arg min wPRn ȷ gJw ` λ 2 }w} 2 8 }g}1 λ signpgq. (8) In words, the vector that minimizes linear functional under an infinity norm penalty is scalar multiple of sign vector. The proof is given in Appendix B. While this connection between Adam, sign descent and steepest descent is perhaps cute, it does not answer basic question: Why does the vector ℓ8 norm have anything to do with neural network training? In particular, taking the weight space to be Rn equipped with 3 Bernstein Newhouse the simple infinity norm seems to throw away the fact that the weight space of neural network is built in structured way out of layers of matrices (and perhaps other tensors). To resolve this conundrum, we suggest that in fact the vector ℓ8 norm on the flattened weight space doesnt have anything to do with deep learning. Instead, there is coincidence at play. The ℓ8 norm enjoys special property summarized by the slogan max of max is max. To see this, consider neural network with list of weight matrices W1, . . . , WL. Let rowrpWlq denote the rth row of the lth weight matrix, and let flattenpW1, . . . , WLq Rn denote the full flattened weight vector. Then we have that: }w}8 max max }rowrpWlq}8 max }Wl}ℓ1Ñℓ8, (9) where the second equality follows via Proposition 8. In words, the infinity norm of the flattened weight vector coincides with the largest ℓ1 to ℓ8 operator norm of the layers. So Equation (9) connects the unstructured space of the flattened weight vector to the structured space of the list of weight matrices. We refer to the object maxl }Wl}ℓ1Ñℓ8 as the max-of-max norm. And sign descent emerges as steepest descent under this norm: Proposition 3 (Sign descent as steepest descent under the max-of-max norm) For any list of gradient matrices G1, ..., GL and any sharpness λ ą 0, consider the problem: arg min W1,...,WL Lÿ xGl, Wly ` l1 ff λ 2 Lmax l1 2 }Wl} ℓ1Ñℓ , (10) where x, denotes the Frobenius inner product, and Wl has the same shape as Gl. For , where : denotes the dual norm, Equation (10) is solved by: step size η ř l1 }Gl}: ℓ1Ñℓ8 1 λ Wl η signpGlq for each layer 1, ..., L. (11) In words, the matrix-aware steepest descent problem of Equation (10) is solved by layerwise sign descent as given in Equation (11). This observationthat sign descent updates are implicitly doing per-matrix gradient normalizationmay be major reason that Adam, sign descent and Lion (Chen et al., 2023) outperform vanilla gradient descent in large language model training (Zhao et al., 2024; Large et al., 2024). The proof is given in Appendix B. All told, this story has shown that Adam without EMA is sign descent and that, coincidentally, sign descent solves two different steepest descent problems: one on the flattened weight space, and one that is aware of the matrix structure of neural architecture. But, at the end of this story, questions linger. Why does the ℓ1 to ℓ8 induced operator norm rear its head? What does it have to do with deep learning? Arent there other induced operator norms on matrices we could equally well consider? For answers to these questions, dear reader, youll have to wait for our next story... story about Shampoo! 4 Old Optimizer, New Norm: An Anthology Story II. Shampoo as Steepest Descent under the Spectral Norm Now, dear reader, we turn our attention to Shampoo (Gupta et al., 2017, 2018). variant of the Shampoo optimizer won the external tuning track of the 2024 AlgoPerf: Training Algorithms competition (Dahl et al., 2023). While the method was originally motivated as generalization of the AdaGrad convex optimizer (Duchi et al., 2011) to tensor spaces, more recent work casts Shampoo as an approximate second-order method (Anil et al., 2020; Morwani et al., 2024). We will show that Shampoowith accumulation disabledis steepest descent under the max spectral norm over layers. To begin, we show that Shampoo updates, without accumulation, are semi-orthogonal matrices. At time step and for each layer, Shampoo collects the gradient matrix Gt and makes the following update to the weight matrix Wt: Lt Lt1 ` GtGT , Rt Rt1 ` GT Gt, GtR1{4 Wt`1 Wt η L1{4 . (12) (13) (14) All operations, including the inverse fourth roots, are matrix operations. The accumulators Lt and Rt are referred to as the left and right pre-conditioners. Practitioners usually replace the simple sums in Equations (12) and (13) with EMAs (Shi et al., 2023). If we disable the accumulation, setting Lt GtGJ and Rt GJ q1{4 Gt pGJ Wt`1 Wt η pGtGJ Wt η UtV , Gt, Shampoo reduces to: Gtq1{4 (15) (16) where Equation (16) is reached by substituting the reduced singular value decomposition into Equation (15). Notice that there is direct (SVD) of the gradient Gt UtΣtV parallel between Equations (6) and (7) for Adam and Equations (15) and (16) for Shampoo. So, Shampoo without accumulation makes semi-orthogonal weight update. In fact: Proposition 4 (Projection to the closest semi-orthogonal matrix) Consider the semi-orthogonal matrices Omˆn : and let }}F denote the Frobenius norm. For any matrix Rmˆn with reduced SVD ΣV J: Rmˆn : AAJ Im or AJA In ( arg min APOmˆn }A G}F J, (17) where the minimizer is unique if and only if the matrix has full rank. So, Shampoo without accumulation projects the gradient matrix to the closest semiorthogonal matrix in Frobenius norm. The proof is in Appendix B. Why might this be good idea, you ask? Well, for one thing, its steepest descentthis time under the maximum spectral norm }}ℓ2Ñℓ2 (Definition 1) over all the matrices in the network: Proposition 5 (Shampoo as steepest descent under the spectral norm) For any list of gradient matrices G1, ..., GL and any sharpness λ ą 0, consider the problem: arg min W1,...,WL Lÿ xGl, Wly ` 5 ff λ 2 Lmax l1 }Wl} 2 ℓ2Ñℓ , (18) Bernstein Newhouse where x, denotes the Frobenius inner product and Wl has the same shape as Gl. Suppose for each 1, ..., L. Then Equation (18) that Gl has reduced SVD given by Gl UlΣlV is solved with step size η l1 tr Σl and an update: ř 1 λ Wl η UlV for each 1, ..., L. (19) This solution for Wl is unique if and only if the matrix Gl is of full rank. The proof is given in Appendix B. novelty of this proposition in contrast to prior work on stochastic spectral descent (Carlson et al., 2015, 2016) is our use of max norm over layers to handle the multi-layer case. However, our main contribution here is to draw the connection between Proposition 5 and Shampoo as in Equations (15) and (16). So, Shampoo without accumulation is steepest descent under the spectral norm. Why might this be good idea in deep learning? The idea that we wish to advance is that one can derive upper bounds on the loss of machine learning models in terms of spectral norms. Here we present the simplest possible example: linear model and the square loss. Proposition 6 (Bounding the square loss of linear predictor) Consider matrix Rdoutˆdin that we shall think of as linear predictor mapping an input Rdin to an output Rdout. Given dataset of samples tpx1, y1q, ..., pxn, ynqu, where ? the ith input is normalized such that }xi}2 din, we can construct the square loss: LpW : 1 2n nÿ 1 dout i1 2 2. }yi xi} (20) Then, for any matrix Rdoutˆdin thought of as weight update, it holds that: LpW ` ď LpW ` xW LpW q, ` 1 2 din dout 2 }W } ℓ2Ñℓ2, (21) where x, is the Frobenius inner product. In words: the square loss of linear predictor admits an upper bound that is quadratic in the spectral norm of the weight perturbation. Choosing the weight perturbation to minimize this upper bound is precisely steepest descent under the spectral norm! The proof is given in Appendix B. This optimizer design pattern, which starts by deriving an upper bound on the loss (as in Proposition 6) and then minimizes it (as in Proposition 5), is known generally as majorization-minimization (Lange, 2016). It is an exact and first-principles design pattern, without Hessian approximations or appeals to convex theory. This design pattern is used extensively by Carlson et al. (2015, 2016) to design optimizers for restricted Boltzmann machines and discrete graphical models. Generalizing the pattern to arbitrary network architectures and loss functions requires more advanced machinery (Bernstein et al., 2023; Streeter, 2023; Large et al., 2024). And so, dear reader, we have reached the end of our second story. We have shown that Shampoo without accumulation corresponds to projecting the gradient matrix to the closest 6 Old Optimizer, New Norm: An Anthology Domain Norm Solution Optimizer Cousin Rn Rn Rmˆn Rmˆn Euclidean ℓ2 }g}2 λ }g}1 infinity ℓ8 λ }g}2 signpgq Frobenius S2 }G}F λ }G}F tr Σ spectral S8 λ vanilla gradient descent SGD sign descent Adam vanilla gradient descent SGD spectral descent Shampoo Table 1: Popular optimizers are related to steepest descent under different norms. For vector-valued optimization problems, we consider the steepest descent prob2 }w}2. For matrix-valued problems, we consider lem arg minw gJw ` λ 2 }W }2, where x, is the Frobenius inner product. arg minW xG, ` λ We list the solution for different vector ℓp norms and Schatten Sp norms. The Schatten Sp norm of matrix returns the ℓp norm of its vector of singular values. Finally, ΣV is the reduced singular value decomposition of the gradient. semi-orthogonal matrix, which solves the problem of steepest descent under the spectral norm. And we showed how steepest descent under the spectral norm emerges from upper bounding the square loss of linear predictor. This perspective, of viewing Shampoo as (smoothed out) projection to the space of semi-orthogonal matrices, grounds the algorithm in prior literature on spectral descent (Carlson et al., 2015, 2016; Fan, 2017). And in Appendix A, we discuss how it might unlock new means for computing the Shampoo updates. We summarize our first two stories in Table 1. And we still have one more left to tell... 7 Bernstein Newhouse Story III. Prodigy: Automatically Computing the Escape Velocity For our final story, we speak of Prodigy (Mishchenko and Defazio, 2023). The Prodigy optimizer falls amid series of recent works (Defazio and Mishchenko, 2023; Khaled et al., 2023; Ivgi et al., 2023) that attempt to apply convex theory to design and analyse deep learning optimizers that do not require tuning. In contrast, we argue that Prodigy (without EMA) is but another example of steepest descent, where instead of using the step size η }g}:{λ from Proposition 1, Prodigy uses heuristic to automatically warm up to good step size. This demonstrates the value of Proposition 1 for disentangling the optimizer design problem. If one knows good norm }} but is ignorant of the sharpness }t}1 gJt from parameter λ, then one may obtain the step direction by solving arg max Proposition 1, while using another means to find good step size. Then let us make our case. We focus on Algorithm 3 in the Prodigy paper, since this is the version used in their experiments. We first show that with EMA switched off, Prodigy implements sign gradient descent with step size that warms up automatically. Ignoring the numerical stabilization and learning rate schedule, Prodigy is given by: mt β1 mt1 ` p1 β1q ηt gt, g2 vt β2 vt1 ` p1 β2q η2 , β2q η2 β2 rt1 ` p1 rt β2q η2 β2 st1 ` p1 rt }st}1 wt`1 wt ηt mt{ st ηt`1 max vt, ηt, ? , pw0 wtq, gJ gt, (22) (23) (24) (25) (26) (27) where denotes the time step and gt Rn the gradient vector. While this system of updates may seem intimidating, if we switch off EMA by setting β1 β2 0, the Prodigy updates simplify dramatically to just sign gradient descent with dynamical step size as follows: ηt, gJ (28) , ηt`1 max wt`1 wt ηt signpgtq. pw0wtq }gt}1 (29) But Proposition 2 showed that sign descent is steepest descent under the infinity norm. Therefore Equations (28) and (29) prove our claim that Prodigy without EMA is steepest descent, although with dynamically chosen step size denoted ηt. All that remains is to understand the dynamical rule, given by Equation (28), for choosing the step size ηt. We shall argue that this dynamical rule can be understood to approximate heuristic algorithm for achieving, but not exceeding, what we shall call escape velocity: Choose very small initial step size η0small enough to be priori sure that η0 ! η, where η denotes escape velocity: the unknown but optimal initial step size; At each step, check if the weights wt have escaped the linearization of the loss around the initial weights w0if not, double the step size according to ηt`1 2 ˆ ηt; Once the weights wt have escaped the initial linearization, stop increasing the step size. We say that the step size ηt has reached escape velocity η. 8 Old Optimizer, New Norm: An Anthology The rationale behind this procedure is that if we knew the optimal initial step size η, then the weights should escape the initial linearization of the loss in single step. Formally, the directional derivative pw1 w0qJg1 must vanish if the step size is chosen optimally (Cauchy, 1847). If the directional derivative in the direction of the first weight update is still negative pw1 w0qJg1 ă 0, then we could have taken larger step. Said another way, we can use the angle that the gradient g1 makes with the change in weights w1 w0 to tell us whether or not we should increase the step size. Notice that procedure has no reliance on convexity. With this in mind, let us massage Prodigys step size update (Equation (28)) as follows: ηt`1 max ηt, gJ pw0wtq }gt}1 max ηt, }gt}2 }gt} ˆ }wt w0}2 ˆ cos θ , (30) where θ denotes the angle between the gradient gt and the difference in weights w0 wt. To help make sense of this expression, we make two assumptions: 1. The gradient is dense vector in Rn, meaning that }gt}2{}gt}1 1{ ? n; 2. wt is still close enough to the initialization w0 that cos θ 1. Under these assumptions, Equation (30) becomes just ηt`1 max pηt, }wt w0}RMSq, where the root mean square (RMS) norm is defined via }}RMS : }}2. Combined with Equation (29), this allows us to estimate the size of the weight change at step ` 1: 1 ? }wt`2 wt`1}RMS ηt`1 }signpgtq}RMS max pηt, }wt w0}RMSq ě }wt w0}RMS, where we have used the fact that sign vector has unit RMS norm. In words, while assumptions (1) and (2) hold, the step size at time ` 1 is equivalent to the whole progress up to step t. This suggests exponential growth in the step size that continues until assumption (2) breaks, which we think of as the step size reaching the escape velocity η Now we wish to point out that this procedure is just one amongst space of line search methods that one might consider (Armijo, 1966; Riedmiller and Braun, 1993; Kenneweg et al., 2024). For instance, Prodigys decision to only let ηt increase and never decrease could be sub-optimal. And the decision to measure the angle between the gradient and the weight difference wt w0 has alternatives. One could instead use the most recent weight difference wt wt1. Lastly, in place of relying on the norm ratio }g}2{}g1} to implicitly convert the ℓ2 norm }wt w0}2 into the RMS norm }wt w0}RMS, one could consider more explicit method. For instance, we found rule akin to ηt`1 ηt ˆ p1 ` cos θq to work well in some preliminary experiments. Our time grows short, dear reader, and our third story draws to an end. We have argued that Prodigy without EMA is sign descentan example of steepest descentwith particular mechanism for warming up the step size. Starting with tiny initial step size, Prodigy multiplicatively increases the step size until the weights escape the initial locally linear region of the loss. Prodigys step size adjustment is based on the angle between the gradient and the total weight change. This is form of online line search. This highlights that once one has chosen norm, the steepest descent framework allows freedom to estimate the step size in various different ways. 9 Bernstein Newhouse"
        },
        {
            "title": "Epilogue",
            "content": "This anthology has presented new ways of understanding old optimizers. Proposition 1 decouples the optimizer design problem into two pieces: first choosing norm and second finding step size. This design space is already broad. We have argued that Adam chooses the infinity norm (Proposition 2) or equivalently the max-of-max norm (Proposition 3), which respects layered matrix structure. Shampoo chooses the spectral norm (Proposition 5). Prodigy chooses the same norm as Adam, and then uses heuristic to automatically warm up to good step size, as in Equation (28), which we term reaching escape velocity. Through the lens of steepest descent, the decisions that Adam, Shampoo and Prodigy make may seem arbitrary. In fact, we think that they are somewhat arbitrary. And there may be more principled ways to make these decisions. To demonstrate this point, we now introduce tool called the modular norm (Large et al., 2024) and its corresponding steepest descent algorithm. The modular norm generalizes the norms that appeared in Proposition 3 for Adam and Proposition 5 for Shampoo. Formally: Proposition 7 (Steepest descent under the modular norm) Given scalar coefficients s1, . . . , sL ą 0 and norms }}1, . . . , }}L, we define the modular norm as the mapping: W1, . . . , WL ÞÑ max ts1}W1}1, . . . , sL}WL}Lu . The corresponding steepest descent problem is given by: arg min W1,...,WL Lÿ xGl, Wly ` l1 ff λ Lmax l1 s2 }Wl} 2 , (31) (32) where x, denotes the Frobenius inner product, and for each 1, ..., the two matrices }Gk}: Wl and Gl are of the same shape. If we define the global step size η k, then the solution to Equation (32) is given by: k1 1 sk ř 1 λ Wl η sl arg max }Tl}l1 xGl, Tly for each layer 1, ..., L. (33) In words, steepest descent under the modular norm updates each layer in direction informed by that layers norm and with global step size computed as weighted sum of the dual norms of the gradients over layers. The proof of this proposition is given in Appendix B. When confronted with the modular norm, its natural to ask how one should assign norms to layers. And there are so many norms to choose from! Beyond the familiar ℓ2 Ñ ℓ2 spectral norm, many other induced operator norms are computationally tractable: Proposition 8 (ℓ1 Ñ ℓp and ℓp Ñ ℓ8 induced operator norms are tractable) For j1, and 1 ď ď 8: matrix Rmˆn with rows trowipM qum i1 and columns tcoljpM qun }M }ℓ1Ñℓp max }coljpM q}p; }M }ℓpÑℓ8 max }rowipM q} p1 . (34) In words, the ℓ1 Ñ ℓp operator norm is the largest ℓp norm of the columns; the ℓp Ñ ℓ8 operator norm is the largest dual ℓp norm over the rows. The proof is given in Appendix B. 10 Old Optimizer, New Norm: An Anthology To assign norm to layer, we believe that one should consider the role that layer plays in the neural network. For instance, since linear layers are typically used to map to and from vectors with roughly unit RMS norm, it is appropriate to equip linear layers with the induced RMS to RMS operator norm (Yang et al., 2023), which resolves to rescaled spectral norm. And since embedding layers map from one-hot vectors to vectors with roughly unit RMS norm, it is appropriate to equip embedding layers with the ℓ1 to RMS operator norm, which resolves to rescaled ℓ1 to ℓ2 operator norm. So embedding layers and linear layers should be equipped with different norms despite the weight space being matrix space in both cases. In short, the algorithm designer has freedom to choose input and output norms for layers that capture differences in how the layers are used; inducing the corresponding operator norm on the layers weights provides control over how the optimizer learns representations. We believe that picking the right norms could improve the speed and scalability of neural network training. We are seeing evidence that equipping neural network layers with better norms can lead to learning rate transfer across scale (Yang et al., 2023; Large et al., 2024). And since Shampoo won the external tuning track of the 2024 AlgoPerf competition (Dahl et al., 2023), it is garnering interest as fast training method. The second story in our anthology shows that Shampoo is closely connected to the spectral norm. In conclusion, this work highlights perspective on optimizer design as choosing two things: norm and step size. We have shown that three popular methodsAdam, Shampoo and Prodigyfit within this perspective. We hope that researchers can design improved training algorithms by choosing norms and step sizes more intentionally. Though this be madness, yet there is method int. Hamlet"
        },
        {
            "title": "Acknowledgements",
            "content": "We are grateful to Tim Large and Phillip Isola for invaluable discussions on the stories in this anthology. We also thank Jack Gallagher, Keller Jordan and Victor Butoi for very helpful conversations. 11 Bernstein Newhouse"
        },
        {
            "title": "References",
            "content": "Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order optimization for deep learning. arXiv:2002.09018, 2020. Cited on pages 5 and 15. Larry Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific Journal of Mathematics, 1966. Cited on page 9. Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic gradients. In International Conference on Machine Learning, 2018. Cited on page 3. Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, 2018. Cited on page 3. Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue. Automatic Gradient Descent: Deep Learning without Hyperparameters. arXiv:2304.05187, 2023. Cited on page 6. David Carlson, Volkan Cevher, and Lawrence Carin. Stochastic spectral descent for Restricted Boltzmann Machines. In International Conference on Artificial Intelligence and Statistics, 2015. Cited on pages 3, 6, and 7. David Carlson, Ya-Ping Hsieh, Edo Collins, Lawrence Carin, and Volkan Cevher. Stochastic spectral descent for discrete graphical models. Selected Topics in Signal Processing, 2016. Cited on pages 3, 6, and 7. Augustin-Louis Cauchy. Méthode générale pour la résolution des systèmes déquations simultanées. Comptes Rendus Hebdomadaires des Séances de lAcadémie des Sciences, 1847. Cited on page 9. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc Le. Symbolic discovery of optimization algorithms. In Neural Information Processing Systems, 2023. Cited on page 4. George E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson. Benchmarking neural network training algorithms. arXiv:2306.07179, 2023. Cited on pages 5 and 11. Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by D-adaptation. In International Conference on Machine Learning, 2023. Cited on page 8. John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal Machine Learning Research, 2011. Cited on page 5. 12 Old Optimizer, New Norm: An Anthology Kai Fan. Unifying the stochastic spectral descent for Restricted Boltzmann Machines with Bernoulli or Gaussian inputs. arXiv:1703.09766, 2017. Cited on pages 3 and 7. Vladimir Feinberg, Xinyi Chen, Y. Jennifer Sun, Rohan Anil, and Elad Hazan. Sketchy: Memory-efficient adaptive regularization with frequent directions. In Neural Information Processing Systems, 2023. Cited on page 15. Vineet Gupta, Tomer Koren, and Yoram Singer. unified approach to adaptive regularization in online and stochastic optimization. Technical report, Google Brain, 2017. Cited on page 5. Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, 2018. Cited on page 5. Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics, 2008. Cited on page 15. Maor Ivgi, Oliver Hinder, and Yair Carmon. DoG is SGDs best friend: parameter-free dynamic step size schedule. In International Conference on Machine Learning, 2023. Cited on page 8. Philip Kenneweg, Tristan Kenneweg, and Barbara Hammer. Improving line search methods for large scale neural network training. In International Conference on Artificial Intelligence, Computer, Data Sciences and Applications, 2024. Cited on page 9. Ahmed Khaled, Konstantin Mishchenko, and Chi Jin. DoWG unleashed: An efficient universal parameter-free gradient descent method. In Neural Information Processing Systems, 2023. Cited on page 8. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. Cited on page 3. Slobodan Lakić. On the computation of the matrix k-th root. Journal of Applied Mathematics and Mechanics, 1998. Cited on page 15. Kenneth Lange. MM Optimization Algorithms. Society for Industrial and Applied Mathematics, 2016. Cited on page 6. Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein. Scalable optimization in the modular norm. arXiv:2405.14813, 2024. Cited on pages 2, 4, 6, 10, and 11. Per-Gunnar Martinsson and Joel A. Tropp. Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica, 2020. Cited on page 15. Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameterfree learner. arXiv:2306.06101, 2023. Cited on page 8. Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham Kakade, and Lucas Janson. new perspective on Shampoos preconditioner. arXiv:2406.17748, 2024. Cited on page 5. 13 Bernstein Newhouse Martin Riedmiller and Heinrich Braun. direct adaptive method for faster backpropagation learning: The RPROP algorithm. In International Conference on Neural Networks, 1993. Cited on pages 3 and 9. Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, and Michael Rabbat. distributed dataparallel PyTorch implementation of the distributed Shampoo optimizer for training neural networks at-scale. arXiv:2309.06497, 2023. Cited on page 5. Matthew Streeter. Universal majorization-minimization algorithms. arXiv:2308.00190, 2023. Cited on page 6. Shiqing Sun and James C. Spall. Connection of diagonal Hessian estimates to natural gradients in stochastic optimization. In Information Sciences and Systems, 2021. Cited on page 3. Tijmen Tieleman and Geoffrey Hinton. RMSprop. Coursera: Neural Networks for Machine Learning, Lecture 6.5, 2012. Cited on page 3. Greg Yang, James B. Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv:2310.17813, 2023. Cited on page 11. Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham Kakade. Deconstructing what makes good optimizer for language models. arXiv:2407.07972, 2024. Cited on page 4. 14 Old Optimizer, New Norm: An Anthology Appendix A. Computational Strategies for Shampoo Let Rmˆn be gradient matrix with reduced SVD ΣV J. By Equations (15) and (16), the corresponding Shampoo update (with EMA disabled) is given by: η pGGJq1{4 pGJGq1{4 η J. (35) Here we list every means we know of computing or approximating this equation. First, we mention that pGGJq1{4 pGJGq1{4 pGGJq1{2 pGJGq1{2, so if one is willing to compute inverse matrix roots, one need only compute either pGGJq1{2 or pGJGq1{2, whichever has smaller dimension. With that said, to compute Equation (35), one may: 1. Do the SVD. Apply an SVD routine to compute , Σ and and just discard Σ. 2. Do sketching. Sketching is randomized method (Martinsson and Tropp, 2020) that can be used to approximate the SVD. See, for instance, Sketchy (Feinberg et al., 2023). 3. Do Newton iteration for inverse pth roots. Inverse matrix roots such as pGGJq1{2 can be computed via Newton iteration (Lakić, 1998). This is discussed in Chapter 7 of Higham (2008)s book. And see Anil et al. (2020)s paper. 4. Do Newton-Schulz iteration. We developed Newton-Schulz iteration for computing J, adapted from Equation 5.22 in Higham (2008)s book. In short, if we set X0 G{}G}ℓ2Ñℓ2 (or alternatively X0 G{}G}F ) and iterate: Xt`1 3 2 Xt 1 2 XtX t Xt, (36) 3 2 1 2 x3 and see that, for 0 ă ă then as Ñ 8, the sequence Xt Ñ J. To see this, one should plot the univariate cubic function pxq : 3, iterating this cubic will push closer and closer to `1. The final step is to realize that the effect of the iteration in Equation (36) is to apply this cubic pxq to each singular value of Xt. This also shows that the spectral normalization X0 G{}G}ℓ2Ñℓ2 is stronger than what is required: we need only ensure that X0 has all singular values greater than zero and less than 3 in order for the iteration to converge. ? ? Finally, there are in fact family of degree 2n ` 1 polynomial iterations of the form Xt`1 Xt ` XtX Xt ` pXtX 2Xt ` ... ` pXtX t qnXt (37) for suitable a, b, c, ..., that could be used instead of Equation (36). One should choose coefficients a, b, c, ..., so that the univariate polynomial gpxq ` x3 ` x5 ` ... ` x2n`1 is suitable approximation to signpxq. Which of these methods is most useful in practice may depend on factors such as the condition number of the matrix or the nature of the available computational resources. 15 Bernstein Newhouse Appendix B. Proofs Proposition 1: Steepest descent (38) (39) (40) (41) First, lets study the minimization under the change of variables t, where ě 0 encodes the magnitude and is unit vector (}t} 1) encoding the direction: ȷ ȷ min wPRn gJw ` λ 2 }w} 2 gJt ` min cě0 min cě0 min cě min tPRn:}t}1 min tPRn:}t}1 }g}: ` gJt ȷ λ 2 , 2 }t} ȷ λ 2 c2 λ 2 c2 ` Inspecting Equation (39), we see that the minimizer for the direction is given by: arg min tPRn:}t} gJt arg max tPRn:}t}1 gJt And similarly, by inspecting Equation (40), the minimizer for the magnitude is given by: arg min cě0 }g}: ` ȷ λ 2 c2 }g}: λ . (42) Multiplying these expressions, we obtain the minimizer for w, yielding the result. Proposition 2: Sign descent as steepest descent under the infinity norm The result follows by applying Proposition 1. We just need that arg max and also that the dual norm }g}: 8 }t}81 gJt gJ signpgq }g}1. : max }t}81 gJt signpgq, Proposition 3: Sign descent as steepest descent under the max-of-max norm The result follows from Proposition 7 by setting all the scalars s1, ..., sL to one and all the norms }}1, ..., }}L to the ℓ1 to ℓ8 operator norm. All we need is to show that the argmax at each matrix space 1, ..., satisfies: arg max }Tl}ℓ1Ñℓ8 1 trpGJ Tlq signpGlq. (43) But this holds because, by Proposition 8, }T }ℓ1Ñℓ8 maxi }colipT q}8 maxij Tij, and therefore all components in the argmax must be of unit size and gradient aligned. 16 Old Optimizer, New Norm: An Anthology Proposition 4: Projection to the closest semi-orthogonal matrix To begin, we observe that the minimizer over semi-orthogonal matrices of the distance }A G}F is the same as the maximizer over semi-orthogonal matrices of the alignment xA, Gy, where x, denotes the Frobenius inner product. This is because: 2 2 2 2 xA, Gy ` }G} , }A} }A G} (44) and the term }A}2 ř Now, let is fixed at }A}2 σi uivJ minpm, nq for semi-orthogonal matrix Omˆn. denote the SVD of G. Then the alignment satisfies: xA, Gy tr ÿ ÿ σi viuJ σi uJ Avi ď i ÿ σi, (45) where the second equality follows by the cyclic property of the trace, and the inequality is since being semi-orthogonal means that uJAv ď 1 for any two unit vectors and v. Next, observe that for the semi-orthogonal matrix ÿ ÿ ÿ xA, Gy σi j uJ ujvJ vi σi, ř uivJ , we have that: (46) since the tuiu and tviu are orthonormal. Comparing against Equation (45), we see that indeed maximizes the alignment, since it achieves the upper bound of σi. And therefore also minimizes the distance }A G}F amongst semi-orthogonal matrices A. Note that if is the matrix that has the tuiu as columns, and likewise for and the tviu, then this solution may equivalently be expressed as V J. ř All that remains is to explore the uniqueness of this solution: If is full rank, the solution is unique. being full rank means that all the singular values σi are positive. In this case, we see from Equation (45) that to maximize the alignment the semi-orthogonal matrix must satisfy uJ Avi 1 for all i. Since has spectral norm one, in turn this requires that Avi ui and AJui vi for all i. ř These conditions uniquely pick out . uivJ If is not full rank then the solution is not unique. This solution is just as good: A: uivJ ` uipviqJ. i:σią0 i:σi0 This completes the proof. ÿ ÿ (47) Proposition 5: Shampoo as steepest descent under the spectral norm First, we apply Proposition 7 with scalars s1, ..., sL set to one and all norms set to }}ℓ2Ñℓ2 . This tells us that the solution is given by Wl η arg max Tlq for each 1, ..., and with η . We just need to resolve the dual norm and evaluate the argmax. }Tl}l1 trpGJ k1 }Gk}: ℓ2Ñℓ2 ř 1 λ ř σi uivJ ΣV we have: Lets start with the dual norm. For matrix with SVD 17 Bernstein Newhouse }G}: ℓ2Ñℓ : max }T }ℓ2Ñℓ2 1 tr GJT tr ÿ ÿ σi viuJ σi uJ vi ď ÿ σi tr Σ, (48) where the upper bound follows from the spectral norm constraint on . But this upper bound is attained by setting (also resolving the argmax) and so }G}: ℓ2Ñℓ2 tr Σ. The uniqueness claim follows by the same argument as for Proposition 4. Proposition 6: Bounding the square loss of linear predictor First observe that the square loss is quadratic in so there are no cubic terms or higher. The bound must agree to first-order with the first-order Taylor expansion of LpW ` q, which is precisely LpW ` xW LpW q, y, since otherwise the bound would be violated for sufficiently small . To obtain the second-order piece of the bound, its easiest just to multiply out LpW ` and see that the second-order piece of LpW ` satisfies: 1 2n nÿ 1 dout i1 2 }W xpiq} 2 ď 1 2n nÿ 1 dout i1 }W } 2 2 ℓ2Ñℓ2 }xpiq} 2 1 2 din dout 2 }W } ℓ2Ñℓ2, (49) where the last equality uses the input normalization }xpiq}2 ? din. We are done. Proposition 7: Steepest descent under the modular norm For each layer 1, ..., L, we decompose Wl into its magnitude and direction: Wl clTl, for cl ě 0 and }Tl}l 1. Under this change of variables, the minimization becomes: Lÿ min W1,...,WL xGl, Wly ` l1 Lÿ λ 2 Lmax 2 s2 }Wl} ff ff λ 2 Lmax l1 ff c2 l min c1,...,cLě0 cl min }Tl}l1 xGl, Tly ` Lÿ l1 min c1,...,cLě0 min ηě Lÿ l1 cl}Gl}: ` Lmax l c2 s2 λ 2 ff }Gl}: ` λ 2 η , η sl (50) (51) (52) (53) where Equation (53) follows by observing that at the minimum we must have s1c1, ..., sLcL all taking the same value of η ě 0 (still to be determined), since otherwise we could increase by increasing any of the slack cl without paying penalty in terms of the sum ř cl}Gl}: 18 Old Optimizer, New Norm: An Anthology the max. We can now read off the minimizers from Equations (51) to (53): xGl, Tly arg max }Tl}l1 xGl, Tly; Tl arg min }Tl}l1 η sl 1 cl Lÿ ; 1 η λ k1 sk }Gk}: k. (54) (55) (56) Combining, we obtain the overall minimizer for each 1, ..., via Wl cl Tl η sl arg max xGl, Tly, where η is given by Equation (56), proving the result. Proposition 8: ℓ1 Ñ ℓp and ℓp Ñ ℓ8 induced operator norms are tractable Lets start with the ℓ1 Ñ ℓp operator norm. Here we observe that, in matrix-vector multiplication, each component of an input vector selects and scales column of the matrix: }M }ℓ1Ñℓp max }x}11 }M x}p max }x}11 ÿ coljpM qxj ď max }x}11 ÿ xj }coljpM q}p (57) }x}1 max }coljpM q}p (58) }coljpM q}p, (59) ď max }x}11 max by the triangle inequality and Hölders inequality. But the upper bound in Equation (59) is }coljpM q}p with the largest norm, then attained by selecting the column index arg max setting xj 1 and the other input components to zero. So }M }ℓ1Ñℓp maxj }coljpM q}p. Next, lets deal with the ℓp Ñ ℓ8 operator norm. Here we break up matrix-vector product in terms of the dot product between the vector and the matrix rows: }M }ℓpÑℓ8 max }x}p1 }M x}8 max }x}p1 max max max max }x}p1 }rowipM q}: p. xJrowipM xJrowipM (60) (61) (62) The proof is completed by recalling that the vector ℓp norm is dual to the vector ℓq norm for 1{p ` 1{q 1. In other words, }}: . }}"
        }
    ],
    "affiliations": [
        "MIT CSAIL, United States"
    ]
}