{
    "paper_title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "authors": [
        "Matthew Y. R. Yang",
        "Hao Bai",
        "Ian Wu",
        "Gene Yang",
        "Amrith Setlur",
        "Aviral Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b."
        },
        {
            "title": "Start",
            "content": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Matthew Y. R. Yang1, Hao Bai2, Ian Wu1, Gene Yang1, Amrith Setlur1 and Aviral Kumar1 1Carnegie Mellon University, 2University of Illinois Urbana-Champaign 6 2 0 2 0 2 ] . [ 1 9 0 2 4 1 . 1 0 6 2 : r Figure 1: Intervention training (InT) for improving credit assignment. InT proposes single-step interventions to replace incorrect intermediate steps in reasoning traces (1). Conditioned on these localized corrections, the model can generate counterfactual continuations that succeed where the original failed (2). We then perform supervised fine-tuning on these interventions, enabling effective credit assignment by upweighting the likelihood of the interventions in place of the mistakes. Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While natural remedy is to train process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying model-generated solution is easier than generating correct one from scratch, the model identifies the first error in its reasoning and proposes single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b. 1. Introduction Post-training large language models (LLMs) with reinforcement learning (RL) has proven to be highly effective strategy for improving their reasoning capabilities. As RL is scaled to more challenging tasks, training becomes increasingly dominated by incorrect rollouts, which are often long-horizon trajectories composed of many smaller, structured reasoning steps. Many such rollouts contain substantial portions of correct reasoning, yet outcome-based RL treats these correct steps the same as the critical mistakes Corresponding author(s): matthew.y.r.yang@gmail.com; This work was done at CMU. Project website: https://intervention-training.github.io/. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning that cause failure. This indiscriminate penalty can lead to undesirable behaviors including increased verbosity or frequent, premature shifts in the reasoning process, which have been widely observed when applying outcome-reward RL [5, 49]. Moreover, on sufficiently difficult problems where no correct rollout is produced at all, outcome-reward RL provides no learning signals, as advantages collapse to zero. In principle, addressing this challenge requires solving the problem of credit assignment: if we could identify the intermediate step at which reasoning trace goes astray and selectively penalize that step while reinforcing other possible promising steps, we could drive the model toward more favorable outcomes. However, existing approaches to credit assignment are difficult to apply in the LLM setting. For reasoning traces, estimating the credit (i.e., value) of each step typically requires running multiple branched rollouts conditioned on given prefix, which is prohibitively expensive [18, 21, 28]. Prior work attempts to amortize this process by training explicit value functions (or process reward models) [28, 39], but how we should train such functions over long reasoning traces remains an open question. Moreover, even with reasonably accurate value function, optimizing it to identify an alternate step is itself challenging [48, 63], because of the vast space of possible future steps. How can we perform credit assignment without training value function? Rather than explicitly learning value function, we ask the model itself to identify how failed trajectories can be locally corrected. This approach relies on an LLMs ability to implicitly perform value estimation and optimization jointly, in order to propose an alternative, improved step, which we refer to as corrective intervention. Our main idea is to reduce the task of generating end-to-end solutions to the substantially easier task of verifying individual steps, which can be accomplished via textual comparison against reference solution (i.e., textual diff ). In our method, we instruct base model to analyze the differences between reference solution and an incorrect reasoning trace that it previously generated, with the goal of identifying the first point at which the trace goes wrong and proposing single-step corrective intervention. When conditioned on this intervention, the model can then generate counterfactual reasoning traces that succeed from right before the point of failure even when the original rollout failed. We then train the model to internalize these interventions by applying supervised fine-tuning (SFT) on them, followed by RL. By fine-tuning the model on targeted single-step interventions, we selectively reduce the likelihood of the incorrect reasoning steps that the base model would otherwise produce, shifting it toward more favorable steps instead. Our approach remains notably simple and computationally efficient, as it avoids branched rollouts, explicit value-function training, or modifying the RL objective to include step-level rewards. Throughout this procedure, we never rely on larger model; instead, we leverage the asymmetry [41] in task difficulty between instruction-following, verification, and generation within the same model to perform credit assignment. We call this approach Intervention Training (InT). We compare InT to several alternative approaches that leverage reference solutions for learning, including methods that train policies directly on reference solutions or on self-generated reflections conditioned on those solutions. In settings where the base model is already strong (e.g., Qwen3 for mathematics) and the SFT dataset is relatively small (1K examples), we observe that InT makes particularly effective use of reference solutions to improve pass@洧녲 performance, which sets up the fine-tuned model for online RL. Notably, this coincides with InT producing more on-policy trajectories (those with high likelihood under the base model), whereas other approaches tend to produce more off-policy ones. After online RL, InT improves performance by nearly 10% on average across four challenging mathematical reasoning benchmarks, including 14% gain on IMO-AnswerBench [30], which consists of IMO-level problems curated by former medalists. These results demonstrate InT as simple and effective paradigm for improving the reasoning capabilities of LLMs, through improved credit assignment. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning 2. Preliminaries, Notation, and Problem Statement Learning objective. Training LLMs for reasoning typically involves an LLM 洧랢, model-generated reasoning traces 洧랢(x) produced on input prompts 洧랣, and binary reward 洧(x, y) {0, 1} quantifying the correctness of the final answer. The goal is to maximize average reward, given by: max 洧랢 洧냫(洧랢) := Ex洧랣,y洧랢(x)[洧(x, y)]. We can maximize the objective above by updating the policy with the policy gradient [50]: 洧랢 洧랢 + 洧띺 Ex洧냥train,y洧랢(x)[洧(x, y) 洧랢 log 洧랢(yx)], (1) where 洧냥train is the set of training problems and 洧랢 is the policy used to generate samples for training. In policy gradient methods such as GRPO [42], 洧랢 = 洧랢old is periodically updated copy of 洧랢, with additional correction terms in Equation 1 to account for the distribution shift between 洧랢old and 洧랢. We can also express SFT with similar update rule, where 洧랢 represents pre-collected offline dataset consisting of only correct trajectories, so that Equation 1 effectively maximizes likelihood on these traces. Credit assignment. Reasoning trajectories can naturally be decomposed into sequence of individually complete segments1, = (y0, y1, . . . , y洧녢 ), where each single reasoning step y洧노 may contribute unequally to the final answer. Ideally, learning signals should reflect this uneven contribution. However, in Equation 1, the policy is updated with binary rewards that entirely depend on the correctness of the final answer (outcome rewards). Outcome-reward RL normalizes these rewards across responses sampled for the same prompt x, yielding advantages. When the advantage 洧냢(x, y洧녰) = 洧(x, y洧녰) 1/洧녵 洧녵 洧녱=1 洧(x, y洧녱) is positive, every step in the response y洧녰 is uniformly reinforced; when it is negative, all steps are uniformly discouraged. With this form of reward allocation, steps that play no role in reaching the solution may still be upweighted, while steps that are locally correct but followed by later mistakes may be suppressed. In long-horizon settings where trajectories y洧녰 consist of hundreds of steps, as in LLM reasoning, the resulting noise can overwhelm the learning signal, preventing meaningful progress from occurring. These limitations of outcome-reward RL manifest in two ways. First, when training with large token budgets, rewards often plateau early and do not improve substantially over the course of training [29, 41, 54]. Second, responses become either excessively verbose or prematurely truncated, often failing to maximize the underlying outcome reward [5, 36, 49]. Avoiding these issues requires accurately identifying which steps are responsible for success or failure and disentangling correct steps from incorrect responses and incorrect steps from otherwise correct responses. This problem of determining how individual intermediate steps contribute to the final outcome is known as credit assignment. If we can estimate the credit of each intermediate step with step-level value function (also called Process Reward Models, or PRMs), we can update 洧랢 by encouraging the correct parts of solution while discouraging the incorrect ones. However, this approach faces two major obstacles in practice. First, reliably estimating such value functions is difficult due to its cost [18, 28, 39] and training instabilities [48, 63]. Second, even when sufficiently accurate PRM is available, simply penalizing incorrect parts of solution does not ensure that they will be replaced with higher-value, correct alternatives. As result, policy optimization remains challenging problem, since efficiently searching for suitable replacement steps is difficult task in itself. 1For the LLMs we study, this decomposition is implemented by splitting the generated text using the delimiter nn (see Figure 3), following prior work [36]. 3 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Problem statement. Our goal is to develop an effective approach to credit assignment. In particular, we focus on assigning credit within incorrect rollouts by pinpointing the incorrect steps that derail the solution. In our setting, we assume access to reference solution (e.g., human-written solution) for each problem in our training set, as they are commonly available in existing open-source math datasets. Why focus on credit assignment for incorrect traces? As we move toward more challenging RL tasks, incorrect rollouts dominate trajectories observed during training. For example, Figure 11 illustrates that, on curated set of Olympiad-level Math problems, more than 80% of rollout groups contain no successful trajectories at the start of training. Being able to extract useful learning signals from these failed rollouts would therefore tap into an abundant source of supervision that would otherwise be completely discarded by outcome-reward RL, as advantages within these groups collapse to zero. There are two additional reasons for focusing on incorrect rollouts. First, credit assignment is more difficult for longer trajectories, as the contribution of individual reasoning steps is harder to disentangle from that of subsequent steps. Prior work has shown that long trajectories overwhelmingly correspond to incorrect rollouts, with failed attempts averaging 10,000 to 15,000 more tokens than successful ones [11, 45, 49]. By focusing on incorrect rollouts, we target the regimes where credit assignment is most difficult. Second, while positive advantages reinforce the trajectories that lead to success, negative advantages only indicate suboptimality without specifying how the trajectories should be modified. This results in high-variance gradients and inflated entropy during training [41], making learning inefficient. Addressing credit assignment in incorrect rollouts is, therefore, critical to enabling sample-efficient learning especially when successful trajectories are rare. Where within incorrect trajectories is credit assignment needed? In Figure 2, we visualize the locations of the first error that derails incorrect trajectories. We find that these errors can occur at diverse set of points throughout the trajectory, often far from the initial steps, with more than 60% of the first errors arising after the 50th step. Consequently, effective credit assignment for incorrect rollouts requires method that can precisely identify errors occurring at many different points along trajectory. Figure 2: Location of first occurring mistakes in incorrect trajectories. Incorrect rollouts often do not begin with mistakes and may contain significant number of correct preceding steps/tokens. Detected by Gemini 2.5 Pro with Prompt 3. 3. Credit Assignment via Self-Proposed Interventions We develop scalable approach for credit assignment in LLMs. This reduces to two technical challenges: (i) identifying which reasoning steps are problematic and should receive low credit, and (ii) searching for alternative steps that attain higher credit. Most existing solutions to problem (i) rely on estimating value functions, which remains computationally and statistically expensive for LLMs. Approaches to problem (ii) typically require step-level on-policy RL; however, given the inherent difficulties of RL training for LLMs, these methods introduce an additional layer of complexity for practitioners. In this section, we introduce an alternative approach to credit assignment that bypasses these challenges by leveraging the difficulty gap between the tasks of instruction-following and problem-solving for LLMs. Main idea. Our approach collapses the two steps above into single procedure that directly proposes an improved reasoning step conditioned on the policys current trajectory. While such procedure might 4 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 3: Example mistakes encountered by Qwen3-4B-Instruct and proposed interventions. By correcting the mistakes with interventions, the rollouts lead to correct final answers as opposed to originally incorrect ones. appear to require stronger LLM, we show that meaningful corrective steps can be generated using the very same model. The key is to exploit the asymmetry in task difficulty between generating correct solutions from scratch and verifying individual steps by comparing against the reference solution. In practice, base models are often far more reliable at the latter: they can compare generated reasoning trace against known correct solution even when they cannot produce the solution themselves. Accordingly, we instruct the base model to use test-time compute to effectively perform diff between reasoning trace and human-written reference solution, identify the step at which the trace goes astray, and propose single-step corrective intervention. Next, we explain our approach for implementing this idea. 3.1. Proposing Interventions through Self-Verification We leverage instruction-following LLMs to verify their own reasoning traces and propose counterfactual steps that lead to improved outcomes, circumventing the need for expensive rollouts or explicit training and optimization of PRMs. We implement this idea via two-stage prompting procedure. First, given model-generated reasoning trace, we instruct the base model to verify each step of the trajectory for logical correctness, conditioned on access to reference solution. This produces list of reasoning steps that are potentially incorrect, which we require the model to index explicitly by their position in the trajectory. We then focus on proposing corrective alternative for the first incorrect step. This design choice is motivated by the fact that the initial error must be corrected for an incorrect rollout to become correct. Moreover, errors that occur later in the trajectory are often consequences of earlier mistakes; once an early incorrect step is fixed, the remainder of the trajectory is unlikely to follow the same erroneous path, rendering corrections to later errors potentially unnecessary. with step number 洧노*, we proceed to the second stage, in Upon identifying the first incorrect step y洧노* which the model generates an alternative next step y洧노* . We refer to this alternative as an intervention, whose purpose is to steer the remainder of the trajectory toward the correct answer. We provide concrete examples of generated single-step interventions in Figure 3, with additional examples using single in Appendix and B. Concretely, we obtain the error location 洧노* and corrective step y洧노* to replace y洧노* 5 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Intervention Proposed By Rollouts Are Conditioned On Coverage Accuracy Omni-MATH subset with pass@128 = 0, values estimated with 32 rollouts problem 1. - 2. - problem + correct prefix y<洧노* problem + correct prefix y<洧노* + original step y洧노* 3. - problem + correct prefix y<洧노* + intervention y洧노* 4. 4B-Instruct (Ours) problem + correct prefix y<洧노* + intervention y洧노* 5. 4B (Ours) problem + correct prefix y<洧노* + intervention y洧노* 6. 30B-A3B-Instruct (Ours) problem + correct prefix y<洧노* + intervention y洧노* 7. 4B-Instruct w/o. ref. sol. 40 / 334 31 / 334 29 / 334 80 / 334 73 / 334 101 / 334 0.0984% 0.0726% 0.0713% 1.56% 2.65% 2.87% 47 / 0.171% Subset with hint-guided pass@16 = 0, values estimated with 16 rollouts problem + hint + correct prefix y<洧노* 8. - problem + correct prefix y<洧노* + intervention y洧노* 9. 4B-Instruct (Ours) problem + hint + correct prefix y<洧노* + intervention y洧노* 10. 4B-Instruct 11 / 176 18 / 176 25 / 176 2.38% 3.05% 4.62% Final Training Set 11. 4B-Instruct (Ours) problem + correct prefix y<洧노* + intervention y洧노* 1076 / 4048 4.27% Table 1: Rollouts conditioned on proposed interventions and alternative strategies. Accuracy measures the average reward, and coverage measures the number of problems that see at least one correct rollout. Conditioning rollouts on our proposed interventions yields the highest coverage and average reward on the hard problem set, outperforming approaches that do not use interventions (e.g., na칦ve rollouts or conditioning on hints) and those that generate interventions under weaker supervision (e.g., without reference solutions). We also find that our interventions can be combined with existing approaches such as hint-guided rollouts (see rows 8-10) to obtain stronger results. 4B-Instruct denotes Qwen3-4B-Instruct-2507, 4B denotes Qwen3-4B, and 30B-A3B denotes Qwen3-30B-A3B-Instruct-2507. query 洧랢( x, y, pInT); the instruction pInT is provided in Box 2. Our prompt borrows components from Huang & Yang [14] to produce step-by-step verification logs of the trajectory and to identify the first mistake by classifying each step. Because 洧노* and y洧노* are generated by the same model that produced the original y, our approach enables credit assignment via self-generated feedback. Additionally, in the few cases where the intervention contained the final answer itself; we discard these cases to prevent any leakage from the reference solution, so that the model does not simply learn to memorize the final answer, but rather learns to complete the solution. Next, we evaluate whether interventions generated in this manner results in successful rollouts on problems that the base model could not solve at first. , followed by the intervention y洧노* 3.2. Do Interventions Improve Success Rates? We empirically evaluate self-generated interventions using the Qwen3-4B-Instruct-2507 base model by measuring whether proposed intervention improves the probability of success on rollouts that would otherwise fail. Specifically, we concatenate the problem with the reasoning steps preceding the identified error y<洧노* . We then sample continuations from the model by conditioning on this prefix, i.e., 洧랢( x, y<洧노*, y洧노*) (row 4 4B-Instruct in Table 1), and evaluate the resulting trajectories to obtain an unbiased estimate of the expected reward when continuing from the intervention. In Figure 4 (left), we isolate the effect of interventions by comparing this value against two baselines: (1) continuing the rollout from the erroneous step without applying the intervention 洧랢( x, y<洧노*, y洧노*) (row 3 above 4B-Instruct), and (2) resampling from the step immediately preceding the error 洧랢( x, y<洧노*) (row 2). Our results show that interventions increase the average reward from 0.0713% to 1.56%. This 22 increase demonstrates that while the base model may be unable to solve these problems initially, it can leverage self-verification capabilities to generate single-step interventions and successfully steer the model toward correct solution. y洧노* to replace the original erroneous step y洧노* 6 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Beyond average reward, we also examine if utilizing our proposed interventions enables the model to solve new problems. To assess coverage, we compute the number of problems on which we can generate at least one correct rollout (out of 32) under each of the three settings in Figure 4 (right), which correspond to rows 2, 3, 4 in Table 1. Continuing from interventions achieves the highest coverage, solving 80 out of 334 unique problems compared to 29 out of 334 when continuing from the error. This justifies the efficacy of interventions, and further suggests that finetuning models to internalize interventions could potentially increase the fraction of problems receiving non-zero rewards during downstream RL, which would enable learning across broader range of tasks. Figure 4: Performance comparison of interventions. Continuing the rollout by concatenating the proposed intervention step y洧노* to the prefix y<洧노* yields substantially higher accuracy (left) and larger number of unique problems solved (right) than either continuing with the original erroneous step y洧노* or ). continuing directly from the prefix without any intervention ( Results: Interventions outperform hint-guided rollout generation. Having established that interventions improve reward and enable solving problems that were previously unsolvable, we now compare this approach to alternative ways of leveraging reference solutions. As discussed in Section 6, several concurrent works incorporate reference solutions into RL by conditioning the policy on partial solution prefixes as hints. These hints exploit the instruction-following capabilities of base models to steer generation toward the correct region of the solution space, enabling successful completions on otherwise difficult problems. Since hints cannot directly reveal the solution, we construct them by randomly sampling prefix from the initial one-third of the reference solution. We study whether hint-guided rollouts are more effective than intervention-based rollouts, and whether the two approaches can be combined. Table 1 compares these settings. Conditioning on interventions alone (row 9; 洧랢( x, y<洧노*, y洧노*)) achieves higher problem coverage (18/176 problems solved) than hint-guided rollouts (row 8) that solves only 11/176 problems. However, the two methods are complementary: conditioning on both hints and interventions (row 10) yields the best overall coverage (25/176 problems solved). This result is perhaps natural: hints guide the model toward promising initial direction, while interventions enable fine-grained credit assignment when errors arise in the middle of rollout. Ablation: Reference solutions improve effectiveness of interventions. We next isolate the role of reference solutions in generating effective interventions. In particular, we test the alternative hypothesis that the verificationgeneration asymmetry of the base model alone [40] is sufficient to produce highquality interventions. To do so, we evaluate an ablation shown in Table 1, row 7, where we prompt the model to propose an intervention given only an incorrect reasoning trace, without the reference solution. On the one hand, conditioning on interventions outperforms the na칦ve baselines (rows 13), doubling accuracy and increasing coverage by 7, supporting the claim that effective interventions can be yielded even without privileged information from reference solutions. On the other hand, interventions generated in this manner are significantly weaker than those generated with reference solutions (row 4), which clearly highlights the critical role of reference solutions in producing high-quality interventions. Ablation: Instruction-following capabilities of the base model are critical. We find that the instructionfollowing ability of the base model is key determinant of intervention effectiveness. In row 5 of Table 1, we evaluate Qwen3-4B, model optimized primarily for reasoning rather than instruction-following. Despite its strong performance on math reasoning benchmarks, this model frequently fails to adhere to the instructions required for intervention generation. For example, it may switch to directly solving 7 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning the problem midway through comparing the reasoning trace with the reference solution, or neglect to enclose proposed interventions within the required <intervention></intervention> tags. Such instruction-following failures prevent the model from producing usable interventions, resulting in lower coverage (73 out of 334, row 5) compared to self-generated interventions from the instruction-tuned model (80 out of 334, row 4). These results demonstrate that effective intervention generation depends not only on raw reasoning ability, but also on robust instruction-following abilities. Ablation: More capable base models propose better interventions. Finally, we study how the size of the model used to propose interventions affects performance. In addition to using the base model, Qwen3-4B-Instruct-2507, which also generates the original reasoning traces, we experiment with larger model from the same family, Qwen3-30B-A3B-Instruct-2507 (row 6), to generate interventions. Using the larger model yields an approximately 2 improvement in accuracy and solves 21 additional problems out of 334, indicating that the quality of generated interventions scales with model capacity. Takeaways: Effectiveness of our proposed interventions Conditioning rollouts on proposed interventions increases success rates by 20. Hint-guided rollout generation is complementary to interventions. Both improve overall reward. Instruction-following capability is critical: the instruction-tuned 4B-Instruct model solves 7 more problems (out of 334) than the reasoning-focused 4B model. Intervention quality scales with model capacity: interventions proposed by 30B-A3B-Instruct achieve 2 higher accuracy and solves more problems compared to 4B-Instruct. 4. InT: Intervention Training for Credit Assignment Next, we use the generated interventions for training. Our training procedure is deliberately simple: we perform supervised fine-tuning on the collected intervention data to patch the base model, preparing it for subsequent RL training. Then RL training goes as usual, but now it can learn from these very challenging problems that would have otherwise largely produced incorrect rollouts during RL. As we show in our experiments, this patching step is sufficient to enable the model to solve previously unsolvable problems and leads to improvements in performance. We now describe our training procedure in detail. 4.1. Training via Supervised Fine-Tuning on Intervention Data natural starting point for leveraging generated interventions during training is by running supervised fine-tuning (SFT) on the collected intervention data. The goal of this step is to teach the model to internalize the corrective patterns expressed by interventions, so that at test time, when reference solutions are no longer available, it can spontaneously generate similar corrective steps. This immediately raises the central design question surrounding this simple training procedure: which parts of an intervention-guided rollout should be used in training? Consider generating full solution trajectory using an intervention. For problem and an initial response 洧랢( x), we generate an intervention y洧노* 洧랢( x, y, pInT) to replace the erroneous step y洧노* Configuration Prefix, no suffix, filter - no filter - no prefix + suffix Coverage Accuracy 202/235 196/235 162/235 111/235 7.71% 5.06% 2.87% 2.31% Table 2: study of SFT design choices for learning from interventions. We compare different SFT configurations: whether the prefix y<洧녰 is cloned, the suffix y>洧녰 is included, and interventions are filtered to retain only those that yield at least one correct rollout. Cloning the prefix and excluding the suffix while applying the correctness filter yields the highest coverage and accuracy. The 235 problems were subsampled from DeepScaleR. . We then continue 8 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 5: Intervention training (InT). InT verifies incorrect rollouts against reference solutions to identify the step 洧노* at which the first error y洧노* ) occurs, proposes alternative step y洧노* ), and performs SFT on these steps before RL. , the suffix y>洧노* lead to correct final answer. the response by sampling y>洧노* 洧랢( x, y<洧노*, y洧노*). Given the resulting intervention-guided trajectory, we consider several design choices for SFT: whether to train on the prefix y<洧노* , and whether to include only the interventions whose continuations y>洧노* Main idea. We hypothesize that cloning only the proposed intervention without its preceding prefix would be insufficient. If the prefix y<洧노* is not reinforced during SFT, the fine-tuned model may generate for the same problem at test-time, introducing different errors than those in alternative prefixes the original rollout that was used for training. In such cases, the intervention y洧노* is no longer relevant. While one could in principle mitigate this issue by collecting multiple prefixes from the base model and generating interventions for each, doing so is impractical: covering the combinatorial space of possible prefixes is already expensive, and anticipating which prefixes the fine-tuned model will produce is even more challenging as the output distribution 洧랢( x) shifts after fine-tuning. Motivated by these and the proposed considerations, we follow Qu et al. [34] and instead choose to clone both the prefix y<洧노* intervention. We observe in our results in Table 2 that this choice leads to substantially improved coverage and accuracy, solving 40 more problems out of 235 compared to not cloning the prefix. <洧노* is beneficial. We hypothesize that cloning correct Next, we examine whether cloning the suffix y>洧노* suffix could reduce the space of future sequences explored during subsequent RL training, thereby limiting exploration which is problematic. Empirically, we find that this is indeed the case and cloning the suffix is detrimental. As shown in Table 2, training on full successful trajectories (y<洧노*, y洧노*, y>洧노*) nearly halves the number of unique problems solved compared to training only on the prefix and intervention (y<洧노*, y洧노*). Finally, we observe that filtering interventions to retain only those that lead to at least one correct continuation, evaluated over 32 rollouts, further improves post-SFT performance by 6 out of 235 problems. In summary, the optimal SFT configuration consists of cloning the prefix and intervention, excluding the suffix, and applying correctness filter to retain interventions that admit at least one successful continuation. Our final SFT objective is given by: 洧랢洧냫(洧랢) Ex洧냥train,y洧랢(x) s.t. 洧(x,y)= [ 洧랢 log 洧랢(y洧노* y<洧노*) + ] 洧랢 log 洧랢(y洧노 y<洧노) . (2) 洧노* 洧노= 4.2. Fine-Tuning via Reinforcement Learning After fine-tuning the base LLM on intervention data for small number of steps, we proceed with RL posttraining using standard outcome-reward methods such as GRPO [42]. If the model has successfully internalized the intervention patterns, its rollouts should avoid many of the errors observed in the base model 9 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning while reinforcing previously correct behaviors. Starting RL from this SFT-initialized model then further amplifies these corrective behaviors, suppresses unproductive reasoning segments, and enables the extraction of learning signal from rollouts that previously yielded none. We refer to our approach of training LLMs on intervention data as InT (Intervention Training). Our practical approach shown in Algorithm 1 first patches the LLM on intervention data (Line 7) and then subsequently runs RL. We provide schematic illustration in Figure 5. Algorithm 1 InT: Intervention Training Require: Base LLM 洧랢, Intervention generation prompt pInT, Problems train 洧 1: Data Collection: 2: for each 洧 3: Generate 4: 洧노*, y洧노* 洧랢( 5: InT InT 洧 洧 6: end for 7: Patching: 洧랢 8: RL training: 洧랢 9: return 洧랢 x) {} InT 洧 train do 洧랢( x, y, pInT) (error location, intervention) SFT(洧랢, (x, concat(y<洧노* , y洧노* )) InT) 洧 RL(洧랢, train) 洧 Summary: Intervention Training (InT) Performing SFT on the proposed interventions from Section 3 with the prefix, without the suffix on filtered traces leads to the best performance and provides good initialization for online RL. 5. Experimental Evaluation of InT The goal of our experiments is to evaluate the effectiveness of InT in improving credit assignment. We compare models trained with InT against alternative approaches on challenging problems where standard RL training predominantly produces incorrect rollouts. We additionally perform RL post-training on top of models produced by InT and competing methods, and report the resulting performance. Overall, we find that InT achieves the strongest performance across all settings. In particular, it reduces the fraction of problems that yield only incorrect rollouts during RL, consistently increases pass@洧녲 across all evaluated values of 洧녲, and delivers strong gains on standardized benchmarks, as detailed below. 5.1. Experimental Setup, Comparisons, and Evaluation Metrics As discussed in the problem statement in Section 2, we focus on RL training over hard problems. To construct dataset of such problems, we draw from the following sources: (1) Polaris (53k problems), filtering for instances that achieve zero accuracy under 64 rollouts; (2) AceReason-Math (50k problems), similarly filtered for zero accuracy under 64 rollouts; (3) Omni-MATH (4.4k problems), filtered for zero accuracy under 128 rollouts; and (4) IMO-AnswerBench (360 problems), after removing those reserved for the test set. Reference solutions are available for most problems from Polaris, AceReason-Math [29], and Omni-MATH [10]; for the remaining problems, we generate reference solutions using Gemini. After applying these difficulty filters, we obtain pool of approximately 4500 hard problems. We then generate interventions for this set and retain only those that yield non-zero reward in at least one of 32 rollouts, resulting in 1076 problems with corresponding interventions. As shown in Table 1 (top), we further select subset of 334 problems to compare na칦ve rollouts against rollouts with interventions. Baselines and comparisons. To evaluate the efficacy of InT, we compare it against several alternative approaches for patching model behavior on 洧냥train, followed by running RL. Our primary comparisons are: (1) Reference Solution SFT + RL, which distills reference solutions into the base model prior to RL; (2) Standard RL, which applies RL directly to the base model; (3) Self-Reflection SFT + RL, where the model is prompted to rewrite entire solutions rather than propose single-step interventions (see Appendix E), followed by SFT and RL; and (4) Hint-guided RL, which runs RL on int-augmented InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 6: Performance of the patched model. Supervised fine-tuning (SFT) on interventions increases pass@洧녲 on both the training set and IMO-AnswerBench for 洧녲 = 16, 32, . . . , 1024. We evaluate on 32 randomly sampled problems from each split. To match the RL training setup, we sample with temperature = 1.0, top_p = 1.0, and top_k = problems (we use the dataset released by [35], recent hint-guided RL method targeting problems of comparable difficulty). For all baselines and for InT, we run GRPO for 400 training steps. Cost analysis of PRM training. natural comparison to our approach is to train process reward model (PRM) and use step-level rewards derived from it. However, high-quality PRMs are currently not available for the Qwen3 models we use in this work. While prior studies have demonstrated that PRMs can be trained and utilized under favorable conditions [39], doing so requires substantial training and data collection compute. For example, the Qwen2.5-Math-PRM series relied on roughly three million rollouts together with complex filtering pipelines. Additionally, recent work suggests that existing PRMs tend to underperform on difficult problems [8]. Given these challenges, we do not include PRM-based comparison, and focus on alternative methods that leverage reference solutions to improve performance. Compared to PRM-based approaches, InT provides much simpler and more scalable mechanism for credit assignment. Evaluation benchmarks and metrics. Prior work on RL-trained reasoning models primarily evaluates performance on intermediate competition math benchmarks such as AIME 2025 and HMMT 2025. However, these benchmarks are already quite old compared to release dates of current base models. To address this limitation, we evaluate InT on substantially harder standardized benchmarks. Specifically, we report results on IMO-AnswerBench [30], AMO-Bench [2], and Apex Shortlist [3], all of which consist of Olympiad-level problems released after the public release of our base model, Qwen3-4B-Instruct2507. In addition, we scrape the November 2025 HMMT competition from the official HMMT website to evaluate on problems constructed after the release of the base model, further reducing the risk of training-set contamination. For IMO-AnswerBench, we manually select 40 problems: 10 each from Algebra, Combinatorics, Geometry, and Number Theory, restricting to problems with easily verifiable answers (e.g., integers or simple fractions) to ensure that evaluation reflects mathematical reasoning rather than sensitivity to answer parsing. We apply an analogous filter to AMO-Bench, removing problems whose answers cannot be verified automatically (e.g., proof-style responses). 1. 5.2. Results: SFT with Interventions We first examine the performance of InT immediately after SFT on intervention data (Section 4; patching). Prior work suggests that RL in LLMs primarily reallocates probability mass toward already-existing successful trajectories, effectively converting high pass@洧녲 in base model into high pass@1 after RL training [12, 56, 57]. From this perspective, pass@洧녲 serves both as diagnostic of whether the model is capable of sampling correct solutions and as an upper bound on downstream accuracy achievable through subsequent RL training. Motivated by this view, we compare the pass@洧녲 of the SFT model to that of the base model on both the training and test sets in Figure 6. The SFT model consistently achieves higher pass@洧녲 across 洧녲 = 16, 32, . . . , Figure 7: Average probability of intervention tokens. Intervention tokens receive higher probability under the trained model vs. the base model on both the training and test sets, indicating that SFT causes the model to learn to produce intervention-like steps even on unseen test problems. 11 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 9: InT trains on tokens that are more likely under the base model and attains highest performance on both train and test problems. (left) Average negative log-likelihood (NLL) computed over 64 sampled traces. Ref. Sol. denotes reference solutions written by humans or the Gemini-2.5-Pro model. R1 Think and R1 Summary correspond to content inside and after the DeepSeek-R1 <think> tags, respectively. Self-reflect refers to the self-reflection baseline in which the model is prompted to rewrite entire incorrect solutions given reference solution. InT trains on the most on-policy traces as it exhibits the lowest NLL. (middle) Train pass@洧녲 on 64 sampled training problems. InT achieves the highest pass@洧녲 across all values of 洧녲. (right) Test pass@洧녲 on IMO-Bench, AMO-Bench, and Apex Shortlist. InT again attains the best performance across all 洧녲. on both splits, indicating that fine-tuning on intervention data enriches the distribution of successful trajectories and provides strong initialization for RL. In the next section, we show that further RL training from this checkpoint indeed translates these gains into improved final performance. We also find that likelihood of intervention tokens increases post-SFT (Figure 7), indicating that InT enables the model to internalize interventions successfully. Next, we compare InT against alternative approaches to generate corrections to incorrect rollouts. Specifically, we evaluate performance after running SFT on Self-Reflection traces, Reference solutions, and thinking traces from DeepSeek-R1 (R1 Think and R1 Summary, which correspond to content inside and after the <think> tags, respectively), to compare InT with fine-tuning on traces generated from more capable LLM. Across both training and test sets, we find that InT consistently improves pass@1 and pass@洧녲 relative to all comparisons in Figure 9(train middle, test right). Finding: InT traces are more likely under the base model. Beyond improvements in pass@洧녲, InT generates traces for SFT that yield the highest likelihood under the base model. Figure 9(left) reports the average negative log-likelihood (NLL) of tokens drawn from trajectories generated by different sources, including human-written reference solutions, Gemini-generated reference solutions, DeepSeek-R1 outputs, as well as self-reflection and InT trajectories. We find that InT trajectories are the most likely under the base model distribution, followed by self-reflection, R1 outputs, and reference solutions. This observation is important because fine-tuning on highly off-policy traces is known to be problematic. Prior work [17] shows that such traces are often memorized by the model, and fitting them can significantly distort the base models next-token distribution. Consistent with this perspective, Figures 9(middle, right) show that the relative performance of different methods closely tracks the ordering of their NLL values. While this correlation is not necessarily causal, it motivates closer examination of why highly off-policy traces pose challenges for fine-tuning. Figure 8: Entropy of the nexttoken distribution for various models. Fine-tuning on highly off-policy traces leads to elevated entropy, while intervention-based SFT preserves low-entropy distribution closer to the base model, yielding more stable initialization for subsequent RL. Why does cloning off-policy traces perform poorly? Fine-tuning on highly off-policy traces requires shifting probability mass away from tokens with high likelihood under the base model toward tokens it considers unlikely. This redistribution naturally increases the entropy of the next-token distribution, an 12 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning effect we observe empirically in Figure 8. Such high-entropy initializations are problematic for subsequent RL training, as they lead to overly random rollouts that hinder effective exploration and learning. One might attempt to mitigate this issue by training for longer, allowing the model to eventually assign higher likelihood to the off-policy tokens. However, this introduces different failure mode: prolonged fine-tuning on off-policy data (e.g., human-written reference solutions) distorts the base models existing reasoning patterns, since these traces differ substantially from the behaviors and skills the model has already learned. As result, cloning off-policy trajectories performs poorly regardless of whether training is stopped early or run for longer, and might not produce good RL initializations. Figure 10: Interventions are short. Top: Intervention are typically under 200 tokens. Bottom: full rollouts are nearly 7k tokens on average. In contrast, InT generates traces with most tokens coming from the base model because interventions are short compared to full rollouts 10, leading to trajectories that with higher likelihood under the base model. This avoids large shifts in entropy, as shown in Figure 8: the resulting entropy of the next-token distribution after applying InT is roughly comparable to that of the base model and can thus support effective exploration during subsequent RL training. 5.3. Results: InT + RL Solves Most Hard Training Problems Next, we show that on our curated training set of difficult problems, initializing RL from the patched checkpoint produced by InT leads to the largest increase in reward and the greatest reduction in the fraction of problems that produce no correct rollouts (the zero-advantage ratio), as shown in Figure 11. This result confirms that training on interventions can effectively help avoid errors made by the base model during RL. Moreover, we observe similar effect on test problems. On two unseen problems selected from the IMO shortlist, InT is able to correct errors made by the base model, as illustrated qualitatively in Appendix F. Figure 11: Training reward and zero-advantage ratio during RL. Left: Average reward over RL training iterations on the hard training set. Right: Zero-advantage ratio, defined as the fraction of problems for which the model never produces correct rollout. Initializing RL from the intervention-patched SFT checkpoint (InT) yields both higher reward and substantially lower zero-advantage ratio, indicating that intervention training reduces persistent failure modes and enables effective learning from problems that previously yielded no signal. The next best approach is Self-Reflection SFT + RL, but as we show in Table 4, this approach leads to much weaker test performance. 5.4. Results: InT Expands the pass@洧녲 Frontier on Test Problems We next present the main results of InT on the full evaluation set described in Section 5.1. Specifically, we track pass@洧녲 performance across RL training iterations from 0 to 400 for three models: (i) the base Qwen3-4B-Instruct model, (ii) Qwen3-4B-Instruct fine-tuned on reference solutions, and (iii) Qwen3-4BInstruct patched with self-generated interventions using InT. We find that initializing RL from the model produced by InT leads to the largest improvements in pass@洧녲 throughout training. As summarized in Figure 12, InT outperforms both the base model and reference solution SFT across RL iterations. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 12: Pass@洧녲 across RL training iterations on test problems. We plot pass@洧녲 from 0 to 400 RL iterations for three model initializations: (i) the base model patched with InT, (ii) the base model fine-tuned on reference solutions directly via SFT, and (iii) the base model itself. Across all values of 洧녲, InT achieves the highest pass@洧녲 through training (right). 5.5. Results: Performance on Standardized Benchmarks In the previous section, we showed that InT pushes the pass@洧녲 frontier across all values of 洧녲 on both training and test problems. We now examine whether these gains translate into improved pass@1 performance on held-out benchmarks. To this end, we evaluate InT on four challenging reasoning benchmarks: IMO-AnswerBench, HMMT 2025 (November), AMO-Bench, and Apex Shortlist. As shown in Table 4, InT achieves an average score of 33.72 across these benchmarks, corresponding to 59% relative improvement over the base model (21.17) and 19% improvement over the standard SFT+RL baseline (28.26). On IMO-AnswerBench, high-quality test set curated by IMO medalists, InT attains score of 25.62, more than double the base model performance (11.68) and surpassing standard RL (23.46). Qwen3-4B-Instruct-2507 DeepSeek-R1-0528-Qwen3-8B gpt-oss-20b Qwen3-4B-Instruct-2507 + InT + RL (Ours) Model Table 3: Evaluation on IMO-AnswerBench. Performed different context lengths following official recommendations. 11.68% (16K) 18.44% (32K) 23.36% (32K) 25.62% (16K) IMO-AnswerBench Moreover, we find that performing SFT on the reference solution does not synergize well with subsequent RL, attaining an average score of 20.76 that is lower than that of the untuned base model (21.17). This suggests that na칦vely fine-tuning on off-policy reference solutions may restrict effective exploration or induce overfitting that hinder downstream RL, as previously discussed in Section 5.2. In contrast, InT does not exhibit this degradation. By remaining largely on-policy, it enables RL to further improve performance, as reflected by consistent gains across all metrics. While Self-Reflection SFT shows promise on certain benchmarks (e.g., 36.72 on AMO-Bench), it generalizes less robustly than InT. On IMO-AnswerBench, Self-Reflection attains only 15.53, compared to 25.62 for InT. Finally, we note that the training set itself is highly challenging: the base model achieves only 5.53 accuracy prior to any fine-tuning, underscoring its difficulty. InT achieves the highest reward value of 28.83, boosting the training reward higher than any other method while not falling into memorization traps of off-policy SFT. Summary: Intervention Training (InT) Results SFT data that have lower likelihood under the base model (more off-policy) tend to produce finetuned models with higher entropy and worse pass@洧녲 performance across 洧녲 = 1, 2, ..., 128. InT leads to the greatest increase in training reward and decrease in zero-advantage ratio. InT achieves the highest test accuracy on variety of difficult benchmarks, including nearly 14% increase in IMO-AnswerBench, surpassing gpt-oss-20b with 4B model. 14 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Model Base + RL + Hint-guided RL + SFT on ref. solutions + RL + SFT on self-reflections + RL IMO-AnswerBench HMMT 2025 Nov AMO-Bench pass@8 Apex Shortlist pass@8 洧냥train 5.53 13.47 23.06 19.07 23. 11.68 23.46 16.89 11.56 15.53 41.61 46.46 47.27 27.45 38.65 20.79 22.72 22.23 20.51 23.93 26.24 35.21 33.34 25.19 36.72 Average 21.17 28.26 28.56 20.76 27. + InT + RL (Ours) 25.62 49.77 36.16 28.22 28. 33.72 Table 4: Performance on challenging math benchmarks. Pass@1 (IMO-AnswerBench, HMMT 2025 Nov) and pass@8 (AMO-Bench, Apex Shortlist) estimated using 128 rollouts for different training methods, all based on Qwen3-4B-Instruct-2507. 洧냥train reports the average train reward at step 400, smoothed using an exponential moving average. InT achieves the strongest overall performance followed by hint-guided RL, standard RL, and other SFT-based approaches. 6. Related Work Credit assignment in LLM reasoning. The effectiveness of long-horizon RL with outcome rewards [7, 15, 16, 46] is often crippled by credit assignment: it is unclear which intermediate steps in long response should be credited for the outcome reward. Credit assignment is usually not problem when responses are short and the model can produce enough successful rollouts to contrast them against incorrect ones, but can pose as significant bottlenecks when the horizon increases [9, 40, 52]. While most methods reward each token with the outcome-level advantage [26, 55, 59], others use process reward models (PRMs) to assign dense token or step-level rewards [24, 36, 47] that can reinforce correct steps and promote unlearning of incorrect ones. Although PRMs may improve RL compute efficiency [38, 39], the notion of what process reward should measure and how to calculate it precisely is not yet settled. Some work derives process rewards by running branched rollouts which is costly [18, 28, 39], while other work trains PRMs with human annotations and suffers from reward hacking [6, 48]. Even after training PRM, one must still devise mechanisms to optimize it at per-step level in order to identify alternative steps that lie on correct trajectory, task that is itself nontrivial [48, 63]. In contrast, our work leverages the asymmetry between models instruction-following and solution generation capabilities to enable direct verification of reasoning trace and construction of corrective step that redirects the trajectory. This approach effectively amortizes value estimation and policy optimization into single procedure, resulting in simpler and cheaper approach for credit assignment. While our approach, in principle, can be viewed as using generative models for some form of verification [20, 21, 27, 60], it is distinct from these prior works in that we task the model with explicitly pointing out the location of single, key mistake, rather than verifying every step and judging the solution to produce binary judgement of outcome-level or process-level correctness. We amortize value estimation with policy optimization to directly produce an improved step (intervention). Moreover, unlike the majority of work on generative reward models or verifiers, our focus is on improve training directly rather than using inference-time methods (e.g., best-of-洧녜 search). Learning from natural language feedback. related line of work explores using natural language feedback to improve RL training. These approaches typically leverage feedback to refine rollouts, which are then used to update the policy. For example, Chen et al. [4] combine human feedback with separate refinement model to improve policy-generated outputs that are distilled back into the policy via SFT. Yan et al. [53] use teacher model to generate correction trajectories for off-policy RL, while Zhang et al. [61] employ critique-guided self-refinement to generate improved trajectories for off-policy RL. In contrast, our approach generates short, targeted natural language feedback to correct individual steps within otherwise on-policy trajectories. Moreover, interventions are proposed by the same model that produced the incorrect reasoning trace, without relying on human feedback or stronger external models. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Additionally, there have been class of text gradient methods [22, 31, 58] that leverage LLMs to generate textual feedback to improve test-time performance. While our method is similar in that it also employs textual learning signal, it differs in two key ways: the feedback is short and targeted, and it is used to improve training rather than solely to enhance inference-time behavior. Finally, as we show in Section 3, the use of reference solution substantially improves performance compared to approaches that only rely on the priors in the base model to generate corrective feedback. Hint-guided RL. Several concurrent approaches also leverage reference solutions in RL, but differ in how this information is used. Rather than using reference solutions for verification and credit assignment, these methods guide the policy directly. For example, [1, 23, 35, 62] incorporate expert-generated guidance in the form of partial-solution prefixes, i.e., static rationale snippets that the model conditions on during training. While such approaches substantially improve exploration by enabling the model to sample successful trajectories on otherwise unsolvable problems, they do not directly address credit assignment on incorrect traces. As result, our method is complementary to this line of work and can be naturally combined with it, synergy we demonstrate empirically in Section 3. Moreover, we find in Figure 2 that for substantial fraction of problems, the first incorrect step occurs deep into the reasoning trace. In particular, over 70% of errors appear after the first 50 steps. Existing hint-guided methods typically repurpose prefix of reference solution as hint, and cannot directly tackle errors that arise later in trace. One could attempt to mitigate this by providing longer hints, but this approach risks enabling the model to simply rely on the hint itself, effectively learning to cheat. Training with interventions outside of LLMs. Applying interventions at points of failure has been explored in domains beyond LLM training. For example, DAgger [37] mitigates compounding errors in imitation learning by querying an expert for labels on states visited by the learners own policy. HG-DAgger [19] adapts this framework to self-driving, and subsequent work extends it to robotic manipulation and long-horizon control tasks [13, 25, 32, 51]. Across these domains, intervention-based methods typically achieve improved data efficiency and faster convergence compared to behavior cloning, due to more effective credit assignment. In contrast to this line of sprior work, where interventions are provided by humans, we show that model can self-propose interventions to correct its own reasoning given reference solution, by leveraging asymmetries in its capabilities. 7. Discussion and Perspectives on Future Work In this work, we introduce InT, simple yet effective approach for assigning credit to incorrect rollouts. InT addresses this challenge by instructing the model to self-verify and propose single-step corrective interventions. We then fine-tune the model on these constructed interventions and perform RL posttraining on the resulting patched model. By enabling more precise credit assignment, InT achieves strong performance in both training and test settings, allowing off-policy reference solutions to improve already capable models while avoiding the typical drawbacks of off-policy supervised fine-tuning. Looking ahead, several promising directions emerge from this work: 1. Self-improvement by combining different LLM capabilities. In this work, we leverage the asymmetry between generation and verification capabilities of LLMs to assign credit to incorrect rollouts. natural next step is to further strengthen verification by explicitly training models on verification tasks, potentially eliminating the reliance on reference solutions. This would enable more autonomous self-improvement by removing the dependency on human-provided solutions. Moreover, stronger verification would allow LLMs to tackle tasks that require step-by-step correctness 16 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning checking, such as IMO-Proof Bench [30]. Ideally, verifiers would be trained in continual fashion, improving alongside the solution generation model. Concurrent work by Shao et al. [43] explores this direction by training meta-verifier that validates verifications themselves, but training this meta-verifier itself needs human or bigger model supervision. Beyond verification, another key capability for self-improvement is problem generation. For example, could LLMs learn to construct their own training problems by modeling trajectories encountered by expert problem proposers, such as those in IMO-style competitions? If these components can be combined, they suggest fully autonomous system in which one model proposes problems, another solves them, and verifier provides reward signals that drive continual parameter updates. 2. Credit assignment in continual learning. In this work, we study credit assignment in single-turn setting for solving math problems. An important direction for future work is to extend these ideas to continual improvement regimes, where rollouts are progressively compressed into LLMgenerated summaries and or neural memory modules, and the effective context evolves over time. For example, how can credit be traced back to earlier decisions that may persist only through memory representations? How can we disentangle errors arising from poor generation from those caused by imperfect memory, and jointly improve memory architectures alongside the generation model? These questions are interesting to study in future work. Acknowledgements We thank Bhavya Agrawalla, Taeyoun Kim, Yuxiao Qu, Zheyuan Hu, Baolong Bi, Jiahe Jin, and the rest of the AIRe lab at CMU for informative discussions, feedback and input on our results and previous version of this paper. This research is supported by gift from Apple, Schmidt Sciences AI2050 Early Career Fellowship, and an Open Philanthropy grant on technical AI safety. This research used the DeltaAI advanced computing and data resource, which is supported by the National Science Foundation (award OAC 2320345) and the State of Illinois. DeltaAI is joint effort of the University of Illinois Urbana-Champaign and its National Center for Supercomputing Applications. We thank the staff at DeltaAI for their continual and timely assistance with compute allocation. Experiments in this paper also utilized H100 GPU resources from the Orchard cluster in the FLAME center at CMU for which we are grateful for. AK also thanks the TRC program at Google Cloud for their continued support."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Hossein Amani, Aryo Lotfi, Nicolas Mario Baldwin, Samy Bengio, Mehrdad Farajtabar, Emmanuel Abbe, and Robert West. Rl for reasoning by adaptively revealing rationales, 2025. URL https://arxiv.org/abs/2506.18110. [2] Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, and Shuang Zhou. Amo-bench: Large language models still struggle in high school math competitions, 2025. URL https://arxiv.org/abs/2510.26768. [3] Mislav Balunovi캖, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi캖, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharen a.ai/. 17 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [4] Angelica Chen, J칠r칠my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Learning from natural language feedback. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum ?id=xo3hI5MwvU. [5] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. URL https://arxiv.org/abs/ 2412.21187. [6] Jie Cheng, Gang Xiong, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Yisheng Lv, and Fei-Yue Wang. Stop summation: Min-form credit assignment is all process reward model needs for reasoning, 2025. URL https://arxiv.org/abs/2504.15275. [7] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [8] Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, and Zhirui Zhang. Is prm necessary? problem-solving rl implicitly induces prm capability in llms, 2025. URL https://arxiv.org/abs/2505.11227. 18 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [9] Zeyu Gan, Yun Liao, and Yong Liu. Rethinking external slow-thinking: From snowball errors to probability of correct reasoning, 2025. URL https://arxiv.org/abs/2501.15602. [10] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omnimath: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. [11] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. [12] Andre He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting grpo beyond distribution sharpening, 2025. URL https://arxiv.org/abs/2506.02355. [13] Zheyuan Hu, Robyn Wu, Naveen Enock, Jasmine Li, Riya Kadakia, Zackory Erickson, and Aviral Kumar. Rac: Robot learning for long-horizon tasks by scaling recovery and correction, 2025. URL https://arxiv.org/abs/2509.07953. [14] Yichen Huang and Lin F. Yang. Winning gold at imo 2025 with model-agnostic verification-andrefinement pipeline, 2025. URL https://arxiv.org/abs/2507.15855. [15] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [17] Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate, 2024. [18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. [19] Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J. Kochenderfer. Hg-dagger: Interactive imitation learning with human experts, 2019. URL https://arxiv.org/abs/1810 .02890. [20] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think, 2025. URL https://arxiv. org/abs/2504.16828. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [21] Seungone Kim, Ian Wu, Jinu Lee, Xiang Yue, Seongyun Lee, Mingyeong Moon, Kiril Gashteovski, Carolin Lawrence, Julia Hockenmaier, Graham Neubig, and Sean Welleck. Scaling evaluation-time compute with reasoning models as process evaluators, 2025. URL https://arxiv.org/abs/25 03.19877. [22] Yoonho Lee, Joseph Boen, and Chelsea Finn. Feedback descent: Open-ended text optimization via pairwise comparison, 2025. URL https://arxiv.org/abs/2511.07919. [23] Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, and Jingzhao Zhang. Questa: Expanding reasoning capacity in llms via question augmentation, 2025. URL https://arxiv.org/abs/2507.13266. [24] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [25] Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao Bao, and Yuke Zhu. Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. The International Journal of Robotics Research, 44(10-11):17271742, 2025. [26] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv.or g/abs/2503.20783. [27] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inferencetime scaling for generalist reward modeling, 2025. URL https://arxiv.org/abs/2504.02495. [28] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision, 2024. URL https://arxiv.org/abs/24 06.06592. [29] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Sur passing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8c a303013a4e2, 2025. Notion Blog. [30] Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025. URL https://aclanthology.org /2025.emnlp-main.1794/. [31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. URL https://arxiv.org/abs/2303.17651. 20 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [32] Ajay Mandlekar, Danfei Xu, Roberto Mart칤n-Mart칤n, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-the-loop imitation learning using remote teleoperation. arXiv preprint arXiv:2012.06733, 2020. [33] Alexandre Pich칠, Ehsan Kamalloo, Rafael Pardinas, Xiaoyin Chen, and Dzmitry Bahdanau. Pipelinerl: Faster on-policy reinforcement learning for long sequence generation, 2025. URL https://arxi v.org/abs/2509.19128. [34] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024. [35] Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, and Aviral Kumar. How to explore to scale RL training of LLMs on hard problems? https://blog.ml.cmu.edu/2025/11/26 /how-to-explore-to-scale-rl-training-of-llms-on-hard-problems, 2025. CMU Machine Learning Blog. [36] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. [37] St칠phane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 627635, 2011. [38] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532, 2024. [39] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [40] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal, 2025. URL https://arxiv.org/abs/2502.12118. [41] Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms, 2025. URL https://arxiv.org/abs/2506.09026. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. [43] Zhihong Shao, Yuxiang Luo, Chengda Lu, Z. Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, and Xiaokang Zhang. Deepseekmath-v2: Towards self-verifiable mathematical reasoning, 2025. URL https://arxiv.org/abs/2511.22570. [44] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [45] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms, 2025. URL https://arxiv.org/abs/2505.00127. [46] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [47] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. [48] Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Hailei Gong, Zewen Ye, Shengjie Ma, and Jianping Zhang. Towards hierarchical multi-step reward models for enhanced reasoning in large language models, 2025. URL https://arxiv.or g/abs/2503.13551. [49] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Thoughts are all over the place: On the underthinking of o1-like llms, 2025. URL https://arxiv.org/abs/25 01.18585. [50] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229256, May 1992. [51] Philipp Wu, Yide Shentu, Qiayuan Liao, Ding Jin, Menglong Guo, Koushil Sreenath, Xingyu Lin, and Pieter Abbeel. Robocopilot: Human-in-the-loop interactive imitation learning for robot manipulation. arXiv preprint arXiv:2503.07771, 2025. [52] Yuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms, 2025. URL https://arxiv.org/abs/2502.0 7266. [53] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. URL https://arxiv.org/abs/2504.149 45. [54] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [55] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [56] Yang Yu. Pass@k metric for rlvr: diagnostic tool of exploration, but not an objective, 2025. URL https://arxiv.org/abs/2511.16231. [57] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. 22 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning [58] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic \"differentiation\" via text, 2024. URL https://arxiv.org/ab s/2406.07496. [59] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. [60] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [61] Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback, 2025. URL https://arxiv.org/abs/2506.03106. [62] Xuechen Zhang, Zijian Huang, Yingcong Li, Chenshun Ni, Jiasi Chen, and Samet Oymak. Bread: Branched rollouts from expert anchors bridge sft & rl for reasoning, 2025. URL https://arxiv. org/abs/2506.17211. [63] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning, 2025. URL https://arxiv.org/abs/2501.07301. 23 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning A. Prompts Box 1: Prompt for intervention self-generation You are an expert mathematician teaching Math Olympiad class. You will be given problem and high-level reference solution to the problem. Your task is to solve the problem step-by-step guided by the high-level reference solution. Problem {insert problem} High-Level Reference Solution {insert reference solution} Great job! Now, student in your class has solved the problem incorrectly. You must leverage your understanding of the reference solution to help him correct his attempt at the problem. Detailed Instructions 1. Detailed Verification Log You must perform step-by-step check of the students attempt against the reference solution. The steps here refer to each numbered substep in the Incorrect Student Attempt (i.e., Substep 1:, Substep 2:, . . . ), and not the high-level steps (e.g., Step 1:) which may contain multiple numbered substeps. This analysis will be presented in Detailed Verification Log, where you justify your assessment of each substep: For correct substeps, brief justification suffices. For substeps with errors or gaps, you must provide detailed explanation. Be careful and check every intermediate result, they are very easy to miss. 2. Identify the First Critical Error For each issue in the detailed verification log, you MUST determine whether it is critical error. critical error must pass the following two checks: 1. critical error is either factual error (e.g., calculation error like 2+3=6) or logical fallacy (e.g., claiming that A>B, C>D implies A-C>B-D) that disrupts the current line of reasoning. Procedure: Explain the specific error. State explicitly that it invalidates the current line of reasoning. 24 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning 2. critical error must not be recovered from. Procedure: Double-check that the error is not corrected or disputed in later steps (e.g., statements such as Wait, but let me double-check this claim. . . ). As long as the issue passes both checks above, it is considered critical error. We are interested in the first critical error that the student makes. 3. Propose an Intervention Step After finding the critical error, you should propose an intervention step that will be inserted before the occurrence of the critical error to steer the student towards the correct solution. The intervention must not give away the exact answer. The student should still be able to learn from the error. You should provide guidance by sketching the next steps that lead toward the correct solution. 4. Output Format Your response MUST be structured into three main sections: 1. Detailed Verification Log 2. Critical Error Report 3. Intervention Step 4.1 Detailed Verification Log Provide the full, step-by-step verification log as defined above, structured in bullet points. When referring to the students attempt or the reference solution, quote the relevant text before providing analysis. 4.2 Critical Error Report In this report, include bulleted list summarizing every issue discovered. For each issue, include: Location: direct quote of the key phrase or equation where the issue occurs. Issue: brief description of the problem and whether it is Critical Error. Stop once the first critical error has been found. 25 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning 4.3 Intervention Step Provide single intervention step to be inserted before the critical error. The content must be written from the students perspective. If there are no critical errors, leave this section empty. Draft of the Intervention Step Format 1. Content: The text of the intervention step. 2. Location: The substep number where it should be inserted, including quote from the students attempt. Self-check Answer each question below with: brief explanation, and final verdict: Yes or No. 1. Does the content of the intervention step include the exact answer from the reference solution (i.e., {correct_answer})? 2. Is the content of the intervention step missing any key insights necessary to solve the problem? 3. Is the location of the intervention step later than the substep containing the first critical error? If the answer to any question is Yes, the draft has failed the self-check. Revise and repeat until all answers are No. If all answers are No, write the final intervention step using the format below. Final Intervention Step Format INSERT_STEP_CONTENT should be the content of SERT_STEP_NUMBER should be an integer indicating the insertion substep. the intervention step, and IN- <intervention> {{ content: INSERT_STEP_CONTENT, location: INSERT_STEP_NUMBER }} </intervention> Incorrect Student Attempt {student solution} 26 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Box 2: Prompt for Gemini intervention {insert problem} {insert reference solution} You have solved the problem correctly. Now, student in your class has attempted the same problem. Your task now is to go over his solution step-by-step and write down detailed verification log, identify the first critical error, and suggest locations in his solution to insert replacement step such that if he follows the replacement step, it will guide him away from the error. Detailed instructions are listed below. Detailed Instructions 1. Detailed Verification Log You must perform step-by-step check of the entire solution. This analysis will be presented in Detailed Verification Log, where you justify your assessment of each step: For correct steps, brief justification suffices. For steps with errors or gaps, you must provide detailed explanation. Please be careful and check every intermediate result, they are very easy to miss. 2. Identify the First Critical Error For each issue in the detailed verification log, you MUST determine whether it is critical error. critical error must pass the following two checks: 1. critical error is either factual error (e.g., calculation error like 2+3=6) or logical fallacy (e.g., claiming that A>B, C>D implies A-C>B-D) that disrupts the current line of reasoning. Procedure: Explain the specific error. State explicitly that it invalidates the current line of reasoning. 2. critical error must not be recovered from. Procedure: Double-check that the error is not corrected or disputed in later steps (e.g., statements such as Wait, but let me double-check this claim. . . ). As long as the issue passes both checks above, it is considered critical error. We are interested in the first critical error that the student makes. 3. Propose Replacement Steps After finding the critical error, you must now identify existing steps in the students solution that you can rephrase such that if the student were to begin from your rewritten step, he will be guided away from the critical error. 27 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Note that replacement steps can occur either: Before the error, to circumvent it completely, or After the error, to recognize the mistake, dispute it, and recover with correct argument. There may be multiple valid replacement locations. Do not omit locations simply because they are close to each other. An entire region (e.g., Step XY) may be valid, and you should include each step in the region. 4. Output Format Your response MUST be structured into three main sections: 1. Detailed Verification Log 2. Critical Error Report 3. Replacement Steps List 4.1 Detailed Verification Log Provide the full, step-by-step verification log as defined above, structured in bullet points. When referring to specific part of the solution, quote the relevant text before providing your analysis. 4.2 Critical Error Report In this report, include bulleted list summarizing every issue discovered. For each issue, include: Location: direct quote of the key phrase or equation where the issue occurs. Issue: brief description of the problem and whether it is Critical Error. Stop once the first critical error has been found. 4.3 Replacement Steps List Summarize all potential recovery locations and replacement steps. Write the steps from the students perspective. The student should be able to continue naturally from your rewritten step. Finally, include curated list of replacement steps to be processed by parser. This list must strictly follow the format below. Do not include any additional explanations or justifications, or the parser will fail. Format: <intervention> [ { location: INSERT_STEP_NUMBER, . . . { location: INSERT_STEP_NUMBER, ] </intervention> content: INSERT_STEP_CONTENT }, content: INSERT_STEP_CONTENT } 28 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning If the students solution is correct and contains no critical errors, leave the list empty. Student Solution {insert student solution} 29 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Box 3: Sample output for intervention generation Detailed Verification Log Substep 1: We are given an infinite sequence of integers 洧녩0, 洧녩1, 洧녩2, . . . that is not eventually periodic... Correct. This sets up the problem accurately. Substep 2: Defines 洧녡 as the set of 2025-tuples (洧녩洧녵+1, . . . , 洧녩洧녵+2025) for 洧녵 0. Correct. Matches the problem statement. Substep 3: We are to find the minimum number of distinct elements in 洧녡 Correct. The goal is clear. Substep 57: Understanding the problem and the non-eventually-periodic condition. Correct. The student correctly identifies the constraint. Substep 1014: Key insight: if 洧녡 is finite, then the sequence is eventually periodic. Critical error in reasoning this claim is false in general. The student incorrectly assumes that finite number of 2025-tuples implies eventual periodicity. This is not true for sequences over an infinite alphabet like Z, even if the block length is fixed. However, the students reasoning in Substep 2534 is flawed. They state: If the set of 2025-tuples is finite, then the sequence is eventually periodic. This is not valid conclusion in general. For example, consider sequence where the values grow without bound but the blocks of length 2025 repeat in non-periodic fashion but such thing is not possible due to the structure of sequences. However, the critical flaw is that the student assumes that finite number of 2025-tuples implies eventual periodicity without justification, and this is only true in the case of finite alphabets or under stronger conditions. But more importantly, the student fails to recognize that the sequence can have finite alphabet (e.g., binary) and still be non-eventually periodic (like the Fibonacci word), while having exactly 洧녲 + 1 distinct blocks of length 洧녲. The students assumption that finite 洧녡 implies eventual periodicity is logical fallacy that invalidates the entire line of reasoning. Substep 15142: The student argues that because 洧녡 cannot be finite (due to eventual periodicity), it must be infinite, and thus the minimum number of elements is infinite. This is incorrect because: The claim that finite 洧녡 eventually periodic is not universally true over infinite alphabets. In fact, over finite alphabet, sequence can be non-eventually periodic and have exactly 洧녲 + 1 distinct blocks of length 洧녲 (e.g., Sturmian sequences). 30 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning The student ignores the possibility of finite-alphabet, non-eventually-periodic sequences with finitely many 2025-tuples which is exactly what the reference solution shows is possible. Therefore, the students conclusion that 洧녡 = is invalid. Critical Error Report Location: If the set 洧녡 is finite, then the sequence is eventually periodic. (Substep 14) Issue: This is factual and logical error. The student incorrectly assumes that finite number of 2025-tuples implies eventual periodicity. This is false in general for sequences over an infinite alphabet (like Z), and even more so when the values can grow without bound. However, the key point is that this claim does not hold in general. For example, sequence over finite alphabet (e.g., {0, 1}) can be non-eventually periodic and still have only finitely many blocks of length 洧녲 specifically, exactly 洧녲 + 1 as in the Fibonacci word. This is critical error because: 1. It is logical fallacy it invalidates the reasoning that 洧녡 must be infinite. 2. It is not recoverable the student never revisits or corrects this assumption. Later steps (e.g., Substep 145) accept as the answer based on this flawed premise. Therefore, this is critical error."
        },
        {
            "title": "Intervention Step",
            "content": "Draft of the intervention step: 1. Content: Consider sequence that takes only finitely many values (e.g., only 0 and 1). Even if the sequence is not eventually periodic, it might still have only finitely many distinct 2025-tuples. For example, the Fibonacci word over {0,1} is not eventually periodic, but every block of length 洧녲 appears exactly once in certain pattern in fact, it has exactly 洧녲 + 1 distinct blocks of length 洧녲. Can you think of way to construct such sequence? Try to understand how the number of distinct blocks relates to the periodicity of the sequence. 2. Location: Substep 14 31 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Self-check: 1. Does the content include the exact answer from the reference solution (i.e., 2026)? No. It avoids giving the answer and instead prompts the student to explore finite-alphabet, non-eventually-periodic sequences. 2. Is the content missing any key insights that are necessary to solve the problem? No. It introduces the idea of finite alphabets and the possibility of finite block complexity in non-eventually-periodic sequences, which is essential. 3. Is the location of the intervention step later than the substep number that contains the first critical error? No. The critical error occurs in Substep 14, and we are inserting the intervention at Substep 14. This is correct it comes before the erroneous conclusion. All checks pass. Final intervention step: \"Consider sequence that takes only finitely many values Even if the sequence is not eventually periodic, <intervention> { \"content\": (e.g., only 0 and 1). it might still have only finitely many distinct 2025-tuples. example, the Fibonacci word over {0,1} is not eventually periodic, but every block of length $ $ appears exactly once in certain pattern - in fact, it has exactly $ k+1 $ distinct blocks of length $ $. you think of way to construct such sequence? Try to understand how the number of distinct blocks relates to the periodicity of the sequence.\", \"location\": </intervention> 14 }"
        },
        {
            "title": "Can",
            "content": "32 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning B. Intervention Examples In this section, we provide few concrete examples of interventions. Figure 13: Example error of Qwen3-4B-Instruct and generated intervention. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 14: Example error of Qwen3-4B-Instruct and generated intervention. 34 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 15: Example error of Qwen3-4B-Instruct and generated intervention. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 16: Example error of Qwen3-4B-Instruct and generated intervention. 36 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning C. Evaluation Configuration For all experiments with Qwen3-4B-Instruct, we follow the official recommended configuration, using temperature 0.7, top-p 0.8, and top-k 20 unless specified otherwise. D. Training Hyperparameters for InT Hyperparameter Qwen3-4B-Instruct-2507 effective_train_batch_size ppo_mini_batch_size learning_rate kl_loss_coef entropy_coeff temperature top_p rollout.n 32 16 1.0e-6 0 0 1.0 1.0 8 Table 5: PipelineRL [33] hyperparameters used for RL runs. Hyperparameter Qwen3-4B-Instruct-2507 dataset_size effective_batch_size num_train_epochs learning_rate lr_scheduler_type min_lr_rate warmup_ratio 1076 64 4 1.0e-6 cosine_with_min_lr 0.1 0.1 Table 6: LLaMa Factory [44] hyperparameters used for SFT runs. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning E. Self-Reflection Baseline Self-reflection. We add details on how we establish the self-reflection baseline. The idea is similar to generating interventions, but rather than outputting single-step interventions, we ask the base model to re-write the entire solution. We find that our method InT consistently outperforms the self-reflection baseline, as shown in Figure 12 and Table 4. The prompt for generating self-reflection traces is shown below. The reference solution is high-level summary of the solution written by humans, and occasionally, stronger model. Similar to InT, we ask the base model to generate its own self-reflections. Box 4: Prompt for self-reflection You are an expert mathematician teaching Math Olympiad class. You will be given problem and high-level reference solution to the problem. Your task is to solve the problem step-by-step guided by the high-level reference solution. # Problem # {Insert Problem} # High-Level Reference Solution # {Insert Reference Solution} {Insert Model Response} Great job! Now, student in your class has solved the problem incorrectly. You must leverage your understanding of the reference solution to rewrite refined version of his attempt at the problem. **Your rewritten solution should be complete solution to the problem. ** # Incorrect Student Attempt # {Insert Student Solution} F. Are Interventions Memorized? We would like to understand whether InT leads to memorized interventions or if it actually learns to generalize to unseen problems. As such, we select two example problems from the IMO Shortlist 2024 outside of our training set, and compare the traces of InT and base model (Qwen3-4B-Instruct). IMO Shortlist 2024, Problem C1. As shown in Figure 17, both models started with an incorrect assumption of the formula, but when the InT trained model got 3 when 洧녵 = 3, it questioned how this was possible and thus correctly updated the hypothesis to (洧녵 2 ). IMO Shortlist 2024, Problem C2. Although both models are able to successfully conclude that even cool numbers must be multiples of four in Figure 18, only the InT model is able to try considering that 洧녵 = 12 may not be cool and therefore the pattern may be more selective than simply being multiple of four. This leads the second model to the right hypothesis. 38 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 17: Diverging Solution Paths between InT and base models on IMO Shortlist 2024, C1. Figure 18: Diverging Solution Paths between InT and base models on IMO Shortlist 2024, C2. 39 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning Figure 19: Pass@洧녲 across RL training iterations: We plot pass@洧녲 performance from 0 to 150 RL iterations for three initializations: (i) base model patched with InT; (ii) base model distilled on oracle traces; and (iii) directly the base model. Int patched model improves pass@k consistently while others mainly sharpen. G. InT with Interventions From Stronger Model (Gemini 2.5 Pro) While prototyping and developing our method, we also evaluated the efficacy of InT with interventions generated by Gemini 2.5 Pro (referred to as Gemini from now on). In particular, we answer the following questions: (1) does SFT on Gemini-generated interventions improve the ability of the fine-tuned model to sample correct traces on hard problems? and (2) how does InT compare with distillation of full expert reasoning traces sampled from the oracle (i.e., Gemini 2.5 Pro)? To this end, we run several experiments comparing InT against running standard RL training and distillation on Gemini-generated solutions, in an attempt to patch the capabilities of e3-1.7B [41]: strong, open-source reasoning LLM fine-tuned on top of Qwen3-1.7B on set of difficult math reasoning problems. G.1. Inference-Only Results Unique Coverage Accuracy Intervention Proposed By Rollouts Are Conditioned On DeepScaleR subset with pass@32 = 0 N/A Gemini 2.5 Pro Gemini 2.5 Pro problem problem + correct prefix y<洧노* problem + correct prefix y<洧노* + intervention y洧노* We provide inference-time results achieved by rolling out from generated interventions concatenated to the prefixes. experiments on different subset of 235 DeepScaleR problems that have pass@32 = 0. Our earlier prompt uses Gemini solutions in place of the reference solutions and does not double-check the generated intervention (shown in Box 3). In this earlier setting, we obtained 3.8 improvement in accuracy and solve 47 more problems compared to naive rollouts. Moreover, we also conducted an additional ablation from intervention which perform rollouts from the location of the mistake but is not conditioned on the intervention (i.e., 洧랢(x, y<洧노)) to isolate for the benefits of localizing the mistake. We find that pinpointing the location of the mistake alone leads to superior gains in accuracy (2.4) and coverage (+ 22 problems compared to the baseline). Table 7: Rollouts conditioned on differently generated interventions. As shown, rollouts conditioned on interventions double the number of problems with at least one correct rollout, and improve the rollout accuracy by more than an order of magnitude. 98 / 235 120 / 235 145 / 235 0.97% 2.37% 3.72% G.2. Experimental Setup and Evaluation Metrics Constructing dataset of hard training problems. We begin our experiments with state-of-the-art <2B parameter model, e3-1.7B [41], already trained with curricula and several best practices for RL to 40 InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning attain strong performance in its scale. Despite its strong performance, this model still fails on large fraction of problems from its hard training set (a 2.5K subset of DeepScaleR problems from Luo et al. [29]). We run 32 rollouts on each problem and collect the subset of problems the model cannot solve at all. We utilize Gemini 2.5 Pro (as of 2025-08-01) as our oracle. Among these 472 unsolved problems, the oracle solves 16% of them in single attempt, suggesting it can provide meaningful interventions on these problems. We retain this subset as our hard problem set 洧륻ard to study the efficacy of patching with different methods. Our main findings are that (i) RL with just small dataset of 64 problems on top of InT outperforms RL on much larger set of 1.2K problems on top of distillation or the base model. On the other hand, (ii) RL with the small dataset on top of the distillation and the base model are infeasible due to collapse of behaviors on OOD sets or zero learning signals. Baselines approaches and comparisons. To evaluate the efficacy of InT, we compare against alternate approaches for patching model behavior on 洧륻ard. Our primary comparisons are: 1) Distillation + RL, which first distills entire reference solutions into the base model before running RL, and 2) Standard RL, which directly continues RL on the hard problem set from the same base checkpoint. Both simulate continued RL run where new hard problems are introduced during training. We also consider SFT-only baselines, where the model is patched via supervised learning on reference solutions or intervention traces for the hard problems, without any further RL. To our knowledge, no existing method is designed to explicitly handle this setting of patching model behavior on previously unsolved hard problems in way that leverages oracle interventions while preserving the benefits of RL. Therefore, we limit our exposition of training trends to 洧륻ard, but also compare with alternate approaches for using intervention data on holdout standardized test sets. Evaluation metrics. Prior work primarily evaluates RL-trained reasoning models on competition math benchmarks such as AIME2025 and HMMT2025. However, progress on these alone does not capture whether models are actually learning from hard training problems, nor whether such training transfers to equally challenging evaluation problems. To address this, we evaluate our trained models on several standardized benchmarks from 2025, including AIME2025, HMMT2025, BRUMO2025, CMIMC2025, and others, as well as an in-distribution (i.i.d.) test set of hard problems 洧릆est hard, similar to 洧륻ard. The i.i.d. test set consists of 64 problems held out from the training pool using the same methodology as used to select 洧륻ard. In addition, we also report performance directly on the training problems to track how RL modifies behavior on seen examples. Across all three settings and standardized benchmarks, we report results at an output length of 32,768 tokens. G.3. InT uniformly pushes the pass@洧녲 frontier upwards on test problems We present our main results for InT on an holdout set of hard problems 洧릆est hard (Fig 19). Here, we plot the pass@洧녲 performance across different RL training iterations from 0 to 150, for three models: (i) base e3-1.7B, (ii) e3-1.7B distilled on full reference solutions; and (iii) e3-1.7B patched on interventions from the oracle (InT). We find that running RL on the base or distilled model does not make any improvements in pass@洧녲 throughout all training steps. On the other hand, running RL on 洧륻ard after we patch e3-1.7B on oracle interventions (InT) leads to consistent improvements in pass@洧녲 during RL. G.4. InT outperforms distillation on standardized evaluations Previously, we saw that InT improves pass@洧녲 over baselines on training and hold-out sets. This mainly tells us that InT makes progress on the hard training problems that were previously unsolved. But, we also care about how this gain in performance translates to performance on standardized benchmarks for InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning (b) (a) Figure 20: Comparison of InT with distillation of reference solutions. (a) Since these are hard problems, running RL initialized from the base model does not improve training reward, while running RL on top of the distilled model or the model produced by InT does improve training reward. We observe that running RL on top of the distilled model degrades model capability (decreasing pass@1 score on held-out set in (b) with more training), even though it continues to make progress on the training set, as shown by decreasing ratio of the percentage of unsolved problems (zero advantage ratio) in (c). (c) math reasoning. Here, we compare the performance of our approach InT on top of the e3-1.7B model. To stress test InT in the setting where we simply continue to run RL from the intervention checkpoint, we run RL training on this checkpoint with only 64 problems in 洧륻ard, on which we collected the interventions. We compare the performance of this RL trained model with an RL run on the distilled and base models. To boost the baselines, we run RL for both using an expanded set of about 1.3k problems sourced from DAPO [55], including the 64 we used for InT. The main reason we perform this injection is that in our preliminary experiments which trained the distilled model only on the small set of 64, we noticed that post RL the model capabilities on standardized evaluations fell drastically (Figure 20(b)), perhaps due off-policy nature of the reference solution. When we run RL on the base model, we also expand the training set for RL, since we find that the reward curve does not rise otherwise (Figure 20(a)), thus we train the base model on mixture of easy problems from DAPO and the 64 problems with InT interventions. Unlike the RL runs on distilled and base checkpoints, InT improves the test performance averaged across multiple hard test datasets, despite being trained on just 64 problems  (Table 8)  . Compared to distillation, we see gains on both in-distribution (洧릆est hard) and standardized benchmarks for hard problems mainly because intervention does not alter the base model distribution as much as distillation on the entire trace. Model RL Data Size OlymMATH Easy OlymMATH Hard HMMT BRUMO e3-1.7B + RL e3-1.7B + Distill + RL e3-1.7B + InT+ RL Model e3-1.7B + RL e3-1.7B + Distill + RL e3-1.7B + InT+ RL 1216 1216 64 AIME 36.25 36.67 36.25 38.75 37.38 41.62 6.75 5.75 7. Beyond AIME CMIMC 22.50 22.50 24.58 Average 洧냥test 20.88 21.75 22.00 23.75 21.56 21.88 27.73 27.38 29. 46.25 47.08 53.75 hard pass@8 15.85 14.8 23.56 Table 8: Pass@1 performance (8 rollouts avg.) of models across standard mathematics benchmarks and pass@8 performance test on the i.i.d. test set, 洧냥 hard. Observe that InT followed by RL attains the highest pass@8 performance on this in-distribution test set for patching the e3-1.7B base model."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of Illinois Urbana-Champaign"
    ]
}