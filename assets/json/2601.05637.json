{
    "paper_title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "authors": [
        "Emily Cheng",
        "Carmen Amo Alonso",
        "Federico Danieli",
        "Arno Blaas",
        "Luca Zappella",
        "Pau Rodriguez",
        "Xavier Suau"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 7 3 6 5 0 . 1 0 6 2 : r GenCtrl Formal Controllability Toolkit for Generative Models Emily Cheng,1,2, Carmen Amo Alonso3, Federico Danieli1, Arno Blaas1, Luca Zappella1, Pau Rodríguez1, Xavier Suau1 1Apple, 2Universitat Pompeu Fabra, 3Stanford As generative models become ubiquitous, there is critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide theoretical framework to formally answer this question. Framing human-model interaction as control process, we propose novel algorithm to estimate the controllable sets of models in dialogue setting. Notably, we provide formal guarantees on the estimation error as function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits. Code: https://github.com/apple/ml-genctrl Correspondence: emilyshana.cheng@upf.edu Date: January 12,"
        },
        {
            "title": "Introduction",
            "content": "The widespread deployment of generative models has driven significant research effort into controlling their outputs (Zhang et al., 2023). diverse array of controlled generation methods has been developed, from prompting (Marvin et al., 2023) and finetuning (Wei et al., 2022a; Ouyang et al., 2022b) to steering towards specific styles or concepts (Li et al., 2024; Rodriguez et al., 2025b; Rimsky et al., 2024; Wu et al., 2024). However, beneath the surface of this empirical progress lies set of foundational, yet largely unexamined, assumptions. Specifically, current methods implicitly assume the model is fundamentally controllable. This rests on three key premises. First, they assume reachability: that desired set of outputs is achievable using given control mechanism and initial prompt. Second, they assume universal controllability, meaning the desired output is reachable from any initial state. Finally, they assume calibration, that is, the output is direct function of the control variable. The absence of tools to formally verify these assumptions casts uncertainty over the reliability and fundamental limits of current controlled generation techniques. In this work, we bring these assumptions to the forefront. We provide tools to verify them by turning to control theory (Sontag, 1998), adapting its methods to formalize the problem for modern AI systems. Casting controlled generation as control process, we propose framework to quantify its reachability and controllability with probabilistic guarantees. Our framework treats generative model as an opaque nonlinear control system; it is not only agnostic to model architecture but is also designed to handle discrete or continuous-valued input and output spaces (e.g., textual prompts and generations). Finally, we demonstrate our framework on user-model dialogue setting, common use-case of current generative models (Google, 2024; Yang et al., 2025; OpenAI, 2023). Our primary contributions are as follows: 1 formal framework for controllability. We introduce control-theoretic framework to rigorously define and quantify the reachable and controllable sets for any opaque system, including generative models. To the best of our knowledge, this provides the first formal language to characterize the operational boundaries of generative model control. PAC algorithms for controllable set estimation. We present Monte Carlo algorithms for reachable and controllable set estimation from opaque system interactions, along with probably-approximately correct (PAC) confidence bounds (Thm. 1 and 2). The algorithms are particularly well-suited for generative models as they are distribution-free, only assume boundedness of the target attribute, and explicitly account for the discrete bottlenecks inherent in many generative systems. Empirical findings. We conduct broad empirical analysis of controllability across models and tasks for text generation with LLMs, and image generation with text-to-image models (T2IMs). Leveraging our theoretical results (Thm. 1 and 2), our experiments reveal that model controllability is not guaranteed. The significant heterogeneity of results across modalities, models, and tasks signals the need for case-specific controllability analyses. An open-source toolkit. We release our framework and algorithms in https://github.com/apple/ml-genctrl, PyTorch library at to facilitate analysis of controllability by the broader research community. This work argues for paradigm shift where the controllability of generative models is not an implicit assumption but an explicit subject of analysis. By providing formal tools to do so, we lay more principled foundation for future work in controllable AI."
        },
        {
            "title": "2 Related Work",
            "content": "Controlling generative models Research on controlling generative model outputs follows three main paradigms: (i) Prompt engineering, e.g., in-context learning or chain-of-thought prompting (Brown et al., 2020; Wei et al., 2022b); (ii) Finetuning, e.g., via RLHF or DPO (Ouyang et al., 2022a; Rafailov et al., 2023), to align the model behavior; (iii) Representation engineering, which directly manipulates model activations (Turner et al., 2023; Li et al., 2024; Suau et al., 2024; Rodriguez et al., 2025a; Wu et al., 2024; Cheng et al., 2024). Our framework can assess any control mechanism including all examples above. In this work we mainly focus on prompting, given its widespread use. Control theory and reachability analysis for generative models Controllability and reachability are core concepts in control and dynamical systems theory (Sontag, 1998; Borrelli et al., 2017). Despite their widespread use in other areas of engineering, such as robotics (Nakamura et al., 2025), the application of control theoretic concepts to machine learning has mainly been limited to reinforcement learning (Recht, 2019), or to analyze training dynamics (Han et al., 2019). Despite recent efforts to develop theory for generative models (Soatto et al., 2023; Marchi et al., 2024), these often rely on impractical assumptions. Extending control-theoretic concepts to large generative models poses several challenges, mostly due to their nonlinear and high-dimensional nature (Bansal et al., 2017). In the control literature, past works have addressed model-free reachable set estimation by proposing data-driven methods. However, existing methods do not apply to generative model outputs for three reasons: either they (i) apply to state and not output reachability (Devonport et al., 2021; Devonport and Arcak, 2020; Dietrich et al., 2024); (ii) impose assumptions, e.g., Lipschitzness, on system dynamics that are not verifiable for opaque LLMs and T2IMs (Choi et al., 2025; Park et al., 2024; Xue et al., 2020); (iii) return continuous reachable set estimates while LLMs and T2IMs reachable sets are countable, making the estimates vacuously large (Devonport and Arcak, 2019; Sivaramakrishnan et al., 2025; Devonport and Arcak, 2020; Dietrich et al., 2024). We bridge these gaps by developing sampling algorithm that estimates non-vacuous reachable and controllable sets for generative models, with probabilistic guarantees. This paves the way for rigorous control-theoretic analysis viable even for modern, opaque large-scale models."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We study control of LLMs or T2IMs in dialogue setting. In Sec. 3.1 we formalize the dialogue process through control-theoretic lens (Sontag, 1998) and in Sec. 3.2 we define reachability and controllability under this dialogue formalization. 2 Figure 1 Dialogue Process. (Left) Schema of dialogue process as control process, showing the roles of each of the concepts introduced in Sec. 3.1. Theorems 1 and 2 tell how many inputs and initial states (respectively) should be sampled to answer Q1 and Q2 with confidence δ. (Right) Example of dialogue process."
        },
        {
            "title": "3.1 Dialogue with a Generative Model as a Control Process",
            "content": "We study dialogue process (DP) with generative models, i.e., LLMs and T2IMs  (Fig. 1)  . Next, we describe the problem setup, which we formalize in the language of dynamical systems theory. Time Domain The time domain where each State Space The state space at given time; for = 0, by the model. In DP with an LLM or T2IM, X corresponds to the tth turn in the dialogue process. is the set that indexes the systems temporal evolution. For DP, = is discrete, is the set of possible states the system can occupy. Each is the space of initial string contexts, and for 1, is the set of strings and of strings and images, respectively. describes the system is the space of possible generations Control Input Space The control input space is the set of all possible inputs that can intervene the system. Each control input denotes an intervention to the system at given time. In practice, is any degree of freedom tuned by the user or system designer to affect generation, e.g., prompt, or activation steering (Rodriguez et al., 2025b; Li can be any type of intervention, we focus on prompting as the primary way users interact with et al., 2024). While generative models. Dynamics Map We frame dialogue as control process where user inputs provide feedback to guide the models behavior. In particular, we treat these inputs as interventions to the generative model. An intervened model ϕ maps prompts and any other control variables to the next model generation, defining dynamics ϕ : , where xt+1 = ϕ(xt, . . . , x0; ut, . . . , u0). 1 Controlled generation aims to achieve desired behavior as quantified by metric (e.g., sentiment score). This is formalized via measurement value and readout map, defined below. Measurement Value The measurement-value space is the set of possible measurable outputs of the system. In particular, the measurement value is how we evaluate the systems behavior at given time. This may correspond to external assessments of the models generation, e.g., sentence length or number of cars in an image. The choice of is task-dependent. Controlled generation, through input interventions, aims to restrict the system ϕ measurements to desired subset Readout Map The readout map : maps the state at time to measurement value. For DPs, h(t, xt) =: h(xt) is given by deterministic function that maps model generations xt to measurement values in . For example, if the attribute being controlled is the LLMs output string length, the readout may be the Python len() function. Y . The above concepts permit rigorous definition of dialogue process between user and intervened model ϕ as discrete-time control system (Thm. 1). As shown in Fig. 1, the user and model take turns in conversation starting from the users initial prompt. Subsequent user prompts are interventions that trigger another dialogue turn after the models response. 1In practice, LLMs come with sampling strategy for extracting from ϕ, e.g., greedy decoding or top-p sampling. Our definition of ϕ abstracts away the sampling strategy, which is already reflected in xt+1. 3 Definition 1. dialogue process is stochastic discrete-time, nonlinear control system parametrized by the tuple (ϕ, ), such that: , h, , , U x0 = initial prompt xt = ϕ(xt1, . . . , x0; ut1, . . . , u0) yt = h(xt) , , , (3.1a) (3.1b) (3.1c) for N, where u0t1 are collection of inputs from . U"
        },
        {
            "title": "3.2 Reachability & Controllability in Dialogue Processes",
            "content": "Theorem 1 establishes dialogue process as control process. In this work, we do not study the question of how to choose an input from to elicit some result (controller design). Instead, we tackle the more fundamental question of what is possible to achieve over time horizon (i) given specific initial state, i.e., prompt x0; and (ii) for any or all initial states. These two are well-studied in control theory: (i) pertains to the question of reachability, while (ii) addresses controllability. In what follows, we formalize these definitions in the context of generative models. Definition 2. Given dialogue process (Thm. 1) with an initial state x0 model with dynamics ϕ is given by , then the reachable set of outputs for the (x0, , t) = { u0, . . . , ut1 and yt = } . (3.2) That is, given an initial state x0 the dynamics steps, with some control sequence u0, . . . , ut1 natural to ask what is reachable from all initial states. Namely, if all x0 using some input sequence in , then the system is controllable on , the reachable output set at time is the set of all reached when integrating . Thm. 2 relies on fixed initial state x0. It is then can reach all values in the set (Sontag, 1998). Formally, Y 0 such that Definition 3. system is controllable on if (x0, , t) = x0 . differentiating feature of LLMs and T2IMs is the discrete nature of string prompts, which makes the reachable set countable (see Sec. for discussion and formal proofs). We refer to this as the discrete bottleneck. This property affects both reachability (Thm. 2) and controllability (Thm. 3), which are preconditions for system control. In Sec. 4 we propose coarse-grained reachability as relaxation of the problem for DPs with discrete bottleneck. In Sec. 4.1 we derive PAC bound to approximate the reachable set of given initial state, then in Sec. 5 present PAC bound to estimate the controllable set of the system. Finally, in Sec. 6 we use our theory to estimate reachable and controllable sets of current LLMs and T2IMs, finding they are seldom controllable on simple tasks."
        },
        {
            "title": "4 Monte Carlo Reachability for Dialogue Processes",
            "content": "Generative models are nonlinear, high-dimensional, and have unknown dynamics. Therefore, it is nontrivial to analytically derive the reachable set. Hence, we rely on statistical guarantees that only depend on the systems empirical trajectories. In Sec. 4.1 we tailor the definition of reachability to the discrete bottleneck of LLMs and T2IMs. Then, we propose sample complexity bound for the reachable set in Sec. 4.2, which we use in Sec. 4.3 in an algorithm that computes an error-bounded reachable set approximation. Since reachable set estimates will rely on sampling, we first set up probabilistic notion of the control process, as in prior work (Devonport and Arcak, 2019): X0 p0, where p0 : 0 Ut pu,t, where pu,t : = ϕ(t; X0, 0t1) [0, 1] is the pushforward of p0 and pu under system dynamics, for [0, 1] is probability density over the set of initial states [0, 1] is probability density over the input space 0 at time pt, where pt : . 0. 0. = h(X t, t) py,t, where py,t : [0, 1] is the probability density over measurements at time 0."
        },
        {
            "title": "4.1 Coarse-Grained Reachability Overcomes the Discrete Bottleneck",
            "content": "The discrete bottleneck in DPs impacts the reachable set: even if the measurement value is continuous-valued (e.g., continuous text formality score), the true reachable set can be countable. This means we cannot directly apply existing 4 reachable set estimators that return continuous sets (Devonport and Arcak, 2019; Alanwar et al., 2021), as the returned sets would be vacuously large. workaround is coarse-grained reachability, simplified but faithful discrete abstraction of the original measurements (Ren and Dimarogonas, 2020). We consider two cases: (i) categorical measurements, e.g., , which require no abstraction (they are already discrete); (ii) continuous-valued measurements, e.g., formality score in [0, 1]. , we allow For the latter, we relax the reachability problem to γ-quantized one: instead of exactly reaching values in an error margin γ (Thm. 4). For instance, rather than requiring 0.3 exactly, we aim for 0.3 toxic, non-toxic 0.05. } { ) be dialogue process per Thm. 1. The γ-quantized Definition 4 (γ-quantized Reachable Set). Let (ϕ, reachable output set at time from x0 is given by , , , h, where refers to the γ(x0, , t) = -norm and γ { u0, . . . , ut and yt γ , } (4.1) R+.2 γ is For continuous-valued measurements, our goal is now to estimate at most γ away from some point in the true reachable set. For brevity, we refer to reachable set estimation over both categorical or quantized as coarse-grained reachability. γ guarantees each point in γ. Recovering R Y"
        },
        {
            "title": "4.2 Sample complexity bound for coarse-grained reachable set estimation",
            "content": "Y . If = , requiring that is continuous-valued, γ. To build up to this bound, we first find suitable quantization Our goal is PAC bound that estimates measurement-value space then user-defined error in Thm. 4. Let = covering number of error, which we define using the estimate. Then, to construct the p-approximation of ybin formally define p-approximation below: of the is already categorical, -balls of radius γ/2, where γ is the if categorical and the otherwise. Since our algorithm will be based on sampling, we need probabilistic notion of (0, 1) that tunes the precision of t), we keep all points lying in the bins p; any bin with density < is considered negligible and discarded. We has finite cardinality, i.e., that is minimal cover of be the number of bins in the discretization: q. In particular, the user sets small threshold with probability mass py,t(ybin) q is bounded. If , t) (hereon using (x0, Y Y Y Definition 5 (p-Approximate Measurement Value). Alternative and equivalent definitions are given for the categorical and γ-quantized cases. Categorical: The (p)-approximation of is γ-Quantized: The (p, γ)-approximation of , py,t( ybin py,t(ybin) , where ybin p . t,p := { γ is t,p := cover. y { ) } { : } ybin is an -ball of radius γ/2 in Now, using of the result into the below Theorem (see Sec. F.1 for separate statements): (γ) t,p , we state the sample complexity bound. For brevity, we combine categorical and quantized versions } Theorem 1: Sample complexity bound, coarse-grained reachability have finite cardinality . Let be the number of i.i.d. samples drawn from Yt, i.e., Let yi i. Fix δ (0, 1). If (cid:18) max N, log(δ/N ) p) log(1 (cid:19) , t) { -ball centered at yi with radius γ). Proof in Sec. F.1. δ, where ˆ i=1 if categorical, and ˆ = yi 1 } then P( ( (γ) t,p (yi, γ) is the ˆ = i=1B (yi, γ) if quantized yi i=1 with each } { (4.2) Theorem 1 shows that, after samples, one is confident that all reachable bins in δ, and the precision of the estimate by p. Then, if target set is given by 1 δ. By construction, ˆ unreachable with probability guaranteed to lie within γ of reached point. 1 is tight estimate of the true have been covered; the confidence is not included in ˆ is t, that (γ) : all points in ˆ are 2In practice, Rn can consist of several unrelated attributes 1 . . . like formality and sentence length. The -norm makes the theory already hold for multiple orthogonal readout dimensions."
        },
        {
            "title": "4.3 Monte Carlo Sampling Algorithm for Reachability in Dialogue Processes",
            "content": "Theorem 1 provides the foundation for Monte Carlo algorithm to estimate DPs reachable sets. The procedure to build the reachable set estimate ˆ is stated in Alg. 1a. It shares the same structure for both discrete and continuous cases with case-specific post-processing of the samples. Under sample size requirements from Thm. 1, Alg. 1a guarantees P( δ. Crucially, this guarantee uses no information about the underlying model, nor the dynamics. Then, the number of trajectories does not rely on whether the model is stochastic3 nor the timestep. In Sec. F.4, we empirically validate the bound in Thm. 1. In Sec. F.3, we analyze dependence of on all parameters, finding O(N log ). ) γ t,p (m) ˆ R"
        },
        {
            "title": "5 Monte Carlo controllability for dialogue processes",
            "content": "Having established reachable sets for individual initial states (Alg. 1a), we now quantify controllability of the system. from all initial states, one can naturally estimate controllable Since controllability entails reaching subset sets by intersecting the reachable sets of initial states. To measure how controllable the system is, we first define partial controllability (Sontag, 1998), then derive PAC bound (Thm. 2) on the DPs partially controllable set. This yields Alg. 1b, an algorithm for controllable set estimation. As Alg. 1b will rely on reachable set estimates that are p-approximate (Thm. 5), we port over the notion of p-approximate for controllability. Since we focus on the discrete bottleneck, we directly consider the discretization of the measurement space . To formalize partial controllability, we introduce measure µ : of initial states that p-approximately reach ybin in define the α-controllable set of initial states x0. Formally, α : the set of bins in q. Namely, µ : ybin that are p-approximately reached by large proportion ( [0, 1], where µ(ybin) quantifies the true proportion γ t,p(x0)]. We use µ to α) Px0p0 [ybin (cid:55) 1 Definition 6 (α-controllable set). Given control system (ϕ, is given by µ(ybin) ybin α 1 . α = { } , , T , h, ), the α-controllable set α at time Example 5.1: α-controllable set 10 .\" Say each outcome in length= } with possible inputs Generate sentence of length Consider an initial state space is p-approximately reached by exactly 3 states (75% of 1 { states, they dont have to be the same ones each time). Then, for α = 0.25 (i.e., 25%), the α-controllable set α (Thm. 6) is length= BOS, Hi, Hello, Hey 2, 3, 5, 8 2, 3, 5, 8 { } } { } { . α by sampling initial states, sampling their reachable sets, and taking We aim to estimate the true α-controllable set their intersection: ˆ 0 ). The key question is how many initial state samples are needed to approximate = α with high confidence. To answer the question, we need notion of approximation error between the estimated . natural choice is the measure under µ of false positives in ˆ ˆ α α , formally, with respect to and the target ).4 With enough samples, we can bound µ( ˆ µ( ˆ α C (0, 1) with high probability: α ) below desired ϵ i=1 t(x(i) ˆ C C Theorem 2: Sample complexity bound, α controllable set Fix ϵ, δC, p, α i.i.d. x(i) i=1 0 } { p0 and their reachable set estimates (0, 1) (and γ R+). Let δR be the confidence from Thm. 1. Given initial states , (5.1) then P(µ( ˆ α ) < ϵ) (1 δC)(1 i=1 t(x(i) 0 ). Proof in Sec. H. ˆ 3For deterministic ϕ, ˆR(m) ϕ, ˆR(m) reflects exact (or within-γ) controllability, and p-approximation only captures sampling uncertainty. For stochastic reflects controllability in-distribution: reflects stochasticity of both sampling and ϕ. See Sec. for discussion. 4There are no false negatives ˆCt is strict overapproximation of Cα , as ˆCt Cα by construction taking successive intersections shrinks the running controllable set until convergence. 6 i=1, if ˆ { t(x(i) 0 ) } log ϵδC log(1 α) = (cid:84)k δR)k, where ˆ Algorithm 1a: Monte Carlo Reachability Estimation for Dialogue Processes Input: Initial state x0, confidence level, probability threshold (δ, p) (0, 1)2 Algorithm 1b: Monte Carlo Controllability Estimation for Dialogue Process Input: Confidence level, partial controllability, error, probability threshold (δ, α, ϵ, p) (0, 1)4 // Thm. Input: If γ-quantized: quantization parameter γ > 0 Compute initial state k, reachable set sample sizes m; // Thm. 2, Input: If γ-quantized: quantization parameter γ > 0 Compute sample size m; // Sample generation for = 1 to do Integrate DP to time t: xt = ϕt(x0, u) sampling input sequence pU ; Evaluate output: yt = h(xt, ut); = (cid:83)m return ˆR(m) else {y(i) }m i=1; i=1 B(y(i) , γ) if γ-quantized reachability see Sec. // Sample generation for = 1 to do Sample initial state: x(i) Sample reachable set: ˆR(m) 0 p0; (x(i) 0 ); // Alg. 1a return Empirical controllable set: ˆCt = (cid:84)k i=1 ˆRt(x(i) 0 ); Algorithm 1 Pseudocode for Monte Carlo reachability (left) and controllability (right) estimation. Theorem 2 is distribution-free, only needing to be finite and consistent over initial states. Like Thm. 1, Thm. 2 is applicable to any control system, including stochastic ones. See Sec. for how to use Thm. 1 and 2 for hypothesis testing. Y"
        },
        {
            "title": "5.1 Monte Carlo sampling algorithm for controllable sets",
            "content": "Algorithm 1b outlines the procedure to estimate the controllable set by intersecting the reachable sets of initial states. δR)k depends on confidences δR on each reachable Note that in Thm. 2, the overall confidence 1 set and δC related to sampling enough initial states. Given δ, we automatically select δC and δR to minimize the total samples = k. See Sec. for hyperparameter selection and impact on sample complexity. δ := (1 δC)("
        },
        {
            "title": "6 Experiments",
            "content": "While Thm. 1 and 2 apply to arbitrary control systems, we demonstrate them on contemporary LLMs and T2IMs, for tasks of varying complexity and various prompting interventions. For space reasons, we show results for greedily decoded LLMs and T2IMs here, with stochastic results in Sec. A.3. Evaluation Metrics We collect metrics on the estimated controllable set to inform potential controller design. controller maps desired y. To test existence, we measure the estimated ˆ , cvg ts coverage of better). With confidence δ, cvg implies controller exists from [0, 1] ( (Thm. 3). For each x0, we also assess its functional properties as proxies α of initial states to cvg-fraction of 1 for calibration: Spearman ρ(u0, yt) for monotonicity, Pearson R(u0, yt) for linearity, and MAE(u0, yt) for identity. Ideally, metrics are stable across different initial prompts x0; while we report them, we leave controller design for future work as our focus is on controllability. to control input that drives x0 ˆ / Y (cid:55)"
        },
        {
            "title": "6.1 Controlling Formality in LLMs",
            "content": "Setup We request an LLM to generate text with given formality, engaging the LLM in dialogue until the goal is reached. We test 0-shot and 5-shot prompting for the initial user request u0 Unif(0, 1). Subsequent feedback ut1 deterministically fills template with the previous output yt1. For instance, if the last scored formality was incorrect, the user feedback is Your answer was too [formal informal]. asked for story of formality u0, and you produced story of formality yt1. See Sec. J.1 for all prompt templates. We test three instruction-tuned LLMs: SmolLM3-3B (HuggingFace, 2025), Qwen3-4B (Yang et al., 2025), Gemma3-4B (Team et al., 2025). As initial states we sample x0 from Mistral-7B-v0.1-Instruct (Jiang et al., 2023), asking for conversation opening. While Wolf et al. (2024) find that, when prompts semantically conflict with the task, LLMs need longer interaction to succeed, our setting is shielded from this effect as our x0 are unrelated to task semantics. As readout map we use formality neural scorer (Babakov et al., 2023)5. We use the same controllability and reachability hyperparameters for 5We verify that the requested formality values are reachable with some input. E.g., we manually ensured there are solutions to Generate story with formality 0.0\" (as well as 1.0). 7 Figure 2 (Top, Middle) 5-turn Dialogue Process trajectories for formality task. Controllable set dynamics are shown for (left to right) models SmolLM3-3B, Qwen3-4B, and Gemma3-4B on text formality control task, using 0-shot (top) and 5-shot (bottom) prompting as the initial input. Each linecolor represents different initial state. The α-controllable sets (α = 0.1) are shown in yellow, where full controllability (best-case) would be seen by an entirely yellow = 5. While none of the models are fully controllable 0-shot, and all show formal bias, Gemma3-4B and Qwen3-4B are the most controllable with 5 shots by = 5 (confidence δ = 0.05). (Bottom) Summarized metrics for 5-shot at = 5. The left figure shows the final output in the DP as function of the requested input. The next figures show violin plots of each metric on the formality task, where each point is metric for single x0, demonstrating Qwen3-4B is the most controllable and faithful to the user request for this setting (cvg = 1.0, median MAE = 0.09). all experiments, see Tab. J.4. Controllability is not guaranteed Fig. 2 (top, middle) rows show formality trajectories for 5-turn DP. We test 0-shot (top) and 5-shot (middle) prompting, where the α controllable set (α = 0.1) is highlighted in yellow. In the 0-shot setting, none of the models is fully controllable within 5 steps, although we observe growth of the controllable set. In the 5-shot setting, Qwen3-4B and Gemma3-4B reach full controllability by = 5 with confidence δ = 0.05. On this task, and with the given control, SmolLM3-3B is not controllable. The left-most plot in Fig. 2-(bottom) shows the observed outputs as function of the inputs, evidencing the different model behaviors as well as the poor faithfulness of outputs with respect to inputs on this task. The remaining four figures show averaged metrics across initial states. Both Qwen3-4B and Gemma3-4B are controllable (cvg= 1.0). However, these models show differences in calibration, represented by ρ, and MAE, Qwen3-4B being the most faithful to user requests. Examples or feedback? Whether feedback (0-shot, 5 turns) or examples (5-shots, 1 turn) is more important for controllability highly depends on the model. Indeed, for Qwen3-4B and Gemma3-4B, examples matter more than feedback, seen by wider yellow coverage for = 1 (5-shot prompting, middle row) than for = 5 (0-shot prompting, top row); the opposite pattern holds for SmolLM3-3B. Dialogue results in overshoots The trajectories in Fig. 2 show strong formality overshoots even with the favorable feedback given, which contains both the requested formality u0 and the produced formality yt1. One would expect the model to converge to the target formality, however it is not the case. The overshoot effect is more visible in the 5-shot setting both for Qwen3-4B and Gemma3-4B. Larger models are more controllable Controllability closely relates to the expressivity of the system (Sontag, 1996). As larger LLMs are more behaviorally and representationally expressive (Biderman et al., 2024; Cheng et al., 2025), we tested whether they are more controllable. Fig. 3 shows the controllability of Qwen sizes from 0.6B to 14B (0-shot, 5 turns), where indeed controllability (cvg) and calibration to the user request (ρ, R, MAE) increase with size. However, calibration metrics saturate at 4B parameters, suggesting performance gains on text formality are most salient at small 0.25 from the request. For reference, the error sizes. Overall, calibration is poor even for the 14B model, with an MAE ] = 1 tolerance for this experiment is γ = 0.1, and uniformly generated text formalities would have MAE= E[ 3 where and are both distributed as Unif[0, 1]. 8 Figure 3 Larger models are more controllable and calibrated on text formality. For Qwen sizes ranging from 0.6B to 14B (x-axis), we requested text formalities ranging in [0, 1], with 0-shot prompting and one dialogue turn. While controllability (right) increases reliably up to 14B, the correlation (left plots) between the user request and the output formality, given by R, plateaus around 8B. All calibration metrics (R, ρ, MAE) increase most drastically for smaller sizes (0.6B 1.7B) and appear to saturate for larger sizes. Figure 4 Object generation task for T2IMs. We prompt the model with White background. [N] [obj]s. with = {0 . . . 20} and obj = {80 COCO classes}. The left figure shows the average output object count as function of the requested input. The next figures contain violin plots of each evaluation metric obj, evidencing differences in models. Notably, FLUX-s achieves median ρ, > 0.9 and median MAE = 3.52, showing much better controllability and faithfulness than the rest."
        },
        {
            "title": "6.2 Controlling Number of Objects in T2Is",
            "content": "Setup In this task, we query T2IMs to generate White background. for all obj detector (Minderer et al., 2023). and . As initial states we use x0 = BOS, and as readout map we use 0-shot object } 80 COCO classes [N] [obj]s. with 0 . . . 20 } Unif { { Reachability and faithfulness are sensitive to task semantics Figure 4 shows (left) the input-output faithfulness averaged across the 80 different objects requested, and the averaged statistics across objects in the other plots. Note the large variance in all plots, showing that the requested object biases how desired outputs and actual generated values are correlated. In general, we observe that controlling the number of objects is harder than expected, with FLUX-s showing the best behavior in this setting with MAE of 3.52 and strong calibration shown by ρ, > 0.9."
        },
        {
            "title": "6.3 Additional Experimental Results",
            "content": "In addition to formality and object count (Sec. 6.1 and 6.2), we test controllability for more tasks, shown in Sec. for space reasons. For LLMs, the tasks are: generate (i) positive and (iii) sentence whose average word length is [2.0, 10.0]. For T2IMs, the tasks are: (iv) white background with an object at specific location in {top left, top right, bottom left, bottom right, center} for all 80 MS-COCO object categories, and (v) an image with [0, 1.0] saturation. Note that tasks (i,ii,iv) are discrete and (iii,v) are quantized. integer; (ii) string of length even, odd 10 } { } { 1 These additional experiments corroborate the conclusions reached in Sec. 6.1 and 6.2. Controllability (and calibration) are highly task dependent, for example Gemma3-4B achieves strong results on (i,ii), with almost perfect calibration, while the same model shows poor calibration for formality. Task (iii) appears harder, with Gemma3-4B showing much better controllability, as evidenced by stark difference in trajectories in Fig. A.12. Controllability and calibration also improve with model size, shown for the string length and average word length tasks (i, iii) in Fig. A.8 and A.9. Interestingly, whether the LLM was greedily decoded or sampled did not affect the high-level trends, see Sec. A.3. T2IMs show poor controllability of object location (iv), worse than object count; while image saturation (v) is not controllable. Sec. J.1 contains all the textual templates used, and Sec. an example of our toolkit code. Summary of Experimental Findings Our extensive experiments reveal that controllability is fragile and inconsistent property in modern generative models. This fragility is evident even on the seemingly simple tasks we designed, which represent lower bound on the complexity of real-world applications. We found that no single model or prompting 9 strategy guarantees control across all tasks. For instance, while Gemma3-4B and Qwen3-4B achieved full controllability on the formality task with 5-shot examples, they required iterative dialogue to improve, and SmolLM3-3B remained uncontrollable. Conversely, for T2IMs, even the best-performing model (FLUX-s) exhibited significant errors in object counting and failed to control for object location, demonstrating that controllability is highly sensitive to both task semantics and the chosen model. Ultimately, these results validate the utility of our proposed framework for identifying controllability failures."
        },
        {
            "title": "7 Limitations and Conclusion",
            "content": "Limitations Guarantees from the proposed framework hold for given control system, whose input distribution, readout map, and initial state distribution are defined by the practitioner. Guarantees do not transfer to new choices of these variables. Thus, task-specific takeaways from our experiments are only relevant to the setting in that task. The practitioner is responsible for their choice of input distribution, readout map, and initial state distribution for their specific use-case. Our theoretical framework rigorously estimates DPs controllable set with guarantees (Theorems 1 and 2). As practical byproduct, Algorithms 1a and 1b return all sampled trajectories, including inputs, states, and measurement values. This reveals which inputs elicit which measurement values, as well as regions that are systematically uncontrollable (Section 6). However, because we treat the models as black-box, our framework does not provide interpretability tools that causally diagnose controllability failures on model internals; this is outside the scope of our work. In Theorem 1, the sample complexity to estimate the reachable set scales with its covering number . When estimating joint reachability over attributes, the covering number of the reachable set grows not with the attribute spaces extrinsic dimension d, but rather its intrinsic dimension (Kégl, 2002), which is typically lower in practice. This alleviates sample complexity explosion due to the curse of dimensionality to some extent. Still, Theorem 1 does not scale well when estimating intrinsically complex reachable sets with high precision. This remains an important open problem not only for our setting, but for high-dimensional reachable set estimation in general (Bansal and Tomlin, 2020; Lin and Bansal, 2023; Devonport and Arcak, 2020). As workaround, we recommend constructing quantization of the attribute space with tractable cardinality ; for instance, corresponding to binary safe vs. unsafe regions of attribute space (N = 2). Conclusion This work challenges the implicit assumption of controllability that is fundamental to current efforts in generative AI. We introduce formal framework, grounded in control theory, and practical algorithm to estimate the controllable sets of any opaque model with statistical guarantees. Our empirical analysis reveals that controllability is not given but fragile property, highly dependent on the model, task, and initial state (prompt). We therefore argue for paradigm shift where controllability moves from an implicit assumption to an explicit object of analysis. By providing an analysis toolkit, we set grounds for developing safer and more reliable controllable AI. We foresee potential uses in generative model safety and compliance that include rigorously comparing different control mechanisms, estimating reachable sets under adversarial inputs, enforcing controllability during training, and accounting in policy and deployment."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure reproducibility, we base our work on open source libraries and public datasets, we will also make our code publicly available both in GitHub and as python package. In addition, we provide the main algorithms for Monte Carlo reachability and controllability in Alg. 1, details on the experimental setup in Sec. J, input distributions related to each task in Sec. J.1, and sample of the python package in Sec. K."
        },
        {
            "title": "Ethics Statement",
            "content": "We adhere to ICLRs Code of Ethics. Our work introduces theoretical framework and codebase to assess the controllability of generative models. Our results show that model controllability is fragile and it calls to switch focus from simply controlling to first understanding their fundamental limits. We believe such shift in mindset is necessary to attain more robust, transparent, and safe generative models."
        },
        {
            "title": "References",
            "content": "A. Alanwar, A. Koch, F. Allgöwer, and K. H. Johansson. Data-driven reachability analysis using matrix zonotopes. In A. Jadbabaie, J. Lygeros, G. J. Pappas, P. A. Parrilo, B. Recht, C. J. Tomlin, and M. N. Zeilinger, editors, Proceedings of the 3rd Conference on Learning for Dynamics and Control, volume 144 of Proceedings of Machine Learning Research, pages 163175. PMLR, 07 08 June 2021. URL https://proceedings.mlr.press/v144/alanwar21a.html. N. Babakov, D. Dale, I. Gusev, I. Krotova, and A. Panchenko. Dont lose the message while paraphrasing: study on content preserving style transfer. In E. Métais, F. Meziane, V. Sugumaran, W. Manning, and S. Reiff-Marganiec, editors, Natural Language Processing and Information Systems, pages 4761, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-35320-8. S. Bansal and C. J. Tomlin. Deepreach: deep learning approach to high-dimensional reachability. 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 18171824, 2020. URL https://api.semanticscholar.org/CorpusID:226246253. S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin. Hamilton-jacobi reachability: brief overview and recent advances. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages 22422253. IEEE, 2017. S. Biderman, H. Schoelkopf, L. Sutawika, L. Gao, J. Tow, B. Abbasi, A. F. Aji, P. S. Ammanamanchi, S. Black, J. Clive, A. DiPofi, J. Etxaniz, B. Fattori, J. Z. Forde, C. Foster, M. Jaiswal, W. Y. Lee, H. Li, C. Lovering, N. Muennighoff, E. Pavlick, J. Phang, A. Skowron, S. Tan, X. Tang, K. A. Wang, G. I. Winata, F. Yvon, and A. Zou. Lessons from the trenches on reproducible evaluation of language models. ArXiv, abs/2405.14782, 2024. URL https://api.semanticscholar.org/CorpusID:269982020. F. Borrelli, A. Bemporad, and M. Morari. Predictive control for linear and hybrid systems. Cambridge University Press, 2017. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. E. Cheng, M. Baroni, and C. A. Alonso. Linearly controlled language generation with performative guarantees. In MINT: Foundation Model Interventions, 2024. URL https://openreview.net/forum?id=V2xBBD1Xtu. E. Cheng, D. Doimo, C. Kervadec, I. Macocco, L. Yu, A. Laio, and M. Baroni. Emergence of High-Dimensional Abstraction Phase in Language Transformers. In International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=0fD3iIBhlV. J. J. Choi, C. A. Strong, K. Sreenath, N. Cho, and C. J. Tomlin. Data-driven hamiltonian for direct construction of safe set from trajectory data, 2025. URL https://arxiv.org/abs/2504.03233. A. Devonport and M. Arcak. Data-Driven Reachable Set Computation using Adaptive Gaussian Process Classification and Monte Carlo Methods, Oct. 2019. URL http://arxiv.org/abs/1910.02500. arXiv:1910.02500 [eess]. A. Devonport and M. Arcak. Estimating reachable sets with scenario optimization. In A. M. Bayen, A. Jadbabaie, G. Pappas, P. A. Parrilo, B. Recht, C. Tomlin, and M. Zeilinger, editors, Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 7584. PMLR, 1011 Jun 2020. URL https: //proceedings.mlr.press/v120/devonport20a.html. 11 A. Devonport, A. Saoud, and M. Arcak. Symbolic abstractions from data: pac learning approach. In 2021 60th IEEE Conference on Decision and Control (CDC), page 599604. IEEE Press, 2021. doi: 10.1109/CDC45484.2021.9683316. URL https: //doi.org/10.1109/CDC45484.2021.9683316. E. Dietrich, A. Devonport, and M. Arcak. Nonconvex scenario optimization for data-driven reachability. In A. Abate, M. Cannon, K. Margellos, and A. Papachristodoulou, editors, Proceedings of the 6th Annual Learning for Dynamics &amp; Control Conference, volume 242 of Proceedings of Machine Learning Research, pages 514527. PMLR, 1517 Jul 2024. URL https://proceedings.mlr. press/v242/dietrich24a.html. E. Dietrich, R. Devonport, S. Tu, and M. Arcak. Data-driven reachability with scenario optimization and the holdout method, 2025. URL https://arxiv.org/abs/2504.06541. Google. Gemma open models, 2024. URL https://ai.google.dev/gemma. J. Han, Q. Li, et al. mean-field optimal control formulation of deep learning. Research in the Mathematical Sciences, 6(1):141, 2019. HuggingFace. SmolLM3: smol 3B, multilingual, long-context reasoner smollm3.com. https://smollm3.com/, 2025. [Accessed 09-09-2025]. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. B. Kégl. Intrinsic dimension estimation using packing numbers. In Neural Information Processing Systems, 2002. URL https: //api.semanticscholar.org/CorpusID:6008434. K. Li, O. Patel, F. Viégas, H. Pfister, and M. Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024. A. Lin and S. Bansal. Verification of neural reachable tubes via scenario optimization and conformal prediction. In Conference on Learning for Dynamics & Control, 2023. URL https://api.semanticscholar.org/CorpusID:266210261. M. Marchi, S. Soatto, P. Chaudhari, and P. Tabuada. Heat death of generative models in closed-loop learning. In 2024 IEEE 63rd Conference on Decision and Control (CDC), pages 15241530. IEEE, 2024. G. Marvin, N. Hellen, D. Jjingo, and J. Nakatumba-Nabende. Prompt engineering in large language models. In International conference on data intelligence and cognitive informatics, pages 387402. Springer, 2023. M. Minderer, A. A. Gritsenko, and N. Houlsby. Scaling open-vocabulary object detection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=mQPNcBWjGc. K. Nakamura, L. Peters, and A. Bajcsy. Generalizing safety beyond collision-avoidance via latent-space reachability analysis. arXiv preprint arXiv:2502.00935, 2025. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022a. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022b. H. Park, V. Vijay, and I. Hwang. Data-driven reachability analysis for nonlinear systems. IEEE Control Systems Letters, 8:26612666, 2024. doi: 10.1109/LCSYS.2024.3510595. I. Pinelis. Optimum Bounds for the Distributions of Martingales in Banach Spaces. The Annals of Probability, 22(4):1679 1706, 1994. doi: 10.1214/aop/1176988477. URL https://doi.org/10.1214/aop/1176988477. Publisher: Institute of Mathematical Statistics. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. B. Recht. tour of reinforcement learning: The view from continuous control. Annual Review of Control, Robotics, and Autonomous Systems, 2(1):253279, 2019. W. Ren and D. V. Dimarogonas. Symbolic abstractions for nonlinear control systems via feedback refinement relation. Automatica, 114:108828, 2020. ISSN 0005-1098. doi: https://doi.org/10.1016/j.automatica.2020.108828. URL https://www.sciencedirect.com/ science/article/pii/S0005109820300261. 12 N. Rimsky, N. Gabrieli, J. Schulz, M. Tong, E. Hubinger, and A. Turner. Steering llama 2 via contrastive activation addition. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1550415522, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.828. URL https://aclanthology.org/2024.acl-long.828/. P. Rodriguez, A. Blaas, M. Klein, L. Zappella, N. Apostoloff, M. Cuturi, and X. Suau. Controlling language and diffusion models by transporting activations, 2025a. P. Rodriguez, M. Klein, E. Gualdoni, A. B. adn Luca Zappella, M. Cuturi, and X. Suau. End-to-end learning of sparse interventions on activations to steer generation, 2025b. V. Sivaramakrishnan, K. C. Kalagarla, R. Devonport, J. Pilipovsky, P. Tsiotras, and M. Oishi. Saver: toolbox for sampling-based, In Proceedings of the 28th ACM International Conference on Hybrid Systems: probabilistic verification of neural networks. Computation and Control, HSCC 25, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400715044. doi: 10.1145/3716863.3718045. URL https://doi.org/10.1145/3716863.3718045. S. Soatto, P. Tabuada, P. Chaudhari, and T. Y. Liu. Taming ai bots: Controllability of neural states in large language models. arXiv preprint arXiv:2305.18449, 2023. E. Sontag. Kalmans controllability rank condition: From linear to nonlinear. 02 1996. doi: 10.1007/978-3-662-08546-2_25. E. Sontag. Mathematical Control Theory: Deterministic Finite-Dimensional Systems. 01 1998. doi: 10.1007/978-1-4612-0577-7. X. Suau, P. Delobelle, K. Metcalf, A. Joulin, N. Apostoloff, L. Zappella, and P. Rodriguez. Whispering experts: Neural interventions In Forty-first International Conference on Machine Learning, 2024. URL https: for toxicity mitigation in language models. //openreview.net/forum?id=2P6GVfSrfZ. G. Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière, L. Rouillard, T. Mesnard, G. Cideron, J. bastien Grill, S. Ramos, E. Yvinec, M. Casbon, E. Pot, I. Penchev, G. Liu, F. Visin, K. Kenealy, L. Beyer, X. Zhai, A. Tsitsulin, R. Busa-Fekete, A. Feng, N. Sachdeva, B. Coleman, Y. Gao, B. Mustafa, I. Barr, E. Parisotto, D. Tian, M. Eyal, C. Cherry, J.-T. Peter, D. Sinopalnikov, S. Bhupatiraju, R. Agarwal, M. Kazemi, D. Malkin, R. Kumar, D. Vilar, I. Brusilovsky, J. Luo, A. Steiner, A. Friesen, A. Sharma, A. Sharma, A. M. Gilady, A. Goedeckemeyer, A. Saade, A. Feng, A. Kolesnikov, A. Bendebury, A. Abdagic, A. Vadi, A. György, A. S. Pinto, A. Das, A. Bapna, A. Miech, A. Yang, A. Paterson, A. Shenoy, A. Chakrabarti, B. Piot, B. Wu, B. Shahriari, B. Petrini, C. Chen, C. L. Lan, C. A. Choquette-Choo, C. Carey, C. Brick, D. Deutsch, D. Eisenbud, D. Cattle, D. Cheng, D. Paparas, D. S. Sreepathihalli, D. Reid, D. Tran, D. Zelle, E. Noland, E. Huizenga, E. Kharitonov, F. Liu, G. Amirkhanyan, G. Cameron, H. Hashemi, H. Klimczak-Plucińska, H. Singh, H. Mehta, H. T. Lehri, H. Hazimeh, I. Ballantyne, I. Szpektor, I. Nardini, J. Pouget-Abadie, J. Chan, J. Stanton, J. Wieting, J. Lai, J. Orbay, J. Fernandez, J. Newlan, J. yeong Ji, J. Singh, K. Black, K. Yu, K. Hui, K. Vodrahalli, K. Greff, L. Qiu, M. Valentine, M. Coelho, M. Ritter, M. Hoffman, M. Watson, M. Chaturvedi, M. Moynihan, M. Ma, N. Babar, N. Noy, N. Byrd, N. Roy, N. Momchev, N. Chauhan, N. Sachdeva, O. Bunyan, P. Botarda, P. Caron, P. K. Rubenstein, P. Culliton, P. Schmid, P. G. Sessa, P. Xu, P. Stanczyk, P. Tafti, R. Shivanna, R. Wu, R. Pan, R. Rokni, R. Willoughby, R. Vallu, R. Mullins, S. Jerome, S. Smoot, S. Girgin, S. Iqbal, S. Reddy, S. Sheth, S. Põder, S. Bhatnagar, S. R. Panyam, S. Eiger, S. Zhang, T. Liu, T. Yacovone, T. Liechty, U. Kalra, U. Evci, V. Misra, V. Roseberry, V. Feinberg, V. Kolesnikov, W. Han, W. Kwon, X. Chen, Y. Chow, Y. Zhu, Z. Wei, Z. Egyed, V. Cotruta, M. Giang, P. Kirk, A. Rao, K. Black, N. Babar, J. Lo, E. Moreira, L. G. Martins, O. Sanseviero, L. Gonzalez, Z. Gleicher, T. Warkentin, V. Mirrokni, E. Senter, E. Collins, J. Barral, Z. Ghahramani, R. Hadsell, Y. Matias, D. Sculley, S. Petrov, N. Fiedel, N. Shazeer, O. Vinyals, J. Dean, D. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, J.-B. Alayrac, R. Anil, Dmitry, Lepikhin, S. Borgeaud, O. Bachem, A. Joulin, A. Andreev, C. Hardin, R. Dadashi, and L. Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. A. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023. J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In 10th International Conference on Learning Representations, 2022a. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. Y. Wolf, N. Wies, O. Avnery, Y. Levine, and A. Shashua. Fundamental limitations of alignment in large language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Z. Wu, A. Arora, Z. Wang, A. Geiger, D. Jurafsky, C. D. Manning, and C. Potts. Reft: Representation finetuning for language models. Advances in Neural Information Processing Systems, 37:6390863962, 2024. B. Xue, M. Zhang, A. Easwaran, and Q. Li. Pac model checking of black-box continuous-time dynamical systems. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 39(11):39443955, 2020. doi: 10.1109/TCAD.2020.3012251. 13 A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. H. Zhang, H. Song, S. Li, M. Zhou, and D. Song. survey of controllable text generation using transformer-based pre-trained language models. ACM Comput. Surv., 56(3), Oct. 2023. ISSN 0360-0300. doi: 10.1145/3617680. URL https://doi.org/10.1145/3617680."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides supplementary information and extended results that complement the main body of the paper. Section presents additional experimental results, including detailed α-controllable set evolutions for various tasks and models, and further analysis of image generation tasks. Section formalizes the definition of control system used throughout our work. Section comments on how to interpret reachability and controllability probabilistic guarantees for stochastic and deterministic systems. Section provides detailed discussion and adaptation of Theorem 1 from Devonport and Arcak (2019) for output reachability in discrete-time systems. The concept of discrete bottleneck in dialogue processes, particularly for LLMs and T2IMs, is explored in Sec. E. Detailed proofs for the continuousvalued and categorical versions of Thm. 1 (reachability) are presented in Sec. F.1, along with an analysis of its sample complexity in Sec. F.3. Section extends the reachability analysis to expected output values for stochastic systems. The proof of Thm. 2 (controllability) is given in Sec. H, with discussion on auto-parameter selection for its hyperparameters in Sec. H.3 and its sample complexity. Section illustrates how our framework can be applied to hypothesis testing. Finally, Sec. provides extended details on our experimental setup, including input distributions in Sec. J.1, and Sec. introduces our open-source controllability package."
        },
        {
            "title": "A Extended results",
            "content": "A.1 Setup We covered each setting in LLMs Tasks Prompting method stochastic/deterministic, where the prompting methods, all in dialogue, span 0-shot and 5-shot prompting, the model is held stochastic or deterministic (greedy decoding), and the LLMs and TASKs are given in Sec. 6. Due to the large amount of experiments, and because 5-shot prompting gave the best results, we show the controllability summaries (e.g., Fig. 2 bottom) for the 5-shot setting only, and full DP trajectories for the deterministic setting (both 0and 5-shot). A.2 Results: Deterministic systems Overall, the extended task results confirm the takeaways in Sec. 6. We see significant heterogeneity between models, seen by the varying controllable set estimates in the rightmost plots in Fig. A.5, Fig. A.6, and Fig. A.7. Especially striking is that, for the same model, performance on different tasks is not predictable: for instance, although Gemma34B excels at formality and average word length tasks (Fig. 2 and A.6), it surprisingly is not controllable to the full measurement-value space for the task, one that is trivial for humans. even, odd { } For detailed temporal view, Fig. A.12 shows, for 0 and 5-shot prompting, the α-controllable sets dynamic evolution for all deterministic LLMs TASKS not in Fig. 2. Figure A.5 Requesting string length in {1 10} with 5-shot prompting. We ask the LLM to generate string of length characters, where Unif{1 10}. The controllable set estimates are shown on the (right), with Gemma3-4B displaying the highest α-controllability (α = 0.1), to 80% of the desired range. The distribution of string lengths yT is plotted with respect to the initial request on the (left). In general, especially for Qwen3-4B and Gemma3-4B, the generations are faithful to the request no matter the initial prompt, seen by points landing on the line = (dashed). This is corroborated by the (middle) three plots, however, SmolLM3-3B shows high variance of the faithfulness metrics ρ, R, and MAE across initial states. 15 Figure A.6 Requesting an average word length in [2.0, 10.0] with 5-shot prompting. We ask the LLM to generate sentence whose average word length is values in [2.0, 10.0], up to an error of γ = 0.1, with 5 shots in the initial input u0. (Right) The controllable set estimates for all models. We find SmolLM3-3B and Qwen3-4B to be controllable only to small fraction (0.18 and 0.07, respectively), and Gemma3-4B to larger fraction (0.61), of requests. (Left) The final output distribution yT is shown compared to the requested average word length; in general, models fail for larger requested lengths interestingly, Qwen3-4B tended to resort to default responses\" such as cat sat on mat\" or The quick brown fox jumped over the lazy dog.\" (Middle) The large spreads in ρ, R, and MAE, where each point represents the metric computed for single initial state, demonstrate large sensitivity of model response to the initial state. Figure A.7 Requesting an even/odd number with 5-shot prompting. We ask the LLM to generate positive even or odd integer, with 5 shots in the initial input u0. (Left) We show the distribution of responses by = 5 turns, including an error category (righthand bars on each barchart), where the LLM response was not parse-able to an integer. The (middle) two plots show the LLMs faithfulness to the request, where each point depicts the mean accuracy or F1 score of given initial states reachable set. While (right) Qwen3-4B achieves perfect controllability of the α-controllable set (α = 0.1) with these inputs, as well as perfect faithfulness to the request (middle), results for other models are sensitive to the initial x0, seen by nonzero variance in the violin plots. Notably, near-perfect faithfulness does not necessarily imply controllability, seen SmolLM3-3B: while it achieves high accuracy and F1 score to the request on average (middle), it is not controllable for this task (right). Figure A.8 Larger LLMs are more controllable and calibrated on string length. For Qwen3 sizes ranging from 0.6B to 14B (x-axis), we requested string lengths ranging in {1 10}, with 0-shot prompting and 5 dialogue turns. Controllability (right) and calibration metrics (left) increase reliably up to 14B, though metrics (cvg, R, ρ) increase most drastically for smaller sizes (0.6B 1.7B) and appear to saturate for larger sizes. 16 Figure A.9 Larger LLMs are more controllable and calibrated on average word length. For Qwen3 sizes ranging from 0.6B to 14B (x-axis), we requested output text whose average word lengths range in [2.0, 10.0], with 0-shot prompting and 5 dialogue turns. Controllability (right) and calibration metrics (left) increase up to 14B in sudden phase transition at 4B where the model gains in performance. Figure A.10 Requesting objects at specific locations to T2IMs. We query T2IM with the prompt White background. [obj] at the [pos] of the image., where pos Unif(top left, top right, bottom left, bottom right, center), for all obj in the 80 COCO classes. As measurement, we divide the image in grid of 3 3 (each area of width and height 1 3 rd of the image) and we use an object detector to determine if the object is in one of the requested areas. We observe that T2IM struggle at placing objects at specific locations, even in the simple setting provided. In the (left) figure we see that models tend to place objects in the center (C in the x-axis), followed by bottom right and left (BR,BL) and finally top right and left (TR,TL). The darker lines are the count of objects not placed in any of these positions. The other plots show the averaged statistics per object, showing that FLUX-s is theoretically controllable (they reach the whole output space with maximal coverage and show better accuracy). However, these results are far from perfect controllability, underscoring the need for rigorous analysis of models. Figure A.11 Requesting image saturation to T2IMs. We query T2IM with the prompt Generate an image with [100 sat]% saturation., where sat Unif[0, 1.0]. This is an example of quantized reachability, therefore we set γ = 0.1 We observe that T2IM FLUX-s and SDXL are theoretically controllable (they reach the whole output space with maximal coverage). However, the models are not calibrated at all to the request, with correlations only reaching ρ, < 0.1. In the (right) plot we show the reachable sets for each input (dark bars) and the controllable set (shaded area). Interestingly, the models show stark difference in terms of controllability in this setting. 17 Figure A.12 Extended controllability trajectories for deterministic LLMs. To supplement Fig. 2 (top, middle), we show here the DP trajectories for the remaining LLM tasks, in the deterministic case. A.3 Results: Stochastic systems Our experiments primarily focused on DPs with deterministic, greedily decoded generative models. This is because the tasks, being goal-oriented, e.g., produce an even/odd number, are more amenable to greedy sampling. However, as generative models may be sampled in practice, for stochastic LLMs (5-shot) we report DP controllability summaries in Fig. A.13. 18 (a) Formality task, stochastic system. (b) Average word length, stochastic system (c) String length, stochastic system (d) Even/odd integer, stochastic system Figure A.13 Controllability results, stochastic LLMs. For the formality task, average word length, string length, and even/odd number (top to bottom), we show the controllability summaries for stochastic systems. The sample complexities are the same as for the deterministic system, see Thm. 2. Adding stochasticity due to sampling results in more diffuse distributions of the output (left), notably for the even/odd integer task, as well as worse faithfulness metrics, as expected. 19 A.4 Example qualitative analysis: Qwen, formality For text formality, we conduct qualitative analysis of Qwen3-4Bs generations (the best performing model) for three randomly sampled trajectories for the 5-shot, 5 turn setting. The resulting dialogues are in Tables A.1-3. In general, Qwen3-4B takes the strategy of choosing the example sentence (of the 5 shots) whose formality is closest to the requested formality. It remains on the theme of the example sentence, modifying one or two words until the formality is as desired. For instance, in requesting 25.65% formality, Qwen3-4Bs initial response wow im so tired of this weather\" which tracks the example Wow Im so tired\" (25% formality). Subsequent responses then continue the theme of being tired of the weather. Similarly, in the 76.19% case, Qwen3-4B copies the general sentence structure of the 75% formal example. For the request to generate text of 96.31% formality, however, Qwen3-4B generates texts that are too formal. It does however still follow the strategy of mimicking the style of the 100% formal example, then modifying few words at time."
        },
        {
            "title": "Role\nUser",
            "content": "Dialogue You are chatbot that follows instructions. will give you an example: Instruction: Generate story that is 100% formal. Result: Kind sir, please make your way to the terrace. Instruction: Generate story that is 75% formal. Result: She bought the book, then she went to the beach. Instruction: Generate story that is 50% formal. Result: Hey! Just went to the store. Instruction: Generate story that is 25% formal. Result: Wow Im so tired. Instruction: Generate story that is 0% formal. Result: hey man hows it goin Assistant User Assistant Knowing this, please follow the next instruction. Generate story that is 25.65% formal. Write the story and only the story. wow im so tired of this weather Your answer was too informal. asked for formality level of 25.65, and you gave sentence with formality 12.27. Please try again. Write the story and only the story. im really fed up with the rain today [Conversation ended, final formality 25.24%] Table A.1 Requesting formality of 25.65% to Qwen3-4B. Role User Dialogue You are chatbot that follows instructions. will give you an example: Instruction: Generate story that is 100% formal. Result: Kind sir, please make your way to the terrace. Instruction: Generate story that is 75% formal. Result: She bought the book, then she went to the beach. Instruction: Generate story that is 50% formal. Result: Hey! Just went to the store. Instruction: Generate story that is 25% formal. Result: Wow Im so tired. Instruction: Generate story that is 0% formal. Result: hey man hows it goin Qwen3-4B User Qwen3-4B Knowing this, please follow the next instruction. Generate story that is 76.19% formal. Write the story and only the story. She arrived at the station, checked her ticket, and boarded the train. Your answer was too informal. asked for formality level of 76.19%, and you gave sentence with formality 64.04%. Please try again. Write the story and only the story. She arrived at the station, presented her ticket, and took seat on the train. [Conversation ended, final formality 76.31%]. Table A.2 Requesting formality of 76.19% to Qwen3-4B."
        },
        {
            "title": "Role\nUser",
            "content": "Dialogue You are chatbot that follows instructions. will give you an example: Instruction: Generate story that is 100% formal. Result: Kind sir, please make your way to the terrace. Instruction: Generate story that is 75% formal. Result: She bought the book, then she went to the beach. Instruction: Generate story that is 50% formal. Result: Hey! Just went to the store. Instruction: Generate story that is 25% formal. Result: Wow Im so tired. Instruction: Generate story that is 0% formal. Result: hey man hows it goin Assistant User Assistant User Assistant User Assistant User Assistant Knowing this, please follow the next instruction. Generate story that is 96.31% formal. Write the story and only the story. Kindly undertake the task of composing narrative that adheres strictly to the conventions of formal language. The story should be presented in manner that reflects the highest degree of formality, with no deviation from the expected tone and structure. The narrative should be self-contained, devoid of any additional commentary or explanation, and should solely consist of the story as requested. Your answer was too formal. asked for formality level of 96.31, and you gave sentence with formality 99.72. Please try again. Write the story and only the story. Kindly proceed to the designated location at once. Your answer was too formal. asked for formality level of 96.31, and you gave sentence with formality 99.64. Please try again. Write the story and only the story. Kindly proceed to the designated location immediately. Your answer was too formal. asked for formality level of 96.31, and you gave sentence with formality 99.62. Please try again. Write the story and only the story. Kindly proceed to the designated location without delay. Your answer was too formal. asked for formality level of 96.31, and you gave sentence with formality 99.64. Please try again. Write the story and only the story. Kindly proceed to the designated location at once. Table A.3 Requesting formality of 96.31% to Qwen3-4B."
        },
        {
            "title": "B Formal Definition of Control System",
            "content": "Definition 7. discrete-time control system is tuple (ϕ, , , , , h), where 1. 2. 3. 4. ϕ : 5. : = is the time domain is the state space, is the input space, measurement value. ; xt, ut ; xt, ut (cid:55) (cid:55) xt+1 is the transition function mapping the current state and input to the next state, yt is the readout map, taking the current state and inputs to the current output or Reachability and Controllability for deterministic vs. stochastic systems Dietrich et al. (2025) explain that PAC-style guarantees for reachable sets fold in two sources of stochasticity: that of system dynamics and that from sampling uncertainty. Similarly, in our case, the interpretation of α, p-approximate controllability differs under deterministic and stochastic dynamics, as explained in Sec. 4.3 of the main text. In particular, α, p-approximate controllability is exact (or within-γ) under deterministic dynamics and deterministic controller, i.e., given desired that takes us to exactly (within γ). Instead, the difference for stochastic dynamics is that the same α, p-approximate controllability now pertains to the distribution of y: exact controllability no longer applies due to stochasticity in the dynamics (and by extension, of the readout h). In short, for stochastic system, the returned ˆ t,p by Alg. 1a returns bestor worst-case reachable set estimate; the returned ˆ by Alg. 1b returns controllable set estimate that inherits the same bestor worst-case distributional interpretation. α proportion of initial states, we are probabilistically guaranteed to find α , for over 1 One question we can ask about stochastic system with continuous-valued outputs is the reachability/controllability of its expected measurement-value, that is, E[y]. We prove in Sec. that E[y] does not suffer from the discrete bottleneck, that is, we can directly repurpose existing methods (Devonport and Arcak, 2019) returning continuous set estimates, rather than finding relaxation for countable sets. We provide such bound (Cor. G.1) on the reachable set of E[y] in Sec. G, as corollary to the main PAC bound in Devonport and Arcak (2019). The latter is stated in Sec. for reference. Then, for controllability, one can simply replace the reachable set estimation subroutine in Alg. 1b with the procedure described in Sec. G. Theorem 1 of Devonport and Arcak (2019) In this section, we compare our method to the Monte Carlo method of Devonport and Arcak (2019) (DA19). Their method takcles the same problem of reachable set estimation, though it is for states and not outputs/measurement-values (our case). Our algorithms and bounds were inspired by DA19, and we propose corollary to their Theorem 1 in Sec. for expected output reachability. For this reason, we state their main result and compare it to our Thm. 1 below. Monte Carlo method of Devonport and Arcak (2019) Devonport and Arcak (2019) (hereon DA19) propose an elegant Monte Carlo sampling approach to estimate an overapproximating interval of the reachable set. While their original result focuses on state reachability and for continuous-time systems, here we directly adapt their approach to output reachability and for discrete-time systems. DA19s sampling algorithm states that to estimate the forward reachable set at time t: 1. Take i.i.d. samples of the initial state 2. Evaluate outputs y(i) 3. Take ˆ = y(x(i) , u(i) x(i) 0 } { ) at time t. (m) as the smallest axis-aligned interval containing all y(i) . i=1 p0 and of the input u(i) i=1 } { pU . The forward reachable set at time can then be defined as an event ω Noting that p(ω) = p(xt t, then the true reachable set ω), ω is the smallest ω in the probability space (Rn, t, py,t). with probability 1. The sampling procedure can only produce approximations of the reachable set. To define this approximation, DA19 introduce the notion of ϵ-accurate reachable set: 23 Definition 8 (ϵ-accurate forward reachable set, Devonport and Arcak (2019)). The ϵ-accurate reachable sets time are the smallest events ω with probability 1 ϵ. t,ϵ at In words, the ϵ-accurate reachable set at time approximates the true one up to probability density of ϵ. This probability density is given by the measure py,t over outputs yt. ϵ. DA19 further define an This means that the ϵ-accurate reachable set Rn such that contains an ϵ-accurate reachable set. overapproximation of an ϵ-accurate reachable set as any set This means that pt(xt ϵ. Crucially, with enough samples, we can construct set that overapproximates or contains the ϵ-accurate reachable set. This is the key insight of DA19s main result, which holds if pt,y is continuous (see Sec. for details): t,ϵ is such that pt(xt t,ϵ) = 1 t,ϵ) = 1 pt(xt R) Theorem 9 (Output forward reachability, adapted Devonport and Arcak (2019)). Let (ϵ, δ) 2n ϵ log 2n δ , (0, 1) and = Rn. If (D.1) then ˆ Note that ˆ 1 sufficient for ϕ to be controllable on (m) overapproximates an ϵ-accurate output reachable set with confidence δ, i.e., P( t,ϵ (m)) ˆ δ. (m), the ϵ-accurate output reachable set, is guaranteed to contain the true reachable set with probability , it is necessary but not δ. However, it may also contain outputs that are unreachable. Hence, given target set that (m). ˆ D.1 Comparison to our Thm. Here are where DA19 and our method differ: 1. They consider state space reachability, while we consider output reachability. 2. They assume continuous reachable set, while in our setting it is countable. In particular, continuity of pt is implied in their proof of Theorem 1 which constructs the bound, whereas in our setting it does not apply. 3. DA19 returns an overapproximating interval (a bounding box) of probabilistically approximate reachable set, while we return set that is guaranteed within-γ to hold probabilistically approximate reachable set. There are several notable points where DA19 and our Thm. 1 coincide: 1. The probabilistic scaffolding in Sec. 3 of the control process is the same, in order to i.i.d. sample yt. 2. The sampling algorithm (sampling i.i.d. trajectories), except for the construction of the returned set."
        },
        {
            "title": "E Discrete bottleneck in dialogue processes",
            "content": "Here, we discuss why, and under what conditions, generation with LLMs and T2IMs has discrete bottleneck. That is, even when the readout map is continuous-valued, the reachable set could be countable (if the readout map is categorical, we are already in discrete regime). For the analysis in this section, we assume deterministic readout map whose image is continuous-valued. We consider several cases: if the model is stochastic vs. deterministic, and whether the model generates text (discrete) or images (continuous). In order to analyze the reachable set of an LLM or T2IMs generation, we need to consider each forward pass, which is more granular level than each dialogue turn (our temporal resolution of interest in the main text). We denote τ as the timestep indexing number of token generations. To make the results as general as possible, i.e., encompassing activation steering which acts at each forward pass, we write all results assuming inputs may be introduced during each forward pass τ . E.1 Conditions for discrete bottleneck, deterministic system We start by showing that deterministic LLMs, which produce distribution over their finite discrete vocabulary space Σ, will have countable reachable sets. 24 Lemma 10 (Deterministic intervened LLMs reachable sets are countable). The output attributes of an intervened is the LLM ϕ can be controlled, after τ token generations, to set of cardinality at most min( vocabulary size. τ ), where τ , Σ Σ Σ im(ϕ(x0, u)) im(ϕ( , )) ). We repeat this reasoning to integrate values of 2 Proof. We use cardinality argument. We consider the first token generation. Given any x0 ϕ is deterministic, we have )) , im (h( the vocabulary size. Hence )) , τ , τ ) Σ min( min( , for all x0 . Then, we apply Thm. 15 to see that the dialogue process with deterministic ϕ can be controlled to set of maximum size τ , , is less min( than min( τ , so that τ . We have bounded the size of the reachable set for given x0 τ ), as the largest intersection between the reachable sets τ , Σ ). Since the readout map is deterministic, = 1, which implies min( , τ ) for initial states x(i) im (h(τ ; x0, u1τ )) , fix im(ϕ( . As , Σ . Further, im(ϕ(x0, 0 (x(i) 0 , , = 1 and ui Σ τ ). U )) Σ , Lemma 11 (Deterministic, intervened T2IMs reachable sets have cardinality at most of deterministic intervened T2IM ϕ can be controlled to set of cardinality at most of all possible input strings. Σ Σ ). The output attributes is cardinality , where Σ Proof. (Sketch). We use similar argument to Thm. 10. reachable set at time = 1 is only bottlenecked by its initial prompt space that its reachable sets cardinality is maximum Σ , with equality if is injective. If the readout map (Sec. 3) is deterministic, then the 0 = Σ and its input space . It follows This means that for LLMs, there is guaranteed discrete bottleneck from the vocabulary space. For T2IMs, if the input space is continuous, e.g., activation steering, then there is no discrete bottleneck. In our case, as we study discrete-valued inputs of finite length (prompting), there is necessarily discrete bottleneck for T2IMs. E.2 Conditions for discrete bottleneck, stochastic system Here, we consider under which conditions stochastic systems will have discrete bottleneck. The discrete bottleneck is guaranteed for LLMs, whose input and output spaces are both strings however, stochastic T2IMs will have continuous output sets. Lemma 12 (Stochastic intervened LLMs reachable sets are countable). The set of output attributes of an intervened LLM ϕ after τ token generations, have cardinality at most is the vocabulary size. τ , where Σ Σ Proof. The number of potential outputs during each forward pass is the number of vocabulary items generations, the maximum number of possible outputs is τ . Σ . After τ token Σ Lemma 13 (Stochastic intervened T2IMs reachable sets can be uncountable). The set of output attributes of an intervened T2IM ϕ can be uncountable. Proof. The proof follows from replacing the cardinality of vocabulary space in Thm. 12 with the cardinality of T2IMs generation space. As T2IMs generate images, which are continuous, the reachable set of stochastic T2IMs is uncountable. Σ E.3 Expected outputs controllability Stochastic generative models define distribution that is sampled at inference time. While the output attributes themselves are bottlenecked, their expectations may not be. The expectation of the attribute over the models distribution τ , where τ is the number of tokens generated in the case of an LLM is only bottlenecked by the input cardinality (τ = 1 for T2IM). Lemma 14 (Stochastic LLMs are expected output controllable on cardinalityvalue E[y] of stochastic LMs ϕ, where the next token is sampled from to sets of cardinality at most τ by token generation τ . τ sets). The expected measurement Σ1,6 can be controlled pΣ with pΣ 6Notation: Σ1 is called the Σ 1-simplex\", or the space of categorical probability distributions over Σ categories. 25 Σ with im(ϕ) Proof. We first consider stochastic LLMs. We use the same cardinality argument as in Thm. 10. The proof follows . This means that the expected output attribute EspΣ [y] can from replacing im(ϕ) maximally vary over as many possible values as pΣ, which is , the cardinality of bottlenecks the expected output attribute. im (h(x0, τ , by Analogous to the proof of Thm. 10, it follows that, given x0, the reachable sets cardinality (x0, τ . integrating the system. Then, applying Thm. 15, the size of the controllable set is maximum , where the choice of input )) is less than min( . Then, given any x0 Σ1 Σ1 R ) = , τ ) U , = = U Reachability: Theorem 1 F.1 Theorem 1: continuous-valued version We state the γ-quantized version (for continuous-valued measurements) of Thm. 1. The proof for the categorical version follows directly. Theorem 3: Sample complexity bound, γ-quantized reachability. Let the number of i.i.d. samples drawn from Yt, i.e., be continuous and bounded. Let be the covering number of i=1 with each yi } (cid:18) yi (cid:19) { with -balls of radius γ/2. Let be i. Fix δ (0, 1). If , (F.1) max N, log(δ/N ) p) log( then P( γ t,p i=1B (yi, γ)) δ, where (y, γ) is the ball centered at with radius γ. Proof. The problem reduces to asking how many independent samples are needed to hit all bins with likelihood 1 δ, where the probability of sampling each bin is > p. γ (m) = t,p, the probability sample hits the bin yj Let i.i.d. samples be drawn from py,t: ˆ of outside bin is then py,t(yi / yj P( ˆ p)m. bin) yj bin) (m) (1 1 yi { bin in i=1 (omitting from the RHS for readability). By definition γ p. The probability sample lands t,p is py,t(yi land outside nontrivial bin is p. The probability all samples = yj bin) } Then, by the union bound over maximum of bins, the probability at least one bin is never hit is at most (1 Subtracting from 1, the probability that all bins in γ t,p are hit by some sample is P(all yj bin hit) P( γ t,p is γ-reached by the samples) 1 (1 p)m. δ. We do so by guaranteeing 1 We want to guarantee that P( Then, solving for m, if γ t,p is γ-reached by the samples) 1 p)m. (F.2) (1 p)m δ. 1 (F.3) then with probability be greater than the number of bins . It follows that if δ, 1 log(δ/N ) p) log(1 γ t,p is γ-reached by the samples ˆ , (m). By the pigeonhole principle, also needs to then all bins, each with radius γ/2 (side-length γ), are hit with probability (cid:18) max N, (cid:19) , log(δ/N ) p) log( (F.4) δ. 1 γ t,p is within γ of some sampled point in ˆ (m). It By (F.2), we are now guaranteed w.p. γ t,p is contained in γ-cover of ˆ (m), or P( δ. follows that w.p. i=1B δ that each point in γ t,p (yi, γ)) δ, the set 1 1 26 F.2 Corollary of Theorem 1: categorical version Corollary F.1: Sample complexity bound, categorical reachability be discrete with finite cardinality . Let be the number of i.i.d. samples drawn from Yt, i.e.,"
        },
        {
            "title": "Let\nY\nwith each yi",
            "content": "i. Fix δ (0, 1). If (cid:18) max N, (cid:19) , log(δ/N ) p) log( yi i=1 } { (F.5) then P( t,p { yi i=1) } 1 δ. Proof. Follows from proof of Thm. 3. F.3 Sample complexity of Theorem 1 Figure F.14 Sample complexity growth with hyperparameters. From left to right, we see sample complexity growth with respect to δ (constant p), (constant δ), and (constant δ). Overall, the most drastic change in sample complexity comes from decreasing p. Sample complexity also increases sharply as δ draws closer to 0. In contrast, sample complexity is robust to increasing in fine-grained regime (N 50). The sample complexity remains tractable for parameters (δ, p, ) = (0.05, 0.05, 100), giving = 149. Figure F.14 visualizes the growth of with respect to each parameter. The dependence of on δ and is given by the form of Thm. 1, and are seen as sharp growth as and δ grow closer to 0. The dependence on is more drastic (Figure middle) compared to δ (Figure left). The dependence of on is less straightforward. We sketch why grows as bins . Then, cannot exceed the probability of bin under uniform density: We use the observation that log(1 p, valid for small (0, 1): p) log(δ/N ) log(1 p) = log(N/δ) (log log δ) (N log ). (N log ). First, fix the number of maxpt,yPt,y minj pt,y(yj bin) = 1 . (F.6) (F.7) (F.8) F.4 Empirical validation of Theorem 1 The sample complexity bound in Thm. 1 is distribution-free and holds for any control system. Therefore, the tightness of the bound likely differs across experimental settings, and specifically for different distributions the underlying generative model induces. Therefore, instead of formal analysis of tightness, which would be highly nontrivial in the absence of the DPs distributional information, in this section we perform sanity check that probabilistic guarantee in Thm. 1 holds in our setting of interest, i.e., on generative model. 27 F.5 Setup We test only the most general case, that is, for stochastic system ϕ and γ-quantized reachability. In particular, we use Gemma3-4B on the text formality task, for 0-shot prompting and = 1. The initial state is x0=BOS. All other hyperparameters and prompt templates are replicated from Tab. J.4 and J.5 and the input distribution and readout map inherit the formality setting described in the main text. For the DP defined for the above initial state, input distribution, and readout map, we first proxy the true distribution py,t by sampling and scoring = 10000 responses from Gemma3-4B. On this proxied true distribution, we compute γ t,p as described in Thm. 5. the true\" (p, γ)-approximate reachable set (m) γ To validate Thm. 1, we run Alg. 1a 200 times. Each time, we check whether the guarantee ˆ t,p holds. Then, we average over the 200 runs to obtain the empirical confidence. If the empirical confidence complies with the desired confidence parameter δ, then it serves as sanity check for Thm. 1. Results Figure F.15 validates Thm. 1 in practice, where we plot the empirical confidence against the sample complexity. Each point on the line represents the mean over 200 runs. Theorem 1 recommends minimum sample complexity of = 104, which empirically adheres to the guarantee that ˆ δ = 0.95. For this particular setting, Thm. 1 is reasonably tight, given the fewest samples to satisfy the guarantee is around 40. This represents roughly 2.5 γ t,p above the desired confidence 1 inefficiency in the bound. (m) Figure F.15 Empirical validation of Theorem 1."
        },
        {
            "title": "G Expected output reachable set",
            "content": "Theorem 14 says that considering expected outputs under stochastic dynamics no longer suffers from the discrete bottleneck. We present corollary to DA19s Theorem, (stated in this papers Thm. 9) that estimates the reachable set of the expected output value for stochastic dynamics. Corollary G.1: Expected output forward reachability for stochastic system Let (ϵ, δ) (0, 1) and let (ϵµ, δµ) (0, 1). If satisfies δµ)m(1 (1 2n(1 ϵ 2n )m) δ, 1 (G.1) (m),y ϵµ then ˆ the true expected output with confidence δ, i.e., P( (m),y of Thm. 9 pushed out by ϵµ, overapproximates an ϵ-accurate reachable set of ) , which is the ˆ δ. 1 (m),y ϵµ EY1 [t0,t1],ϵ ˆ 28 Proof. We extend the proof of Thm. 9 to the expectation of yt. The key addition to the proof of Thm. 9 is uncertainty in Eyt caused by sampling, for which we use vector variant of Hoeffdings inequality from Pinelis (1994). The inequality states that, for samples of random variable R, and choice of ϵµ > 0, then"
        },
        {
            "title": "Rd that has bounded deviations from the mean",
            "content": "EY 2 where δµ = 2 exp approximate the true mean EY with high probability. ϵ2 µN 2R2 > 0. That is, for enough samples P( EY 2 ϵµ) 2R2 ϵ2 µ δµ, log 2 δµ , 0 < δµ < 1, the empirical mean will (G.2) Hoeffdings inequality enters when choosing the halfspaces of the interval covering the reachable set. In Thm. 9, each from the samples in the interval. We modify this definition here. Let approximations of the true means { all yj; that is, all sampled means yj are within ϵµ of the true mean µy Rn, there are 2n faces\" in Thm. 9. Recall that, as yt i, = 1 . . . 2n is defined as the halfspace facing away\" k=1 yj,k, be sampled j=1. For choice of ϵµ and confidence δµ, let be such that (G.2) holds for } , with confidence δµ. Now, define the halfspaces j=1, where yj = 1 yj} { (cid:80)N"
        },
        {
            "title": "Eyj",
            "content": "i as the faces of the interval covering all sampled means yj, then shifted each by ϵµ away from the samples. The probability that all true means are contained within the bounding box defined by the is at least the probability that all true means lie within ϵµ of their corresponding sample. Because the samples are independent, this probability is given by P( j=1( yj µy 2 < ϵµ)) (1 δµ)m. (m),y ϵµ be the smallest bounding interval of the sampled means, pushed out by ϵµ. If µy We now proceed following the proof of Thm. 9, defining the halfspace Let ˆ i, then H δµ)m, pt( or pt( i) ϵ (m),y is an ϵ-accurate reachable set. Overall, let the event be that ˆ pt( ˆ contains an ϵµ ϵ-accurate reachable set. Our goal is to bound P(A). We do so by conditioning on an event B: bound the true means\". Then, P(A) lie outside the with respect to the true means µy 2n , with probability greater than (1 reduces to the setting in DA19: the δµ)m. Then, w.p. (1 i) = ϵ ϵ = B). The event i) (m),y ϵµ P(B)P(A , = ) pt( 1 (m),y ϵµ ˆ m. i, i\". (G.3) P(B) is at least (1 δµ)m. Now, given B, that is, given the P(A B) 2n(1 ϵ 2n bound the true means, (Theorem 1, DA19) )m, Then, the overall confidence is given by P(pt( ˆ (m),y ϵµ ) ϵ) = P(B)P(A B) (1 (cid:124) If the number of samples satisfies δµ)m(1 2n(1 (cid:123)(cid:122) target probability 1δ δµ)m (1 (cid:123)(cid:122) (cid:125) (cid:124) (i) Equation (G.3) (1 (cid:124) ϵ 2n 2n(1 (cid:123)(cid:122) (ii) Theorem 1, DA19 )m) (cid:125) δ, 1 then ˆ (m),y ϵµ contains the ϵ-accurate reachable set with confidence level δ. ϵ 2n . )m) (cid:125) (G.4) (G.5) (G.6) Note opposing dependencies of factors (i) and (ii) on m. The probability increases, while the probability that do not contain samples decreases to 0 as m. do not contain samples increases as H Controllability: Theorem 2 H.1 Output controllable set Definition 15 (Output controllable set). Given control system (ϕ, time is given by , , X , y), the output controllable set = (cid:92) x0X0 t(x0, ), at (H.1) or the intersection of the reachable sets of all initial states x0 0. 29 H.2 Proof of Theorem 2 We state the proof of Thm. 2 for the quantized case. The proof of the categorical case immediately follows by removing γ everywhere it is written. Proof. Let be the event that µ( ˆ t,p(x(i) ˆ that 0 ) target. We have P(E) the RHS individually. t(x(i) γ α ) < ϵ. We want to P(E) to be greater than some 1 δ. Now, let be the event 0 ) for all = 1 . . . k. That is, is the event that all sampled reachable sets contain the true )P(F ). Our strategy will be to lower bound the RHS by lower-bounding each term on P(E We start with P(F ). By independence of the sampled reachable sets, we have P(F ) (1 δR)k, by Thm. 3. t(x(i) p0, = 1 . . . k. We aim to bound the error µ( ˆ 0 ) to be the empirical intersection of sampled sets, ). For brevity, we rewrite = ˆ α α to C α have k. All points ybin in the true (α)-controllable set α. All points in B, are bins that have incorrectly survived samples. In other words, each element ybin α , but inside the empirical ). Abbreviate ˆ i=1 ˆ = We now consider the term P(E where each x(i) 0 mean all points outside the true µ(ybin) > 1 of is such that µ(ybin) < 1 α. Our aim is to find number of sampled sets after which P(µ(B) < ϵ ) 1 δC. This is achieved with Markovs inequality: P(µ(B) > ϵ ) E[µ(B) ϵ ] . Then, if E[µ(B) µ(ybin) < 1 ]/ϵ δC, then (H.2) holds. Note that E[µ(B) ] is equal to the probability ybin with α is hit by samples, given that each sampled reachable set is accurate. This likelihood is at most E[µ(B) ] = P(ybin : µ(ybin) < 1 α)k. (1 α hit by samples all reachable set samples accurate) Then, if then P(µ(B) < ϵ ) 1 δC, as desired. Putting the two terms together, we have that if µ( ˆ α ) ϵ. α)k (1 ϵ δC = log ϵδC log(1 α) , log ϵδC log(1α) , then with probability at least (1 δC)(1 δR)k, H.3 Auto-parameter selection for Theorem 2 There are several free parameters in Thm. 1 and 2. If the end goal is controllability analysis, then given target confidence 1 δC), we can compute the optimal values for δR and δC such that the total sample (1 complexity = is minimized. The intuition is that δR and δC are tradeoff between more reachable set samples (m) and more initial state samples (k). δR)k(1 δ 30 (H.2) (H.3) (H.4) (H.5) (H.6) (H.7) Formally, this can be written as the following constrained optimization problem: minimize = k (Total number of samples) subject to log(δR/N ) log(1 p) δC) log(ϵ log(1 α) δR)k(1 δC) (0, 1) (1 δ, p, α, ϵ N2 m, with fixed parameters and optimization variables N2, δC, δR (0, 1). (Reachability, Thm. 1) (Controllability, Thm. 2) 1 δ (Overall confidence constraint) (H.8) (H.9) (H.10) (H.11) (H.12) (H.13) (H.14) If we only require an overall confidence δ on the controlllable set (RHS of Theorem 2), we can abstract away the decision of δR and δC. By doing dense grid search over (δC, δR) on np.geomspace(1e-6, 0.999, 250)2, we can find the minimum total number of samples needed to satisfy all the constraints in Thm. 1 and 2 and (H.11). The extra time taken by this step is negligible compared to the time it takes to sample the dialogue process. H.4 Sample complexity of Theorem 2 We split the analysis into two parts: (i) dependence of on hyperparameters, see Fig. H.16, then (ii) dependence of the total sample complexity = on hyperparameters, using the subroutine described in Sec. H.3. Figure H.16 Number of initial state samples dependence on hyperparameters. From left to right, we see the number of initial state samples grow with respect to δC (constant α), α (constant δC ), and ϵ (constant δC ). Overall, the most drastic change in sample complexity comes from decreasing α, the partial controllability parameter. Sample complexity also increases sharply as δC approaches 0. In contrast, sample complexity is robust to increasing ϵ, for ϵ closer to 1, especially for more lenient partial controllability (higher α). Figure H.16 shows that the number of sampled initial states highly depends on α (middle panel), while comparatively robust to the confidence parameter δC and error ϵ. What is more important is the total number of samples n. We performed the procedure in Sec. H.3 to abstract away the choice of δC and δR, and analyzed the dependence of total sample complexity on p, , ϵ, and α. Figure H.17 shows how varies with each hyperparameter. Sample complexities hover near 107 for very small ( 0.001) values of α and p. 31 Figure H.17 Total sample complexity as function of hyperparameters. We vary the total number of samples in controllable set estimation as function of each fixed hyperparameter (δ, ϵ, α, p, ), one at time. When varying each parameter (each panel), we fix all others except for , which varies, at constant value of 0.05. Given overall confidence δ, we auto-select δC and δR using the algorithm in Sec. H.3."
        },
        {
            "title": "I Hypothesis testing",
            "content": "Because Thm. 1 and 2 estimate confidence sets, they lend themselves to hypothesis testing. We give an intuition in the below example for the reachable set: Example I.1: Hypothesis testing, reachable set 1. Q: have toxicity verifier. Is it possible to control my LLMs toxicity to [0, 0.4] with prompting? am fine with an error of γ = 0.05, and consider region to have nontrivial probability if its probability is greater than = 0.001. 2. Quantize. Estimate = 0.4/0.05 = 8 bins. 3. Null hypothesis: My target range [0, 0.4] is 0.05-quantized reachable. 4. Set the p-value cutoff to, e.g., δ = 0.05. 5. Sample 6. Suppose we obtain ˆ 7. Reject the null hypothesis, concluding that 2204 toxicity scores (applying Thm. 1). (m) = [0, 0.07] [0.5, 0.8] . is not reachable within γ with p-value cutoff δ. Y"
        },
        {
            "title": "J Extended experimental setup",
            "content": "Table J.4 Hyperparameters for all controllability experiments. Variable α ϵ δ γ Description partial controllability, Thm. 2 controllable set error, Thm. 2 controllable set confidence, Thm. 2 reachability precision, Thm. 1 quantization, Thm. 1 Value 0.1 0.05 0.05 0.05 0.1 Table J.5 Model sampling hyperparameters. For the stochastic setting, i.e., where we sample the underlying LLM, as well as for initial state generation p0 with Mistral, we use the following sampling parameters. max_new_tokens temperature (stochastic ϕ) temperature (p0) top_p (stochastic ϕ) top_p (p0) top_k 100 0.7 0.9 0.9 0.95 50 J."
        },
        {
            "title": "Input distributions",
            "content": "We here describe the input distributions related to each task in the experimental results. LLM task templates \" Generate strictly positive {} integer .\" Generate an even/odd number Template 0-shot \"\"\" You are chatbot that follows instructions . will give you some examples : Instruction : Generate strictly positive odd integer . Only generate the integer and nothing else. Result : Instruction : Generate strictly positive even integer . Only generate the integer and nothing else. 33 Result : 94726 Instruction : Generate strictly positive odd integer . Only generate the integer and nothing else. Result : 7 Instruction : Generate strictly positive even integer . Only generate the integer and nothing else. Result : 200 Instruction : Generate strictly positive odd integer . Only generate the integer and nothing else. Result : Knowing this , please follow the next instruction . Instruction : Generate strictly positive {} integer . Only generate the integer and nothing else. \"\"\" Generate an even/odd number Template 5-shot \"Your answer was incorrect . asked for { desired_value } number , and you gave an { last_str }. Please try again. Strictly write the integer without quotes and do not write anything else.\" Generate an even/odd number Feedback template \"Write string of {} characters . Strictly write the string without quotes and do not write anything else.\" Generate number of characters Template 0-shot \"\"\" You are chatbot that follows instructions . will give you some examples : Instruction : Write string of 2 characters . Result : Hi Instruction : Write string of 5 characters . Result : Hello Instruction : Write string of 7 characters . Result : abcdefg Instruction : Write string of 4 characters . Result : Cars Instruction : Write string of 10 characters . classifier Knowing this , please follow the next instruction . Instruction : Write string of {} characters . Strictly write the string without quotes and do not write anything else. \"\"\" Generate number of characters Template 0-shot \"Your answer was too { custom_str }. asked for string consisting of { desired_value } characters , and you produced one with { last_outputs } characters . Please try again. Strictly write the string without quotes and do not write anything else.\" Generate number of characters Feedback template \" Generate story that is {:.2f}% formal . Write the story and only the story.\" Generate formality Template 0-shot \"\"\" You are chatbot that follows instructions . will give you an example : Instruction : Generate story that is 100% formal . Result : Kind sir , please make your way to the terrace . Instruction : Generate story that is 75% formal . Result : She bought the book , then she went to the beach. Instruction : Generate story that is 50% formal . Result : Hey! Just went to the store. Instruction : Generate story that is 25% formal . Result : Wow Im so tired. Instruction : Generate story that is 0% formal . Result : hey man how it goin Knowing this , please follow the next instruction . Generate story that is {}% formal . Write the story and only the story. \"\"\" Generate formality Template 5-shot \"Your answer was too { formal_str }. asked for formality level of { desired_value :.2f}, and you gave sentence with formality {100 * last_outputs :.2f}. Please try again. Write the story and only the story.\" Generate formality Feedback template \" Generate sentence where the average word length is exactly {} letters . Strictly write the sentence without quotes and do not write anything else.\" Generate word length Template 0-shot \"\"\" You are chatbot that follows instructions . will give you an example : Instruction : Generate sentence where the average word length is exactly 4 letters . Result : The pig jumped above her. Instruction : Generate sentence where the average word length is exactly 3.7 letters . Result : She bought the book , then she went to the beach. Instruction : Generate sentence where the average word length is exactly 3.5 letters . Result : Hey! Just went to the store. Instruction : Generate sentence where the average word length is exactly 3 letters . Result : Wow Im so tired. Instruction : Generate sentence where the average word length is exactly 10 letters . Result : Stop redistribution ! Knowing this , please follow the next instruction . Generate sentence where the average word length is exactly {} letters . Strictly write the sentence without quotes and do not write anything else. \"\"\" Generate word length Template 5-shot \"Your answer was too { custom_str }. asked for string whose average word length is { desired_value :.2f} letters , and you produced one with average word length { last_outputs :.2f}. Please try again. Strictly write the sentence without quotes and do not write anything else.\" 35 Generate word length Feedback template Text-to-Image task templates \" White background . {} { object }.\" # object choices : COCO classes Generate objects Template 0-shot \" White background . { object } at the {pos} of the image.\" # object choices : COCO classes # pos choices : [\" top left\", \"top right\", \" bottom left\", \" bottom right\", \" center \"] Generate object at position Template 0-shot \"{:.2f}% saturation .\" Generate saturation Template 0-shot"
        },
        {
            "title": "K Controllability package",
            "content": "We open-source Python package to test controllability and estimate reachable sets. The package is completely built on PyTorch and supports loading of local or HuggingFace models to realize initial state or input distributions, as well as the readout map. An example usage is given below: import torch from controllability . systems . control_system import ControlSystem from controllability . verifiers . reachability import Reachability from controllability . verifiers . controllability import Controllability # Define initial states , input distributions initial_states_distribution = ... input_distribution = ... # Define control system dialogue_process = ControlSystem . from_model_name ( \" google /gemma -3-4b-it\", output_map =\" huggingface /text - classifier \", output_space =[[0 , 1]], input_distribution = input_distribution , # model_config = { do_sample =True , ...} ) # Reachability experiment time_horizon = 5 reachability_verifier = Reachability . from_problem_type ( \" quantized \", dialogue_process , input_distribution , p=0.05 , delta =0.05 ) reachable_set = reachability_verifier . get_reachable_set ( initial_state =\"Hello! \", time_horizon ) # Controllability experiment controllability_verifier = Controllability . from_reachability_problem ( reachability_verifier , epsilon =0.05 , alpha =0.1 , delta =0.05 ) controllable_set = controllability_verifier . get_controllable_set ( initial_states_distribution , 36 time_horizon ) Basic usage of our toolkit"
        },
        {
            "title": "L Use of AI Writing Assistance",
            "content": "We acknowledge the use of large language model (LLM) to assist in refining the language, grammar, and clarity of this manuscript. All content was originally drafted by the human authors, and all AI-generated suggestions were critically reviewed, edited, and approved by the authors, who retain full responsibility for the final text. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        }
    ],
    "affiliations": [
        "Apple",
        "Stanford",
        "Universitat Pompeu Fabra"
    ]
}