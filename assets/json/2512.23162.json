{
    "paper_title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "authors": [
        "Yufan He",
        "Pengfei Guo",
        "Mengya Xu",
        "Zhaoshuo Li",
        "Andriy Myronenko",
        "Dillan Imans",
        "Bingjie Liu",
        "Dongren Yang",
        "Mingxue Gu",
        "Yongnan Ji",
        "Yueming Jin",
        "Ren Zhao",
        "Baiyong Shen",
        "Daguang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 6 1 3 2 . 2 1 5 2 : r 2025-12SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Yufan He1, Dillan Imans3 Yueming Jin5 Pengfei Guo1, Bingjie Liu4 Ren Zhao6 Mengya Xu2, Dongren Yang4 Zhaoshuo Li1 Andriy Myronenko Mingxue Gu1 Yongnan Ji1 Baiyong Shen6 Daguang Xu 1 1NVIDIA 2The Chinese University of Hong Kong 5 National University of Singapore Equal First Author 6 Ruijin Hospital 3Sung Kyun Kwan University 4 Wenzhou Medical University Data scarcity remains fundamental barrier to achieving fully autonomous surgical robots. While largescale visionlanguageaction (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video-action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, world model designed for surgical physical AI. We curated the Surgical Action-Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. Its able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse-dynamics model to infer pseudo-kinematics from synthetic surgical videos, producing synthetic paired videoaction data. We demonstrate that surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on real surgical robot platform. Our approach offers scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data-efficient surgical robot policies. Figure 1 We curate SATA dataset with surgical videos and detailed text annotations for physical AI. powerful world model (SurgWorld) is built using Cosmos2.5 [1] and SATA, which is able to generate high quality, generalizable videos for surgical robots. We are also the first to illustrate the efficacy of surgical world modeling for autonomous surgical robots. 1. Introduction Autonomous robotic surgery promises to enhance precision, reduce surgeon fatigue, and scale complex procedures. Yet, despite extensive research, training robust robotic policies for surgical manipulation remains exceptionally challenging. major bottleneck is the lack of large, diverse datasets that include both high-fidelity visual observations (e.g., endoscopic video) and synchronized robot kinematics or control commands. Collecting such paired demonstrations is prohibitively expensive, constrained by operating room access, patient safety, and regulatory hurdles. In parallel, the robotics community has seen tremendous progress in large visionlanguageaction (VLA) models. Models such as RT-2 [2], OpenVLA [3], and GR00T [4] has demonstrated the potential of foundation models to generalize across diverse robotic manipulation tasks. These models leverage large-scale 2025 NVIDIA. All rights reserved. SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Figure 2 The overall workflow. The SurgWorld model is first pretrained with large scale surgical videos with text annotations, based on Cosmos 2.5 [1]. For downstream task with specific robot type and task, we finetune SurgWorld and train the inverse dynamic model (IDM) for the specific embodiment. In step 3 we generate synthetic video rollouts from SurgWorld and get pseudo kinematics from the IDM. We use both real data and synthetic data to train the surgical VLA model. multimodal datasets that couple visual observations, language descriptions, and robot actions, enabling rich world understanding and robust policy learning, yielding significantly improved generalization to novel objects and commands in non-surgical manipulation domains. Similarly, recent work on synthetic data generation such as DreamGen [5] shows that video world-models combined with inverse dynamics can produce synthetic paired videoaction datasets that boost policy learning in general manipulation tasks. However, extending such capabilities to surgical robotics remains significant challenge. Unlike household or industrial domains, surgical robotics suffers from severe data scarcity, particularly the lack of large-scale datasets with synchronized visual and kinematic information. Surgical data collection is constrained by privacy regulations, ethical considerations, and the cost of robotic surgical systems, limiting the ability to train data-hungry models such as VLAs and imitation learning (IL) policies. Synthetic physics-based simulators [6, 7] attempt to fill the gap, but often suffer from large visual and dynamic domain shift to real surgical systems and lacking soft body simulation, limiting policy transfer. To address these limitations, we propose SurgWorld, unified framework that leverages surgical world model to enable scalable policy learning. Specifically, we curate and annotate the Surgical ActionText Alignment (SATA) dataset with expert-labeled surgical video clips covering four core surgery actions with detailed spatial, interaction, and anatomical context. Using SATA, we train diffusion-based world model [1] capable of generating photorealistic, taskconsistent surgical scenes. We then employ an inverse dynamics model (IDM) to infer pseudo-kinematics, producing synthetic paired videoaction data that can be directly used for surgical VLA policy learning. We validate our approach on surgical robotic platform performing needle pick-up and hand-over tasksa fundamental and dexterous actions representative of real surgical manipulation. We use both real surgical demonstrations with kinematic supervision and synthetic videos labeled with pseudo-actions to train the GR00T N1.5 VLA model [4]. Our results demonstrate that incorporating synthetic world model data leads to substantial improvements in policy performance with lower trajectory prediction error. While prior works such as DreamGen [5] explored world-model-based learning from synthetic videos, these domains lack the unique visual and physical complexity of surgery, where specular tissue surfaces, endoscopic occlusion, and constrained tool motion present distinct modeling challenges. Recent efforts have begun addressing this gap within surgical contexts: GAS [8] applies world-model-based reinforcement learning for grasping, SurgWM [9] generates controllable surgical videos with dynamic prediction, and the Suturing World Model [10] forecasts tool trajectories for automated suturing. However, these approaches remain limited to narrow tasks or visual prediction without explicit integration of text grounding and kinematics. SurgWorld is the first to integrate large-scale text-aligned surgical video modeling with pseudo-kinematics generation for embodied policy learning and bridging the gap between unlabeled surgical videos and robot actions. Scalable data generation without collecting in-vivo trajectories can dramatically accelerate surgical autonomy while maintaining patient safety. Our framework addresses the core bottleneck of data scarcity in surgical robotics and opens scalable path toward autonomous surgical skill acquisition. 2 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling In summary, our key contributions are: 1. We curate the Surgical ActionText Alignment (SATA) dataset, large-scale surgical videotext corpus comprising 2,447 expert-annotated video clips (over 300k frames) that capture fine-grained spatial relationships and tooltissue interactions across 8 procedures, designed specifically to support the development of physical AI models. 2. We develop the first surgical world model thats based upon state-of-the-art physical AI world models, and finetuned with SATA, demonstrating strong generalizability, high video quality, and realistic dynamics. 3. We are the first to connect surgical world models with robot learning by synthesizing videoaction data using inverse dynamics models, achieving substantial performance improvements in surgical robot learning. Our results highlight the potential of generative world modeling to complement real surgical data and pave the way for foundation models that enable scalable, autonomous, and safe surgical policy learning. 2. Related Work VLA Models and Imitation Learning (IL). Largescale visionlanguageaction (VLA) models have recently emerged as powerful paradigm for generalpurpose robotic policy learning. Recent open-source efforts such as OpenVLA [3], ùúã0 [11], and GR00T N1 [4] have been trained on massive and diverse data sources, leveraging diverse robot embodiments and multi-task datasets and can robustly handle real-world variability. To train those VLAs, imitation learning (IL) remains the most direct and scalable approach, typically via behavior cloning (BC) [12]. However, BC suffers from covariate shift, producing rapidly worsening performance when the dataset is limited [13]. In robotics, several works aim to improve data efficiency using offline datasets [14, 15] or synthetic augmentation [16]. However, despite these advances, current VLA models still rely heavily on imitation learning on large scale paired imageaction datasets, which are scarce in specialized domains such as surgical robotics. Surgical World Models and Video generation. Learning compact models of the world has long been goal of model-based reinforcement learning (MBRL). World Models [17] demonstrated that compact latent dynamics models can simulate plausible trajectories for policy learning. Subsequent works such as PlaNet [18], Dreamer [19], and DreamerV3 [20] extended these ideas to high-dimensional visual environments, enabling agents to imagine future rollouts and train in latent space. In the surgical domain, video generation and world modeling are only beginning to emerge. Endora [21] integrates spatiotemporal transformer with latent diffusion backbone for endoscopic video synthesis. SurGen [22] proposes text-guided diffusion model tailored to laparoscopic cholecystectomy. VISAGE [23] formulates future surgical video prediction conditioned on single frame and an action scene graph, modeling tooltissue interactions for temporally coherent generation. GAS [8] applies world-model-based reinforcement learning for robust grasping across diverse objects. SurgWM [9] introduces controllable surgical video generation and interactive dynamics prediction from visual inputs alone. Suturing World Model [10] learns taskspecific dynamics to anticipate tool trajectories during automated suturing. Cosmos-Surg-dVRK [24] focuses on policy evaluation using action conditioned video generation model. While these studies mark important progress, they are limited to single-task or objectspecific scenarios, and rely on narrowly scoped datasets lacking high-quality textaction alignment or procedural diversity, and many are not open-sourced, limiting reproducibility and broader impact. In contrast, our approach leverages the curated SATA dataset to train surgical world model capable of generating photorealistic, task-consistent videos explicitly designed for physical AI and downstream robot policy learning. Learning Policy Models from Videos. Learning policy models directly from video has become increasingly important due to the lack of datasets with kinematics. Ye et allet@tokeneonedot [25] and Jang et allet@tokeneonedot [5] propose latent-action pretraining scheme using internet-scale videos without robot action labels to bootstrap vision-languageaction models. Hu et allet@tokeneonedot [26] train predictive visual representations via video diffusion models and embed an implicit inverse-dynamics policy conditioned on these representations. Bharadhwaj et allet@tokeneonedot [27] use human-video generation to condition robot policies on novel scenarios, reducing reliance on robot-collected data. Li et allet@tokeneonedot [28] present unified video-action latent model that jointly handles forward/inverse dynamics, video generation, and action inference within one framework. Tian et allet@tokeneonedot [29] close the loop by using inverse-dynamics models conditioned on predicted visual states for large-scale manipulation training. These developments directly inspire our approach, which uses IDM to connect world models with surgical robot learning. Automated Surgical Robotics. Automation in surgical robotics have increasingly leveraged learningbased approaches. Long et allet@tokeneonedot [30] introduced vision-based embodied intelligence framework that enables zero-shot sim-to-real transfer for 3 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling variety of laparoscopic assistive tasks using imitation learning, Kim et allet@tokeneonedot [31] proposed the Surgical Robot Transformer (SRT), which addresses the da Vinci robots inaccurate kinematics by formulating actions in hybrid-relative space, achieving high success rates on fundamental tasks. The same group later developed SRT-H [32] that enables step-level autonomy in complex procedures such as cholecystectomy. While these works demonstrate impressive progress in task autonomy, they also reveal critical need for large-scale, diverse surgical demonstration datasets, which in surgery is uniquely challenging: obtaining large-scale paired visualkinematic datasets requires specialized hardware and surgeon supervision. 3. Method The overall workflow is shown in Fig. 2. The surgical world model is obtained by finetuning Cosmos-Predict2.5 [1] model on surgical videos with detailed annotations. Then we demonstrate its efficacy in downstream surgical robot tasks. Since the world model has not seen specific surgical robot embodiments, we finetune the world model on robots and task specific data. Meanwhile we build an inverse dynamic model (IDM) for this specific robot. The world model generates video rollouts and IDM model label those videos with pseudo action kinematics. 3.1. Dataset Curation (SATA) Surgical ActionText Alignment Dataset. We introduce SATA, large-scale surgical actiontext alignment dataset comprising 2,447 expert-annotated video clips (over 300k frames) collected across 8 different surgery types. Each clip captures one of four fundamental actions: needle grasping (689), needle puncture (989), suture pulling (475), and knotting (294), which provides diverse visual and procedural coverage. SATA is curated by aggregating and re-annotating videos from credentialed YouTube surgical channels [33] and publicly available datasets, including GraSP [34], SAR-RARP50 [35], Multiypass140 [36], SurgicalActions160 [37], AutoLaparo [38], and HeiCo [39]. Unlike surgical VLM datasets, such as SurgVLM-DB [40], which focus primarily on semantic reasoning and instruction following, SATA is specifically designed for physical AI : its fine-grained action labels and detailed text descriptions capture precise tooltissue interactions and spatial relationships needed for training world models. The four action categories are defined by decomposing the suturing procedure into its fundamental steps and annotated according to the following criteria: Needle grasping: Approaching and securing the needle with smooth, controlled trajectory, emphasizing the dynamic go-to-grasp motion rather than the subsequent static hold. Needle puncture: Inserting the needle into tissue with precise control over its entry angle and depth. Suture pulling: Drawing the suture thread through tissue after puncture completion, typically by pulling on the needle or thread. Knotting: Looping and tightening the suture material to secure the tissue layers together. Each clip is paired with rich textual description detailing (i) spatial relationships between surgical instruments, (ii) the anatomical structure being manipulated, and (iii) the description of instrumenttissue interaction. For example: The left needle driver punctures the right side of the patients dorsal venous complex. Additional statistics and dataset breakdowns for SATA are provided in the supplementary materials. Real World Trajectories. For real-world validation, we aim to demonstrate that synthetic videos generated by the world model can enhance autonomous surgical robot policy learning. Due to the high cost and regulatory constraints of in-vivo experiments, we evaluate our method using the Needle Pickup and Hand-Over task on rubber pad. The experiments are conducted on commercial endoscopic surgical system (robot and manufacturer anonymized), which consists of stereo endoscope and two articulated robotic forceps (left and right arms). During the task, the left arm grasps the needle tip and hand it over to the right arm. We collected total of 60 successful humanteleoperated demonstrations for training and testing. Each episode includes synchronized endoscopic video (average length: 217 frames) and corresponding action kinematics. In addition to these task-specific demonstrations, we also utilized 66 out-of-domain episodes (around 60k action frames pairs) depicting general robot movements unrelated to the needle pickup task. These data are used to pretrain foundational inverse dynamics model (IDM) for the surgical robot, providing transferable motion understanding across tasks. The robot states are the same as the action kinematics with the same dimension and values. The action kinematics at each timestep is represented as 20-dimensional continuous vector: aùë° = [pùêø,rùêø,ùëîùêø,pùëÖ,rùëÖ,ùëîùëÖ], where each term encodes the motion of the left (L) and right (R) instruments relative to the endoscope frame. Specifically, pùêø = [ùë•ùêø,ùë¶ùêø,ùëßùêø] R3 represents the translational 4 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling video dynamics knowledge to the endoscopic surgical domain, reducing the amount of domain-specific data required while preserving temporal coherence and realism. To adapt the pretrained model to the surgical domain, we fine-tune it on the curated SATA dataset and real-world surgical trajectories described in previous section, which enables the model to capture the unique visual dynamics of robotic endoscopic scenes, such as instrumenttissue interaction, limited fieldof-view motion, and constrained articulation patterns. Unlike policy models, our model conditions only on the first observed frame ùêº0 and predicts future trajectories, capturing the temporal evolution of the surgical scene. We adopt Low-Rank Adaptation (LoRA) [42] to efficiently specialize Cosmos-Predict2.5 [1] for the surgical domain while preserving its general video modeling capabilities. LoRA modules are inserted into the transformers attention and feed-forward layers, enabling parameter-efficient finetuning with minimal forgetting. During adaptation, the model learns to predict future latent video frames conditioned on an initial observation and text prompt. Given an initial frame ùêº0, the world model produces rollout ^ùêº1 : ùëá = ùí≤ùúÉ(ùêº0), where ùí≤ùúÉ denotes Cosmos-Predict2.5 [1] augmented with LoRA adapters. spatiotemporal encoder extracts features from ùêº0, transformer-based latent dynamics module models temporal evolution, and decoder reconstructs the predicted frames. We adopt the Flow Matching (FM) formulation [43] to train the surgical video world model due to its conceptual simplicity and practical effectiveness. More training detail of SurgWorld can be found in the supplementary material. 3.3. Inverse Dynamic Models and Policy Models We follow the DreamGen IDM design [5, 44] and use GR00T N1.5 [4] as the policy model. The model architectures are shown in Fig. 3. These two models predict robotic actions with DIT [45] and flow matching heads [43]. The major difference is that IDMs inputs are two frames from the same video (T = 16 frames apart), and the model predicts the robot actions for every frame between these two input frames, while the GR00T policy model takes the current frame and text prompt, together with the robot state to predict the actions for future 16 frames. 4. Experiments 4.1. Surgical World Model Evaluation We first evaluate the proposed Surgical World Model on (i) video generation quality using the curated SATA dataset of internet surgical videos and (ii) 5 Figure 3 The model architecture for inverse dynamic models (IDM) and the vision language action foundation model (GR00T N1.5). They share similar architectures but IDM does not use text prompt nor robot state. offset (cartesian coordinates) of the left forcep tip with respect to the endoscope frame (represented by meters). rùêø = [ùëüùêø1,...,ùëüùêø6] R6 denotes the 6D rotation representation of the left instruments end-effector orientation. The 6D rotation formulation [41] is used to avoid discontinuities and ensure smooth interpolation in SO(3) by dropping the last column of the rotation matrix. ùëîùêø indicates the gripper jaw opening in radians. The same definitions apply for the right forcep with pùëÖ, rùëÖ, and ùëîùëÖ. This yields total of 20 control dimensions. All translation and rotation components are expressed relative to the endoscopes coordinate frame to ensure view-consistent control. 3.2. Surgical World Model In this work, we adopt Cosmos-Predict2.5 [1], large-scale video world model pretrained on diverse robotic and embodied datasets, as our base model and adapt it to the surgical domain. Cosmos-Predict2.5 [1] leverages diffusion-based latent video prediction with transformer backbones to simulate high-fidelity spatiotemporal dynamics. Its large-scale pretraining on heterogeneous robotic and human teleoperation videos provides strong priors for object interactions, tool motion, and scene dynamics, making it particularly well-suited for domains with limited labeled data, such as surgical robotics. By leveraging these pretrained representations, we can efficiently transfer general SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Table 1 Quantitative evaluation of surgical video generation on the curated SATA dataset. We report FVD and VBench metrics: dynamic degree (DD), imaging quality (IQ), and overall consistency (OC). Method Zero-shot Action-category SurgWorld FVD DD 26.9 175.4 26.5 143.0 62.4 106.5 IQ OC 18.0 48.7 18.1 49.0 21.5 49. erated from the same conditioning frame under four distinct textual prompts: one-time/two-time/threetime needle handover, and needle puncture. The multi-step handover cases are particularly noteworthy, because during data curation all multi-time handovers are decomposed into single handover segments. Therefore, the twoand three-time handover sequences represent novel compositions not explicitly observed during training. The model accurately follows each instruction, producing coherent sequences that reflect increasing motion complexity while preserving visual realism. These results demonstrate strong textvideo alignment and show that the SurgWorld can recombine learned primitives to generate anatomically plausible and temporally consistent surgical behaviors from prompt-level conditioning. Human Expert Evaluation. While quantitative metrics such as FVD and VBench capture perceptual and temporal quality, they fail to fully reflect the clinical realism required in surgical video generation. To bridge this gap, we conducted human evaluation study to assess the anatomical plausibility, instrument behavior, and semantic faithfulness of generated videos from surgical perspective. Three surgical experts (one surgeon supervising two residents) independently evaluated 50 video samples generated by different world model variants using three-level clinical quality rubric (scores from 1 to 3, higher is better) across the following criteria: TextVideo Alignment: Assesses whether the generated scenes match the textual prompt. Tool Consistency: Evaluates whether surgical instruments correspond to the prompt and remain physically consistent throughout the video sequence. Anatomical Structure: Measures the plausibility of tissue and organ appearance and reactions to interaction. detailed description of the rating rubric for each score level is in the supplementary materials. Figure 6 summarizes the averaged scores using radar plot. The SurgWorld achieves the highest ratings across 6 Figure 4 Qualitative comparison of three variants of Cosmos-Predict2.5 [1] on the SATA dataset. Red arrows highlight incorrect surgical tools or actions in the generated frames. few-shot adaptation to collected real trajectories. The goal is to assess both perceptual fidelity and transferability to downstream policy learning. Video Generation on SATA. We evaluate three variants of Cosmos-Predict2.5 [1]: (1) Zero-Shot, using the base model evaluated without any domain adaptation; (2) Action-Category, finetuned using coarse, category-level captions where all videos within the same action category share an identical prompt; and (3) SurgWorld, finetuned on SATAs finegrained, expert-curated textual descriptions capturing detailed tooltissue interactions and spatial relations. We report Fr√©chet Video Distance (FVD) [46] and follow [1] to report the three representative VBench [47] metrics. As shown in Table 1, SurgWorld trained with curated prompts achieves the lowest FVD and highest alignment scores, indicating substantial gains in perceptual realism and semantic coherence over zero-shot and coarse category-level prompt baselines. Qualitative results in Figure 4 further highlight these differences in challenging scenario where the initial frame contains no visible surgical tools. The Zero-Shot model hallucinates an incorrect instrument due to limited domain priors, and the Action-Category model initiates wrong action (tissue puncture). In contrast, the SurgWorld correctly follows the textual instruction and completes the intended needle grasping motion with consistent behavior. New Behavior Generalization. To evaluate the models capacity to generalize to diverse and semantically consistent actions, we test its response to varying textual prompts describing distinct surgical behaviors. Figure 5 presents representative video rollouts genSurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Figure 5 New behavior generalization via strong textvideo alignment. Given the same conditioning frame, our surgical world model generates distinct video rollouts corresponding to four task prompts: (1) one-time needle handover, (2) two-time needle handover, (3) three-time needle handover, and (4) needle puncture. Table 2 Few-shot finetuning results on real surgical trajectories. We report task success rate (SR) and image quality metrics: FVD and VBench metrics. FT and PT indicate whether the model is finetuned on the 5 real trajectories or pretrained on the SATA dataset, respectively. Method Zero-shot Finetuned-Orig SurgWorld FT PT SR FVD DD 53.6 85.7 89.3 235.2 212.5 207. 0.0 51.8 73.2 IQ OC 20.1 70.3 21.1 72.0 22.4 73.3 Figure 6 Human expert evaluation of generated surgical videos. Radar plot summarizing expert ratings across three criteria using 13 quality scale. all dimensions, reflecting superior text grounding, consistent instrument manipulation, and anatomically realistic tissue behavior. In contrast, the Zero-shot and Action-category variants exhibit weaker temporal coherence and less stable tooltissue interaction, underscoring the importance of prompt-level curation for clinically faithful video generation. Few-Shot Adaptation. We further evaluate the few-shot finetuning capability of the proposed surgical world model using only 5 real-world trajectories from the Needle Pick-Up and Hand-Over task (Section 3.1). Three configurations are compared: (1) Zero-Shot, using the original model directly; (2) Finetuned-Orig, where the model is finetuned from the original Cosmos-Predict2.5 [1] checkpoint; and (3) SurgWorld, where the model is first pretrained on the curated SATA dataset and then finetuned on the 5 trajectories. For evaluation, we generate videos from 56 hold-out initial frames selected from the 66 out-of-domain episodes described in Section 3.1. These frames are chosen based on proper initial needle and forceps configurations to ensure physically meaningful task initialization and comparable scene context. We report success rate (SR) and the same suite of video quality metrics as in Table 1. To assess the success rate, surgical experts evaluate the completeness of the trajectories in the generated videos. 7 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling episodes together with varying 5, 10, 20 (thus three separate IDMs) training episodes. We started from the pretrained Franka IDM checkpoint from DreamGen [5] and finetuned the model for 10k steps with learning rate of 1e-4. The IDM is then used to generate pseudo-labeled kinematics for the synthetic videos. Robot Policy Results. For all the experiments, we start from the pretrained GR00T N1.5 checkpoint which is strong starting point. Our base policy (Real Only) is the model finetuned only with 5, 10, 20 real training data with learning rate 1e-4, 200 steps. For 56 and 560 synthetic data (Real+Synthetic, Real+Synthetic 10x), we first finetune with learning rate 1e-4 for 400 steps, then we further finetune with 5, 10, 20 real training data with learning rate 1e-4 and 200 steps. We test these 6 policy models on the 40 held out test episodes. An example of left arm cartesian trajectory (first 3 digits of the models 20 dimension output) is shown in Fig. 7. The trajectory (left arm movement) of Real + Syn 10x showed closer similarity to the groundtruth. We calculate the mean square error (MSE) for all 20 dimension action prediction compared with groundtruth and averaged across all test data. The result is shown in Fig. 8. We separate the cartesian, rotation and jaw since they have different physical meaning. We can see that the average MSE is the largest for the VLA finetuned only with real data, while with synthetic videos the average MSE gets lower. The trend holds for both varying real trajectory number and varying synthetic video number. Similar trend can be observed for varying data finetuning hyperparameters and for VLAs other then GR00T model (e.g. ùúã0.5 [48]) , as shown in the supplementary. 5. Conclusion We present SurgWorld, the first surgical world model that connects high-quality synthetic surgical videos generation with robot action learning. By curating the SATA dataset and integrating inverse dynamics modeling, SurgWorld generates synthetic data that can improve downstream policy training, enabling scalable and safe surgical robot data curation. However, SurgWorld still has major limitations. It requires finetuning datasets from unseen robot embodiments for both world model and IDM, which require additional data curation efforts. Meanwhile, pseudo-kinematics from IDM still lack ground-truth precision and may introduce residual noise. Lastly, the current SATA dataset, though diverse and finely annotated for physical AI, is not covering all the publicly available datasets. Future work will focus on extending SATA with more complex and broader procedures and improving IDM for better pseudo-kinematic. 8 Figure 7 An example of left arm cartesian trajectory (first 3-dim of action space). Comparing Real only (Blue), Real + Syn 10x (Green), and groundtruth (Red). As shown in Table 2, SurgWorld achieves the best overall performance, with 73.2% success rate and the lowest FVD among all variants. Compared to direct finetuning from the original Cosmos-2.5 checkpoint, SATA pretraining yields consistently better video quality metrics, indicating improved temporal stability and perceptual fidelity. These results demonstrate that large-scale surgical video pretraining substantially enhances the models ability to adapt from limited real-world data, enabling robust and data-efficient generation for downstream surgical policy learning. 4.2. Robotic Policy Experiments For the 60 human teleoperated data from surgical robots, we split the last 40 as held out test set. For the rest of the data we perform experiments by gradually increasing the training data number. We use 5, 10, and 20 data for finetuning the world model, IDM, and the GR00T policy. All videos are resized to 224224 and augmented with color jitter, and the jaw opening and cartesian actions are normalized with min-max normalization. World Model Rollouts. We finetune SurgWorld using 5, 10, and 20 real-world videos and each finetuned model is used to generate synthetic videos conditioned on the initial frames from the out-of-domain episodes. In total, two sets of 56 single-rollout (1) and 560 multi-rollout (10, generated with 10 random seeds) synthetic videos are generated for each data regime. These two sets are used to show if number of synthetic data matters. IDM Training and Pseudo Action Labeling. For IDM model training, we use the out-of-the-domain SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Figure 8 Trajectory MSE (Standard deviation) on 40 test data. We use 5, 10, 20 real data to finetune the policy, starting from GR00TN1.5 pretrained checkpoint (Real), and checkpoints pretrained from 56 (Real + Synthetic) and 560 (Real + Synthetic 10x) synthetic data."
        },
        {
            "title": "References",
            "content": "[1] Arslan Ali, Bai Junjie, Bala Maciej, Balaji Yogesh, and et al. World simulation with video foundation models for physical ai, 2025. [2] Anthony Brohan, Noah Brown, Julio Carbajal, Yevgen Chebotar, Danny Driess, Chelsea Finn, Chen Fu, Brian Ichter, Alex Irpan, Ryan Julian, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [3] Haotian Liu, Andy Zeng, Brian Ichter, Anthony Brohan, and Lerrel Pinto. Openvla: An open-source vision-language-action model for general-purpose robots. arXiv preprint arXiv:2409.10124, 2024. [4] Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [5] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through video world models. arXiv preprint arXiv:2505.12705, 2025. [6] Fei Kong, Lixuan Wang, Xingtong Li, Yuan Gao, and Qi Dou. Surgicalgym: high-fidelity simulation platform for surgical robot learning. arXiv preprint arXiv:2310.12345, 2023. [7] Tianyi Zhang, Yiming Yang, Xingtong Li, Qi Dou, and Russell Taylor. Surrol: An open-source reinforcement learning framework for surgical robot learning. In International Conference on Robotics and Automation (ICRA), 2023. [8] Hongbin Lin, Bin Li, Chun Wai Wong, Juan Rojas, Xiangyu Chu, and Kwok Wai Samuel Au. World models for general surgical grasping. arXiv preprint arXiv:2405.17940, 2024. [9] Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra PK Poudel, and Binod Bhattarai. Surgical vision world model. In MICCAI Workshop on Data Engineering in Medical Imaging, pages 110. Springer, 2025. [10] Mehmet Kerem Turkcan, Mattia Ballo, Filippo Filicori, and Zoran Kostic. Towards suturing world models: Learning predictive models for robotic surgical tasks. arXiv preprint arXiv:2503.12531, 2025. [11] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. ùúã0: vision-language-action flow model for general robot control. arXiv preprint arXiv.2410.24164, 2024. [12] Dean A. Pomerleau. Alvinn: An autonomous land vehicle in neural network. In Advances in Neural Information Processing Systems (NeurIPS), 1989. [13] St√©phane Ross, Geoff Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. [14] Dzmitry Bahdanau, Kelvin Xu, and Joelle Pineau. Offline sequence learning with policy gradient. arXiv preprint arXiv:1906.03113, 2019. [15] Aviral Kumar, Justin Fu, Michael Soh, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 9 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling [16] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Sergey Levine, Gaurav Sukhatme, and Pieter Abbeel. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via In randomized-to-canonical adaptation networks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [17] David Ha and J√ºrgen Schmidhuber. World models. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [18] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning (ICML), 2019. [19] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Dream to control: Learning behaviors by latent imagination. International Conference on Learning Representations (ICLR), 2020. [20] Danijar Hafner, Justas Pasukonis, Jimmy Ba, and Mohammad Norouzi. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [21] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, and Yixuan Yuan. Endora: Video generation models as endoscopy simulators. In International conference on medical image computing and computer-assisted intervention, pages 230240. Springer, 2024. [22] Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Dhamanpreet Kaur, Rohan Shad, and William Hiesinger. Surgen: Text-guided diffusion model for surgical video generation. arXiv preprint arXiv:2408.14028, 2024. [23] Yousef Yeganeh, Rachmadio Lazuardi, Amir Shamseddin, Emine Dari, Yash Thirani, Nassir Navab, and Azade Farshad. Visage: Video synthesis using action graphs for surgery. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 146156. Springer, 2024. [24] Lukas Zbinden, Nigel Nelson, Juo-Tung Chen, Xinhao Chen, Ji Woong, Mahdi Azizian, Axel Krieger, Sean Huver, et al. Cosmos-surg-dvrk: World foundation model-based automated online evaluation of surgical robot policy learning. arXiv preprint arXiv:2510.16240, 2025. [25] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. [26] Cheng Hu et al. Video prediction policy: generalist robot policy with predictive visual representations. In International Conference on Machine Learning (ICML), 2025. [27] Hrishikesh Bharadhwaj et al. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. In Conference on Robot Learning (CoRL), 2025. [28] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. [29] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. arXiv preprint arXiv:2412.15109, 2024. [30] Yonghao Long, Anran Lin, Derek Hang Chun Kwok, Lin Zhang, Zhenya Yang, Kejian Shi, Lei Song, Jiawei Fu, Hongbin Lin, Wang Wei, et al. Surgical embodied intelligence for generalized task autonomy Science in laparoscopic robot-assisted surgery. Robotics, 10(104):eadt3093, 2025. [31] Ji Woong Kim, Tony Zhao, Samuel Schmidgall, Anton Deguet, Marin Kobilarov, Chelsea Finn, and Axel Krieger. Surgical robot transformer (srt): Imitation learning for surgical tasks. arXiv preprint arXiv:2407.12998, 2024. [32] Ji Woong Kim, Juo-Tung Chen, Pascal Hansen, Lucy Xiaoyang Shi, Antony Goldenberg, Samuel Schmidgall, Paul Maria Scheikl, Anton Deguet, Brandon White, De Ru Tsai, et al. Srt-h: hierarchical framework for autonomous surgery via language-conditioned imitation learning. Science robotics, 10(104):eadt5254, 2025. [33] Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, and Axel Krieger. Generalpurpose foundation models for increased autonomy in robot-assisted surgery. Nature Machine Intelligence, 6(11):12751283, 2024. [34] Nicol√°s Ayobi, Santiago Rodr√≠guez, Alejandra P√©rez, Isabela Hern√°ndez, Nicol√°s Aparicio, Eug√©nie Dessevres, Sebasti√°n Pe√±a, Jessica Santander, Juan Ignacio Caicedo, Nicol√°s Fern√°ndez, et al. Pixel-wise recognition for holistic surgical scene understanding. arXiv preprint arXiv:2401.11174, 2024. [35] Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam, Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai Wang, Yang Liu, et al. Sar-rarp50: Segmentation of surgical instrumentation and action recognition on robot-assisted radical prostatectomy challenge. arXiv preprint arXiv:2401.00496, 2023. [36] Sanat Ramesh, Diego DallAlba, Cristians Gonzalez, Tong Yu, Pietro Mascagni, Didier Mutter, Jacques Marescaux, Paolo Fiorini, and Nicolas Padoy. Weakly supervised temporal convolutional networks for fine-grained surgical activity recognition. IEEE Transactions on Medical Imaging, 42(9):25922602, 2023. 10 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling [37] Klaus Schoeffmann, Heinrich Husslein, Sabrina Kletz, Stefan Petscharnig, Bernd Muenzer, and Christian Beecks. Video retrieval in laparoscopic video recordings with dynamic content descriptors. Multimedia Tools and Applications, 77(13):1681316832, 2018. [38] Ziyi Wang, Bo Lu, Yonghao Long, Fangxun Zhong, Tak-Hong Cheung, Qi Dou, and Yunhui Liu. Autolaparo: new dataset of integrated multi-tasks for image-guided surgical automation in laparoscopic hysterectomy. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 486496. Springer, 2022. [39] Lena Maier-Hein, Martin Wagner, Tobias Ross, Annika Reinke, Sebastian Bodenstedt, Peter Full, Hellena Hempe, Diana Mindroc-Filimon, Patrick Scholz, Thuy Nuong Tran, et al. Heidelberg colorectal data set for surgical data science in the sensor operating room. Scientific data, 8(1):101, 2021. [40] Zhitao Zeng, Zhu Zhuo, Xiaojun Jia, Erli Zhang, Junde Wu, Jiaan Zhang, Yuxuan Wang, Chang Han Low, Jian Jiang, Zilong Zheng, et al. Surgvlm: large vision-language model and systematic evaluation benchmark for surgical intelligence. arXiv preprint arXiv:2506.02555, 2025. [41] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57455753, 2019. [42] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [44] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [45] William Peebles and Saining Xie. Scalable diffusion In Proceedings of the models with transformers. IEEE/CVF international conference on computer vision, pages 41954205, 2023. [46] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Rapha√´l Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. arXiv preprint arXiv:1812.01717, 2019. [47] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [48] Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. ùúã0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Human Expert Evaluation Criteria Human expert evaluation criteria. To assess perceptual realism and clinical fidelity, we conducted human expert study with 3 surgical experts (one surgeon guides two residents to rate generated videos). Each expert independently evaluated 50 videos generated by different world model variants using three-level clinical quality rubric (scores from 1 to 3, higher is better) across the following criteria: TextVideo Alignment: Assesses whether the generated scenes match the textual prompt and surgical viewing perspective. 1: Scene roughly aligns with the prompt but exhibits camera mismatch or unrealistic perspective. 2: Scene alignment is correct and transitions are mostly natural with minor motion jumps or deformations. 3: Scene fully aligns with prompt and surgical viewpoint; transitions are smooth and visually coherent. Tool Consistency: Evaluates whether surgical instruments correspond to the prompt and remain physically consistent throughout the sequence. 1: Tool type matches the text but exhibits frequent deformation or discontinuity. 2: Tools remain largely consistent with reduced deformation; grasping and manipulation appear mostly realistic. 3: Tools behave continuously and naturally, accurately performing realistic grasping and needle handling without visual artifacts. Anatomical Structure: Measures the plausibility of tissue and organ appearance and their reactions to tool interaction. Figure 9 The illustration of the needle pick up and hand over task for the specific surgical robot platform. 1: Basic anatomical layout correct but unrealistic structure or misaligned tissue response. reproduction with"
        },
        {
            "title": "Improved detail",
            "content": "2: occasional deformation inconsistencies. 3: Anatomically accurate structures with realistic responses (e.g., traction, deformation, bleeding) closely resembling real surgery. 7. SurgWorld Training We adopt the Flow Matching (FM) formulation [43] to train the surgical video world model due to its conceptual simplicity and practical effectiveness. FM defines velocity-based target in latent space, providing direct training signal that improves optimization stability and sample quality. Formally, given data sample ùêº (e.glet@tokeneonedot, video frames), noise vector ùúñ ùí© (0, ùêº), and timestep ùë° [0,1] sampled from logit-normal distribution, the interpolated latent is defined as: ùêºùë° = (1ùë°)ùêº +ùë°ùúñ, where the corresponding ground-truth velocity is ùë£ùë° = ùúñùêº. (1) (2) The model predicts this velocity via network ùë¢ùúÉ(ùêºùë°,ùë°,ùëê), where ùëê represents the conditioning frame ùêº0 and corresponding text prompt, and parameters ùúÉ correspond to the trainable LoRA adapters. The flow-matching loss is then the mean squared error (MSE) between predicted and ground-truth velocities: ‚Ñí(ùúÉ) = Eùêº,ùúñ,ùëê,ùë° ùë¢ùúÉ(ùêºùë°,ùë°,ùëê)ùë£ùë° 2 2. (3) 8. Additional Results 8.1. More Visual Comparisons To further illustrate the effect of few-shot adaptation on real surgical scenes, Figure 10 presents qualitative comparisons across three model configurations, including Zero-Shot, Finetuned-Orig, and SurgWorld, evaluated under two representative initial conditions. Each rollout begins from distinct endoscopic view with different needle and forceps arrangements, enabling controlled examination of how well each model handles realistic surgical complexity. The Zero-Shot model, which has no domain adaptation, frequently produces implausible motions and incorrect surgical tools, leading to early task failure. The Finetuned-Orig model reduces some hallucinations but still struggles with consistent grasping behavior and coherent tool interaction. In contrast, the SurgWorld model (SATA-pretrained then finetuned on 1 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Figure 10 Qualitative comparison of few-shot finetuning variants in real-world surgical scenes. We visualize predicted video rollouts from three model configurationsZero-Shot, Finetuned-Orig, and SurgWorldunder two representative initial states (Case 1 and Case 2). Red arrows highlight incorrect surgical tools or hallucinations in the generated frames. five real trajectories) generates smooth and accurate motions that correctly execute the intended manipulation sequence. Red arrows mark common failure modes, including tool hallucination and incorrect action execution, emphasizing the improvement brought by domain-specific pretraining. These visual results underscore the importance of the proposed SATApretrained SurgWorld model for reliable few-shot generalization in real-world surgical environments. 8.2. Additional Policy Results In this section we provide additional policy evaluation results. The task of Needle pick up and handover\" is illustrated in Fig. 9. Multi-view Robotic Policy Internet video datasets typically contain only single-view endoscopic videos, and obtaining multi-view surgical videos for training multi-view world model is currently not feasible. As result, SurgeWorld generates only single-view synthetic videos. However, downstream surgical robots may use multiple cameras for multi-view policy learning. Here, we show that even when real surgical data includes multiple viewssuch as additional left and right wrist camerasthe single-view synthetic videokinematic pairs can still improve multi-view VLA policy performance. We use the exact same training scheme and the same synthetic data as in the main paper, but we train on the 5, 10, and 20 real demonstrations while including the two additional wrist cameras, resulting in data with three views. Note that GR00T N1.5 VLA can process varying numbers of input views with the same weights. The results are shown in Fig. . 11. Varying Hyperparameters We also performed experiments varying the training steps. We repeated the GR00T policy training with varying 1k and 10k finetuning steps on the real data as shown in Fig. 12 and Fig. 13. Other VLA ùúã0.5 To validate if SurgWorld and IDM can improve other VLAs, we applied the same strategy to another recent foundational VLA model ùúã0.5 [48], which showed strong open world generalization. We simply change the GR00T policy to ùúã0.5 and repeated the experiments as shown in Fig. 14. SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Table 3 Source distribution of SATA video clips across four fundamental surgical actions. Dataset AutoLaparo [38] GraSP [34] HeiCo [39] Multiypass140 [36] SAR-RARP50 [35] SurgicalActions160 [37] YouTube [33] Knotting Needle Grasping Needle Puncture 47 37 16 1 118 7 68 9 3 677 42 1 6 940 Suture Pulling 30 28 4 413 Figure 11 Multi-View real data finetuning. Trajectory MSE (Standard deviation) on 40 test data. We use 5, 10, 20 real data with 3-View camera input to finetune the policy, starting from GR00T N1.5 pretrained checkpoint (Real), and checkpoints pretrained from 56 (Real + Synthetic) and 560 (Real + Synthetic 10x) synthetic data Single View. Figure 12 1k step finetuning: Trajectory MSE (Standard deviation) on 40 test data. We use 5, 10, 20 real data to finetune the policy for 1k steps, starting from GR00T N1.5 pretrained checkpoint (Real), and checkpoints pretrained from 56 (Real + Synthetic) and 560 (Real + Synthetic 10x) synthetic data. 3 SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling Figure 13 10k step finetuning. Trajectory MSE (Standard deviation) on 40 test data. We use 5, 10, 20 real data to finetune the policy for 10k steps, starting from GR00T N1.5 pretrained checkpoint (Real), and checkpoints pretrained from 56 (Real + Synthetic) and 560 (Real + Synthetic 10x) synthetic data. Figure 14 ùúã0.5 results. Trajectory MSE (Standard deviation) on 40 test data. We use 5, 10, 20 real data to finetune the policy, starting from GR00T N1.5 pretrained checkpoint (Real), and checkpoints pretrained from 56 (Real + Synthetic) and 560 (Real + Synthetic 10x) synthetic data."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National University of Singapore",
        "Ruijin Hospital",
        "Sung Kyun Kwan University",
        "The Chinese University of Hong Kong",
        "Wenzhou Medical University"
    ]
}