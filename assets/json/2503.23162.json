{
    "paper_title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations",
    "authors": [
        "Zhenyu Tang",
        "Chaoran Feng",
        "Xinhua Cheng",
        "Wangbo Yu",
        "Junwu Zhang",
        "Yuan Liu",
        "Xiaoxiao Long",
        "Wenping Wang",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 6 1 3 2 . 3 0 5 2 : r NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations Zhenyu Tang1*, Chaoran Feng1*, Xinhua Cheng1,2, Wangbo Yu1, Junwu Zhang1, Yuan Liu2, Xiaoxiao Long2, Wenping Wang3, Li Yuan1 1Peking University 2Hong Kong University of Science and Technology 3Texas A&M University Figure 1. NeuralGS directly compresses original 3DGS with neural fields into compact and rendering-efficient representation. NeRFbased methods like Mip-NeRF 360 [3] typically require minimal storage with slow rendering speeds while 3D Gaussian Splatting [16] (3DGS) methods achieve fast rendering but demand hundreds of megabytes storage. NeuralGS combines the compact neural fields with 3DGS by encoding 3D Gaussian attributes with neural fields, achieving significant reduction in model size and real-time rendering speed."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and complex encoding and quantization strategy. In this paper, we aim to develop simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring small storage size even for large-scale scene. To achieve this, we adopt clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving 45 average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields. The code and visual demos will be released soon. 1 *These authors contributed equally to this work. Corresponding author. 1https://pku-yuangroup.github.io/NeuralGS/ 1. Introduction Novel view synthesis (NVS) is fundamental task in 3D vision, with substantial applications across fields such as virtual reality [5], augmented reality [48], and media generation [30, 32, 39, 45]. This task aims to generate photorealistic images of 3D scenes from novel views, given limited multi-view data. Neural radiance field (NeRF) [25] has already gained significant attention as 3D scene representation for its compact structure and exceptional capability to reconstruct large-scale scenes [2, 3, 10, 29, 38]. However, persistent challenge hindering the widespread adoption of NeRF lies in the computational bottlenecks imposed by volumetric rendering [6], which limit the utilization in real scenes that require fast rendering speeds. 3D Gaussian Splatting (3DGS) [16] has emerged as an alternative representation, utilizing point-based representation associated with 3D Gaussian attributes. Unlike the slow volume rendering of NeRFs, 3DGS utilizes fast differentiable splatting technique, achieving exceptionally fast rendering speeds and promising image quality. However, employing point-based representations inherently leads to substantial storage demands, as millions of points and their attributes are stored independently, which significantly hinders the compactness of 3DGS as scene representation. To address the above size issue, some 3DGS compression methods [9, 28, 42] mainly adopt pruning and quantization on Gaussian. While recent SoTA 3DGS compression methods, like HAC [4] and ContextGS [40], achieve impressive compression rate based on Scaffold-GS [24] which adopts anchors to predict local Gaussians by neural networks. These methods mainly focus on compressing these anchors by complex encoding and quantization strategies, which demonstrate much higher compression rate than the original 3DGS-based compression methods. However, these methods cannot be applied on the original 3DGS due to the absence of additional anchor structures, while the main focus of the current research community is still on the original 3DGS without additional anchors. This still remains an open question about how to directly and effectively compress the original 3DGS without any anchors. In this paper, we explore an orthogonal direction to ScaffoldGS-based methods by directly compressing the original 3DGS. Our method is based on the observation that neural fields like NeRF are able to represent complex scenes with small sizes. Thus, rather than proposing complex quantization like previous original 3DGS compression methods, our target is to adopt the neural fields in the compression of the original 3DGS. Adopting neural fields in compression is not trivial. straightforward solution is to directly employ multi-layer perceptron (MLP) to map the positions of Gaussians to their attributes, which could represent these attributes with compact neural field. However, only fitting single MLP to represent Gaussian attributes leads to large fitting errors, severely degenerating the rendering quality, because the Gaussians show strong spatial variations. Even nearby 3D Gaussians have totally different attributes, resulting in significant difficulty in fitting with single MLP. To address the aforementioned issues, we propose NeuralGS, novel framework designed for the post-training compression of original 3DGS, which bridges 3D Gaussian splatting and neural radiance field for compact and efficient 3D scene representation. We adopt three strategies to facilitate the effective encoding of 3D Gaussian attributes with neural fields as follows: First, instead of fitting all attributes of all Gaussians equally, we compute the importance of each Gaussian according to their contributions to the renderings. Gaussians with low importance are first pruned to reduce the Gaussian numbers. Furthermore, the importance of Gaussians acts as weights in the fitting process, which ensures that important Gaussians are fitted with high accuracy. Second, an important observation from us is that the attributes of Gaussians do not change smoothly with their positions. For example, Gaussian with small scale factor could have neighboring Gaussian with an extremely large scale factor, which prevents the neural fields from accurately fitting them due to the smoothness nature of neural fields. To reduce attribute variability among Gaussians, we cluster 3D Gaussians based on their attributes to preserve similarity among Gaussians within the same cluster. For different clusters, we use different tiny neural fields (MLPs) to map the positions of Gaussians to the remaining attributes, which significantly reduces the fitting errors and improves the compactness of 3D representation. Third, we further fine-tune the learned NeuralGS representation with training images and propose frequency loss to improve the reconstruction quality. We find that the MLPs often have difficulty in learning the high-frequency signals of Gaussian attributes. Thus, we incorporate frequency loss, that puts emphasis on the high-frequency details of renderings, along with the original rendering loss in the fine-tuning process to recover fine details. In the end, our NeuralGS only needs to store the positions of important Gaussians and the weights of the corresponding tiny MLPs for all clusters, substantially reducing storage requirements compared to the original 3DGS. NeuralGS achieves about 45 and 52 model size reduction on the Mip-NeRF360 dataset [3] and DeepBlending dataset [14], respectively, which outperforms all existing original 3DGS-based methods [9, 12, 26, 28, 42]. Meanwhile, as an original 3DGS-based method, NeuralGS for the first time achieves comparable compression rate to those Scaffold-GS [24]-based compression methods [4, 40]. Figure 2. The detailed architecture of our proposed NeuralGS. (A) In Sec. 3.2, for each Gaussian GSj in the scene, we first calculate its global importance score Sj (Eq.1) and prune unimportant Gaussians. (B) In Sec. 3.3, we cluster the pruned Gaussians and use different tiny MLPs to map the positions to Gaussian attributes of different clusters with the loss (Eq.3) using the importance score as weights. (C) In Sec. 3.4, we fine-tune the tiny MLPs of all clusters with photorealistic loss (Eq.4) and frequency loss (Eq.5) to restore quality. 2. Related Works 2.1. Novel View Synthesis Neural radiance field (NeRF) [25] proposes to use MLPs to represent scene, and this compact representation has brought view synthesis quality to new stage. However, NeRF-based methods [3, 13, 15, 19, 27, 29, 33] struggle to achieve real-time rendering speed in large-scale scenes, limiting their practical use. The idea of utilizing multiple MLPs is also explored by KiloNeRF [33] for efficient rendering. Recently, 3D Gaussian Splatting (3DGS) [16] and its variants [20, 22, 24, 31, 34, 35, 37, 43, 44, 46], offer state-of-the-art scene reconstruction by utilizing set of optimized 3D Gaussians that can be rendered efficiently. 2.2. Compression of 3D Gaussian Splatting Although 3DGS achieves superior performance and high rendering speed compared to NeRF-based methods, it typically requires hundreds of megabytes to store 3D Gaussian attributes, posing challenges for its practical application in large-scale scenes. Several existing works [1, 8, 9, 18, 28, 42] have made initial attempts to compress 3DGS models, primarily using pruning to reduce the number of 3D Gaussians, vector quantization to discretize Gaussian attributes into shared codebooks, and context-aware entropy encoding. Specifically, Lee et al. [18] introduced novel volumebased masking strategy that effectively reduces the number of Gaussians without impacting performance. CompressGS [28] employs the sensitivity to compress both color and Gaussian parameters into compact codebooks while utilizing entropy coding to minimize statistical redundancies in the codebooks. LightGS [9] reduces the number of Gaussians through pruning and effectively minimizes the size of color attributes using distillation mechanism. CompactGS [26] proposed 2D grid-based representation to compress the attributes. The compression works [4, 23, 40] are based on more advanced method ScaffoldGS [24] with smaller model size and superior rendering quality than the original 3DGS. In spite of the strong performances, ScaffoldGS and these compresssion works introduce complex anchor-based structure with less scalability. In contrast, we choose to compress original 3DGS which is simpler and more scalable and employ compact neural fields to encode Gaussian attributes with tiny MLPs. 3. Method 3.1. Overview General idea. The general idea of NeuralGS is to adopt compact neural fields to compress original 3DGS. Specifically, given 3D Gaussians reconstructed from multi-view images, we learn MLP networks to map the 3D positions of Gaussians to their attributes including opacities, spherical harmonic coefficients, scales, and rotations. These MLP networks can be regarded as set of neural attribute fields. In this case, we only need to store the positions of all 3D Gaussians and the MLP parameters, which are highly compact thanks to the compactness of neural field representaFigure 3. Details of Cluster-based Neural Field Fitting. The positions of the 3D Gaussians within each cluster are fed into the corresponding tiny MLP to fit the attributes with the importance weighted loss. During rendering, the predicted outputs are then split into the respective attributes of the Gaussians, i.e., rotation, scale, opacity, color, and SH coefficients. tions. When we need to render from NeuralGS, we only need to decode these attributes from these MLPs. Challenges and solutions. However, fitting the neural field is not trivial task because naively fitting compact MLP network on all Gaussian attributes leads to severe fitting errors and inferior rendering quality. To improve the fitting process, we propose three essential strategies, as shown in Fig. 2. First, not all Gaussians contribute equally to the final renderings and some of them are entirely redundant without any effects on the rendering quality. Thus, this motivates us to compute the importance of all 3D Gaussians in Sec. 3.2, which is used in pruning out useless Gaussians and in an importance-aware fitting process. Second, we find that the Gaussian attributes do not distribute evenly or smoothly in the 3D space, where small-scale Gaussian could have large-scale neighbor. This uneven distribution severely hinders the fitting process because the MLP networks naturally fit into smooth fields but have difficulty handling abrupt changes. Thus, in Sec. 3.3, we propose to first cluster the attributes of Gaussians and then fit neural field for each cluster instead of solely using single neural field for each Gaussian. Third, in Sec. 3.4, we further improve the rendering quality of NeuralGS by fine-tuning on input multi-view images with rendering loss and frequency loss. 3.2. Global Importance Importance Computation. Each Gaussian contributes differently to the final renderings in 3DGS [16]. To quantify this, we define global importance score for each Gaussian, representing its contribution to the rendering result. Inspired by [9, 42], the importance can be calculated based on each Gaussians contribution to every pixel pi across all training views. We use the criterion 1(GSj, pi) to determine whether Gaussian GSj overlaps with pixel pi after (cid:81)k1 projection onto the 2D plane. At last, we can iterate over all training pixels and sum up the accumulated opacity of GSj, denoted as αk l=1 (1 αl), to compute each Gaussians contribution to the rendering result. Here, is the index of the Gaussian GSj in the depth ordering for pixel pi and α is the opacity. This importance score can be further refined by incorporating the 3D Gaussians normalized volume Vnorm. Finally, the global importance score can be expressed as: Sj = HW (cid:88) i=1 1(GSj, pi) (Vnorm)β αk k1 (cid:89) (1 αl), (cid:18) Vnorm = min max l=1 (cid:19) (cid:19) , 0 , 1 . (cid:18) Vmax (1) (2) Here, S, , H, and represent the importance score, the number of training views, the image height, and the image width, respectively. Vmax90 denotes the 90% largest volume of all sorted Gaussians, and β is the hyperparameter to enhance the scores flexibility. Importance-based Pruning and Weighting. Thus, we rank each Gaussian based on its importance score, allowing us to prune Gaussians with lower contributions to the renderings, thereby reducing the total number of Gaussians. Additionally, the importance scores of the 3D Gaussians can be used as weights in the subsequent fitting process, ensuring that important Gaussians are fitted with higher accuracy. 3.3. Cluster-based Neural Field Fitting The original 3DGS does not ensure any attribute similarities between neighboring Gaussians. Two neighboring Gaussians could have totally different colors or scales, which poses challenges in the neural field fitting. To address this, we propose an attribute-based clustering strategy to ensure attribute similarity within the same cluster and fit separate neural fields for different clusters as shown in Fig. 3. Attribute-based Clustering. Specifically, we employ Kmeans [21] to cluster the 3D Gaussians into clusters, denoted as C1, C2, . . . , CK. In this case, the attributes of Gaussians in the same cluster will be similar and easy for neural field fitting. Given the significant distributional differences across attributes, we first normalize each attribute to the range [1, 1] by computing its maximum and minimum values to unify the scales of different attributes and avoid over-reliance on certain attributes during clustering. Neural Fields. After assigning each 3D Gaussian to cluster, we use different tiny MLPs for different clusters to map Gaussian positions within each cluster to the normalized attributes of these Gaussians. Each tiny MLP consists of five layers with positional encoding, followed by tanh activation function [7]. The fitting processes for different clusters are conducted in parallel for efficiency. Importance-Weighted Fitting Loss. We apply mean squared error (MSE) loss when fitting Gaussian attributes. Recognizing that each Gaussian contributes differently to the renderings, we use importance scores as fitting weights, which ensures that Gaussians with higher importance are fitted more accurately. Our loss function is defined as: Loss = 1 jP Sj (cid:80) (cid:88) jP Sj F(xj) ˆyj2 . (3) Here, represents the Gaussian index set of cluster, denotes the importance score, F() is the tiny MLP corresponding to the cluster, is the spatial position of the Gaussian, and ˆy is the normalized Gaussian attributes. 3.4. Fine-tuning After fitting, there still remain some residuals which degrade rendering quality. To address this, we incorporate fine-tuning stage to restore the image quality. In this process, we fix spatial positions of the 3D Gaussians and only fine-tune the tiny MLPs corresponding to each cluster. The photorealistic loss Lrender, is then computed by combining the mean absolute error (MAE) loss L1 and the SSIM loss LSSIM with the weight λ as follows: Lrender = (1 λ)L1 + λLSSIM. (4) Frequency Loss. We observe that the fitted attributes often lose high-frequency details, such as dense grass. Thus, we introduce frequency loss to emphasize these highfrequency details for faster convergence. Specifically, we use fourier transform to convert the rendered image and the ground truth Igt into frequency representations and Fgt. (u, v) consists of amplitude (cid:12) (cid:12) and phase (u, v), where (u, v) denotes the coordinates in the frequency spectrum. We then introduce high-pass filter with (cid:12)F (u, v)(cid:12) fixed bandwidth to extract high-frequency information, denoted as ˆF (u, v) and ˆFgt(u, v). We define (cid:12) (cid:12) = (cid:12) (cid:12) ˆF (u, v)(cid:12) (cid:12) and ˆF (u, v) = ˆF (u, v) ˆFgt(u, v). Thus, the frequency loss Lfreq and the total loss LTotal can be formulated as follows: (cid:12) ˆFgt(u, v)(cid:12) (cid:12) ˆF (u, v)(cid:12) (cid:12) (cid:12) H1 (cid:88) Lfreq = (cid:12) 1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆF (u, v)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + ˆF (u, v) u=0 v=0 LTotal = Lrender + λfreqLfreq. (cid:12) (cid:12) (cid:12) (cid:12) , (5) (6) Here, H, and λfreq denote the image height, width, and the hyperparameter to balance the loss. Model Parameters. In the end, we only need to store the positions of the pruned 3D Gaussians and the fine-tuned MLP weights for each cluster, significantly reducing the model size. Min-max values are shared across all clusters with only negligible 59 floating numbers (2e4MB) needed. 4. Experiments 4.1. Experimental Settings Evaluation Datasets and Metrics. We adopt four datasets (1) Mip-NeRF360 [3] offers scene-scale for comparison. data for view synthesis, containing nine real-world largescale scenes: five unbounded outdoor scenes and four indoor scenes with complex backgrounds. (2) Tank and Temple [17] is unbounded dataset that includes two scenes: train and truck. (3) Deep Blending [14] contains two indoor scenes: drjohnson and playroom. (4) NeRF Synthetic [25] contains eight small-scale objects. For all datasets, we maintain the same train-test splits as the official setting of 3DGS [16] and utilize PSNR, SSIM [41], LPIPS [47], and model size to evaluate image quality and compression ratio. Baselines. We use 3DGS [16] as our baseline method and compare with compression techniques [9, 12, 18, 26, 28, 42] based on original 3DGS. For qualitative and quantitative comparisons, we use the official code of each method with the default configurations for training and rendering. Additional comparison with Scaffold-GS-based methods is also included in Sec. 4.2.4 and the supplementary material. Implementation Details. We implement our NeuralGS based on the official codes of 3DGS [16] and conduct training on various scenes using NVIDIA A100 GPUs. During pruning, we remove 40% of the less important 3D Gaussians. For indoor scenes, we typically set the number of clusters between 40 and 80, while for outdoor scenes, is generally set between 100 and 140. Each cluster is assigned tiny MLP to fit the Gaussian attributes for 60k iterations. All MLPs used in our method are 5-layer MLPs with Tanh activation function and positional encoding. To restore rendering quality, we further fine-tuned the fitted MLPs for 25k iterations, with λ and λfreq set to 0.2 and 0.01, respecS 3 ] 6 1 [ h ] 9 [ . e L ] 8 1 [ L ] 2 1 [ a N ) O ( o t Figure 4. Qualitative results of the proposed method compared to 3DGS and existing compression methods. tively. Please refer to supplementary materials for more video results and specific implementation details. 4.2. Experimental Results 4.2.1. Quantitative Results. The quantitative evaluation results across different datasets are presented in Tables 1 and Table 2. Specifically, compared to the original 3DGS [16], NeuralGS achieves significant compression ratios while preserving rendering quality. Our method reduces the model size by approximately 45, 36 and 52 on the Mip-NeRF 360 dataset, Deep Blending dataset and Tanks&Templates dataset, respectively. Furthermore, our approach achieves the highest PSNR rendering metrics across all three datasets, outperforming existing compression methods [9, 12, 18, 26, 28, 42] based on the original 3DGS and even surpassing the original 3DGS by 0.49 dB on the Deep Blending dataset. These improvements are primarily attributed to the cluster-based neural fields, effectively facilitating the compactness of 3D representations. Table 2 presents the quantitative results on the NeRFSynthetic dataset. Consistent with our previous observations, our method significantly reduces model storage from Table 1. Quantitative results evaluated on Mip-NeRF 360 [3], Tanks&Temples [17], and Deep Blending [14] datasets. We highlight the best-performing results in red and the second-best results in yellow for all compression methods. Dataset Method Mip-NeRF 360 [3] Tanks&Temples [17] Deep Blending [14] PSNR SSIM LPIPS Storage PSNR SSIM LPIPS Storage PSNR SSIM LPIPS Storage Mip-NeRF 360 [3] CVPR 2022 3DGS [16] TOG 2023 CompressGS [28] CVPR 2024 Lee et al. [18] CVPR 2024 CompactGS [26] ECCV 2024 MesonGS [42] ECCV 2024 EAGLES [12] ECCV 2024 LightGS [9] NeurIPS 2024 27.69 27.48 26.98 27.01 25.95 27.08 27.18 26.93 0.795 0. 0.801 0.797 0.780 0.800 0.809 0.798 0.238 0.222 0.242 0.248 0.267 0.245 0.241 0.250 9.0 MB 755.5 MB 28.72 MB 48.80 MB 30.43 MB 27.51 MB 60.82 MB 48.71 MB 22.16 23. 23.32 23.29 22.68 23.31 23.27 22.92 0.757 0.844 0.830 0.829 0.813 0.836 0.839 0.817 0.261 0.178 0.194 0.202 0.221 0.195 0.211 0.242 9.0 MB 438.9 MB 17.73 MB 39.43 MB 18.70 MB 17.47 MB 31.05 MB 24.74 MB 29.01 29.42 29.40 29.71 28.90 29.40 29.78 27.11 0.895 0.900 0.899 0.900 0.891 0.903 0.907 0.872 0.255 0. 0.252 0.257 0.282 0.257 0.249 0.309 8.6 MB 672.8 MB 25.96 MB 43.21 MB 14.28 MB 25.64 MB 58.55 MB 33.45 MB NeuralGS (Ours) 27.35 0. 0.240 16.90 MB 23.63 0.841 0.192 12.06 MB 29.91 0.906 0.254 12.98 MB Table 2. Quantitative results of the proposed method evaluated on the NeRF-Synthetic [25] dataset. We highlight the best results in red and second-best results in yellow for compression methods. Table 3. Comparisons with ScaffoldGS-based Methods evaluated on the Mip-NeRF 360 dataset [3]. The best is in bold, while the second-best is underlined. Dataset Method NeRF Synthetic Dataset [25] PSNR SSIM LPIPS Storage Mip-NeRF 360 [3] 3DGS [16] CompressGS [28] Lee et al. [18] CompactGS [26] MesonGS [42] EAGLES [12] LightGS [9] 32.44 33.75 32.94 33.10 31.04 32.92 32.54 32.70 0.961 0.970 0.967 0.962 0.954 0.966 0.963 0. 0.048 0.031 0.033 0.038 0.050 0.037 0.039 0.040 4.62 MB 69.89 MB 3.82 MB 5.54 MB 2.20 MB 3.67 MB 5.78 MB 7.84 MB NeuralGS (Ours) 33. 0.967 0.036 1.62 MB 69.9 MB to 1.6 MB, achieving an impressive 43 compression ratio while maintaining rendering quality comparable to original 3DGS [16]. Moreover, compared to compression methods based on original 3DGS, our approach demonstrates substantial improvements in bitrate consumption. 4.2.2. Qualitative Results. Figure 4 presents qualitative comparison between our proposed NeuralGS and other compression methods [9, 12, 18, 26, 28] based on the original 3DGS, providing the specific details with zoomed-in views. By leveraging compact cluster-based neural fields to encode the Gaussian attributes, our method greatly retains rendering quality with clearer textures and sharper edges even using reduced model size. 4.2.3. Rendering Time As shown in Table 4, we compare the average storage size and rendering speed with the original 3DGS [16]. For rendering speed, we measure the frame rate or Frames Per Second (FPS) based on the total time taken to render all camera views in the dataset. Since we use multiple neural fields to encode Gaussian attributes, MLPs are used to decode the attributes of all 3D Gaussians before testing FPS, which constitutes one-time amortized cost for loading the attributes. From Table 4, it is observed that, due to the reduced number Method PSNR SSIM LPIPS Storage CompGS [23] ACM MM 2024 HAC [4] ECCV 2024 ContextGS [40] NeurIPS 2024 NeuralGS (Ours) 27.21 27.41 27.52 27.35 0.802 0.805 0.807 0.806 0.241 0.239 0.238 0.240 17.13MB 16.02MB 14.31MB 16.90MB Table 4. Performance comparison with 3DGS [16]. Rendering FPS and model size (MB) are reported. The rendering speed of both methods is measured on our machine. Dataset Method 3DGS [16] NeuralGS Mip-NeRF 360 Tanks&Temples Deep Blending FPS 112 135 Size 756 16.9(45 ) FPS 162 211 Size 439 12.1(36 ) FPS 118 137 Size 673 13.0(52 ) of 3D Gaussians by pruning, our method achieves higher rendering speed compared to 3DGS while requiring significantly less model size with compact neural fields. 4.2.4. Comparison with Scaffold-GS-based Methods We further provide additional comparison with ScaffoldGS-based compression methods [4, 23, 40]. Though these methods rely on the additional voxel structure to produce very high compression rate, our approach directly compresses the original 3DGS by using neural fields, achieving comparable compression rates and performance with these ScaffoldGS-based methods as shown in Table 3. 4.3. Ablation Studies In this subsection, we conduct ablation studies on the Deep Blending dataset to demonstrate the effectiveness of each improvement. Specifically, our core idea is to use global neural fields to encode Gaussian attributes, enhancing the compactness of 3D representation. Hence, our vanilla NeuralGS employs single tiny MLP to fit the Gaussian attributes of the entire scene, followed by basic fine-tuning to restore quality. As shown in Table 5, we incrementally Figure 5. NeuralGS allows progressive loading new clusters in the playroom scene to obtain more details and sharper texture. Table 5. Quantitative ablation study on the Deep Blending [14] dataset by progressively adding our proposed improvement. Dataset Method 3DGS [16] Deep Blending Dataset [14] PSNR SSIM LPIPS Storage 29.42 0.900 0.247 672.8 MB Vanilla NeuralGS + Cluster-based fitting + Importance weight + Frequency loss (Ours) 23.54 28.82 29.56 29. 0.795 0.891 0.903 0.906 0.523 0.294 0.269 0.254 7.32 MB 12.92 MB 12.96 MB 12.98 MB fitted by the neural fields with higher accuracy. As shown in Table 5 and Figure 6, adding importance scores as fitting weights, without introducing additional parameters, can further enhance quality and provide better textures. Effectiveness of Frequency Loss. During the fine-tuning stage, we observed that within limited number of training iterations, MLPs tend to be less sensitive to high-frequency details. As shown in the second row of Figure 6, incorporating the frequency loss helps transform the blurry edges of leaves to be sharper. The quantitative results in Table 5 further demonstrate the improvement in rendering quality by lastly introducing the frequency loss. We also provide results for only adding the frequency loss in the appendix. 4.4. JPEG-like Progressive Loading Benefiting from our usage of different neural fields to fit the Gaussians within different cluster, we can transmit and decode Gaussian attributes cluster by cluster in streamable manner like JPEG [36]. Specifically, we can sort clusters from the largest to the smallest based on the number of Gaussians and progressively transmit the positions along with the corresponding tiny MLP weights. During transmission, Gaussian attributes can be decoded simultaneously, as shown in Figure. 5, enabling progressive loading for the entire scene and making it suitable for streamable applications. From the magnified images, it is evident that newly loaded clusters contribute additional details and shaper texture, allowing the scene to gradually become clearer. Figure 6. Ablation study about the impact of importance weight (IW) and frequency loss (FL) in the bicycle and stump scenes. incorporate each improvement to validate the effectiveness of our approach. More ablations with different adding sequences and the study on the effect of different cluster number are shown in the appendix of supplementary materials. Effectiveness of Cluster-based Fitting. As shown in Table 5, the Vanilla NeuralGS results in significant degradation of rendering quality for 3D scene compared to the original 3DGS [16]. This is primarily due to the large variations of 3D Gaussians, where single tiny MLP tends to produce substantial fitting errors. To address this issue, we designed clustering approach based on Gaussian attributes to maintain similarity within each cluster and assigned different tiny MLPs to fit the Gaussians of each cluster. As shown in Table 5, utilizing different tiny neural fields for different clusters significantly reduces fitting errors, leading to 5.3 dB improvement in PSNR and 10% increase in SSIM, thereby substantially enhancing rendering quality. Effectiveness of Importance Weight. Notably, it is unnecessary to equally fit every Gaussian in the scene. Instead, we use the importance score of each Gaussian to represent its contribution to the renderings. This importance score is applied as weight for the tiny MLP of each cluster during the fitting process, ensuring that important Gaussians are 5. Future Work Our current work primarily focuses on 3D scene reconstruction. Considering the rapid advancements in 4D scene reconstruction, future work could extend to 4D scenes, focusing on leveraging neural fields to further compress timedependent 4D scenes and reduce the memory requirements. 6. Conclusion In this paper, we introduce NeuralGS, novel and effective post-training compression for original 3DGS. The core of our approach lies in leveraging compact neural fields to encode the Gaussian attributes with MLPs. We design multiple neural fields based on clusters and incorporate importance scores as fitting weights to enhance the fittting quality of Gaussian attributes. Additionally, we introduce frequency loss during the fine-tuning stage to further preserve high-frequency details. Extensive experiments demonstrate that our method achieves comparable or even superior performance to existing compression methods while utilizing less model size. Overall, NeuralGS paves the way for directly compressing original 3DGS with neural fields."
        },
        {
            "title": "References",
            "content": "[1] Muhammad Salman Ali, Sung-Ho Bae, and Enzo Elmgs: Enhancing memory and compuTartaglione. tation scalability through compression for 3d gaussian splatting. arXiv preprint arXiv:2410.23213, 2024. 3 [2] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-NeRF: Multiscale Representation for Anti-Aliasing In International Conference on Neural Radiance Fields. Computer Vision (ICCV), 2021. 2 [3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded In Computer Vision Anti-Aliased Neural Radiance Fields. and Pattern Recognition (CVPR), 2022. 1, 2, 3, 5, 7 [4] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In European Conference on Computer Vision, 2024. 2, 3, 7, 1 [5] Jianmei Dai, Zhilong Zhang, Shiwen Mao, and Danpu Liu. view synthesis-based 360 vr caching system over mecenabled c-ran. IEEE Transactions on Circuits and Systems for Video Technology, 30(10):38433855, 2019. [6] Robert Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. ACM Siggraph Computer Graphics, 22(4): 6574, 1988. 2 [7] Engui Fan. Extended tanh-function method and its applications to nonlinear equations. Physics Letters A, 277(4-5): 212218, 2000. 5 [8] Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, and Zhaoxiang Zhang. Trim 3d gaussian splatting for accurate geometry representation. arXiv preprint arXiv:2406.07499, 2024. 3 [9] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245, 2023. 2, 3, 4, 5, 6, 7 [10] Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, and Yonghong Tian. Ae-nerf: Augmenting event-based neural radiance fields for non-ideal conditions and larger scene. arXiv preprint arXiv:2501.02807, 2025. 2 [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. 2 [12] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023. 2, 5, 6, [13] Shrisudhan Govindarajan, Zeno Sambugaro, Towaki Takikawa, Daniel Rebain, Weiwei Sun, Nicola Conci, Kwang Moo Yi, Andrea Tagliasacchi, et al. Lagrangian hashing for compressed neural field representations. arXiv preprint arXiv:2409.05334, 2024. 3 [14] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 2, 5, 7, 8, 1 [15] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation for efficient anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1977419783, 2023. 3 [16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (TOG), 2023. 1, 2, 3, 4, 5, 6, 7, 8 [17] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36 (4):113, 2017. 5, 7, 2 [18] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21719 21728, 2024. 3, 5, 6, [19] Jae Yong Lee, Yuqun Wu, Chuhang Zou, Derek Hoiem, and Shenlong Wang. Plenoptic png: Real-time neural radiance fields in 150 kb. arXiv preprint arXiv:2409.15689, 2024. 3 [20] Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, and Kui Jia. Analytic-splatting: Anti-aliased 3d gaussian splatting via analytic integration. arXiv preprint arXiv:2403.11056, 2024. 3 [21] Aristidis Likas, Nikos Vlassis, and Jakob Verbeek. The global k-means clustering algorithm. Pattern recognition, 36 (2):451461, 2003. 5 [22] Bangya Liu and Suman Banerjee. Swings: Sliding window gaussian splatting for volumetric video streaming with arbitrary length. arXiv preprint arXiv:2409.07759, 2024. 3 [23] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 29362944, 2024. 3, 7, 1, 2 [24] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2, 3, 1 [25] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View In European Conference on Computer Vision Synthesis. (ECCV), 2020. 2, 3, 5, [26] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. Compact 3d scene representation via selforganizing gaussian grids. arXiv preprint arXiv:2312.13299, 2023. 2, 3, 5, 6, 7 [27] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant Neural Graphics Primitives with Multiresolution Hash Encoding. ACM Transactions on Graphics (TOG), 41(4):102:1102:15, 2022. 3, 2 [28] Simon Niedermayr, Josef Stumpfegger, and Rudiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1034910358, 2024. 2, 3, 5, 6, 7 [29] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs. In Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [30] Yatian Pang, Tanghui Jia, Yujun Shi, Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Xing Zhou, Francis EH Tay, and Li Yuan. Envision3d: One image to 3d with anchor views interpolation. arXiv preprint arXiv:2403.08902, 2024. 2 [31] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, Ser-Nam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. 3 [32] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [33] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with In Proceedings of the IEEE/CVF thousands of tiny mlps. international conference on computer vision, pages 14335 14345, 2021. 3 [34] Muhammad Salman Ali, Chaoning Zhang, Marco Cagnazzo, Giuseppe Valenzise, Enzo Tartaglione, and Sung-Ho Bae. Compression in 3d gaussian splatting: survey of methods, trends, and future directions. arXiv e-prints, pages arXiv 2502, 2025. 3 Advances in Neural Information Processing Systems, 36: 3211732128, 2023. 3 [36] Athanassios Skodras, Charilaos Christopoulos, and Touradj Ebrahimi. The jpeg 2000 still image compression standard. IEEE Signal processing magazine, 18(5):3658, 2001. 8 [37] Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, and Eunbyung Park. F-3dgs: Factorized coordinates and representations for 3d gaussian splatting. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 79577965, 2024. 3 [38] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: Modular Framework for Neural Radiance Field Development. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. 2 [39] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cyimage-to-3d generacle3d: High-quality and consistent arXiv preprint tion via generation-reconstruction cycle. arXiv:2407.19548, 2024. [40] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex Kot, and Bihan Wen. Contextgs: Compact 3d gaussian splatting with anchor level context model. arXiv preprint arXiv:2405.20721, 2024. 2, 3, 7, 1 [41] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 [42] Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, and Zhi Wang. Mesongs: Posttraining compression of 3d gaussians via efficient attribute transformation. arXiv preprint arXiv:2409.09756, 2024. 2, 3, 4, 5, 6, 7 [43] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1944719456, 2024. 3 [44] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat-3dgs: context-adaptive triplane approach to rateIn The Thirteenth distortion-optimized 3dgs compression. International Conference on Learning Representations. 3 [45] Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Wangbo Yu, Munan Ning, and Li Yuan. Repaint123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting. arXiv preprint arXiv:2312.13271, 2023. [46] Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, and Eric Xing. Fregs: 3d gaussian splatting with progressive frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2142421433, 2024. 3, 2 [35] Zihang Shao, Chaoran Feng, Xuanye Fang, Yaxin Li, Jiangrong Shen, and Qi Xu. Eicil: joint excitatory inhibitory cycle iteration learning for deep spiking neural networks. [47] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [48] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. Stereo magnification: NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Implementation Details Pipeline Designs. We remove 40% of the redundant Gaussians based on the evaluated importance scores to reduce the total number of Gaussians. After pruning, the Gaussian attributes are converted to half-precision to further minimize model size. For small-scale objects, we typically set the number of clusters between 6 and 10, while for indoor scenes, is set between 40 and 80, and for outdoor scenes, is set between 100 and 140. Each cluster is assigned half-precision tiny MLP to fit the Gaussian attributes for 60k iterations. All MLPs are 5-layer MLPs with Tanh activation functions, using 10 levels of positional encoding. The hidden layer dimension is set to 128, and the output dimension is Dopacity + Dscale + Drotation + Dcolor + DSH , where Dopacity = 1, Dscale = 3, Drotation = 3, Dcolor = 3, and DSH = 45. Here, the rotation attribute always ends with zero, allowing Drotation to omit the trailing zero. We further fine-tune the tiny MLPs for the 25k iterations to restore quality. The Adam optimizer is utilized during both the MLP fitting and fine-tuning stages, with the learning rate decaying from 1e3 to 8e5 for fitting and from 3e4 to 6e6 for fine-tuning. Code Release. We implement our proposed method NeuralGS based on the official code of 3DGS [16]. Upon the publication of the paper, we will release the source code. 8. More Results Visualization. For ease of reference, we also provide public webpage, including the additional videos for comparison, which further demonstrates that our proposed method achieves comparable rendering quality with the original 3DGS with the significantly reduced model size. Comparisons with ScaffoldGS-based Methods. Given ScaffoldGS [24] already has smaller model size and superior rendering quality than 3DGS, the ScaffoldGSbased compression methods (HAC [4], CompGS [23], and ContextGS [40]) show strong performance. In contrast, our method compresses the more commonly used original 3DGS. In spite of the strong performances, ScaffoldGS inIn troduce new complex structure with less scalability. comparison, the original 3DGS is simpler, more scalable, and has broader applications than ScaffoldGS. NerualGS achieves the best result in 3DGS-based methods and comparable to ScaffoldGS-based works (see Table 6 and Table 7). We believe that NeuralGS is promising and inspiring direction to directly compress the original 3DGS for Figure 7. The standard deviation(std) comparison of Gaussian position and opacity in the bicycle scene under different strategies. (1) represents the std of all Gaussians, (2) denotes the average std of attributes across all clusters after clustering directly based on the raw attributes, and (3) indicates the average std of attributes across all clusters after clustering based on normalized attributes. Table 6. Comparisons with ScaffoldGS-based Methods evaluated on the Mip-NeRF 360 dataset [3]. The best is in bold, while the second-best is underlined. Method PSNR SSIM LPIPS Storage CompGS [23] ACM MM 2024 HAC [4] ECCV 2024 ContextGS [40] NeurIPS 2024 NeuralGS (Ours) 27.21 27.41 27.52 27.35 0.802 0.805 0.807 0.806 0.241 0.239 0.238 0.240 17.13MB 16.02MB 14.31MB 16.90MB Table 7. Comparisons evaluated on the Deep Blending [14]. The best is in bold, while the second-best is underlined. Method PSNR SSIM LPIPS Storage CompGS [23] ACM MM 2024 HAC [4] ECCV 2024 ContextGS [40] NeurIPS 2024 NeuralGS (Ours) 29.43 30.08 30.19 29.91 0.900 0.905 0.908 0.906 0.269 0.258 0.259 0. 11.07MB 7.75MB 8.18MB 12.98MB the community. More Ablation Study. In Table. 8, we provide ablations with different adding sequences based on vanilla NeuralGS and multiple running deviations under the full setting to show the effectiveness of each improvement. CF, IW, and FL refer to cluster-based fitting, importance weight, and frequency loss. Adding only the frequency loss improves PSNR by 1.27 dB than the vanilla NeuralGS. Ablation Study on Pruning. In our experiments, we observed that pruning effectively reduces the number of Gaussians by removing less significant Gaussians. With the same Table 8. More ablation study evaluated on the Deep Blending dataset [14]. CF, IW, and FL refer to cluster-based fitting, importance weight, and frequency loss. CF IW FL PSNR SSIM LPIPS Storage 23.54 24.81 29.32 29.56 29. deviations 0.795 0.835 0.899 0.903 0.523 0.384 0.272 0.269 0.906 0.07 0.001 0.0008 0. 7.32MB 7.33MB 12.94MB 12.95MB 12.98MB Table 9. Quantitative ablation study on pruning for the Deep Blending [14] dataset, retaining other improvements. Dataset Method Deep Blending Dataset [14] PSNR SSIM LPIPS Storage w/o pruning w/ pruning (Ours) 29.33 29.91 0.902 0.906 0.263 0. 17.86 MB 12.98 MB Table 10. Model size of the two components on Mip-NeRF 360 [3], Tanks&Temples [17], and Deep Blending [14] datasets. Dataset Mip-NeRF 360 [3] Tanks&Temples [17] Deep Blending [14] Storage position MLPs position MLPs position MLPs Size(MB) 8.23 8.66 4. 7.13 7.31 5.67 Table 11. Different cluster numbers on Deep Blending dataset [14] Clusters Number PSNR SSIM LPIPS Storage 1 10 80 130 26.18 28.02 29.91 30.09 0.846 0.882 0.906 0.908 0.357 0.298 0.254 0.253 7.32MB 8.09MB 12.98MB 17.32MB number of clusters, the reduction in Gaussian numbers enhances the fitting accuracy for important Gaussians, thereby minimizing fitting errors. Thus, the superior fitting quality will be provided for the tiny MLPs in the subsequent finetuning stage, ultimately improving the final rendering quality. As shown in Table 9, the reduction of Gaussian numbers by pruning significantly decreases fitting errors and improves reconstruction quality with the reduced model size. Ablations on Different Cluster Numbers. We present quantitative results for different cluster numbers in Table 11 to investigate the effect of overand under-clustering on reconstruction performance. Storage Analysis. Lastly, We only need to store the position attributes of the pruned Gaussians and the fine-tuned tiny MLPS weights for each cluster. The min-max values for normalization are shared across all clusters, with only negligible 59 floating numbers (2e-4MB) needed. Table 10 illustrates the storage of each component across Mip-NeRF 360 dataset, Tanks&Templates dataset, and Deep Blending. Cluster attributes for MLP fitting instead of local fitting. 3DGS does not show local smoothness on attributes Table 12. Comparison of localand attribute-based clustering. Dataset Method Deep Blending [14] PSNR SSIM LPIPS Storage local-based(position only) attribute-based(all attributes) 28.95 29.91 0.899 0.906 0.261 0. 13.02MB 12.98MB and nearby Gaussians could show significantly different attributes, which makes local clustering unsuitable and fitting difficult. Therefore, we adopt attribute-based clustering for MLP fitting instead of local fitting. We also compare clustering based on local-based (position) and attributebased clustering in Table. 12. Training Time and Memory Usage. For small-scale object scenes, NeuralGS requires approximately 25 minutes for training time. For unbounded scenes, NeuralGS takes around 70 minutes to complete the training due to the increased number of points and more tiny MLPs for more clusters. As another post-training compression method, MesonGS [42] requires 100 minutes with fine-tuning stage to recover rendering quality. For large outdoor scenes, 24GB of memory is needed for 120 clusters. Attribute Distribution. As shown in Figure 7, the original 3D Gaussian attributes exhibit significant variations, and clustering based on attributes substantially enhances the similarity among Gaussians within the same cluster. However, as indicated by std of the position and opacity attribute in (2), direct clustering based on raw attributes tends to overly rely on attributes with large differences for original values, such as position. To address this issue, we perform Gaussian clustering using normalized attributes, which further improves the attribute similarity within each cluster. 9. Additional Discussion on Related Works Difference with FreGS for frequency loss. Our approach differs from FreGS [46]. To enhance high-frequency detail during fine-tuning, we incorporate fixed-bandwidth highfrequency supervision with the traditional L1/SSIM loss, while FreGS discards traditional losses and progressively introduces high-frequency components from the ground truth, starting from low frequencies. Difference with anchor-based methods. Our method fundamentally differs from the local anchor-based works [4, 23, 40]. 3D Gaussians within each cluster are globally distributed after attribute-based clustering. The tiny MLP of each cluster is global neural field which maps locations to attributes of globally distributed Gaussians within each cluster, similar to NeRF [25], whereas anchors are feature vectors used for local Gaussian prediction, akin to Voxels (Plenoxel [11], InstantNGP [27]). Since NeRF is ideally more compact than Voxels to represent the same scene, we adopt NeRF-like neural fields to fit attributes. Difference between neural fields and quantization. Note that our neural field representations are more compact than simple quantization [9, 28]. Our approach employs multiple neural fields based on clustering, allowing predictions to vary with spatial positions, which means that we learn compact function to map locations to different attributes. In contrast, vector quantization typically relies on shared codebook to map similar attributes to the same index, limiting the flexibility of attribute fitting."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Peking University",
        "Texas A&M University"
    ]
}