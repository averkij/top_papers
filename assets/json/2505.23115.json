{
    "paper_title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving",
    "authors": [
        "Yunshen Wang",
        "Yicheng Liu",
        "Tianyuan Yuan",
        "Yucheng Mao",
        "Yingshi Liang",
        "Xiuyu Yang",
        "Honggang Zhang",
        "Hang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 1 1 3 2 . 5 0 5 2 : r Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving Yunshen Wang1,2,, Yicheng Liu1,, Tianyuan Yuan1,, Yucheng Mao1, Yingshi Liang2, Xiuyu Yang1, Honggang Zhang2, Hang Zhao1, Abstract Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications. I. INTRODUCTION Vision-based 3D occupancy prediction is task focused on estimating the semantic labels and occupancy states of each voxel in scene from visual inputs, helping autonomous vehicles perceive their 3D environment with centimeter-level precision. Despite the recent progress in dataset, models and benchmarks [1][3], accurately predicting the 3D occupancy grids is still highly challenging task. Recent approaches in 3D occupancy prediction, which we refer to as discriminative methods, directly learn mapping from images to occupancy grids and have become the de facto choice. However, there are inherent challenges in solving the 3D occupancy prediction task using discriminative methods: 1) The unique nature of predicting occupancy from visual inputssuch as the complex 3D structures, the intricate relationships between 3D labels and the existence of multimodal distributionsmakes this task distinct and more challenging compared to other supervised learning problems. However, discriminative methods directly learn the mapping from images to occupancy, rather than estimating the underlying distributions, which prevents them from incorporating prior knowledge of 3D scenes. This often leads to unrealistic and inconsistent results (refer to the black and orange circle in Fig. 1). Moreover, these methods fail to capture the multimodal nature of the distributions, increasing the burden on downstream tasks. 2) Obtaining perfect occupancy map labels is almost infeasible. Existing benchmarks like KITTI360 [26] and Occ3D [39] rely on equal contribution 1 Institute for Interdisciplinary Information Sciences, Tsinghua University. 2 Beijing University of Posts and Telecommunications. Corresponding to: hangzhao@mail.tsinghua.edu.cn LiDAR scans from one or multiple traversals to create mesh reconstructions of scenes. However, partial observations and sensor noise lead to imperfect labels, which hinder effective model learning. Given these challenges, we believe that modeling occupancy prediction as generative modeling problem offers promising solution. By directly modeling the occupancy data distribution and performing conditional sampling, generative modeling captures the prior knowledge of complex 3D structures and semantics while naturally considering the inherent multi-modality of the occupancy prediction task. Furthermore, mainstream generative models, such as Diffusion Models exhibit inherent robustness to noise, which can mitigate the issue of harmful noise in occupancy labels. Moreover, generative models aim to model the underlying distribution of 3D world occupancy, rather than optimizing direct mapping from images to occupancy. This results in better generalization across scenarios with varying sensor setups, such as input images that differ from the training set or lack certain information. Motivated by this, we explore how to leverage diffusion including investigating models for occupancy prediction, how to perform generative modeling and conditional sampling, and explore other interesting properties. Our extensive experiments demonstrate that generative modeling for occupancy prediction offers series of powerful advantages over discriminative modeling. First, directly modeling the data distribution introduces strong prior for 3D scene occupancy, enhancing the models perceptual capabilities, resulting in more realistic, consistent, and accurate outcomes. For regions with high uncertainty, such as those with insufficient observation, occlusion, or high levels of noise, our model exhibits superior perceptual capabilities. Such holistic, uncertaintyaware, and multimodal-considerate perception results also better support downstream planning tasks, as shown by our experimental results. Our key contributions are summarized as follows: We frame occupancy prediction as process involving generative modeling followed by conditional sampling, from which we summarized four appealing properties compare to discriminative counterpart. We explore five key design aspects of utilizing conditional generative modeling for the occupancy prediction task. Through extensive experiments, we demonstrate that incorporating diffusion models can significantly improve the performance of occupancy prediction. The occuFig. 1: The diagram illustrates the occupancy data production process (a), the discriminative pipeline (b), and the generative pipeline (c). Black and orange circles highlight the deficiencies in the results from both the data production process and the discriminative pipeline, in contrast to the more reasonable results produced by the generative pipeline. pancy features generated by our method also benefit downstream planning tasks. II. FORMULATION We formulate the task of occupancy prediction using diffusion models [4][7], which can express complex multimodal distributions and exhibit robustness to noise. Below, we provide an overview of diffusion models and their adaptation for occupancy prediction. A. Diffusion Models Diffusion models progressively add noise to data and reverse this process to generate samples. For continuous data, the forward process adds Gaussian noise at each step: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), (1) where βt controls the noise level at step t. The reverse process, parameterized by θ, denoises the data: TABLE I: Comparison of different occupancy representations for modeling with diffusion models. DiffOcc(***) denoting the adaptation of diffusion models to the specified representations denoted as ***. Model BEVFormer DiffOcc(latent) DiffOcc(triplane) DiffOcc(discrete) mIoU 31.78 30.65 23.67 30.23 TABLE II: Comparison of mIoU for different guidance techniques and scales, obtained using discrete diffusion models. Guidance Tech Scale 0.5 Scale 1 Scale 2 Scale 3.5 CFG CG 27.34 26.50 31.78 29.45 31.98 29.98 32.45 29.97 C. Occupancy Prediction as Conditional Generation We adapt diffusion models to predict occupancy by modifying the output to represent occupancy grids and conditioning the reverse process on input multi-view images C. This modifies Eq. (2) to: pθ(xt1xt) = (xt1; µθ(xt, t), σ2 θ (t)I), (2) pθ(xt1xt, C) = (xt1; µθ(xt, t, C), σ2 θ (t)I), (7) For discrete data, diffusion models [6], [7] replace Gaussian noise with discrete corruption process, using transition matrix Qt [7]. For discrete variables xt, xt1 {1, . . . , K}, the forward process becomes: q(xtxt1) = Cat(xt; = xt1Qt), (3) The reverse process then predicts the discrete state transitions: pθ(xt1xt) = Cat(xt1; = fθ(xt, t)). (4) B. Training Diffusion Models Starting from sample x0, the marginal at step is q(xtx0), with the posterior: q(xt1xt, x0) = q(xtxt1)q(xt1x0) q(xtx0) . (5) The reverse process is optimized by minimizing the KL divergence between the forward process and the predicted reverse process: loss = DKL (q(xt1xt, x0) pθ(xt1xt)) . (6) and for discrete models Eq. (4) becomes: pθ(xt1xt, C) = Cat(xt1; = fθ(xt, t, C)). (8) III. KEY DESIGN DECISIONS A. Denoiser Network Architecture We use U-Net variant [8] as the denoiser network, trained to predict the clean mask x0 rather than directly predicting xt1, following the x0 parameterization approach. point cloud segmentation network [9] is adapted for denoising, with modifications to handle occupancy data and time embeddings. The time embeddings are implemented using sinusoidal positional encodings, which are further processed by small neural network consisting of two linear layers and SiLU activation [10] to enhance representational capacity. B. Visual Encoder Since directly generating 3D occupancy from 2D images may lead to hallucinations, we employ BEV (Birds-EyeView) model as the visual encoder to lift 2D images to 3D Fig. 2: An overview of using diffusion models for occupancy prediction. base BEV model is employed to encode the input into high-dimensional features, which serve as conditions for the diffusion models. The diffusion model is then able to model different representations of the occupancy grid data. features. The BEV model runs once during both training and inference. In our experiments we use BEVFormer [11] as the visual encoder. C. Options for Diffusion Modeling across Representations Given the various representations available for modeling 3D occupancy grid data, it is essential to explore which representation can be more effectively modeled by diffusion models. In this work, we examine three types of representations for diffusion modeling: spatial latent, triplane, and discrete categorical variables, and compare their performance. Fig. 2 shows an overview. Spatial Latent. We encode 3D occupancy grid data into spatial latent representations to reduce computational cost and achieve compactness. The autoencoder consists of 3D convolutional encoder with skip connections and an implicit MLP decoder with fully-connected layers. After encoding, the diffusion process is applied to the latent representations, as described in Eq. (1) and Eq. (7). The spatial is then decoded to predict semantic class probabilities for reconstructing the occupancy grids. latent Triplane. The triplane representation consists of three planes: hxy RChXhYh , hxz RChXhZh, and hyz RChYhZh , where Ch is the feature dimension, and Xh, Yh, and Zh represent spatial dimensions. The same encoder as described in the spatial latent section is used, and spatial latent representations are transformed into triplane via average pooling. For 3D coordinate = (x, y, z), the triplane feature h(p) is the sum of bilinear interpolations from each plane. The diffusion models aim to model these triplane representations, using the process defined in Eq. (1) and Eq. (7). Discrete categorical variables. Since occupancy grid data is discrete and categorical, we use discrete diffusion process for modeling, as defined in Eq. (3) and Eq. (8). We add noise to the occupancy grids using uniform transition matrices Qt in Equation Eq. (3), as TABLE III: Comparison of mIoU for different conditions. Results obtained using conditional sampling with CFG scale of 3.5. Cond. Option C-PR C-L C-R mIoU 32.45 31.77 40. introduced by Nichol and Dhariwal [12] and adapted for categorical data by Hoogeboom et al. [6]. learnable embedding layer projects the corrupted discrete labels into high-dimensional continuous feature space for input to the denoiser."
        },
        {
            "title": "We train and validate all",
            "content": "three models on the occ3dNuScenes dataset [3] with consistent settings. Results in Tab. indicate that discrete categorical variables outperform the other representations in generative pipeline, as measured by mIoU. This is likely due to the discrete nature of occupancy grid data and potential information loss from the autoencoder. Thus, we use discrete categorical variables to represent occupancy grids for the remainder of this paper. D. Guidance techniques Classifier-free guidance (CFG) [13] and classifier guidance (CG) [14] are commonly used in conditional image generation to enhance the influence of conditions, such as text prompts. CFG adjusts the logits of the conditional (ℓc) and unconditional (ℓu) models with guidance scale s, formulated as ℓ = (s+1)ℓcsℓu. In contrast, CG uses classifier to compute gradients of the condition log-probability, directing the generation towards the desired outcome. We compare these guidance techniques in our conditional sampling process and find that CFG outperformed CG, as shown in Tab. II. This is likely because the classifier used in CG, discriminative occupancy prediction model [15], has limited classification performance compared to image classifiers, leading to significant errors. Consequently, we choose CFG as the guidance technique for our sampling process. Fig. 3: Qualitative results on Occ3D-nuScenes validation set. Fig. 4: Qualitative results of multi-modality. E. Condition Options To leverage the advantages of conditional diffusion models, we conduct experiments using different conditions provided by the base BEV models to assess their impact on occupancy prediction. We evaluate three types of conditions: (1) predictions from the BEV model (C-PR), where the diffusion model refines these initial predictions; (2) logits from the final classifier layer (C-L); and (3) representations before the final classifier layers (C-R), which allows for endto-end training of both the visual encoder and the diffusion model. As shown in Tab. III, representations before the final classifier layers achieve the best performance. This suggests that these features offer more informative guidance for the diffusion models generative process, further enhanced by end-to-end training. Therefore, we use these representations as conditions for the diffusion model. IV. FASCINATING PROPERTIES OF DIFFUSION MODELS FOR OCCUPANCY PREDICTION In this section, we provide insights and intuitions on modeling occupancy prediction as diffusion modeling task and highlight its advantages over other discriminative methods. A. 3D Scene Prior Real-world occupancy data frequently encompass intricate 3D structures and detailed object shapes, including pedestrians, buildings, and vegetation. Diffusion models capture and model these complexities as 3D scene prior more effectively than discriminative methods. This prior enables joint modeling of semantic relationships among voxels, allowing the generation of highly probable outcomes under conditional guidance while preserving scene consistency and plausibility. Consequently, this leads to more precise occupancy perception, outperforming discriminative models, as demonstrated in Sec. V-B. Our qualitative results in Fig. 3 illustrate the accuracy and reliability of our occupancy predictions. Moreover, in regions with incomplete observations Fig. 5: Ground Truth vs. Predictions, Our model provides denser and more coherent occupancy estimations compared to point cloud-derived ground truths (for instance, it includes complete drivable surfaces). or occlusions, the inclusion of 3D scene prior inherently equips the model to infer missing information, resulting in more comprehensive perception outputs (see Sec. V-C). This enhancement is crucial for effective downstream planning, as demonstrated in our experiments in Sec. V-F. B. Robustness to Noisy Data During 3D occupancy data annotation, issues such as insufficient observations and sensor noise often lead to imperfect labels, posing challenges for discriminative models and leading to blurred or incomplete predictions. In contrast, diffusion models inherently handle such noise due to their denoising capabilities. Their forward diffusion process acts as an augmentation technique, helping to counteract the impact of noisy occupancy labels. As shown in Sec. V-D, our quantitative analysis demonstrates that diffusion models exhibit superior robustness to noise compared to traditional methods, leading to more accurate occupancy predictions. C. Multi-Modal Occupancy Distributions Predicting occupancy grids from multi-view images is inherently ill-posed because there are multiple occupancy configurations that can match the same image observations, resulting in multi-modal conditional distribution q(xC). Discriminative models, however, are limited to producing single prediction and fail to capture this multi-modality, which can hinder downstream tasks such as planning where multiple scenarios need to be considered. In contrast, diffusion models excel at representing multi-modal distributions, allowing them to generate diverse and realistic samples that align with camera observations. As demonstrated in Fig. 4, diffusion models effectively capture this variability, providing richer and more accurate predictions. D. Dynamic Inference Steps Leveraging the multi-step sampling process, our approach facilitates dynamic inference, offering versatile balance between computational resources and the quality of predictions. More discussions are in Sec. V-E. A. Experimental Setup V. EVALUATION Our experimental setup is based on our best practices outlined in Sec. III. Benchmark. We evaluate our model on the Occ3D-nuScenes dataset [3]. This dataset covers spatial range from -40m to 40m in the and axes, and from -1m to 5.4m in the axis, with occupancy labels provided in 0.4m voxel grids across 17 categories. The data TABLE IV: 3D Occupancy Prediction Performance on the Occ3D-nuScenes Validation Dataset. The evaluation is conducted using LiDAR mask. The symbol means the backbone is pretrained using the nuScense segmentation. Cons. Veh represents construction vehicle, and Driv. Sur is short for driveable surface. indicates our own re-implementation. DiffOcc (x) denotes the use of representations obtained from as conditions for diffusion models. b B s a o r r r c i r . C V y t i e r a u r . t fl w r O a e n i e T o fi 1.7 7.2 2.7 35. R101 9001600 4.2 7.6 4.4 6.0 R101 9001600 22.36 8.43 32.09 40.19 12.15 14.99 17.04 16.18 16.49 27.88 48.75 27.64 28.87 25.15 11.16 14.84 SwinB 5121408 24.46 4.74 26.64 12.48 23.13 22.25 13.29 10.97 13.92 13.18 23.28 21.21 67.62 33.02 43.37 44.18 19.11 23.42 256704 38.33 8.52 43.85 26.25 42.79 49.59 22.94 27.65 27.73 26.98 34.41 36.73 78.36 41.41 47.01 49.80 44.89 42.54 R101 SwinB 5121408 39.08 7.17 46.11 22.09 25.1 34.77 37.89 78.92 42.13 50.63 50.54 49.04 45.91 R101 8641600 42.32 8.37 49.86 28.11 51.45 58.97 24.82 80.8 42.97 51.25 45.38 44.33 42.32 46.2 51.57 24.08 25.15 27.11 34.6 32.29 30.15 37.75 46.77 14.9 7.4 1. 9.3 4.9 3.9 5.6 3.0 5. 6.3 7.9 7.1 R101 9001600 35.62 4.96 41.48 15.81 44.65 50.97 R101 8641600 42.15 8.51 49.22 28.35 50.37 58.48 25.58 20.9 20.74 25.87 22.32 33.11 35.59 78.05 39.04 44.49 48.17 41.08 38.25 50.8 51.63 44.83 42.15 34.3 31.16 30.49 38.38 45. 81.2 44.07 Method MonoScene [16] BEVFormer [11] RenderOcc [17] FB-Occ [15] BEVDet [18] PanoOcc [19] BEVFormer [11] PanoOcc [19] DiffOcc (BEVFormer) R101 9001600 42.67 8.59 47.61 26.02 54.21 58.11 DiffOcc (PanoOcc) R101 8641600 43.08 7.86 48.67 25.66 55.73 57.14 27.17 32.33 30. 26.2 29.55 31.1 29.61 38.93 46.19 81.23 44.01 50.99 51.6 51.93 49.56 30.1 40.42 48.48 82.01 43.98 52.06 52.03 49.98 48.69 TABLE V: 3D occupancy prediction performance in camerainvisible regions. TABLE VI: 3D occupancy prediction performance at long distances. Method BEVFormer [11] BEVDet [18] FB-Occ [15] RenderOcc [17] PanoOcc [19] DiffOcc (BEVFormer) DiffOcc (PanoOcc) b B R101 SwinB R101 SwinB R101 R101 R101 m 21.42 21.79 24.84 22.57 30.13 36.45 36.83 collection vehicle is equipped with LiDAR, five radars, and six cameras, ensuring 360-degree environmental perception. Settings. Our framework is designed to be plug-and-play, and given the exceptional performance and generalizability of various mainstream occupancy prediction methods, we selected several off-the-shelf BEV models pretrained using established methodologies as our base models for generating conditions. During training, we used 1000 steps and aligned the remaining training details with those of leading occupancy prediction methods to ensure fairness. For inference, we used 10-step process and set the guidance scale for CFG to 3.5, unless stated otherwise. B. Comparison with State-of-the-art Methods [19] encoders as visual representations for We evaluated our model using BEVFormer [11] and to produce highPanoOcc dimensional the generative process. We compared its performance with several popular methods [15][18]. As shown in Tab. IV, our approach achieved 7.05 mIoU improvement over BEVFormer and 0.97 mIoU gain over PanoOcc. These results underscore the effectiveness and versatility of our generative modeling. C. Reasoning with Prior in Camera-Invisible Regions To demonstrate the superior performance of generative models in complex perception tasks like occupancy prediction, we evaluated the mIoU in camera-invisible regions. We defined these regions based on the camera-invisible labels Method BEVFormer PanoOcc DiffOcc (BEVFormer) DiffOcc (PanoOcc) Backbone mIoU R101 R101 R101 R101 26.03 30.13 30.73 32.04 TABLE VII: mIoU scores of different methods across various visible probability values. Each column represents the mIoU scores for voxels where the visibility probability is below the specified threshold, showcasing the performance of each method under varying degrees of visibility constraints. Method 5% 10% 20% 50% BEVFormer DiffOcc (BEVFormer) 28.1 33.82 29.55 36.02 32.26 39.69 33.59 41.64 in the Occ3D-nuScenes datasets and compared our method against state-of-the-art approaches, as detailed in Tab. V. Real-world occupancy datasets often rely on annotations derived from aggregated LiDAR point clouds, which may not cover all invisible regions, limiting the evaluation across entire scenes. However, qualitative results indicate that our method consistently delivers realistic and reasonable predictions throughout the entire scene, as illustrated in Fig. 5. D. Performance In Noisy Regions To evaluate the models performance in noisy regions, we evaluated the mIoU in noise-prone distant areas (20 meters away), as detailed in Tab. VI. We also calculated the visibility probabilities of all voxels near the ego vehicle across the Occ3D-nuScenes [3] datasets training set and evaluated mIoU in regions with lower visibility probabilities, as shown in Tab. VII. Our results show that generative models outperform discriminative methods in these regions, highlighting their superior ability to learn the true scene distribution and maintain robustness in high-noise environments. E. Inference Steps. In our experiments, we found that maintaining the complete sequence of inference steps identical to the training TABLE VIII: Comparison of occupancy prediction effectiveness across different models. G.T. Occ. refers to the use of ground-truth occupancy annotations. BEVFormer represents results from the standard BEVFormer model, while DiffOcc uses our proposed diffusion-based objectives. indicates that no visible masks were applied during training or evaluation. Method G.T. Occ. BEVFormer DiffOcc (BEVFormer) BEVFormer DiffOcc (BEVFormer) L2 (m) 3s 2s 3.02 3.39 3.08 5.27 2.91 4.20 4.73 4.29 7.71 4.12 Avg. 1s 2.95 3.31 3.01 5.15 2.87 2.97 2.52 2.39 1.66 1.86 Collision (%) 2s 3.77 3.26 3.26 4.29 2.74 3s Avg. 4.53 4.72 4.37 8.22 4.87 3.76 3.50 3.34 4.72 3.16 1s 1.62 1.82 1.66 2.46 1.56 TABLE IX: mIoU scores of different sample steps during inference. Steps 1 2 10 15 50 mIoU 40.48 40.63 42.12 42. 41.99 40.47 phase significantly reduces performance. This issue may be due to discrepancies between the training and testing distributions [20]. Notably, using only the initial few steps and leveraging the reconstructed x0 led to performance saturation. We observed that performance peaks around 10 to 15 steps. The trade-off between performance and the number of inference steps for DiffOcc is illustrated in Tab. IX. F. planning We evaluate the quality of occupancy prediction from new perspective: its impact on downstream planning tasks. The ultimate goal of perception modules in autonomous driving is to support planning. However, the commonly used IoU metric, calculated only on visible grids (filtered by visible mask), overlooks the importance of predicting complete, physically consistent, and realistic occupancy scenean essential factor for decision-making in planning. We argue that our diffusion-based method provides more informative occupancy predictions for planning modules by leveraging implicit 3D scene priors. To validate this, we modify simple planning module based on UniAD [21], replacing its BirdsEye-View (BEV) features with ground-truth occupancy annotations. During evaluation, we assess the effectiveness of occupancy predictions from different models. As shown in Tab. VIII, our method outperforms the discriminative model both with and without visible masks. When trained and tested without visible masks, the performance of the discriminative model drops significantly, whereas our method surpasses even the ground-truth occupancy annotations. This demonstrates that our model offers more informative and comprehensive environment perception. VI. RELATED WORKS A. 3D Occupancy Prediction and Completion With the growing importance of vision-centric autonomous driving systems, an increasing number of researchers are focusing on 3D occupancy prediction tasks [3], [15][17], [22][31]. related task is Semantic Scene Completion (SSC), which aims to estimate dense semantic space from partial observations [1], [32][34]. Although both tasks produce similar outputs, SSC emphasizes reconstructing 3D scene geometry and semantics from sparse data, whereas 3D occupancy prediction focuses on accurately representing the occupancy of 3D space, particularly for both dynamic and static objects within the sensor-visible range. Our model utilizes generative approaches to address prediction tasks and can also be applied to completion tasks. B. Generative Models for Autonomous Driving and Robotics Generative models have found extensive applications in autonomous driving and robotics [35][42]. Specifically, in the realm of perception, MapPrior [31] introduced novel BEV perception framework that integrates traditional discriminative BEV perception model with learned generative model for semantic map layouts. UltraLiDAR [41] pioneered the use of VQ-VAE to complete and generate realistic LiDAR point clouds. Copilot4D [39] developed discrete-diffusionbased model tailored for 4D LiDAR point clouds, achieving state-of-the-art performance. Similarly, DiffBEV [43] leveraged diffusion models to generate more comprehensive BEV representation. The most relevant work to ours is OccGen [44], but it treats diffusion models as coarse-tofine process while overlooking many properties of diffusion models in occupancy prediction. VII. CONCLUSION & DISCUSSION Conclusion. Our experiments demonstrate the superior performance of Diffocc in challenging scenarios, offering more accurate and realistic predictions. This advance not only enhances perception capabilities but also benefits downstream planning tasks, highlighting the potential of generative modeling for improving autonomous systems. Disccusion. Inference Latency is also an important consideration. Tab. IX shows that our model can achieve good performance with just 1-2 sampling steps. Utilizing faster base model and acceleration techniques for diffusion models can further enhance the applicability of generative models for occupancy prediction. Hallucination is common concern for generative models; however, the mIoU, as discriminative metric, shows that generative modeling achieves superior perception accuracy compared to discriminative models. Moreover, the performance improvements in planning tasks further demonstrate that this modeling approach does not induce hallucinations detrimental to downstream tasks."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work is supported by National Key R&D Program of China (2022ZD0161700) and Tsinghua University Initiative Scientific Research Program."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Liao, J. Xie, and A. Geiger, Kitti-360: novel dataset and benchmarks for urban scene understanding in 2d and 3d, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [2] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, Semantickitti: dataset for semantic scene understanding of lidar sequences, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 92979307. [3] X. Tian, T. Jiang, L. Yun, Y. Wang, Y. Wang, and H. Zhao, Occ3d: large-scale 3d occupancy prediction benchmark for autonomous driving, arXiv preprint arXiv:2304.14365, 2023. [4] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. [5] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [6] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forre, and M. Welling, Argmax flows and multinomial diffusion: Learning categorical distributions, Advances in Neural Information Processing Systems, vol. 34, pp. 12 45412 465, 2021. [7] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, Structured denoising diffusion models in discrete state-spaces, Advances in Neural Information Processing Systems, vol. 34, pp. 17 981 17 993, 2021. [8] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional image networks for biomedical computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 2015, pp. 234241. image segmentation, in Medical [9] X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and D. Lin, Cylindrical and asymmetrical 3d convolution networks for lidar segmentation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 99399948. [10] S. Elfwing, E. Uchibe, and K. Doya, Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, Neural networks, vol. 107, pp. 311, 2018. [11] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai, Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers, in European conference on computer vision. Springer, 2022, pp. 118. [12] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised learning using nonequilibrium thermodynamics, in International conference on machine learning. PMLR, 2015, pp. 22562265. [13] J. Ho and T. Salimans, Classifier-free diffusion guidance, arXiv preprint arXiv:2207.12598, 2022. [14] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, Advances in neural information processing systems, vol. 34, pp. 87808794, 2021. [15] Z. Li, Z. Yu, D. Austin, M. Fang, S. Lan, J. Kautz, and J. M. Alvarez, Fb-occ: 3d occupancy prediction based on forward-backward view transformation, arXiv preprint arXiv:2307.01492, 2023. [16] A.-Q. Cao and R. de Charette, Monoscene: Monocular 3d semantic scene completion, in CVPR, 2022, pp. 39914001. [17] M. Pan, J. Liu, R. Zhang, P. Huang, X. Li, L. Liu, and S. Zhang, Renderocc: Vision-centric 3d occupancy prediction with 2d rendering supervision, arXiv preprint arXiv:2309.09502, 2023. [18] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, Bevdet: Highperformance multi-camera 3d object detection in bird-eye-view, arXiv preprint arXiv:2112.11790, 2021. [19] Y. Wang, Y. Chen, X. Liao, L. Fan, and Z. Zhang, Panoocc: Unified occupancy representation for camera-based 3d panoptic segmentation, arXiv preprint arXiv:2306.10013, 2023. [20] Z. Lai, Y. Duan, J. Dai, Z. Li, Y. Fu, H. Li, Y. Qiao, and W. Wang, Denoising diffusion semantic segmentation with mask prior modeling, arXiv preprint arXiv:2306.01721, 2023. [21] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, Planning-oriented autonomous driving, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [22] X. Wang, Z. Zhu, W. Xu, Y. Zhang, Y. Wei, X. Chi, Y. Ye, D. Du, J. Lu, and X. Wang, Openoccupancy: large scale benchmark for surrounding semantic occupancy perception, in ICCV, 2023. [23] Y. Zhang, Z. Zhu, and D. Du, Occformer: Dual-path transformer for vision-based 3d semantic occupancy prediction, in ICCV, 2023. [24] Y. Wei, L. Zhao, W. Zheng, Z. Zhu, J. Zhou, and J. Lu, Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving, in ICCV, 2023, pp. 21 72921 740. [25] S. Zuo, W. Zheng, Y. Huang, J. Zhou, and J. Lu, Pointocc: Cylindrical tri-perspective view for point-based 3d semantic occupancy prediction, arXiv preprint arXiv:2308.16896, 2023. [26] W. Gan, N. Mo, H. Xu, and N. Yokoya, simple attempt for 3d occupancy estimation in autonomous driving, arXiv preprint arXiv:2303.10076, 2023. [27] A.-Q. Cao and R. de Charette, Scenerf: Self-supervised monocular 3d scene reconstruction with radiance fields, in ICCV, 2023, pp. 9387 9398. [28] Y. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu, Tri-perspective view for vision-based 3d semantic occupancy prediction, in CVPR, 2023, pp. 92239232. [29] Y. Li, Z. Yu, C. Choy, C. Xiao, J. M. Alvarez, S. Fidler, C. Feng, and A. Anandkumar, Voxformer: Sparse voxel transformer for camerabased 3d semantic scene completion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 90879098. [30] W. Tong, C. Sima, T. Wang, L. Chen, S. Wu, H. Deng, Y. Gu, L. Lu, P. Luo, D. Lin, et al., Scene as occupancy, in ICCV, 2023, pp. 8406 8415. [31] X. Zhu, V. Zyrianov, Z. Liu, and S. Wang, Mapprior: Birds-eye view map layout estimation with generative models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 82288239. [32] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, Joint 2d3d-semantic data for indoor scene understanding, arXiv preprint arXiv:1702.01105, 2017. [33] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 58285839. [34] J. Lee, W. Im, S. Lee, and S.-E. Yoon, Diffusion probabilistic models for scene-scale 3d categorical data, arXiv preprint arXiv:2301.00527, 2023. [35] K. Yang, E. Ma, J. Peng, Q. Guo, D. Lin, and K. Yu, Bevcontrol: Accurately controlling street-view elements with multi-perspective consistency via bev sketch layout, arXiv preprint arXiv:2308.01661, 2023. [36] A. Swerdlow, R. Xu, and B. Zhou, Street-view image generation from birds-eye view layout, IEEE Robotics and Automation Letters, 2024. [37] X. Li, Y. Zhang, and X. Ye, Drivingdiffusion: Layout-guided multiview driving scene video generation with latent diffusion model, arXiv preprint arXiv:2310.07771, 2023. [38] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D.-Y. Yeung, and Q. Xu, Magicdrive: Street view generation with diverse 3d geometry control, arXiv preprint arXiv:2310.02601, 2023. [39] L. Zhang, Y. Xiong, Z. Yang, S. Casas, R. Hu, and R. Urtasun, Learning unsupervised world models for autonomous driving via discrete diffusion, arXiv preprint arXiv:2311.01017, 2023. [40] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang, Panacea: Panoramic and controllable video generation for autonomous driving, arXiv preprint arXiv:2311.16813, 2023. [41] Y. Xiong, W.-C. Ma, J. Wang, and R. Urtasun, Learning compact representations for lidar completion and generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10741083. [42] B. Lange, M. Itkina, and M. J. Kochenderfer, Lopr: Latent occupancy prediction using generative models, arXiv preprint arXiv:2210.01249, 2022. [43] J. Zou, Z. Zhu, Y. Ye, and X. Wang, Diffbev: Conditional diffusion model for birds eye view perception, arXiv preprint arXiv:2303.08333, 2023. [44] G. Wang, Z. Wang, P. Tang, J. Zheng, X. Ren, B. Feng, and C. Ma, Occgen: Generative multi-modal 3d occupancy prediction for autonomous driving, in European Conference on Computer Vision. Springer, 2024, pp. 95112."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Institute for Interdisciplinary Information Sciences, Tsinghua University"
    ]
}