{
    "paper_title": "You Do Not Fully Utilize Transformer's Representation Capacity",
    "authors": [
        "Gleb Gerasimov",
        "Yaroslav Aksenov",
        "Nikita Balagansky",
        "Viacheslav Sinii",
        "Daniil Gavrilov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 5 4 2 9 0 . 2 0 5 2 : r You Do Not Fully Utilize Transformers Representation Capacity Gleb Gerasimov * 1 2 Yaroslav Aksenov * 1 Nikita Balagansky 1 3 Viacheslav Sinii 1 Daniil Gavrilov"
        },
        {
            "title": "Abstract",
            "content": "In contrast to RNNs, which compress previous tokens into single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce LayerIntegrated Memory (LIMe), simple yet powerful approach that preserves the models overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research. 1. Introduction Transformers (Vaswani et al., 2017) have become central architecture in modern machine learning, powering stateof-the-art solutions in language modeling, computer vision, and beyond. Their ability to capture complex patterns arises from deeply stacked layers that refine contextual representations. However, despite their success, standard Transformer decoders maintain single residual stream per layer, forcing the model to compress all previously learned features into the immediately preceding hidden state (Srivastava et al., 2015; He et al., 2015). This design choice can lead to representation collapse phenomenon in which different tokens or features become indistinguishable in deeper layers (Voita et al., 2019; Barbero et al., 2024; Arefin et al., 2024). The problem is particularly pronounced when learning from *Equal contribution 1T-Tech 2HSE University 3Moscow Institute of Physics and Technology. Correspondence to: Yaroslav Aksenov <y.o.aksenov@tbank.ru>. Preprint lengthy sequences, where subtle token distinctions risk being squeezed out by limited floating-point precision and finite hidden-state capacity. In this paper, we propose Layer-Integrated Memory1 (LIMe), simple yet powerful extension to masked multihead self-attention that enables the model to retrieve and integrate representations from all earlier layers rather than relying solely on the most recent hidden state. LIMe accomplishes this by learning special routing mechanism that efficiently blends multi-layer features for both keys and values, all while preserving the core Transformer structure and adding negligible overhead. Through extensive language modeling experiments and indepth analysis, we demonstrate three main contributions. First, LIMe consistently outperforms standard Transformer baselines (Grattafiori et al., 2024) and other state-of-the-art modifications (Zhu et al., 2024) across diverse range of one-shot benchmarks. Second, our empirical investigations show that LIMe effectively counters representation collapse: it preserves higher entropy in deeper layers, increases separability for closely related tokens, and improves overall representational diversity. Third, we provide insights into the depthwise circuits LIMe learns, revealing how crucial lexical or syntactic cues from earlier layers are seamlessly reintroduced in later layers. Together, these findings indicate that explicitly integrating multi-layer memory not only enhances performance but also yields richer, more interpretable internal representations. Our results suggest promising direction for building deeper and more robust Transformers. By decoupling the burden of storing all relevant context from the single residual stream, LIMe opens the door to range of architectural advances that harness earlier-layer features more effectively. 2. Related Work Since the works of Srivastava et al. (2015) and He et al. (2015), deep networks have been described as series of modules that sequentially refine residual stream to predict target (i.e., with residual connections). The Transformer model (Vaswani et al., 2017) is no exception. Even modern 1Source code is available by the link https://github.com/corlteam/lime 1 You Do Not Fully Utilize Transformers Representation Capacity 3.1. Attention Mechanism The attention mechanism enables the model to assign different levels of importance to tokens when encoding particular token. Scaled dot-product attention is central to the Transformer model. Given queries Rnd, keys Rnd, and values Rnd, where is the sequence length, the attention function is: Attention(Q, K, V) = softmax (cid:16) QK (cid:17) V. The factor bilizing gradients during training. helps mitigate large dot-product values, staFigure 1. Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has substantially lower loss with similar amount of FLOPs. See Section 5.1 for more details. LLMs (Grattafiori et al., 2024; Jiang et al., 2023; Qwen et al., 2024; DeepSeek-AI et al., 2024) still rely on residual connections and normalizations. Despite the effectiveness of transformers and residual connections, this setup requires storing all the features needed to solve the task in single vector. Tenney et al. (2019) showed that different tasks have different optimal layer indices, while Voita et al. (2019) demonstrated that LMs are particularly vulnerable to representations being squeezed when different tokens appear in similar hidden states. Hahn & Rofin (2024) noted that transformers cannot model sensitive functions for large sequence lengths and Barbero et al. (2024) showed that pretrained models cannot separate long sequences with small changes in hidden state space. These issues are commonly referred to as representation collapse. To address this issue, various approaches to hidden state aggregation have been proposed (Fang et al., 2023; Yang et al., 2021), but these methods are mostly ad hoc and are trained on downstream discriminative tasks. Recently, Zhu et al. (2024) proposed using multiple residual streams that can interact with each other, although this increases the hidden state size of the model. Arefin et al. (2024) introduced additional regularization to prevent representation collapse. 3. Preliminary To prevent the model from accessing future tokens during training, masking is applied in self-attention so that each position only attends to past outputs. The multi-head self-attention mechanism is defined as: MultiHead(Q, K, V) = Concat(head1, . . . , headH ) W(O), where each head is computed by headi = Attention(cid:0)Q W(Q) , W(K) , W(V ) (cid:1). Rddh , W(V ) Rddh , W(K) Here, W(Q) Rddh, and W(O) RHdhd are learned projection matrices, 1 . . . H, dh is the dimensionality of each head (usually dh < d), and is the number of heads. The concatenation of heads is performed along the last dimension. 3.2. Gated Feed-Forward Networks In the LLaMA architecture, each decoder layer contains Gated Feed-Forward Network that is applied identically to every position. The Gated MLP can be written as: (cid:1)(cid:105) (cid:1) SiLU(cid:0)x Wf +bf (cid:104) (cid:0)x Wg+bg FFN(x) = Wo + bo. where denotes element-wise multiplication and Wg, Wf , Wo, bg, bf , bo are learned parameters. 3.3. Residual Connections Residual connections are crucial in Transformers, helping address vanishing gradients and enabling deeper networks to be trained effectively. In both encoder and decoder blocks, each sub-layer (e.g., multi-head attention or feed-forward) incorporates residual connection. The decoder-only Transformer architecture comprises identical layers. Each layer includes masked multi-head self-attention sub-layer followed by position-wise feedforward network. Residual connections and layer normalization are applied around each sub-layer to facilitate training. Formally, let be the input to sub-layer and F(x) its function (e.g., multi-head attention or feed-forward network). The output of the sub-layer with residual connection is: = + F(cid:0)LayerNorm(x)(cid:1). 2 You Do Not Fully Utilize Transformers Representation Capacity Table 1. LM Evaluation Harness benchmarks (accuracies in %) on 1.2B models with num-fewshots 1. The rightmost column shows average accuracy across the tasks. Proposed methods outperform both LLaMA and HyperConnections (Zhu et al., 2024) baselines. See Section 5.1 for more details. Model ARC-E ARC-C Winogrande COPA MultiRC RTE HellaSwag PIQA LLaMA HC LIMe Dynamic LIMe Static 69.5 70.1 72.7 71.1 38.7 38.4 39.5 39.3 55.2 53.0 53.1 56. 75.0 77.0 79.0 75.0 42.8 42.9 43.0 43.1 54.5 51.6 52.4 55.2 53.1 54.4 54.4 53.9 72.5 73.5 72.9 72.2 Avg 57.7 57.6 58.4 58.3 These connections provide each sub-layer access to both the transformed output and the original inputs, promoting more stable and efficient training. the next layers output Xr by aggregating features from {X0, . . . , Xr1}. We define the Key-Value projection inputs for head as For the following sections, let X0 be the sequence embeddings and Xℓ be the hidden representation after the ℓ-th decoder layer. 4. Method We introduce mechanism called Layer-Integrated Memory (LIMe) to augment Transformer decoders with multi-layer memory. Starting from the second decoder layer (ℓ = 2), we replace all Masked Multi-Head Attention (MHA) heads with specialized heads that attend to learnable convex combination of the representations from all earlier layers (including the original embeddings). We describe below how LIMe forms these combinations and parameterizes their computation, either statically or dynamically. 4.1. LIMe Overview In standard Transformer decoder, each ℓ-th layer (1 ℓ L) takes as input residual stream Xℓ1 and produces updated representations Xℓ. The MHA sub-layer typically derives queries, keys, and values from Xℓ1. In contrast, LIMe modifies the key-value side of attention while keeping queries the same. Specifically, queries remain projection of Xℓ1; keys and values are generated from convex combination of all preceding layer outputs {X0, . . . , Xℓ1}, where X0 is the initial token embedding. In order to simplify our notation, we denote LIMe layers that retrieve information as (with 2 L). Meanwhile, we denote layers that provide memorized features as (with 0 1). 4.2. LIMe Router To form keys and values from multiple past representations, LIMe uses an inter-layer router. For each layer 2, unique router outputs weight matrix RHr, where is the number of attention heads. Forming Convex Mixtures. Let Xr1 Rtd be the residual stream after layer 1. We want to compute Z(r) = Emp(mh,r) (cid:2)Xm (cid:3) Rtd, (1) where p(m h, r) = α(r) h,m are per-level head-specific routing coefficients from weight matrix R, softmaxnormalized to sum to 1 per head to form valid discrete distributions, giving each LIMe router the ability to emphasize higheror lower-level features. In Z(r) , the superscript indicates that these features are computed at layer r. Henceforth, to simplify notation, we omit the superscript in subsequent derivations. Key-Value Projections. Each head maps Zh to keys Kh and values Vh via learnable projection matrices W(K) Rddh and W(V ) Rddh: Kh = Zh W(K) , Vh = Zh W(V ) . Thus, instead of relying solely on Xr1, each head can attend to the specific learned mixture of all prior layers. Query Projection. Queries remain projection of the residual stream via W(Q) Rd(hdh), split into separate queries per head: Qh = Xr1 W(Q) . This preserves standard decoder attention on the query side, while the key-value side gains deeper contextual information. By using the LIMe Router block, LIMe heads can draw from any prior layers representations, creating layer-wise memory in the decoder. Unlike standard skip connections or naive averaging, LIMe learns distinct weightings over past states per head, capturing richer dependencies. This design resembles Ladder Networks (Rasmus et al., 2015) in that it can forget information that can be recovered later via routers. Compared to naive multi-head attention, LIMe adds negligible overhead that is linear in the sequence length, which can be further minimized with pruning techniques as 3 You Do Not Fully Utilize Transformers Representation Capacity baselines: LLaMa (Grattafiori et al., 2024) and Hyper Connections (Zhu et al., 2024). Training Setup. All models have approximately 1.2B parameters and share the same underlying transformer architecture (see Table 4). Each model is trained from scratch on FineWeb Edu subset with about 50B tokens. The full training setup can be found in Appendix A. Results and Evaluation. Figure 1 displays the training loss curves, demonstrating that both Dynamic LIMe and Static LIMe converge more rapidly than LLaMa. When normalized by floating-point operations (FLOPs), both LIMe variants achieve lower perplexities than LLaMa, thereby indicating improved parameter efficiency. Table 1 presents results on the one-shot LM Eval Harness benchmarks, which further highlight the advantages conferred by LIMe. Overall, these results suggest that LIMes layer-integrated memory mechanism consistently provides benefits over baseline methods. In the next section, we go deeper into the factors driving these gains. For detailed efficiency information, see Section D. 5.2. Analysing Learned Routings in LIMe One of the goals of our work is to illuminate how LIMe routes information across multiple prior layers, thereby mitigating representation collapse. To this end, we examine the learned routing weights (both static and dynamic) and analyze the resulting semantic circuits that emerge within each Transformer layer. In the static case, each heads weights are shared across the entire batch, whereas the dynamic router can modulate its mixture coefficients on per-token basis. To approximate these dynamic priors, we aggregated routing outputs over 50,000 sequences from FineWeb Edu. For full, detailed view of LIMe Dynamic and Static Router distributions, see Appendix Figure 13. Appendix Figure 10 further summarizes how much weight is allocated specifically to the final residual stream, averaged over all heads. Notably, even in the deepest LIMe layers, subset of heads still attends to lower-level representations, which helps prevent crucial lexical distinctions from being compressed out. Layer-Wise Mixtures and Emphasis on Shallow Representations. Figure 2 illustrates how much each LIMe layer relies on the most recent residual stream versus lower-level features. Under standard self-attention, this weight would be fixed at 1.0. More detailed per-head Self Retrieval weights are visualized in Figure 3. In LIMe we see that early layers strongly incorporate embeddings or low-level features, suggesting specialized morphological circuits that cluster Figure 2. Mean retrieval weight for each representation (m) among later layers (r). In both cases, in the last layers, models tend to retrieve information from previous layers rather than from the current one. In the case of Dynamic LIMe, there is clear bump in retrieving from the first layer. See Section 5.2 for more details. we discuss in Section 5.4. Note that this architecture can be viewed as modification of the input to the attention block, preserving compatibility with methods for efficient MHA computation (Dao et al., 2022; Dao, 2024; Shah et al., 2024). For more details on method efficiency, refer to Section D. 4.3. Static and Dynamic Routers The LIMe Routers coefficients Rh,m in layer can be parameterized in two ways. In the static router, is learned matrix Static Router. for each layer, RHr, which is the same for all tokens in batch. We apply softmax over the entries of to obtain α(r) h,m = exp(Rh,m) j=0 exp(Rh,j) (cid:80)r1 , 0 . . . 1. This approach is simple and adds minimal overhead. In the dynamic router, is computed Dynamic Router. by linear function of Xr1. For Xr1 Rtd, we define R(Xr1) := DynamicRouter(Xr1), where DynamicRouter : Rtd RtHr. Here, each token can produce its own routing matrix, making it more flexible, but adding some computational and memory costs. The pseudocode for LIMe is available in Appendix E. 5. Experiments 5.1. Language Modeling We evaluate the effectiveness of LIMe by comparing two variantsLIMe Static and LIMe Dynamicagainst two 4 You Do Not Fully Utilize Transformers Representation Capacity Figure 3. Self Retrieval weights for each head of Static and Dynamic LIMe. Both models assign higher weights to the latest representation in the middle layers, but tend to retrieve lower-level features later. The depicted weights decrease significantly in almost all heads, although some of them still use self-retrieval paths, suggesting the outputs refinement stage. Moreover, we can see that Dynamic LIMes first layers heavily rely on low-level features due to their sequence conditioning. See Section 5.2 for more details. tokens by subword or lexical patterns. Moreover, in the static router, layers 1116 distinctly favor layer 10, hinting at next-token prediction circuits that consistently re-use mid-level context. Turning to the dynamic router, we observe that middle layers, although similar to self-attention, allocate substantial fraction of their prior to representations from the first layer. The final layer places its heaviest emphasis on layers 24 instead, possibly to refine earlier morphological or syntactic cues before producing the output logits. Remarkably, Dynamic LIMes first layers hardly rely on embeddings features, in contrast to Static, due to their ability to fine-tune weights conditioned on the sequence. Expected Retrieved Representation. To quantify how each LIMe layer distributes attention across all prior representations (for notation, please refer to 4.1), we define the joint distribution p(m, r) = p(m h, r) p(h r), where the term p(h r) is obtained by normalizing the output-norm contribution of each head in the MHA output projection (i.e., normalization over the per-head slices of W(O)). Here, is the layer from which we consider the lookup of the previous layers. We then compute the expected retrieved representation as E(cid:2)mr(cid:3) = (cid:88) h=1 p(h r) r1 (cid:88) j=0 p(cid:0)j h, r(cid:1) j. (2) Figure 4 shows that in early layers, both LIMe Static and LIMe Dynamic closely track standard self-attention, as their Figure 4. Expected retrieved representation for each LIMe layer (r). Both static and dynamic variants tend to retrieve information from early layers. See Section 5.2 for more details. Figure 5. Values matrix entropy on FineWeb Edu subset by layers. Both Dynamic and Static LIMe have more diverse values than LLaMa, which indicates more information stored in LIMe. expected representation levels lie near the diagonal. However, at greater depths, these LIMe variants diverge more substantially from self-attention, indicating heavier reliance on lower-level features. This selective retrieval from earlier layers alleviates the need to preserve every relevant feature in single residual stream. By layer 16, LIMes expected retrieved representation (m) is markedly lower than that of standard self-attention (e.g., = 1), suggesting that LIMe routes nontrivial portion of attention to preceding layers rather than focusing solely on the immediately previous hidden state. This flexibility helps leverage earlier-layer nuances and mitigates representation collapse. Interpreting Dynamic Router embeddings. To shed light on the semantic roles of dynamic LIMe routers, we draw inspiration from the Logit Lens approach (Nostalgebraist, 2020). We represent each routers learned transformation as an embedding and retrieve its nearest neighbors 5 You Do Not Fully Utilize Transformers Representation Capacity (top-30) from the token embedding matrix. The decoded tokens offer clues about the semantic route that particular router emphasizesfor instance, punctuation marks, function words, or morphological fragments. Table 2 illustrates subset of these semantic circuits: for example, one circuit focuses on English suffixes (layers 4/9), another on intensifiers and comparative modifiers (layer 15), and another on subordinating conjunctions (layer 10). These findings align with our broader hypothesis that LIMes multilayer routing fosters more specialized token pathways, alleviating the pressure on later layers to maintain all salient information in single residual vector. Overall, our analysis indicates that LIMe leverages earlierlayer features in more flexible and targeted fashion compared to conventional self-attention. By explicitly allocating heads to retrieve distinct representations, LIMe not only reduces the risk of collapse but also exposes interpretable circuits that capture morphological, lexical, and syntactic patterns at varying depths. Exploring more fine-grained interpretability approaches (e.g., sparse autoencoders) remains promising direction for future research. 5.3. Representation collapse analysis Recent work has shown that large language models (LLMs) can suffer from representation collapse when representing long sequences, forcing subtle token distinctions to become inseparable in deeper layers (Voita et al., 2019; Arefin et al., 2024). We investigate this phenomenon by comparing LLaMa (Grattafiori et al., 2024), LIMe (Static), and LIMe (Dynamic) via two complementary approaches: (i) quantifying hidden-states and values diversity with matrix-based Renyi entropy (Arefin et al., 2024) and (ii) measuring and visualizing linear separability of layer-wise embeddings of closely related tokens (is, are, was, were) (Voita et al., 2019). These two methodologies directly measure representation collapse in language models; Matrix-Based Renyi Entropy. Following Arefin et al. (2024), we measure the diversity of hidden representations at layer ℓ by forming the Gram matrix = (ℓ) (ℓ) RT , where (ℓ) contains the d-dimensional representations of tokens. Let {λi(K)}T i=1 be the eigenvalues (cid:0)Z (ℓ)(cid:1) = of K. We define the α-order Renyi entropy as Sα (cid:20) (cid:80)T (cid:17)α(cid:21) (cid:16) λi(K) tr(K) i=1 . Each eigenvalue is normal1 1α log ized by tr(K), ensuring the probabilities sum to 1. Higher Sα indicates greater variance (i.e., lower collapse). Although Arefin et al. (2024) measured matrix entropy on gathered hidden states instead of values from MHA to evaluate models representation collapse, we compare LLaMa and LIMe on values representations. LIMe can store and retrieve information outside of the current representation; thats why its entropy measured on hidden states can be statistically the same as LLaMas, but LIMe can still distinguish tokens by its values. Figure 5 shows that both LIMe router parameterizations yield significantly higher matrix entropy of gathered MHA values compared to LLaMa. For visualized entropy on hidden states refer to Appendix Figure 11. Layer-Wise Token Separability. To more directly assess representation collapse, we replicate the methodology of Voita et al. (2019), extracting 1668 occurrences each of is, are, was, were from the FineWeb Edu corpus. At each layer ℓ, we record Value embeddings (i.e. the output of the linear projection of the attention values), and Hidden states (i.e. the residual stream Xℓ). We then project these representations into two-dimensional space via t-SNE and visualize how well the tokens cluster. For the same reason as in Matrix-Based Renyi Entropy, we compare LLaMa and LIMe on values visualizations in Figure 6. In LLaMa, deeper-layer representations for such similar tokens often collapse into overlapping regions, reflecting the models inclination to compress all relevant information into single representation. For hidden states clusters comparison, refer to Appendix Figure 15. Linear Classification Results. To additionally quantify these observations, we train linear four-way classifier (for is, are, was, were) on layer-wise representations. Figure 7 shows mean classification accuracies (with five-fold cross-validation) for value embeddings layer by layer. We observe that LIMe consistently exhibits higher classification accuracy than LLaMa, confirming that LIMes value representations avoid collapse. Dynamic LIMe shows better representation capacity than Static due to its conditioning on the sequence. For classification results on hidden states see Appendix Figure 12. Together, these results corroborate our theoretical motivation: by allowing each head to attend directly to earlier-layer representations, LIMe expands the overall representational capacity. This multi-layer routing reduces collapse in the values while freeing deeper hidden states from the burden of storing all lexical nuances leading to higher overall entropy on values (Figure 5) and improved model performance  (Table 1)  . 5.4. Deep networks performance Scaling Transformers to large depths often causes representation collapse, thus motivating our investigation of LIMe in 32-, 64-, and 128-layer architectures. We compare Static LIMe to the baseline LLaMA, each configured with eight attention heads per layer. To mitigate computational overhead, we apply top-p pruning strategy to the LIMe router 6 You Do Not Fully Utilize Transformers Representation Capacity Figure 6. t-SNE of similar tokens values among layers. Values obtained from LIMe are separable in later layers, while values in LLaMA become mixed and lose information about tokens. See Section 5.3 for more details. Table 2. Table with examples of tokens where semantic circuits activate in Dynamic LIMe. layer which makes the query, level of queried representation, head number. This result indicates that the model learns depthwise circuits to bypass information without change to further layer. See Section 5.2 for more details. Semantic circuit description 4 23 0 Primarily partial word segments that illustrate English morphological composition. 9 8 15 10 1 1 1 3 range of English suffixes or near-suffix fragments that highlight morphological building blocks and transformations. Primarily affixes and stems that indicate morphological processes in English. 10 23 collection of intensifiers, qualifiers, and comparative modifiers that adjust tone and degree in writing. Primarily subordinating conjunctions and discourse markers for conditions or reasons, illustrating causation, contingency, and contrast. Tokens examples lex, ache, isters, ique, ley, elling, ets, ry. ist, ised, ishing, osed, ized, ense, istic, ish, ened, inch. izing, ically, ified, ission, ational, ist, ering. very, various, respective, relatively, highly, latter, largely, particularly. Because, If, Although, While, There, According, Unlike, However, It, Even. weights at every layer, while it retains sufficient flexibility to outperform LLaMA at all tested depths (Figure 8). See Section for pruning details. 1. Shallow processing, where initial layers focus on lexical cues and local context (mostly retrieving first-layer features). Empirically, LIMe exhibits superior scaling behavior relative to LLaMA as the model depth increases, suggesting that LIMes ability to directly route information from earlier layers effectively enlarges the representational capacity, while LLaMAs single residual stream struggles to maintain fine-grained features from distinct layers. In Figure 9, one observes several distinct peaks in the average weight distribution, signifying that certain mid-level layers act as information streams reused by multiple subsequent layers. It also reveals that while early layers rely heavily on their immediate predecessors (i.e., strong selfattention to layer 1), deeper layers revisit earlier representations more frequently. Collectively, these routing patterns form three distinct stages in the networks depth: 2. Mid-layer aggregation, where representations converge around local layers, presumably capturing more abstract semantics. 3. Late-stage refinement, where the final layers selectively gather both low-level embeddings and mid-layer feature maps, often skipping the immediately preceding layers. For the full distribution of the 128-layer LIMe Router, see Figure 14. Overall, these experiments results indicate that the optimal scaling architecture is probably deeper than that of transformer models. We leave these experiments for future work. You Do Not Fully Utilize Transformers Representation Capacity Figure 7. Values classification accuracy measured with standard deviation over 5 cross-validation folds. Values in later layers obtained from LIMe can be linearly separated with nearly 1.0 accuracy, while accuracy for values from LLaMA is much lower. See Section 5.3 for more details. Figure 8. Training losses for deep architectures. The LIMe architecture significantly outperforms the baseline, especially in the case of 128 layers. See Section 5.4 for more details. 6. Conclusion and Future Work In this work, we introduced Layer-Integrated Memory (LIMe), simple yet effective modification to Multi-Head Attention that mitigates representation collapse in Transformer decoders. By allowing each attention head to form convex combinations of all previous layer outputs, LIMe addresses the fundamental constraint of finite hidden-state capacity. Empirically, this mechanism elevates the models overall representational power with negligible parameter and computational overhead. This flexible retrieval capability eases the burden on later layers, enabling them to focus on integrative decision making rather than continually preserving fine lexical or syntactic cues in single residual Figure 9. Retrieval weights statistics for 128-layer LIMe model trained with top-p pruning. The mean retrieval weight from subsequent layers (green curve) displays several distinct peaks, indicating that the model acquires multiple information streams in self-supervised fashion. The mean self-retrieval weight (orange curve), where 1.0 denotes self-attention, decreases across later layers, forming three consecutive layer groups with different information-processing patterns. See Section 5.4 for further details. stream. While LIMe demonstrates consistent gains in both performance and representational diversity, our studies also raise broader questions for future research: (1) at which width to depth ratio LIMe models scale the best, due to their observed ability to form information circuits; (2) how sparse LIMe Routers can be and still keep LIMes representational superiority. Our findings highlight that preserving some intermediate representations can unlock greater flexibility and improve language modeling performance. We hope these insights stimulate further exploration of multi-layer memory mechanisms in large-scale sequence modeling, guiding the design of more robust, interpretable, and effective Transformer architectures. 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Arefin, M. R., Subbaraj, G., Gontier, N., LeCun, Y., Rish, I., Shwartz-Ziv, R., and Pal, C. Seq-vcr: Preventing collapse You Do Not Fully Utilize Transformers Representation Capacity in intermediate transformer representations for enhanced reasoning. arXiv preprint arXiv: 2411.02344, 2024. Barbero, F., Banino, A., Kapturowski, S., Kumaran, D., Araujo, J. G. M., Vitvitskyi, A., Pascanu, R., and Velickovic, P. Transformers need glasses! information over-squashing in language tasks. arXiv preprint arXiv: 2406.04267, 2024. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. DeepSeek-AI, Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Yang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Chen, J., Yuan, J., Qiu, J., Song, J., Dong, K., Gao, K., Guan, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Pan, R., Xu, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Zheng, S., Wang, T., Pei, T., Yuan, T., Sun, T., Xiao, W. L., Zeng, W., An, W., Liu, W., Liang, W., Gao, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Chen, X., Nie, X., Sun, X., Wang, X., Liu, X., Xie, X., Yu, X., Song, X., Zhou, X., Yang, X., Lu, X., Su, X., Wu, Y., Li, Y. K., Wei, Y. X., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Zheng, Y., Zhang, Y., Xiong, Y., Zhao, Y., He, Y., Tang, Y., Piao, Y., Dong, Y., Tan, Y., Liu, Y., Wang, Y., Guo, Y., Zhu, Y., Wang, Y., Zou, Y., Zha, Y., Ma, Y., Yan, Y., You, Y., Liu, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Huang, Z., Zhang, Z., Xie, Z., Hao, Z., Shao, Z., Wen, Z., Xu, Z., Zhang, Z., Li, Z., Wang, Z., Gu, Z., Li, Z., and Xie, Z. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv: 2405.04434, 2024. Fang, Y., Cai, Y., Chen, J., Zhao, J., Tian, G., and Li, G. Cross-layer retrospective retrieving via layer attention. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/forum?id=pvgEL1yS3Ql. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., 9 Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, You Do Not Fully Utilize Transformers Representation Capacity A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. Hahn, M. and Rofin, M. Why are sensitive functions hard for transformers? In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1497315008, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.800. URL https:// aclanthology.org/2024.acl-long.800/. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. arXiv preprint arXiv: 1512.03385, 2015. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. arXiv preprint arXiv: 2310.06825, 2023. Nostalgebraist. Interpreting GPT: lens, URL https://www.alignmentforum. the logit 2020. org/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv: 2412.15115, 2024. Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and Raiko, T. Semi-supervised learning with ladder In Neural Information Processing Systems, networks. 2015. URL https://api.semanticscholar. org/CorpusID:5855183. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL https: //arxiv.org/abs/2407.08608. Srivastava, R. K., Greff, K., and Schmidhuber, J. Highway networks. arXiv preprint arXiv: 1505.00387, 2015. 10 You Do Not Fully Utilize Transformers Representation Capacity Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the In Korhonen, A., Traum, D., classical NLP pipeline. and M`arquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 45934601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1452. URL https://aclanthology.org/ P19-1452/. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. NEURIPS, 2017. Voita, E., Sennrich, R., and Titov, I. The bottom-up evolution of representations in the transformer: study with machine translation and language modeling objectives. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 43964406, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1448. URL https://aclanthology.org/D19-1448/. Yang, S., Chi, P., Chuang, Y., Lai, C. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J., Chang, X., Lin, G., Huang, T., Tseng, W., Lee, K., Liu, D., Huang, Z., Dong, S., Li, S., Watanabe, S., Mohamed, A., and Lee, H. SUPERB: speech processing universal performance benchIn Hermansky, H., Cernocky, H., Burget, L., mark. Lamel, L., Scharenborg, O., and Motlıcek, P. (eds.), 22nd Annual Conference of the International Speech Communication Association, Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021, pp. 1194 doi: 10.21437/INTERSPEECH. 1198. ISCA, 2021. 2021-1775. URL https://doi.org/10.21437/ Interspeech.2021-1775. Zhu, D., Huang, H., Huang, Z., Zeng, Y., Mao, Y., Wu, B., Min, Q., and Zhou, X. Hyper-connections. arXiv preprint arXiv: 2409.19606, 2024. 11 You Do Not Fully Utilize Transformers Representation Capacity A. Training Setup Both Static LIMe and Dynamic LIMe apply the router from the second decoder layer onward, as described in Section 4. For Dynamic LIMe, we use weight decay only on the router parameters, whereas for Static LIMe we omit weight decay on the router. We found that increasing the router learning rate to 1 102 boosts model performance by speeding up router convergence and circuit formation. We initialize the static router weights with standard normal distribution scaled by 0.1 to encourage near-uniform mixture of previous layers. In the dynamic case, we use simple linear layers without bias to produce the routing coefficients. Table 3. Key training hyperparameters used in all experiments. Hyperparameter Optimizer Learning Rate LIMe Router Learning Rate Weight Decay β1 β2 ϵ Scheduler Warmup Steps Min LR Mixed Precision Gradient Clipping Sequence Length Batch Size Training Steps Value AdamW 0.001 0.01 0.1 0.9 0.95 1 108 cosine 200 1 106 bf16 1.0 2048 1024 20,000 Table 4. Model architecture for all variants (LLaMa, Hyper Connections, and LIMe) at the 1.2B scale. Parameter Vocab Size Hidden Size Intermediate Size Number of Hidden Layers Number of Attention Heads Tie Word Embeddings Value 50,257 2048 8192 16 32 True B. Pruning details After softmax-normalizing the router weights, we sort them in descending order along the layer dimension and select the minimal subset of weights whose cumulative sum exceeds = 0.9; the remaining weights are interpreted as zero weights. Table 5 summarizes the number of pruned router weights at = 0.9 for different models depths. Table 5. Number of pruned LIMe Routers weights at top-p = 0.9 for various model depths. As we can see in deep models training results (5.4), retrieval paths pruning does not stop LIMe from being superior compared to LLaMA. Model Depth Total Router Weights Pruned Weights (%) 32-layer 64-layer 128-layer 7,936 32,256 130,048 12 1,845 (23%) 6,795 (21%) 24,632 (19%) You Do Not Fully Utilize Transformers Representation Capacity C. Additional LIMe Analysis Router weights, both static and dynamic, suggest that the model reuses previous hidden states, especially in later layers. As shown in Figure 13, early layers prioritize the most recent representation (high diagonal values), resembling standard self-attention. However, from layer 12 onward, this pattern shifts, with lower weights assigned to recent states in favor of earlier ones. Figure 10 confirms this trend, showing higher weights on the latest hidden state in early layers. Figure 3 further illustrates that while most heads follow this pattern, some in later layers resemble self-attention, likely to update retrieved representations. Figures 6 and 15 show that LIMe layers yield more separable value states than LLaMA, while hidden states are mixed. Despite collapsed hidden states in both models, the LIMe model suffers from this to lesser extent, as it attends to past representations as well. Figures 11 and 5 suggest this applies to entire contexts, albeit less distinctly. Scaling LIMe to 128 layers reinforces these retrieval patterns. As seen in Figure 14, the first 80 layers mainly perform self-attention (strong diagonal weights), while later layers distribute attention across earlier representations. Distinct vertical stripes indicate that certain representations remain consistently attended to across layers. Figure 10. Self Retrieval weights averaged across heads for each LIMe layer. 13 You Do Not Fully Utilize Transformers Representation Capacity Figure 11. Hiddens matrix entropy on FineWeb Edu subset by layers. We can see that hidden states in LIMe can be not very diverse for the model to provide better performance on language tasks. For details, see Section 5.3. Figure 12. Hidden states classification accuracy measured with standard deviation over 5 cross-validation folds. Although LLaMas deeper layers maintain stronger linear separability, LIMes hidden states become slightly harder to cluster in later layers due to its ability to smoothly move on to predicting the next token using the full hidden states dimensionality. 14 You Do Not Fully Utilize Transformers Representation Capacity Figure 13. Learned static weights and dynamic prior distribution calculated on subset of Fineweb Edu. Each cell represents retrieval probability for each layer in the specific head. Figure 14. All weights for deep static LIMe with 128 layers. We can see explicitly the repeated routing patterns resembling refinement process. You Do Not Fully Utilize Transformers Representation Capacity Figure 15. t-SNE of similar tokens hidden states among layers. Although hidden states are not separable in later layers for both models, unlike LLaMA, LIMe can make updates attending to the previous representations, which leads to high values separability. See Section 5.3 for more details. Table 6. Comparing efficiency for all 1.2B models: both Dynamic and Static LIMe enjoy negligible parameter and FLOPs increase, and smaller peak memory than HC during training. When the Key-Value cache is utilized, this memory advantage extends to inference as well (*). number of heads, number of layers, sequence length, hidden dimension, Hyper Connections (Zhu et al., 2024) expansion rate. Model # Parameters (B) FLOPs (T) Peak Memory Overhead over LLaMA excluding parameters Train Inference LLaMA LIMe Static LIMe Dynamic HC Dynamic 1.1767 1.1768 (+0.008%) 1.1856 (+0.075%) 1.1771 (+0.030%) D. Efficiency 2.97 2.98 (+0.3%) 3.01 (+1.3%) BT 2.98 (+0.3%) 0 (L 1)BT HD (cid:17) 2 1 (cid:16) L(L+1) + (L 1)BT HD 2LBT [(R 1)D + R(R + 2))] 0 BT HD () LBT + BT HD () BT [(R 1)D + R(R + 2))] We evaluate LIMe in terms of parameter usage, memory overhead, and computational cost in comparison to LLaMA (Grattafiori et al., 2024) and Hyper-connections (HC) (Zhu et al., 2024). As shown in Table 6, LIMe adds only marginal number of extra parameters and forward FLOPs relative to LLaMA, while also avoiding the substantial memory overhead incurred by approaches that maintain multiple copies of residual streams (e.g., HC). In particular, during training, LIMe can reuse the intermediate layer outputs that are already stored for backpropagation, thereby imposing negligible extra cost. During inference, memory usage remains minimal as all the required tensors are already stored in the key-value cache. Computational overhead can be further reduced by decreasing the number of activated routes to several dozen, retaining only the most critical circuits (e.g., through top-p pruning or ℓ0 regularization during training). These optimizations can be efficiently implemented using Triton/CUDA kernels. 16 You Do Not Fully Utilize Transformers Representation Capacity E. LIMe Pseudocode 1 class StaticRouter(nn.Module): 2 def __init__(self, num_heads, n_repr, init_coef): 3 4 5 6 7 self.weights = nn.Parameter(torch.randn(num_heads, n_repr) * init_coef) def forward(self, stacked_hiddens): # stacked_hiddens: [n_repr, batch, time, d] probs = self.weights.softmax(dim=-1) return torch.einsum(rl,lbtd->brtd, probs, stacked_hiddens) 8 9 class DynamicRouter(nn.Module): 10 def __init__(self, hidden_size, num_heads, n_repr): 12 13 14 15 16 18 19 self.proj = nn.Sequential( nn.Linear(hidden_size, num_heads * n_repr, bias=False), Reshape(batch, time, num_heads, n_repr) ) def forward(self, stacked_hiddens): last = stacked_hiddens[-1] probs = self.proj(last).softmax(dim=-1) return torch.einsum(\"btrl,lbtd->brtd\", probs, stacked_hiddens) # [batch, time, hidden_size] # stacked_hiddens: [n_repr, batch, time, d] 20 21 class LIMeAttentionProjection(nn.Module): 22 def __init__(self, num_heads, hidden_size, head_dim): 23 25 26 27 self.proj = nn.Parameter(torch.empty(1, num_heads, hidden_size, head_dim)) nn.init.kaiming_uniform_(self.proj, a=math.sqrt(5)) def forward(self, x): # x: [batch, heads, time, hidden_size] return @ self.proj 28 29 class LIMeSdpaAttention(BaseSelfAttention): 30 def __init__(self, hidden_size, num_heads, head_dim): 31 32 34 35 36 37 38 40 41 self.k_proj_lime = LIMeAttentionProjection(num_heads, hidden_size, head_dim) self.v_proj_lime = LIMeAttentionProjection(num_heads, hidden_size, head_dim) self.q_proj_default = nn.Linear(hidden_size, num_heads * head_dim) def forward(self, last_hidden, lime_representations, **kwargs): key_states = self.k_proj_lime(lime_representations) value_states = self.v_proj_lime(lime_representations) query_states = self.q_proj_default(last_hidden).permute(batch, num_heads, time, head_dim) query_states, key_states = apply_rotary_pos_emb(query_states, key_states, **kwargs) attn_output = scaled_dot_product_attention(query_states, key_states, value_states, **kwargs) return self.o_proj(attn_output.transpose(1, 2).reshape(batch, time, -1)) 42 43 class LIMeLayer(BaseDecoderLayer): def __init__(self, hidden_size, lime_config, layer_idx, num_heads, head_dim): 45 46 47 48 50 51 52 53 54 56 57 58 59 router_cls = DynamicRouter if lime_config.dynamic else StaticRouter self.attention_router = router_cls(**lime_config, n_repr=layer_idx) self.self_attn = LIMeSdpaAttention(hidden_size, num_heads, head_dim) self.lime_layernorm = LlamaRMSNorm(hidden_size) def forward(self, last_hidden, past_hiddens, **kwargs): residual = last_hidden last_hidden = self.input_layernorm(last_hidden) lime_representations = self.attention_router(self.lime_layernorm(past_hiddens)) attn_out = self.self_attn(last_hidden, lime_representations, **kwargs) last_hidden = residual + attn_out # ... standard LLaMA MLP block ... return last_hidden"
        }
    ],
    "affiliations": [
        "HSE University",
        "Moscow Institute of Physics and Technology",
        "T-Tech"
    ]
}