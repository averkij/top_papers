{
    "paper_title": "DiffuEraser: A Diffusion Model for Video Inpainting",
    "authors": [
        "Xiaowen Li",
        "Haolan Xue",
        "Peiran Ren",
        "Liefeng Bo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as a prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, a video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning,which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency."
        },
        {
            "title": "Start",
            "content": "DiffuEraser: Diffusion Model for Video Inpainting TECHNICAL REPORT"
        },
        {
            "title": "Haolan Xue",
            "content": "Peiran Ren Tongyi Lab, Alibaba Group {lxw262398, haolan.xhl, peiran.rpr, liefeng.bo}@alibaba-inc.com https://github.com/lixiaowen-xw/DiffuEraser.git"
        },
        {
            "title": "Liefeng Bo",
            "content": "5 2 0 2 7 1 ] . [ 1 8 1 0 0 1 . 1 0 5 2 : r Figure 1. Performance comparison between the proposed model, DiffuEraser, and Propainter. (a) Texture Quality: DiffuEraser generates more detailed and refined textures compared to the transformer-based Propainter. (b) Temporal Consistency: DiffuEraser demonstrates superior temporal consistency in the inpainted content compared to Propainter."
        },
        {
            "title": "Abstract",
            "content": "Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning, which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms stateof-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency. 1. Introduction Video inpainting aims to complete masked regions with content that is both plausible and temporally consistent. Previous video inpainting algorithms primarily rely on two mechanisms: 1) Flow-based pixel propagation methods, which utilize optical flow to restore texture details and objects by leveraging information from adjacent frames; and 2) Transformer-based video inpainting methods, which excel at completing the structural aspects of objects [26]."
        },
        {
            "title": "Current mainstream algorithms typically combine these",
            "content": "two approaches, consisting of three modules or stages: 1) Flow completion, 2) Feature propagation, and 3) Content generation. This solution categorizes masked pixels into two types: 1) Known pixels, which have appeared in some masked frames and can be propagated to other frames through flow completion and feature propagation modules, ensuring consistency between the completed content and the unmasked regions; and 2) Unknown pixels, which have never appeared in any masked frames and are generated by the content generation module, thereby enhancing the structural integrity of the results. The state-of-the-art algorithm, Propainter [46], exemplifies this approach and comprises three key modules: recurrent flow completion, dual-domain propagation, and mask-guided sparse Transformer. It effectively propagates known pixels across all frames and demonstrates an initial ability to generate unknown pixels. However, when the mask size is large, the generative capability of the Transformer model proves insufficient, leading to significant artifacts, as illustrated in Figure 1. Consequently, there is need for more powerful models with enhanced generative capabilities. The Stable Diffusion model, which has recently gained prominence in the field of image and video generation, presents promising candidate. In this work, we first decompose the video inpainting task into three sub-problems and then propose corresponding solutions for each. Specifically, the three key challenges are: the propagation of known pixels, the generation of unknown pixels, and the temporal consistency of the completed content. Our main contributions are summarized as follows: 1. Video Inpainting Diffusion: We introduce motion module for the image inpainting model BrushNet, which is based on diffusion models. The powerful generative capability of diffusion models overcomes the blurring and mosaic artifacts associated with Transformer-based models, thereby completing object structures and generating more detailed content. 2. Injected Priors: We incorporate priors into the diffusion model, enabling easier initialization to mitigate noisy artifacts and serving as weak condition to suppress the generation of unwanted objects. 3. Enhanced Temporal Consistency: We improve the temporal consistency of long-sequence inference by expanding the temporal receptive fields of both the prior model and the diffusion model. Additionally, we further enhance temporal continuity at the intersections between clips by leveraging the temporal smoothing property of the Video Diffusion Model. 2. Related Works Diffusion Models. The advent of diffusion models [14, 32, 34] has significantly enhanced the quality and creativity of image and video generation. In the realm of image synthesis, diffusion models have driven substantial progress including text-to-image generation across various tasks, [5, 29], controllable image generation [24, 43], image editing [1, 12, 22], personalized image generation [6, 28], and image inpainting [27, 16], among others. Building on these advancements, video diffusion models incorporating additional motion modules have also gained significant traction. Key applications in this domain include text-to-video generation [11, 8, 10, 13, 15, 31], controllable video generation [3, 4, 36, 39], video editing [19, 23, 38, 21], and various training-free video synthesis methods [44, 25]. Video Inpainting. Video inpainting aims to fill masked regions in videos with plausible content while maintaining temporal consistency. Early approaches based on 3D convolution and shifting operations exhibited limited performance. The emergence of methods leveraging optical flow and Transformer architectures has significantly improved the quality of video inpainting. Flow-based pixel propagation methods [7, 41, 42] excel at restoring textures and details by utilizing information from adjacent frames. In contrast, Transformer-based methods [40, 20, 18, 46] are adept at completing the structural aspects of objects. Among these, Propainter [46] stands out as representative approach, comprising recurrent flow completion, dualdomain propagation, and mask-guided sparse Transformer. Propainter effectively propagates known pixels across all frames and demonstrates an initial ability to generate unknown pixels. However, its generative capacity is limited when dealing with large masks, leading to noticeable artifacts. With the rising popularity of diffusion models, diffusionbased video inpainting methods have begun to emerge [17, 37, 30, 9, 45, 47]. These approaches leverage the powerful generative capabilities of diffusion models to enhance both the detail and structural integrity of the inpainted regions, addressing some of the limitations observed in Transformerbased methods. BIVDiff[30] is training-free framework via bridging image and video diffusion models. AVID[45] Figure 2. Overview of the proposed video inpainting model DiffuEraser, based on stable diffusion. The main denoising UNet performs the denoising process to generate the final output. The BrushNet branch extracts features from masked images, which are added to the main denoising UNet layer by layer after zero convolution block. Temporal attention is incorporated after self-attention and cross-attention to improve temporal consistency. and CoCoCo[47] improved text-guided video inpainting by integrating motion module to Text-to-Image(T2I) model. [37] proposes language-driven video inpainting via Multimodal Large Language Models, which uses natural language instructions to guide the inpainting process. Nevertheless, they always suffer from the inherent hallucinations of diffusion models. FloED[9] with less hallucination proposes dedicated dual-branch architecture that incorporates motion guidance with multi-scale flow adapter to enhance temporal consistency, focusing on object removal and background restoration. FFF-VDI[17] propagates the noise latent information of future frames to fill the masked area of the first frames noise latent code, improving temporal consistency and suppressing hallucination effects. However, these methods do not effectively address the temporal consistency and stability needed for long-sequence inference and there is still room for improvement in detail and structural integrity. In contrast, DiffuEraser can generate temporally consistent results with enhanced detail and more complete structure for long-sequence inference, all without requiring text prompt. 3. Methodology 3.1. Network Overview Our network architecture is inspired by AnimateDiff [11], integrating motion module into the image inpainting model. For the image inpainting component, we select BrushNet [16], which enhances the main denoising UNet by adding an additional branch to extract features from masked images. An overview of our proposed model, DiffuEraser, is depicted in Figure 2. The architecture comprises the primary denoising UNet and an auxiliary BrushNet. The BrushNet branch receives conditional latent input composed of masked images, masks, and noisy latents, with dimensions [n, f, h/4, w/4, 9]. Features extracted by BrushNet are integrated into the denoising UNet layer by layer after zero convolution block. The denoising UNet processes noisy latents with dimensions [n, f, h/4, w/4, 4]. To enhance temporal consistency, temporal attention mechanisms are incorporated following both self-attention and crossattention layers. After denoising, the generated images are blended with the input masked images using blurred masks. We define the video inpainting problem by decomposing it into three sub-problems: propagation of known pixels (pixels that have appeared in some masked frames), generation of unknown pixels (pixels that have never appeared in any masked frames), and maintaining temporal consistency of the completed content. Specifically: 1. Propagation of Known Pixels: The motion module inherently supports temporal propagation, allowing the restoration of texture details and objects in the current frame using information from adjacent frames. Additionally, we leverage the enhanced propagation capabilities of the prior model, which offers longer propagation range and more sophisticated propagation mechanism. Specifically, we apply DDIM inversion on the inpainting results from the prior model and incorporate them into the noisy latent. See Section 3.2 for details. We utilize Propainter as our prior model. Beyond supporting the propagation of known pixels, the injected prior facilitates easier initialization for DiffuEraser, enabling the generation of meaningful completed content and suppressing noisy artifacts and visual hallucinations commonly associated with diffusion models. 2. Generation of Unknown Pixels: Utilizing the robust generative capabilities of the stable diffusion model, our approach can generate plausible content with more details and textures for unknown pixels. 3. Temporal Consistency of Completed Content: While the motion module ensures temporal consistency within individual inferences (each handling clip of 22 frames in our setting), discrepancies arise at the boundaries between clips during long-sequence processing. To address this, we expand the temporal receptive field of the model. This is achieved by performing pre-inference, where video frames are sampled at an optimal rate and processed collectively as single clip. This enables the model to see frames from broader temporal context. Subsequently, the insights gained from pre-inference are used to guide the frame-by-frame inference, incorporating information from distant frames and thereby enhancing the overall temporal continuity. See Section 3.3 for details. As demonstrated in other studies, the generative capability of stable diffusion models and the temporal consistency provided by motion modules are well-established. In this paper, we focus on illustrating the advantages of incorporating priors and optimizing temporal consistency across clips during long-sequence inference. 3.2. Incorporation of Priors As illustrated in Figure 3, our model occasionally generates meaningless noisy artifacts within masked regions. For instance, the masked area above the sea level may appear as random noise instead of coherent content. inverted results into the noisy latent, as depicted in Figure 4. The prior provides initialization information that enables the model to generate meaningful and stable completed content, effectively eliminating the noisy artifacts shown in Figure 3. Additionally, the prior acts as weak condition to suppress the generation of unwanted objects, mitigating visual hallucinations often encountered in diffusion models. Figure 4. Incorporation of priors. We introduce priors during inference by performing DDIM inversion on the outputs of the prior model and adding them to the noisy latent. The selection of the prior model significantly impacts the final results. After experimental comparisons, we selected Propainter as our prior model. Notably, any blur and mosaic artifacts present in the prior do not adversely affect our models outputs; instead, they are refined and eliminated, resulting in inpainted regions with richer textures and greater detail. Figure 5 compares the results before and after incorporating priors, demonstrating that the introduction of priors effectively suppresses noisy artifacts and the emergence of unwanted objects, thereby significantly enhancing the accuracy and stability of the inpainting results. Figure 3. Example of noisy artifacts generated by the model. The masked region above the sea level is not completed correctly and resembles random noise. To address these artifacts, we enhance the noisy latentan integral part of the models input. Inspired by DDIM Inversion [33], we introduce priors during inference. Specifically, we perform DDIM Inversion on the outputs of chosen lightweight inpainting model and incorporate the Figure 5. Comparison of inpainting results before and after incorporating priors. Figure 6. Utilizing the temporal smoothing property of the Video Diffusion Model (VDM) to enhance consistency at the intersections of clips. 3.3. Optimizing Temporal Consistency for Longinherent inconsistencies between the first and last frames."
        },
        {
            "title": "Sequence Inference",
            "content": "While the motion module maintains good temporal consistency within individual clips(for example, 22 frames), noticeable discrepancies emerge at the boundaries between consecutive clips during long-sequence inference, as shown in Figure 7. To ensure seamless temporal consistency across the entire video, we implement the following optimizations. 3.3.1 Leveraging the Temporal Smoothing Property of the Video Diffusion Model (VDM) The absence of specific temporal conditioning leads to significant changes in completed content between clips, problem that cannot be resolved by merely overlapping Inspired by the concept of interpolatneighboring clips. ing between timesteps to obtain intermediate results [9], we adopt staggered denoising approach along sequential timesteps. This method utilizes the inherent temporal smoothing property of VDM to enhance consistency between clips. During inference, even-numbered timesteps remain inferred from the starting position of the clip, while oddnumbered timesteps are inferred from the midpoint of the clip, Figure 6. This staggered denoising leverages VDMs temporal smoothing property to blend frames at clip intersections smoothly. The underlying rationale is that, despite identical latent inputs, the denoising results for overlapped frames from adjacent clips differ due to VDMs temporal smoothing property, which adjusts overlapped frames to be temporally consistent with the starting frame. By applying this smoothing property at clip intersections, we achieve more seamless transitions. When processing long videos divided into multiple clips, preliminary optimizations lead to multiple adjustments at clip intersections. After optimization, these transitions are smoothed into single gradual change from the first to the last frame of the entire video. However, complete consistency across the entire video remains unattainable due to Figure 7. Temporal consistency optimization for long-sequence inference. 3.3.2 Expanding the Temporal Receptive Field single inference pass can process only limited number of frames(for instance, 22 frames in our setting), which restricts the temporal receptive field and prevents the propagation of known pixels from distant frames. Additionally, information sharing between different clips is constrained, resulting in inconsistencies in detailed content despite similar semantics across clips. This leads to frequent and noticeable changes during long-sequence inference, as illustrated in Figure 7. To mitigate this, we expand the temporal receptive field of the inference process through the following two strategies. 1. Enhancing Priors for Comprehensive Pixel Propagation Using Propainter as an example, we first sample the input video frames and perform pre-propagation to extend known pixels across the entire time domain, surpassing the temporal limitations of single propagation pass (which typically handles dozens of frames), as shown in Figure 8(a). Full propagation of known pixels ensures that the completed content remains consistent with the unmasked regions, thereby stabilizing the results. Subsequently, the inpainting results of the sampled frames guide frame-by-frame propagation, allowing the information obtained from pre-propagation to be integrated into every frame, as depicted in Figure 9(a). This optimization enables Propainter to utilize information from distant frames more effectively, ensuring that known pixels are stably propagated across the entire time domain. Consequently, the prior provided to DiffuEraser is more accurate and stable. Nonetheless, DiffuErasers limited temporal receptive field still results in significant changes at clip intersections. Figure 9. The temporal consistency obtained from pre-propagation or pre-inference is maintained throughout all remaining frames. sues inherent in long-sequence inference, as demonstrated in Figure 7. 4. Experiments Datasets. We utilized the Panda-70M dataset [2], splitting videos at scene cuts and filtering them based on matching scores to obtain 3,183,727 short video clips paired with captions. During training, we generated mask sequences with random rates, directions, and shapes to simulate video inpainting and object removal tasks. Training Details and Metrics. We employed twostage training strategy with resolution of 512. In the first stage, we trained the BrushNet and the main denoising UNet without the motion module to enhance content generation capabilities. In the second stage, we trained the motion module of the main denoising UNet to improve temporal consistency. The fist stage is trained on 4 NVIDIA A100 GPUs for 100,000 steps with batch size of 16, and the second stage is trained on 8 NVIDIA A100 GPUs for 80,000 steps with 22-frame video sequences and batch size of 1. Both models were optimized using the L2 loss function and learning rate of 1e-5. Efficiency. Leveraging Phased Consistency Models (PCM) [35], our model can generate samples in only two steps, significantly improving inference efficiency. For instance, processing 10-second video at 540p and 25 FPS using Nvidia GPU L20 requires about 200 seconds. Qualitative Comparison. Figure 1 illustrates comparison between our model and Propainter both in texture Figure 8. Perform pre-propagation or pre-inference to expand the temporal receptive field of model. 2. Expanding the Temporal Receptive Field of DiffuEraser for consistent generation of unknown pixels To further enhance temporal consistency, we also expand the temporal receptive field of DiffuEraser. Similar to the prior optimization, we introduce pre-inference step where video frames are sampled and processed as single inference pass, thereby broadening the temporal context and ensuring consistent content generation across the entire video, as shown in Figure 8(b). Following pre-inference, the results guide frame-byframe inference, ensuring that the content consistency established during pre-inference is maintained throughout all remaining frames, as illustrated in Figure 9(b). The core principle behind these optimizationsboth for priors and DiffuEraseris to extend the temporal receptive field to encompass the entire video duration, rather than being confined to individual clips. The optimization of prior ensures comprehensive propagation of known pixels, maintaining result correctness, while the optimization of DiffuEraser focuses on the consistent generation of unknown pixels, ensuring overall stability. Together, these enhancements effectively resolve the temporal consistency isquality and temporal consistency. For more comparison results, see Figure 10,11,12,13. Our model effectively propagates known pixelsthose that appear in some masked framesto all frames, while also generating unknown pixelsthose that never appear in any masked frameswith high consistency and stability. 5. Conclusion and Discussion In this paper, we introduce DiffuEraser, video inpainting model based on stable diffusion. We address the video inpainting task by decomposing it into three subproblems: propagation of known pixels (pixels appearing in some masked frames), generation of unknown pixels (pixels never appearing in any masked frames), and maintaining temporal consistency of the completed content. For each sub-problem, we propose tailored solutions. For the generation of unknown pixels, the powerful generative capabilities of the stable diffusion model help DiffuEraser effectively overcome the blurring and mosaic issues prevalent in Transformer-based models. Additionally, we mitigate the inherent hallucinations of stable diffusion models by incorporating priors, ensuring more accurate and realistic inpainting results. In terms of propagating known pixels, the motion module within the denoising UNet, combined with the enhanced propagation properties provided by priors, ensures the sufficient and consistent propagation of known pixels across frames. This prevents conflicts between the completed content and the unmasked regions, thereby improving the correctness and stability of the results. To address temporal inconsistencies between clips for long-sequence inference, we expand the temporal receptive field for both prior model and DiffuEraser, significantly enhancing the consistency of completed content across all frames. Furthermore, we leverage the temporal smoothing property of VDM to further enhance temporal coherence at the intersections between clips. The concepts of incorporating priors and the methods to improve temporal consistency for long-sequence inference are also applicable to variety of other video editing tasks, such as object replacement and local stylization. These applications will be further explored in future works. Experimental results demonstrate that DiffuEraser outperforms state-of-the-art methods in both content completeness and temporal consistency, establishing it as superior approach for video inpainting tasks."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. [2] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos In Proceedings of with multiple cross-modality teachers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [3] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023. [4] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models, 2023. [5] Aditya Ramesh et al. Hierarchical text-conditional image generation with clip latents, 2022. [6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. [7] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. In Proc. European Flow-edge guided video completion. Conference on Computer Vision (ECCV), 2020. [8] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models, 2024. [9] Bohai Gu, Hao Luo, Song Guo, and Peiran Dong. Advanced video inpainting using optical flow-guided efficient diffusion. arXiv preprint arXiv:2412.00857, 2024. [10] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, and Hang Xu. Reuse and diffuse: Iterative denoising for text-to-video generation. [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. [16] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion, 2024. Figure 10. Texture quality comparison between DiffuEraser and Propainter. Figure 11. Texture quality comparison between DiffuEraser and Propainter. Figure 12. Temporal consistency comparison between DiffuEraser and Propainter. Figure 13. Temporal consistency comparison between DiffuEraser and Propainter. [17] Minhyeok Lee, Suhwan Cho, Chajin Shin, Jungho Lee, Sunghun Yang, and Sangyoun Lee. Video diffusion models are strong video inpainter, 2024. [18] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. Towards an end-to-end framework for flow-guided video inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [19] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing. In arXiv, 2023. [20] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fuseformer: Fusing fine-grained information in transformers for video inpainting. In International Conference on Computer Vision (ICCV), 2021. [21] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control, 2023. [22] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real arXiv preprint images using guided diffusion models. arXiv:2211.09794, 2022. [23] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors, 2023. [24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. [25] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. [26] Weize Quan, Jiaxi Chen, Yanli Liu, Dong-Ming Yan, and Peter Wonka. Deep learning-based image and video inpainting: survey, 2024. [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In arXiv preprint arxiv:2208.12242, 2022. [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. [30] Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, and Limin Wang. Bivdiff: training-free framework for general-purpose video synthesis via bridging image and In Proceedings of the IEEE/CVF video diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), pages 73937402, June 2024. [31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. [32] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. [33] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, October 2020. [34] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. [35] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. Phased consistency model. [36] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability, 2023. [37] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video arXiv inpainting via multimodal large language models. preprint arXiv:2401.10226, 2024. [38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [39] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textual and structural guidance. arXiv preprint arXiv:2306.00943, 2023. [40] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for video inpainting. In The Proceedings of the European Conference on Computer Vision (ECCV), 2020. [41] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided In European Conference transformer for video inpainting. on Computer Vision, pages 7490. Springer, 2022. [42] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided flow completion and style fusion for video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 59825991, June 2022. [43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [44] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Controlvideo: Zhang, Wangmeng Zuo, and Qi Tian. Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. [45] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. arXiv preprint arXiv:2312.03816, 2023. [46] Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. ProPainter: Improving propagation and In Proceedings of IEEE transformer for video inpainting. International Conference on Computer Vision (ICCV), 2023. [47] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. ArXiv, abs/2403.12035, 2024."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}