{
    "paper_title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
    "authors": [
        "Jiazi Bu",
        "Pengyang Ling",
        "Pan Zhang",
        "Tong Wu",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 2 1 4 2 6 0 . 0 1 4 2 : r BROADWAY: BOOST YOUR TEXT-TO-VIDEO GENERATION MODEL IN TRAINING-FREE WAY Jiazi Bu1,4 Pengyang Ling2,4 Pan Zhang4 Tong Wu3 Xiaoyi Dong4 Yuhang Zang4 Yuhang Cao4 Dahua Lin3,4 Jiaqi Wang4 1Shanghai Jiao Tong University 2University of Science and Technology of China 3The Chinese University of Hong Kong https://github.com/Bujiazi/BroadWay/ 4Shanghai Artificial Intelligence Laboratory Figure 1: Given diffusion based text-to-video (T2V) backbone, BroadWay can improve its synthesis quality in training-free and plug-and-play manner, enhancing both the temporal consistency and the motion magnitude in the sampled results."
        },
        {
            "title": "ABSTRACT",
            "content": "The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and lack of motion, often resulting in near-static video. In this work, we have identified correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, trainingfree method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance Equal Contribution. Corresponding Author. 1 improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the field has observed substantial progress in the evolution of diffusion-based models specifically dedicated to video generation tasks, notably in text-to-video synthesis (Khachatryan et al., 2023; Blattmann et al., 2023b; Guo et al., 2023b; Chen et al., 2024). Despite these advancements, the practical applicability of generated videos remains limited due to inadequate quality. This suboptimal performance is characterized by two predominant issues: firstly, portion of the generated videos exhibit structurally implausible and temporally inconsistent artifacts, and secondly, another subset of the generated videos demonstrates markedly restricted motion, bordering on the static nature of still image. Prior methodologies have primarily concentrated on enhancing video generation quality through advances in training mechanisms, such as improving the quality of training data (Blattmann et al., 2023a), scaling training data (Wang et al., 2024b), refining model architecture (Hong et al., 2022) and training strategies (Chen et al., 2024). However, these approaches often entail substantial costs. This work endeavors to improve video generation quality in the inference phase, specifically in the realm of text-to-video generation, without necessitating training, introducing additional parameters, augmenting memory or sampling time. In current video generation models, an encoder-decoder architecture (Ronneberger et al., 2015) is typically utilized, wherein the decoder is comprised of multiple blocks. Each block integrates several temporal attention modules (Guo et al., 2023b), facilitating the modeling of motion within the generated videos. We have two observations about the temporal attention module. The first is correlation between artifact presence and the inter-block divergence of temporal attention maps. Specifically, video generation processes exhibiting structurally implausible and temporally inconsistent artifacts demonstrate greater disparity between the temporal attention maps of different decoder blocks. Conversely, processes devoid of such evident artifacts exhibit reduced disparity among these maps, as illustrated in Fig. 3(a). The second is correlation between the amplitude of motion in generated videos and the energy of the corresponding temporal attention maps, defined in the method section. Specifically, videos that exhibit higher degree of motion amplitude and richer variety of motion patterns are observed to possess greater energy within their temporal attention maps, as illustrated in Fig. 2. Figure 2: Generated videos with richer motion typically exhibit higher energy. Based on the observations, we present BroadWay, training-free approach with negligible additional cost to improve the generation quality of T2V diffusion models. BroadWay is composed of two principal components: Temporal Self-Guidance and Fourier-based Motion Enhancement, both meticulously engineered to refine the temporal attention module within T2V models. Temporal SelfGuidance leverages the temporal attention map from the preceding block to inform and regulate that of the current block. This approach effectively mitigates the disparity between the temporal attention maps across various decoder blocks, thereby normalizing their disparity. As result, videos that initially exhibit structural implausibility and temporal inconsistency, significantly reduce such artifacts through the application of Temporal Self-Guidance, as shown in the first and second rows in Fig. 1. Furthermore, Fourier-based Motion Enhancement modulates the high-frequency components of the temporal attention map, thereby amplifying the energy of the map, as detailed in the methodology section. This enhancement circumvents the generation of videos that closely resemble static image. With the Fourier-based Motion Enhancement, videos that were previously characterized by minimal 2 Figure 3: MSE distance between temporal attention maps of different levels in UNet. motion exhibit an increased amplitude and more diverse range of motion patterns, as illustrated in the third and last rows in Fig. 1. We evaluate the performance of BroadWay on various popular T2V backbones, including those with additional motion modules trained from frozen T2I models and those trained end-to-end directly for T2V tasks. Our experiments show promising results, demonstrating the effectiveness and strong adaptability of BroadWay. Furthermore, additional experiments reveal that BroadWay also exhibits potential in the image-to-video (I2V) domain, further expanding the applicability of BroadWay across various video generation tasks. Our contributions are summarized: (1) We conduct deeper analysis of the temporal attention module widely adopted in current T2V backbones, and observe two correlations between the generated videos and corresponding temporal attention maps. (2) We propose BroadWay, which significantly improves the quality of text-to-video generation without necessitating training, introducing additional parameters, augmenting memory or sampling time. (3) BroadWay can be seamlessly integrated with various mainstream open-source T2V backbones like AnimateDiff and VideoCrafter2, demonstrating strong applicability and extensibility."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 TEXT-TO-VIDEO DIFFUSION MODELS Given textual prompt, text-to-video (T2V) diffusion models (Singer et al., 2022; Hong et al., 2022; Wang et al., 2023b; Chen et al., 2023a; Wang et al., 2023a; 2024a; Khachatryan et al., 2023) aim to synthesize image sequences that maintain both temporal consistency and textual alignment. Unlike text-to-image (Ding et al., 2021; Zeqiang et al., 2023; Saharia et al., 2022; Podell et al., 2023) that emphasizes perfecting individual images, T2V poses heightened challenge of maintaining both visual aesthetics for each frame and the realistic motion between frames. To this end, most approaches incorporate extra motion modeling modules into existing image diffusion architecture, leveraging the underlying image priors. For instance, AnimateDiff (Guo et al., 2023b) introduced trainable temporal attention layers to frozen text-to-image models to effectively capture the frameto-frame correlations Some works (Blattmann et al., 2023b; Chen et al., 2024) combined temporal convolution modules and temporal attention layers for modeling short/long range dependencies. To alleviate motion synthesis difficulty, Ge et al. (Ge et al., 2023) suggested employing temporally related noise to enhance temporally consistent. Nevertheless, due to the scarcity of high-quality video data and the intricacies of motion synthesis, the current available T2V models still struggle to harmonize motion strength with motion consistency. This work identifies that the consistency across temporal attention blocks indicates the continuity of synthesized video sequences while the energy within the temporal attention maps dominates the magnitude of motion, and thus proposes training-free strategy to unlock the potential of exiting T2V models by encouraging uniform motion modeling and enhanced frequency energy. 2.2 DIFFUSION FEATURE CONTROL Controlling targeted diffusion features to manipulate specific attributes has been demonstrated to be an effective strategy in the realm of image and video synthesis (Chefer et al., 2023; Kim et al., 2023b; Xiao et al., 2023; Liu et al., 2023; Qi et al., 2023),. Prompt2Prompt (Hertz et al., 2022) re3 vealed that the cross attention maps domain the image layout. DSG (Yang et al., 2024) proposed that spatial means of diffusion features represent the appearance, which offers simple approach for image property manipulation, such as size, shape, and location. FreeControl (Mo et al., 2023) suggested to perform image structure guidance by aligning the PCA features with given reference image in spatial self-attention block, providing versatile counterpart of ControlNet (Zhang et al., 2023). DIFT (Tang et al., 2023) observed that the semantic corresponding can be directly extracted by spatially measuring the difference between diffusion feature. MotionClone (Ling et al., 2024) demonstrates the sparse control of temporal attention maps facilitates training-free motion transfer, enabling reference-based video generation. FreeU (Si et al., 2024) suggested re-weighting the contribution of skip features and backbone features by using spectral modulation and structure-related scaling, promoting the emphasis on backbone semantics. In this work, we propose Temporal Self-Guidance to facilitates uniform motion modeling across blocks by narrowing the disparities between temporal attention maps. This is work together with Fourier-based Motion Enhancement, which boosts motion magnitude by amplifying frequency energy, thus elevating the quality of the generated videos"
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 LATENT DIFFUSION MODEL In the context of T2V generation, latent diffusion model (Rombach et al., 2022b) is widely as backbone as its significant advancement in image synthesizing. Typically, based on pre-trained autoencoder E() and D(), video sequences are projected into the latent space, in which denoising network ϵθ is encouraged to learn the mapping from noised video latent zt to pure video latent z0. Mathematically, the noised video latent zt obeys the following distribution: zt = αtz0 + 1 αtϵ, (1) where αt is pre-defined parameter representing noise schedule (Ho et al., 2020), ϵ (0, 1) is the added noise, and U(1, ) denotes time step. To restore z0 from zt, denoising network ϵθ is forced to estimate the noise component in zt, which can be expressed as: L(θ) = Ez0,ϵ,t (cid:2)ϵt ϵθ(zt, c, t) 2 (cid:3) , (2) where represents the textual prompt of z0. During sampling, zt is initialized with Gaussian noise and undergoes iterative denoising conditioned on for prompt-aligned generation. 3.2 TEMPORAL ATTENTION MECHANISM The biggest difference between video generation and image generation lies in the synthesis of motion, i.e., the modeling of correlation between video sequences. This is typically achieved by temporal attention mechanism, which establishes feature interactions across frames via self-attention operations in temporal dimension. For 5D video diffusion feature RBCF HW , where and represent batch axis and frame time axis, and denotes spatial resolution, temporal attention performs self-attention in its 3D reshaped variant R(BHW )CF , in which the generated attention map R(BHW )F reflects the temporal correlation between frames."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 TEMPORAL SELF-GUIDANCE Temporal attention modules are extensively integrated at various hierarchical stages within the upsampling blocks of T2V architectures (Blattmann et al., 2023b; Guo et al., 2023b; Chen et al., 2023a; 2024). These modules, derived from different tiers of the diffusion UNet, are employed to capture inter-frame dependencies at multiple resolutions. Although the multi-level progressive refinement approach in modeling frame-wise correlations offers advantages, our observations indicate that the temporal attention maps across different hierarchical levels can exhibit considerable discrepancies, potentially leading to structurally implausible or temporally inconsistent video outputs. To substantiate this hypothesis, we analyzed 100 structurally and motion-degraded videos alongside 100 well-generated videos. We computed the mean and standard deviation of the mean squared error 4 Figure 4: Temporal Self-Guidance. Temporal Self-Guidance contributes to the restoration of collapsed structures and consistency of motion in the synthesized video. (MSE) distances between the temporal attention maps of up blocks.1 and subsequent blocks as illustrated in Fig. 3 (a). Our findings reveal that significant disparities between temporal attention maps across different blocks are associated with the occurrence of implausible structures and temporal inconsistencies in the generated videos. To mitigate the excessive divergence between temporal attention maps across various upsampling blocks, we introduce straightforward yet potent temporal self-guidance mechanism. This mechanism involves the infusion of the temporal attention map of up blocks.1 into subsequent blocks, modulated by guidance ratio α. The adjustment is mathematically represented as: Am = Am + α(Am 1 Am), (3) where Am denotes the temporal attention map of the m-th upsampling block (m = 2, 3), and Am 1 refers to the temporal attention map of up blocks.1, which is upsampled to match the spatial dimensions of Am. As depicted in Fig. 3 (b) and Fig. 4, the implementation of temporal self-guidance effectively alleviates the excessive modeling disparity between different hierarchical levels of temporal attention modules, thereby diminishing structurally implausible and temporally inconsistent artifacts in the resultant video generation. Beyond addressing the structural implausibility and temporal inconsistency issues resolved by Temporal Self-Guidance, we have observed that some generated videos, including those corrected by Temporal Self-Guidance, still suffer from paucity of motion, often appearing nearly static. To tackle this, we introduce novel strategy aimed at amplifying the motion amplitude and diversity within the generated videos by capitalizing on the energy inherent in the temporal attention maps. 4.2 FOURIER-BASED MOTION ENHANCEMENT 4.2.1 ENERGY REPRESENTATION OF MOTION MAGNITUDE The temporal attention map encapsulates rich set of motion-related information that is pivotal for the generation of dynamic video content. We find that the energy encapsulated within the temporal attention map is indicative of the motion amplitude present in the generated video. To elaborate, consider temporal attention map R(BHW )F , where represents the batch size, denotes the spatial resolution, and is the number of frames. The energy of this map can be quantified by the following equation: = 1 1 (cid:88) 1 (cid:88) i=0 j=0 A...,i,j2, (4) as illustrated in Fig. 5 (a). To substantiate the correlation between the energy of the temporal attention map and the motion magnitude in the generated video, we employ the RAFT (Teed & Deng, 2020) to extract the optical flow, using the average magnitude of this flow as metric for motion strength. Our findings reveal positive correlation: videos with greater motion magnitudes are associated with higher energies within their temporal attention maps. This insight motivates us to manipulate the motion magnitude in the generated videos by modulating the energy intensity of 5 Figure 5: Visualization of energy in temporal attention map. (a) The energy map of the generated video. (b) Videos with larger motion magnitude typically exhibit higher energy, where the motion magnitude is calculated using the optical flow of the generated videos. the temporal attention maps. By doing so, we aim to enhance the dynamism and variability of the motion in the videos. 4.2.2 MOTION ENHANCEMENT BY FREQUENCY SPECTRUM RE-WEIGHTING To enhance the motion amplitude in generated videos by amplifying the energy of the temporal attention map, we must overcome the challenge posed by the softmax normalization inherent in attention maps, which precludes straightforward numerical scaling. To address this, we employ sequenceto-sequence discrete frequency decomposition technique, specifically the Fast Fourier Transform (FFT), to the temporal attention map. For given temporal attention map R(BHW )F , we decompose it into its high-frequency and low-frequency components as follows: = F(A), AH = A...,iH , iH [ 2 τ, 2 + τ ], (5) 2 τ ) ( + τ, 1], AL = A...,iL, iL [0, 2 where denotes the FFT operation, C(BHW )F is the complex-valued matrix resulting from applying the FFT to A, and τ is hyperparameter that determines the frequency range for the high-pass and low-pass filters. As demonstrated in Fig. 6 (a), experiments involving the selective removal of high-frequency or low-frequency components from the temporal attention map during the denoising process have yielded insightful observations. Videos that retain only the lowfrequency components tend to exhibit nearly static structure, closely mirroring the characteristics of their unmodified counterparts. In contrast, videos that include solely high-frequency components display abundant motion but are marred by inconsistency and persistent flickering. These findings suggest that the essence of motion in generated videos is predominantly encapsulated within the high-frequency components of their temporal attention maps. Motivated by these insights, we introduce scaling factor β to modulate the high-frequency components AH . The process of scaling and reconstructing the temporal attention map is formalized by the following equation: = (cid:101)F(βAH + AL), (6) where (cid:101)F represents the inverse Fast Fourier Transform (iFFT) operation, and signifies the temporal attention map with the scaled high-frequency components. Based on aforementioned equations, the following theorems can be proven. Please refer to Section A.1.1 for comprehensive proof. Theorem 1. For any β 0, where denotes the softmax dimension associated with A, and is an all-ones matrix. possesses the softmax property. Specifically, (cid:80) = (cid:80) = I, Therefore, can replace as the new temporal attention map in the decoder. Theorem 2. If β > 1, then the energy of as Ex. Conversely, if 0 < β < 1, then , denoted as is less than Ex. x, is greater than the energy of A, denoted 6 Figure 6: Frequency decomposition and manipulation. (a) Results obtained by directly removing either the high-frequency or low-frequency components from the temporal attention map. The motion in generated videos is primarily present in the high-frequency components of the temporal attention map. (b) Results obtained by scaling the high-frequency components by factor of β. Figure 7: BroadWay Operations. (a) Temporal Self-Guidance. The temporal attention map from up blocks.1 is injected into the corresponding modules of up blocks.2/3 with guidance ratio α, in order to enhance the structural plausibility and temporal consistency. (b) Fourier-based Motion Enhancement. scaling factor β is applied to the high-frequency components of the temporal attention map, thereby amplifying the magnitude of the motion. As illustrated in Fig. 6 (b), setting β = 1.5 amplifies the energy of the temporal attention maps, leading to greater motion magnitude. Conversely, setting β = 0.5 results in reduced motion. 4.3 BROADWAY Leveraging Temporal Self-Guidance and Fourier-based Motion Enhancement, we introduce BroadWay, parameter-free method that enhances the quality of text-to-video generation without increasing memory requirements or sampling time. As illustrated in Fig. 7, BroadWay initially applies Temporal Self-Guidance to improve the structural coherence and temporal consistency of the video. Subsequently, Fourier-based Motion Enhancement is employed to amplify motion dynamics. To ensure that the motion magnitude of generated videos processed by BroadWay exceeds that of the original, unenhanced videos, the energy of the temporal attention map after Fourier-based Motion Enhancement, denoted as E3, must be greater than the energy of the original temporal attention map, denoted as E1. To achieve this, the scaling factor β is defined as function of the energies before and after Temporal Self-Guidance, E1 and E2, respectively: β(E1, E2) = max{β0, (cid:115) E1 EL 2 EH }, (7) 7 Figure 8: Samples synthesized by AnimateDiff with or without BroadWay. The samples utilizing the BroadWay exhibit enhanced structural plausibility, temporal consistency, and an increased richness in motion dynamics. where β0 is user-given value of β to control the motion magnitude. EH 2 denoting the energies of the high-frequency and low-frequency of the attention map after applying Temporal Self-Guidance, respectively. Please refer to Section A.1.2 for detailed proof for Eq. 7. 2 and EL"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTS SETUP Setting up. We mainly conduct our experiments on two mainstream diffusion based T2V backbones with superior visual quality: AnimateDiff (512 512) (Guo et al., 2023b) and VideoCrafter2 (320 512) (Chen et al., 2024). Results synthesized by vanilla T2V backbones are used as benchmark. The BroadWay parameters are set to α = 0.6, β = 1.5 in default for AnimateDiff, α = 0.1, β = 10 in default for VideoCrafter2. BroadWay operations are only applied during the first 20% steps of the denoising process. DDIM sampler (Song et al., 2020) with classifier-free guidance (Ho & Salimans, 2022) is adopted in the inference phase. Evaluation metrics. We report three metrics for quantitative evaluation. First, we conduct user study with 30 participants to assess Video Quality, considering both structure coherence and motion magnitude. Secondly, we compare the Optical Flow values of 1000 videos generated by Vanilla T2V backbones and BroadWay-enhanced backbones. Additionally, we employ multimodal large language model, GPT-4o (Achiam et al., 2023), for comprehensive Multimodal-Large-LanguageModel (MLLM) Assessment on hundreds of generated videos. Refer to Section A.3 for details. 5.2 QUALITATIVE COMPARISON As presented in Fig. 8 and Fig. 9, with the integration of BroadWay, various T2V backbones demonstrates notable performance improvement compared to their vanilla synthesis results. For instance, giving AnimateDiff the prompt green wool doll is displayed on the wooden table., BroadWay enhances the structural consistency of the synthesized video, preventing the collapse Figure 9: Samples synthesized by VideoCrafter2 with or without BroadWay. The samples utilizing the BroadWay exhibit enhanced structural plausibility, temporal consistency, and an increased richness in motion dynamics. of the dolls head and tail. Moreover, in the jeep driving on the grass near forest. case, BroadWay amplifies the dynamic effects of the scene, making the jeep exhibit more pronounced motion. For VideoCrafter2, when provided with the prompt horse jumping over fence during race, crowd cheering., BroadWay reconstructs the structure of the rider and horse, addressing the issue of structural anomalies in the horses legs while enhancing the overall motion to appear more synchronized and aesthetically pleasing. In cases like penguin sliding on ice, snowy landscape in the background., BroadWay preserves the original structural integrity while introducing richer, more dynamic motion to the scene. In summary, BroadWay effectively improves the structural consistency of synthesized videos while amplifying their motion dynamics, resulting in significant enhancement in the overall synthesis quality of the T2V backbones. 5.3 QUANTITATIVE EVALUATION User Study. As shown in Table 1 (a), we present the voting results, expressed as percentages, for vanilla T2V backbones and BroadWay-enhanced backbones. Our analysis shows that BroadWay receives the majority of votes, demonstrating that BroadWay provides substantial improvement to the T2V diffusion model in terms of overall video quality, taking into account both structure coherence and motion magnitude. Motion Magnitude. To objectively evaluate the motion magnitude, RAFT (Teed & Deng, 2020) is introduced to estimate the forward optical flow between consecutive frames, and the average intensity value of estimated optical flow is used to quantify the motion magnitude within the video. As presented in Table 1 (b) BroadWay shows substantial improvements in mean motion intensity, indicates its efficacy in producing large-magnitude motion. MLLM Assessment. In light of the impressive strides made by Multimodal-Large-LanguageModels (MLLM) recently in image/video understanding, the state-of-the-art MLLM, i.e., GPT4o (Achiam et al., 2023), is employed for video quality assessment, covering structural rationality and motion consistency. As can be observed in Table 1 (c)-(d), BroadWay exhibits notable gains in both metrics, validating its role in substantially improving overall video quality. Method AnimateDiff + BroadWay VideoCrafter2 + BroadWay Table 1: Quantitative results with or without BroadWay. (b) Optical Flow (c) Structural Rationality (a) Video Quality (d) Motion Consistency 25.42% 74.58% 30.54% 69.46% 1.5743 2.4673 1.5555 3.6204 41.94% 58.06% 18.48% 81.52% 34.62% 65.38% 39.60% 60.40% Figure 10: Ablation study on BroadWay components. The left panel illustrates an instance of inconsistency artifacts present in the original video, whereas the right panel exhibits scenario where the original video lacks sufficient motion. 5.4 ABLATION STUDY Effects of Temporal Self-Guidance (TSG). Temporal Self-Guidance plays critical role in reinforcing structural integrity, thus effectively mitigating structural breakdowns and preventing motion artifacts, as can be observed in Fig. 10 (a). However, it cannot enhance the magnitude of motion, showing limited improvement in scenarios with little motion, as shown in Fig. 10 (b). Effects of Fourier-based Motion Enhancement (FME). Fourier-based Motion Enhancement is responsible for amplifying the motion dynamics in generated videos. In scenarios where the motion is insufficient, this technique effectively increases the dynamic content, as shown in Fig. 10 (b). However, motion enhancement alone does not guarantee appealing video quality when structural breakdown occurs, as illustrated in Fig. 10 (a). Effects of BroadWay. By integrating Temporal Self-Guidance with Fourier-based Motion Enhancement, BroadWay achieves simultaneous enhancement of both the structural integrity and motion dynamics in generated videos (Fig. 10 (a)-(b) Ours vs. Vanilla). 5.5 IMAGE-TO-VIDEO Similar to text-to-video (T2V) tasks, image-tovideo (I2V) is also significant research area within video diffusion models. Here we employ SparseCtrl (Guo et al., 2023a), strong and flexible structure control method, as the I2V backbone to preliminarily validate the potential of BroadWay in image-to-video tasks. As illustrated in Fig. 11, the infusion of BroadWay into SparseCtrl serves to enhance the dynamic effects of the synthesized video while preserving the structural integrity of the reference image. Specifically, we observe that the video synthesized with BroadWay exhibits more vivid wave motions, and the reflections of the setting sun display enhanced dynamic aesthetics. These experimental results demonstrate that BroadWay effectively enhances the quality of both T2V and I2V video generation tasks, positioning it as versatile and powerful booster for video diffusion models. Figure 11: Generated results by SparseCtrl with or without BroadWay."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this study, we present BroadWay, training-free method to improve the quality of text-tovideo generation without introducing additional parameters, augmenting memory or sampling time. BroadWay is composed of Temporal Self-Guidance and Fourier-based Motion Enhancement. The former improves the structural plausibility and temporal consistency by reducing the disparity between the temporal attention maps across various decoder blocks. The later enhances the magnitude and richness of motion by scaling the high frequency of the temporal attention maps. The proposed method can be easily integrated with other T2V models in plug-and-play manner, offering general and effective solution to enhance video generation quality during inference phase."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17281738, 2021. Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel Machines. MIT Press, 2007. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023b. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023a. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023b. Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Animateanything: Fine-grained open domain image animation with motion guidance. arXiv eprints, pp. arXiv2311, 2023. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 73467356, 2023. 11 Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2293022941, 2023. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1069610706, 2022. Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023a. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023b. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. fast learning algorithm for deep belief nets. Neural Computation, 18:15271554, 2006. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using textto-image diffusion models. arXiv preprint arXiv:2310.01107, 2023. Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. arXiv preprint arXiv:2312.00845, 2023. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models In Proceedings of the IEEE/CVF International Conference on are zero-shot video generators. Computer Vision, pp. 1595415964, 2023. Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 77017711, 2023a. Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 77017711, 2023b. 12 Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023. Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. arXiv preprint arXiv:2303.04761, 2023. Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023. Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, et al. Follow-your-click: Open-domain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024. Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. arXiv preprint arXiv:2312.07536, 2023. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofavideo: Controllable image animation via generative motion field adaptions in frozen image-tovideo diffusion model. arXiv preprint arXiv:2405.20222, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and arXiv preprint Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1593215942, 2023. Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022b. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedIn Medical image computing and computer-assisted intervention ical image segmentation. MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In CVPR, 2024. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36: 13631389, 2023. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 402419. Springer, 2020. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024a. Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation with text-free videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65726582, 2024b. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2023c. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023. Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. FastcomarXiv preprint poser: Tuning-free multi-subject image generation with localized attention. arXiv:2305.10431, 2023. Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. arXiv preprint arXiv:2405.14864. 14 Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, He, Liu, Chen, Cun, Wang, Shan, et al. Make-your-video: Customized video generation using textual and structural guidance. IEEE Transactions on Visualization and Computer Graphics, 2024. Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, and Ye Shi. Guidance with spherical gaussian constraint for conditional diffusion. In International Conference on Machine Learning, 2024. Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. Lai Zeqiang, Zhu Xizhou, Dai Jifeng, Qiao Yu, and Wang Wenhai. Mini-dalle3: Interactive text to image by prompting large language models. arXiv preprint arXiv:2310.07653, 2023. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. arXiv preprint arXiv:2403.15378, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "In the appendix, we present the proof of Fourier-based Motion Enhancement (Section A.1), additional qualitative results (Section A.2), details of our quantitative evaluation (Section A.3), as well as the limitations of our method(Section A.4), as supplement to the main paper. A.1 PROOF OF FOURIER-BASED MOTION ENHANCEMENT In this section, we provide detailed proof of how Fourier-based Motion Enhancement alters the energy of the temporal attention map in the denoising process. A.1.1 FREQUENCY MANIPULATION Given temporal attention map R(BHW )F with batch size B, spatial resolution and frame number , since we treat it as batch of attention sequences, we will next discuss the operations performed on single softmax sequence x[n] of length . Mathematically, the operation of mapping the sequence x[n] to the frequency domain is performed by the Discrete Fourier Transform (DFT): X[k] = 1 (cid:88) n=0 x[n] ej 2π kn, = 0, 1, . . . , 1, (8) Parsevals theorem states that the energy of sequence is preserved under frequency domain transformation, meaning that the energy Ex of sequence x[n] is the same in both the time and frequency domains. This theorem can be expressed as follows: Ex = 1 (cid:88) n=0 x[n]2 = 1 F 1 (cid:88) k=0 X[k]2, (9) As mentioned in Section 4.2.2, Fourier-based Motion Enhancement uses threshold index τ to separate the high-frequency and low-frequency components of the sequence, scaling the high-frequency components by factor of β. This operation can be expressed as: 2 τ, 2 + τ ], [k] = (cid:26) β X[k] X[k] [ otherwise, (10) After applying this manipulation, the energy of current attention sequence [n] is given by: = 1 [ /[ (cid:88) 2 τ, 2 +τ ] 2[k] + β2 (cid:88) 2 τ, k[ 2 +τ ] 2[k]], (11) Thus the energy change amount caused by Fourier-based Motion Enhancement can be computed as: = Ex (β2 1) = (cid:88) 2[k], k[ 2 τ, 2 +τ ] Clearly, in the scenario where β > 1, Fourier-based Motion Enhancement will lead to an increase in the energy of the attention sequence (E > 0), while the opposite will result in decrease in energy (E < 0), which elucidates the mechanism by which Fourier-based Motion Enhancement effectively enhances motion magnitude in synthesized videos. Furthermore, it can be demonstrated that the attention sequence processed by Fourier-based Motion Enhancement remains softmax sequence. This property is preserved because the DC component X[0] of the attention sequence, which determines the sum of the sequence, is not modified throughout the operation. By plugging = 0 into Eq. 8, we can ascertain this property: X[0] = 1 (cid:88) n=0 x[n] = 1 (cid:88) n= [n] = 1, 16 (12) A.1.2 ADAPTIVE β As depicted in Fig. 7, let E1 denote the the energy of the temporal attention map before applying BroadWay operations, E2 the energy after Temporal Self-Guidance, and E3 the energy after Fourierbased Motion Enhancement. Here, we demonstrate that using the adaptive β as defined in Eq. 7 ensures that E3 E1. Based on the separation of high-frequency and low-frequency components in the sequence as described in Section A.1.1, we can compute the energy of the high-frequency and low-frequency parts of the sequence x[n], denoted as EH and EL 1 = EH EH ="
        },
        {
            "title": "1\nF",
            "content": "x , respectively: (cid:88) k[ 2 +τ ] 2 τ, (cid:88) /[ 2 τ, 2 +τ ] 2[k], 2[k], (13) (14) (19) According to Eq. 9 and Eq. 13, it is evident that the following relationship holds: Ex = EH + EL , Furthermore, we can concisely express the energy manipulation performed by Fourier-based Motion Enhancement described in Section A.1.1, as follows: = β2EH + EL , (15) which indicates: E3 = β2EH 2 + EL 2 , (16) Therefore, to ensure E3 E1, it is necessary to ensure that β adheres to the following condition: 2 + EL 2 E1, β2EH (17) The critical value of β, denoted as βc, that satisfies this condition is: (cid:115) βc = E1 EL 2 EH 2 , (18) In BroadWay operations, the user-specified β, denoted as β0, will be compared with the critical value βc, and the larger of the two will be selected as the actual β value in Fourier-based Motion Enhancement: β = (cid:26) β0 βc β0 βc, β0 < βc, By adopting such adaptive β value, it can be theoretically guaranteed that the energy of the temporal attention map is increased during BroadWay operations, thereby enhancing the motion magnitude in synthesized videos. A.2 ADDITIONAL QUALITATIVE RESULTS In this section, we provide additional qualitative comparison results of BroadWay on AnimateDiff (Fig. 12, Fig. 13, Fig. 17 and Fig. 14) and VideoCrafter2 (Fig. 15, Fig. 16, Fig. 18). A.3 MATERIALS USED IN QUANTITATIVE EXPERIMENTS User Study Details. In our user study, each participant receives 50 videos synthesized by Vanilla T2V backbones and 50 videos synthesized by BroadWay-enhanced backbones. These videos are sampled from the same random seeds to ensure fair comparison. For each video pair from Vanilla and Vanilla+BroadWay, participants are required to select the video they perceive as superior based on overall Video Quality, considering both structure coherence and motion magnitude, and cast their vote accordingly. The videos were presented in randomized order to reduce potential bias, and participants were allowed ample time to review each pair before making their selections. MLLM Prompt. Here, we present the prompt used in the MLLM assessment. 17 query = \"\"\" You are provided with two sets of video frames, each containing 4 representative frames, along with shared textual prompt that was used to generate both videos. Your task is to perform comparative evaluation of the two videos, focusing on their structure rationality / motion consistency. \"\"\".strip() prefix_1 = \"\"\" Here is the frame data of video_1: \"\"\" prefix_2 = \"\"\" Here is the frame data of video_2: \"\"\" suffix = \"\"\" Based on your evaluation of motion consistency, choose the video set you find to be superior. If you determine that the first set of frames (Video_1) is better, respond with \"A\". If the second set (Video_2) is superior, respond with \"B\". Return only \"A\" or \"B\" based on your assessment. \"\"\" A.4 LIMITATIONS Parameter Sensitivity. The default values of BroadWay parameters α and β are relatively robust within specific T2V backbone but may not be universally optimal for different backbones. Users seeking enhanced visual quality are encouraged to manually adjust these parameters. Increasing α can lead to stronger motion dynamics, while higher value of β enhances structural consistency. Performance Upper Bound. Although BroadWay demonstrates the capability to unlock the synthesis potential of various T2V backbones, the synthesized videos remain confined within the sampling distribution of the original T2V backbone. Therefore, the upper performance bound of our proposed method is still constrained by the original T2V backbone. 18 Figure 12: More results on AnimateDiff (Object Motion Enhancement). 19 Figure 13: More results on AnimateDiff (Object Motion Enhancement). Figure 14: More results on AnimateDiff (Corrupted Case Repair). 21 Figure 15: More results on VideoCrafter2 (Object Motion Enhancement). Figure 16: More results on VideoCrafter2 (Corrupted Case Repair). 22 Figure 17: More results on AnimateDiff (Camera Motion Enhancement). Figure 18: More results on VideoCrafter2 (Camera Motion Enhancement)."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}