{
    "paper_title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "authors": [
        "Tianwei Lin",
        "Wenqiao Zhang",
        "Sijing Li",
        "Yuqian Yuan",
        "Binhe Yu",
        "Haoyuan Li",
        "Wanggui He",
        "Hao Jiang",
        "Mengze Li",
        "Xiaohui Song",
        "Siliang Tang",
        "Jun Xiao",
        "Hui Lin",
        "Yueting Zhuang",
        "Beng Chin Ooi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT."
        },
        {
            "title": "Start",
            "content": "HealthGPT: Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3, Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5 1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba, 4The Hong Kong University of Science and Technology, 5National University of Singapore Project Page Code 5 2 0 2 7 1 ] . [ 2 8 3 8 9 0 . 2 0 5 2 : r Figure 1: HealthGPT enables medical multi-modal comprehension and generation, outperforming both state-of-the-art unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling complex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation. Abstract We present HealthGPT, powerful Medical Large VisionLanguage Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by tailored hierarchical visual perception approach and three-stage learning strategy. To effectively learn the HealthGPT, we devise comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT."
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) (Liu et al. 2023; OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have demonstrated outstanding open-world visual comprehension and reasoning abilities through language-based interactive dialogue over the past years, simultaneously opening up new opportunities for applications in specialized domains. 1 Specifically, recent studies (Li et al. 2024a; Tu et al. 2024) have utilized pre-trained large language models (LLMs) and visual instruction data to build interactive diagnostic tools and treatment planning systems, revealing the immense potential of LVLMs in medical scenarios. However, these studies primarily concentrate on visual comprehension tasks that produce text-based outputs, such as medical visual question answering (Li et al. 2024a) or report generation (Nath et al. 2024), and deficient the drawing capability needed for medical visual generation. In practice, integrating visual comprehension and generation can significantly enhance the multifunctionality of medical LVLMs. Recent studies have increasingly focused on developing unified LVLMs capable of comprehending and generating content across diverse visual modalities. Earlier approaches predominantly utilized continuous visual tokens fed into LLMs, using the LLMs themselves as conditional generators for external generative models (Ge et al. 2024; Wu et al. 2023; Dong et al. 2023). More recent research has explored the use of discrete visual tokens for image representation and generation within fully autoregressive framework (Team 2024; Wang et al. 2024a; Xie et al. 2024). These methods not only enhance controllability but also demonstrate early success in open-world, any-to-any tasks, highlighting the preliminary potential of unified autoregressive learning paradigm in multi-modal tasks. While unified LVLMs have achieved initial success in general scenarios, such unified framework remains underexplored in the medical domain. Adapting the aforementioned general unified model paradigm to the medical domain presents two major challenges: (i) High-scale and -quality Data Limitations. Open-world models necessitate extensive pre-training on billions or even more diverse, multi-modal data samples for comprehension and generation tasks (Lu et al. 2024; Team 2024). However, the accessible medical data significantly lacks in scale and quality compared to natural multi-modal datasets. Its specialized and domain-specific characteristics make it challenging to develop unified medical model from scratch. (ii) Conflicts between Comprehension and Generation. Comprehension tasks often strip away visual details to focus on abstraction, while generation tasks require detailed preservation, making tokens sensitive to all visual alterations. As shown in Figure 2, which features experiments conducted on medical images, the performance in comprehension (or generation) tasks steadily decreases as the proportion of generation (or comprehension) data increases, and vice versa. This highlights dilemma in autoregressive multi-modal training, stemming from the need to maintain consistency between preand post-LVLMs. While some methods have explored mutual enhancement between comprehension and generation (Pan et al. 2024; Tong et al. 2024), improvements still exhibit diminishing returns, with performance degradation remaining significant issue. Figure 2: With fixed amount of comprehension (generation) data, increasing the proportion of the other type leads to significant performance degradation. To tackle the aforementioned challenges, we propose HealthGPT (see Figure 1) , which progressively adapts pre-trained LLM as an unified medical multi-modal model with small amount of visual instruction data. We devise innovative Parameter-Efficient Fine-Tuning (PEFT) approach (Ding et al. 2023), called Heterogeneous Low-Rank Adaptation (H-LoRA), which decouples the learning process of LVLMs for comprehension and generation tasks. Inspired by the plug-and-play nature of LoRA (Hu et al. 2021), H-LoRA enables the model to store heterogeneous comprehension and generation knowledge in independent plugins, thus avoiding joint optimization issues caused by conflicts between comprehension and generation tasks. In addition, we also consider the variety of sub-tasks among comprehension or generation tasks. Qualitative research highlights the limitations of single LoRA in handling multidimensional task scenarios, mainly due to catastrophic forgetting and interference (Liu et al. 2024d; Lin et al. 2024). To address this, we draw on the concept of Mixture of Experts (MoE) (Masoudnia and Ebrahimpour 2014) and introduce LoRA experts. The aim is to dynamically transfer task-shared knowledge to adapt to downstream tasks. Unlike MoELoRA (Luo et al. 2024a), H-LoRA employs reversible matrix block multiplication to combine LoRA experts, significantly reducing the overhead of multiple matrix multiplications. Notably, when using four experts, it requires only 67% of the MoELoRA training time. To effectively leverage H-LoRA in HealthGPT, we further introduce Hierarchical Visual Perception (HVP) and devise corresponding Three-stage Learning Strategy (TLS). HVP: we separate visual details learning from Vision transformer (ViT) for comprehension and generation. As is widely recognized, the ViT encodes visual concepts with increasing abstraction, generally, becoming finer as we progress over levels (Vig 2019). Thus, we maintain the visual features of the anterior and posterior layers to accommodate the differing requirements for visual granularity in comprehension and generation tasks while preventing po2 tential task interference. TLS: In the first and second stages, given the heterogeneity between comprehension and generation tasks, we first train H-LoRA plugins for HealthGPT to incorporate both medical comprehension and generation knowledge, thus endowing the LLMs with capabilities for vision-language alignment and vision-to-vision reconstruction. Additionally, through minimal mixed-task training, we built fusion embedding layers and output heads that merge text and visual tokens, establishing unified LVLM foundation for visual instruction fine-tuning. In the third stage, by only training the H-LoRA plugins, HealthGPT is able to rapidly adapt to wide range of downstream medical tasks, covering various types of medical comprehension and generation tasks. To effectively implement our approach, we have curated dataset for training unified medical LVLMs, called VL-Health, including seven comprehension tasks and five generation tasks (Figure 1). Through quantitative analysis and validation on multi-modal tasks, the results demonstrate that HealthGPT is capable of unifying medical multimodal abilities in data-constrained scenarios, achieving performance comparable to or better than existing state-of-theart (SOTA) models across multiple metrics. Overall, the main contributions of this paper are summarized as follows: Unified Med-LVLM. We introduce HealthGPT, which, to the best of our knowledge, is the first unified framework for multi-modal comprehension and generation in complex medical scenarios. Effective Learning Paradigm. We present H-LoRA, an optimized multi-LoRA PEFT architecture based on taskgated decoupling, is designed to effectively mitigate data conflict issues. Holistic Training Dataset. We curated VL-Health, comprehensive dataset designed for both comprehension and generation tasks. Superior Downstream Improvements: Extensive experiments are conducted and the results confirm HealthGPTs effectiveness in medical vision-language comprehension and generation."
        },
        {
            "title": "2 Related Work\nMedical Vision Large Language Models. Recently, medi-\ncal vision large language models (Med-VLLMs) have made\nsignificant progress, demonstrating excellent performance\nin understanding medical images and responding to human\nqueries based on these images (Zhou et al. 2023; Tian et al.\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\nical visual encoder (MedClip) (Wang et al. 2022) with a\nfine-tuned LLM , using a simple linear transformation layer\nto achieve alignment between visual and textual informa-\ntion, significantly enhancing the understanding of medical\nimages. On this basis, LLaVA-Med (Li et al. 2024b) fur-\nther enhances visual-text alignment in medical contexts by",
            "content": "selecting high-quality image-text pairs from PubMed papers and synthesized VQA datasets. BiomedGPT (Luo et al. 2024b) employs BERT-style encoder and GPT-style decoder architecture, pre-trained on interdisciplinary datasets. Compared to commercial models like Med-PaLM (Singhal et al. 2023), BiomedGPT significantly reduces model size while maintaining superior performance. However, issues of language adaptability and dataset specificity still remain. To address these, HuatuoGPT-Vision (Chen et al. 2024a) introduces the PubMedVision dataset, which contains 1.3 million high-quality medical samples, significantly improving the models adaptability across diverse medical applications. However, current Med-VLLMs mainly focus on medical comprehension and lack the capability for the medical vision-language generation. Unified Visual Comprehension and Generation Models. Recent research has increasingly concentrated on creating unified LVLMs that are adept at understanding and producing content across various visual modalities. NExTGPT (Wu et al. 2023) achieves perception and generation for arbitrary combinations of multi-modal inputs and outputs by aligning LLMs. Similarly, SEED (Ge et al. 2023), SEEDX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) employ learnable queries and leverage next-token prediction to generate visual tokens, providing conditional inputs to external generation modules. Unlike these methods, which function as external conditioners, Unified-IO (Lu et al. 2022), Unified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024) internalize multi-modal generation tasks within unified Transformer architecture by extending multi-modal vocabularies, enabling direct generation based on next-token prediction. Building on this concept, Lumina-mGPT (Liu et al. 2024a) and ANOLE (Chern et al. 2024) further enhance the generation capabilities of unified models using high-quality data, particularly improving the quality and flexibility of image generation."
        },
        {
            "title": "3 Preliminaries\nLarge Vision-Language Models. The input to a LVLM typ-\nically consists of an image ximg and a discrete text sequence\nxtxt. The visual encoder E img converts the input image ximg\ninto a sequence of visual tokens V = [vi]Nv\ni=1, while the\ntext sequence xtxt is mapped into a sequence of text to-\nkens T = [ti]Nt\ni=1 using an embedding function E txt. The\nLLM MLLM(·|θ) models the joint probability of the token\nsequence U = {V, T }, which is expressed as:",
            "content": "Pθ(RU) = Nr(cid:89) i=1 Pθ(ri{U, r<i}), (1) where = [ri]Nr i=1 is the text response sequence. The LVLM iteratively generates the next token ri based on r<i. The optimization objective is to minimize the cross-entropy loss of the response R. It is worth noting that most LVLMs adopt 3 Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing task-specific hard router to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner. design paradigm based on ViT, alignment adapters, and pre-trained LLMs(Liu et al. 2023, 2024b), enabling quick adaptation to downstream tasks. VQGAN. VQGAN (Esser, Rombach, and Ommer 2021) employs latent space compression and indexing mechanisms to effectively learn complete discrete representation of images. VQGAN first maps the input image ximg to latent representation = E(x) through encoder E. Then, the latent representation is quantized using codebook = {zk}K k=1, generating discrete index sequence = [im]N m=1, where im represents the quantized code index: = Quantize(zZ) = arg min zkZ zk2. (2) In our approach, the discrete index sequence serves as supervisory signal for the generation task, enabling the model to predict the index sequence ˆI from input conditions such as text or other modality signals. Finally, the predicted index sequence ˆI is upsampled by the VQGAN decoder G, generating the high-quality image ˆximg = G(ˆI). Low Rank Adaptation. LoRA(Hu et al. 2021) effectively captures the characteristics of downstream tasks by introducing low-rank adapters. The core idea is to decompose the bypass weight matrix Rdindout into two lowrank matrices {A Rdinr, Rrdout }, where min{din, dout}, significantly reducing learnable parameters. The output with the LoRA adapter for the input is then given by: = xW0 + αxW/r = xW0 + αxAB/r, (3) where matrix is initialized with Gaussian distribution, while the matrix is initialized as zero matrix. The scaling factor α/r controls the impact of on the model."
        },
        {
            "title": "4.1 Unified Autoregressive Generation.\nHealthGPT (Figure 3) utilizes a discrete token representa-\ntion that covers both text and visual outputs, unifying visual\ncomprehension and generation as an autoregressive task. For\ncomprehension, Mllm receives the input joint sequence U\nand outputs a series of text token R = [r1, r2, . . . , rNr ],\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\nNr(cid:89)",
            "content": "Pθ(R U) = Pθ(ri U, r<i). (4) i=1 For generation, Mllm first receives special start token START IMG, then generates series of tokens corresponding to the VQGAN indices = [i1, i2, . . . , iNi], where ij Vvq, and Vvq represents the index range of VQGAN. Upon completion of generation, the LLM outputs an end token END IMG: Pθ(I U) = Ni(cid:89) j=1 Pθ(ij U, i<j). (5) Finally, the generated index sequence is fed into the decoder G, which reconstructs the target image ˆximg = G(I)."
        },
        {
            "title": "4.2 Hierarchical Visual Perception\nGiven the differences in visual perception between compre-\nhension and generation tasks—where the former focuses on\nabstract semantics and the latter emphasizes complete se-\nmantics—we employ ViT to compress the image into dis-\ncrete visual tokens at multiple hierarchical levels. Specif-\nically,\nthe image is converted into a series of features\n{f1, f2, . . . , fL} as it passes through L ViT blocks.",
            "content": "4 To address the needs of various tasks, the hidden states are divided into two types: (i) Concrete-grained features Con = {f1, f2, . . . , fk}, < L, derived from the shallower layers of ViT, containing sufficient global features, suitable for generation tasks; (ii) Abstract-grained features Abs = {fk+1, fk+2, . . . , fL}, derived from the deeper layers of ViT, which contain abstract semantic information closer to the text space, suitable for comprehension tasks. The task type (comprehension or generation) determines which set of features is selected as the input for the downstream large language model:"
        },
        {
            "title": "F img",
            "content": "T = (cid:40) Con, Abs, if = generation task if = comprehension task (6) We integrate the image features img and text features into joint sequence through simple concatenation, which is then fed into the LLM Mllm for autoregressive generation. T"
        },
        {
            "title": "4.3 Heterogeneous Knowledge Adaptation",
            "content": "We devise H-LoRA, which stores heterogeneous knowledge from comprehension and generation tasks in separate modules and dynamically routes to extract task-relevant knowledge from these modules. At the task level, for each task type , we dynamically assign dedicated H-LoRA submodule θT , which is expressed as: = MLLM(Uθ, θT ), θT = {AT , BT , RT outer}. (7) At the feature level for single task, H-LoRA integrates the idea of Mixture of Experts (MoE) (Masoudnia and Ebrahimpour 2014) and designs an efficient matrix merging and routing weight allocation mechanism, thus avoiding the significant computational delay introduced by matrix splitting in existing MoELoRA (Luo et al. 2024a). Specifically, we first merge the low-rank matrices (rank = r) of LoRA experts into unified matrix: Amerged, Bmerged = Concat({Ai}k 1), Concat({Bi}k 1), (8) where Amerged Rdinrk and Bmerged Rrkdout . The k-dimension routing layer generates expert weights Rtoken numk based on the input hidden state x, and these are expanded to Rtoken numrk as follows: expanded = αkW/r 1r, (9) where denotes the replication operation. The overall output of H-LoRA is computed as: OH-LoRA = (xAmerged expanded)Bmerged, (10) where represents element-wise multiplication. Finally, the output of H-LoRA is added to the frozen pre-trained weights to produce the final output: = xW0 + OH-LoRA. (11) 5 Figure 4: Data statistics of VL-Health."
        },
        {
            "title": "4.4 Training Pipeline\n1st Stage: Multi-modal Alignment. In the first stage, we\ndesign separate visual adapters and H-LoRA submodules for\nmedical unified tasks. For the medical comprehension task,\nwe train abstract-grained visual adapters using high-quality\nimage-text pairs to align visual embeddings with textual\nembeddings, thereby enabling the model to accurately de-\nscribe medical visual content. During this process, the pre-\ntrained LLM and its corresponding H-LoRA submodules\nremain frozen. In contrast, the medical generation task re-\nquires training concrete-grained adapters and H-LoRA sub-\nmodules while keeping the LLM frozen. Meanwhile, we ex-\ntend the textual vocabulary to include multimodal tokens,\nenabling the support of additional VQGAN vector quanti-\nzation indices. The model trains on image-VQ pairs, en-\ndowing the pre-trained LLM with the capability for image\nreconstruction. This design ensures pixel-level consistency\nof pre- and post-LVLM. The processes establish the initial\nalignment between the LLM’s outputs and the visual inputs.\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\nThe submodules of H-LoRA share the word embedding\nlayer and output head but may encounter issues such as\nbias and scale inconsistencies during training across dif-\nferent tasks. To ensure that the multiple H-LoRA plugins\nseamlessly interface with the LLMs and form a unified base,\nwe fine-tune the word embedding layer and output head us-\ning a small amount of mixed data to maintain consistency\nin the model weights. Specifically, during this stage, all H-\nLoRA submodules for different tasks are kept frozen, with\nonly the word embedding layer and output head being op-\ntimized. Through this stage, the model accumulates foun-\ndational knowledge for unified tasks by adapting H-LoRA\nplugins.\n3rd Stage: Visual Instruction Fine-Tuning. In the third\nstage, we introduce additional task-specific data to fur-\nther optimize the model and enhance its adaptability to\ndownstream tasks such as medical visual comprehension\n(e.g., medical QA, medical dialogues, and report generation)\nor generation tasks (e.g., super-resolution, denoising, and",
            "content": "Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension tasks. Bold and underlined text indicates the best performance and second-best performance, respectively. Type Model # Params Medical LVLM close VQA-RAD all SLAKE all close PathVQA all close MMMU -Med OMVQA Avg. Comp. Only Comp. & Gen. Med-Flamingo LLaVA-Med HuatuoGPT-Vision BLIP-2 LLaVA-v1.5 InstructBLIP Yi-VL InternVL2 Llama-3.2 Show-o Unified-IO 2 Janus HealthGPT-M3 HealthGPT-L14 8.3B 7B 7B 6.7B 7B 7B 6B 8B 11B 1.3B 7B 1.3B 3.8B 14B 58.6 60.2 66.9 43.4 51.8 61.0 52.6 64.9 68.9 50.6 46.2 70.9 73.7 77.7 43.0 48.1 53.0 36.8 42.8 44.8 42.1 49.0 45.5 33.9 32.6 52.8 55.9 58.3 47.0 58.4 59.8 41.6 37.1 66.8 52.4 66.6 72.4 31.5 35.9 34.7 74.6 76. 25.5 44.8 49.1 35.3 37.7 43.3 38.4 50.1 52.1 17.9 21.9 26.9 56.4 64.5 61.9 62.3 52.9 48.5 53.5 56.0 54.9 60.0 62.8 52.9 52.5 51.9 78.7 85.9 31.3 35.7 32.0 28.8 31.4 32.3 30.9 31.9 33.6 28.2 27.0 27.9 39.7 44. 28.7 30.0 42.0 27.3 32.7 25.3 38.0 43.3 39.3 22.7 25.3 30.0 43.3 49.2 34.9 41.3 50.0 26.9 44.7 29.0 50.2 54.5 63.2 45.7 33.0 26.8 68.5 74.4 41.4 47.6 50.7 36.1 41.5 44.8 44.9 52.5 54.7 42.6 33.8 33.5 61.3 66. Table 2: The experimental results for the four modality conversion tasks. Model CT to MRI (Brain) SSIM PSNR MSE CT to MRI (Pelvis) SSIM PSNR MSE MRI to CT (Brain) SSIM PSNR MSE MRI to CT (Pelvis) SSIM PSNR MSE pix2pix CycleGAN BBDM Vmanba DiffMa HealthGPT-M3 HealthGPT-L14 71.09 54.76 71.69 69.54 71.47 79.38 79.73 32.65 32.23 32.91 32.67 32.74 33.03 33.10 36.85 40.56 34.44 36.42 35.77 33.48 32.96 59.17 54.54 57.37 63.01 62.56 71.81 71.92 31.02 30.77 31.37 31.47 31.43 31.83 31. 51.91 55.00 48.06 46.99 47.38 43.45 43.09 78.79 63.75 86.40 79.63 79.00 85.06 85.31 33.85 31.02 34.12 34.12 34.13 34.40 34.29 28.33 52.78 26.61 26.49 26.45 25.49 26.20 72.31 50.54 79.26 77.45 78.53 84.23 84.96 32.98 29.89 33.15 33.53 33.68 34.29 34. 36.19 67.78 33.60 31.85 30.51 27.99 28.13 modality conversion). Notably, by this stage, the word embedding layer and output head have been fine-tuned, only the H-LoRA modules and adapter modules need to be trained. This strategy significantly improves the models adaptability and flexibility across different tasks."
        },
        {
            "title": "5.1 Data and Experimental Setup",
            "content": "Data Details. We curate VL-Health dataset (see Figure 4). For medical visual comprehension, we leverage multiple medical-specific datasets, including PubMedVision (Chen et al. 2024a), LLaVA-Med (Li et al. 2024b), PathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al. 2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau et al. 2018). Additionally, we incorporate high-quality openworld data from LLaVA-1.5 (Liu et al. 2024b) to preserve the models general knowledge and instruction-following capabilities. For generation tasks, we construct reconstruction dataset based on LLaVA-558k (Liu et al. 2024b), and also explore two key tasks in personalized medical image enhancementsuper-resolution and modality conversionusing the IXI (Davies et al. 2014) and SynthRAD2023 (Thummerer et al. 2023) datasets. Detailed data selection and instruction templates are in the Appendix. Model Details. We select CLIP-L/14 (Radford et al. 2021) as the visual encoder and used the hidden states of its second and penultimate layers as concrete-grained and abstract-grained features for models dynamic hierarchical visual perception. Drawing on the successful experiences of LLaVA, we employ MLP to align the multi-modal feature embeddings. We choose the parameter-efficient phi-3mini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the base model. For visual comprehension and generation tasks, we set the rank of H-LoRA to 16 and 64, with four experts. Additionally, we use the f8-8192 version of VQGAN as the image indexing and upsampling module. 5.2 Main Experiments Comprehension. We compare HealthGPT with several existing models, including medical-specific LVLMs (e.g., Med-Flamingo (Moor et al. 2023), LLaVA-Med (Li et al. 2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well as recent open-world LVLMs (e.g., BLIP-2 (Li et al. 2023b), LLaVA-v1.5 (Liu et al. 2024b), InstructBLIP (Dai et al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen 6 Table 3: Comparison results of super-resolution task. Model SSIM PSNR MSE LPIPS SRGAN DASR Real-ESRGAN LIIF BSRGAN HealthGPT-M3 HealthGPT-L14 71.34 71.57 67.30 73.27 69.97 78.19 77.94 32.01 32.34 31.87 32.13 31.97 32.76 32.71 41.27 38.25 42.57 40.14 41.52 34.47 35.19 24.50 19.17 20.64 22.93 28.72 12.02 12. Figure 5: Performance comparison of LoRA, MoELoRA, and H-LoRA under different rank settings. et al. 2024b), Llama-3.2 (Dubey et al. 2024)). Additionally, we test several SOTA unified visual comprehension and generation models, including Show-o (Xie et al. 2024), Unified-IO 2 (Lu et al. 2024), and Janus (Wu et al. 2024). The experimental results are shown in Table 1, with the following key observations: (i) SOTA Results Compared with LVLMs: In medical visual comprehension tasks, HealthGPT demonstrates superior performance, significantly outperforming both medical-specific models (e.g., HuatuoGPT-Vision) and general-purpose models (e.g., Llama-3.2). (ii) Surpassing Current Unified LVLMs: Despite being trained on billions of data points, unified models still exhibit poor generalization performance in medical visual comprehension. For instance, Unified-IO 2 scored only 33.8. In contrast, HealthGPT-M3, with only 3.8B parameters, scored 61.3 on the medical multi-modal unified task, significantly outperforming existing unified models in medical downstream scenarios. (iii) Stable Improvement with Large Base Model: Our method demonstrates excellent scalability, with HealthGPT-L14 achieving score of 66.4 in the larger model configuration. This result significantly outperforms all other models, highlighting the effectiveness of scaling up the base model for enhanced performance in medical tasks. Generation. We study three key tasks in medical imaging. (i) Modality Conversion: In this task, we focus on the conversion between CT and MRI modalities for the brain and pelvic regions, designing four specific sub-tasks. All comparative models (Pix2Pix (Isola et al. 2017), CycleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a), 7 Figure 6: The loss visualization (a) and performance comparison (b) with respect to different visual perceptions. Vmamba (Liu et al. 2024e), and DiffMa (Wang et al. 2024b)) trained separate model for each sub-task, while HealthGPT unify all tasks into single training process. The experimental results, shown in Table 11, demonstrate that our approach outperforms other methods across multiple evaluation metrics. For instance, in the CT2MRI-Brain task, HealthGPT-M3 achieves an SSIM of 79.38, significantly surpassing traditional methods like Pix2Pix (71.09) and the recent DiffMa (71.47). (ii) Super-Resolution: We conduct 4 super-resolution experiments on the IXI dataset, with the results presented in Table 3. Notably, most existing methods fail to fully leverage the prior knowledge of key structures in medical images, resulting in significant shortcomings in detail recovery. In contrast, our method significantly mitigates this issue. Specifically, HealthGPT-M3 excels in key metrics such as SSIM, PSNR, and ISE, achieving scores of 78.19, 32.76, and 34.47, respectively. Additionally, HealthGPT-M3 achieves the lowest score of 12.34, further validating its exceptional performance in human visual perception. (iii) Reconstruction: We compare HealthGPT-M3 with unified models with reconstruction capabilities, such as Unified-IO 2 and SEED-X. The results show that our approach performs better controllability for visual reconstruction. We also train HealthGPT-L14 with similar number of trainable parameters to the M3 version. Hence, the similar performance between the two models meets our expectations. Details are in the Appendix. 5. In-Depth Study Effect of Heterogeneous Low-Rank Adaptation. H-LoRA provides an optimized multi-LoRA architecture for multitask learning. We conduct extensive validation of this structure, with results presented in Table 4, comparing the performance of LoRA, MoELoRA, and H-LoRA in medical unified comprehension and generation tasks. In the majority of comprehension tasks and all generation tasks, H-LoRA demonstrates superior performance, particularly in the OmniMedVQA benchmark, where it improved from 64.90 to 68.50. Notably, despite some applications of MoELoRA in certain scenarios, it do not show advantages in this task and Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual comprehension and generation tasks. Model VQA-RAD all close SLAKE all close Comp. PathVQA all close MMMU -Med Gen. OMVQA RECOM MTRANS SR Training Time HealthGPT w/ 71.3 +LoRA +MoELoRA 72.5 73.7 +H-LoRA 57.2 57.2 55.9 70.0 66.4 74. 53.4 52.4 56.4 76.4 73.2 78.7 38.6 36.0 39.7 41.30 39.30 43.30 65.10 64.90 68.50 62.67 67.31 67. 59.99 59.76 60.30 65.88 65.91 66.14 1.00 1.49 1.00 Table 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach. Training Strategy VQA-RAD all close SLAKE all close Comp. PathVQA all close MMMU -Med OMVQA CT MRI Brain Pelvis Brain Pelvis Gen. HealthGPT w/ Mixed-Training 3-stage-Training 56.6 72.5 37.9 55.2 45.0 77.9 32.9 59.6 65.7 79.7 33.6 49. 44.0 42.7 48.9 68.5 65.64 70.84 62.75 72.99 56.61 65.26 50.77 61. Figure 7: Case study of report-to-CXR under different instructions. (a) shows normal CXR image for comparison. (b) and (c) illustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions. had training time approximately 50% longer than LoRA. Figure 5 illustrates the performance of the three PEFT methods in medical visual comprehension and generation tasks across different ranks, with H-LoRA consistently outperforming the other methods in all scenarios, demonstrating significant advantages in handling diverse tasks. Different Learning Strategy. We propose three-stage learning strategy for H-LoRA that decouples comprehension and generation tasks. Unlike methods that train both tasks simultaneously, our approach reduces performance degradation from task conflicts (see Table 5). In the medical visual comprehension task, mixed training causes catastrophic forgetting and degrades visual reconstruction, whereas our strategy effectively uses the medical embedding knowledge in pre-trained LLMs to mitigate these conflicts. Meanwhile, we examine how fusing heterogeneous H-LoRA plugins in the second training stage results in minimal performance degradation. Detailed results are in the Appendix. Hierarchical Visual Perception Analysis. We conduct an ablation analysis on visual perceptual inputs for comprehension and generation tasks. Figure 6 shows that comprehension tasks converge more efficiently with abstract-grained inputs, while generation tasks perform better with concretegrained inputs. This highlights the importance of the hierarchical visual perception we propose, suggesting that tailoring visual inputs for specific tasks at different hierarchies can significantly improve efficiency. Report-to-CXR Task. We further explore the medical image generation task without reference images, using small amount of MIMIC-CXR data (Johnson et al. 2019) for instruction fine-tuning. Figure 7 annotates images with varying injury degrees and locations, comparing them to healthy CXR images. We observe that HealthGPT effectively generates CXR images based on the instructions, showcasing its potential in healthcare education and auxiliary diagnosis."
        },
        {
            "title": "6 Conclusion\nHealthGPT introduces an innovative PEFT approach by\nintegrating collaborative and competitive modules, which\nsignificantly improves the efficiency and effectiveness of\nmulti-task learning. In the proposed CME benchmark tests,\nHealthGPT not only achieves faster response speed but\nalso outperforms existing multi-LoRA architectures in per-\nformance. Future research will further explore the game-",
            "content": "8 theoretic framework based on competition and collaboration in multi-LoRA architectures, expanding the potential of PEFT. Ge, Y.; Ge, Y.; Zeng, Z.; Wang, X.; and Shan, Y. 2023. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041."
        },
        {
            "title": "References",
            "content": "Abdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gunasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.; Kauffmann, P.; et al. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Bae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.; Oh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-ExtMIMIC-CXR-VQA: Complex, Diverse, And Large-Scale Visual Question Answering Dataset for Chest X-ray Images. Chen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen, G. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al. 2024a. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280. Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong, W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. Chern, E.; Su, J.; Ma, Y.; and Liu, P. 2024. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135. Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.; Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv:2305.06500. Davies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E. A. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The IXI Dataset. Accessed: 2025-01-30. Ding, N.; Qin, Y.; Yang, G.; Wei, F.; Yang, Z.; Su, Y.; Hu, S.; Chen, Y.; Chan, C.-M.; Chen, W.; et al. 2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3): 220 235. Dong, R.; Han, C.; Peng, Y.; Qi, Z.; Ge, Z.; Yang, J.; Zhao, L.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Esser, P.; Rombach, R.; and Ommer, B. 2021. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1287312883. Ge, Y.; Zhao, S.; Zhu, J.; Ge, Y.; Yi, K.; Song, L.; Li, C.; Ding, X.; and Shan, Y. 2024. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396. He, X.; Zhang, Y.; Mou, L.; Xing, E.; and Xie, P. 2020. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Hu, Y.; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y.; and Luo, P. 2024. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2217022183. Isola, P.; Zhu, J.-Y.; Zhou, T.; and Efros, A. A. 2017. Imageto-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 11251134. Johnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren, M. P.; Deng, C.-y.; Peng, Y.; Lu, Z.; Mark, R. G.; Berkowitz, S. J.; and Horng, S. 2019. MIMIC-CXR-JPG, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. Lau, J. J.; Gayen, S.; Ben Abacha, A.; and DemnerFushman, D. 2018. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1): 110. Li, B.; Xue, K.; Liu, B.; and Lai, Y.-K. 2023a. Bbdm: Imageto-image translation with brownian bridge diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, 19521961. Li, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang, J.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llavamed: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36. Li, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang, J.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llavamed: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36. Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 1973019742. PMLR. Lin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y.; Li, H.; Yu, Z.; He, W.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting lowrank adaptation with expert collaboration and competition. arXiv preprint arXiv:2408.09856. Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y.; and Wu, X.-M. 2021. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), 16501654. IEEE. Liu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y.; Li, H.; and Gao, P. 2024a. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657. Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024b. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2629626306. Liu, H.; Li, C.; Li, Y.; Li, B.; Zhang, Y.; Shen, S.; and Lee, Y. J. 2024c. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. https://llava-vl.github.io/blog/2024-0130-llava-next/. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruction Tuning. In NeurIPS. Liu, Q.; Wu, X.; Zhao, X.; Zhu, Y.; Xu, D.; Tian, F.; and Zheng, Y. 2024d. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1104 1114. Liu, Y.; Tian, Y.; Zhao, Y.; Yu, H.; Xie, L.; Wang, Y.; Ye, Q.; and Liu, Y. 2024e. VMamba: Visual State Space Model. arXiv preprint arXiv:2401.10166. Lu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.; Hoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 26439 26455. Lu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi, A. 2022. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations. Luo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu, K. 2024a. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2402.12851. Luo, Y.; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y.; Qiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multimodal large language model for biomedicine. IEEE Journal of Biomedical and Health Informatics. Masoudnia, S.; and Ebrahimpour, R. 2014. Mixture of experts: literature survey. Artificial Intelligence Review, 42: 275293. Moor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y.; Leskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), 353367. PMLR. Nath, V.; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu, Y.; Liu, Z.; Yin, H.; Law, Y. M.; Tang, Y.; et al. 2024. Vilam3: Enhancing vision-language models with medical expert knowledge. arXiv preprint arXiv:2411.12915. OpenAI. 2023. GPT-4V(ision) System Card. https://cdn. openai.com/papers/GPTV System Card.pdf. Pan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.; Chua, T.-S.; Zhuang, Y.; and Zhang, H. 2024. AutoarXiv Encoding Morph-Tokens for Multimodal LLM. preprint arXiv:2405.01926. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PMLR. Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung, H. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972): 172180. Team, C. 2024. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818. Thawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal, H.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan, F. S. 2023. Xraygpt: Chest radiographs summarization arXiv preprint using medical vision-language models. arXiv:2306.07971. Thummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff, J. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.; and Maspero, M. 2023. SynthRAD2023 Grand Challenge dataset: Generating synthetic CT for radiotherapy. Medical physics, 50(7): 46644674. Tian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y. 2023. The role of large language models in medical image processing: narrative review. Quantitative Imaging in Medicine and Surgery, 14(1): 1108. Tong, S.; Fan, D.; Zhu, J.; Xiong, Y.; Chen, X.; Sinha, K.; Rabbat, M.; LeCun, Y.; Xie, S.; and Liu, Z. 2024. MetaMorph: Multimodal Understanding and Generation via Instruction Tuning. arXiv preprint arXiv:2412.14164. Tu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.; Chang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al. 2024. Towards generalist biomedical AI. NEJM AI, 1(3): AIoa2300138. Vig, J. 2019. multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714. Wang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y.; Wang, J.; Zhang, F.; Wang, Y.; Li, Z.; Yu, Q.; et al. 2024a. Emu3: 10 arXiv preprint Next-token prediction is all you need. arXiv:2409.18869. Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip: Contrastive learning from unpaired medical images and text. arXiv preprint arXiv:2210.10163. Wang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft Masked Mamba Diffusion Model for CT to MRI Conversion. arXiv preprint arXiv:2406.15910. Wu, C.; Chen, X.; Wu, Z.; Ma, Y.; Liu, X.; Pan, Z.; Liu, W.; Xie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation. arXiv:2410.13848. Wu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023. arXiv preprint Next-gpt: Any-to-any multimodal arXiv:2309.05519. Xie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.; Gu, Y.; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528. Young, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang, G.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024. arXiv preprint Yi: Open foundation models by 01. ai. arXiv:2403.04652. Zhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li, Y.; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. survey of large language models in medicine: Progress, application, and challenge. arXiv preprint arXiv:2311.05112. Zhu, J.-Y.; Park, T.; Isola, P.; and Efros, A. A. 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, 22232232. llm."
        },
        {
            "title": "Appendix",
            "content": "This is the Appendix for HealthGPT: Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation. This Appendix is organized as follows: Section presents the experimental implementation details, the training process of HealthGPT, and the specifics of VL-Health. Section systematically provides an analysis of Heterogeneous Low-Rank Adaptation. Section shows supplementary experimental results to validate the effectiveness of HealthGPT. A.1 Model Details"
        },
        {
            "title": "A Implementation Details",
            "content": "We employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as visual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing concrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated with text tokens and input into the large language models (LLMs). HealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al. 2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with 8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further augment the models capacity for understanding both visual and textual input. Figure 6 shows the details. Table 6: Overview of the Components of HealthGPT. ViT Model HealthGPT-M3 CLIP-L/14 HealthGPT-L14 CLIP-L/14 Adapter 2-layer MLP 2-layer MLP MLP-dims Model dims 1024 1024 3072 5120 LLM Phi-3-mini Phi-4 Params Vocab Size 3.8B 14B 40206 H-LoRA Rank 16(Comp.), 64(Gen.) 8(Comp.), 32(Gen.) A.2 Training Details In this study, we propose three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adaptation (H-LoRA). We provide detailed hyperparameter configuration for the models three-stage training process. The specific hyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the models learning efficacy and final performance. Table 7: Overview of Hyperparameter Configurations. Hyperparameter StageHealthGPT-M3 Stage-2 Stage-3 Stage-1 HealthGPT-L14 Stage-2 Stage-3 Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Optimizer Adapter LR Learning Rate Global Batch Size Weight Decay Dropout Rate LR Scheduler Max Sequence Length AdamW 1e-3 / 256 2e-5 2e-4 64 0 0. 0 Warm Up 2048 AdamW 2e-5 2e-4 32 0 0.05 Constant 2048 AdamW 2e-5 2e-4 128 64 0 0.05 Warm Up AdamW 1e-3 / 256 2e-5 1e-4 64 0 0.05 0 Warm Up AdamW 2e-5 2e-4 32 0 0.05 Constant 2048 AdamW 2e-5 2e-4 128 64 0 0.05 Warm Up 2048 It is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension and generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to this issue, which is the reason for the slight differences in hyperparameters between HealthGPT-M3 and HealthGPT-L14. 12 Figure 8: VL-Health dataset collection distribution. A.3 VL-Health The construction of the VL-Health dataset involves two key steps: (i) data collection, (ii) data processing, as detailed below: Data Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of the tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets such as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al. 2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations to assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal datasets like LLaVA-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical knowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream task categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction. The IXI (Davies et al. 2014) dataset, containing large number of healthy brain MRI images, is suitable for training superresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual reports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides large number of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we rewrote and adjusted the LLaVA-558k (Liu et al. 2024b) dataset. Data Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we standardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and evaluation. Additionally, considering that multi-image data has minimal impact on performance but introduces extra padding and training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extraction, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used VQGAN-generated indices to supervise the generation tasks. Data Statistics This section provides detailed statistical information about the VL-Health dataset to offer more comprehensive understanding. Data Overview: To ensure balanced development of the models comprehension and generation capabilities, in addition to the LLaVA-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802 additional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instructionfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation instruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, enhancing the models overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (approximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA (approximately 52,000 images), LLaVA-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 images). Multiple question-answer pairs were retained for each image to enhance the models understanding and generalization of the image content. Table 8 shows the data distribution of VL-Health for three-stage learning strategy, where mixed-47k is based on the sampling of all data in stage-1. Diversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultrasound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encompasses wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary diseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides comprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases. Table 8: Data distribution of VL-Health in three-stage learning strategy. Medical Task Comp. Gen. Medical Task Comp. Gen. Stage-2 Stage-1 LLaVA-558k, PubMedVision-PT LLaVA-558k Stage-3 LLaVA Med, MIMIC CXR VQA, PubMedVision-FT, LLaVA-665k, PathVQA, SLAKE, VQA-RAD IXI, SynthRAD2023, MIMIC-CHEST-XRAY Mixed-47k Data Format. All data samples are converted into unified instruction-response format for training and evaluation. Specifically, the VL-Health dataset consists of the following components: Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding HLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices. Task Instruction: Guides the model to interpret the image and generate response, covering various aspects of the image and specifying the output format. Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and formatting requirements. Input Image: Provides the visual signal for the model to process. Target Image Index: In generation tasks, this is added as multi-modal token to the response for autoregressive generation. Analysis of Heterogeneous Low-Rank Adaptation We propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across tasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based on MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1. Algorithm 1: H-LoRA Algorithm i=1, RComp. }k outer ), generation-based H-LoRA modules ({AGen. Input: concrete-grained visual features Con, abstract-grained visual features Abs, comprehension-based H-LoRA modules ({AComp. outer), task type (comprehension or generation), number of LoRA experts k, origin linear layer weights W0, text features , hidden state Output: final output // Select task-specific image features if = generation task then }k i=1, RGen. img Con else if = comprehension task then img Abs }k i=1, RT i=1, {BT i=1, {Bi}k outer // Assign task-specific H-LoRA submodule end if concat(F img, ) // Concatenate image features and text features {Ai}k }k i=1, Router {AT // Merge LoRA experts matrices Amerged concat({Ai}k Bmerged concat({Bi}k R(h) // Generate routing weights based on input hidden state expanded α W/r 1r // Expand routing weights to match merged matrices OH-LoRA (x Amerged expanded) Bmerged // Compute H-LoRA output using element-wise multiplication W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output Return i=1) i=1) We further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods use the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps involved. Computational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the following steps: (i) Expert Multiplication: MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multiplication: One multiplication with the Router is required. (iii) Router Output Expansion: MoELoRA needs to perform 14 expansion operations on the Routers output weights to generate the appropriate shapes that match the dimensions of the input and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, dot product with the intermediate state of the expert is required, resulting in multiplications. (v) Addition: Finally, addition operations are required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each operation is the same, the additional time complexity introduced when equipping fully connected layer with MoELoRA is: O(2k +1+k +k +k) = O(5k +1). Thus, MoELoRA introduces an additional time overhead of O(5k +1) during computation. H-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices. Specifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating larger and matrix, instead of performing independent operations for each expert. This process can be implemented through matrix initialization without additional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multiplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion: H-LoRA only requires one expansion operation on the Routers output weights. (iv) Dot Product: H-LoRA only requires one dot product between the Routers output and the experts intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation to accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by H-LoRA is: O(2 + 1 + 1 + 1 + 1) = O(6). Comparing the two, we see that MoELoRA introduces linear increase in additional time complexity with respect to the number of experts k, resulting in complexity of O(5k + 1), while H-LoRAs additional time complexity is fixed at O(6), independent of k. We observe that when is small, the time complexity differences between MoELoRA and H-LoRA are negligible. However, as increases, MoELoRAs computational overhead grows linearly, while H-LoRAs remains constant. This makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will further demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency in practical applications. Supplemental Experimental Results In this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design philosophy. C.1 Results: OmniMedVQA Benchmark OmniMedVQA (Hu et al. 2024) is novel, large-scale medical visual question answering (VQA) benchmark designed to encompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our experimental results are presented in Table 9. Table 9: Performance comparison of OmniMedVQA Benchmark. Type Model # Params Comp. Only Comp. & Gen. Med-Flamingo LLaVA-Med HuatuoGPT-Vision BLIP-2 LLaVA-v1.5 InstructBLIP Yi-VL InternVL2 Llama-3.2 Show-o Unified-IO 2 Janus HealthGPT-M3 HealthGPT-L14 8.3B 7B 7B 6.7B 7B 7B 6B 8B 11B 1.3B 7B 1.3B 3.8B 14B OmniMedVQA Medical LVLM CT X-ray FDM MiS OCT MRI USS Avg. 34.9 41.3 50.0 26.9 44.7 29.0 50.2 54.5 63.2 33.0 26.8 45.7 68.5 74.4 27.6 45.0 40.4 22.7 52.9 31.9 55.0 58.1 65.2 34.2 30.9 50.7 78.5 80. 60.0 55.3 59.3 29.1 49.2 38.6 67.6 59.1 62.5 30.8 32.6 54.2 89.3 99.7 37.0 31.6 62.3 36.9 42.1 30.6 62.6 64.0 82.1 22.0 25.3 62.7 88.2 88.6 33.9 32.8 41.5 29.1 55.7 22.2 47.1 57.9 55.2 50.4 37.7 54.8 81.9 86.6 30.4 53.6 60.1 21.4 49.7 25.5 40.3 49.1 68.6 33.8 37.7 36.8 51.4 62.2 25.5 42.7 51.4 22.3 35.5 34.1 27.7 53.2 71.4 30.9 12.3 35.9 54.6 64.1 30.1 28.4 35.3 26.6 28.0 20.1 51.2 40.2 37.6 29.0 10.8 24.9 35.3 39. Through our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7 sub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as 15 general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach effectively mitigates performance degradation caused by generation tasks, serving as unified model; (iii) HealthGPT-L14 excels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing other models. C.2 Stability Analysis of Number of Experts We investigated the impact of the number of LoRA experts on model performance within multi-LoRA architecture, conducting extensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented in Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When = 8, the training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs better. It is estimated that at = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from completing training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids introducing additional training delays compared to LoRA but also outperforms MoELoRA. Table 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At = 32, MoELoRA was unable to complete training. Model n=2 Comp. Gen. n=4 Time Comp. Gen. n=8 Time Comp. Gen. n=32 Time Comp. Gen. Time HealthGPT w/ +MoELoRA +H-LoRA 50.3 51.5 62.98 63.48 1.22 0.99 50.0 52. 64.33 64.71 1.49 1.00 50.8 53.6 63.71 64.98 2.09 0.99 / 53. / 64.74 5.81 1."
        },
        {
            "title": "Impact of Heterogeneous Knowledge Fusion on Performance",
            "content": "C.3 Traditional unified models often utilize mixed training methods, which may result in performance degradation due to variations in task modes. To address this, we propose three-phase learning strategy to support H-LoRA, effectively mitigating inter-task conflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA plugins into new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure 9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the second phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict issues arising from mixed training in medical scenarios. C.4 Human Evaluation. We further conduct human evaluation on the VQA-RAD, SLAKE, and PathVQA benchmarks, which contain 1,000 open-ended questions. Specifically, we recruit 5 clinicians to rank the randomly shuffled responses from HealthGPT-L14, LLaVA-Med, HuatuoGPTVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation, questions were randomly selected, and the model-generated responses were anonymized and ranked. The results, as shown in Figure 10, indicate that HealthGPT was frequently selected as the best answer. This suggests that HealthGPT has further application potential in medical care scenarios. C.5 Reconstruction Performance Currently, unified models that align visual features based on reconstruction tasks include pre-LVLMs, post-LVLMs, as well as UnifiedIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the controllability of visual generation in rigorous settings such as medical contexts, we evaluated the performance of these models in medical image reconstruction in Table 11. Experimental results demonstrate that HealthGPT exhibits the most stable reconstruction performance with small amount of data. 16 Figure 9: Performance changes before and after the stage-2. C.6 Case Study Figures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results generated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively guiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12 demonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the image. Table 11: The experimental results for the four reconstruction tasks. Model CT(Brain) SSIM PSNR MSE CT(Pelvis) SSIM PSNR MSE MRI (Brain) SSIM PSNR MSE MRI(Pelvis) SSIM PSNR MSE SEED-X Unified-IO 2 HealthGPT-M3 20.18 83.93 91.73 27.66 36.09 36.42 112.11 17.95 15.46 21.53 85.36 94.26 28.02 35.10 37. 102.87 25.46 12.53 4.90 87.50 88.76 27.62 34.25 33.97 112.86 25.47 27.05 6.31 86.31 84.40 27.89 33.53 33. 106.21 29.80 32.62 Figure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset. 17 Figure 11: Case of modality transfer. 18 Figure 12: Case of MRI image super-resolution."
        }
    ],
    "affiliations": [
        "Alibaba",
        "National University of Singapore",
        "The Hong Kong University of Science and Technology",
        "University of Electronic Science and Technology of China",
        "Zhejiang University"
    ]
}