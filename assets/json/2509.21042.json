{
    "paper_title": "Behind RoPE: How Does Causal Mask Encode Positional Information?",
    "authors": [
        "Junu Kim",
        "Xiao Liu",
        "Zhenghao Lin",
        "Lei Ji",
        "Yeyun Gong",
        "Edward Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 2 4 0 1 2 . 9 0 5 2 : r BEHIND ROPE: HOW DOES CAUSAL MASK ENCODE POSITIONAL INFORMATION? Junu Kim1,2 Xiao Liu2 Zhenghao Lin2 Lei Ji2 Yeyun Gong2 Edward Choi1 1 KAIST 2 Microsoft Research {kjune0322,edwardchoi}@kaist.ac.kr} {xiao.liu.msrasia,zhenghaolin,leiji,yegong}@microsoft.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While explicit positional encodings such as RoPE are primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPEs relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as source of positional information alongside explicit positional encodings."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformer decoders (Vaswani et al., 2017) with rotary positional embeddings (RoPE) (Su et al., 2024) have been widely adopted in recent large language models (LLMs) (Grattafiori et al., 2024; Abdin et al., 2024; Yang et al., 2025a). The way positional information is provided to model is known to be closely tied to model performance (Dufter et al., 2022) and its length generalization ability (Zhao et al., 2024). Consequently, recent work has sought to improve LLMs, in terms of both LLM performance (Barbero et al., 2025) and length generalization (Peng et al., 2024; Chen et al., 2023; Liu et al., 2024), by analyzing and modifying RoPE. However, these models contain another source of positional information: the causal mask. It is commonly viewed as mechanism that blocks access to future tokens, but it also provides positional information. Although the exact mechanism remains unclear (Zuo et al., 2025), recent studies have shown that models without explicit positional encodings can still model sequential data and even achieve performance comparable to models with RoPE (Haviv et al., 2022; Kazemnejad et al., 2023). Similar to RoPE, analyzing how the causal mask encodes positional information and its properties is crucial for understanding model behavior, as well as its implications for performance and length generalization. Although several recent studies have attempted to analyze how the causal mask encodes positional information (Haviv et al., 2022; Chi et al., 2023; Kazemnejad et al., 2023), its exact nature remains unclear (Zuo et al., 2025). Thus in this paper, we first prove that even without parameters, causal input dependencies, or feedforward network, the causal mask can induce position-dependent patterns in attention scores (Figure 1). These patterns consistently favor closer keys to each query, assigning them higher attention scores. This behavior closely resembles that of many explicit positional encoding schemes (Press et al., 2022; Su et al., 2024; Vaswani et al., 2017). Through empirical analysis, we then demonstrate that our explanation aligns well with practical outcomes and uncovers several useful characteristics. First, by simulating Transformer decoder without parameters and without explicit positional encoding, we confirm that our explanation accurately Work done during an internship at Microsoft Research Asia. 1Codes available at: https://github.com/starmpcc/causal mask encodes positional Figure 1: Causal mask induces positional information even in the absence of causal input dependencies, feed-forward networks, or parameters. (a) With the input assumption, the first-layer attention scores collapse to 1 on the diagonal and 0 elsewhere. (b) The attention output is then computed as weighted sum of the x(0) ). (c) After ℓ2 normalization, (d) subsequent-layer attention scores reveal clear position-dependent pattern, assigning higher weights to nearby querykey pairs, behavior similar to common positional encodings. (we omit the superscript (0) and simply write xi in place of x(0) captures how positional information emerges. We also find that the resulting position-dependent attention patterns exhibit properties similar to positional encodings, yet quite differ qualitatively from both conventional absolute and relative forms. Furthermore, training Transformer decoder on web corpus without explicit positional encodings shows that the emergence of positional information in practice is consistent with our explanation. However, while the underlying mechanism matches, we observe that in the real model the position-dependent attention patterns are strongly influenced by the learned parameters. In addition to our theoretical and empirical analysis on the properties of the causal mask, we study how it interacts with RoPE inside modern LLMs, which typically use both together. Simulations of parameter-free Transformer show that when combined with RoPE, the causal mask distorts RoPEs relative attention pattern into non-relative one. Our analysis further reveals that this non-relative pattern arises only in the presence of the causal mask. We consistently observe this phenomenon at non-negligible scale in modern LLMs that use RoPE, including Llama-3.1-8B (Grattafiori et al., 2024), Phi-4 (Abdin et al., 2024), and Qwen3-8B (Yang et al., 2025a). Our contributions are as follows: We prove that the causal mask can induce position-dependent patterns in attention scores, even in the absence of parameters, causal input dependencies, or feedforward network. Through empirical analysis, we demonstrate that our explanation accounts for the behavior of Transformer decoders without explicit positional encoding. We show that the causal mask biases RoPEs relative attention pattern toward non-relative one, and we observe this bias in modern LLMs. These results suggest that future research on positional information in Transformer decoders should account for the joint effects of both RoPE and the causal mask."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "A typical way to inject positional information into Transformers is through positional encodings. They generally encourage higher attention scores for querykey pairs that are closer together (Press et al., 2022; Su et al., 2024; Vaswani et al., 2017). Broadly, there are two types of positional encod2 ings. Absolute positional encodings assign information based on each tokens fixed position in the sequence, such as sinusoidal encoding and learnable absolute positional encodings (Vaswani et al., 2017). Relative positional encodings represent positions based on the distance between tokens, such as T5 relative PE (Raffel et al., 2020), ALiBi (Press et al., 2022), and RoPE (Su et al., 2024). Among these, RoPE has become widely adopted in recent LLMs (Abdin et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a), and its properties have inspired methods that improve both language modeling performance (Barbero et al., 2025; Yang et al., 2025b) and length generalization (Peng et al., 2024; Chen et al., 2023; Liu et al., 2024). Although positional encoding is typically considered the sole source of positional information in Transformer decoders, recent work has shown that the causal mask can also play this role. Haviv et al. (2022) first demonstrated that Transformer decoder can model natural language without explicit positional encodings, achieving performance comparable to the model with RoPE. Because the causal mask is typically viewed simply as mechanism for blocking access to future tokens, several works have attempted to uncover how it encodes positional information. In the same work, Haviv et al. (2022) hypothesized that the causal mask enables counting of predecessor tokens, though they did not provide proof. They also suggested that the positional information induced by the causal mask resembles absolute positional encoding. Kazemnejad et al. (2023) later proved that the causal mask can represent both absolute and relative encodings under specific parameter configuration. Also, they showed that the attention pattern without positional encoding is closely resemble to those of T5s relative positional embeddings. Chi et al. (2023) offered the first mathematical explanation, showing that the causal mask increases the variance of hidden states with token position. However, they did not clarify how Transformers exploit variance from single hidden state. More recently, Zuo et al. (2025) empirically showed that nearby hidden states exhibit higher cosine similarity than distant ones, and that this tendency is much stronger than variance change. Extending this line of work, we explain how the causal mask can encode positional information, thereby justifying the pattern observed by Zuo et al. (2025). We also show that the positional information from causal mask behavior quite differs from both absolute and relative positional encodings."
        },
        {
            "title": "3 THEORETICAL ANALYSIS",
            "content": "3.1 PRELIMINARIES Let the input token embeddings be (0) = [x(0) ] Rnd, where is the number of input tokens and is the model hidden size. Superscripts indicates layers; when clear from context, we omit them. Formally, single-head, pre-LN (Xiong et al., 2020) Transformer decoder layer without bias is function : Rnd Rnd with (l) = (l)(X (l1)) is defined as: 1 , , x(0) (l) = LayerNorm(X (l1)), = WQ, = WK, = WV , = Softmax(Causal( QK )), = (AV )WO + (l1), (l) = FFN(LayerNorm(O(l))) + O(l) Where WQ, WK, WV , WO Rdd are parameters, and operation Causal() applies strictly uppertriangular mask to prevent attention to future positions. 3.2 HOW DOES CAUSAL MASK ENCODE POSITIONAL INFORMATION? Here, we show that the causal mask can induce position-dependent pattern in attention score, even when the input sequence has no causal dependency, no parameters, and no feed-forward module. In addition, we show the pattern allocates higher attention scores to closer query-key pairs, akin to the behavior of typical positional encodings (Press et al., 2022; Su et al., 2024; Vaswani et al., 2017). Figure 1 sketches the high-level mechanism by which the causal mask encodes positional information. To simplify the derivation, we employ ℓ2 normalization (without the technique, and later show that LayerNorm (with the assume each input embedding vector x(0) term) as the normalization term) exhibits analogous behavior. We ) = α. has unit norm, and for = j, E(x(0) , x(0) i 3 Note that this assumption does not impose causal structure on the inputs and includes an i.i.d. case (α = 0). While α = 0 is sufficient for our core claim, we allow 0 α < 1 to better explain the trained models behavior (Section 4.2). Under these simplifications, single layer (X) acts as: (X) = Softmax(Causal(Y ))Y + X, = L2Norm(X), (1) where L2Norm denotes row-wise ℓ2 Normalization. The operator Causal() applies the strictly upper triangular mask so that query at position only attends to keys at position i, and Softmax is taken row-wise. Formally, our goal is to show that the pairwise inner product after normalization y(2) is function of the indices and (i.e. not constant across positions). , y(2) Since each input has unit norm, x(0) products and those after applying the causal mask is: , x(0) i = 1, we have (1) = (0). The Gram matrix of inner (Y (1)Y (1))i,j = (cid:26)1 (i = j) α (i = j) Causal(Y (1)Y (1))ij = 1 inf α (i = j) (i < j) (i > j) (2) The row-wise softmax then gives Softmax(Causal(Y (1)Y (1)))ij = (i, j) + (i 1)eα where (i, j) = 0 eα (i = j) (i < j) (i > j). Accordingly, = (2e + (i 1)eα)x(0) + (cid:80)i1 k=1 eαx(0) + (i 1)eα , x(1) , and then normalize by x(1) 2 and x(1) 2 to x(1) = (cid:80)i k=1 (i, k)x(0) + (i 1)eα + x(0) We first compute the raw inner product x(1) obtain y(2) , y(2) . (cid:0)(2e + (i 1)eα)x(0) x(1) , x(1) = For > j, + (cid:80)i k=1 eαx(0) (cid:1)(cid:0)(2e + (i 1)eα)x(0) + (cid:80)j1 l=1 eαx(0) (cid:1) (e + (i 1)eα)(e + (j 1)eα) (2e + (i 1)eα)(2e + (j 1)eα)x(0) +(2e + (i 1)eα) (cid:80)j1 , x(0) ℓ +(2e + (j 1)eα) (cid:80)i1 + (cid:80)i1 , x(0) (e + (i 1)eα)(e + (j 1)eα) ℓ=1 eαx(0) k=1 eαx(0) , x(0) k= (cid:80)j1 ℓ=1 e2αx(0) , x(0) ℓ (2e + (i 1)eα)(2e + (j 1)eα)α + (2e + (i 1)eα) eα(j 1)α +(2e + (j 1)eα) eα(cid:0)1 + (i 2)α(cid:1) + e2α(j 1)(cid:0)1 + (i 2)α(cid:1) (e + (i 1)eα)(e + (j 1)eα) α(2e + (i 1)eα) + eα(cid:0)1 + (i 2)α(cid:1)(cid:17) 2(cid:0)e + (j 1)eα(cid:1) (cid:16) (e + (i 1)eα)(e + (j 1)eα) (cid:16) 2 2αe + eα(cid:0)1 + α(2i 3)(cid:1)(cid:17) + (i 1)eα = g(i) x(1) , x(1) = = = = For x(1) 2, x(1) 2 2 = (2e + (i 1)eα)2 + 2(2e + (i 1)eα)eαα(i 1) + e2α(i 1)(1 + (i 2)α) (e + (i 1)eα)2 = h(i)2 (3) 4 Figure 2: Simulation of Transformer without parameter and explicit positional encoding results. We visualized averaged attention scores for each layer with α = 0 and 0.2. The y-axis represents query indices, and the x-axis represents key indices. Put everything together, we can get y(2) , y(2) = x(1) x(1) , x(1) 2x(1) 2 = g(i) h(i)h(j) (4) , y(2) Thus, y(2) is not constant across query-key indices i, j. This shows that the causal mask alone induces position-dependent inner product structure in the normalized representations, even without any causal assumptions on input, without parameters, and without feed-forward network. In other words, causal mask itself can serve as mechanism for encoding positional information. Next, we show that the position-dependent attention pattern induced by the causal mask at the second layer behaves similar to typical positional encodings. Typical positional encodings infuse bias into the attention score, making queries more strongly associated with nearby keys than with distant ones (Press et al., 2022; Su et al., 2024; Vaswani et al., 2017). Formally, we show the attention score in the second layer strictly increases on the key index over fixed query index i. From Equation 4, for such an i, the g(i)/h(i) term becomes constant, so the score depends only on 1/h(j). We can compute h(j + 1) h(j) and verify that h(j) decreases strictly with (see Appendix A.1). As result,the attention score at the second layer increases strictly with j, as long as < i. In the case = 1, since the inner product of two normalized vectors with different directions is always less than 1, we obtain y(2) , y(2) i1 < y(2) , y(2) = 1. Therefore, for any fixed i, the attention score increases strictly with on the range in layer 2. In other words, closer keys receive higher scores, matching the behavior of common positional encodings. We empirically show the behavior of later layers in the following section."
        },
        {
            "title": "4 EMPIRICAL ANALYSIS",
            "content": "4.1 TRANSFORMER SIMULATION WITHOUT PARAMETERS We further examine the behavior of positional information from the causal mask through simulation of Transformer without parameters and without positional encodings, as defined in Equation 1. Specifically, we sampled 50 vectors with = 64 that satisfy Equation 2 in expectation. Each vector is generated by combining shared Gaussian component with an independent Gaussian noise 5 component, ensuring the desired inner-product structure without introducing causal dependency. Figure 2 presents the simulated attention scores across layers for α = 0 and α = 0.2, averaged over 100,000 simulations. We also conducted the same experiment with ℓ2 Normalization replaced by LayerNorm, and observed similar tendencies, as confirmed in Appendix B. First, consider the case α = 0, corresponding to the first row of Figure 2. Under our standing assumption, the attention matrix of the first layer has ones on the diagonal and zeros elsewhere. In the second layer, position-dependent pattern begins to emerge, consistent with our earlier derivations. Notably, the attention score strictly increases for with fixed i, consistent with our theoretical analysis. However, as shown in Figure 1, the exact across-position differences are still small at this stage, yielding only faint pattern in the upper-left of the matrix. By layers three and four, the pattern becomes more pronounced, and also strictly increasing for with fixed i, resembling the behavior of common positional encodings. This attention pattern aligns with Figure 1 in Zuo et al. (2025), which illustrates that cosine similarity between hidden states in Transformer decoder without positional encoding is higher for closer i, pairs. Our analysis explains this phenomenon, since cosine similarity is equivalent to the inner product after ℓ2 normalization. Next, we examine the case α = 0.2, illustrated in the second row of Figure 2. While strict increase in the attention score for (with fixed) also occurred, the overall pattern is bit different. At the second layer, the position-dependent pattern is much clearer compared to the α = 0 case, but the pattern seems nearly independent of i. This effect occurs because, when α = 0, the numerator and denominator share identical highest-order terms in g(i), causing rapid convergence to fixed value. The same saturation is more evident in the later layers. In both cases, we observed that the attention scores within each off-diagonal band (i.e., excluding the main diagonal) were highly non-uniform. Since relative positional encodings inject information based on the token distance, attention scores within the diagonal are expected to remain uniform when the input is zero-mean Gaussian noise without learnable parameters (see Appendix Figure 10 for an example with RoPE). This discrepancy indicates that the causal mask behaves quite differently from relative positional encodings. On the other hand, Wang & Chen (2020) showed that absolute positional encodings, including sinusoidal and learnable embeddings, yield attention score heatmaps symmetric along the bottom-left to top-right axis. In the case of the causal mask, however, the heatmap does not satisfy such symmetry. Taken together, the behavior of positional information from the causal mask quite differs from both typical absolute and relative positional encodings. 4.2 ANALYSIS OF TRAINED MODEL WITHOUT POSITIONAL ENCODING We conducted an empirical study to examine whether similar attention patterns arise when training Transformer decoder without positional encoding. To this end, we trained model based on the Llama-3 architecture (Grattafiori et al., 2024) having 1.5B parameters (22 layers, hidden dimension 2048, head dimension 64) on 20 billion tokens from the Fineweb-Edu corpus (Penedo et al., 2024). The model was trained with an AdamW optimizer (Loshchilov & Hutter, 2019), cosine learningrate scheduler with warmup, peak learning rate of 3 104, global batch size of 1M tokens, and context length of 1024. Since input embeddings in the language model contain no positional information (Dufter et al., 2022), we analyze the representations from the input embeddings up to the Q(2)K (2) in order to examine how position-dependent attention score patterns emerge. Figure 3 presents the Gram matrix heatmap of inner products of the attention intermediates. The attention patterns from later layers are shown in Appendix Figure 8. For those figures, we used 1,000 snippets sampled from held-out set drawn from Fineweb-Edu, each consisting of the first 50 tokens of document. First, because most embeddings are nearly orthogonal to one another, their inner products after LayerNorm are close to zero except along the main diagonal (a). This corresponds to the case α = 0 in our earlier formulation. According to the simulation results in the previous section (Figure 2), under this condition the position-dependent patterns in the second-layer attention scores should appear only faintly. In contrast, the trained model exhibits much stronger, i-independent pattern in Q(2)K (2) (i), which more closely resembles the α = 0 case. This is due to the learned parameters affect the position-dependent pattern induced by the causal mask. Moving from (a) to (b), we observe that the ratio of off-diagonal to diagonal values increases substantially. This effect is attributed to the influence of WQ and WK, as have similar effect to Figure 3: Inner-product Gram matrix heatmap of trained Transformer decoder without positional encoding. We sample 1,000 sequences of length 50 from the held-out set, compute attention intermediates, and then calculate averaged inner products across heads and samples. increasing α. Although vivid line appear at the 0th column, it can be interpreted as an attention sink phenomenon (Xiao et al., 2024), the analysis of which is beyond the scope of this paper. After applying the causal mask, the softmax, and multiplying by , the resulting attention map in (c) shows that the main diagonal values decrease with increasing i, j, while the off-diagonal values remain nearly uniform across positions. The diagonal corresponds to h(i)2 with the residual term removed, while the off-diagonal corresponds to g(i) with the residual term removed. We denote these by h(i) and g(i), respectively. Both can be computed in manner similar to h(i) and g(i), and they exhibit analogous properties (Appendix A.2). In particular, h(i) decreases strictly with i, accounting for the decline along the diagonal, whereas g(i) is nearly independent of and only weakly dependent on i, explaining the uniformity of the off-diagonal elements. In case of (d), the ratio between the diagonal and off-diagonal elements is significantly increased compared to (c). This can also be interpreted as the effect of WO, analogous to the relationship between (a) and (b). After adding the residual (e), the trend is almost identical to (d). This behavior can be explained in terms of h(i) and g(i). Figure 4: Simulation of Transformer without parameters using RoPE. We visualize the attention scores across layers, where the y-axis denotes query indices and the x-axis denotes key indices. The top row shows the raw attention scores, while the bottom row shows scores normalized by subtracting the mean of each diagonal. Once LayerNorm is applied (f), the pattern closely matches the theoretical and simulation results, where attention increases strictly with for fixed i. Passing through the feedforward network and applying the residual (g) appears to laterally invert the pattern, whereas subsequent LayerNorm restores it (h). Since we excluded the feedforward network from our theoretical analysis, we cannot account for why it inverted the relationship, nor for how LayerNorm reversed it. However, note that the position-dependent pattern has already emerged at (f), and therefore does not conflict with our theoretical analysis. Finally, in the second-layer self-attention (i), we clearly observe the expected pattern of strict increase with i, in agreement with our theoretical analysis. Although the behavior of (g) and (h) remains unexplained, our analysis shows how positional information emerge from trained Transformer without explicit positional encoding. Also, we confirmed that the positional patterns induced by the causal mask are strongly parameter-dependent."
        },
        {
            "title": "INTERACTION BETWEEN CAUSAL MASK AND ROPE",
            "content": "In the previous section, we showed that the causal mask can induce position-dependent attention patterns. Building on this, we now analyze how RoPE, widely used in modern LLMs, interacts with the causal mask. We first use simulations to examine the attention patterns that emerge from this interaction, and then check whether the same patterns appear in modern LLMs. 5.1 TRANSFORMER SIMULATION WITHOUT PARAMETERS We extended our earlier simulation in Section 4.1 by applying RoPE, with θRoPE = 10000 and α = 0. The results with non-zero α are provided in Appendix Figure 9. As shown in the first row of Figure 4, the first layer behaves as described by Barbero et al. (2025): when the input is independent Gaussian noise, RoPE alone does not affect the inner products. However, from the second layer onward, vivid relative pattern emerges due to the mixing of the input vectors caused by the first layers self-attention operation. By the third layer, this relative pattern becomes even clearer. Notably, similar to our earlier experiments without explicit positional encoding, the left portion of the attention maps appears darker than other regions. To highlight this effect, we averaged attention score along each diagonal and subtracted them and displayed on the second row, which revealed the pattern even more distinctly. Importantly, because RoPE is relative positional encoding, such patterns do not appear without causal mask (as in Transformer encoders; see Appendix Figure 10). This indicates that the causal mask biases the relative attention pattern from RoPE toward non-relative one. 8 Figure 5: Diagonal-normalized attention heatmap of LLMs (first 4 layers). Attention scores were computed for 1,000 sequences of length 1,024 and averaged across sequences and heads. The Attention Sink effect flattens the overall pattern, so the color scale is adjusted using the 1% and 99% quantiles. The y-axis denotes query indices, and the x-axis denotes key indices. 5.2 ANALYSIS OF LLMS We analyze whether the same phenomenon is observed in modern LLMs trained with RoPE, including Llama-3.1 8B Grattafiori et al. (2024), Phi-4 (Abdin et al., 2024), and Qwen3-8B (Yang et al., 2025a). Using the same setup as done at section 4.2, we performed inference on 1,000 samples from the same Fineweb-Edu (Penedo et al., 2024) held-out set, except increased sample length of 1024. Figure 5 shows the averaged attention scores for the first four layers of each model. Additional results, including the normalized and original scores for all layers, are provided in Appendix E. Consistent with our simulation, we can observe the non-relative pattern among the models except for the first layer. Also, considering the typical attention score of these models are in [101, 101] scale (Appendix E) and the non-relative patterns in [1, 1] scale, this effect could not be dismissed as negligible. This confirms that the attention patterns in practice are influenced by both RoPE and the causal mask, meaning that the Transformer Decoder trained with RoPE both relies on the positional information from RoPE and the causal mask. Since this pattern non-uniformly emphasizes the first few keys, it may potentially negatively affect length generalization."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Positional information is crucial for both the performance and length generalization of Transformer decoderbased models. While positional encodings are typically regarded as the primary source of positional information, the causal mask also conveys such information. In this work, we show how the causal mask encodes positional information and that its behavior is closely tied to common positional encodings. We further show that the causal mask influences the positional information derived from RoPE. Our study, however, has two key limitations. First, although Transformer decoder layers include feed-forward networks and learnable parameters, their effects have been only limitedly analyzed. Second, it remains unclear whether the interaction between RoPE and the causal mask directly affects performance and length generalization. Overall, our findings highlight the causal mask as critical source of positional information, alongside explicit positional encodings such as RoPE. Exploring methods to jointly leverage the causal mask and explicit positional encodings may yield additional gains in LLM performance and length generalization."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. Latent positional information is in the self-attention variance of transformer language models without positional embeddings. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 11831193, 2023. Philipp Dufter, Martin Schmitt, and Hinrich Schutze. Position information in transformers: An overview. Computational Linguistics, 48(3):733763, 2022. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models withIn Findings of the Association for out positional encodings still learn positional information. Computational Linguistics: EMNLP 2022, pp. 13821390, 2022. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. In The Twelfth International Conference on Learning Representations, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yu-An Wang and Yun-Nung Chen. What do position embeddings learn? an empirical study of pretrained language model positional encoding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 68406849, 2020. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pp. 1052410533. PMLR, 2020. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr Locatelli. Rope to nope and back again: new hybrid attention strategy. arXiv preprint arXiv:2501.18795, 2025b. Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, and Ting Liu. Length extrapolation of transformers: survey from the perspective of positional encoding. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 99599977, 2024. Chunsheng Zuo, Pavel Guerzhoy, and Michael Guerzhoy. Position information emerges in causal transformers without positional encodings via similarity of nearby embeddings. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 94189430, 2025."
        },
        {
            "title": "A ADDITIONAL DERIVATIONS",
            "content": "A.1 STRICTLY DECREASING OF h(j) ON From Equation 3 h(j)2 = (2e + (j 1)eα)2 + 2(2e + (j 1)eα)eαα(j 1) + e2α(j 1)(1 + (j 2)α) (e + (j 1)eα)2 h(j + 1)2 h(j)2 = = (cid:0)(2e + jeα)2 + 2(2e + jeα)eααj + e2αj(1 + (j 1)α)(cid:1)(e + (j 1)eα)2 (cid:0)(2e + (j 1)eα)2 + 2(2e + (j 1)eα)eαα(j 1) + e2α(j 1)(1 + (j 2)α)(cid:1) (e + jeα)2 (e + jeα)2(e + (j 1)eα)2 (cid:0)(α1)e4α +2(α1)e3α+1(cid:1)j2 +(cid:0)(2α6)eα+3 +2(α1)e3α+1 +(α1)e4α(cid:1)j + (2α 4)eα+3 + (2α + 4)e2α+2 (e + jeα)2(e + (j 1)eα)2 . Since 0 α < 1, each coefficient (of j2, j, and the constant term) is negative. The denominator is trivially positive, hence h(j + 1)2 h(j)2 < 0, so h(j + 1) h(j) < 0 ( h(j) > 0). Thus, h(j) is strictly decreasing on j. A.2 h(i) AND g(i) Without residual network, single layer (X) acts as: (X) = Softmax(Causal(Y )Y Accordingly, x(1) = (cid:80)i k=1 (i, k)x(0) + (i 1)eα = ex(0) + (cid:80)i1 + (i 1)eα k=1 eαx(0) In same manner with above calculation, we can finally get g(i) = x(1) , x(1) = eα + eα(1 + α(i 2) + (i 1)eα , x(1) h(i)2 = x(1) = e2 + 2e1+αα(i 1) + e2α(i 1)(1 + α(i 2)) (e + (i 1)eα)2 We also can confirm h(i) is strictly decreasing on i, by showing h(i + 1)2 h(i) = (α 1)eα(2e3 2e2+α(i 1) + e3αi(i 1)) (e + ieα)2(e + (i 1)eα) < 0."
        },
        {
            "title": "ANALYSIS",
            "content": "First, Figure 6 displays extension of Figure 2 with additional α values. With higher α value, the pattern faster saturate rapidly to constant on j. scaling, instead of ℓ2 normalNext, we conducted the same experiment using LayerNorm with ization without scaling. The results are shown in Figure 7. Since LayerNorm normalizes vectors times larger than with ℓ2 to have norm of normalization. This makes the softmax distribution sharper than in the ℓ2 case, thereby reducing the influence of positional information introduced by the causal mask. However, simply changing the scaling factor from to recovers behavior similar to ℓ2 normalization, validating our findings. d, the inner product after scaling becomes Figure 6: Extended result of Figure 2. 13 Figure 7: Simulation of Transformer without parameter and explicit positional encoding results, with LayerNorm. We replaced ℓ2 normalization to LayerNorm, and applied scaling (first and second row) and scaling (third and fourth row)."
        },
        {
            "title": "POSITIONAL ENCODING",
            "content": "Figure 8: Attention pattern of trained Transformer without explicit positional encoding. The plots are drawn with same manner to Figure 3."
        },
        {
            "title": "D ROPE ATTENTION PATTERN ANALYSIS RESULTS",
            "content": "Figure 9: The extended result of Figure 4 with α = 0.5 Figure 10: The extended result of Figure 4 without causal mask (i.e. Transformer Encoder)"
        },
        {
            "title": "E LLM ROPE PATTERNS",
            "content": "16 Figure 11: Llama-3-8B Attention Pattern 17 Figure 12: Phi-4 Per-Layer Attention Pattern 18 Figure 13: Qwen3-8B Per-Layer Attention Pattern 19 Figure 14: Llama-3-8B Attention Pattern (Normalized) 20 Figure 15: Phi-4 Attention Pattern (Normalized) 21 Figure 16: Qwen-8B Attention Pattern (Normalized)"
        }
    ],
    "affiliations": [
        "KAIST",
        "Microsoft Research"
    ]
}