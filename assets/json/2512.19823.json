{
    "paper_title": "Learning to Refocus with Video Diffusion Models",
    "authors": [
        "SaiKiran Tedla",
        "Zhoutong Zhang",
        "Xuaner Zhang",
        "Shumian Xin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io"
        },
        {
            "title": "Start",
            "content": "SAIKIRAN TEDLA, Adobe, USA & York University, Canada ZHOUTONG ZHANG, Adobe, USA XUANER ZHANG, Adobe, USA SHUMIAN XIN, Adobe, USA 5 2 0 2 2 2 ] . [ 1 3 2 8 9 1 . 2 1 5 2 : r Fig. 1. We present method for refocusing images using video diffusion models to predict focal stacks, allowing users to select their preferred focus. Trained on large smartphone dataset captured in diverse real-world environments, our model generates realistic defocus effects and performs effectively on real scenes. Focus is cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce novel method for realistic post-capture refocusing using video diffusion models. From single defocused image, our approach generates perceptually accurate focal stack, represented as video sequence, enabling interactive refocusing and unlocking range of downstream applications. We release large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io CCS Concepts: Computing methodologies Computational photography; Image processing. Additional Key Words and Phrases: Image refocusing; video diffusion models ACM Reference Format: SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin. 2025. Learning to Refocus with Video Diffusion Models. In SIGGRAPH Asia 2025 Conference Papers (SA Conference Papers 25), December 1518, 2025, Hong Kong, Hong Kong. ACM, New York, NY, USA, 11 pages. https://doi.org/10. 1145/3757377.3763873 Authors Contact Information: SaiKiran Tedla, tedlasai@yorku.ca, Adobe, USA & York University, Canada; Zhoutong Zhang, zhoutongz@adobe.com, Adobe, USA; Xuaner Zhang, cecilia77@berkeley.edu, Adobe, USA; Shumian Xin, sxin@adobe.com, Adobe, USA. This work is licensed under Creative Commons Attribution-NonCommercialNoDerivatives 4.0 International License. SA Conference Papers 25, Hong Kong, Hong Kong 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2137-3/2025/12 https://doi.org/10.1145/3757377."
        },
        {
            "title": "Introduction",
            "content": "Focus is pivotal element of photography, yet modern autofocus systems can easily fail in dynamic or complex scenes. Photographers frequently wish to shift the focal plane post-capture, whether for aesthetic or corrective reasons. Existing techniques for adjusting focus typically require specialized hardware such as light field cameras [Levoy et al. 2006; Ng et al. 2005; Zhang et al. 2018] or rely on multiple images [Abuolaim and Brown 2020; Alzayer et al. 2023] and depth maps [Fiss et al. 2014; Moreno-Noguer 2007]. While these methods are effective, they demand resources and expertise beyond the reach of most users. Moreover, many approaches focus on producing all-in-focus images [Chen et al. 2022; Lee et al. 2021; Zamir et al. 2022], which do not replicate the natural defocus effects found in optical systems. In this paper, we present novel method for post-capture refocusing that requires only single defocused image and leverages video diffusion models. As illustrated in Figure 1, our approach produces realistic focal stacka sequence of images with progressively shifting focusenabling interactive refocusing via simple slider. Refocusing, especially from severe blur, is inherently ill-posed. This observation prompts us to treat it as generative task. We cast refocusing as multi-frame generation problem, where the focal stack is modeled as temporally coherent video sequence. As seen in Figure 2, we observe that pre-trained video diffusion models have priors for doing focus pulls. We hypothesize these priors are helpful for performing refocusing in variety of scenarios. Thus, we propose to use video diffusion models to reconstruct the entire focal stack conditioned on just single defocused input. By introducing minimal modification to the classifier-free guidance SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. 2 SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin Fig. 2. Illustration showing that video diffusion models have priors on generating focal stacks. The video sequence is generated by prompting model [Adobe Inc. 2025] to generate focus pull of bug on leaf. mechanism, our method consistently yields perceptually realistic focal stacks across range of challenging scenarios. To facilitate the development and evaluation of our approach, we introduce new large-scale focal stack dataset containing 1,637 samples captured on an iPhone 12 under variety of real-world conditions. This dataset offers practical benchmark for refocusing techniques, covering diverse scenes and lighting scenarios. Although its size is relatively modest, we leverage the strong priors embedded in pre-trained video diffusion models to achieve robust performance. Our method consistently produces perceptually realistic results and remains resilient across challenging capture conditions. By bridging state-of-the-art generative modeling with everyday smartphone photography, we deliver advanced refocusing capabilities to broad user base, expanding the potential for post-capture editing. We summarize our contributions as follows: In Section 3, we introduce large-scale focal stack dataset of real-world scenes captured on modern smartphone. In Section 4, we propose novel approach to adapt video diffusion models for refocusing by casting the task as focal stack prediction. In Section 5, we demonstrate that our method allows realistic refocusing, enabling both defocus deblurring and natural defocus effects."
        },
        {
            "title": "2 Related work",
            "content": "In this section, we review existing methods for manipulating focus and depth of field in images and then discuss why diffusion models provide compelling solution for refocusing."
        },
        {
            "title": "2.1 Focus and depth-of-field manipulation",
            "content": "Image refocusing. The primary prior work addressing refocusing from single image is RefocusGAN [Sakurikar et al. 2018]. It employs two-stage approach: first, predicting an all-in-focus (AiF) image, then generating defocused image at the desired focus. RefocusGAN can construct full focal stack by iteratively applying the second stage. RefocusGAN is trained exclusively on synthetic data. In contrast, our method directly produces full focal stacks in one pass and is trained on real-world smartphone captures, leading to greater realism in everyday photography scenarios. Light field photography. Light field imaging captures the spatialdirectional information of light rays in scene, enabling post-capture refocusing [Levoy et al. 2006; Ng et al. 2005; Ruan et al. 2021]. However, such methods require specialized hardware and are less practical for mainstream consumer photography. SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Defocus deblurring and defocus blur generation. Defocus deblurring techniques aim to transform out-of-focus images into AiF versions using standard [Chen et al. 2022; Lee et al. 2021; Ruan et al. 2024; Wang et al. 2022; Zamir et al. 2022] or diffusion-based restoration networks [Chen and Liu 2025; Feng et al. 2025; Kong et al. 2025]. Other methods utilize dual-pixel data and multiplane representations to enable refocusing [Xin et al. 2021]. On the opposite end, bokeh and defocus generation approaches rely on an AiF image and depth map to produce defocused results [Ignatov et al. 2020; Peng et al. 2022; Seizinger et al. 2023]. Alzayer et al. [Alzayer et al. 2023] instead use dual captures (wide and ultrawide) to jointly estimate an AiF image, depth map, and editing control. In contrast, our goal is to generate full focal stack from single defocused image, eliminating the need for multiple inputs or sensor-specific data. Controllable image generation. Recently work [Fang et al. 2024; Yuan et al. 2025] has explored controllable text-to-image generation conditioned on camera parameters, such as focal length, aperture, and shutter speed. Fang et al. [Fang et al. 2024] briefly discuss refocusing via SDEdit [Meng et al. 2022], the method does not consistently preserve content identity and often alters the original scene. Focal stack and light field datasets. Most prior methods for depthof-field manipulation [Levoy et al. 2006; Peng et al. 2022; Sakurikar et al. 2018; Sheng et al. 2024] use synthetic focal stacks or light-field data, which may not reflect real-world photography conditions. Currently, the largest publicly available real-world dataset [Herrmann et al. 2020] contains 510 focal stacks captured with Google Pixel 3. To advance research in this domain, we introduce new largescale dataset comprising 1637 focal stacks captured under diverse real-world scenarios using an iPhone 12."
        },
        {
            "title": "2.2 Diffusion models",
            "content": "Generative image enhancement and editing. Diffusion models have demonstrated their effectiveness in tasks such as image restoration and generation, including super-resolution [Sahak et al. 2023] and impainting [Lugmayr et al. 2022; Zhu et al. 2023]. Similar to our refocusing task, diffusion models can be used for deblurring [Chen and Liu 2025; Ren et al. 2023; Zhu et al. 2023]; however, these methods typically only convert blurred image into an all-in-focus image. Thus, they require modifications or additional conditioning mechanisms to predict focal stacks. Diffusion models are particularly powerful for image editing, leveraging text-based controls [Kawar et al. 2023; Zhang et al. 2023] or parameter sliders [Guerrero-Viu et al. 2024]. Their generative flexibility makes them especially appealing for creative edits or tasks involving significant content restoration. Generative camera control. Our work is inspired by those that finetune video diffusion models for pose-conditioned generation [Bahmani et al. 2025; He et al. 2025]. These works contain no explicit 3D constraints and only rely on implicit priors within the video model. Our approach also models focal stacks implicitly by utilizing the prior from video diffusion models and our data. Generative video interpolation. Our approach treats refocusing as video-generation problem, partly inspired by generative video interpolation research. For instance, pixel-based video diffusion Learning to Refocus with Video Diffusion Models Fig. 3. Eight sample focal stacks from our dataset consisting of 1637 total scenes. More focal stacks are visualized in supplementary material montage. models can interpolate between given start and end frames [Jain et al. 2024], while latent video diffusion methods incorporate autoencoder features [Danier et al. 2024] or motion priors [Huang et al. 2024]. These methods primarily focus on frame interpolation, whereas our task requires extrapolation from single defocused image to produce an entire focal stack."
        },
        {
            "title": "3 Large focal stack dataset",
            "content": "We introduce new focal stack dataset that, to our knowledge, is the largest publicly available collection of real-world smartphone focal stacks."
        },
        {
            "title": "3.1 Data collection",
            "content": "The dataset consists of 1,637 scenes, each captured with custom rig of five iPhones (Figure 11) similar to Herrmann et al. [Herrmann et al. 2020]. Each phone simultaneously records focal stack and an ultrawide shot for each scene, creating rich source of multi-view data. We collect focal stacks by moving the iPhones focus from its minimum to maximum distance in nine increments. In the iPhone API, this corresponds to focus values from 0 to 0.8, in steps of 0.1. Although the API supports values up to 1.0, we found 0.8 to be the typical upper bound in practical scenarios. For the current work, we leverage only the central cameras focal stack. However, the full dataset (including ultrawide captures and all cameras) will be publicly available to support future research needing depth or stereo information. The dataset comprises 1,637 scenes split into 1,474 training and 163 test focal stacks. large portion focuses on macro photography, where the shallow depth-of-field enables the richest focal shifts on the iPhone. To ensure diversity and realism, we capture both controlled and in-the-wild scenes. Controlled shots are taken in Fig. 4. The original focal stack exhibits misalignment caused by focal breathing, evident from the noticeable edge movements. Our pre-processing corrects these distortions, resulting in well-aligned stack that enables more accurate refocusing. lightboxes with fixed cameras, stable lighting, and minimal scene motion, while natural scenes come from varied environments (e.g., homes, bookstores, universities) with changing lighting and slight background motion. All images are captured in RAW format at resolution of 40323024, maintaining consistent exposure and white balance settings."
        },
        {
            "title": "3.2 Data processing",
            "content": "We apply post-processing to ensure high-quality alignment and realism within each focal stack. The initial focal stack contains focal breathing, resulting in slight changes in the field of view and distortion at each focal position. We perform careful processing to remove these focal breathing effects as seen in Figure 4, because without this correction, the model would have to learn how to perform field of view changes and distortion corrections in addition to SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. 4 SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin refocusing. In the supplementary, we show that our model and baselines fail to attain good refocusing results without our processing steps. Raw conversion. We begin by converting each RAW image to the sRGB color space using Adobe Camera Raw (ACR) [Adobe 2025]. Distortion correction. Next, we apply calibrated radial distortion profile per focus setting to correct lens aberrations at each step of the focal stack. Alignment. Due to focal breathing, images are not perfectly aligned after distortion correction. We address this by applying pre-calculated scale transformation so that all images in the stack align with the first frame (which has the smallest field of view). All-in-focus image generation. Finally, we generate an AiF image for each stack using Helicon Focus [HeliconFocus 2025] in depth mode, providing high-quality reference for evaluating defocus deblurring methods. After these processing steps, each scene in our dataset is represented by nine aligned focal stack images and one AiF image, offering comprehensive benchmark for real-world refocusing research. sampling of scenes is shown in Figure 3, and more scenes are visualized in the supplementary material dataset montage."
        },
        {
            "title": "4 Method",
            "content": "We aim to synthesize perceptually realistic focal stack from single defocused image. Key to our approach is the observation that focal stack can be viewed as video, where each frame corresponds to different focus distance. Leveraging this perspective, we employ video diffusion model to jointly reconstruct the entire focal stack in one sampling process. This contrasts RefocusGAN [Sakurikar et al. 2018], which refocuses one frame at time. In Section 4.1, we provide an overview of diffusion models as the foundation of our work. In Section 4.2, we detail our modifications to the classifier-free guidance (CFG) mechanism of state-of-the-art video diffusion model [Blattmann et al. 2023], enabling robust and realistic single-shot generation of focal stacks."
        },
        {
            "title": "4.1 Preliminaries",
            "content": "Latent diffusion models. Diffusion models learn data distributions by reversing forward Gaussian noising process. While pixel diffusion models [Ho et al. 2022] operate directly in the high-dimensional pixel space, latent diffusion models [Blattmann et al. 2023] work within compressed latent space ùíõ, generated by pre-trained variational auto-encoder (VAE) [Rombach et al. 2022]. This significantly reduces computational overhead, especially for processing video data. In this work, we adopt Stable Video Diffusion (SVD) [Blattmann et al. 2023] for its simplicity, effectiveness, and public availability. In latent diffusion models (LDMs), the denoiser ùúñùúÉ is trained to reverse the noising process at each timestep ùë°. Let ùíõùë° be the noisy latent state at timestep ùë°, and let ùúñ (0, 1) be the Gaussian noise. The training objective is to minimize the discrepancy between the predicted noise and the true noise: ùêøùêøùê∑ùëÄ := EùúñN (0,1),ùíõùë° ,ùë° (cid:104) ùúñ ùúñùúÉ (ùíõùë°, ùë°)2 2 (cid:105) . (1) SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Classifier-free guidance. Classifier-free guidance (CFG) [Ho and Salimans 2021] is mechanism that allows diffusion models to incorporate additional conditioning signals, such as starting frame or text prompt. In latent video diffusion models, the denoiser ùúñùúÉ (ùíõùíï, ùíÑ, ùë°) is trained with conditioning input ùíÑ. During sampling, the preùúñùúÉ (ùíõùë° , ùíÑ, ùë°) is computed as linear combination of condicted noise (cid:101) ditional and unconditional noise estimates: ùúñùúÉ (ùíõùë°, ùíÑ, ùë°) = (1 + ùë§)ùúñùúÉ (ùíõùë°, ùíÑ, ùë°) ùë§ùúñùúÉ (ùíõùë° , ùë°), (cid:101) (2) where ùë§ is weight scalar that balances the influence of the conditioned and unconditioned outputs. In SVD, the video latent ùíõùë° is sequence of per-frame latents , . . . , ùíá ùêπ ], where ùíá ùëù represents the latent for frame ùëù in ùíõùë° = [ùíá 1 video with ùêπ frames. The model conditions on the first frame latent ùíá 1 by replicating it across all frames: ùúñùúÉ = (1 + ùë§)ùúñùúÉ (ùíõùë° , [ùíá 1 (cid:101) , . . . , ùíá 1], ùë°) ùë§ùúñùúÉ (ùíõùë° , [0, . . . , 0], ùë°). (3) This approach enables single network to handle both conditioned and unconditioned noise estimates by zeroing out the conditioning tensor for the unconditioned case."
        },
        {
            "title": "4.2 Modified classifier-free guidance",
            "content": "Our goal is to generate the entire focal stack from single image. To accomplish this, we condition video diffusion model on one selected focal position from fixed-length stack in our dataset, and then, in single sampling pass, predict all frames including the selected one. In the original SVD, conditioning is restricted to the first frame by replicating its latent across all frames, effectively ignoring positional information. This replication introduces ambiguity: if focal position ùíá ùëù is chosen randomly for each training batch and duplicated across the entire stack, the model sees inconsistent focal positions and struggles to learn clear mapping between the input and output focal positions. To overcome this limitation, we introduce position-dependent conditioning. Specifically, rather than replicating the chosen latent ùíá ùëù across all positions, we place it only at the corresponding position ùëù in the conditioning tensor, while setting all other positions to zero (Figure 5). Formally, for focal position ùëù: ùúñùúÉ = (1 + ùë§)ùúñùúÉ (ùíõùë° , [0, . . . , ùíá ùëù, . . . , 0], ùë°) ùë§ùúñùúÉ (ùíõùë° , [0, . . . , 0], ùë°). (4) (cid:101) Our approach takes inspiration from VIDIM [Jain et al. 2024], which concatenates frames along the time axis for video interpolation and treats conditioning frames as part of the input sequence. However, unlike interpolation methods that typically assume fixed start and end frames, our task demands flexibility, as the input frame can appear at any position in the focal stack. By ensuring the UNet [Blattmann et al. 2023] learns to reconstruct focal stacks aligned with the input focal position, our modification enables position-dependent refocusing. During training, we randomly select focal positions ùëù for each batch, ensuring the model covers the entire range of the stack. This approach effectively leverages the known focal stack structure while introducing only minor modifications to the existing diffusion framework. Architecture details are given in the supplementary. Learning to Refocus with Video Diffusion Models 5 image, and the output to be full focal stack. Finally, as image diffusion baselines, we use InstructPix2Pix (IP2P) [Brooks et al. 2023] and Swintormer [Chen and Liu 2025]. IP2P utilizes prompts for editing control. We train IP2P by providing input and output focus paired images along with prompts such as Change focal position 1 to focal position 4. Swintormer is minimally modified to allow for focal stack prediction. More details are given in the supplementary. All networks are trained for 200,000 epochs with batch size of 4 using our large focal stack dataset and the same splits. Each batch element uses randomly selected focal position index for the input frame. For the RefocusGAN reimplementations, we train both networks for 200,000 epochs. Training is performed on full images resized to 896 640 pixels rather than patches to account for the slight shifts in radial distortion that occur within the stack, even with our data processing pipeline. We use the Adam optimizer for all methods. For the baseline methods, we use learning rate of 0.0001 and an L2 loss. Our method uses learning rate of 0.0004 and trains the denoising process on random timestep per iteration, also with an L2 loss. During training, we retrain only the UNet transformer blocks in our model and and apply an exponential moving average (EMA) [Blattmann et al. 2023] with decay rate of 0.00001. At test time, we sample our model once using CFG weighting of 1.5. For the IP2P baseline, we performed grid search to identify the best hyperparameters, selecting text guidance scale of 7.5 and an image guidance scale of 3.0."
        },
        {
            "title": "5.2 Results on our dataset",
            "content": "Qualitative results. We visualize sample results in Figure 6 and use NAFNet [Chen et al. 2022] for comparison (additional comparisons with NAFNet and IP2P are available in our supplementary demo) as it outperformed other baselines in most scenarios. The generative nature of our model enables it to generate/hallucinate high-frequency details more effectively than baseline methods. For instance, in the second row, the dolls hairs appears more realistic and are not blurred into the background as seen in the baselines. Similarly, coarser structures, such as the bricks in the background of the first row and the pine cone in the third row, are reconstructed with better perceptual realism. By leveraging generative modeling and score-based objectives, our method utilizes strong priors about the world to deblur and introduce realistic defocus. In contrast, baseline methods suffer from regression-to-the-mean effect as described by Milanfar and Delbracio [2023]. This effect occurs when refocusing over larger focal position jumps, where multiple plausible solutions exist for the refocused image. As result, non-generative methods average these solutions, resulting in the blurring artifacts observed in the baseline results in Figure 6. Quantitative results. Our evaluation focuses on the perceptual quality of refocused images, primarily using LPIPS and FID metrics. We utilize FID as it is common metric in generative tasks that measures the realism of the generated image distribution to the ground truth distribution. Thus, we computed FID between the set of generated images and set of ground-truth images. It is important to note that distortion metrics such as PSNR are not optimized by generative methods. Thus, our diffusion-based approach performs 2-4dB worse than deblurring methods in PSNR (see supplementary) SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Fig. 5. Illustration of our modified classifier-free guidance for focal stack with ùêπ = 3 frames. During training, the latent corresponding to randomly selected focal position is passed to the video diffusion model at the matching position, while all other conditioning inputs are set to zero."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate our method across diverse tasks to demonstrate its perceptual realism, share-readiness, and versatility. Section 5.1 details implementation and comparisons with state-of-the-art methods. Section 5.2 presents qualitative, quantitative, and ablation results on our dataset. Section 5.3 tests real-world use with user-provided images. Section 5.4 examines generalization to various camera types. Section 5.5 explores downstream tasks like all-in-focus generation, synthetic DoF, and motion deblurring. Section 5.6 discusses limitations and future directions. 5."
        },
        {
            "title": "Implementation details",
            "content": "We compare our method against two reimplementations of RefocusGAN [Sakurikar et al. 2018] and modified deblurring networks [Chen et al. 2022; Zamir et al. 2022]. Since RefocusGAN lacks publicly available code, we reimplemented its two-stage pipeline: the first network constructs the all-in-focus image, and the second network performs blurring, conditioned on the all-in-focus image, input image, and focal position. We evaluate two versions of RefocusGAN: one using NAFNet [Chen et al. 2022] (RGAN-N) and another using Restormer [Zamir et al. 2022] (RGAN-R). We also implement strong baseline using minimally modified (just edits to first/last convolutions) versions of these two deblurring networks tailored to our problem. Standard deblurring networks dont predict focal stacks and instead accept 3-channel inputs and predict an allin-focus image. Thus, we modify the input to be sparse focal stack concatenated along the channel dimension, with only one non-zero 6 SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin Fig. 6. Refocusing results from various input focal positions to output focal positions. We compare our method to RGAN-NAF [Sakurikar et al. 2018], NAFNet [Chen et al. 2022], and ground truth (GT). Our method reconstructs coarse structures in the pine-cone (third row) and fine details in hair (second row). SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Table 1. Quantitative evaluations of refocusing methods on our focal stack dataset. Best results are highlighted in bold. Learning to Refocus with Video Diffusion Models 7 LPIPS FID Refocusing from focal position Method F1 F2 F3 F4 F6 F7 F8 F9 F2 F4 F5 F6 F7 F8 Ablation IP2P Swintormer RGAN-Rest RGAN-NAF Restormer NAFNet Ours Ablation IP2P Swintormer RGAN-Rest RGAN-NAF Restormer NAFNet Ours Ablation IP2P Swintormer RGAN-R RGAN-N Restormer NAFNet Ours 0.246 0.163 0.006 0.008 0.009 0.006 0.006 0.088 0.363 0.219 0.188 0.175 0.169 0.164 0.159 0.144 0.368 0.228 0.209 0.199 0.196 0.188 0.184 0. 0.359 0.194 0.161 0.157 0.149 0.144 0.140 0.141 0.313 0.217 0.169 0.160 0.155 0.147 0.144 0.139 0.347 0.229 0.203 0.194 0.190 0.183 0.178 0.157 0.409 0.224 0.190 0.186 0.179 0.173 0.170 0.165 0.260 0.216 0.151 0.146 0.142 0.134 0.131 0.139 0.334 0.239 0.201 0.196 0.191 0.184 0.179 0. 0.452 0.262 0.218 0.214 0.203 0.201 0.195 0.190 0.210 0.214 0.125 0.120 0.118 0.111 0.106 0.138 0.329 0.251 0.203 0.201 0.196 0.191 0.185 0.174 0.485 0.289 0.249 0.245 0.232 0.232 0.224 0.212 0.174 0.214 0.003 0.003 0.005 0.006 0.005 0.115 0.328 0.263 0.208 0.209 0.203 0.200 0.193 0. 0.508 0.306 0.277 0.274 0.259 0.260 0.251 0.229 0.232 0.238 0.134 0.132 0.126 0.124 0.118 0.160 0.328 0.269 0.207 0.209 0.203 0.203 0.196 0.192 0.514 0.305 0.285 0.282 0.266 0.268 0.259 0.232 0.275 0.253 0.169 0.169 0.160 0.160 0.156 0.171 0.310 0.256 0.187 0.189 0.183 0.186 0.179 0. 0.504 0.288 0.268 0.265 0.250 0.249 0.242 0.219 0.488 0.262 0.239 0.235 0.223 0.220 0.213 0.196 F1 59.40 26.38 0.540 0.61 0.60 0.57 0.52 10.76 87.13 26.25 26.19 24.74 22.43 20.69 20.44 21.20 Refocusing from focal position 0.298 0.250 0.176 0.177 0.166 0.164 0.160 0.168 0.315 0.232 0.170 0.170 0.160 0.157 0.154 0.156 81.30 37.85 0.260 33.26 31.19 28.37 26.86 23.35 66.99 35.54 37.59 28.56 27.85 24.40 23.27 21.77 Refocusing from focal position 9 0.276 0.226 0.143 0.143 0.133 0.135 0.133 0. 0.279 0.189 0.004 0.004 0.006 0.006 0.005 0.010 90.06 41.16 0.280 45.76 43.65 40.74 37.57 27.37 84.49 39.48 45.05 45.32 42.88 40.36 36.79 28.05 102.5 32.14 34.12 32.94 32.05 30.68 31.42 27.40 53.02 34.20 30.42 22.72 21.77 19.81 19.07 21.56 83.16 41.01 42.82 44.68 43.24 40.70 37.07 28. 114.7 40.88 40.68 39.45 37.78 37.76 37.65 31.78 38.21 30.48 23.02 14.05 13.46 11.44 10.57 20.17 82.89 42.69 41.93 44.16 42.77 40.62 37.91 29.33 125.1 47.19 48.51 47.44 45.49 45.78 44.44 37.16 32.58 30.78 14.72 0.19 0.25 0.61 0.43 16.63 82.76 42.57 40.73 41.32 40.52 38.94 36.53 29. 133.3 50.60 56.07 55.80 53.36 53.17 51.51 42.61 43.89 32.81 15.58 14.99 13.61 12.08 11.74 23.67 80.74 41.67 38.30 36.83 36.43 35.89 33.95 30.69 137.5 52.59 59.23 60.76 57.83 57.26 55.42 44.67 55.85 38.87 23.80 24.19 22.29 21.77 21.75 26.48 73.82 37.82 34.93 29.36 28.36 29.36 27.76 28. 134.0 51.31 55.96 58.32 56.21 54.87 53.44 43.39 64.35 42.82 27.64 29.06 26.89 25.70 25.11 27.54 63.75 32.11 28.30 17.39 15.93 16.37 16.27 24.33 126.7 46.19 49.38 51.57 50.56 47.51 46.33 36.34 74.36 39.58 28.18 30.20 28.02 25.59 25.17 25.62 66.35 28.73 18.32 0.33 0.27 0.55 0.43 12. when refocusing, consistent with observations in other generative works [Sahak et al. 2023; Wang et al. 2024]. significant factor contributing to this drop is latent-space compression by the VAE, as our model struggles in the identity case (reconstructing input focus). However, in designing our method, we prioritize perceptual plausibility over exact pixel fidelity, accepting modest losses in reconstruction accuracy to achieve more visually appealing results. In Table 1, we compare our methods performance on refocusing tasks across different focal positions in the focal stack: the first (position 1), the middle (position 5), and the last (position 9). Our SVD-based approach achieves superior perceptual quality for larger focal position shifts, such as refocusing from positions 1 or 9. However, at position 5, where many scenes are naturally in focus, and rich in high-frequency details, baseline deblurring networks perform better due to their direct operation in the pixel domain, preserving these details more effectively. By contrast, VAE compression in our method struggles with such fine-grained reconstruction. We observe our method outperforms IP2P and Swintormer, which suggests that image diffusion may be missing priors that video diffusion models have on focal stacks or image diffusion may require separate contribution with carefully constructed conditioning setup. Despite challenges with high-frequency details at small refocusing distances, our results highlight the value of generative methods for refocusing over large refocusing distances, particularly in realworld scenarios where users frequently shift focus between the foreground and background. Gamma and noise sensitivity. We additionally find our method to be robust under variations in tone mapping (gamma) and noise levels. Specifically, we test two gamma correction settings (ùõæ = 0.5 and ùõæ = 2) and additive Gaussian noise with ùúé = 0.05 applied to the input images. Quantitative and qualitative results are reported in the supplementary. User study. Additionally, we conduct user study at refocusing distances greater than 4 positions (as this is the regime in which our method performs well). We show users the input image and two refocused versions: NAFNet (the best baseline) and ours. We then ask users to vote for the version they prefer. Order is randomized per trial. We used 20 random scenes and had 20 users. Our method received 88.25% of votes and NAFNet got 11.75% of votes. This study shows our method generates user-preferred refocusing results. Ablation study. We explore an alternative way of conditioning on the input focal position by replacing our position-dependent scheme with the original motion profile ID from SVD [Blattmann et al. 2023]. In this ablation, we assign the motion profile ID to the input focal position and replicate the input image latent across all frames as in the original SVD approach. As seen in Table ??, this setup fails to learn to refocus and merely reproduces the input image in every video frame. This ablation highlights the necessity of our position-dependent conditioning with modified CFG for generating valid focal stacks. SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. 8 SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin Fig. 7. Qualitative results of refocusing images provided by everyday iPhone users. We compare against the best baseline, NAFNet [Chen et al. 2022]. Our method has superior refocusing results and handles hard scenes with humans (bottom left) or thin structures (bottom right)."
        },
        {
            "title": "5.3 Results on everyday photos",
            "content": "We aim to evaluate our model on scenes collected from casual users, so we gathered small set of images from everyday iPhone users and tested our method. When focal position metadata was available, we used the frame closest to the captured focal position as the conditioning signal. When metadata was unavailable, we sampled the model by placing the conditioning frame at each focal position and selected the output that produced the most reasonable result through manual inspection. Additionally, we find our model works well with rough estimates of input focal position (near is position 1 and far is position 9), and we discuss further in the supplementary. Refocusing results for these user-provided images are in Figure 7. For instance, when refocusing the bread (top left), our method reveals natural char on the breads surface. In another example, caustics in the glass (top right) are corrected during refocusing, and bokeh effects from reflected lights in the glass are effectively reconstructed. Our method also performs well on scenes with faces, preserving fine details in hair (bottom left). Finally, we demonstrate our models capability in challenging scene (bottom right), where the user struggled to focus on thin plant in their hand; our method successfully focuses the desired subject despite the difficulty."
        },
        {
            "title": "5.4 Generalization to other cameras",
            "content": "Our method generalizes well to images captured by other smartphone cameras, film cameras, and DSLRs. We demonstrate refocusing results on real-life scenes captured with these devices. Because the focal position metadata for these devices does not align with iPhone focal positions, we apply the same procedure described in Section 5.3 to refocus their images. Sample results from various cameras are shown in Figure 8. Our method qualitatively outperforms SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. state-of-the-art methods across all examples and reconstructs details in eyes (left col., middle row), hair (right col.), and pottery (middle col.), while preserving the original color profiles of the photos. We visualize additional qualitative comparisons on lightfield datasets in the supplementary."
        },
        {
            "title": "5.5 Downstream tasks",
            "content": "Depth-of-field manipulation. Our methods ability to generate full focal stacks in one shot enables range of depth-of-field (DoF) editing applications, including creating all-in-focus (AiF) images. We use HeliconFocus to generate AiF images by providing the full focal stack or selectively control the DoF by passing subset of the focal stack. An example of defocus editing is visualized in Figure 12, where our method produces more perceptually accurate AiF and modified DoF images compared to the baseline. Our approach effectively reconstructs coarse details, such as the spaces between the boxes, and fine details, like the coloring on the birds beak. Motion Deblurring. Our method can also handle cases with slight motion blur when performing refocusing. Figure 9 shows an example with foreground motion blur from scene motion. When refocusing to bring the background into focus, we observe that the foreground motion blur is reduced. Refocusing the image back to its original focus further reduces the motion blur. Notably, the nail of the person holding the bird and the birds wing appear sharp, with minimal motion blur after refocusing twice. Our intuition for this behavior is that diffusion models learn to generate images within the data distribution trained on. Since our dataset contains images without motion blur, our model also learns to generate images without motion blur. Learning to Refocus with Video Diffusion Models 9 Fig. 8. Qualitative results showcasing our methods ability to generalize across different cameras, including film cameras, DSLRs, and smartphones. We compare against the best baseline, NAFNet [Chen et al. 2022]. Our method produces more realistic results than NAFNet across all the cameras."
        },
        {
            "title": "5.6 Failure cases and limitations",
            "content": "Failure cases. Our method does not generalize very well to scenes exhibiting extreme defocus or bokeh effects, which are common in images captured by DSLRs with big apertures. Such levels of defocus are not present in our iPhone training dataset, limiting the models ability to generalize to these scenarios. As shown in Figure 10, our method fails to correct the exaggerated bokeh in DSLR images. Nonetheless, we are confident that by later incorporating datasets from large-aperture cameras into the training set, our method can effectively learn to refocus such images and handle the challenges posed by extreme defocus. Finally, our model is limited to refocusing images, and when applied to video frames, temporal consistency is not enforced, leading to temporal artifacts (see supplementary)."
        },
        {
            "title": "6 Conclusions",
            "content": "We present novel application of video diffusion models to postcapture refocusing, offering solution to reconstruct realistic focal stacks from single defocused images. Our approach modifies the classifier-free guidance mechanism and surpasses existing methods in perceptual quality under larger focal position changes and demonstrates robustness across challenging scenarios. It performs well for refocusing images provided by users from everyday settings and generalizes to cameras beyond the training set, such as film cameras, DSLRs, and other smartphones. Beyond refocusing, our method proves effective for DoF editing and motion blur reduction. We will also release dataset of 1,637 real focal stacks captured on smartphone, featuring RAW-format images with realistic lighting variations and everyday environments, providing valuable benchmark for refocusing methods. Future work. We now discuss two future directions for this work. First, our method struggles at reconstructing detail at small focal position changes because we utilize latent diffusion model. We believe this could be improved by using pixel diffusion model, latent model with better autoencoder, or hybrid. Second, incorporating data from large-aperture cameras and additional sensors could improve generalization to scenarios with various defocus properties, potentially allowing aperture size or focal length as an additional conditioning factor for learning to refocus."
        },
        {
            "title": "Acknowledgments",
            "content": "A special thanks to Karanpreet Raja for building the focal stack viewer. Additionally the authors would like to thank Siddhu Tedla, Keshav Pandiri, Trevor Canham, Justin So, Giuliana Mariano, and Yashi Uppalapati for providing images. SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. 2023. Multiscale structure guided diffusion for image deblurring. In CVPR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. Lingyan Ruan, Martin B√°lint, Mojtaba Bemana, Krzysztof Wolski, Hans-Peter Seidel, Karol Myszkowski, and Bin Chen. 2024. Self-Supervised Video Defocus Deblurring with Atlas Learning. In SIGGRAPH. Lingyan Ruan, Bin Chen, Jizhou Li, and Miu-Ling Lam. 2021. AIFNet: All-in-Focus Image Restoration Network Using Light Field-Based Dataset. IEEE TCI 7 (2021). Hshmat Sahak, Daniel Watson, Chitwan Saharia, and David Fleet. 2023. Denoising diffusion probabilistic models for robust image super-resolution in the wild. arXiv preprint arXiv:2302.07864 (2023). Parikshit Sakurikar, Ishit Mehta, Vineeth Balasubramanian, and PJ Narayanan. 2018. RefocusGAN: Scene refocusing using single image. In ECCV. Tim Seizinger, Marcos Conde, Manuel Kolmet, Tom Bishop, and Radu Timofte. 2023. Efficient multi-lens bokeh effect rendering and transformation. In CVPR. Yichen Sheng, Zixun Yu, Lu Ling, Zhiwen Cao, Xuaner Zhang, Xin Lu, Ke Xian, Haiting Lin, and Bedrich Benes. 2024. Dr.Bokeh: DiffeRentiable Occlusion-aware Bokeh Rendering. In CVPR. Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. 2024. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision (2024), 121. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. 2022. Uformer: general u-shaped transformer for image restoration. In CVPR. Shumian Xin, Neal Wadhwa, Tianfan Xue, Jonathan Barron, Pratul Srinivasan, Jiawen Chen, Ioannis Gkioulekas, and Rahul Garg. 2021. Defocus map estimation and deblurring from single dual-pixel image. In CVPR. Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, and Stanley Chan. 2025. Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis. In CVPR. Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2022. Restormer: Efficient Transformer for High-Resolution Image Restoration. In CVPR. Chi Zhang, Guangqi Hou, Zhaoxiang Zhang, Zhenan Sun, and Tieniu Tan. 2018. Efficient auto-refocusing for light field camera. Pattern Recognition 81 (2018). Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. 2023. Sine: Single image editing with text-to-image diffusion models. In CVPR. Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. 2023. Denoising diffusion models for plug-and-play image restoration. In CVPR. 10 SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, and Shumian Xin"
        },
        {
            "title": "References",
            "content": "Abdullah Abuolaim and Michael Brown. 2020. Defocus deblurring using dual-pixel data. In ECCV. Adobe. 2025. Adobe Camera Raw. https://helpx.adobe.com/ca/camera-raw/. Adobe Inc. 2025. Adobe Firefly. https://www.adobe.com/products/firefly.html. Accessed: 2025-05-09. Hadi Alzayer, Abdullah Abuolaim, Leung Chun Chan, Yang Yang, Ying Chen Lou, JiaBin Huang, and Abhishek Kar. 2023. DC2: Dual-camera defocus control by learning to refocus. In CVPR. Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. 2025. VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control. ICLR (2025). Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In CVPR. Kang Chen and Yuanjie Liu. 2025. Efficient Defocus Deblurring Networks based on Diffusion Models. In ICASSP. Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. 2022. Simple Baselines for Image Restoration. In ECCV. Duolikun Danier, Fan Zhang, and David Bull. 2024. LDMVFI: Video frame interpolation with latent diffusion models. In AAAI. Mauricio Delbracio and Peyman Milanfar. 2023. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. TMLR (2023). I-Sheng Fang, Yue-Hua Han, and Jun-Cheng Chen. 2024. Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models. In SIGGRAPH Asia. Haoxuan Feng, Haohui Zhou, Tian Ye, Sixiang Chen, and Lei Zhu. 2025. Residual Diffusion Deblurring Model for Single Image Defocus Deblurring. In AAAI. Juliet Fiss, Brian Curless, and Richard Szeliski. 2014. Refocusing plenoptic images using depth-adaptive splatting. In ICCP. Julia Guerrero-Viu, Milos Hasan, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero, Diego Gutierrez, Belen Masia, and Valentin Deschaintre. 2024. Texsliders: Diffusion-based texture editing in clip space. In SIGGRAPH. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. 2025. CameraCtrl: Enabling Camera Control for Text-to-Video Generation. In ICLR. HeliconFocus. 2025. HeliconFocus. https://www.heliconsoft.com/ Charles Herrmann, Richard Strong Bowen, Neal Wadhwa, Rahul Garg, Qiurui He, Jonathan Barron, and Ramin Zabih. 2020. Learning to autofocus. In CVPR. Jonathan Ho and Tim Salimans. 2021. Classifier-free diffusion guidance. NeurIPS Workshops (2021). Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. 2022. Video diffusion models. NeurIPS. Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, and Wenming Yang. 2024. Motion-aware latent diffusion models for video frame interpolation. In ACM ICM. Andrey Ignatov, Jagruti Patel, and Radu Timofte. 2020. Rendering natural camera bokeh effect with deep learning. In CVPRW. Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. 2024. Video interpolation with diffusion models. In CVPR. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing with diffusion models. In CVPR. Lingshun Kong, Jiawei Zhang, Dongqing Zou, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, and Jinshan Pan. 2025. DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models. arXiv preprint arXiv:2502.03810 (2025). Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun Cho, and Seungyong Lee. 2021. Iterative filter adaptive network for single image defocus deblurring. In CVPR. Marc Levoy, Ren Ng, Andrew Adams, Matthew Footer, and Mark Horowitz. 2006. Light field microscopy. In SIGGRAPH. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion probabilistic models. In CVPR. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2022. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In ICLR. Moreno-Noguer. 2007. Active Refocusing of Images and Videos. In SIGGRAPH. Ren Ng, Marc Levoy, Mathieu Br√©dif, Gene Duval, Mark Horowitz, and Pat Hanrahan. 2005. Light field photography with hand-held plenoptic camera. Ph. D. Dissertation. Stanford university. Juewen Peng, Zhiguo Cao, Xianrui Luo, Hao Lu, Ke Xian, and Jianming Zhang. 2022. BokehME: When neural rendering meets classical rendering. In CVPR. SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Learning to Refocus with Video Diffusion Models 11 Fig. 9. We observe that our method can mitigate slight motion blur when refocusing. In the first image, both the hand and bird lie within the depth of field yet appear blurry due to motion. By refocusing to the background (focal position 9), these elements become noticeably sharper. When we subsequently refocus this output back to the original focal position, the hand and bird retain their improved clarity, indicating that the motion blur is substantially reduced. Fig. 10. Our method struggles with extreme defocus levels that exceed those present in our training dataset. When attempting to refocus in such cases, it cannot accurately recover fine detailslike the illuminated lightsdue to the excessively large bokeh. Fig. 11. The rig used to capture our dataset. For this work, we only use the central camera focal stacks. SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. Fig. 12. Illustration of generating all-in-focus (AiF) and depth-of-field (DoF) edited images via HeliconFocus applied to our reconstructed focal stacks. For AiF synthesis, we use all frames in the stack; for DoF edits, we use only focal positions 1-5. We compare these AiF/DoF edited outputs from our method against the best-performing baseline, NAFNet [Chen et al. 2022], as well as the ground-truth (GT) reference."
        }
    ],
    "affiliations": [
        "Adobe, USA",
        "York University, Canada"
    ]
}