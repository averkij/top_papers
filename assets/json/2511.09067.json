{
    "paper_title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique",
    "authors": [
        "Gailun Zeng",
        "Ziyang Luo",
        "Hongzhan Lin",
        "Yuchen Tian",
        "Kaixin Li",
        "Ziyang Gong",
        "Jianxiong Guo",
        "Jing Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 7 6 0 9 0 . 1 1 5 2 : r MM-CRITIC: Holistic Evaluation of Large Multimodal Models as Multimodal Critique Gailun Zeng , Ziyang Luo , Hongzhan Lin , Yuchen Tian Kaixin Li , Ziyang Gong , Jianxiong Guo, Jing Ma Hong Kong Baptist University, Beijing Normal-Hong Kong Baptist University National University of Singapore, Beijing Normal University Shanghai Jiao Tong University gailun_zeng@u.nus.edu, jianxiongguo@bnu.edu.cn majing@comp.hkbu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in languageonly settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MMCRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MMCRITIC and provide comprehensive assessment of leading LMMs critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MMCritic."
        },
        {
            "title": "Introduction",
            "content": "The critique ability of language models plays pivotal role in fostering self-improvement (Liu et al., 2024) and enabling trustworthy AI (Krishna, 2023; Lin et al., 2025), e.g., critique-capable models can provide feedback on student answers or essays, supporting personalized learning in educational applications (Parker et al., 2024). This capability has been extensively explored in the context of Large Language Models (LLMs) (Lan et al., Corresponding Authors. 2024; Lin et al., 2024c; Sun et al., 2024). However, as Large Multimodal Models (LMMs) gain proficiency across diverse multimodal tasks involving captioning (Chai et al., 2024) and visual reasoning (Wang et al., 2025), their potential to analyze and critique becomes increasingly important, not only for refining their own outputs but also for serving as AI assistants capable of providing feedback in complex, real-world scenarios (Xiong et al., 2024; Luo et al., 2025). The rise of LMMs brings new challenges and opportunities for critique in multimodal contexts. For example, LMMs must reason over and align information from multiple modalities (e.g., image and text), which introduces complexity in both understanding and critique generation. Thus, evaluating such critique capabilities in LMMs is critical for advancing their alignment, reliability, and reasoning depth across modalities (Yasunaga et al., 2025; Li et al., 2024c). Recently, several efforts have been made to evaluate or enhance the critique capabilities of LMMs. Notably, Multimodal RewardBench (Yasunaga et al., 2025) and VL-RewardBench (Li et al., 2024c) investigate the judging abilities of LMMs by presenting two responses to multimodal question and asking the model to select the better one. These benchmarks primarily frame the critique as binary classification task focused on simple preference prediction, without delving into more finegrained aspects of the critique capabilities. Beyond preference modeling, LLaVA-Critic (Xiong et al., 2024) introduces an open-source LMM trained to effectively evaluate the responses of other LMMs. While it showcases the potential of LMMs for judging, it is primarily designed for model training rather than systematic evaluation. Similarly, CriticV (Zhang et al., 2024) explores the use of LMMs as critics to catch errors made in multimodal reasoning tasks. Although it demonstrates that LMMs can act as effective critics, it focuses on case studies and empirical validation, rather than establishing Figure 1: Multi-dimensional critique evaluation in MM-CRITIC. Basic critique includes binary correctness and textual feedback (Critique Accuracy, Critique Score); correction and comparative critique correspond to Correction Critique Score and Preference Accuracy, respectively."
        },
        {
            "title": "Taxonomy Hierachy",
            "content": "MLLM-as-a-Judge (Chen et al., 2024a) Multimodel RewardBench (Yasunaga et al., 2025) VL-RewardBench (Li et al., 2024c)"
        },
        {
            "title": "Scalar\nScalar\nScalar",
            "content": "MM-CRITIC (ours) Scalar/Textual 3 1 1 3 12 6 3 1 1 1 3 Table 1: Comparison between related benchmarks and MM-CRITIC. comprehensive benchmark for critique capability. Taken together, these works highlight the growing interest in multimodal critique, yet reveal lack of standardized, holistic evaluation that assesses LMMs as general-purpose multimodal critics across tasks and critique dimensions. To fill this gap, we propose novel benchmark, MM-CRITIC, designed to comprehensively and reliably measure critique capability of LMMs. To ensure the comprehensiveness of MM-CRITIC, firstly, we propose granular evaluation scheme, where we employ both scalar and textual metrics to evaluate the critique capabilities of LMMs across three dimensions, namely basic critique, correction critique, and comparative critique, as shown in Figure 1. Second, MM-CRITIC sources diverse data from MEGA-BENCH (Chen et al., 2024b), comprehensive multimodal task benchmark encompassing 8 main task scenarios and over 500 specific tasks. Then, we leverage broad range of LMMs with different model sizes to collect enough responses based on the selected specific tasks, which ensures that the generated responses exhibit distinguishable levels of quality. Finally, MM-CRITIC totally includes 4471 model response samples. Based on this, we also organized sub-datasets for fine-grained critique evaluation, namely correction critique and comparative critique. To enhance the reliability of evaluation, we incorporate reference critiques to assist the judge model (i.e., GPT-4.1) in evaluating LMMs critiques. This effectively mitigates potential evaluation bias in textual critique introduced by judge models (e.g., GPT-4.1) (Wang et al., 2023a; Li et al., 2024a; Tan et al., 2024). Specifically, considering the characteristics of task types, we deliberately design detailed scoring rubric checklist that include both common rubrics and task-type-specific rubrics. Besides, we also provide each tasks grounded answer to the annotation model (i.e., GPT-4o) since these selected tasks belong to different domains and the grounded answer, including expert-human level knowledge, helps the annotator generate both reasonable and reliable reference critiques. Then, the checklist combined with grounded answers is embedded into the prompt to guide GPT-4o in annotating both the response quality scores and reference critiques. Based on the annotated response quality, it is feasible to construct the sub-dataset for correction critique from low-quality responses and generate the sub-dataset for comparative critique by forming response pairs with different quality levels. Overall, as shown in Table 1, MM-CRITIC demonstrates substantial improvements in terms of comprehensiveness over prior benchmarks. The reference critiques help us define Critique Score metrics that can score the textual critique contents generated by LMMs, where we anchor the reference critiques at score of 8 to represent human levels and prompt the judge model to compare LMMs contents with the reference critiques and give comparative scores. We also employ the common Critique/Preference Accuracy as scalar metrics. We conduct extensive experiments on leading closed-source and open-source LMMs. The results validate the effectiveness of MM-CRITIC and reveal LMMs critique capabilities, where the scaling law is clearly observed and models within the same series exhibit consistently improved critique performance as their parameter sizes increase. Extensive case analysis demonstrates that this approach of reference-critique-based evaluation significantly enhances the reliability of the judgments. Then, further experiments and analyses reveal set of implicit yet intriguing insights: Correction critique scores are generally lower than basic critique scores, indicating that Correction critique remains challenging task for LMMs. In comparative critique, pairwise combinations of medium/high-quality responses are particularly difficult to judge. There exists an inherent relationship between response quality and critique scores. Results show that medium-quality responses tend to receive the lowest critique scores compared to both highand low-quality ones, highlighting the unique challenges posed by evaluating critiques of medium-quality responses. The judgment bias of models may be related to the richness of critique text, as GPT-4.1 tends to assign higher scores to longer, more elaborate critiques."
        },
        {
            "title": "2 Related work",
            "content": "Application. The critique ability of models has been extensively explored in recent years as means of assessing response quality across variety of tasks while reducing reliance on costly human annotations (Pan et al., 2023; Wang et al., 2023b; Zheng et al., 2023). Advanced LLMs, such as GPT4, have demonstrated strong alignment with human judgments (Tan et al., 2025), prompting the extension of this paradigm to multimodal settings. SOTA LMMs, including GPT-4o, are increasingly employed to evaluate responses in multimodal tasks, significantly alleviating the need for manual evaluation (Luo et al., 2024). Beyond judging, critique also plays crucial role in enhancing LMM performance. During inference, textual critiques that identify response flaws and suggest improvements enable iterative refinement (Madaan et al., 2023). During training, scalar-valued critique signals are commonly used to construct response pairs with clear quality differences (Liu et al., 2024), facilitating methods such as preference learning to further boost model capabilities (Li et al., 2024d). Besides, critique capability facilitates series of downstream applications, such as harmful content detection (Chen et al., 2025; Lin et al., 2024a; Huang et al., 2024), sarcasm understanding (Chen et al., 2024c) and GUI-based tasks (Yang et al., 2025; Li et al., 2025). Evaluation. Due to multimodal complexity, it is non-trivial to reasonably evaluate LLMs performance on specific applications, necessitating the importance of task-specific benchmarkings. Recently, diverse evaluation frameworks have emerged. Multimodal trustworthy AI shows new challenges in fact checking (Wang et al., 2024) and harmful content audit (Lin et al., 2024b, 2023). Coding, as the widely-discussed research direction, when considering rich visual programming environments, the evaluation and exploration of LLMs capabilities is valuable (Li et al., 2024b; Fu et al., 2024). Deep understanding of visual components is still unexplored (Gong et al., 2025; Yang et al., 2024). These benchmarks are conducive to better master models deficiency and carry out targeted model capability enhancement (Cheng et al., 2024). range of benchmarks has been developed to assess the critique capabilities of models. Initial efforts predominantly focused on the language domain, evaluating models ability to judge text-based responses (Lin et al., 2024c; Lan et al., 2024). More recent work has extended this evaluation to the multimodal setting, primarily using scalar-valued critiques to measure alignment with human judgments on standard multimodal tasks (Li et al., 2024c; Yasunaga et al., 2025; Chen et al., 2024a). As shown in Table 1, our proposed MM-Critic advances beyond existing benchmarks by incorporating richer critique dimensions and wider variety of task types, enabling more comprehensive and nuanced evaluation of critique ability."
        },
        {
            "title": "3.1 Overview",
            "content": "MM-CRITIC is holistic evaluation benchmark for multimodal tasks, covering 8 major task categories and over 500 specific tasks. An overview of MMCRITIC is presented in Table 6 (Appendix A), comprising 4471 samples distributed across four sub-datasets: core, core single-image, open, and open single-image. The core and core single-image sub-datasets include large-scale and commonly seen tasks, formulated as closed-ended questions with unique ground-truth answers. In contrast, the other two sub-datasets contain open-ended questions, for which only reference answers are provided. Each sample also contains question and response generated by various LMMs, along with series of annotations, such as response quality scores and reference textual critiques, produced by GPT-4o based on rigorous scoring rubric checklist. These annotations play crucial role in enhancing the evaluation process, where the reference critique can significantly improve the reliability of the judge models assessments. The construction of MM-CRITIC follows threestep process: 1) selecting diverse samples from comprehensive multimodal benchmark (Chen et al., 2024b) and using series of LMMs to generate wide range of responses; 2) designing rigorous scoring rubric checklist to guide GPT-4o in evaluating response quality and generating reference critiques in human-expert-like manner; 3) constructing sub-datasets for correlation and comparative critique based on the annotated response quality."
        },
        {
            "title": "3.2 Multimodal Task and Response Collection",
            "content": "MM-CRITIC is constructed based on MEGABENCH (Chen et al., 2024b), comprehensive evaluation suite encompassing over 500 real-world multimodal tasks across 8 distinct categories. To build the original dataset (see Table 5 in Appendix A), we first randomly sampled two instances from each specific task, covering wide spectrum of mainstream text-image tasks, thereby ensuring the diversity and representativeness of MM-CRITIC. Subsequently, we employed range of LMMs with varying capability levels (see Table 8 in Appendix A) to generate responses at different quality levels. Through this dual-faceted approach, i.e., diverse task coverage and stratified response generation, MM-CRITIC serves as holistic benchmark for evaluating LMMs critique capability."
        },
        {
            "title": "3.3 Reference Critique Construction",
            "content": "Notice that the reliability of model-based judging methods remains an open question, and the bias between human and model-based evaluations still poses significant challenge (Li et al., 2024a; Tan et al., 2024). To partially mitigate this bias and enhance the reliability of model judges (e.g., GPT-4o), we designed detailed scoring rubric checklist and employed it with each tasks grounded answer to guide GPT-4o in generating reasonable annotations. Note that grounded answers include rich humanexpert knowledge since MM-CRITIC is composed of different domains, such as coding and mathematics, and truly needs domain-specific experts to provide professional answers. The complete scoring rubric checklist and prompt can be found in Table 10 and Figure 5 (Appendix B). The following lists all annotations: Correctness. We utilize the GPT-4o to judge the correctness of the generated responses by LMMs, where the task answers are available. Response Quality Score. GPT-4o assigns each response score ranging from 0 to 10 based on the rigorous scoring rubric checklist. This scoring process aims to stratify response quality and assist further analysis, such as revealing correlations between response quality and the generated critiques. Reference Critique. In addition to scalar evaluation, textual analysis is more complex yet essential, as the textual content provides deeper insight into each LMMs critique capability. Guided by the scoring rubric checklist and the given humanexpert answer, GPT-4o is prompted to emulate human experts in generating textual reference critique. This reference critique is considered highquality and anchored at score of 8. Reference Correction Critique. For relatively low/medium-quality responses, it is reasonable to generate correction critiques that reflect the selfimprovement potential of LMMs. Therefore, for suboptimal responses, the correction critiques generated by GPT-4o with access to the ground-truth answers, can be regarded as reliable and convincing feedback."
        },
        {
            "title": "Comparative Critique",
            "content": "We constructed dedicated sub-datasets tailored to the two critique dimensions. For correctness critique, we derived subset from the core dataset of MM-CRITIC, deliberately selecting samples labeled with low or medium response quality. For comparative critique, we construct three pairwise sub-datasets from the core dataset of MM-CRITIC, leveraging the labeled response quality scores. Specifically, responses with scores in the range of [0, 4] are categorized as low-quality, [5, 7] as medium-quality, and [8, 10] as high-quality. Based on this categorization, we generate three types of pairwise combinations: (low, medium), (medium, high), and (low, high). Table 7 (Appendix A) lists the detailed statistics of the sub-datasets."
        },
        {
            "title": "4.1 Comprehensive Critique Dimensions",
            "content": "It is essential to consider evaluation metrics comprehensively and especially ensure that they align with our scenarios, namely LMMs critique. Following previous work (Lan et al., 2024; Zhang et al., 2025), MM-CRITIC is designed to thoroughly evaluate the critique abilities of LMMs across multiple dimensions. From the perspective of quantifiability, evaluation metrics can be categorized into scalar and textual forms. To ensure rigorous assessment, we adopt suite of metrics covering both scalar and textual evaluations. Scalar metrics are primarily considered objective evaluation tools. Among them, accuracy is one of the most fundamental metrics. We define Critique Accuracy to measure models ability to correctly judge the validity of given response, and Preference Accuracy to evaluate how well the model selects the better response from pairwise comparison. Textual critique, while inherently difficult to assess objectively due to its open-ended nature, remains critically important. common approach involves conducting subjective analyses on set of representative cases. However, such case studies are impractical for evaluating large-scale datasets. To address this limitation, we propose transforming subjective evaluation into an approximate objective assessment. Specifically, we employ GPT-4o to generate reliable reference critiques, anchored at score of 8, which serve as pivots to guide the judge model in evaluating textual critiques. In this way, textual critiques can also be scored, denoted as Critique Score, and the reliability of these scores is empirically validated in our experiments."
        },
        {
            "title": "4.2 Objective and Subjective Evaluation",
            "content": "Critique Accuracy. The direct critique ability is to judge whether the response is correct. Thus, we define Critique Accuracy as the average accuracy across all samples, formulated as: 1 ACCcritic = I(ˆyi = yi), (cid:88)N (1) i= where is the number of samples, ˆyi denotes the models judgment of correctness for the i-th response, yi is the ground-truth correctness label, and I() is the indicator function that returns 1 if and only if the condition holds, and 0 otherwise. Preference Accuracy. We construct subset of pairwise response samples from MM-CRITIC to evaluate the models comparative ability to identify the better response between two options of differing quality. Preference Accuracy is defined as the average accuracy of correct selections across all pairwise samples, formulated as: ACCprefer ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88)N i=1 I(ˆci = ci), (2) where is the number of samples, ˆci denotes the models preferred choice for the i-th response pair, ci points to the higher quality response in pairwise sample, and I() is the indicator function too. Critique Score. As mentioned above, textual critiques can be approximately and objectively assessed using scalar metric, termed Critique Score, defined as Score =:"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Scorei(critiqueLMM, critiquereference), (3) where is the number of samples, Scorei() denotes the judged score for the i-th critique, bounded within [0, 10]. Here, critiquereference is highquality reference critique anchored at score of 8, and critiqueLMM is the model-generated critique being evaluated against the reference. With the assistance of reference critique, it is feasible to score any textual contents, namely, basic and correction textual critiques here. The judge prompt and critique prompts can be found in Figure 6, 7, 8, and 9 (Appendix B)."
        },
        {
            "title": "Model",
            "content": "o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b-instruct Qwen2.5-vl-72b-instruct Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout"
        },
        {
            "title": "Core",
            "content": "Core Single-image"
        },
        {
            "title": "Open",
            "content": "Open Single-image Avg."
        },
        {
            "title": "Score",
            "content": "0.896 0.832 0.833 0.834 0.826 0.865 0.803 0.839 0.839 0.828 0.748 0.508 0.783 0.721 0.703 0.759 0.804 0.757 7.924 7.499 6.634 8.113 6.495 8.558 7.523 Proprietary Models 7.952 7.429 6.534 8.080 6.460 8.549 7.490 0.906 0.826 0.762 0.799 0.774 0.865 0. 0.897 0.834 0.836 0.828 0.828 0.865 0.806 7.877 7.807 6.549 8.102 6.500 8.246 8.066 Open-weight Models (Larger than 30B) 8.138 6.817 7.531 5.971 0.811 0.838 0.836 0.812 0.852 0.803 0.804 0.742 8.566 7.089 7.743 6. 8.208 6.931 7.489 5.811 Open-weight Models (Less than 30B) 5.509 5.007 5.093 5.289 6.419 6.921 5.822 0.590 0.780 0.750 0.687 0.739 0.773 0.767 0.546 0.711 0.728 0.721 0.645 0.720 0.797 6.155 4.573 5.179 5.700 6.744 7.297 6.336 5.130 4.617 5.185 5.201 6.566 7.107 5. 0.856 0.789 0.690 0.808 0.756 0.866 0.806 0.794 0.808 0.845 0.705 0.611 0.788 0.759 0.688 0.671 0.744 0.818 7.976 7.637 6.416 8.097 6.340 8.325 8.274 8.495 7.133 7.784 6.250 6.643 4.806 5.351 5.759 6.944 7.700 6. 0.900 0.830 0.821 0.831 0.818 0.865 0.806 0.829 0.834 0.830 0.768 0.546 0.777 0.734 0.695 0.742 0.783 0.768 7.933 7.503 6.580 8.099 6.474 8.514 7.600 8.216 6.911 7.538 5.938 5.400 4.765 5.161 5.302 6.531 7.082 5. Table 2: Main results about ACCcritic and Score on different sub-datasets."
        },
        {
            "title": "5 Evaluation and Analysis",
            "content": "In this section, we comprehensively analyze the critique capability of representative LMMs, and the main results are in Table 2. Subsequently, we conduct series of in-depth experiments and analyses, where several intriguing insights are revealed."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 2 presents the complete results across MMCRITIC. Several general observations can be drawn. First, closed-source LMMs generally outperform open-source counterparts in critique performance. Notably, the o4-mini model achieves SOTA performance in terms of ACCcritic, while Gemini-2.5-pro attains the highest Critique Score. Second, model size (i.e., parameter scale) has significant impact on performance. Further exploring the experimental findings, we observe that the performance differences between the core and core single-image datasets, as well as between the open and open single-image datasets, are marginal. This may be attributed to the overlap of tasks within each sub-dataset category, leading to similar model behavior across them. In addition, the overall critique performance appears to be suboptimal when the model size is below 30 billion parameters. Among these smaller models, only Gemma-3-27B demonstrates relatively strong performance, achieving an ACCcritic of 0.783 and critique score of 7.082. These results suggest that model size of approximately 30 billion parameters may represent threshold for effectively supporting LMMs critique capabilities. Scaling Law. To verify whether the scaling law holds in the context of critique evaluation in MMCRITIC, Figure 2 visualizes the ACCcritic results across LLMs with increasing model sizes. The results clearly indicate that ACCcritic scores for models within the same series (e.g., the Gemma-3 series) consistently improve as the parameter size increases. Even among closed-source LMMs, larger models consistently outperform their smaller counterparts within the same series, e.g., Gemini-2.5pro outperforms Gemini-2.5-flash. This indirectly supports the reliability of our critique evaluation and demonstrates the robustness of MM-CRITIC."
        },
        {
            "title": "5.2 Further Analysis",
            "content": "Effects of Task Type. While Table 2 presents the overall results for each sub-dataset, MMCRITIC, as comprehensive benchmark, covers diverse range of tasks categorized into eight primary task types. Therefore, it is crucial to examine model performance across these distinct task categories to gain deeper insights. Appendix provides detailed model performance results across Figure 2: Scaling law on ACCcritic across models. Note that the parameter sizes of all closed-source LMMs are estimated, as their exact values are not publicly available. However, the relative scale among them is preserved for example, Gemini-2.5-flash is known to be smaller than Gemini-2.5-pro. the eight task types for each sub-dataset in Table 11,12,13,14,15,16,17, and 18. These detailed results indicate that the overall SOTA models also maintain strong performance across all task types. Notably, Claude-3.7-Sonnet, as high-performing LMM, consistently achieves top results on coding tasks across each sub-dataset. Multiple Critique Dimensions. To more effectively evaluate models self-improvement capability, we introduce two additional critique dimensions: correctness critique and comparative critique. The former assesses the models ability to identify and correct errors in corresponding responses, while the latter evaluates the models capacity to select the better response from pair of differing-quality answers. Based on the main results in Table 2, we select representative and high-performing openand closed-source LMMs for further in-depth experiments. Table 3 shows that the closed-source model Gemini-2.5-pro achieves the highest score in the correctness critique, which is consistent with its overall performance in the main results. Notably, the average correctness critique scores across models are generally lower than their corresponding critique scores in the main evaluation, suggesting that correctness critique poses greater challenge. Table 4 reveals two key findings: First, among the closedand open-source LMMs, Gemini2.5-pro and Llama-4-maverick demonstrate the strongest performance. Second, the (medium, high) pairwise sub-dataset is evidently the most difficult, likely due to the subtle differences in quality between medium and high responses, making preference judgment more challenging. Effects of Response Quality. In MM-CRITIC, we employed GPT-4o to score the quality of all generated responses. This naturally motivates an exploration of the relationship between response quality and the corresponding critique scores. Figure 3 reveals some intriguing insights: highquality responses tend to get high critique scores. Another interesting observation is that mediumquality responses are the most challenging, producing the lowest critique scores among the three groups. This result aligns with intuitive reasoning, as responses that are either good or poor exhibit more distinct characteristics, whereas mediumquality responses pose greater challenges for critique. This provides direction for further enhancing model performance, specifically by focusing more on medium-quality responses. Reliability and Bias of Subjective Evaluation. In utilizing judge model for subjective evaluation, it is crucial to ensure the reliability of its assessments and to reduce the discrepancy between human and model judgments. To this end, we deliberately designed scoring rubrics grounded in expert human reasoning  (Table 10)  , and employed GPT4o to generate reference critiques based on these rubrics, with access to the ground-truth answers. When utilizing judge model to evaluate LMMs critique scores, the evaluation reliability can be significantly improved by providing corresponding reference critique anchored at score of 8. Why do we need reference critiques? In Figure 10 (Appendix E), we present mathematical reasoning task in which the model-generated re-"
        },
        {
            "title": "Model",
            "content": "o4-mini GPT-4o Claude-3.7-sonnet Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-27b"
        },
        {
            "title": "Planning Knowledge",
            "content": "Information Extraction Mathematics Coding Science Metric Avg."
        },
        {
            "title": "Task types",
            "content": "5.636 5.606 7.406 7.152 7.152 6.458 6.625 3.225 7.061 6.097 6.129 7.267 7.630 5.103 4.500 4.690 3.452 6.065 6.290 5.323 6.839 7.152 5.516 5.074 5.111 3.100 6."
        },
        {
            "title": "Proprietary Models",
            "content": "7.625 5.875 8.375 8.875 3.188 Open-weight Models 3.000 6.200 5.500 4.813 5.171 5.114 6.114 7.852 4.857 4.448 4.100 1.471 5.400 7.000 6.424 5.969 7.471 3. 5.294 4.909 4.600 4.381 6.457 6.500 7.943 8.667 4.600 6.514 6.118 2.324 7.114 6.324 6.794 7.100 7.735 5.242 6.500 6.333 1.941 7.242 6.220 5.980 7.041 7.794 5. 5.486 5.410 2.970 6.217 Table 3: Correction critique scores on different task types. Figure 3: The distribution of critique scores across responses of different quality levels, where low-, medium-, and high-quality correspond to labeled response quality ranges of [0, 4], [5, 7], and [8, 10], respectively."
        },
        {
            "title": "Model",
            "content": "ACCprefer G2 G"
        },
        {
            "title": "Proprietary Models",
            "content": "o4-mini GPT-4o Claude-3.7-sonnet Gemini-2.5-pro Grok-2-vision 0.836 0.848 0.835 0.860 0.867 0.658 0.589 0.579 0.716 0.475 Open-weight Models Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-27b 0.733 0.858 0.854 0. 0.507 0.542 0.658 0.615 G3 0.831 0.740 0.785 0.939 0.687 0.696 0.744 0.821 0.757 Table 4: ACCprefer results across models on the three comparative pairwise sub-datasets, where G1, G2, and G3 represent the response quality combinations of (low, medium), (medium, high), and (low, high), respectively. sponse is incorrect. The critique model, o4-mini, successfully identifies this error and provides comprehensive textual critique, including detailed reasoning steps and counterexample. When evaluated by the judge model, the critique is recognized as superior to the reference critiqueparticularly due to the inclusion of the counterexampleand is assigned higher score of 9, compared to the referFigure 4: The relationship between the average length of textual critiques and critique scores across models. ence critiques anchored score of 8. This case study demonstrates that the use of reference critique effectively guides the judge models assessment, thereby enhancing the reliability of the evaluation compared to scoring without such reference. Why does the Judge model always exhibit evaluation bias? As mentioned above, the use of reference critiques can effectively enhance the reliability of model-based evaluations. However, discrepancies between model and human judgments inevitably persist. Therefore, it is crucial to conduct an in-depth analysis of the potential factors contributing to this bias. After examining large number of cases, we observed an emerging pattern: the critique score appears to be positively correlated with the length of the textual critique. As shown in Figure 4, none of the models achieve critique score exceeding 8 when their average text length is below 1000. Besides, an unexpected observation emerges between Qwen2.5-vl-32b and Qwen2.5-vl-72b: the smaller model outperforms the larger one in terms of critique score. However, this result becomes more interpretable when considered from the perspective of textual length. Since longer critiques often entail more comprehensive, step-by-step reasoning, we find that the judge model tends to assign higher scores to such responses. Potential Bias Caused by Model Style. To assess potential bias from specific model style (e.g., GPT-based series), we constructed sub-dataset from MM-CRITIC. Reference critiques were generated by Gemini-2.5-flash, and scoring was judged by GPT-4.1 and Claude-4.0-sonnet, respectively. As the Table 19 in Appendix shown, five models were evaluated: o4-mini, GPT-4o, GPT-4o-mini, Claude-3.7-sonnet, and Gemini-2.5-pro. Results show that model rankings remain consistent with those judged by GPT-4.1 with reference critiques annotated by GPT-4o. Only o4-mini and Claude3.7-sonnet swapped ranks when judging by Claude4.0-sonnet, which is acceptable given their small score gap (0.166) in the main experiments (in Table 19). These findings suggest that GPT-4o does not significantly affect the fairness or validity of our evaluation. Case study. Case studies, particularly those involving poor-performing examples, can provide valuable insights into the limitations of both the evaluation methodology and the critique capabilities of the models. We present representative cases to facilitate in-depth analysis in Appendix E. Instruction following or formulaic step-by-step reasoning? key finding is the conflict between following instructions and generating detailed reasoning. While prompts request brief, direct answers, some LMMs consistently produce step-bystep explanations, ignoring instructions. Though such reasoning can improve response quality, it may not match user expectations for concise replies, the case can be found in Figure 11 (Appendix E)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce MM-CRITIC, holistic and reliable benchmark for evaluating the critique abilities of LMMs across multiple dimensions. Extensive experiments demonstrate the basic critique performance of leading LMMs and validate the reliability of MM-CRITIC through the observed scaling law. Further analysis reveals valuable insights, including the correlation between response quality and critique scores, varying levels of critique difficulty across dimensions, and potential judgment biases linked to critique text richness. Our MMCRITIC offers solid foundation for benchmarking and advancing the critique capabilities of LMMs, fostering the development of more explainable and trustworthy multimodal systems."
        },
        {
            "title": "Limitations",
            "content": "Note that there are still some drawbacks and limitations about MM-CRITIC. Although MM-CRITIC is comprehensive benchmark, it currently focuses only on text and image modalities, lacking broader evaluation across other multimodal domains such as video, audio, and 3D data. The reference critiques and scoring annotations are generated by GPT-4o guided by rubric checklists. While this design improves consistency, it still relies on single model as the annotator, which may introduce systematic biases or limitations inherent to GPT-4o. MM-CRITIC evaluates model critique in static context using predefined prompts and samples. In real-world scenarios, critique often occurs interactively or iteratively, which is not yet captured by the current benchmark."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is partially supported by Tencent RhinoBird Focused Research Program (Value-aligned Credible Large Language Model), the National Natural Science Foundation of China (NSFC) under Grant No. 62202055, the Guangdong Basic and Applied Basic Research Foundation under Grant No. 2025A1515012843, the Start-up Fund from Beijing Normal University under Grant No. 312200502510, the Internal Fund from Beijing Normal-Hong Kong Baptist University under Grant No. UICR0400003-24 and No. UICR020002225, and the Interdisciplinary Intelligence SuperComputer Center of Beijing Normal University (Zhuhai)."
        },
        {
            "title": "References",
            "content": "Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher D. Manning. 2024. Auroracap: Efficient, performant video detailed captioning and new benchmark. CoRR, abs/2410.03051. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. 2024b. Mega-bench: Scaling multimodal arXiv evaluation to over 500 real-world tasks. preprint arXiv:2410.10563. Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, and Jing Ma. 2025. AdamMeme: Adaptively probe the reasoning capacity of multimodal large language models on harmfulness. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 42344253, Vienna, Austria. Association for Computational Linguistics. Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen. 2024c. Cofipara: coarse-to-fine paradigm for multimodal sarcasm target identification with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 96639687. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. 2024. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476. Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, and Jing Ma. 2024. Scratcheval: Are gpt-4o smarter than my child? evaluating large multimodal models with visual programming challenges. arXiv preprint arXiv:2411.18932. Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, and Rongrong Ji. 2025. Space-10: comprehensive benchmark for multimodal large language models in compositional spatial intelligence. arXiv preprint arXiv:2506.07966. Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, and Jing Ma. 2024. Towards lowresource harmful meme detection with lmm agents. arXiv preprint arXiv:2411.05383. Satyapriya Krishna. 2023. On the intersection of selfcorrection and trust in language models. CoRR, abs/2311.02801. Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. 2024. Criticeval: Evaluating large-scale language model as critic. Advances in Neural Information Processing Systems, 37:6690766960. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. 2024a. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. 2025. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. 2024b. Mmcode: Benchmarking multimodal large language models for code generation with visually rich programming problems. arXiv preprint arXiv:2404.09486. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. 2024c. Vlrewardbench: challenging benchmark for vision-language generative reward models. CoRR, abs/2411.17451. Shengzhi Li, Rongyu Lin, and Shichao Pei. 2024d. Multi-modal preference alignment remedies regression of visual instruction tuning on language model. CoRR, abs/2402.10884. Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, and Tat-Seng Chua. 2025. FACT-AUDIT: An adaptive multi-agent framework for dynamic fact-checking evaluation of large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 360381, Vienna, Austria. Association for Computational Linguistics. Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang. 2024a. Towards explainable harmful meme detection through multimodal debate between large language models. In Proceedings of the ACM Web Conference 2024, pages 23592370. Hongzhan Lin, Ziyang Luo, Jing Ma, and Long Chen. 2023. Beneath the surface: Unveiling harmful memes with multimodal reasoning distilled from large language models. arXiv preprint arXiv:2312.05434. Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. 2024b. Goat-bench: Safety insights to large multimodal models through meme-based social abuse. ACM Transactions on Intelligent Systems and Technology. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024c. Criticbench: Benchmarking llms for critique-correct reasoning. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 15521587. Association for Computational Linguistics. Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. 2024. Diving into selfevolving training for multimodal reasoning. Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. 2025. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. arXiv preprint arXiv:2508.14704. Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan S. Kankanhalli, and Junnan Li. 2024. Videoautoarena: An automated arena for evaluating large multimodal models in video analysis through user simulation. CoRR, abs/2411.13281. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. CoRR, abs/2308.03188. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Yuan Tang, Alejandro Cuadron, Chenguang Wang, Raluca A. Popa, and Ion Stoica. 2025. Judgebench: benchmark for evaluating llm-based judges. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation and synthesis: survey. arXiv preprint arXiv:2402.13446. Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, and Jing Ma. 2024. Mfc-bench: Benchmarking multimodal fact-checking with large visionlanguage models. arXiv preprint arXiv:2406.11288. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023a. Shepherd: critic for language model generation. arXiv preprint arXiv:2308.04592. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023b. Shepherd: critic for language model generation. CoRR, abs/2308.04592. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. 2025. Multimodal chain-of-thought reasoning: comprehensive survey. CoRR, abs/2503.12605. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. CoRR, abs/2410.02712. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. 2025. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. 2024. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256. Michael J. Parker, Caitlin Anderson, Claire Stone, and YeaRim Oh. 2024. large language model approach Internato educational survey feedback analysis. tional Journal of Artificial Intelligence in Education. Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. 2025. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. CoRR, abs/2502.14191. Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. 2024. The critique of critique. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 9077 9096. Association for Computational Linguistics. Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, et al. 2025. Codecriticbench: holistic code critique benchmark for large language models. arXiv preprint arXiv:2502.16614. Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, and Dongzhan Zhou. 2024. Critic-v: VLM critics help catch VLM errors in multimodal reasoning. CoRR, abs/2411.18203. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Dataset and LMMs Information."
        },
        {
            "title": "Task Type",
            "content": "Core Core Single-image Open Open Single-image Total Sub-datasets"
        },
        {
            "title": "Total",
            "content": "266 146 142 112 66 58 54 36 880 146 80 108 78 60 28 40 6 546 24 10 52 32 - 4 4 4 18 8 46 4 - 4 4 - 84 454 244 348 226 126 94 102 46 1640 Table 5: The statistics of tasks that are selected to generate responses for our benchmark."
        },
        {
            "title": "Task Type",
            "content": "Core Core Single-image Open Open Single-image Total Sub-datasets"
        },
        {
            "title": "Perception\nPlanning\nKnowledge\nInformation Extraction\nMathematics\nCoding\nScience\nMetric",
            "content": "668 320 394 290 189 170 161 90 435 238 319 231 179 82"
        },
        {
            "title": "Total",
            "content": "2282 1620 61 26 150 51 - 12 12 7 319 54 23 137 12 - 12 12 - 1218 607 1000 584 368 276 303 115 4471 Table 6: Dataset statistics of MM-CRITIC."
        },
        {
            "title": "Correction",
            "content": "Comparison Group 1 Group 2 Group"
        },
        {
            "title": "Perception\nPlanning\nKnowledge\nInformation Extraction\nMathematics\nCoding\nScience\nMetric",
            "content": "35 35 35 35 35 35 35 35 30 30 30 30 22 30"
        },
        {
            "title": "Total",
            "content": "280 208 30 1 16 13 - 7 5 4 76 30 23 30 30 24 15 26 23 Table 7: Dataset statistics of sub-datasets for correction and comparative critique in MM-CRITIC."
        },
        {
            "title": "Source",
            "content": "InternVL2.5-4B InternVL2.5-8B InternVL2.5-26B Phi-3.5-vision-instruct Phi-4-multimodal-instruct Qwen2.5-vl-3b-Instruct Qwen2.5-vl-7b-Instruct Deepseek-vl2-tiny Llava-1.5-7b-hf Llava-onevision-qwen2-7b-ov-hf Llama-3.2-11b-vision-Instruct Pixtral-12b https://huggingface.co/OpenGVLab/InternVL2_5-4B https://huggingface.co/OpenGVLab/InternVL2_5-8B https://huggingface.co/OpenGVLab/InternVL2_5-26B https://huggingface.co/microsoft/Phi-3.5-vision-instruct https://huggingface.co/microsoft/Phi-4-multimodal-instruct https://huggingface.co/Qwen/Qwen2.5-vl-3b-Instruct https://huggingface.co/Qwen/Qwen2.5-vl-7b-Instruct https://huggingface.co/deepseek-ai/deepseek-vl2-tiny https://huggingface.co/llava-hf/llava-1.5-7b-hf https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-ov-hf https://huggingface.co/unsloth/Llama-3.2-11b-vision-Instruct https://huggingface.co/mistral-community/pixtral-12b Table 8: The list of used LMMs for generating responses."
        },
        {
            "title": "Source",
            "content": "openai/o4-mini openai/gpt-4o-2024-11-20 openai/gpt-4o-mini anthropic/claude-3.7-sonnet google/gemini-2.5-flash-preview google/gemini-2.5-pro-preview x-ai/grok-2-vision-1212 qwen/qwen2.5-vl-32b-instruct qwen/qwen2.5-vl-72b-instruct mistralai/pixtral-large-2411 meta-llama/llama-4-maverick qwen/qwen-2.5-vl-7b-instruct meta-llama/llama-3.2-11b-vision-instruct mistralai/pixtral-12b google/gemma-3-12b-it google/gemma-3-27b-it meta-llama/llama-4-scout google/gemma-3-4b-it https://openrouter.ai/openai/o4-mini https://openrouter.ai/openai/gpt-4o-2024-11-20www https://openrouter.ai/openai/gpt-4o-mini https://openrouter.ai/anthropic/claude-3.7-sonnet https://openrouter.ai/google/gemini-2.5-flash-preview https://openrouter.ai/google/gemini-2.5-pro-preview https://openrouter.ai/x-ai/grok-2-vision-1212 https://openrouter.ai/qwen/qwen2.5-vl-32b-instruct https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct https://openrouter.ai/mistralai/pixtral-large-2411 https://openrouter.ai/meta-llama/llama-4-maverick https://openrouter.ai/qwen/qwen-2.5-vl-7b-instruct https://openrouter.ai/meta-llama/llama-3.2-11b-vision-instruct https://openrouter.ai/mistralai/pixtral-12b https://openrouter.ai/google/gemma-3-12b-it https://openrouter.ai/google/gemma-3-27b-it https://openrouter.ai/meta-llama/llama-4-scout https://openrouter.ai/google/gemma-3-4b-it Table 9: The list of used LMM APIs through OpenRouter."
        },
        {
            "title": "Coding",
            "content": "Correctness: For tasks with ground truth, carefully check the response whether gives correct answer; For tasks with open answers, carefully analyze the accuracy of generated responses, including but not limited to the following aspects: consistent with reference answer factual knowledge Response Quality: Carefully analyze the quality of generated responses, including but not limited to the following aspects: correct spelling/grammar readability and comprehensibility effectiveness or usefulness. Factuality: To check the generated response whether it is in line with facts. If response is based on false premise, it can be regarded as bad sample. Relevance: Consider whether the generated content is relevant to the question. If the content is unrelated to the question, it can be reviewed as low quality. Detail: This criterion aims to check whether the generated content contains sufficient and correct detail. response is considered lower quality if it is overly brief and lacks details. Effectiveness: This criterion aims to check whether the answers effectively extract information, based on the question. If the generated answers do not provide effective information, they can be regarded as lower quality. Feasibility: It is vital to assess whether the generated planning is feasible in the real world. If the planning is unfeasible, it can be viewed as lower quality. Factuality: To check whether the generated response is in line with scientific facts. If response is based on false premise, it can be regarded as bad sample. Effective Utilization: To check whether the generated response is in line with scientific facts. If response is based on false premise, it can be regarded as bad sample. Correctness: Correctness-based for mathematics is multi-step checking criterion, firstly assess the generated response whether it provides correct reasoning process (if it includes), Secondly check the generated response whether it provides the correct answer. If the generated response provides inappropriate reasoning and wrong answer, it can be regarded as of very poor quality. If the generated response provides correct reasoning and wrong answer, it can be regarded as of relatively high quality, compared with the above case. Program Grammar: This criterion aims to check whether the generated codes align with specific program language features. If the generated codes utilize non-existent program language features, they can be regarded as of low quality. Correctness: Correctness-based for coding is multi-step checking criterion, firstly assess the generated response whether it provides correct coding framework (if it includes), Secondly check the generated response whether it provides correct output of codes. If the generated response provides an unreasonable coding framework and wrong output, it can be regarded as of very poor quality. If the generated response provides both correct coding framework and output, it can be regarded as of relatively high quality, compared with the above case. Table 10: The score rubrics for different task types. Human experts use these score rubrics to check and annotate. Basic Reference Critique Generation Prompt for GPT-4o You are professional critical AI specialist who can evaluate the response generated by vision large language model with corresponding domain knowledge of questions. You need to refer to the following rubrics: #Common for All Tasks: ##Correctness: For tasks with ground truth, carefully check the response whether gives correct answer; For tasks with open answers, carefully analyze the accuracy of generated responses, including but not limited to the following aspects: consistent with reference answer factual knowledge. ##Response Quality: Carefully analyze the quality of generated responses, including but not limited to the following aspects: correct spelling/grammar readability and comprehensibility effectiveness or usefulness. # Besides, the question is about application_name, you also need to carefully refer to the emphasized rubrics: (Corresponding rubric for different task types, here is an example for Knowledge) ##Factuality: To check whether the generated response is in line with facts. If response is based on false premise, it can be regarded as bad sample. ##Relevance: Consider whether the generated content is relevant to the question. If the content is unrelated to the question, it can be reviewed as low quality. #The following is the question and the response generated by vision large language model: Start of Question $Question (include images)$ End of Question Start of Response $Response$ End of Response #Here is the ground truth answer (or reference answer), which can effectively help you give reliable evaluations about the response: Start of Answer $Answer$ End of Answer #Evaluation Steps: ##First, you need to score the response quality, and the score ranges from 0 to 10 as an integer, -[0,3] corresponds to low-quality response, -[4,7] corresponds to medium-quality response, -[8,9] corresponds to high-quality response, -10 corresponds to correct response. ## Second, you need to give textual critique including but not limited to the following requirements: - Provide detailed, point-by-point feedback on the answer. - Each critique should be specific and self-contained. - Clearly identify any issues, avoiding vague or ambiguous descriptions. - Offer constructive suggestions for improvement. #Output Format: Provide the evaluation in JSON format as follows: ```json { \"correct\": \"Based on the ground truth answer (if have), indicate whether the assistants response is [Correct, Error]\" \"response_quality\": \"A specific integer score ranging from 0 to 10 \", \"reference_critique\": \"Based on the evaluation, give comprehensive textual critique\" \"reference_correct\": \"Based on the evaluation, give modification if the response is not of good quality enough.\" } ``` Figure 5: Basic Reference Critique Generation Prompt for GPT-4o. Critique evaluation judge prompt with reference critique. You are professional critique evaluation judge who can evaluate the critique generated by vision large language model based on the corresponding question and response. #: The following are the question and generated response, and critique/correct that need to be evaluated, respectively. Start of Question and Response $Question (include images)$ $Response$ End of Question and Response Start of Critique $Critique/Correct$ End of Critique #Here is the reference critique/correct: Start of Reference Critique $Reference Critique/Correct$ End of Reference Critique #Very important rules !!!: ##1. If you think the generated critique is better than the reference critique, you could provide score higher than 8 ##2. Please first generate your explanation for the generated critique by comparing it with the reference critique, and then you must generate the score, where the score ranges from 0 to 10 as an integer. #Output Format: Provide the evaluation in JSON format as follows: ```json { \"explanation\": \"Based on the reference critique, give explanation about the score given by yourself\", \"score\": \"A specific integer score ranging from 0 to 10 \", } ``` Figure 6: Critique evaluation judge prompt with reference critique. Basic critique prompt. You are professional expert in critique who can evaluate the responses generated by large multimodal models and analyze response quality. #: The following are the question and the generated response from large multimodal models. Start of Question $Question (include images)$ End of Question Start of Response $Response $ End of Response #Very important rules !!!: ##1. Carefully understand the question, and then analyze the response. ##2. Please first generate your judgment of correctness about the response, and then give comprehensive textual critique to explain. #Output Format: Provide the evaluation in JSON format as follows: ```json { \"correct\": \"Based on the ground truth answer (if have), indicate whether the assistants response is [Correct, Error]\", \"critique\": \"Give comprehensive textual critique, due to the question and response\", } ``` Figure 7: Basic critique prompt. Correction critique prompt. You are professional expert in critique who can evaluate the responses generated by large multimodal models and analyze response quality. #: The following are the question and the generated response from large multimodal models. Start of Question $Question (include images)$ End of Question Start of Response $Response $ End of Response #Very important rules !!!: ##1. Carefully understand the question, and then analyze the response. ##2. The original response is not good enough, and you should give your own response to better answer the question. #Output Format: Provide the evaluation in JSON format as follows: ```json { \"modified answer\": \"Since the response is unsatisfactory, give your own response here, due to the question\", } ``` Figure 8: Correction critique prompt. Comparative critique prompt. You are professional comparative critique evaluation judge who can evaluate the responses generated by two different large multimodal models and choose the better one. #: The following are the question and two generated response from two different models. Start of Question $Question (include images)$ End of Question Start of Response $Response A$ End of Response Start of Response $Response B$ End of Response #Very important rules !!!: ##1. Carefully compare the two responses, and then choose the better one. ##2. Please first generate your explanation for the choice by comparing the two responses, and then you must clearly state your choice following the format: \"choice\": X, where is or B, corresponding to response and response B. #Output Format: Provide the evaluation in JSON format as follows: ```json { \"choice\": \"chose the better response quality model and indicate your choice is [\"A\",\"B\"]\", \"explanation\": \"Based on the two responses, give explanation about the choice given by yourself\", } ``` Figure 9: Comparative critique prompt."
        },
        {
            "title": "C Experimental Results",
            "content": "Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 7.913 7.537 6.65 8.084 6.478 8.524 7.567 8.245 6.933 7.445 5.785 5.245 4.509 5.230 5.317 6.572 7.285 5.996 7.987 7.473 6.653 8.066 6.358 8.831 7.654 8.286 7.332 7.889 5.603 5.211 4.889 5.266 5.482 6.571 7.235 5. Proprietary Models 7.806 7.496 6.831 8.176 6.386 8.325 7.112 7.817 7.288 6.691 8.137 6.242 8.380 7.509 Open-weight Models (Larger than 30B) 7.983 6.892 7.289 5.648 8.237 6.194 7.495 6.406 Open-weight Models (Less than 30B) 5.239 4.300 5.233 5.218 6.761 7.032 5. 4.663 4.854 4.876 4.623 5.678 6.135 5.563 8.080 7.444 6.346 8.283 6.596 8.987 7.474 7.861 6.725 7.272 5.484 4.962 4.847 5.162 4.899 6.463 7.169 5.543 80.84 7.523 6.437 8.058 6.786 8.524 7.645 8.413 7.430 7.459 5. 4.680 4.658 4.927 5.034 6.838 7.262 5.537 7.869 7.686 6.516 8.230 7.121 8.537 7.760 8.377 7.155 7.842 6.054 5.049 4.642 5.198 5.599 6.992 7.290 5.985 8.049 7.937 6.742 7.178 6.784 8.786 7.695 8.566 6.914 7.263 6. 6.078 4.575 5.791 5.139 6.765 7.031 5.633 7.924 7.499 6.634 8.113 6.495 8.558 7.523 8.208 6.931 7.489 5.811 5.130 4.617 5.185 5.201 6.566 7.107 5.771 Table 11: Critique scores of Core subset on different task types. Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 7.950 7.285 6.508 8.102 6.384 8.542 7.476 7.946 7.180 7.480 6.151 5.670 5.113 5.113 0.657 6.429 6.987 6. 8.004 7.455 6.680 8.158 6.459 8.692 7.861 8.496 7.149 7.788 5.672 5.697 5.391 5.391 0.833 6.526 7.009 5.713 Proprietary Models 7.934 7.548 6.507 7.961 6.418 8.462 6.811 7.909 7.596 6.592 8.070 6.225 8.377 7. Open-weight Models (Larger than 30B) 8.318 7.270 7.497 5.897 8.007 7.090 7.254 6.232 Open-weight Models (Less than 30B) 5.511 5.280 5.280 0.668 6.573 7.173 5.891 4.706 4.453 4.453 0.621 6.000 5.987 5.652 8.05 7.370 6.469 8.154 6.768 8.933 7.798 7.844 6.953 7.588 5. 5.658 4.925 4.925 0.752 6.361 7.270 5.446 7.790 7.418 6.457 8.104 6.623 8.427 7.250 8.074 7.200 7.583 5.808 5.211 5.078 5.078 0.620 6.268 6.194 5.225 8.103 7.180 6.398 8.133 7.010 8.500 7.765 8.329 7.330 7.800 6. 5.769 5.122 5.122 0.689 6.863 7.333 6.172 7.111 7.418 6.167 6.944 6.722 8.750 6.944 9.000 6.176 7.000 5.500 5.533 5.846 5.846 0.444 5.389 5.750 5.667 7.953 7.429 6.534 8.080 6.460 8.549 7.490 8.138 7.157 7.531 5. 5.509 5.093 5.093 0.687 6.419 6.921 5.822 Table 12: Critique scores of Core-single-image subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 7.817 7.808 6.869 8.233 6.049 8.327 8.117 8.882 7.000 7.855 6. 5.696 0.796 5.230 5.818 6.700 6.467 6.500 8.077 7.933 6.615 8.591 6.615 8.875 8.417 9.000 7.042 7.895 6.077 7.214 0.727 5.640 5.417 - 8.474 6.318 Proprietary Models 7.510 7.980 5.745 7.627 6.549 7.878 7. 7.953 7.818 6.747 8.169 6.718 8.340 8.242 Open-weight Models (Larger than 30B) 8.419 7.034 7.832 6.541 8.033 6.800 7.575 6.040 Open-weight Models (Less than 30B) 6.638 0.682 5.333 5.987 7.025 7.500 6.386 5.737 0.722 4.941 5.061 5.176 5.889 5.810 - - - - - - - - - - - - - - - - - - 7.727 7.700 6.833 8.636 6.083 8.273 8.500 9.300 7.000 7.818 6.000 2.583 0.750 3.250 5.250 5.889 6.875 6.083 8.500 7.818 5.833 7.818 6.500 7.182 8. 9.364 7.500 7.833 6.167 5.000 0.600 4.750 4.000 7.600 7.417 5.917 7.857 5.600 5.857 7.00 5.714 6.000 7.500 7.800 5.429 5.000 5.000 3.000 0.0 5.714 3.667 4.000 4.000 9.000 7.877 7.807 6.549 8.102 6.500 8.246 8. 8.566 6.974 7.743 6.342 6.155 0.711 5.179 5.700 6.744 7.297 6.336 Table 13: Critique scores of Open subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 8.037 7.316 0.593 8.038 6.264 8.431 8. 8.857 6.843 7.794 6.333 7.042 4.721 5.143 5.755 6.800 7.489 6.620 8.087 7.875 0.739 8.684 6.238 8.895 8.261 8.667 7.909 8.300 5.227 7.313 4.905 5.222 5.130 6.250 7.957 6.095 Proprietary Models 8.083 8.500 0.750 8.667 6.833 8.750 8.750 7.897 7.575 0.679 7.916 6.276 8.283 8.235 Open-weight Models (Larger than 30B) 8.392 6.788 7.739 6.447 9.000 7.667 8.714 6.500 Open-weight Models (Less than 30B) 6.828 4.750 5.450 5.815 7.102 7.872 6.535 5.917 5.222 6.091 6.417 6.444 7.667 7. - - - - - - - - - - - - - - - - - - 8.083 7.500 0.917 8.636 6.583 8.091 8.500 8.667 7.333 7.636 6.083 3.833 5.273 4.778 5.333 6.400 6.818 6. 8.167 8.556 0.900 8.333 6.833 7.667 8.143 - 7.167 7.333 5.455 4.667 4.727 5.091 6.200 7.091 6.909 5.800 - - - - - - - - - - - - - - - - - - 7.976 7.637 0.690 8.097 6.340 8.325 8.274 8.495 6.992 7.784 6.250 6.643 4.806 5.351 5.759 6.944 7.700 6.521 Table 14: Critique scores of Open-singel-image subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 0.872 0.847 0.818 0.808 0.840 0.826 0.782 0.823 0.805 0.823 0.719 0.490 0.736 0.693 0.707 0.744 0.816 0.748 0.974 0.917 0.852 0.925 0.903 0.941 0.875 0.923 0.935 0.945 0.867 0.621 0.893 0.832 0.830 0.877 0.922 0. Proprietary Models 0.940 0.848 0.852 0.884 0.841 0.943 0.841 0.868 0.809 0.821 0.821 0.772 0.838 0.782 Open-weight Models (Larger than 30B) 0.797 0.802 0.774 0.727 0.882 0.852 0.881 0.787 Open-weight Models (Less than 30B) 0.501 0.747 0.703 0.688 0.765 0.778 0. 0.462 0.805 0.724 0.584 0.661 0.684 0.693 0.899 0.792 0.840 0.870 0.831 0.898 0.847 0.821 0.790 0.810 0.747 0.535 0.800 0.789 0.704 0.769 0.809 0.739 0.916 0.754 0.821 0.844 0.818 0.889 0.794 0.860 0.845 0.819 0. 0.426 0.804 0.793 0.649 0.757 0.824 0.724 0.869 0.821 0.795 0.814 0.814 0.789 0.769 0.831 0.809 0.815 0.748 0.490 0.796 0.610 0.796 0.768 0.739 0.752 0.783 0.738 0.678 0.698 0.667 0.775 0.671 0.738 0.667 0.675 0. 0.469 0.716 0.618 0.588 0.638 0.727 0.656 0.896 0.832 0.833 0.840 0.826 0.865 0.803 0.839 0.824 0.828 0.748 0.508 0.783 0.721 0.704 0.759 0.804 0.757 Table 15: ACCcritic of Core subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 0.843 0.811 0.789 0.792 0.786 0.792 0.762 0.773 0.793 0.789 0.784 0.597 0.727 0.703 0.657 0.701 0.737 0.746 0.975 0.916 0.958 0.921 0.899 0.940 0.915 0.948 0.950 0.925 0. 0.702 0.854 0.906 0.833 0.842 0.876 0.907 Proprietary Models 0.938 0.855 0.804 0.850 0.861 0.946 0.794 0.868 0.805 0.815 0.792 0.790 0.838 0.778 Open-weight Models (Larger than 30B) 0.807 0.790 0.821 0.735 0.787 0.848 0.846 0. Open-weight Models (Less than 30B) 0.539 0.711 0.696 0.668 0.727 0.765 0.734 0.436 0.843 0.774 0.621 0.676 0.665 0.712 0.916 0.824 0.859 0.823 0.826 0.907 0.858 0.791 0.827 0.836 0.844 0.671 0.853 0.716 0.752 0.796 0.906 0.777 0.951 0.875 0.866 0.870 0.878 0.933 0. 0.796 0.902 0.849 0.885 0.539 0.828 0.781 0.620 0.676 0.582 0.764 0.880 0.787 0.822 0.841 0.847 0.806 0.827 0.785 0.847 0.858 0.839 0.611 0.758 0.714 0.689 0.823 0.824 0.753 0.889 0.944 0.833 0.611 0.778 0.875 0. 1.0 0.889 0.778 0.667 0.600 0.692 0.692 0.444 0.444 0.500 0.611 0.897 0.834 0.836 0.828 0.828 0.865 0.806 0.811 0.838 0.836 0.812 0.590 0.780 0.750 0.687 0.739 0.773 0.767 Table 16: ACCcritic of Core-single-image subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Gemma-3-4b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 0.900 0.731 0.721 0.733 0.689 0.891 0.750 0.853 0.705 0.691 0.661 0.391 0.796 0.836 0.709 0.600 0.533 0.707 1.0 0.933 0.923 0.955 1.0 1.0 0. 0.923 0.909 0.947 0.885 0.933 0.727 0.846 0.917 - 0.947 0.955 Proprietary Models 0.824 0.840 0.608 0.608 0.667 0.707 0.745 0.933 0.884 0.807 0.873 0.820 0.896 0.859 Open-weight Models (Larger than 30B) 0.849 0.853 0.869 0. 0.833 0.760 0.725 0.680 Open-weight Models (Less than 30B) 0.583 0.682 0.667 0.733 0.683 0.775 0.828 0.579 0.722 0.765 0.606 0.412 0.444 0.619 - - - - - - - - - - - - - - - - - - 1.0 0.900 0.917 1.0 1.0 1.0 0.833 1.0 0.917 1.0 0.917 0.083 0.750 0.583 0.833 0.777 0.750 0.917 0.667 0.545 0.750 0.727 0.583 0.545 0.666 0.818 0.667 0.667 0.583 0.400 0.600 0.750 0.500 0.600 0.583 0. 0.857 0.400 -0.429 0.500 0.429 0.667 0.714 0.600 0.571 0.333 0.200 0.0 0.0 0.571 0.0 0.0 0.0 1.0 0.906 0.826 0.762 0.799 0.774 0.865 0.818 0.852 0.803 0.804 0.742 0.546 0.711 0.728 0.721 0.645 0.720 0. Table 17: ACCcritic of Open subset on different task types. Model o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-flash Gemini-2.5-pro Grok-2-vision Qwen2.5-vl-32b Qwen2.5-vl-72b Pixtral-large Llama-4-maverick Qwen2.5-vl-3b Qwen2.5-vl-7b Llama-3.2-11b-vision Pixtral-12b Gemma-3-12b Gemma-3-27b Llama-4-scout Perception Planning Knowledge Information Extraction Mathematics Coding Science Metric Avg. Task types 0.778 0.684 0.593 0.717 0.722 0.863 0.680 0.429 0.759 0.853 0.608 0.625 0.744 0.833 0.528 0.700 0.638 0. 1.0 0.875 0.739 1.0 0.870 0.895 0.957 1.0 0.957 0.900 0.818 0.9375 0.857 0.778 0.870 0.750 1.0 0.905 Proprietary Models 0.583 0.833 0.750 0.750 0.583 0.833 0.833 0.891 0.796 0.679 0.803 0.745 0.866 0. Open-weight Models (Larger than 30B) 0.835 0.818 0.839 0.722 1.0 0.667 0.857 0.667 Open-weight Models (Less than 30B) 0.613 0.778 0.741 0.706 0.684 0.754 0.847 0.416 0.778 0.636 0.583 0.333 0.667 0.833 - - - - - - - - - - - - - - - - - - 0.916 0.900 0.917 1.0 1.0 1.0 1.0 0.833 0.917 0.917 0.917 0.166 0.909 0.889 0.833 0.750 0.727 0.917 0.750 0.889 0.900 0.833 0.750 0.750 0.857 - 0.667 0.750 0. 0.5 0.818 0.636 0.818 0.636 0.636 0.600 - - - - - - - - - - - - - - - - - - 0.856 0.789 0.690 0.808 0.756 0.866 0.806 0.794 0.808 0.845 0. 0.611 0.788 0.759 0.688 0.671 0.744 0.818 Table 18: ACCcritic of Open-single-image subset on different task types."
        },
        {
            "title": "Core",
            "content": "Core Single-image"
        },
        {
            "title": "Open",
            "content": "Open Single-image Avg."
        },
        {
            "title": "Score",
            "content": "o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-pro o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-pro o4-mini GPT-4o GPT-4o-mini Claude-3.7-sonnet Gemini-2.5-pro 0.896 0.832 0.833 0.834 0.865 0.896 0.832 0.896 0.834 0.865 0.896 0.832 0.833 0.834 0. Annotator: GPT-4o, Critique Judge: GPT-4.1 7.924 7.499 6.634 8.113 8.558 0.897 0.834 0.836 0.828 0.865 7.952 7.429 6.534 8.080 8.549 0.906 0.826 0.762 0.799 0.865 7.877 7.807 6.549 8.102 8. 0.856 0.789 0.690 0.808 0.866 Annotator: Gemini-2.5-flash, Critique Judge: GPT-4.1 8.383 8.617 7.617 8.583 8.833 0.897 0.834 0.897 0.828 0.865 7.745 7.618 6.400 7.691 8.055 0.906 0.826 0.906 0.799 0.865 8.600 8.100 7.183 8.583 9. 0.856 0.789 0.856 0.808 0.866 Annotator: Gemini-2.5-flash, Critique Judge: Claude-4.0-sonnet 7.457 7.000 6.761 7.652 7.978 0.897 0.834 0.836 0.828 0.865 7.383 6.450 5.500 7.250 8.300 0.906 0.826 0.762 0.799 0. 7.189 6.566 5.377 7.038 7.566 0.856 0.789 0.690 0.808 0.866 7.976 7.637 6.416 8.097 8.325 8.273 7.745 6.636 8.278 8.345 7.727 6.945 5.600 7.636 7.800 0.900 0.830 0.821 0.831 0. 0.900 0.830 0.821 0.831 0.865 0.900 0.830 0.821 0.831 0.865 7.933 7.503 6.580 8.099 8.514 8.261 8.035 6.978 8.297 8.587 7.439 6.724 5.772 7.383 7.920 Table 19: Model Performance ranking comparison under different combinations of annotation and judge models."
        },
        {
            "title": "E Case study",
            "content": "Figure 10: An example of visual mathematical reasoning task, where the response is clearly incorrect, demonstrates that the o4-mini model provides an accurate judgment along with comprehensive textual critique. When evaluating its critique score, it is evident that the presence of the reference critique effectively guides the judge model to assign high-quality score of 9, surpassing even the reference critique in some aspects. Figure 11: An example from the brand logo recognition and elaboration task, where the critique is generated by Qwen2.5-vl-32b. The model produces detailed, step-by-step reasoning critique. When scored by the judge model, it explicitly explains its preference for lengthier, somewhat redundant reasoninghighlighted in bold reddemonstrating bias toward richer textual justifications. Figure 12: An example from the StackOverflow debug QA task, where the critique is generated by claude-3.7-sonnet. The critique provides complete analysis and exactly points out the original responses error. Compared with the reference critique, the judge model accurately evaluates that Claude-3.7-sonnet, as well-known pioneer model of coding, outperforms the reference critique. Figure 13: An example from the GUI agent application task, where the critique is generated by Genimi-2.5-pro. It can accurately tell the correctness of the models response, and the textual critique score is slightly below the anchored reference critique score (e.g., 8), where the judge model provides reasonable explanation (marked in red). Figure 14: An example of comparative critique from the face keypoint detection task, where the critique is generated by Pixtral-large(24-11). The original responses are scored for their response quality scores by the annotator model (GPT-4o). As (low, medium) pairwise comparison, it is easy to distinguish the better one with high-performance model."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "Beijing Normal-Hong Kong Baptist University",
        "Hong Kong Baptist University",
        "National University of Singapore",
        "Shanghai Jiao Tong University"
    ]
}