{
    "paper_title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "authors": [
        "Zhouliang Yu",
        "Yuhuan Yuan",
        "Tim Z. Xiao",
        "Fuxiang Frank Xia",
        "Jie Fu",
        "Ge Zhang",
        "Ge Lin",
        "Weiyang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 2 7 4 0 . 2 0 5 2 : r Generating Symbolic World Models via Test-time Scaling of Large Language Models Zhouliang Yu1,2,* Yuhuan Yuan3,* Tim Z. Xiao4 Fuxiang Frank Xia5 Jie Fu6 Ge Zhang7 Ge Lin3, Weiyang Liu4, 1The Chinese University of Hong Kong 2 The Hong Kong University of Science and Technology 3The Hong Kong University of Science and Technology (Guangzhou) 4Max Planck Institute for Intelligent Systems, TÃ¼bingen 5Environmental Systems Research Institute, Inc. 6Shanghai Artificial Intelligence Laboratory 7SEED, Bytedance"
        },
        {
            "title": "Abstract",
            "content": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimalitya task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate symbolic world model where classic searching algorithms, such as A, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce simple yet effective algorithm, which first employs Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks."
        },
        {
            "title": "1 Introduction",
            "content": "Enabling large language models (LLMs) to plan in complex scenarios like Barman, Floortile, and Termes remains an open problem. While recent LLMs like OpenAI-o1 excel at complex reasoning tasks, including coding and mathematics, they still struggle with deductive reasoning and principled planning that requires the consideration of optimality, constraints, and complex state transitions. This limitation persists in o1 even using self-critique techniques and multiple answer re-sampling strategies. natural solution is translating the world abstraction from natural language into Planning Domain Definition Language (PDDL), which utilizes first-order logic (FOL) to explicitly describe states and relationships. Compared to natural language, the formal nature of PDDL simplifies verification and enables the precise specification of constraints and objectives, facilitating the seamless integration of off-the-shelf planning algorithms., However, it remains huge challenge to translate natural language descriptions into PDDL domains with satisfactory accuracy. Current LLMs perform poorly in this translation task due to two key challenges: the scarcity of high-quality PDDL training data and the complexity of maintaining logical consistency across predicates and actions. Traditionally, the translation process has heavily relied on human expertise and manual refinement, making it difficult to automate and scale [GVSK23]. *Equal contribution. Corresponding Authors. Project page: https://vmlpddl.github.io Technical Report v1 1 Figure 1: An overview of the proposed method. Our test-time compute scaling approach consists of two main steps: (1) Best-of-N Sampling for PDDL Initialization (see Section 3.2): We start by running parallel sampling process to generate multiple chain-of-thought responses that are composed of the formalized PDDL-based world model representation Di and the natural language thought Ti. (2) Closed-loop Iteration with iVML (see Section 3.3): We use Instance Verbalized Machine Learning (iVML) to iteratively improve the solutions. The iVML incorporates: (1) An optimizer LLM fopt that evaluates the solutions from the previous iteration, and (2) learner LLM flearner that learns from the feedback and updates the PDDL-based world model Di. The most optimal PDDL-based world model would be sent to the systematic search engine for planning. To address these challenges, our work leverages LLMs to generate PDDL-based symbolic world models for task planning without requiring model finetuning. We achieve this through simple yet effective strategy to scale the test-time computation of LLMs. Specifically, we start by generating multiple PDDL domains from the input text query using Best-of-N (BoN) sampling. This step aims to find good initial solution that is error-free and logically correct. Then we refine the best initial solution by optimizing the text with instance verbalized machine learning (iVML) such that the generated PDDL domain can gradually fit the input query (e.g., natural language descriptions, PDDL problems). Our method is guided by the insight that effective test-time scaling can elicit stronger reasoning capabilities over formal languages like PDDL, thereby compensating for the scarcity of high-quality PDDL training data. We start by applying BoN sampling to explore the solution space and select the best initial solution, and then iVML aims to exploit the solution space around this initialization such that the solution gets gradually improved. VML [XBSL24] is test-time training approach designed to iteratively refine learner LLMs text prompt based on feedback from an optimizer LLM, which takes into account the training data and learning objectives. The learner LLMs text prompt characterizes data patterns to perform inductive inference tasks such as regression and classification. In our paper, we apply VML to PDDL domain refinement and propose the instance VML (iVML) framework that reformulates VML for instance learning. Unlike the original tasks in [XBSL24], PDDL domain refinement is an instance learning 2 task without training data to produce verbalized gradients. Therefore, rather than requiring training data, we use LLMs to verify the validity of PDDL domains and generate critiques that serve as verbalized gradients. Specifically, the learner LLM receives an initial PDDL domain and generates refined version based on critiques from an optimizer LLM, iteratively eliminating logical inconsistencies and grammatical errors. However, the performance of iVML is highly dependent on the quality of initial solutions, as poor initialization can result in slow convergence or suboptimal solutions. While BoN sampling emphasizes exploration by independently generating diverse solutions, it does not leverage past predictions from LLMs, hence limiting its ability to exploit the solution space. iVML, in contrast, emphasizes exploitation by iteratively refining solutions based on the optimizer LLMs feedback. To achieve good balance between exploration and exploitation, we propose to combine these complementary strategies for generating high-quality PDDL domains. This hybrid approach leverages the strengths of both methods by applying iVML to refine solutions initially generated through BoN sampling. Our contributions are listed below: Scalable PDDL domain generation: We propose an effective test-time scaling approach for automatic and scalable PDDL domain generation without additional model training. Using Qwen2.5Coder-7B as the base model, our approach achieves state-of-the-art performance with an 85.2% success rate on the NL2Domain task and 71.4% on Prob2Domain, substantially surpassing o1s performance (41.7% and 33.7% respectively). Application of VML to instance learning. We introduce the iVML framework to adapt VML to instance learning, where there is no training data available. We use LLMs to check the validity of PDDL domains and generate textual critiques as gradients to iteratively update PDDL domains. Efficient test-time compute scaling: We enhance VML with BoN sampling initialization, effectively balancing exploration and exploitation to achieve faster convergence and obtain better solutions. Robust planning through PDDL abstraction: We demonstrate that PDDL-based formal abstraction enables more robust planning compared to direct LLM-based approaches. Our method successfully handles complex domains such as Barman and Termes, where existing LLM-based planners fail."
        },
        {
            "title": "2.1 LLMs for Task Planning",
            "content": "a Recent advances in large language models (LLMs), such as OpenAI-o1 [ZCY+24] and Qwen [YYH+24], have shown promise in handling common reasoning tasks such as GSM8K [CKB+21] and HumanEval [CTJ+21]. The gain of reasoning and planning capabilities of LLMs can be attributed to (but not limited to) several factors: (1) Extensive training on reasoning datasets: [YXWK24] fine-tunes LLMs with distilled chain-of-thought datasets to improve plausible reasoning, and recent models have scaled this approach using larger and more diverse datasets, which has enhanced performance on tasks such as mathematical reasoning, coding, and logical reasoning. However, such an approach raises concerns about whether the observed gains are attributable to data contamination [SKN24, MAS+24]. (2) Self-improvement during inference time: Some approaches incorporate verifiers that provide synthetic feedback during inference, using self-critique [XBSL24], process reward models [ZHB+24] or simple sparse objective reward [ZCS+23] guide improvement. However, [SKN24] shows that imperfect verifiers increase false positive samples during scaling up reasoning at inference time. Despite these plausible advancements in common benchmarks, LLMs face complex reasoning and planning challenges. Results from constraint-heavy and spatially complex planning tasks, for example, Termes (see Figure 2) demonstrate that LLMs continue to struggle with planning tasks requiring intricate multi-step logical reasoning, simultaneous management of multiple constraints, and manipulation of spatial relationships [VMO+24, WLB+24, QLF+25]. These challenges often lead to inconsistent or suboptimal outcomes or worse, hallucinations (see 3 Figure 2: OpenAI-o1 plans for Termes: o1 frequently exhibits hallucination during the planning process. Specifically, in steps three and four, the LLM violates predefined rules when selecting and leveraging actions. Additionally, step four hallucinates the achievement of the goal, leading to incorrect or unrealistic outcomes. Even when using o1 itself to evaluate the hallucinated plan, it incorrectly identifies the plan as valid."
        },
        {
            "title": "Technical Method",
            "content": "[GVSK23]"
        },
        {
            "title": "Each action separately\nwithin PDDL domains",
            "content": "3 domains: Household, Logistics, Tyreworld"
        },
        {
            "title": "Human experts",
            "content": "[ZVL+24] Whole PDDL problems (initial state and goal) 2 domains: Gripper, Blockworld"
        },
        {
            "title": "283 IPC domains (NL2Domain)\n332 IPC domains (Prob2Domain)",
            "content": "Test-time scaling without model training & human experts Table 1: Comparison between current PDDL synthesis methods and ours. Figure 2). Although LLMs can approximate state transitions [HGM+23], they do so by probabilistically predicting subsequent tokens based on patterns learned from vast datasets, rather than through logical deduction or structured inference [Kam24]. Inspired by traditional model-based reinforcement learning approaches [AS24], which solve decision-making problems by predicting discrete world models for heuristic search, our method significantly enhances LLMs planning capabilities through two-stage process. First, we predict symbolic Planning Domain Definition Language (PDDL)-based world model using an instance verbalized machine learning approach. This stage transforms the problem into structured, symbolic representation, enabling more logical and systematic reasoning. Second, we leverage heuristic search methods, such as A, to efficiently find optimal solutions within this structured framework."
        },
        {
            "title": "2.2 World Model Generation",
            "content": "However, automatically generating scalable PDDL-based world models by LLMs is still challenging. Current LLMs rely heavily on either human-in-the-loop or extensive training data to plausibly be superior in generating PDDL on limited scenarios. For example, [GVSK23] leverages multiple human experts to refine the logical error in individual PDDL action expressions generated by LLMs. [ZVL+24] collect more than 132,027 SFT data to train LLM on limited simple planning scenarios, (e.g., BlockWorld and Grippers). 4 In contrast, our approach focuses on automation and scalability across diverse planning scenarios without additional training (no training data is needed). We compare our method and current works in Table 1. Several methods have also explored using LLMs to generate world model representations other than PDDL. For example, GIF-MCTS [DMAM24] and World-coder [TKE24] translate natural language descriptions of world models into Gym-like Python code [BCP+16], and using pre-collected trajectories to validate and providing feedback for wrong state-transition predictions iteratively. Our work, however, focuses on more general planning scenarios without pre-collected validation datasets to provide critique feedback and ensuring the correctness of world modeling."
        },
        {
            "title": "2.3 Adaptation of LLMs",
            "content": "In the absence of pre-collected validation datasets, the adaptation of LLMs for the PDDL-based world model presents unique challenges. Parameter-efficient finetuning methods (e.g., [HWAZ+22, QLF+23, LQF+24, DQY+23]) enable effective adaptation of LLMs to downstream tasks, they still require high-quality training data. If there lacks sufficient amount of training data, in-context learning [BMR+20, WTB+22, DLD+22] offers an alternative adaptation approach. Prompt optimization techniques [ZMH+22, PIL+23, YWL+24] further enhance adaptation performance by deriving improved instruction prompts from limited training data. Recently, [XBSL24] proposes an iterative framework to update model-characterizing text prompts with LLMs. [PIL+23, YBB+24] introduce the concept of textual gradients as criteria for updating text prompts. Although finetuning methods can achieve effective adaptation, they risk causing catastrophic forgetting in pretrained LLMs, potentially compromising their general instruction-following capabilities. Therefore, test-time adaptation of LLMs to downstream tasks has emerged as practical solution. In our paper, we introduce simple yet effective test-time adaptation method for PDDL-based world model generation that scales efficiently with test-time computing and requires no model finetuning."
        },
        {
            "title": "3 The Proposed Test-time Compute Scaling Approach",
            "content": "Our proposed methods aim to explore the following key questions: How can PDDL serve as good world representation for planning? Natural language task planning faces significant challenges in state estimation, constraint-based plan generation, and plan validation. How can we create explicit, unambiguous world models? (see Section 3.1) How can we effectively generate PDDL-based world models? Generating symbolic world models requires not only natural language understanding but also sophisticated deductive reasoning to maintain logical consistency across all model components. Without such formal modeling sophistication, models risk generating inconsistent state transitions and producing suboptimal plans. To address this challenge, we enhance LLMs reasoning capabilities through an instance verbalized machine learning algorithm (see Section 3.3), initializing it with good candidates generated by best-of-N sampling."
        },
        {
            "title": "3.1 PDDL-based World Model Representation",
            "content": "Classical planning problems are inherently complex. Even determining plan satisfiability [RN16]whether any solution exists for given planning problemis NP-hard. Planning problems that involve optimization under constraints pose even greater challenges for natural language planners. Our approach, which utilizes the PDDL-based representation, offers several advantages: (1) PDDL employs logical system to express atoms and predicates derived from STRIPS [FN71] (i.e., Stanford Research Institute Problem Solver), and therefore it provides formal and unambiguous syntax for representing world models. PDDL-based world modeling employs explicit representations that not only enrich the description of actions and states but also ensure precise and straightforward validation, thereby eliminating ambiguity. (2) Natural 5 language plan prediction often reduces to an n-gram task of ungrounded token generation. We provide the description of the Termes problem in both natural language (see Figure 2) and PDDL (see the text box below) to illustrate the difference. By adopting PDDL as representation, we transform this task into explicit classical planning. This formulation enables the use of search algorithms such as A. With the help of the PDDL representation, planning graph (e.g., Figure 6) can be used to provide improved heuristic estimates when searching through the state space."
        },
        {
            "title": "Termes in Planning Domain Definition Language",
            "content": "Domain: ( i ( domain m ) ( : u m s ( : e : i : a e e d o ) numb e o i b t ) ( : d t ( g ? i n ? numb ) ( ? i n ) ( hasb k ) (SUCC ? n1 numb ? n2 numb ) (NEIGHBOR ? p1 i n ? p2 i n ) ( IS DEPOT ? i n ) ) ( : i move ( ? from i n ? o i ? numb ) : a e : c i n ( and ( ? from ) ( NEIGHBOR ? from ? ) ( g ? from ? ) ( g ? ? ) ) ) : e ( : i moveup ( ? from ) ) ( ? ) ( and ( ) ( ? from i n ? hfrom numb ? o i ? numb ) : a e : c i n ( and ( ? from ) ( NEIGHBOR ? from ? ) ( g ? from ? hfrom ) ( g ? ? ) ( SUCC ? ? hfrom ) ) : e ( ? from ) ) ( ? ) ) ) ( and ( ( : i movedown . . . ( : i l b k . . . ( : i removeb k . . . ( : i r e o . . . ( : i e o o . . . ) Problem: ( i ( b t e 0003800364 x3x3random_towers_4x3_3_1_3 ) ( : domain m ) m 0003800364 x3x3random_towers_4x3_3_1_3 ; t ; 0 ; 0 ; ; 0 ; l 0 ; 0 ; ; 0 ; Maximal g : 3 ( : e s e : 0 R0D 0 0 0 0 0 0 0 t : 0 0 3 0 0 0 0 0 0 n0 numb . . . . . . pos 00 i n . . . . . . ) ( : t ( g pos 00 n0 ) . . . . . . ( pos 20) (SUCC n1 n0 ) . . . . . . (NEIGHBOR pos 00 pos 1 0 ) . . . . . . ( IS DEPOT pos 20) ) ( : l ( and ( g pos 00 n0 ) . . . . . . ( ( hasb k ) ) ) ) )"
        },
        {
            "title": "3.2 Best-of-N Sampling for PDDL Initialization",
            "content": "Our test-time scaling approach adopts two-stage coarse-to-fine optimization process. The first stage is to coarsely search good initial solution, and the second stage is to iteratively refine this solution in fine-grained manner. To efficiently get diverse set of plausible solutions, before LLM iteratively the internal logic with iVML, we adopt Best-of-N sampling to find good solution as the initial PDDL-based world model. For each problem, the LLM generates candidate solutions in parallel and retains the 6 samples with the highest log-likelihoods. This process involves three main steps: candidate generation, scoring, and selection. During sampling, high temperature parameter is used to add more randomness and diversity to the solution space. Each candidate ci is assigned score Si based on the sum of the log-likelihoods of its generated tokens: Si = LiX t= log pt (cid:16) w(i) (cid:17) , {1, 2, . . . , } (3.1) where:Li is the length of candidate ci, w(i) token w(i) chosen as the initialization points to be optimized by iVML. at position t. During the selection phase, the top candidates with the highest scores are is the t-th token of candidate ci, pt (cid:17) (cid:16) w(i) is the probability of 3.3 iVML: Instance Verbalized Machine learning With the BoN sampling to select the candidates as the initial solution, we introduce instance verbalized machine learning to refine both the generated PDDL domain and the natural language chain of thought. iVML is an adaptation of verbalized machine learning [XBSL24] to the instance optimization setting, where the goal is to optimize and refine single instance (i.e., PDDL domains in this paper). In iVML, functions are parameterized using natural language rather than numerical values. Viewing an LLM as the inference engine, we can evaluate such natural language parameterized function, and optimize its model parameters in the natural language space. In our setting, we are given description from the planning domain either in natural language for NL2Domain or in problem code for Prob2Domain, and we aim to generate an accurate corresponding PDDL domain description D, i.e., = arg min L(G, D) (3.2) where L() is loss function defining the closeness between and D. Solving Equation (3.2) is difficult as both and are text, and L() is hard to define unless abstractly using natural language. Using the iVML framework, we can approximately solve Equation (3.2) with an iterative algorithm that alternates between two natural language parameterized functions at the iteration i: Fi = fopt(L, G, Ti1, Di1), Ti, Di = fupdate(Fi, Ti1, Di1), (3.3) (3.4) where Ti1 and Di1 correspond to the current thoughts and the current PDDL domain, Fi is the feedback from the optimizer function fopt(), Ti and Di are the updated thoughts and PDDL domain output from the update function fupdate(). D0 is initialized from the best-of-N sampleing. These two functions are evaluated through separate LLMs calls. We show the prompt templates for fopt() and fupdate() below: Prompt template for fopt() You will be provided natural language description of planning domain, and its corresponding PDDL domain code with intermediate thoughts explaining each predicate and action. Your task is to generate critical feedback on the PDDL domain code based on the natural language description. You should evaluate the grammar and logic of the PDDL domain codes, and the logic error in the intermediate thoughts. PDDL synthesis problem: {G} natural language chain of thoughts: {Ti1} Generated PDDL domain: {Di1} 7 Prompt template for fupdate() You will be provided PDDL domain code and critical feedback on the PDDL domain code based on the natural language description. Your task is to generate new PDDL domain code that is more consistent with the natural language description. PDDL synthesis problem: {G} Natural language chain of thoughts at the previous turn: {Ti1} Generated PDDL domain at the previous turn: {Di1} The error of the PDDL domain {Fi1} iVML with BoN initialization effectively balances exploration and exploitation. BoN employs broad exploration strategy, maintaining diverse candidate solutions to probe distinct regions of the combinatorial search space. While this approach mitigates initialization bias, it suffers from diminishing returns: beyond critical sample size, the probability of discovering novel valid solutions decays due to redundant model generations. In contrast, iVML performs verbalized in-context exploitation by iteratively refining BoN-selected candidates based on objectives defined in natural language. This closedloop process enables precise error correction (e.g., resolving precondition conflicts in PDDL actions) but remains susceptible to local minimaa fundamental challenge in non-convex optimization [SYC+24]. Our approach combines BoN and iVML to achieve an effective balance between exploration and exploitation, addressing the limitations of each method alone through two-phase optimization framework: (1) BoN initialization that generates multiple diverse and high-quality initializations {D0 i=1, and (2) iVML refinement that optimizes these initial solutions with verbalized machine learning. (i)}k"
        },
        {
            "title": "4 Experiments and Results",
            "content": "We conduct extensive experiments to compare our test-time scaling algorithm to existing state-of-the-art methods on competition-level PDDL domain synthesis tasks. Our method improves PDDL generation across nearly all tested LLMs. By using PDDL as an intermediate abstraction layer, we have shifted the role of LLMs from acting as planners to generating PDDL-based world models. The generated PDDL-based world model, combined with classical planner in the loop, helps to reduce hallucinations when using LLMs directly as planners."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Evaluation tasks and datasets. We evaluate several test-time scaling methods on the International Planning Competition benchmark1, which encompasses diverse complex planning domains and problems. Our evaluation focuses on two key PDDL domain synthesis tasks, including (1) NL2Domain which aims to convert Natural Language Descriptions to PDDL Domains; and (2) Prob2Domain which aims to derive necessary PDDL domains from PDDL problems. The evaluation metric used here is the success rate of the generated PDDL domain passing the PDDL validation system [HL03]. Large language model settings. The backbone LLMs in our experiment include Qwen2.5-Instruct (0.5B-72B parameters) [YYH+24], LLaMA3.1-Instruct (8B and 70B parameters) [DJP+24], and Yi-1.5Chat (6B, 9B, and 34B parameters) [YCL+24]. We also incorporate specialized code-oriented LLMs, specifically Qwen2.5-Coder and Yi-1.5-Coder. In addition to open-source LLMs, we benchmark against OpenAIs proprietary models, including GPT-4o, o1-mini, and o1-preview. We test our proposed methods on Qwen models in zero-shot setting without model finetuning. Chain of thought prompting. All baselines here utilize chain-of-thought (CoT) prompting by default. Current LLMs have been extensively trained on datasets that include step-by-step reasoning besides 1https://github.com/potassco/pddl-instances 8 Model Params NL2Domain (%) Problem2Domain (%) Avg. (%) Open-Source Models Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Qwen2.5-Coder Qwen2.5-Coder Llama3.1-Instruct Llama3.1-Instruct Yi-1.5-Chat Yi-1.5-Chat Yi-1.5-Chat Yi-Coder Yi-Coder GPT-4o o1-mini o1-preview BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instrcut BoN-8-Qwen2.5-Instruct BoN-8-Qwen2.5-Coder iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Instruct iVML-5-BoN-8-Qwen2.5-Coder 0.5B 1.5B 3B 7B 14B 32B 72B 1.5B 7B 8B 70B 6B 9B 34B 1.5B 9B - - - 0.5B 1.5B 3B 7B 14B 32B 72B 7B 0.5B 1.5B 3B 7B 14B 32B 72B 7B 0.0 0.0 2. 5.7 21.6 24.0 38.5 0.0 21. 0.0 1.1 0.4 6.7 12.0 0. 9.9 Closed-Source Models 5.3 41.7 55.8 Our Methods 0.0 (+0.0) 2.1 (+2.1) 11.7 (+9.5) 9.2 (+3.5) 51.6 (+30.0) 66.8 (+46.7) 60.8 (+22.3) 73.1 (+51.2) 0.0 (+0.0) 2.8 (+2.8) 18.7 (+16.6) 21.9 (+16.2) 77.0 (+55.4) 86.2 (+62.2) 78.4 (+39.9) 85.2 (+63.3) 0.0 0. 1.5 11.7 25.3 31.6 32.8 0. 18.4 0.0 0.0 1.8 9.3 8. 0.0 14.5 50.0 33.7 52.4 0.0 (+0.0) 0.3 (+0.3) 1.2 (+0.3) 34.6 (+22.9) 62.0 (+36.7) 71.1 (+39.5) 73.8 (+41.0) 63.3 (+44.9) 0.0 (+0.0) 0.3 (+0.3) 1.8 (+0.3) 49.1 (+37.3) 80.4 (+55.1) 90.9 (+59.3) 86.4 (+53.6) 71.4 (+53.0) 0.0 0.0 0. 8.7 23.5 27.8 35.7 0.0 20. 0.0 0.6 1.1 8.0 10.4 0. 12.2 27.7 37.7 54.1 0.0 1. 6.5 21.9 56.8 70.9 67.3 68. 0.0 1.6 10.3 35.5 78.7 88. 82.4 78.3 Table 2: comparison of performance in PDDL domain synthesis. BoN-8 refers to BoN sampling with 8 candidates, while iVML-5-BoN-8 denotes five iterations of iVML training initialized with BoN-8. final answers [WS23]. This enables the models to generate better reasoning traces during inference. The detailed CoT prompt template for our method is provided in Appendix D. Sampling hyperparameters. To generate diverse PDDL domain synthesis paths, we use temperature sampling (T = 0.7) for both the BoN and iVML algorithms."
        },
        {
            "title": "4.2 Main Results in PDDL Domain Synthesis",
            "content": "Current LLMs perform poorly in PDDL domain synthesis. Despite advances in code and math reasoning, LLMs exhibit fundamental limitations in PDDL-based formal synthesis. For instance in Table 2, 9 Figure 3: Left: The performance trend of iVML with increasing training epochs. Right: The performance trend of BoN with increasing sampling numbers. Qwen2.5-Instruct (72B) achieves only 38.5% and 32.8% accuracy in NL2Domain and Prob2Domain tasks, respectively. The results suggest that existing LLMs still fall short in symbolic reasoning tasks. Search-augmented reasoning enhances formal synthesis. In Table 2, among the closed-source models, o1-preview emerges as the top performer with an average accuracy of 54.1%, outperforming other models in both NL2Domain and Problem2Domain tasks. The o1-series models, which integrate search-based reasoning during inference [QLZ+24, ZCY+24], demonstrate significant improvements over standard instruction LLMs. For example, GPT-4o achieves only an average accuracy of 27.7%. Code-oriented models outperform general-purpose models. In Table 2, we can observe that code-specialized models (e.g., Qwen2.5-Coder and Yi-1.5-Coder) demonstrate superior performance than their general-purpose counterparts. For example, Qwen2.5-Coder (7B) outperforms Qwen-Instruct (7B) by 16.2% NL2Domain (i.e., 21.9% compared to 5.7%). We hypothesize that the improvements stem from: (1) Implicit formalization training: code datasets teach type systems and predicate logic, (2) Syntax-sensitive decoding: token-wise likelihood aligns with PDDLs Lisp-like structure, and (3) Autoformalization priors: the code datasets that interleave natural language comments with pieces of code are high-quality datasets for chain-of-thought reasoning and autoformalization. Test-time scaling is helpful for LLM at almost all scales. The BoN sampling method demonstrates universal effectiveness across the Qwen model family (e.g., 1.5B to 72B parameters), significantly improving PDDL domain synthesis accuracy. For example in Table 2, BoN sampling with 8 candidates (BoN-8) improves Qwen2.5-Instruct (14B) from 21.6% to 51.6% on NL2Domain. Gains persist through Qwen2.5Instruct (72B) with 22.3% improvement on NL2Domain. Test-time compute scaling requires no further training or architectural changes, making it computationally efficient addition to scaling up LLMs parameter numbers. iVML can provide robust and consistent improvement. iVML delivers robust performance gains over BoN across multiple model scales, demonstrating the power of iterative self-improvement in PDDL domain synthesis. As illustrated in Table 2, five iterations of iVML training with BoN-8 initialization (iVML-5-BoN-8) enables Qwen2.5-Instruct (32B) to achieve 86.2% on NL2Domain, outperforming base BoN-8 with 19.4% improvement. The results position iVML as scalable and efficient framework for enhancing LLM performance in formal synthesis tasks."
        },
        {
            "title": "4.3 Convergence Comparison between BoN and iVML",
            "content": "Experiment settings. This section presents comparative analysis of the convergence behavior of BoN and iVML in PDDL domain synthesis tasks. The computational efficiency and synthesis success 10 rates of these methods depend on two parameters: the sampling budget for BoN , and the number of training epochs for iVML . Through controlled experiments, we examine how parametric variations affect synthesis effectiveness and identify the conditions under which their performance converges. Our empirical evaluation uses Qwen2.5-Coder (7B) as the backbone LLM. The initialization of iVML is based on BoN-8 if not otherwise specified. Convergence of BoN and iVML. In Figure 3, we observe that BoN sampling undergoes two phases. Phase 1 (N 32): Accuracy improves sublinearly with the sampling budget, demonstrating the exploration efficiency of candidate sampling for enhancing accuracy. Phase 2 (N > 32): Performance reaches saturation with observable degradation trends. In contrast, iVML exhibits monotonic performance improvement up to = 80, significantly surpassing the saturated success rate achieved by BoN. For example, BoN saturates at 260 domains for NL2Domain and fails to exceed 300 domains for Prob2Domain at = 256. In comparison, iVML successfully synthesizes more than 270 domains for NL2Domain and achieves around 310 domains for Prob2Domain at = 80, demonstrating its superior performance over BoN. Case study. The qualitative case study is presented in Table 3, where we show BoN often fails to generate the correct code. For example, in TyreWorld, despite explicitly stating the precondition that the container is open, BoN still generates the invalid predicate closed ?container to represent the relationship. Unlike BoNs brute-force sampling approach, iVML leverages an in-context self-refinement mechanism to (1) identify constraint violations (e.g., illegal block stacking or invalid preconditions); (2) generate counterfactual natural language feedback to guide revisions; (3) strengthen domain-specific reasoning priors through iterative updates. Consequently, iVML can fix the BoN error and formulate the corrected predicate not (closed?container) Analysis. In Figure 3, we observe BoNs early saturation and performance degradation. This observation aligns with inference scaling flaws [SKN24], where the optimal number of resampling steps for code synthesis tasks (e.g., HumanEval [AON+21] and MBPP [CTJ+21]) is always finite and low. This phenomenon arises from the lack of in-context learning with feedback from verifier. However, iVML uses self-critique mechanism combined with in-context learning to create synthetic curriculum, enabling the model to learn to avoid error patterns in PDDL synthesis progressively. In contrast, BoN struggles with such dependencies due to its reliance on static sampling and the lack of iterative refinement."
        },
        {
            "title": "4.4 Ablation Study of Initialization Strategies",
            "content": "Experiment settings. This section investigates the effect of initialization strategies on iVML through controlled experiment. We evaluate three LLMsQwen2.5-Coder (7B), Deepseek-Coder-Instruct-v1.5 (7B), and LLaMa-3.1-Instruct (8B)across two different initialization settings: single-pass and BoN-8. Among these models, LLaMa-3.1-Instruct (8B) is defined as weak model in PDDL synthesis, as it achieves zero success rate in the main experiment in Table 2. The purpose of including it in this study is to investigate whether our approach can enhance LLaMas capabilities in PDDL synthesis, enabling its transition from weak to strong model. BoN vs. Single-pass sampling. BoN sampling, as an initialization strategy, provides iVML with the dual advantages of accelerated convergence and improved solution quality. For example, in Figure 4, with BoN-8 initialization in NL2Domain, Deepseek-Coder saturates earlier than its single-pass counterpart at = 16, while achieving higher accuracy (reaching approximately 270 successful domains compared to fewer than 200 in single-pass). Unlike single-pass sampling, which is analogous to random initialization in traditional optimization, e.g., stochastic gradient descent, BoN generates diverse set of initial candidate solutions. The solution diversity can effectively improve iVML with expanded exploration. By covering broader range of the solution space, BoN helps to avoid early convergence to suboptimal solutions. This means that the algorithm is less likely to get stuck in local minima, which are common challenges in complex optimization problems [Bak19]. By selecting high-quality initial solution candidates, BoN guides 11 Natural Language BoN@8 BoN@8 + iVML@ BlockWorld (stack): The arm can stack block on top of another if it holds the top block and the bottom block is clear. Post-action, the arm is empty, the top block rests on the bottom block, and the bottom block is no longer clear. (:action stack :parameters (?top - block ?bottom - block) :precondition (and (holding ?top) (clear ?bottom)) :effect (and (not (holding ?top)) (on ?top ?bottom) (clear ?bottom)) Incorrectly clear the bottom block, which should not happen when stacking block on the bottom block. (:action stack :parameters (?top - block ?bottom - block) :precondition (and (holding ?top) (clear ?bottom)) :effect (and (arm-empty) (not (holding ?top)) (not (clear ?bottom)) (on ?top ?bottom))) TyreWorld(fetch): The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container. (:action fetch :parameters (?obj - object ?container - container) :precondition (and (in ?obj ?container) (closed ?container)) :effect (and (in ?obj ?container) (not (in ?obj ?container))) (:action return-against :parameters (?current-city - city ?start-city - city) :precondition (and (in ?currentcity) ?start-city (connected ?current-city)(not-complete)) :effect (and (in ?start-city) (complete))) TSP(return-against): This action also completes the tour by allowing the agent to return to the starting city against the direction of connection. The precondition is similar to the return-along action, except the city is not previously visited. The effect is that the agent returns to the starting point and the tour is marked as complete. Termes(remove-block): Create block at the depot. The robot will have the block The logic for the fetch action is incorrect. Precondition: Incorrectly checks if the object is inside the container and the container is open. Effect: Incorrect logic. The effect should be (not (in ?obj ?container)) instead of (in ?obj ?container) (not (in ?obj ?container)) (:action fetch :parameters (?obj - object ?container - container) :precondition (and (in ?obj ?container) (not (closed ?container))) :effect (not (in ?obj ?container)) The precondition for this action should have (connected ?current-city ?start-city) instead of (connected ?start-city ?current-city) to correctly reflect the travel direction. (:action return-against :parameters (?current-city - city ?start-city - city) :precondition (and (in ?current-city) ?current-city ?start-city) (not-complete)) :effect (and (in ?start-city) (complete))) (connected (:action remove-block :parameters (fromPos toPos) :precondition (and (height fromPos ?h) (SUCC ?h ?h-down) (height ?h-down) toPos fromPos) (not (has-block))) :effect (and (has-block) (height toPos ?h))) (at The effect of the remove-block action does not correctly update the height of the blocks position. It should be (height toPos ?h-down), but it is currently (?h-down). (:action remove-block :parameters (fromPos toPos) :precondition (and (height fromPos ?h) (SUCC ?h ?hdown) (height toPos ?h-down) (at fromPos) (not (has-block))) :effect (and (has-block) (height toPos ?h-down))) Table 3: The comparison highlights the differences between Best-of-N sampling (BoN) and iVML in synthesizing action-level PDDL code. The red text marks where BoN@8 produces logically incorrect code, while the blue text shows how iVML detects these inaccuracies and applies the necessary corrections. the optimization process of iVML toward better optimality. Performance of weaker LLMs. From Figure 4, we observe that iVML with BoN-8 initialization can improve LLaMas performance in the NL2Domain task, which yields around 10 correct domains compared to zero in single-pass mode. However, the performance is far worse than Qwen2.5-Coder (7B) and Deepseek-Coder-Instruct-v1.5 (7B). We attribute this performance gap to LLaMas limited exposure to structured logical reasoning during pretraining, deficiency in its pretraining knowledge that test-time compute scaling methods (e.g., iVML and BoN) cannot effectively address."
        },
        {
            "title": "4.5 PDDL Problem Generation",
            "content": "Experiment settings. This section investigates the effectiveness of our approach while generalized to PDDL problem synthesis. In contrast to the PDDL domain, which outlines the general framework or environment defined for planning tasks, the PDDL problem defines specific instance of the planning task within that domain. This involves defining two main components (1) Initial state: the starting state of the world, defined by the predicates that are true initially, and (2) Goal state: The objective that the planner aims to achieve. We adopt the Planetarium [ZVL+24] benchmark, which evaluates LLMs capacity to 12 Figure 4: The performance of iVML on NL2Domain tasks across different initialization settings. Figure 5: The performance of iVML on Prob2Domain tasks across different initialization settings."
        },
        {
            "title": "Model",
            "content": "Gemma 1.1 IT 2B Gemma 1.1 IT 7B Mistral v0.3 Instruct 7B GPT-4o Ours (Qwen2.5-Coder-7B)"
        },
        {
            "title": "Setting",
            "content": "Zero-shot Fine-tuned Zero-shot Fine-tuned Zero-shot Fine-tuned Zero-shot BoN-16 Ours (Qwen2.5-Coder-7B) iVML-1-BoN-16 Success Rate (%) 0. 94.21 0.00 98.79 0.01 99.00 35. 99.24 99.60 Table 4: Performance comparison of different models on PDDL problem generation generate precise PDDL problems from natural language descriptions. These tasks are challenging due to the lack of planning background knowledge and the complex context described by the problem. The evaluation methods outlined in [ZVL+24] test LLMs in both zero-shot and fine-tuned settings. The baselines being evaluated include GPT-4, Gemma 1.1 IT models [TMH+24] with 2B and 7B parameters, as well as Mistral v0.3 Instruct (7B) [JSM+23]. Main results. The results are presented in Table 4. Planetarium fine-tuned Gemma and Mistral on training dataset containing 132,027 examples from the two-class PDDL problem code dataset, potentially raising overfitting concerns as Gemmas accuracy increased dramatically from near 0.0% to over 98.8%. Our method enhances the Qwen2.5-Coder (7B) model through test-time scaling techniques, achieving 99.24% correctness rate with BoN-16 sampling. This improves further to 99.60% when combining iVML-1 with BoN-16 for solution initialization. Comparison between SFT and iVML. The results in Table 4 provide comparison between supervised finetuning (SFT) and iVML. This reveals three key advantages of our method (listed as follows). (1) Preventing catastrophic forgetting: Unlike SFTs static alignment to fixed data distributions, iVML enforces structured reasoning priors through in-context learning, enabling adaptation to diverse problem constraints without catastrophic forgetting. (2) Refinement through in-context optimization: Building upon BoNs high-quality initialization, iVML performs in-context instance optimization to correct subtle errors, elevating correctness from 99.24% (BoN-16) to 99.60% (iVML-1-BoN-16). (3) Computational"
        },
        {
            "title": "Floortile Barman Tyreworld Grippers Termes Blockworld",
            "content": "LLM-as-Planner Methods Pass@1 GPT-4o self-critique Pass@8 Pass@ o1-mini self-critique Pass@8 Pass@1 o1-preview self-critique 0.0 10.0 13.3 5.3 5.3 0. 0.0 5.0 Pass@8 33.3 6.7 13. 33.3 33.3 33.3 33.3 13.3 6. 33.3 0.0 0.0 45.0 50.0 35. 70.0 33.3 35.0 85.0 Our Methods (PDDL as Abstraction) Qwen2.5-7B-Coder Qwen2.5-7B-Coder BoN-4 BoN-16 Qwen2.5-7B-Coder BoN-4-iVML-5 0.0 100. 0.0 Qwen2.5-7B-Coder BoN-16-iVML-5 100.0 0.0 100.0 100. 100.0 0.0 100.0 100.0 100.0 23. 33.3 45.0 57.1 61.9 61.9 38. 33.3 66.7 100.0 100.0 100.0 100. 4.8 0.0 10.0 23.8 23.8 52. 0.0 4.7 19.0 81.0 100.0 100. 100.0 4.8 14.2 23.8 38.1 47. 23.1 4.7 9.5 33.3 9.5 0. 71.4 81.0 Table 5: PDDL abstraction vs. LLM-as-Planner. The comparison uses plan accuracy as the evaluation metric. Our PDDL-based method uses the Fast Downward system [Hel06] for heuristic search and plan validation. efficiency: iVML requires significantly less compute than SFT while demonstrating strong performance on complex reasoning tasks, including those in long-tail domains."
        },
        {
            "title": "4.6 Comparison to LLM-as-Planner Methods",
            "content": "In the previous sections, we demonstrated that iVML enhances LLMs ability to generate high-quality PDDL-based world models. In this section, we compare our method, which utilizes synthesized PDDL domains as world models, with LLMs-as-a-Planner methods that use natural language for world modeling and planning. detailed description of the tested planning cases is provided in Appendix C. Experimental settings. LLMs often hallucinate for tasks such as generating feasible or optimal plans, understanding the planning problem, and strictly following the rules, particularly in complex planning problems (e.g., Termes and Barman) [WLB+24]. To investigate whether these hallucinations arise from limitations in prompt engineering, we provide LLMs with explicit instructions on the rules they must follow and require them to verify rule compliance at each step of the planning process. Additionally, we employ two strategies to reduce uncertainty in LLM-generated plans: (1) Introducing the Pass@8 metric to evaluate the probability that at least one of the top 8-generated plans is correct, and (2) Allowing LLMs to self-evaluate their plans and refine them based on these assessments. The baseline implementations of LLM-as-Planner methods are based on GPT-4o, o1-mini, and o1-preview, respectively. Discussion on LLM-as-Planner methods. The observations are as follows: (1) rule violation: Despite explicitly informing LLMs to examine rule violation at each step, the generated plans still contain elements that inherently violate the predefined rules. For example, in the step 3 and 4 of Figure 2, o1 incorrectly places the block from pos-0-1 to pos-1-2, mistakenly assuming that they are neighboring positions. This action violates the rule governing the placement of blocks. (2) incorrect state transition estimation: The LLMs fail to accurately estimate state transitions. For instance, after moving the block from pos-1-1 to pos-1-2, o1 incorrectly assumes that the heights of both positions are 0, reflecting an inability to track state changes correctly. (3) incorrect goal achievement estimation: o1 attempts to achieve the goal in manner that disregards all constraints and relies on flawed state estimations, which results in plan that 14 is not only incorrect but also violates the fundamental rules of the task. (4) incorrect self-evaluation: o1 fails to identify errors in its planning process, as it consistently assumes that its own responses are correct. This prevents it from correcting mistakes, further compounding the inaccuracies in its generated plans. PDDL abstraction vs. LLM-as-Planner. We present the numerical results in Table 5. For classical planning problems, using natural language as planning abstraction proves suboptimal. Even when equipped with self-critique capabilities, the o1-preview system achieves only 5.0 on Floortile, 6.7 on Barman, and 4.7 on Termes benchmarks. This limitation stems from natural languages inherent ambiguity and lack of formal precision required for precise planning representation. LLM-as-Planner methods do not perform planning; instead, they treat planning tasks as n-gram text completion problems, leading to plans that may lack feasibility, violate the constraints, or fail to accurately reflect the underlying problem structure. In contrast, our approach leverages PDDL representations to explicitly model state transitions. Through BoN-16-iVML-5 generation of high-quality world models combined with heuristic search algorithms (i.e., A), our method solves nearly all tested instances across Floortile, Barman, and Termes domains."
        },
        {
            "title": "5 Concluding Remarks and Current Limitations",
            "content": "Our work introduces test-time scaling framework for automated PDDL synthesis that integrates best-of-N sampling with instance verbalized machine learning. This approach demonstrates that effectively scaling test-time computation of open-source LLMs can outperform state-of-the-art closed-source LLM planners, including OpenAIs o1-mini.Our hybrid method employs two-phase optimization paradigm: (1) BoN initialization which generates diverse candidate solutions to explore critical regions of the search space, addressing the cold-start problem in formal language synthesis. (2) iVML refinement which iteratively improves the BoN initial solutions through self-critique and natural language feedback, resolving logical inconsistencies and syntactic errors. Leveraging BoNs stochastic search to initialize iVMLs refinement process, our method achieves faster convergence and higher-quality PDDL domains. The effectiveness of iVML in PDDL problem synthesis even surpasses models specifically fine-tuned for this task. The results show that our proposed test-time compute scaling approach can enhance LLMs formal reasoning and planning capabilities. By generating PDDL-based symbolic world models, we enable explicit model-based planning with classical search algorithms (e.g., A*), avoiding the error-prone state transitions inherent in direct LLM-as-planner approaches. Beyond PDDL synthesis, our work provides general framework for scaling up test-time compute of LLMs for formal language synthesis. The limitations of our work include: (1) challenges in semantic verification for autoformalization: Consistent with prior work in PDDL synthesis (e.g., [GVSK23, ZVL+24, VMO+24]), our evaluation relies on VAL [HL03] for syntax validation and plan verification. While VAL ensures syntactic correctness (e.g., predicate arity, type consistency) and plan executability (e.g., action preconditions and effects), it cannot detect semantic inconsistencies that violate domain intent or commonsense logic. This limitation parallels broader challenges in autoformalization, where even formal mathematical proof [ZHP21] struggles to verify semantic alignment between informal specifications and formal outputs through compiler checking. (2) simulation assumptions: Our evaluation relies on an idealized simulation environment with two key assumptions. First, actions execute perfectly, with no execution misalignment. Second, the state space is fully observable, with no sensor noise or occlusions. These idealized conditions differ significantly from real-world robotic manipulation scenarios."
        },
        {
            "title": "References",
            "content": "[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 11 15 [AS24] Forest Agostinelli and Misagh Soltani. Learning discrete world models for heuristic search. In Reinforcement Learning Conference, 2024. 4 [Bak19] Kyri Baker. Learning warm-start points for ac optimal power flow. In International Workshop on Machine Learning for Signal Processing, 2019. [BCP+16] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 5 [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 5 [CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 3 [CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 3, 11 [DJP+24] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [DLD+22] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. 5 [DMAM24] Nicola Dainese, Matteo Merler, Minttu Alakuijala, and Pekka Marttinen. Generating code world models with large language models guided by monte carlo tree search. arXiv preprint arXiv:2405.15383, 2024. 5 [DQY+23] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220235, 2023. 5 [FN71] Richard E. Fikes and Nils J. Nilsson. Strips: new approach to the application of theorem proving to problem solving. In IJCAI, 1971. 5, 21 [GVSK23] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. In NeurIPS, 2023. 1, 4, [Hel06] Malte Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191246, 2006. 14 [HGM+23] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. 4 [HL03] Richard Howey and Derek Long. Vals progress: The automatic validation tool for pddl2.1 used in the international planning competition. In ICAPS Workshop on \"The Competition: Impact, Organization, Evaluation, Benchmarks\", 2003. 8, 15 16 [HWAZ+22] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 5 [JSM+23] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 13 [Kam24] Subbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of Sciences, 1534(1):1518, 2024. 4 [LQF+24] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, et al. Parameter-efficient orthogonal finetuning via butterfly factorization. In ICLR, 2024. 5 [MAS+24] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. [PIL+23] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \"gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023. 5 [QLF+23] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard SchÃ¶lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In NeurIPS, 2023. 5 [QLF+25] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Xiao, Katherine Collins, Joshua Tenenbaum, Adrian Weller, Michael Black, and Bernhard SchÃ¶lkopf. Can large language models understand symbolic graphics programs? In ICLR, 2025. 3 [QLZ+24] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. 10 [RN16] Stuart Russell and Peter Norvig. Artificial intelligence: modern approach. Pearson, 2016. [SKN24] Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. Inference scaling flaws: The limits of llm resampling with imperfect verifiers. arXiv preprint arXiv:2411.17501, 2024. 3, 11 [SYC+24] Elad Sharony, Heng Yang, Tong Che, Marco Pavone, Shie Mannor, and Peter Karkus. Learning multiple initial solutions to optimization problems. arXiv preprint arXiv:2411.02158, 2024. 8 [TKE24] Hao Tang, Darren Key, and Kevin Ellis. Worldcoder, model-based llm agent: Building world models by writing code and interacting with the environment. arXiv preprint arXiv:2402.12275, 2024. 5 [TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 13 [VMO+24] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In NeurIPS, 2024. 3, 15 [WLB+24] Kevin Wang, Junbo Li, Neel Bhatt, Yihan Xi, Qiang Liu, Ufuk Topcu, and Zhangyang Wang. On the planning abilities of openais o1 models: Feasibility, optimality, and generalizability. arXiv preprint arXiv:2409.19924, 2024. 3, 14 [WS23] Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829, 2023. 9 [WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 5 [XBSL24] Tim Xiao, Robert Bamler, Bernhard SchÃ¶lkopf, and Weiyang Liu. Verbalized machine learning: Revisiting machine learning with language models. arXiv preprint arXiv:2406.04344, 2024. 2, 3, 5, [YBB+24] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496, 2024. 5 [YCL+24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 8 [YWL+24] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In ICLR, 2024. 5 [YXWK24] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. [YYH+24] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 3, 8 [ZCS+23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. 3 [ZCY+24] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024. 3, 10 [ZHB+24] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. 3 [ZHP21] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. 15 [ZMH+22] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. 5 [ZVL+24] Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael Littman, and Stephen Bach. Planetarium: rigorous benchmark for translating text to structured planning languages. arXiv preprint arXiv:2407.03321, 2024. 4, 12, 13,"
        },
        {
            "title": "C Tasks in Case Study",
            "content": "D Prompt Template for Chain-of-Thought"
        },
        {
            "title": "E Generated Domains",
            "content": "F Prompts for LLM-as-Planner Methods State-based graph for Termes"
        },
        {
            "title": "I Human in the Loop Experiment",
            "content": "21 21 22 23 24 30 31"
        },
        {
            "title": "A Planning Problem Formulation",
            "content": "The classical planning problem [FN71] in artificial intelligence involves finding sequence of actions that transition an agent from an initial state to desired goal state within deterministic and fully observable environment. It is formalized as tuple S, A, T, s0, G, where: is the set of all possible states; is the set of all possible actions; : is the state transition function, specifying the outcome state resulting from applying an action in state; s0 is the initial state; is the goal condition, predicate over states. The objective is to find sequence of actions a1, a2, . . . , an such that applying these actions successively transitions the system from s0 to state sn satisfying the goal condition G: sn = (sn1, an) = (T (. . . (T (s0, a1), a2), . . . , an1), an) with sn = Previous works that adopt LLMs as planners estimate state transitions implicitly within language latent spaces, lacking explicit representations of the state space required for classical planning. This implicit representation can make it challenging to ensure consistency, validity, and completeness in the planning process. In contrast, our work leverages LLMs to generate explicit representations of the state space by creating PDDL domains. We utilize the generative capabilities of LLMs to produce formal PDDL models from high-level descriptions of the planning tasks. This approach bridges the gap between natural language specifications and formal planning models. By generating PDDL domains using LLMs, we obtain: 1. explicit definitions of the set of states through predicates and objects, 2. formal specifications of actions A, including their preconditions and effects, 3. deterministic state transition function derived from the action definitions, 4. abilities to cooperate with clearly defined initial state s0 and goal condition in PDDL syntax. Thus the new objective under this background is: Given high-level descriptions of planning tasks, our objective is to leverage LLMs to create PDDL domains that can represent state transitions explicitly. By generating these explicit representations, we enable classical planning algorithms to efficiently search for plans using the defined state transitions during the planning process."
        },
        {
            "title": "B STRIPS Formulation",
            "content": "The states are expressed through set of predicates that describe the properties of objects in the environment. Each predicate represents relationship or characteristic, such as on(A, B), which indicates that object is on top of object B. state can be represented as: S0 = {on(A, B), clear(C)} This representation includes relationships that capture the positions and statuses of the objects within the environment. Actions in PDDL are tightly bound to the representation of states through the use of preconditions and effects. Each action is defined by specifying what must be true in the current state for the action to be applicable (preconditions), as well as what changes in the state when the action is performed (effects). For example, consider an action move(A, B, C), indicating that move the object from to C. The preconditions and effects could be defined as follows: Preconditions: Pre(move(A, B, C)) = {on(A, B), clear(C)}and Effects: Eff(move(A, B, C)) = {on(A, C), clear(B)}. They indicate that for the action move(A, B, C) to be executed, object must be positioned on B, and must be clear of any objects. And after moving from to C, is now on C, and is clear. The transition can be formulated as: (S, A) S. State transitions occur according to defined sequence of 21 actions that an agent may take, leading to new configurations of the world. For instance if the action move(A, B, C) on S0, the transition can be represented as: S1 = (S0, move(A, B, C)) {on(A, C), clear(B)} By repeatedly exploring all possible actions, we can form state transition graph consisting of nodes (states) and directed edges (actions) that connect these nodes based on the actions that lead to different states. We continue this process by expanding the graph from the newly added nodes until the goal state is reached and added to the graph, or there are no more actions left to explore."
        },
        {
            "title": "C Tasks in Case Study",
            "content": "Barman. The main goal of the Barman domain is to simulate the task of bartender who prepares and serves cocktails by manipulating ingredients, tools, and glassware within bar setting. In this scenario, the agent is tasked with creating specific cocktail by following series of actions that involve: 1. Identifying and dispensing the necessary ingredients required for the cocktail from the available dispensers. 2. Using bar tools such as shakers and shot glasses effectively, ensuring they are clean and suitable for use. 3. Coordinating the use of both hands to pick up and handle objects while ensuring that hands are free when needed and that objects are properly placed on surfaces like the bar counter when not in use. 4. Adhering to the proper sequence of steps for cocktail preparation, which includes dispensing ingredients into the shaker, mixing them, and then pouring the mixture into shot glass. 5. Maintaining the correct state of all objects involved, such as keeping the shaker and glasses clean and empty before use, and updating their states appropriately as actions are performed. 6. Successfully preparing the cocktail and having it contained in the shot glass, thereby fulfilling the goal of serving the drink as intended. Gripper. Gripper problem is designed to test the agents ability to manage resources and plan actions in scenario involving manipulating multiple objects across different locations. The agent, represented by one or more robots, must strategically perform the following tasks: Pick Up Balls: The agent must use its available grippers to pick up the balls from their initial locations. This requires careful hand management to ensure grippers are free and available when needed. Transport Balls: The robot needs to navigate between rooms to move balls to their specified target locations efficiently. This involves planning the correct sequence of moves and ensuring that the robot is in the correct room with the appropriate objects. Drop Balls: The agent must release the balls in the designated rooms. This requires ensuring that the robots grippers are properly aligned and that the release actions are performed at the correct time. Manage Resources: Throughout the task, the agent must effectively manage both its grip and position within the environment, making sure that it follows constraints such as carrying capacity and room access. Tyreworld. The main goal of the Tyreworld problem is to simulate the challenges of vehicle maintenance and tire management in scenario where vehicle may suffer random tire failures. The primary objectives include: 1. Replacing Flat Tires: The agent must effectively replace flat tires with intact ones on the vehicles hubs. 2. Inflating Tires: The intact tires must be inflated before being mounted onto the vehicle. 3. Ensuring Secure Fastening: After replacing and inflating the tires, the nuts on the hubs must be securely tightened to ensure the wheels are safely attached. 3. Resource Management: The agent must manage limited resources (e.g., spare tires, tools like jacks and wrenches) strategically to minimize the risk of failure or being stranded. 4. Navigating Uncertainty: The agent must effectively plan actions while accounting for the possibility of tire failures and other uncertainties in the environment. 5. Reaching the Destination: Ultimately, the goal is to ensure the vehicle is properly equipped with intact, inflated tires, allowing it to continue its journey successfully. 6. The Tyreworld problem serves to test an agents planning, resource management, and adaptability in unpredictable scenarios related to vehicle maintenance. Floor-tile. The main goal of Floor-tile is to enable the robot to navigate the environment, manage its colors, and paint the tiles according to specific requirements. The robot must efficiently utilize its 22 movements and actions to achieve its painting objectives while adhering to the constraints of tile occupancy and color availability. In Floor-tile, three object types are defined: robot, tile, and color. Initially, the robot is located on specific tile while holding color. It can move up, down, right, or left with different costs: moving up costs 3, while moving down, right, or left costs 1. The robot can paint tile above or below for cost of 2, and changing its color incurs cost of 5. Termes. In Termes, robot operates in an environment to manage and manipulate blocks at different positions. The robot can perform several actions, including moving between adjacent positions, placing blocks onto stacks, removing blocks from stacks, and creating or destroying blocks at designated depot locations. Each position on the grid has specific height, and the robot must respect these height constraints when moving or manipulating blocks. The robot can only carry one block at time, and it must be at the same height level to move horizontally or at height difference of one to move vertically. The goal is to efficiently use these capabilities to achieve specific block arrangements or configurations within the environment, adhering to the constraints of adjacency, height, and block availability. Prompt Template for Chain-of-Thought"
        },
        {
            "title": "Prompts for Our Methods",
            "content": "You will be given natural language description of planning problem. Your task is to translate this description into PDDL domain code. This includes defining predicates and actions based on the information provided. Information about the AI agent will be provided in the natural language description. Note that individual conditions in preconditions and effects should be listed separately. For example, object1 is washed and heated should be considered as two separate conditions object1 is washed and object1 is heated. Also, in PDDL, two predicates cannot have the same name even if they have different parameters. Each predicate in PDDL must have unique name, and its parameters must be explicitly defined in the predicate definition. It is recommended to define predicate names in an intuitive and readable way. Remember: Ignore the information that you think is not helpful for the planning task. You are only responsible for domain generation. Before you generate the concrete domain code, you should first generate natural language thought about the meaning of each variable, and the step-by-step explaination of the domain code. Even if didnt provide the exact name of the predicates and actions, you should generate them based on the information provided in the natural language description. Template is: ### Thought: predicates1: the name of predicate1, explanation of predictate1 ... predicaten: the name of predicaten, explanation of predictaten action1: the name of action1, explanation of action ... actionn: the name of action, explanation of action <thought> ### Domain: pddl The concrete pddl code for domain.pddl Now its your time to generate the solution, you have to follow the format provided above. NL_Description: Natural language description of the planning domain"
        },
        {
            "title": "Barman",
            "content": "Domain. ( i ( domain barman ) : i ( : u m s ( : e hand e v g s s h ( : d t h r t e ) : i ) t e e i e n c i e e ( a ? t e ) ( d ? hand ? t e ) ( handempty ? hand ) ( empty ? t e ) ( t s ? t e ? e e ) ( a ? t e ) ( d ? t e ? e e ) ( p e ? p e ? r e ) ( k empty e ? k ? e ) . . . . . . ( : i r : a e : c i n ( and ( a ? ) ( a ? ) ) : e ( and ( t ( ? hand ? t e ) ( handempty ? ) ) ( ( handempty ? ) ) ( d ? ? ) ) ) . . . . . . . . . . . . . . . . . . ( : i e ( : i i s ( : i e l o ( : i emptys ( : i l s ( : i pours toc n a ( : i pours touseds e ( : i emptys e ( : i l s e ( : i h . . . . . . ( : i pours e tos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ) . . . . . . . . . . . . Problem. ( i ( b p ) ( : domain barman ) ( : e s e 1 k n d t 2 r e 3 r e i n 1 p e 2 p e 3 p e r t hand t 1 t 2 t 3 t 4 t r e 1 k l 1 k l 2 k l 3 k l 0 1 2 e f ) ) ( : t ( a s e 1 ) . . . . . . ( a s 1 ) . . . . . . ( a a 1 ) ( empty k 1 ) . . . . . . ( k l r 1 k l 1 r e 1 ) . . . . . . ( : l ( and ( t s t 1 k l 1 ) ( t s t 2 k l 3 ) ( t s t 3 k l 2 ) Plan. (grasp left shot4) (fill-shot shot4 ingredient2 left right dispenser2) (pour-shot-to-clean-shaker shot4 ingredient2 shaker1 left l0 l1) (clean-shot shot4 ingredient2 left right) (fill-shot shot4 ingredient1 left right dispenser1) (pour-shot-to-used-shaker shot4 ingredient1 shaker1 left l1 l2) (refill-shot shot4 ingredient1 left right dispenser1) (leave left shot4) (grasp right shaker1) (shake cocktail3 ingredient1 ingredient2 shaker1 right left) (pour-shaker-to-shot cocktail3 shot2 right shaker1 l2 l1) (empty-shaker right shaker1 cocktail3 l1 l0) (clean-shaker right left shaker1) (leave right shaker1) (grasp left shot4) (pour-shot-to-clean-shaker shot4 ingredient1 shaker1 left l0 l1) (clean-shot shot4 ingredient1 left right) (fill-shot shot4 ingredient3 left right dispenser3) (pour-shot-to-used-shaker shot4 ingredient3 shaker1 left l1 l2) (refill-shot shot4 ingredient3 left right dispenser3) (leave left shot4) (grasp right shaker1) (shake cocktail1 ingredient1 ingredient3 shaker1 right left) (pour-shaker-to-shot cocktail1 shot1 right shaker1 l2 l1) (empty-shaker right shaker1 cocktail1 l1 l0) (clean-shaker right left shaker1) (leave right shaker1) (grasp right shot4) (pour-shot-to-clean-shaker shot4 ingredient3 shaker1 right l0 l1) (clean-shot shot4 ingredient3 right left) (fill-shot shot4 ingredient2 right left dispenser2) (grasp left shaker1) (pour-shot-to-used-shaker shot4 ingredient2 shaker1 right l1 l2) (leave right shot4) (shake cocktail2 ingredient2 ingredient3 shaker1 left right) (pour-shaker-to-shot cocktail2 shot3 left shaker1 l2 l1) ; cost = 36 (unit cost)"
        },
        {
            "title": "BlockWorld",
            "content": "Domain. ( i ( domain c o ) ( : u m s : i : a y ) ( : d t ( a ? ) ( ont e ? ) ( armempty ) ( d ? ) ( on ? ? ) ) ( : i i p ( ? ob ) : a e : c i n ( and ( a ? ob ) : e ( ( and ( d ? ob ) ( armempty ) ) ) ) ( ( ont e ? ob ) ( a ? ob ) ) ( armempty ) ) ( ( ont e ? ob ) ) ( : i putdown : a e : c i n ( and ( d ? ob ) ) : e ( and ( a ? ob ) ( ( d ? ob ) ) ) ) ( ? ob ) ( armempty ) ( : i t : a e : c i n ( and ( a ? e ) : e ( ( and ( armempty ) ( d ? ob ) ) ) ) ( ? ob ? e ) ( a ? ob ) ( : i n c : a e : c i n ( and ( on ? ob ? e ) : e ? ob ) ) ( and ( d ? ob ) ( armempty ) ) ) ( ? ob ? e ) ( ) ) ( a ? ob ) ( ont e ? ob ) ( d ? ob ) ) ( on ? ob ? e ) ( ( a ? e ) ) ( a ? ob ) ( armempty ) ) ( ( on ? ob ? e ) ) ( ( a Problem. ( i ( b BWrand 12) ( : domain c o ) ( : e b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b11 b12 ) ( : t ( armempty ) ( ont e b1 ) ( on b2 b5 ) ( on b3 b8 ) ( on b4 b12 ) ( on b5 b7 ) ( on b6 b1 ) ( on b7 b10 ) ( ont e b8 ) ( ont e b9 ) ( on b10 b11 ) ( ont e b11 ) ( on b12 b9 ) ( a b2 ) ( a b3 ) ( a b4 ) ( a b6 ) ) ( : l ( and ( on b5 b10 ) ( on b6 b12 ) ( on b7 b4 ) ( on b8 b3 ) ( on b9 b2 ) ( on b10 b8 ) ( on b11 b7 ) ( on b12 b11 ) ) ) ) Plan. (unstack b3 b8) (putdown b3) (pickup b8) (stack b8 b3) (unstack b2 b5) (putdown b2) (unstack b4 b12) (putdown b4) (unstack b5 b7) (stack b5 b2) (unstack b7 b10) (stack b7 b4) (unstack b10 b11) (stack b10 b8) (pickup b11) (stack b11 b7) (unstack b12 b9) (stack b12 b11) (unstack b5 b2) (stack b5 b10) (unstack b6 b1) (stack b6 b12) (pickup b9) (stack b9 b2) ; cost = 24 (unit cost)"
        },
        {
            "title": "Termes",
            "content": "Domain. ( i ( domain m ) ( : u m s ( : e : i : a e e d o ) numb e o i b t ) ( : d t ( g ? i n ? numb ) ( ? i n ) ( hasb k ) (SUCC ? n1 numb ? n2 numb ) (NEIGHBOR ? p1 i n ? p2 i n ) ( IS DEPOT ? i n ) ) ( : i move ( ? from i n ? o i ? numb ) : a e : c i n ( and ( ? from ) ( NEIGHBOR ? from ? ) ( g ? from ? ) ( g ? ? ) ) ) : e ( : i moveup ( ? from ) ) ( ? ) ( and ( ) ( ? from i n ? hfrom numb ? o i ? numb ) : a e : c i n ( and ( ? from ) ( NEIGHBOR ? from ? ) ( g ? from ? hfrom ) ( g ? ? ) ( SUCC ? ? hfrom ) ) : e ( ? from ) ) ( ? ) ) ) ( and ( ( : i movedown . . . ( : i l b k . . . ( : i removeb k . . . ( : i r e o . . . ( : i e o o . . . ) Problem. ( i ( b t e 0003800364 x3x3random_towers_4x3_3_1_3 ) ( : domain m ) m 0003800364 x3x3random_towers_4x3_3_1_3 ; t ; 0 ; 0 ; ; 0 ; l 0 ; 0 ; ; 0 ; Maximal g : 3 ( : e s e : 0 R0D 0 0 0 0 0 0 0 t : 0 0 3 0 0 0 0 0 0 n0 numb . . . . . . pos 00 i n . . . . . . ) ( : t ( g pos 00 n0 ) . . . . . . ( pos 20) (SUCC n1 n0 ) . . . . . . (NEIGHBOR pos 00 pos 1 0 ) . . . . . . ( IS DEPOT pos 20) ) ( : l ( and ( g pos 00 n0 ) . . . . . . ( ( hasb k ) ) ) ) ) Plan. (unstack b3 b8) (putdown b3) (pickup b8) (stack b8 b3) (unstack b2 b5) (putdown b2) (unstack b4 b12) (putdown b4) (unstack b5 b7) (stack b5 b2) (unstack b7 b10) (stack b7 b4) (unstack b10 b11) (stack b10 b8) (pickup b11) (stack b11 b7) (unstack b12 b9) (stack b12 b11) (unstack b5 b2) (stack b5 b10) (unstack b6 b1) (stack b6 b12) (pickup b9) (stack b9 b2) ; cost = 24 (unit cost) 26 Floor-tile Domain. ( i ( domain o e ) ( : u m s ( : e r t : i : i c s ) o e ) e ( : d t ( o ? o ? e ) ( up ? e ? e ) ( down ? e ? e ) ( h ? e ? e ) ( t ? e ? e ) ( a ? e ) ( n ? e ? o ) ( o s ? o ? o ) ( i l l ? o ) ( e l ? o ) ) ( : c n ( : i changec r ( a s ) ) : a e : c i n ( and ( o s ? ? ) ( o s ? ? ) ) : e ( and ( ( ? o ? o ? c2 o ) ( i l l ? c2 ) ) ( o s ? ? c2 ) ( r e ( a s ) 5 ) ) ) ( : i a up : a e : c i n ( and ( o s ? ? ) : e ( a ? ) ) ( and ( ( ? o ? e ? e ? o ) ( n ? ? ) ( o ? ? ) ( up ? ? ) ( r e ( a s ) 2 ) ) ( a ? ) ) ) ( : i a down . . . ( : i up . . . ( : i down . . . ( : i i ( : i e . . . . . . Problem. ( i ( b p03 432) ( : domain o e ) ( : e t _ 0 1 e _ 0 2 e _ 0 3 . . . . . . e _ 4 1 e _ 4 2 e _ 4 3 e o 1 o 2 o i l o ) ( : t o 1 e _ 2 3) o 1 t ) o 2 e _ 1 1) o 2 c ) (= ( a s ) 0 ) ( o ( o s ( o ( o s ( i l l h ) ( i l l l ) ( a . . . . . . e _ 0 1) ( up e _ 1 1 e _ 0 1) ( down e _ 0 1 e _ 1 1) ( h ( t e _ 0 2 e _ 0 1) e _ 0 1 e _ 0 2) . . . . . . . . . . . . . . . . . . . . . . . . ) ( : l ( and ( n t _ 1 1 t ) ( n t _ 1 2 c ) . . . . . . ) ) ( : r i i ( a s ) ) ) Plan. (up robot1 tile_2-3 tile_3-3) (left robot1 tile_3-3 tile_3-2) (paint-up robot1 tile_4-2 tile_3-2 white) (up robot2 tile_1-1 tile_2-1) (down robot1 tile_3-2 tile_2-2) (up robot2 tile_2-1 tile_3-1) ...... ; cost = 54 (general cost)"
        },
        {
            "title": "Tyreworld",
            "content": "Domain. ( i ( domain e l ) ( : e j e o h n b n n hub e ) ( : d t ( open ? ) ( s ? ) ( e ? ) ( ? ? ) ( s ? ? ) ( h ? ? ) ( o d ? ) ( ong n ? ) . . . . . . ( : i open : a e : c i n ( and ( o d ? ) : e ( ? t e ) ( and ( open ? ) ( ( s ? ) ) ( s ? ) ) ) ) ( : i l : a e : c i n ( open ? ) : e ( and ( s ? ) ( ? t e ) ( ( open ? ) ) ) ) ( : i e : a e : c i n ( and ( ? ? ) ( and ( e ? ) : e . . . . . . ( ( ? ? t e ) ( open ? ) ) ( ? ? ) ) ) ) ( : i putaway ( : i o n . . . . . . ( : i i e . . . . . . ( : i a up . . . . . . ( : i a down . . . . . . ( : i undo . . . . . . ( : i doup . . . . . . ( : i removew l ( : i putonw l ( : i n t . . . . . . . . . . . . . . . . . . Problem. ( i ( b t w d 1) ( : domain e l ) ( : e wrench k pump l t t e 1 w1 e ) e hub1 hub s 1 ( : t ( a o ) ( pump t ) ( wrench t ) ( o d t ) ( s o ) hub1 ) ( a 1 ) ( 1 t ) ( not l d 1 ) ( on w1 ( ong n e hub1 ) ( h t 1 hub1 ) ( t d hub1 ) ) ( : l ( and ( on 1 hub1 ) ( wre nch t ) ( l d 1 ) ( h t 1 hub1 ) ( w1 t ) ( a o ) ( pump t ) ( s o ) ) ) ) Plan. (open boot) (fetch r1 boot) (fetch wrench boot) (fetch jack boot) (loosen nuts1 the-hub1) (jack-up the-hub1) (undo nuts1 the-hub1) (remove-wheel w1 the-hub1) (put-away w1 boot) (put-on-wheel r1 the-hub1) (do-up nuts1 the-hub1) (jack-down the-hub1) (put-away jack boot) (tighten nuts1 the-hub1) (put-away wrench boot) (fetch pump boot) (inflate r1) (put-away pump boot) (close boot) ; cost = 19 (unit cost) 28 Prompts for LLM-as-Planner Methods Prompts for o1 on Termes Problem Description. You control robot that can take the following actions to build complex structures. Move from position to another. The new position and the old position must be at the same height. Move up from position to another, and the height at the new position is one block higher than the old position. Move down from position to another, and the height at the new position is one block lower than the old position. Place block at neighboring position from the robots current position. The robot must have block. The current height at the robots position and the blocks position must be the same. block cannot be placed at the depot. The height at the blocks position will be one block higher than the current height. Remove block at neighboring position from the robots current position. The robot must not have block. block cannot be removed from the depot. The current height at the robots position must be the same as the new height at the blocks position. The new height at the blocks position will be one block lower than the current height. Create block at the depot. The robot will have the block. Destroy block at the depot. The robot must have block. Now consider planning problem. The problem description is: The robot is on grid with 4 rows and 3 columns. pos-0-0 pos-0-1 pos-0-2 pos-1-0 pos-1-1 pos-1-2 pos-2-0 pos-2-1 pos-2-2 pos-3-0 pos-3-1 pos-3-2 The robot is at pos-2-0. The depot for new blocks is at pos-2-0. The maximum height of blocks is 3. Your goal is to build blocks so that the height at pos-1-2 is 3. Rule: You cannot have an unplaced block at the end. Examine whether you follow the rule at each step! Can you provide an optimal plan, in the way of sequence of behaviors, to solve the problem? And what is the final optimal cost? 29 State-based graph for Termes Figure 6 shows the planning graph that contains explicit state transition during planning for Termes. Figure 6: The planning graph for Termes"
        },
        {
            "title": "Prompts for Our Methods on Termes",
            "content": "You will be given natural language description of planning problem. Your task is to translate this description into PDDL domain code. This includes defining predicates and actions based on the information provided. Information about the AI agent will be provided in the natural language description. Note that individual conditions in preconditions and effects should be listed separately. For example, object1 is washed and heated should be considered as two separate conditions object1 is washed and object1 is heated. Also, in PDDL, two predicates cannot have the same name even if they have different parameters. Each predicate in PDDL must have unique name, and its parameters must be explicitly defined in the predicate definition. It is recommended to define predicate names in an intuitive and readable way. Remember: Ignore the information that you think is not helpful for the planning task. You are only responsible for domain generation. Before you generate the concrete domain code, you should first generate natural language thought about the meaning of each variable, and the step-by-step explaination of the domain code. Even if didnt provide the exact name of the predicates and actions, you should generate them based on the information provided in the natural language description. Template is: ### Thought: predicates1: the name of predicate1, explanation of predictate1 ... predicaten: the name of predicaten, explanation of predictaten action1: the name of action1, explanation of action1 ... actionn: the name of action, explanation of actionn <thought> ### Domain: pddl The concrete pddl code for domain.pddl Now its your time to generate the solution, you have to follow the format provided above. NL_Description: You control robot that can take the following actions to build complex structures. Move from position to another. The new position and the old position must be at the same height. pddl action name: move Move up from position to another, and the height at the new position is one block higher than the old position. pddl action name: move-up Move down from position to another, and the height at the new position is one block lower than the old position. pddl action name: move-down Place block at neighboring position from the robots current position. The robot must have block. The current height at the robots position and the blocks position must be the same. block cannot be placed at the depot. The height at the blocks position will be one block higher than the current height. pddl action name: place-block Remove block at neighboring position from the robots current position. The robot must not have block. block cannot be removed from the depot. The current height at the robots position must be the same as the new height at the blocks position. The new height at the blocks position will be one block lower than the current height. pddl action name: remove-block Create block at the depot. The robot will have the block. pddl action name: create-block Destroy block at the depot. The robot must have block. pddl action name: destroy-block An example problem PDDL file to the domain is: pddl (define (problem prob) (:domain termes) ; Initial state: ; 0 0 R0D ; 0 0 0 ; 0 0 0 ; Goal state: ; 0 0 0 ; 0 1 0 ; 0 0 0 ; Maximal height: 1 (:objects n0 - numb n1 - numb pos-0-0 - position pos-0-1 - position pos-0-2 - position pos-1-0 - position pos-1-1 - position pos-1-2 - position pos-2-0 - position pos-2-1 - position pos-2-2 - position ) (:init (height pos-0-0 n0) (height pos-0-1 n0) (height pos-0-2 n0) (height pos-1-0 n0) (height pos-1-1 n0) (height pos-1-2 n0) (height pos-2-0 n0) (height pos-2-1 n0) (height pos-2-2 n0) (at pos-2-0) (SUCC n1 n0) (NEIGHBOR pos-0-0 pos-1-0) (NEIGHBOR pos-0-0 pos-0-1) (NEIGHBOR pos-0-1 pos-1-1) (NEIGHBOR pos-0-1 pos-0-0) (NEIGHBOR pos-0-1 pos-0-2) (NEIGHBOR pos-0-2 pos-1-2) (NEIGHBOR pos-0-2 pos-0-1) (NEIGHBOR pos1-0 pos-0-0) (NEIGHBOR pos-1-0 pos-2-0) (NEIGHBOR pos-1-0 pos-1-1) (NEIGHBOR pos-1-1 pos-0-1) (NEIGHBOR pos-1-1 pos-2-1) (NEIGHBOR pos-1-1 pos-1-0) (NEIGHBOR pos-1-1 pos-1-2) (NEIGHBOR pos-1-2 pos-0-2) (NEIGHBOR pos-1-2 pos2-2) (NEIGHBOR pos-1-2 pos-1-1) (NEIGHBOR pos-2-0 pos-1-0) (NEIGHBOR pos-2-0 pos-2-1) (NEIGHBOR pos-2-1 pos-1-1) (NEIGHBOR pos-2-1 pos-2-0) (NEIGHBOR pos-2-1 pos-2-2) (NEIGHBOR pos-2-2 pos-1-2) (NEIGHBOR pos-2-2 pos-2-1) (IS-DEPOT pos-2-0) ) (:goal (and (height pos-0-0 n0) (height pos-0-1 n0) (height pos-0-2 n0) (height pos-1-0 n0) (height pos-1-1 n1) (height pos-1-2 n0) (height pos-2-0 n0) (height pos-2-1 n0) (height pos-2-2 n0) (not (has-block)) ) ) )"
        },
        {
            "title": "I Human in the Loop Experiment",
            "content": "We conducted human-in-the-loop experiments to refine the process of writing PDDL (Planning Domain Definition Language) domain code with the interaction between artificial intelligence and humans. The experiment involved graduate students majoring in AI and robotics, focusing on evaluating the effectiveness of human-AI collaboration in generating accurate and semantically meaningful PDDL domain files that strictly adhere to given specifications. The planning domain selected for this study was Termes, which necessitated the translation of robot actions into PDDL. These actions included horizontal and vertical movements, block placement and removal, and depot management within simulated environment. In the initial phase, participants interacted directly with an AI agent by providing prompts based on descriptions of robot actions. While the AI-generated PDDL code passed basic validation, it often lacked proper definition of action preconditions or the accurate use of predicates, such as the SUCC predicate, which denotes an ordered relationship between items. In response to these limitations, the students refined their prompting strategy by incorporating more structured instructions and examples, resulting in improved outcomes. Simultaneously, students manually coded PDDL domain files, utilizing the AI agent to ensure grammatical correctness. This approach facilitated the creation of logically comprehensive domain files, though several iterations were still required to achieve successful validation. Through this iterative process, students provided critical insights into the current strengths and weaknesses of AI in comprehending complex logical structures and semantic nuances within specialized domains like PDDL. Their findings underscored the challenges associated with writing precise PDDL code and emphasized the need for an automated pipeline to facilitate PDDL domain synthesis."
        }
    ],
    "affiliations": [
        "Environmental Systems Research Institute, Inc.",
        "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
        "SEED, Bytedance",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}