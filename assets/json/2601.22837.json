{
    "paper_title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
    "authors": [
        "Bin Wu",
        "Mengqi Huang",
        "Weinan Jia",
        "Zhendong Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 1 7 3 8 2 2 . 1 0 6 2 : r NativeTok: Native Visual Tokenization for Improved Image Generation Bin Wu Mengqi Huang Weinan Jia Zhendong Mao {lilimomotion,jiawn}@mail.ustc.edu.cn, {zdmao,huangmq}@ustc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "VQ-based image generation typically follows two-stage pipeline: tokenizer encodes images into discrete tokens, and generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) Meta Image Transformer (MIT) for latent image modeling, and (2) Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates single token conditioned on prior tokens and latent features. We further design Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok. Project Page: https://github.com/wangbei1/Nativetok"
        },
        {
            "title": "Introduction",
            "content": "Over the past few years, the vision community has witnessed remarkable progress in deep generative models, such as diffusion models[14, 18, 25, 26, 28] and autoregressive models[5, 10, 12, 16, 23, 30, 32, 36], elevating image generation quality to unprecedented levels. Inspired by the success of large language models (LLMs), large visual models have recently attracted increasing research interest, primarily due to their compatibility with established innovations from LLMs (e.g., scaling laws) and their potential to unify language and vision towards general artificial intelligence (AGI). Different from the natural tokenization for language such as BPE (Byte Pair Encoding)[3, 29], the visual tokenization, i.e., converting images into discrete tokens analogous to those used in language models, remains longstanding challenge for VQ-Based large visual models. Most modern large visual models follow two-stage paradigm, i.e., (1) the first tokenization stage learns an image tokenizer by reconstructing images, converting high-dimensional pixels into compressed discrete visual tokens, and (2) the second generation stage trains generative model to model the token distribution via next-token prediction. Recent works have focused on designing improved image tokenizers with higher compression ratios to accelerate * Corresponding author. 1 Figure 1 Illustration of our motivation. (a) Existing disordered tokenization overlooks the essential requirement of relational modeling in the generation stage, as it fails to introduce any token dependency constraints during tokenization, resulting in gap between the two stages of image generation. (b) Our approach proposes native visual tokenization, which considers not only reconstruction quality but also imposes relational constraints during tokenization, thereby coupling the two stages of image generation. generation process, or higher reconstruction quality to enhance generation quality. For example, DQVAE [19] introduces more compact tokenizer by dynamically allocating varying numbers of tokens to image regions based on their information density. VAR [31] proposes multi-scale tokenizer combined with coarse-to-fine \"next-scale prediction\" generation strategy. TiTok [37] significantly accelerates generation by compressing images into shorter one-dimensional serialized representations. In summary, existing methods primarily focus on first independently designing better tokenizer, and then training generative model to capture the distribution of the resulting visual tokens. However, we argue that existing methods overlook the intrinsic dependency between the tokenization and generation stages, optimizing each stage with separate objectives. This leads to fundamental misalignment between the disordered tokenization outputs and the structured dependency modeling required during generation. Specifically, as illustrated in Fig.1(a), the first stage typically relies on reconstruction loss for supervision during tokenization, without imposing constraints on the intrinsic relationships among tokens, while the second stage essentially models the intrinsic distributional relationships among the tokens learned in the first stage. This misalignment prompts critical inquiry: How can generative model accurately learn visual token distributions if the tokens themselves are inherently disordered in nature? As result, the generation stage can only learn biased and incomplete distribution from these disordered visual tokens, leading to sub-optimal generative performance. Moreover, persistent gap exists in current two-stage generation paradigm, i.e., better tokenization performance in the first stage does not necessarily result in improved generation in the second stage, as it may lead to more complex token interrelationships that further violates the autoregressive principle. To address the above challenge, we introduce the concept of native visual tokenization, which tokenizes images in native visual order that inherently aligns with the subsequent generation process, leading to more effective and accurate visual token modeling, as illustrated in Fig.1(b). We argue that visual information in an image also follows global causal order. Analogous to human perception, when observing an image, we tend to first recognize its primary structural components, followed by finer details such as textures. This observation suggests that the process of visual understanding is inherently autoregressive in nature. Built upon this concept, we propose NativeTok, novel visual tokenization framework that disentangles visual context modeling from visual dependency modeling, thereby enabling the joint optimization of image reconstruction quality and token ordering without conflict. As result, NativeTok is capable of producing native visually ordered 2 Figure 2 The overview of our NativeTok framework. (a) In Meta Image Transformer (MIT), the image information is initially modeled by Pixel Transformer, then compresses the image into the latent space. During the subsequent generation process, the latent space information of the image remains locked. (b) During the token sequence generation process, we define Mixture of Causal Expert Transformer (MoCET). The ith expert transformer block is responsible for generating the ith token. In each generation step, we concatenate the locked latent space image information, all previously generated tokens, and the current mask token, and feed them into the corresponding expert transformer. Once token is generated, it remains fixed. (c) Once all tokens are generated, they are fed into the decoder to reconstruct the image. token sequence directly during the tokenization stage. Specifically, NativeTok comprises three components: (1) the Meta Image Transformer (MIT), which models image patch features with high-dimension for effective visual context modeling, followed by dimension switcher that compresses the context output for subsequent efficient visual dependency modeling. (2) The Mixture of Causal Expert Transformer (MoCET) features novel ensemble of lightweight expert transformer blocks, each tailored to specific token position. This position-specific specialization allows MoCET to model causal dependencies with greater precision, yielding an ordered token representation. (3) Finally, the ordered token representation is quantized into discrete tokens and subsequently decoded by transformer-based decoder. Moreover, we further propose novel Hierarchical Native Training strategy to efficiently train NativeTok across varying tokenization lengths. NativeTok not only accomplishes the reconstruction task of the first stage but also explicitly constrains the intrinsic relationships within the token sequence, making it easier for the second-stage generation process to capture and model these dependencies, thereby achieving close integration between the two stages."
        },
        {
            "title": "2 Related Work",
            "content": "2."
        },
        {
            "title": "Image Tokenization",
            "content": "Image Tokenization is fundamental and crucial step in image generation. Image tokenization involves using autoencoders to compress images from high-dimensional, high-resolution space into low-dimensional, lowresolution latent space, and then using decoder to reconstruct the image from this latent space. CNN-based autoencoder tokenizers, such as VQ-VAEs [27, 32] ,encode images to discrete representation, forming codebook of image information. DQ-VAE [19] focuses on the information density of different image regions and encodes images using information-density-based variable-length coding. Transformer-based autoencoder tokenizers, such as ViT-VQGAN [35] and Efficient-VQGAN [4], also achieve excellent reconstruction results 3 through the use of transformers. Titok [37] and MAETok[7] achieves excellent reconstruction results by compressing images into one-dimensional sequences with high compression ratio. Many works further explore improvements in the vector quantization step, such as SoftVQ[6], MoVQ[38], and RQ-VAE[22], while FSQ [24] propose LFQ methods. In recent work, FlexTok [1] incorporates characteristics of continuous VAEs, utilizing flexible-length 1D encodings to enable visually ordered reconstruction process, while GigaTok [34] also achieve wonderful generation results. However, these works completely neglects the dependency relationships among tokens that need to be modeled in the second stage, resulting in fundamental discrepancy between the token dependencies established in the first stage and the image modeling approach in the second stage and do not treat the two stages as unified process. Therefore, we propose NativeTok, which introduces strong constraints when modeling token dependencies to capture more accurate intrinsic relationships. This enables the generator to more easily learn such associations during the generation process, thereby achieving true integration of the tokenization and generation stages."
        },
        {
            "title": "2.2 Image Generation",
            "content": "Image Generation typically learns and generates images from the image latent space generated in the image tokenization stage, which enhances generation efficiency. Existing generation methods primarily include three paradigms: GANs[13, 15, 20, 21], diffusion models[14, 18, 25, 26, 28], and autoregressive models[5, 10, 12, 16, 23, 30, 32, 36]. These methods predict the next token or group of tokens, based on conditions and previously generated tokens, until the entire token sequence is generated. However, previous image tokenization approaches fail to consider the inherent requirement of the second-stage generator to model the internal dependencies among tokens. As result, the generated unordered sequence can only lead to biased and inherently inaccurate model."
        },
        {
            "title": "3.1 NativeTok framework",
            "content": "To achieve the goal of native visual tokenization and ensure tight integration between the tokenization and generation stages, we design the NativeTok framework, as illustrated in Fig. 2. As previously discussed, our aim is to model the ordered relationships among tokens during the tokenization stage. This requires the first-stage tokenization process to ultimately produce an ordered, causally token sequence that aligns with the native visual order: zi = Encoder(X, z0, z1, ..., zi1). (1) The modeling of token zi depends only on the previous tokens z0 to zi1 and the image information X, and is independent of any subsequent zj(j>i). To achieve such generation process, naive approach is to apply causal mask [33]. However, we find that this method is ineffective in practice, as the transformer must simultaneously perform image self-modeling and token generation, making it difficult to generate tokens sequentially. Moreover, its performance heavily depends on the capacity of the original model and does not yield substantial improvements, as confirmed by the experimental results presented in the subsequent ablation studies. Therefore, we propose novel framework that adopts divide-and-conquer design. It decouples complex context modeling of image content from the dependency modeling among image tokens. In the image context modeling stage, we employ bidirectional attention to capture global representations of the image. In the token dependency modeling stage, we introduce native visual constraints for token-level dependency modeling, while still allowing each token to adaptively attend to the full image context. Specifically, the NativeTok framework consists of the following two components: Meta Image Transformer (MIT): Instead of directly using the original image embedding for token modeling, we first model the input image through the Meta Image Transformer IT , which consists of series of transformers, followed by fully connected network (FNN) acting as dimension switcher to reduce 4 dimensionality, ultimately obtaining Xlatent, where Xlatent represents the rich contextual information of the image in the latent space. Xlatent = FNN(MIT(X)). (2) Mixture of Causal Expert Transformer (MoCET): Each token used for representation is modeled by separate expert transformer. Specifically, our generation process is as follows: (1) Lock XLatent, keeping it unchanged during the generation process to ensure that each subsequent token generation focuses on the same image information. (2) we define an ordered lightweight expert transformer sequence = {T0, T1, . . . , TL}, (3) of the same length as the token sequence. The ith expert transformer block of is solely responsible for generating the ith token. (3) In the generation step of each token, we concatenate the locked latent space image information, all previously generated tokens, and the current padding token, and feed them into the corresponding transformer. We retain only single vector from the original padding token positions as the currently generated token. For example, in the generation process of the ith token: zi = Ti(Xlatent, z0, z1, . . . , zi1, zpadding), (4) where Ti denotes the expert transformer block responsible for modeling the ith token in the Mixture of Causal Experts T. The tokens zj (j < i) represent those that have already been generated, and zi refers to the current token modeled at the position of zpadding. Once zi is generated, it is fixed and incorporated into the modeling of subsequent tokens, and the process continues until the entire token sequence is produced. The divide-and-conquer design of NativeTok enables the modeling of visual token dependencies while allowing each image token to adaptively select from the global image context. This strategy aligns well with the previously discussed human perception-inspired modeling paradigm, which progresses from global structures to finer details. Image generation: To comprehensively evaluate the impact of explicitly introducing constraints during the tokenization stage, we conduct experiments using both autoregressive (AR) and MaskGIT-style generation frameworks. For the AR setting, we adopt LlamaGen [11] as the generator, while for the MaskGIT setting, we follow recent work [2, 5]."
        },
        {
            "title": "3.2 Training: Hierarchical Native Training strategy",
            "content": "In MoCET, the number of expert layers scales with the token count, as each new token requires an additional expert block. While this design yields strong reconstruction, it also increases training costs. To mitigate this, based on Titoks[37] two-stage training approach, we propose the Hierarchical Native Training strategy. As shown in Fig. 3, we first train 32-token NativeTok with full parameters. For the 64-token version, we reuse the Meta Image Transformer and the first 32 experts, duplicating their weights into the new experts. We then freeze the reused modules and train only the newly added 32 experts and decoder, reducing trainable parameters to 56%. The same procedure applies to the 128-token model, where only the additional experts are trained. To further boost reconstruction quality, we freeze parameters for 90% of the steps and perform full fine-tuning in the last 10%. Figure 3 Hierarchical Native Training: We freeze the Meta Image Transformer and existing experts, training only newly added experts initialized with reused weights. This reduces training costs while ensuring the new experts inherit prior modeling capabilities."
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "We evaluate our method on class-conditional ImageNet-1K[8] with 256 256 image resolution. The standard Fréchet Inception Distance (FID)[17] is adopted for evaluating the reconstruction and generation quality (denoted as rFID and gFID). rFID is calculated over the entire validation set. gFID follows ADM [9] by generating 50K samples for FID evaluation in the context of image generation."
        },
        {
            "title": "4.2 Main Experiment",
            "content": "To evaluate the impact of native visual tokenization on generation quality, we conduct experiments under both autoregressive (AR) and MaskGIT-style paradigms. For AR generation, we compare NativeTok32 against TiTok-L-32 [37] and VQGAN, using LlamaGen-B as the generator. As shown in Table 1, our method achieves 0.23 improvement in gFID over VQGAN. When the final token sequence length is fixed at 32, NativeTok significantly outperforms TiTok-L-32 [37], reducing the gFID from 7.45 to 5.23. Notably, despite slightly worse reconstruction metric (rFID = 2.57), NativeTok achieves better generation performance under the same AR generator. Figure 4 In (a), the top and bottom rows correspond to the original images and their reconstructed results. In (b), we showcase examples of generated images. Tokenizer #Params #Tokens rFID (LlamaGen-B) gFID VQGAN TiTok-L-32 NativeTok32 72M 641M 616M 256 32 32 2.19 2.21 2.57 5.46 7.45 5.23 Table 1 Main comparison of AR generation performance. Params denotes the number of parameters in the tokenizer. Tokens denotes the length of the token sequence produced during the tokenization stage. All gFID scores are reported using LlamaGen-B as the generator. In Table 2, we further compare the best-performing version of NativeTok,which is NativeTok128 within the MaskGIT framework, using MaskGIT-UVit-L [2, 5] as the generator, with several state-of-the-art methods. 6 Under this generation paradigm, NativeTok achieves gFID of 2.16 with only 287M params, demonstrating strong generation performance. This set of experiments confirms that the ordered token sequences generated Tokenizer #Tokens Codebook Size rFID Generator gFID Gen-Type VAE 1024 RQ-VAE 256 MaskGIT-VQGAN 256 ViT-VQGAN VQGAN 1024 256 GigaTok-S-S TiTok-S-128 VAR MAR-KL-16 MAGVIT-v2 FlexTok(d18-d18) NativeTok128 256 680 256 256 256 128 diffusion-based generative models - 0.62 UViT-L/2 DiT-XL/2 SiT-XL/2 transformer-based generative models 16384 1024 8192 16384 3.20 2.28 1.28 2.19 16384 4096 >4096 - 16384 64000 4096 1.01 1. 0.90 1.22 1.39 1.61 Ours 1.19 RQ-Transformer MaskGIT-ViT VIM-Large LlamaGen-B LlamaGen-L LlamaGen-B MaskGIT-UVit-L MaskGIT-UVit-L VAR transformer MAR-H Open-MAGVIT2-AR-L AR Transformers 3.40 2.27 2.06 4.45 7.55 4.17 5.46 4.21 4.05 2.50 1.97 1.92 2.35 2.51 2. 287M 675M 675M 1.4B 3.8B 1.7B 111M 343M 111M 287M 287M 2B 943M 804M 1.33B Diffusion Diffusion Diffusion AR Mask AR AR AR AR Mask Mask AR AR AR AR MaskGIT-UVit-L 2. 287M Mask Table 2 Comparison of Tokenizers and Generators. In the table, denotes the number of model parameters, and Gen.Type denotes the generative model type. Specifically, for TiTok-S-128, 2.50 and 1.97 represent the results of 8 and 64 sampling steps, respectively, while the 2.16 result for NativeTok128 is obtained with 8 sampling steps. by NativeTok consistently improve generation quality in both AR and MaskGIT-style settings. Since the second-stage generative model essentially learns the distribution established during tokenization, introducing token-level constraints enables more structured and learnable dependencies. Examples of reconstruction and generation results are presented in Fig. 4."
        },
        {
            "title": "4.3 Ablations",
            "content": "Model #Params TitokL32 TitokL(mask)32 NativeTok32 641M 641M 616M FID 12.99 12.95 11. Model Strategy rFID Speed model Sample/s NativeTok64 NativeTok64 NativeTok64 reuse+fine tune full reuse 6.50 6.46 6.22 1.53s 1.15s - VQGAN Titok-L-32 NativeTok32 233.02 136.32 119.85 (a) Different Attention Mechanisms. (b) Training Strategies Comparison. (c) Encoding Speed Comparison. Comparison of different attention mechanisms: In Table 3a, we compare different attention mechanisms used in NativeTok. We first conducted comparative analysis of TitokL32, TitokL(mask)32 (which incorporates causal masks into its encoder), and NativeTok32. This comparison further validates the structural efficiency of our proposed framework, especially when compared to traditional approaches that simply apply causal masks at the model input level, as the rFID significantly decreases from 12.99 and 12.95 to 11.19 under the same number of training steps. Comparison of Different Training Strategies. Experiments here is designed to evaluate the efficiency of the proposed Hierarchical Native Training strategy and to identify the optimal training configuration which are conducted on two A800 GPUs with per-GPU batch size of 64. In Table 3b, we conduct experiments on NativeTok64 using three training strategies: (1) full-parameter training with random initialization, (2) full reuse of pretrained weights without modification (3) reuse training for 90% of the steps followed by full-parameter fine-tuning in the remaining 10%. Results show that the Hierarchical Native Training strategy significantly improves training efficiency, reducing time per batch from 1.53s to 1.15s. At the same time, rFID improves from 6.50 to 6.46. After an additional 10% of full-parameter fine-tuning, rFID is further reduced to 6.22, demonstrating the efficiency of this training strategy. Comparison of Encoding Speed. As shown in Table 3c, although NativeTok employs longer MoCET sequence, its attention modeling is performed in low-dimensional latent space. Owing to the O(n2) complexity of transformers, this results in moderate drop in encoding speed. However, the impact remains limited and does not significantly affect overall efficiency."
        },
        {
            "title": "4.4 Visualization",
            "content": "Figure 5 Visualization: In the right-hand figure, the x-axis denotes the token position index, and the y-axis represents the corresponding probability values. When modifying adjacent token representations, we compare how the probability distribution of the next token changes accordingly. In Fig. 5, we present visualization results that highlight the accuracy of NativeToks token sequence to perturbations in preceding tokens. To investigate this, we first input condition into the generator and set two indices, and j. Tokens before position are deterministically selected as the most probable choices, while tokens between positions and are sampled according to their probability distribution. We then visualize the probability distribution of the token generated at position j+1. Specifically, we extract the top-100 highest-probability positions from the output distributions of the same generator under two slightly different inputs. We categorize these into overlapping and non-overlapping subsets for comparison. Fig. 5(b) and Fig. 5(c) show the results for NativeTok32 and TiTokL-32, respectively, under two settings: modifying 2 tokens (top row) and 4 tokens (bottom row). This observation is further supported by quantitative metrics showing significant drop in the overlap of high-probability positions in the final token distribution, demonstrate that NativeTok yields smaller overlapping region, suggesting that the generator models its token sequences with greater sensitivity and accuracy."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose the concept of native visual tokenization and introduce NativeTok, novel visual tokenizer that encodes images into ordered token sequences aligned with generation stage. By incorporating native visual order and hierarchical training strategy, NativeTok bridges the gap between tokenization and generation, enabling improved image generation."
        },
        {
            "title": "6 Acknowledgment",
            "content": "This research is supported by National Natural Science Foundation of China under Grant 623B2094."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oğuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. arXiv preprint arXiv:2502.13967, 2025. [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [3] Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. arXiv preprint arXiv:2004.03720, 2020. [4] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan: Towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73687377, 2023. [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [6] Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. arXiv preprint arXiv:2412.10958, 2024. [7] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. arXiv preprint arXiv:2502.03444, 2025. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [13] Hongchang Gao, Jian Pei, and Heng Huang. Progan: Network embedding via proximity generative adversarial network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 13081316, 2019. [14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2316423173, 2023. [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [16] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1069610706, 2022. 9 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [19] Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yongdong Zhang. Towards accurate image coding: Improved autoregressive image generation with dynamic vector quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2259622605, June 2023. [20] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. [21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [22] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern using residual quantization. Recognition, pages 1152311532, 2022. [23] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked In Proceedings of the IEEE/CVF generative encoder to unify representation learning and image synthesis. Conference on Computer Vision and Pattern Recognition, pages 21422152, 2023. [24] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [29] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. 1999. [30] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [31] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. [32] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [34] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint arXiv:2504.08736, 2025. [35] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 10 [36] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [37] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37: 128940128966, 2025. [38] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. NativeTok: Native Visual Tokenization for Improved Image Generation"
        },
        {
            "title": "Appendix",
            "content": "F.1 Training and Testing Protocols Configuration NativeTok"
        },
        {
            "title": "Meta Image Transformer layers\nMeta Image Transformer dim\nMoCET layers\nMoCET dim\ndecoder layers\ndecoder dim\ntotal parameters\ntraining strategy\nTrainable params rate\nspeed",
            "content": "NativeTok64 Meta Image Transformer layers Meta Image Transformer dim MoCET layers MoCET dim decoder layers decoder dim total parameters training strategy Trainable params rate speed speed of full parameter fine-tune NativeTok128 Meta Image Transformer layers Meta Image Transformer dim MoCET layers MoCET dim decoder layers decoder dim total parameters training strategy Trainable params rate speed speed of full parameter fine-tune Training Configuration dateset image resolution random crop random flip device per gpu batch size global batch size optimizer base learning rate learning rate schedule end learning rate warmup steps weight decay optimizer momentum training steps Value 18 1024 32 256 24 1024 616M full param training 100% 63.44/s/gpu Batch 18 1024 64 256 24 1024 666M HNT strategy 56% 54.14/s/gpu Batch 40.48/s/gpu Batch 18 1024 128 256 24 1024 766M HNT strategy 55% 39.44/s/gpu Batch 30.75/s/gpu Batch ImageNet1K 256256 True True NVIDIA A800 4 64 256 AdamW 1e4 cosine 1e5 5K 1e4 β1, β2 = 0.9, 0.99 500K Table 4 Configuration of NativeTok. For image generation using NativeTok, we provide the configuration and training strategies across three model scales: NativeTok32, NativeTok64, and NativeTok128. All models are trained on ImageNet-1K at resolution 256 256 with center cropping and horizontal flipping as the only augmentations. For NativeTok32, we use Meta Image Transformer with 18 layers and 1024 hidden dimensions, followed by Mixture of Causal Expert Transformer composed of 32 layers with 256 hidden dimensions. The decoder has 24 layers and 1024 hidden dimensions. This model contains total of 616M parameters and is trained with full parameter updates. The training throughput is 63.44 samples per second per GPU. For NativeTok64, we increase the Mixture of Causal Expert Transformer to 64 layers while keeping other components unchanged. The model size grows to 666M parameters. We employ the Hierarchical Native Training (HNT) strategy over 450K training steps, resulting in only 56% of parameters being updated during initial training. The training speed is 54.14 samples/s/GPU, and 40.48 samples/s/GPU for full-parameter fine-tuning. For NativeTok128, we maintain the same architecture as NativeTok64 but scale the training to larger Mixture of Experts model with the same 64 expert layers. The parameter count rises to 766M. HNT strategy is used again with 55% trainable parameters. Training throughput is 39.44 samples/s/GPU, dropping to 30.75 samples/s/GPU for full fine-tuning. Training configuration. All models are trained using 4 NVIDIA A800 GPUs with per-GPU batch size of 64 (global batch size 256). We use the AdamW optimizer with β1 = 0.9, β2 = 0.99, base learning rate of 1 104, cosine decay schedule down to 1 105, and weight decay of 1 104. The warm-up phase lasts 5K steps, and the full training takes 500K steps. Hierarchical Native Training strategy. For larger model variants (NativeTok64 and NativeTok128), we use the Hierarchical Native Training (HNT) strategy, which gradually expands trainable parameter subsets to mitigate optimization difficulty in large-scale mixtures of experts. We provide the complete configuration in table 4. F.2 Detailed Results of Preliminary Experiments Model NativeTok32 NativeTok64 NativeTok128 NativeTok32 NativeTok64 NativeTok128 Params stage1 616M 666M 766M add decoder finetune 616M 666M 766M rFID 5.10 3.54 2. 2.57 1.89 1.19 Table 5 Detailed Performance of NativeTok. Detailed Performance of NativeTok. We report the performance of different NativeTok variants across both training stages, as summarized in Table 5. To quantitatively assess reconstruction quality, we measure rFID scores (lower is better) under two settings: Stage 1 only and Stage 1 + decoder fine-tuning. As shown in Table 5, increasing model capacity from NativeTok32 to NativeTok128 consistently enhances reconstruction quality. Under Stage 1 alone, the rFID decreases from 5.10 to 2.86 with increasing model scale. With additional decoder fine-tuning, the rFID further improves to 2.57, 1.89, and 1.19 for NativeTok32, NativeTok64, and NativeTok128, respectively. 13 In addition, the parameter count reflects the impact of increasing the number of experts in the MoCET module. Although larger token counts result in more MoCET parameters, the increase is sublinear: every additional 32 tokens introduce only about an 8% increase in parameters relative to the base model. F.3 Qualitative Visualization We provide more detailed quantitative metrics corresponding to the visualization section in the main paper in Table 6. Specifically, we modify short token subsequences of varying lengths from 1 to 4 and compute the average top-100 overlap rate across 1,000 ImageNet classes. We compare the results of NativeTok and TiTok under the same setting. The results show that NativeTok consistently exhibits lower overlap rate, indicating that the generator is more sensitive to token-level variations. This reflects stronger alignment between the token representations and the generation behavior, resulting in more precise outputs. We provide several reconstruction visualizations in Figure 7. From left to right: original image, NativeTok128 reconstruction, NativeTok64 reconstruction, and NativeTok32 reconstruction. Number of Inconsistent Tokens NativeTok Overlap Ratio 1 2 3 56.26% 50.89% 48.34% 48.44% TiTok Overlap Ratio 78.66% 75.78% 73.43% 68.37% Table 6 Comparison of Token Overlap Ratios between NativeTok and TiTok. Figure 6 Reconstructions during the token generation process where the token count increases from 0 to 32. In Figure 6, we visualize the morphological changes of NativeTok32 and Titok32 reconstructions during the token generation process where the token count increases from 0 to 32. Our observations reveal that: Under Titok32s bidirectional attention mechanism, earlier tokens inherently contain information from subsequent tokens. In contrast, NativeTok32 with unidirectional attention progressively incorporates novel visual information not present in previous tokens. We provide several qualitative examples of generated images in Figure 8. F.4 Limitations While our proposed method demonstrates promising performance in both reconstruction and generation tasks, several limitations remain to be addressed in future work. Two-stage Training Pipeline. Similar to previous methods such as TiTok, our framework still adopts twostage training scheme, where the image tokenizer is trained independently from the autoregressive generative model. Although we partially alleviate the gap between these stages through hierarchical causal training and 14 decoder fine-tuning, fully unified end-to-end training pipeline remains unexplored. Bridging this gap could further improve token quality and model coherence, especially in downstream autoregressive tasks. Limited Training Scale. Due to computational constraints and limited resources, we were only able to train the autoregressive generator using the smallest variant, NativeTok 32. While this variant already shows clear advantages over its TiTok counterpart, it does not fully exploit the benefits of higher-capacity tokenizers like NativeTok 128. As result, our generation results may not reflect the upper bound of our proposed frameworks potential. Future work will include large-scale training of the autoregressive model on more capable tokenizer variants, which is expected to yield further improvements. 15 Figure 7 We provide demonstration of reconstruction results. From left to right: the original image, reconstruction image by NativeTok128, reconstruction image by NativeTok64, and reconstruction image by NativeTok32. 16 Figure 8 Generation examples"
        }
    ],
    "affiliations": [
        "University of Science and Technology of China"
    ]
}