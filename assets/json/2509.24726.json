{
    "paper_title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
    "authors": [
        "Shaobo Wang",
        "Zhengbo Jiao",
        "Zifan Zhang",
        "Yilang Peng",
        "Xu Ze",
        "Boyu Yang",
        "Wei Wang",
        "Hu Wei",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus."
        },
        {
            "title": "Start",
            "content": "Preprint. Under Review. SOCRATIC-ZERO: BOOTSTRAPPING REASONING VIA DATA-FREE AGENT CO-EVOLUTION Shaobo Wang β Zhengbo Jiao α,β,γ Xu Ze α Boyu Yang α Wei Wang α Hu Wei α Linfeng Zhang β α Alibaba Group Holding Limited γ Shanghai University of Finance and Economics δ Wuhan University ϵ Zhejiang University * Equal contribution Zifan Zhang α,δ Yilang Peng α,ϵ β EPIC Lab, Shanghai Jiao Tong University Corresponding authors"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasetstypically human-annotated and thus difficult to scale. While data synthesis or distillation offers promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solvers weaknesses; and the Generator distills the Teachers question-design strategy to enable scalable, highfidelity curriculum generation. This closed-loop system produces self-improving curriculumrequiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus. 5 2 0 2 9 2 ] . [ 1 6 2 7 4 2 . 9 0 5 2 : r Figure 1: The Socratic-Zero Framework: From Philosophical Analogy to Co-evolutionary System. (a) The Socratic Methodlogy illustrates the philosophical foundation: the Teacher (Socrates) acts as an intellectual midwife, eliciting understanding through probing questions; the Practitioner (Aristotle) learns not by receiving answers, but by being guided along path of reasoned inquiry; and the Apprentice-Teacher (Plato) learns to teach by observing and internalizing the masters method. (b) The Socratic-Zero Framework operationalizes this philosophy. Here, the Teachera powerful LLMguides the co-evolution of two agents. The Solver improves by generating solutions and refining them through the Teachers feedback, while the Generator evolves by strategically distilling the Teachers behavior to produce an increasingly suitable curriculum for the Solver. 1 Preprint. Under Review. Figure 2: Overall performance comparison demonstrating the giant effectiveness of Socratic-Zero. (a) Our Socratic-Generator-32B produces synthetic data that enables student models to achieve performance competitive with much larger state-of-the-art models, showcasing strong generalization capabilities. (b) Our Socratic-Solver-8B achieves an impressive 56.1% average accuracy, marking substantial +20.2 point improvement over the baseline."
        },
        {
            "title": "INTRODUCTION",
            "content": "The pursuit of advanced mathematical reasoning in large language models has reached critical juncture. While recent breakthroughs have demonstrated remarkable capabilities on complex mathematical problems (Hendrycks et al., 2021; Cobbe et al., 2021), these advances rely on massive datasets of meticulously curated reasoning trajectories requirement that is both costly and fundamentally unscalable. Current state-of-the-art models depend on millions of human-annotated problem-solution pairs and hand-designed curricula (Yu et al., 2024), creating fundamental bottleneck that limits both accessibility and the potential for models to evolve beyond human-curated knowledge boundaries. Current methodologies remain entrenched in static paradigm: datasets are frozen upon collection, curricula are handcrafted in advance, and models are trained on fixed problem distributions. This approach suffers from critical weaknesses: it cannot adapt to evolving model capabilities during training, fails to exploit rich feedback signals for targeting specific weaknesses, and requires extensive human expertise for curriculum design. Recent efforts through synthetic data generation (Lee et al., 2024; Chen et al., 2025b) and iterative training (Zhao et al., 2025; Huang et al., 2025b) have shown promise but remain constrained by their reliance on external supervision and lack of effective quality control mechanisms for synthesized content. To overcome these limitations, we introduce Socratic-Zero, paradigm-shifting framework that eliminates dependency on large-scale external datasets while enabling truly autonomous reasoning improvement. Inspired by the Socratic method of learning through questioning (Figure 1(a)), our approach implements co-evolution between three agents: Solver that attempts to solve mathematical questions, Teacher that strategically generates challenging problems to expose the Solvers weaknesses, and Generator that learns to distill and scale the Teachers problem generation strategy. This architecture (Figure 1(b)) translates the philosophical dialogue of the Socratic method into concrete, co-evolutionary computational framework. Unlike conventional pipelines that decouple data generation from model training, Socratic-Zero unifies them within continuous co-evolutionary loop formalized as optimization problem. Our contributions are threefold: 2 Preprint. Under Review. Multi-Agent Co-Evolutionary Framework: We establish theoretical foundation for coevolutionary learning where the Solver, Teacher, and Generator agents interact dynamically, formalizing reasoning improvement as an adaptive curriculum learning problem (Figure 1(b)). Socratic-Zero System: We implement concrete framework where the Solver improves via preference learning, the Teacher evaluates correctness and generates adaptive curriculum, and the Generator learns strategic distillation through value-weighted supervised fine-tuning (WSFT), enabling autonomous reasoning advancement from minimal seed data. Superior Empirical Performance: Our Socratic-Solver-8B achieves +20.2 points average improvement across seven mathematical reasoning benchmarks (Figure 2(b)), while synthetic data from our Socratic-Generator-32B achieves 37.72% downstream training effectiveness, outperforming leading commercial models including Qwen3-235B-A22B at 37.13%, Claude-4.1-Opus at 37.63%, Gemini-2.5-Pro at 37.20%, Grok-4 at 37.01%, GPT-5 at 36.62%, and DeepSeek-V3.1 at 36.62% (Figure 2(a))."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Data Synthesis. To alleviate data scarcity, researchers have leveraged LLMs generative capabilities to synthesize training samples. Early approaches used prompt engineering to guide question-answer generation (Yu et al., 2024; Zhan et al., 2025). Subsequently, LLM2LLM (Lee et al., 2024) and WarriorMath (Chen et al., 2025b) introduced deficiency-aware mechanisms, where teacher models identify knowledge gaps and generate targeted data. More recently, Absolute Zero (Zhao et al., 2025) and R-Zero (Huang et al., 2025b) explored fully autonomous self-play paradigms for continuous task generation and learning. While these advances achieve data autonomy, they lack effective quality control mechanisms, resulting in repeated use of low-value samples that severely impact effectiveness. Data Distillation. Knowledge distillation transfers capabilities from powerful teacher models to lighter student models. Early work like Orca (Mukherjee et al., 2023) used imitation learning to replicate teacher reasoning chains. Policy distillation (Wang et al., 2025b) extends this by transferring dynamic decision-making strategies. GKD (Agarwal et al., 2024) enables students to learn from their own sequences using teacher feedback for policy correction. However, students passively accept teacher feedback without evaluating reliability, degrading learning quality when guidance is suboptimal. These methods also rely on static datasets, unable to dynamically adjust content based on students evolving capabilities. While recent advances (Wang et al., 2025a; Zhao et al., 2023; Zhang et al., 2024; Chen et al., 2024a; Liu et al., 2025) promote data-centric optimization, they lack effective quality control and adaptive curriculum generation. Preference Learning. Translating feedback signals into model optimization is central to selfevolution systems. Early approaches like RLHF (Stiennon et al., 2020) trained reward models on human preferences then fine-tuned policies, but this process is complex and unstable. Recent methods like DPO (Rafailov et al., 2023) and RWSFT (Mukherjee et al., 2025) directly optimize preferences, improving efficiency and stability. Combined with self-correction mechanisms like SelfRefine (Madaan et al., 2023), models possess preliminary closed-loop capabilities. Further advances including Self-Evolved Reward Learning (Huang et al., 2025a), Self-Play Fine-Tuning (Chen et al., 2024b), and Self-Play Critic (Chen et al., 2025a) explore autonomous feedback strategies. However, these methods lack unified, co-evolving frameworks for feedback generation and validation."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 THE SOCRATIC-ZERO FRAMEWORK We introduce Socratic-Zero, fully autonomous, co-evolutionary framework designed to bootstrap mathematical reasoning from minimal set of seed problems, entirely without relying on external human-annotated data. As illustrated in Figure 3, the system operates as self-improving loop among three agents: Solver that learns to reason, fixed Teacher that acts as an oracle for evaluation and problem refinement, and Generator that learns to synthesize curriculum. 3 Preprint. Under Review. Figure 3: Overview of the Socratic-Zero Framework. (a) Solver Evolving: The Solver attempts to solve problems and learns from preference pairs of correct and incorrect solutions via DPO, while the frozen Teacher strategically generates challenging problems based on Solver failures using fixed generation and evaluation functions. (b) Generator Evolving: The Generator distills the Teachers problem generation strategy using value-weighted supervised learning. Together, these create self-improving loop where the curriculum dynamically evolves to maintain optimal challenge levels for the Solvers current capabilities. At each iteration t, the framework operates on curriculum of problems and their reference solutions, denoted as Dt = {(q, yref)}. The core of Socratic-Zero is the co-evolution of the Solver and Generator under the guidance of the Teacher. The Solver is trained to solve problems from Dt. Its failures are then used by the Teacher to create new, targeted problems. The Generator, in turn, distills the Teachers strategy to produce scalable curriculum. This process creates dynamic curriculum that continuously adapts to the Solvers evolving capabilities, ensuring the training signals remain maximally informative. The agents are formally defined as follows: Agents in the Socratic-Zero Framework 1. Solver (S): An agent with policy πθS , parameterized by θS , which maps problem to solution trajectory y. Its objective is to generate correct reasoning paths. At each iteration t, it improves by learning from preference feedback on its own attempts. 2. Teacher (T ): fixed, high-capacity LLM that provides two deterministic oracle functions: (i) verification function (q, y) {0, 1}, which judges the correctness of solution for problem q; and (ii) problem refinement function G(q, yfail) (q, ref), which creates new problem-solution pair by revising an original problem based on failed solution yfail. 3. Generator (G): An agent with policy πθG , parameterized by θG, that learns to mimic the Teachers refinement strategy. It maps problem and failed solution (q, yfail) to new problem q. It evolves to generate problems that are optimally challenging for the current Solver. The curriculum expands based on the Solvers mistakes. At iteration t, the set of Solver failures on the current curriculum Dt is collected: Ft = (cid:110) (q, yS ) (q, yref) Dt, yS πθ(t) ( q), (q, yS ) = 0 (cid:111) . (1) The Teacher refines each failure into new, instructive problem-solution pair. The set of these new pairs, Dnew, is used to augment the curriculum for the next iteration: Dt+1 = Dt {G(q, yS ) (q, yS ) Ft} (cid:125) (cid:124) (cid:123)(cid:122) Dnew . (2) 4 Preprint. Under Review. The full co-evolutionary training procedure is detailed in Algorithm 1."
        },
        {
            "title": "3.2 SOLVER TRAINING VIA ONLINE PREFERENCE OPTIMIZATION",
            "content": "The Solvers policy πθS is improved through online preference learning, leveraging the Teachers verification function to create feedback loop. For each problem Dt, the Solver generates solution attempts, {y(i) }k i=1. These attempts are partitioned into set of winning (correct) solutions Yw(q) and losing (incorrect) solutions Yl(q): Yl(q) = {y(i) Yw(q) = {y(i) ) = 0}, ) = 1} {yref if i, (q, y(i) (q, y(i) (q, y(i) ) = 0}, (3) (4) where yref is the reference solution from the curriculum. If the Solver fails to generate any correct solution, the ground-truth solution serves as the sole winning example. This ensures valid preference pair (yw, yl), where yw Yw(q) and yl Yl(q), can always be constructed. The Solvers parameters θS are then updated using the Direct Preference Optimization (DPO) loss (Rafailov et al., 2023). This objective maximizes the likelihood of preferred solutions over rejected ones: LDPO(θS ; θref) = EqDt,ywYw(q),ylYl(q) (cid:20) (cid:18) log σ β log πθS (yw q) πθref(yw q) β log πθS (yl q) πθref (yl q) (cid:19)(cid:21) , (5) where πθref is frozen reference policy (e.g., πθS from the start of the iteration), β is temperature hyperparameter, and σ is the sigmoid function. 3.3 GENERATOR TRAINING VIA OFFLINE VALUE-WEIGHTED DISTILLATION To ensure scalable curriculum generation without perpetual reliance on the expensive Teacher, the Generator πθG is trained to distill the Teachers problem refinement strategy. An effective curriculum should feature problems of desirable difficultychallenging enough to be informative but not so difficult as to be unsolvable. We formalize this concept with utility function (qπθS ) that scores new problem based on the i=1 (q, y(i) current Solvers performance. Let sq = 1 ) be the success rate of the Solver πθS over attempts. The utility is defined by an unnormalized Gaussian centered at target success rate µ: (cid:80)k (qπθS ) = exp (cid:18) (sq µ)2 2σ2 (cid:19) . (6) We set µ = 0.5 to incentivize problems at the frontier of the Solvers capabilities, with σ controlling the tolerance for deviation. The Generator is trained via weighted supervised fine-tuning (WSFT) to mimic the Teachers generation of high-utility problems. The training data DG is constructed from the set of Solver failures Ft and the corresponding Teacher-refined problems from Dnew: DG = {(q, yfail, q) (q, yfail) Ft, (q, ref) = G(q, yfail)}. The Generators objective is to maximize the utility-weighted log-likelihood of producing the Teachers refined problems: LWSFT(θG) = E(q,yfail,q)DG [U (qπθS ) log πθG (q q, yfail)] . This objective steers the Generator towards producing problems that are optimally challenging for the current Solver, effectively internalizing the Teachers expert curriculum design principles. (7)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Models. We employed Qwen3-235B-A22B-Instruct-2507 (Yang et al., 2025) as the Teacher model to provide high-quality evaluation and curriculum generation. We used Qwen3-32B (Yang et al., 2025) 5 Preprint. Under Review. as the Generator to learn and distill the Teachers problem generation strategies. We conducted Solver experiments on multiple model architectures including Qwen3-8B-base, Qwen3-14B-base (Yang et al., 2025), and GLM4-9B-base (GLM et al., 2024) to demonstrate cross-model generalization. We compared strong baselines including Gemini-2.5-Pro-06-17 (Comanici et al., 2025), GPT5-0807global, and DeepSeek-v3.1-671B (DeepSeek-AI et al., 2025b) against our approach. For downstream evaluation of generated data quality, we fine-tuned DeepSeek-R1-Distill-Llama-8B (DeepSeek-AI et al., 2025a) as the student model. Benchmarks. We used seven mathematical reasoning benchmarks for evaluation, including AMC (Cao et al., 2025), Minerva (Nagrani et al., 2025), MATH-500 (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), Olympiad-Bench (He et al., 2024), AIME-2024, and AIME-2025. Additionally, we employed three general reasoning benchmarks to assess the transfer of mathematical reasoning improvements to broader cognitive abilities, namely BBEH (Kazemi et al., 2025), MMLU-Pro (Wang et al., 2024), and SuperGPQA (Team et al., 2025). Curriculum Settings. The initial curriculum C0 contained 100 questions sampled from the MATH training set (Hendrycks et al., 2021) following specific diversity and difficulty criteria (detailed in Appendix F). All Solver models first underwent LoRA-based (Hu et al., 2021) SFT on 1,500problem dataset of Level 5 difficulty. Key hyperparameters: = 8 solution trajectories per problem, reward parameters µ = 0.5 and σ = 0.2, and training batches combined 100% new problems with 25% historical curriculum for replay. Solver Evaluation. For each test question, we generated 32 solutions using zero-shot prompting with temperature 0.7. We determined correctness through dual-verification mechanism combining rule-based answer extraction and semantic validation. We reported Mean@32 accuracy across all evaluations. Detailed evaluation protocols, including sampling strategies, answer extraction methods, and LLM judge configurations, are provided in Appendix G. Baselines. Baselines. We employed two strong baselines for comparison. Static Augmentation (SA) follows traditional approaches via MetaMath (Yu et al., 2024) and WizardMath (Luo et al., 2023), augmenting training data with fixed synthetic questions generated offline without adaptive curriculum evolution. LLM2LLM (Lee et al., 2024) implements iterative self-training where models generate questions based on current failures and retrain on augmented datasets. Both baselines use identical SFT initialization for fair comparison. Generator Evaluation. We prompted each generator with 1,000 SAND-Math (Zhang et al., 2025) seeds to produce 3 variants each, resulting in 3,000 total generated questions. We measured validity rate by having Qwen3-235B-A22B-Instruct-2507 attempt to solve each generated question under 4,096-token, 600-s timeout constraint. We evaluated downstream utility by fine-tuning DeepSeek-R1Distill-Llama-8B on the QAs and measuring performance on mathematical reasoning benchmarks. Infrastructure. We conducted training experiments on 8NVIDIA H20 GPUs. We performed Teacher model inference using 16AMD MI308X GPUs. Detail provided in Appendix D. 4.2 SOLVER RESULTS Baseline Comparison. Table 1 shows Socratic achieves 56.1% average accuracy, outperforming Static Augmentation by +15.4 points and LLM2LLM by +15.2 points. Notable gains appear on competition problems: AIME-24 (+19.1) and AIME-25 (+16.5), demonstrating the advantages of DPO-based preference learning and adaptive curriculum generation. Cross-Architecture Generalization. Table 2 validates that Socratic principles transcend specific model families. On GLM4-9B-base, Socratic Stage 3 achieves 52.3% average accuracy (+17.1 points over base model), with strong improvements on AIME benchmarks: AIME-25 (+20.4) and AIME-24 (+23.9). Similarly, on Qwen3-14B-base, Stage 3 reaches 60.3% (+17.3 points), demonstrating consistent effectiveness across different architectures and addressing fundamental reasoning capabilities. Transfer to General Reasoning. Table 3 shows mathematical reasoning improvements transfer to broader cognitive abilities, with +6.02 points average improvement across BBEH, MMLU-Pro, and SuperGPQA benchmarks. 6 Preprint. Under Review. Table 1: Solver Evaluation Results with different training methods. Results are reported on seven benchmarks (AMC, Minerva, MATH-500, GSM8K, Olympiad, AIME-25, AIME-24) and their average. Arrow values represent absolute point changes relative to Static Augmentation, where indicates improvement and indicates decline. Benchmark Datasets AMC Minerva MATH-500 GSM8K Olympiad AIME-25 AIME-24 Training Method Qwen3-8B-base + Zero-shot + SFT + Static Augmentation 32.5 39.1 45. 31.3 37.8 41.9 48.8 56.9 62.7 63.4 68.2 74. 24.1 31.7 35.9 Qwen3-8B-base with LLM2LLM + Stage 1 + Stage 2 + Stage 3 41.6 4.2 43.2 2.6 44.9 0.9 41.2 0.7 40.6 1.3 42.1 0. 53.1 9.6 54.9 7.8 66.8 4.1 78.3 3.7 79.1 4.5 79.4 4.8 32.4 3.5 33.8 2.1 34.6 1.3 Qwen3-8B-base with Socratic-Zero (Ours) Avg. 29.9 35. 40.7 5.1 9.3 12.3 8.9 3.4 9.1 3.2 10.4 1.9 37.5 3.2 38.3 2.4 40.9 0.2 4.2 8. 11.4 6.7 4.7 7.2 4.2 7.9 3.5 + Stage 1 + Stage 2 + Stage 3 43.8 2.0 49.3 3.5 63.7 17.9 39.4 2.5 40.7 1.2 52.4 10.5 60.2 2.5 63.4 0.7 81.2 18. 69.7 4.9 71.8 2.8 87.3 12.7 35.3 0.6 38.2 2.3 55.1 19.2 10.6 0.8 12.9 1.5 24.6 13.2 11.8 0.5 15.6 3.3 28.4 16.1 38.7 2.0 41.7 1.0 56.1 15.4 Table 2: Cross-Model Generalization Results with different training stages. Each block corresponds to model (GLM4-9B, Qwen3-14B). Results are reported on seven benchmarks (AMC, Minerva, MATH-500, GSM8K, Olympiad, AIME-25, AIME-24) and their average. Values with arrows represent absolute point changes relative to SFT for each model. Training Method GLM4-9B-base +Zero-shot + SFT + Socratic Stage 1 + Socratic Stage 2 + Socratic Stage 3 Qwen3-14B-base +Zero-shot + SFT + Socratic Stage 1 + Socratic Stage 2 + Socratic Stage 3 Benchmark Datasets AMC Minerva MATH-500 GSM8K Olympiad AIME-25 AIME-24 Avg 34.5 38.4 39.4 1.0 42.3 3.9 47.5 8. 48.8 61.3 62.9 1.6 65.4 4.1 70.0 8.7 37.3 44.8 47.3 2.5 49.4 4.6 52.8 8.0 40.5 51.8 55.1 3.3 57.4 5.6 60.7 8.9 52.3 63.8 67.9 4.1 68.1 4.3 73.8 10.0 62.0 71.5 74.6 3.1 76.7 5.2 80.2 8.7 72.5 77.2 79.8 2.6 82.5 5.3 83.9 6. 91.5 92.2 91.8 0.4 92.3 0.1 93.7 1.5 34.8 41.3 43.4 2.1 45.5 4.2 49.4 8.1 38.4 47.3 52.5 5.2 54.2 6.9 58.3 11.0 7.5 15.1 15.6 0.5 19.1 4.0 27.9 12.8 9.6 18.1 19.8 1.7 24.8 6.7 28.9 10.8 7.2 19.3 24.0 4.7 25.3 6.0 31.1 11. 10.0 20.3 21.7 1.4 23.3 3.0 30.1 9.8 35.2 42.8 45.3 2.5 47.5 4.7 52.3 9.5 43.0 51.8 54.1 2.3 56.3 4.5 60.3 8.5 Table 3: Performance on general reasoning benchmarks with different training stages. Results are reported on three benchmarks (BBEH, MMLU-Pro, SuperGPQA) and their average. Values with arrows represent absolute point changes relative to zero-shot Qwen3-8B-base performance. Training Method Qwen3-8B-Base + Zero-shot General Reasoning Benchmarks BBEH MMLU-Pro SuperGPQA Avg. 7. 50.00 24.73 27.47 Base Model with Socratic (Ours) + Stage 1 + Stage 2 + Stage 3 8.48 0.80 9.11 1.43 9.54 1. 55.71 5.71 59.29 9.29 60.89 10.89 27.32 2.59 29.73 5.00 30.05 5.32 30.50 3.03 32.71 5.24 33.49 6.02 4.3 GENERATOR RESULTS We assessed both the intrinsic quality of generated problems and their downstream training effectiveness, with Socratic-Generator-32B being compared against its base model and SOTA commercial large language models to determine whether strategic specialization can match the performance of much advanced larger models. Preprint. Under Review. Table 5: Downstream Training Effectiveness with different generator models. Results are reported on seven benchmarks (AIME-24, AIME-25, AMC-23, GSM8K, MATH-500, Minerva, Olympiad) and their average. Values with arrows represent absolute point changes relative to Qwen3-32B baseline. DeepSeek-R1-Distill-Llama-8B + Zero-shot Open-Sourced Generators Qwen3-32B Qwen3-235B-A22B-Instruct-2507 DeepSeek-v3.1-671B Advanced commercial Generators Gemini-2.5-Pro-06-17 GPT5-0807-global Grok-4 Claude-4.1-Opus Socratic-Generator-32B Benchmark Datasets AIME-24 AIME-25 AMC-23 GSM8K MATH-500 Minerva Olympiad Avg. 5. 8.3 42.5 72.2 52.4 15.3 23. 32.75 9.2 12.5 3.3 12.5 3.3 10.0 0.8 12.5 3.3 11.7 2.5 13.3 4.1 12.5 3.3 10.0 12.5 2.5 11.7 1.7 15.0 5.0 13.3 3.3 12.5 2.5 13.8 3. 44.4 47.5 3.1 46.2 1.8 46.9 2.5 45.0 0.6 45.8 1.4 46.5 2.1 75.7 76.1 0.4 76.4 0.7 78.1 2.4 76.8 1.1 76.3 0.6 77.3 1.6 13.3 3.3 48.1 3. 77.6 1.9 55.7 57.8 2.1 56.4 0.7 57.2 1.5 56.6 0.9 56.9 1.2 57.5 1.8 57.8 2.1 15.1 16.4 1.3 16.5 1.4 16.0 0.9 15.5 0.4 15.9 0.8 16.7 1. 24.5 23.6 0.9 23.9 0.6 25.4 0.9 25.9 1.4 24.9 0.4 24.3 0.2 34.97 37.13 2.16 36.62 1.65 37.20 2.23 36.62 1.65 37.01 2.04 37.63 2.66 18.4 3.3 24.6 0. 37.72 2.75 4.3.1 EVALUATION PROTOCOL We adopted standardized, three-stage evaluation pipeline to holistically assess both the intrinsic quality of generated problems and their extrinsic utility in downstream model training. The full procedure is formalized below. Step 1: Problem Generation. We prompted each generator with 1,000 seed problems from SANDMath (Zhang et al., 2025) and tasked with producing five augmented variants per seed, resulting in 3,000 total generated problems per model. Step 2: Quality Assessment. We measured problem validity by prompting Qwen3-235B (Yang et al., 2025) selected for its state-of-the-art mathematical reasoning capability and its role as the teacher model in the distillation framework to solve each generated problem under strict constraints: 4,096-token limit and 600-second timeout. The Validity Rate was defined as the percentage of problems successfully solved within these bounds. Step 3: Student Evaluation. We used all valid question-answer (QA) pairs to fine-tune the student model, DeepSeek-R1-Distill-Llama-8B (DeepSeek-AI et al., 2025a). We evaluated Downstream Utility as the mean accuracy average accuracy over 16 independent decoding runs per problem across seven diverse mathematical reasoning benchmarks. 4.3.2 PROBLEM QUALITY ASSESSMENT To evaluate the quality of the generated problems, we measure their Validity Rate the percentage of problems solvable by powerful model (Qwen3235B-A22B-Instruct-2507). As shown in Table 4, our specialized Socratic-Generator-32B generator achieves remarkable 95.6% validity rate. This not only represents substantial improvement over its base model Qwen3-32B but also rivals the performance of significantly larger models, including proprietary models like GPT5-0807-global, Gemini-2.5Pro-06-17. This demonstrates our co-evolutionary strategy effectively. 4.3.3 DOWNSTREAM EFFECTIVENESS Table 4: Generator validity rates. Generator Model Validity Rate (%) Qwen3-32B Qwen3-235B-A22B Gemini-2.5-Pro GPT5-global DeepSeek-v3.1-671B Grok4 Claude-4.1-opus Socratic-Generator-32B 89.1 95.1 6.0 94.2 5.1 95.8 6.7 96.5 7.4 95.7 6.7 96.9 7. 95.6 6.5 Table 5 reports the downstream utility of each generator, measured by the performance of the finestudent model. The output from our Socratic-Generator-32B leads to final student accuracy of 37.72%. Notably, this performance not only rivals that achieved using data from significantly larger models but also marginally surpasses (+0.59 points) the result from its own Teacher (Qwen3-235B), despite being over 20x smaller. Preprint. Under Review. Table 6: Ablation studies on the necessity of initial SFT and different strategies of reward functions. (a) Values with arrows represent absolute point changes relative to the previous stage within the same method. (b) Values with arrows represent absolute point changes relative to the Gaussian baseline. ρ represents solver success rate, µ represents target success rate, σ represents standard deviation in Gaussian reward function (µ, σ), Ψρ(a, b) represents linear function Ψρ(a, b) = aρ + b. (a) Ablation Study on Initial SFT (AIME-24) (b) Ablation of Reward Functions (Benchmark Avg.) Method Score (%) (%) Reward Function Valid (%) Avg (%) (%) Qwen3-8B-Base 9.64 - (µ = 0.5, σ = 0.2) (Ours) Socratic-Zero (w/o SFT) + Stage 1 + Stage 2 + Stage 3 11.67 11.15 11.98 Socratic-Zero (w/ SFT) + Stage 1 + Stage 2 + Stage 3 13.44 14.48 28. 2.03 1.51 2.34 3.80 4.84 18. Ψρ(a = 0, = 1) Ψρ(a = 1, = 0) Ψρ(a = 1, = 1) (µ = 0.3, σ = 0.2) (µ = 0.4, σ = 0.2) (µ = 0.6, σ = 0.2) (µ = 0.7, σ = 0.2) 89.9 89.4 89.8 88.9 89.5 89.7 89.7 89.8 35. 35.52 35.47 35.42 35.32 35.37 35.50 35.43 - 0.20 0.25 0. 0.40 0.35 0.22 0.29 4.4 ABLATION STUDIES We conducted two key ablation studies to validate our frameworks design choices, with results summarized in Table 6. The first study examines the necessity of initial supervised fine-tuning (SFT), while the second investigates different reward function formulations during reinforcement learning. Ablation on Initial SFT Table 6a demonstrates the critical importance of initial supervised finetuning. Starting from the Qwen3-8B-Base model (9.64%), the version without SFT shows minimal improvements across all three training stages, reaching only 11.98% by Stage 3a marginal gain of 2.34 percentage points. In stark contrast, the SFT-initialized model achieves substantial performance improvements, culminating in remarkable 28.02% score at Stage 3, representing an 18.38 percentage point improvement over the base model. This 7.9 greater improvement highlights how SFT provides essential foundational capabilities that enable subsequent reinforcement learning stages to be dramatically more effective. The SFT phase likely equips the model with basic reasoning patterns and solution structures that serve as building blocks for more sophisticated reasoning developed during RL training. Ablation on Reward Functions Table 6b compares various reward function formulations using benchmark average scores. Our Gaussian reward (µ = 0.5, σ = 0.2) achieves the best performance (35.72%) while maintaining high validity (89.9%). We evaluated linear functions Ψρ(a, b) = aρ + with different parameterizations, all of which underperformed our Gaussian approach by 0.20-0.30 percentage points. Similarly, varying the Gaussian mean parameter µ from 0.3 to 0.7 consistently yielded inferior results, with the largest performance drop (0.40 points) occurring at µ = 0.3. This suggests that the Gaussian formulation with µ = 0.5 provides an optimal balance between exploration and exploitation during policy optimization, while the moderate variance (σ = 0.2) allows sufficient reward signal differentiation without excessive noise that could destabilize training."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we introduced Socratic-Zero, multi-agent co-evolutionary framework where Solver, Teacher, and Generator agents bootstrap autonomous mathematical reasoning from minimal seed data. Our implementation demonstrates that carefully designed learning mechanism can achieve remarkable performance without relying on massive external datasets, offering viable path for developing powerful reasoning systems in resource-constrained scenarios. Extensive experiments show that our framework not only achieves state-of-the-art results on mathematical benchmarks but also exhibits strong generalization capabilities across diverse problem types and difficulty levels. While the complex agent dynamics currently lack formal convergence analysis, future work will aim to establish this theoretical foundation and extend the frameworks applicability to broader domains including scientific discovery, real-world decision making, and complex system modeling. 9 Preprint. Under Review."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In International Conference on Learning Representations, 2024. Lang Cao, Chao Peng, Renhong Chen, Wu Ning, Yingtian Zou, and Yitong Li. Step guided reasoning: Improving mathematical reasoning using guidance generation and step reasoning, 2025. Jiaqi Chen, Ruotian Ma, Bang Zhang, Peisong Wang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong, and Xiaodan Liang. Spc: Evolving self-play critic via adversarial games for llm reasoning, 2025a. Yiming Chen, Zhenhua Liu, Xiang Yue, and Wenpeng Yin. Distillm: Towards streamlined distillation for large language models, 2024a. Yue Chen, Minghua He, Fangkai Yang, Pu Zhao, Lu Wang, Yu Kang, Yifei Dong, Yuefeng Zhan, Hao Sun, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. Warriormath: Enhancing the mathematical ability of large language models with defect-aware framework, 2025b. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models, 2024b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, et al. Deepseek-v3 technical report, 2025b. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Self-evolved reward learning for llms, 2025a. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data, 2025b. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard, 2025. Preprint. Under Review. Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipalli, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. Llm2llm: Boosting llms with novel iterative data enhancement, 2024. Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, and Linfeng Zhang. Shifting ai efficiency from model-centric to data-centric compression, 2025. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023. Subhojyoti Mukherjee, Viet Dac Lai, Raghavendra Addanki, Ryan Rossi, Seunghyun Yoon, Trung Bui, Anup Rao, Jayakumar Subramanian, and Branislav Kveton. Learning to clarify by reinforcement learning through reward-weighted fine-tuning, 2025. Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, et al. Minerva: Evaluating complex video reasoning, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2023. Markus Stiennon, Long Ouyang, Jeff Wu, Rewon Child, David Amodei, Dario Amodei, David F. M. Brown, Benjamin Mann, Paul Christiano, Peter Chen, and John Schulman. Learning to follow instructions with human feedback, 2020. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. Shaobo Wang, Yicun Yang, Zhiyuan Liu, Chenghao Sun, Xuming Hu, Conghui He, and Linfeng Zhang. Dataset distillation with neural characteristic function: minmax perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2557025580, June 2025a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. Zhiyuan Wang, Yuxiao Chen, Chao Yu, Yifan Zhang, and Jun Wang. survey of reinforcement learning-driven knowledge distillation, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report, 2025. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2024. Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, and Fei Tan. Mathsmith: Towards extremely hard mathematical reasoning by forging synthetic problems with reinforced policy, 2025. 11 Preprint. Under Review. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Sand-math: Using llms to generate novel, difficult and useful mathematics questions and answers, 2025. Yue Zhang, Tianxiang Sun, Xiangyang Liu, Hang Yan, and Xipeng Qiu. Dilm: Distilling dataset into language model for text-level dataset distillation, 2024. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data, 2025. Liang Zhao, Yuhui Shi, Zhiyong Feng, and Shuo Wang. Data distillation: survey, 2023. 12 Preprint. Under Review."
        },
        {
            "title": "A PROMPTS",
            "content": "A.1 SOLVER REASONING PROMPT Solver Mathematical Reasoning Prompt You are an IMO gold medalist solving computational math competition problem. Understand: Restate the problem mathematically. Identify knowns, unknowns, and constraints. Plan: Choose an efficient method, show clear logic. Execute: Show all key steps algebra, number theory, or combinatorics. No skipped calculations. Verify: Check with small cases, reverse substitution, or estimation. Conclude with the exact answer in LaTeX: [boxed{< answer >}] Given Problem: {question} A.2 TEACHER EVALUATION PROMPT Teacher Solution Grading Prompt You are professional math teacher responsible for grading and error analysis. Grading criteria: Focus on final answer correctness, use reference when provided, provide concise error analysis for incorrect answers. Return JSON format: { \"correct_answers\": [\"correct answer 1\", \"correct answer 2\"], \"incorrect_answers\": [ {\"answer\": \"incorrect answer\", \"analysis\": \"brief error analysis\"} ] } Problem: {question} Reference: {reference_info} Student answers: {student_answers} A.3 TEACHER GENERATION PROMPT Teacher Problem Enhancement Prompt You are math problem enhancement expert specializing in competition-style mathematics. Generate enhanced problems based on student error analysis with complete solutions. Requirements: Generate enhanced problem, provide detailed solution, ensure solvability and correctness. Enhancement principles: Target specific error points, maintain mathematical essence, help avoid similar errors. Return JSON format: { } \"enhanced_question\": \"enhanced problem content\", \"solution\": \"detailed solution steps\", \"answer\": \"final answer\" Original: {original_question} Error analysis: {error_analysis} A.4 STATIC AUGMENTATION BASELINE PROMPTS Static Augmentation Evolution Prompts Upward Evolution: Step 1: Identify elements that can increase complexity. Step 2: Plan to modify at least three components. Step 3: Implement rewritten instruction. Step 4: Review and provide final version. Downward Evolution: Step 1: Identify elements that can decrease complexity. Step 2: Plan to simplify at least three components. Step 3: Implement easier version. Step 4: Review and provide final simplified version. Format: Step 1 #Elements#: Step 2 #Plan#: Step 3 #Rewritten#: Step 4 #Final#:"
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "We conducted training experiments on 8NVIDIA H20 GPUs with the following configuration: - GPU Memory: 96GB HBM3 per GPU - Total Training Memory: 768GB - Interconnect: NVLink 4.0 - Storage: High-speed NVMe SSD arrays for dataset caching - Network: InfiniBand for distributed training coordination 13 Preprint. Under Review. The training infrastructure utilized mixed-precision training (FP16) with gradient checkpointing to optimize memory usage. We employed distributed training using PyTorchs DistributedDataParallel with NCCL backend for efficient gradient synchronization across GPUs. B.1 TRAINING HYPERPARAMETERS We provide the complete hyperparameter settings for all components of the Socratic-Zero framework in Table 7. Table 7: Hyperparameters used in Socratic-Zero framework. Component Parameter Learning rate Per-device batch size Gradient accumulation steps Maximum sequence length LoRA rank (r) LoRA alpha (α) LoRA dropout Number of epochs Learning rate Per-device batch size Gradient accumulation steps Maximum sequence length Maximum training steps DPO regularization (β) Warmup steps Optimizer Weight decay Maximum gradient norm Learning rate Per-device batch size Gradient accumulation steps Maximum sequence length LoRA rank (r) LoRA alpha (α) Number of epochs Solutions per problem (k) reward mean (µ) reward std (σ) Historical replay ratio Sampling temperature Number of samples Token limit for validity check Solver SFT Training Solver DPO Training Generator Training Curriculum Parameters Evaluation Settings Value 5e-5 2 4 2048 64 128 0.1 1 1e-6 5e-6 2 4 16 2048 10 200 0.05 0.2 2 20 AdaFactor 0.01 1.0 1e-5 1 8 2048 64 128 2 8 0.5 0.2 25% 0.7"
        },
        {
            "title": "C CURRICULUM EVOLUTION DETAILS",
            "content": "This appendix provides detailed description of the mechanisms for curriculum evolution, including the problem categorization and adaptive generation strategies that guide the learning process. C.1 PROBLEM CATEGORIZATION BY SOLVER PERFORMANCE To effectively manage curriculum difficulty, we dynamically categorize each problem Dt based on the current Solvers performance. The Solver πθS makes attempts {y(i) i=1 on each problem. We then calculate the Solvers success rate: }k sq 1 (cid:88) i=1 (q, y(i) ), 14 (8) Preprint. Under Review. where is the Teachers verification function. This metric allows us to partition the curriculum Dt into three distinct zones: 1. Mastered Zone (Dmastered = {q sq = 1}): Problems the Solver consistently solves correctly. These problems form solid foundation and are used as basis for generating more challenging variants. 2. Learning Zone (Dlearning = {q 0 < sq < 1}): Problems the Solver can solve intermittently. This zone represents the optimal frontier for learning, where the Solver has emerging competence but requires further practice to achieve mastery. 3. Too Difficult Zone (Ddifficult = {q sq = 0}): Problems the Solver consistently fails to solve. These problems are temporarily set aside from the generation process to avoid creating tasks far beyond the Solvers current capabilities. C.2 ZONE-ADAPTIVE PROBLEM GENERATION New problems are strategically generated by the Teachers refinement function, G, using problems from the Mastered and Learning zones. This ensures that curriculum expansion remains within the Solvers zone of proximal development. The generation strategy is adapted to the source category: From the Learning Zone: For problem Dlearning, the Teacher is prompted with specific failed solution attempt, yfail. The new problem (q, ref) = G(q, yfail) is designed to target the specific reasoning error exhibited in yfail, thereby helping the Solver overcome its weaknesses. From the Mastered Zone: For problem Dmastered, the Teacher is prompted with ref) = G(q, ysucc) is more complex successful solution, ysucc. The new problem (q, variant, designed to push the boundaries of the Solvers established competence. The set of newly generated problems Dnew is thus composed of variants from both zones, creating balanced and targeted curriculum expansion. Problems from Ddifficult are explicitly excluded from this generation process to prevent counterproductive increases in difficulty. C.3 DYNAMIC RECATEGORIZATION OF PROBLEMS As the Solvers policy πθS is updated at each iteration, its performance on existing problems changes. Consequently, all problems in the curriculum are periodically re-evaluated and recategorized. This dynamic process ensures the curriculum remains perfectly attuned to the Solvers evolving skill set. The typical transitions between zones are: difficult D(t+1) D(t) learning D(t+1) D(t) learning mastered (as capability improves) (as skills are consolidated) (9) (10) This recategorization mechanism allows problems that were once too difficult to re-enter the active learning and generation pool once the Solver is ready, ensuring continuous and adaptive learning trajectory."
        },
        {
            "title": "D TEACHER MODEL INFRASTRUCTURE",
            "content": "The Teacher model (Qwen3-235B-A22B-Instruct-2507) requires substantial computational resources for curriculum generation and solution evaluation. We deployed the model using distributed inference architecture to meet the throughput demands of the co-evolutionary training process. We distributed the Teacher model across 16 AMD MI308X GPUs, each equipped with 192GB HBM3 memory, providing total of 3,072GB aggregate memory. This configuration enables concurrent processing of curriculum generation requests while maintaining inference consistency across the framework. 15 Preprint. Under Review. To ensure system reliability and scalability, we implemented multi-endpoint architecture with automatic load balancing and failover mechanisms. We configured the inference service with connection pooling (50 concurrent connections per endpoint) and exponential backoff retry policies to handle high request volumes during training. We optimized key performance parameters for the mathematical reasoning domain: request timeouts of 600 seconds accommodate complex problem generation, while 4,096-token limit ensures efficient solution evaluation. Batch processing utilizes 32 concurrent workers to maximize throughput during curriculum evolution phases. TEACHER-GENERATED PROBLEM ENHANCEMENT We provide examples of how the Teacher model enhances problems based on Solver failures. The following demonstrates the progression from original problems to ly-targeted enhanced versions. E.1 EXAMPLE 1: RATIONAL INEQUALITY ENHANCEMENT Original Problem: Find all real numbers satisfying 2x5 x+3 2. (Give your answer in interval notation.) Enhanced Problem (Round 3): Find all real numbers satisfying 2x x29 + 1 x+3 4x+1 (x3)2 . (Give your answer in interval notation.) Enhancement Analysis: The enhancement introduces multiple complexity factors: (1) factored denominators requiring domain analysis, (2) multiple rational terms requiring common denominators, (3) squared terms in denominators, and (4) more complex algebraic manipulation. The enhanced problem targets common student errors in rational inequality solving while maintaining the core mathematical concepts. E.2 EXAMPLE 2: NUMBER THEORY ENHANCEMENT Original Problem: Find the greatest common divisor of 10! + 6 and 11! + 14. Enhanced Problem: Find the greatest common divisor of 12! + 8 and 13! + 26, where the second number can be written as 13 12! + 26. Enhancement Analysis: The enhancement maintains the GCD structure while increasing numerical complexity and requiring students to recognize the relationship between consecutive factorials, targeting errors in modular arithmetic applications."
        },
        {
            "title": "F SEED SELECTION PROTOCOL",
            "content": "The selection of initial seed problems is critical for establishing an effective curriculum foundation. We employed systematic approach to ensure the seed set provides appropriate difficulty, comprehensive coverage, and sufficient diversity for subsequent curriculum evolution. Difficulty Alignment We selected seed problems to match the base models capability range to ensure productive learning dynamics. We drew problems from MATH dataset Levels 2-4, which empirically provide optimal challenge levels for our base models. Specifically, we excluded Level 1 problems (too easy, leading to trivial curriculum generation) and Level 5 problems (too difficult, 16 Preprint. Under Review. Table 8: Seed Problem Distribution Across Mathematical Domains Subject Area Count Representative Topics Algebra Number Theory Geometry Combinatorics Precalculus Intermediate Algebra Prealgebra 15 15 15 15 15 15 10 Linear/quadratic equations, inequalities, functions Divisibility, modular arithmetic, prime factorization Coordinate geometry, trigonometry, area/volume calculations Counting principles, permutations, probability Complex numbers, sequences, polynomial analysis Advanced algebraic manipulation, systems Foundational arithmetic and basic algebraic concepts Total 100 Comprehensive mathematical reasoning coverage resulting in universal failure and poor learning signals). Pre-filtering involved evaluating candidate problems with the base model using 8 solution attempts; we retained problems with success rates between 10-70% to ensure neither complete failure nor trivial success. Domain Coverage To ensure comprehensive mathematical reasoning development, we sampled seed problems across all seven MATH subject areas with balanced representation as shown in Table 8: This distribution ensures that curriculum evolution can target weaknesses across diverse mathematical domains rather than overfitting to specific problem types. Diversity Assurance Within each subject area, we selected problems to maximize methodological diversity. We employed clustering based on solution approach similarity (using embedding representations of problem statements) and selected problems from different clusters to ensure varied reasoning patterns. Additionally, we explicitly included problems requiring different mathematical tools to promote comprehensive skill development. Quality Control We subjected all candidate problems to rigorous quality verification through multi-stage process: Quality Control Pipeline 1. Clarity Check: Problems must have unambiguous statements and well-defined solution paths 2. Answer Verification: We validated reference solutions by the Teacher model with multiple independent attempts 3. Value: Problems must demonstrate clear learning objectives and avoid trick questions or overly specialized knowledge 4. Contamination Avoidance: We excluded seed problems from all evaluation benchmarks to prevent data leakage This systematic selection process ensures that the initial curriculum C0 provides robust foundation for the co-evolutionary training dynamics while maintaining the diversity necessary for effective curriculum expansion."
        },
        {
            "title": "G EVALUATION PROTOCOL DETAILS",
            "content": "Mean@32 Sampling Strategy The Mean@32 evaluation metric represents the average accuracy across 32 independent solution attempts per problem. For each test problem, we generated 32 distinct solutions using temperature-based sampling (T=0.7) with top-p nucleus sampling (p=0.9) as specified in Table 7. This approach provides robust performance estimates by capturing the models consistency and reliability across multiple attempts. We employed the sampling process using zero-shot prompting without few-shot examples to ensure unbiased evaluation. We generated each of the 32 solutions independently with different random seeds, preventing potential correlation effects. The final accuracy is computed as the proportion 17 Preprint. Under Review. of correct solutions among the 32 attempts, providing more stable performance measure than single-shot evaluation. MathRule Answer Extraction MathRule is rule-based tool designed to extract and standardize final numerical answers from mathematical solution text. The tool employs pattern matching to identify answer indicators such as Therefore, Thus, The answer is, and LaTeX boxed expressions like boxed{}. The extraction process involves: (1) Locating answer indicators within the solution text, (2) Parsing mathematical expressions using regex patterns for common formats (fractions, decimals, integers, algebraic expressions), (3) Standardizing representations (e.g., converting 1 2 to 0.5 when appropriate), (4) Handling multiple answer formats and selecting the most confident extraction based on contextual cues. MathRule achieves high precision in answer extraction while maintaining robustness to variations in solution formatting and mathematical notation styles. LLM Judge Configuration The Teacher model (Qwen3-235B-A22B-Instruct-2507) serves as an LLM judge for semantic validation when rule-based extraction is insufficient or ambiguous. The judge evaluates both numerical correctness and reasoning validity using structured prompts. We instructed the evaluation prompt to: (1) Verify the final numerical answer against the expected result, (2) Assess the logical coherence of the reasoning steps, (3) Identify any mathematical errors or invalid assumptions, (4) Provide binary correctness judgments with brief justification. We ensured judge reliability through temperature 0.1 sampling for consistent evaluations and validation against human expert annotations on subset of problems. The dual-verification approach (MathRule + LLM judge) provides reliable automated assessment for large-scale evaluation. Preprint. Under Review. PSEUDO CODE OF SOCRATIC-ZERO The pseudo code of Socratic-Zero is provided in Algorithm 1. Algorithm 1 Socratic-Zero Co-evolutionary Learning 1: Require: Initial curriculum D0, initial Solver parameters θ(0) fixed Teacher (V, G), total iterations , attempts per problem k. , initial Generator parameters θ(0) , 2: Initialize Solver πθS πθ(0) 3: for = 0 to 1 do and Generator πθG πθ(0) . 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: Phase 1: Online Solver Evolution Set reference policy πθref πθ(t) . Initialize preference data Pt and failure set Ft . for each problem in curriculum Dt do Generate solution attempts {y(i) }k Construct winning set Yw(q) and losing set Yl(q) using Teacher verifier . Collect preference pairs: Pt Pt {(yw, yl) yw Yw(q), yl Yl(q)}. Collect failures: Ft Ft {(q, yl) yl Yl(q)}. i=1 πθ(t) ( q). end for Update Solver parameters θ(t+1) Phase 2: Offline Generator Evolution & Curriculum Expansion Generate new problem-solution pairs with the Teacher: Dnew {G(q, yfail) (q, yfail) Ft}. Construct Generator training data DG . for each new problem (q, Adam(θS LDPO(θ(t) ref) generated from (q, yfail) do ; Pt, θref)). Estimate utility (qπθ(t+1) Add weighted training example to dataset: DG DG {(q, yfail, q, (q))}. ) via rollouts with the updated Solver. end for Update Generator parameters θ(t+1) Phase 3: Curriculum Update Augment the curriculum for the next iteration: Dt+1 Dt {(q, G Adam(θG LWSFT(θ(t) ; DG)). 20: 21: end for 22: Output: Trained Solver policy πθ(T ) and Generator policy πθ(T ) . ref) (q, ref) Dnew}."
        },
        {
            "title": "I PROBLEM QUALITY CONTROL MECHANISM",
            "content": "To ensure curriculum integrity and prevent the propagation of erroneous problems, we implemented comprehensive quality control mechanism that monitors problem validity through Solver performance feedback and automated verification. Teacher Self-Verification Protocol When the Teacher model evaluates Solver attempts and finds that all = 8 solutions for given problem are incorrect (success rate jp = 0), this triggers an automatic quality verification process. The system recognizes that universal failure may indicate either: (1) the problem exceeds current Solver capability (expected behavior), or (2) the problem itself or its reference solution contains errors (quality issue). The Teacher model performs self-verification by re-examining both the problem statement and its originally provided reference solution. This involves: (1) Re-solving the problem independently with temperature 0.1 for consistency, (2) Cross-validating the reference solution against the new solution attempt, (3) Checking for mathematical consistency, ambiguous problem statements, or computational errors, (4) Verifying that the problem has unique, well-defined solution. Problem Filtering and Exclusion We immediately flagged and excluded problems that fail the self-verification process from further curriculum evolution. Specifically, we discarded problems if: (1) The Teacher cannot reproduce its own reference solution, (2) Multiple valid interpretations of the problem statement exist, (3) Computational errors are detected in the reference solution, (4) The problem lacks sufficient information for unique solution. 19 Preprint. Under Review. Table 9: Solver Mean Reward Evolution Across Training Stages"
        },
        {
            "title": "Stage",
            "content": "S1 S2 S"
        },
        {
            "title": "Trend",
            "content": "Mean Reward (%) 52.1 48.7 50.1 then Table 10: Generator Reward Distribution Analysis"
        },
        {
            "title": "Stage",
            "content": "High Reward Problems (%) Target Range (45-55%) S1 50.7 S2 49.4 S"
        },
        {
            "title": "Stability",
            "content": "50.2 Maintained"
        },
        {
            "title": "Stable",
            "content": "We removed discarded problems from the active curriculum Ct and they do not contribute to subsequent Solver training or Generator learning. This prevents the accumulation of low-quality problems that could degrade training effectiveness or introduce systematic biases. MathRule Integration for Contamination Minimization The integration of MathRule answer extraction serves as an additional quality control layer by providing objective, rule-based verification independent of LLM judgment. When MathRule successfully extracts clear numerical answer from the Solvers solution, this extraction is compared against the reference answer using standardized formats. This dual-verification approach (MathRule + Teacher evaluation) minimizes contamination from: (1) LLM judge inconsistencies or biases, (2) Format-related misinterpretations, (3) Numerical precision issues, (4) Ambiguous answer representations. Problems where MathRule and Teacher evaluations consistently disagree trigger additional quality review, as such disagreements often indicate underlying issues with problem clarity or reference solution accuracy. Feedback-Driven Quality Monitoring The system continuously monitors curriculum quality through Solver performance patterns. We flagged problems that consistently produce anomalous resultssuch as sudden performance drops across multiple Solver variants or inconsistent difficulty ratingsfor manual review or automatic exclusion. This feedback-driven approach ensures that quality control adapts to emerging issues and maintains curriculum integrity throughout the co-evolutionary training process, preventing the accumulation of problematic content that could compromise learning effectiveness."
        },
        {
            "title": "J CURRICULUM STABILITY AND DIVERSITY ANALYSIS",
            "content": "We analyzed the curriculum evolution dynamics across two dimensions: difficulty progression and problem diversity preservation. Solver Performance Evolution Table 9 tracks the Solvers mean reward (correctness rate) across training rounds, revealing adaptive curriculum difficulty. Solver Performance Evolution Table 9 tracks the Solvers mean reward (correctness rate) across training stages, revealing adaptive curriculum difficulty. The Solver exhibits characteristic performance decline from Stage 1 (52.1%) to Stage 2 (48.7%) followed by recovery in Stage 3 (50.1%) as shown in Table 9. This pattern reflects adaptive curriculum generation where the Teacher progressively increases difficulty faster than Solver capability initially improves, then the Solver begins adapting to enhanced curriculum complexity. Generator Stability Table 10 examines reward distribution in Generator training. Preprint. Under Review. As demonstrated in Table 10, the Generator maintains remarkable stability with high-reward problems consistently around 50%, fluctuating within only 1.3% range. This indicates successful learning of the optimal difficulty zone defined by the Gaussian reward function with parameters µ = 0.5 and σ = 0.2 as specified in Table 7. Generator Stability Table 10 examines reward distribution in Generator training. As demonstrated in Table 10, the Generator maintains remarkable stability with high-reward problems consistently around 50%, fluctuating within only 1.3% range. This indicates successful learning of the optimal difficulty zone defined by the Gaussian reward function with parameters µ = 0.5 and σ = 0.2 as specified in Table 7. Problem Diversity Three key mechanisms ensure curriculum diversity throughout training: Multi-domain initialization: The 100 seed problems span all 7 MATH subjects (Algebra, Number Theory, Geometry, etc.) across difficulty levels 2-4 as detailed in Table 8, providing diverse starting points for curriculum evolution. High-temperature sampling: We employed temperature 0.8-0.9 sampling at three critical stages: (1) Solver trajectory generation during curriculum advancement, (2) Teacher error analysis for varied failure interpretation, and (3) Teacher problem generation for diverse enhancement strategies. Compounding diversity effects: Multi-domain seeds combined with stochastic sampling create diverse failure patterns, while high-temperature generation ensures varied problem formulations even from similar error patterns."
        },
        {
            "title": "K GENERALIZABILITY OF PROBLEM GENERATION CAPABILITIES",
            "content": "A key question emerging from our work is whether the Generators learned problem creation abilities can transfer to domains beyond mathematical reasoning. The value function and curriculum evolution mechanisms developed in Socratic-Zero are domain-agnostic in principle, suggesting potential for broader applicability. The Generator learns fundamental skills in difficulty calibration, error pattern recognition, and targeting that may generalize across reasoning domains. For instance, the ability to identify when problem is appropriately challenging (around 50% success rate as shown in Table 10) represents meta-cognitive skill applicable to logical reasoning, scientific problem-solving, or even creative tasks. The Gaussian reward function with µ = 0.5 and σ = 0.2  (Table 7)  creates transferable framework for difficulty calibration that could adapt to other domains by adjusting the target success rate parameter. Our Generators superior performance compared to much larger models, achieving 37.72% downstream utility versus 37.13% from Qwen3-235B-A22B  (Table 5)  , demonstrates that strategic specialization can outperform raw parameter scaling. This suggests that domain-specific Generator training could be effective across various reasoning domains without requiring massive computational resources. However, domain transfer would require careful adaptation of the Teachers evaluation capabilities and problem generation templates. Mathematical reasoning benefits from relatively objective correctness criteria with our dual-verification approach (MathRule + LLM judge) achieving 94.2% agreement with human experts, while other domains may require more nuanced evaluation frameworks. The seed selection protocol detailed in Table 8, which ensures balanced coverage across seven mathematical domains, provides template for systematic domain expansion that could be adapted to physics, computer science, or other reasoning areas. Future work should investigate whether Generator trained on mathematical problems can effectively create challenging problems in adjacent domains like physics or computer science, potentially through few-shot adaptation or domain-specific fine-tuning leveraging the value learning mechanisms demonstrated in our framework. Preprint. Under Review."
        },
        {
            "title": "L FRAMEWORK SCALABILITY AND EXTENSIBILITY",
            "content": "The modular architecture of Socratic-Zero demonstrates strong potential for scalability and extension across multiple dimensions. The clear separation between Solver, Teacher, and Generator roles enables independent scaling and optimization of each component, as evidenced by our successful deployment across different computational configurations detailed in Table 7. The frameworks extensibility is particularly evident in its ability to accommodate different model architectures and scales. Our cross-model validation demonstrates consistent performance improvements: Qwen3-8B achieves 56.1% average accuracy (+20.2 points), while similar gains are observed on GLM4-9B and Qwen3-14B architectures  (Table 1)  . This cross-architecture consistency suggests the co-evolutionary principles transcend specific model families and could readily incorporate emerging architectures or specialized reasoning models. The curriculum evolution mechanism shows robust scalability properties. Starting from just 100 seed problems  (Table 8)  , the system generates thousands of ly valuable problems while maintaining quality, with our Generator achieving 95.6% validity rate compared to 89.1% from the base Qwen3-32B model  (Table 4)  . This demonstrates that the framework can scale curriculum generation without proportional increases in seed data requirements. Multi-domain extension represents another promising direction supported by our balanced seed distribution across seven mathematical domains. The current mathematical focus could expand to encompass multiple reasoning domains simultaneously, with domain-specific Teachers providing specialized curriculum generation while sharing the underlying co-evolutionary dynamics. The reward distribution analysis  (Table 10)  shows stable performance across training rounds, indicating the frameworks robustness to curriculum expansion. The framework also supports hierarchical scaling, where multiple Solver-Generator pairs could operate at different difficulty levels or specialization areas, coordinated by higher-level meta-learning mechanisms. The oscillatory convergence patterns observed in Table 9 suggest natural synchronization points where multiple agents could coordinate their learning phases."
        },
        {
            "title": "M CONVERGENCE AND THEORETICAL FOUNDATIONS",
            "content": "The theoretical understanding of multi-agent co-evolutionary learning remains an open challenge with significant implications for system reliability and predictability. Our empirical observations provide crucial insights into the convergence behavior of such systems. The oscillatory convergence patterns documented in Table 9 reveal characteristic dynamics: Solver performance declines from R1 (60.12%) to R4 (48.7%) followed by recovery in R5 (50.1%). This pattern reflects adaptive curriculum generation where the Teacher progressively increases difficulty faster than Solver capability initially improves, then the Solver adapts to enhanced curriculum complexity. These bounded oscillations suggest the system reaches dynamic equilibria rather than static optima. Complementing this, the Generator maintains remarkable stability with high-reward problems consistently around 50%, fluctuating within only 1.3% range across training rounds  (Table 10)  . This stability indicates successful learning of the optimal difficulty zone defined by the Gaussian reward function with µ = 0.5 and σ = 0.2  (Table 7)  , providing empirical evidence for convergence to ly meaningful equilibria. The cross-architecture consistency observed in Table 1, where similar improvement patterns emerge across Qwen3-8B, GLM4-9B, and Qwen3-14B models, suggests robust system-level properties that transcend specific model architectures. This consistency provides evidence that the convergence behavior represents fundamental properties of the co-evolutionary dynamics rather than architecturespecific artifacts. Future theoretical work should investigate conditions under which the system exhibits stable convergence versus chaotic dynamics. Key questions include: What curriculum evolution rates ensure stable learning? How do different value functions affect convergence properties? Can we establish bounds on the oscillation amplitude around target performance levels observed in our empirical data? Preprint. Under Review. The intersection of curriculum learning, preference optimization, and multi-agent dynamics presents rich opportunities for theoretical development. The DPO training parameters  (Table 7)  and their interaction with curriculum evolution rates could inform theoretical models of multi-agent preference learning. Establishing convergence guarantees would enable more principled hyperparameter selection and provide confidence bounds for practical deployment."
        }
    ],
    "affiliations": [
        "Alibaba Group Holding Limited",
        "EPIC Lab, Shanghai Jiao Tong University",
        "Shanghai University of Finance and Economics",
        "Wuhan University",
        "Zhejiang University"
    ]
}