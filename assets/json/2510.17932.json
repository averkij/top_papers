{
    "paper_title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
    "authors": [
        "Jiahao Tang",
        "Henry Hengyuan Zhao",
        "Lijian Wu",
        "Yifei Tao",
        "Dongxing Mao",
        "Yang Wan",
        "Jingru Tan",
        "Min Zeng",
        "Min Li",
        "Alex Jinpeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 2 3 9 7 1 . 0 1 5 2 : r FROM CHARTS TO CODE: HIERARCHICAL BENCHMARK FOR MULTIMODAL MODELS Jiahao Tang1 Henry Hengyuan Zhao2 Lijian Wu1 Yifei Tao3 Dongxing Mao1 Yang Wan1 Jingru Tan1 Min Zeng1 Min Li1 Alex Jinpeng Wang 1 1CSU-JPG, Central South University 2National University of Singapore 3Nanyang Technological University"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Chart2Code, new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on codebased evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code"
        },
        {
            "title": "INTRODUCTION",
            "content": "Charts are one of the most powerful tools for communicating complex ideas. From scientific publications to business reports, they distill large amounts of structured data into clear and persuasive visuals. With the rapid progress of large multimodal models (LMMs) (OpenAI, 2025; Anthropic, 2025), it becomes increasingly realistic to envision AI systems that not only interpret visual charts (Wang et al., 2024b) but also generate executable plotting code, task we refer to as chart-to-code (chart2code). Such capabilities can significantly enhance productivity by automating visualization creation, enabling reproducibility. Yet, the reality of how people use charts tells different story. Users rarely stop at simple chart reproductionthey need to edit figures by changing chart types, merging datasets, or adding new elements; they often work with long tables that must be distilled into interpretable plots; and they expect precise control over layout and style to ensure clarity. On the other hand, current LMMs (OpenAI, 2025; Anthropic, 2025; Deitke et al., 2024) achieve impressively high scores on existing chart2code benchmarks Yang et al. (2025a); Zhao et al. (2025b), suggesting that the problem is close to being solved. However, when applied to these more common and demanding scenarios, the very same models often struggle, revealing substantial gaps in their practical ability (refer to Appendix for examples). This discrepancy creates mismatch between reported benchmark Equal Contribution. Projecet lead Corresponding author. 1 Figure 1: Chart2Code covers three progressively challenging levels: reproduction, editing, and long-table to chart generation. It provides user-driven and diverse benchmark that better reflects real-world chart2code demands. performance and real-world utility, highlighting the need for benchmark that more comprehensively reflects everyday chart2code challenges. Motivated by this observation, we introduce Chart2Code (Figure 1), new benchmark designed to rigorously evaluate chart generation capabilities of LMMs under progressively challenging conditions. Chart2Code consists of three levels: Level 1 (Chart Reproduction) targets mimicking reference figure and instruction; Level 2 (Chart Editing) requires complex and precise editing, such as changing chart types or adding new elements; Level 3 (Long-Table to Chart Generation) presents the most demanding setting, where models must convert long, unprocessed data tables into faithful charts from user instructions. This hierarchical design reflects real-world usage while progressively increasing difficulty, and its distinctions from prior benchmarks are highlighted in Table 1. We comprehensively benchmark 25 state-of-the-art LMMs, including both proprietary and openweight models, across the three levels of Chart2Code. Our results show that while LMMs demonstrate promising capabilities on simple reproduction tasks, their performance deteriorates sharply on complex editing and long-context data-to-chart generation. Together, these findings reveal the unsolved challenges of chart2code generation and point to future directions for building more reliable visualization assistants. In summary, our contributions are threefold:① We present Chart2Code, the first hierarchical benchmark targeting chart2code generation with progressively more challenging tasks. ② We propose multi-level evaluation protocols that jointly assess code executability and visual fidelity, offering comprehensive lens on model performance. ③ We provide an extensive empirical study across 25 mainstream LMMs, yielding new insights into their strengths, weaknesses, and design trade-offs for chart generation. 2 Table 1: Comparison of existing chart-to-code benchmarks. Ref. Fig.: Reference Figure; Instr.: Instruction; Text Data: Text-format data; Fig. Data: Figure-format data; L1: Chart reproduction; L2: Chart editing; L3: Long-table-to-chart generation; NL: Natural language."
        },
        {
            "title": "Benchmark",
            "content": "CharXiv (Wang et al., 2024b) Plot2Code (Wu et al., 2025) AcademiaChart (Zhang et al., 2024) Chartmimic (Yang et al., 2025a) ChartEdit (Zhao et al., 2025b) Chart2Code (Ours)"
        },
        {
            "title": "Input Type",
            "content": "Instr. Text Data Fig. Data Task Cat. L1 L2 L3 Ref. Fig."
        },
        {
            "title": "Metric",
            "content": "Rule-based GPT-score"
        },
        {
            "title": "Code",
            "content": ""
        },
        {
            "title": "2 RELATED WORK",
            "content": "Large Multimodal Models. Thanks to the success of proprietary LMMs such as GPT-5 (OpenAI, 2025), Gemini-2.5-Pro (Comanici et al., 2025), and Claude-Sonnet-4 (Anthropic, 2025), we see the dawn of building AI agents for addressing realistic applications. In the academic community, we see enormous excellent open-source models: MiMo-VL (Xiaomi & Team, 2025), QwenVL-series (Bai et al., 2025; Wang et al., 2024a), and InternVL-series (Wang et al., 2025; Zhu et al., 2025), MolMo (Deitke et al., 2024), Kimi-VL (Team et al., 2025) LLaVA-series (Li et al., 2024a; Liu et al., 2024; Li et al., 2024b), Deepseek-VL (Lu et al., 2024), and GLM-4V (GLM et al., 2024). Agentic Benchmarks. The rapid progress of foundation LLMs and LMMs has motivated the creation of diverse agentic benchmarks, spanning GUI automation (Xie et al., 2024; Zhao et al., 2025a; Lin et al., 2024; Koh et al., 2024), agentic coding (Jimenez et al., 2024; Yang et al., 2025b), tool use (Yao et al., 2025), AI research assistance (Nathani et al., 2025), and chart reasoning (Wang et al., 2024b). We focus on chart2code, practical task central to everyday workflows for researchers and professionals. Despite progress, even the best proprietary LMMs still fail to generate faithful charts from long, raw tables, underscoring the need for future modeling advances. Chart Understanding to Code Generation. Chart understanding has evolved through series of benchmarks that progressively expand task complexity. ChartQA (Masry et al., 2022) first established large-scale visual question answering over charts, combining queries with logical and visual reasoning. ChartXiv (Wang et al., 2024b) advanced this line by introducing scientific charts with expert-designed questions, further exposing the gap between multimodal models and human performance. Moving beyond QA, Chart2Code benchmarks address faithful chart generation. ChartMimic (Yang et al., 2025a) formalized this by requiring code synthesis from chart images and instructions, while ChartEdit (Zhao et al., 2025b) emphasized interactive modification, where models must edit chart-rendering code following natural-language instructions. Extending chart generation more generally, StarVector (Rodriguez et al., 2025) proposed vision-language approach to directly produce scalable vector graphics from visual or textual inputs. Although GPT-4o achieves high scores on ChartMimic (83.2) and ChartEdit (93.6), it still struggles with realistic chart2code tasks, motivating new, more challenging benchmark for reliable evaluation."
        },
        {
            "title": "3.1 TASK DEFINITION OF CHART2CODE",
            "content": "Chart2Code can be represented as: = (R, I, D) where, is the reference chart (e.g., screenshot), is the instruction and is the Python code generated by LMM (f ). represents optional input data types, Chart2Code supports three kinds of data formats: textual data, image data (e.g., screenshot), and Excel files. To ensure rigor and comprehensiveness, we designed three tasks of increasing difficulty. Level 1 (Chart Reproduction): This task consists of two subsettings. The first setting requires the LMM to directly generate the executable code that can reproduce the reference chart (R). This task primarily explores the models visual understanding capabilities. The second setting requires the LMM to extract the required table data from the data file and generate Python code based on the 3 style and format of the given reference chart (R). It is closely aligned with real-world chart creation needs and not included in previous studies (Yang et al., 2025a; Wu et al., 2025; Zhang et al., 2024). Level 2 (Chart Editing): At this level, the LMM edits the reference chart (R) as instructed, with operations like style changes, type swaps, data edits, or multi-subplot generation. The LMM is expected to generate code that meets the editing requirements and adheres to the style and format of chart. Level 3 (Long-Table to Chart Generation): The final level asks the LMM to accurately gather the target data points from the extremely long data and unprocessed sheet and then produce the executable code, referencing the style and format of the given reference chart (R). It is the hardest task, which targets the most realistic scenario in data visualization or business presentations, assuming the user is not data visualization expert."
        },
        {
            "title": "3.2.1 DATA CURATION",
            "content": "Chart Data: Our chart figure sources primarily consist of three aspects. First, we collected approximately 5,000 paper charts from Arxiv, spanning from January 2024 to July 2025, covering various fields such as CSEE, Physics, Statistics, and Economics, to ensure diversity and modernity in the chart types. Second, we gathered 1,000 example charts from function libraries such as Matplotlib, Seaborn, WordCloudX, Scipy, as well as Matlab plotting example tutorials. Finally, we filtered 300 difficult charts from the ChartMimic (Yang et al., 2025a) dataset. Raw Data: Our benchmark collects raw data from sources such as Kaggle, Annual Reports, and publicly available data from various company websites. The raw data includes Excel spreadsheets, figures, text, and other formats, covering multiple domains such as corporate financial reports, flight route data, weather data, GDP data, and car sales figures. Additionally, we have intentionally selected data of varying lengths to test the LLMs ability to analyze and process long text data."
        },
        {
            "title": "3.2.2 DATA FILTERING",
            "content": "Chart Data: We propose gathering-distribution data selection process. First, we gather data from various sources into chart pool, which is then roughly filtered by 10 undergraduate computer science students based on chart type and information complexity. Based on this initial selection, we reduce the data to 3,000 charts to ensure that the resulting data contains diverse range of visual elements and chart types. Next, the gathered data is distributed by category to 5 experts with many years of experience in Python plotting for independent evaluation. The evaluation criteria are refined into three dimensions: data complexity, visual complexity, and programming complexity. Each dimension is independently assessed to select more valuable charts as part of the benchmark data. Finally, the charts from various categories are aggregated to form the 719 reference figures in the benchmark. Raw Data: Since the raw data we collected contains various data formats, we first use automated scripts to filter out the raw data that exhibits rich numerical performance and is suitable for plotting. After that, we conduct manual checks to preserve the diversity of the raw data as much as possible. The final selection includes 39 Excel files, 80 raw data figures, and 36 raw data text files."
        },
        {
            "title": "3.2.3 DATA ANNOTATION",
            "content": "During the data annotation process for the three-level tasks, we employed an interactive data annotation method based on Python scripts and agents, which we refer to as the human-AI interactive annotation process. Specifically, in the level 1 data annotation process, annotators, with the assistance of the LMM, recreate the selected data by writing Python code. The data generated here directly serves as the first setting of the Level 1 task. Subsequently, based on the 719 scripts, annotators select and modify suitable chart types using the data from the 80 raw table figures and 36 raw table text files, resulting in 108+36 customized entries for the second setting of the task. In the Level 2 annotation process, annotators first categorize and summarize chart editing operations commonly encountered in real-world scenarios. They then modify the code with the help of prompt engineering and Python code injection, leveraging the programming capabilities of LLM. While the 4 Figure 2: Collected charts distribution. Table 2: Deatiled data statistic."
        },
        {
            "title": "Statistic",
            "content": "GT Charts Total charts - Level 1 / 2 / 3 charts Unique charts - Unique Level 1 / 2 / 3 charts Instructions Total instructions - Level 1 / 2 / 3 instructions Unique instructions - Unique instructions - Level 1 / 2 / 3 Maximum instruction length - Level 1/ 2 / 3 Average instruction length - Level 1 / 2 / 3 GT Code (Lengths/Lines) Maximum code length - Level 1 / 2 / 3 Average code length - Level 1 / 2 / 3 Maximum code lines - Level 1 / 2 / 3 Average code lines - Level 1 / 2 / 3 Extremely Long-Table Data Total Excel files Average lines per file Maximum lines Average data entries Maximum data entries"
        },
        {
            "title": "Number",
            "content": "2,023 863 / 1,010 / 150 804 719 / 0 / 85 2,509 899 / 1,010 / 150 1,305 145 / 1,010 / 150 224 / 544 / 390 137.8 / 307.6 / 178.9 96,563 / 7,855 / 790,130 2,621.6 / 2,880.6 / 29,899.8 842 / 219 / 388 69.9 / 82.9 / 51.3 39 1,063.6 10,960 810,329.3 51,391 LLM may lack proficiency in the chart2code task, its programming ability is exceptional. Through this process, we obtained over 4,700 edited and modified scripts, which were further filtered through the data selection process, ultimately yielding 1,010 high-quality Level 2 data entries. For Level 3 data annotation, annotators first analyzed the content of the 39 diverse data tables, formulated statistical data requirements, and extracted and processed the data from the tables. This process resulted in 150 Level 3 data entries."
        },
        {
            "title": "3.3 DATA STATISTICS AND ANALYSIS",
            "content": "Chart2Code comprises 2,023 tasks across three levels863/1,010/150 for L1/L2/L3spanning 22/19/12 chart families (e.g., radar, heatmap, scatter, box, tree, error-bar, pie, multidiff; see Fig. 2). To maximize diversity, Level 1 emphasizes unique charts (719 unique). Level 2 reuses Level 1 charts with at least one edit instruction per chart, resulting in 1,010 unique, non-duplicated edits. Level 3 (LT2Chart) includes 85 charts and 150 instructions derived from web-sourced long tables, making annotation and ground-truth code especially challenging. As summarized in Tab. 2, the instruction scale and substantial code lengths highlight the breadth and difficulty of Chart2Code."
        },
        {
            "title": "3.4 EVALUATION",
            "content": "To comprehensively evaluate the performance of various models on the Chart2Code benchmark, we first establish the code executability rate as the primary evaluation metric. This directly measures the models ability to generate functional visualization code, and its calculation is detailed in equation 1. Secondly, we introduce multi-level, multi-dimensional evaluation method to assess model performance at both the code-level and the chart-level. At the code-level, we propose base evaluation method that calculates the similarity of visual outcomes by parsing and matching matplotlib.Figure objects across eight dimensions. Our base evaluation method offers faster assessment, more comprehensive dimensions, and superior evaluation performance (see Appendix E.2 for details). Similarly, to provide broader code assessment, we employ GPT-5-mini (OpenAI, 2025) to score the code without execution, assessing its prospective visual output to derive comprehensive LLM-score (see Appendix E.3 for details). At the chart-level, we similarly use GPT-5-mini to assess the predicted charts, yielding an LMMscore. Although LLMs like GPT-5 may not excel at the Chart2Code generation task itself, they possess keen ability to judge the similarity between both code and charts. The direct evaluation of charts is most aligned with human intuition, making it more suitable as the final evaluation score. 5 Table 3: Evaluation results on Chart Reproduction (Level 1) with various LMMs. Each task includes reference chart as input. DR: input without the table data. CRD: input with customized text-format table data. CFD: input with customized figure-format table data. Exec. Rate: execution rate; We use GPT-5-mini as the base model for both LLM-score and LMM-score; Model Direct Reproduction(DR) Exec.Rate LLM-Score LMM-Score Customize Raw Data(CRD) Exec.Rate LLM-Score LMM-Score Customize Figure Data(CFD) Exec.Rate LLM-Score LMM-Score 0.3807 0.2553 0.3575 0.2341 0.8117 Proprietary 100 97.2 94.4 97.2 94. 0.6763 0.4878 0.6070 0.6325 0.6525 Open-Source LMMs (non-thinking) 0.0154 0.0376 0.0431 0.1374 0.0664 0.1207 0.0723 0.1463 0.1389 0.0459 0.1389 0.2316 0.1983 0.0994 0.1893 0.0943 0.2531 11.11 5.56 61.11 72.22 75.00 80.56 80.56 0 86.11 66.67 86.11 69.44 69.44 75.00 100 4.55 77.78 0.4225 0.4213 0.5374 0.5887 0.5950 0.6082 0.5712 - 0.6169 0.5628 0.6169 0.6068 0.6237 0.5952 0.6273 0.2400 0.2546 Open-Source LMMs (thinking) 0.2294 0.2130 0.2730 69.44 86.11 72.22 0.6053 0.6644 0.3367 0.2661 0.236 0.2238 0.2662 0.2503 0.1550 0.0825 0.1114 0.2081 0.1367 0.1628 0.1183 - 0.1732 0.1183 0.1732 0.2421 0.1852 0.1515 0.1989 0.4600 0.2368 0.2582 0.2248 0. 87.04 88.89 85.19 65.74 83.96 0 0 10.19 61.11 30.56 51.85 37.74 0 57.41 44.74 57.41 41.67 46.30 44.44 37.96 0.97 70.37 33.33 38.89 39.81 0.6145 0.5538 0.6082 0.5756 0.5978 - - 0.2539 0.4641 0.4235 0.5518 0.5715 - 0.4450 0.2904 0.4450 0.4962 0.5155 0.5952 0.5532 0.0500 0.2412 0.5807 0.5578 0. 0.2214 0.2273 0.2382 0.1962 0.2075 - - 0.0145 0.1379 0.0519 0.1373 0.0568 - 0.1028 0.0130 0.1028 0.1407 0.1732 0.0910 0.1688 0.4100 0.2698 0.2172 0.1455 0.2780 Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed-1.5-VL Seed-1.6-VL LLaVA-OV-Qwen2-7B-SI LLaVA-OV-Qwen2-7B-OV DeepSeek-VL-7B kimi-VL-A3B Qwen2-VL-7B Qwen2-VL-72B InternVL-2.5-8B InternVL-2.5-38B InternVL-3-8B GLM-4V-9B Intern-VL-3.5-8B MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen2.5-VL-7B Qwen2.5-VL-72B Molmo-7B-D Qwen3-VL-30B MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen3-VL-30B 90.4 96.38 87.48 85.81 84.70 32.82 11.13 48.68 68.85 64.39 75.66 66.89 86.23 66.34 72.18 66.34 37.83 44.65 65.64 65.36 34.77 64.67 55.77 50.35 45.06 0.6286 0.5629 0.6334 0.5536 0.5237 0.1820 0.2651 0.2854 0.4409 0.3364 0.4368 0.3348 0.4577 0.4371 0.2881 0.4371 0.5439 0.4959 0.4197 0.5118 0.2164 0.5293 0.5261 0.6555 0."
        },
        {
            "title": "4.1 EXPERIMENTS SETUP",
            "content": "Models. We conducted tests on 25 widely-used open-source models and proprietary models to evaluate their performance on our benchmark. For the open-source models, we selected 12 representative vision-language models, with total parameters ranging from 7B to 72B, including: Qwen2-VL (7B, 72B), Qwen2.5-VL (7B, 72B), Deepseek-VL (7B), Kimi-VL (7B), MiMo-VL-SFT (7B), MiMo-VLRL (7B), InternVL-2.5 (8B, 38B), InternVL-3 (8B, 38B), InternVL-3.5 (8B, 38B), GLM-4V (9B), LLAVA-onevision-si (7B), LLAVA-onevision-ov (7B), Molmo (7B),Qwen3-VL. For proprietary models, we selected the five most popular multimodal large models, including: Gemini-2.5-pro, Claude-sonnet-4, GPT-5, Seed-1.5-VL, and Seed-1.6-VL. Configuration. All experiments were conducted on NVIDIA V100 GPUs. Qwen2-VL-7B and Qwen2.5-VL-7B models were executed on single GPU. MiMo-VL-SFT, MiMo-VL-RL, and LLaVA-OneVision (LLaVA-OV) required two GPUs, with inference parallelized across devices due to memory constraints. Similarly, the InternVL series (2.5-VL-8B, 3-VL-8B, 3.5-VL-8B), Kimi-VL, DeepSeek-VL, and GLM-4V models were evaluated using two GPUs with model parallelism. We set the maximum output length to 8,192 tokens for Level 1 and 2, and 32,768 tokens for Level 3. Empirically, non-thinking models required only 4,096 tokens, with negligible truncation except for the largest InternVL-3.5-38B model. The decoding temperature was fixed at 0.1 across all models. To preserve visual fidelity, we fed images at their native resolution and used the maximum input pixel setting supported by each model to ensure complete processing of chart details."
        },
        {
            "title": "4.2.1 LEVEL-WISE COMPARISON OF MODELS",
            "content": "Level 1. As shown in Tab. 3, proprietary models lead across Direct Mimic (DM), Customize Raw Data (CRD), and Customize Figure Data (CFD), achieving high executability but only moderate visual fidelityfor example, Gemini-2.5-Pro reaches 90.4/100/87.04% ER on DM/CRD/CFD while LMM-Scores stay around 0.220.38. CRD is easy to run (e.g., Gemini and Qwen2.5-VL-72B at 100% ER) yet still low-fidelity (0.150.27), confirming execution = fidelity. CFD is the hardest: top proprietary models keep 85% ER but LMM-Scores remain 0.220.24, and many 6 Table 4: Evaluation results on Chart Editing (Level 2) with various LMMs."
        },
        {
            "title": "Type",
            "content": "LLM-Score Code-Level Chart-Level LMM-Score"
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed-1.5-VL Seed-1.6-VL LLaVA-OV-Qwen2-7B-SI LLaVA-OV-Qwen2-7B-OV DeepSeek-VL-7B kimi-VL-A3B Qwen2-VL-7B Qwen2-VL-72B InternVL-2.5-8B InternVL-2.5-38B InternVL-3-8B InternVL-3-38B GLM-4V-9B Intern-VL-3.5-8B MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen2.5-VL-7B Qwen2.5-VL-72B Molmo-7B-D Qwen3-VL-30B Exec. Rate 90.30 91.19 90.58 63.17 72.38 1.19 2.57 21.68 49.5 24.95 55.05 21.29 68.22 4.55 67.43 10.69 27.23 20.59 21.88 33.36 71.49 0.99 41.39 0.6217 0.5737 0.5812 0.5106 0. 0.3507 0.3163 0.2523 0.3901 0.2846 0.4013 0.3341 0.4544 0.3491 0.4720 0.2011 0.4015 0.4378 0.4325 0.3524 0.5018 0.2471 0.54 0.8842 0.8110 0.8467 0.8230 0.8013 0.6964 0.6013 0.6206 0.7270 0.5825 0.7704 0.7002 0.7902 0.5914 0.7853 0.6910 0.7350 0.8462 0.7506 0.7374 0.8229 0.8152 0."
        },
        {
            "title": "Proprietary",
            "content": "0.9613 0.9587 0.9499 0.9538 0.9471 0.5093 0.4714 0.4835 0.4408 0.4714 0.5170 0.4776 0.4815 0.4582 0.4453 0.7560 0.6736 0.7047 0.6983 0.6884 Open-Source LMMs (non-thinking) 0.7833 0.6863 0.7350 0.9074 0.7711 0.9044 0.8362 0.9405 0.9447 0.9410 0.7794 0.9056 0.9205 0.8941 0.8592 0.9509 0.5636 0. 0.4074 0.4488 0.2436 0.3411 0.2723 0.3464 0.3148 0.4146 0.3389 0.4133 0.2357 0.3566 0.4201 0.3823 0.3296 0.4467 0.1000 0.4623 0.3002 0.2030 0.1820 0.3196 0.2385 0.3345 0.2955 0.3745 0.3645 0.3994 0.2196 0.3718 0.4231 0.4035 0.3302 0.4242 0.2275 0.4501 0.5249 0.5685 0.4031 0.5724 0.4693 0.6086 0.5421 0.6334 0.5561 0.6525 0.4604 0.6121 0.6505 0.6431 0.5944 0.6673 0.3477 0.6911 Open-Source LMMs (thinking) 0.6330 0.5869 0.6096 0.7166 0.7312 0.4871 0.4928 0.4538 0.5913 0.4883 0.5744 0.5530 0.6361 0.5421 0.6538 0.5003 0.6505 0.6666 0.6564 0.5780 0.6815 0.3082 0. 0.9636 0.9563 0.9581 0.9400 0.9431 0.7889 0.8154 0.7922 0.9033 0.8141 0.9098 0.8536 0.9338 0.8556 0.9235 0.7472 0.8998 0.9200 0.9405 0.8887 0.9348 0.3476 0.9384 MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen3-VL-30B 27.62 24.16 42.38 0.5076 0.4562 0.5213 0.7560 0.7404 0. 0.9449 0.9286 0.9549 0.4109 0.3686 0.4718 0.4379 0.3980 0.4453 0.7006 0.6812 0.6924 0.6859 0.6617 0.7046 0.9446 0.9385 0. 0.5742 0.5317 0.5663 0.5126 0.5151 0.3157 0.3512 0.2583 0.3701 0.3181 0.3928 0.3344 0.4311 0.3419 0.4528 0.2953 0.3964 0.4615 0.4459 0.3603 0.4739 0.3488 0.3611 0.4819 0.4496 0.4947 0.2459 0.2147 0.2506 0.1975 0.1863 0.0875 0.0366 0.0433 0.1039 0.0780 0.1140 0.0869 0.1367 0.0943 0.1476 0.0770 0.1466 0.1573 0.1399 0.0974 0.1684 0.1347 0.2257 0.1737 0.1774 0. Table 5: Evaluation results on Long-Table to Chart task (Level 3) with various LMMs."
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed-1.5-VL Seed-1.6-VL Exec. Rate 29.33 38.00 38.00 18.67 40."
        },
        {
            "title": "Type",
            "content": "LLM-Score Code-Level Figure-Level LMM-Score"
        },
        {
            "title": "Proprietary",
            "content": "0.7276 0.5676 0.5676 0.7252 0.7030 0.9733 0.7963 0.7963 0.8929 0.8833 1.0000 1.0000 1.0000 1.0000 1.0000 0.7727 0.8148 0.8148 0.8869 0.7972 0.6701 0.3731 0.3731 0.5502 0.5396 0.7880 0.5881 0.5881 0.7182 0. 0.8291 0.7175 0.7175 0.7804 0.8128 0.9470 0.9062 0.9062 0.9690 0.9244 0.3516 0.5125 0.5125 0.0000 0.0000 0.0361 0.007 0.0362 0.0611 0.0547 open-source models drop sharply (some 0 ER). Larger open-source backbones (Qwen2/2.5-VL-72B, InternVL-3-8B/38B) close part of the execution gap but not the fidelity gap. notable outlier is Seed-1.6-VL with DM LMM-Score 0.812, suggesting evaluator/model calibration effects. Level 2. The results are presented in Tab. 4. Proprietary models sustain 90% ER (Gemini 90.49, Claude 90.92, GPT-5 90.59) and excel on code-level subskillsespecially Layout/Type 0.95 0.96yet figure-level remains modest (0.180.22), evidencing persistent gap between syntactic compliance and rendered-image fidelity. Strong open-source systems improve executability (e.g., Qwen2.5-VL-72B 71.89%) with solid code-level scores (Layout 0.94, Type 0.92), but figure-level still lags (0.120.14). Smaller backbones struggle (e.g., LLaVA-OV-Qwen2-7B variants 2.71% ER). Thinking helps procedure more than pixels: MiMo-VL-7B-RL ER improves 16.5428.32, and MiMo-VL-7B-SFT figure-level nudges 0.12030.1367, but absolute fidelity remains low; the unusually high 0.4713 figure-level for MiMo-VL-7B-RL (non-thinking) merits. Level 3. Tab. 5 presented the results. Coverage is limited because the benchmark is very hard: only couple of open-source models could even complete inference, and on the proprietary side, five models were run, but overall ER is still <50%, primarily due to long-context inputs exceeding the maximum input limits. Among those that ran, ER drops to 2940% (e.g., Gemini 29.33%), while code-level stays strong (Layout = 1.0; high Grid/Type), indicating structurally plausible code under long context. However, figure-level fidelity collapses (Gemini 0.0361, Claude 0.007, GPT-5 0.0362; Seed-1.5/1.6-VL 0.061/0.055), showing that turning lengthy raw tables into pixel-accurate charts is the main bottleneck; the Seed rows also show LLM-Score = 0 with non-zero LMM-Score, hinting at evaluator/model coupling or edge-case artifacts that warrant robustness checks. 7 Figure 3: Correlation of the model performance (i.e, LMM-score) on different manually annotated difficulty levels (i.e., Easy, Medium, Hard) on Level 1, 2, 3, respectively. Figure 4: Left: Both proprietary and open-source models generalize well on Level 1 and Level 2 tasks when calculating the LLM-score for predicted code assessment. Right: Proprietary models tend to obtain higher LMM-scores on the Level 1 task rather than the Level 2, while open-source models perform poorly on both tasks (scores are lower than 0.5)."
        },
        {
            "title": "4.2.2 ANALYSIS",
            "content": "Execution vs. Complexity: From level 2 to Level 3, ER for proprietary systems drops from 90% in Tab. 4 to 2940% on Level 3 (Gemini 29.33, Claude 38.00, GPT-5 38.00 in Tab. 5). This mirrors the jump in reasoning load (long-context/table parsing, multi-constraint edits), showing that being able to run code at level 2 does not translate to robust end-to-end success at Level 3. We concluded execution success declines steeply with task complexity, even for top proprietary models. Code vs. Visual Fidelity: On level 2 (Tab. 4), proprietary models score very high on Layout/Type (e.g., Gemini 0.9606/0.9638, Claude 0.9591/0.9575, GPT-5 0.9509/0.9602), yet figure-level GPTScore is only 0.180.22 (Gemini 0.2134, Claude 0.1844, GPT-5 0.2201). On Level 3 (Tab. 5), the gap widens: code-level remains strong (e.g., Layout = 1.0000 across models), but LMM-Score collapses (Gemini 0.0361, Claude 0.007, GPT-5 0.0362, Seed1.5/1.6-VL 0.0611/0.0547). This demonstrates that while code-level compliance is generally high, it does not guarantee pixel-level visual correctness, making figure-level fidelity the primary bottleneck. Chart Reproduction Challenge: In Tab. 3, proprietary models still execute but with lower fidelity (e.g., Gemini CFD ER 87.04 with LMM-Score 0.22; Claude 88.89/0.227; GPT-5 85.19/0.238). Open-source models suffer larger drops (e.g., InternVL-3-8B 57.41/0.103, Qwen2-VL72B 51.85/0.137; several models hit 0 ER). Compared to DM/CRD in the same table, CFD exposes weaknesses in axis/series alignment, legend consistency, scaling, and style carry-over. We concluded reproducing existing charts (CFD) is the hardest subtask in Level1. Scaling Open-Source Backbones: In Tab. 4, Qwen2.5-VL-72B reaches 71.89 ER with strong code-level, yet figure-level is only 0.1437; InternVL-3-38B shows 61.51 ER and similar codelevel strength (Layout 0.9406, Type 0.9216), but figure-level remains 0.1205. This contrasts with 8 proprietary models 90% ER and still-low figure-level (0.180.22), underscoring that fidelity, not executability, is the persistent gap. These result shows larger open-source backbones close part of the execution gap on level 2, but figure-level fidelity gains are modest. Thinking Helps Procedural Compliance: On level 2 (Tab. 4), MiMo-VL-7B-RL ER rises from 16.54 28.32 when enabling thinking; MiMo-VL-7B-SFT nudges 22.27 23.57. LLM-side (code-level GPT-Score) also improves slightly. However, figure-level remains low or mixed (e.g., MiMo-SFT 0.1203 0.1367; MiMo-RL thinking row lacks figure-level). The net effect suggests that chain-of-thought/planning aids procedural compliance, yet post-render pixel-level exactness requires additional mechanisms (e.g., render-then-verify loops). This indicates Thinking variants help instruction following and executability, but visual fidelity improvements are inconsistent. Metric Sensitivity: In Level 1 (Tab. 3), Seed-1.6-VL shows an unusually high DM LMM-Score 0.812, far above peers. In level 2 (Tab. 4), MiMo-VL-7B-RL (non-thinking) reports an unusually high figure-level 0.4713, exceeding proprietary models (0.180.22). In Level 3 (Tab. 5), Seed1.5/1.6VL LLM-Score = 0.0000 despite non-zero LMM-Scores (0.0611/0.0547). These inconsistencies motivate robustness checks (multi-crop/image-space perturbation, secondary scorers, human spotchecks) and discussion on metric sensitivity to style choices. Several metric anomalies indicate evaluator calibration and modelevaluator coupling effects that merit auditing. Table-to-Chart Gap: On Level 1 CRD (Tab. 3), multiple models achieve very high ER (e.g., Gemini 100; Qwen2.5-VL-72B 100), yet LMM-Score remains low ( 0.150.27 across models). On level 2 (Tab. 4), code-level Data/Text/Type scores are solid for leading models (e.g., Gemini 0.756/0.620/0.964, GPT-5 0.704/0.596/0.960), but figure-level stays around 0.180.22, highlighting the gap between semantic correctness and visual exactness. Table to chart is relatively easy to execute but still hard to render faithfully."
        },
        {
            "title": "4.3 DISCUSSION.",
            "content": "Model Performance Across Manually Defined Difficulty Levels. In this experiment, we ask the human labeler to split each level into easy, medium and hard, in total three levels, and each subset contains 30 samples. As shown in Fig. 3, model performance exhibits clear correlation with manually annotated difficulty levels across all benchmark stages. On Level 1, proprietary models (e.g., GPT-5, Gemini-2.5-Pro, Claude-Sonnet-4) maintain relatively strong scores across Easy, Medium, and Hard subsets, though the overall fidelity remains moderate. In contrast, most open-source models show low scores and struggle particularly on harder cases. On Level 2, performance declines noticeably even for proprietary models, with overall scores dropping to 0.200.26 and sharper degradation from Easy to Hard, indicating sensitivity to increased editing complexity. By Level 3, almost all models fail regardless of difficulty level: LMM-scores converge near zero, showing that long-context table-to-chart generation overwhelms current models. These trends suggest that while models can partially track difficulty scaling on simpler tasks, the hardest scenarios effectively collapse their ability to produce faithful visualizations. Code Generalization Holds, Visual Fidelity Lags. As shown in Fig. 4, the performance trends differ substantially when measured by LLM-score versus LMM-score. On the left, both proprietary and open-source models generalize reasonably well from Level 1 to Level 2 when evaluated with LLM-score, indicating that code-level syntax and structure can often be preserved across tasks. On the right, however, the LMM-score reveals sharper divide: proprietary models achieve relatively higher visual fidelity on Level 1 than on Level 2, whereas open-source models perform poorly on both levels, with most scores remaining below 0.5. This contrast highlights that while models can maintain code-level compliance, translating such compliance into pixel-level faithful renderings remains key unsolved challenge, particularly for open-source systems."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "We presented Chart2Code, hierarchical benchmark for chart-to-code generation that spans three progressively challenging levels: chart reproduction, chart editing, and long-table to chart generation. Our large-scale evaluation of 25 state-of-the-art LMMs shows clear trend: while current models manage simple reproduction reasonably well, they struggle with complex editing and long-context 9 visualization, exposing substantial gaps in practical capability. These findings underscore the unsolved challenges of chart-to-code generation and call for models with stronger reasoning, generalization, and robustness. Despite its contributions, Chart2Code has two key limitations. First, all tasks are currently in English; extending to multilingual chart2code remains an open and important direction. Second, our evaluation relies on large language models as judges to assess code correctness and visual fidelity. While this enables scalable and nuanced evaluation, it may introduce inaccuracies or biases compared to fully human assessment. Future work will explore multilingual expansion and more reliable evaluation protocols, further enhancing the benchmarks coverage and trustworthiness."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 4. Preprint, 2025. URL https://www.anthropic.com/ news/claude-4. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pp. arXiv2409, 2024. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. 10 Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b. Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen WU, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, and Mike Zheng Shou. VideoGUI: benchmark for GUI automation from instructional videos. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=jSKtxmxc0M. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, June 2024. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology.org/2022.findings-acl.177. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Mikhail Plekhanov, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Nicolaus Foerster, Yoram Bachrach, William Yang Wang, and Roberta Raileanu. MLGym: new framework and benchmark for advancing AI research agents. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=ryTr83DxRq. OpenAI. Gpt-5 system card. introducing-gpt-5/. Preprint, 2025. URL https://openai.com/index/ Juan A. Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images and text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1617516186, June 2025. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinhao Li, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yuhao Dong, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, and Zongyu Lin. Kimi-vl technical report, 2025. URL https://arxiv.org/abs/2504.07491. Seed Team. Seed1.6 tech introduction. Preprint, 2025. URL https://seed.bytedance. com/en/seed1_6. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 11 Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal LLMs. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https: //openreview.net/forum?id=cy8mq7QYae. Chengyue Wu, Zhixuan Liang, Yixiao Ge, Qiushan Guo, Zeyu Lu, Jiahao Wang, Ying Shan, and Ping Luo. Plot2Code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 30063028, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 9798-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.164. URL https://aclanthology. org/2025.findings-naacl.164/. LCT Xiaomi and Core Team. Mimo-vl technical report. arXiv preprint arXiv:2506.03569, 2025. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=tN61DTr4Ed. Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran XU, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, and Yujiu Yang. Chartmimic: Evaluating LMMs cross-modal reasoning capability via chart-to-code generation. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum?id=sGpCzsfd1K. John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. SWE-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2025b. URL https: //openreview.net/forum?id=riTiq3i21b. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. {$tau$}-bench: benchmark for underline{T}ool-underline{A}gent-underline{U}ser interaction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=roNSXZpUDN. Zhehao Zhang, Weicheng Ma, and Soroush Vosoughi. Is GPT-4V (ision) all you need for exploring vision-language models capability in automating academic data visualization? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen reproducing academic charts. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 8271 8288, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.485. URL https://aclanthology.org/2024. findings-emnlp.485/. Henry Hengyuan Zhao, Kaiming Yang, Wendi Yu, Difei Gao, and Mike Zheng Shou. Worldgui: An interactive benchmark for desktop gui automation from any starting point, 2025a. URL https://arxiv.org/abs/2502.08047. Xuanle Zhao, Xuexin Liu, Haoyue Yang, Xianzhen Luo, Fanhu Zeng, Jianling Li, Qi Shi, and Chi Chen. Chartedit: How far are mllms from automating chart analysis? evaluating mllms capability via chart editing. arXiv preprint arXiv:2505.11935, 2025b. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A LLM Usage Statement",
            "content": "B User-Centric Case Studies"
        },
        {
            "title": "C Data Curation",
            "content": "C.1 Chart Image Data . C.2 raw data filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D More Analysis",
            "content": "D.1 Details Evaluation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Metric Details",
            "content": "E.1 Overall . . . . . E.2 Base Evaluation . . . . . E.2.1 Color Score . E.2.2 grid Score . . . . . . E.2.3 Layout score . E.2.4 Legend score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.5 data parameter score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.6 visual parameter score . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2. type score . E.2.8 text score . E.3 LLM-Evaluation . E.4 LMM-Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Run configurations",
            "content": "G Open-Source Model Components"
        },
        {
            "title": "I Model Source",
            "content": "I.1 I.2 I.3 level 1 . level 2 . level 3 . . . . . . ."
        },
        {
            "title": "J Evaluation Code",
            "content": "J.1 color . J.2 Grid . . . J.3 Layout . . . . J.4 Legend . J.5 Visual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 14 18 18 19 20 23 23 23 24 26 28 29 30 31 32 33 34 35 36 36 42 47 51 51 52 54 55 J.6 Data . J.7 Text . J.8 Type . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "K Prompt",
            "content": "K.1 generation Prompt . . K.2 LLM-Score Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 58 59"
        },
        {
            "title": "A LLM USAGE STATEMENT",
            "content": "We disclose the use of Large Language Models (LLMs) in this research in several capacities. First, during the preparation of this manuscript, we utilized an LLM for grammatical correction and stylistic refinement to improve the papers readability. Second, and central to our methodology, multiple LLMs served as the subjects of our experiments to test our proposed benchmark. Furthermore, the evaluation metrics for our benchmark involved using an LLM to assess the comprehensive quality of the results. We explicitly state that we have never relied on LLMs to generate core research ideas, methodologies, experimental designs, or conclusions. All technical contributions and analyses presented herein are the original work of the authors. USER-CENTRIC CASE STUDIES In this section, we showcase representative examples that reflect scenarios commonly encountered by human users. One example is Level 2 task (\"Error Sample\"), where the model must not only generate chart code but also edit the original data to produce the target visualization. We observe that most Large Multimodal Models (LMM) fail on this seemingly routine setting, which highlights their difficulty in handling tasks that are trivial for humans. Moreover, as illustrated in the subsequent cases (\"LLM capability exploration\"), existing LMMs often produce wrong answers even for basic perception tasks, such as recognizing image content or extracting key chart information. These failures indicate that if models cannot reliably solve such everyday scenarios, it is even less likely they can succeed in the more complex challenge of chart2code."
        },
        {
            "title": "Error Sample",
            "content": "Instruction: Analyze inventory distribution by category. - Highlight sufficient inventory in Grooming Tools and Kids Clothing - Highlight insufficient inventory in Toys & Games and Books & Stationery - Use separate colored sections in the chart to distinguish these two groups Generate runnable Python code matching the uploaded image style. Data text: { \"Grooming Tools\": {\"in_stock\": 15.2, \"out_of_stock\": 14.8}, \"Kids Clothing\": {\"in_stock\": 12.5, \"out_of_stock\": 13.2}, \"Toys & Games\": {\"in_stock\": 8.3, \"out_of_stock\": 9.1}, \"Books & Stationery\": {\"in_stock\": 7.1, \"out_of_stock\": 8.2}, \"Health & Wellness\": {\"in_stock\": 6.8, \"out_of_stock\": 7.4}, \"Cameras & Accessories\": {\"in_stock\": 6.5, \"out_of_stock\": 7.0}, \"Beauty & Personal Care\": {\"in_stock\": 6.2, \"out_of_stock\": 6.7}, \"Mens Clothing\": {\"in_stock\": 5.9, \"out_of_stock\": 6.3}, \"Womens Clothing\": {\"in_stock\": 5.4, \"out_of_stock\": 6.0}, \"Shoes & Footwear\": {\"in_stock\": 5.1, \"out_of_stock\": 5.8} } Reference Figure GT Figure"
        },
        {
            "title": "Wrong Answer",
            "content": "Doubao-Seed-1.6:"
        },
        {
            "title": "Wrong Answer",
            "content": "Claude Sonnet 4: Wrong Answer GPT-5: Correct Answer Gemini-2.5-pro: Data analysis statistics are incorrect; colors have not properly mimicked the style; the pie chart should be donut chart. One sub-figure is missing; the colors do not follow the reference style; the data extraction is incorrect; the legend style is wrong. Color style error; data analysis error. The image is highly faithful; the data analysis is correct; there are minor flaws but acceptable."
        },
        {
            "title": "LLM capability exploration",
            "content": "Question: Please, based on this image, tell me if there is any title text in the picture? If so, what is its position, is it single or multiple lines, and what are its size and color? Figure:"
        },
        {
            "title": "Correct Answer",
            "content": "Answer: Yes; Upper; Single; fontsize=12; Black;"
        },
        {
            "title": "Correct Answer",
            "content": "Doubao-1.5-thinking-vision-pro: Yes; Upper; Single; fontsize=14; Black;"
        },
        {
            "title": "Correct Answer",
            "content": "Qwen2.5-VL-72B: Yes; Center top; Single; fontsize=14-16; Black;"
        },
        {
            "title": "Correct Answer",
            "content": "Claude-opus-4: Yes; Center top; Single; Large; Black;"
        },
        {
            "title": "Wrong Answer",
            "content": "Gemini-2.5-pro: Yes; Center top; Multiple; Large; Black;"
        },
        {
            "title": "Correct Answer",
            "content": "GPT-5: Yes; Center top; Single; fontsize=16; Black;"
        },
        {
            "title": "LLM capability exploration",
            "content": "Question: Please describe the arrangement and relative position of the legend in this chart: Is it arranged horizontally, vertically, or in grid? Which side of the main graph is it located on (top, bottom, left, right, or embedded within)? Figure:"
        },
        {
            "title": "Correct Answer",
            "content": "Answer: Horizontally(N*1); Lower right;"
        },
        {
            "title": "Wrong Answer",
            "content": "Doubao-1.5-thinking-vision-pro: Horizontally(N*1); Upper right;"
        },
        {
            "title": "Correct Answer",
            "content": "Qwen2.5-VL-72B: Horizontally(N*1); Lower right;"
        },
        {
            "title": "Wrong Answer",
            "content": "Claude-opus-4: Horizontally(N*1); Middle right;"
        },
        {
            "title": "Wrong Answer",
            "content": "Gemini-2.5-pro: Horizontally(N*1); Middle right;"
        },
        {
            "title": "Wrong Answer",
            "content": "GPT-5: Horizontally(N*1); Right;"
        },
        {
            "title": "LLM capability exploration",
            "content": "Question: Please describe the grid lines in this chart: Are they horizontal, vertical, or both? Are the lines dashed or solid? Figure:"
        },
        {
            "title": "Correct Answer",
            "content": "Answer: Only horizontal grid lines; Dashed line;"
        },
        {
            "title": "Wrong Answer",
            "content": "Doubao-1.5-thinking-vision-pro: Only horizontal grid lines; Solid line;"
        },
        {
            "title": "Correct Answer",
            "content": "Qwen2.5-VL-72B: Only horizontal grid lines; Dashed line;"
        },
        {
            "title": "Wrong Answer",
            "content": "Claude-opus-4: Only horizontal grid lines; Solid line;"
        },
        {
            "title": "Correct Answer",
            "content": "Gemini-2.5-pro: Only horizontal grid lines; Dashed line;"
        },
        {
            "title": "Correct Answer",
            "content": "GPT-5: Only horizontal grid lines; Dashed line;"
        },
        {
            "title": "LLM capability exploration",
            "content": "Question: Please describe the primary tick marks on the axes of this chart: whether they exist, their thickness and orientation (facing outward or inward), as well as the position and rotation angle of the tick labels. Figure:"
        },
        {
            "title": "Correct Answer",
            "content": "Answer: No; Lower; 45 degrees."
        },
        {
            "title": "Wrong Answer",
            "content": "Doubao-1.5-thinking-vision-pro: Implied; Lower; 0 degrees."
        },
        {
            "title": "Wrong Answer",
            "content": "Qwen2.5-VL-72B: No; Lower; 0 degrees."
        },
        {
            "title": "Wrong Answer",
            "content": "Claude-opus-4: No; Lower; 0 degrees."
        },
        {
            "title": "Wrong Answer",
            "content": "Gemini-2.5-pro: No; Lower; 0 degrees."
        },
        {
            "title": "Wrong Answer",
            "content": "GPT-5: No; Lower; 0 degrees."
        },
        {
            "title": "C DATA CURATION",
            "content": "To construct comprehensive and challenging chart benchmark, we collected rich dataset of chart images and their corresponding raw data from multiple sources. C.1 CHART IMAGE DATA Our chart image library is primarily composed of three parts, designed to cover wide range of chart types, visual styles, and information densities. Charts from Academic Literature: We extracted chart images from approximately 5,000 PDF documents by crawling and parsing papers from the preprint server arXiv using automated scripts. These publications span from January 2024 to July 2025 and cover multiple disciplines, including 18 Figure 5: Timestamps distribution of chart sources from arxiv preprint. computer science, physics, statistics, and economics, timestamps distribution of chart sources from arxiv preprint 5. This ensures that our dataset not only includes common statistical charts but also covers the highly customized and information-dense visualizations frequently found in academic research, guaranteeing both diversity and state-of-the-art relevance. Examples from Programming Communities and Tutorials: To include standard charts generated directly from code, we collected 1,000 example charts from the official documentation and tutorials of several mainstream data visualization libraries. Sources include official plotting examples from Matplotlib, Seaborn, Plotly, WordCloudX, Scipy, and Matlab. This portion of the data provides set of stylistically consistent and high-quality golden standard references for the benchmark. Existing Chart Datasets: To further increase the difficulty of the benchmark, we selected 300 of the most structurally and elementally challenging complex charts from the existing ChartMimic (Yang et al., 2025a) dataset, based on its inherent difficulty labels and our own pre-assessment. Preliminary Collection and Deduplication: First, we gathered all charts from the three aforementioned sources into unified database. We then performed preliminary automated deduplication and format standardization. Coarse Filtering: We recruited 10 senior undergraduate students majoring in computer science to conduct an initial screening of the chart pool. The screening criteria were primarily based on the clarity of the chart type (i.e., whether it is common chart type) and its information complexity (e.g., the number of data series, density of text labels). This stage aimed to quickly eliminate ambiguous, overly simplistic, or low-quality images, reducing the dataset size from approximately 6,300 to 3,000. Expert Evaluation and Annotation: We invited five doctoral students and researchers, each with over three years of experience in data visualization, to serve as experts for fine-grained evaluation of the filtered charts. We assigned the charts to the experts by category (e.g., line charts, bar charts, scatter plots) and asked them to independently score each chart from 1 to 5 across three dimensions: Data Complexity: Refers to the dimensional and structural complexity of the underlying data required for the chart. Visual Complexity: Refers to the richness of visual elements in the chart, such as markers, colors, annotations, and dual axes. Programming Complexity: Refers to the programming skills and volume of code required to reproduce the chart, such as the need for complex layouts or custom functions. Final Adjudication: We selected charts that achieved high composite score across the three dimensions and had high inter-rater agreement (> 0.8). For charts with disagreements, two core researchers made the final decision. Through this process, we finalized set of 719 high-quality reference charts. C.2 RAW DATA FILTERING Automated Preprocessing: We developed automated scripts to parse raw data files in various formats (e.g., Excel, CSV, TXT, JSON). These scripts prioritized the selection of data tables that contain abundant numerical, time-series, or categorical information suitable for visualization. Manual Verification and Diversity Preservation: Subsequently, we manually reviewed the data filtered by the scripts, discarding any incomplete or poorly formatted data. During this process, we placed special emphasis on preserving the diversity of data sources and domains to ensure the final dataset was not biased towards any specific field. Ultimately, we constructed raw database containing 39 Excel files, 80 structured data files (such as CSVs), and 36 semi-structured text files."
        },
        {
            "title": "D MORE ANALYSIS",
            "content": "D.1 DETAILS EVALUATION RESULTS The detail results on DR, CRD and CFD as show in Tab 6, Tab 7 and Tab 8. Fig. 7, Fig. 8, Fig. 9, Fig. 10, and Fig. 11 present the comparative results of human scores versus model scores. Table 6: Details Evaluation results on level-1 Direct mimic result"
        },
        {
            "title": "Type",
            "content": "GPT-Score Code-Level Figure-Level GPT-Score"
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed1.5-VL Seed1.6-VL Exec. Rate 90.4 96.38 87.48 85.81 84.7 LLaVA-OV-Qwen2-7B-SI 32.82 LLaVA-OV-Qwen2-7B-OV 11.13 DeepSeek-VL-7B 48.68 68.85 kimi-VL-A3B 64.39 Qwen2-VL-7B 75.66 Qwen2-VL-72B 66.89 InternVL-2.5-8B 86.23 InternVL-2.5-38B 21.97 InternVL-3-8B 86.79 InternVL-3-38B 72.18 GLM-4V-9B 66.34 Intern-VL-3.5-8B 37.83 MiMo-VL-7B-RL 44.65 MiMo-VL-7B-SFT 65.64 Qwen2.5-VL-7B 65.36 Qwen2.5-VL-72B 34.77 Molmo-7B-D 64.67 Qwen3-30B 0.7289 0.6210 0.6210 0.5397 0.5418 0.0808 0.1868 0.2166 0.4242 0.2883 0.3881 0.3013 0.3907 0.3593 0.4280 0.2220 0.4173 0.5716 0.5530 0.3653 0.4922 0.0660 0."
        },
        {
            "title": "Proprietary",
            "content": "0.9119 0.7334 0.7273 0.8302 0.0.7698 0.9815 0.9528 0.9495 0.9486 0.9568 0.6272 0.5948 0.6024 0.5449 0.5643 0.5221 0.4772 0.4741 0.4565 0.4307 0.6762 0.5582 0.5545 0.6218 0.5728 Open-Source LMMs (non-thinking) 0.4773 0.5875 0.6723 0.7035 0.6249 0.7450 0.6491 0.7381 0.5581 0.7407 0.5704 0.7606 0.8455 0.8374 0.6928 0.7506 0.4970 0.7647 0.6356 0.7896 0.7989 0.8958 0.7947 0.8815 0.7845 0.8881 0.8481 0.8959 0.7164 0.8749 0.9667 0.9577 0.8573 0.9286 0.7248 0.9428 0.1356 0.3875 0.3315 0.4928 0.3054 0.4561 0.3232 0.4251 0.3712 0.4146 0.2721 0.4163 0.5599 0.5456 0.4266 0.5086 0.1239 0.6325 0.0582 0.1319 0.2021 0.3705 0.2262 0.3467 0.2836 0.3526 0.3268 0.3621 0.1465 0.3733 0.4532 0.4322 0.3389 0.4253 0.0649 0.4598 0.1982 0.3156 0.3864 0.5020 0.4269 0.4952 0.4315 0.5192 0.5376 0.5137 0.3565 0.5395 0.6348 0.6091 0.4847 0.6053 0.1661 0.6134 Open-Source LMMs (thinking) 0.8555 0.7515 0.8397 0.7808 0.8216 0.3330 0.4486 0.4365 0.5882 0.4858 0.5970 0.5052 0.5990 0.5456 0.6414 0.4471 0.6103 0.7814 0.7814 0.5610 0.6960 0.3648 0.7476 0.9290 0.8985 0.9002 0.8824 0.8583 0.4719 0.6079 0.6848 0.8323 0.7023 0.8077 0.7274 0.8230 0.7766 0.8190 0.5973 0.8405 0.9127 0.9212 0.7982 0.8718 0.3739 0.8945 MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen3-30B 55.77 50.35 45. 0.4890 0.5458 0.5262 0.8531 0.8844 0.8649 0.9535 0.9526 0.9692 0.6420 0.6511 0.5610 0.4453 0.4459 0.4563 0.6108 0.6048 0. 0.7328 0.7907 0.7540 0.9212 0.8966 0.8786 0.6286 0.5629 0.6334 0.5536 0.5237 0.182 0.265 0.2854 0.4409 0.3364 0.4368 0.3448 0.3448 0.3802 0.4577 0.2881 0.4371 0.5439 0.4959 0.4197 0.5118 0.2164 0.5293 0.4959 0.6555 0.5582 0.3807 0.2553 0.3575 0.2341 0. 0.0154 0.0376 0.0431 0.1374 0.0664 0.1207 0.0723 0.1283 0.0949 0.1463 0.0459 0.1389 0.2316 0.1983 0.0994 0.1893 0.0943 0.2531 0.1983 0.213 0.2730 Discrepancy Between LLM-Score and LMM-Score. Figure 6 illustrates model performance across ten representative task cases, evaluated by both LLM-score for code quality (left) and LMMscore for rendered chart fidelity (right). clear discrepancy emerges: proprietary models such as GPT-5, Gemini-2.5-Pro, and Claude-Sonnet-4 achieve consistently high LLM-scores across most tasks (often 0.7), indicating strong code-level compliance. However, their corresponding LMMscores are much lower (typically 0.35), showing that syntactically correct code often fails to produce visually faithful charts. Open-source models, in contrast, underperform on both metrics, with particularly low LMM-scores across all tasks. This contrast highlights that current models generalize relatively well at the code level but remain fundamentally limited in achieving pixel-level chart fidelity, especially on diverse and challenging task cases. Figure 6: Analysis of model performance on different task cases with LLM-score and LMM-score. 20 Table 7: Details Evaluation results on level-1 Customize mimic result"
        },
        {
            "title": "Type",
            "content": "GPT-Score Code-Level Chart-Level GPT-Score"
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed1.5-VL Seed1.6-VL LLaVA-OV-Qwen2-7B-SI LLaVA-OV-Qwen2-7B-OV DeepSeek-VL-7B kimi-VL-A3B Qwen2-VL-7B Qwen2-VL-72B InternVL-2.5-8B InternVL-2.5-38B InternVL-3-8B InternVL-3-38B GLM-4V-9B Intern-VL-3.5-8B MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen2.5-VL-7B Qwen2.5-VL-72B Molmo-7B-D Qwen3-30B Exec. Rate 100.0 97.2 94.4 97.2 94.4 11.1 5.56 61.11 72.22 75.00 80.56 80.56 88.89 36.11 0.0000 66.67 86.11 69.44 69.44 75.00 100 4.55 77.78 0.5343 0.4543 0.4667 0.4041 0. 0.0649 0.5159 0.3967 0.3927 0.3158 0.3487 0.3129 0.4004 0.3832 0.0000 0.3746 0.3520 0.4459 0.4198 0.3404 0.5109 0.4563 0."
        },
        {
            "title": "Proprietary",
            "content": "0.9167 0.6571 0.7059 0.7714 0.7206 0.9167 0.8571 0.8824 0.9143 0.9118 0.4683 0.2571 0.3824 0.4857 0.4412 0.7429 0.7423 0.6748 0.7234 0.7532 0.8172 0.6995 0.7336 0.8124 0.8280 Open-Source LMMs (non-thinking) 0.2500 0.0000 0.8056 0.4808 0.7407 0.7586 0.7586 0.7812 0.8462 0.0000 0.7500 0.7742 0.7200 0.8400 0.7778 0.8470 1.0000 0.6786 0.9167 0.8333 0.8796 0.8333 0.8889 0.8276 0.8966 0.8750 0.7692 0.0000 0.8611 0.8710 0.8400 0.8800 0.8889 0.9492 1.0000 0.7381 0.2500 0.0000 0.3056 0.5000 0.3086 0.3793 0.2816 0.4479 0.1923 0.0000 0.4167 0.3548 0.4000 0.5200 0.4074 0.4606 0.0000 0.4167 0.4231 0.4184 0.7091 0.5681 0.6272 0.6121 0.5705 0.6993 0.5247 0.0000 0.5298 0.6169 0.6766 0.7619 0.5479 0.4127 0.4691 0.6858 0.8278 0.7333 0.7940 0.7757 0.8429 0.7814 0.7896 0.8477 0.7821 0.0000 0.8090 0.7258 0.8074 0.7703 0.7732 0.6653 1.0000 0.7945 Open-Source LMMs (thinking) 0.5098 0.6607 0.4702 0.4886 0.7236 0.6105 0.6077 0.5918 0.6138 0.5239 0.5563 0.4869 0.6180 0.5873 0.0000 0.5417 0.5660 0.6239 0.6851 0.5996 0.6808 0.8369 0.6280 0.9259 0.9333 0.8922 0.9143 0.9314 1.0000 1.0000 0.9352 0.9359 0.9259 0.9195 0.8966 0.9479 0.8974 0.0000 0.9167 0.8602 0.9333 0.9067 0.9012 0.9362 1.0000 0.9405 MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen3-30B 69.44 86.11 72. 0.5157 0.4746 0.4370 0.7643 0.7545 0.8846 0.9452 0.9269 0.8846 0.4226 0.3838 0.4231 0.4246 0.3741 0.7488 0.7014 0.6769 0. 0.6854 0.6574 0.6378 0.6053 0.9351 0.9231 Table 8: Details Evaluation results on level-1 figure mimic result"
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Seed1.5-VL Seed1.6-VL LLaVA-OV-Qwen2-7B-SI LLaVA-OV-Qwen2-7B-OV DeepSeek-VL-7B kimi-VL-A3B Qwen2-VL-7B Qwen2-VL-72B InternVL-2.5-8B InternVL-2.5-38B InternVL-3-8B InternVL-3-38B GLM-4V-9B Intern-VL-3.5-8B MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen2.5-VL-7B Qwen2.5-VL-72B Molmo-7B-D Qwen3-30B Exec. Rate 87.04 88.89 85.19 65.74 83.96 0.00 0.00 10.19 10.19 30.56 51.85 37.74 43.52 17.60 0.0000 44.74 57.41 41.67 33.33 46.30 37.96 0.97 70.37 0.5026 0.4582 0.4580 0.4189 0. 0.0000 0.0000 0.2625 0.2204 0.3006 0.4136 0.2574 0.4171 0.3111 0.0000 0.0997 0.3515 0.3981 0.4383 0.4726 0.3767 0.0392 0."
        },
        {
            "title": "Proprietary",
            "content": "0.8546 0.6632 0.6042 0.6981 0.8028 0.9787 0.9583 0.9583 0.9625 0.9859 0.4143 0.3445 0.3445 0.3258 0.3536 0.6060 0.5941 0.5941 0.6110 0.6184 0.7013 0.5972 0.5972 0.6482 0.6838 Open-Source LMMs (non-thinking) 0.0000 0.0000 0.6403 0.5152 0.5758 0.6161 0.5917 0.6979 0.7719 0.0000 0.4773 0.5597 0.8000 0.7639 0.7800 0.7228 0.0000 0.6509 0.0000 0.0000 0.7273 0.8182 0.9293 0.9821 0.9917 0.9787 0.8421 0.0000 0.8258 0.9111 0.9556 1.0000 1.0000 0.9268 1.0000 0.9430 0.0000 0.0000 0.2541 0.0909 0.1818 0.3574 0.3310 0.2533 0.2456 0.0000 0.2045 0.2604 0.3380 0.5278 0.4253 0.3333 1.0000 0.3609 0.0000 0.0000 0.1797 0.3333 0.4154 0.5842 0.3931 0.5575 0.5458 0.0000 0.2854 0.4797 0.5814 0.6989 0.6106 0.6006 0.0222 0.6166 0.0000 0.0000 0.4121 0.5348 0.5569 0.6795 0.4434 0.6921 0.6850 0.0000 0.4894 0.5866 0.6853 0.7546 0.7686 0.7103 0.4000 0.6623 Open-Source LMMs (thinking) 0.3534 0.3703 0.6503 0.6532 0.6984 0.0000 0.0000 0.4572 0.3488 0.4855 0.5865 0.4432 0.5865 0.5287 0.0000 0.2233 0.5004 0.5799 0.6494 0.6306 0.6130 0.2454 0.6459 0.9128 0.9406 0.9406 0.8989 0.9493 0.0000 0.0000 0.8048 0.8485 0.8929 0.9446 0.8625 0.9546 0.8947 0.0000 0.7652 0.8717 0.9667 0.9343 0.9533 0.9057 0.5000 0.9219 MiMo-VL-7B-RL MiMo-VL-7B-SFT Qwen3-30B 46.30 38.89 39. 0.4726 0.4162 0.4625 0.7800 0.7302 0.7093 1.0000 1.0000 0.9767 0.4253 0.4286 0.3295 0.6106 0.5642 0.5988 0.7686 0.6719 0. 0.6306 0.6298 0.6275 0.9533 0.9516 0.9085 21 0.6763 0.63 0.607 0.6325 0.6525 0.4225 0.4213 0.6273 0.5887 0.595 0.6082 0.5712 0.6298 0.44 0.0000 0.5628 0.6169 0.6068 0.6053 0.5952 0.6273 0.2400 0.2546 0.2582 0.6644 0. 0.2661 0.236 0.2238 0.2503 0.2662 0.155 0.0825 0.1989 0.2081 0.1367 0.1628 0.1183 0.1884 0.1023 0.0000 0.1183 0.1732 0.2421 0.2582 0.1515 0.1989 0.4600 0.2368 0.2248 0.3368 0.6145 0.5538 0.6082 0.5756 0.5978 0.00 0.00 0.2539 0.2539 0.4235 0.5518 0.5715 0.6698 0.4178 0.0000 0.2904 0.445 0.4962 0.5807 0.5155 0.5532 0.0500 0.2412 0.5155 0.5578 0. 0.2214 0.2273 0.2382 0.1962 0.2075 0.00 0.00 0.0145 0.0145 0.0591 0.1373 0.0568 0.1728 0.0805 0.0000 0.013 0.1028 0.1407 0.2172 0.1732 0.1688 0.4100 0.2698 0.1732 0.1455 0."
        },
        {
            "title": "Type",
            "content": "GPT-Score Code-Level Figure-Level GPT-Score Figure 7: Human vs model performance: LLM-score and LMM-score across level 1 direct tasks. Figure 8: Human vs model performance: LLM-score and LMM-score across level 1 customize tasks. Figure 9: Human vs model performance: LLM-score and LMM-score across level 1 figure tasks. 22 Figure 10: Human vs model performance: LLM-score and LMM-score across level 2 tasks. Figure 11: Human vs model performance: LLM-score and LMM-score across level 3 tasks."
        },
        {
            "title": "E METRIC DETAILS",
            "content": "E.1 OVERALL To better evaluate the performance of different models, we conduct comparative assessments from two levels: the code-level and the chart-level. Throughout the evaluation process, we first examine the executability of the generated code. The execution rate is defined as the ratio between the number of executable code snippets that successfully generate images (s) and the total number of tasks (t). Formally, the execution rate is expressed as: exec_rate = . (1) The execution rate is reported as percentage. At the code-level, we first extract plotting elements from the matplotlib.Figure object and propose eight evaluation dimensions as the base evaluation. The detailed specifications are given in E.2. Subsequently, we leverage gpt-5-mini to perform holistic similarity assessment of the codes visualization results, thereby providing more reliable confidence score at the code level. We refer this as LLM-Score. At the chart-level, we input the executable code into gpt-5-mini for image-based evaluation. By designing specific prompts, the large multimodal model (LMM) assesses multiple dimensions and produces an aggregated score. This chart-level evaluation offers an intuitive similarity measure of the visual outputs, thereby serving as direct indicator of model performance. We refer this as LMM-Score. The implementation details of these two evaluation mechanisms are described as follows. E.2 BASE EVALUATION To evaluate visualization effects from the code perspective, we investigated commonly used Python plotting libraries and found that Seaborn, Matplotlib, NetworkX, and WordCloud all rely on Matplotlibs underlying plotting functions. When using these libraries for plotting, Figure object is generated in memory, which contains all the elements of the plot. This implies that we can extract all visualization-related elements from the Figure object and compare the GT_code with the generated_code to evaluate their visualization effects. More Efficient. Unlike ChartMimic (Yang et al., 2025a), which depends on code tracers and code injection, our evaluation method is substantially more efficient. In practice, ChartMimic must execute both the GT_code and generated_code for each evaluation dimension, resulting in up to twelve executions for single generated_code. This process incurs significant computational overhead in both time and memory. By contrast, our method executes the GT_code and generated_code only once, caches their corresponding Figure objects, and then evaluates multiple dimensions directly on these objects, thereby greatly reducing execution cost. More General. In comparison to ChartMimics (Yang et al., 2025a) hard-coded rules, which exhibit limited adaptability and strong dependence on specific Matplotlib versions, our evaluation method is inherently more general. ChartMimic enforces rule-based matching of plotting elements, which not only imposes strict version constraints but also leaves many elements unsupported. Our approach instead parses the Figure object directly, which comprehensively encapsulates all elements in memory, ensuring greater robustness and version independence. More Versatile. Whereas ChartMimic (Yang et al., 2025a) is restricted to narrow set of functions from specific libraries, our method offers broad applicability. By operating directly on core Matplotlib objects, our approach seamlessly extends to all visualization libraries that build upon Matplotlibs primitives, thereby achieving substantially stronger cross-library generalization. More Precise. Unlike ChartMimic (Yang et al., 2025a), which evaluates function call patterns rather than visual outputs, our method emphasizes the visualization results themselves. ChartMimic leaves gap between code execution and rendered charts, while our approach directly inspects visual objects such as Line and Patch. This enables more faithful and precise evaluation of visualization quality at the code-to-visualization level. E.2.1 COLOR SCORE Traditional approaches typically treat all colors in chart as an unordered set, neglecting the binding relationship between colors and specific data items(Yang et al., 2025a). To address this issue, we propose an efficient and more professional method for color extraction strategy designed to parse colors and their corresponding semantic information from Matplotlibs graphical objects Figure. This strategy decomposes the chart into different types of visual elements and organizes the extracted color information into structured mapping, which can be expressed as: {ElementType {DataKey HexColor}} (2) where: ElementType: Refers to the object to which the color is applied, such as the fill color of bar chart (patch_face), the line color of line chart (line_color), the color of scatter plot (scatter_color), or the background of the axes (axes_bg). DataKey: Refers to the specific data entity bound to the color. This is typically the label in the legend, the tick label on the axis, or the content of text element. HexColor: The standardized hexadecimal color code. After obtaining the structured color data, we design set of weighted evaluation metrics to quantify the color fidelity between generated_code and GT_code. The core principle of this evaluation is that not all colors are equally important. For example, errors in the colors of data series are more severe than errors in the colors of axis grid lines. To this end, we introduce element-type weights (wt), assigning predefined weight to each ElementType t. Core data elements (e.g., patch_face, line_color) are assigned high weights (e.g., 1.0), whereas auxiliary or decorative elements (e.g., figure_bg, spine) are assigned lower weights (e.g., 0.01). 24 The evaluation is performed only on the element types and data keys shared by both generated_code(gen) and GT_code(gt). This ensures valid comparison, avoiding mismatches such as comparing line color in generated with bar color in gt_code. The total weighted similarity Stotal serves as the core of our model, and is computed as: Stotal = (cid:88) (cid:88) wt σ(cid:0)Cgen,t,k, Cgt,t,k (cid:1), (3) tTgenTgt kKgen,tKgt,t where: Tgen and Tgt denote the sets of all element types present in the generated chart and the ground-truth chart, respectively. Kgen,t and Kgt,t denote the sets of all data keys under element type in the generated and ground-truth charts, respectively. wt is the predefined weight for element type t. Cgen,t,k and Cgt,t,k are the colors corresponding to element type and key in the generated and ground-truth charts, respectively. σ(C1, C2) is function measuring the similarity between two hexadecimal colors. The color similarity function σ(C1, C2) is used to quantify the visual closeness between two colors. In our implementation, we adopt normalized reversed Euclidean distance in the RgenB color space to compute similarity. First, the hexadecimal color is converted into its RGB representation (R, G, B). The Euclidean distance between two colors C1 and C2 is defined as: d(C1, C2) = (R1 R2)2 + (G1 G2)2 + (B1 B2)2. (4) The maximum possible distance in the RGB space corresponds to the distance between (0, 0, 0) and (255, 255, 255), i.e., dmax = 3 2552. (5) We then normalize the distance and transform it into similarity score σ within the range [0, 1]: σ(C1, C2) = 1 d(C1, C2) dmax . (6) When two colors are identical, σ = 1.0; when they differ maximally, σ = 0.0. To provide comprehensive and interpretable evaluation results, we map the computed total weighted similarity (Stotal) to three standard metrics widely used in the information retrieval domain: Precision, Recall, and F1-Score. Total Weight: We first compute the total weights of the generated chart and the ground-truth chart, representing the maximum theoretically achievable similarity score. Wgen = (cid:88) (cid:88) wt, Wgt = (cid:88) (cid:88) wt. tTgen kKgen,t tTgt kKgt,t (7) Precision: Measures the accuracy of all color elements in the generated chart. It answers the question: Among all generated colors, what proportion is correct? Stotal Wgen Precision = (8) . Recall: Measures the extent to which all color elements in the ground-truth chart are correctly reproduced in the generated chart. It answers the question: Among all required colors, what proportion has been correctly generated? Recall ="
        },
        {
            "title": "Stotal\nWgt",
            "content": ". 25 (9) F1-Score: The harmonic mean of Precision and Recall, providing single comprehensive evaluation score. 2 Precision Recall Precision + Recall . (10) F1-Score = E.2.2 GRID SCORE We define structured Grid State Descriptor. For each subplot ax in chart, we extract the visibility of its X-axis and Y-axis grid lines, and encode them as Boolean dictionary: {x_grid_visible : bool, y_grid_visible : bool}. (11) We traverse all Axes objects within Figure, and for each subplot where at least one grid line (X-axis or Y-axis) is visible, we generate grid state descriptor. Ultimately, the grid configuration of an entire chart is abstracted as list of such descriptors, which can be mathematically regarded as multiset. For example, in Figure with two subplots, where the first subplot has only Y-axis grid lines and the second subplot has both X-axis and Y-axis grid lines, the grid configuration is represented as: {x_grid_visible : False, y_grid_visible : True}, {x_grid_visible : True, y_grid_visible : True} (12) This structured representation is not only precise but also completely ignores the specific styles of grid lines (e.g., color, linewidth). Instead, it focuses solely on their presence, which captures the core semantics and makes the evaluation more robust. After extracting the multisets of grid state descriptors from the generated figure (Ggen) and the ground-truth figure (Ggt), we further use the F1 metric to measure the accuracy of this parameter. We define the following notations: Ggen: the multiset of grid state descriptors extracted from the generated figure. Ggt: the multiset of grid state descriptors extracted from the ground-truth figure. The number of true positives (TP) is defined as the cardinality of the intersection between the two multisets: = Ggen Ggt. (13) True Positives (TP) true positive is defined as grid state descriptor that appears in Ggen and exactly matches one in Ggt. The total number of true positives is given by the size of the intersection of these two multisets: = Ggen Ggt. (14) Precision Precision measures the proportion of correctly activated grid configurations among all grid configurations in the generated figure (i.e., those that also exist in the ground-truth figure): Precision = Ggen = Ggen Ggt Ggen . (15) If Ggen = 0, we define Precision = 1.0. Recall Recall measures the proportion of required grid configurations in the ground-truth figure that are successfully reproduced in the generated figure: Recall = Ggt = Ggen Ggt Ggt . (16) If Ggt = 0, we define Recall = 1.0. F1-Score The F1-score, as the harmonic mean of precision and recall, provides single comprehensive metric: F1-Score = 2 . (17) Precision Recall Precision + Recall 26 E.2.3 LAYOUT SCORE For each individual subplot (i.e., an Axes object) in chart, we create unique and quantitative Layout Descriptor. This descriptor fully defines the size and position of the subplot within virtual grid (GridSpec). Instead of relying on pixel coordinates, we extract the underlying structural information from Matplotlibs SubplotSpec object. For each subplot ax in Figure, we extract the following six key parameters to construct its layout descriptor D: nrows (R): the total number of rows in the corresponding GridSpec. ncols (C): the total number of columns in the corresponding GridSpec. row_start (rs): the starting row index of the grid cells occupied by the subplot. row_end (re): the ending row index of the grid cells occupied by the subplot. col_start (cs): the starting column index of the grid cells occupied by the subplot. col_end (ce): the ending column index of the grid cells occupied by the subplot. Thus, the layout of each subplot can be precisely represented as 6-tuple: = (R, C, rs, re, cs, ce). (18) By traversing all Axes objects in Figure, the overall layout can be abstracted as multiset of these layout descriptors D, denoted as L. We define the following notation: Lgen: the multiset of layout descriptors extracted from the generated figure. LGT : the multiset of layout descriptors extracted from the ground-truth figure. True Positives (TP) true positive represents layout descriptor that exists in Lgen and exactly matches one in Lgt. The total number of true positives is defined as the size of the intersection of these two multisets: = Lgen Lgt (19) This indicates the number of subplots that are correctly generated and placed in the correct positions. Precision Precision measures the proportion of correctly generated subplots among all generated subplots: Precision = Lgen = Lgen Lgt Lgen (20) Here, Lgen denotes the total number of subplots in the generated figure. low precision indicates that the model produced redundant or incorrectly placed subplots. Recall Recall measures the proportion of required subplots in the ground-truth figure that were successfully generated: Recall = Lgt = Lgen Lgt Lgt (21) Here, Lgt denotes the total number of subplots in the ground-truth figure. low recall suggests that the model failed to generate all required subplots. F1-Score The F1-score, as the harmonic mean of precision and recall, provides single balanced metric for evaluating the overall quality of the layout: F1-Score = 2 Precision Recall Precision + Recall (22) 27 E.2.4 LEGEND SCORE We propose Dual-Constraint Matching Framework for Legend Evaluation. This framework decomposes legend evaluation into independent assessments of the semantic and spatial properties of each individual legend entry, and quantifies the consistency between the generated and ground-truth figures through flexible matching algorithm. Consequently, it provides more comprehensive and robust evaluation scheme. Our method does not treat the legend as single entity but decomposes it into collection of independent legend entries. For each visible legend object in the chart, we traverse all its text labels and create an atomic, structured Legend Descriptor for each label. The descriptor is defined as 2-tuple that captures both semantic and spatial information: = (t, B) (23) where: is string representing the textual content of the legend entry. This element captures the semantic correctness of the legend. is 4-tuple (x0, y0, x1, y1) representing the bounding box of the entire legend object containing the text entry, expressed in the screen rendering coordinate system. This element captures the spatial correctness of the legend. By traversing all legends from both the Axes objects and the Figure object itself, we can extract all visible legend entries of chart and represent them collectively as multiset of descriptors D, denoted as L. After extracting the multisets of legend descriptors Lgen and Lgt from the generated and ground-truth figures, respectively, we design dual-constraint matching algorithm to compute their similarity. The algorithm can flexibly operate in two modes: semantic-only matching or combined semantic and spatial matching. descriptor Dgen = (tgen, Bgen) from Lgen matches descriptor Dgt = (tgt, Bgt) from Lgt if and only if one or both of the following constraints are satisfied: Semantic Constraint: The text content of the two descriptors must be identical: tgen = tgt. (24) Positional Constraint: The bounding boxes of the legend objects containing the descriptors must have positive intersection area: Areaintersection(Bgen, Bgt) > 0. (25) For two bounding boxes B1 = (x1,0, y1,0, x1,1, y1,1) and B2 = (x2,0, y2,0, x2,1, y2,1), the intersection area is computed as: xA = max(x1,0, x2,0) yA = max(y1,0, y2,0) xB = min(x1,1, x2,1) yB = min(y1,1, y2,1) (26) Areaintersection = max(0, xB xA) max(0, yB yA) The algorithm finds unique matching pairs that satisfy the above constraints (removing matched descriptors from the pool) and computes the total number of true positives (TP). Based on TP, we perform the final quantitative evaluation using standard precision, recall, and F1-score metrics: Precision = Lgen , Recall = Lgt , 28 F1-Score = 2 Precision Recall Precision + Recall . (27) E.2.5 DATA PARAMETER SCORE The primary goal of data visualization is to faithfully and accurately convey the underlying data. We introduce an evaluation framework designed to quantify the fidelity of charts data parameters. This framework inspects the chart at deep level, directly verifying the correctness of its underlying data. The first step of the framework is to identify and extract the data parameters that directly define the data representation of the chart. Through introspection of Matplotlib plotting elements, we categorize these parameters into distinct types. The set of data parameters, denoted as Kdata, is explicitly defined as: Kdata = {xdata, ydata, offsets, xy, verts, width, height, sizes}. (28) These parameters directly correspond to the geometric and positional properties of chart elements: For line plots (Line2D), we extract xdata and ydata. For bar charts (Rectangle), we extract the lower-left corner coordinates xy, as well as width and height. For filled plots (Polygon), we extract all vertex coordinates verts. For scatter plots (Collection), we extract the center coordinates offsets and the point sizes sizes. Through this process, each chart is decomposed into multiset of element-parameter dictionaries. Data parameters, especially those represented as arrays, cannot be compared using simple equality operators. To robustly handle variations in data point ordering or floating-point precision, we define dedicated similarity function S(v1, v2). The core logic for data parameters is as follows: Numeric Type: For scalar values, we use numpy.isclose to determine whether two floating-point numbers are approximately equal within tolerance ϵ: S(v1, v2) = (cid:26)1 0 if v1 v2 ϵ otherwise (29) Array-like Type: For array data, which is crucial for evaluating data parameters, we adopt the Jaccard similarity coefficient to measure the overlap between the contents of two arrays. Let V1 and V2 denote the sets of elements in v1 and v2, respectively: S(v1, v2) = V1 V2 V1 V2 (30) This method is insensitive to the order of data points and accurately reflects the true content overlap between two datasets. After quantifying the similarity between parameters, we employ two-stage algorithm to compute the final evaluation metrics. Element Matching: To address differences in element order and quantity across charts, we use greedy optimal matching algorithm. For each element egt in the ground-truth chart, the algorithm searches among elements of the same type in the generated chart to find the best match gen that maximizes the total similarity across all parameters. This matching is performed globally, considering all parameter types. The result is set of successful matches: = {(egen, egt)}. (31) Data Metric Computation: Once the matching set is obtained, we focus exclusively on data parameters to aggregate the scores. The total true positive score for the data dimension, Pdata, is computed as the sum of similarities across all matched pairs. We iterate over the union of keys to ensure penalties for missing or extra parameters: 29 = (cid:88) (cid:88) S(egen[k], egt[k]) (32) (egen,egt)M k(keys(egen)keys(egt))Kdata Next, we count the total number of data parameters in the generated chart and the ground-truth chart, denoted as Ndata,gen and Ndata,gt, respectively. Finally, we compute the precision, recall, and F1-score for the data dimension: Precision = Recall = , Ndata,gen Ndata,gt , F1-Score = 2 Precision Recall Precision + Recall . (33) E.2.6 VISUAL PARAMETER SCORE The visual style of chart is also an important component of chart reproduction quality. Visual style is governed by set of visual parameters, such as line styles, marker shapes, element transparency, and so on. Correct usage of these parameters not only affects the aesthetic quality and professionalism of the chart, but also directly determines whether it adheres to specific design guidelines or user instructions. We propose framework, running in parallel with the data parameter evaluation, specifically designed to quantify the consistency of chart with respect to its visual parameters. This framework builds upon the parameterized representation established in E.2.5. After extracting all parameters of an element, we identify the set of visual parameters (Kvisual) by exclusion. parameter key is classified as visual parameter if it satisfies: / Kdata and / Kignore (34) where Kdata is the predefined set of data parameters, and Kignore is the set of parameters handled by other evaluators (e.g., color). Typical visual parameters include: linestyle, linewidth, marker, markersize, alpha, and so on. The extraction process is performed in parallel with that of the data parameters, but subsequent evaluation computations focus exclusively on this subset of parameters. We employ the same general similarity function S(v1, v2) introduced in the equation 29 and equation 30 to compare the values of visual parameters. Its robustness is equally applicable to various data types of visual parameters: String type: For parameters such as linestyle (e.g., - vs ) or marker (e.g., vs x), the function performs direct string equality comparison. Numeric type: For parameters such as linewidth (e.g., 1.5 vs 2.0) or alpha (e.g., 0.8 vs 1.0), the function uses numpy.isclose to perform tolerance-based comparison. This consistent definition of similarity ensures intrinsic coherence across different evaluation dimensions. Element Matching: We reuse the set of matched element pairs = {(egen, egt)} obtained through the greedy optimal matching algorithm. This implies that the matching of elements is determined based on their overall similarity (data + visual), consistent with human perception we always perceive an element as whole. Establishing match indicates that both the data and visual aspects will be evaluated for that pair. Visual Metric Computation: Given the set of matched pairs , we focus exclusively on the visual parameters to aggregate the scores. We compute the total true positive score for the visual dimension (T Pvisual), defined as the sum of visual parameter similarities across all matched pairs: Pvisual = (cid:88) (cid:88) S(egen[k], egt[k]) (35) (egen,egt)M k(keys(egen)keys(egt))Kvisual Similarly, we count the total number of visual parameters in the generated and ground-truth charts, denoted as Nvisual,gen and Nvisual,gt, respectively. Finally, the precision, recall, and F1-score for the visual dimension are computed as: Precisionvisual = Recallvisual = , Pvisual Nvisual,gen Pvisual Nvisual,gt , F1-Scorevisual = 2 Precisionvisual Recallvisual Precisionvisual + Recallvisual . (36) E.2."
        },
        {
            "title": "TYPE SCORE",
            "content": "We propose an evaluation framework based on Artist Class Introspection. Unlike methods that rely on the visual rendering of charts, this framework directly inspects the object model constructed in memory by the plotting library (Matplotlib). By examining the core drawing artists (i.e., primitive graphical objects) and their associated classes, the framework deterministically and robustly infers the composition of chart. The key idea is that Matplotlib employs different classes of artist objects for different types of plots. For example, line plot is rendered using Line2D objects, whereas bar chart is rendered using Rectangle objects. Leveraging this intrinsic correspondence, we can infer the chart types present in figure by identifying which classes of artist objects it contains. Our algorithm operates by traversing all subplots (Axes) within matplotlib.Figure object and inspecting the list of artists contained in each subplot (e.g., ax.lines, ax.patches, ax.collections, etc.). The algorithm aggregates all detected chart types within figure into set. This set-based representation has significant advantage: it naturally supports the identification and evaluation of composite charts. For example, chart that overlays line plot on top of bar chart will be recognized as containing both bar_or_hist and line. The number of true positives is defined as the size of the intersection between the two sets, that is, the number of chart types present in both the generated chart and the reference chart: Precision measures the proportion of correct chart types among all generated chart types: = Tgen Tgt Precision = Tgen = Tgen Tgt Tgen where Tgen denotes the total number of distinct chart types detected in the generated chart. Recall measures the proportion of reference chart types that are successfully generated: Recall = Tgt = Tgen Tgt Tgt (37) (38) (39) where Tgt denotes the total number of distinct chart types in the reference chart. The F1-Score is the harmonic mean of precision and recall, providing comprehensive evaluation metric: F1-Score = 2 Precision Recall Precision + Recall (40) 31 E.2."
        },
        {
            "title": "TEXT SCORE",
            "content": "We propose text evaluation framework based on semantic categorization and fuzzy matching. In this framework, all textual elements in chart are categorized according to their functional roles, and fuzzy matching algorithm based on edit distance is applied among texts within the same category. This enables quantitative evaluation of chart text that is both strict and robust. To achieve precise evaluation of textual roles, we first design an extractor (_extract_texts_from_figure) that introspects the matplotlib Figure object to identify and classify all visible textual elements. Instead of treating all texts as an undifferentiated set, we categorize them into predefined semantic classes. Through this process, the entire textual content of chart is transformed into structured Text Map, denoted as . Its form is dictionary that maps each category name to the list of text strings belonging to that category: = {c [t1, t2, . . .]}. For example, Ttitle represents the list of all subplot titles in the figure. This categorization mechanism ensures context-aware evaluation and prevents, for instance, an axis label from being incorrectly compared with title. After obtaining the text maps of the generated chart and the reference chart, Tgen and Tgt, we designed an evaluation algorithm to quantify their consistency. To tolerate minor textual differences, we adopt the Levenshtein Ratio as the similarity function between two strings s1 and s2, denoted as SL(s1, s2). This function is based on computing the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into the other (i.e., the Levenshtein Distance), and normalizes the value to the interval [0, 1]: SL(s1, s2) = 1 LevenshteinDistance(s1, s2) max(s1, s2) (41) higher value of SL indicates greater similarity between the two strings. Identical strings achieve similarity of 1. Our evaluation algorithm operates independently within each semantic category. For each category c, the algorithm searches for the best match gt for every generated text tgen Tgen,c from the available reference texts Tgt,c, such that SL(tgen, tgt) is maximized. To prevent one-to-many matches, once reference text is matched, it is removed from the candidate pool. We then accumulate the similarity scores of all best matches across all categories to obtain total similarity score (T Pscore), which can be regarded as weighted sum of true positives: Pscore = (cid:88) (cid:88) cC tgenTgen,c max tgtT gt,c SL(tgen, tgt) (42) where denotes the union of all text categories present in both charts, and reference texts in category c. gt,c is the set of unmatched Finally, we compute the total number of generated and reference texts (Ngen and Ngt), and derive the Precision, Recall, and F1-Score as follows: Precision ="
        },
        {
            "title": "T Pscore\nNgen",
            "content": ", Ngen = (cid:88) Tgen,c Recall ="
        },
        {
            "title": "T Pscore\nNgt",
            "content": ", Ngt = (cid:88) Tgt,c F1-Score = 2 Precision Recall Precision + Recall 32 (43) (44) (45) E.3 LLM-EVALUATION This study designs and implements multi-dimensional visualization code evaluation framework based on Large Language Models (LLMs). The framework does not execute code or render images; instead, it leverages the powerful code understanding and reasoning capabilities of LLMs to perform static analysis directly on the source code of both the generated and reference scripts. By decomposing the complex problem of visual similarity into series of well-defined and mutually orthogonal evaluation dimensions, and by designing strict scoring instructions for each, our framework provides comprehensive, in-depth, and interpretable quantitative assessment of chart code quality. We deconstruct the ambiguous task of code quality assessment into six specific and independent evaluation dimensions, denoted as Di. This approach makes the LLMs evaluation task more focused and renders the final results more diagnostic and interpretable. The six dimensions are defined as follows: Data Handling and Transformation: Evaluates the logic for processing, calculating, and transforming raw data prior to plotting. Chart Type and Mapping: Evaluates the choice of core plotting functions and the mapping of data columns to visual channels (e.g., x-axis, y-axis, size, color). Visual Aesthetics: Evaluates the settings of purely visual style parameters, such as colors, line styles, and markers. Labels, Titles, and Legend: Evaluates the presentation and content of all textual elements. Figure Layout and Axes: Evaluates the canvas size, subplot structure, axis ranges, and scales. Auxiliary Elements and Ticks: Evaluates the configuration of auxiliary elements such as grid lines, reference lines, and axis spines. The evaluation prompt is in K. E.4 LMM-EVALUATION The ultimate criterion for evaluating automatically generated charts should be human visual perception. Although programmatic evaluation and source code analysis can technically ensure the correctness of chart components and parameters, they may not fully capture all visual details, artifacts, or the overall aesthetic coherence in the final rendered image. To establish an evaluation system that more closely approximates \"gold standard,\" we argue for the necessity of directly assessing the final visual outputthe chart image itself. To this end, this study designs and implements holistic chart image evaluation framework based on Vision-Language Models (VLMs). This framework utilizes advanced multimodal large models by simultaneously providing them with both the reference and the generated images, supplemented by set of rigorous evaluation instructions, to directly quantify the visual similarity between the two. This end-to-end visual evaluation method can capture wide range of discrepancies, from macroscopic layout to microscopic pixel-level differences, thereby providing comprehensive and holistic quality score. Here, we adopt holistic evaluation approach, assessing all visual aspects in single call. To ensure rigor, we extend and reinforce the philosophy of deduction-based scoring system. The instructions require the model to assume perfect score of 100, and then to deduct points for every visual discrepancy it finds between the two images. The evaluation prompt is in K."
        },
        {
            "title": "F RUN CONFIGURATIONS",
            "content": "During the experiment, the parameter settings for various open-source and proprietary models were as follows. For details, please refer to the table below: Table 9: Run configurations for all models. Unset values indicate that their default values are being used. For Proprietary models, we are unable to use Top-P of exactly 1 due to their API settings, and we end up using value of 0.99999. Temp. denotes temperature. We use model pages code to set up the run configurations whenever possible. Model Version/HF Checkpoint Do Sample level 1 2 Max level 3 Max Temp. Top-P Proprietary Multimodal Large Language Models GPT-5 OpenAI (2025) Claude 4 Sonnet Anthropic (2025) Gemini-2.5-pro Comanici et al. (2025) doubao-seed-1-5 Guo et al. (2025) doubao-seed-1-6 Team (2025) gpt-5-2025-08-07 claude-4-sonnet-20250523 gemini-2.5-pro-20250617 seed1.5-VL-20250513 seed1.5-VL-20250625 Open-Source Multimodal Large Language Models Qwen2-VL-7B Wang et al. (2024a) Qwen2-VL-72B Wang et al. (2024a) Qwen2.5-VL-7B Bai et al. (2025) qwen2.5-VL-72B Bai et al. (2025) deepseek-VL-7B Lu et al. (2024) kimi-VL-A3B Team et al. (2025) MiMo-VL-7B-RL Xiaomi & Team (2025) MiMo-VL-7B-SFT Xiaomi & Team (2025) GLM-4-9b GLM et al. (2024) Intern-VL 2.5 8B Chen et al. (2024) Intern-VL 2.5 38B Chen et al. (2024) Intern-VL 3 8B Zhu et al. (2025) Intern-VL 3 38B Zhu et al. (2025) Intern-VL 3.5 8B Wang et al. (2025) Intern-VL 3.5 38B Wang et al. (2025) llava-onevision-qwen2-7b-si Li et al. (2024a) llava-onevision-qwen2-7b-ov Li et al. (2024a) Qwen/Qwen2-VL-7B-Instruct Qwen/Qwen2-VL-72B-Instruct Qwen/Qwen2.5-VL-7B-Instruct Qwen/Qwen2.5-VL-72B-Instruct deepseek-ai/deepseek-vl-7b-base moonshotai/Kimi-VL-A3B-Thinking XiaomiMiMo/MiMo-VL-7B-RL-2508 XiaomiMiMo/MiMo-VL-7B-SFT-2508 zai-org/glm-4-9b OpenGVLab/InternVL2_5-8B OpenGVLab/InternVL2_5-38B OpenGVLab/InternVL3-8B OpenGVLab/InternVL3-38B OpenGVLab/InternVL3_5-8B OpenGVLab/InternVL3_5-38B lmms-lab/llava-onevision-qwen2-7b-si lmms-lab/llava-onevision-qwen2-7b-ov True True True True True True True True True True True True True True True True True default default default default default 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 8192 55000 55000 55000 16000 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 32768 0 0 0 0 0 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 1 1 1 1 1 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 OPEN-SOURCE MODEL COMPONENTS We have listed the main components of the open-source models used in our work below: Table 10: We summarize the visual and language components of the open-source models evaluated in our benchmark, along with the input resolutions used in our evaluation. Here, original denotes that we use the default image size, as the corresponding models support dynamic resolution inputs. Note that for DeepSeekVL-7B and GLM-4-9B , we apply maximum input size constraint to accommodate their requirements."
        },
        {
            "title": "Model",
            "content": "Qwen2-VL-7B Qwen2-VL-72B Qwen2.5-VL-7B Qwen2.5-VL-72B Deepseek-VL-7B Kimi-VL-A3B MiMo-VL GLM-4-9B InternVL-2.5-8B InternVL-2.5-38B InternVL-3-8B InternVL-3-38B InternVL-3.5-8B InternVL-3.5-38B"
        },
        {
            "title": "Language\nModel",
            "content": "Qwen2-VL ViT-14-224 Qwen2-VL-LLM-7B Qwen2-VL ViT-14-224 Qwen2-VL-LLM-72B Qwen2.5-VL ViT-14-224 Qwen2.5-VL-LLM-7B Resolution origianl origianl origianl Qwen2.5-VL ViT-14-224 Qwen2.5-VL-LLM-72B origianl SigLIP-384-SO400M & SAM-ViT-Base"
        },
        {
            "title": "MoonViT",
            "content": "Qwen2.5-ViT CLIP ViT-L-14-336 DeepSeek-LLM-7B 1152 1152*"
        },
        {
            "title": "Moonlight Model",
            "content": "MiMo-7B InternLM-7B origianl origianl 1120 1120* InternViT-6B-448px-V2_ internlm2_5-7b-chat InternViT-6B-448px-V2_5 Qwen2.5-32B-Instruct InternViT-300M-448px-V2_5 Qwen2.5-7B InternViT-6B-448px-V2_5 Qwen2.5-32B InternViT-300M & InternViT-6B InternViT-300M & InternViT-6B Qwen3-8B Qwen3-38B Qwen2-7B Qwen2-7B origianl origianl origianl origianl origianl origianl origianl origianl llava-onevision-qwen2-7b-si SigLIP-384-SO400M llava-onevision-qwen2-7b-ov SigLIP-384-SO400M"
        },
        {
            "title": "H MODEL LICENSE",
            "content": "Table 11: Summary of licenses in models that are evaluated in Chart2Code. Entries marked with Not Applicable indicate that authors do not have an explicit code license displayed within the codebase or model checkpoint page."
        },
        {
            "title": "Model License Code License",
            "content": "Proprietary GPT-5 Proprietary Claude 4 Sonnet Proprietary Gemini-2.5-pro Proprietary doubao-seed-1.6 Proprietary doubao-seed-1.5 qwen Qwen2-VL-7B qwen Qwen2-VL-72B qwen qwen2.5-VL-7B qwen qwen2.5-VL-72B deepseek deepseek-VL-7B MIT kimi-VL-A3B MIT MiMo-VL-7B-RL MIT MiMo-VL-7B-SFT glm-4 GLM-4-9B Apache-2.0 Intern-VL 2.5 8B Apache-2.0 Intern-VL 2.5 38B Apache-2.0 Intern-VL 3 8B Apache-2.0 Intern-VL 3 38B Apache-2.0 Intern-VL 3.5 8B Apache 2.0 Intern-VL 3.5 38B llava-onevision-qwen2-7b-si Apache 2.0 llava-onevision-qwen2-7b-ov Apache 2.0 Proprietary Proprietary Proprietary Proprietary Proprietary Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 MIT MIT Apache 2.0 Apache 2.0 Apache 2.0 MIT MIT MIT MIT MIT MIT Apache 2.0 Apache 2."
        },
        {
            "title": "I MODEL SOURCE",
            "content": "Table 12: The release time and model source of LMMs used in our benchmark."
        },
        {
            "title": "Source",
            "content": "GPT-5 Claude 4 Sonnet Gemini-2.5-pro doubao-seed-1.5 doubao-seed-1.6 Qwen2-VL-7B Qwen2-VL-72B qwen2.5-VL-7B qwen2.5-VL-72B deepseek-VL-7B kimi-VL-A3B MiMo-VL-7B-RL MiMo-VL-7B-SFT GLM-4-9B Intern-VL 2.5 8B Intern-VL 2.5 38B Intern-VL 3 8B Intern-VL 3 38B Intern-VL 3.5 8B Intern-VL 3.5 38B llava-onevision-qwen2-7b-si llava-onevision-qwen2-7b-ov 2025-08-07 2025-05-23 2025-06-17 2025-05-11 2025-06-11 2024-09-18 2024-09-18 2025-01-26 2025-01-26 2024-03-09 2024-08-20 2025-08-10 2025-08-10 2024-06-19 2024-11-21 2024-11-21 2025-04-10 2025-04-10 2025-08-25 2024-08-25 2024-07-29 2024-07-25 Closed-source Models https://openai.com/zh-Hans-CN/index/introducing-gpt-5/ https://www.anthropic.com/news/claude-4 https://deepmind.google/models/gemini/pro/ https://www.volcengine.com/product/doubao https://www.volcengine.com/product/doubao Open-source Models https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct https://huggingface.co/deepseek-ai/deepseek-vl-7b-base https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL-2508 https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT-2508 https://huggingface.co/zai-org/glm-4-9b https://huggingface.co/OpenGVLab/InternVL2_5-8B https://huggingface.co/OpenGVLab/InternVL2_5-38B https://huggingface.co/OpenGVLab/InternVL3-8B https://huggingface.co/OpenGVLab/InternVL3-38B https://huggingface.co/OpenGVLab/InternVL3_5-8B https://huggingface.co/OpenGVLab/InternVL3_5-38B https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-si https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov 36 I.1 LEVEL 1 Level 1 Direct sample Instruction: You are Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others.I have plot generated by Python code, but dont have the corresponding code that generated this plot. Your task is to generate the Python code that can perfectly reproduce the picture based on the image provide. Here are the requirements for the task: 1. Data Extraction: Extract the actual data from the provided image. Based on the visual features of the plot, you must infer the data and recreate the plot. 2. Recreate the Image: Generate the Matplotlib code that reproduces the image exactly as it appears, including all elements such as: - Plot type (scatter, line, bar, etc.) - Axis labels and titles - Colors, markers, line styles, and other visual styles - Any legends, annotations, or gridlines present in the image 3. Self-contained Code: The Python code should be complete, executable, and self-contained. It should not require any external data files or variables not already present in the code. Your objective is to extract the any necessary details from the image and generate Python script that accurately reproduces the plot. Now, please generate the Python code to reproduce the picture below. Reference figure: GT Code: # == CB_38 figure code == import matplotlib.pyplot as plt import numpy as np import matplotlib.colors as mcolors # == CB_38 figure data == capabilities = { Basic Knowledge: [ Vocabulary, Syntax, Semantics, Discourse, Logic, Math Reasoning ],...} datasets = ... def lighten_color(color, amount=0.5): rgb = mcolors.to_rgb(color) return tuple(rgb[i] + (1.0 - rgb[i]) * amount for in range(3)) ... inner_a, size_a, col_a, outer_a, osize_a, ocol_a = prepare_sunburst(capabilities) inner_b, size_b, col_b, outer_b, osize_b, ocol_b = prepare_sunburst(datasets) # == figure plot == fig = plt.figure(figsize=(18.0, 6.0)) plt.subplots_adjust(left=0.05, right=0.85, wspace=0.7) # -- (a) Evaluation capabilities sunburst -- ax1 = fig.add_subplot(1, 3, 1) ... wedges1, _ = ax1.pie(size_a, radius=0.8, labels=None, startangle=90, colors=col_a, wedgeprops=dict (width=0.3, edgecolor=white)) centre = plt.Circle((0, 0), 0.5, color=lightgray, linewidth=0) ax1.add_artist(centre) ax1.text(0, 0, ExplicitnSemantics, ha=center, va=center, fontsize=10, weight=bold) ax1.set(aspect=equal) ax1.set_title((a) Evaluation capabilities, fontsize=12, pad=45) ax1.legend(wedges1, inner_a, title=Capabilities, loc=center left, bbox_to_anchor=(1.3, 0.5), fontsize=9, frameon=False) # -- (b) Evaluation datasets sunburst -- ... centre2 = plt.Circle((0, 0), 0.5, color=lightgray, linewidth=0) ax2.add_artist(centre2) # -- (c) Overview of evaluation results (radar) -- ax3 = fig.add_subplot(1, 3, 3, projection=polar) = len(categories) angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist() angles += angles[:1] ... ax3.xaxis.set_ticks(angles[:-1]) ax3.set_xticklabels([]) ax3.grid(True, linestyle=:) ax3.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0]) ax3.set_ylim(0, 1) ... ax3.set_title((c) Overview of evaluation results, fontsize=12, pad=45) ax3.legend(loc=lower center, bbox_to_anchor=(0.5, -0.5), ncol=3, fontsize=7, frameon=False) 37 Level 1 Direct sample 2 Instruction: You are Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others.I have plot generated by Python code, but dont have the corresponding code that generated this plot. Your task is to generate the Python code that can perfectly reproduce the picture based on the image provide. Here are the requirements for the task: 1. Data Extraction: Extract the actual data from the provided image. Based on the visual features of the plot, you must infer the data and recreate the plot. 2. Recreate the Image: Generate the Matplotlib code that reproduces the image exactly as it appears, including all elements such as: - Plot type (scatter, line, bar, etc.) - Axis labels and titles - Colors, markers, line styles, and other visual styles - Any legends, annotations, or gridlines present in the image 3. Self-contained Code: The Python code should be complete, executable, and self-contained. It should not require any external data files or variables not already present in the code. Your objective is to extract the any necessary details from the image and generate Python script that accurately reproduces the plot. Now, please generate the Python code to reproduce the picture below. Reference figure: GT Code: import numpy as np import matplotlib.pyplot as plt hours = np.arange(24) angles = 2 * np.pi * hours / 24 stationary = ... bus_counts = np.array(... fig = plt.figure(figsize=(10,10)) ... for th in angles: ax.plot([th, th], [0, 160], color=grey, linewidth=0.5) baseline = 100 theta = np.linspace(0, 2*np.pi, 360) ax.plot(theta, np.full_like(theta, baseline), linestyle=--, color=black, linewidth=1) inner_circle = np.mean(no2) ax.plot(theta, np.full_like(theta, inner_circle), linestyle=--, color=grey, linewidth=1) ax.text(0, 0, rNO$_2$Clock, fontsize=18, fontweight=bold, ha=center, va=center) bar_width = 2*np.pi/24 * 0.2 offsets = np.array([-1.5, -0.5, 0.5, 1.5]) * bar_width for vals, off, color, label in zip( [stationary, bus_counts, truck_counts, industry_proximity], offsets, [tab:blue,tab:red,lightpink,skyblue],.. ax.bar(angles + off, vals * 100, bottom=baseline, width=bar_width, color=color, label=label) scale = 0.8 no2_scaled = baseline + (no2 - baseline) * scale ln, = ax.plot(angles, no2_scaled, color=black, linewidth=2, label=NO2 max value) ax.fill(angles, no2_scaled, color=grey, alpha=0.7) for ang, orig_val, in zip(angles, no2, no2_scaled): ax.text(ang, + 2, f{orig_val}, ha=center, va=bottom, fontsize=8, color=black) ax.text(np.deg2rad(30), baseline+15, +ve, fontsize=14, fontweight=bold, ha=center) ax.text(np.deg2rad(30), baseline-15, -ve, fontsize=14, fontweight=bold, ha=center) ax.legend(loc=upper right, bbox_to_anchor=(1.1,1.1), fontsize=10) plt.show() 38 Level 1 Direct sample 3 Instruction: You are Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others.I have plot generated by Python code, but dont have the corresponding code that generated this plot. Your task is to generate the Python code that can perfectly reproduce the picture based on the image provide. Here are the requirements for the task: 1. Data Extraction: Extract the actual data from the provided image. Based on the visual features of the plot, you must infer the data and recreate the plot. 2. Recreate the Image: Generate the Matplotlib code that reproduces the image exactly as it appears, including all elements such as: - Plot type (scatter, line, bar, etc.) - Axis labels and titles - Colors, markers, line styles, and other visual styles - Any legends, annotations, or gridlines present in the image 3. Self-contained Code: The Python code should be complete, executable, and self-contained. It should not require any external data files or variables not already present in the code. Your objective is to extract the any necessary details from the image and generate Python script that accurately reproduces the plot. Now, please generate the Python code to reproduce the picture below. Reference figure: GT Code: import matplotlib.pyplot as plt import numpy as np # == New radar figure data == labels = [Compute, Storage, Networking, Database, AI/ML, Security] num_metrics = len(labels) # angle of each axis in the plot (in radians) angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist() # complete the loop angles += angles[:1] # Values for each industrys cloud service adoption (0-100 scale) data = ... industries = list(data.keys()) # New modern color scheme colors = ... # == figure plot == fig, axes = plt.subplots(2, 5, axes = axes.ravel() figsize=(15.0, 9.0), # Slightly larger for readability subplot_kw=dict(polar=True)) for ax, name in zip(axes, industries): vals = data[name] # close the loop vals_loop = vals + vals[:1] = industries.index(name) .... ax.set_yticks(rticks) ax.set_yticklabels([f\"{int(x)}\" for in rticks], fontsize=8) ax.set_ylim(0, max_val * 1.1) # Add small buffer to max_val # title ax.set_title(name, fontsize=12, fontweight=bold, pad=10) # light grid ax.grid(color=gray, linestyle=--, linewidth=0.5, alpha=0.7) ax.spines[polar].set_linewidth(1.0) ... ... plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for potential suptitle plt.suptitle(Cloud Service Adoption Across Industries, fontsize=16, fontweight=bold, y=0.99) plt.savefig(\"./datasets_level2/radar_15.png\", bbox_inches=\"tight\", dpi=300) # Save the figure plt.show() 39 Level 1 Customized (raw data) sample 1 Instruction: want to use heatmap to show the variation range of each category for each month, with the horizontal axis representing time and the vertical axis representing the three categories: Energy, Metals, and Food. The color intensity represents the magnitude of the variation. Please refer to the uploaded image style to generate runnable Python code. Reference figure: Raw data: \"dates\": [ \"2020-01-01\", \"2020-02-01\", ... \"2024-08-01\", \"2024-09-01\" ], \"commodities\": [ \"Energy\", \"Metals\", \"Food\" ], \"values\": [ [ 4.7, ... -8.1 ], [ 1.6, ... -4.7 ], [ 8.8, ... -0.3 ] ] GT Code: import numpy as np import matplotlib.pyplot as plt # Data dates = .. commodities = [\"Energy\", \"Metals\", \"Food\"] values = .. data = np.array(values) # Plot fig, ax = plt.subplots(figsize=(14, 6)) fig.subplots_adjust(bottom=0.25) # Determine symmetric range around zero max_abs = np.max(np.abs(data)) im = ax.imshow(data, cmap=RdYlBu_r, aspect=auto, vmin=-max_abs, vmax=max_abs) ... # Labels and title ax.set_xlabel(Month, fontsize=14) ax.set_title(Monthly Commodity Price Change (%), fontsize=16, fontweight=bold) # Gridlines ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True) ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True) ax.grid(which=minor, color=white, linestyle=-, linewidth=2) ax.tick_params(which=minor, bottom=False, left=False) # Colorbar cbar = fig.colorbar(im, ax=ax, orientation=horizontal, pad=0.3, aspect=40, shrink=0.8) cbar.set_label(Change (%), fontsize=12) cbar.ax.tick_params(labelsize=12) plt.show() GT Figure: 40 Level 1 Customized (table figure) sample 1 Instruction: want to use the data from the uploaded director compensation table (PNG) and create combination chart based on the style of the reference combination chart: the horizontal axis represents the names of the directors, the bar chart displays cash compensation, stock awards, and total compensation respectively, and dashed line chart highlights the trends of these three items. Thank you! Adjust the image size to match the aspect ratio of the reference image; use the dark blue, cyan, and light gray tones from the reference image; for the x-axis labels, tilt them 45 degrees and align them to the right, mimicking the text style of the reference image; add title centered at the top, with font effects similar to the reference image; set the y-axis scale range and intervals according to the reference image; keep the legend position consistent with the reference image, arranged horizontally at the top; apply dashed line styles as in the reference image, and mimic the marker shapes from the reference image. Reference figure: Data figure: GT Code: import numpy as np import matplotlib.pyplot as plt plt.rcParams.update({ font.family: sans-serif, font.sans-serif: [Arial] }) names = [Dr. Ruey-Bin Kao, Julien Mininberg, ..., Eva Manolis] fees = [81250, 38599, ..., 90307] stock_awards = [199997, 199995, ..., 199997] total = [281247, 238594, ..., 290304] = np.arange(len(names)) fig, ax = plt.subplots(figsize=(12, 6)) ax.grid(axis=y, linestyle=--, alpha=0.7) ax.bar(x - 0.25, fees, 0.25, label=Fees Earned, color=#1f77b4, alpha=0.8) ax.bar(x, stock_awards, 0.25, label=Stock Awards, color=#4c9dbd, alpha=0.8) ax.bar(x + 0.25, total, 0.25, label=Total Compensation, color=#e0e0e0, alpha=0.8) ax.plot(x, fees, --o, color=#1f77b4, label=Fees Line) ax.plot(x, stock_awards, --o, color=#ff7f0e, label=Stock Awards Line) ax.plot(x, total, --o, color=#2ca02c, label=Total Line) ax.set_xticks(x) ax.set_xticklabels(names, rotation=45, ha=right) ax.set_ylabel(Compensation ($)) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, loc=upper center, bbox_to_anchor=(0.5, 1.15), ncol=3) plt.tight_layout() plt.show() GT Figure: 41 I. LEVEL 2 Level 2 sample 1 Instruction: Use GridSpec to create complex 1+2 layout. The top section will feature large subplot (spanning the entire width) to display \"raincloud plots\" (half-violin plots + box plots + scatter plots) for all four categories... enabling an in-depth comparison of these two distinctly different distributions. On this basis: - Set the overall canvas size to 14 inches wide 10 inches high. - Continue using four fixed colors: light orange #FFC0A0, light green #B0E0B0, light purple #B9A0E0, and beige #FFE4C4. Use red line to mark the mean value in the histograms. - Use GridSpec layout with two rows and two columns. The first row spans both columns for the top plot, while the second row places the two histograms side by side, one in each column. The row height ratio should be explicitly set to 2:1. .... - Rotate the X-axis tick labels of the top subplot counterclockwise by 20 degrees. - Maintain white background and gray grid lines (#D3D3D3). Reference figure: GT Code: # == line_19 figure code == import matplotlib.pyplot as plt import numpy as np import matplotlib.gridspec as gridspec # == line_19 figure data == epochs = np.arange(1, 31) # FinTabNet variants ftn_a1 = np.array([... ... # == Data Processing for Dashboard == # 1. Group data fintabnet_group_data = np.array([ftn_a1, ftn_a2, ftn_a3, ftn_a4, ftn_a5, ftn_a6]) pt1m_based_group_data = np.array([pubtables, pt1m_av1, pt1m_av6]) all_models_data = np.vstack([fintabnet_group_data, pt1m_based_group_data]) all_models_labels = [FTN.a1, FTN.a2, FTN.a3, FTN.a4, FTN.a5, FTN.a6, PubTables, PT1M+FTN.av1, PT1M+FTN.av6] ... # 3. Final performance data ... # 4. Significant surpass point diff = pt1m_av6 - pubtables surpass_margin = 0.05 surpass_epoch_idx = np.where(diff > surpass_margin)[0] first_surpass_epoch = epochs[surpass_epoch_idx[0]] if len(surpass_epoch_idx) > 0 else None ... # Plot 3: Key Model Showdown ... plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # plt.savefig(\"./datasets/line_19.png\") plt.show() GT figure: 42 Level 2 sample 2 Instruction: Create comprehensive, dashboard-style analytical view that juxtaposes raw data trends, statistical distributions, and localized details. 1. Layout Modifications: Use GridSpec to create complex 2x2 grid layout. The top-left main plot (spanning the 1st row and 1st column) is composite chart (three CCA lines + CKA bar chart). The top-right subplot (spanning the 1st row and 2nd column) is box plot, used to display the overall data distribution of four data series (cca_top1, cca_top3, cca_top10, cka). The large bottom plot (spanning the 2nd row and all columns) is \"zoomed-in\" view of the main plot, specifically focusing on the \"Center Layer\" in the range of 10 to 20 for the CCA line chart details. 2. Chart Type Conversion and Combination: In the top-right subplot, create box plot for each of the four datasets and set appropriate labels. In the bottom zoomed-in plot, only draw the three CCA line charts and omit the CKA bar chart to emphasize the localized CCA dynamics. ... Additional Requirements: Set the canvas size to 1510 inches. Use 22 GridSpec layout with width ratios [2,1] and height ratios [1,1]. The top-left main plot occupies the 1st row and 1st column, the top-right box plot occupies the 1st row and 2nd column, and the bottom zoomed-in plot spans the 2nd row across all columns... For the box plots, use fill color of #d3d3d3, black borders, and red median lines. For the zoomed-in region rectangle, use gray fill with transparency 0.2, red dashed border, and red dashed connecting lines. Reference figure: GT Code: import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from matplotlib.patches import Rectangle, ConnectionPatch import numpy as np layers = list(range(2, 30)) cca_top1 = [0.997, 0.997, ... # Create figure with constrained layout fig = plt.figure(figsize=(15, 10), constrained_layout=True) gs = gridspec.GridSpec(2, 2, width_ratios=[2, 1], height_ratios=[1, 1], figure=fig) ... ax_main_twin = ax_main.twinx() # --- Main Plot (Top-Left) --- bar_width = 0.6 ... labels = [h.get_label() for in handles] ax_main.legend(handles, labels, loc=lower center, ncol=4, fontsize=10, bbox_to_anchor=(0.5, -0.25)) # --- Box Plot (Top-Right) --- data_for_boxplot = [cca_top1, cca_top3, cca_top10, cka] box_labels = [CCA:Top 1, CCA:Top 3, CCA:Top 10, CKA] ... ax_box.grid(True, axis=y, linestyle=--, linewidth=0.5, alpha=0.7) # --- Zoomed Plot (Bottom) --- zoom_range = (10, 20) ax_zoom.plot(layers, cca_top1, color=#1f77b4, marker=o, markersize=6, lw=1.5) ... ax_zoom.grid(True, linestyle=--, linewidth=0.5, alpha=0.7) # --- Visual Connection --- rect = Rectangle((zoom_range[0], 0.84), zoom_range[1] - zoom_range[0], 1.002 - 0.84, facecolor=grey, alpha=0.2, edgecolor=red, linestyle=--) ... fig.add_artist(con2) plt.show() GT figure: 43 Level 2 sample Instruction: Create comprehensive, dashboard-style multi-panel analysis plot to deeply explore the relationships between model performance, tool wear growth, and model comparisons. The specific requirements are as follows: Reference figure: GT Code: import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import matplotlib.gridspec as gridspec np.random.seed(0) runs = np.arange(5, 15) mean35 = [2.5, .. std35 = [0.5,... fig = plt.figure(figsize=(18, 10)) gs = gridspec.GridSpec(2, 2, width_ratios=[3, 2], height_ratios=[1, 1]) pos1 = runs - 0.2 pos2 = runs + 0.2 .. vp1 = ax_main.violinplot(data35, positions=pos1, widths=0.4, showmedians=True) ax_main.grid(True, linestyle=\"--\", alpha=0.6) ax_main.set_title(\"A) Model Performance Distribution vs. Tool Wear\", fontsize=16, loc=left) ax_wear = ax_main.twinx() ax_wear.plot(runs, tool_wear, color=\"red\", marker=\"o\", markersize=6, linewidth=2) ax_wear.set_ylabel(\"Tool Wear (mm)\", color=\"red\", fontsize=14) ... ax_growth.grid(axis=y, linestyle=--, alpha=0.6) median35 = [np.median(d) for in data35] median4 = [np.median(d) for in data4] highlight_run_idx = max_growth_idx + 1 ax_compare.scatter(median35, median4, c=runs, cmap=viridis, s=60, alpha=0.8) ... ax_compare.grid(True, linestyle=--, alpha=0.6) ax_compare.text(0.95, 0.05, GPT-4 Better, transform=ax_compare.transAxes, ha=right, va=bottom, fontsize=12, color=green, style=italic) ... ax_main.annotate(Max Wear Growth, xy=(max_growth_run, 4.0), xytext=(max_growth_run, 5.0), arrowprops=dict(facecolor=#e31a1c, shrink=0.05, width=1.5, headwidth=8), fontsize=12, color=#e31a1c, ha=center, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\" white\", ec=\"#e31a1c\", lw=1)) fig.suptitle(\"Comprehensive Analysis of LLM-based Digital Twin Performance\", fontsize=20, y=0.98) plt.tight_layout(rect=[0, 0, 1, 0.95]) plt.show() GT figure: 44 Level 2 sample 4 Instruction: 1. Use GridSpec to create complex dashboard-style layout: - The left side contains main plot occupying 2x2 space. - The right side contains two subplots, each occupying 1x1 space. 2. **Main Plot (Left Side)**: - Retain the original bar chart and exponential trend line. - Display the absolute values and trends of the annual research count. 3. **Top-Right Subplot**: - Convert the original data into an area chart. - Show the cumulative total of research counts to analyze the expansion of overall scale. 4. **Bottom-Right Subplot**: - Use donut chart to display the proportion of research counts from the last three years (20222024) relative to their total. - Highlight the distribution of recent contributions. 5. Add titles to all subplots and ensure unified visual style for clear communication and coordinated layout. **Additional Modifications**: - Adjust the overall canvas size to 16 inches 9 inches. - Configure the layout as GridSpec(2,3): - The main plot occupies the first and second columns of all rows. - The top-right subplot is placed in the first row, third column. - The bottom-right subplot is placed in the second row, third column. - **Styling**: - Main plot bar color: #1a5276. - Main plot trend line color: red. - Area chart fill color: #5dade2, line color: #1a5276. - Donut chart colors: [#1abc9c, #f1c40f, #e74c3c]. - Donut chart percentage text: white and bold. - Overall title font: size 22, bold. - Subplot titles font: size 16. - Axis titles font: size 14. - Tick labels font: size 12. - Top-right chart annotations font: size 12, bold. - Donut chart center text font: size 14, bold. - Pie chart percentage text font: size 8, bold. Reference figure: GT Code: import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec years = np.array([2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]) = np.arange(len(years)) ... gs = gridspec.GridSpec(2, 3, figure=fig) ax1 = fig.add_subplot(gs[:, 0:2]) ... ax1.set_xlabel(Year, fontsize=14, color=grey) ax1.set_ylabel(Number of Studies, fontsize=14, color=grey) ... for spine in [top, right]: ax2.spines[spine].set_visible(False) ax2.grid(axis=y, linestyle=--, alpha=0.7) ax2.text(years[-1], cumulative_y[-1], Total:n {cumulative_y[-1]}, ha=right, va=top, fontsize=12, fontweight=bold) colors = [#1abc9c, #f1c40f, #e74c3c] wedges, texts, autotexts = ax3.pie(last_3_years_data,... ax3.add_artist(centre_circle) ax3.set_title(Contribution in Last 3 Years, fontsize=16, pad=10) ax3.text(0, 0, fTotal:n{sum(last_3_years_data)}, ha=center, va=center, fontsize=14, fontweight=bold) plt.setp(autotexts, size=8, weight=\"bold\", color=\"white\") plt.tight_layout(rect=[0, 0, 1, 0.95]) plt.show() GT figure: Level 2 sample 5 Instruction: Create 2x2 dashboard to comprehensively compare model performance. 1. **Top-left plot (Performance Trend Comparison):** Divide the models into two groups: FinTabNet and PT1M-based. ... 2. **Top-right plot (Final Performance Ranking):** Use horizontal bar chart to show the final accuracy of all 9 models at the last epoch..y. 3. **Bottom-left plot (Key Model Showdown):** Plot the performance curves of the best model pt1m_av6 and the baseline model pubtables separately. Identify the epoch where pt1m_av6 first surpasses pubtables by more than 0.05 in accuracy, and use axvspan to highlight the region from ... 4. **Bottom-right plot (Performance vs. Stability):** Create scatter plot where the X-axis represents the average accuracy of each model (mean over 30 epochs), and the Y-axis represents the standard deviation of accuracy. This plot evaluates whether high performance is accompanied by high instability. Add text labels to the best-performing, most stable, and most unstable models on the plot. Additional Modifications: - Set the overall canvas size to 1612 inches. - Use 2-row, 2-column GridSpec layout with row spacing of 0.4 and column spacing of 0.3. - Use bold font size of 20 for the main title, regular font size of 12 for subplot titles, axis labels, and tick marks, and font size of 10 for legends... and semi-transparency. Use font size 9 for labels and adjust them horizontally by 0.002. - Use dashed grid lines with approximately 30 Reference figure: GT Code: import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import matplotlib.gridspec as gridspec data = { \"TopK_65k_256\": [-0.4625, -0.4049, ... clean_data = {k: [x for in if is not None] for k, in data.items()} ... fig.suptitle(\"Comprehensive Analysis of Pearson Correlation\", fontsize=20, fontweight=bold) # --- Top Plot: Raincloud Plot --- order = [\"TopK_65k_256\", \"MatryoshkaTopk_65k_256\", \"GemmaScope_65k\", \"Random_init_65k_256\"] colors = [\"#FFC0A0\", \"#B0E0B0\", \"#B9A0E0\", \"#FFE4C4\"] ... # Jittered points sns.stripplot(x=\"SAE Type\", y=\"Pearson Correlation\", data=df, order=order, ax=ax_main,.. # Boxplot sns.boxplot(x=\"SAE Type\", y=\"Pearson Correlation\", data=df, order=order, ax=ax_main, ... ax_main.tick_params(axis=x, labelsize=12, labelrotation=-20) ax_main.tick_params(axis=y, labelsize=12) # --- Bottom-Right Plot: Histogram for Random_init --- random_data = df[df[\"SAE Type\"] == \"Random_init_65k_256\"][\"Pearson Correlation\"] ... sns.despine(fig=fig) plt.tight_layout(rect=[0, 0, 1, 0.96]) plt.show() GT figure: I.3 LEVEL 3 Level 3 sample 1 Instruction: have an Excel spreadsheet to analyze, which contains fuel types and corresponding horsepower values. Please generate plotting code based on the style of the grouped box plot uploaded to display the horsepower distribution for different fuel types. Use canvas size precisely 13 inches wide and 8 inches high, with the color scheme set to Set3. The entire chart should contain only one subplot, without complex layouts like GridSpec. The title should be \"Horsepower by Fuel Type,\" the X-axis label should be \"Fuel Type,\" and the Y-axis label should be \"Horsepower (hp).\" Keep all text at Matplotlibs default font size and style; rotate the X-axis tick labels 45 degrees; finally, apply tight layout to ensure there is no excess whitespace between elements. Reference Figure: GT Code: import matplotlib.pyplot as plt import seaborn as sns import pandas as pd data_x_groups = [plug in hyrbrid, Petrol, Diesel, Hybrid, ...] data_y_values = [963.0, 563.0, 381.0, 1160.0, ...] df = pd.DataFrame({ fuel_types: data_x_groups, horsepower_num: data_y_values }) plt.figure(figsize=(13, 8)) sns.boxenplot(data=df, x=fuel_types, y=horsepower_num, palette=Set3) plt.title(Horsepower by Fuel Type) plt.xlabel(Fuel Type) plt.ylabel(Horsepower (hp)) plt.xticks(rotation=45) plt.tight_layout() plt.show() GT Figure: Level 3 sample 2 Instruction: Based on the Excel table to be analyzed, mimic the drawing style of the image uploaded as an attachment to create scatter plot of Email1 length and Email2 length. The specific requirements are as follows: 1. Set the image size to 8 inches wide and 8 inches high; 2. Use cross-shaped markers for the scatter plot, with fixed size of 200, marker border width of 2, and map the \"coolwarm\" color scheme starting from sample index 1; 3. Add color bar on the right side, with gap of 0.05 between the color bar and the main plot, and set the aspect ratio to 1:30; 4. Add gray dashed arrows on the color bar, with the arrow style as \"\", line type as dashed, line width of 2, pointing from above (2.8) to below (2.8) on the color bar scale; 5. Replace the color bar label with \"Index\", rotate it vertically by 90 degrees, font size 14, bold; 6. The main title of the chart is \"(a) Correlation of Email1 and Email2 Lengths\", 47 font size 24, bold, 20 units from the top edge, with vertical position set to 1.05; 7. Both the horizontal axis title \"Email1 Length\" and the vertical axis title \"Email2 Length\" should use font size 18, bold style, with distance of 10 units from the axis labels; 8. Fix the axis range from 10 to 40, adjust the tick label font size to 14, and do not display grid lines. Reference Figure: GT Code: import numpy as np import matplotlib.pyplot as plt # Data: lengths of Email1 and Email2 email1_len = np.array([18, 20, ... email2_len = np.array([26, 23, ... # Color by index = np.arange(1, len(email1_len) + 1) # Plot fig, ax = plt.subplots(figsize=(8, 8)) sc = ax.scatter(email1_len, email2_len, c=t, cmap=coolwarm, s=200, marker=x, linewidths=2) .. ax.set_title( (a) Correlationnof Email1 and Email2 Lengths, fontsize=24, fontweight=bold, pad=20, y=1. ) ax.set_xlabel(Email1 Length, fontsize=18, fontweight=bold, labelpad=10) ax.set_ylabel(Email2 Length, fontsize=18, fontweight=bold, labelpad=10) ... ax.grid(False) plt.tight_layout() plt.show() GT Figure: Level 3 sample 3 Instruction: have an Excel spreadsheet to analyze, which contains two columns of data: mental_health_history and depression. want to compare the distribution of depression scores between groups with and without mental health history, mimicking the style of the image uploaded as an attachment, and generate box plot with width of 10 inches and height of 6 inches: - Use fill color \"#FFA07A\" for the group without mental health history and \"#20B2AA\" for the group with mental health history. The box edges, whiskers, caps, and median line colors should be \"#CC8062\" and \"#1A8E88\" (corresponding to the two groups). - Do not display outliers; - Plot scatter points offset by 0.2 on either side of the box, with scatter point colors matching the corresponding box fill color. The point edge color should be white, with an edge width of 0.5, size 50, opacity 0.8, and add random jitter of 0.04 horizontally; - Set the overall background color to \"#E5F7FD,\" grid line color to white, and style to solid lines; - X-axis tick labels should be \"No History\" and \"With History,\" with font size of 14; - Y-axis should display range from 0 to 30 with step of 5, and tick label font size should be 14; - Y-axis title should be \"Depression Score,\" with font size of 18 and bold; - Finally, call automatic layout adjustment to prevent label overlap. Reference Figure: GT Code: import json import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import matplotlib.colors as mcolors # Load data from JSON data_json = {\"x\": [\"0\", \"1\", ... data = json.loads(data_json) ... # Define groups and labels groups = [0, 1] group_labels = [No History, With History] # Define colors colors = [\"#FFA07A\", \"#20B2AA\"] dark_colors = [ mcolors.to_hex(np.clip(np.array(mcolors.to_rgb(c)) * 0.8, 0, 1)) for in colors] # Set theme sns.set_theme( ... # Plot fig, ax = plt.subplots(figsize=(10, 6)) box_offset = +0.2 point_offset = -0.2 jitter = 0.04 for i, in enumerate(groups): vals = df.loc[df[mental_health_history] == g, depression].values # Boxplot ax.boxplot( ... # Customize axes ax.set_xticks(range(len(groups))) ax.set_xticklabels(group_labels, fontsize=14) ... plt.tight_layout() plt.show() GT Figure: 49 Figure 12: Selected charts of the Chart2Code."
        },
        {
            "title": "J EVALUATION CODE",
            "content": "J.1 COLOR"
        },
        {
            "title": "Color evaluation code",
            "content": "class ColorEvaluator: TYPE_WEIGHTS = { patch_face: 1.0, line_color: 1.0, scatter_color: 1.0, scatter_palette: 0.7, text_color: 1.0, poly3d_palette: 0.7, patch_edge: 0.01, axes_bg: 0.01, figure_bg: 0.01, spine: 0.01, tick_label: 0.05, axis_label: 0.05, title: 0.05, legend_text: 0.05, legend_bg: 0.01, } DEFAULT_WEIGHT = 0.1 def __init__(self) -> None: self.metrics = ColorMetrics() def __call__(self, gen_fig: Optional[Figure], gt_fig: Optional[Figure]) -> ColorMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"can not find Figure \" return self.metrics try: generation_data = self._extract_colors_from_figure_expert(gen_fig) gt_data = self._extract_colors_from_figure_expert(gt_fig) self._calculate_metrics(generation_data, gt_data) except Exception as e: logger.error(f\"color evaluate error: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_colors_from_figure_expert(self, figure: Figure) -> Dict[str, Dict[str, str]]: extracted_data = defaultdict(dict) fallback_counters = defaultdict(int) if color := convert_color_to_hex(figure.patch.get_facecolor()): extracted_data[figure_bg][ figure] = color for ax in figure.axes: if color := convert_color_to_hex(ax.patch.get_facecolor()): extracted_data[axes_bg][f ax_{id(ax)}] = color if ax.get_legend(): for handle, label in zip(ax.get_legend().legend_handles, ax.get_legend().get_texts()): key = label.get_text() color = None if hasattr(handle, get_facecolor): color = convert_color_to_hex(handle. get_facecolor()) elif hasattr(handle, get_color): color = convert_color_to_hex(handle.get_color()) if color: if isinstance(handle, plt.Rectangle): extracted_data[patch_face][key] = color else: extracted_data[line_color][key] = color try: tick_labels = [tick.get_text() for tick in ax.get_xticklabels()] for i, patch in enumerate(ax.patches): if color := convert_color_to_hex(patch.get_facecolor()): key = tick_labels[i] if < len(tick_labels) and tick_labels[i] else None if not key: key = f\"patch_{fallback_counters[patch_face]}\"; fallback_counters[ patch_face] += 1 if key not in extracted_data[patch_face]: extracted_data[patch_face][key] = color if e_color := convert_color_to_hex(patch.get_edgecolor()): key = tick_labels[i] if < len(tick_labels) and tick_labels[i] else f\" patch_edge_{i}\" extracted_data[patch_edge][key] = e_color except Exception as e: logger.warning(f\"handing Patches error: {e}\") try: for line in ax.lines: if color := convert_color_to_hex(line.get_color()): key = line.get_label() 51 if not key or key.startswith(_): key = f\"line_{fallback_counters[line_color ]}\"; fallback_counters[line_color] += 1 if key not in extracted_data[line_color]: extracted_data[line_color][key] = except Exception as e: logger.warning(f\"handing Lines error: {e}\") color try: for collection in ax.collections: colors = collection.get_facecolors() if len(colors) == 0: continue if len(set(map(tuple, colors))) == 1: if color := convert_color_to_hex(colors[0]): key = collection.get_label() if not key or key.startswith(_): key = f\"scatter_group_{fallback_counters[ scatter_color]}\"; fallback_counters[scatter_color] += 1 if key not in extracted_data[scatter_color]: extracted_data[scatter_color ][key] = color else: for in {convert_color_to_hex(c) for in colors if is not None}: key = f\"palette_color_{fallback_counters[scatter_palette]}\"; fallback_counters[scatter_palette] += extracted_data[scatter_palette][key] = except Exception as e: logger.warning(f\"handle Collections error: {e}\") try: for text in ax.texts: if color := convert_color_to_hex(text.get_color()): key = text.get_text() if key: extracted_data[text_color][key] = color except Exception as e: logger.warning(f\"handle Texts error: {e}\") if (color := convert_color_to_hex(ax.title.get_color())): extracted_data[title][title ] = color if (color := convert_color_to_hex(ax.xaxis.label.get_color())): extracted_data[ axis_label][xlabel] = color if (color := convert_color_to_hex(ax.yaxis.label.get_color())): extracted_data[ axis_label][ylabel] = color return dict(extracted_data) J.2 GRID"
        },
        {
            "title": "Grid evaluation code",
            "content": "class GridEvaluator: def __init__(self) -> None: self.metrics = GridMetrics() def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> GridMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: generation_grids = self._extract_grids_from_figure(gen_fig) gt_grids = self._extract_grids_from_figure(gt_fig) self._calculate_metrics(generation_grids, gt_grids) except Exception as e: logger.error(f\"Error during grid evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_grids_from_figure(self, fig: plt.Figure) -> List[Dict]: \"\"\"Directly extracts grid information from Figure object.\"\"\" grids = [] for ax in fig.axes: x_grid_visible = any(line.get_visible() for line in ax.get_xgridlines()) y_grid_visible = any(line.get_visible() for line in ax.get_ygridlines()) if x_grid_visible or y_grid_visible: grids.append({ x_grid_visible: x_grid_visible, y_grid_visible: y_grid_visible }) return grids def _calculate_metrics(self, generation_grids: List[Dict], gt_grids: List[Dict]) -> None: \"\"\"Calculates precision, recall, and F1-score for grid usage.\"\"\" if not generation_grids and not gt_grids: 52 self.metrics.precision = 1.0; self.metrics.recall = 1.0; self.metrics.f1 = 1.0 return if not gt_grids or not generation_grids: self.metrics.precision = 0.0; self.metrics.recall = 0.0; self.metrics.f1 = 0.0 return n_correct = 0 gt_grids_copy = gt_grids.copy() for gen_grid in generation_grids: if gen_grid in gt_grids_copy: n_correct += 1 gt_grids_copy.remove(gen_grid) self.metrics.precision = n_correct / len(generation_grids) if generation_grids else 1.0 self.metrics.recall = n_correct / len(gt_grids) if gt_grids else 1.0 if self.metrics.precision + self.metrics.recall > 0: self.metrics.f1 = 2 * self.metrics.precision * self.metrics.recall / (self.metrics. precision + self.metrics.recall) else: self.metrics.f1 = 0. J.3 LAYOUT"
        },
        {
            "title": "Layout evaluation code",
            "content": "class LayoutEvaluator: def __init__(self) -> None: self.metrics = LayoutMetrics() def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure], gen_file_path: str, gt_file_path: str) -> LayoutMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: generation_layouts = self._extract_layout_from_figure(gen_fig, gen_file_path) gt_layouts = self._extract_layout_from_figure(gt_fig, gt_file_path) self._calculate_metrics(generation_layouts, gt_layouts) except Exception as e: logger.error(f\"Error during layout evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_layout_from_figure(self, fig: plt.Figure, file_path: str) -> List[Dict]: if \"/graph\" in file_path: return [dict(nrows=1, ncols=1, row_start=0, row_end=0, col_start=0, col_end=0)] layout_info = [] for ax in fig.axes: spec = ax.get_subplotspec() if spec is None: continue gs = spec.get_gridspec() nrows, ncols = gs.get_geometry() row_start, row_end = spec.rowspan.start, spec.rowspan.stop - 1 col_start, col_end = spec.colspan.start, spec.colspan.stop - 1 layout_info.append(dict( nrows=nrows, ncols=ncols, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end )) return layout_info def _calculate_metrics(self, generation_layouts: List[Dict], gt_layouts: List[Dict]) -> None: if not generation_layouts and not gt_layouts: self.metrics.precision = 1.0; self.metrics.recall = 1.0; self.metrics.f1 = 1.0 return if not gt_layouts or not generation_layouts: self.metrics.precision = 0.0; self.metrics.recall = 0.0; self.metrics.f1 = 0.0 return n_correct = 0 gt_layouts_copy = gt_layouts.copy() for layout in generation_layouts: if layout in gt_layouts_copy: n_correct += 1 gt_layouts_copy.remove(layout) self.metrics.precision = n_correct / len(generation_layouts) if generation_layouts else 1.0 self.metrics.recall = n_correct / len(gt_layouts) if gt_layouts else 1.0 if self.metrics.precision + self.metrics.recall > 0: self.metrics.f1 = 2 * self.metrics.precision * self.metrics.recall / (self.metrics. precision + self.metrics.recall) else: self.metrics.f1 = 0.0 53 J.4 LEGEND"
        },
        {
            "title": "Legend evaluation code",
            "content": "class LegendEvaluator: def __init__(self, use_position: bool = True) -> None: self.metrics = LegendMetrics() self.use_position = use_position def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> LegendMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: gen_fig.canvas.draw() gt_fig.canvas.draw() generation_legends = self._extract_legends_from_figure(gen_fig) gt_legends = self._extract_legends_from_figure(gt_fig) self._calculate_metrics(generation_legends, gt_legends) except Exception as e: logger.error(f\"Error during legend evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_legends_from_figure(self, fig: plt.Figure) -> List[Dict]: legends_info = [] renderer = fig.canvas.get_renderer() all_legends = fig.legends[:] for ax in fig.axes: if ax.get_legend(): all_legends.append(ax.get_legend()) for legend in set(all_legends): if not legend or not legend.get_visible(): continue legend_bbox = legend.get_window_extent(renderer) for text_obj in legend.get_texts(): if text_obj.get_visible() and text_obj.get_text(): legends_info.append({ \"text\": text_obj.get_text(), \"bbox\": (legend_bbox.x0, legend_bbox.y0, legend_bbox.x1, legend_bbox.y1) }) return legends_info def _calculate_metrics(self, generation_legends: List[Dict], gt_legends: List[Dict]) -> None: if not generation_legends and not gt_legends: self.metrics.precision = 1.0; self.metrics.recall = 1.0; self.metrics.f1 = 1.0 return if not gt_legends or not generation_legends: self.metrics.precision = 0.0; self.metrics.recall = 0.0; self.metrics.f1 = 0.0 return n_correct = 0 gt_legends_copy = gt_legends.copy() for gen_legend in generation_legends: best_match = None for gt_legend in gt_legends_copy: if gen_legend[\"text\"] == gt_legend[\"text\"]: if self.use_position: gen_box, gt_box = gen_legend[\"bbox\"], gt_legend[\"bbox\"] xA = max(gen_box[0], gt_box[0]); yA = max(gen_box[1], gt_box[1]) xB = min(gen_box[2], gt_box[2]); yB = min(gen_box[3], gt_box[3]) interArea = max(0, xB - xA) * max(0, yB - yA) if interArea > 0: best_match = gt_legend break else: best_match = gt_legend break if best_match: n_correct += 1 gt_legends_copy.remove(best_match) self.metrics.precision = n_correct / len(generation_legends) if generation_legends else 1.0 self.metrics.recall = n_correct / len(gt_legends) if gt_legends else 1.0 if self.metrics.precision + self.metrics.recall > 0: self.metrics.f1 = 2 * self.metrics.precision * self.metrics.recall / (self.metrics. precision + self.metrics.recall) else: self.metrics.f1 = 0. 54 J.5 VISUAL"
        },
        {
            "title": "Visual evaluation code",
            "content": "class ParameterEvaluator: def __init__(self) -> None: self.metrics = ParameterMetrics() self.DATA_PARAM_KEYS = {xdata, ydata, offsets, xy, verts, width, height, sizes} self.IGNORED_PARAMS = {color, c, colors, label, labels, edgecolor, facecolor} def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> ParameterMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: gen_params = self._extract_params_from_figure(gen_fig) gt_params = self._extract_params_from_figure(gt_fig) self._calculate_strict_metrics(gen_params, gt_params) except Exception as e: logger.error(f\"Error during parameter evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_params_from_figure(self, fig: plt.Figure) -> List[Dict]: extracted_params = [] for ax in fig.axes: for line in ax.lines: params = { type: line, xdata: np.array(line.get_xdata()).tolist(), ydata: np.array( line.get_ydata()).tolist(), linestyle: line.get_linestyle(), linewidth: line.get_linewidth(), marker: line.get_marker(), markersize: line.get_markersize(), alpha: line.get_alpha() } extracted_params.append(params) for patch in ax.patches: params = {alpha: patch.get_alpha()} if isinstance(patch, Rectangle): params.update({ type: rectangle_patch, xy: np.array(patch.get_xy()).tolist(), width: patch.get_width(), height: patch.get_height(), }) extracted_params.append(params) elif isinstance(patch, Polygon): params.update({ type: polygon_patch, verts: np.array(patch.get_xy()).tolist(), }) extracted_params.append(params) for collection in ax.collections: params = {type: collection, alpha: collection.get_alpha()} if hasattr(collection, get_offsets): params[offsets] = np.array(collection.get_offsets()).tolist() if hasattr(collection, get_sizes): params[sizes] = np.array(collection.get_sizes()).tolist() if len(params) > 2: extracted_params.append(params) return extracted_params def _calculate_value_similarity(self, val1: Any, val2: Any) -> float: if val1 is None and val2 is None: return 1.0 if val1 is None or val2 is None: return 0.0 try: if isinstance(val1, str): val1 = float(val1) if isinstance(val2, str): val2 = float(val2) except (ValueError, TypeError): pass if isinstance(val1, (int, float, np.number)) and isinstance(val2, (int, float, np.number)): return 1.0 if np.isclose(val1, val2) else 0.0 if isinstance(val1, (bool, str)): return 1.0 if str(val1) == str(val2) else 0.0 if isinstance(val1, (list, np.ndarray)): if not isinstance(val2, (list, np.ndarray)): return 0.0 if not len(val1) and not len(val2): return 1.0 if not len(val1) or not len(val2): return 0.0 try: v1 = np.asarray(val1, dtype=float).flatten() v2 = np.asarray(val2, dtype=float).flatten() intersection = np.intersect1d(v1, v2).size 55 union = np.union1d(v1, v2).size return intersection / union if union > 0 else 1. except (ValueError, TypeError): set1, set2 = set(str(v) for in val1), set(str(v) for in val2) return len(set1.intersection(set2)) / len(set1.union(set2)) if set1.union(set2) else 1.0 return 0.0 def _calculate_strict_metrics(self, gen_elements: List[Dict], gt_elements: List[Dict]): if not gen_elements and not gt_elements: self.metrics.data_metrics = self.metrics.visual_metrics = ScoreBlock(1.0, 1.0, 1.0) return total_data_score, total_visual_score = 0.0, 0.0 gt_data_count, gt_visual_count = 0, 0 gen_data_count, gen_visual_count = 0, 0 unmatched_gen_elements = gen_elements[:] for gt_elem in gt_elements: best_score, best_match_index = -1.0, -1 for i, gen_elem in enumerate(unmatched_gen_elements): if gt_elem.get(type) == gen_elem.get(type): current_score = sum(self._calculate_value_similarity(gt_elem.get(k), gen_elem.get(k )) for in gt_elem if != type) if current_score > best_score: best_score, best_match_index = current_score, if best_match_index != -1: matched_gen_elem = unmatched_gen_elements.pop(best_match_index) all_keys = set(gt_elem.keys()) set(matched_gen_elem.keys()) for key in all_keys: if key in self.IGNORED_PARAMS or key == type: continue category = data if key in self.DATA_PARAM_KEYS else visual gt_val, gen_val = gt_elem.get(key), matched_gen_elem.get(key) score = self._calculate_value_similarity(gt_val, gen_val) if category == data: total_data_score += score else: total_visual_score += score for key in gt_elem: if key in self.IGNORED_PARAMS or key == type: continue if key in self.DATA_PARAM_KEYS: gt_data_count += 1 else: gt_visual_count += 1 for gen_elem in gen_elements: for key in gen_elem: if key in self.IGNORED_PARAMS or key == type: continue if key in self.DATA_PARAM_KEYS: gen_data_count += 1 else: gen_visual_count += 1 data_p = total_data_score / gen_data_count if gen_data_count > 0 else 1.0 if not gt_data_count else 0.0 data_r = total_data_score / gt_data_count if gt_data_count > 0 else 1.0 if not gen_data_count else 0.0 data_f1 = 2 * (data_p * data_r) / (data_p + data_r) if (data_p + data_r) > 0 else 0.0 self.metrics.data_metrics = ScoreBlock(data_p, data_r, data_f1) visual_p = total_visual_score / gen_visual_count if gen_visual_count > 0 else 1.0 if not gt_visual_count else 0.0 visual_r = total_visual_score / gt_visual_count if gt_visual_count > 0 else 1.0 if not gen_visual_count else 0. visual_f1 = 2 * (visual_p * visual_r) / (visual_p + visual_r) if (visual_p + visual_r) > 0 else 0.0 self.metrics.visual_metrics = ScoreBlock(visual_p, visual_r, visual_f1) J.6 DATA"
        },
        {
            "title": "Data evaluation code",
            "content": "# --- V10: Hardened Evaluator Class with Strict Logic --- class ParameterEvaluator: def __init__(self) -> None: self.metrics = ParameterMetrics() self.DATA_PARAM_KEYS = {xdata, ydata, offsets, xy, verts, width, height, sizes} self.IGNORED_PARAMS = {color, c, colors, label, labels, edgecolor, facecolor} def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> ParameterMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" 56 return self.metrics try: gen_params = self._extract_params_from_figure(gen_fig) gt_params = self._extract_params_from_figure(gt_fig) self._calculate_strict_metrics(gen_params, gt_params) except Exception as e: logger.error(f\"Error during parameter evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_params_from_figure(self, fig: plt.Figure) -> List[Dict]: extracted_params = [] for ax in fig.axes: for line in ax.lines: params = { type: line, xdata: np.array(line.get_xdata()).tolist(), ydata: np.array( line.get_ydata()).tolist(), linestyle: line.get_linestyle(), linewidth: line.get_linewidth(), marker: line.get_marker(), markersize: line.get_markersize(), alpha: line.get_alpha() } extracted_params.append(params) # --- HERE IS THE FIX --- # Differentiate between different types of patches for patch in ax.patches: params = {alpha: patch.get_alpha()} # If its Rectangle (from bar, hist), get width and height if isinstance(patch, Rectangle): params.update({ type: rectangle_patch, xy: np.array(patch.get_xy()).tolist(), width: patch.get_width(), height: patch.get_height(), }) extracted_params.append(params) # If its Polygon (from fill, violinplot), get vertices elif isinstance(patch, Polygon): params.update({ type: polygon_patch, verts: np.array(patch.get_xy()).tolist(), }) extracted_params.append(params) # Can add more patch types here (e.g., Circle, Ellipse) if needed for collection in ax.collections: params = {type: collection, alpha: collection.get_alpha()} if hasattr(collection, get_offsets): params[offsets] = np.array(collection.get_offsets()).tolist() if hasattr(collection, get_sizes): params[sizes] = np.array(collection.get_sizes()).tolist() if len(params) > 2: # Check if any data was actually added besides type and alpha extracted_params.append(params) return extracted_params def _calculate_value_similarity(self, val1: Any, val2: Any) -> float: \"\"\"Strictly compares two values, handling numerics, strings, and lists/arrays.\"\"\" if val1 is None and val2 is None: return 1.0 if val1 is None or val2 is None: return 0.0 try: if isinstance(val1, str): val1 = float(val1) if isinstance(val2, str): val2 = float(val2) except (ValueError, TypeError): pass if isinstance(val1, (int, float, np.number)) and isinstance(val2, (int, float, np.number)): return 1.0 if np.isclose(val1, val2) else 0.0 if isinstance(val1, (bool, str)): return 1.0 if str(val1) == str(val2) else 0.0 if isinstance(val1, (list, np.ndarray)): if not isinstance(val2, (list, np.ndarray)): return 0.0 if not len(val1) and not len(val2): return 1.0 if not len(val1) or not len(val2): return 0.0 try: v1 = np.asarray(val1, dtype=float).flatten() v2 = np.asarray(val2, dtype=float).flatten() intersection = np.intersect1d(v1, v2).size union = np.union1d(v1, v2).size return intersection / union if union > 0 else 1.0 except (ValueError, TypeError): set1, set2 = set(str(v) for in val1), set(str(v) for in val2) return len(set1.intersection(set2)) / len(set1.union(set2)) if set1.union(set2) else 1.0 return 0.0 def _calculate_strict_metrics(self, gen_elements: List[Dict], gt_elements: List[Dict]): if not gen_elements and not gt_elements: self.metrics.data_metrics = self.metrics.visual_metrics = ScoreBlock(1.0, 1.0, 1.0) return 57 total_data_score, total_visual_score = 0.0, 0.0 gt_data_count, gt_visual_count = 0, 0 gen_data_count, gen_visual_count = 0, 0 unmatched_gen_elements = gen_elements[:] for gt_elem in gt_elements: best_score, best_match_index = -1.0, -1 for i, gen_elem in enumerate(unmatched_gen_elements): if gt_elem.get(type) == gen_elem.get(type): current_score = sum(self._calculate_value_similarity(gt_elem.get(k), gen_elem.get(k )) for in gt_elem if != type) if current_score > best_score: best_score, best_match_index = current_score, if best_match_index != -1: matched_gen_elem = unmatched_gen_elements.pop(best_match_index) all_keys = set(gt_elem.keys()) set(matched_gen_elem.keys()) for key in all_keys: if key in self.IGNORED_PARAMS or key == type: continue category = data if key in self.DATA_PARAM_KEYS else visual gt_val, gen_val = gt_elem.get(key), matched_gen_elem.get(key) score = self._calculate_value_similarity(gt_val, gen_val) if category == data: total_data_score += score else: total_visual_score += score for key in gt_elem: if key in self.IGNORED_PARAMS or key == type: continue if key in self.DATA_PARAM_KEYS: gt_data_count += 1 else: gt_visual_count += 1 for gen_elem in gen_elements: for key in gen_elem: if key in self.IGNORED_PARAMS or key == type: continue if key in self.DATA_PARAM_KEYS: gen_data_count += 1 else: gen_visual_count += 1 data_p = total_data_score / gen_data_count if gen_data_count > 0 else 1.0 if not gt_data_count else 0.0 data_r = total_data_score / gt_data_count if gt_data_count > 0 else 1.0 if not gen_data_count else 0.0 data_f1 = 2 * (data_p * data_r) / (data_p + data_r) if (data_p + data_r) > 0 else 0.0 self.metrics.data_metrics = ScoreBlock(data_p, data_r, data_f1) visual_p = total_visual_score / gen_visual_count if gen_visual_count > 0 else 1.0 if not gt_visual_count else 0. visual_r = total_visual_score / gt_visual_count if gt_visual_count > 0 else 1.0 if not gen_visual_count else 0.0 visual_f1 = 2 * (visual_p * visual_r) / (visual_p + visual_r) if (visual_p + visual_r) > 0 else 0.0 self.metrics.visual_metrics = ScoreBlock(visual_p, visual_r, visual_f1) J.7 TEXT"
        },
        {
            "title": "Text evaluation code",
            "content": "class TextEvaluator: def __init__(self) -> None: self.metrics = TextMetrics() def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> TextMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: generation_texts = self._extract_texts_from_figure(gen_fig) gt_texts = self._extract_texts_from_figure(gt_fig) self._calculate_metrics(generation_texts, gt_texts) except Exception as e: logger.error(f\"Error during text evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_texts_from_figure(self, fig: plt.Figure) -> Dict[str, List[str]]: \"\"\"Extracts and categorizes all text elements from Figure object.\"\"\" texts = { \"title\": [], \"xlabel\": [], \"ylabel\": [], \"tick_label\": [], \"suptitle\": [], \"legend_text\": [], \"annotation\": [] } if fig._suptitle and fig._suptitle.get_text(): texts[\"suptitle\"].append(fig._suptitle.get_text()) 58 for ax in fig.axes: if ax.title.get_text(): texts[\"title\"].append(ax.title.get_text()) if ax.xaxis.label.get_text(): texts[\"xlabel\"].append(ax.xaxis.label.get_text()) if ax.yaxis.label.get_text(): texts[\"ylabel\"].append(ax.yaxis.label.get_text()) for label in ax.get_xticklabels() + ax.get_yticklabels(): if label.get_text(): texts[\"tick_label\"].append(label.get_text()) if legend := ax.get_legend(): for text in legend.get_texts(): if text.get_text(): texts[\"legend_text\"].append(text.get_text()) for text in ax.texts: # Annotations and ax.text() if text.get_text(): texts[\"annotation\"].append(text.get_text()) # Filter out empty lists return {k: for k, in texts.items() if v} def _calculate_metrics(self, generation_texts: Dict[str, List[str]], gt_texts: Dict[str, List[ str]]) -> None: \"\"\"Calculates strict metrics based on categorized text similarity.\"\"\" if not generation_texts and not gt_texts: self.metrics.precision = 1.0; self.metrics.recall = 1.0; self.metrics.f1 = 1.0 return total_similarity_score = 0.0 total_gt_text_count = sum(len(texts) for texts in gt_texts.values()) total_gen_text_count = sum(len(texts) for texts in generation_texts.values()) all_categories = set(gt_texts.keys()) set(generation_texts.keys()) for category in all_categories: gt_list = gt_texts.get(category, []) gen_list = generation_texts.get(category, []) if not gt_list or not gen_list: continue # Find best match for each generated text using Levenshtein ratio unmatched_gt = gt_list[:] for gen_text in gen_list: if not unmatched_gt: break best_score = -1 best_match_index = -1 for i, gt_text in enumerate(unmatched_gt): score = levenshtein_ratio(gen_text, gt_text) if score > best_score: best_score = score best_match_index = if best_match_index != -1: total_similarity_score += best_score unmatched_gt.pop(best_match_index) self.metrics.precision = total_similarity_score / total_gen_text_count if total_gen_text_count > 0 else 1.0 if not gt_texts else 0.0 self.metrics.recall = total_similarity_score / total_gt_text_count if total_gt_text_count > 0 else 1.0 if not generation_texts else 0.0 if self.metrics.precision + self.metrics.recall > 0: self.metrics.f1 = 2 * self.metrics.precision * self.metrics.recall / (self.metrics. precision + self.metrics.recall) else: self.metrics.f1 = 0. J.8 TYPE"
        },
        {
            "title": "Type evaluation code",
            "content": "class ChartTypeEvaluator: def __init__(self) -> None: self.metrics = ChartTypeMetrics() def __call__(self, gen_fig: Optional[plt.Figure], gt_fig: Optional[plt.Figure]) -> ChartTypeMetrics: if gen_fig is None or gt_fig is None: self.metrics.status = ExecutionStatus.FAILED 59 self.metrics.error_message = \"Could not get valid Figure object\" return self.metrics try: generation_chart_types = self._extract_chart_types_from_figure(gen_fig) gt_chart_types = self._extract_chart_types_from_figure(gt_fig) self._calculate_metrics(generation_chart_types, gt_chart_types) except Exception as e: logger.error(f\"Error during chart type evaluation: {e}\", exc_info=True) self.metrics.status = ExecutionStatus.FAILED self.metrics.error_message = str(e) return self.metrics def _extract_chart_types_from_figure(self, fig: plt.Figure) -> Dict[str, int]: \"\"\" (V11 - Strict Version) Identifies chart types by inspecting the specific classes of artists present in Figure object. \"\"\" types = set() for ax in fig.axes: # Check for specific artist types to identify plot types if any(isinstance(artist, Line2D) for artist in ax.lines): types.add(line) if any(isinstance(artist, Rectangle) for artist in ax.patches): types.add(bar_or_hist) if any(isinstance(artist, Wedge) for artist in ax.patches): types.add(pie) if any(isinstance(artist, PathCollection) for artist in ax.collections): types.add(scatter) if any(isinstance(artist, PolyCollection) for artist in ax.collections): types.add(fill_or_stack) # e.g., fill_between, stackplot, violinplot if any(isinstance(artist, QuadMesh) for artist in ax.collections): types.add(heatmap_or_grid) # e.g., pcolormesh, hist2d if any(isinstance(artist, plt.matplotlib.image.AxesImage) for artist in ax.images): types.add(image) # Convert set to the Counter-like dictionary format for consistency return {chart_type: 1 for chart_type in types} def _calculate_metrics(self, generation_chart_types: Dict[str, int], gt_chart_types: Dict[str, int]) -> None: \"\"\"Calculates strict metrics based on the sets of detected chart types.\"\"\" if not generation_chart_types and not gt_chart_types: self.metrics.precision = 1.0; self.metrics.recall = 1.0; self.metrics.f1 = 1.0 return gen_types_set = set(generation_chart_types.keys()) gt_types_set = set(gt_chart_types.keys()) # True Positives: Types present in both ground truth and generation n_correct = len(gen_types_set.intersection(gt_types_set)) # Total number of types detected in the generated plot total_generated = len(gen_types_set) # Total number of types that should have been in the plot total_gt = len(gt_types_set) self.metrics.precision = n_correct / total_generated if total_generated > 0 else 1.0 if not gt_types_set else 0. self.metrics.recall = n_correct / total_gt if total_gt > 0 else 1.0 if not gen_types_set else 0.0 if self.metrics.precision + self.metrics.recall > 0: self.metrics.f1 = 2 * self.metrics.precision * self.metrics.recall / (self.metrics. precision + self.metrics.recall) else: self.metrics.f1 = 0."
        },
        {
            "title": "K PROMPT",
            "content": "K.1 GENERATION PROMPT DM_prompt \"\"\"You are Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others.I have plot generated by Python code, but dont have the corresponding code that generated this plot. Your task 60 is to generate the Python code that can perfectly reproduce the picture based on the image provide. Here are the requirements for the task: 1. **Data Extraction**: Extract the actual data from the provided image. Based on the visual features of the plot, you must infer the data and recreate the plot. 2. **Recreate the Image**: Generate the Matplotlib code that reproduces the image exactly as it appears, including all elements such as: - Plot type (scatter, line, bar, etc.) - Axis labels and titles - Colors, markers, line styles, and other visual styles - Any legends, annotations, or gridlines present in the image 3. **Self-contained Code**: The Python code should be complete, executable, and self-contained. It should not require any external data files or variables not already present in the code. Your objective is to extract the any necessary details from the image and generate Python script that accurately reproduces the plot. Now, please generate the Python code to reproduce the picture below. The output format must be strictly as follows: python # Your Python code here to reproduce the image. \"\"\" CRD_template"
        },
        {
            "title": "You are a Python developer proficient in data",
            "content": "visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others. Your task is to generate Python code that can perfectly reproduce plot based on reference image, natural language instruction, and the corresponding data. Here are the requirements for the task: 1. **Use Provided Data**: You must use the data provided below in the generated code. Do not infer data from the image. 2. **Follow Instructions**: Adhere to the specific plotting instructions provided. 3. **Match Reference Image Style**: Use the reference image to understand the required visual style (colors, markers, line styles, labels, titles, legends, etc.) and replicate it as closely as possible. 4. **Self-contained Code**: The Python code should be complete, executable, and self-contained. It should not require any external data files. All data must be included within the script. 61 **Instruction:** {instruction_text} **Data:** {data_text} Now, based on the instruction, the data, and the reference image below, please generate the Python code. The output format must be strictly as follows: \"\"\" CFD_prompt"
        },
        {
            "title": "You are a Python developer proficient in data",
            "content": "visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others."
        },
        {
            "title": "Your task is to generate Python code that reproduces a",
            "content": "plot. You will be given specific instructions, data source image, and style reference image. Here are the general requirements: 1. **Data Extraction**: Extract the necessary data from the data source image. 2. **Style Replication**: Replicate the visual style ( colors, markers, layout, etc.) from the style reference image. 3. **Follow Instructions**: Adhere to the specific instructions provided for the task. 4. **Self-contained Code**: The Python code must be complete, executable, and self-contained, without needing external data files. --- **Specific Task Instructions:** {task_instructions} --- Now, using the data from the data source image and applying the style from the reference image according to the instructions, please generate the Python code. The output format must be strictly as follows: python # Your Python code here to reproduce the image. \"\"\" level2_prompt \"\"\"You are an expert Python developer specializing in data visualization with libraries like Matplotlib. 62 have an image of plot and set of instructions to modify it. Your task is to generate the Python code that would produce the *modified* plot. Here are the requirements: 1. **Understand the Base Image**: Analyze the provided image to understand the original plots data and structure. 2. **Apply Edits**: Carefully read the instructions provided below and apply them to the base plot. 3. **Generate Modified Code**: Generate single, selfcontained, and executable Python script that produces the final, edited visualization. The code should not require any external data files. **Editing Instructions:** --- {instructions} ---"
        },
        {
            "title": "Your objective is to generate a Python script that",
            "content": "accurately reproduces the plot *after* applying the given instructions. The output format must be strictly Python code block. python # Your Python code here to generate the MODIFIED image. \"\"\" level3_prompt \"\"\"You are Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, pandas, and others. Your task is to generate Python code that creates plot based on the provided data and instructions. You will be given specific instructions, data in text format ( extracted from an Excel file), and style reference image. Here are the general requirements: 1. **Use Provided Data**: The data you need to plot is provided below in CSV format. Each sheet from the original Excel file is clearly marked. You should use libraries like pandas and io.StringIO to parse this CSV data. 2. **Style Replication**: Replicate the visual style ( colors, markers, layout, fonts, etc.) from the style reference image. 3. **Follow Instructions**: Adhere to the specific instructions provided for the task. 4. **Self-contained Code**: The Python code must be complete, executable, and self-contained. The data should be defined directly within the code (e.g., in 63 pandas DataFrame loaded from string), without needing to read any external files. --- **Specific Task Instructions:** {task_instructions} --- **Data from Excel File (in CSV format):** {excel_data_string} --- Now, using the data provided above and applying the style from the reference image according to the instructions, please generate the Python code. The output format must be strictly as follows: python # Your Python code here to reproduce the image. \"\"\" K.2 LLM-SCORE PROMPT"
        },
        {
            "title": "System Prompt",
            "content": "You are an exceptionally strict and meticulous image analyst. Your task is to evaluate the visual similarity of two chart images. You must be extremely critical. Any deviation, no matter how small, must be penalized heavily. perfect score is reserved only for images that are visually indistinguishable to the human eye. Your analysis must be based solely on the visual information in the images provided. Compare the Ground Truth Image and the Generated Image . Based ONLY on their visual information, evaluate their similarity. **Evaluation Rules:** 1. **Strictness is Key:** Start with perfect score of 100 and deduct points for EVERY visual difference, including but not limited to: chart type, data points, colors, line styles, markers, labels (content, font, and position), titles, legends, axes (limits, ticks, scaling), layout, aspect ratio, and any other visual element. 2. **Identical Means Identical:** score of 100 is ONLY for images that are pixel-perfect or visually indistinguishable. Even tiny difference in line thickness or single different pixel color must result in lower score. 3. **Heavy Penalties:** Apply significant penalties for noticeable differences. For example, different color map or missing legend should lead to large deduction. 64 Return ONLY single JSON object with two keys: \"score\" ( an integer from 0 to 100) and \"reason\" (a concise, expert analysis in English, detailing every detected difference that justifies the score deduction). Do not include any other text, markdown, or explanations outside the JSON object. LMM-Score Prompt"
        },
        {
            "title": "You are a meticulous and strict expert Python data",
            "content": "visualization analyst. Your task is to compare two Python plotting scripts and evaluate the visual similarity of their final outputs based on SINGLE, specific dimension. Your analysis must be based **solely on the provided code **. Do not execute it. Your evaluation must be critical and detail-oriented. **Scoring Philosophy:** Assume perfect score of 100, then **deduct points for every deviation** you find, no matter how minor. score of 100 is reserved ONLY for scripts that produce visually indistinguishable plots. You must return ONLY single JSON object with two keys: \"score\" (an integer from 0 to 100) and \"reason\" (a concise, expert analysis in English). Do not include any other text in your response. \"\"\" \"\"\" data_handling_and_transformation: { prompt: \"\"\" Critically evaluate the DATA SOURCE and its TRANSFORMATION. - Focus on: How the numerical data passed to the plotting function is generated. - Check: Hardcoded lists/arrays, pandas or numpy array creation (e.g., np.linspace ), data filtering (df[...]), mathematical operations (np.sin(x), df [a] * 100), and data aggregation. **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** Data generation and transformations are functionally identical (e.g., [1, 2, 3] vs np.array([1, 2, 3])). - **-5 points:** Trivial differences in floating-point precision that are visually unnoticeable (e.g., np.pi vs 3.14159). - **-25 points:** Different data filtering or selection that results in subset or different ordering of the same underlying data. - **-50 points:** different mathematical transformation is applied to the same base data (e.g., np.sin(x) vs np.cos(x)). - **-75 points:** The fundamental data sources are different (e.g., plotting df [col_A] vs df[col_B]). - **-100 points:** Data is completely unrelated in source, shape, and scale. \"\"\", weight: 0.20 }, chart_type_and_mapping: { prompt: \"\"\" Critically evaluate the CORE CHART TYPE and DATA-TO-VISUALS MAPPING. - Focus on: The primary plotting function call (e.g., plt.plot, ax.bar, sns. heatmap). - Check: Which variables are mapped to which axes (e.g., x=df[time], y=df[value ]) and other visual properties (size=, hue=). **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** The exact same plotting function is used with the same data-toaxis mappings. - **-15 points:** visually similar plot type is used (e.g., plt.plot() vs plt. scatter()). - **-50 points:** different plot type is used, but its still plausible for the data (e.g., plt.bar() vs plt.plot() for time series). The core data variables on the axes are the same. - **-75 points:** Key data mappings are swapped or incorrect (e.g., and axes are flipped; x=sales, y=time vs x= time, y=sales). - **-100 points:** fundamentally different and inappropriate chart type is used (e.g ., plt.pie() vs sns.lineplot()). \"\"\", weight: 0.25 }, visual_aesthetics: { prompt: \"\"\" Critically evaluate the VISUAL AESTHETICS like colors, markers, and line styles. - Focus on: Explicitly set styling arguments. - Check: color, linestyle (or ls), linewidth (or lw), marker, markersize, alpha, cmap (for heatmaps /scatter), palette (for seaborn). **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** All explicit style arguments are identical. - **-10 points:** minor style attribute is different (e.g., linewidth=1.5 vs linewidth=2.0, or marker=o vs marker =x). - **-30 points:** The primary color is different (e.g., color=blue vs color =green). Or, one uses default color while the other specifies one. - **-50 points:** Multiple style attributes are different (e.g., color and linestyle). - **-75 points:** The overall aesthetic is completely different (e.g., solid blue line vs transparent, dashed red line with markers). \"\"\", weight: 0.20 }, labels_titles_and_legend: { prompt: \"\"\" Critically evaluate all TEXTUAL ELEMENTS: labels, titles, and legends. - Focus on: The content and presence of all text. - Check: ax.set_title(), ax.set_xlabel(), ax.set_ylabel(), fig.suptitle(), and the label argument in plotting calls used by ax.legend(). **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** All text elements are present and have identical content. - **-5 points:** Minor, non-substantive differences exist (e.g., \"Sales Data\" vs \" Sales data\", or minor typo). - **-20 points:** text element is present in both, but the content is substantively different (e.g., \"Sales in 2023\" vs \" Profit in 2024\"). - **-40 points:** key text element is missing in one script (e.g., one has title, the other does not). - **-60 points:** Multiple key text elements are missing or incorrect. - **-100 points:** No text elements are present in one or both scripts. \"\"\", weight: 0.15 }, figure_layout_and_axes: { 67 prompt: \"\"\" Critically evaluate the FIGURE LAYOUT and AXES configuration. - Focus on: The overall canvas, subplot structure, and axis properties. - Check: plt.figure(figsize=...), plt. subplots(), axis limits (ax.set_xlim, ax.set_ylim), axis scales (ax.set_xscale ), and axis direction (ax.invert_yaxis() ). **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** Figure size, subplot structure, limits, and scales are all identical. - **-10 points:** Figure size is different, but the aspect ratio is similar. - **-25 points:** Axis limits are different, but the data range shown is largely the same. - **-50 points:** Axis scales are different ( e.g., linear vs log). This is major visual change. - **-75 points:** The subplot structure is different (e.g., subplots(1, 2) vs subplots(2, 1)). - **-100 points:** Completely different layouts (e.g., single plot vs. complex grid of subplots). \"\"\", weight: 0.15 }, auxiliary_elements_and_ticks: { prompt: \"\"\" Critically evaluate AUXILIARY elements, grid, spines, and ticks. - Focus on: Non-data visual elements that provide context or structure. - Check: ax.grid(), ax.axhline(), ax. axvspan(), ax.spines[...], ax. tick_params(), and explicit tick setting (ax.set_xticks). **Scoring Rubric (Start at 100, deduct points ):** - **-0 points:** All auxiliary elements and tick configurations are identical. - **-15 points:** An element is present in both but with different styling (e.g., solid grid vs dashed grid). Or, tick label formatting differs. - **-30 points:** An important element is present in one but missing in the other (e. g., one script calls ax.grid(True) and the other does not). 68 - **-50 points:** major contextual element is missing (e.g., crucial ax.axhline(y =0, ...) that indicates baseline). Or, spines are hidden in one but not the other. - **-75 points:** Major differences in tick locations (e.g., xticks are explicitly set to different values). \"\"\", weight: 0.05 } }"
        }
    ],
    "affiliations": [
        "CSU-JPG, Central South University",
        "Nanyang Technological University",
        "National University of Singapore"
    ]
}