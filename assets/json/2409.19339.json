{
    "paper_title": "Visual Question Decomposition on Multimodal Large Language Models",
    "authors": [
        "Haowei Zhang",
        "Jianzhe Liu",
        "Zhen Han",
        "Shuo Chen",
        "Bailan He",
        "Volker Tresp",
        "Zhiqiang Xu",
        "Jindong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce a systematic evaluation framework including a dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce high-quality sub-questions. To address this limitation, we propose a specific finetuning dataset, DecoVQA+, for enhancing the model's question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and a training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets."
        },
        {
            "title": "Start",
            "content": "Haowei Zhang* 1 Bailan He 3 Jianzhe Liu* 1 Zhen Han 2 Volker Tresp 3,4 Zhiqiang Xu 5 1Technical University of Munich, 2Amazon Web Services, 3LMU Munich, 4Munich Center for Machine Learning, 5MBZUAI, 6University of Oxford {haowei.zhang, jianzhe.liu}@tum.de https://vqd-emnlp2024.github.io/ Shuo Chen 3 Jindong Gu 6 4 2 0 2 7 ] . [ 2 9 3 3 9 1 . 9 0 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce systematic evaluation framework including dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce highquality sub-questions. To address this limitation, we propose specific finetuning dataset, DecoVQA+, for enhancing the models question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets."
        },
        {
            "title": "Introduction",
            "content": "Answering complex questions is challenging task, especially when the questions require implicit multi-step reasoning to answer. Question Decomposition (QD) is an effective strategy to address this issue. Most related work studies the efficacy of QD with unimodal textual large language models * Equal Contribution. Corresponding authors: Zhen Han <hanzhen02111@163.com>, this work does not relate to his position at AWS; Jindong Gu <jindong.gu@outlook.com> (LLMs) in enhancing complex textual question answering tasks (Patel et al., 2022; Dua et al., 2022; Zhou et al., 2023; Qi et al., 2023). Although some recent works (You et al., 2023; Qi et al., 2023) have explored question decomposition within the context of visual question answering (VQA) tasks, they follow the paradigm of performing unimodal QD based on the image caption. Typically, they conduct two-step process: first, generating caption for the image using captioning model, and then performing question decomposition using an unimodal textual LLM based on the complex question and the generated image caption. Relying solely on the image caption instead of the image itself may lead to significant information loss. Recent advancements in Multimodal Large Language Models (MLLMs) have enabled MLLMs to directly perceive image information for answering questions. Yet, how to perform QD on complex visual questions using such MLLMs has been less explored. In the following, we refer to question decomposition using MLLMs on VQA as Visual Question Decomposition (VQD). In this work, we primarily explore the following research questions: How can we quantitatively assess the VQD ability of MLLMs? How proficient are existing MLLMs in VQD, or specifically, how is the quality of sub-questions generated by MLLMs? How can we enhance the VQD ability of MLLMs and enable the models to properly determine when to decompose and when not to, facing questions with varying difficulties? To assess the question decomposition capability of MLLMs, significant obstacle is the absence of metrics for evaluating models question decomposition abilities. Recent work (You et al., 2023; Qi et al., 2023) evaluates the models question decomposition ability by measuring the final answers accuracy. However, relying solely on whether model can correctly answer the original question is an implicit measure of its decomposition ability. Especially, even if the final answer is correct, we have observed various issues in the decomposed sub-questions: for example, some MLLMs produce many repetitive sub-questions, or some subquestions are entirely irrelevant to the original question, as shown in Figure 1. Figure 1: Cases showing that even if the model correctly answers the original question, the generated subquestions are of low quality: they are irrelevant or repeated from the original question. Ideally, the sub-questions should be highly relevant to the original question and not repetitive with the original question or other sub-questions. Besides, they should be relatively easy to be grounded, whose answer can be derived from images or pretrained commonsense knowledge. Figure 2 shows detailed comparison between sub-questions of highand low-quality. To this end, we propose SubQuestRater, an evaluation framework for assessing MLLMs question decomposition ability. Specifically, considering the observed common deficiencies of existing MLLMs question decomposition ability, we choose three critical criteria to assess the quality of question decomposition: 1) Non-Repetition, 2) Relevance, and 3) Groundedness. SubQuestRater quantifies the quality of each sub-question by assigning scores based on each criterion. Besides, it is necessary to have an evaluation dataset containing complex questions requiring decomposition. However, current QA datasets, even specifically aiming at complex reasoning, such as A-OKVQA (Schwenk et al., 2022), still contain large number of simple questions that do not require decomposition to answer. Given the lack of publicly available benchmarks solely focusing on complex questions that require decomposition, we introduce specific question decomposition evaluation dataset. With the help of our proposed evaluation criteria and benchmarks, we evaluate several MLLMs including MiniGPT-v2 (Chen et al., 2023a), LLaVA-1.5 (Liu et al., 2024), etc. The results show that, the decomposed sub-questions generated by these MLLMs cannot perform satisfactorily, demonstrating repetition, irrelevance, or ungroundedness in many cases. To enhance MLLMs capability for VQD, we propose new finetuning dataset tailored for question decomposition, DecoVQA. It is the first public dataset that consists of manually annotated subquestions for complex questions. The provided subquestions feature high quality in the view of nonrepetition, relevance and groundedness. To prevent catastrophic forgetting, samples with simple questions in the form of direct answering are added to construct DecoVQA. We finetune MLLMs on this dataset with LoRA (Hu et al., 2021). Furthermore, we find that existing MLLMs struggle to determine whether they need question decomposition to enhance their reasoning performance when facing problems of varying difficulty. To address this issue, we propose training pipeline with an upgraded version of DecoVQA, i.e. DecoVQA+, with an extra QA round asking models whether to decompose, and novel objective function combining next-token prediction loss (NTP loss) and binary cross entropy loss (BCE loss) to fine-tune MLLMs. In addition to applying the conventional NTP loss for general reasoning, we design BCE loss that aims to penalize the errors in deciding whether to decompose questions. Extensive experiments show that MLLMs after finetuning achieve higher answer accuracy and learn to know when to decompose properly. To summarize, the main contributions of our work are as follows: 1. We are the first to systematically investigate MLLMs ability on visual question decomposition. We propose comprehensive evaluation framework, SubQuestRater, which includes benchmark dataset and novel evaluation metrics, to quantitatively evaluate the quality of generated sub-questions from diverse perspectives. 2. We find that existing MLLMs are insufficient to produce sub-questions with high quality. Efficient finetuning of MLLMs on our proposed dataset, DecoVQA, significantly improves their VQD ability. 3. We propose finetuning pipeline with an upgraded dataset, DecoVQA+, and specific training objective for selective VQD, demonstrating improvements in the models decision-making regarding decomposing questions or direct answering, as well as the accuracy of final answers."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Question Decomposition Question decomposition has shown impressive capabilities in improving the reasoning performance of language models. Successive Prompting (Dua et al., 2022) and Least-to-Most Prompting (Zhou et al., 2023) are two representative works that break complicated question into simpler ones iteratively. Decomposed Prompting (Khot et al., 2023) introduces modular setup of question decomposition, which makes it easy to optimize prompts, pretrained models and symbolic functions for different sub-tasks. Additionally, question decomposition is capable of increasing the reasoning faithfulness while achieving the accuracy improvement (Radhakrishnan et al., 2023). Recent studies have explored the potential of VQD. IdealGPT (You et al., 2023) leverages LLMs iteratively to raise sub-questions and determines the final reasoning answer. Socratic Questioning (Qi et al., 2023) utilizes LLMs to generate sub-questions and answer them, stimulating robust recursive thinking. However, all of these studies rely on the reasoning ability of language models, overlooking the visual information that images can bring to question decomposition. The literature most relevant to our work is (Khan et al., 2023), which explored prompting MLLMs to answer VQA questions with question decomposition in the zeroshot settings. However, it only applies VQD as prompting technique and evaluates whether VQD could enhance the VQA accuracy. That work does not delve into the quality of generated subquestions along the entire reasoning process, which does not explicitly analyze how well the questions are decomposed. 2.2 Multimodal LLMs To address the modality gap, MiniGPT-v2 (Chen et al., 2023a) and LLaVA-1.5 (Liu et al., 2024) apply linear connection layer to connect the frozen pre-trained vision module and the language model. Besides, they provide task-oriented instruction training pipeline to decrease instructional ambiguity across various vision-language tasks. GPT-4 with Vision (GPT-4V) (OpenAI et al., 2024) is powerful MLLM based on instructing GPT-4 and has shown outstanding capability on diverse benchmarks. Besides instructionfollowing ability, Qwen-VL (Bai et al., 2023b) series outperforms in range of vision-language tasks, supporting multilingual conversations and dialogues involving multiple interleaved images. Furthermore, InternVL-1.5 (Chen et al., 2024b) introduces strong vision encoder, dynamic highresolution images, and high-quality bilingual dataset to enhance the comprehensive capability of MLLMs further. While these models have demonstrated impressive performance across various benchmarks (Bai et al., 2023b), they continue to struggle with complex tasks that require advanced reasoning (Khan et al., 2023). Numerous techniques, such as parameter-efficient tuning methods like prompting (Gu et al., 2023), incontext learning (Alayrac et al., 2022), and chainof-thought reasoning (Zhang et al., 2022), can be used to help models handle unseen or complex tasks, but each has its limitations. Parameterefficient tuning is vulnerable to robustness issues when dealing with out-of-distribution inputs (Chen et al., 2024a), in-context learning often fails to fully utilize multimodal information, focusing predominantly on text (Chen et al., 2023b), and chainof-thought reasoning is prone to adversarial attacks (Wang et al., 2024). In contrast, our approach employs question decomposition, breaking down complex queries into simpler, more manageable sub-questions, which enhances the models reasoning capabilities."
        },
        {
            "title": "3 How well can MLLMs decompose",
            "content": "questions? Existing works commonly use the accuracy of the final answer to demonstrate models ability to decompose questions. However, this evaluation method is imprecise and implicit. As shown in Figure 1, MLLM generates sub-questions with low quality, yet can still provide correct answer. However, these ineffective sub-questions fail to provide the expected assistance in answering the original question and do not help the models reasoning process. To address this, we differentiate the models question decomposition skills from the accuracy of answering and propose SubQuestRater, an evaluation framework focusing explicitly on VQD ability. The framework consists of criteria that are specifically designed to assess the quality of sub-questions and an evaluation dataset. To determine the criteria for the proposed framework, we have analyzed the generated subquestions by existing MLLMs including MiniGPTv2, LLaVA, etc., and we have observed the most common issues among them: Some sub-questions repeat the original question or other sub-questions (e.g., semantically equivalent), while others are not relevant to the original question. Besides, some sub-questions cannot be answered from images or commonsense knowledge. These issues largely influence the quality of sub-questions. After conducting the manual review and analysis of the subquestions, we proposed three criteria to assess the quality of sub-questions, as follows: Non-Repetition This criterion ensures that subquestions do not repeat. The definition of repetition here is the case where sub-questions discuss the same topic with the same or different phrasing. For example, in Figure 1, the original question asks why the mens vests are orange, yet all the sub-questions repeatedly talk about orange vests, causing only redundancy. Relevance This criterion judges whether subquestion truly contributes to answering the original question. For example, if the original question asks about the relationship between two people sitting at table, but the sub-questions inquire about the colors of their clothes or the shapes of the table, these sub-questions are irrelevant. Such distractions can even mislead the model and reduce its performance. Groundedness This criterion evaluates whether sub-question can be answered using information directly provided by the image or from commonsense knowledge. Given relevant and not repeated sub-question, if it cant be grounded from image or commonsense knowledge, it would still be unhelpful. For example, if the original question asks whether it is safe to cross the road now, and subquestion inquires what is the time displayed on the traffic light, which helps answer the original question if it can be inferred from the image. However, the image only shows the yellow traffic light and there is no number on the light indicating the remaining time. Therefore, the sub-question is considered ungrounded. Figure 2: Question decomposition examples of high quality and low quality given certain image and question. more detailed explanation of the criteria with cases is given in Figure 2. By employing this evaluation framework, we have three quantifiable metrics for each sub-question. Algorithm 1 visually demonstrates the complete evaluation process for each sub-question within this framework. Moreover, we have constructed an evaluation benchmark dataset, since there is currently no dataset composed entirely of complex questions that require decomposition to answer, we construct an evaluation dataset. We manually selected 100 complex questions each from A-OKVQA (Schwenk et al., 2022) and VQAIntrospect (Selvaraju et al., 2020), making total of 200 questions worth decomposing. A-OKVQA serves as benchmark necessitating substantial understanding of external knowledge to formulate accurate responses. VQA-Introspect is VQA dataset that contains large number of samples that need complex visual reasoning to answer. AOKVQA samples are in the form of multiple choice while VQA-Introspect provides open-ended questions. Since these two public datasets have large number of simple questions which do not need decomposition to answer, we construct an evaluation dataset based on them instead of directly evaluating on them. After establishing the evaluation framework, we choose GPT-4V as the scoring model due to its powerful comprehensive reasoning performance. To ensure that GPT-4Vs judgments align Criteria MiniGPT-v2 LLaVA-1.5 Qwen-VL-Chat InternVL-Chat-V1-5 GPT-4V Non-Repetition Relevance Groundedness 47.52 36.65 43.30 42.19 37.33 44.17 32.10 27.15 26. 82.41 73.42 78.01 97.40 75.36 84.57 Table 1: Average scores of VQD ability on three criteria of popular existing MLLMs, evaluated with SubQuestRater. The performance of GPT-4V is also provided for reference. with human judgments, we have conducted alignment experiments. As shown in Appendix A, the results demonstrate that the scoring gap between the judgments of GPT-4V and human beings is small. It is reliable to adopt GPT-4V as the scoring model. We have measured the VQD ability of popular existing MLLMs with SubQuestRater, including MiniGPT-v2 (Chen et al., 2023a), LLaVA-1.5 (Liu et al., 2024), Qwen-VL-Chat (Bai et al., 2023b) and InternVL-Chat-V1-5 (Chen et al., 2024b). The results in Table 1 show that these existing models cannot generate satisfactory sub-questions."
        },
        {
            "title": "Decomposition Capability",
            "content": "Given that the existing MLLMs have poor performance on VQD, this section further explores how to improve the VQD ability of MLLMs. An intuitive method to enhance the decomposition performance of MLLMs is to finetune the models on dataset tailored for VQD. Specifically, we need dataset to finetune the models, which exclusively focuses on complex questions with high-quality sub-questions in the view of Non-Repetition, Relevance, and Groundedness. However, there does not exist such public VQA dataset. Therefore, we propose specialized dataset, termed DecoVQA, to improve the VQD ability. Furthermore, for effective VQD, models also need to have an improved ability to decide when to decompose questions. We also explain the finetuning pipeline with our proposed dataset and novel training objective to achieve that goal in detail, as discussed below. 4.1 Dataset Construction of DecoVQA Question Selection & Decomposition Annotation In our exploration of question decomposition for VQA in Table 1, we recognize that not all questions in existing benchmark datasets necessitate decomposition for answering. Many questions are straightforward and can be addressed without employing decomposition strategy. Our focus, therefore, is on questions that demand complex reasoning, making them suitable candidates for the decomposition annotation. For this purpose, we have selected A-OKVQA and VQA-Introspect as our primary data source, as these two datasets contain complex questions requiring external knowledge and visual reasoning to answer respectively. To identify appropriate samples from AOKVQA and VQA-Introspect, we adopt specific pre-selection strategies, shown in Appendix F.1 detailedly. After that, we conduct manual review and pick 200 complex samples that require decomposition from pre-selected samples. Then we manually annotated these samples with logical sub-questions. The details of the annotation process are shown in Appendix F.2. Dataset Statistics After decomposition annotation, we collected 100 samples from A-OKVQA and 100 samples from VQA-Introspect with highquality sub-questions from the perspective of NonRepetition, Relevance, and Groundedness. To prevent overfitting and catastrophic forgetting, we manually picked out another 100 samples from AOKVQA and 100 samples from VQA-Introspect, which are simple and VQD doesnt contribute to higher performance for them. These simple samples are added to DecoVQA in the form of direct answering. Overall, DecoVQA has 400 balanced samples in total. 4.2 DecoVQA+ To enhance the capability of MLLMs in selective decomposition, we add an extra QA round on the basis of DecoVQA to enable the models to learn when to decompose properly, facing questions with various difficulties. This extra QA round contains query asking the models if they would directly answer without any decomposition, given an image and question. The labels for simple questions are \"yes\" while the ones for complex questions with human-annotated sub-questions are \"no\". The extra QA round is added in front of all existing Figure 3: Comparison of VQD ability of different models across three evaluation criteria. Each bar chart represents specific criterion, comparing the average scores of the original model (in blue) and the corresponding model finetuned with DecoVQA+ (in orange). The vertical axis shows the average scores, while the horizontal axis lists the models. The difference in bar height indicates the performance gain achieved through finetuning. QA rounds of DecoVQA. We demonstrate the full prompt of training sample in Figure 6. We refer to this upgraded version as \"DecoVQA+\". loss aims to penalize the errors in deciding whether to decompose, compared to the labels given in each sample of DecoVQA+. The superiority of DecoVQA+ is proven in Appendix J, compared to the existing dataset VQAIntrospect. In the ablation study on DecoVQA+, as shown in Table 6, we compare the complete DecoVQA+ to the version with only 100 or 200 samples. The model achieves very similar results after being finetuned with different versions of DecoVQA+. On the one hand, the ablation study proves that our proposed dataset has sufficient samples to train the model. On the other hand, it also indicates that our finetuning pipeline remains efficient, even if there is lack of high-quality finetuning data in most real-world cases. To find out whether MLLMs learn to identify questions that need decomposition, we develop an evaluation dataset, Whether2Deco. It consists of 200 simple questions where direct answering is sufficient to answer them correctly and 200 complex questions that need VQD to answer, which are organized in the form of the extra round in DecoVQA+. The questions are equally sampled from A-OKVQA and VQA-Introspect. The statistics of all utilized public datasets and newly proposed datasets are shown in Table 8. 4.3 Training Objective It is intuitive to finetune MLLMs to improve models performance on VQD. However, directly applying the conventional next-token prediction loss (NTP loss) on the finetuning for selective VQD may not be appropriate. To improve MLLMs capability of identifying the questions that need to be decomposed, we propose training objective, SelectiveVQD Loss, combining the NTP loss and binary cross entropy loss (BCE loss). The BCE When the model is asked whether it would perform VQD, we firstly find the specific token position for \"yes\" or \"no\" in the sentence, select the logits of these two specific tokens in that position, i.e. \"yes\" and \"no\", and then transform these two logits into probabilities through softmax: P(yes) = P( ˆws = yes ˆws {yes, no}) = Sof tmax(logit( ˆws = yes), logit( ˆws = no)) P(no) = P( ˆws = no ˆws {yes, no}) = 1 P(yes), (1) where is the specific token position in the sentence for \"yes\" or \"no\" and ws is the specific token of \"yes\" or \"no\". We compute the BCE loss between these two probabilities, and compute the cumulative NTP loss across all conversation rounds for each sample: Loss = (cid:80)M BCELoss = [yslogP(yes) + (1 ys)log(1 P(yes))] (2) i=1 logP( ˆwi = wi ˆwi1, ..., ˆw1), (3) where ys is the binary label indicating whether specific sample needs decomposition or not. wi denotes the i-th token in the ground truth sentence while ˆwi denotes the predicted i-th token and is the number of tokens of the prediction. The final combined SelectiveVQD Loss is weighted sum of both NTP loss and BCE loss: SelectiveV QDLoss = (cid:80)N j=1(λ Lossj + ω BCELossj), (4) where denotes the j-th training sample and denotes the total number of training samples. The NTP loss is computed for the entire training sample, and the BCE loss focuses specifically on determining whether to decompose the given question in the selective stage. λ and ω are two tunable hyperparameters to balance the weights of the two losses in the final combined loss."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment Setup Models With SubQuestRater, we compare the VQD ability of four popular MLLMs: MiniGPT-v2, LLaVA-1.5, Qwen-VL-Chat, and InternVL-ChatV1-5 before and after finetuning on our proposed datasets. We finetune all these MLLMs on DecoVQA, DecoVQA+, and DecoVQA+ with SelectiveVQD Loss to see the improvement in their VQD capability. Additionally, we evaluate the improvement in VQA accuracy and the models capability to appropriately determine when to decompose questions through finetuning. Datasets The finetuning datasets include DecoVQA and DecoVQA+. DecoVQA+ adds an extra QA round based on DecoVQA, asking models whether to decompose questions before decomposition. As for used evaluation datasets, we assess the VQD capability of MLLMs before and after finetuning on the proposed evaluation dataset in SubQuestRater, which contains 200 complex questions. The prompt for evaluating the VQD ability is shown in Figure 5. Besides, we evaluate the VQA accuracy on A-OKVQA, GQA (Hudson and Manning, 2019), and VQA-Introspect, containing complex reasoning questions (please refer to Appendix D.1 for more statistical details). As for AOKVQA and VQA-Introspect, the subsets of data used for inference are different from the subsets selected for constructing our finetuning datasets, preventing the problem of data leakage. We also evaluate the accuracy on Whether2Deco to test whether MLLMs are able to determine when to decompose questions properly. The prompt for evaluation experiments on accuracy is under selective VQD setting, which is in the same form of samples in DecoVQA+ shown in Figure 6. 5.2 Quantitative Evaluation The quantitative evaluation involves two parts: evaluation of decomposed sub-questions under SubQuestRater framework and accuracy comparison on VQA datasets and Whether2Deco. The finetuning is efficient based on the dataset with small number of samples. The supplementary evaluation on MMBench (Liu et al., 2023) in Appendix shows our finetuning does not hurt the all-around performance of MLLMs, but even slightly improves the comprehensive performance in many aspects. Evaluation of Decomposed Sub-questions To compare the VQD ability of the MLLMs before and after finetuning, we conduct evaluation with the SubQuestRater framework. Figure 3 illustrates the comparison of average scores of sub-questions generated by four MLLMs before and after finetuning with DecoVQA+. It can be observed that finetuned models have outperformed their original versions on all three criteria, indicating the VQD ability has been enhanced significantly through finetuning. Some of the models even show nearly double the scores in some criteria after finetuning. In addition to the average score, we also compare the number of samples before and after finetuning, which achieve high score (75-100) and low score (0-25) on three criteria, as shown in Figure 8. The results show that there are considerably more high-scored samples and less low-scored samples through finetuning. The VQD abilities of other finetuned checkpoints are listed in Table 9. It shows that all finetuned versions of MLLMs have an improvement in VQD ability compared to the original model. Additionally, we have conducted experiments by varying the number of samples in DecoVQA, which ultimately lead to similar results, as shown in Table 5, indicating that there is no need to add more samples to DecoVQA+ for further improvements. VQA Accuracy & Whether2Deco Accuracy Firstly, we investigate whether better VQD performance leads to higher accuracy. To prove that point, we compare the accuracy of models finetuned with DecoVQA and their corresponding baselines, as shown in the second and the first row of each model respectively in Table 2. It is clear that the models with higher VQD capability through finetuning achieve higher accuracy than their original versions in most experiments. Existing MLLMs are unable to decide when to decompose appropriately and tend to make fiftyfifty guess, as shown in the first line of each model. For improving the performance in selective decomposition, it is very important for models to learn when to decompose, facing questions with varying difficulties, since unnecessary decomposition may Models MiniGPT-v finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss 41.2 60.6 (+19.4) 60.7 (+19.5) 64.0 (+22.8) 44.2 50.4 (+6.2) 50.7 (+6.5) 51.7 (+7.5) 62.1 71.8 (+9.7) 72.1 (+10.0) 72.5 (+10.4) A-OKVQA GQA VQA-Introspect Whether2Deco LLaVA-1.5 finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss Qwen-VL-Chat finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss InternVL-Chat-V1finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss 67.7 69.4 (+1.7) 72.7 (+5.0) 73.9 (+6.2) 71.4 72.0 (+0.6) 73.1 (+1.7) 73.3 (+1.9) 80.7 83.5 (+2.8) 83.3 (+2.6) 83.7 (+3.0) 52.1 52.8 (+0.7) 57.2 (+5.1) 56.7 (+4.6) 67.2 73.5 (+6.3) 75.4 (+8.2) 75.8 (+8.6) 53.5 58.0 (+4.5) 59.3 (+5.8) 59.1 (+5.6) 77.8 75.9 (-1.9) 83.6 (+5.8) 83.9 (+6.1) 64.8 66.4 (+1.6) 66.5 (+1.7) 66.8 (+2.0) 80.5 86.0 (+5.5) 86.9 (+6.4) 87.3 (+6.8) 46.8 42.8 (-4.0) 61.0 (+14.2) 71.5 (+24.7) 49.3 4.8* (-44.5) 68.8 (+19.5) 75.0 (+25.7) 48.0 43.3 (-4.7) 58.8 (+10.8) 61.8 (+13.8) 58.3 53.5 (-4.8) 67.0 (+8.7) 68.3 (+10.0) Table 2: Comparison of VQA accuracy (%) on external knowledge (A-OKVQA) and visual reasoning (GQA and VQA-Introspect) datasets and Whether2Deco accuracy (%) before and after fine-tuning MLLMs. DecoVQA+ is constructed based on DecoVQA, with an extra QA round asking MLLMs whether the question needs VQD to answer or not. *Here LLaVA-1.5 fails to follow the pre-defined answering template. Figure 4: Cases showing the comparison of question decomposition by different models before and after finetuning. The left image demonstrates MiniGPT-v2s decomposition on A-OKVQA, while the right image shows LLaVA-1.5s decomposition on VQA-Introspect. mislead the reasoning process. From the second row of each model, we can see that higher quality of sub-questions does not mean better performance on determining when to decompose questions. To address this problem, we finetune the models with DecoVQA+. As shown in the third row of each model in Table 2, there is significant improvement in the accuracy on Whether2Deco after fine-tuning with DecoVQA+, compared to the baseline and the checkpoint finetuned by DecoVQA. Moreover, the accuracy of VQA tasks also increases because of the better whether-to-decompose policy of the finetuned models. To further enhance the ability of MLLMs to perform selective decomposition, we train the models with SelectiveVQD Loss. In contrast to the training with only the NTP loss, the models achieve higher accuracy on Whether2Deco and also VQA tasks in most cases. If compared to the original models before finetuning, the accuracy on all evaluation datasets increases significantly. The results of evaluation experiments with different random seeds in Figure 9 show the stable effectiveness of our entire pipeline. We also compare our proposed VQD pipeline with the existing paradigm of unimodal QD based on the image caption, as shown in 12. The results demonstrate that VQD outperforms the unimodal QD method. The comparison between our finetuning and In-context Learning proposed in (Khan et al., 2023) is shown in Appendix L."
        },
        {
            "title": "Acknolwedgement",
            "content": "The authors acknowledge support by the German Federal Ministry for Education and Research (BMBF), funding project Software Campus 2.0 / C-R-KG (FKZ 01IS17048). 5.3 Qualitative Evaluation"
        },
        {
            "title": "References",
            "content": "In this subsection, we will use several examples to visually illustrate the changes of sub-questions before and after finetuning with DecoVQA+. Figure 4 shows that the quality of the sub-questions has indeed been significantly improved after finetuning. The sub-questions generated by finetuned models are not repetitive, relevant to the original question and grounded, instead of ineffective decomposition or low-quality sub-questions originally. More case studies are shown in Figure 15."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper systematically investigates VQD capabilities on MLLMs. We propose systematic evaluation framework for VQD, SubQuestRater, including dataset and evaluation metrics to quantitatively measure the generated sub-questions by MLLMs. SubQuestRater is applied to popular MLLMs and we find that they are inadequate to produce high-quality sub-questions. To enhance the capability of MLLMs to decompose questions, specialized dataset DecoVQA with human-annotated sub-questions is proposed. To further improve the ability to perform selective VQD, we propose training pipeline with an upgraded dataset DecoVQA+ and novel training objective. Finetuned MLLMs demonstrate significant improvement in the quality of generated sub-questions and the policy of whether-to-decompose. Additionally, the models also achieve higher VQA accuracy under selective VQD through finetuning on our proposed datasets."
        },
        {
            "title": "Limitations",
            "content": "The main limitations in our work include: 1) Question Decomposition can be extended into complex task decomposition for an agent (multiple subtasks), leaving it as future work. 2) We apply finetuning to increase MLLMs VQD ability, which requires the models detailed parameter information. Thus, community users could not apply our method for enhancing closed-source MLLMs. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: visual language model for few-shot learning. Preprint, arXiv:2204.14198. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV). Jinze Bai, Shuai Bai, Yunfei Chu, and et al. 2023a. Qwen technical report. Preprint, arXiv:2309.16609. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. Qwen-vl: versatile vision-language model for understanding, loPreprint, calization, text reading, and beyond. arXiv:2308.12966. Zheng Cai, Maosong Cao, Haojiong Chen, and et al. Preprint, Internlm2 technical report. 2024. arXiv:2403.17297. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. Preprint, arXiv:2310.09478. Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, and Volker Tresp. 2024a. Benchmarking robustness of adaptation methods on pre-trained visionlanguage models. Advances in Neural Information Processing Systems, 36. Shuo Chen, Zhen Han, Bailan He, Mark Buckley, Philip Torr, Volker Tresp, and Jindong Gu. 2023b. Understanding and improving in-context learning on visionlanguage models. arXiv preprint arXiv:2311.18021, 1(2). Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Preprint, arXiv:2404.16821. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: visual question answering benchmark requiring external knowledge. Preprint, arXiv:1906.00067. OpenAI, Josh Achiam, Steven Adler, and et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomIn Proceedings of the posing complex questions. 2022 Conference on Empirical Methods in Natural Language Processing, pages 12511265, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Is question decomposition unit all we need? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 45534569, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. David Freedman, Robert Pisani, and Roger Purves. 2007. Statistics (international student edition). Pisani, R. Purves, 4th edn. WW Norton & Company, New York. Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 2023. systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685. Drew A. Hudson and Christopher D. Manning. 2019. Gqa: new dataset for real-world visual reasoning and compositional question answering. Preprint, arXiv:1902.09506. Zaid Khan, Vijay Kumar G, Samuel Schulter, Manmohan Chandraker, and Yun Fu. 2023. Exploring In Adquestion decomposition for zero-shot vqa. vances in Neural Information Processing Systems, volume 36, pages 5661556627. Curran Associates, Inc. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: modular approach for solving complex tasks. Preprint, arXiv:2210.02406. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. Preprint, arXiv:2310.03744. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023. Mmbench: Is your multi-modal model an all-around player? Preprint, arXiv:2307.06281. Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, and Lifu Huang. 2023. The art of SOCRATIC QUESTIONING: Recursive thinking with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 41774199, Singapore. Association for Computational Linguistics. Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukošiute, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. 2023. Question decomposition improves the faithfulness of model-generated reasoning. Preprint, arXiv:2307.11768. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint, arXiv:1908.10084. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: benchmark for visual question answering using world knowledge. Preprint, arXiv:2206.01718. Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020. Squinting at vqa models: Introspecting vqa models with sub-questions. In CVPR 2020. Hugo Touvron, Louis Martin, Kevin Stone, and et al. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, and Jindong Gu. 2024. Stop reasoning! when multimodal llms with chain-of-thought reasoning meets adversarial images. Conference on Language Modeling. Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. 2023. IdealGPT: Iteratively decomposing vision and language reasoning via large In Findings of the Association language models. for Computational Linguistics: EMNLP 2023, pages 1128911303, Singapore. Association for Computational Linguistics. Jerrold Zar. 2005. Spearman rank correlation. Encyclopedia of Biostatistics, 7. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought promptarXiv preprint ing in large language models. arXiv:2210.03493. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex Preprint, reasoning in large language models. arXiv:2205.10625. Alignment of Judgements from GPT-4V and Human Reviewers To ensure that the judgments from GPT-4V and human beings are highly aligned, human reviewers manually evaluate the sub-questions generated by MiniGPT-v2 and its finetuned checkpoint on three criteria defined in SubQuestRater. We compare the judgments from GPT-4V and human reviewers from three perspectives: 1) Comparison of average score, 2) Pearson correlation coefficient (Freedman et al., 2007), 3) Spearman correlation coefficient (Zar, 2005). The comparison of average score serves as coarse-grained evaluation of overall alignment, with results shown in Table 3. Pearson and Spearman correlation coefficients assess linear and monotonic relationships between two sets of judgments respectively, with results shown in Table 4. The results demonstrate that the judgments from GPT-4V and human reviewers on all three criteria are highly aligned."
        },
        {
            "title": "B Ablation Studies",
            "content": "The results of ablation studies, as shown in Table 5 and in Table 6. The samples from all the versions of DecoVQA+ follow balanced distribution. For instance, DecoVQA+100 has 25 complex MC and 25 complex open-ended questions that need VQD while it has the other 25 simple MC and 25 simple open-ended questions that do not need VQD to answer. The models finetuned by DecoVQA+ with Model Criteria GPT-4V Human Error Rate Original MiniGPT-v2 MiniGPT-v2 finetuned by DecoVQA Non-Repetition 51.92 Relevance Groundedness 40.86 47. Non-Repetition 94.22 Relevance Groundedness 74.54 86. 52.37 39.34 47.84 93.79 75.79 87. -0.86% 3.86% -0.40% 0.46% -1.65% -1.27% Table 3: Comparison of average scores of the judgements on SubQuestRater dataset from GPT-4V and human reviewers on three criteria. We regard judgements from human reviewers as the ground truth when computing the error rate. Model Criteria Pearson Spearman Original MiniGPT-v2 MiniGPT-v2 finetuned by DecoVQA Non-Repetition 0.828 Relevance Groundedness 0.813 0. Non-Repetition 0.867 Relevance Groundedness 0.804 0. 0.820 0.813 0.795 0.864 0.784 0. Table 4: Pearson and Spearman correlation coefficients of the judgements on SubQuestRater dataset from GPT4V and human reviewers on three criteria. All results are statistically highly significant (p-value < 0.001). varying sample numbers generate similar results on VQD ability, VQA accuracy, and Whether2Deco accuracy, demonstrating that 400 samples are sufficient to ensure both efficient and reliable finetuning. MiniGPT-v2 DecoVQA+100 DecoVQA+200 DecoVQA+400 Non-Repetition Relevance Groundedness 91.56 70.19 87. 91.48 72.59 85.49 88.35 71.64 83.15 LLaVA-1.5 DecoVQA+100 DecoVQA+200 DecoVQA+400 Non-Repetition Relevance Groundedness 71.92 67.92 80. 88.97 79.41 89.28 94.18 78.67 85.63 Table 5: Ablation study about VQD ability on finetuning models with DecoVQA+ with varying sample number. DecoVQA+400 is the version of DecoVQA+ with which we finetune MLLMs in other experiments."
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Models The versions of models we use are listed as follows, corresponding official tokenizers are applied for all the models: MiniGPT-v2, which is based on Llama2-Chat7B-HF (Touvron et al., 2023). A-OKVQA GQA VQA-Introspect Whether2Deco Models MiniGPT-v2 finetuned by DecoVQA+100 finetuned by DecoVQA+200 finetuned by DecoVQA+400 41.2 59.3 (+18.1) 61.3 (+20.1) 60.7 (+19.5) 44.2 51.4 (+7.2) 50.9 (+6.7) 50.7 (+6.5) 62.1 70.1 (+8.0) 73.4 (+11.3) 72.1 (+10.0) LLaVA-1.5 finetuned by DecoVQA+100 finetuned by DecoVQA+200 finetuned by DecoVQA+400 67.7 73.4 (+5.7) 74.4 (+6.7) 72.7 (+5.0) 52.1 53.6 (+1.5) 57.9 (+5.8) 57.2 (+5.1) 67.2 72.5 (+5.3) 78.7 (+11.5) 75.4 (+8.2) 46.8 54.3 (+7.5) 63.8 (+17.0) 61.0 (+14.2) 49.3 56.0 (+6.7) 69.0 (+19.7) 68.8 (+19.5) Table 6: Ablation study about VQA accuracy and Whether2Deco accuracy on finetuning models with DecoVQA+ with varying sample number. DecoVQA+400 is the version of DecoVQA+ with which we finetune MLLMs in other experiments. LLaVA-1.5, which is based on Vicuna-13B v1.5 (Chiang et al., 2023) and with lora as the pretraining schedule. Qwen-VL-Chat, which is based on Qwen7B (Bai et al., 2023a). InternVL-Chat-V1-5, which is based on InternLM2-20B (Cai et al., 2024). GPT-4-vision-preview, which is based on GPT4 (OpenAI et al., 2024). C.2 Finetuning Settings For all the mentioned open-source MLLMs, we use their official GitHub repository code to perform LoRA finetuning on the connection layer between the two modalities. All of the models are trained on 2 A40 GPU until the training loss converges. C.3 Inference Settings We use batch size of 1 in all inference tasks. Greedy search is used for all inferences. The parameters may be sub-optimal. C.4 Prompts C.4.1 Prompt for Scoring VQD Ability The complete prompt for scoring VQD ability, or specifically, the quality of sub-questions is shown in Figure 5. C.4.2 Prompt for Selective VQD As shown in Figure 6, firstly, we perform selective stage, which asks the model whether to decompose the question. If the model answers that it can directly answer the question without decomposition by \"Yes\", then implement the direct answering; if the model answers that it needs to decompose the question firstly by \"No\", then implement threephase decomposition process."
        },
        {
            "title": "D Datasets",
            "content": "D.1 Public Datasets The statistics of used public datasets are listed in Table 7 . A-OKVQA (Schwenk et al., 2022) is complex knowledge-based benchmark for VQA. As an augmented version of OK-VQA (Marino et al., 2019), the questions in A-OKVQA are not only diverse but also require wide-ranging commonsense and knowledge outside the image to answer. A-OKVQA has both multiple choice and openended question forms for each sample, and we select multiple choice here to cover more question types in the experiments. GQA (Hudson and Manning, 2019) features compositional questions related to real-world images, utilizing semantic representations of both scenes and questions to reduce language priors and conditional influences. VQA-Introspect (Selvaraju et al., 2020) is new dataset based on reasoning split from VQA (Antol et al., 2015) dataset, which contains complex reasoning questions with the open-ended form. VQA-Introspect consists of 200K perception questions as sub-questions to help answer difficult reasoning questions. Though this public dataset provides us large number of subquestions, questions with varying difficulties are mixed together and no label is pointing it out, which leads to bad finetuning results for selective decomposition. We randomly sampled 3,000 questions for the evaluation on VQA-Introspect. D.2 Proposed Datasets The statistics of proposed datasets are listed in Table 8."
        },
        {
            "title": "F Details of Data Construction",
            "content": "F.1 Pre-selection Strategies for Selecting Samples To identify questions from the A-OKVQA dataset that would benefit from decomposition, we employ specific pre-selection strategy. Initially, we used MLLM to perform zero-shot inference on the dataset, process we term \"direct inference\". Subsequently, we engaged the same model in another (a) An example of evaluation on effective sub-questions. (b) An example of evaluation on ineffective sub-questions (error). Figure 5: Prompt for scoring the quality of sub-questions with GPT-4V. (a) An example of prompt when the model chooses to directly answer the given question. (b) An example of prompt when the model chooses to decompose the given question. Figure 6: Prompt of selective decomposition samples in DecoVQA+. Dataset Dataset Type Question Type # Images # Questions A-OKVQA GQA VQA-Introspect external knowledge multiple choice visual reasoning visual reasoning open-ended questions open-ended questions 6,030 398 17, 6,702 12,578 22,793 Table 7: Experimental statistics for pubilc datasets used in the paper. Dataset Usage Motivation # Images # Questions SubQuestRater DecoVQA DecoVQA+ Whether2Deco evaluation measuring the quality of sub-questions improving the VQD ability finetuning improving the selective VQD ability finetuning testing the models ability to identify whether question requires decomposition evaluation 200 397* 397* 395* 200 400 400 400 Table 8: Experimental statistics for proposed datasets in the paper. *Several images correspond to more than one question. Algorithm 1: Evaluation algorithm for the quality of sub-questions :Sub-question :Set of sub-questions from one sample :Binary score for 3 criteria of sub-question B1, B2, B3 :Lists of binary scores for b1, b2, b3 s1, s2, s3 criteria of sub-questions :Score for 3 criteria for sample Check if there are effective sub-questions if == then s1 = 0 s2 = 0 s3 = 0 else for in do b1, b2, b3 = {ScoreM odel(q) Q} for [1,3] do AppendtoList(bi, Bi) for [1,3] do si = CalculateAverage(Bj) return s1, s2, s3 Figure 7: In some cases, GPT-4V will also produce subquestions that do not fit our criteria. round of zero-shot inference, but this time utilizing question decomposition prompt. In this round, the model is asked to decompose the main question into sub-questions, then answer these, and finally proceed to answer the main question. We refer to this method as \"decompose inference\". We choose MiniGPT-v2 as the multimodal LLM here. Our primary focus was on questions that were incorrectly answered in direct inference but correctly in decompose-inference, as these exhibited high likelihood of requiring decomposition. To find appropriate samples from the VQAIntrospect dataset, we adopt an automated preselection strategy based on BLEU (Papineni et al., 2002) metric. BLEU metric is originally used to measure the quality of machine translation, here we use it as metric to assess repetition. Since VQA-Introspect has provided large number of redundant sub-questions, we firstly filter out semantically repetitive sub-questions for each sample to prevent from overfitting. higher BLEU score between two sub-questions means that one of the sub-questions is repetitive. Then we set threshold number and choose the samples with remaining number of sub-questions exceeding the threshold. F.2 Annotation Process Given the proficiency of GPT-4V in VQD, as shown in Table 1, we utilize GPT-4V to generate initial sets of decomposed sub-questions for each selected sample. Subsequently, we perform meticulous manual review to these sub-questions. During this process, we eliminate sub-questions that do not contribute meaningfully towards answering the main question and also remove redundant sub-questions that share similar semantic content. Additionally, we supplement the sets with new sub-questions in instances where the decomposition logic appears incomplete, ensuring more thorough and effective decomposition process."
        },
        {
            "title": "G Robust Evaluation for MC Datasets",
            "content": "To compute the accuracy of the inference results under multiple choice setting, since an exact match of either option index or word can lead to serious underestimation, the first step is to map the model answer into one of four options. We have designed robust algorithm to evaluate the accuracy of multiple choice based on the method provided by A-OKVQA. As demonstrated in Alogrithm 2, if no or several options are detected in the model Figure 8: Comparison of VQD ability of different models across three evaluation criteria. Each bar chart represents specific criterion. The first row compares the number of the high-scored (75-100) samples generated by the original model (in cyan) and the corresponding model finetuned with DecoVQA+ (in yellow). The second row compares the number of the low-scored (0-25) samples generated by the original model (in pink) and the corresponding model finetuned with DecoVQA+ (in blue). The vertical axis shows the number of high-scored samples or low-scored samples, while the horizontal axis lists the models. The difference in bar height indicates the performance gain achieved through finetuning. MiniGPT-v2 original Model finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss Non-Repetition Relevance Groundedness 47.52 36.65 43. 93.72 74.17 85.98 88.35 71.64 83.15 90.58 73.73 84.53 LLaVA-1.5 original Model finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss Non-Repetition Relevance Groundedness 42.19 37.33 44.17 92.04 81.62 86.19 94.18 78.67 85.63 92.68 78.48 84.39 Qwen-VL-Chat original Model finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss Non-Repetition Relevance Groundedness 32.10 27.15 26.49 80.66 69.52 77.34 89.03 68.73 78.92 89.15 67.15 77.51 InternVL-Chat-V1original Model finetuned by DecoVQA finetuned by DecoVQA+ finetuned by DecoVQA+ with SelectiveVQD Loss Non-Repetition Relevance Groundedness 82.41 73.42 78.01 87.40 81.11 87.62 92.76 83.38 90.15 94.11 83.30 89. Table 9: Comparison of VQD abilities of all the original models and their corresponding finetuned versions. answer during the exact match step, we use SentenceTransformer (Reimers and Gurevych, 2019) to map the model answer to one option or use GPT4 to do the mapping when the answer is too long. We have observed that if the answer sentence is too long, especially when there is more than one option mentioned in the answer, the mapping by SentenceTransformer tends to be random and misleading. For computing the accuracy over openended questions, given that the reference answer in used datasets has only one or two words, if the reference answer is mentioned in the model output, the output is considered as correct. Algorithm 2: Robust algorithm for measuring accuracy on MC datasets :Model answer ˆa :Mapped option :Number of exact match options τ :Threshold of sentence length Attempt exact match and get mentioned options from model answer if == 1 then ˆa = ExactMatch(a) else if len(tokenize(a)) τ then ˆa = SentenceTransformer(a) else ˆa = GPT-4(a) return ˆa Figure 9: Variance of inference experiments with MiniGPT-v2 and LLaVA-1.5, plotted as error bars. Each experiment is conducted with three different random seeds, keeping other settings unchanged."
        },
        {
            "title": "H Variance",
            "content": "To verify the stability of our proposed method, each experiment was done with three different random seeds, while keeping other settings unchanged. The variance results in Figure 9 show that random seeds influence the accuracy of the model output very slightly. Does the finetuning hurt the all-around performance? Finetuning may lead to catastrophic forgetting, which hurts the essential all-around performance of MLLMs. MMBench (Liu et al., 2023) is systematic pipeline that evaluates the comprehensive abilities of MLLMs. Figure 10, 11, 12 and 13 demonstrate the evaluation results of different checkpoints on MMBench. It shows that our finetuning does not do harm to most of the abilities, while some of them are even improved after finetuning. Finetuning with DecoVQA+ vs. with VQA-Introspect The existing public dataset VQA-Introspect has already provided us with complex visual reasoning questions with sub-questions. However, not all questions are complex enough to require decomposition, and large number of provided subquestions are repetitive and superficial. To compare with the quality of our proposed dataset, we also finetune MLLMs with the entire training set of VQA-Introspect (excluding the samples used in the evaluation experiments). As shown in Table 10 and 11, the performance of the MLLMs finetuned with DecoVQA+ is much better than the ones finetuned with VQA-Introspect. The results demonstrate that the quality of our proposed dataset outperforms the existing public dataset with subquestions. Figure 10: Results of different checkpoints of MiniGPT-v2 across the 20 L-3 ability dimensions defined in MMBench. Figure 11: Results of different checkpoints of LLaVA-1.5 across the 20 L-3 ability dimensions defined in MMBench. Figure 12: Results of different checkpoints of Qwen-VL-Chat across the 20 L-3 ability dimensions defined in MMBench. Figure 13: Results of different checkpoints of InternVL-Chat-V1-5 across the 20 L-3 ability dimensions defined in MMBench. MiniGPT-v Finetuned by VQAintrospect Finetuned by DecoVQA+ Non-Repetition Relevance Groundedness 17.20 13.08 14.87 88.35 71.64 83.15 LLaVA-1.5 Finetuned by VQAintrospect Finetuned by DecoVQA+ Non-Repetition Relevance Groundedness 21.52 76.90* 93.50* 94.18 78.67 85.63 Table 10: Comparison of VQD abilities on MLLMs before and after finetuning with VQA-Introspect and with DecoVQA+. *Here for most of the original questions, LLaVA-1.5 produces one high quality sub-question, then repeats it for 2-3 times, causing relatively high score on Relevance and Groundedness, yet very low in Non-Repetition score. Models MiniGPT-v finetuned by VQAIntrospect finetuned by DecoVQA+ 41.2 48.8 (+7.6) 60.7 (+19.5) 44.2 39.6 (-4.6) 50.7 (+6.5) 62.1 63.7 (+1.6) 72.1 (+10.0) A-OKVQA GQA VQA-Introspect Whether2Deco LLaVA-1.5 finetuned by VQAIntrospect finetuned by DecoVQA+ 67.7 68.4 (+0.7) 72.7 (+5.0) 52.1 51.8 (-0.3) 57.2 (+5.1) 67.2 81.1 (+13.9) 75.4 (+8.2) 46.8 37.3 (-9.5) 61.0 (+14.2) 49.3 4.8* (-44.5) 68.8 (+19.5) Table 11: Comparison of VQA accuracy (%) on external knowledge (A-OKVQA) and visual reasoning (GQA and VQA-Introspect) datasets and Whether2Deco accuracy (%) before and after fine-tuning MLLMs with VQA-Introspect and with DecoVQA+. *Here LLaVA1.5 fails to follow the pre-defined answering template, but to perform pure question decomposition instead of selective decomposition."
        },
        {
            "title": "Method",
            "content": "Existing researches (You et al., 2023; Qi et al., 2023) tend to use convincing captioning model to convert images to the language descriptions, and then perform the unimodal question decomposition with LLMs. Table 12 shows the accuracy gap under the selective VQD inference setting between MLLMs and their corresponding LLMs with GPT4V as the captioning model. Since critical information in images is often lost during the captioning process, it is very possible for the subsequent inference with QD to fail to answer questions correctly. To sum up, VQD is better than the method \"caption + QD\". Models VQA-Introspect MiniGPT-v2 62.1 Llama2-Chat-7B-HF 46.2 LLaVA-1.5 Vicuna-13B-v1.5 67.2 62. Table 12: Comparison of VQA accuracy (%) between MLLMs and their corresponding language models on VQA-Introspect. To fairly compare with the ICL method used in (Khan et al., 2023), we apply the same 2-shot demonstration as the one applied in that paper to decompose questions. The prompt template is shown in Figure 14. The performance comparison in Table 13 and Table 14 shows that the models achieve significantly better performance in VQD ability, VQA accuracy and Whether-to-decompose accuracy through our finetuning pipeline, compared to the ICL method proposed in (Khan et al., 2023). Model Non-Repetition Relevance Groundedness MiniGPT-v2 (zero-shot) MiniGPT-v2 (ICL) MiniGPT-v2 (finetuned by DecoVQA+) LLaVA-1.5 (zero-shot) LLaVA-1.5 (ICL) LLaVA-1.5 (finetuned by DecoVQA+) 47.52 54.65 88.35 42.19 69.45 94.18 36.65 49.64 71.64 37.33 65.32 78.67 43.30 49.97 83. 44.17 62.58 85.63 Table 13: Comparison of VQD ability between MLLMs finetuned by DecoVQA+ and inference with ICLmethod across three evaluation criteria. Model A-OKVQA GQA VQA-Introspect Whether2Deco MiniGPT-v2 (zero-shot) MiniGPT-v2 (ICL) MiniGPT-v2 (finetuned by DecoVQA+) LLaVA-1.5 (zero-shot) LLaVA-1.5 (ICL) LLaVA-1.5 (finetuned by DecoVQA+) 41.2 40.1 64.0 67.7 65.1 73.9 44.2 43.6 51.7 52.1 51.3 56.7 62.1 60.5 72.5 67.2 67.4 75. 46.8 46.8 71.5 49.3 49.3 75.0 Table 14: Comparison of Accuracy (%) between MLLMs finetuned by DecoVQA+ and inference with ICL-method."
        },
        {
            "title": "M More case studies",
            "content": "L Comparison with In-context Learning"
        },
        {
            "title": "Method",
            "content": "More case studies in addition to Figure 4 are shown in Figure 15. Besides finetuning, In-context Learning (ICL) is also potential approach for VQD. The previous work (Khan et al., 2023) has explored VQD based on ICL methods. Therefore, we add an experiment to compare the performance with our finetuning pipeline and with the ICL method."
        },
        {
            "title": "N Licensing",
            "content": "Our proposed datasets SubQuestRater Dataset, DecoVQA, DecoVQA+, and Whether2Deco are built upon the public datasets A-OKVQA and VQAIntrospect. A-OKVQA has the Apache-2.0 License. Prompt under ICL setting (two-shot) Please firstly decompose the given question into several image-relevant sub-questions to help you answer the given question. Please avoid giving repeated sub-questions or generating an excessive number. Feel free to suggest an appropriate quantity based on your judgment. Here are two examples you can follow to decompose the question: Example 1 Question: Is the banana ripe enough to eat? Sub-questions: 1. Is the banana yellow? Example 2 Question: Is it cold outside? Sub-questions: 1. Are any people wearing jackets? Input Question: {question} Sub-questions: Figure 14: Prompt under ICL setting (two-shot) The licenses of the code for the mentioned MLLMs are listed as follows: MiniGPT-v2 has the BSD-3Cluase License, LLaVA-1.5 has the Apache-2.0 License, Qwen-VL-Chat has the Tongyi Qianwen License and InternVL-Chat-V1-5 has the MIT License. We publicize all of our proposed datasets and our code under the MIT License. (a) Cases with MiniGPT-v2 before and after being finetuned by DecoVQA+. (b) Cases with LLaVA-1.5 before and after being finetuned by DecoVQA+. (c) Cases with Qwen-VL-Chat before and after being finetuned by DecoVQA+. (d) Cases with InternVL-Chat-V1-5 before and after being finetuned by DecoVQA+. Figure 15: Case studies showing the comparison of VQD performance by MLLMs before and after finetuning by DecoVQA+."
        }
    ],
    "affiliations": [
        "Amazon Web Services",
        "LMU Munich",
        "MBZUAI",
        "Munich Center for Machine Learning",
        "Technical University of Munich",
        "University of Oxford"
    ]
}