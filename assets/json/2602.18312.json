{
    "paper_title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty",
    "authors": [
        "Zhaoming Xie",
        "Kevin Karol",
        "Jessica Hodgins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm."
        },
        {
            "title": "Start",
            "content": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty ZHAOMING XIE, KEVIN KAROL, and JESSICA HODGINS, Robotics AI Institute, USA 6 2 0 2 0 2 ] . [ 1 2 1 3 8 1 . 2 0 6 2 : r Fig. 1. Our system is able to learn smooth time-varying linear feedback policies for simulated character to perform challenging motion skills. Reinforcement learning provides framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding reward term that penalizes large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce new architecture called Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to fully connected neural network. We demonstrate that Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving number of motion imitation tasks with different characteristics, including dynamic motions such as backflip and various challenging parkour skills. Finally, Authors Contact Information: Zhaoming Xie, zxie@rai-inst.com; Kevin Karol, kkarol@ rai-inst.com; Jessica Hodgins, jhk@rai-inst.com, Robotics AI Institute, Cambridge, Massachusetts, USA. 2026 ACM XXXX-XXXX/2026/2-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn we apply this approach to create policies for dynamic motions on physical quadrupedal robot equipped with an arm. CCS Concepts: Computing methodologies Animation; Control methods; Reinforcement learning. Additional Key Words and Phrases: Linear Control, Physics-based Character Animation, Legged Robots ACM Reference Format: Zhaoming Xie, Kevin Karol, and Jessica Hodgins. 2026. Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty. 1, 1 (February 2026), 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "Introduction",
            "content": "Deep reinforcement learning (DRL) has proven effective in physicsbased character animation and robotics. However, policies that naively optimize for task rewards often favor unrealistic, highfrequency control signals. Even when the policies are constrained to imitate high-quality motion data, these unrealistic behaviors still prevail. This problem can be attributed to the learned policies being overly sensitive to slight changes in input signals. Deploying policies that are sensitive in this way on robots is more problematic, because input signals come from noisy sensory measurements and the control bandwidth is limited by what the physical actuators can achieve. Prior work addressed this issue by applying heavy penalties on the rate of change of control signals and randomizing policy , Vol. 1, No. 1, Article . Publication date: February 2026. 2 Zhaoming Xie, Kevin Karol, and Jessica Hodgins inputs to decrease sensitivity to variation. However, these methods are often task-dependent and require significant trial-and-error effort to tune. More recent work proposes using the Lipschitz-constrained policies to improve the smoothness of learned control policy. In this method, gradient penalty is imposed on the likelihood of control action under the current policy [Chen et al. 2025]. While this method has been effective for locomotion tasks, it remains unclear how well it extends to more challenging scenarios. Furthermore, it relies on large number of action samples to accurately estimate the true sensitivity of the actions with respect to the inputs and imposes significant computational overhead due to the additional backpropagation required for the gradient penalty. We propose an action Jacobian penalty, which penalizes the norm of the Jacobian of the control action with respect to the state of the character. This penalty significantly reduces high-frequency signals in learned policies across variety of motion imitation tasks. However, similar to the Lipschitz-constrained policies, computing the Jacobian of policy parameterized by fully connected feed forward (FF) neural network incurs significant computational overhead. To address this, we introduce Linear Policy Net (LPN), new architecture for parameterizing control policies. Instead of directly outputting control action, an LPN outputs feedback control matrix using only task information. This feedback matrix then operates on the state of the character to produce the control action. Computing the Jacobian of the action with respect to the state, and computing the gradient of the Jacobian penalty with respect to the network parameters, is reduced to simple forward and backward pass of the neural network. This approach effectively reduces the computation time required to impose the Jacobian penalty during training. In DeepMimic style tasks, learning LPN is equivalent to learning time-varying linear feedback policy. This parametrization significantly restricts the class of controller the policy can learn, compared to standard FF neural network. Our surprising finding is that this restriction does not negatively impact learning performance or the quality of the final policies. On the contrary, LPN is comparable to, and in some cases even surpasses, its FF counterparts. We demonstrate the effectiveness of LPN with Jacobian penalty on various motion imitation tasks using simulated human character, including challenging motions such as backflip, the table tennis footwork drill and challenging parkour skills that require nontrivial interactions with the environment. Furthermore, we show that the learned time-varying linear feedback control policies extracted from LPN are capable of controlling physical quadrupedal robot with an arm to perform combination of dynamic hopping and arm-swing motions."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we review related work in physics-based character animation with deep reinforcement learning and methods to train smooth control policies. We also review related work in synthesizing linear feedback controllers for simulated characters and robots. , Vol. 1, No. 1, Article . Publication date: February 2026."
        },
        {
            "title": "2.1 Deep Reinforcement Learning",
            "content": "DRL has become popular framework for physics-based character animation. Realistic character animations are often synthesized through imitation of kinematic references from motion capture data, e.g., [Peng et al. 2018; Zhang et al. 2025]. An advantage of DRL compared to other approaches is its scalability and versatility. Over the past decade, wide range of specialized tasks have been tackled with DRL, including scene interaction [Yu et al. 2021], object manipulations [Xu et al. 2025a; Yu et al. 2025], multi-character interaction [Zhang et al. 2023], music instrument play [Wang et al. 2024b] and sports play [Kim et al. 2025; Wang et al. 2024a]. By scaling up the system, hours of diverse human motion capture data can be reproduced in simulation, e.g., [Luo et al. 2023; Tessler et al. 2024; Won et al. 2020]. The versatility of this framework also opens up applications such as human motion tracking in virtual reality [Lee et al. 2023] and sim-to-real humanoid control [Liao et al. 2025; Luo et al. 2025; Ruben et al. 2024]."
        },
        {
            "title": "2.2 Learning Smooth Policies",
            "content": "Reinforcement learning tends to exploit high frequency signals to achieve high task rewards, which can produce jittery motions that degrade motion quality [Xie et al. 2023] and results in sim-to-real failure for robotics applications. High frequency signals can be reduced by adding filter to the action [Bergamin et al. 2019], however, this significantly reduces response to perturbation and can impact performance when performing dynamic motions. Directly penalizing the rate of change of actions in the form of reward signal has been the prevailing approach in sim2real RL for quadrupedal and humanoid robot control [Hwangbo et al. 2019; Liao et al. 2025; Miller et al. 2025]. This approach relies on random exploration of the policy during training to discover smooth behaviors, requires manual tuning to achieve balance between task completion and behavior regularization. Chen et al. [2025]; Mysore et al. [2021] directly penalized the change of action from the policy, via approximation to the Jacobian of the action with respect to the policy input. While this solution requires less per-task tuning, it imposes significant computational costs resulting in long training times and limiting it to simple control tasks like locomotion."
        },
        {
            "title": "2.3 Linear Feedback Control",
            "content": "Before DRL became popular, control policies were often formulated as linear feedback controllers. Motion can be segmented into stages and linear feedback controller can be designed for each stage to generate locomotion motion controllers [Yin et al. 2007] and various athletic behaviors [Hodgins et al. 1995]. These segmented controllers require substantial human effort as each motion needs custom designed segmentations and input features . Sampling based strategies can be used to search for linear feedback controllers. Ding et al. [2015] learns reduce-order linear feedback controllers via an evolutionary algorithm by directly sampling the feedback matrix. Liu et al. [2016] learns time varying linear feedback controller via iterative sampling and linear regression. These systems are able to learn robust linear feedback policies for dynamic motions such as backflip and cartwheel. However, these systems are usually complex, require manual definition of state representation specific to given motion, and have not been shown to generalize to motions that require more complex environment interactions such as vaulting and wall climbing. Through linear matrix parameterization, reinforcement learning or evolutionary algorithms can also be used to train linear feedback policy. This approach has been shown on various locomotion tasks. These policies either exploit unrealistic behaviors such as high frequency leg motions, e.g., [Mania et al. 2018], or require carefully designed features, e.g., [Krishna et al. 2021]. Time varying linear feedback control policies can also be synthesized using model-based control approaches such as differential dynamic programming and its variants, e.g.,[Li and Todorov 2004; Muico et al. 2009]. However, linear feedback controllers obtained via model-based approaches are often brittle, and often require online replanning using computationally expensive model predictive control approaches, e.g., [Eom et al. 2019]. There is also work that applies linearization to the equation of motion. This method allows for an efficient MPC formulation because it only requires solving small scale Quadratic Programming (QP) problem online. This approach has proven to be effective for quadrupedal and bipedal locomotion, e.g., [Bishop et al. 2025; Di Carlo et al. 2018]. However, the resulting controllers are still nonlinear due to the presence of inequality constraints such as the friction cone constraints."
        },
        {
            "title": "3 Problem Setup and System Overview",
            "content": "Our problem formulation is similar to DeepMimic [Peng et al. 2018], with reference motion = {틙洧눖1, 틙洧눖2, . . . , 틙洧눖洧녢 } provided as input, where 틙洧눖洧노 R洧녵 specifies the full pose of the character state at timestep 洧노. control policy is map 洧랢 : R洧녵 R洧녵 R洧녴 where the character state 洧눖洧노 and the reference state 틙洧눖洧노 are used to generate control action via 洧눅洧노 = 洧랢 (洧눖洧노, 틙洧눖洧노 ). The control action 洧눅洧노 is target angle for each actuated joint of the character. joint level Proportional-Derivative (PD) controller is then used to actuate the character in physics simulation. The control policy is trained with DRL to drive the character to imitate the reference. In our experiment, we use the same humanoid character as DeepMimic and simulated quadrapedal robot, modeling the Boston Dynamics Spot quadraped, with arm attached. Our reward set up is simplified version of DeepMimic, where 洧 = 0.3洧릃os + 0.3洧릂rientation + 0.4洧륽oint with 洧릃os = exp(50 틙洧눛 洧눛 2), 洧릂rientation = exp(10 틙洧눓洧눕 洧눍 洧눓洧눕 洧눍2), 洧륽oint = exp(2 틙洧눎 洧눎2), where 洧눛, 洧눓洧눕 洧눍 and 洧눎 are the root position, root orientation and the joint angles of the character respectively. Similar to DeepMimic, we also apply reference state initialization and early termination to improve the learning efficiency. We use Mujoco [Todorov et al. 2012] to simlulate the characters. The simulation runs at 120 Hz while the policy is updating the joint target at 30 Hz. deep neural network is used to parameterize the policy. In this paper, we experiment with both the standard Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty 3 fully connected feedforward (FF) neural net and the Linear Policy Net (LPN) (described in Section 5). During training, the actions are sampled from fixed Gaussian distribution where the mean is the output of the policy and the covariance matrix is diagonal matrix with diagonal element 洧 2 = 0.01. We use Proximal Policy Optimization (PPO) [Schulman et al. 2017] to optimize the policies. At each iteration of the PPO algorithm, 50 parallel simulation environments are used to collect 2500 samples by using the control policy to interact with the simulations. PPO uses these samples to optimize for loss function LPPO, via gradient descent on the policy parameters. For all the experiments in this paper, we run PPO for maximum of 5000 iterations, which takes around 2.5 hours on workstation with 12 CPU cores for running the simulations and NVIDIA RTX A6000 for neural network inference and optimization. Most of the time the training already converges after 2000 iterations, which takes about 1 hour with 5 million samples collected. This performance is comparable to GPU-based simulation framework in terms of both training time and number of simulation samples required, e.g., [Zhang et al. 2025]. Additional reward terms are often added to the original DeepMimic loss to reduce motion artifacts such as motion jitteriness or high impact, especially for robotics applications [Chen et al. 2025; Liao et al. 2025]. These additional terms cause additional tuning efforts and often require task specific tuning. In this work, we propose to use Linear Policy Net with an action Jacobian penalty as regularization loss. This approach introduces smooth behaviors to the policies with minimal tuning and compute overhead while also maintaining learning efficiency. We now describe each component in the more detail."
        },
        {
            "title": "4 Action Jacobian Penalty",
            "content": "Reinforcement learning relies on injecting Guassian noise to the policy output for exploration and data collection. This process creates unrealistic high frequency signals that can achieve high rewards. Reinforcement learning then tends to fit the policies to these high frequency signals, resulting in policies that can produce undesired and unnatural jittery control signals. This problem is more pronounced in robotics applications, where real world noise from sensor measurements and imperfect motor commands amplify the high frequency control signals until they are visible. Existing work relies on adding term in the reward function to penalize the difference between the actions over consecutive time steps to regularize action change over time. But the regularization effect tends to be small compared to the other rewards to avoid reducing the effectiveness of the task reward. The size of this effect sometimes causes it to become ineffective in the face of challenging tasks since various reward terms are summed over and the effect of the regularization need to be discovered by the inherent noisy nature of the exploration procedure. We propose directly regularizing the policy with an action Jacobian penalty. Instead of using it as reward signal, we directly add loss term to the PPO optimization: Ltotal = LPPO + 洧녻JacLJac, , Vol. 1, No. 1, Article . Publication date: February 2026. 4 Zhaoming Xie, Kevin Karol, and Jessica Hodgins where 洧녻Jac is tunable weighting factor, and LJac = J2 is the square of the Frobenius norm of J, with the Jacobian of the policy: = 洧녩1 洧1 ... 洧녩洧녴 洧1 . . . . 洧녩1 洧멇롐 ... 洧녩洧녴 洧멇롐 We use 洧녻Jac = 10 across all our experiments. The action Jacobian captures the sensitivity of the action generated by the policy with respect to the changes in character state. For 洧놈 with larger norm, small variation in the character state will cause large changes in action. The resulting motion will exhibit high frequency joint oscillation. By penalizing the magnitude of the action Jacobian, policy will generate smoother control signals. There are prior works that try to approximate 洧놈 to encourage smooth policy. Mysore et al. [2021] use sampling strategies around the collected data point to approximate the Jacobian, which incurs significant computation costs to get an accurate approximation. Chen et al. [2025] penalizes the gradient of likelihood of sampled action. This approach is equivalent to penalizing J洧녢 (洧눅 洧눅mean)2, where 洧눅 is the sampled action during exploration and 洧눅mean is the mean of the Gaussian distribution the policy will sampled from. This penalty only optimizes for specific direction of the Jacobian and requires more samples to optimize the full Jacobian. We propose directly optimizing for the norm of the full Jacobian, which can be achieved via auto-differentiation and backpropagation. While penalizing the norm of the action Jacobian is straightforward thanks to the autograd features in modern deep learning framework such as PyTorch [Paszke et al. 2019], it imposes significant computation overhead. Optimizing for the action Jacobian penalty with fully connected neural network, each iteration of PPO is about 1.5 times slower in our implementation than using only the PPO loss."
        },
        {
            "title": "5 Linear Policy Net",
            "content": "We introduce Linear Policy Net (LPN) to parameterize our policies. See Figure 2. The input to the policy is the tuple {洧눖洧노, 틙洧눖洧노 } that represents the state of the simulated character and the desired state from the reference motion at time step 洧노. The reference motion 틙洧눖洧노 is fed into two layer MLP, the output of the MLP is feedback matrix 洧쮫롐 R洧녴洧녵 and feedforward action 洧눏洧노 R洧녴, where 洧녵 is the dimension of the state of the character and 洧녴 is the dimension of the control action. Control action 洧눅洧노 = 洧쓇롐 洧눖洧노 + 洧눏洧노 + 틙洧눅洧노 is then applied to advance the simulation, where 틙洧눅洧노 is the reference joint angle corresponding to the actuated joints, extracted from the reference state. Note that 洧쓇롐 and 洧눏洧노 do not depend on the state of the character, in the case of fixed sequence of reference motion, this approach is equivalent to learning time-varying linear feedback control policy, where the feedback matrix and feedforward action only depend on time. There are several design choices to be made and we will discuss them in this section. , Vol. 1, No. 1, Article . Publication date: February 2026. Fig. 2. We introduce Linear Policy Net. fully connected multilayer perceptrons (MLP) is used to generate feedback matrix 洧쮫롐 and feedforward action 洧눏洧노 from the reference state 틙洧눖洧노 . The final control action is generated by applying the linear feedback matrix on the simulation state plus the feedforward terms. 5."
        },
        {
            "title": "Input Features",
            "content": "Prior work on learning linear feedback controllers often includes manual design features such as the end effector positions in the input [Ding et al. 2015; Liu et al. 2016]. DeepMimic similarly includes maximal coordinate of the character that includes positions and orientations of the body parts [Peng et al. 2018]. We choose to use the minimal coordinate system of the character as our policy input, which includes the displacement in the position and orientation of the root to the desired configuration specified in the reference motion, the root linear and angular velocity, and the joint angle and joint velocity of the actuated joints of the character. These features can also be easily obtained from state estimator on physical quadrupedal robot, making sim-to-real straightforward. Another option is to use both the reference state and character state as input to the MLP. While this makes the policy more expressive, the linear feedback matrices would then depend on the character state. We found this dependency unnecessary as the LPN can successfully learn time-varying linear feedback policies for dynamic motion in both simulation and on physical robot without it."
        },
        {
            "title": "5.2 Action Space",
            "content": "An added benefit of learning linear feedback matrix is that we can update the feedback matrix and feedforward term 洧쓇롐, 洧눏洧노 at slower rate than 洧눅洧노 . Such hierarchy is already present in current DRL frameworks for legged robot control, where the policy is updating the joint target angle at slow rate of around 50 Hz, while there is joint level PD controller running at much higher frequency of 500 Hz or more. Within our framework, we can inference the MLP at even lower rate, down to 15 Hz for some tasks, while maintaining the update frequency of the joint PD target computation and joint level PD control. Note that because the learnable parameters in the system are in the MLP layer, we can potentially use the feedback matrix 洧쓇롐, 洧눏洧노 as our action space, and then do the linear feedback within the simulation loop, hidden from the learning loop. However, this approach increases the action space dimension from 洧녴 to (洧녵 + 1) 洧녴, making learning much harder."
        },
        {
            "title": "6.2 Comparison",
            "content": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty 5 Because the control policy learned by LPN is in the form of 洧눅洧노 = 洧쓇롐 洧눖洧노 +洧눏洧노 + 틙洧눅洧노 , with 洧쓇롐 being independent of 洧눖洧노 , the action Jacobian at time step 洧노 is equal to 洧쓇롐 . Computing the action Jacobian penalty and computing its gradient with respect to the learnable parameters then becomes simple forward pass and backward pass of the MLP layer in the LPN. This process incurs minimal additional computation cost because the forward and backward pass are already required for evaluating the PPO loss term."
        },
        {
            "title": "6 Evaluations",
            "content": "We evaluate our systems on wide range of physics-based character animation tasks and on physical quadrupedal robot. In all the examples, the MLP layer in the LPN is 2-layer fully connected neural network with hidden layer size 256. For baseline methods, we use the 2-layer fully connected network (FF) that is typically used in these tasks, with hidden layer size 256."
        },
        {
            "title": "6.1 DeepMimic Task with Simulated Humanoid",
            "content": "We apply our method to number of motion imitation tasks for simulated humanoid. These tasks can be classified into four categories. Locomotion Tasks. We apply our framework to train the humanoid to imitate reference motions for walking and running. We use the reference data from DeepmMimic [Peng et al. 2018]. Gymnastic Motion. To demonstrate that our system works on dynamic motions, we train the LPN with the action Jacobian penalty to imitate range of dynamic gymnastic motions, including backlip, sideflip and cartwheel. While time-varying linear feedback policy has been shown to work with these motions previously [Liu et al. 2016], it requires human designed input features that may not generalize to other motions. We demonstrate that simple input features work across all the motions we considered. Imitating Single Sequence. We apply our framework to train the humanoid to imitate 15 second clip of motion capture of table tennis footwork drill. The data is obtained from [Wang et al. 2024a], and involves dynamic whole body motion where the subject rapidly hops sideways while performing fast forehand drive. We also demonstrate tracking of break dance motion, with motion retargetted to the simulated character using motion capture data from [CMU Graphics Lab 1999]. This result demonstrates the generalization of our approach beyond cyclic motion. Environment Interaction. We demonstrate that our system is also able to learn to interact with the environment with non-trivial contact, such as parkour motion, with motion capture dataset from [Xu et al. 2025b; Zhang et al. 2025]. In particular, we train policies to execute reverse vault sequence, wall climbing sequence and double kong sequence, which has been shown to be challenging to learn using DeepMimic [Zhang et al. 2025]. We also learn to imitate soccer juggling sequence, following the set up from [Xie et al. 2022], to demonstrate the interactions with dynamic environment. We compare our system with four alternatives, all implemented with FF neural network: Feedforward Neural Net with an action Jacobian Penalty, where we apply the action Jacobian penalty during training. This is labeled as FF + Jac Pen. No regularization, where we directly optimize for LPPO using the imitation reward. This is labeled as No Reg. Action Change Penalty, where reward 洧르ction = 洧녻action 洧눅洧노 洧눅洧노 1 2 is used to penalize the changes in action between two timesteps. The weight 洧녻action is tunable parameter. We experiment with three sets of weights: 洧녻action = 0.01, 0.1, 1. We label them as reward 0.01, reward 0.1 and reward 1. Lipschitz Constraint Policy [Chen et al. 2025], where instead of minimizing the norm of the action Jacobian, penalty is applied to LLipschtiz = 洧눅洧눅mean 2 , where 洧눅mean is the mean of the Gaussian distribution the policy will sample from while 洧눅 is the sampled action. This method is equivalent to minimizing 洧놈 洧녢 (洧눅 洧눅mean)2. It does not require computation of the whole Jacobian matrix but will only optimize in the direction of the sampled 洧눅. We use the same weighting factor as the action Jacobian penalty and optimize for Ltotal = LPPO + 10LLiptschitz. This is labeled as Lipschitz. 洧녬洧 We compare the learning performance on walking, backflip and the table tennis footwork drill, taking the average over three training runs with different random seeds. The learning curve is shown in Fig. 3. The reward is computed only with the imitation reward. Applying large action penalty (Reward 1) slows down task learning but results in better motion imitation performance, except for the backflip, where large action change penalty causes learning to fail. FF with an action Jacobian Penalty takes almost twice as many learning iterations to converge compared to other methods, and each iteration is about 1.5 time slower due to the Jacobian penalty computation. The Lipschitz constraint policy shows fast convergence in walking and backflip tasks, but converges slowly for the footwork task. Furthermore, it fails to produce smooth policy compared to the alternatives, as we will show later. The LPN with the Jacobian penalty has the fastest learning convergence across all tasks with minimal computation overhead per iteration. To quantify the smoothness of policy, We collect data by rolling out the policy in simulation. For walking and the backflip, we run the policy for five motion cycles and for the table tennis footwork drill, we run the policy over the complete sequence. We also quantify the smoothness of each policy using the following metrics: Action Smoothness: This metric evaluates the average action change of policy: (cid:205)洧녢 +1 洧노 =1 洧눅洧노 洧눅洧노 1 2 洧녢 . High Frequency Ratio: We compute the Fourier transform of the action output over time. Humans typically have control bandwidth of about 10 Hz [Hogan 2022], we consider signal content higher than 10 Hz to be unnatural. We use the proportion of the energy that is higher than 10 Hz with respect to the total energy to characterize the smoothness of the signal. , Vol. 1, No. 1, Article . Publication date: February 2026. 6 Zhaoming Xie, Kevin Karol, and Jessica Hodgins Table 1. Comparison of smoothness of policies trained with different methods, average over three runs with different random seeds. We highlight the best performance. The LPN has top three performance in all the measures except for the motion jerk metric for the backflip task. Metric by Motion Type LPN FF+Jac Pen Lipschitz No Reg reward 0.01 reward 0.1 reward 1 Walking Action Smoothness High Frequency Ratio Motion Jerk Backflip Action Smoothness High Frequency Ratio Motion Jerk Footwork Action Smoothness High Frequency Ratio Motion Jerk 0.0016 0.9 115.6 0.0014 2.1 108.5 0.0040 9.5 139.5 0.0031 8.8 134.2 0.061 8.7 140. 0.009 1.3 116.6 0.042 4.0 109.6 0.014 5.6 124.6 0.195 33.5 170.8 0.036 21.0 164.6 0.148 26.6 168. 0.053 27.1 178.1 0.0032 9.2 134.3 0.068 11.8 132.9 0.018 13.6 143.5 0.0025 5.5 134.0 0.031 2.8 111. 0.010 2.0 118.0 0.0015 1.5 106.53 N/A N/A N/A 0.005 0.5 94.6 Motion Jerk We compute the jerk metric of the motion, following [Rohrer et al. 2002]. From the joint acceleration signal sampled at 120 Hz, we use finite differencing to compute the joint jerk. We then evaluate the jerk metric by dividing the mean jerk magnitude by the peak speed, average over the 28 joints on the character. lower value in the jerk metric corresponds to smoother motion. The result is recorded in Table 1. Heavily penalizing the action change in the reward (reward 1) can produce smooth policies. However, it fails to learn dynamic tasks like backflip. reward 0.1 is also able learn smooth behaviors, with reduced leaning convergence rate, especially for the backflip task. Task specific reward tuning is necessary to obtain the best performance with the action change penalty. The Lipshcitz constraint policies consistently fail to achieve smooth policies compared to the other methods. FF policies with an action Jacobian penalty are able to learn smooth policies that are competitive with the methods that use reward that heavily penalizes action change, but they are slow to train. LPN is able to learn smooth policies while maintaining fast learning convergence rate. Note that in the backflip task, the smoothness metric for LPN is worse than the feedforward neural network policies with Jacobian penalty or appropriate reward set up. We conjecture that the backflip is challenging motion for time-varying linear feedback control policies, requiring the LPN to produce higher frequency action. We also plot the action of the pitch joint on the left ankle for the backflip and table tennis footwork in Fig. 4 to provide visual demonstration of the importance of regularizing the control action, either via an action change penalty as reward signal or using our action Jacobian penalty. Without action regularization, policies tend to generate actions that change rapidly, resulting in jittery motions, potentially causing the feet to chatter on the ground."
        },
        {
            "title": "6.3 Linear Policy Net Evaluations",
            "content": "Reduced-Order Linear Feedback Policy. Inspired by [Ding et al. 2015], we experiment with how to obtain reduced-order linear , Vol. 1, No. 1, Article . Publication date: February 2026. policy by using low rank linear feedback matrices. We perform singular value decomposition on the learned feedback matrices to compute their low rank approximation. Specifically, for feedback matrix 洧쓇롐 , its singular value decomposition is in the form of 洧쓇롐 = 洧높 洧뛐洧놓洧녢 , with 洧높 and 洧놓 being orthogonal matrices that form bases for the action space and state space of the character respectively, and 洧뛐 is diagonal matrix, whose diagonal elements quantify the importance of the corresponding dimension in those bases. best rank 洧녲 approximation of the feedback matrix can be obtained via 洧쓇롐,洧노 = 洧높洧녲 洧뛐洧녲 洧놓洧녢 洧녰 , where 洧눘洧녰 and 洧눙洧녰 are the 洧녰th column of 洧높 and 洧놓 respectively. 洧녲 = (cid:205)洧녲 洧녰=1 洧랥洧녰 洧눘洧녰洧눙洧녢 Our action space is 28 dimensional corresponding to the 28 joints on the character. We find that we can lower the rank of the feedback matrices learned by LPN while still maintaining performance. For example, sequence of rank 14 feedback matrices are able to retain the performance of walking policy. We can further reduce the rank of these matrices down to two. While they can still command the character to walk, the motion quality degrades. We can also find low rank approximation of the policies for other motions. For example, the rank of backflip policy can be reduced to 20, the rank of cartwheel policy can be reduced to 22, and the rank of table tennis footwork drill policy can be reduced to 18. Terrain Adaptation. We adapt our backflip policy and cartwheel policy to handle uneven terrain by finetuning them on sinusoidal terrain. Even though these policies do not perceive the terrain, the underlying linear feedback structure is able to handle the perturbation. This experiment demonstrates the robustness of the time varying linear feedback polices learned by LPN. Linear Feedback Update Rate. LPN is trained to update the linear feedback matrix and the feedforward action at rate of 30 Hz. We experiment with how much we can slow down the update rate while still maintaining good motion tracking performance. We find that we can update the feedback matrix at 10 Hz with walking policy, while policies for other motions fail when we try to lower the update rate below 30 Hz. In [Liu et al. 2016], the linear feedback control policies can be queried at slow rate of 10 Hz, even for highly dynamic motions such as backflip. It will be interesting to figure out how to train LPN to operate at this slower rate. Policy Distillation and Transitions between Skills. We can train policies for different motions and distill them into single policy, following the procedure in [Truong et al. 2024]. The resulting policies can track different reference motions in sequence. In particular, we train LPN policy to track jumping, sideflip and backflip via distillation. The resulting policies can then execute jumping, sideflip and backflip in sequence. We also train FF policy via the same distillation procedure, it fails to perform the agile transition between sideflip and backflip."
        },
        {
            "title": "6.4 Sim-to-real on a Quadrupedal Robot",
            "content": "We demonstrate that the time-varying linear feedback policies trained with our framework can be readily applied to physical robot. We use quadrupedal robot Spot with an arm attached to the body as our platform. To reduce the sim-to-real gap, we implement the actuator model of the leg motors that correspond to the physical actuator model specified by the manufacturer, following [Miller et al. 2025]. The results are shown in the supplementary video. To train locomotion policy, reference motion is generated for pacing gait on Spot using handcrafted sinusoid for the joints on the legs, with gait cycle of 0.6 second, similar to [Xie et al. 2021]. During training, the joints on the arm are set to random target sampled around the current configuration every 0.5 second. The result is time-varying linear feedback policy that can maintain stable pacing motion while executing fast arm movement. Instead of querying the LPN during run time, we precompute sequence of linear matrices offline and apply them in sequence. We update the linear feedback matrices at 15 Hz, and compute the joint target angles for the PD controller using linear feedback at 30 Hz. This significantly reduces the computation load required to maintain stable motions with FF policy, which requires inference of neural network at 50 Hz [Miller et al. 2025]. To demonstrate combination of an agile arm movement and an agile lower body movement, we generate hopping motion via trajectory optimization of single rigid body model [Di Carlo et al. 2018], and an agile table tennis stroke motion for the arm using kinematic MPC planner [Nguyen et al. 2025]. After synchronizing these motions, we create reference motion for Spot that is similar to the table tennis footwork practice drill of human player. Again, an LPN is able to learn time-varying linear feedback policy to execute the motion."
        },
        {
            "title": "7 Conclusions and Discussion",
            "content": "In this paper, we present framework to efficiently train smooth time-varying linear control policies for motion imitation tasks for simulated character and physical quadrupedal robot. surprising finding is that this time-varying linear policy exists for wide range of motions without needing special feature engineering. Another way to synthesize time-varying linear feedback policy is via model-based control such as differential dynamic programming (DDP), although matrices obtained via DDP are often less robust. It would be interesting to explore how to combine both Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty 7 approaches, so that we can gain the benefit of sample efficiency from model-based approaches while retaining the robustness afforded by the DRL framework. For example, we could warm start the feedback matrices with solutions from the DDP method, e.g., [Levine and Koltun 2013]. Given that our our learned policies are in the simple form of linear matrices, the policies can potentially be more easily explained than typical black box feedforward neural network policy. For example, it is possible that there exists DDP formulation that with the appropriate costs and transition dynamics can reproduce the same feedback matrices learned by our system. Inverse optimal control techniques can potentially be used to search for this formulation, e.g., using [Amos et al. 2018]. While the action Jacobian penalty produces smooth policies for many motion imitation tasks, it only considers the derivatives with respect to state but not time. For dynamic motions such as backflip, where the state of the character has to change rapidly, penalizing the action Jacobian alone is not guaranteed to reduce the changes of action in time. This observation may also explain why using the action Jacobian penalty is less effective for the backflip in our experiment. As future work, it will be interesting to explore how we can learn piece-wise linear policies in the state space. For example, we can segment the state space into regions where there exists linear control policy for each region. Neural network policies that use ReLU as an activation function can already be used to generate such regions based on the activation patterns [Cohan et al. 2022; Tjeng et al. 2017]. However, without regularization, such regions are usually too fine-grained. Learning linear feedback control policies that have large region of attractions allowing us to divide the state space into manageable pieces might further improve policy robustness and explainability. We focus on the imitation of relatively short motion segments compared to current DRL systems that can scale to imitating long motion sequences. It is conceivable that by scaling the system to imitate more and longer motions, one may find one-to-one correspondence between feasible motion capture data and the set of feedback matrices. Such correspondence can enable us to learn policy generator, where generative model such as diffusion model [Rocca et al. 2025] can be used to generate the linear feedback policies. While we demonstrate skill composition via policy distillation, we are not yet able to transition between arbitrary skills. Scaling the system to large motion dataset and building control graph [Liu et al. 2016] can potentially automate more diverse transitions. Our formulation limits our use case to DeepMimic style motion imitation tasks. Expanding the formulation to other tasks, e.g., using adversarial motion imitation [Zhang et al. 2025], or tasks where motion capture data is not available will be an interesting direction."
        },
        {
            "title": "References",
            "content": "Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and Zico Kolter. 2018. Differentiable mpc for end-to-end planning and control. Advances in Neural Information Processing Systems 31 (2018). Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019. DReCon: data-driven responsive control of physics-based characters. ACM Transactions On Graphics (TOG) 38, 6 (2019), 111. Arun Bishop, Juan Alvarez-Padilla, Sam Schoedel, Ibrahima Sory Sow, Juee Chandrachud, Sheitej Sharma, Will Kraus, Beomyeong Park, Robert Griffin, John , Vol. 1, No. 1, Article . Publication date: February 2026. 8 Zhaoming Xie, Kevin Karol, and Jessica Hodgins Dolan, et al. 2025. The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control. In 2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids). IEEE, 17. Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao, Yanjie Ze, Zhongyu Li, Shankar Sastry, Jiajun Wu, Koushil Sreenath, Saurabh Gupta, et al. 2025. Learning smooth humanoid locomotion through Lipschitz-constrained policies. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 47434750. CMU Graphics Lab. 1999. CMU Graphics Lab Motion Capture Database. Setareh Cohan, Nam Hee Kim, David Rolnick, and Michiel van de Panne. 2022. Understanding the evolution of linear regions in deep reinforcement learning. Advances in Neural Information Processing Systems 35 (2022), 1089110903. Jared Di Carlo, Patrick Wensing, Benjamin Katz, Gerardo Bledt, and Sangbae Kim. 2018. Dynamic locomotion in the mit cheetah 3 through convex model-predictive control. In 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 19. Kai Ding, Libin Liu, Michiel Van de Panne, and KangKang Yin. 2015. Learning reducedorder feedback policies for motion skills. In Proceedings of the 14th ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 8392. Haegwang Eom, Daseong Han, Joseph Shin, and Junyong Noh. 2019. Model predictive control with visuomotor system for physics-based character animation. ACM Transactions on Graphics (TOG) 39, 1 (2019), 111. Jessica Hodgins, Wayne Wooten, David Brogan, and James OBrien. 1995. Animating human athletics. In Proceedings of the 22nd Annual Conference on Computer graphics and Interactive Techniques. 7178. Neville Hogan. 2022. Contact and physical interaction. Annual Review of Control, Robotics, and Autonomous Systems 5, 1 (2022), 179203. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. 2019. Learning agile and dynamic motor skills for legged robots. Science Robotics 4, 26 (2019), eaau5872. Minsu Kim, Eunho Jung, and Yoonsang Lee. 2025. PhysicsFC: Learning User-Controlled Skills for Physics-Based Football Player Controller. ACM Transactions on Graphics (TOG) 44, 4 (2025), 121. Lokesh Krishna, Utkarsh Mishra, Guillermo Castillo, Ayonga Hereid, and Shishir Kolathaya. 2021. Learning linear policies for robust bipedal locomotion on terrains with varying slopes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 51595164. Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won, and Alexander Winkler. 2023. Questenvsim: Environment-aware simulated motion tracking from sparse sensors. In ACM SIGGRAPH 2023 Conference Proceedings. 19. Sergey Levine and Vladlen Koltun. 2013. Guided policy search. In International Conference on Machine Learning. PMLR, 19. Weiwei Li and Emanuel Todorov. 2004. Iterative linear quadratic regulator design for nonlinear biological movement systems. In First International Conference on Informatics in Control, Automation and Robotics, Vol. 2. SciTePress, 222229. Qiayuan Liao, Takara Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, and Karen Liu. 2025. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241 (2025). Libin Liu, Michiel Van De Panne, and KangKang Yin. 2016. Guided learning of control graphs for physics-based characters. ACM Transactions on Graphics (TOG) 35, 3 (2016), 114. Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. 2023. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1089510904. Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Casta침eda, ZiAng Cao, Jiefeng Li, David Minor, Qingwei Ben, et al. 2025. Sonic: Supersizing motion tracking for natural humanoid whole-body control. arXiv preprint arXiv:2511.07820 (2025). Horia Mania, Aurelia Guy, and Benjamin Recht. 2018. Simple random search of static linear policies is competitive for reinforcement learning. Advances in neural information processing systems 31 (2018). A.J. Miller, Fangzhou Yu, Michael Brauckmann, and Farbod Farshidian. 2025. HighPerformance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures. 2025 IEEE International Conference on Robotics and Automation (ICRA) (May 2025), 99819988. Uldarico Muico, Yongjoon Lee, Jovan Popovi캖, and Zoran Popovi캖. 2009. Contact-aware nonlinear control of dynamic characters. In ACM SIGGRAPH 2009 papers. 19. Siddharth Mysore, Bassel Mabsout, Renato Mancuso, and Kate Saenko. 2021. Regularizing action policies for smooth control with reinforcement learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 18101816. David Nguyen, Zulfiqar Zaidi, Kevin Karol, Jessica Hodgins, and Zhaoming Xie. 2025. Whole Body Model Predictive Control for Spin-Aware Quadrupedal Table Tennis. arXiv preprint arXiv:2510.08754 (2025). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019). , Vol. 1, No. 1, Article . Publication date: February 2026. Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG) 37, 4 (2018), 114. Michele Rocca, Sune Darkner, Kenny Erleben, and Sheldon Andrews. 2025. Policy-Space Diffusion for Physics-Based Character Animation. ACM Transactions on Graphics 44, 3 (2025), 118. Brandon Rohrer, Susan Fasoli, Hermano Igo Krebs, Richard Hughes, Bruce Volpe, Walter Frontera, Joel Stein, and Neville Hogan. 2002. Movement smoothness changes during stroke recovery. Journal of Neuroscience 22, 18 (2002), 82978304. Ruben, Espen, Michael, Georg, Jared, Steven, David, and Moritz. 2024. Design and control of bipedal robotic character. (2024). John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. 2024. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG) 43, 6 (2024), 121. Vincent Tjeng, Kai Xiao, and Russ Tedrake. 2017. Evaluating robustness of neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356 (2017). Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 50265033. Takara Everest Truong, Michael Piseno, Zhaoming Xie, and Karen Liu. 2024. Pdp: Physics-based character animation via diffusion policy. In SIGGRAPH Asia 2024 Conference Papers. 110. Jiashun Wang, Jessica Hodgins, and Jungdam Won. 2024a. Strategy and skill learning for physics-based table tennis animation. In ACM SIGGRAPH 2024 Conference Papers. 111. Ruocheng Wang, Pei Xu, Haochen Shi, Elizabeth Schumann, and Karen Liu. 2024b. F칲relise: Capturing and physically synthesizing hand motion of piano performance. In SIGGRAPH Asia 2024 Conference Papers. 111. Jungdam Won, Deepak Gopinath, and Jessica Hodgins. 2020. scalable approach to control diverse behaviors for physically simulated characters. ACM Transactions on Graphics (TOG) 39, 4 (2020), 331. Kaixiang Xie, Pei Xu, Sheldon Andrews, Victor Zordan, and Paul Kry. 2023. Too stiff, too strong, too smart: Evaluating fundamental problems with motion control policies. Proceedings of the ACM on Computer Graphics and Interactive Techniques 6, 3 (2023), 117. Zhaoming Xie, Xingye Da, Michiel Van de Panne, Buck Babich, and Animesh Garg. 2021. Dynamics randomization revisited: case study for quadrupedal locomotion. In 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 49554961. Zhaoming Xie, Sebastian Starke, Hung Yu Ling, and Michiel van de Panne. 2022. Learning soccer juggling skills with layer-wise mixture-of-experts. In ACM SIGGRAPH 2022 Conference Papers. 19. Michael Xu, Yi Shi, KangKang Yin, and Xue Bin Peng. 2025b. Parc: Physics-based augmentation with reinforcement learning for character controllers. In ACM SIGGRAPH 2025 Conference Papers. 111. Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui. 2025a. Intermimic: Towards universal whole-body control for physics-based human-object interactions. In Proceedings of the Computer Vision and Pattern Recognition Conference. 12266 12277. KangKang Yin, Kevin Loken, and Michiel Van de Panne. 2007. Simbicon: Simple biped locomotion control. ACM Transactions on Graphics (TOG) 26, 3 (2007), 105es. Ri Yu, Hwangpil Park, and Jehee Lee. 2021. Human dynamics from monocular video with dynamic camera movements. ACM Transactions on Graphics (TOG) 40, 6 (2021), 114. Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, and Qifeng Chen. 2025. Skillmimic-v2: Learning robust and generalizable interaction skills from sparse and noisy demonstrations. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. 111. Yunbo Zhang, Deepak Gopinath, Yuting Ye, Jessica Hodgins, Greg Turk, and Jungdam Won. 2023. Simulation and retargeting of complex multi-character interactions. In ACM SIGGRAPH 2023 Conference Proceedings. 111. Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, and Xue Bin Peng. 2025. ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators. arXiv preprint arXiv:2505.04961 (2025). Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty 9 Fig. 3. Learning curves of various methods, average over 3 runs with different random seeds. The total reward is evaluated only on the motion imitation reward. Fig. 4. Control action of the pitch joint on the left ankle for the backflip and table tennis footwork drill. , Vol. 1, No. 1, Article . Publication date: February 2026."
        }
    ],
    "affiliations": [
        "Robotics AI Institute, USA"
    ]
}