{
    "paper_title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "authors": [
        "Tianle Chen",
        "Chaitanya Chakka",
        "Arjun Reddy Akula",
        "Xavier Thomas",
        "Deepti Ghadiyaram"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 2 6 2 8 2 2 . 1 1 5 2 : r Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs Tianle Chen1* Chaitanya Chakka1* Arjun Reddy Akula2 Xavier Thomas1 Deepti Ghadiyaram1 1Boston University 2Google DeepMind tianle@bu.edu chvskch@bu.edu arjunakula@google.com xthomas@bu.edu dghadiya@bu.edu *Equal contribution Figure 1. We propose MMA-Bench to expose how MLLMs behave when sight, sound, and language conflict. Each example presents controlled modality (e.g., audio, video, or text) conflict and asks two modality-specific questions - one about the video and one about the audio. Correct answers differ across modalities, forcing the model to attend to the reliable modality. These structured contradictions reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks."
        },
        {
            "title": "Abstract",
            "content": "Despite remarkable advancements in Multimodal Large Language Models (MLLMs), fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe models reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide critical analysis of the brittleness of both openand closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audiovisual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both 1 interpretability tools and clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be available at cskyl.github.io/MMABench/ 1. Introduction Consider this scenario: video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? Now, envision third case: you experience the video with both sight and sound, but there is distracting and incoherent text descriptions appearing alongside it. In all these situations, most humans, with some effort, can easily describe the events in the video, ignore the distractors, and rely on the available modality. But what about present-day Multimodal Large Language Models (MLLMs) [3, 7, 10, 23, 30]? We investigate this question in our work. Despite rapid advancements in the design and deployment of large-scale MLLMs, most models [5, 10, 11, 21, 24] are trained on datasets that overwhelmingly assume that all available modalities are aligned. This may inadvertently make these models struggle when one modality is missing, noisy, or conflicts with another. This in turn makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Such attacks may coerce an MLLM to generate harmful content, leak sensitive information, or perform unauthorized actions. Given the rapid deployment of MLLMs in robotics [34, 37] and medical diagnostics [13, 20], such brittleness may lead to unacceptable risks and unsafe outcomes. In our work, we approach this challenging problem from the ground up. First, we aim to understand how current day MLLMs reason when multiple modalities are missing or semantically misaligned. To this end, we propose MultiModal Alignment (MMA)-Bench, to systematically control the presence or alignment of one modality (video, audio, text) at time. Next, we employ suite of black-box and white-box techniques to understand MLLMs behavior under these control settings. Our analysis revealed critical flaw: despite being trained as multimodal systems, all MLLMs over-rely on text and collapse when modalities conflict. As the famous George Orwells quote goes all animals are equal but some animals are more equal than others we indeed find that though present day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Based on these findings, we propose simple modalityaware tuning method to instill modality grounding. Our fine-tuned models show stronger cross-modal grounding, interpretable attention redistribution, and measurable performance gains on both in-domain and out-of-domain benchmarks. We make the following contributions: We introduce MMA-Bench, comprehensive benchmark to probe resilience to modality perturbations of MLLMs. We provide black-box and white-box diagnostic framework that reveals why current MLLMs fail under conflicting multimodal inputs. We propose modality-aware fine-tuning that substantially improves cross-modal grounding. 2. Related Work AudioVisual Evaluation Benchmarks. Prior benchmarks have explored robustness and reasoning in audiovisual large language models (LLMs), yet most evaluate whether the two modalities cooperate correctly rather than whether they can be selectively used when cues conflict. AVTrustBench [5] and AVHBench [24] focus on global visual-audio consistency and hallucination detection, while fine-grained datasets such as VGGSounder [40], DAVE [21], and AURA [22] assess cross-modal robustness but still assume semantic alignment between sight and sound. Other suites (e.g., DAVE, AURA, OmniVideoBench [17]) provide compositional or logical reasoning tests but seldom require distinct answers for visual In contrast, MMA-Bench isolates and auditory content. modality sensitivity by pairing each video with visualand an audio-focused question whose correct answers may diverge under semantic conflict. This design enables disentangled diagnosis of visual and auditory reasoning especially when modalities pose conflicting information while preserving natural multimodal context. Mitigating Modality Bias and Misalignment. Efforts to reduce modality bias fall broadly into training-free and training-based approaches. Training-free steering methods (e.g., AutoSteer [28], MC2 [36]) adjust attention or residual activations at inference to balance modality preference, whereas training-based strategies impose auxiliary objectives such as temporal ordering or cross-modal consistency (Arrow-of-Time [33]). Complementary decoding schemes like AVCD [16] and Fork-Merge [15] mitigate hallucination without retraining, and distillation frameworks such as Bridging Ears and Eyes [14] align encoder representations across sensory domains. Unlike these approaches, our modality-aware supervised fine-tuning directly teaches models to reason selectively using both semantically aligned and deliberately misaligned videoaudio pairs. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. 3. Multi-Modal Alignment (MMA) Bench: Debugging Alignment Gaps in MLLMs This work focuses on critical question: how do current day MLLMs behave when multiple modalities are seman2 Figure 2. Automated data curation pipeline for building MMA-Bench. Our twostage pipeline converts raw AudioSet [9] into clean, semantically aligned audiovideo samples. Stage 1 simplifies the ontology by pruning action-less, ambiguous (e.g., audio event hiss could be associated with stream or cat ), and restricted classes (e.g.,heart murmur). Stage 2 retains videos based on the simplified audio events. Here, only clips where the audible event is clearly produced by visible object are retained yielding high-quality subset which is further post-processed (Sec. 3). YouTube videos (typically 10s long) annotated with 527 audio event classes. However, as noted in [8, 29, 32, 40], AudioSet labels are ambiguous and noisy. Critically, it suffers from the following issues: (a) missing objects: many audio events (e.g., alarm) lack visible object, (b) ontology issue: the ontology of the audio events is excessively finegrained (e.g., crying is further split into whimper, baby cry, infant cry), and (c) label noise: composite scenes contain imprecise labels (e.g., video is labeled violin when other instruments are being played). (d) multi-label ambiguity: each clip is annotated with multiple hierarchical labels, many of which do not correspond to single clear audiovisual event. We address these issues by designing rigorous, multistage filtering pipeline combining ontology pruning [25] , automated cross-modal verification, and human inspection. Specifically, we first remove abstract or overly fine-grained audio class nodes (and their associated videos, since many such fine-grained distinctions cannot be visually grounded, and merging them into parent labels would introduce incorrect audiovisual alignment ) from the eval split of the AudioSet ontology containing 20, 371 samples in 2 stage pipeline as summarized in Fig. 2. Next, we filter videos with multiple audio events and with missing sounding objects by passing through Qwen2.5-Omni with rigid 4 question checks as shown in Fig. 3. After these refinements, we go through the filtered samples manually to ensure high data quality of aligned samples, resulting in 658 single-label aligned videos addressing the multi-label ambiguity in AudioSet To generate the misaligned samples, we start with an aligned sample (e.g., church bell video paired with the church bells ringing audio) and we randomly swap the audio track with different audio event (e.g., dog barking). This procedure creates one misaligned counterpart for every aligned sample, yielding an equal number of aligned and Figure 3. Verification of audio-visual semantic alignment. After automated pruning  (Fig. 2)  , each clip undergoes 4 simple yes/no consistency checks. sample is kept only if it passes all checks. tically misaligned? Existing multimodal benchmarks emphasize tasks such as holistic visual-audio judgment [5] and cross-modal hallucination detection [24], which primarily evaluate whether models can detect or maintain overall visual-audio consistency. These datasets only provide binary alignment labels (e.g., aligned or misaligned) rather than modality-specific semantic annotation for video and audio separately. By contrast, we wish to probe modality-specific reasoning of MLLMs by asking visualand audio-focused questions separately (illustrated in Fig. 1) To this end, we propose Multi-Modal Alignment (MMA) Bench, the first dedicated benchmark to assess modality sensitivity of MLLMs. We outline the following desiderata while designing MMA-Bench: Selection Criteria for Aligned Samples Sounding object must be clearly visible in the video. Audio track must correspond to the visible object in the scene. Video must not contain distracting or composite audio sources. Data curation: We construct MMA-Bench starting from the evaluation split of AudioSet [9], which contains 20, 3 Table 1. Dataset statistics across all creation stages. The table shows the number of training and evaluation samples retained after each curation step, concluding with the final dataset including misaligned counterparts. Stage Original AudioSet split After ontology pruning & downloading After automated filtering After human verification Final dataset (with misaligned samples) Train Eval 22,160 3892 1,207 1,207 13,277 20,371 3518 1,102 658 1,316 Misalignment Type Example Video Audio Text (Baseline) Video Semantic Audio Text Video Audio Irrelevant caption Text Video Audio Long context Text Video Frames Zeroed Audio Video Audio removed Audio Church bell video paired with church bell ringing audio. Church bell video paired with dog barking audio. Caption describes dog while video shows church bell. Long irrelevant paragraph appended before the question. Eyes closed: all visual frames replaced with black frames. Ears shut: original audio track replaced with silence. Table 2. Controlled modality misalignment and ablation scenarios studied in Sec. 4. We selectively perturb or remove one modality while keeping the others unchanged to isolate specific sources of multimodal brittleness in MLLMs. misaligned examples. The final benchmark thus contains 1, 316 videos in total half aligned and half misaligned spanning 49 representative sound categories. summary of all sizes at each curation step is provided in Table 1. This single-labeled, semantically aligned set forms the foundation for all subsequent fine-tuning and interpretability experiments in our work. (Details in Appendix A.) Tasks: Every video, whether the audio and visual streams are aligned or misaligned, in our benchmark is associated with two question-answer pairs. Visual-object classification: Which class best describes the visual content of this video? Audio-event classification: Which class best describes the sound in this video? These paired tasks probe each modality independently: the same video is queried twice, once for visual reasoning and once for auditory reasoning, allowing us to directly assess whether models can attend to the correct modality rather than relying on cues. Using MMA-Bench, we next analyze how current MLLMs behave under these controlled settings. 4. Are modern MLLMs Truly Multi-modal? Our key goal is to systematically understand how MLLMs intrinsically integrate visual, text, and audio representa- (a) Qwen2.5-Omni-7B. (b) VideoLLaMA2. (c) PandaGPT. (d) Gemini-2.0-Flash-Lite. Figure 4. Unimodal probing under visual and auditory ablation. Each subplot reports classification accuracy under visualfocused (left bars) and audio-focused (right bars) prompts. Audio removed replaces the sound track with silence, while Frames zeroed replaces all video frames with black images. Visual-focused prompt: \"Which class best describes the visual content of this video? {Accordion, Bird, Cat,...}. using single word or phrase.\" Options: Answer Audio-focused prompt: \"Which class best describes the audio content of this video? {Accordion, Bird, Cat,...}. using single word or phrase.\" Options: Answer Figure 5. Visualand audio-focused prompts used for understanding modality sensitivity of MLLMs . Each prompt encourages the model to focus on one modality and allows controlled comparison of visual and auditory reasoning within MLLMs. tions. To this end, we employ a) black-box probing, where we provide different kinds of inputs and study model performance, and b) white-box probing, where we closely inspect cross-attention interactions of several intermediate attention layers and heads to understand the interplay of multiple modalities. To systematically understand how modalities interact, we study various controlled misalignment input scenarios summarized in Table 2 and discuss next. 4.1. Black-box Interpretability and Findings 4.1.1. Experiment setup We use the MMA-Bench constructed in Sec 3 for the blackbox analysis, unless specified otherwise. Unimodal data: To examine the contribution of each modality for overall reasoning, we construct two unimodal variants of each video: (i) eyes closed, where we fill all frames with zeros, resulting in all black frames. while retaining the original audio, and (ii) ears shut, where we strip 4 (a) Qwen2.5-Omni-7B. (b) VideoLLaMA2. (a) Qwen2.5-Omni-7B. (b) VideoLLaMA2. (c) PandaGPT. (d) Gemini-2.0-Flash-Lite. (c) PandaGPT. (d) Gemini-2.0-Flash-Lite. Figure 6. Performance of semantic misaligned (Misalign) and aligned (Baseline) videoaudio pairs under visual-only (left bar group) and audio-only (right bar group) prompts. the audio while retaining the original visual frames of the video. Models studied: We study 3 open-sourced models: VideoLLaMA2 [3], PandaGPT [23], and Qwen2.5-Omni7B [30] and one closed-sourced model: Gemini-Flash-2.0Lite [7]. Results are averaged across 3 random seeds. Prompts: We use the same prompts detailed in MMABench in Sec 3 and append different class labels as options to create classification task, as shown in Fig 5. Each prompt targets single modality while the model still receives both modalities as input unless explicitly ablated in the following analyses. 4.1.2. Performance when one modality is removed What do models see with ears shut? To assess visual reasoning without auditory input, we replace each clips audio with silence and evaluate models using visual-focused 5. This corresponds to the middle bars prompts in Fig. (Audio Removed) in the visual-prompt columns of Fig. 4. All models retain comparable performance but show consistent slight drop, indicating that audio offers modest complementary cues. Qwen2.5-Omni-7B and VideoLLaMA2 exhibit larger declines (45%), suggesting stronger crossmodal use of sound, whereas Gemini-2.0-Flash-Lite and PandaGPT degrade minimally (1%), relying more on visual content. Evaluating with audio-focused prompts under the same silent condition is an inherently ill-posed setting. All models still achieve non-trivial accuracy, for instance, Qwen2.5Omni-7B reaches 46.6% and Gemini-2.0-Flash-Lite attains 59.2% despite receiving no audio at allby inferring their answers purely from visual cues. This indicates that when the requested modality is absent, models do not reliably folFigure 7. Model sensitivity to misleading textual context. Each subfigure reports accuracy under visual-only (left bar group) and audio-only (right bar group) prompts, comparing clean (Baseline) versus misleading-text (Text Misalign) conditions. low the desired modality instructions and instead fall back on whichever modality remains informativetypically the visual stream, (as further analyzed in Sec. 4.1.3) , which dominates their learned representations. What do models hear with eyes closed? We next test auditory reasoning by zeroing out visual frames while keeping the original audio intact (Fig. 4, see the rightmost bars in each subplot labeled Frames Zeroed). Using audiofocused prompts in Fig. 5, most models exhibit significant accuracy drop under this condition, indicating strong dependence on visual context for auditory understanding. VideoLLaMA2 [3], however, shows slight improvement, likely benefiting from its audio-only pretraining that promotes more independent audio processing. When the task is reversedusing visual-focused prompts with no visual inputthe setting becomes ill-posed. All models except PandaGPT still return high accuracy by inferring the visual answer from audio cues, suggesting that they rely on residual cross-modal correlations rather than abstaining when the requested evidence is absent. PandaGPT does not exhibit this behavior and instead fails to generate coherent outputs under such out-of-distribution conditions. 4.1.3. Video Semantic Audio Text What do models hear and see when input is semantically misaligned? To examine how models handle conflicting modalities, we use the semantically misaligned subset of MMA-Bench described in Sec. 4.1.1 where the video and audio carry inconsistent semantics. The aligned setting serves as the baseline. As shown in Fig. 6, all models degrade under misalignment, but the extent and pat5 misleading textual context in the prompt. Qwen2.5-Omni7B and PandaGPT show the most severe drops, larger than those observed under audiovisual misalignment. This indicates that both models depend heavily on textual context. VideoLLaMA2 exhibits more moderate decline, suggesting partial resistance to contradictory language but still limited ability to ground its predictions to the desired modality in the presence of misleading text. Irrelevant Long-Context Caption: To test long-context robustness under irrelevant long-context, we append 10, 000 random text tokens after each visualand audio-focused prompt while keeping the original videoaudio inputs unchanged. As shown in Fig 8, all models experience degradation across both prompts: Qwen2.5-Omni-7B and VideoLLaMA2 drop by roughly 1520%, and Gemini-2.0Flash-Lite shows similar decline despite its larger scale. PandaGPT is excluded due to its limited context window (400 tokens). These results confirm that current MLLMs struggle to retain relevant multimodal grounding once overwhelmed by long, irrelevant text. Takeaways from Black-box and White-box Interpretability 1. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. 2. Text triumphs: Small text distractions cripple models, ignoring clear audio-visual cues. 3. Conflict reflects in attention shifts: Prompt-driven attention separation increases under misalignment 4.2. White-box Interpretability and Findings In this section, we study how multiple modalities interact for reasoning tasks using white-box interpretation techniques. As in Sec. 4.1, our baseline is when all modalities are aligned and we compare them against misaligned scenarios. We use Qwen2.5-Omni-7B and VideoLLaMA2, visualand audio-only prompts  (Fig. 5)  and 100 randomly sampled videos from MMA-Bench detailed in Sec. 4.1.1. Specifically, we study how attention patterns evolve as multimodal inputs traverse through the MLLM decoder using statistical metric: Cohens-D (d) [6]. Given two distributions D1 and D2 capturing n1 and n2 samples, their sample means µ1, µ2, and standard deviations s1 and s2, the Cohens-D of the two distributions is defined as: d(D1, D2) = µ1 µ2 sp , where sp is the weighted average between s1 and s2: (cid:115) sp = (n1 1)s2 1 + (n2 1)s2 2 n1 + n2 2 (1) (2) (a) Qwen2.5-Omni-7B. (b) VideoLLaMA2. (c) PandaGPT. (d) Gemini-2.0-Flash-Lite. Figure 8. Performance degradation under long-context interference across MLLMs. Each subfigure shows accuracy under visual-only (left axis) and audio-only (right axis) prompts when irrelevant long-context text is appended. tern differ sharply across architectures. Qwen2.5-Omni-7B and VideoLLaMA2 suffer notable accuracy drops on both visualand audio-focused prompts. Roughly 2025% and 1520% declines, respectively, yet still maintain reasonable performance This pattern indicates that both models integrate information from multiple modalities but fail to selectively suppress misleading cues when semantics conflict. By contrast, Gemini-2.0-Flash-Lite and PandaGPT display distinct shortcut-taking behavior. Their visual accuracy remains nearly unchanged, sometimes even slightly improved, while their audio accuracy collapses to near-random levels (14%). These models rely almost exclusively on vision, i.e., seeing without listening resulting in strong degradation of audio performance when modalities conflict. 4.1.4. Video Audio Text We evaluate models on semantically aligned videoaudio pairs but prepend each input text prompt in two ways: (a) deliberately irrelevant short text and (b) irrelevant long context. We wish to study the role of distracting text cues while performing visual or audio classification tasks. Misleading short text: To evaluate the impact of misleading textual context, we prepend each question with false caption that contradicts the videos dominant content (e.g., inserting Video caption: Vehicle. for nonvehicle scene). (Details prompt in Appendix E). We then query using both visualand audio-focused prompts to test whether inconsistent text biases their reasoning about the corresponding modality. As shown in Fig. 7, all models suffer substantial performance degradation when misleading captions are introduced, demonstrating that models are highly susceptible to 6 positive d(D1, D2) indicates D1 has larger mean than D2. Larger absolute values of reflect greater separation between the two distributions [6]. In our analysis, D1 and D2 correspond to attention distributions under visualand audio-focused prompts, respectively. Finding 1: Textual attention dominance. When examining every attention head across all transformer layers , textual tokens consistently exhibit the highest attention magnitudes. On average, 81.29% of the total attention weights aggregated over all the layers in Qwen2.5-Omni7B is concentrated within text tokens and around 56.29% in VideoLLaMA2. This echos our observation in Sec. 4.1.4 where all models show very sharp performance decline under textual misalignment. Finding 2: Modality Selectivity Under Misalignment Next, to focus exclusively on visualaudio interactions, we ignore the attention weights of text tokens and re-normalize the remaining attention over visual and audio tokens. This enables us to study the relative importance of the two modalities. Then, we define D1 as the attention weight distribution of visual tokens under visual-only prompt while D2 is the attention weight distribution of visual tokens under audio-only prompt  (Fig. 5)  . As shown in Fig. 9 (a), Qwen2.5-Omni-7B exhibits consistently positive CohensD values for video tokens, indicating higher separation between the attention values under the two prompt settings. Further, in the event that the video and audio are misaligned (red line), Cohens-D values are much higher. This indicates that the model shifts attention more strongly between modalities when the semantic relation between them is disrupted. This strongly corroborates our analysis in Sec. 4.1.3. Conversely, when we repeat this analysis using audio tokens instead, as shown in Fig. 9 (b), we see negative Cohens-D values. This suggests that D2, the attention weight distribution of audio tokens under audio-only prompt, has higher mean value compared to D1 during both aligned (blue line) and misaligned (red line) cases. This is intuitive as it reflects model allocating more attention to auditory inputs under audio-focused prompts (D2). These trends confirm that Qwen2.5-Omni-7B tries to attend to the modality emphasized in the prompt. However, these attention shifts remain relatively weak. We hypothesize that stronger, more decisive reweighting is required for robust modality-specific reasoning. Indeed, we show that after finetuning , Cohens-D values increase meaningfully ( Sec. 5.2.1). Finding 3: Modality early layers vs later layers: From Fig. 9, Cohens-D magnitudes increase across layers, peaking around layer 17 for video tokens and 22 for audio in misaligned case. This means that the deeper layers change their attention more strongly when the prompt focus switches from video to audio. Rather than showing D1 = Visual-focused prompt D2 = Audio-focused prompt (a) Visual tokens (b) Audio tokens Figure 9. Layer-wise Cohens-D trends for aligned vs. misaligned samples. Misaligned samples (red dotted lines) exhibit consistently higher Cohens-D magnitudes for both visual (left) and audio tokens (right), indicating stronger modality-selective attention shifts under conflict. separation between modalities, this likely reflects that the higher layers are deciding which modality to trust - sign of semantic integration rather than raw fusion. 5. Experiments We hypothesize that models like Qwen2.5-Omni showing larger attention shifts, reflected by higher Cohens-D, adapt better when finetuned. We proceed to validate this next. 5.1. Implementation Details Data pre-processing: We start off from the training split of the AudioSet dataset and perform the 2 stage automated filtering detailed in Sec 3. We then rescale each training video to preserve the aspect ratio [1, 3, 26], followed by center crop to size 504 504. After filtering, we retain 1,207 audiovisual aligned samples from the training split of AudioSet spanning 58 sound classes. To simulate semantic misalignment, each aligned video is paired with 10 clips from distinct classes. We replace the audio tracks with that of the original aligned video, giving 13, 277 videos. Details about video sample pre-processing are in Appendix. Training objectives: We fine-tune Qwen2.5-Omni-7B using the modality-specific semantic classification tasks introduced in Sec. 3. The model is trained to answer the visualfocused and audio-focused questions correctly for each clip, based on the intended modality, even though both modalities are still provided as input. This encourages the model to ground its predictions in the appropriate modality while maintaining balanced cross-modal reasoning. Fine-tuning setup: We adopt lightweight parameterefficient fine-tuning strategy using LoRA adapters [12] applied to all linear projection layers in both the attention and feed-forward modules of Qwen2.5-Omni-7B. This design preserves pretraining knowledge while allowing efficient learning. All experiments are implemented using the public LLaMA-Factory pipeline [39], which provides stable support for multimodal fine-tuning and LoRA configuration management. Detailed configuration in Appendix. 7 Visual Prompt (%) Audio Prompt (%) Align Misalign Align Misalign Model Closed-Source Baselines Gemini-2.5-Pro Gemini-2.0-Flash Gemini-2.0-Flash-Lite Open-Source Baselines Qwen3-Omni-30B-Instruct Qwen2.5-Omni-7B (Base) VideoLLaMA2 ChatBridge PandaGPT 97.90 96.71 94.89 92.88 76.68 56.35 51.64 28.75 Qwen2.5-Omni-7B + Ours 94.68 95.28 91.91 94.11 83.73 58.72 36.11 54.71 29.79 94.37 60.37 57.21 59.19 57.39 46.60 36.12 41.61 13. 88.14 24.95 9.42 4.04 14.58 25.16 18.46 7.07 1.18 79.79 Table 3. Benchmarking against State-of-the-Art. Comparison of our fine-tuned model against wide range of baselines. Bold indicates the best performance, and underline indicates the second best. Our method (bottom row) achieves the highest audio robustness by significant margin, outperforming even 30B-parameter and proprietary models in handling conflicting modalities. 5.2. Quantitative Results We evaluate the fine-tuned Qwen2.5-Omni-7B on the MMA-Bench benchmark introduced in Sec. 3. The bestperforming checkpoint from the fine-turning step detailed in Sec. 5.1 is selected based on average validation accuracy over 1K-sample set (details in Appendix). Semantic misalignment. summarized in Table 3, modality-aware fine-tuning substantially improves Qwen2.5-Omni-7Bs robustness across both aligned and misaligned settings. The tuned model achieves 2040% absolute gains on visualand audio-focused prompts, maintaining balanced performance even when modalities conflict. This demonstrates that explicit alignment supervision effectively mitigates the models visual dominance and strengthens cross-modal integration. As To contextualize these gains, we compare our fine-tuned Qwen2.5-Omni-7B against broader set of multimodal architecturesincluding VideoLLaMA2, PandaGPT, Gemini-2.0-Flash-Lite, Gemini-2.0-Flash, Gemini-2.5-Pro, Qwen3-Omni-30B, and ChatBridge under identical evaluation protocols. Despite its smaller scale (7B vs. 30B), our model achieves the highest overall robustness, particularly under audiovisual conflict: while large models like Gemini-2.5-Pro and Qwen3-Omni-30B collapse to 24.95% and 14.58%, the fine-tuned Qwen2.5-Omni-7B reaches 79.79%. This >50% margin shows that targeted modality alignment is more effective than scaling alone. Moreover, this improvement does not compromise visual reasoning. Qwen2.5-Omni-7B maintains 94.68% accuracy on visual-aligned benchmarks, comparable to Gemini-2.5Pro (97.90%) and exceeding Qwen3-Omni-30B (92.88%). These results confirm that our fine-tuning strategy disentangles modality pathways, allowing selective attention to the correct sensory input without degrading overall multimodal Table 4. Unimodal ablation results for Qwen2.5-Omni-7B. Accuracy (%) when one modality is removed, before and after modality-aware fine-tuning. Condition Visual Prompt Audio Prompt Aligned Audio Removed Frames Zeroed Audio Removed + Ours Frames Zeroed + Ours 76.68 71.49 45.28 95.30 8.51 46.60 54.39 33. 16.17 82.52 Table 5. Effect of misleading textual context on Qwen2.5Omni-7B. Accuracy (%) before and after modality-aware finetuning under two settings: (a) misleading captions prepended to the query (text misalignment) and (b) 10K random tokens appended after the query (long context). Condition Visual Prompt Audio Prompt (a) Text Misalignment Qwen2.5-Omni-7B + Ours (Fine-tuned) 37.81 91.88 (+54.07) 11.98 28.63 (+16.65) (b) Long Context (10K Tokens) Qwen2.5-Omni-7B + Ours (Fine-tuned) 63.65 78.02 (+14.37) 34.75 28.36 (-6.39) capability. 2. Unimodal Ablation Table 4 reports results when one modality is removed. Fine-tuning improves unimodal reasoningvisual accuracy increases by over 20% with silent audio, and audio accuracy by nearly 50% when video frames are zeroed. Conversely, performance declines in scenarios where visual prompts are used without visual input, suggesting less hallucinating and more reliable model. Table 6. Zero-shot evaluation on AVHBench. Performance of Qwen2.5-Omni before and after modality-aware fine-tuning. The first three tasks are binary classification (accuracy in %), and the last is an open-ended captioning task evaluated using METEOR. Task Qwen2.5-Omni +Ours Video-driven Audio Hallucination Audio-driven Video Hallucination AV Matching AV Captioning (METEOR) 71.67 80.65 49.57 15.50 79.86 85.32 49.57 15. 5.3. Zero-shot Evaluation on AVHBench We evaluate the zero-shot generalization ability of our modality-aware-tuned model on AVHBench [24], recent benchmark diagnosing audiovisual hallucination and cross-modal consistency in MLLMs. As summarized in Table 6, modality-aware fine-tuning yields clear gains on hallucination-oriented tasks, improving accuracy on both Video-driven Audio Hallucination (V2A) and Audio-driven Video Hallucination (A2V) by +8.2% and +4.7%, respectively. These results indicate that modality-aware-tuning helps in reducing hallucinations, very different task, on unseen data. Performance on AV Matching remains unchanged, while AV Captioning shows marginal improvement (+0.1 METEOR). This equally important result suggests that modality-aware-tuning improves grounding but does not negatively impact other, broader reasoning or generative tasks. (More details in Appendix.) 6. Limitations and Future Work This work provides first, crucial step towards systematic interpretation of MLLM robustness to semantic misalignments. Our analysis reveals significant performance instabilities and an over-reliance on text or visual modalities, highlighting key vulnerabilities in current models. While this evaluation is currently limited to Qwen2.5-Omni-7B and classification-style QA, our immediate goal is to extend the analysis to larger Qwen models and other architectures like PandaGPT [23] and VideoLLaMA2 [3]. Future work will further broaden this scope to include open-ended reasoning and more fine-grained temporal misalignment tasks. 7. Acknowledgements We thank our collaborators and colleagues for their valuable feedback and discussion throughout this project. We also respectfully acknowledge that Arjun Akula participated in an advisory capacity only. D1 = Visual-focused prompt D2 = Audio-focused prompt (a) Video tokens (Aligned) (b) Audio tokens (Aligned) (c) Video tokens (Misaligned) (d) Audio tokens (Misaligned) Figure 10. Layer-wise Cohens-D before and after modalityaware tuning. Higher absolute values indicate stronger separation between attention distributions under D1 (visual prompt) and D2 (audio prompt). After tuning, the model reallocates attention more decisively toward the modality emphasized by the prompt, reflecting improved modality-selective reasoning. 3. Misleading textual context. Table 5 shows the effects of irrelevant textual information under two settings: (a) prepended false captions and (b) appended long irrelevant text. Under misleading captions, modality-aware tuning yields large gains: over 50% for visual prompts and 16% for audio prompts. Importantly, the model was never exposed to text corruption during training. We stress that this emergent robustness arises from aligning the visual and audio modalities. For long-context inputs, tuning improves visual-prompt accuracy but causes mild drop for audio prompts, indicating that alignment supervision alone cannot fully resolve failures caused by extremely long irrelevant text. 5.2.1. Attentions After Modality-Aware Tuning To understand how modality-aware tuning improves the models multimodal understanding behavior, we analyze layer-wise attention shifts before (solid blue for aligned and red dotted for misaligned) and after training (solid green for aligned and dotted lilac for misaligned) using the CohensD metric introduced in Sec. 4.2. As shown in the Fig 10, after fine-tuning, the absolute magnitude of Cohens-D increases across layers, particularly in deeper ones. This suggests that the model now adjusts its attention more decisively toward the modality emphasized by the prompt. We believe that the performance improvements are not merely from memorization, but from more principled reallocation of attention toward the most relevant modality as noted in the prompt."
        },
        {
            "title": "References",
            "content": "[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in neural information processing systems, 34:2420624221, 2021. 7 [2] Yue Chen, Ziliang Zhao, and Andrew Zisserman. Vggsoundfusion: benchmark for audio-visual learning with hierarchical labels. In ICCV Workshops, 2023. 2 [3] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 2, 5, 7, 9, 4 [4] Sheng-Yi Chou and Hung-yi Lee. Ontology-aware audio tagging with hierarchical graph attention networks. In IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. 2 [5] Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, and Dinesh Manocha. Avtrustbench: Assessing and enhancing reliability and robustness in audio-visual llms. arXiv preprint arXiv:2501.02135, 2025. 2, 3, 8 [6] Jacob Cohen. Statistical power analysis for the behavioral sciences. routledge, 2013. 6, 7 [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 5 [8] Duygu Dogan, Huang Xie, Toni Heittola, and Tuomas Virtanen. Multi-label zero-shot audio classification with temporal In 2024 18th International Workshop on Acousattention. tic Signal Enhancement (IWAENC), pages 250254. IEEE, 2024. 3 [9] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanIn 2017 IEEE internalabeled dataset for audio events. tional conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. 3, 1 [10] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518015190, 2023. 2 [11] Mark Hamilton, Andrew Zisserman, John Hershey, and William Freeman. Separating the chirp from the chat: Self-supervised visual grounding of sound and language. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1311713127, 2024. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 7 [13] Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, and Yehui Yang. Towards multimodal large language model with pixel-level insight for biomedicine. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 37793787, 2025. 2 [14] Xilin Jiang, Junkai Wu, Vishal Choudhari, and Nima Mesgarani. Bridging ears and eyes: Analyzing audio and visual large language models to humans in visible sound recognition and reducing their sensory gap via cross-modal distillation. arXiv preprint arXiv:2505.06803, 2025. 2 [15] Chaeyoung Jung, Youngjoon Jang, Jongmin Choi, and Joon Son Chung. Fork-merge decoding: Enhancing multimodal understanding in audio-visual large language models. arXiv preprint arXiv:2505.20873, 2025. 2 [16] Chaeyoung Jung, Youngjoon Jang, and Joon Son Chung. Avcd: Mitigating hallucinations in audio-visual large language models through contrastive decoding. arXiv preprint arXiv:2505.20862, 2025. 2 [17] Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, et al. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms. arXiv preprint arXiv:2510.10689, 2025. [18] Jiahui Li, Bowen Zhou, and Xin Tang. Av-mix: Balanced audio-visual event dataset with ontology-guided class consolidation. In CVPR, 2024. 2 [19] Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, and Kaipeng Zhang. To think or not to think: study of thinking in rule-based visual reinforcement fine-tuning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 9 [20] Fenglin Liu, Tingting Zhu, Xian Wu, Bang Yang, Chenyu You, Chenyang Wang, Lei Lu, Zhangdaihong Liu, Yefeng Zheng, Xu Sun, et al. medical multimodal large language model for future pandemics. NPJ Digital Medicine, 6(1): 226, 2023. 2 [21] Gorjan Radevski, Teodora Popordanoska, Matthew Dave: Diagnostic arXiv preprint Blaschko, and Tinne Tuytelaars. benchmark for audio visual evaluation. arXiv:2503.09321, 2025. 2 [22] Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, and Ramani Duraiswami. Aura: finegrained benchmark and decomposed metric for audio-visual reasoning. arXiv e-prints, pages arXiv2508, 2025. 2 [23] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 2, 5, [24] Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. Avhbench: crossmodal hallucination benchmark for audio-visual large language models. arXiv preprint arXiv:2410.18325, 2024. 2, 3, 9 [25] Ludovic Tuncay, Etienne Labbe, and Thomas Pellegrini. Hierarchical label propagation: model-size-dependent perIn ICASSP 2025formance booster for audioset tagging. 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 3 10 [39] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 7 [40] Daniil Zverev, Thaddaus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, and Koepke. Vggsounder: Audio-visual evaluations for foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10271037, 2025. 2, 3 [26] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. [27] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. 8 [28] Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, and Shumin Deng. Automating steering for safe multimodal large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 792814, 2025. 2 [29] Huang Xie and Tuomas Virtanen. Zero-shot audio classification via semantic embeddings. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:12331242, 2021. 3 [30] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 2, 5, 3, 4 [31] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. [32] Xuenan Xu, Pingyue Zhang, Ming Yan, Ji Zhang, and Mengyue Wu. Enhancing zero-shot audio classification using sound attribute knowledge from large language models. arXiv preprint arXiv:2407.14355, 2024. 3 [33] Zihui Xue, Mi Luo, and Kristen Grauman. Seeing the arrow of time in large multimodal models. arXiv preprint arXiv:2506.03340, 2025. 2 [34] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025. 2 [35] Sean Yang, Bernease Herman, and Bill Howe. Ontologue: Declarative benchmark construction for ontological multilabel classification. Advances in Neural Information Processing Systems, 35:2246322476, 2022. 2 [36] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. Evaluating and steering modality preferences in multimodal large language model. arXiv preprint arXiv:2505.20977, 2025. 2 [37] Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, and Donglin Wang. Vlas: Vision-language-action model with speech instructions arXiv preprint for customized robot manipulation. arXiv:2502.13508, 2025. [38] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chatbridge: Bridging modalities with large language model as language catalyst. arXiv preprint arXiv:2305.16103, 2023."
        },
        {
            "title": "Appendix Table of Contents",
            "content": "We provide detailed information regarding dataset curation, implementation specifics, and extended experimental analysis in the following sections: Appendix details the AudioSet ontology simplification process and the rationale for removing ambiguous classes. Appendix describes the rigorous two-stage sample curation pipeline, including the automated consistency checks and the human verification protocol. Appendix outlines the video data preprocessing steps, specifically spatial resizing and temporal standardization. Appendix provides the configuration for modalityaware fine-tuning, including LoRA hyperparameters and data augmentation strategies. Appendix presents extended white-box interpretability results, focusing on head-wise attention statistics and comparisons with VideoLLaMA2. Appendix visualizes token-level attention heatmaps to quantitatively demonstrate the text dominance in current MLLM architectures. Appendix displays the specific prompt templates used for textual misalignment and distraction experiments. Appendix evaluates the cross-class generalization capabilities of the fine-tuned model on unseen categories. Appendix extends the black-box analysis to additional baselines (Gemini-Pro, Qwen3-Omni, ChatBridge) across semantic, unimodal, and textual misalignment settings. Appendix introduces the None of the Above zero-shot abstention evaluation to diagnose hallucination from silence. Appendix investigates the impact of Chain-of-Thought (CoT) prompting on resolving sensory conflict and alignment drift. Appendix presents gallery of qualitative examples comparing the improved grounding of our model against baseline failure modes. A. AudioSet Ontology Details and Dataset Simplification The audio event classes in AudioSet [9] are structured as hierarchical graph ontology, organizing sounds based on semantic granularity. This structure captures relationships ranging from broad concepts (roots) to fine-grained events (leaves). However, this hierarchy introduces significant label redundancy for our purposes. For example, single video sample is typically annotated with every label along the path from the root to the leaf: clip might carry the tags Animal (root), Domestic animals, pets (intermediate node), and Dog (leaf). While these labels all reference the same acoustic event, treating them as distinct, Figure 11. AudioSet Ontology tree of root Natural Sounds Nodes colored red are marked restricted by the original ontology. Nodes with gradient indicate that they have multiple parents from either same or different roots. equal-weighted classes creates multi-label ambiguity. The complete ontology forest consists of 6 distinct root categories. Furthermore, the graph structure allows nodes to have multiple parents, leading to cross-branch ambiguity. For instance, the label Hiss appears as child node under both Snake (within the Animals root) and Steam (within the Natural Sounds root). Additionally, the ontology also provides restrictions field which marks some of the classes as blacklisted due to being obscure and some as abstract since they are only added to build the tree structure. Consequently, the raw dataset averages 2.7 class labels per sample. This necessitated the rigorous ontology pruning and filtering pipeline described in Section 3 to extract the distinct, single-source events required for precise misalignment analysis. One of the roots of the ontology is shown in Figure 11 In the original AudioSet ontology (527 annotated classes in the publicly provided version) several issues become salient when the labels are to be used in multimodal audio-visual classification setting along with above ones: High cardinality of classes The large number of target labels leads to fragmentation of the training distribution and increases long-tail class frequency problems (many classes with very few samples), which limits the ability of deep models to learn robust representations. Excessive specificity of leaf-level labels Many classes reside at fine granularity levels of the ontology (e.g., highly specific sound events). With limited samples per specific class, models may suffer from overfitting, or may not capture the underlying general concept effectively; this reduces generalization in the audio-visual domain. Mismatch of context/complexity and model capacity when including very high number of classes in system (especially one with limited context length or capacity, e.g., audio-visual transformers with 16k32k token budgets), the representational burden increases and may impair training convergence or downstream inference efficiency. 1 Following this, we next perform actual sampling of the videos briefed in the next section. A.2. Class guided video filtering While ontology pruning reduces semantic redundancy at the class level, the raw AudioSet annotations often contain multi-label noise and hierarchical overlap between parent and child nodes. To ensure label consistency and preserve strong audio-visual grounding, we perform multi-stage sample filtering pipeline that operates directly on annotated samples Ancestor removal and child-only retention. Each sample in AudioSet is typically annotated with both high-level parent labels (e.g., Vehicle) and their corresponding finegrained children (e.g., Car horn). Since the ontology already encodes these hierarchical relations, we retain only the most specific leaf-level annotations per sample, effectively preventing redundant supervision and ensuring single representative label per sound event. This approach is consistent with hierarchical audio event cleaning procedures used in recent multimodal datasets such as AVMix [18] and VGGSound-Fusion [2]. Cross-tree label exclusion. Samples whose annotations span multiple disjoint ontology roots (e.g., both Vehicle and Animal categories) are removed to avoid semantic conflicts and multi-branch ambiguity. We retain multi-label samples only if all labels belong to the same ontology subtree, since they can reasonably represent merged or semantically proximate events within common context (e.g., Car and Truck would merge to Vehicle). This constraint enforces intra-tree label coherence, following hierarchical consistency principles similar to those in ontology-aware graph tagging frameworks [4]. Pruned-class and unknown-class removal. After ontology pruning, samples that refer any removed or restricted class are filtered out. We further rename the remaining labels to match the consolidated set of final classes and eliminate all residual multi-label samples, resulting in purely single-label dataset. This ensures each retained instance corresponds to distinct, visually and acoustically grounded class concept. After the above cleaning procedures, the resulting sample counts per split is mentioned in table 7. B. Sample Curation and Quality Verification"
        },
        {
            "title": "Protocol",
            "content": "Following the ontology-based pruning and class pruning, we perform rigorous instance-level filtering pipeline to ensure that every video in MMA-Bench possesses single, unambiguous semantic signal grounded in both modalities. This process consists of an automated consistency check followed by human verification. Figure 12. Distribution of 49 audio events in the MMA-Bench A.1. Stage 1: AudioSet Ontology Simplification To alleviate the above mentioned problems in the class set, we devise 4 step process to reduce and prune the ontology to simpler set: Ontology-based leaf absorption First, we traverse each root node in the ontology and restrict to maximum depth of three levels (depth parameter chosen based on the granularity and visual-action relevance of classes). All descendant nodes beneath that depth are absorbed into their ancestor, thereby reducing the number of labels and increasing per-class sample counts. This operation also aligns with ontology-aware classification practices [35] (e.g., using parent-child relations to regularize label prediction) Removal of actionless sound classes Because our downstream task emphasizes audiovisual modeling (i.e., we expect both an audible event and visible object or moving action in the video), we remove audio event classes whose semantic reference lacks visually observable action-object dynamic. For example, the class Alarm has strong auditory presence but often lacks an associated visible moving object in the video (e.g., static doorbell box). By excluding such classes, we focus on sound events that reliably cooccur with visible objects/actions, thereby improving alignment of audio and visual modalities. Removal of ambiguous and restricted classes We further exclude classes that (a) have ambiguous semantics (e.g., broadly defined or vocabulary overlap), (b) appear under multiple parent paths in the ontology (leading to labelhierarchy conflict), or (c) correspond to restricted or obscure annotations (those that refer to abstract notions rather than concrete audible/visible objects). This step reduces semantic noise and ensures clearer label set that better supports multimodal alignment. After applying these filtering and consolidation operations, the original 527 classes are reduced to 61 classes. Each of the remaining classes was manually verified to ensure that (i) it has an audible event and (ii) visually identifiable object or action generating that event. On average, each retained class absorbed approximately 68 leaf nodes from the original ontology, substantially increasing per-class sample counts and improving data balance across categories. word cloud of all the classes is shown in Figure 12 B.1. Automated Consistency Checks As illustrated in the methodology figure (Main Paper, Fig. 3), raw videos often contain off-screen sounds, background noise, or occluded objects that match the class label metadata but fail to provide clear audio-visual grounding [29]. To filter these, we employ probing-based verification pipeline using Qwen2.5-Omni-7B [30] as judge. For candidate video associated with specific target class (e.g., Cat), we subject the sample to four distinct existence queries. The sample is retained only if the model answers Yes to all four conditions (Logical AND gate): 1. Visual Unimodal Check (Audio Removed): We remove the audio track and query: Is the [Class c] clearly visible in this video? This ensures the object is not occluded or off-screen. 2. Auditory Unimodal Check (Frames Zeroed): We replace the visual stream with black frames and query: Is the sound of [Class c] clearly audible in this video? This ensures the audio event is distinct and not merely inferred from visual context. 3. Cross-Modal Visual Check: We provide the full audiovisual input and ask the visual-focused question. This verifies that the presence of audio does not distract the model from identifying the visual object. 4. Cross-Modal Auditory Check: We provide the full audio-visual input and ask the audio-focused question. This verifies that the visual context does not suppress the recognition of the audio event. If the model fails any of these four checksfor instance, if the audio is present but the visual object is not clearly identifiablethe sample is automatically rejected. This strict consistency requirement filters out weak or ambiguous alignments. B.2. Human Verification Protocol To eliminate subtle misalignments that automated models might miss (e.g., faint overlapping speech or ambiguous background music), the samples passing the automated pipeline underwent manual inspection. Annotators: We have two graduate-level researchers familiar with audio-visual analysis to independently review each candidate video. Criteria: Annotators were instructed to verify three conditions: Object Visibility: The sounding object must be the primary focus of the frame (e.g., video labeled Car must show the car, not just road). Audio Purity: The audio track must be clean, with the target sound being the dominant event. Videos with loud background music or intelligible human speech (unless the class is speech-related) were discarded. Action Dynamics: The video must depict dynamic visual action corresponding to the audio event. Static videos, slideshows, or clips where the sounding object is Table 7. Audioset split-wise statistics before and after the automatic filtering pipeline(without manual inspection). Audioset Split # Original samples # filtered samples train unbalanced train balanced evalsplit 2041789 22160 20371 182970 3892 3518 (a) (b) Figure 14. Video level statistics of the training dataset (a) shows histogram of different resolutions present in the dataset. Notice the peaks around 320x240 and 640x480 (4:3), 1280x720, 1920x1080 (16:9). Figure (b) shows histogram of different temporal lengths(in seconds) of the videos. Notice that almost all of them are 10 seconds with < 5% samples less than 10 seconds. motionless were discarded to ensure temporal grounding. We enforced strict consensus protocol. video was included in the final MMA-Bench dataset only if both annotators marked it as Pass. Any sample receiving split vote (one Accept, one Reject) was discarded to maximize the precision of the benchmark. This rigorous process resulted in the final set of 658 high-fidelity aligned videos used for our experiments. C. Video data preprocessing How did we choose the frame size to resize videos to? Although the base Qwen2.5-Omni-7B [30] model supports variable-resolution and variable-rate video inputs, uniform spatiotemporal dimensions are required during training to enable efficient batching and stable convergence. We analyze the raw temporal and spatial distributions of our dataset as shown in Fig. 14, and standardize all samples accordingly. Each video is rescaled and cropped to square frame of 504 504 pixels. This target size corresponds closely to the statistical median of the datasets native resolutions, resulting in minimal upsampling (< 5%) of lower-resolution samples. Given each patch is of size 14 14, using 504504 yields an integer number of patches (1296). Since Qwen2.5-Omni-7B uses the same 14 14 patch size as its visual backbone and employs TMRoPE (Time-aligned Multimodal Rotary Position Embeddings), which seamlessly extends rotary embeddings across video frames, choosing resolution divisible by the patch size ensures clean and stable patch grid that aligns naturally with its positional encoding design. This resizing is applied only during training to address batch sampling limitations in Qwen2.5-Omni-7B. Ensure same temporal length: To ensure consistent temporal context across training samples, we enforce fixed clip length of 10 s. Videos shorter than this duration (approximately 4.8 % of the corpus) are discarded to avoid temporal padding artifacts and maintain temporal coherence in attention windows. For longer clips, we truncate frames to achieve the same duration, yielding consistent sequence lengths across the dataset. This uniformity simplifies batching, improves multimodal synchronization with the corresponding audio segment, and stabilizes optimization for transformer-based video encoders. D. Finetuning Details summary of the fine-tuning configuration is shown in Table 8. To maintain balanced distribution between aligned and misaligned data, all aligned samples are duplicated ten times to match the number of generated misaligned examples. This ensures that the model observes equal proportions of aligned and misaligned conditions during optimization, encouraging it to learn modality-consistent reasoning rather than frequency-biased correlations. E. White-box analysis: Additional Cohens-"
        },
        {
            "title": "Dtrends",
            "content": "E.1. Headwise Statistics We observe the trend of Cohens-D metric introduced in Section 4.2 under each head of the decoder, aggregated across all the layers. We report the trends of both Qwen2.5Omni-7B [30] and VideoLLaMA2 [3] under aligned video samples in Fig. 16. Although they do still follow modality selectivity (positive magnitudes for video tokens and negative for audio tokens), there lacks clear trend as we move across the heads compared to layer-wise statistics. This demonstrates that modality selectivity is strongly organized at layer-level granularity than head-wise. Table 8. Configuration of our modality-aware LoRA fine-tuning based on the LLaMA-Factory pipeline. Only LoRA adapter parameters are trained, while the Qwen2.5-Omni-7B backbone and vision tower remain frozen. Component Setting Model / Input Model Audiovideo input Max video / image res. Fine-tuning Method Type Target layers LoRA rank (r) Stage Frozen modules Training Setup Learning rate Batch size Warmup ratio Epochs Precision Qwen2.5-Omni-7B Enabled (use audio in video) 13K / 262K px LoRA (parameter-efficient) All attention proj. (WQ, WV ) SFT (modality-aware) Backbone, vision tower 1 104 (cosine) 4 (accum.=4) 0.1 5 FP (a) Head-wise Cohens-D for Video tokens (b) Head-wise Cohens-D for Audio tokens in Figure 16. Headwise-wise Cohens values remain noisy, revealing modality shifts are primarily organized in layers (a) Cohens-D of Video tokens (b) Cohens-D of Audio tokens Figure 18. Layer-wise Cohens-D trends for aligned vs. misaligned samples in VideoLLaMA2 Misaligned samples (red dotted lines) have very similar Cohens-D magnitudes for both visual (left) and audio tokens (right), following the under-performance observations with misaligned samples in black-box analysis (Section 4.1). E.2. VideoLLaMA2trends We report the Cohens-D trends for VideoLLaMA2 under both aligned (blue solid) and misaligned (red dotted) settings in Figure 18 . Unlike Qwen2.5-Omni, VideoL4 LaMA2 shows almost identical magnitudes across the two conditions. This indicates that the model does not substantially reallocate attention when the modalities are misaligned. This behavior is consistent with the black-box findings in Section 4.1, where VideoLLaMA2 performs worse than Qwen2.5-Omni. F. Attention heatmaps To complement our quantitative analyses, Figure 19 visualizes the per-token attention distribution across video, audio, and text inputs for each generated output step in the last layer of the model. striking pattern emerges: text tokens receive the overwhelming majority of attention, while video and audio tokens take very minimal mass across all decoding steps. This text-dominant behavior is consistent with our black-box findings in Sec. 4, where misleading or long-context text frequently overrides multimodal signals. Because text tokens absorb such disproportionate share of attention, raw attention values are not directly comparable across modalitiesthe multimodal tokens are effectively drowned out by the textual prefix. For this reason, in our white-box analysis we discard all text-token attention and compute normalized attention exclusively over audio and video tokens. This isolation ensures that our measurements (e.g., Cohens-D shifts under alignment vs. misalignment) reflect the relative allocation between the multimodal streams, rather than being dominated by the trivial effect of text-heavy attention patterns. Together, these heatmaps provide qualitative confirmathough modern tion for the core narrative of our study: MLLMs accept multimodal inputs, their internal attention routing is overwhelmingly text-centric. G. Misleading Text Prompt Figure 20 outlines an example for Visual-focused and Audio-focused prompt used for experiments involving textual misalignment. We include distraction class as ideo caption along with the actual question, testing the robustness of the model towards misleading text. H. Cross-Class Generalization and Compositionality To assess whether modality-aware tuning improves general multimodal reasoning rather than memorization of seen classes, we conduct split-class evaluation. The model is fine-tuned on subset of 29 out of 58 classes (the trained set) and evaluated on both the trained and untrained halves using the same semantic misalignment benchmark. This setup allows us to isolate the transfer of alignment reasoning to novel categories that share no class-level overlap with the training set. As shown in Table 9, the fine-tuned model maintains strong gains not only on the trained classes but Table 9. Cross-Class Generalization under Semantic Misalignment. Performance (%) of Qwen2.5-Omni-7B before and after modality-aware fine-tuning. Seen and Unseen denote classes seen or unseen during fine-tuning. Setting Visual Prompt Audio Prompt Qwen2.5 +Ours Qwen2.5 +Ours"
        },
        {
            "title": "Aligned\nMisaligned",
            "content": "76.3 58.1 72.3 60.9 96.3 95.4 95.3 92.0 41.4 27.4 61.8 24. 43.2 53.1 76.4 55.1 also on previously unseen ones. This indicates that the tuning process improves generalizable capability to detect and reconcile cross-modal inconsistencies rather than overfitting to specific category semantics. I. Black-Box Experiment with Other Baseline"
        },
        {
            "title": "Models",
            "content": "I.1. Semantic Misalignment Experiments We extend our evaluation to broader set of architectures, including Gemini-2.5-Pro [7], Gemini-2.0-Flash [7], Qwen3-Omni-30B [31], and ChatBridge [38]. As shown in Fig. 21, we observe consistent asymmetry in how conflicting modalities affect inference. When targeted with visual-focused prompts, models demonstrate relative resilience to contradictory audio. While performance dips slightly compared to the aligned baseline (e.g., Qwen3-Omni drops approximately 9%), the models largely succeed in isolating the visual signal. This suggests that the visual encoders in these MLLMs provide dominant and stable representation that is difficult to override with auxiliary sensory inputs. Conversely, performance on audio-focused prompts decreases under misalignment. The accuracy of all tested models drops significantly when the visual stream contradicts the audio content (e.g., Gemini-2.0-Flash and Qwen3Omni drops below 15%). However, we suspect this is due to the inherent frailty of audio representations, rather than preference for visual information. Even in the aligned setting, audio tasks have significantly lower baseline scores than visual tasks, indicating that audio is weaker signal for all current MLLMs. As result, when faced with cross-modal conflict, the stronger visual representation effectively suppresses the weaker auditory signal, causing the observed degradation. 5 Figure 19. Attention heatmap of few sampled input tokens against output tokens of Qwen2.5-Omni at layer 28. Notice that majority of attention mass is within textual tokens, indicating the textual prior of current MLLMs and their strong influence in model performance. Visual-focused capprompt with misleading Video caption: Vehicle. Which tion: class best describes the visual content of this video? Options: {Classes List}. Audio-focused prompt with misleading caption: Video caption: Vehicle. Which class best describes the audio content of this video? Options: {Classes List}. Figure 20. An example prompt used for text-misalignment evaluation. Each question is preceded by deliberately incorrect caption that contradicts the true visual or auditory content, testing the models resistance to misleading textual priors. I.2. Unimodal Experiments To isolate the contribution of each modality, we performed unimodal ablation study on the extended baseline models by selectively zeroing out visual or auditory inputs in Fig. 22. This analysis reveals distinct hierarchy in how these MLLMs process sensory information. When evaluating with visual-focused prompts, removing the audio track (Audio Removed) results in negligible performance drops across all models (e.g., Gemini-2.5-Pro: 97.90% 97.23%). This confirms that visual reasoning in MLLMs does not rely on auditory cues for disambiguation. Conversely, when evaluating audio-focused prompts, removing the visual stream (Frames Zeroed) causes performance decline compared to the baseline (e.g., Qwen3Omni: 57.39% 44.68%). This suggests that what ap- (a) Gemini-2.5-Pro (b) Gemini-2.0-Flash (c) Qwen3-Omni-30B (d) ChatBridge Figure 21. Extended analysis of semantic misalignment. Performance comparison between aligned inputs (Base) and conflicting video-audio inputs (Misalign) across additional baseline models. While visual reasoning (left bars) remains relatively robust, auditory reasoning (right bars) suffers significant degradation under conflict, confirming that the visual dominance bias generalizes across model architectures and scales. pears to be auditory reasoning in the baseline setting is partially supported by visual context, without which the audio encoder struggles to classify events accurately. Another revealing trend appears when models are asked to classify audio but are provided with only visual inputs (Audio Removed under Audio Prompt). Notably, accuracy improved significantly compared to the multimodal 6 (a) Gemini-2.5-Pro (b) Gemini-2.0-Flash (a) Gemini-2.5-Pro (b) Gemini-2.0-Flash (c) Qwen3-Omni-30B (d) ChatBridge Figure 22. Unimodal probing on extended baselines. Classification accuracy under visual-focused and audio-focused prompts when inputs are ablated. baseline. For instance, Gemini-2.5-Pro rises from 60.37% to 95.55%, and Gemini-2.0-Flash increases from 57.21% to 91.79%. In this forced-choice setting, where models are constrained to select valid semantic class, this behavior reflects consistent visual-to-audio inference mechanism: the models effectively utilize visual context (e.g., seeing dog) to deduce the likely associated sound (e.g., bark). While this associative reasoning is advantageous when one modality is missing, the fact that pure visual inference outperforms the full multimodal baseline suggests that visual priors are the dominant driver of semantic decision-making in these architectures. We further investigate the implications of this behavior in Sec. J, where we introduce an abstention option (None of the above) to distinguish between helpful inference and ungrounded hallucination. I.3. Misalignment via text Using the misleading captions in Figure 20, we evaluate the baseline models under aligned and text-misaligned conditions  (Fig. 23)  . The results show clear asymmetry: visualprompt accuracy is comparatively robust for stronger models, whereas audio-prompt accuracy collapses across all architectures. Under Visual prompt setting, Gemini-2.5-Pro and Qwen3-Omni-30B remain stable under text misalignment, dropping only 1.9% and 0.6% respectively. In contrast, Gemini-2.0-Flash and ChatBridge show pronounced degradation, falling 16.9% and 29.9%. These sharp reductions indicate higher susceptibility to linguistic interference, implying weaker decoupling between the visual stream and the textual conditioning. Textual misalignment severely harms all models under Audio-focused prompt with drops as high as 22.7%. Even in the aligned setting, audio accuracies begin far below their (c) Qwen3-Omni-30B (d) ChatBridge Figure 23. Impact of misleading textual context. Accuracy comparison when contradictory caption is prepended to the query. (a) Gemini-2.5-Pro (b) Gemini-2.0-Flash (c) Qwen3-Omni-30B (d) ChatBridge Figure 24. Performance under long-context interference. visual counterparts, pointing to inherently weaker or noisier audio representations. When the caption contradicts the sound, the linguistic modality - typically the strongest and most trusted input channel for modern MLLMs - overrides the fragile auditory signal, resulting in severe degradation. Overall, text misalignment disproportionately disrupts audio-based reasoning, while visual-based reasoning remains fairly robust only for the strongest models. I.4. Irrelevant long context caption To examine the robustness of different model families to long-range distractors, we prepend large amounts of irrelevant text (garbage context) far before the multimodal 7 query and evaluate performance under aligned versus longcontext conditions  (Fig. 24)  . The results reveal clear divide between closed-source and open-source architectures in their ability to resist interference from distant textual noise. Gemini-2.5-Pro and Gemini-2.0-Flash show relatively mild degradation when exposed to long, irrelevant context. Although accuracy decreases across both prompt types, the drops remain moderate and the models preserve much of their original performance(the biggest drop among both the settings between both models is 10.4%). This suggests that these closed-source models maintain strong grounding mechanisms that prevent distant, semantically unrelated text from dominating the multimodal encoding. Their internal routing of cross-modal attention appears to effectively constrain where language context can influence downstream reasoning. In contrast, Qwen3-Omni-30B and ChatBridge suffer severe declines in accuracy under long-context interference. Qwen experiences major reductions for both visualand audio-prompt settings, and ChatBridge undergoes complete collapse in both cases. These sharp failures indicate that iropen-source models lack robust long-range filtering: relevant early-context is able to hijack the models hidden representations, overwhelming the true multimodal signal. The vulnerability is especially pronounced for audioconditioned inference, where the already weaker audio representations are easily overwritten by distractor text. Taken together, these results demonstrate that closedsource architectures exhibit significantly stronger resilience to irrelevant long-context noise, while open-source models are far more susceptible to interference, especially when relying on fragile auditory cues. J. Unimodal Abstention Evaluation: The None of the Above Experiment In the unimodal study presented in the main paper (Section 4.1.2), models were forced to select class from predefined list even when the relevant modality was removed (e.g., answering Which class best describes the visual content of this video? given black video). This setting is inherently ill-posed, as it forces the model to either guess randomly or hallucinate based on the remaining modality. To provide more rigorous evaluation of modality dependence, we introduce Zero-Shot Abstention Test. Inspired by the methodology in AVTrustBench [5], we append the option None of the above to the candidate list. In this setting, if model is asked to describe the visual content of black frame, the only correct behavior is to reject the semantic classes and select None of the above. failure to do so indicates that the model is hallucinating information from the remaining modality (e.g., hearing the visual content). We evaluate all baselines and our alignment-tuned model in this setting. Importantly, our fine-tuned model was Visual-focused prompt: \"Think step by step about what the visual content shows. First provide your thinking process, then give the final answer in the format: <class>. Answer: Final \" Audio-focused prompt: \"Think step by step about what the audio content shows. your thinking process, then give the final answer in the format: <class>. Answer: First provide Final \" Figure 25. Chain-of-Thought (CoT) Prompting Strategy. We explicitly instruct the model to articulate its thinking process before providing the final classification, aiming to force logical separation of modalities. never exposed to None of the above labels or unimodal data during training, making this zero-shot stability test. The results are summarized in Table. 10. We observe strong resistance to abstention across SOTA baselines. Gemini-2.0-Flash, for example, records near-zero accuracy in missing-modality settings (3.04% for Frames Zeroed, 1.06% for Audio Removed). Instead of signaling ignorance, the model forces prediction based on the single available modality in > 95% of trials. Our finetuned model (Qwen2.5-Omni-7B+Ours) demonstrates remarkable emergence of abstention capability in the visual domain. In the Frames Zeroed setting, our model achieves an accuracy of 90.27%, significantly outperforming the base Qwen2.5-Omni-7B (10.94%) and the 30Bparameter Qwen3-Omni (15.05%). This indicates that our modality-aware fine-tuning successfully taught the model that visual questions require visual evidence. By learning to distinguish between aligned and misaligned pairs during training, the model effectively learned to disregard audio cues when performing visual reasoning. Consequently, when visual evidence is absent (black frames), it refuses to let the audio track dictate the visual answer, correctly defaulting to None of the above. On the other hand, in the Audio Removed setting, our model scores 0%, similar to several baselines. This suggests that while we successfully blocked the Audio Visual leakage, the Visual Audio shortcut remains strong. When the audio is silent, the model likely still perceives the visible object (e.g., dog) and is compelled to predict the associated sound (e.g., Bark), illustrating the extreme difficulty of overcoming visual dominance in MLLMs. K. Can Reasoning Traces Fix Misalignment? (Chain-of-Thought Evaluation) Recent literature [27] suggests that prompting MLLMs to think step-by-step (Chain-of-Thought or CoT) can po8 Model Gemini-1.5-Pro Gemini-2.0-Flash Qwen3-Omni-30B Qwen2.5-Omni-7B ChatBridge PandaGPT Qwen2.5-Omni-7B + Ours 94.68 Visual Prompt Audio Prompt Standard Align Audio Removed Abstention Test Frames Zeroed Align Standard Abstention Test Frames Zeroed Audio Removed 97.90 96.71 92.88 76.68 51.64 28.75 95.28 91.91 83.73 58.72 54.71 29.79 94.37 47.42 3.04 15.05 10.94 55.77 11. 90.27 60.37 57.21 57.39 46.60 41.61 13.12 88.14 24.95 9.42 14.58 25.16 7.07 1.18 79.79 3.79 1.06 11.71 9.86 37.69 0. 0.00 Table 10. Zero-Shot Abstention Performance. We add None of the above to the options and remove one modality (Frame Zeroed or Audio Removed). high score indicates the model correctly abstains from answering when the data is missing. Our fine-tuned model (bottom row) shows exceptional zero-shot abstention in the visual domain (90.27%), proving it no longer hallucinates visual answers from audio cues. dow and overrides the learned audio alignment. This confirms that robust modality alignment requires intrinsic parameter optimization rather than extrinsic prompt engineering, and that CoT might not help with perceptual grounding. L. Qualitative Analysis of Improved Modality"
        },
        {
            "title": "Grounding",
            "content": "We present selection of qualitative examples in Figure 26 28 to visualize the practical improvement of alignmentaware fine-tuning compared to standard baselines. In scenarios characterized by semantic misalignment, such as video depicting dog paired with the sound of phone ringing, baseline models frequently succumb to visual dominance and incorrectly predict barking sound. Our model successfully decouples these conflicting sensory streams, correctly attending to the auditory signal despite the visual contradiction. Furthermore, the improved model demonstrates significant resilience to textual interference. When provided with misleading captions that contradict the actual sensory content, our model ignores the linguistic hallucination and grounds its response in the verified audio-visual evidence, confirming that the training process effectively reduces the over-reliance on priors from both the visual and textual modalities. Method Visual Prompt (%) Audio Prompt (%) Align Misalign Align Misalign Qwen2.5-Omni-7B (Standard) Qwen2.5-Omni-7B + CoT 76.68 66. Qwen2.5-Omni-7B + Ours (Standard) 94.68 94.71 Qwen2.5-Omni-7B + Ours + CoT 58.72 53.25 94.37 94.66 46.60 45.14 88.14 58.46 25.16 31. 79.79 55.14 Table 11. Impact of Chain-of-Thought (CoT) Prompting. Comparing standard inference vs. CoT. While CoT provides marginal gains for the base model in specific niches, it degrades the robust audio grounding of our fine-tuned model, likely by re-introducing visual priors during the reasoning generation step. tentially strengthen reasoning capabilities and improve interpretability. To investigate whether inference-time reasoning can resolve sensory conflict without parameter updates, we evaluated both the base Qwen2.5-Omni-7B and our alignment-tuned variant using the CoT prompt structure illustrated in Figure 25. The results, summarized in Table 11, reveal two counter-intuitive findings that challenge the assumption that CoT is beneficial for multimodal misalignment. Contrary to the expectation that reasoning traces would filter noise, applying CoT to the base Qwen model resulted in performance regression on visual tasks (Visual Base: 76.68% 66.18%). This aligns with recent findings that encouraging the model to think might not always help [19]. In our misalignment setting, the model likely uses the reasoning steps to describe the conflicting audio or visual priors, effectively confusing itself rather than resolving the conflict. The most striking result is the impact of CoT on our finetuned model (Qwen2.5-Omni-7B+Ours). While visual performance remains stable, auditory performance collapses (Audio prompt with aligned samples: 88.14% 58.46%). We hypothesize that during the thinking phase, the model likely defaults to describing the dominant visual input (visual dominance), which re-contaminates the context win9 Figure 26. Qualitative Results Gallery. Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input. 10 Figure 27. Qualitative Results Gallery. Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input. 11 Figure 28. Qualitative Results Gallery. Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input."
        }
    ],
    "affiliations": [
        "Boston University",
        "Google DeepMind"
    ]
}