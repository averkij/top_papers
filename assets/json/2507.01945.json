{
    "paper_title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory",
    "authors": [
        "Nan Chen",
        "Mengqi Huang",
        "Yihao Meng",
        "Zhendong Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/."
        },
        {
            "title": "Start",
            "content": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory Nan Chen1, Mengqi Huang1, Yihao Meng2, Zhendong Mao1 1University of Science and Technology of China 2Hong Kong University of Science and Technology {chen nan,huangmq}@mail.ustc.edu.cn, ymengas@connect.ust.hk, {zdmao}@ustc.edu.cn 5 2 0 2 2 ] . [ 1 5 4 9 1 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Animation colorization is crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal longterm color consistency can be achieved through dynamic global-local paradigm, i.e., dynamically extracting global color consistent features relevant to the current generation. Specifically, we propose LongAnimation, novel framework, which mainly includes SketchDiT, Dynamic Global-Local Memory (DGLM), and Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce Color Consistency Reward. During inference, we propose color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at LongAnimation. 1. Introduction Animation is one of the most entertaining and aesthetic forms in the video industry. Animation colorization is crucial part of real animation industry production. The real Zhendong Mao is the corresponding author. Figure 1. Comparison with existing paradigm. (a) Existing studies achieve local color consistency by fusing the overlaps of adjacent video segments, suffering low long-term color consistency. (b) Our dynamic global-local paradigm dynamically extracts color features of global historical segments as global memory and the color features of the latest generated segment as local memory, achieving high long-term color consistency. All segments are generated from one same reference image. animation industry first draws and colors one key frame (not just the first frame, but also the frame in the sequence). Then, sketch sequences around key frame, especially long sequences up to 10 30 seconds (300 to 1000 frames), are colored based on the key frame. Coloring such long sequences is extremely labor-intensive, which makes the research of automated long animation coloring highly significant. The primary goal of long animation coloring is maintaining long-term color consistency, which means the same object should have consistent colors in all frames. Existing methods mainly explore short-term (within 100 frames) animation colorization. Early work [47] colors each frame based on image models and splices them into videos with weak color consistency. Recent studies [25, 43, 45], based on video models, explore various control on fixed segment (e.g., 16 frames), such as frame interpolation [43], single-layer coloring [45], and character-based coloring [25]. These methods do not consider longer animation coloring. LVCD [12], based on the Unet video model, further attempts to use local paradigm to extend colorization up to 100 frames, that is, fusing the overlapping features of adjacent segments, as shown in Fig. 1 (a). In summary, existing methods are either limited to fixed short-term coloring or use local paradigm to achieve limited coloring extension. However, existing methods have inherent defects, i.e., neglecting the global color relationship, which makes it difficult to maintain color consistency over longer animations. Autoregressively using the last frame of the previous generation as the reference image for the next segment causes noise error accumulation in long videos, while using only one fixed reference image to control all segments ignores the relationship between segments. Based on fixed reference image, the local paradigm fuses the overlapping frame features of adjacent segments to partially constrain the consistency of adjacent segments rather than the color consistency between all segments. Additionally, some animations involve large movements, which makes keeping long-term consistency with local paradigm more challenging. As result, erratic color details appear in long animations (e.g., the girls yellow hat turns red in the 100th frame, and the boys hat changes color in the 300th frame), as shown in Fig. 1(a). We argue that an ideal long animation coloring performance could be achieved by the dynamic global-local paradigm, which aims to achieve long-term (e.g., 500 frames) consistency by dynamically fusing global and local color features, as shown in Fig. 1(b). The history animations contain color consistency features and redundant features. Global memory can dynamically extract color consistency features, achieving global color consistency constraint. Additionally, global memory can model low-frequency animation changes to achieve more stable colorization, reducing color flickering, as shown in Fig. 8. Local memory is used for short-term segments to ensure smooth local transitions. By the joint memory of global and local segments, the dynamic global-local paradigm can effectively improve the long-term color consistency of animations (e.g., the consistent color of the girl and boys hat in the Fig. 1(b)). In this paper, we propose novel DiT-based long animation coloring framework, LongAnimation, which achieves long-term color consistency by dynamically extracting color features related to current generation from global generation information. LongAnimation mainly includes three key components: (1) SketchDiT, which aims to efficiently extract reference features to support the subsequent dynamic memory mechanism; (2) Dynamic Global-Local Memory (DGLM) mechanism, which aims to extract longterm consistency features related to the current generation from historical animations, and (3) Color Consistency Reward, which aims to refine the color consistency. Specifically, we first propose the SketchDiT architecture for the animation colorization task, which achieves efficient fusion of hybrid features of reference images, sketches and text based on DiT-based video model. The innovative DGLM mechanism, which first introduces the Long Video Understanding model to dynamically compress generated animations, adaptively extracts global color consistent features. Furthermore, we propose the Color Consistency Reward to further refine the models coloring ability by aligning the low-frequency features extracted from the Long Video Understand model of the generated animation and reference animation. During inference, we propose color consistency fusion to achieve smooth transitions, which utilizes latent fusion in the later denoising stage. LongAnimation can stably colorize long animations averaging 500 frames, at least 5 times more than past methods [12, 25, 43] could achieve. Our main contributions are summarized as follows: Concepts. We propose novel dynamic global-local paradigm, which aims to achieve accurate long-term animation colorization by dynamically extracting global and local consistent color features. Technology. We propose LongAnimation, which designs novel DGLM module based on dynamic compression of long video understanding model to adaptively extract historical color features related to the current generation. To facilitate DGLM module, we design novel SketchDiT to efficiently control the DiT-based video model. Experiments. LongAnimation significantly outperforms previous advanced models on the open-domain long animation coloring task, improving short-term and longterm performance by 35.1% and 49.1% respectively on the widely used metric Frechet Video Distance (FVD). 2. Related Work 2.1. Long Video Generation Visual generation has gradually evolved from controllable T2I [13, 19, 50] to controllable T2V [2124, 36], in which key issue is long video generation. Training-free studies [20, 28] extend the video length by window fusion or frequency control. DitCTrl [2] follows window fusion method in DiT [27] architecture. Training-based studies [8, 30, 48] usually rely on the most recent segment to generate the next segment. The most recent study [11] dynamically updates historical segment features by fine-tuning LoRA during inference, achieving longer memory. However, long video generation with finer-grained control (e.g., sketch, ID) is more challenging as it requires more precise long-term consistency of control conditions, not just with text. Some studies [12, 49] introduce local fusion but rarely consider global consistency. LongAnimation innovatively uses Long Video Understanding model to extract global temporal features for long controllable video generation without fine-tuning during inference. 2.2. Long Video Understanding The multimodal community has explored many methods for long video understanding (LVU). Early studies [10, 16, 18] processes videos through sparse sampling (e.g., 16 and 24 frames). However, sparse sampling discards numerous frames, causing biased understanding. To achieve better understanding, some studies [6, 17, 31, 37, 38, 41] compress video features to feed more frames into LVU models. Some studies [6, 17, 41] reduce the token numbers per frame or use token merging to compress features. Recent Video-XL [31] and ReTAKE [37] use KV cache to compress visual input, achieving fine-grained visual perception. We use Video-XL, which can process thousands of frames, to extract fine-grained global historical features. 2.3. Reference-based Line Art Video Coloring Reference-based line art video coloring focuses on injecting reference frame color (not just the first frame) into the animations. Early study ACOF [47] proposes transferring color for each frame by optical flow and splicing them into videos. Recent studies mainly use the sketch to generate consistent videos based on the Video Diffusion Model. ToonCrafter [43] proposes an animation interpolation diffusion model, which requires the first and last reference frames. LVCD [12] introduces video ControlNet to control the motion, using local fusion to improve the consistency of segment splicing. LayerAnimate [45] proposes Layer ControlNet to achieve single object control. AniDoc [25] can generate character animations given character image without background. Recent studies [12, 25, 43, 45] focus on generating short-term animations (e.g., within 100 frames) based on the UNet video model. However, long animation shots often last 10 30 seconds (300 to 1000 frames) in real scenarios. We propose LongAnimation to achieve efficient long-term coloring on the DiT architecture. 3. Methodology The pipeline of LongAnimation is depicted in Fig. 2, which mainly consists of three parts: SketchDiT, Dynamic Global-Local Memory and Color Consistency Reward. During training, given reference image I, frames of sketches {Sf }F =1, and corresponding text descriptions, LongAnimation extracts hybrid reference features of these inputs through SketchDiT, which is designed to facilitate the subsequent Dynamic Global-Local Memory mechanism. When generating subsequent segments, Dynamic Global-Local Memory dynamically compresses historical features, which adaptively extracts global consistency features related to the current hybrid features obtained from SketchDiT, ultimately generating frames of long-consistency animation {If }F =1. Color Consistency Reward is used to refine long-term color consistency during training while color consistency fusion is used to smooth In this section, we transition segments during inference. will first introduce preliminaries. Then, we will describe the SketchDiT, Dynamic Global-Local Memory and Color Consistency Reward. Next, we will introduce the color consistency fusion. 3.1. Preliminaries Video Diffusion Transformer. Our work is based on CogvideoX [46], one of the excellent performing DiT-based video generation models [7, 14, 46]. The text encoder t() first encodes the text into ct. Next, the reference image and video are encoded by the 3D VAE encoder v() into reference image token ci and video token z. Then, ci is padded and concatenated with along the channel dimension, followed by concatenation with ct along the sequence dimension. Finally, the combined sequence is fed into the DiT model as input. The DiT denoiser ϵθ() is trained by: LDiT = z,ϵ,t ϵ ϵθ (zt, t, ct, ci)2 2 , (1) where ϵ refers to standard noise and means denoising timestep. zt is the video hidden tensor at t-th timestep. Reward model without using the gradient. In image or video generation, reinforcement learning (RL) algorithms use rewards to align with preferences. There are two main types: gradient-based rewards (GR) [42, 44] and nongradient-based rewards (NGR) [1, 5, 15]. GR requires the reward gradient for optimization, whereas NGR does not. The models generate images or videos in pixel space via VAE and then score them using the reward. Backpropagating gradients through the 3D VAEs of most DiT-based video models [14, 46] is computationally expensive. Therefore, we choose NGR to optimize the model. The objective function Lr is defined as maximizing the expected reward, with its gradient θLr in diffusion models expressed as: (cid:34) θLr = r(x0) (cid:88) t=1 (cid:35) θ log pθ(zt1zt) , (2) where x0 donates the generated video in the pixel space and zt denotes the latents in the -th denoising step. is the total steps. Omit various control conditions for simplicity. Figure 2. Overview of the LongAnimation. (a) During training, the reference information is fed into the CogvideoX [46] and SketchDiT, respectively, for efficient extraction of hybrid reference features. These reference features are then fused with the historical information in Dynamic Global-Local Memory (DGLM) for consistency generation. (b) For the first segment generation, the reference features are fed into SketchDiT and then directly sent to the video model. (c) For the subsequent segment generation, DGLM dynamically extracts historical features, which are adaptively fused with current reference features from the SketchDiT before being fed into the video model. 3.2. Model Architecture 3.2.1. SketchDiT As shown in Fig. 2 (a), sketches {Sf }F SketchDiT is designed to efficiently extract the hybrid representation of reference image, sketches and text, facilitating the implementation of Dynamic Global-Local Memory. =1 are first encoded by 3D VAE encoder v() to the sketch token ck. The token is then concatenated with the padded reference image token ci along the channel dimension, and subsequently concatenated with text features ct in the sequence dimension as the input of SketchDiT. We introduce text control conditions in SketchDiT to enable text and reference image to jointly guide animation coloring (e.g., background generation), which could not be achieved by past methods. In animation production, the reference image typically does not always align with the first frame of the sketches. Creators only need to draw keyframe, from which smooth videos before and after the key frame can be generated using sketches. Therefore, during training, we propose to randomly select the reference image from historical key frame (e.g., the keyframe that is hundreds of frames away from the current segment.) rather than being fixed as the first frame of the segment to enhance the robustness. Then, the concatenated tokens are fed into the SketchDiT S() to get the hybrid multimodal feature S(ct, ci, ck). Following CogvideoX, SketchDiT adopts DiT modules (where to save computing resources and time consumption), each of which contains 3D attention layer and Feed Forward Network (FFN). To accelerate training, we use the first layers of CogvideoX as the initial weights for SketchDiT. The hybrid multimodal features S(ct, ci, ck) are then injected into the vision branches of different layers 3D Attention of CogvideoX parallelly. Specifically, to further reduce overfitting, we perform skip-layer control of the base model, which is as follows: zn = zn + γS(ct, ci, ck) {2, 4, . . . , }, (3) where zn refers to the visual features obtained after the 3D Attention in the n-th DiT block. denotes the number of DiT blocks in CogvideoX. γ is weight factor. During training, we freeze the weights of CogVideoX to preserve its original text-guided capabilities. For the first generation, the features S(ct, ci, ck) are directly fed into CogvideoX, as shown in Fig. 2 (b). For subsequent generation, the features S(ct, ci, ck) are fused with relevant global features of Dynamic Global-Local Memory before being fed into CogvideoX, as shown in Fig. 2 (c). 3.2.2. Dynamic Global-Local Memory Historical animations contain both color consistency features related to the current generation and redundant features. To extract the color consistency features related to the current generation, we propose the Dynamic Global-Local Memory (DGLM) mechanism as shown in Fig. 2(c). The long video understanding (LVU) model Video-XL [31] is used to extract historical features. After estimating the visual feature changes between adjacent frames using CLIP [29, 39], different segments (e.g., 2, 4, 8 frames, offering more fine-grained compression than 3D-VAE) are dynamically selected for the video. Subsequently, these frame segments are fed into the multimodal large language model (MLLM), which autoregressively generates visual token <vs> for each segment. Each token <vs> is influenced by all previous frame segments and their <vs> tokens, while the original frame features are finally offloaded. The key and value of the <vs> token at each layer are stored in the KV cache to improve efficiency. Some studies [4, 33] indicate that the middle layers of MLLM focus more on global visual features than the final layer. Specifically, we define the most recently generated segments as local video Vl and all historically generated frames as global video Vg. Considering these segments comprehensively can achieve better temporal consistency. Specifically, the global video Vg and the local video Vl are fed into the LVU model, which extracts the keys {kg, kl}M and values {vg, vl}M of the visual token <vs> stored in the KV cache, where refers to the extracted layer number. {kg, kl}M are then fed into the FFNbased projector to align the dimensions of the hybird feature S(ct, ci, ck). Then the extracted -layer KV cache features and features S(ct, ci, ck) are sequentially fed into cross-attention layers, adaptively extracting global visual features relevant to the current generation as follows: and {vg, vl}M Softmax (cid:32) ](cid:1) (cid:0)[km , km (cid:33) (cid:0)[vm , vm ](cid:1) , (4) , km ] and value = Wv [vm where the Query = Wq S(ct, ci, ck), key = Wk [km ]. Wq, Wk and Wv are weight parameters. [ , ] denotes feature concatenation. Summarily, DGLM achieves long-term color consistency by dynamically compressing historical features and adaptively extracting current-relevant features. , vm Figure 3. Overview of color consistency fusion during inference. The dashed latent of the overlap part is finally discarded. 3.2.3. Color Consistency Reward While the DGLM module significantly improves long-term color consistency, minor color details remain flawed. Inspired by non-gradient reward studies[1, 5] in image generation, we propose the Color Consistency Reward (CCR) to refine the long-term color consistency of animation. Many past studies [32, 35] have shown that self-attention in Transformer acts as low-pass filter, which means it captures low-frequency features (e.g., colors in anime) better than high-frequency features (e.g., sketches). Thus, by aligning the -layer KV cache features from the LVU model of the generated anime with those of the reference anime, the generated anime colors can be closer to the real anime colors. The reward function is as follows: (cid:88) = (cid:0)km ref km 2 + vm ref vm2 2 (cid:1) , (5) m=1 ref, vm Where {km ref} donates the features of the m-th KV cache of the reference video, and {km, vm} donates the features of the m-th KV cache of the generated video. The NGR loss from Eq. (2) is used to optimize CCR. Aligning the KV caches of the generated video with the reference video further reduces the gap between inference and training. The KV caches are extracted from reference videos during training, while these features are extracted from past generated videos during inference. Therefore, the CCR can further refine long-term color consistency. 3.3. Inference Paradigm While LongAnimation can achieve global consistency between segments, smooth transitions between adjacent segments also need to be processed. Recent Studies [2, 49] explore latent blending strategies in other fields of video generation by blending overlapping segments at all timesteps. However, we discover using latent blending strategies for all denoising steps would disrupt visual details (e.g., brightness), which is obvious in dark brightness animations, as shown in Fig. 7. This phenomenon mainly occurs when blending from the early denoising stage. On the one hand, the early stage focuses on the overall layout features, while the later stage focuses on the visual details (e.g., Methods Short Term (14 Frames) Long Term (Average 500 frames) LPIPS SSIM PSNR FVD FID LPIPS SSIM PSNR ToonCrafter[43] LVCD[12] AniDoc[25] LVCD* Ours 0.196 0.203 0.142 0.126 0.054 0.457 0.732 0.759 0.811 0.867 18.57 22.86 24.13 26.66 27. 564.48 520.51 427.03 288.70 187.48 52.91 89.39 70.31 78.95 37.80 0.238 0.223 0.169 0.162 0.068 0.440 0.722 0.743 0.776 0.868 18.01 22.77 23.17 22.94 26.71 FVD 751.20 734.85 531.32 473.02 240.57 FID 89.87 104.90 76.67 94.86 40.75 Improvement 57.1% 6.9% 2.1% 35.1% 28.6% 58.0% 11.8% 15.3% 49.1% 46.9% Table 1. Quantitative comparison with existing methods. LongAnimation achieves the best performance in both short-term and longterm animation coloring. Compared to short-term animation coloring, LongAnimation shows greater improvement in long-term animation coloring. Bolded numbers indicate the best performance. Figure 4. Qualitative comparison with existing methods. LongAnimation achieves long-term color consistency through Dynamic GlobalLocal Memory (e.g., the grils dress and leaves). In contrast, previous methods exhibit unstable color changes. We highly recommend watching the videos in the supplement material for more direct and clear understanding. fine-grained color, brightness). The overall layout features gained by the early denoising fusion are redundant for the animation coloring task. On the other hand, current DiTbased video models usually jointly decode video latents, rather than decoding each frame separately like the previous UNet models. It means that unnecessary perturbations to partial latents affect the decoding of all latents. Since visual detail features (e.g., color, brightness) are mainly refined in late denoising stage, we propose fine-grained color consistency fusion in late denoising stage, as shown in Fig. 3. Formally, given two adjacent frame segments Pi1 and Pi, each containing frames. The frame number overlapped by two segments is C. fusion factor α = 1 C+1 is set to control the fusion intensity. α ensures that frames closer to the boundary have lower weights, while internal frames have higher weights. During denoising, when [T, tst], the frame segments are directly concatenated, with the overlapping part taking half from each segment. When [tst, 0], latent fusion is applied to the overlapping parts of adjacent segments as follows: it = α zk zk it + (1 α) zF C+k (i1)t , (6) where [1, + 1] donates the k-th overlapping segment frame. zk it denotes the video latent of the k-th overlapping frame in segment Pi during the denoising step t. In summary, color consistency fusion smoothes color transitions between adjacent segments by fusing overlap segments during the late denoising stage of inference. Figure 5. Text and reference image jointly control background generation, which could not be achieved by previous methods. 4. Experiments 4.1. Experimental Setups is Implementation. implemented on Our model CogVideoX-1.5-5B [46], which can stably generate 81 frames. Sakuga-42M [26] is used as our training dataset. Our work is solely for non-commercial scientific research. We filter high-aesthetic and dynamic video clips with lengths greater than 91 frames, retaining about 80k videos for training. The model is trained on 6 A100 GPUs with learning rate 1e-5. The model is trained at resolution of 1024 576. We set the SketchDiT layers = 6 while CogVideoX-1.5 layers = 42. SketchDiT is trained for 30k steps in the first stage, and GLM is trained for 10k steps in the second stage. Finally, we use Color Consistency Reward to refine the coloring ability for 10k steps further. More details are in Appendix A.1. Test Dataset. We randomly select 3k samples from the Sakuga test set for short-term coloring testing. To ensure fair comparison with past models, we only use the first 14 frames generated by each model (42k frames in total) for short-term analysis. For long-term animation generation, we select videos with more than 300 frames from the Sakuga test and filter samples with aesthetic scores above 0.8 and dynamic scores above 0.4 (200 videos, approximately 100k frames) to test long-term generation. Binaryzation. Standard animation creation usually uses binary lines. Anidoc [25] points out that LVCD [12] directly uses grayscale sketches, which causes the model to recover the hidden colors from the sketch rather than from the reference image. During training, we extract the sketches by the method [3] and binarize them by setting values greater than 200 to 0 and all other values to 1. Evaluation metrics. We evaluate the animation colorization effect from two aspects: 1) Video quality: FID [9] and FVD [34] are used to evaluate the generated frame quality and video quality. 2) Frame colorization similarity. Since the sketches in the animation are extracted from the original animation, we measure the generated animation frames and the original frames through PSNR, LPIPS, and SSIM [40]. For all metrics, we resize the frame to 256 256 and norFigure 6. Ablation Studies of modules. Compared to SketchDiT, DGLM markedly enhances color consistency (e.g., the girls hair). CCR further refines color details (e.g., the girls hairband). malize the pixel to [0, 1] following recent studies [12, 25]. Prior SOTAs. We compare our model with open source state-of-the-arts. These methods are all trained on UNetbased video diffusion models, which are LVCD [12], ToonCrafter [43], and AniDoc [25]. Note that in Tab. 1 and Fig. 4, LVCD [12] uses grayscale sketches in the [0, 255] range according to the default settings. All other methods use binary sketches. Autoregressively using the last frame of the previous segment as the reference image for the next generation will lead to the accumulation of noise errors, as shown in Appendix B.1. Therefore we set all methods to use the same reference frame for generating all segments. 4.2. Main Results In this section, We will compare LongAnimation with prior SOTAs through qualitative and quantitative analyses. Quantitative results. As shown in Tab. 1, our method achieves the best performance in both short-term and longterm animation coloring tasks. For short-term animation generation, LongAnimation improves frame similarity (LPIPS) and video quality (FVD) by 57.1% and 35.1% compared to sub-optimal results, respectively. For longterm video generation, LongAnimation improves frame similarity (LPIPS) and video quality (FVD) by 58.0% and 49.1% compared to sub-optimal results, respectively. LongAnimation shows greater improvement in long-term generation compared to short-term generation, indicating the effectiveness of our model in long-term generation. Qualitative results. Fig. 4 shows the qualitative comparison between LongAnimation and existing methods. Our method achieves long-term color consistency through Dynamic Global-Local Memory. More qualitative comparisons are presented in Appendix B.2. To retain the text-guided ability, we introduce text features to SketchDiT. As shown in Fig. 5, LongAnimation can achieve long-term text-guided video background generation based on masked reference foregrounds, which past models Figure 7. Ablation Studies of timestep on color consistency fusion (CCF). Adopting CCF can keep the consistency of the spliced segments (e.g., white objects in frames 71 and 72). However, using CCF in the early stage of denoising (tst = 50&40 ) interferes with other features in latent space, which affects the brightness of other frames (e.g., frames 250 and 350). The problem does not occur when CCF is used in the late stage of denoising (tst = 20). ID Settings LPIPS SSIM PSNR FVD 0 1 2 3 SketchDiT 0 + Local Memory 0 + Global Memory 0 + DGLM 3 + CCR 0.086 0.080 0.078 0.076 0.068 0.838 0.843 0.854 0.862 0.868 24.46 24.76 25.38 26.11 26.71 321.62 315.05 297.93 261.76 240.57 Table 2. Ablation studies on each module. Compared with using only SketchDiT (ID-0), introducing both global memory and local memory (DGLM, ID-3) significantly improves performance. Compared to local memory (ID-1), global memory (ID-2) performs better. CCR further refines the coloring performance (ID-4). Settings LPIPS SSIM PSNR FVD w/o CCF (tst = 0) CCF tst = 50 CCF tst = 40 CCF tst = 0.073 0.089 0.083 0.068 0.860 0.833 0.850 0.868 26.39 25.40 25.71 26.71 266.99 337.05 301.62 240.57 Table 3. Ablation studies for Color Consistency Fusion (CCF), where tst represents the timestep for the latent fusion. Injecting local features from the early stage of denoising (tst = 50 or tst = 40) reduces the video quality. [12, 25, 43] can not achieve. LongAnimation shows strong generalization and potential for application. 4.3. Ablation Studies To demonstrate the effectiveness of essential components, we conduct extensive ablation experiments. All ablation experiments are tested on long video (average 500 frames). We present the key ablation study in the main text, with the remaining ablation experiments shown in Appendix B.3. Effectiveness of Each Module for LongAnimation. As indicated in Tab. 2 and Fig. 6, compared to using only SketchDiT (ID-0), introducing DGLM (ID-3) significantly Figure 8. (a) PSNR with existing methods. LongAnimation outperforms previous methods in PSNR metric. (b) PSNR relative decay ratio in the low-frequency domain. Our method exhibits the least attenuation in low-frequency information (e.g., color), indicating that our proposed Dynamic Global-Local Memory can better maintain long-term color consistency. (c) PSNR relative decay ratio in the high-frequency domain. Our method exhibits the least attenuation in high-frequency information (e.g., sketches). Since all methods are controlled by sketches, their attenuation in high frequencies is generally smaller compared to low frequencies. improves long-term color consistency, with the frame similarity metric (LPIPS) improving by 11.6% and the video quality metric (FVD) improving by 18.6%. ID-1 and ID2 validate the effectiveness of global video and local video respectively. Compared to ID-3, Color Consistency Reward (CCR) (ID-4) further refines the long-term coloring effect, with frame similarity metric (LPIPS) improving by 10.5% and video quality metric (FVD) improving by 8.0%. Effectiveness of Color Consistency Fusion (CCF). As indicated in Tab. 3 and Fig. 7, latent fusion from the early denoising stage (tst = 50&40) results in decrease in frame similarity and video quality metrics compared to not using CCF. However, latent fusion from the late denoising stage (tst = 20) leads to improvements in these metrics. This indicates that CCF from the late denoising stage not only provides better color consistency in the fusion frames but also maintains the brightness in other frames. 4.4. Frequency Analysis As shown in Fig. 8 (a), LongAnimation has the best PSNR performance. To further demonstrate the superiority of LongAnimation, we perform frequency domain analysis by applying the Fourier transform to the videos generated by each model and reference videos. After separating highfrequency (e.g., sketch) and low-frequency (e.g., color) features, we calculate the PSNR for each frequency component. For fair comparison, we calculate the PSNR degradation rate between long videos and short videos (i.e., 14 frames) generated by each model. As shown in Fig. 8 (b), our method has the weakest decay ratio at low frequencies, improved by 8.2% at 500 frames compared to sub-optimal method. It shows that our method better preserves the lowfrequency (i.e., color) features, while the low-frequency preservation of past methods diminishes over time. Since all methods use high-frequency sketch control, high-frequency decay is generally small, as shown in Fig. 8 (c). 5. Conclusion In this paper, we propose LongAnimation, novel long animation coloring framework to achieve long-term color consistency. LongAnimation dynamically compresses historical features and adaptively extracts features relevant to current generation by Dynamic Global-Local Memory mechanism. Color consistency is further refined by Color Consistency Reward. To support DGLM, we propose SketchDiT to extract hybrid reference features. Extensive experiments demonstrate the superiority of our method, showing strong application prospects in the animation industry."
        },
        {
            "title": "References",
            "content": "[1] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3, 5 [2] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. arXiv preprint arXiv:2412.18597, 2024. 2, 5 [3] Caroline Chan, Fredo Durand, and Phillip Isola. Learning to generate line drawings that convey geometry and semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79157925, 2022. 7 [4] Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, et al. Explainable and interpretable multimodal arXiv large language models: comprehensive survey. preprint arXiv:2412.02104, 2024. 5 [5] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3, 5 [6] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 3 [7] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. 3 [8] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [10] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 3 [11] Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, KaiWei Chang, Lingjie Li, Kevin Lin, Chung-Ching Lin, Jianfeng Wang, Zhengyuan Yang, et al. Slowfast-vgen: Slowfast learning for action-driven long video generation. arXiv preprint arXiv:2410.23277, 2024. 2 [12] Zhitong Huang, Mohan Zhang, and Jing Liao. Lvcd: reference-based lineart video colorization with diffusion models. ACM Transactions on Graphics (TOG), 43(6):111, 2024. 2, 3, 6, 7, 8 [13] Weinan Jia, Mengqi Huang, Nan Chen, Lei Zhang, and Zhendong Mao. Dˆ 2it: Dynamic diffusion transformer for accurate image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 12860 12870, 2025. [14] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [15] Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et al. Parrot: Pareto-optimal multi-reward reinforcement learning framework for text-to-image generation. In European Conference on Computer Vision, pages 462478. Springer, 2024. 3 [16] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 3 [17] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 3 [18] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 3 [19] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. [20] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. 2 [21] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 2 [22] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. [23] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 60186026, 2025. [24] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [25] Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, and Huamin Qu. Anidoc: Animation creation made easier. arXiv preprint arXiv:2412.14173, 2024. 2, 3, 6, 7, [26] Zhenglin Pan, Yu Zhu, and Yuxuan Mu. dataset: Scaling up cartoon research. arXiv:2405.07425, 2024. 7 Sakuga-42m arXiv preprint [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2 [28] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 2 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [30] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. [31] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 3, 5 [32] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng Yan. Inception transformer. Advances in Neural Information Processing Systems, 35:2349523509, 2022. 5 [33] Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, and Dongyan Zhao. Probing multimodal large language models for global and local semantic representations. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1305013056, Torino, Italia, 2024. ELRA and ICCL. 5 [34] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [35] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962, 2022. [36] Wenchuan Wang, Mengqi Huang, Yijing Tu, and Zhendong Mao. Dualreal: Adaptive joint training for lossless identity-motion fusion in video customization. arXiv preprint arXiv:2505.02192, 2025. 2 [37] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Retake: Reducing temporal and knowledge redundancy for long video understanding. arXiv preprint arXiv:2412.20504, 2024. 3 [38] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 3 [39] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurarXiv preprint arXiv:2409.01071, rent memory bridges. 2024. 5 [40] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [41] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. [42] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. In European Conference on Computer Vision, pages 108124. Springer, 2024. 3 [43] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Tooncrafter: Generative cartoon interpolation. ACM Transactions on Graphics (TOG), 43(6):111, 2024. 2, 3, 6, 7, 8 [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 3 [45] Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, and Zhaoxiang Zhang. Layeranimate: Layer-specific control for animation. arXiv preprint arXiv:2501.08295, 2025. 2, 3 [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 4, 7 [47] Yifeng Yu, Jiangbo Qian, Chong Wang, Yihong Dong, and Baisong Liu. Animation line art colorization based on the optical flow method. Computer Animation and Virtual Worlds, 35(1):e2229, 2024. 2, [48] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 2 [49] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 3, 5 [50] Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, and Zeyu Wang. Magiccolor: Multi-instance sketch colorization. arXiv preprint arXiv:2503.16948, 2025."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Science and Technology of China"
    ]
}