{
    "paper_title": "Depth Anything with Any Prior",
    "authors": [
        "Zehan Wang",
        "Siyu Chen",
        "Lihe Yang",
        "Jialei Wang",
        "Ziang Zhang",
        "Hengshuang Zhao",
        "Zhou Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models."
        },
        {
            "title": "Start",
            "content": "Zehan Wang1*, Siyu Chen1, Lihe Yang2, Jialei Wang1, Ziang Zhang1, Hengshuang Zhao2, Zhou Zhao1 1Zhejiang University; 2The University of Hong Kong https://prior-depth-anything.github.io/ 5 2 0 2 5 1 ] . [ 1 5 6 5 0 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This work presents Prior Depth Anything, framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 realworld datasets, matching or even surpassing previous taskspecific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing flexible accuracy-efficiency trade-off while evolving with advancements in MDE models. 1. Introduction Fine-detailed and dense metric depth information is fundamental pursuit in computer vision [11, 14, 16, 37, 49, 60] and robotics applications [51, 52, 63]. Although monocular depth estimation (MDE) models [4, 21, 25, 27, 34, 55, 56, 58] have made significant progress, enabling complete and detailed depth predictions, predicted depths are relative and lack precise metric information. On the other hand, depth measurement technologies, such as Structure from Motion (SfM) [40] or depth sensors [28], provide precise but often incomplete and coarse metric information. In this paper, we explore prior-based monocular depth *Equal Contribution. Figure 1. Core Motivation. We progressively integrate complementary information from metric measurements (accurate metrics) and relative predictions (completeness and fine details) to produce dense and fine-grained metric depth maps. estimation, which takes RGB images and measured depth priors as inputs to output detailed and precise metric depth maps. We unify different depth estimation tasks by abstracting various types of depth measurements as depth prior. To clarify the scope, we first outline common types of depth priors and their primary applications: 1) Sparse points (depth completion): Depth from LiDAR or SfM [40] is typically sparse. Completing sparse depth priors is crucial for applications such as 3D reconstruction [11, 37] and autonomous driving [7, 20, 45]. 2) Low-resolution (depth super-resolution): Lowpower Time of Flight (ToF) cameras [28], commonly used in mobile phones, capture low-resolution depth maps. Depth super-resolution is essential for spatial perception, VR [35], and AR [43] in portable devices [22]. 3) Missing areas (depth inpainting): Stereo matching failures [32, 39] or 3D Gaussian splatting edits [31, 59] may leave large missing areas in depth maps. Filling these gaps is vital for 3D scene generation and editing [59]. 4) Mixed prior: In real-world scenarios, different depth priors often coexist. For instance, structured light cameras [24] often generate low-resolution and incomplete depth maps, due to their limited working range. Handling these mixed priors is vital for practical applications. In Tab. 1, we detail the patterns of each prior. Existing methods mainly focus on specific limited priors, limiting their use in diverse, real-world scenarios. In this work, we 1 Methods Marigold-DC [47] Omni-DC [66] PromptDA [29] DepthLab [30] Sparse Point Target Task Depth Completion Depth Completion SfM LiDAR Extreme Depth Super-resolution Depth Inpainting Prior Depth Anything All-Rounder Lowresolution Missing Area Range Shape Object Mixed Table 1. Applicable scenarios of current prior-based monocular depth estimation models. SfM: sparse matching points from SfM, LiDAR: sparse LiDAR line patterns, Extreme: extremely sparse points (100 points), Range: missing depth within specific range, Shape: missing regular-shaped areas, Object: missing depth of an object. propose Prior Depth Anything, motivated by the complementary advantage between predicted and measured depth maps, as illustrated in Fig. 1. Technically, we design coarse-to-fine pipeline to explicitly and progressively combine the depth prediction with measured depth prior, achieving impressive robustness to any image with any prior. We first introduce coarse metric alignment to pre-fill incomplete depth priors using predicted relative depth maps, which effectively narrows the domain gap between various prior types. Next, we apply the fine structure refinement to rectify misaligned geometric structures in the pre-filled depth priors caused by inherent noises in the depth measurements. Specifically, the pre-filled depth prior (with accurate metric data) and the relative depth prediction (with fine details and structure) are provided as additional inputs to conditioned MDE model. Guided by the RGB image input, the model can combine the strengths of two complementary depth sources for the final output. We evaluate our model on 7 datasets with varying depth priors. It achieves zero-shot depth completion, superresolution, and inpainting within single model, matching or outperforming previous models that are specialized for only one of these tasks. More importantly, our model achieves significantly better results when different depth priors are mixed, highlighting its effectiveness in more practical and varying scenarios. Our contribution can be summarized as follows: We propose Prior Depth Anything, unified framework to estimate fine-detailed and complete metric depth with any depth priors. Our model can seamlessly handle zeroshot depth completion, super-resolution, inpainting, and adapt to more varied real-world scenarios. We introduce coarse metric alignment to pre-fill depth priors, narrowing the domain gap between different types of depth prior and enhancing the models generalization. We design fine structure refinement to alleviate the inherent noise in depth measurements. This involves conditioned MDE model to granularly merge the pre-filled depth prior and prediction based on image content. Our method exhibits superior zero-shot results across various datasets and tasks, surpassing even state-of-the-art methods specifically designed for individual tasks. 2. Related Work 2.1. Monocular Depth Estimation Monocular depth estimation (MDE) is fundamental computer vision task that predicts the depth of each pixel from single color image [2, 15, 17]. Recently, with the success of foundation models [5], some studies [4, 21, 27, 34, 54 56, 58] have attempted to build depth foundation models by scaling up data and using stronger backbones, enabling them to predict detailed geometric structures for any image. MiDaS [34] made the pioneering study by training an MDE model on joint datasets to improve generalization. Following this line, Depth Anything v1 [55] scaled training with massive unlabeled image data, while Depth Anything v2 [56] further enhanced its ability to handle fine details, reflections, and transparent objects by incorporating highly precise synthetic data [6, 36, 48, 50, 57]. Although these methods have demonstrated high accuracy and robustness, they primarily produce unscaled relative depth maps due to the significant scale differences between indoor and outdoor scenes. While Metric3D [25, 58] and Depth Pro [4] achieve zero-shot metric depth estimation through canonical camera transformations, the precision remains limited compared to measurement technologies. Our method builds on the strength of existing depth foundation models, which excel at precisely capturing relative geometric structures and fine details in any image. By progressively integrating accurate but incomplete metric information in the depth measurements, our model can generate dense and detailed metric depth maps for any scene. 2.2. Prior-based Monocular Depth Estimation In practical applications, depth measurement methods like multi-view matching [12] or sensors [19, 42] can provide accurate metric information, but due to their inherent nature or cost limitations, these measurements often capture incomplete information. Some recent studies have tried to use this measurement data as prior knowledge in the depth estimation process to achieve dense and accurate metric depth. These methods, however, primarily focus on specific patterns of depth measurement, which can be categorized into three types based on their input patterns: Depth Completion As noted in [37], SfM reconstructions from 19 images often result in depth maps with only 0.04% valid pixels. Completing the sparse depth maps with observed RGB images is fundamental computer vision task [8, 9, 33, 44, 61, 65]. Recent approaches like OmniDC [66] and Marigold-DC [47] have achieved certain levels of zero-shot generalization across diverse scenes and varying sparsity levels. However, due to the lack of explicit scene geometry guidance, they face challenges in extremely sparse scenarios. Depth Super-resolution Obtaining high-resolution metric depth maps with depth cameras usually demands significant power. more efficient alternative is to use low-power sensors to capture low-resolution maps and then enhance them using super-resolution. Early efforts [23, 53, 62, 64], however, show limited generalization to unseen scenes. Recent PromptDA [29] achieves effective zero-shot superresolution by using the low-resolution map as prompt for depth foundation models [56]. Depth Inpainting As discussed in [27, 56], due to inherent limitations in stereo matching and depth sensors, even ground truth depth data in real-world datasets often have significant missing regions. Additionally, in applications like 3D Gaussian editing and generation [10, 31, 59], there is need to fill holes in depth maps. DepthLab [30] first fills holes using interpolation and then refines the results with depth-guided diffusion model. However, interpolation errors reduce its effectiveness for large missing areas or incomplete depth ranges. These previous methods have two main limitations: 1) Poor performance when prior is limited. 2) Difficulty generalizing to unseen prior patterns. Our approach, Prior Depth Anything, tackles these challenges by explicitly using geometric information from depth prediction in coarse-to-fine process, achieving impressive generalization and accuracy across various patterns of prior input. 3. Prior Depth Anything Advanced monocular depth estimation models excel in predicting detailed, complete relative depth maps with precise geometric structures for any image. In contrast, depth measurement technologies can provide metric depth maps but suffer from inherent noise and varying patterns of incompleteness. Inspired by the complementary strengths of estimated and measured depth, we introduce Prior Depth Anything to progressively and effectively merge the two depth sources. To handle diverse real-world scenarios, we take measurement depth in any form as the metric prior, producing fine-grained and complete metric depth maps for any image with any prior. 3 3.1. Preliminary R3HW and its corresponding Given an RGB image RHW , prior-based monocumetric depth prior Dprior lar depth estimation takes the and Dprior as input, aiming RHW that is detailed, to output the depth map Doutput complete, and metrically precise. As discussed in 1, depth priors obtained by different measurement techniques often display various forms of incompleteness. To handle various priors with unified framework, we uniformly represent the coordinates of valid positions in Dprior as = i=0, which pixels are valid. xi, yi} { 3.2. Coarse Metric Alignment As shown in Fig. 2, different types of depth priors exhibit distinct missing patterns (e.g. sparse points, low-resolution grids, or irregular holes). These differences in sparsity and incompleteness restrict models ability to generalize across various priors. To tackle this, we propose pre-filling missing regions to transform all priors into shared intermediate domain, reducing the gap between them. However, interpolation-based filling, used in previous methods [29, 30], preserves pixel-level metric information but ignores geometric structure, leading to significant errors in the filled areas. On the other hand, global alignment [10, 11], which scales relative depth predictions to match priors, maintains the fine structure of predictions but loses critical pixel metric details. To address these challenges, we propose pixel-level metric alignment, which aligns geometry predictions and metric priors at pixel level, preserving both predicted structure and original metric information. Pixel-level Metric Alignment We first use frozen MDE RHW . model to obtain relative depth prediction Dpred Then, by explicitly utilizing the accurate geometric structure in the predicted depth, we fill the invalid regions in Dprior pixel by pixel. Considering the pre-filled coarse depth map ˆDprior, which inherits all the valid pixels in Dprior: ˆDprior(x, y) = Dpred(x, y), where (x, y) (1) xk, yk} { For each missing pixel (ˆx, ˆy), we first identify its closK k=1 from the valid pixel set using est valid points k-nearest neighborhood (kNN). Then, we compute the optimal scale and shift parameter that minimizes the leastsquares error between the depth values of Dpred and Dprior at the supporting points: (cid:88) s,t k=1 s, = arg min Dpred(xk, yk) + Dprior(xk, yk) (2) With the optimal scale and shift t, we fill the missing pixels in ˆDprior by linearly aligning the predicted depth 2 Figure 2. Prior Depth Anything. Considering RGB images, any form of depth prior Dprior, and relative prediction Dpred from frozen MDE model, coarse metric alignment first explicitly combines the metric data in Dprior and geometry structure in Dpred to fill the incomplete areas in Dprior. Fine structure refinement implicitly merges the complementary information to produce the final metric depth map. value at (ˆx, ˆy) to metric prior: 3.3. Fine Structure Refinement ˆDprior(ˆx, ˆy) = Dpred(ˆx, ˆy) + (3) Distance-aware Re-weighting Our pilot study shows that simple pixel-level metric alignment achieves reasonable accuracy and generalization. However, two limitations remain: 1) Discontinuity Risk: Adjacent pixels in missing regions may select different k-nearest neighbors, resulting in abrupt depth changes. 2) Uniform Weighting: Nearby supporting points offer more reliable metric cues than distant ones, but equal weighting in the least squares overlooks this geometrical correlation, leading to suboptimal alignment. To handle this issue, we further introduce distance-aware weighting for more smooth and accurate alignment. Within the alignment objective of Eq. 2, we re-weight each supporting point based on its distance to the query pixel, modifying Eq. 2 to: s, = arg min s,t (cid:88) k=1 Dpred(xk, yk) + (ˆx, ˆy) (xk, yk) 2 Dprior(xk, yk) 2 (4) This simple modification ensures smoother transitions between regions and improves robustness by emphasizing geometrically closer measurements. In summary, by explicitly integrating the accurate metric information from Dprior and the fine geometric structures from Dpred, we cultivate the pre-filled dense prior ˆDprior, which offers two main advantages: 1) Similar Pattern: Filling the missing area narrows the differences between various prior types, improving the generalization across different scenarios. 2) Fine Geometry: The filled regions, derived from linear transformations of depth prediction, natively preserve the fine geometric structures, significantly boosting performance when prior information is limited. 4 Although the prefilled coarse dense depth is generally accurate in metric, the parameter-free approach is sensitive to noise in depth priors. single noisy pixel on blurred edges can disrupt all filled regions relying on it as supporting point. To tackle these errors, we further implicitly leverage the MDE models ability in capturing precise geometric structures in RGB images, learning to rectify the noise in priors and produce refined depth. Metric Condition Specifically, we incorporate the prefilled prior ˆDprior as extra conditions to the pre-trained MDE model. With the guidance of RGB images, the conditioned MDE model is trained to correct the potential noise and error in ˆDprior. To this end, we introduce condition convolutional layer parallel to the RGB input layer, as shown in Fig. 2. By initializing the condition layer to zero, our model can natively inherit the ability of pre-trained MDE model. Geometry Condition In addition to leveraging the MDE models inherent ability in capturing geometric structures from RGB input, we also incorporate existing depth predictions as an external geometry condition to help refine the coarse pre-filled prior. The depth prediction Dpred, obtained from the frozen MDE model, is also passed into the condition MDE model through zero-initialized conv layer. Scale Normalization Then, we normalize the metric condition ˆDprior and geometry condition Dpred to [0,1] for two key benefits: 1) Better Scene Generalization: different scenes (e.g. indoor vs. outdoor) have significant depth scale differences. Normalization removes this scale variance, improving performance across diverse scenes. 2) Better MDE Model Generalization: predictions from different frozen MDE models also have varying scales. Normalizing Dpred enables test-time model switching, offering flexible accuracy-efficiency trade-offs for diverse demands, and enabling seamless improvements as MDE models advance. Model Encoder DAv2 Depth Pro Omni-DC Marigold-DC DepthLab PromptDA ViT-L ViT-L - SDv2 SDv2 ViT-L NYUv ScanNet ETH-3D DIODE KITTI S+M L+M S+L S+M L+M S+L S+M L+M S+L S+M L+M S+L S+M L+M S+L 4.59 4. 4.95 4.87 5.08 5.41 4.34 4.22 4.55 4.44 4.62 4.51 6.82 10.99 7.49 11.57 10.23 12.46 10.20 11.12 10.97 9.18 6. 9.29 12.92 8.42 8.91 9.24 7.08 6.29 3.26 3.38 3. 9.02 8.63 5.40 4.17 2.86 7.25 9.67 10.05 4.77 2.26 6.33 8.62 39.29 23.12 30.96 4.70 17.00 3.76 17.13 15.27 4.21 15.44 18.34 9.01 18.73 18.24 9.97 18.55 21.61 54.35 22.14 2.76 2.87 3.38 3.64 3.37 5.24 4.23 4.98 8.83 3.81 3.82 5.80 2.88 2.19 5. 4.80 6.73 6.40 4.59 5.13 7.45 2.09 2.15 7.87 4.36 5.82 PriorDA (ours) 2.09 DAv2-B+ViT-S DAv2-B+ViT-B 2.04 Depth Pro+ViT-B 2. 2.88 2.82 2.82 3.17 3.09 3.08 2.14 2.11 2.06 2.56 2.50 2.48 2.94 2.86 2.82 1.65 1.56 1. 3.96 3.84 3.83 4.16 4.08 4.05 3.76 3.62 3.40 4.43 4.30 4.23 4.89 4.73 4.60 3.97 3.86 3. 8.38 8.40 8.12 8.53 8.57 8.25 Table 2. Zero-shot depth estimiation with mixed prior. All results are reported in AbsRel . S: Extreme setting in sparse points, L: 16 in low-Resolution, M: Shape (square masks of 160) in missing area. We highlight Best, second best results. Depth Pro+ViT-B indicates the frozen MDE and conditioned MDE. DAv2-B: Depth Anything v2 ViT-B [56], SDv2: Stable Diffusion v2. [38]. Model Encoder DAv2 Depth Pro Omni-DC Marigold-DC DepthLab PromptDA ViT-L ViT-L - SDv2 SDv2 ViT-L NYUv2 ScanNet ETH-3D DIODE KITTI SfM LiDAR Extreme SfM LiDAR Extreme SfM LiDAR Extreme SfM LiDAR Extreme SfM LiDAR Extreme 5.31 4.80 4.85 4.47 4.77 4. 5.88 5.27 4.60 4.12 4.68 4.23 5.84 5.68 7.41 5.31 6.61 6. 12.45 12.55 10.53 9.08 14.37 8.98 9.83 6.45 8.86 6.05 2.12 2.87 2.65 1.90 4.30 5.92 18.68 17.59 2.02 6.09 2.63 4.32 2.13 1.76 3.56 9.87 6.30 16.96 18.13 15. 1.88 2.71 2.57 4.65 2.27 2.12 13.82 6.40 5.09 15.18 25.02 18.86 3.96 1.98 4.99 5.12 8.41 2.03 16.67 7.45 8.01 18.18 25.46 18.58 5.27 4.13 3.34 6.19 6.88 4.77 25.91 37.17 8.66 17.93 22.26 21.96 9.25 6.19 4.17 5.62 40.29 21.39 PriorDA (ours) DAv2-B+ViT-S 2.42 DAv2-B+ViT-B 2.38 Depth Pro+ViT-B 2.36 2.09 2.04 2.01 Table 3. Zero-shot depth completion (e.g. sparse points prior). All results are reported in AbsRel . SfM: points sampled with SIFT [32] and ORB [39], LiDAR: 8 LiDAR lines, Extreme: 100 random points. 3.76 3.76 3.28 3.65 3.64 3.37 2.01 1.97 1.96 3.88 3.77 3. 4.81 4.73 4.12 1.61 1.50 1.54 1.90 1.82 1.84 2.19 2.15 2.14 2.01 1.95 1.96 3.73 3.74 3. 5.25 5.16 5.07 3.90 3.85 3.84 3.26 3.10 3.13 Synthetic Training Data As discussed in [27, 56], real depth datasets often face issues such as blurred edges and missing values. Therefore, we leverage synthetic datasets, Hypersim [36] and vKITTI [6], with precise GT to drive our conditioned MDE model to rectify the noise in measurements. From the precise ground truth, we randomly sample sparse points, create square missing areas, or apply downsampling to construct different synthetic priors. To mimic real-world measurement noise, we add outliers and boundary noise to perturb the sampled prior, following [66]. Learning Objective As mentioned earlier, both the metric and geometry conditions are normalized. Thus, we apply the de-normalization transformation to convert the output into the ground truth scale. Following ZoeDepth [3], we use the scale-invariant log loss for pixel-level supervision. 3.4. Implementation Details Network Design During training, we utilize the Depth Anything V2 ViT-B model as the frozen MDE model to produce relative depth predictions. During inference, the frozen MDE model can be swapped with any other pretrained model. The k-value of the kNN process in Sec. 3.2 is set to 5. We initialize the conditioned MDE Model with two versions of Depth Anything V2: ViT-S and ViT-B. Training Setting We train the conditioned MDE model for 200K steps with batch size of 64, using 8 GPUs. The AdamW optimizer with cosine scheduler is employed, where the base learning rate is set to 5e-6 for the MDE encoder and 5e-5 for the MDE decoder. 4. Experiment 4.1. Experimental Setting Benchmarks Our method aims to provide accurate and complete metric depth maps in zero-shot manner for any image with any prior. To cover any image, we evaluate models on 7 unseen real-world datasets, including NYUv2 [42] and ScanNet [13] for indoor, ETH3D [41] and DIODE [46] for indoor/outdoor, KITTI [18] for outdoor, ARKitScenes [1] and RGB-D-D [23] for captured low-resolution depth. To cover any prior, we construct 9 individual patterns: sparse points (SfM, LiDAR, Extremely sparse), low-resolution (captured, x8, x16), and missing areas (Range, Shape, Object). We also mix these patterns to simulate more complex scenarios. Baselines We compare with two kinds of methods: 1) Post-aligned MDE: Depth Anything v2 (DAv2) [56] and Depth Pro [4]; and 2) Prior-based MDE: Omni-DC [66], Marigold-DC [47], DepthLab [30] and PromptDA [29]. Model Encoder DAv2 Depth Pro Omni-DC Marigold-DC DepthLab PromptDA ViT-L ViT-L - SDv2 SDv2 ViT-L PriorDA (ours) DAv2-B+ViT-S DAv2-B+ViT-B Depth Pro+ViT-B ARKitScenes RGB-D-D NYUv2 ScanNet ETH-3D DIODE KITTI AbsRel 3.67 3.25 2.14 2.17 2.10 1. 2.09 1.94 1.95 RMSE 0.0764 0.0654 0.0435 0.0448 0.0411 0.0347 0.0414 0.0404 0.0408 AbsRel 4.67 4.28 2.09 2.15 2.13 2.79 2.07 2.02 2.02 RMSE 0.1116 0.1030 0.0685 0.0672 0.0624 0.0708 0.0597 0.0581 0. 16 8 16 8 16 8 4.77 5.13 4.64 4.85 6.27 7.38 12.49 11.20 8.33 4.48 4.83 4.17 4.40 5.88 6.79 8.20 8 16 1.57 3.11 1.29 2.65 1.86 4.09 1.83 3.32 1.63 2.83 2.33 4.75 2.60 3.73 1.89 3.19 2.60 4.50 1.61 1.75 1.87 1.93 1.80 2.56 1.73 2.79 1.60 2.50 2.06 3.91 1.72 2.73 1.61 2.45 2.00 3.79 1.72 2.74 1.58 2.43 1.99 3.77 2.81 4.28 4.42 3.18 3.09 3.10 3. 4.71 6.60 6.16 3.73 4.36 4.23 4.15 8 9.54 6.76 16 11.22 9. 8.35 4.05 5.17 9.47 17.17 22.90 4.95 3.92 4.54 4.65 4.44 8.20 8.24 7.99 Table 4. Zero-shot depth super-resolution (e.g. low-resolution prior). ARKitScenes and RGB-D-D provide captured low-resolution depth. For other datasets, results are reported in AbsRel , with low-resolution maps created by downsampling the GT depths. Model Encoder DAv2 Depth Pro Omni-DC Marigold-DC DepthLab PromptDA ViT-L ViT-L - SDv2 SDv2 ViT-L NYUv2 ScanNet ETH-3D DIODE KITTI Range Shape Object Range Shape Object Range Shape Object Range Shape Object Range Shape Object 17.40 5.24 10.89 9.20 6.56 6. 16.75 4.64 6.74 16.76 15.39 6.80 68.76 8.23 19.22 51.55 29.20 13.41 31.12 14.93 17.94 10.37 34.28 17.28 37.44 34.74 13.53 14.51 16.11 8.19 29.47 4.81 17.97 38.83 7.75 25.43 35.42 8.94 15.06 23.24 5.94 13.79 22.89 5.44 19.83 2.37 7.72 25.36 2.15 17.14 1.97 6.18 23.85 2.66 10.87 21.17 2.08 10.40 30.61 2.75 10.53 41.01 6.51 17.17 40.43 13.60 18.66 36.67 20.88 23.14 35.86 17.87 21.89 46.21 24.94 27.42 49.50 25.66 28.29 55.79 32.74 38.29 39.33 7.59 18.97 33.44 9.21 8.71 6.66 7. PriorDA (ours) DAv2-B+ViT-S 16.86 2.30 DAv2-B+ViT-B 16.61 2.30 Depth Pro+ViT-B 16.31 2.17 5.87 5.73 5.87 Table 5. Zero-shot depth inpainting (e.g. missing area prior). All results are reported in AbsRel . Metrics are calculated only on the masked (inpainted) regions. Range: masks for depth beyond 3m (indoors) and 15m (outdoors), Shape: average result for square masks of sizes 80, 120, 160, and 200, Object: object segmentation masks detected by YOLO [26]. 36.59 5.58 10.77 30.04 6.67 36.64 5.94 30.79 6.29 9.72 34.90 4.86 11.99 30.44 5.47 21.16 1.98 21.90 1.76 22.72 1.76 14.29 2.01 14.48 1.99 14.18 1. 7.99 7.52 6.04 5.72 5.49 5.59 6.52 6.09 6.21 4.2. Comparison on Mixed Depth Prior We quantitatively evaluate the ability to handle challenging unseen mixed priors in Tab 2. In terms of absolute performance, all versions of our model outperform compared baselines. More importantly, our model is less impacted by the additional patterns. For example, compared to the setting that only uses sparse points in Tab 3, adding missing areas or low-resolution results in only minor performance drops (1.962.01, 3.08 in NYUv2). In contrast, Omni-DC (2.632.86, 3.81), and Marigold-DC (2.132.26, 3.82) show larger declines. These results highlight the robustness of our method to different prior inputs. 4.3. Comparison on Individual Depth Prior Zero-shot Depth Completion Tab 3 shows the zero-shot depth completion results with different kinds and sparsity levels of sparse points as priors. Compared to OmniDC [66] and Marigold-DC [47], which are specifically designed for depth completion and rely on sophisticated, timeconsuming structures, our approach achieves better overall performance with simpler and more efficient designs. Zero-shot Depth Super-resolution In Tab 4, we present results for super-resolution depth maps. On benchmarks where low-resolution maps are created through downsampling (e.g. NYUv2 [42], ScanNet [13], etc.), our approach achieves performance comparable to state-of-the-art methods. However, since downsampling tends to include overly specific details from the GT depths, directly replicating noise and blurred boundaries from GT leads to better results instead. Therefore, ARKitScenes [1] and RGB-D-D [23] are more representative and practical, as they use low-power cameras to capture the low-resolution depths. On these two benchmarks, our method achieves leading performance compared to other zero-shot methods. Zero-shot Depth Inpainting In Tab 5, we evaluate the performance of inpainting missing regions in depth maps. In the practical and challenging Range setting, our method achieves superior results, which is highly meaningful for improving depth sensors with limited effective working ranges. Additionally, it outperforms all alternatives in filling square and object masks, demonstrating its potential for 3D content generation and editing. 4.4. Qualitative Analysis In Fig 3, we provide qualitative comparison of the outputs from different models. Our model consistently outperforms previous approaches, offering richer details, sharper boundaries, and more accurate metrics. 6 Figure 3. Qualitative comparisons with previous methods. The depth prior or error map is shown below each sample. Figure 4. Error analysis on widely used but indeed noisy benchmarks [13, 42]. Red means higher error, while blue indicates lower error. Fig 4 visualizes the error maps of our model. The errors mainly occur around blurred edges in the ground truth of real data. Our method effectively corrects the noise in labels while aligning with the metric information from the prior. These beyond ground truth cases highlight the potential of our approach in addressing the inherent noise in depth measurement techniques. More visualizations can be found in the supp. M S+M L+M S+L"
        },
        {
            "title": "Encoder",
            "content": "S S+M L+M S+L Interpolation Ours (w/o re-weight) Ours 7.93 2.92 2.42 3.88 3.44 3. 8.96 6.91 6.70 8.38 3.22 2.60 4.55 4.53 4.32 7.99 4.36 4.40 Table 6. Accuracy of pre-filled depth maps with different strategies. To independently compare each pre-fill strategy, we directly compare the pre-filled maps with ground truth."
        },
        {
            "title": "Unseen",
            "content": "Depth Anything V2 ViT-S 2.15 2.77 2.68 2.22 ViT-B 1.97 2.73 2.50 2.02 ViT-L 1.92 2.71 2.29 1.97 ViT-G 1.87 2.70 2.22 1.94 2.87 3.20 2.82 3.09 2.79 3.04 2.76 3."
        },
        {
            "title": "Depth Pro",
            "content": "ViT-L 1.96 2.74 2.35 2.01 2.82 3.08 Table 9. Effect of using different frozen MDE models. The conditioned MDE model is ViT-B version here. M S+M L+M S+L Model Encoder Param Latency(ms)"
        },
        {
            "title": "None\nInterpolation",
            "content": "2.50 3.71 46.07 2.50 3.53 3.40 2.68 2.19 Ours (w/o re-weight) 2.13 2.86 1.99 2.82 2.06 4.28 2.58 2."
        },
        {
            "title": "Ours",
            "content": "3.74 3.64 2.94 3.56 2.94 3.25 2.90 3.11 Table 7. Effect of pre-filled strategies on generalization. We train models with various pre-fill strategies using only sparse points and evaluate their ability to generalize to unseen types of depth priors. Metric Geometry S+M L+M S+L 5.46 2.10 1.96 5.29 2.94 2.74 5.48 2.58 2.48 5.36 2.17 2. 5.30 3.02 2.82 5.46 3.31 3.08 Table 8. Effect of each condition for conditioned MDE models. 4.5. Ablation Study We use Depth Anything V2 ViT-B as the frozen MDE and ViT-S as the conditioned MDE for ablation studies by default. All results are evaluated on NYUv2. Accuracy of different pre-fill strategy As shown in Fig 6, our pre-fill method outperforms simple interpolation across all scenarios by explicitly utilizing the precise geometric structures in depth prediction. Additionally, the reweight mechanism further enhances performance. Pre-fill strategy for generalization From Tab 7, we observe that our pixel-level metric alignment helps the model generalize to new prior patterns, and the re-weighting strategy further enhances the robustness by improving the accuracy of the pre-filled depth map. Effectiveness of fine structure refinement Comparing the pre-filled coarse depth maps in Tab 6 with the final output accuracy in Tab 3, 4, 5 and 2, the performance improvements after fine structure refinement (sparse points: 2.422.01, low-resolution: 3.512.79, missing areas: 6.702.48, S+M: 2.602.09, L+M: 4.322.88, S+L: 4.403.17) demonstrate its effectiveness in rectifying misaligned geometric structures in the pre-filled depth maps while maintaining its accurate metric information. Effectiveness of metric and geometry condition We evaluate the impact of metric and geometry guidance for the conditioned MDE model in Tab 8. The results show that combining both conditions achieves the best performance, emphasizing the importance of reinforcing geometric information during the fine structure refinement stage. Omni-DC Marigold-DC DepthLab PromptDA PriorDA (ours) - SDv2 SDv2 ViT-L 85M 1,290M 2,080M 340M 334 30,634 9,310 32 97M+25M 157 19+123+15 DAv2-B+ViT-S DAv2-B+ViT-B 97M+98M 161 19+123+19 Depth Pro+ViT-B 952M+98M 760 618+123+ Table 10. Analysis of inference efficiency. x+x+x represents the latency of the frozen MDE model, coarse metric alignment, and conditioned MDE model, respectively. Testing-time improvement We investigate the potential of test-time improvements in Tab 9. Our findings reveal that larger and stronger frozen MDE models consistently bring higher accuracy, while smaller models maintain competitive performance and enhance the efficiency of the entire pipeline. These findings underscore the flexibility of our model and its adaptability to various scenarios. In Tab 10, we analyze the Inference efficiency analysis inference efficiency of different models on one A100 GPU 640. Overall, compared for an image resolution of 480 to previous approaches, our model variants achieve leading performance while demonstrating certain advantages in parameter number and inference latency. For more detailed breakdown, we provide the time consumption for each stage of our method. The coarse metric alignment, which relies on kNN and least squares, accounts for the majority of the inference latency. However, it still demonstrates significant efficiency advantages compared to the sophisticated OmniDC and diffusion-based DepthLab and Marigold-DC. 5. Application To demonstrate our models real-world applicability, we employ prior-based monocular depth estimation models to refine the depth predictions from VGGT, state-of-the-art 3D reconstruction foundation model. VGGT provides both depth and confidence map. We take the top 30% most confident pixels as depth prior and apply different prior-based models to obtain finer depth predictions. 1 Table 11 reports VGGTs performance in monocular and multi-view depth estimation, along with the effectiveness 1For models less adept at handling missing pixels (DepthLab, PromptDA), the entire VGGT depth prediction was provided as prior."
        },
        {
            "title": "Monocular Depth Estimation",
            "content": "Multi-view Depth Estimation"
        },
        {
            "title": "NYU",
            "content": "3.54 (-) +Omni-DC +Marigold-DC +DepthLab +PromptDA +PriorDA 4.12 (0.58) 4.06 (0.52) 3.56 (0.02 ) 3.43 (-0.11) 3.45 (-0.09) ETH-3D 4.94 (-) 6.08 (1.14) 5.43 (0.49) 4.92 (-0.02) 4.97 (0.03) 4.43 (-0.51)"
        },
        {
            "title": "KITTI",
            "content": "6.56 (-) 6.85 (0.29) 7.63 (1.07) 7.97 (1.41) 6.50 (-0.06) 6.39 (-0.17) ETH-3D 2.46 (-) 2.64 (0.18) 2.81 (0.35) 2.25 (-0.21) 2.48 (0.02) 1.99 (-0.47)"
        },
        {
            "title": "KITTI",
            "content": "18.75 (-) 18.66 (-0.09) 18.86 (0.11) 19.47 (0.72) 18.91 (0.16) 18.61 (-0.14) Table 11. Results of refining VGGT depth prediction with different methods. All results are reported as AbsRel. of different prior-based methods as refiners. We observe that only our PriorDA consistently improves VGGTs predictions, primarily due to its ability to adapt to diverse priors. These surprising results highlight PriorDAs broad application potential. 6. Conclusion In this work, we present Prior Depth Anything, robust and powerful solution for prior-based monocular depth estimation. We propose coarse-to-fine pipeline to progressively integrate the metric information from incomplete depth measurements and the geometric structure from relative depth predictions. The model offers three key advantages: 1) delivering accurate and fine-grained depth estimation with any type of depth prior; 2) offering flexibility to adapt to extensive applications through test-time module switching; and 3) showing the potential to rectify inherent noise and blurred boundaries in real depth measurements."
        },
        {
            "title": "References",
            "content": "[1] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. In NeurIPS, 2021. 5, 6 [2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In CVPR, 2021. 2 [3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv:2302.12288, 2023. 5 [4] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. In ICLR, 2025. 1, 2, 5 [5] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv:2108.07258, 2021. 2 [6] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv:2001.10773, 2020. 2, 5 [7] Manuel Carranza-Garcıa, Javier Galan-Sales, Jose Marıa Luna-Romera, and Jose Riquelme. Object detection using depth completion and camera-lidar fusion for autonomous driving. Integrated Computer-Aided Engineering, 2022. 1 [8] Xinjing Cheng, Peng Wang, and Ruigang Yang. Learning depth with convolutional spatial propagation network. TPAMI, 2019. 3 [9] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang. Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion. In AAAI, 2020. 3 [10] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3 [11] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d gaussian splatting in few-shot images. In CVPR, 2024. 1, 3 [12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. [13] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 5, 6, 7 [14] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In CVPR, 2022. 1 [15] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. In NeurIPS, 2014. 2 [16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In ICCV, 2023. 1 [17] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In CVPR, 2018. 2 [18] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. [19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel 9 Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 2013. 2 [20] Christian Hane, Lionel Heng, Gim Hee Lee, Friedrich Fraundorfer, Paul Furgale, Torsten Sattler, and Marc Pollefeys. 3d visual perception for self-driving cars using multi-camera system: Calibration, mapping, localization, and obstacle detection. Image and Vision Computing, 2017. 1 [21] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and YingCong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. In ICLR, 2025. 1, 2 [22] Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie Zhang, Chunyu Lin, Meiqin Liu, and Yao Zhao. Towards fast and accurate real-world depth superresolution: Benchmark dataset and baseline. In CVPR, 2021. 1 [23] Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie Zhang, Chunyu Lin, Meiqin Liu, and Yao Zhao. Towards fast and accurate real-world depth superresolution: Benchmark dataset and baseline. In CVPR, 2021. 3, 5, [24] Daniel Herrera, Juho Kannala, and Janne Heikkila. Joint depth and color camera calibration with distortion correction. TPAMI, 2012. 1 [25] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. TPAMI, 2024. 1, 2 [26] Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, 2023. 6 [27] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 1, 2, 3, [28] Robert Lange and Peter Seitz. Solid-state time-of-flight range camera. IEEE Journal of quantum electronics, 2001. 1 [29] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In CVPR, 2025. 2, 3, 5 [30] Zhiheng Liu, Ka Leong Cheng, Qiuyu Wang, Shuzhe Wang, Hao Ouyang, Bin Tan, Kai Zhu, Yujun Shen, Qifeng Chen, and Ping Luo. Depthlab: From partial to complete. arXiv:2412.18153, 2024. 2, 3, 5 [31] Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, and Yang Infusion: Inpainting 3d gaussians via learning depth Cao. completion from diffusion prior. arXiv:2404.11613, 2024. 1, 3 [32] David Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 2004. 1, [33] Jin-Hwi Park, Chanhwi Jeong, Junoh Lee, and Hae-Gon Jeon. Depth prompting for sensor-agnostic depth estimation. In CVPR, 2024. 3 [34] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 1, 2 [35] Alex Rasla and Michael Beyeler. The relative importance of depth cues and semantic edges for indoor mobility using simulated prosthetic vision in immersive virtual reality. In Proceedings of the 28th ACM symposium on virtual reality software and technology, 2022. 1 [36] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. 2, 5 [37] Barbara Roessle, Jonathan Barron, Ben Mildenhall, Pratul Srinivasan, and Matthias Nießner. Dense depth priors for neural radiance fields from sparse input views. In CVPR, 2022. 1, 3 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 5 [39] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In ICCV, 2011. 1, [40] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In CVPR, 2016. 1 [41] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highresolution images and multi-camera videos. In CVPR, 2017. 5 [42] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 2, 5, 6, 7 [43] Mel Slater and Sylvia Wilbur. framework for immersive virtual environments (five): Speculations on the role of presence in virtual environments. Presence: Teleoperators & Virtual Environments, 1997. [44] Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, and Ping Tan. Bilateral propagation network for depth completion. In CVPR, 2024. 3 [45] Yifu Tao, Marija Popovic, Yiduo Wang, Sundara Tejaswi Digumarti, Nived Chebrolu, and Maurice Fallon. 3d lidar reconstruction with probabilistic depth completion for robotic navigation. In IROS, 2022. 1 [46] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv:1908.00463, 2019. 5 [47] Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, and Anton Obukhov. Marigold-dc: Zero-shot monocular depth completion with guided diffusion. arXiv:2412.13389, 2024. 2, 3, 5, 6 [48] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Irs: large naturalisKaiyong Zhao, and Xiaowen Chu. tic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. In ICME, 2021. 2 10 [49] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. [65] Yiming Zuo and Jia Deng. Ogni-dc: Robust depth compleIn ECCV, tion with optimization-guided neural iterations. 2024. 3 [66] Yiming Zuo, Willow Yang, Zeyu Ma, and Jia Deng. Omnidc: Highly robust depth completion with multiresolution depth integration. arXiv:2411.19278, 2024. 2, 3, 5, 6 [50] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, 2020. 2 [51] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR, 2019. 1 [52] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, and Vivienne Sze. Fastdepth: Fast monocular depth estimation on embedded systems. In ICRA, 2019. 1 [53] Chuhua Xian, Kun Qian, Zitian Zhang, and Charlie CL Wang. Multi-scale progressive fusion learning for depth map super-resolution. arXiv:2011.11865, 2020. [54] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. In ICLR, 2025. 2 [55] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 1, 2 [56] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. 1, 2, 3, 5 [57] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: largescale dataset for generalized multi-view stereo networks. In CVPR, 2020. 2 [58] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In ICCV, 2023. 1, 2 [59] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv:2406.09394, 2024. 1, [60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 1 [61] Youmin Zhang, Xianda Guo, Matteo Poggi, Zheng Zhu, Guan Huang, and Stefano Mattoccia. Completionformer: Depth completion with convolutions and vision transformers. In CVPR, 2023. 3 [62] Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Zudi Lin, and Hanspeter Pfister. Discrete cosine transform network for guided depth map super-resolution. In CVPR, 2022. 3 [63] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3dvla: 3d vision-language-action generative world model. In ICML, 2024. 1 [64] Zhiwei Zhong, Xianming Liu, Junjun Jiang, Debin Zhao, and Xiangyang Ji. Guided depth map super-resolution: survey. ACM Computing Surveys, 2023. 3 11 of the Depth Anything v2 model exhibit stronger capabilities, training conditioned MDE models based on larger backbones is an important direction for future work. Additionally, following Depth Anything, all training images 518. In contrast, PromptDA is natively are resized to 518 trained at 1440 1920 resolution. Therefore, training at higher resolutions to better handle easily accessible highresolution RGB images is another crucial direction for our future research. S+M L+M S+L k=3 k=5 k=10 k=20 2.00 1.97 2.00 2. 2.52 2.16 2.31 2.27 2.74 2.73 2.74 2.76 2.10 2.04 2.09 2.16 2.83 2.82 2.83 2.83 3.07 3.09 3.12 3.14 Table 12. Impact of different k-value on the accuracy of the final output Doutput. M S+M L+M S+L"
        },
        {
            "title": "Only Sparse\nThree Pattern",
            "content": "1.99 1.96 2.82 2.74 2.26 2.48 2.06 2.01 2.90 2.82 3.11 3. Table 13. Impact of used prior patterns during training. A. More Ablation Study Effect of k-value in Coarse Metric Alignment In Tab 12, we analyze the impact of the k-value on the accuracy of the final output Doutput. Overall, our method is non-sensitive to the selection of k-value, with most selections yielding strong structural results. This highlights the effectiveness of the nearest neighbor approach in preserving detailed metric information. B. More Qualitative Results To further explore the boundaries of our models capabilities and its potential to rectify the ground truth depth, we offer more error analysis with different patterns of priors on the 7 unseen datasets (Figure 5 for RGB-D-D, Figure 6 for ARKitScenes, Figure 7 for NYUv2, Figure 8 for ScanNet, Figure 9 for ETH-3D, Figure 10 for DIODE, Figure 11 for KITTI). C. More Training Details For each training sample, we randomly select one of three patterns (e.g. sparse points, low-resolution, or missing areas) with equal probability to sample the depth prior from the ground truth depth map. Specifically: for sparse points, we randomly select 100 to 2000 pixels as valid; for lowresolution, we downsample the GT map by factor of 8; and for missing areas, we generate random square mask with side length of 160 pixels. It is worth mentioning that, we find that using multiple patterns or only using sparse points lead to similar results, as shown in Tab 13. This indicates that our methods ability to generalize to any form of prior stems from the coarse metric alignment, rather than the use of multiple patterns during training. D. Limitations and Future Works Currently, our largest conditioned MDE model is initialized with Depth Anything v2 ViT-B. Given that larger versions 12 Figure 5. Error analysis on RGB-D-D. Figure 6. Error analysis on ARKitScenes. 13 Figure 7. Error analysis on NYUv2. Figure 8. Error analysis on ScanNet. 14 Figure 9. Error analysis on ETH-3D. Figure 10. Error analysis on DIODE. 15 Figure 11. Error analysis on KITTI."
        }
    ],
    "affiliations": [
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}