{
    "paper_title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning",
    "authors": [
        "Junwei Luo",
        "Yingying Zhang",
        "Xue Yang",
        "Kang Wu",
        "Qi Zhu",
        "Lei Liang",
        "Jingdong Chen",
        "Yansheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 8 8 5 7 0 . 3 0 5 2 : r When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning Junwei Luo1, Yingying Zhang2, Xue Yang3, Kang Wu1, Qi Zhu4*, Lei Liang2, Jingdong Chen2, Yansheng Li1 1Wuhan Universtiy 2Ant Group 3Shanghai Jiao Tong University 4University of Science and Technology of China {luojunwei,kangwu,yansheng.li}@whu.edu.cn {qichu.zyy,leywar.liang,jingdongchen.cjd}@antgroup.com yangxue-2019-sjtu@sjtu.edu.cn zqcrafts@mail.ustc.edu.cn https://github.com/VisionXLab/LRS-VQA"
        },
        {
            "title": "Abstract",
            "content": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. 1. Introduction Benefiting from the rapid advancement of Large Language Models (LLMs) [1, 9, 12, 15, 67], Large Visual-Language Models (LVLMs) have demonstrated strong capabilities in *indicates interns at Ant Group. Corresponding author. Figure 1. High-resolution strategy comparison for modular LVLMs. (a) and (b) show existing grid-based cropping methods face challenges when processing large RSIs. (c) The proposed dynamic pyramid-based token pruning strategy can dynamically select image tiles of key regions related to the input text, balancing image detail and computational cost. perceiving and understanding visual information in textbased multimodal interactions [7, 25, 35, 57]. Currently, LVLMs have been extensively studied and applied across various fields like remote sensing (RS) intelligent interpretation [10, 28, 43, 46, 65, 75, 79]. Advances in satellite imaging technology allow for the acquisition of large remote sensing images (RSIs) that cover extensive ground areas and contain detailed land use information. Therefore, natural language-based intelligent analysis of large RSIs is valuable for applications such as com1 plex scene understanding [33], urban planning [31, 56], infrastructure development [62], and monitoring [85]. Some RS LVLMs handle high-resolution input through position embedding interpolation [24, 78]. Additionally, various dynamic high-resolution strategies designed for general LVLMs [29, 35, 72] can be adapted to RS LVLMs, such as EarthDial [53] (based on InternVL1.5 [8]) and GeoPixel [49] (using InternLM-XComposer2.5 [76]). However, these approaches for high-resolution image processing are limited when handling large RSIs exceeding 10,00010,000 pixels. As depicted in Fig. 1, these methods typically employ limited pre-defined grid partitions, leading to detail loss when large RSIs are excessively downsampled. Conversely, unlimited grids suffer from prohibitive time and memory costs (e.g., over 5 million vision tokens for large RSI in LLaVA-1.5 [35]). This necessitates balancing resolution and computational efficiency when processing large RSI. To overcome the above limitations of existing strategies in handling large RSIs, we propose text-guided token pruning strategy that consists of two key components: Region Focus Module (RFM) and Dynamic Image Pyramid (DIP). The RFM identifies text-relevant key vision tokens by leveraging the capability distilled from the LLM component. Based on the RFMs output, we select critical image tiles and conduct token pruning in coarse-to-fine manner within the DIP. This enables the LVLM to focus and zoom in key areas, avoiding to process all visual tokens, thereby facilitating flexible and efficient inference. Moreover, benchmarks for LVLMs in understanding large RSIs remain insufficiently developed. Although MME-Realworld [83] includes large RSIs with high-quality manually annotated questions, its RS subset suffers from limited question diversity and image sizes. To more comprehensively characterize the challenges of large RSI perception, we construct new benchmark called LRS-VQA (Large Remote Sensing image Visual Question Answering). LRS-VQA features larger size images with lengths up to 27,328 pixels and 8 distinct question types. Our key contributions are as follows: We introduce Region Focus Module (RFM) that distills the ability from LLM part of LVLM to efficiently identify text-related key vision tokens. Based on RFM and DIP, we propose an efficient coarseto-fine inference strategy for large RSIs, which achieves balance between resolution and efficiency through key tile selection and vision token pruning. We construct new benchmark named LRS-VQA, featuring larger image sizes and more diverse question types than existing benchmarks, to reflect the challenges of large RSIs perception. The proposed method is architecture-agnostic and shows performance improvements and efficiency gains. 2. Related Work 2.1. LVLMs for High-Resolution Comprehension Numerous LVLMs designed for high-resolution image or video perception have recently emerged. They can be primarily categorized into the following paradigms: Methods that carefully design image cropping and padding strategy. Methods like [8, 16, 25, 34, 35, 38, 72, 82] traverse pre-defined partition grids and obtain the most suitable grid, to divide the high-resolution image into image tiles. These methods encounter limitations when processing large RSIs as in Fig. 1. Methods based on multiple visual branches. These approach [30, 42, 55] employ additional high-resolution visual encoders like SAM [22] encoder or ConvNext [39] to process images with higher input resolution. Visual chain-of-thoughts methods. These approaches utilize search-and-focus strategy [51, 52, 58, 61] or external RAG pipeline [84] to identify text-related key image regions. However, such pipelines require multiple LVLM inferences, making them more complex and cumbersome. 2.2. Vision Token Reduction in LVLMs Visual token pruning for transformers has been classic research topic, it aims to accelerate computation by dynamIt has been extenically reducing less important tokens. sively studied in both natural language processing [21, 71] and computer vision domains [14, 23, 44, 48, 60]. Recently, variety of token pruning methods have been proposed for LVLMs, which can be categorized into three main types: token pruning conducted in the vision encoder [4, 29, 50, 59, 69, 70], token pruning performed in the LLM component [6, 17, 18, 64, 73, 80], and collaborative approaches that integrate both [5, 26, 27, 32, 66, 74, 77]. Although vision-part pruning methods efficiently reduce vision tokens, when processing large images, its highly time-consuming to traverse pre-defined grids and handle numerous image tiles with the vision encoder. Moreover, the absence of language guidance makes it hard to identify foreground regions in complex RSIs. The LLM-part pruning methods require feeding all vision tokens into the LLM. Therefore, long visual sequences from large images may exceed LLM length limits and incur high computational costs for attention score ranking. Among the collaborative approaches, the methods most closely related to ours are LLaVA-Mini [77] and FlexAttention [27]. The former employs modality pre-fusion to compress visual information; however, its LLM-like pre-fusion module still struggles with handling extremely long vision sequences. The latter integrates text-guided high-resolution features within the LLM component. Nevertheless, the allowable image size for its high-resolution view remains limited, and it can only index high-resolution features once. 2 Figure 2. The pipeline of the proposed method. The entire process iterates in coarse-to-fine manner, dynamically retrieving highresolution features from the next DIP level (leftward orange arrow) or performing token pruning at the current level (rightward orange arrow) based on the output of the RFM module at each iteration. During training, the RFM distillation text-related attention from the LLM; during inference, RFM generates the attention scores for the input vision tokens. GSD means ground sample distance. 3. Preliminaries In this section, we present the foundational concepts of text-image attention within the LLM component of highresolution LVLMs. The proposed RFM leverages this mechanism to guide the text-related key image tile selection and the vision token pruning. Mainstream modular LVLMs include three components [41]: pre-trained vision encoder, feature projector, and pre-trained decoder-only LLM. Conventional highresolution strategies [8, 16] process the original image Iimg into thumbnail view Ithumb and set of image tiles Itiles. Then the vision encoder and the projector transforms Ithumb and Itiles into vision token embeddings Tvis = [T lr vis], which contain low-resolution tokens lr vis from the Ithumb and higher-resolution hr vis from the Itiles. Tvis are concatenated with text token embeddings Ttxt from the tokenized text instruction, forming multimodal token sequence = [Tvis, Ttxt]. The LVLM processes via transformer-based decoder layers with causal self-attention, where the attention score can be represented as: vis, hr = Attention(Q, K) = Softmax ( QK ) , (1) where Rnn is the self-attention map, Q, Rnd are the query and key matrices derived from the input embeddings through linear transformations, is the input sequence length, and is the feature dimension. As determines the weight each token assigns to the former tokens in the sequence, by analyzing the attention from text tokens to vision tokens, we can identify text-related important vision tokens. Some studies [20, 73] observe that in the deep layers of LLM part in LVLMs, attention is predominantly allocated to text-related vision tokens compared to other vision tokens. This insight inspired us to design coarse-to-fine mechanism to focus on key image regions. 4. Method and Benchmark 4.1. Method To achieve efficient high-resolution image understanding, we propose method that combines text-guided key region localization with multi-level dynamic image pyramid (DIP), as shown in Fig. 2. Initially, the DIP is constructed based on the input large image. At the low-resolution DIP level, the Region Focus Module (RFM) provides attention distribution for the initial vision tokens. These result guides the retrieval of corresponding image tiles from Figure 3. The proposed RFM and attention distillation strategy. The left part indicates our core idea: distill accurate text-related key region localization ability from the LLM part of the LVLM. The right part shows the distillation details. We only select specific layer pairs for distillation to avoid hidden state discontinuities. sys token represents the tokens from the system prompt. higher-resolution DIP levels or trigger token pruning at the current level. This iterative process could continue through the pyramid until reaching the original resolution. Below, we introduce our approach in three components: DIP construction in Sec. 4.1.1, RFM and attention distillation in Sec. 4.1.2, and their integration in Sec. 4.1.3. 4.1.1. The Construction of Dynamic Image Pyramid Given an original large RSI Iimg, we construct an imagelevel pyramid with variable number of layers to generate series of image tiles at different ground sample distances (GSDs), enabling coarse-to-fine inference. First, we iteratively downsampling Iimg by factor of 2, generating series of images {I 1 init} until the shorter side reaches pre-defined minimum length. Let basic image tile size be (e.g., 336 336 for CLIP-L14), for the p-th scaled image init, we calculate the number of image tiles along the height and width as follows: init, . . . , init,"
        },
        {
            "title": "N p",
            "content": "h = Hp/B, where Hp, Wp are the height and width of tively. For proper tiling, we compute scale factor rp as: = Wp/B, init, respec- (2) rp = min ("
        },
        {
            "title": "N p",
            "content": "h Hp ,"
        },
        {
            "title": "N p",
            "content": "w Wp ) . (3)"
        },
        {
            "title": "The image I p",
            "content": "init is then resized to using rp, followed by padding to preserve the original aspect ratio. After resizing, is partitioned into non-overlap tiles tiles. Tiles from all scaled images are combined in reverse order, and integrated with the thumbnail view Ithumb to form the final 4 tiles, . . . , pyramid: IDIP = {Ithumb, 1 tiles, 2 tiles}, which consists of + 1 layers with progressively increasing resolutions. During training, we select the thumbnail Ithumb and 1 tiles as visual inputs for computational efficiency. During inference, all pyramid layers become accessible to enable multi-scale inference, as in the left part of Fig. 2. 4.1.2. Attention Distillation for Key Region Focus In this section, we introduce the core idea and details of the Region Focus Module (RFM), lightweight module designed to replicate the text-guided region localization capability inherent in LVLMs LLM part. RFM can be seamlessly integrated during the supervised fine-tuning (SFT) stage of LVLM, and generates attention scores for vision tokens during inference. Prior findings [20, 64, 73] demonstrate that the deep layers in the LLM part of LVLM could accurately localize textrelated key regions in the image through cross-model attention. Inspired by this observation, our core idea is to distill this text-guided key region focus capability from the LLM, as shown in the left part of Fig. 3. After distillation, the trained RFM can help to select text-related key image tiles and prune vision tokens before the LLM, reducing computational overhead from the image encoding stage. Specifically, assuming the LLM has layers l1, l2, . . . , lM , the proposed RFM adopts the same architecture with layers r1, r2, . . . , rR, where R<M . Each RFM layer ri is initialized from selected LLM layer lmi, where {m1, m2, . . . , mR} forms sparse subset of LLM layer indices. For layer-wise distillation, we select pairs (K<R) from RFM and LLM as student-teacher pairs, represented as {(rk, lmk ) = 1, 2, . . . , K}. This strategy avoids rigidly aligning each ri with lmi, which would neglect the non-teacher layers in the LLM, resulting in inconsistency between adjacent RFM layers when training. As depicted in the right part of Fig. 3, the concatenated multimodal tokens = [Tvis, Ttxt] are fed into both the RFM and the LLM during training. For the k-th studentteacher layer pair (rk, lmk ), we extract the attention scores between the last text token and all vision tokens from the multi-head self attention (MHSA). For the h-th head, the self-attention is represented as Ak,h tea Rjnv , where is the number of dialogue turns for multi-turn instruction and nv is the length of vision tokens. We apply the Kullback-Leibler (KL) divergence loss for distillation: stu , Ak,h k= Lh kl = [DKL (Ak,h tea Ak,h stu ) + λhrDKL (Ak,h,hr tea Ak,h,hr stu )] , tea , Ak,h,hr stu (4) where Ak,h,hr denote attention corresponding to hr vis, and λhr is the weighting factor. Additionally, we enforce stronger alignment constraints to the attention of hr vis by using an additional Mean Squared Error (MSE) loss: Lh mse = k=1 MSE (Ak,h,hr stu , Ak,h,hr tea ) . The total distillation loss is computed as: Ldistill ="
        },
        {
            "title": "1\nKH",
            "content": "H h=1 (λmseLh mse + λklLh kl) , (5) (6) Algorithm 1 Coarse-to-Fine Token Pruning with DIP Require: {I 1 tiles} in IDIP, K-layer RFM, initial multimodal tokens , vision encoder and projector V(), token saving ratio α, tile number threshold Nmax tiles, . . . , tiles, 2 Ensure: Retained tokens retain vis 1: Definition: Top-α(X) selects elements in with values in the top α proportion. 2: for = 1 to do 3: 4: 5: 6: 7: Compute attention: AK,hr Normalize: Identify key positions: K,hr if < then stu RFM(T )[T p,hr vis ] key Top-α(AK,hr stu ) key {t Map coordinates and select key tiles: p+1 p+1 tiles mapped from K,hr key } if p+1 key > Nmax then Prune tokens: retain break vis {t p,hr vis K,hr key } Encode selected tiles: p+1,hr Update [T lr vis, p+1,hr vis , Ttxt] vis V(I p+1 key ) Prune tokens at final layer: retain K,hr key } vis {t p,hr vis 8: 9: 10: 11: 12: 13: 14: 15: 16: else end if else end if 17: 18: end for 19: return retain vis where is the number of selected student-teacher layer pairs, is the number of attention heads, and λmse, λkl are hyperparameters. For the flash-attention [11], we employ specialized value matrix to extract attention maps like [80] to ensure compatibility. 4.1.3. Text-Guided Token Pruning with Pyramid Given the extensive irrelevant background content in large RSIs, the core of our strategy lies in iteratively selecting text-related key image tiles and performing token pruning, thereby achieving coarse-to-fine inference, as illustrated in the below part of Fig. 2. Specifically, we integrate DIP and RFM for inference, as detailed in Alg. 1. For simplicity, the tokens from the system prompt are omitted. We initialize the vision input using Ithumb and vis, p,hr tiles to generate the initial Tvis = [T lr vis ], where = 1, then the concatenated tokens are fed into the RFM. From the last layer of the RFM, we compute the average attention scores from all heads in the MHSA, and extract the attention scores AK,hr . By selecting stu the top-λ proportion from AK,hr , we identify the indices of stu key tokens and map their coordinates to image tile-level coordinates, to select key image tiles p+1 tiles . Based key from p+1 of p,hr vis directly or replace it with vision tokens p+1,hr on the number of p+1 key , we determine whether to prune the p,hr from vis vis higher-resolution tiles. If p+1,hr is obtained, the tile sevis lection process is repeated recursively until the last layer of DIP is reached. This approach enables LVLM to focus only on processing few high-resolution image tiles in coarseto-fine manner, thereby reducing computational complexity while preserving critical text-related image details. 4.2. The Construction of LRS-VQA Benchmark Considering the limitations of existing benchmarks for evaluating LVLMs perception of RSIs in terms of question diversity and image size, we propose new benchmark for more comprehensively evaluation. 4.2.1. Annotation Pipeline The annotation pipeline of LRS-VQA is illustrated in Fig. 4. We collect 3 publicly remote sensing datasets: FAIR1M1.0 [54], GLH-Bridge [31], and STAR [33], and identify all unique targets based on the object detection labels to generate unique references from them (e.g., the top-most ship and the largest dock). Subsequently, we crop the region with the unique target from the original image, and feed it, along with prompt containing the unique reference, into 5 Dataset MME-RealWorld-RS [83] LRS-VQA (Ours) Image Size 689-11500 1024-27328 Image Num Ques. Cate. Ques. Num 3 3738 7333 1298 1657 QS Format Single-choice Open-end Table 1. Comparison of existing benchmarks for evaluating LVLMs perception capabilities in large RSIs. Figure 5. The accuracy trends of Qwen2-VL across varying input maximum pixels. This demonstrates that accuracy on both the manually annotated MME-RealWorld-RS and our proposed LRSVQA exhibit positive correlation with resolution improvement, proving the effectiveness of LRS-VQA in evaluating LVLMs high-resolution RSI perception capabilities. 5. Experiments 5.1. Experimental Details Data. During the pre-training (PT) phase, we utilized the same 558K data used in LLaVA-1.5 [36]. For the SFT phase, we collect 484K samples, including 300K sampled from LLaVA-1.5-665K, 146K sampled from RSVQAHR [40], and 38K template-based samples from 3 RS datasets [13, 31, 33] using their labels. Note some images in this 38k data are from the same source as LRS-VQA, but there is no overlap or duplication between them. Details can be found in the Appendix A.2.1. Benchmarks and evaluation metrics: i) MMERealWorld-RS: the RS part of MME-Realworld [83], containing 1,298 RSIs with expert-annotated questions in three types: color, count, and position. We follow the official evaluation script but modify the prompt by removing The best answer is: to address Vicuna-1.5-based models tendency to respond with only option A. ii) LRS-VQA: it consists of 3 parts: LRS-FAIR, LRS-Bridge, and LRS-STAR, containing 2,272, 1,062, and 3,999 QA pairs, respectively. For the short open-ended format, we adopt structured evaluation metric following [19, 63], using WordNet [45], with semantic similarity threshold of 0.8. Experimental settings. We employ existing LVLMs official weights or api for evaluation. For comparison, Figure 4. The construction pipeline of the proposed LRS-VQA dataset. The visual prompt (red box) is inspired by SoM [68]. GPT-4V [47] to generate question-answer pairs about the targets color, shape, status, etc. Details of the overall construction process are provided in the Appendix A.1. 4.2.2. Visual-Centric Quality Check and Refinement To ensure the quality of the generated question-answer pairs, we employ powerful Qwen2-VL [57] for quality assessment. Qwen2-VL supports manual adjustment of the maximum pixel resolution, allowing us to evaluate whether increased resolution improves accuracy while keeping the LLM unchanged. Using this approach, we filter out question types where accuracy can not improve with resolution. Through iterative filtering, manual review, and prompt refinement, we obtain the LRS-VQA dataset, which could effectively assess the large RSI perception capability of LVLMs, as shown in Fig. 5. LRS-VQA contains eight question-answer types: count, color, category, shape, status, reasoning, rural/urban classification, and target background. It features greater diversity in question types and larger image sizes than MME-Realworld-RS as in Tab. 1, highlighting the complexities of large RSI perception. 6 Leaderboard Data Vis. Encoder LLM Max Size Qwen2-VL [57] LLaVA-OV [25] IXC-2.5 [76] LLaVA-UHD-v2 [81] LLaVA-FlexAttn [27] SEAL [61] MGM-HD [30] SliME [82] LLaVA-1.5 [35] RSUniVLM [37] Geochat [24] GPT-4o [2] GPT-4o-mini [2] Claude-3.5-Sonnet [3] Comparison LLaVA-1.5* LLaVA-1.5-p4* Geochat* SLiME* LLaVA-UHD-v2* Ours (LLaVA-1.5) LLaVA-Next-p4* LLaVA-Next-p9* LLaVA-Next-p25* Ours (LLaVA-Next) Qwen2-7B Qwen2-7B - 4.8M - QwenViT SigLip CLIP-L14 1.42M CLIP-L14+JBU Vicuna-1.5-7B 1.22M 0.95M 3.0M 2.0M 1.22M 1.2M 1.53M - - - CLIP-L14 CLIP-L14 Mixed CLIP-L14 CLIP-L14 SigLip CLIP-L14 - - - 3,3333,333 2,3042,304 InternLM2-7B 4,0964,096 6721,008 Vicuna-1.5-7B 1,0081,008 Vicuna-1.5-7B Vicuna-1.5-7B 1,5361,536 6721,008 Vicuna-1.5-7B 336336 Vicuna-1.5-7B 336336 Qwen2-0.5B 504504 Vicuna-1.5-7B - - - - - - - 1.04M CLIP-L14 CLIP-L14 CLIP-L14 CLIP-L14 CLIP-L14+JBU CLIP-L14 Vicuna-1.5-7B 1.04M CLIP-L14 Qwen2-7B 336336 672672 504504 6721,008 6721,008 Dynamic 672672 1,0081,008 1,6801,680 Dynamic MMERW-RS 49.73 53.53 36.12 40.77 37.75 30.55 31.51 28.54 26.38 25.39 28.62 28.92 6.69 25.74 34.40 38.20 33.68 34.56 38.55 39.04 39.12 40.35 39.65 41.89 LRSFAIR 23.80 20.61 25.25 22.82 19.57 21.29 17.90 17.11 18.76 21.02 20.18 22.15 18.67 12.95 18.24 19.18 21.51 21.98 20.77 22.97 21.06 21.14 20.99 21.85 LRSBridge 38.12 35.11 38.41 32.57 29.99 34.75 35.92 32.09 30.70 32.61 24.54 31.84 31.99 26. 32.28 35.43 35.97 33.20 36.57 36.89 36.27 37.25 36.38 38.24 LRSSTAR 27.87 26.08 27.30 26.08 22.76 21.29 20.13 22.99 22.63 24.72 13.75 27.40 25.85 13.29 24.17 26.50 25.86 25.10 25.79 27.48 25.80 26.10 26.18 26.67 Avg. Acc 34.88 33.83 31.77 30.56 27.52 26.97 26.36 25.18 24.62 25.93 21.77 27.58 20.80 19.67 27.30 29.83 29.25 28.71 30.42 31.59 30.56 31.21 30.80 32.16 Table 2. Leaderboard and performance comparison on LRS-VQA and MME-RealWorld-RS. MME-RW-RS indicates the MMERealWorld-RS. Data refers to the total instruction data used during PT and SFT. pX indicates the max number is for the pre-defined grids. method* indicates we reproduce the SFT stage of existing methods using the 484k instruction data. Setting Color Count Position Acc FPS anyres-p25 41.56 w/ PruMerge++ 43.27 w/ VisionZip 42.71 w/ FastV 39.52 w/ PDrop 41.00 w/ prune (Ours) 43.98 DIP-3layer w/ prune (Ours) 44.08 31.05 28.38 24.55 30.10 30.54 30. 46.14 32.78 34.55 43.99 47.77 49.16 39.65 0.188 34.86 0.152 33.98 0.183 37.93 0.192 39.85 0.184 41.28 0.165 30.94 49.26 41.31 0.267 Table 3. Comparison with different token reduction methods based on LLaVA-Next-Qwen2. To enable fair comparison, we adopt the anyres-p25 setting, where the maximum number of predefined grids is 25 (1,6801,680 pixels). FPS is computed based on the inference time for 800 images from MME-Realworld-RS. we reproduce other methods using the same 484k SFT data. Since the LLaVA-NeXTs training code is not opensource, we integrate the anyres strategy and Qwen2 into the LLaVA-1.5 framework, which serves as the codebase for our approach. All the experiments are conducted on 4 NVIDIA A100 80GB GPUs. For our method, the minimum length for DIP is 1,008 pixels. For Vicuna-1.5 and Qwen2, the Nmax is set to 40 and 80, respectively. The vision token saving ratio α is 0.25, λhr, λmse, λkl are 2.0, 1.0, 1.0, Setting anyres-p144 w/ PruMerge++ w/ VisionZip w/ FastV w/ PDrop w/ prune (Ours) DIP-4layer w/ prune (Ours) Vis. Tokens (Total) Vis. Tokens (to LLM) TFLOPs (B) 83520 83520 83520 83520 83520 83520 55296 21312 5328 9280 21312 21312 5760 2376 243.37 43.75 83.56 109.21 101.62 82.56 36. Table 4. Comparison of computational efficiency with different token reduction methods based on LLaVA-Next-Qwen2. We assume inferring 40004000 pixel image (anyres-p144) and report the theoretical TFLOPs of vision tokens after the vision projector. respectively. The RFM-LLM layer pairs are [1, 5, 11, 14] with distillation applied to the first and last pairs. More experimental details are in the Appendix A.2.2. 5.2. Leaderboard and Comparison Leaderboard on LRS-VQA. We evaluate total of 11 open-source LVLMs, including methods for high-resolution [27, 36, 76, 82], chain-of-thought reasoning [61], multiple visual encoders [30], and RS LVLMs [24, 37]. We also 7 Method anyres-p25 w/ prune (Ours) Drop Ratio Color Count Pos Acc 0% 41.56 25% 42.47 50% 42.39 75% 43.98 90% 41.51 31.05 31.08 31.24 30.42 29.77 46.14 39.65 47.41 40.40 48.45 40.77 49.16 41.28 46.70 39.41 Table 5. Ablation study on different prune ratios with LLaVANext-Qwen2, when pruning vision tokens based on RFM results without DIP. This table demonstrates that pruning irrelevant tokens in high-resolution RSIs can improve performance. Max Size Color Count Pos Acc FPS 1,0081,008 2,0162,016 4,0324,032 8,0648,064 Dynamic 42.87 44.08 44.86 43.98 44.70 29.85 30.94 29.85 30.42 31. 47.89 49.26 47.18 45.11 49.72 40.29 41.31 40.72 39.91 41.89 0.271 0.267 0.221 0.204 0.238 Table 6. Ablation study on fixing the number of DIP layers reached in inference with LLaVA-Next-Qwen2. Rows 1-4 correspond to fixing the number of DIP layers being 2-5, respectively. RFM Layers LLM Layers Color Count Pos Acc 3 4 5 6 41.35 [1,8,14] 44.70 [1,5,11,14] [1,5,10,12,14] 43.98 [1,3,6,9,12,14] 42.31 28.30 31.00 29.36 31. 47.89 39.27 49.72 41.89 48.13 40.58 47.18 40.26 Table 7. Ablation study on different RFM-LLM layer-pair configurations in MME-RealWorld-RS, with LLaVA-Next-Qwen2. evaluate 3 closed-source MLLMs. Results in Tab. 2 indicate that LRS-VQA is more challenging than MME-RealWorldRS on average, reflecting the complexities of large RSIs. Comparison with high-resolution methods. As shown in Tab. 2, we select 5 different high resolution strategies, performing SFT on the same dataset for comparison. Our method performs better across all 4 datasets. Comparison with token reduction methods. As shown in Tab. 3 and Tab. 4, we reproduce 4 plug-and-play methods for comparison on MME-RealWorld-RS. For PruneMerge++ [50] and our RFM-based pruning, the retain ratio is 25%. For VisionZip [69], the number of retained tokens is 64. For PDrop [64] under Qwen2, pruning layers are set to [7,14,21]. Additionally, we employ 3-layer DIP, supporting up to 2,0162,016 pixels, which improves accuracy by 1.46% and boosts FPS by 39% compared to other methods. For Tab. 4, we consistently use 4-layer DIP. Since our method dynamically selects image tiles, we calculate the average number of tiles selected from similar size images in the dataset. Above results indicate that existing methods often overlook the high cost of processing many Figure 6. Text-related visual attention localization in the last layer if trained RFM under LLaVA-Next-Qwen2. text-irrelevant image tiles in large images. 5.3. Ablation Results We conducted ablation studies on the MME-RealWorld-RS, from the following aspects to perform an in-depth analysis: Different ratios for RFM-based pruning. As shown in Tab. 5, key finding is that in large RSIs, the complex background and the small foreground regions enable high pruning rates to deliver performance benefits. Fixed DIP layers for inference. As shown in Tab. 6, we fix the number of DIP layers reached during inference. If an images DIP is shallower than the specified number, the last layer is used. Results show that forcing traversal to higher resolutions can degrade performance since not all questions require image details. Therefore, our strategy dynamically selects the termination DIP layer based on the input text, balancing accuracy and efficiency. Distillation settings for the RFM module. We explore the impact of different RFM-LLM layer-pairs in Tab. 7. Results show that introducing too many layers may hinder effective distillation and increase training and inference time, while too few layers may prevent RFM from effectively learning text-aware localization ability. Visualization results. In Fig. 6, we visualize the attention maps generated by the trained RFM for vision tokens from initial input image tiles. The results demonstrate that text references to targets effectively guide the RFMs attention output, supporting our coarse-to-fine inference. 6. Conclusion This paper presents text-guided token pruning method tailored for efficiently processing large remote sensing images (RSIs). To address the limitations of existing high-resolution approaches when handling large RSIs, our method integrates the Region Focus Module (RFM) and Dynamic Image Pyramid (DIP) to focus on text-relevant key vision tokens while reducing computational overhead. Furthermore, we introduce new benchmark, LRS-VQA, 8 with 7,333 QA pairs covering 8 question types, and features larger image sizes and diverse range of question types. Future work. Vision-language understanding of large RSIs remains challenging. Exploring alternative perspectives, such as long-context transfer from LLMs to LVLMs, to seek solutions is also meaningful."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Open AI. Hello gpt-4o. 2024. 7 [3] AI Anthropic. Claude 3.5 sonnet model card addendum. page 3, 2024. 7 [4] Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, and Bo Ji. Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models, 2024. 2 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [6] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 2 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 1 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 2, [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 1 [10] Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Kartik Kuckreja, Fahad Shahbaz Khan, Paolo Fraccaro, Alexandre Lacoste, and Salman Khan. Geobench-vlm: Benchmarking vision-language models for geospatial tasks. arXiv preprint arXiv:2411.19325, 2024. 1 [11] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. 5 [12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 1 [13] Jian Ding, Nan Xue, Gui-Song Xia, Xiang Bai, Wen Yang, Michael Ying Yang, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, et al. Object detection in aerial images: large-scale benchmark and challenges. IEEE transactions on pattern analysis and machine intelligence, 44(11): 77787796, 2021. 6 [14] Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu, Zhenglun Kong, Xin Meng, Zhengang Li, Xue Lin, Zhenman Fang, et al. Heatvit: Hardware-efficient adaptive token pruning for vision transformers. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 442455. IEEE, 2023. [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [16] Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and highIn European Conference on Computer resolution images. Vision, pages 390406. Springer, 2025. 2, 3 [17] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. Rethinking token reduction in mllms: Towards unified arXiv preprint paradigm for training-free acceleration. arXiv:2411.17686, 2024. 2 [18] Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, Hengshuang Zhao, Hang Xu, Lu Hou, Wei Zhang, and Xiaodan Liang. Hires-llava: Restoring fragmentation input in high-resolution large vision-language models. arXiv preprint arXiv:2407.08706, 2024. 2 [19] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. 6 [20] Omri Kaduri, Shai Bagon, and Tali Dekel. Whats in the image? deep-dive into the vision of vision language models. arXiv preprint arXiv:2411.17491, 2024. 3, [21] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 784794, 2022. 2 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2 [23] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. Spvit: Enabling faster vision transformers via In European conference latency-aware soft token pruning. on computer vision, pages 620640. Springer, 2022. 2 9 [24] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2, 7 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 7 [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [27] Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan. Flexattention for efficient high-resolution vision-language modIn European Conference on Computer Vision, pages els. 286302. Springer, 2024. 2, [28] Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yu Yi, and Xue Yang. simple aerial detection baseline of multimodal language models. arXiv preprint arXiv:2501.09720, 2025. 1 [29] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. 2 [30] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2023. 2, 7 [31] Yansheng Li, Junwei Luo, Yongjun Zhang, Yihua Tan, JinGang Yu, and Song Bai. Learning to holistically detect bridges from large-size vhr remote sensing imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2, 5, 6 [32] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. 2024. 2 [33] Yansheng Li, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Youming Deng, Wenbin Wang, Xian Sun, Haifeng Li, Bo Dang, Yongjun Zhang, Yi Yu, and Yan Junchi. Star: first-ever dataset and large-scale benchmark for scene graph generation in large-size satellite imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2, 5, [34] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2676326773, 2024. 2 [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 2, 7 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 6, 7 [37] Xu Liu and Zhouhui Lian. Rsunivlm: unified vision language model for remote sensing via granularity-oriented mixture of experts. arXiv preprint arXiv:2412.05679, 2024. 7 [38] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. 2 [39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [40] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing, 58 (12):85558566, 2020. [41] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jiawen Liu, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 3 [42] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-ofresolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. 2 [43] Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, et al. Skysensegpt: fine-grained instruction tuning dataset and model for remote sensing visionlanguage understanding. arXiv preprint arXiv:2406.10100, 2024. 1 [44] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1230912318, 2022. 2 [45] George Miller. Wordnet: lexical database for english. Communications of the ACM, 38(11):3941, 1995. 6 [46] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. Lhrs-bot: Empowering remote sensing In with vgi-enhanced large multimodal language model. European Conference on Computer Vision, pages 440457. Springer, 2024. 1 [47] OpenAI. Gpt-4v(ision) system card. 2023. 6 [48] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:1393713949, 2021. [49] Akashah Shabbir, Mohammed Zumri, Mohammed Bennamoun, Fahad Khan, and Salman Khan. Geopixel: Pixel grounding large multimodal model in remote sensing. arXiv preprint arXiv:2501.13925, 2025. 2 [50] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 2, 8 10 [51] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2025. 2 [52] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. 2 [53] Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell Watson, Levente Klein, Fahad Shahbaz Khan, et al. Earthdial: Turning multi-sensory earth observations to interactive dialogues. arXiv preprint arXiv:2412.15190, 2024. [54] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, et al. Fair1m: benchmark dataset for finegrained object recognition in high-resolution remote sensing imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 184:116130, 2022. 5 [55] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2025. 2 [56] Junjue Wang, Zhuo Zheng, Zihang Chen, Ailong Ma, and Yanfei Zhong. Earthvqa: Towards queryable earth via relational reasoning-based remote sensing visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 54815489, 2024. 2 [57] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 6, 7 [58] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. arXiv preprint, 2024. 2 [59] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. [60] Siyuan Wei, Tianzhu Ye, Shen Zhang, Yao Tang, and Jiajun Liang. Joint token pruning and squeezing towards more aggressive compression of vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20922101, 2023. 2 [61] Penghao Wu and Saining Xie. V*: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 2, 7 [62] Aoran Xiao, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, and Naoto Yokoya. Foundation models for remote sensing and earth observation: survey. arXiv preprint arXiv:2410.16602, 2024. 2 [63] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 6 [64] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 2, 4, 8 [65] Xizhe Xue, Guoting Wei, Hao Chen, Haokui Zhang, Feng Lin, Chunhua Shen, and Xiao Xiang Zhu. Reo-vlm: Transforming vlm to meet regression challenges in earth observation. arXiv preprint arXiv:2412.16583, 2024. 1 [66] Dawei Yan, Pengcheng Li, Yang Li, Hao Chen, Qingguo Chen, Weihua Luo, Wei Dong, Qingsen Yan, Haokui Zhang, and Chunhua Shen. Tg-llava: Text guided llava via learnable latent embeddings. arXiv preprint arXiv:2409.09564, 2024. [67] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 1 [68] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023. 6 [69] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024. 2, 8 [70] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024. 2 [71] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. Tr-bert: Dynamic token reduction for accelerating bert inference. arXiv preprint arXiv:2105.11618, 2021. 2 [72] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [73] Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. Fit token pruning arXiv preprint and prune: Fast and training-free visual for multi-modal large language models. arXiv:2409.10197, 2024. 2, 3, 4 11 language geo-foundation models: survey. arXiv preprint arXiv:2406.09385, 2024. 2 [74] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. 2 [75] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Skyeyegpt: Unifying remote sensing vision-language tasks via instruction ISPRS Journal of Photuning with large language model. togrammetry and Remote Sensing, 221:6477, 2025. 1 [76] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 2, [77] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large mularXiv preprint timodal models with one vision token. arXiv:2501.03895, 2025. 2 [78] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Jun Li, and Xuerui Mao. Earthmarker: visual prompting multimodal large language model for remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 2024. 2 [79] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and Xuerui Mao. Earthgpt: universal multi-modal large language model for multi-sensor image comprehension in rearXiv preprint arXiv:2401.16822, mote sensing domain. 2024. 1 [80] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 2, 5 [81] Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, et al. Llava-uhd v2: an mllm integrating highresolution feature pyramid via hierarchical window transformer. arXiv preprint arXiv:2412.13871, 2024. 7 [82] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 2, 7 [83] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world arXiv preprint scenarios that are difficult for humans? arXiv:2408.13257, 2024. 2, 6 [84] Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, and Jianwei Yin. Enhancing ultra high resolution remote sensing imagery analysis with imagerag. arXiv preprint arXiv:2411.07688, 2024. [85] Yue Zhou, Litong Feng, Yiping Ke, Xue Jiang, Junchi Towards visionYan, Xue Yang, and Wayne Zhang."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}