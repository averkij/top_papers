{
    "paper_title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
    "authors": [
        "Suhwan Choi",
        "Yongjun Cho",
        "Minchan Kim",
        "Jaeyoon Jung",
        "Myunchul Joe",
        "Yubeen Park",
        "Minseo Kim",
        "Sungwoong Kim",
        "Sungjae Lee",
        "Hwiseong Park",
        "Jiwan Chung",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications."
        },
        {
            "title": "Start",
            "content": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction Suhwan Choi1, Yongjun Cho1, Minchan Kim1, Jaeyoon Jung1, Myunchul Joe1, Yubeen Park1, Minseo Kim2, Sungwoong Kim2, Sungjae Lee2, Hwiseong Park1, Jiwan Chung2, Youngjae Yu2 4 2 0 2 2 ] . [ 1 3 7 2 1 0 . 0 1 4 2 : r Fig. 1: Humans often give abstract navigation directions using simple instruction, relying on the recipients commonsense to bridge the gaps. With CANVAS, robots can interpret and act on these directions like humans do, fostering shared understanding of the environment. It shows how robots can use commonsense to translate vague human instructions into concrete actions, navigating across diverse settings in our COMMAND dataset, which we plan to open-source as valuable resources for imitation learning in commonsense-aware navigation. Abstract Real-life robot navigation involves more than just reaching destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records 0% total success rate, CANVAS achieves total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications. I. INTRODUCTION Real-life robot navigation scenarios involve addressing complex objectives that extend far beyond simply reaching destination. For example, an agricultural spraying robot Equal Contribution. 1MAUM.AI, 2Yonsei University. Videos, datasets, and models: worv-ai.github.io/canvas must maximize field coverage [1], while package delivery robot must adhere to road lanes and use crosswalks when transitioning between sidewalks. [2], [3] In both cases, robots need to optimize their movements while responding to the specific requirements of the scenario. Humans typically communicate these scenario-specific goals through high-level guidance, such as verbal commands [4][6], rough sketches of the desired route [7], [8], or combination of both [9]. While such guidance outlines the robots overall objectives, it often lacks the specificity required for precise execution. To convert these abstract and imprecise instructions into actionable navigation plans, robots need commonsense knowledge. In the context of robotics, commonsense refers to the general understanding humans naturally use to make decisions, covering aspects such as human desires, physics, and causality [10]. Robots must leverage this knowledge to flexibly adjust their paths, ensuring their decisions align with human expectations by adhering to commonsense constraints posed by the environment and the users true intentions. In response to these challenges, we introduce CANVAS (Commonsense-Aware NaVigAtion System), novel framework for integrating abstract human instructions into robot navigation. Our approach utilizes both visual and linguistic inputs, such as rough sketch trajectories on map images or textual descriptions. These multimodal instructions are processed by vision-language model that generates incremental navigation targets. By leveraging the commonsense knowledge embedded in pre-trained vision-language models [11][14], robots can develop versatile understanding of commonsense navigation dynamics. Quantifying successful navigation behaviors using rewards [15][17] is particularly difficult in commonsense-aware navigation. Therefore, we employ imitation learning, enabling the robot to comprehend user intent behind noisy and imprecise instructions from human demonstrations. [18] Additionally, we introduce COMMAND (COMMonsenseAware Navigation Dataset), comprehensive dataset designed to train commonsense-aware navigation robots. The dataset features three simulated environments with distinct characteristics (office, street, and orchard). To facilitate imitation learning for instruction-following tasks, we provide 3,343 fully human-annotated navigation results from the simulated environments. Notably, COMMAND offers 48 hours of driving data, which is nearly three times longer than GoStanford [19], covering 219 km and thereby enriching the datasets diversity and scope. Furthermore, we propose two metrics to evaluate the commonsense adherence of navigation algorithms: Trajectory Deviation Distance (TDD) and Instruction Violation Rate (IVR). Our results show that CANVAS consistently outperforms ROS NavStack [20] across all environments with noisy sketch instructions. Particularly in the challenging orchard environment, CANVAS navigates effectively, while ROS NavStack fails due to its reliance on rule-based algorithms [21]. CANVAS trajectory closely mirrors human demonstrations with fewer commonsense constraint violations, indicating better understanding of human expectations. Despite being trained only on simulated data, CANVAS also excels in real-world scenarios, demonstrating strong Sim2Real transfer capabilities. Our contributions are threefold: 1) We introduce CANVAS, novel framework that allows humans to easily communicate with robots using multimodal inputs, ensuring that robots effectively achieve navigation goals, even when human instructions are vague or noisy. 2) We introduce COMMAND, dataset for training commonsense-aware navigation robots, featuring 48 hours of driving data over 219 kilometers, with fully human-annotated sketch instruction and navigation outcomes. 3) We present extensive experiments demonstrating that CANVAS outperforms ROS NavStack in success rate, collision rate, trajectory deviation distance, and instruction violation rate. To support further research, we are open-sourcing CANVAS and COMMAND for imitation learning in commonsense-aware robot navigation. II. RELATED WORK A. Robot Navigation Historically, robot navigation systems were largely rulebased [21][23], relying on set of predefined rules, as seen in frameworks like the ROS NavStack [20]. Following the successful application of deep learning to robotics, more flexible neural navigation approaches have emerged. Visual navigation models, such as NoMaD [24], ViNT [25], and GNM [26], utilize images as goal representations. However, since their high-level planning heavily relies on the topological graph with first-person visual observations, they cannot handle unvisited locations and are sensitive to environmental changes. [27] Vision-language navigation integrates visual information from sensors with language instructions [5], [6], [28]. However, the ambiguous nature of language instructions poses limitations on controlling detailed navigation routes. [29] Recently, LIM2N [9] enabled users to control robots through natural language and sketch trajectories, combining high-level goals with precise motion paths for more intuitive interaction. However, the systems demand for highly accurate and detailed instructions, coupled with its vulnerability to missing details or small mistakes, reduces its usability for non-expert users and limits its effectiveness in broader range of real-world applications [4]. Our proposed method, CANVAS, differentiates itself by addressing the challenge of interpreting abstract or noisy human instructions. CANVAS converts visual and linguistic instructions into detailed navigation actions, utilizing commonsense knowledge to fill in the gaps. This integration of commonsense enables CANVAS to dynamically adapt its navigation strategies across diverse contexts, resulting in enhanced task execution compared to ROS NavStack. NoMaD [24] NaVid [6] LIM2N [9] CANVAS Instruction Misleading (III-A.3) Custom Dataset Environment Scenes Image Real Indoor, Outdoor Real Indoor Language Sketch, Language Sketch, Language Real, Simulation Real, Simulation Indoor, Outdoor Indoor TABLE I: Comparison between various robot navigation methods. B. Imitation Learning in Robotics Imitation Learning (IL) enables agents to learn tasks by mimicking expert demonstrations, eliminating the need for predefined rules or reward functions typically required in Reinforcement Learning (RL) [30]. By directly leveraging expert behavior, IL has proven particularly advantageous in situations where designing reward function is challenging or exploration involves potential risks [31]. As result, there has been growing interest in applying IL to robot navigation [32], [33]. key challenge in IL is modeling multimodal action distributions [34]. One solution is to quantize actions into discrete tokens, simplifying the action space [34][38]. Autoregressive prediction of quantized actions effectively reduces the complexity of modeling diverse and feasible action sequences. CANVAS builds upon this idea by converting continuous waypoints into 128 discrete waypoint tokens using K-means clustering [39], [40]. This approach enhances the ability of robots to model multimodal action distributions, enabling robust navigation strategies that adapt to diverse human instructions and environmental variations. III. DATASET AND TASK An interactive robot navigation framework should fulfill two key objectives: first, humans should be able to communicate desired routes and requirements intuitively; second, the Fig. 2: Data collection pipeline for COMMAND dataset. (a) First, we create diverse navigation environments and extract maps. (b) Then, human annotators sketch routes on the maps based on the guidelines. (c) Finally, we use teleoperation to collect human demonstrations. Red line shows the roughly sketched routes, while the blue line shows the human-demonstrated trajectory. FD refers to Frechet distance. robot should accurately interpret and execute those instructions. However, achieving these goals can be challenging. Simplifying communication for humans often complicates it for robots because humans naturally assume the listener shares their commonsense knowledge. This commonsense allows humans to infer meaning even when instructions are incomplete or imprecise but also causes robots to struggle without explicit and precise input. To address this challenge, we introduce COMMANDa comprehensive experiment suite designed to assess whether robots can use commonsense understanding to transform abstract or occasionally noisy human instructions into the most desired trajectory. COMMAND was collected in three distinct environments: office, street, and orchard, using NVIDIA Isaac Sim [41]. The data consists of high-quality sketch instruction labels and teleoperation data, all by human experts. COMMAND contains 48 hours of driving data covering distance of 219 kilometers. In this section, we describe the dataset curation process and the task definition that builds upon it. A. Dataset datapoint in COMMAND includes canvas map, sketch and language instructions, commonsense constraints, and teleoperation records. The canvas maps, linked to each environment, not only provide occupancy information but also serve as communication interface between humans and robots. We assume humans provide instructions in two modalities: sketch instructions that consist of hand-drawn trajectories on maps for the robot to follow, and language instructions that outline goals and related requirements. When collecting sketch instructions S, we introduce both Precise and Misleading conditions to gather training data that enables our model to handle noisy sketch instructions more robustly. In addition to the instructions, we define set of commonsense constraints C, which are derived from the navigation environment and the language instructions L. These constraints help evaluate whether the robot exhibits appropriate navigation behaviors, such as using crosswalks. We also include human teleoperation records to optimize and evaluate the robot behavior against human actions. An overview of our data collection pipeline is illustrated in Figure 2, and the statistics are provided in Table II. We detail each step below. 1) Environments: COMMAND features three simulated environments, each tailored to specific scenarios. The simulated environments include tasks such as coffee delivery in an office, package delivery on the street, and agricultural spraying in an orchard. An expert designer creates each simulated environment using Unreal Engine [42]. 2) Canvas Map Extraction: The simulated environments are subsequently exported to NVIDIA Isaac Sim, where occupancy maps are extracted programmatically. 3) Sketch Instruction Labeling: The data workers draw sketch trajectories on the canvas maps following sketch guidelines manually crafted by the authors. When the Precise condition is included, the workers trace the most efficient route, closely following the guidelines. In contrast, when the Misleading condition is included, data workers are instructed to deliberately introduce noise by drawing trajectories that pass through walls or objects. All sketch instructions undergo manual inspection to ensure quality. 4) Teleoperation: The data workers are then provided both the sketch and language instructions to teleoperate the virtual robot in NVIDIA Isaac Sim. We record front view images and canvas map images along with the human-drawn trajectory. After collecting the teleoperation data, we adopt the Frechet distance (FD) [43] to measure the discrepancy between the sketch trajectory and the human-demonstrated trajectory. This metric, which indicates noise in the sketch instructions, tends to be higher in the Precise condition and lower in the Misleading condition. B. Task Definition At each timestep t, the robot manages two states: the front view image Xf (t) and the robots hindsight trajectory [44] up to timestep 1, denoted as H(t). The front view image Xf (t) is captured by the robots camera, while H(t) is tracked through odometry to log the robots past positions. We combine the sketch instruction and hindsight trajectory H(t) onto the same map to create the canvas map image Xc(t). At each step, the robot generates an action Split Environment Office Count Avg. Time Avg. Distance Avg. FD (P / M) % of Misleading 2,263 31s 32.8m 1.05 / 1.77 31% Train Street Road 403 57s 80.4m 0.97 / 2.02 51% Sidewalk 410 103s 150.0m 3.03 / 3.50 51% Test Orchard Office 267 172s 191.6m 1.91 / 3.76 40% 10 39s 38.7m 0.77 / 1.62 50% Street Road 20 56s 91.0m 1.28 / 1.65 50% Sidewalk 10 127s 152.0m 1.32 / 2.33 50% Orchard Gallery 10 182s 232.88m 1.51 / 2.27 50% 10 76s 48.8m 0.68 / 1.36 50% Real Office 10 30s 16.0m 1.44 / 1.63 50% TABLE II: Statistics for the train and test set. The train dataset includes 48 hours of driving data over 219 kilometers, while the test dataset consists of 1.6 hours. FD refers to Frechet distance, where (P / M) stands for Precise and Misleading. The unit of FD is meters. y(t) = [w0, w1, w2, w3], which is sequence of waypoints. This action is conditioned on the front view image Xf (t), the canvas map image Xc(t), and the language instruction L. Formally, the robots action is defined as: y(t) = R(Xf (t), Xc(t), L) the hindsight the end of each iteration, At trajectory is updated by appending p(t), which represents the robots position updated through predicted waypoints y(t), resulting in H(t + 1) = (p(1), p(2), ..., p(t)). This update is reflected in the next canvas map image Xc(t + 1), while the front view image Xf (t + 1) is also updated based on the robots new position. This process continues until the robot either reaches the destination (within 0.5 meters) or maximum timestep = is reached. IV. METHOD To address the problem formulated in Section III, we introduce CANVAS, navigation system designed to bridge the gap between abstract human instructions and concrete robot actions by leveraging commonsense understanding from pretrained vision-language models (VLMs) [45]. In this section, we provide an overview of the model architecture, as well as the training and inference processes. A. Architecture The model architecture is illustrated in Figure 3. We adopt VLM denoted as πθ. The front view image Xf (t) and canvas map image Xc(t) are processed through vision encoder gϕ(). This results in two visual features, Zf = gϕ(Xf (t)) and Zc = gϕ(Xc(t)). projector pϕ is used to project these visual features into the word embedding space, producing sequence of visual tokens τv = pϕ(Zf , Zc). sequence of language tokens τl is also obtained from the language instruction L. Both the visual tokens τv and language tokens τl are then fed into the large language model denoted as fϕ(), which outputs the waypoint tokens [w0, w1, w2, w3] = fϕ(τv, τl). Due to the reasons mentioned in Section II-B, we apply the simplest method, K-means clustering, to discretize continuous waypoints into tokens, with empirical testing showing that K=128 outperformed 32, 64, or 256. Fewer tokens can hinder precise actions like navigating narrow passages. B. Training CANVAS is designed to generate actions as sequence of waypoint tokens. robots trajectory can be represented by Fig. 3: Overview of the CANVAS framework. It processes the front view image Xf (t), canvas map Xc(t), and language instruction to generate waypoint tokens, which are passed to PD controller to move the robot. consecutive waypoints. During training, we minimize the negative log-likelihood loss, which is formulated as follows: J(πθ) = (cid:88) 3 (cid:88) n=1 t=0 log πθ (wn Xf (t)n, Xc(t)n, Ln) The model reframes the navigation as classification problem, where it predicts the next waypoint based on the current state and given instructions. As explained in Related Work Section II-B, by treating navigation as classification task, the model can manage multimodal distributions, enhancing both stability and accuracy in complex environments. C. Inference During inference, the model-generated discrete waypoint tokens convert into continuous waypoints, which are then input into Proportional-Derivative (PD) controller to produce linear and angular velocities (v, ω) for the robots actuators. V. EXPERIMENTS In this section, we aim to answer the following questions: A. Can CANVAS handle variety of commonsense-aware navigation tasks in simulated environments? B. Can CANVAS be transferred to real-world environment in zero-shot manner? C. How much does leveraging the pre-trained weights of the VLM enhance CANVAS performance? D. When NavStack fails, in what ways can CANVAS succeed? E. Is CANVAS fast enough for real-time navigation? Fig. 4: The left side of the figure compares CANVAS-L and CANVAS-S, showing CANVAS-L using the crosswalk despite misleading sketch instruction. The right side compares CANVAS-L and NavStack, illustrating CANVAS-L avoiding small obstacles, such as rocks. Experimental Setup. In COMMAND, successful navigation requires the robot to reach the target location without collisions, while also respecting commonsense constraints. As result, four key metrics were used to evaluate performance: SR, CR, TDD, and IVR. Success Rate (SR) represents the proportion of successful episodes, while Collision Rate (CR) captures the proportion of episodes with collisions. Trajectory Deviation Distance (TDD) measures how closely the model follows human demonstrations, using the interquartile mean of Frechet distances. Finally, Instruction Violation Rate (IVR) assesses the proportion of episodes where human evaluators identified violations of commonsense constraints, such as keeping to the right lane or using crosswalks. TDD was calculated only for success cases, as including failure cases would skew the metric. We compare CANVAS with ROS NavStack [20], straightforward yet effective rule-based navigation system. For this system, we converted the sketch instructions into step-by-step, point-to-point inputs, but language instructions could not be accommodated. The same hyperparameters were used for all experiments with NavStack. We evaluate two variations of CANVAS. CANVAS-S modifies the original Idefics2 8B [45] by swapping the vision encoder for SigLIPL [46] and the text encoder for Qwen2-0.5B [47], reducing the model size from 8B to 0.7B to better accommodate real-world deployment. In contrast, CANVAS-L retains the original Idefics2 8B [45] architecture with its pre-trained weights. Both models utilize 128 waypoint tokens. In the main experiments, both models were inferred using single NVIDIA H100 GPU. All experiments were evaluated over three iterations for each test dataset, with randomized starting orientations. In the real-world environment, SLAM with FAST-LIO2 [48] was used to find the robots current position. A. Results in the Simulated Environments Seen Environments. Table III shows CANVASs performance in three seen environments: office, street (road, sidewalk), and orchard. Under precise instructions, CANVAS achieves similar SR and CR to NavStack in the office and street (road), where navigation is easier, indicating that CANVAS can learn the essential navigation behaviors effectively from human demonstrations. However, in more challenging Method Total SR() CR() TDD() SR() CR() TDD() SR() Misleading Precise Seen Environment Office NavStack 87% 13% 0.846m 0%* CANVAS-S 100% 0% 0.730m 87% CANVAS-L 100% 0% 0.802m 100% Street (Road) 100% 0% 1.654m 0%* NavStack CANVAS-S 100% 0% 1.189m 100% 3% 1.117m 97% CANVAS-L 97% Street (Sidewalk) 53% 53% 1.450m 0%* NavStack CANVAS-S 60% 40% 1.451m 47% CANVAS-L 87% 13% 1.394m 53% 0% 0%* NavStack 73% 60% 1.561m 60% CANVAS-S CANVAS-L 67% 47% 1.759m 60% 87% Orchard - Unseen Environment Gallery NavStack CANVAS-S CANVAS-L 100% 7% 100% 0% 0.783m 0%* 87% 13% 0.773m 33% 33% 0.9m 100%* - 13% 0.843m 93% 0.753m 100% 0% - 100%* 0% 3% - - 1.075m 100% 1.236m 97% 100%* - 53% 2.379m 54% 47% 1.839m 70% - 100%* - 33% 1.448m 67% 53% 1.392m 64% - 100%* - 66% 0.938m 60% 66% 0.856m 67% - TABLE III: Evaluation results on simulated environments. *: NavStack was not tested in the misleading scenario because it is not equipped to handle such situations. Environment Method Street (Road) Street (Sidewalk) NavStack CANVAS-S CANVAS-L NavStack CANVAS-S CANVAS-L Precise Misleading IVR() 7% 0% 17% 7% 0% 0% IVR() 100%* 7% 30% 100%* 26% 13% TABLE IV: Evaluation of violation rates for commonsense constraints in street environment. environments like the street (sidewalk) and orchard, CANVAS significantly outperforms NavStack. detailed analysis of CANVASs performance is provided in Section V-D."
        },
        {
            "title": "Table",
            "content": "IV compares the IVR between CANVAS and NavStack. In the street (road), commonsense constraints include lane adherence, while in the street (sidewalk), they involve crossing roads correctly and staying on the sidewalk. CANVAS consistently achieves lower IVR, even with misleading instructions, by learning commonsense driving rules from demonstrations, unlike NavStacks reliance on explicit programming. Unseen Environment. We exclude the gallery environment during training to evaluate CANVASs performance Fig. 5: We developed physical robot and created realistic simulated environment that replicates real-world conditions. Fig. 6: We classify the failure cases of NavStack [20] in various simulated environments. in unseen settings. As demonstrated in Table III, CANVAS continues to show strong navigation capabilities, even in scenarios with noisy guidance. B. Results in the Real-World Environment While COMMAND collects human demonstrations across variety of simulated environments designed to resemble real-world conditions, potential concern is whether these simulations fully capture the complexity of the real world. Therefore, it is important to demonstrate that the CANVASs effective navigation in simulation can extend to real-world to assess its realenvironments. As shown in Figure 5, world performance, we tested CANVAS in an actual office environment that was used as the basis for the simulation. Despite being trained solely on simulated data, CANVAS demonstrated strong Sim2Real transfer capabilities, performing reliably in real-world scenario. Method NavStack CANVAS-S CANVAS-L Precise Misleading SR() 100% 77% 93% SR() 0%* 60% 33% Total SR() - 69% 63% TABLE V: Evaluation results on real environments. C. Ablation Study"
        },
        {
            "title": "We explore the importance of",
            "content": "leveraging pre-trained weights from the VLM. As demonstrated in Table VI, these weights were crucial for CANVASs performance, especially in both unseen simulated and real-world settings. This indicates that the knowledge encapsulated in the pretrained VLMs offered strong foundation for CANVAS to learn how to incorporate them in developing generalizable understanding of driving dynamics. Environment Method Seen - Office Unseen - Gallery Real - Office CANVAS-L w/o Pre-training CANVAS-L w/o Pre-training CANVAS-L w/o Pre-training Precise Misleading SR() 100% 100% 100% 60% 93% 73% SR() 100% 87% 33% 40% 33% 33% Total SR() 100% 93% 67% 50% 63% 53% TABLE VI: Ablation study on the effect of VLM pre-training. D. Additional Case Study We perform qualitative analysis to examine the factors behind the success of CANVAS in comparison to NavStack [20]. Figure 6 highlights typical failure cases for NavStack, where the robot is unable to reach its destination. In 56% of these cases, failures result from stumbling over rocks in the orchard environment. Figure 4 compares CANVAS and NavStack in this primary failure scenario. The orchard has uneven terrain, and NavStack struggles to avoid small but hazardous obstacles like rocks because its limited perception cant distinguish between rocks and passable areas such as grass. In contrast, CANVAS utilizes visual inputs from the camera to reliably detect unexpected obstacles and assesses their navigation risk based on learned experiences from demonstrations. Additionally, we assess CANVAS-S and CANVAS-L on their adherence to commonsense constraints. As shown in Figure 4, CANVAS-S disregards the use of crosswalk, whereas CANVAS-L successfully adheres to the implicit commonsense rule of crossing at designated points. E. Real-Time Navigation Feasibility Study Finally, we evaluate the feasibility of deploying CANVAS in real-time applications. Figure 5 details the robot design. CANVAS demonstrates real-time inference capabilities, with an average latency of 400ms for CANVAS-S and 800ms for CANVAS-L, all within the available memory limits. These results highlight CANVASs potential to efficiently handle real-world navigation tasks without substantial delays. VI. CONCLUSION We present CANVAS, novel commonsense-aware navlearns from human demonstrations igation system that through imitation learning. CANVAS allows intuitive human instructions using abstract sketches and natural language while leveraging commonsense reasoning to bridge the gap between vague human guidance and concrete robot actions. With the COMMAND dataset for imitation learning and pretrained vision-language models, CANVAS allows robots to understand implicit human intent and make decisions aligned with human expectations. Experiments show that CANVAS outperforms ROS NavStack, strong rule-based system, with higher success rates, fewer collisions, and better trajectory alignment with human demonstrations, all while adhering to commonsense constraints. Additionally, CANVAS exhibits strong performance in both unseen and real-world environments, highlighting its generalization capabilities. By opensourcing the COMMAND dataset and CANVAS, we hope to contribute to active research on imitation learning techniques for commonsense reasoning in robot navigation."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research was partly supported by GINT. The authors would like to thank GINT for providing feedback on the agricultural spraying robot."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Z. Li, X. Liu, H. Wang, J. Song, F. Xie, and K. Wang, Research on robot path planning based on point cloud map in orchard environment, IEEE Access, 2024. 1 [2] H. Wang, L. Zhang, Q. Kong, W. Zhu, J. Zheng, L. Zhuang, and X. Xu, Motion planning in complex urban environments: An industrial application on autonomous last-mile delivery vehicles, Journal of Field Robotics, vol. 39, no. 8, pp. 12581285, 2022. 1 [3] Y. Du, N. J. Hetherington, C. L. Oon, W. P. Chan, C. P. Quintero, E. Croft, and H. Machiel Van der Loos, Group surfing: pedestrianbased approach to sidewalk robot navigation, in 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 6518 6524. 1 [4] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I. Reid, S. Gould, and A. van den Hengel, Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 2 [5] C. Huang, O. Mees, A. Zeng, and W. Burgard, Visual language maps for robot navigation, in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023. 1, 2 [6] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and W. He, Navid: Video-based vlm plans the next step for vision-and-language navigation, arXiv preprint arXiv:2402.15852, 2024. 1, [7] M. Skubic, D. Anderson, S. Blisard, D. Perzanowski, and A. Schultz, Using hand-drawn sketch to control team of robots, Autonomous Robots, 2007. 1 [8] F. Boniardi, A. Valada, W. Burgard, and G. D. Tipaldi, Autonomous indoor robot navigation using sketch interface for drawing maps and routes, in 2016 IEEE International Conference on Robotics and Automation (ICRA), 2016. 1 [9] W. Zu, W. Song, R. Chen, Z. Guo, F. Sun, Z. Tian, W. Pan, and J. Wang, Language and sketching: An llm-driven interactive multimodal multitask robot navigation framework, in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. 1, 2 [10] J.-P. Toberg, A.-C. Ngonga Ngomo, M. Beetz, and P. Cimiano, Commonsense knowledge in cognitive robotics: systematic literature review, Frontiers in Robotics and AI, 2024. 1 [11] H. Chen, H. Tan, A. Kuntz, M. Bansal, and R. Alterovitz, Enabling robots to understand incomplete natural language instructions using commonsense reasoning, in 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020. 1 [12] K. Zhou, K. Zheng, C. Pryor, Y. Shen, H. Jin, L. Getoor, and X. E. Wang, Esc: Exploration with soft commonsense constraints for zeroshot object navigation, in Proceedings of the 40th International Conference on Machine Learning, 2023. [13] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in visionand-language navigation with large language models, Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 1 [14] A. S. Chen, A. M. Lessing, A. Tang, G. Chada, L. Smith, S. Levine, and C. Finn, Commonsense reasoning for legged robot adaptation with vision-language models, arXiv preprint arXiv:2407.02666, 2024. 1 [15] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames, in International Conference on Learning Representations, 2020. 2 [16] M. Yin, T. Li, H. Lei, Y. Hu, S. Rangan, and Q. Zhu, Zero-shot wireless indoor navigation through physics-informed reinforcement learning, in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. 2 [17] H. Taheri and S. R. Hosseini, Deep reinforcement learning with enhanced ppo for safe mobile robot navigation, arXiv preprint arXiv:2405.16266, 2024. 2 [18] Y. Ding, C. Florensa, M. Phielipp, and P. Abbeel, Goal-conditioned imitation learning, Advances in Neural Information Processing Systems, 2019. [19] N. Hirose, F. Xia, R. Martın-Martın, A. Sadeghian, and S. Savarese, Deep visual mpc-policy learning for navigation, IEEE Robotics and Automation Letters, 2019. 2 [20] Open Robotics, Github - ros-planning/navigation, 2023, https:// github.com/ros-planning/navigation version: noetic, 1.17.3 accessed: 2024.09.15. 2, 5, 6 [21] J. Christian Andersen, O. Ravn, and N. A. Andersen, Autonomous rule-based robot navigation in orchards, IFAC Proceedings Volumes, 2010. 2 [22] F. Gul, W. Rahiman, and S. S. Nazli Alhady, comprehensive study for robot navigation techniques, Cogent Engineering, 2019. 2 [23] B. Liu, Z. Guan, B. Li, G. Wen, and Y. Zhao, Research on slam algorithm and navigation of mobile robot based on ros, in 2021 IEEE International Conference on Mechatronics and Automation (ICMA), 2021. 2 [24] A. Sridhar, D. Shah, C. Glossop, and S. Levine, Nomad: Goal masked diffusion policies for navigation and exploration, in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. 2 [25] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine, Vint: foundation model for visual navigation, in Proceedings of The 7th Conference on Robot Learning, 2023. 2 [26] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine, Gnm: to drive any robot, in 2023 IEEE general navigation model International Conference on Robotics and Automation (ICRA), 2023. [27] F. Blochliger, M. Fehr, M. Dymczyk, T. Schneider, and R. Siegwart, Topomap: Topological mapping and navigation based on visual slam maps, in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. 2 [28] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. Wang, Vision-andlanguage navigation: survey of tasks, methods, and future directions, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. 2 [29] W. Wu, T. Chang, X. Li, Q. Yin, and Y. Hu, Vision-language navigation: survey and taxonomy, Neural Computing and Applications, 2023. 2 [30] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, BC-z: Zero-shot task generalization with robotic imitation learning, in 5th Annual Conference on Robot Learning, 2021. 2 [31] M. Zare, P. M. Kebria, A. Khosravi, and S. Nahavandi, survey of imitation learning: Algorithms, recent developments, and challenges, IEEE Transactions on Cybernetics, 2024. 2 [32] H. Karnan, G. Warnell, X. Xiao, and P. Stone, Voila: Visualobservation-only imitation learning for autonomous navigation, in 2022 International Conference on Robotics and Automation (ICRA), 2022. 2 [33] X. Pan, T. Zhang, B. Ichter, A. Faust, J. Tan, and S. Ha, Zeroshot imitation learning from demonstrations for legged robot visual navigation, in 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020. [34] N. M. Shafiullah, Z. Cui, A. A. Altanzaya, and L. Pinto, Behavior transformers: Cloning modes with one stone, Advances in neural information processing systems, 2022. 2 [35] L. Metz, J. Ibarz, N. Jaitly, and J. Davidson, Discrete sequential prediction of continuous actions for deep rl, arXiv preprint arXiv:1705.05035, 2017. 2 [36] R. Dadashi, L. Hussenot, D. Vincent, S. Girgin, A. Raichuk, M. Geist, and O. Pietquin, Continuous control with action quantization from demonstrations, arXiv preprint arXiv:2110.10149, 2021. 2 [37] Y. Chebotar, Q. Vuong, K. Hausman, F. Xia, Y. Lu, A. Irpan, A. Kumar, T. Yu, A. Herzog, K. Pertsch, et al., Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions, in Conference on Robot Learning, 2023. 2 [38] S. Lee, Y. Wang, H. Etukuru, H. J. Kim, N. M. M. Shafiullah, and L. Pinto, Behavior generation with latent actions, in Proceedings of the 41st International Conference on Machine Learning, 2024. 2 [39] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, et al., Rt-2: Visionlanguage-action models transfer web knowledge to robotic control, arXiv preprint arXiv:2307.15818, 2023. 2 [40] Z. Li, K. Li, S. Wang, S. Lan, Z. Yu, Y. Ji, Z. Li, Z. Zhu, J. Kautz, Z. Wu, et al., Hydra-mdp: End-to-end multimodal planning with multi-target hydra-distillation, arXiv preprint arXiv:2406.06978, 2024. [41] NVIDIA, Isaac Sim - Robotics Simulation and Synthetic Data NVIDIA Developer, https://developer.nvidia.com/isaac/sim, Accessed: 2024.09.15. 3 [42] Epic Games, Unreal engine, https://www.unrealengine.com, [Accessed: 2024.09.15]. 3 [43] B. Aronov, S. Har-Peled, C. Knauer, Y. Wang, and C. Wenk, Frechet distance for curves, revisited, in AlgorithmsESA 2006: 14th Annual European Symposium, Zurich, Switzerland, September 11-13, 2006. Proceedings 14, 2006. 3 K. Hausman, C. Finn, Q. Vuong, and T. Xiao, Rt-trajectory: Robotic task generalization via hindsight trajectory sketches, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. 3 [45] H. Laurencon, L. Tronchon, M. Cord, and V. Sanh, What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 4, [46] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 5 [47] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, et al., Qwen2 technical report, arXiv preprint arXiv:2407.10671, 2024. 5 [44] J. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas, K. Rao, W. Yu, C. Fu, K. Gopalakrishnan, Z. Xu, P. Sundaresan, P. Xu, H. Su, [48] W. Xu, Y. Cai, D. He, J. Lin, and F. Zhang, Fast-lio2: Fast direct lidar-inertial odometry, IEEE Transactions on Robotics, 2022."
        }
    ],
    "affiliations": [
        "MAUM.AI",
        "Yonsei University"
    ]
}