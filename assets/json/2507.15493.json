{
    "paper_title": "GR-3 Technical Report",
    "authors": [
        "Chilam Cheang",
        "Sijin Chen",
        "Zhongren Cui",
        "Yingdong Hu",
        "Liqun Huang",
        "Tao Kong",
        "Hang Li",
        "Yifeng Li",
        "Yuxiao Liu",
        "Xiao Ma",
        "Hao Niu",
        "Wenxuan Ou",
        "Wanli Peng",
        "Zeyu Ren",
        "Haixin Shi",
        "Jiawen Tian",
        "Hongtao Wu",
        "Xin Xiao",
        "Yuyang Xiao",
        "Jiafeng Xu",
        "Yichu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 9 4 5 1 . 7 0 5 2 : r GR-3 Technical Report"
        },
        {
            "title": "ByteDance Seed",
            "content": "Full Author List in Contributions and Acknowledgements"
        },
        {
            "title": "Abstract",
            "content": "We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, π0, on wide variety of challenging tasks. We hope GR-3 can serve as step towards building generalist robots capable of assisting humans in daily life. Date: July 22, 2025 Correspondence: wuhongtao.123@bytedance.com Project Page: https://seed.bytedance.com/GR"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of intelligent generalist robots that are capable of assisting humans with daily tasks has been long-standing goal in robotics research [3, 7, 911, 13, 67]. key challenge stems from the immense diversity of the real world, requiring robot policies to possess strong generalization capabilities to handle wide range of novel scenarios. Additionally, many daily tasks are inherently long-horizon and require complex dexterous manipulation, demanding robot policies to be highly robust and reliable. Recent advances in Vision-Language-Action (VLA) models [9, 11, 29, 37, 67] have paved promising path towards developing intelligent generalist robot policies. These models are built upon pre-trained visionlanguage models (VLMs) [4, 14, 19, 39] and integrate the action prediction capabilities, enabling robots to perform wide range of tasks following natural language instructions. Despite these progresses, instruction following remains significant challenge, especially for out-of-distribution instructions that involve novel object categories that are unseen in robot trajectory data and/or complex concepts that require sophisticated reasoning [11, 49]. Moreover, VLA models typically require large amounts of demonstrations for policy training, posing substantial challenges for efficient adaptation to novel settings. Finally, ensuring robustness in complex long-horizon tasks remains challenging due to accumulative errors, especially for tasks involving dexterous skills, such as manipulating deformable objects. 1 Figure 1 Overview. GR-3 is able to learn from three types of data: vision-language data, robot trajectory data, and human trajectory data. It is able to perform dexterous and long-horizon tasks with exceptional robustness and generalize well to novel objects, environments, and instructions. In this report, we introduce GR-3, large-scale vision-language-action (VLA) model that 1) strictly follows languages and generalizes well to novel objects, environments, and instructions, 2) efficiently learns from few-shot human trajectory data for rapid adaptation to novel settings, and 3) performs long-horizon and dexterous tasks with high robustness and reliability  (Fig. 1)  . GR-3 takes as inputs natural language instruction, observations from the environment, and the robot state. It outputs an action chunk to control bi-manual mobile robot in an end-to-end manner. Specifically, GR-3 is built upon pre-trained VLM [2] and predicts actions via flow-matching [47, 48]. We perform careful studies on model architecture, and introduce set of well-chosen design choices, which we find crucial for instruction following capabilities and long-horizon task performance. To enhance generalization capabilities, we co-train GR-3 on robot trajectory data and large-scale vision-language data covering wide variety of vision-language tasks. This training recipe enables GR-3 to not only handle objects from novel categories, but also understand abstract concepts relating to sizes, spatial relationships, and common-sense knowledge  (Fig. 2)  , which are unseen within the robot trajectory data. Besides, we showcase that GR-3 can be efficiently fine-tuned with minimal human trajectory data collected via VR devices, enabling rapid and cost-effective adaptation to novel settings. Alongside GR-3, we introduce ByteMini, versatile bi-manual mobile robot designed with high flexibility and reliability, capable of accomplishing wide range of challenging tasks in the real world. We perform extensive real-world experiments across three challenging tasks: 1) generalizable pick-and-place, 2) long-horizon table bussing, and 3) dexterous cloth manipulation. GR-3 consistently outperforms the state-of-the-art baseline, π0, across all tasks. It showcases strong capabilities in generalizing to objects from novel categories and understanding complex semantics. Moreover, it efficiently adapts to novel objects with only 10 human trajectories per objects. Finally, GR-3 excels in performing long-horizon and dexterous tasks with remarkable robustness, achieving high average task progress in the challenging tasks of table bussing and cloth manipulation. 2 Figure 2 Capabilities. GR-3 strictly follows instructions and is capable of understanding unseen instructions involving abstract concepts. It performs robustly and reliably on long-horizon table bussing and dexterous cloth manipulation. 3 Figure 3 The GR-3 Model. GR-3 is co-trained on both robot trajectories and vision-language data with flow-matching objective (left) and next-token-prediction objective (right), respectively."
        },
        {
            "title": "2 The GR-3 Model",
            "content": "GR-3 is an end-to-end vision-language-action (VLA) model πθ. It controls bi-manual robot with mobile base by generating k-length action chunk at = at:t+k conditioned on the input language instruction l, observation ot, and robot state st, i.e., at = πθ(l, ot, st). GR-3 adopts the mixture-of-transformers architecture [45]. It processes the observation images from multiple camera views and the language instruction with pre-trained vision-language model (VLM), i.e., Qwen2.5-VL-3B-Instruct [2], and predicts the action chunk with an action diffusion transformer (DiT) [56]. Specifically, GR-3 employs flow matching for action prediction [9, 47, 48]. The flow prediction is conditioned on the current robot state st and the KV cache outputted from the VLM backbone. The k-length action chunk at is represented as tokens and concatenated with the robot state token to create the input token sequence for the action DiT. The flow matching timestep is injected via the adaptive layer norm (AdaLN) [57]. We apply the causal attention mask in the action DiT to model the temporal dependency inside the action chunk. To ensure fast inference, the action DiT contains half the number of layers compared to the VLM backbone and utilizes only the KV cache from the latter half of the VLM layers. In total, GR-3 contains 4B parameters. In our early explorations, we observed frequent instability during training. Inspired by QK norm [26], we apply additional RMSNorm [78] after the linear layers inside both attention and feed-forward networks (FFN) within the DiT blocks. This design choice drastically improves the stability across the whole training process. In addition, we found that it significantly improves the language following capability in our downstream experiments, as shown in Sec. 5."
        },
        {
            "title": "3 Training Recipe",
            "content": "We train the GR-3 model on mixture of data sources, including robot trajectory data for imitation learning, web-scale vision-language data for co-training, and human trajectory data for few-shot generalization. This training recipe allows GR-3 to 1) generalize to novel objects, environments, and instructions, 2) efficiently adapt to unseen settings at low cost, and 3) perform long-horizon and dexterous tasks robustly. 3."
        },
        {
            "title": "Imitation Learning with Robot Trajectory Data",
            "content": "We train GR-3 with an imitation learning objective by maximizing the log-likelihood of the policy on set of expert demonstrations D: Specifically, we supervise the action prediction with flow-matching loss during training: max θ E{at,ot,st,l}D [log πθ(at ot, st, l)] . Laction(θ) = E{at,ot,st,l}D (cid:2) vθ(l, ot, st, aτ ) u(aτ at) 2(cid:3) . (1) (2) where τ (0, 1) is the flow matching timestep and denotes the episode timestep. aτ is the noisy action chunk where ϵ (0, I) is random noise, and u(aτ = (1 τ )ϵ + τ at at) = at ϵ is the ground-truth 4 = aτ + vθ(l, ot, st, aτ label for flow prediction. To accelerate training, we compute the flow-matching loss on multiple sampled flow matching timesteps for one forward pass of the VLM backbone [42]. During inference, the action chunk is initialized from random noise aτ =0 (0, I) and integrated from τ = 0 to τ = 1 with the Euler method, i.e., aτ +τ )τ . We set τ = 0.2 in the experiment. We collect real robot trajectories with teleoperation. To make the collection process more controllable and maximize the data diversity, we develop data collection scheduler  (Fig. 4)  to inform teleoperators with 1) the action to perform, 2) the object combination, and 3) the background setting. At the beginning of each trajectory collection, the system generates new configuration for the teleoperator to arrange the environment accordingly. The implementation of the scheduler enables us to effectively manage the overall data distribution and thoroughly randomize the collected data, greatly enhancing the richness and variability of the dataset. In addition, post-collection quality checks are conducted to refine the dataset by filtering out invalid and low-quality data. Previous work [38] indicates that policies can take advantage of spurious correlations from multiple viewpoints in action prediction instead of properly attending to the language condition. To address this issue, we incorporate task status as an additional action dimension for auxiliary supervision. The task status can be one of the following: Ongoing (0), Terminated (1), Invalid (-1). The Ongoing status indicates that the robot is in progress with the task, while the Terminated status signifies that the robot has successfully completed the task. The Invalid status indicates that the given instruction is invalid given the current observation. For example, if there are no knives on the table, put the knife into the woven basket is considered invalid. During training, we randomly replace the language instruction with an invalid instruction, and train the model to predict the Invalid status without supervision on the other dimensions of the action chunk. This design forces the action DiT to attend to the language instruction and estimate the task status, substantially improving the language following capabilities, as shown in Sec. 5.2."
        },
        {
            "title": "3.2 Co-Training with Vision-Language Data",
            "content": "To endow GR-3 with the generalization capabilities to follow out-of-distribution (OOD) instructions, we jointly train GR-3 on both robot trajectories and vision-language data  (Fig. 3)  [11]. The robot trajectory data trains both the VLM backbone and the action DiT with the flow-matching objective. The vision-language data trains only the VLM backbone with the next-token-prediction objective. For simplicity, we dynamically mix vision-language data with robot trajectories across mini-batches with equal weights. As result, the co-training objective is the sum of the next-token-prediction loss and the flow-matching loss. Through co-training with vision-language data, GR-3 is able to effectively generalize to unseen objects and understand novel semantics of complex concept in zero-shot manner. We curate large vision-language dataset from mixture of data sources [25, 30, 39, 62, 75]. This curated dataset covers wide range of tasks  (Fig. 4)  , including image captioning, visual question answering, image grounding, and interleaved grounded image captioning. We also develop filtering and re-annotation pipeline to improve the quality of the dataset for effective co-training. The co-training not only helps GR-3 maintain the strong vision-language capabilities from the pre-trained VLM, but also enables the action DiT to leverage these capabilities in action prediction, effectively improving the generalization capabilities in downstream manipulation tasks."
        },
        {
            "title": "3.3 Few-Shot Generalization with Human Trajectory Data",
            "content": "GR-3 is versatile VLA model that can be easily fine-tuned to adapt to novel settings. However, collecting real-robot trajectories is both time-consuming and costly. Recent advances in VR devices and hand tracking technology create promising opportunity to learn actions directly from human trajectory data [27, 34, 59]. In this report, we extend the efficient fine-tuning capabilities of GR-3 to the challenging setting of few-shot learning from minimal human trajectories. Specifically, given novel setting, we collect small amount of human trajectory data with PICO 4 Ultra Enterprise. Human trajectories can be efficiently collected with VR devices at rate of approximately 450 trajectories per hour, substantially outpacing the teleoperated robot trajectory collection, which collects about 250 trajectories per hour. This efficiency facilitates rapid and cost-effective adaptation to novel settings. 5 Figure 4 The GR-3 Data. We leverage three types of data during training: robot trajectory data (top), human trajectory data (middle), and vision-language data (bottom). Concretely, the collected human trajectory data contains egocentric videos and human hand trajectories. We use the same labeling pipeline for robot trajectories to label human trajectory data with languages. After completing the first stage of training with vision-language data and robot trajectories, we incorporate the human trajectory data and perform co-training on all three types of data. Unlike robot trajectories, human trajectory data contains only an egocentric view and hand trajectories without arm joint states or gripper states. We consequently pad blank images for the missing wrist views and train the model with only the hand trajectory for human trajectory data."
        },
        {
            "title": "4.1 The ByteMini Robot",
            "content": "The ByteMini robot  (Fig. 5)  is deployed for data collection and policy rollout. This 22-DoF bi-manual mobile robot is specifically designed with three core objectives: flexible manipulation, high reliability, and user-friendliness. Flexible manipulation. The 7-DoF unbiased robotic arm, featuring unique sphere wrist joint configuration [22], achieves human-like dexterity. The compact sphere wrist design  (Fig. 5)  overcomes the critical limitation of traditional SRS-configured arms [54], whose non-compact wrist dimensions hinder flexible operation in confined spaces. The arm elbow is specifically engineered to allow large internal adduction to 2.53 rad, enabling the two arms to execute delicate operations within the area of the robot chest. 6 Figure 5 The ByteMini Robot. We show the robot specifications, multi-camera views, and motion range of the unique wrist sphere joint. High reliability. The demanding workload during data collection and policy rollout requires ByteMini to possess extremely high stability and consistency. We leverage an omni mobile platform integrated with lift mechanism to achieve spatial mobility and vertical height adjustment stably. To further enhance reliability and ensure motion consistency, the actuators in the arms are designed based on the Quasi Direct Drive (QDD) principle [35], which is well-known for its stability and high transparency. User-friendliness. To improve usability, we integrate portable screen and an NUC on the robot, supported by dual-lithium battery that delivers extended endurance of over 10 hours across diverse scenarios. Additionally, ByteMini is equipped with wireless E-stop, enabling rapid response to critical situations. We mount RGBD cameras on the head and the two wrists. The wrist cameras enable close-up observation for fine manipulation."
        },
        {
            "title": "4.2 System & Control",
            "content": "Whole-body compliance control. whole-body compliance control framework [65] treats all degrees of freedom (DoFs) as holistic structure to retarget arbitrary teleoperated human motion to feasible robot motion. Manipulability optimization, singularity avoidance, and physical joint limits are simultaneously addressed within real-time optimal control problem to maximize robot dexterity. This generates fluid and continuous motion across large workspaces for diverse long-horizon manipulation tasks, producing high-quality expert trajectories for policy training. The compliant force controller enables highly dynamic motion and physical interaction with the environment, enhancing safety and data collection efficiency. Whole-body teleoperation. During teleoperated data collection, whole-body retargetting via Meta VR Quest provides intuitive and user-friendly control to directly map human motions to robot end-effectors. Teleoperators can simultaneously control arm, lift mechanism, gripper, and mobile base movements, facilitating seamless data collection for complex long-horizon tasks in the real world. Trajectory optimization for policy rollout. We use the predicted action chunk to control 19 DoFs of the robot (excluding the 3 DoFs from the lift mechanism and head) for policy rollout. We incorporate pure pursuit [15] and trajectory optimization to enhance the stability and smoothness of the trajectories generated by GR-3 during policy rollout. The real-time parameterized optimization minimizes jerk and ensures seamless transition between waypoints and across trajectories. 7 Figure 6 Experiment Settings of Generalizable Pick-and-Place. (a) Test objects that are seen during training. (b) Test objects that are unseen during training. (c) The Basic environment is seen during training. The others are out-of-distribution environments that are unseen during training."
        },
        {
            "title": "5 Experiments",
            "content": "We conduct extensive real-world experiments to comprehensively evaluate the performance of GR-3. Throughout experiments, we aim to answer four questions: Does GR-3 strictly follow instructions, including those that are unseen during training? Can GR-3 generalize to out-of-distribution settings, including novel objects, environments, and instructions? Is GR-3 capable of performing few-shot learning from human trajectory data and transfer to the robot embodiment? Can GR-3 effectively learn robust policy capable of performing long-horizon and dexterous task? We consider three tasks: generalizable pick-and-place, long-horizon table bussing, and dexterous cloth hanging. Please visit the project page for video demos. We compare our method with the state-of-the-art method π0 [9]. We follow the instruction in the official GitHub repository1 and fine-tune π0 from the provided base model, which is pre-trained on large-scale robot data, for each of the three tasks individually."
        },
        {
            "title": "5.1 Generalizable Pick-and-Place",
            "content": "To assess the generalization capabilities of GR-3 in out-of-distribution settings, we evaluate on generalizationfocused pick-and-place task. In total, we collected 35k robot trajectories covering 101 objects, accounting for total of 69 hours for this task. We annotate the robot trajectories with the put into instruction, where is the object category and is the container. For the baseline model, we fine-tune π0 with these robot trajectory data. For GR-3, we co-train the model with both the robot trajectory data and vision-language data. During training, we augment the images of the robot trajectories with photometric augmentations to improve the robustness against the changing environment. We also compare with variant of our method, GR-3 w/o Co-Training, which trains the model with only robot trajectories. This ablation study helps us 1https://github.com/Physical-Intelligence/openpi 8 Figure 7 Experiment Results of Generalizable Pick-and-Place. (a) Results on generalizable pick-and-place under four different settings. (b) Results on few-shot generalization with human trajectories. evaluate the impact of vision-language co-training and identify the specific benefits it contributes to the model performance. Settings. We evaluate under four different settings: 1) Basic, 2) Unseen Environments, 3) Unseen Instructions, and 4) Unseen Objects. In Basic, we evaluate in an environment that is seen during training. We evaluate with 54 objects that are seen during training (Fig. 6(a)) to test the basic instruction following capabilities. In Unseen Environments, we evaluate with the same set of objects in four different real world environments that are unseen during training: checkout counter, meeting room, desk, and break room (Fig. 6(c)). The layouts of objects are kept consistent with the Basic setting. In Unseen Instructions, we prompt the model with instructions requiring complex concept understanding, e.g., put the left coke into the carton and put the animal with tentacles into the carton. In Unseen Objects, we evaluate using 45 objects that are unseen in the robot trajectory data (Fig. 6(b)). We assess the model performance with the instruction following (IF) rate and the success rate, which measure the ability of the model to follow instructions and its overall performance on task completion, respectively. For the IF rate, trial is considered successful if the robot correctly approaches the object specified by the given instruction. For the success rate, trial is considered successful if the robot puts the target object into the container. For both metrics, higher score reflects stronger capability. In Basic and Unseen Environments, we split the 54 seen objects into nine Basic Instruction Following. mini-batches with six objects per batch. In each rollout, we prompt the model to pick one object from all the six objects according to the given instruction. To guarantee the comparability of results across different models, we position the objects according to pre-captured mask, ensuring that the object layout of mini-batch remains as consistent as possible during evaluation. As shown in Fig. 7(a), GR-3 surpasses π0 in both Basic and Unseen Environments in terms of the IF rate and success rate. The modest performance degradation between Basic and Unseen Environments highlights the robustness of GR-3 against environment changes. Additionally, we observe no significant performance differences between GR-3 and GR-3 w/o Co-Training in these two settings, suggesting that co-training does not impact the model performance on seen objects. Generalizable Instruction Following. In Unseen Instructions, we aim to test the capabilities to understand abstract concepts relating to sizes, spatial relationships, and common-sense knowledge. Example instructions include put the coke next to the sprite into the carton, put the largest object into the carton, and put the 9 marine animal into the carton. These instructions are unseen in the robot trajectory data and require the model to reason the complex semantic in the instructions. In Unseen Objects, we split the 45 unseen objects into nine mini-batches with five objects per batch, i.e., the model needs to select one object from five objects per rollout. This setting is particularly challenging as among the 45 objects, more than 70% of them are from categories that are unseen in the robot trajectory data. As shown in Fig. 7(a), GR-3 surpasses π0 in both settings by large margin, highlighting its superior generalization capabilities. It boosts the success rate from 40% to 77.1% in Unseen Instructions and from 40% to 57.8% in Unseen Objects. GR-3 also substantially outperforms GR-3 w/o Co-Training in both settings, indicating that co-training with vision-language (VL) data contributes to the strong generalization capabilities. The VLA model effectively transfers rich knowledge from large-scale VL data into policy learning and enables powerful zero-shot capabilities in novel settings. We also observe that training GR-3 with only robot trajectories yields an inferior performance compared to the π0 baseline. We hypothesize that the performance superiority of π0 stems from its large-scale cross-embodiment pre-training [9]. Few-Shot Generalization from Human Trajectory Data. We also evaluate the few-shot generalization capabilities [31, 71] with human trajectories collected with VR devices. This is challenging for the reason that 1) the model needs to learn from cross-embodiment data and 2) the data is scarce. Specifically, we collected up to 10 human trajectories per object for the 45 unseen objects in the setting of Unseen Objects (Fig. 6(b)). The overall duration of the 450 human trajectories is about 30 minutes. We incrementally train GR-3 based on the checkpoint trained with robot trajectories and VL data. We perform co-training for an additional 20k steps by further incorporating human trajectory data alongside the robot trajectories and VL data. We evaluate the performance under different few-shot settings (1-, 5-, and 10-shot) on both seen and unseen objects (Fig. 7(b)). Comparing to the zero-shot performance of the base model, we are able to continue improving the IF rate and success rate with more human trajectory data on unseen objects and boost the success rate from 57.8% to 86.7% with only 10 human trajectories per object. Additionally, we notice that there is no evident performance drop on seen objects, indicating promising sample-efficient and cost-effective fine-tuning strategy to adapt pre-trained VLA models to downstream novel settings."
        },
        {
            "title": "5.2 Long-Horizon Table Bussing",
            "content": "We perform experiments on table bussing task to evaluate the robustness of GR-3 on handling long-horizon manipulation  (Fig. 8)  . In this task, the robot is required to clean up table with messy utensils, food, to-go box, and plastic bussing box. To complete the task, the robot needs to 1) pack the food into the to-go box, 2) put all the utensils into the bussing box, and 3) put all the trash into the rubbish bin. Given the large workspace, the robot needs to move its mobile base from the to-go box to the bussing box to accomplish the whole task (Fig. 8(a)). We evaluate the models in flat setting and an instruction-following (IF) setting. The Flat Setting. In this setting, we prompt the robot with general task instruction, clean up the dining table, to complete the entire task autonomously within single run (Fig. 8(a)). The flat setting helps us evaluate the robustness of the model on handling long-horizon tasks. We use the average task progress, which calculates the ratio of successfully completed sub-tasks to the total number of sub-tasks, as the evaluation metric. value of 1.0 indicates complete success and fractional value corresponds to partial success. In total, we evaluate on five different sets of objects for this setting. The Instruction-Following (IF) Setting. In this setting, we further evaluate how the model follows instructions. We prompt the robot with multiple sub-task descriptions in roll, e.g., put the paper cup into the rubbish bin, to clean up the table. The robot performs each sub-task starting from home position. We use the average sub-task success rate as the evaluation metric. In total, the IF setting covers six different sets of instructions (Fig. 8(b)): Basic: The object layout closely resembles those in the training data. Multiple Objects: We add multiple instances of subset of object categories into the scene. We include instructions to command the robot to put all the instances belonging to these categories into the bussing box or rubbish bin. 10 Figure 8 Experiment Settings & Results of Table Bussing. (a) Flat: the robot is required to perform long-horizon table bussing in single run. (b) Instruction following (IF): the robot is prompted with multiple sub-task descriptions in roll. (c) Test objects. (d) Results on the flat and instruction following (IF) settings. Multiple Destinations: We add woven basket into the scene and command the robot to put utensils into either the woven basket or the bussing box. Multiple Objects & Destinations: We combine the above two settings and instruct the robot to move all the instances of an object category to one of the two destinations. Novel Destinations: The robot is required to move objects to destination that does not appear with the object in the training data, e.g., put the fork into the rubbish bin. Invalid Tasks: In real-world application, the robot needs to deal with complex instructions among which some may be invalid. For example, if there are no blue bowls on the table, put the blue bowl into the plastic box would be considered invalid. In such scenario, we want the policy to reject from performing wrong valid tasks [52]. In this setting, we prompt the model with tasks that can not be completed with the given observation. The trial is considered successful only if the model refrains from manipulating any objects within 10 seconds. Implementations. In total, we collected approximately 101 hours of robot trajectories for this task. For the baseline method, we fine-tune π0 on these robot trajectories. For GR-3, we co-train on both the robot trajectories and vision-language (VL) data. We also perform ablation studies with two variants of our method, GR-3 w/o Norm and GR-3 w/o Task Status (TS). GR-3 w/o Norm removes the introduced RMSNorm in the attention and FFN of the DiT blocks. GR-3 w/o TS does not incorporate task status during training. For all methods, we train two separate models, flat version and an IF version, for the two settings, respectively. 11 Figure 9 Experiment Settings of Dexterous Cloth Manipulation. (a) Seen and unseen clothes in the test set. (b) The Basic and Position settings. For the flat version, we randomly sample between the general task and sub-tasks as the language instruction. For the IF version, we use only sub-tasks as the instruction during training. Results. As shown in Fig. 8(d), GR-3 outperforms π0 in both settings, especially in the IF setting (53.8% vs 97.5% in terms of success rate). While π0 is able to perform long-horizon table bussing, it struggles with instruction following, especially in out-of-distribution scenarios. It is not able to distinguish between forks and spoons. And in Novel Destinations, it puts objects into containers that appear together with the objects in the training data instead of following the given instructions. On the other hand, GR-3 strictly follows instructions in all six test sets. It generalizes well to multiple objects and destinations and is able to refain from performing wrong tasks in Invalid Tasks. Removing the RMSNorm hurts the performance in both settings, especially in the IF setting. GR-3 w/o Norm is not able to follow instructions well. In particular, it is not able to generalize to novel destinations. These results underscore the crucial role of RMSNorm in enhancing the instruction following capabilities. Without the task status, the IF capabilities also drop, highlighting the effectiveness of task status in aiding the VLA model to follow instructions."
        },
        {
            "title": "5.3 Dexterous Cloth Manipulation",
            "content": "In this experiment, we evaluate GR-3 on dexterous manipulation of deformable objects. In particular, we challenge the model to hang clothes onto drying rack with cloth hangers  (Fig. 2)  . In this task, the robot needs to 1) pick up the hanger, 2) place the clothes onto the hanger, and 3) hang the clothes on the drying rack. In the final step, the robot needs to rotate its mobile base from the table to the drying rack to hang the clothes. In total, we collected 116 hours of robot trajectories for this task. We train π0 on these data. For GR-3, we co-train on these robot trajectories and vision-language data. We perform evaluation under three different settings: Basic, Position, and Unseen Instances. Settings. For Basic, we evaluate on six clothes that are seen during training. The clothes are positioned similarly as in the training data. For Position, we rotate and crumple the clothes as shown in Fig. 9(b). The Position setting evaluates the models robustness in handling challenging clothes layouts. In Unseen Instances, we evaluate the models ability to generalize to clothes that are not seen during training. Specifically, we evaluate with four unseen clothes (Fig. 9(a)). While all the clothes are long-sleeved in the training data, two of the unseen clothes in the test set are short-sleeved. We use average task progress as the evaluation metric, where full success hanging the shirt on the drying rack corresponds to 1.0. The entire process is divided into four key milestones: 1) picking up the hanger, 2) placing the right shoulder on the hanger, 3) placing the left shoulder on the hanger, and 4) hanging the shirt on the drying rack (Fig. 10(a)). Each milestone contributes fractional score towards the overall task progress. 12 Figure 10 Experiment Results on Dexterous Cloth Manipulation. (a) Sankey diagram of success (solid) and failure (hatch) across entire rollouts of the Basic setting. (b) Average task progresses of π0 and GR-3 in the three evaluation settings. Results. Results are shown in Fig. 10. GR-3 outperforms π0 in all three evaluation settings. It achieves an average task progress of 86.7% and 83.9% in Basic and Position, respectively, showcasing its proficiency in handling complex dexterous tasks and its robustness to positional variations. Furthermore, GR-3 is able to generalize to unseen clothes with novel patterns and sleeve length, achieving an average task progress of 75.8%. To probe into the rollout process, we show the Sankey diagram of success and failure across the four milestones of the Basic setting in Fig. 10(a). The most challenging part for both model is to place the left shoulder on the hanger after the right shoulder. This is because the robot needs to pull out the left collar, which is often folded behind the hanger, for grasping while holding the hanger. Another failure mode occurs when the hanger slips away from the gripper during the course of placing the left shoulder on it, leading to failures in the last step."
        },
        {
            "title": "6 Related Work",
            "content": "Generalist Manipulation Policies. Building generalist manipulation policies capable of following instructions to effectively interact with the physical world has been long-standing challenge in robotics research [3, 6, 7, 9 11, 29, 37, 43, 44, 64, 66, 68]. Prior works [33, 53, 61, 74] propose to learn representations from large-scale data for downstream policy learning, enabling more robust robot behaviors in complex tasks. Recent advancements in vision-language-action (VLA) models adopt various approaches to enhance policy generalization and improve manipulation capabilities. popular line of works [9, 18, 29, 37, 41, 49, 55, 58, 60, 68, 70] train policies with cross-embodiment data that encompasses trajectories collected from different robot embodiments [17, 21, 36, 55, 69]. Along with the real-world robot trajectories, others [9, 11, 29] leverage pre-trained vision-language models to develop robot policies, showcasing strong capabilities in generalization to unseen settings. Another way to improve generalization is to perform future prediction on web-scale video datasets [8, 13, 20, 28, 40, 73] or learn latent actions [7, 12, 77] from action-less videos. In this work, we introduce GR-3, VLA model that is jointly trained on both robot trajectories and vision-language data, and can be efficiently fine-tuned with few-shot human trajectories. Throughout extensive experiments, we show that GR-3 is able to 1) strictly follow instructions and generalize to novel objects, environments, and instructions, 2) efficiently adapt to novel settings from few-shot human trajectory data, and 3) perform long-horizon and dexterous tasks with high robustness. Multi-Modal Co-Training for Robot Manipulation. Collecting real-world robot trajectories is costly and time-consuming. As result, broadening data sources becomes critical when it comes to scaling up policy training [32, 46]. popular approach is to initialize policies from pre-trained vision encoders [33, 50, 53, 61, 74] or more recently, pre-trained vision-language models [7, 9, 11, 29]. Built upon this framework, it is natural 13 to incorporate multi-modal data during training besides robot trajectories [11, 29, 58, 63, 76]. In particular, GATO [63] builds generalist agent capable of performing wide range of tasks, including image captioning and block stacking with real robot. It converts data from different modalities into sequence of tokens and trains large transformer for next-token prediction. RT-2 [11] demonstrates that co-fine-tuning large vision-language models on both robot trajectories and vision-language data significantly boosts generalization. By co-training on heterogeneous data, π0.5 [29] showcases exceptional generalization across environments and objects, enabling effective deployment in real-world unseen scenarios. In this work, GR-3 adopts similar co-training strategy. We curate well-designed web-scale vision-language dataset from multiple data sources and perform large-scale co-training on wide range of vision-language tasks. Through large-scale co-training, GR-3 showcases strong capabilities in zero-shot generalization to unseen objects and complex instructions that require abstract concept understanding. Leveraging Human Data for Policy Training. To improve data efficiency, incorporating human data into policy training has become popular approach in robotics research. popular line of works [1, 5, 51, 72] extract different types of representations from human videos to enhance policy training. Wu et al. [73] and Cheang et al. [13] propose to leverage large-scale human videos [16, 23, 24] for generative video pre-training. Recently, with the development of hand tracking and VR devices, Qiu et al. [59] and Kareer et al. [34] show that human videos with hand trajectories can improve policy performance on robot embodiments via co-training with small amount of robot data. In this report, we follow this line of research and show that GR-3 can effectively adapt to novel settings through few-shot learning from human trajectories."
        },
        {
            "title": "7 Limitations & Conclusions",
            "content": "Limitations and future work. Despite the powerful performance in challenging tasks, GR-3 has limitations. While it showcases strong generalization capabilities, it makes mistakes in following unseen instructions involving novel concepts and objects, and struggles with grasping objects with unseen shapes. We plan to scale up the model and training data to continue improving the model performance on handling novel scenarios. In addition, similar to all imitation learning methods, GR-3 can be stuck in out-of-distribution states during rollouts and fails to recover from failure. In the future, we plan to incorporate reinforcement learning (RL) to strengthen robustness on complex and dexterous tasks and optimize performance beyond the limitation of imitation learning. Conclusions. In this report, we introduce GR-3, powerful vision-language-action (VLA) model that outputs actions to control bi-manual mobile robot. We carefully study the model architecture and develop comprehensive training recipe that combines co-training with large-scale vision-language data, efficient fewshot learning from human trajectories, and effective imitation learning from robot trajectories. Extensive real-world experiments on three challenging tasks showcase that GR-3 excels in understanding complex instructions with abstract concepts, generalizes effectively to novel objects and environments, learns efficiently from minimal human trajectories, and performs long-horizon and dexterous tasks with exceptional robustness and reliability. We hope GR-3 can serve as step towards creating generalist robots capable of assisting humans with diverse tasks in the real world."
        },
        {
            "title": "8 Contributions and Acknowledgements",
            "content": "Authors contributed in the following areas are listed by alphabetical order. Data: Chilam Cheang, Sijin Chen, Yuxiao Liu, Haixin Shi, Yuyang Xiao Model Architecture: Hongtao Wu, Xin Xiao Training: Sijin Chen, Xiao Ma, Hongtao Wu, Xin Xiao, Jiafeng Xu, Yichu Yang Evaluation: Chilam Cheang, Sijin Chen, Yingdong Hu, Wenxuan Ou, Haixin Shi, Hongtao Wu, Xin Xiao, Yichu Yang Robot Development: Zhongren Cui, Liqun Huang, Hao Niu, Zeyu Ren, Jiawen Tian, Jiafeng Xu Robot System: Yifeng Li, Yuxiao Liu, Wenxuan Ou, Wanli Peng Writing: Sijin Chen, Yuxiao Liu, Xiao Ma, Zeyu Ren, Hongtao Wu, Yichu Yang Team Lead: Tao Kong, Hang Li We thank Jinming Guo, Xueyang Hu, Zetian Li, Xuguang Wei, Zhaohao Xiao, Degong Yang, Yifan Yang, Zhouqike Yang, and Yifei Zhou for their help on data curation. We thank Zeyu Long and Tingshuai Yan for their help on model deployment. We thank Yang Liu and Ming Zhao for their help on robot development and maintenance. We are sincerely grateful to all the teleoperators and annotators for their dedicated efforts on data collection and annotation."
        },
        {
            "title": "References",
            "content": "[1] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as versatile representation for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1377813790, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Jose Barreiros, Andrew Beaulieu, Aditya Bhat, Rick Cory, Eric Cousineau, Hongkai Dai, Ching-Hsin Fang, Kunimatsu Hashimoto, Muhammad Zubair Irshad, Masha Itkina, et al. careful examination of large behavior models for multitask dexterous manipulation. arXiv preprint arXiv:2507.05331, 2025. [4] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [5] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. arXiv preprint arXiv:2405.01527, 2024. [6] Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 47884795. IEEE, 2024. [7] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [8] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint arXiv:2310.10639, 2023. [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [11] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [12] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [13] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [14] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023. [15] Craig Coulter. Implementation of the pure pursuit path tracking algorithm. Technical report, 1992. [16] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. [17] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019. 16 [18] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey Levine. Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. arXiv preprint arXiv:2408.11812, 2024. [19] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [20] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. [21] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. [22] Clément Gosselin and Eric Lavoie. On the kinematic design of spherical three-degree-of-freedom parallel manipulators. The International Journal of Robotics Research, 12(4):394402, 1993. [23] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [24] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. [25] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [26] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. [27] Ryan Hoque, Peide Huang, David Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. [28] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. [29] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [30] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. [31] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84208429, 2019. [32] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [33] Siddharth Karamcheti, Suraj Nair, Annie Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. arXiv preprint arXiv:2302.12766, 2023. [34] Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, and Danfei Xu. Egomimic: Scaling imitation learning via egocentric video. arXiv preprint arXiv:2410.24221, 2024. [35] Benjamin Katz, Jared Di Carlo, and Sangbae Kim. Mini cheetah: platform for pushing the limits of dynamic quadruped control. In 2019 international conference on robotics and automation (ICRA), pages 62956301. IEEE, 2019. 17 [36] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [37] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [38] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. [39] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [40] Peiyan Li, Hongtao Wu, Yan Huang, Chilam Cheang, Liang Wang, and Tao Kong. Gr-mg: Leveraging partiallyannotated data via multi-modal goal-conditioned policy. IEEE Robotics and Automation Letters, 2025. [41] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. [42] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [43] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. [44] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. [45] Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. [46] Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint arXiv:2410.18647, 2024. [47] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [48] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. [49] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. [50] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, VincentPierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? Advances in Neural Information Processing Systems, 36:655677, 2023. [51] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. arXiv preprint arXiv:2308.10901, 2023. [52] Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, and Stuart Russell. Should robots be obedient? arXiv preprint arXiv:1705.09990, 2017. [53] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. [54] Dragomir Nenchev, Yuichi Tsumaki, and Mitsugu Takahashi. Singularity-consistent kinematic redundancy resolution for the srs manipulator. In 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), volume 4, pages 36073612. IEEE, 2004. 18 [55] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [56] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [57] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [58] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [59] Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David Yoon, Ryan Hoque, Lars Paulsen, et al. Humanoid policy human policy. arXiv preprint arXiv:2503.13441, 2025. [60] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. [61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [62] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. [63] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. [64] Moritz Reuss, Ömer Erdinç Yağmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. arXiv preprint arXiv:2407.05996, 2024. [65] Luis Sentis and Oussama Khatib. whole-body control framework for humanoids operating in human environments. In 2006 IEEE International Conference on Robotics and Automation (ICRA), pages 26412648. IEEE, 2006. [66] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. [67] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [68] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. [69] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [70] Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. Advances in neural information processing systems, 37:124420124450, 2024. [71] Yaqing Wang, Quanming Yao, James Kwok, and Lionel Ni. Generalizing from few examples: survey on few-shot learning. ACM computing surveys (csur), 53(3):134, 2020. [72] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. 19 [73] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. [74] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. [75] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. [76] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025. [77] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. [78] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        }
    ],
    "affiliations": []
}