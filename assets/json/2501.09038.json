{
    "paper_title": "Do generative video models learn physical principles from watching videos?",
    "authors": [
        "Saman Motamed",
        "Laura Culp",
        "Kevin Swersky",
        "Priyank Jaini",
        "Robert Geirhos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn ``world models'' that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at https://physics-iq.github.io; code at https://github.com/google-deepmind/physics-IQ-benchmark."
        },
        {
            "title": "Start",
            "content": "January 17, 2025 Do generative video models learn physical principles from watching videos? Saman Motameda,1, Laura Culpb, Kevin Swerskyb, Priyank Jainib,, and Robert Geirhosb, aINSAIT, Sofia University; work done while at Google DeepMind; bGoogle DeepMind; Joint last authors. 5 2 0 2 J 4 1 ] . [ 1 8 3 0 9 0 . 1 0 5 2 : r AI video generation is undergoing revolution, with quality and realism advancing rapidly. These advances have led to passionate scientific debate: Do video models learn world models that discover laws of physicsor, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, comprehensive benchmark dataset that can only be solved by acquiring deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at Physics-IQ-website; code at Physics-IQ-benchmark. Can machine truly understand the world without interacting with it? This question lies at the heart of the ongoing debate surrounding the capabilities of AI video generation models. While the generation of realistic videos has, for long time, been considered one of the major unsolved challenges within deep learning, this recently changed. Within relatively short period of time, the field has seen the development of impressive video generation models (13), capturing the imagination of the public and researchers alike. major milestone towards general-purpose artificial intelligence is to build machines that understand the world, and if you cannot understand what you cannot create (as Feynman would say), then the ability of those models to create visually realistic scenes is an essential step towards that capability. However, the degree to which successful generation signals successful understanding is the subject of passionate debate. Is it possible to understand the world without ever interacting with it? Phrased differently, do generative video models learn the physical principles that underpin reality from watching videos? Proponents argue that the way the models are trained predicting how videos continue, a.k.a. next frame prediction is task that forces models to understand physical principles. According to this line of argument, it is impossible to predict the next frame of sequence if the model has no understanding of how objects move (trajectories), that things fall down instead of up (gravity), and how pouring juice into glass of water changes its color (fluid dynamics). As an analogy, large language models are trained in similar fashion to predict the next tokens (characters or words) in text; task formulation that is equally simple but has proven sufficient to enable impressive capabilities and text understanding. Moreover, predicting the future is core principle of biological perception, too: The brain constantly generates predictions about incoming sensory input, enabling energy-efficient processing of information (4) and building mental model of the world as postulated by von Helmholtz (5) and later the predictive coding hypothesis (6). In short, successful prediction signals successful understanding. On the other hand, there are also important arguments contra understanding through observation. According to the causality rationale, watching videos (or to be more precise, training models to predict how videos continue) is passive process, with models unable to interact with the world. This lack of interaction means that model cannot observe the causal effects of an intervention (as, for instance, children are able to when playing with toys). Therefore, model is faced with the nearly impossible task of distinguishing correlation from causation if it is to succeed in understanding physical principles. Furthermore, video models that are touted as promising path towards building general purpose simulators of the physical world (1) arguably experience different world to begin with: the digital world as opposed to the real world that an embodied system (like robot, or virtually all living beings) experience. As consequence, skeptics argue that visual realism by no means signals true understanding: All it takes to produce realistic videos is to reproduce common patterns from the models vast sea of training datashortcuts without understanding (7, 8). In light of these two diametrically opposed perspectives, how can we tell whether generative video models indeed learn physical principles? To address this question in quantifiable, tractable way, we created challenging testbed for physical understanding in video models: the Physics-IQ benchmark. The core idea is to enlist video models to do what they do best: predict the continuation of video. In order to test understanding, we designed range of diverse scenarios where predicting the continuation requires deep understanding of physical principles, going beyond pattern reproduction and testing out-of-distribution generalization. For instance, models are asked to predict how domino chain fallsnormally, vs. when rubber duck is placed in the middle of the chain; or how pillows react when kettlebell vs. piece of paper is dropped onto the pillow. The diverse 1 Do generative video models learn physical principles from watching videos? Fig. 1. Sample scenarios from the Physics-IQ dataset for testing physical understanding in generative video models. Models are shown the beginning of video (single frame for image2video models; 3 seconds for video2video models) and need to predict how the video continues over the next 5 seconds, which requires understanding different physical properties: Solid Mechanics, Fluid Dynamics, Optics, Thermodynamics, and Magnetism. See here for an animated version of this figure. set of scenarios encompass solid mechanics, fluid dynamics, optics, thermodynamics and magnetism, totalling 396 highquality videos filmed from three different perspectives in controlled environment. Samples are shown in Figure 1. We then compare the models prediction to the ground truth continuation using set of metrics that capture different desiderata, and analyze range of current models: Sora (1), Runway Gen 3 (9), Pika 1.0 (10), Lumiere (11), Stable Video Diffusion (12), and VideoPoet (13). Physics-IQ benchmark Dataset. Our goal is to develop dataset that tests the physical understanding capabilities of video generative models on different physical laws like solid mechanics, fluid dynamics, optics, thermodynamics, and magnetism. We therefore created the Physics-IQ dataset which consists of 396 videos each 8 seconds long covering 66 different physical scenarios. Each scenario in our dataset focuses on specific physical law and aims to test video generative models understanding of physical events. These events include examples like collisions, object continuity, occlusion, object permanence, fluid dynamics, chain reactions, trajectories under the influence of forces (e.g., gravity), material properties and reactions, as well as lights, shadows, reflections, and magnetism. Each scenario was filmed at 30 frames per second (FPS) with resolution of 3840 2160 (16:9 aspect ratio) from three different perspectives: left, center, and right using highquality Sony Alpha a6400 cameras equipped with 16-50mm lenses. Each scenario was shot twice (take 1 and take 2 ) under identical conditions to capture the inherent variability of realworld physical interactions. These variations are expected in real-world due to factors like chaotic motion, subtle changes in friction, and variations in force trajectory. In this paper, we refer to the differences observed between these two recordings of the same scenario as physical variance. This results in total of 396 videos (66 scenarios 3 perspectives 2 takes). All our videos are shot from static camera perspective without camera motion. The setup for filming the videos is illustrated in fig. 8. The full dataset and code for evaluating model predictions is open-sourced here: https://github.com/google-deepmind/physics-IQ-benchmark model can predict how challenging, unusual video continues such as chain of dominoes with rubber duck in the middle interrupting the chain. Out-of-distribution scenarios like these test true understanding, since they cannot be solved by reproducing patterns seen or memorized from the training data (e.g. 7, 1416). We therefore test physical understanding of video generative models by taking full video of 8 seconds in which physically interesting event occurs, splitting the video into 3-second conditioning video and 5-second test video which acts as ground truth. The model is then given the conditioning signal: either the 3-second video for video2video models (named multiframe models in figures), or the last frame of thiscalled the switch framein the case of image2video models (named i2v models in figures). Since video models are trained precisely to generate the next frames given the previous frame(s) as conditioning signal, our evaluation protocol matches the paradigm these models were trained for. The switch frame is carefully selected manually for each scenario such that enough information about the physical event and objects in the scenario is provided, while at the same time making sure that successfully predicting the continuation requires some understanding of physics (e.g., in the scenario involving the chain reaction when domino falls, the switch frame corresponds to the moment when the first domino is tipped but has not yet contacted the second domino). We provide video models that support multi-frame conditioning with as many conditioning frames (up to maximum of 3 seconds) as they can accommodate. Some video models (e.g., Runway Gen 3, Pika 1.0, and Sora) generate subsequent frames based on single image. For these models, we provide just the switch frame as the conditioning signal. Figure 9 shows the switch frame for all scenarios in the Physics-IQ dataset. Both multiframe and single-frame conditioned video models can additionally be conditioned on human-written text description of the scene that describes the conditioning part without, however, giving away the answer of how the future unfolds. For evaluating image-to-video (i2v) and multiframe video models, we provide both the captions and the conditioning frame(s) as conditioning signals. Stable Video Diffusion is the only model in our study that does not accept text as conditioning signal. Evaluation protocol. Physical understanding can be measured in different ways. One of the most stringent tests is whether Why create real-world Physics-IQ dataset. The question of whether video generative models can understand physical 2 Do generative video models learn physical principles from watching videos? Fig. 2. Overview of the Physics-IQ evaluation protocol. video generative model produces 5 second continuation of the conditioning frame(s), optionally including textual description of the conditioning frames for models that accept text input. They are compared against the ground truth test frames using four metrics that quantify different properties of physical understanding. The metrics are defined and explained in the methods section. Code to run the evaluation is available at Physics-IQ-benchmark. principles has been explored through range of benchmarks designed to evaluate physical reasoning. Physion (17) and its successor Physion++ (18) use object collisions and stability to assess models ability to predict physical outcomes and infer relevant properties of objects (e.g., mass, friction) during dynamic interactions. Similarly, CRAFT (19) and IntPhys (20) assess causal reasoning and intuitive physics, testing whether models can infer forces or understand object permanence. Intuitive physics has rich history in Cognitive Science and is concerned with understanding how humans build commonsense intuition for physical principles (e.g. 2131). Recent efforts have extended physical reasoning evaluation to generative video models. VideoPhy (32) and PhyGenBench (33) focus on assessing physical commonsense through text-based descriptions rather than visual data. These works emphasize logical reasoning about physical principles but do not incorporate real-world videos or dynamic visual contexts. The Cosmos project (34) aims to enable better embodied AI, including robotics. LLMPhy (35) combines large language model with non-differentiable physics simulator to iteratively estimate physical hyperparameters (e.g., friction, damping, layout) and predict scene dynamics. Other benchmarks, such as CoPhy (36) and CLEVERER (37), emphasize counterfactual reasoning and causal inference in video-based scenarios. ESPRIT (38) couples physical reasoning tasks with explainability via natural language explanations, and PhyWorld (39) evaluates the ability of generative video models to encode physical laws, focusing on physical realism. However, major drawback of these benchmarks is that the data they use is synthetic (see Fig 3 for samples). This introduces real-vs-synthetic distribution shift that may confound results when testing video models trained on natural videos. The Physics-IQ dataset addresses this limitation by providing real-world videos, capturing diverse and complex physical phenomena (see Fig 1). With three views per scenario, controlled and measured physical variance (by recording two takes for each video), and challenging out-of-distribution settings it provides rigorous design for evaluating video generative models. Models. We evaluate eight different video generative models on our benchmark: VideoPoet (both i2v and multiframe) (13), Lumiere (i2v and multiframe) (11), Runway Gen 3 (i2v) Fig. 3. qualitative overview of recent synthetic datasets related to physical understanding (1720, 3639). These datasets are great for the purposes they were designed for, but not ideal for evaluating models trained on real-world videos due to the distribution shift. (9), Pika 1.0 (i2v) (10), Stable Video Diffusion (i2v) (12), and Sora (i2v) (1). We note that Luma (40) and Veo2 (2) are two other popular video generative models that have not been included in our benchmark, because the Luma Labs usage policy prohibits benchmarking and because Veo2 is not generally available at the time of writing. Different models have different requirements for the input conditions (single frame, multi frame, or text conditioning), frame rates (830 FPS), and resolution (between 256 256 and 1280 768). An overview is shown in Table 2. For our study, we matched the models preferred input conditions, frame rates, and resolution exactly by performing pre-processing step on the Physics-IQ videos (see Algorithm 1 for pseudocode). VideoPoet and Lumiere are the only two models in our study that can take multiple frames as conditioning input. These models also include super-resolution stage, where they first generate lower resolution video and subsequently upscale it to higher resolution. Since we noticed that the lower resolution outputs suffice to test physical realism, we skipped the super-resolution step for these models. The benchmark consists of physical interactions where temporal information is decidedly useful to have, thus it is generally to be expected that multiframe models should, in principle, be able do better than i2v models. Do generative video models learn physical principles from watching videos? Metrics for physical understanding. Video generative models commonly use metrics for evaluating the visual quality and realism of the generated videos. These metrics include Peak Signal-to-Noise Ratio (PSNR) (41), Structural Similarity Index Measure (SSIM) (42), Frechet Video Distance (FVD) (43), and Learned Perceptual Image Patch Similarity (LPIPS) (44). These metrics are useful for comparing the appearance, temporal smoothness, and statistics of generated videos with the ground truth. Unfortunately, these metrics are not equipped to assess the understanding of physical laws by video models. For instance, both PSNR and SSIM evaluate pixellevel similarities but are not sensitive to the correctness of motion and interactions in video; FVD captures overall feature distributions but does not penalize model for physically implausible actions and LPIPS focuses on humanlike perception of similarity rather than physical plausibility. While these metrics are great for measuring what they were designed for, they are not equipped to judge whether model understands real-world physics. In our benchmark, we use the following four metrics to track different aspects of physical understanding: Where does action happen? Spatial IoU Where & when does action happen? Spatiotemporal IoU Where & how much action happens? Weighted spatial IoU How does action happen? MSE These four metricsexplained in detail beloware then combined into single score, the Physics-IQ score, by summing the individual scores (with negative sign for MSE where lower= better). This Physics-IQ score is normalized such that physical variancethe upper limit of what we can reasonably expect model to captureis at 100%. Where does action happen? Spatial IoU The location of movement is an important indicator of physical correctness. For instance, in the domino with duck interrupting the chain scenario from Figure 1, only the part of the chain that is to the right side of the duck should tumble, while the other part should remain unchanged. Similarly, the spatial trajectory of moving ball is indicative of whether the movement is realistic. Our Spatial IoU metric compares generated videos against ground truth to determine whether the location of movement/action mirrors ground truth. Since the benchmark videos are filmed from static perspective without camera movement, simple threshold on pixel intensity changes across frames (see Algorithm 2 for pseudocode) easily identifies where movement happens. This leads to binary motion mask video that highlights the regions of motion in scene at any point in time. Spatial IoU then simply generates binary spatial motion mapsimilar, in spirit, to saliency mapby collapsing the masks across the time dimension with max operation. motion map thus simply has 1 whenever action occurred, at any point in time, at particular location; and 0 otherwise. This motion map is compared against the motion map from the real, ground truth video, using Intersection over Union or IoU (a metric commonly used in object detection to measure overlap while penalizing areas that differ): Spatial-IoU = real binary,spatial binary,spatial binary,spatial gen binary,spatial gen real where spatial are the motion maps based on real and generated videos, respectively. Spatial IoU measures whether the location where an action happens is correct. and spatial real gen SpaWhere & when does action happen? Spatiotemporal IoU tiotemporal IoU goes step further than Spatial IoU by also taking into account when an action occurs. Instead of collapsing across time as Spatial IoU does, Spatiotemporal IoU compares the two motion mask videos (based on real and generated videos) frame-by-frame, averaging across t: Spatiotemporal-IoU(Mreal, Mgen) = Mreal Mgen Mreal Mgen where Mreal and Mgen are the binary motion masks for the real and generated videos, respectively. Spatiotemporal IoU thus tracks not only where an action occurs in video, but also whether it occurs at the right time (when). If model does well on Spatial IoU but poorly on Spatiotemporal IoU, this would therefore indicate that the model gets the location of the action right, but the timing wrong. Where does action happen, and & how much action happens? Weighted spatial IoU Weighted spatial IoU is similar to Spatial IoU in the sense that it compares two motion maps. However, instead of comparing binary motion maps (action occurred or did not occur), it also assesses how much action happens at any given location. This distinguishes between e.g. motion caused by pendulum (showing repeated motion in an area) from motion by rolling ball (which passes location only once). Weighted spatial IoU is computed by taking the binary motion mask video (described above in the section on Spatial IoU) and collapsing across the time dimension in weighted fashion (instead of taking the maximum). The weighting simply averages per-frame action. This weighted spatial motion map is then used to compute the metric by summing the pixel-wise minimum of two motion maps and dividing by the pixel-wise maximum: Weighted-spatial-IoU = Pn Pn i=1 i= min (cid:0)M weighted,spatial max (cid:0)M weighted,spatial real,i real,i gen,i , weighted,spatial , weighted,spatial gen,i (cid:1) (cid:1) and weighted,spatial gen where weighted,spatial are the weighted motion real maps representing how much activity/action happes at any location (based on real and generated videos, respectively). Weighted spatial IoU thus measures not only where an action occurs, but also how much action is happening. How does an action happen? MSE Finally, mean squared error (MSE) calculates the average squared difference between corresponding pixel values in two frames (e.g., real and generated frame). Given two frames freal and fgen, the MSE is given by: MSE(freal, fgen) = 1 i=1 (freal,i fgen,i)2 where is the total number of pixels in the frame. MSE focuses on pixel-level fidelity; this is very strict requirement that is sensitive to how objects look and interact. For instance, if 4 Do generative video models learn physical principles from watching videos? Fig. 4. How well do current video generative models understand physical principles? Left. The Physics-IQ score is an aggregated measure across four individual metrics, normalized such that pairs of real videos that differ only by physical randomness score 100%. All evaluated models show large gap, with the best model scoring 24.1%, indicating that physical understanding is severely limited. Right. In addition, the mean rank of models across all four metrics is shown here; the Spearman correlation between aggregated results on the left and mean rank on the right is high (-.87, < .005), thus aggregating to single Physics-IQ score largely preserves model rankings. generative model would show tendency to change the color of objects, this physically unrealistic event would be heavily penalized. MSE therefore tracks aspects that complement the three other metrics. None of them is perfect, and none of them should be used in isolation, but collectively they provide comprehensive assessment of different aspects that quantify physical realism. Since raw MSE values can be hard to interpret, we provide an intuition in Figure 10. Metric for visual realism: MLLM evaluation. In addition to measuring the physical realism, we are interested in tracking how convincingly model can generate realistic videos, as assessed by multimodal large language model or MLLM (in our case: Gemini 1.5 Pro, (45)). Instead of rating videos (which would be sensitive to model biases), we use the gold standard experimental methods from psychophysics, 2AFC paradigm. 2AFC stands for two-alternative-forced-choice. In our case, this means that the MLLM is given pairs of real and generated videos of the same scenario in randomized order. The MLLM is asked to identify the generated video. The MLLM evaluation score is expressed as percentage corresponding to the accuracy across all videos, with chance rate at 50%. Any accuracy that is higher indicates that the MLLM was able to correctly identify at least some of the generated videos; while accuracies close to 50% indicates that video generative model has successfully deceived the MLLM into classifying the generated videos as real, indicating high visual realism. Details on the experiment are described in the appendix."
        },
        {
            "title": "Results",
            "content": "Physical understanding. The goal of our Physics-IQ benchmark is to understand, and quantify, whether generative video models learn physical principles. Therefore, we test all eight models in our study on every scenario and for each camera position (left, center, right) in the benchmark dataset. These samples are visualized in Figure 1. We first report the aggregated Physics-IQ results across all metrics related to physical understanding (Spatial-IoU, Spatiotemporal-IoU, Weighted-spatial-IoU, MSE) in Figure 4. The main takeaway from the left part of this figure is that all the models show massive gap to the physical variance baseline, with the best model scoring only 24.1% out of the possible 100.0%. As we mentioned in the previous section, each scenario was recorded twice (take 1 and take 2 ) to estimate the natural variability in real-world physical phenomena. This estimate is termed the physical variance; the figure is normalized such that pairs of real videos that differ only by physical randomness score 100.0%. The gap between model performance and real videos demonstrates severe lack of physical understanding in current powerful video generative models. Across the different models, VideoPoet (multiframe) (13) ranks best; interestingly, VideoPoet is causal model. For the two models that have both an image2video (i2v) and version conditioned on multiple frames (multiframe), the multiframe variants do better than the i2v variants. This is to be expected given that in order to predict the future as we require models to do on our challenging Physics-IQ benchmark, having access to temporal information (as multiframe variants have) should help. Zooming in on these overall results, Figure 6 breaks down model performance into different categories that include solid mechanics, fluid dynamics, optics, thermodynamics, and magnetism. While there is no category that can be considered solved, performance varies across categories, with some showing promising indications and differences across models. Interestingly, all models perform much better on Spatial-IoU, metric that has the weakest requirement in the sense that it is only sensitive to where an action occurred, not whether it occurred at the right time (as Spatiotemporal-IoU would track) or whether it had just the right amount of action 5 Do generative video models learn physical principles from watching videos? Fig. 5. Relationship between visual realism and physical understanding. Left. multimodal large language model (Gemini 1.5 Pro) is asked to identify the generated video among the real and the generated video for each scenario (MLLM score) in two-alternative forced choice paradigm. Chance rate is 50%; lower scores indicate that the model finds it harder to tell apart generated from real videos (= better realism). Sora-generated videos are hardest to distinguish from real videos for the model, whereas Lumiere (multiframe) is easiest. Right. Do models that produce realistic-looking videos (as assessed by the MLLM score) also score better in terms of physical understanding (as assessed via the Physics-IQ score)? This scatterplot with linear fit and 95% confidence interval as shaded blue area shows that this is not the case: Visual realism is uncorrelated with physical understanding (Pearsons = - 0.46, p=.247 not significant). Note that the axis on this plot is inverted for easier interpretation (up & right are best). Fig. 6. Performance comparison of video generative models across different physical categories (columns) and metrics (rows). For the top three metrics, higher is better; for the last metric lower values are best. Throughout, physical variance (i.e., the performance that is achievable by real videos differing only by physical randomness) is indicated by dashed line. Across metrics and categories, models show considerable lack in physical understanding. More lenient metrics like Spatial-IoU (top row) that only assess where an action occurred lead to higher scores than more strict metrics that also take into account e.g. when or how much action should be taking place. 6 Do generative video models learn physical principles from watching videos? Fig. 7. We here visualize success and failure scenarios within the fluid dynamics and solid mechanics categories for the two best models, VideoPoet and Runway Gen 3, according to our metrics. Both models are able to generate physics plausible frames for scenarios such as smearing paint on glass (VideoPoet) and pouring red liquid on rubber duck (Runway Gen 3). At the same time, the models fail to simulate ball falling into crate or cutting tangerine with knife. See here for an animated version. (as measured by Weighted-spatial-IoU). Furthermore, even relatively simple metric like MSE shows large gap between physically realistic videos and model-generated predictions. The performance of each model on each individual metric is reported in Table 1. As expected based on the aggregated results, VideoPoet (multiframe) performs best on majority of the metrics (3 out of 4), while Stable Video Diffusion performs best on Weighted-spatial-IoU followed by VideoPoet. The fact that Stable Video Diffusion does well in terms of producing action, while struggling with correct object placement and dynamics (as evidenced by weaker Spatial-IoU and Spatiotemporal-IoU scores) is also qualitatively evident in the generated videos which show more than usual movement, with the movement frequently being physically implausible and exaggerated. Similarly, Sora is among the best performing models for Weighted-spatial-IoU but performs weaker on other metrics. Qualitatively, the generated videos from Sora are often visually and artistically superior, but they also frequently include transition cutsdespite instructed not to change the camera perspectivewhich is penalized by several other metrics. We expect that if future version of this model more closely follows the prompt (static camera perspective, no camera movement), its Physics-IQ score would improve substantially. Qualitatively, the best and worst performing scenarios for the highest and lowest ranked models are visualized in Figure 7. These scenarios highlight both instances of physical understanding and failure cases. Visual realism: Multimodal large language model evaluation. Our results indicate striking lack of physical understanding in current generative video models. Why, then, do samples from many of those models that are circulated online look so realistic? We decided to quantify the visual realism of model-generated videos by asking capable multimodal large language model, Gemini 1.5 Pro (45), to identify the generated one out of pair of two videos for each scenario in the PhysicsIQ benchmark). The result of this experiment is presented in Figure 5 (left). The MLLM score evaluates models ability to generate videos that can deceive multimodal large language Model Physical Variance VideoPoet (multiframe) Runway Gen 3 (i2v) Lumiere (multiframe) VideoPoet (i2v) Lumiere (i2v) Stable Video Diffusion (i2v) Pika 1.0 (i2v) Sora (i2v) Spatial IoU 0.645 0.245 0.220 0.170 0.175 0.138 0.139 0.151 0.142 Spatiotemporal IoU 0.512 0.143 0.109 0.146 0.106 0.165 0.054 0.034 0.041 Weighted spatial IoU 0.626 0.054 0.044 0.034 0.057 0.024 0.088 0.026 0. MSE 0.002 0.010 0.015 0.013 0.012 0.016 0.021 0.014 0.036 Physics-IQ Score 100.0 24.1 18.4 18.2 18.0 17.1 13.5 9.5 8.7 Table 1. Comparison of metric scores for different models. The best-performing model for each metric is marked in bold. Note that Physical Variance serves as performance upper bound for each metric, representing the difference between two real videos and capturing the inherent variability in real-world scenarios. model into classifying them as real. Accuracies that are closer to chance performance (50% by randomly guessing) indicate that the MLLM finds it harder to tell apart real from generated videos, thus attesting to the visual realism of the generated video. Overall, the MLLM frequently succeeds in identifying the model-generated video (up to 86.9% accuracy for Lumiere multiframe). However, one model stands out: Sora achieved the best MLLM score of 55.6%, outperforming all other models by significant margin. Runway Gen 3 and VideoPoet (multiframe) ranked second and third, with scores of 74.8% and 77.3%, respectively, albeit with considerable gap behind Sora. This finding quantifies that highly capable models such as Sora indeed succeed in generating visually realistic videos even though their ability to understand physical principles, as shown in the previous section, is very limited. In fact, there is no significant correlation between visual realism and physical understanding: Figure 5 (right) demonstrates that there is distinction between generating realistic videos and comprehending the physical principles of reality."
        },
        {
            "title": "Discussion",
            "content": "We introduced Physics-IQ, challenging and comprehensive real-world benchmark to evaluate physical understanding in video generative models. We analyzed eight models on PhysicsIQ and proposed metrics to quantify physical understanding. 7 Do generative video models learn physical principles from watching videos? The benchmark data and metrics cover wide range of settings and reveal striking discrepancy between visual realism (sometimes present in current models) and physical understanding (largely lacking in current models). Do video models learn physical principles? We investigated the question whether the ability of video generative models to generate realistic-looking videos also implies that they have acquired an understanding of the physical principles that govern reality. Our benchmark shows that this is not the case: all evaluated models currently lack deep understanding of physics. Even the highest-scoring model, VideoPoet (multiframe), only achieves score of 24.1, falling significantly short of the best possible score of 100.0 obtained from the physical variance baseline, which quantifies the natural variability observed between real-world videos. That said, our results dont imply that future models cannot learn better physical understanding through next frame prediction. It remains an open question whether simply scaling the same paradigm further might solve this, or whether alternative (and possibly more interactive) training schemes might be required. Given the success of scaling deep learning, we are optimistic that future-frame prediction alone could lead to much better understanding: while successful prediction does not guarantee successful understanding, acquiring better understanding should certainly help with successful prediction. Learning physics by predicting pixels may sound challenging, but language models are known to learn syntax and grammar from text prediction alone (46). It may be worth pointing out that even though the models in our study often failed to generate physically plausible continuations, most current models were already successful on some scenarios. For example, the highest-ranking model, VideoPoet (multiframe), displayed remarkable physical understanding in certain scenariossuch as accurately simulating paint smearing on glass. In contrast, many lower-ranking models exhibited fundamental errors, such as physically implausible interactions (e.g., objects passing through other objects). Separate studies (e.g. 39) based on synthetic datasets have shown that given large enough dataset size, video models are able to learn specific physical laws. We consider it likely that as models are trained on larger and more diverse corpora of videos, their understanding of real-world physics will continue to improve. We hope that quantifying physical understanding, as we aim to do by open-sourcing the physics-IQ benchmark, might facilitate tracking progress in this area. Visual realism doesnt imply physical understanding. We observed striking discrepancy between visual realism and physical understanding: those two properties are not statistically significantly correlated (Figure 5), thus models that produce the most realistic-looking videos dont necessarily show the most physically-plausible continuations. For instance, in scenario where burning matchstick is lowered into glass full of water (leading to the flame being extinguished), Runway Gen 3 generates continuation where as soon as the flame touches the water, candle spontaneously appears and is lit by the match. Every single frame of the video is high quality in terms of resolution and realism, but the temporal sequence is physically impossible. Such tendency to hallucinate objects into existence is drawback of many generative models and In our an active area of research in deep learning (47). experiments, we observed hallucinations in all models, but more powerful models like Runway Gen 3 and Sora often hallucinated information that was at least consistent with the scenario (e.g. matchstick - candle) rather than arbitrary, indicating at least certain level of understanding. Dataset biases are reflected in model generations. We observed that most models were consistent in producing videos that aligned with the scene and viewpoint provided. Models like Sora and Runway Gen 3 were particularly strong at understanding the given scene and generating subsequent frames that were consistent with respect to object placement and their attributes (shape, color, dynamics). Interestingly, many models also demonstrated biased generations depending on properties of their training. For example, in prototyping experiments we observed that when given conditioning video of red pool table where one ball hits another, as Lumiere starts generating, it immediately turned the red pool table to green one, showing bias to commonly occurring green pool tables. Similarly, videos generated by Sora often featured transition cuts, possibly suggesting training paradigm optimized to generate artistic videos. Metrics and their limitations. Popular metrics to test quality and measure realism of generated videos include PSNR (41), FVD (43), LPIPS (44) and SSIM (42). However, designing metrics that quantify physical understanding in generated videos is challenging endeavor. We proposed suite of metrics to measure the spatial, temporal and perceptual coherence of video models. While individually none of these metrics is perfect, the combined insights from these metrics and the Physics-IQ score that integrates normalized score across these metrics provide holistic assessment of the strengths and weaknesses of video models. That said, none of these metrics directly quantify any physical phenomena and instead serve as proxies. For instance, the MLLM metric provided way to quantify how much generated videos deceive multimodal model. However, the metric is limited by the capability of the underlying multimodal large language model (MLLM) itself. In our analysis, we found that while the MLLM was frequently able to identify generated videos (except for videos generated by Sora), its explanations for the decision were often wrong. As another example, we observed that Stable Video Diffusion often generated videos with large amounts of hallucinations and implausible object motions; nonetheless, it was the best performing model on Weighted-spatial-IoU showing that no metric should be assessed in isolation. This is also evidenced by the fact that e.g. Runway Gen 3 did very well in terms of the spatial location of actions (Spatial-IoU) while scoring poorly on temporal consistency (Spatiotemporal-IoU). We intentionally designed Physics-IQ to be challenging benchmark in order to provide useful signal for model development in the future; in this context it may be worth noting that our metrics may be on the conservative side by strongly penalizing object hallucinations, camera movement (which we prompted models not to do) or shot changes. For instance, Sora tends to show these more frequently than other models, leading to low scores on some metrics. This is not ideal, but we believe that in an area like deep learning where hype is omnipresent, scientific benchmarks should rather err on the side of caution. Do generative video models learn physical principles from watching videos? Outlook. The obvious trends are better models trained on bigger datasets. Will these models essentially solve physical understandingor instead, hit limit after which one can only improve ones understanding the world by interacting with it? The jury is still out on this question, but the benchmark and analyses introduced in this article might help quantifying this either way. In addition to future models, improvements could also come from inference-time compute, such as scaling the number of samples. If this would lead to strong results, it would raise the following question: from models perspective, is reality but one possibility among infinitely many others? The authors would like to thank David Acknowledgments. Fleet, Been Kim, Pieter-Jan Kindermans, Kelsey Allen, Jasmine Karimi, Katherine Hermann, Mike Mozer, Phoebe Kirk, Saurabh Saxena, Daniel Watson, Meera Hahn, Sara Mahdavi, Tim Brooks, Charles Herrmann, Isabelle Simpson, Jon Shlens, and Chris Jones for helpful discussions and/or supporting this project in various ways. References. 1. OpenAI, Sora: OpenAIs Multimodal Agent (https://openai.com/index/sora/) (2024) Accessed: 2024-11-24. 2. DeepMind, Veo2: Our state-of-the-art video generation model (https://deepmind.google/technologies/veo/veo-2/) (2024) Accessed: 2025-01-09. 3. Meta AI, Meta Movie Gen: AI-powered movie generation (https://ai.meta.com/research/movie-gen/) (2024) Accessed: 2024-11-24. 29. JR Kubricht, KJ Holyoak, Lu, Intuitive physics: Current research and controversies. Trends cognitive sciences 21, 749759 (2017). 30. JB Tenenbaum, Kemp, TL Griffiths, ND Goodman, How to grow mind: Statistics, structure, and abstraction. science 331, 12791285 (2011). 31. LS Piloto, Weinstein, Battaglia, Botvinick, Intuitive physics learning in deep-learning model inspired by developmental psychology. Nat. human behaviour 6, 12571267 (2022). 32. Bansal, et al., Videophy: Evaluating physical commonsense for video generation (2024). 33. Meng, et al., Towards world simulator: Crafting physical commonsense-based benchmark for video generation (2024). 34. Agarwal, et al., Cosmos world foundation model platform for physical AI. arXiv preprint arXiv:2501.03575 (2025). 35. Cherian, Corcodel, Jain, Romeres, Llmphy: Complex physical reasoning using large language models and world models (2024). 36. Baradel, Neverova, Mille, Mori, Wolf, Cophy: Counterfactual learning of physical dynamics. arXiv preprint arXiv:1909.12000 (2019). 37. Yi, et al., Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442 (2019). 38. NF Rajani, et al., Esprit: Explaining solutions to physical reasoning tasks. arXiv preprint arXiv:2005.00730 (2020). 39. Kang, et al., How far is video generation from world model: physical law perspective (2024). 40. LA Team, Luma ai (https://lumalabs.ai) (2024) Generative AI platform specializing in 3D content and photorealistic modeling. 41. Hore, Ziou, Image quality metrics: Psnr vs. ssim in 2010 20th international conference on pattern recognition. (IEEE), pp. 23662369 (2010). 42. Wang, AC Bovik, HR Sheikh, EP Simoncelli, Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 600612 (2004). 43. Unterthiner, et al., Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018). 44. Zhang, Isola, AA Efros, Shechtman, Wang, The unreasonable effectiveness of deep features as perceptual metric in Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586595 (2018). 45. GTGP Georgiev, other authors, Gemini 1.5: Unlocking multimodal understanding across 4. HB Barlow, , et al., Possible principles underlying the transformation of sensory messages. millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). Sens. communication 1, 217233 (1961). 5. von Helmholtz, Handbuch der physiologischen Optik: mit 213 in den Text eingedruckten Holzschnitten und 11 Tafeln. (Voss) Vol. 9, (1867). 6. Friston, theory of cortical responses. Philos. transactions Royal Soc. B: Biol. sciences 360, 815836 (2005). 46. Hewitt, CD Manning, structural probe for finding syntax in word representations in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 41294138 (2019). 47. Rawte, Sheth, Das, survey of hallucination in large foundation models. arXiv preprint 7. Geirhos, et al., Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 665673 arXiv:2309.05922 (2023). (2020). 8. Kang, et al., How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385 (2024). 9. Team, Runway (https://runwayml.com) (2024) Platform for AI-powered video editing and generative media creation. 10. PL Team, Pika labs (https://pikalabs.com) (2024) Generative AI platform for creating video and visual content. 11. Bar-Tal, et al., Lumiere: space-time diffusion model for video generation (2024). 12. Blattmann, et al., Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). 13. Kondratyuk, et al., VideoPoet: large language model for zero-shot video generation in Proceedings of the 41st International Conference on Machine Learning, Proceedings of Machine Learning Research, eds. Salakhutdinov, et al. (PMLR), Vol. 235, pp. 2510525124 (2024). 14. Geirhos, et al., Generalisation in humans and deep neural networks. Adv. neural information processing systems 31 (2018). 15. Hendrycks, Dietterich, Benchmarking neural network robustness to common corruptions and perturbations in International Conference on Learning Representations. (2018). 16. Srivastava, et al., Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022). 17. DM Bear, et al., Physion: Evaluating physical prediction from vision in humans and machines (2021). 18. HY Tung, et al., Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Adv. Neural Inf. Process. Syst. 36 (2024). 19. Ates, et al., Craft: benchmark for causal reasoning about forces and interactions. arXiv preprint arXiv:2012.04293 (2020). 20. Riochet, et al., IntPhys: framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616 (2018). 21. McCloskey, Caramazza, Green, Curvilinear motion in the absence of external forces: Naive beliefs about the motion of objects. Science 210, 11391141 (1980). 22. McCloskey, Intuitive physics. Sci. american 248, 122131 (1983). 23. PJ Kellman, ES Spelke, Perception of partly occluded objects in infancy. Cogn. psychology 15, 483524 (1983). 24. ES Spelke, Breinlinger, Macomber, Jacobson, Origins of knowledge. Psychol. review 99, 605 (1992). 25. ES Spelke, Kestenbaum, DJ Simons, Wein, Spatiotemporal continuity, smoothness of motion and object identity in infancy. Br. journal developmental psychology 13, 113142 (1995). 26. Gopnik, et al., theory of causal learning in children: causal maps and bayes nets. Psychol. review 111, 3 (2004). 27. Saxe, Carey, The perception of causality in infancy. Acta psychologica 123, 144 (2006). 28. Agrawal, AV Nair, Abbeel, Malik, Levine, Learning to poke by poking: Experiential learning of intuitive physics. Adv. neural information processing systems 29 (2016)."
        },
        {
            "title": "Supplementary Material",
            "content": "Do generative video models learn physical principles from watching videos? Algorithm 1 Change video FPS with linear interpolation Require: video file , original FPS fpsoriginal, new FPS fpsnew, output dimensions (w, h) (optional) Ensure: video with adjusted FPS and resolution 1: foriginal extract frames from at fpsoriginal 2: duration length of of 3: noriginal number of frames in fpsoriginal 4: nnew duration fpsnew 5: Initialize empty list nnew 6: for 0 to nnew 1 do 7: α (noriginal 1)/(nnew 1) α β α f1 foriginal[i] f2 foriginal[min(i + 1, noriginal 1)] finterpolated (1 β) f1 + β f2 if (w, h) is not None then resize finterpolated to (w, h) 8: 9: 10: 11: 12: 13: 14: Index of the first frame for interpolation Weight for linear interpolation append finterpolated to fnew 15: 16: recreate video from fnew with fpsnew 17: Save Algorithm 2 Generate binary mask video for moving objects Require: Video , output file , threshold τ , update rate α, averaging window size Ensure: Binary mask video highlighting moving objects 1: Initialize video reader for and writer for 2: Read first frames {f1, . . . , fw} and preprocess: grayscale and blur 3: Initialize background model 1 Pw i=1 fi Initial average reduces noise 4: for each frame ft in do 5: 6: 7: 8: Preprocess ft: grayscale and blur Update background (1 α) + α ft Compute difference dt ft Threshold mt 255 if dt > τ , else 0 Morphologically clean mt (opening and closing) 9: 10: Write mt to 11: Save and close Generating binary mask videos. This pseudocode describes method to generate binary mask videos that highlight moving objects. The algorithm combines background subtraction with adaptive updates and morphological operations to detect and cleanly segment motion in video frames. This approach is useful for creating spatial and temporal masks in Physics-IQ evaluations. MLLM evaluation prompt. The following prompt was used in the two-alternative forced-choice paradigm: Your task is to help me sort my videos. mixed up real videos that shot with my camera and similar videos that generated with computer. only know that exactly one of the two videos is the real one, and exactly one of the following two videos is the generated one. Please take look at the two videos and let me know which of them is the generated one. Ill tip you $100 if you do great job and help me identify the generated one. First explain your reasoning, then end with the following statement: For this reason, the first video is the generated one or For this reason, the second video is the generated one. 10 Fig. 8. Illustration of recording setup (top) and perspectives (bottom). Overview of all Physics-IQ scenarios. Figure 9 presents the switch frames (center view) from all 66 scenarios in the Physics-IQ dataset. These frames represent the last frame of the conditioning signal, after which model is asked to generate prediction for the future frames. Visualizing different MSE values. Figure 10 illustrates the relationship between distortion applied to video and MSE (Mean Squared Error) in scene. Note that none of the videos in the benchmark have distortion applied to them; instead, this is inteneded as an visual intuition for how much certain MSE value distorts an image. Model VideoPoet (i2v) VideoPoet (multiframe) Lumiere (i2v) Lumiere (multiframe) Stabble Video Diffusion (i2v) Runway Gen 3 (i2v) Pika 1.0 (i2v) Sora (i2v) Text Condition Multi-frame Condition Single-frame Condition FPS Resolution 8 8 16 16 8 24 24 30 128224 128224 128128 128128 1024576 1280768 1280720 854480 Table 2. Specifications of evaluated video models, including input conditioning, frame rate (FPS), and resolution. Adjusting video frame rate. This pseudocode outlines the method for changing the frame rate (FPS) of video using linear interpolation. It generates smooth transition between original frames while optionally resizing the output resolution. This technique ensures temporal consistency, making it well-suited for generating videos with desired FPS to adapt Physics-IQ for models with different FPS. Do generative video models learn physical principles from watching videos? Fig. 9. The switch frames (here: center view only) of all scenarios in the Physics-IQ benchmark. switch frame is the last conditioning frame before model is asked to predict 5 seconds of future frames. 11 Do generative video models learn physical principles from watching videos? Fig. 10. Since mean squared error (MSE) values can be hard to interpret, this figure shows the effect of distortion applied to the scene, serving as rough intuition for the effect of MSE at different noise levels."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "INSAIT, Sofia University"
    ]
}