{
    "paper_title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "authors": [
        "Atnafu Lambebo Tonja",
        "Srija Anand",
        "Emilio Villa-Cueva",
        "Israel Abebe Azime",
        "Jesujoba Oluwadara Alabi",
        "Muhidin A. Mohamed",
        "Debela Desalegn Yadeta",
        "Negasi Haile Abadi",
        "Abigail Oppong",
        "Nnaemeka Casmir Obiefuna",
        "Idris Abdulmumin",
        "Naome A Etori",
        "Eric Peter Wairagala",
        "Kanda Patrick Tshinu",
        "Imanigirimbabazi Emmanuel",
        "Gabofetswe Malema",
        "Alham Fikri Aji",
        "David Ifeoluwa Adelani",
        "Thamar Solorio"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)"
        },
        {
            "title": "Start",
            "content": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages Atnafu Lambebo Tonja1,, Srija Anand1,2,, Emilio Villa-Cueva1 *, Israel Abebe Azime3, Jesujoba O. Alabi3, Muhidin A. Mohamed4, Debela Desalegn Yadeta5, Negasi Haile Abadi6, Abigail Oppong7, Nnaemeka Casmir Obiefuna8, Idris Abdulmumin9, Naome A. Etori10, Eric Peter Wairagala11, Kanda Patrick Tshinu12, Imanigirimbabazi Emmanuel13, Gabofetswe Malema14, Alham Fikri Aji1, David Ifeoluwa Adelani15, Thamar Solorio1 1MBZUAI, 2AI4Bharat, Indian Institute of Technology, Madras, 3Saarland University, 4Aston University, 5Addis Ababa University, 6Lesan AI, 7Independent, 8Friedrich-Alexander University, 9University of Pretoria, 10University of Minneosta -Twin Cities, 11Lelapa AI,12Tshwane University of Technology, 13Kabale University, 14University of Botswana, 15Mila, McGill University & Canada CIFAR AI Chair 6 2 0 2 ] . [ 1 9 9 6 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Africa is home to over one-third of the worlds languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models show poor performance across evaluated cultures, with near-zero accuracy on openended VQA when queried using native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our AfriMCQA under academic license or CC BY-NC 4.0 on HuggingFace1."
        },
        {
            "title": "Introduction",
            "content": "Africa is one of the most culturally diverse and rapidly growing regions in the world. It is home to more than one-third of the worlds languages (Hammarström, 2018) and population exceeding 1.3 billion, projected to surpass 2.5 billion by 2050 (Simane et al., 2025). African languages have linguistic features that are very different from many high-resource languages represented in LLMs, including rich morphology, use of noun classes, * Equal contribution 1https://huggingface.co/datasets/Atnafu/Afri-MCQA Figure 1: Examples of Afri-MCQA datapoints, containing parallel text and speech QA pairs grounded in culturally relevant images across English and native African languages. tonality, and serial verb constructions, among others (Nurse and Philippson, 2006; Adebara and Abdul-Mageed, 2022). Additionally, many African languages are primarily spoken, with literacy often occurring in colonial or foreign language. This makes speech-based applications such as Multimodal large language models (MLLMs) particularly relevant for enabling access to technology. MLLMs perform well on tasks that require reasoning over visual, spoken, and textual inputs to follow user instructions (Liu et al., 2021a; Kamath et al., 2025; Abdin et al., 2024). However, these systems are developed primarily for high-resource, mostly Western-centric, or as Mihalcea et al. (2025) put it, they have WEIRD coverage. As result, their knowledge and reasoning abilities tend to favor the languages and cultures represented in their training data (Mihalcea et al., 2025). Although most evaluation datasets for lowresource languages are translated (Costa-jussà et al., 2022; Adelani et al., 2024, 2025; Azime et al., 2024; Alabi et al., 2025), researchers have recently made promising progress in creating culturally relevant benchmarks for African languages across various NLP tasks (Adelani et al., 2023; Azime et al., 2025; Muhammad et al., 2025; Yu et al., 2025; Tonja et al., 2024b,a). However, these evaluations are designed for text-only tasks. In the language-vision space, recent benchmarks assess global cultural knowledge through question answering (Romero et al., 2024; Vayani et al., 2025a; Winata et al., 2025), yet their coverage of African languages and contexts remains limited. Africas rich cultural diversity demands AI systems that can effectively serve its communities. critical step toward this goal is evaluating how well current MLLMs understand and reason about African cultural knowledge in multimodal settings. To enable this, we introduce Afri-MCQA , the first multilingual cultural VQA dataset supporting text and speech modalities (as shown in Figure 1). The dataset consists of 7.5k Q&A pairs (in English and native languages) across 15 African languages from 12 countries. Our data collection involves native speakers as annotators, who reside in the countries where their respective languages are spoken. Each language covers 500 imagegrounded Q&A samples, with both text and spoken audio in the native language and English. We evaluate multiple MLLMs across various setups to answer the following research questions: RQ1 : How well do MLLMs understand African cultural contexts in visually-grounded QA? RQ2 : How does input modality (text vs. speech) affect performance? RQ3 : How does query language (native vs. English) affect performance, and do differences reflect language understanding or cultural knowledge gaps? RQ4 : How does task format (Multiple-Choice vs. Open QA) affect accuracy? Our contributions are: (1) We introduce AfriMCQA, the first large-scale multilingual visual cultural QA benchmark for 15 African languages across 12 countries, with parallel text and speech QA created by native speakers. (2) We demonstrate that text-based multilingual ability does not transfer to speech understanding, emphasizing the need for more African language representation in multimodal training. (3) We release Afri-MCQA to advance multimodal research for Africas diverse languages and cultures. has received greater attention. For instance, Liu et al. (2021b) tested models on verifying statements about pairs of culturally related images, but this was framed merely as binary classification task. CVQA (Romero et al., 2024) and CulturalVQA (Nayak et al., 2024) explicitly target cultural knowledge through human-written questions, yet they are limited to small set of languages per continent. ALM-bench (Vayani et al., 2025a) expands the language coverage, but relies heavily on LLMgenerated questions and web-sourced images. Additionally, common limitation across these benchmarks is that they query models only through text, overlooking the importance of speech, an important modality for communities where language is mostly spoken. Previous work on African languages has focused mainly on text-based benchmarks. While some multilingual resources, such as GlobalMMLU (Singh et al., 2024), include small subset of African languages, other region-specific benchmarks provide broader coverage. Examples of these include MasakhaNEWS (Adelani et al., 2023) that evaluates text classification, AfriQA (Ogundepo et al., 2023) for cross-lingual question answering, or larger suites such as AfroBench (Ojo et al., 2025) and IrokoBench (Adelani et al., 2025) that cover multiple text tasks. Across these efforts, consistent finding is that models perform poorly on African languages, with open-weight models showing significant gap to proprietary systems. Despite this progress, all of these evaluations remain text-only, while visual knowledge and multimodal reasoning for African contexts are still largely untested. Existing multi-region multimodal datasets (Romero et al., 2024; Vayani et al., 2025b) include only few African languages and do not adequately capture the regions diversity. AfriMCQA addresses these gaps with three key contributions: (1) broader coverage with 15 languages across 12 countries, (2) inclusion of speech modality with parallel native and African-accented English audio, and (3) diagnostic control experiments that separate linguistic competence from cultural knowledge limitations."
        },
        {
            "title": "3 Dataset",
            "content": "With the growing popularity of LLMs and MLLMs, cultural and multilingual multimodal evaluation To create Afri-MCQA, we selected 15 widely spoken languages in sub-Saharan Africa (by number Datasets CVQA (Romero et al., 2024) WC-VQA (Winata et al., 2025) M5 (Schneider and Sitaram, 2024) HaVQA (Parida et al., 2023) Afri-MCQA (ours) # African Lang 5 1 4 1 15 # Countries QA categories 10 1 - - 10 4 1 4 1 # QA per langs Audio QA Parallel Data 200 - - 6,200 500 Table 1: Data statistics for Afri-MCQA compared to existing VQA datasets that include African languages. of speakers, according to Ethnologue2) across 12 countries, representing an aggregate speaker population of approximately 392.6 million (see Table 2). We hired native language speakers as annotators through Upwork.3 The selection criteria were based on (i) fluency in English, (ii) prior annotation or data collection experience, (iii) high project completion rate on Upwork, and (iv) residence in country where the target language is spoken. After the selection process, we divided the annotation into two phases designed to ensure quality."
        },
        {
            "title": "3.1 Dataset collection",
            "content": "Guideline and Platform Preparation We followed the annotation guidelines of Romero et al. (2024), including image categories, question templates, and distractors, and extended them to include audio recording instructions (for both native language and English) and adding two-step review and verification process. Full annotation guidelines are provided in Appendix I. Training and Screening Phase The first phase consisted of small-scale pilot designed to train and screen annotators. We provided detailed guidelines and training to ensure annotators understood the task criteria and quality standards. After training, each annotator submitted 50 QA samples. We reviewed these submissions to verify alignment with the guidelines. Based on this review, we provided detailed feedback and, when necessary, scheduled follow-up meetings to clarify issues. Only annotators whose submissions met our quality criteria and successfully incorporated feedback were selected for the next phase. Approved samples from this screening phase were included in the final dataset. Main Annotation Phase In this phase, the remaining 450 items per language were collected by annotators selected in the first phase. To ensure quality across the large volume of submissions, we involved language coordinators, experienced native speakers with strong linguistic and cultural knowledge for each language. Coordinators reviewed all submissions for linguistic accuracy, cultural appropriateness, adherence to guidelines, and audio quality. Review guidelines are provided in Appendix N. When issues arose, coordinators discussed them directly with annotators to resolve them. Unresolved disagreements were then raised with the project team for final decision. Language coordinators are co-authors of this paper and played major role in the quality assurance process. After their approval, the project team conducted final review to ensure overall data quality."
        },
        {
            "title": "3.2 Dataset Composition",
            "content": "Each data point in Afri-MCQA includes an image and set of carefully constructed multiple-choice questions in both text and speech modalities, both in English and the native language. Below, we describe each of these components. Image / Category Selection To build the image set for Afri-MCQA, we encouraged annotators to contribute their own images whenever possible. When self-sourcing was not feasible, we permitted the use of open-license images from websites we provided (see Appendix for list of websites). All collected images were categorized into the 10 classes defined by Romero et al. (2024). Figure 2 shows the distributions of images per category (See Appendix for category distributions across languages). Figure 2: Image categories in our dataset and their distributions. 2https://www.ethnologue.com/insights/ ethnologue200/ 3https://www.upwork.com/ Question & Answer Generation For each image, annotators wrote up to 3 multiple-choice QA Lang-Country Family / Branch Akan/Twi-Ghana Niger-Congo / Volta-Niger Amharic-Ethiopia Afro-Asiatic / Ethio-Semitic Chichewa-Malawi Niger-Congo / Bantu Hausa-Nigeria Afro-Asiatic / Chadic Igbo-Nigeria Niger-Congo / Volta-Niger Niger-Congo / Bantu Kikuyu-Kenya Kinyarwanda-Rwanda Niger-Congo / Bantu Niger-Congo / Bantu Luganda-Uganda Afro-Asiatic / Cushitic Oromo-Ethiopia Niger-Congo / Bantu Setswana-Botswana Afro-Asiatic / Cushitic Somali-Somalia Afro-Asiatic / Ethio-Semitic Tigrinya-Eritrea Niger-Congo / Volta-Niger Yoruba-Nigeria Niger-Congo / Bantu Sesotho-Lesotho Niger-Congo / Bantu Zulu-S.Africa Reg West East & West West East East East East South East East West South South #spk #QA #eng(h) #nat(h) 9M 57M 12M 77M 31M 8.1M 18M 10M 37M 14M 22M 9M 46M 13.5M 28M 537 500 501 496 501 495 501 500 512 502 501 537 498 533 528 2.41 1.56 1.41 2.80 1.61 1.66 2.73 2.30 2.20 1.89 1.96 2.13 1.98 2.43 1.51 1.50 3.04 1.59 1.72 2.67 2.42 2.30 2.39 1.99 2.31 2.06 1.90 1.37 Table 2: Overview of languages in Afri-MCQA. #spk = estimated L1 & L2 speakers, #eng (h) = hours of accented English audio, #nat (h) = hours of native language audio. indicates English audio not collected for that language. triplets (question + 1 correct answer + 3 distractors) in both their native language and English. To ensure the benchmark remains challenging, we instructed annotators to design complex questions that require reasoning to answer correctly (See annotation guidelines in Appendix I). Audio Recording To investigate spoken language understanding capabilities, we instructed annotators to record audio for each question and its corresponding answers, reading them clearly in both the native language and English. Therefore, Afri-MCQA includes audio recordings for both questions and answers in native language and African-accented English."
        },
        {
            "title": "4 Experimental setup",
            "content": "Our experimental design is organized to address each research question as follows: Models: To assess how well current MLLMs understand African cultural contexts (RQ1), we selected models based on two criteria: a) support for both image and audio input, enabling multimodal evaluation, and (b) availability of different model sizes within each family to assess scaling effects. From open-weight MLLMs, we selected Qwen 2.5-Omni (3B &7B) (Xu et al., 2025) and Gemma-3n-(2B & 4B)-it (Kamath et al., 2025). For text-only baselines, we include Gemma3 (12B & 27B)-it (Kamath et al., 2025). For comparison with closed-source models, we include Gemini-2.5 Pro (Comanici et al., 2025), which supports audio, text, and vision inputs. Query Modality: To explore how input modality affects performance (RQ2), we evaluate models using both text and audio modalities. For text evaluation, models receive the written version of the question. For audio evaluation, we use native speaker recordings of the same questions and options, allowing us to assess how well current MLLMs handle spoken language inputs both in African languages and accented English. This comparison shows whether model performance generalizes from text to speech. Since we evaluate VQA, all settings include the image related to the question. Query Languages: To explore how query languages affect models performance and whether gaps reflect linguistic or cultural limitations (RQ3), each question is presented in native language and English. This setup allows us to compare model behavior between English and native languages. Task Format: To understand how task format affects model performance (RQ4), we evaluate models on both Multiple-Choice VQA (MC-VQA) and Open-ended VQA. MC-VQA provides answer options, while Open-ended VQA requires answer generation. Comparing these formats shows whether strong MC-VQA performance reflects actual cultural understanding or simply selecting from provided options. For each task format, we use the same prompt templates across models and languages. Prompt: We evaluated all settings and models using Location-aware prompt (adding location/- country as context). We chose this setting as it performed better than image-only prompts (without providing context). We provide results for Imageonly prompts and language-wise results in Appendix C, and the prompts used are in Appendix B."
        },
        {
            "title": "5 Evaluation",
            "content": "This section describes our two evaluation setups and the metrics used in this study."
        },
        {
            "title": "5.1 Cultural VQA Evaluation",
            "content": "We evaluate models on visually-grounded cultural QA using Afri-MCQA in both modalities. For all tasks, models are provided with an image and must use visual information to answer the question. Text-based VQA: For the text modality, we use two evaluation formats: (1) MC-VQA: Models are given an image, text question, and four answer options. They must select the correct option by reasoning over the image and question. (2) Openended VQA: Models receive an image and text question without answer options, and are required to generate the correct answer. This tests their ability to retrieve and reason about cultural knowledge without potential hints in the answer set. Audio-based VQA: Similarly, we evaluate audio modality using two formats: (1) Audio MC-VQA: Same setting as MC-VQA described above, but models are queried through African-accented English and native language speech. (2) Audio openended VQA: Given an image and the question in speech format without answer options and models required to generate the correct answer in text."
        },
        {
            "title": "5.2 Control Experiments",
            "content": "While it is technically challenging to determine whether prediction failures on Afri-MCQA arise from limitations in language understanding or gaps in cultural knowledge, we conduct control experiments on easy tasks that primarily require language understanding in either text or speech form. These evaluations provide evidence to understand the extent to which language-understanding limitations may contribute to the observed failures. Text-based experiments: To probe text understanding, we evaluate on two benchmarks: (1) AfriXNLI (Adelani et al., 2025): natural language inference, and (2) AfriMMLU (Adelani et al., 2025): general knowledge QA. By analyzing performance on these text-only tasks and comparing it with results on Afri-MCQA, we obtain an approximate measure of the models baseline linguistic competence on the studied languages. Audio-based experiments: To probe audio understanding, we conduct two tasks: (1) ASR: transcribing spoken African language audio to text, assessing whether models can accurately capture spoken content as prerequisite for answering questions. (2) Language Identification (LID): identifying which of the 15 languages is spoken, testing the models ability to recognize spoken languages. These tasks reveal whether poor audio VQA results from speech-processing failures or cultural reasoning limitations."
        },
        {
            "title": "5.3 Evaluation metrics",
            "content": "We evaluate models in zero-shot setting using automatic metrics across all tasks, with additional human evaluation for open-ended VQA (text). Automatic Evaluation: We report accuracy scores for MC-VQA and classification tasks, and use GPT-4o-mini (Hurst et al., 2024) as judge for Open-ended VQA. For Open-ended QA, we additionally compute chrF++ (Popovic, 2015) scores and present them in Appendix C. We report Word Error Rate (WER) for ASR. Human Evaluation: We evaluate 50 randomly sampled questions per language on the bestperforming model per family. Bilingual native speakers rated whether model outputs matched the gold answer or were valid alternatives."
        },
        {
            "title": "6 Results",
            "content": "In this Section, we present results organized by evaluation type, beginning with cultural VQA performance across MC-VQA and Open-ended VQA tasks, followed by control experiments."
        },
        {
            "title": "6.1 Cultural VQA Results",
            "content": "We show evaluations on MC-VQA and Open-ended VQA tasks, comparing performance across text and audio modalities in both English and native languages."
        },
        {
            "title": "6.1.1 Text-based QA\nFigure 3 depicts MC-VQA average accuracy (3(a))\nand LLM-as-a-judge scores (3(b)) for open-ended\nQA in both English and native African languages\nusing location-aware prompts.",
            "content": "Open-weight models consistently perform better when the question is in English compared to native languages across both tasks. We also observe significant performance gap between MC-VQA and Open-ended QA, where all models, including Gemini-2.5 Pro, show major drops when tasked with answering in an open-ended setting, even in English. This suggests that generating culturally (a) MC-VQA (Text) (b) Open-ended VQA (Text) Figure 3: Performance comparison of models on text-based question answering tasks: (a) Text MC-VQA (Multiple Choice) and (b) Text Open-ended QA in English and Native languages. (a) MC-VQA (Audio) (b) Open-ended VQA (Audio) Figure 4: Performance comparison of models on audio-based question answering tasks: (a) Audio MC-VQA (Multiple Choice) and (b) Audio Open-ended VQA in English and Native languages. grounded responses is more challenging than selecting from predefined options, particularly for native-language queries, where performance degrades significantly compared to English queries. We also observe that increasing model size among open-weight models does not necessarily translate to improved performance in low-resource languages. Larger variants show limited or no improvement over smaller counterparts in native language QA, indicating that model scaling alone is insufficient to address low-resource challenges. Notably, some smaller models achieve near-zero accuracy for native languages for Open-ended QA. In contrast, Gemini-2.5-Pro outperforms all openweight models across both tasks while maintaining comparable performance between English and native languages, highlighting the current gap between proprietary and open-weight models. Human evaluations on Open-Ended VQA (in Figure 5), conducted on random subset of 50 samples across eight languages, are consistent with the trends observed in automatic evaluations, with Gemini-2.5 Pro performs the best when queried in native languages compared to English."
        },
        {
            "title": "6.1.2 Audio-based QA",
            "content": "Figure 4 shows MC-VQA accuracy (4(a)) and LLM-as-a-judge scores for Open-ended QA (4(b)) on audio inputs in both English and native African languages. Similar to text-based evaluation, openweight models achieve higher performance when queried in English compared to native languages across both tasks. For open-weight models, audio modality is significantly more difficult than text modality, with notable performance degradation across both tasks. Gemini-2.5-Pro, however, demonstrates robust multimodal capabilities, with consistent performance across modalities. The performance drop from MC-VQA to Open-ended QA is also noticeable in the audio modality, with nearly zero accuracy in spoken native languages. These results align with our LID and ASR analyses (Section 6.2.2). Open-weight models demonstrate poor LID capabilities, especially the Qwen variants, which exhibit near-random accuracy and significantly compromised native ASR performance compared to English. These failures also affect performance models in downstream tasks, such as Open-ended VQA. Similar to text evaluations, we find that scaling up model size shows little improvement in native language understanding. Figure 5: Human Evaluation for Text Open-ended VQA. Accuracy across best-performing models for English and Native. We observed that, while most models perform best in the English setting, Gemini-2.5 Pro seems to perform better in the Native language."
        },
        {
            "title": "6.2 Results for Control Experiments",
            "content": "We present the results on control experiments on established benchmarks alongside our cultural QA task, aiming to probe the linguistic competence of the tested models and provide pointers on why models fail on Afri-MCQA."
        },
        {
            "title": "6.2.1 Text-based Experiments\nIn Table 3, we report the performance of the mod-\nels on the control-experiment benchmarks in ad-\ndition to Afri-MCQA. All models showed per-\nformance degradation from English to native lan-\nguages across all benchmarks. Gemini-2.5-Pro\nmaintains the smallest gaps, particularly on Afri-\nMCQA, where the drop is minimal. Open-weight\nmodels show a big drop, with Qwen variants\nshowing the most severe gaps on AfriXNLI and\nAfriMMLU.",
            "content": "When comparing AfriMMLU with Afri-MCQA, open-weight models show considerably higher AfriMMLU scores in English than Afri-MCQA scores, suggesting that while models possess general factual knowledge, they lack Africa-specific cultural understanding. Gemini-2.5-Pro shows smaller gap between these benchmarks, indicating it has acquired more African cultural knowledge during training. AfriXNLI exposes the most severe cross-lingual gaps, particularly for Qwen models, suggesting that linguistic reasoning tasks are more sensitive to language resource availability than factual retrieval tasks. We additionally compute Spearman rank correlations between Afri-MCQA and our text control datasets (see Appendix D). We observe strong and statistically significant correlations between AfriXNLI and AfriMMLU performance for several models. In contrast, correlations between AfriMCQA and AfriXNLI or AfriMMLU are generally weaker and not statistically significant. Hence, while limitations in language understanding may play large role in explaining why models fail in Afri-MCQA, other factors related to African cultural and visual knowledge may be important as well. AfriXNLI AfriMMLU Afri-MCQA Model Eng Nat Gemini-2.5-Pro Gemma3n-4B Gemma3n-2B Qwen-7B Qwen-3B 89.66 78.33 80.66 81.11 65. 76.3 51.24 51.47 34.33 36.7 -13.36 -27.09 -29.19 -46.78 -28.96 Eng 94 62.2 53.6 73.8 65.8 Nat 83.46 37.56 35.74 36.10 34. Eng Nat -10.54 -24.64 -17.86 -37.70 -31.08 78.68 55.23 51.54 49.75 50. 76.27 41.49 40.32 31.30 31.00 -2.4 -13.7 -11.2 -18.46 -19.65 Table 3: Text-based performance (%). Eng = English accuracy, Nat = Native accuracy, = English Native gap (negative = drop)."
        },
        {
            "title": "6.2.2 Audio-based Experiments",
            "content": "Figure 6 shows three evaluations on native African language audio. LID: Gemini-2.5-Pro achieves near-perfect LID accuracy, while Gemma variants show moderate performance. Qwen models perform at near random levels, indicating minimal exposure to African language audio during pretraining. ASR Performance: WER shows similar pattern. Gemini-2.5-Pro maintains reasonable native ASR, whereas Gemma models exhibit substantial degradation. Qwen models produce high error rates, indicating hallucinations rather than meaningful transcription. This aligns with observed results for open-ended VQA (Audio). Gemini-2.5-Pro achieves moderate accuracy, whereas open-weight models score near zero. In light of these results, it is likely that models fail on open-ended VQA (Audio) because they cannot properly identify or transcribe the tested spoken languages. Hence, poor Open-ended VQA (Audio) results come from errors at each step: (1) Open models often fail at identifying African languages, reflecting fundamental gaps in audio representation, (2) they show high ASR errors, suggesting that most spoken content is not perceived adequately for understanding and answering question (3) these errors can carry over to open-ended VQA when models receive wrong transcriptions, as they cannot answer correctly even if they know the cultural content. These findings demonstrate that foundational speech processing capabilities are prerequisites for meaningful evaluation of cultural reasoning in African languages. Figure 6: Audio probing results on native African languages: (a) LID ( higher is better), (b) ASR ( lower is better), and (c) Open-ended VQA ( higher is better). (+) means WER is more than 100%"
        },
        {
            "title": "7 Discussion",
            "content": "We now organize our findings to address each research question, summarizing key patterns observed across models, modalities, and languages. RQ1 : How well do MLLMs understand African cultural contexts in visually-grounded QA? MLLMs show limited understanding of African cultural contexts. As shown in Figures 3 and 4, even the best performing model (Gemini-2.5Pro) achieves only 78% on MC-VQA and 38% on Open-VQA (text-based, English). Smaller models (Gemma3, Qwen2.5) perform substantially worse, ranging from 5059% on MC-VQA and 924% on Open-VQA, indicating significant room for improvement. RQ2 : How does input modality (text vs. speech) affect performance? Performance degrades when switching from text to speech input, particularly for smaller models. As shown in Figures 3 and 4, on MC-VQA, Qwen models drop by approximately 12% while Gemma models show mixed results. The gap is more visible in OpenVQA, where audio-based native language queries yield near-zero accuracy (25%) for smaller models. Control experiments for audio show that this comes from poor language identification (24% for Qwen) and high ASR word error rates (85100%+ for non-Gemini models). RQ3 : How does query language (native vs. English) affect performance, and do differences reflect language understanding or cultural knowledge gaps? English queries consistently outperform native language queries across all models and settings. As shown in Figure 3, the gap ranges from 2% (Gemini-2.5-Pro on MC-VQA) to 19% (Qwen on MC-VQA). Control experiments on AfriXNLI and AfriMMLU  (Table 3)  show that language understanding gaps ( = 1347%) are substantially larger than cultural knowledge gaps ( = 219% on Afri-MCQA), suggesting language understanding is the dominant limitation. However, models also struggle with cultural QA in English, indicating that both linguistic and cultural limitations contribute to poor performance. RQ4 : How does task format (Multiple-Choice vs. Open QA) affect accuracy? As shown in Figures 3 and 4, models perform significantly better on MC-VQA than Open-VQA. Gemini-2.5-Pro achieves 78% on MC-VQA vs. 38% on Open-VQA (a 40% gap). Smaller models show even larger relative drops, with some scoring less than 10% on Open-VQA. This performance gap suggests that MC-VQA benefits from simplified answer selection, while Open-VQA exposes true limitations. Models struggle to generate culturally grounded responses even in English, with performance degrading further for native languages."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced Afri-MCQA, the first large-scale multilingual and multimodal benchmark for African cultural visual QA, covering 15 languages across 12 countries with over 7.5k Q&A pairs in text and speech modalities. Our evaluation shows: 1) MLLMs struggle significantly with African cultural knowledge, 2) speech processing presents critical bottleneck, and 3) gaps persist across languages and task formats. These findings motivate several research directions: (1) speech-first approaches: Many African languages are primarily oral, yet current openweight models lack basic LID and ASR capabilities for these languages; (2) culturally-grounded pretraining: The gap between AfriMMLU and Afri-MCQA performance suggests language data alone is insufficient; models need explicit exposure to African cultural content; and (3) cross-lingual cultural transfer: Models may know cultural facts in English but cannot access them through native language queries, motivating research into cross-lingual knowledge retrieval. We release Afri-MCQA to provide both benchmark and foundation for building more inclusive, culturally aware multimodal systems that better represent African languages and cultures."
        },
        {
            "title": "9 Limitations",
            "content": "We believe Afri-MCQA represents an important step toward more inclusive evaluation by foregrounding African languages and cultural contexts that have long been overlooked in existing benchmarks. Although the dataset spans 15 languages across 12 countries, Africa is home to thousands of languages and cultural groups, many of which remain unrepresented. Furthermore, while our question categories aim to reflect culturally grounded knowledge, culture itself is fluid, subjective, and deeply contextual. Our formulation inevitably abstracts away from finer-grained variations such as regional, generational, or community-specific differences that shape cultural understanding. As with most human-curated datasets, potential biases in data collection remain. Annotators backgrounds and interpretations of cultural relevance may influence the formulation of questions or the selection of images. Additionally, due to computational and financial constraints, we evaluate only limited set of openand closed-source models, so the reported performance gaps may not fully capture the broader landscape. Finally, Afri-MCQA is human-curated dataset created without the involvement of LLMs and with minimal reliance on websourced images. Because this process is inherently time-consuming, the dataset is of moderate size and intended as an evaluation benchmark rather than pretraining or fine-tuning resource, for which it would likely cause overfitting."
        },
        {
            "title": "10 Ethical Considerations",
            "content": "Our work involves the collection of culturally grounded questionanswer pairs in 15 African languages, annotated, spoken and reviewed by native speakers. All annotators participated voluntarily and were compensated fairly for their work in accordance with local wage standards on the Upwork platform. Before beginning annotation, contributors were informed about the goals of the project, the intended use of the dataset for research and evaluation purposes, and their right to withdraw from participation at any stage. We took several steps to ensure cultural sensitivity and respect throughout the data creation process. Question formulation guidelines were designed to avoid harmful stereotypes, offensive content, or culturally inappropriate framing. All annotations were reviewed by language coordinators who are themselves native speakers to check for accuracy, contextual appropriateness, and respectful representation. Despite these efforts, we acknowledge that culture is deeply complex and subjective, and that our dataset may still reflect certain biases or oversimplifications."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. 2024. Phi-4 technical report. Preprint, arXiv:2412.08905. Ife Adebara and Muhammad Abdul-Mageed. 2022. Towards afrocentric NLP for African languages: Where we are and where we can go. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38143841, Dublin, Ireland. Association for Computational Linguistics. David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Haonan Gao, and En-Shiun Annie Lee. 2024. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226245, St. Julians, Malta. Association for Computational Linguistics. David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, Sana Al-azzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Ajayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka Obiefuna, Muhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta Ababu, Saheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris Abdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode, Tolulope Adelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko, Abeeb Afolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo, Jessica Ojo, Oyinkansola Awosan, Tadesse Kebede, Toadoum Sari Sakayo, Pamela Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Kanda Tshinu, Ussen Kimanuka, Thina Diko, Siyanda Nxakama, Sinodos Nigusse, Abdulmejid Johar, Shafie Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire, Jules Jules, Ivan Ssenkungu, and Pontus Stenetorp. 2023. MasakhaNEWS: News topic classification for African languages. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 144159, Nusa Dua, Bali. Association for Computational Linguistics. David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba Oluwadara Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Ijeoma Chukwuneke, Happy Buzaaba, Blessing Kudzaishe Sibanda, Godson Koffi Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Salomey Osei, Shamsuddeen Hassan Muhammad, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, and Pontus Stenetorp. 2025. IrokoBench: new benchmark for African languages in the age of large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 27322757, Albuquerque, New Mexico. Association for Computational Linguistics. Jesujoba Oluwadara Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, and Dietrich Klakow. 2025. Charting the landscape of African NLP: Mapping progress and shaping the road ahead. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2780727841, Suzhou, China. Association for Computational Linguistics. Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Yonas Chanie, Bontu Fufa Balcha, Negasi Haile Abadi, Henok Biadglign Ademtew, Mulubrhan Abebe Nerea, Debela Desalegn Yadeta, Derartu Dagne Geremew, Assefa Atsbiha Tesfu, Philipp Slusallek, Thamar Solorio, and Dietrich Klakow. 2025. ProverbEval: Exploring LLM evaluation challenges for low-resource language understanding. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 62506266, Albuquerque, New Mexico. Association for Computational Linguistics. Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Mitiku Yohannes Fuge, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, and Seid Muhie Yimam. 2024. Walia-LLM: Enhancing Amharic-LLaMA by integrating task-specific and generative datasets. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 432444, Miami, Florida, USA. Association for Computational Linguistics. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Marta R. Costa-jussà, James Crossa nd Onur Çelebi, Maha Elbayad andKenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault andGabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, and Holger Schwenk andJeff Wang. 2022. No language left behind: Scaling humancentered machine translation. Harald Hammarström. 2018. survey of african languages. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. ChoquetteChoo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. 2025. Gemma 3 technical report. Preprint, arXiv:2503.19786. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021a. Visually grounded reasoning across languages and cultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1046710485, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Visually grounded reasoning Elliott. 2021b. arXiv preprint across languages and cultures. arXiv:2109.13238. min, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Paul Röttger, Abigail Oppong, Andiswa Bukula, Chiamaka Ijeoma Chukwuneke, Ebrahim Chekol Jibril, Elyas Abdi Ismail, Esubalew Alemneh, Hagos Tesfahun Gebremichael, Lukman Jibril Aliyu, Meriem Beloucif, Oumaima Hourrane, Rooweither Mabuya, Salomey Osei, Samuel Rutunda, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Lilian Diana Awuor Wanzare, Nelson Odhiambo Onyango, Seid Muhie Yimam, and Nedjma Ousidhoum. 2025. AfriHate: multilingual collection of hate speech and abusive language datasets for African languages. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 18541871, Albuquerque, New Mexico. Association for Computational Linguistics. Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd Van Steenkiste, Lisa Anne Hendricks, Aishwarya Agrawal, et al. 2024. Benchmarking vision language models for cultural understanding. arXiv preprint arXiv:2407.10920. Derek Nurse and Gerard Philippson, editors. 2006. The Bantu Languages. Routledge Language Family Series. Routledge, London, England. Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure FP Dossou, Abdou Aziz Diop, Claytone Sikasote, Gilles Hacheme, et al. 2023. Afriqa: Cross-lingual open-retrieval question answering for african languages. arXiv preprint arXiv:2305.06897. Jessica Ojo, Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Jimmy Lin, Pontus Stenetorp, and David Ifeoluwa Adelani. 2025. Afrobench: how good are large language models on african languages? In Findings of the Association for Computational Linguistics: ACL 2025, pages 1904819095. Shantipriya Parida, Idris Abdulmumin, Shamsuddeen Hassan Muhammad, Aneesh Bose, Guneet Singh Kohli, Ibrahim Said Ahmad, Ketan Kotwal, Sayan Deb Sarkar, Ondˇrej Bojar, and Habeebah Adamu Kakudi. 2023. Havqa: dataset for visual question answering and multimodal research in hausa language. arXiv preprint arXiv:2305.17690. Maja Popovic. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392395. Rada Mihalcea, Oana Ignat, Longju Bai, Angana Borah, Luis Chiruzzo, Zhijing Jin, Claude Kwizera, Joan Nwatu, Soujanya Poria, and Thamar Solorio. 2025. Why ai is weird and shouldnt be this way: Towards ai for everyone, with everyone, by everyone. Shamsuddeen Hassan Muhammad, Idris AbdulmuDavid Romero, Chenyang Lyu, Haryo Wibowo, Santiago Góngora, Aishik Mandal, Sukannya Purkayastha, Jesus-German Ortiz-Barajas, Emilio Cueva, Jinheon Baek, Soyeong Jeong, et al. 2024. Cvqa: Culturallydiverse multilingual visual question answering benchmark. Advances in Neural Information Processing Systems, 37:1147911505. Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Christabelle Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, ShiXiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, and Chong-Wah Ngo. 2025. WorldCuisines: massivescale benchmark for multilingual and multicultural visual question answering on global cuisines. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32423264, Albuquerque, New Mexico. Association for Computational Linguistics. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. 2025. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215. Hao Yu, Jesujoba Oluwadara Alabi, Andiswa Bukula, Jian Yun Zhuang, En-Shiun Annie Lee, Tadesse Kebede Guge, Israel Abebe Azime, Happy Buzaaba, Blessing Kudzaishe Sibanda, Godson Koffi Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Dietrich Klakow, and David Ifeoluwa Adelani. 2025. INJONGO: multicultural intent detection and slot-filling dataset for 16 African languages. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94299452, Vienna, Austria. Association for Computational Linguistics. Florian Schneider and Sunayana Sitaram. 2024. M5 diverse benchmark to assess the performance of large multimodal models across multilingual and multicultural vision-language tasks. arXiv preprint arXiv:2407.03791. Belay Simane, Thandi Kapwata, Natasha Naidoo, Guéladio Cissé, Caradee Y. Wright, and Kiros Berhane. 2025. Ensuring africas food security by 2050: The role of population growth, climateresilient strategies, and putative pathways to resilience. Foods, 14(2):262. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304. Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Ah Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, et al. 2024a. Ethiollm: Multilingual large language models for ethiopian languages with task evaluation. the 2024 Joint International In Proceedings of Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 63416352. Atnafu Lambebo Tonja, Bonaventure FP Dossou, Jessica Ojo, Jenalea Rajab, Fadel Thior, Eric Peter Wairagala, Anuoluwapo Aremu, Pelonomi Moiloa, Jade Abbott, Vukosi Marivate, et al. 2024b. Inkubalm: small language model for low-resource african languages. arXiv preprint arXiv:2408.17024. Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kukreja, et al. 2025a. All languages matter: Evaluating lmms on culturally diverse 100 languages. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1956519575. Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kukreja, et al. 2025b. All languages matter: Evaluating lmms on culturally diverse 100 languages. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1956519575. Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Wang Yutong, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Cheng Ching Lam, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang"
        },
        {
            "title": "A Image Categories",
            "content": "A) Language-wise category distribution Figure 7: Language-wise distribution of the categories. B) Image category distribution Figure 8: Top 6 Image category distribution"
        },
        {
            "title": "B Prompt",
            "content": "For each QA format, we use the same prompt templates to ensure consistency across models and languages. We evaluated models under two distinct prompt conditions to assess the impact of visual and contextual grounding: Image-grounded: [Image] Question: {q} Options: {opts} Image + Location: [Image] Location: {country}. Question: {q} Options: {opts} Figures 9 & 10 show the location-aware prompts we used in both setups for audio and text modalities."
        },
        {
            "title": "Text Prompts",
            "content": "Multiple-Choice QA: SYSTEM You are helpful AI assistant. USER You are given an image. Analyze the image and answer the following multiple-choice question. Only one option is correct. Return only the correct option name i.e. A, B, or D. The question is relevant to {country}. [Image] Question: {question} Options: A. {opt_a}, B. {opt_b}, C. {opt_c}, D. {opt_d} Open-Ended: SYSTEM You are helpful AI assistant. USER You are given an image. Analyze the image and answer the question with short factual answer. The answer should be word or short phrase. Only give the answer, no follow-up or extra information.The question is relevant to {country} [Image] Question: {question}. Figure 9: Location-aware text prompts."
        },
        {
            "title": "Audio Prompts",
            "content": "Multiple-Choice QA: SYSTEM You are helpful AI assistant. USER You are given an image and audio question. Analyse the image and answer the audio Multiple Choice Question. Only one option is correct. Return only the correct option name i.e. A, B, or D. The question is relevant to {country}. [Image] [Audio] Question: {question_audio} Audio Options: A-D {options_audio} Open-Ended: SYSTEM You are helpful AI assistant. USER You are given an image and audio question. Analyze the image and answer the audio question with short factual answer. The answer should be word or short phrase. Only give the answer, no follow-up or extra information. The question is relevant to {country}. [Image] Question: {question_audio} Figure 10: Location-aware audio prompts."
        },
        {
            "title": "C Additional Experiments results",
            "content": "C.1 Text-based QA - here we report additional results for different prompts and per country-language results. C.2 Language wise heatmap for MCQA-text"
        },
        {
            "title": "D Correlation statistics",
            "content": "Country-Lang G-12B G-27B G-3n-2B G-3n-4B Q-2.5 3B Q-2.5 7B Gemni-2.5-pro Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average 76.11 81.16 79.8 84.15 76.35 82.27 87.87 68.02 76.66 81.33 82.95 82.54 66.02 67.38 87.64 78.66 40.54 58.21 40.39 60.48 51.5 60.48 54.34 54.33 51.22 62.27 53.63 55.3 43.42 58.66 74.47 55.23 42.61 54.79 41.37 49.32 45.93 55.05 46.63 50.27 50.17 54.7 48.68 53.21 41.98 55.55 70.08 50.68 41.23 55.14 43.72 56.92 44.76 56.92 47.67 50 45.05 56.93 49.9 50.94 40.7 59.13 74.63 51.54 50.85 63.17 50.19 61.65 55.03 67.97 58.83 56.36 57.54 62.99 55.61 60.41 47.91 58.9 74.47 58. 50.34 62.47 49.8 62.23 54.48 68.16 58.38 56.23 57.89 63.15 55.24 61.74 47.43 59.97 74.63 58.80 41.23 57.2 42.15 53.75 45.73 55.61 47.67 45.52 50.52 56.14 46.62 53.03 37.98 51.49 61.62 49.75 Table 4: Text Prompt - Location/Language AwareEnglish Country-Lang G-12B G-27B G-3n-2B G-3n-4B Q-2.5 3B Q-2.5 7B Gemni-2.5-pro Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average 42.09 40.31 38.43 44.89 35.65 53.55 40.6 43.22 41.57 37.79 38.36 36.93 34.15 39.18 38.21 40.49 30.93 28.44 30.39 35.26 33.13 39.7 35.25 27.5 32.28 29.5 31.08 30.3 28.36 27.8 25.56 31. 78.17 81.67 79.41 81.31 75.77 82.58 85.05 66.12 75.96 77.51 81.27 83.52 68.58 61.88 65.36 76.27 44.5 47.12 41.17 48.36 39.34 57.11 44.42 40.37 47.01 45.13 44.19 39.77 36.85 38.23 37.39 43.39 42.78 42.58 36.86 43.54 36.24 53.18 41.41 42.68 40.52 37.48 38.36 37.87 53.18 39.42 36.26 41.49 43.64 49.38 41.56 48.16 38.56 58.05 44.64 41.19 46.84 45.61 43.82 40.15 35.73 39.9 37.07 43.62 29.38 29.12 31.56 35.26 34.49 39.13 36.76 25.06 34.56 29.34 31.27 32.38 28.68 22.46 29.91 31.29 Table 5: Text Prompt - Location/Language Awarenative Country-Lang G-12B G-27B G-3n-2B G-3n-4B Q-2.5 3B Q-2.5 7B Gemni-2.5-pro 44.5 Ethiopia-amh 57.06 Nigeria-hau 47.64 Nigeria-ibo 58.76 Uganda-lug 50.96 Ethiopia-orm 60.48 Rwanda-kin 54.14 Kenya-kik 54.06 Somali-som 47.89 Eritrea-tir 63.95 Ghana-twi 53.44 Nigeria-yor 55.11 Botswana-tsn 45.19 Malawi-nya 64.99 S.Africa-zul 74.63 Lesotho-sot 55.52 Average 42.09 56.36 44.11 56.84 46.51 57.86 49.09 50.81 41.01 59.01 52.14 53.21 43.75 65.23 75.28 53.28 42.09 57.59 37.64 49.51 44.18 54.86 42.02 49.86 48.94 55.34 47.37 51.7 42.14 48.5 53.82 48.37 72.23 77.31 77.45 80.73 77.32 78.27 86.46 64.09 73.15 81.49 82.2 80.8 65.54 88.76 92.5 78.56 50.51 65.44 47.05 60.88 50.58 67.6 55.75 57.99 56.49 63.15 57.11 61.17 46.79 66.06 74.95 58.76 50.34 64.92 48.43 61.65 52.32 68.53 56.56 56.36 57.01 63.31 55.8 61.74 46.15 64.63 75.28 58. 40.72 58.87 40.98 53.56 44.37 56.92 47.47 46.06 50 56.45 47 52.46 37.82 53.52 60.16 49.75 Table 6: Prompt - Image onlyEnglish(text-based) Figure 11: Language-wise accuracy of seven multimodal LLMs on the text-based VQA task, evaluated separately on English and native African language questions. Country-Lang G-12B G-27B G-3n-2B G-3n-4B Q-2.5 3B Q-2.5 7B Gemni-2.5-pro Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average 43.81 46.94 40.58 50.86 39.53 56.92 45.85 41.19 47.01 45.45 44.19 42.04 38.94 39.66 37.07 44. 00 43.29 43.45 38.43 45.08 34.68 55.24 40.6 43.9 39.82 36.52 39.29 38.06 36.21 39.78 38.37 41. 29.73 30.89 30 35.07 30.04 34.83 33.74 27.77 28.07 29.18 32.02 28.4 28.68 29.39 28.34 30.27 41.76 46.77 41.76 50.48 38.56 57.67 45.45 40.78 47.01 45.61 44 40.34 37.17 39.54 37.72 43.64 43.47 44.5 37.64 44.31 37.2 55.24 41.61 45.12 41.05 31.57 30.71 31.43 35.09 39.3 37.2 41.14 76.8 81.32 79.01 81.11 76.55 80.71 84.84 65.62 73.28 75.91 81.23 81.06 70.19 40 47.64 73.01 29.89 32.16 32.94 36.22 31 37.82 37.17 24.93 33.5 29.66 32.95 31.81 28.84 22.46 30.56 31.46 Table 7: Image only-native (text-based) Country-Lang Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average G-12B chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM Q-2.5 3B Q-2.5 7B G-3n-2B G-3n-4B Gemni G-27B 27 7.28 10.41 20.64 18.44 12.51 17.77 13.77 20.11 20.37 10.2 16.14 16.41 18.48 12.56 16.13 26.2 12.63 10.54 25.45 17.19 18.76 21.82 15.94 28.75 29.05 17.27 20.96 15.17 15.72 11.26 19.14 27.55 7.47 11.04 22.37 19.24 12.06 18.2 13.4 20.47 21.01 9.38 15.16 15.56 17.94 14.01 16. 26.8 13.43 12.13 27.04 19.34 18.36 23.64 16.93 29.29 29.24 17.87 19.66 15.57 16.1 11.82 19.81 17.69 7.73 10.31 15.38 16.86 11.37 14.42 12.59 14.82 16.83 9.13 14.93 11.08 11.48 11.49 13.07 15 7.41 10.14 16.1 11.72 10.18 11.31 13.55 19.17 23.46 13.86 17.37 17.37 8.7 8.44 13.58 20.7 7.7 11.8 17.51 20.08 12.51 16.54 15.03 17.87 18.87 9.92 16.34 13.39 12.05 10.69 14.73 19.2 7.82 8.55 18.29 17.58 12.38 12.12 17.73 22.24 23.09 12.25 20.56 12.77 9.28 8.63 8.56 20.73 10.82 9.41 17.1 19.2 10.82 16.73 13.55 14.64 19.41 10.12 15.55 13.42 17.49 13.37 14. 16.4 8.45 7.75 18.89 16.41 16.38 15.15 16.33 19.23 24.58 15 18.56 18.56 15.72 11.44 15.83 31.53 13.43 15.65 27.91 27.38 16.37 30.33 17.04 24.22 25.71 13.13 20.59 20.72 18.2 14.16 20.75 30.4 11.02 15.9 33.2 25 22.92 31.11 18.73 30.02 32.4 20.6 27.94 22.75 16.67 12.01 23.54 47.88 14.63 25.61 35.7 34.01 18.92 41.2 18.62 35.75 25.71 20.61 22.98 25.24 34.87 24.7 28.43 60.2 25.5 36.18 44.33 38.09 36.53 49.49 25.5 49.01 34.64 43.98 28.14 30.34 39.58 25.7 37.80 Table 8: Location/Language Aware-English(Open-endded VQA -text based) Country-Lang Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average G-12B chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM Q-2.5 3B Q-2.5 7B G-3n-2B G-3n-4B Gemni G-27B 2.47 4.23 5.93 9.48 6.56 8.31 7.47 7.85 0.85 6.19 3.91 7.91 8.99 13.54 7.6 6.75 22.8 3.01 5.96 9.96 6.45 9.38 3.03 7.57 3.98 5.77 6.43 4.39 5.19 10.04 3 7.13 2.91 4.31 5.93 9.22 6.35 8.23 7.82 7.9 0.95 6.3 3.72 7.11 8.44 12.72 7.15 6. 22.6 3.21 5.96 11.16 5.86 9.58 30.3 7.37 4.34 6.89 5.62 4.19 4.99 8.33 2.81 6.88 0.11 2.79 3.65 5.69 5.26 4.8 5.81 3.72 0.04 4.92 2.25 4.4 4.23 4.68 4.73 3.80 6.6 0.6 0.99 3.19 6.05 3.39 1.21 1.39 5.79 2.42 1 2.79 0.4 1.33 3 2.67 1.01 2.9 4.97 4.27 3.26 3.65 4.03 4.42 0.43 4.43 2.3 4.15 3.43 5.21 8.08 3.73 4.2 0.6 0.99 3.19 1.76 3.02 0.61 2.19 5.79 2.61 1.2 2 0.8 4.88 4.88 2.68 0 1.41 0.44 0.5 0.68 0.39 1.84 2.17 0.01 1.88 1.22 2.04 1.71 14.32 7.77 2. 1 2.4 0.99 1.79 0.59 0.2 1.21 1.99 0.18 0.74 0.6 0.6 0.4 10.42 3.38 1.76 0.71 3.83 5.42 8.45 6.27 6.24 9.31 5.64 0.13 6.89 4.3 7.26 9.33 14.75 8 6.43 20.4 2.6 2.98 8.76 4.69 7.19 3.03 4.78 14.1 5.21 7.8 5.99 3.59 10.42 3 6.93 42.17 15.02 24.11 30.33 37.92 25.27 39.17 22.59 27.23 31.63 22.33 34.41 31.5 41.13 28.67 30.22 55.4 30.26 33 40.44 39.84 34.13 47.37 30.48 37.43 33.89 46.18 39.75 34.13 44.51 29.83 38.43 Table 9: Location/Language Aware-native(Open-ended VQA -text based) Country-Lang Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average G-12B chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM Q-2.5 3B Q-2.5 7B G-3n-2B G-3n-4B Gemni G-27B 19.86 6.28 10.57 20.59 18.79 11.41 18.55 14.79 14.71 21.95 10.87 15.75 12.25 16.12 11.99 14.97 15.8 12.63 8.55 24.25 15.04 15.97 20.81 18.53 16.09 29.8 17.87 22.95 9.78 13.25 9.19 16.7 20.04 6.53 9.95 19.68 19.01 11.22 18.2 15.4 14.64 20.66 10.61 16.14 12.97 15.77 11.75 14. 15.8 11.62 8.15 22.47 14.45 15.37 20.2 19.12 15.19 29.24 17.47 22.75 11.38 11.93 9.57 16.31 10.38 7.98 9.78 13.85 12.29 10.29 12.03 8.03 8 8.85 7.02 9.27 8.7 10.61 10.59 9.84 6.6 6.8 3.76 13.92 6.84 7.39 7.68 14.94 6.87 18.44 11.45 17.76 6.19 7.1 6.94 9.51 11.76 7.98 10.5 15.89 14.68 11.38 15.97 12.45 9.84 15.78 8.89 14.95 9.88 10.7 10.55 12.08 9.2 7.01 6.76 15.9 11.33 10.18 11.11 17.13 7.59 21.23 11.24 22.16 6.19 8.52 9.01 11.64 15.42 7.36 8.97 16.43 14.08 9.41 15.12 12.97 10.71 26.57 9.25 13.77 15.33 15.65 12.71 13. 10 8.02 5.96 18.49 8.01 10.78 13.94 15.34 7.78 24.95 14.4 18.36 8.18 12.69 9.94 12.46 24.62 7.35 13.27 27.36 23.32 15.15 28.99 18.04 18.09 25.57 13.48 19.43 15.33 16.36 12.33 19.1 20.8 12.02 11.13 30.2 19.14 18.16 28.48 21.71 12.84 33.33 21 25.35 12.77 13.07 9.38 19.29 40.41 9.37 24.63 49.04 37.92 23.07 49.78 26.79 31.98 36.76 23.2 30.27 26.35 40.02 23.31 31.53 43.8 16.83 30.82 48.21 36.52 32.93 59.19 32.07 40.14 47.87 45.38 39.72 26.35 40.34 23.83 37.6 Table 10: Image only-english(Open-endded VQA -text based) Country-Lang Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot Average G-12B chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM chrf++ LLM Q-2.5 7B Q-2.5 3B G-3n-2B G-3n-4B Gemni G-27B 4.65 3.42 5.21 9.17 6.28 7.66 7.58 7.56 1.14 6.52 3.76 6.74 8.78 14.11 7.28 6.66 17.2 3.01 4.17 10.96 7.03 9.98 3.23 8.17 3.62 6.15 6.22 4.99 5.99 9.66 3.75 6.94 3.78 3.83 5.31 8.14 5.94 7.59 7.57 7.3 1.21 6.24 3.94 6.48 8.3 14.68 7.76 6. 17.6 2 2.98 9.96 6.45 9.78 3.03 8.76 3.8 7.08 6.92 5.59 1.2 10.61 4.69 6.7 3.5 3.12 2.53 4.28 2.01 2.36 4.53 3.29 0.12 5.03 2.3 2.97 3.31 3.44 3.13 2.97 3.2 3.41 2.78 4.38 2.15 2.59 4.04 2.39 1.63 2.42 0.6 2.99 1 2.65 2.25 2.57 0.68 3.12 5.32 4.72 4.68 3.98 6.11 6.21 0.19 5.74 3.33 5.86 5.36 3.72 6.88 4.39 2.4 1.4 0.99 4.58 2.93 2.2 1.62 3.78 0.72 4.28 0.8 8.58 2.79 1.52 10.69 3.29 1.51 3.77 5.41 8.1 5.67 6.69 8.7 5.73 0.74 6.44 4.3 6.6 8.35 13.78 7.55 6. 14.6 3.81 2.58 8.96 3.32 8.58 2.22 5.78 4.16 4.66 8.2 4.19 3.79 10.23 3.19 5.88 1.21 3.44 5.67 7.95 5.54 6.19 8.9 5.7 0.69 6.21 4.22 6.95 8.33 14.06 7.04 6.14 13.8 3.81 2.19 7.57 2.73 7.39 2.63 4.98 4.7 4.66 8 4.59 3.19 9.66 3.19 5.54 42.59 14.67 23.2 31.54 29.32 25.51 39.23 25.02 27.03 29.44 20.6 34.32 29.83 40.69 24.95 29.20 54.8 29.66 29.62 39.04 38.48 35.33 48.08 32.07 35.62 32.22 43.73 36.93 31.94 42.8 25.7 37.07 Table 11: Image only-native(Open-endded VQA -text based) Country-Lang Gemma3n-2B Gemma3n-4B Qwen-3B Qwen-7B Gemini-2.5 Pro Gemma3n-2B Gemma3n-4B Qwen-3B Qwen-7B Gemini-2.5 Pro English Native Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanad-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor Lesotho-sot S.Africa-zul Average 37.78 34.38 27.60 36.96 26.85 27.12 45.78 39.64 37.55 35.86 26.24 36.55 31.17 34.11 44.35 36.17 33.17 41.46 30.09 28.72 51.40 46.10 39.20 40.92 30.73 44.30 35.49 38.62 58.96 42.43 42.96 62.74 40.60 44.41 59.07 47.65 41.23 51.06 49.17 46.98 45.56 48.68 62.83 43.60 44.30 62.53 44.50 45.98 63.06 50.56 42.35 54.60 56.05 52.30 47.58 51.56 87.01 83.52 80.09 83.61 77.15 83.28 85.16 81.73 74.52 83.15 82.14 75.48 84.13 81.61 28.83 29.48 26.52 27.95 27.18 26.86 30.69 33.87 29.11 25.05 24.76 28.27 23.64 29.77 27.04 27.86 32.11 32.97 27.98 32.29 26.18 25.00 32.07 31.81 28.87 26.11 26.42 29.76 25.53 30.00 26. 29.01 31.48 30.03 29.62 29.93 27.15 31.90 29.31 34.39 26.49 27.17 24.88 31.26 26.31 34.00 28.89 29.52 31.27 28.63 32.08 27.51 26.08 33.15 31.44 38.38 27.06 19.70 24.14 34.33 26.68 31.12 31.06 29.51 69.12 82.46 70.80 82.32 56.50 72.34 78.13 72.48 71.46 74.46 75.41 72.35 76.54 59.15 74. 73.41 Table 12: Audio-Loc-Lang Aware(MC-VQA) Country-Lang Gemma3n-2B Gemma3n-4B Qwen-3B Qwen-7B Gemini-2.5 Pro Gemma3n-2B Gemma3n-4B Qwen-3B Qwen-7B Gemini-2.5 Pro English Native Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanda-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor Lesotho-sot S.Africa-zul Average 33.67 31.01 25.18 36.49 23.37 25.26 44.24 36.74 34.50 35.86 24.11 33.11 23.50 31.31 46.40 35.05 32.44 42.18 27.08 30.31 48.59 44.09 32.62 40.29 29.78 36.34 35.25 36.96 57.23 45.18 41.81 63.30 37.65 45.65 57.54 45.49 37.40 50.85 49.64 47.29 47.07 48.16 62.19 44.58 48.50 65.41 39.71 45.96 61.33 45.94 42.50 54.36 55.34 47.67 48.92 50.02 87.75 79.72 77.72 84.28 76.62 83.73 81.79 78.65 71.15 85.31 80.42 74.51 85.99 80.59 27.61 28.17 28.71 27.95 27.93 25.53 28.28 32.49 27.68 23.99 26.65 26.98 24.82 27.45 24.74 28.38 29.44 30.60 23.44 25.00 28.97 31.12 28.16 25.69 22.17 27.19 25.77 26.98 32.55 31.66 37.07 25.24 21.85 27.74 30.83 35.64 26.01 27.89 25.30 32.10 28.01 31.12 29.04 29.47 32.51 29.91 26.86 24.80 27.17 35.04 31.00 35.20 26.78 25.00 24.64 31.97 26.68 28.29 32. 29.20 65.16 79.69 68.87 81.36 55.50 71.81 79.24 69.27 71.22 70.82 72.64 70.39 77.96 59.11 72.07 71.01 Table 13: MC-VQA Audio - Image Only Language Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanda-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor G3n-2B G3n-4B Q-3B Q-7B chrf++ acc. chrf++ acc. chrf++ acc. chrf++ acc. Gemini-2.5 Pro acc. chrf++ 19.21 17.86 13.05 13.02 12.31 17.87 15.99 16.37 18.72 18.15 14.39 16.3 11.55 29.36 18.65 18.16 14.89 6.94 14.47 17.14 18.71 17.80 24.10 21.51 21.08 16.31 21.61 27.61 14.55 11.34 12.09 18.89 15.09 19.13 23.03 19.27 16.39 17.89 11.91 31.01 24.94 20.82 18.68 9.03 18.16 17.39 21.60 22.25 25.16 23.17 21.94 18.71 19.16 18.5 13.05 13.26 11.26 15.86 15.59 16.14 17.02 16.48 15.02 15.93 11. 29.98 19.05 21.22 14.42 7.89 14.47 16.25 19.6 15.49 23.33 18.29 21.59 16.63 21.11 21.14 18.04 17.28 14.48 19.21 16.49 17.09 20.62 17.61 17.54 19.39 12.81 33.06 22.56 24.81 19.76 12.6 18.23 19.48 20.33 20.54 23.56 23.99 27.61 18.99 31.92 49.39 27.9 17.99 30.9 44.58 25.82 30.9 38.46 26.79 23.05 35.64 25.07 45.59 64.04 35.35 31.21 42.13 55.26 43.99 52.34 45.43 36.81 32.39 53.33 49.64 45. Average 15.75 18.39 17.58 20.99 15. 18.31 17.91 21.96 32.11 Table 14: Audio Open-ended for English (Loc-Lang Aware) Language G3n-2B chrf++ acc. G3n-4B chrf++ acc. Q-3B chrf++ acc. Q-7B chrf++ acc. Gemini-2.5 Pro chrf++ acc. Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanda-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor Lesotho-sot S.Africa-zul 12.14 0.58 16.03 9.52 9.42 20.13 12.81 15.13 6.22 17.03 6.78 0.10 8.92 6.46 19.12 11.75 24.85 10.98 13.10 9.90 12.96 14.20 18.49 10.60 15.23 8.58 22.35 19.00 3.56 18.75 12.24 1.53 17.32 12.53 9.01 21.02 11.84 15.41 5.80 14.58 6.61 0.14 8.30 5.30 22.44 13.25 25.85 13.77 16.94 10.71 14.40 17.40 18.29 9.40 12.22 11.38 17.32 19.40 4.13 16.86 5.98 0.00 6.31 2.87 4.22 5.54 2.48 4.96 2.90 4.88 3.00 0.01 2.76 5.14 8.78 3.92 2.61 5.39 0.40 2.83 1.66 1.20 3.59 2.93 3.04 2.00 7.08 1.60 1.27 4. 5.98 0.00 6.31 2.87 4.22 5.54 2.48 4.96 3.27 4.88 3.00 0.01 2.76 6.47 15.36 4.10 2.61 5.59 0.60 2.83 1.66 1.20 3.78 3.72 3.04 2.00 7.26 1.20 2.11 9.24 45.72 47.43 58.40 63.06 41.81 54.02 67.48 56.59 51.83 71.22 59.27 47.97 33.51 50.94 69.90 Average 10.69 14. 10.94 14.75 3.99 2.91 4.54 3. 54.61 57.06 79.30 63.05 81.65 52.08 63.09 73.68 64.26 50.55 74.39 69.17 68.63 72.98 50.84 71.43 66.14 Table 15: Audio Open-ended for Native Audio(Loc-Lang Aware) Language Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanda-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor G3n-2B G3n-4B Q-3B Q-7B chrf++ acc. chrf++ acc. chrf++ acc. chrf++ acc. Gemini-2.5 Pro acc. chrf++ 18.91 13.41 11.83 11.58 11.36 16.33 14.19 14.81 14.36 15.82 13.62 12.36 10.67 28.54 10.56 9.44 13.95 4.86 13.68 13.81 16.7 9.6 20.93 19.39 8.39 13.91 21.52 15.65 11.71 11.02 11.66 16.77 13.96 16.32 17.3 17.02 14.7 12.33 10.65 29.98 13.26 9.69 13.48 6.25 16.58 12.02 19.38 11.94 23.89 22.22 10.11 14.39 19.30 13.42 12.57 12.67 12.72 15.60 15.71 15.25 13.85 14.70 14.63 12.37 10. 29.36 11.24 14.58 14.89 5.79 13.68 14.87 19.15 10.07 22.03 20.33 8.62 13.43 20.01 14.90 13.30 15.60 12.72 18.32 15.41 15.76 15.75 18.99 16.56 12.62 11.40 29.55 15.04 14.13 15.73 6.02 17.24 14.92 18.18 14.67 24.6 22.8 10.87 17.46 35.14 40.36 25.41 23.14 26.90 44.25 25.34 37.54 38.42 34.18 25.76 29.77 27.21 57.49 49.44 32.69 37.35 34.95 57.11 33.5 49.22 42.62 49.89 37.35 39.57 49.4 43. Average 13.79 14.13 14.66 15.63 14. 15.25 15.49 17.01 31.80 Table 16: Audio Open-ended for English (Image only Audio) Language G3n-2B G3n-4B Native Q-3B Q-7B Gemini-2.5 Pro chrF++ LLM-acc chrF++ LLM-acc chrF++ LLM-acc chrF++ LLM-acc chrF++ LLM-acc Ghana-twi Ethiopia-amh Malawi-nya Nigeria-hau Nigeria-ibo Kenya-kik Rwanda-kin Uganda-lug Ethiopia-orm Botswana-tsn Somali-som Eritrea-tir Nigeria-yor Lesotho-sot S.Africa-zul Average 6.96 0.08 8.44 7.45 6.39 9.07 8.11 8.21 9.04 6.97 6.65 0.02 4.18 9.01 9.65 6.68 2.45 3.28 2.68 4.58 0.25 2.13 5.17 4.35 6.44 2.12 4.25 3.21 1.42 2.89 5.12 3.36 6.89 0.15 7.75 7.35 5.51 9.14 7.55 7.51 8.46 6.89 5.81 0.04 4.00 8.41 10. 6.40 4.70 6.11 2.43 3.86 1.00 2.93 5.17 5.49 8.11 3.40 7.55 4.28 3.31 4.22 6.76 4.62 9.83 6.74 9.68 10.69 7.23 9.03 9.84 5.94 4.67 9.69 11.33 4.67 6.84 8.70 7.88 8.18 6.75 1.31 3.65 3.86 0.50 2.13 5.86 2.75 0.72 4.67 10.38 0.43 2.36 1.56 2. 3.31 9.99 5.27 9.38 9.85 6.64 9.09 9.47 5.26 8.15 9.89 11.52 5.35 7.39 8.94 7.67 8.26 5.52 1.09 2.43 4.58 0.75 1.86 3.79 1.37 0.95 3.82 10.14 0.00 2.84 2.00 2.25 2.89 15.72 28.23 20.66 22.09 11.23 20.14 25.57 24.11 27.87 24.55 17.59 15.71 15.73 13.68 34. 21.13 18.40 58.52 21.90 36.63 15.25 23.73 41.03 31.65 40.19 31.62 29.48 36.70 35.63 20.67 40.37 32.12 Table 17: Audio Open-ended for Native (Image only Audio) Gemma3n-2B Gemma3n-4B Qwen2.5-3B Qwen2.5-7B Gemini-2.5 Pro 0.684 (p=0.029) XNLI - MMLU XNLI - Afri-MCQA 0.261 (p=0.498) MMLU - Afri-MCQA 0.373 (p=0.288) 0.88 (p=0.001) 0.636 (p=0.065) 0.463 (p=0.178) 0.254 (p=0.479) -0.388 (p=0.302) 0.236 (p=0.511) 0.426 (p=0.22) 0.316 (p=0.407) 0.479 (p=0.162) 0.831 (p=0.003) -0.248 (p=0.519) -0.222 (p=0.537) Table"
        },
        {
            "title": "J Annotator Demography",
            "content": "Annotator ID Gender Age Group Resides in Africa Ethiopia-amh Nigeria-hau Nigeria-ibo Uganda-lug Ethiopia-orm Rwanda-kin Kenya-kik Somali-som Eritrea-tir Ghana-twi Nigeria-yor Botswana-tsn Malawi-nya S.Africa-zul Lesotho-sot 18-30 30-40 0-40 18-30 30-40 18-30 30-40 18-30 18-30 18-30 30-40 30-40 18-30 18-30 30-"
        },
        {
            "title": "M\nF\nF\nF\nM\nF\nF\nF\nM\nF\nM\nM\nF\nF\nF",
            "content": "Table 19: Annotator Demographics"
        }
    ],
    "affiliations": [
        "AI4Bharat, Indian Institute of Technology, Madras",
        "Addis Ababa University",
        "Aston University",
        "Friedrich-Alexander University",
        "Independent",
        "Kabale University",
        "Lelapa AI",
        "Lesan AI",
        "MBZUAI",
        "Mila, McGill University & Canada CIFAR AI Chair",
        "Saarland University",
        "Tshwane University of Technology",
        "University of Botswana",
        "University of Minnesota - Twin Cities",
        "University of Pretoria"
    ]
}