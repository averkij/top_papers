{
    "paper_title": "Ovis-Image Technical Report",
    "authors": [
        "Guo-Hua Wang",
        "Liangfu Cao",
        "Tianyu Cui",
        "Minghao Fu",
        "Xiaohao Chen",
        "Pengxin Zhan",
        "Jianshan Zhao",
        "Lan Li",
        "Bowen Fu",
        "Jiaqi Liu",
        "Qing-Guo Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce $\\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models."
        },
        {
            "title": "Start",
            "content": "2025-12-01 Ovis-Image Technical Report Ovis Team, Alibaba Group https://github.com/AIDC-AI/Ovis-Image https://huggingface.co/AIDC-AI/Ovis-Image-7B"
        },
        {
            "title": "Abstract",
            "content": "We introduce Ovis-Image, 7B text-to-image model specifically optimized for highquality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as QwenImage and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining strong multimodal backbone with carefully designed, textfocused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models. 5 2 0 2 8 2 ] . [ 1 2 8 9 2 2 . 1 1 5 2 : r Figure 1: Comprehensive illustration of the functional capabilities of Ovis-Image."
        },
        {
            "title": "Introduction",
            "content": "Recent breakthroughs in image generation have significantly enhanced both the visual fidelity and controllability of synthesized images, exemplified by powerful text-to-image models (Wu et al., 2025a; Team, 2025b) and unified multimodal frameworks (OpenAI, 2025; Wang et al., 2025; Google DeepMind, 2025). Nevertheless, despite this rapid progress, achieving reliable, high-quality text rendering within images at low computational cost remains persistent challenge. It requires models to simultaneously master fine-grained visual synthesis and robust language comprehension. In practice, strong textrendering capabilities are typically found only in very large models (Wu et al., 2025a; Team, 2025b), which are difficult to deploy, or in closed-source systems (Seedream et al., 2025; Google DeepMind, 2025; OpenAI, 2025), which hinder integration, customization, and reproducibility. In our prior work, Wang et al. (2025) introduced Ovis-U1, 3B unified model that integrates multimodal understanding, text-to-image generation, and image editing within single framework. It achieves competitive performance on multimodal understanding and generation benchmarks, approaching the behavior of proprietary models like GPT4o (OpenAI, 2025) in many practical scenarios. Nonetheless, with limited parameters, Ovis-U1 struggles with artifacts and hallucinations, and its in-image text quality still lags behind the best closed-source generators (Seedream et al., 2025; Google DeepMind, 2025). Concurrently, recent text-to-image systems (Wu et al., 2025a; Team, 2025b) with especially strong text rendering ability tend to follow two patterns. First, they are tightly integrated with powerful multimodal understanding backbones (Bai et al., 2025), leveraging improved visual perception and OCR-like capabilities to reason about embedded text. Second, they scale model size to tens of billions of parameters. This combination yields impressive text-centric performance but substantially increases the cost of training and deployment in real-world applications. Motivated by these observations, we aim to design specialized generator that prioritizes text rendering while preserving solid visual fidelity on general concepts and keeping computational cost acceptable. Within this design space, we present Ovis-Image, 7B text-to-image model that narrows the gap between efficiency and capability. We retain the successful pattern of coupling strong multimodal backbone with diffusion-based visual decoder, upgrade the multimodal backbone to the more capable Ovis 2.5 (Lu et al., 2025), and scale the vision-side generator to 7B parameters. Despite its compact size, Ovis-Image delivers text rendering performance comparable to much larger 20B-class open models such as Qwen-Image (Wu et al., 2025a) and approaches state-of-the-art closed-source generators like GPT4o and Gemini (Google DeepMind, 2025) on range of text-centric tasks. In practice, it produces sharp, legible, and semantically consistent text for posters, banners, UI mockups, and scenes, while maintaining competitive image quality and prompt adherence on general-purpose generation. In summary, Ovis-Image offers the following key advantages: Strong text rendering at compact 7B scale. Ovis-Image is 7B text-to-image model that delivers text rendering quality comparable to much larger 20B-class systems such as QwenImage and competitive with leading closed-source models like GPT4o in text-centric scenarios, while remaining small enough to run on widely accessible hardware. High fidelity on text-heavy, layout-sensitive prompts. The model excels on prompts that demand tight alignment between linguistic content and rendered typography (e.g., posters, banners, logos, UI mockups, infographics), producing legible, correctly spelled, and semantically consistent text across diverse fonts, sizes, and aspect ratios without compromising overall visual quality. Efficiency and deployability. With its 7B parameter budget and streamlined architecture, OvisImage fits on single high-end GPU with moderate memory, supports low-latency interactive use, and scales to batch production serving, bringing nearfrontier text rendering to applications where tens-of-billionsparameter models are impractical. Taken together, these advantages indicate that high-quality in-image text rendering does not inherently require extremely large models. With appropriate architectural choices and text-centric training pipeline built on top of strong multimodal backbone, it is possible to approach frontier-level text-to-image performance within compact 7B footprint."
        },
        {
            "title": "2 Architecture",
            "content": "The architecture of Ovis-Image is presented in Fig. 2. Ovis-Image builds upon the architecture of OvisU1 (Wang et al., 2025), simplifying certain structures while increasing the parameters of the MMDiT backbone. detailed summary of each module is provided in Tab. 1. 2 Figure 2: The overall architecture of Ovis-Image. The architecture of Ovis-Image builds upon Ovis-U1, enhancing its capabilities by increasing the parameters of MMDiT and streamlining the structural design to create more efficient and refined overall framework. Table 1: The model structure details of Ovis-Image. Module #Param. (B) Pretrain MMDiT Text Encoder VAE Total - 7.37 2.57 AIDC-AI/Ovis2.5-2B 0.08 black-forest-labs/FLUX.1-schnell 10.02 MMDiT & VAE. Building upon Ovis-U1, we employ MMDiT (Esser et al., 2024) with RoPE (Su et al., 2024) as the visual decoder and use flow matching as the training objective. Inspired by Flux.1 Lite (Verd & Martın, 2024), Ovis-Image incorporates structure of 6 double-stream blocks and 27 single-stream blocks. To enhance model capacity, the number of attention heads has been increased to 24. SwiGLU (Dauphin et al., 2017) is utilized as the activation function. Additionally, we integrate the VAE model from FLUX.1-schnell (Labs, 2024) and keep it frozen throughout the training process. Text Encoder. We use the Ovis (Lu et al., 2024) series as the text encoder in our framework. Unlike large language models like Qwen, Ovis is specifically trained on multimodal datasets, enabling superior alignment between visual and textual representations. To enhance computational efficiency, we adopt Ovis2.5-2B (Lu et al., 2025), which demonstrates better performance than Qwen2.5-VL-7B (Bai et al., 2025) on the OpenCompass benchmark suite. Unlike Ovis-U1, Ovis-Image simplifies the architecture by removing the refiner structure and directly utilizing the final hidden states of Ovis as the conditioning input for image generation."
        },
        {
            "title": "3 Data Composition",
            "content": "3.1 Data for Pre-training We pre-train Ovis-Image on large, heterogeneous corpus of imagetext pairs drawn from mixture of web-scale, licensed, and synthetic sources. The corpus covers everyday photographs, illustrations, design assets, and UI-like mockups, with descriptions ranging from short captions to instruction-style prompts. To better align the text with the visual content, we perform large-scale recaptioning in both Chinese and English. In addition to generic visual content, we include data slices where text is salient visual element, such as posters, banners, logos, and UI layouts. 3 To improve data quality, we apply multi-stage filtering pipeline combining simple heuristics, lightweight models, and cross-modal consistency checks to remove corrupted images, severely mismatched or uninformative captions, and content that does not satisfy basic safety and policy requirements. We further perform coarse deduplication to reduce near-duplicate images and prompts. For text rendering in particular, we augment the corpus with synthetic samples generated by rendering engine that composes clean typographic text into diverse backgrounds and layouts, providing the model with controlled examples of fonts, sizes, and placements. 3.2 Data for Supervised Fine-tuning For supervised fine-tuning, we curate higher-quality subset of image-text pairs, emphasizing clean visuals and well-formed prompts. Compared to the pre-training mixture, this corpus shifts toward higherresolution images (typically at 1024 pixels) and covers broad range of aspect ratios to better match real-world use cases. In addition to natural images, we include moderate amount of synthetic data, which provides sharper details, more controlled layouts, and richer coverage of rare concepts. We perform simple balancing over content type, resolution, and aspect ratio to avoid overfitting to few dominant patterns. Overall, this stage refines the models ability to produce high-fidelity, instruction-following generations under consistent high-resolution constraints. 3.3 Data for DPO For the DPO stage, we construct preference dataset on top of the supervised distribution. About 90% of this pool consists of high-quality generations that cover common object categories and everyday scenes with strong aesthetic quality. These images are pre-filtered using an ensemble of automatic scorers (including HPSv3 Ma et al. (2025), CLIP Radford et al. (2021), PickScore Kirstain et al. (2023), and related metrics), so that only samples with both good visual appeal and reasonable prompt alignment are retained. The remaining 10% comes from an in-house collection focusing on design and creative content (e.g., posters, illustrations, and stylized compositions), which exposes the model to more structured layouts and non-photographic styles. For every selected prompt, we take the associated high-quality image from this pool as one candidate and generate second candidate with the SFT model under the same text conditioning. Both images are then evaluated by multiple scoring models, and their scores are combined into an overall ranking. The higher-scoring image is treated as the winner and the other as the loser, yielding preference pair for that prompt. 3.4 Data for GRPO The GRPO stage operates on prompt distribution that is deliberately different from the one used for DPO. Instead of broad, general-purpose generation prompts, we focus on compact set of text-rendering prompts that stress the models ability to place and stylize text in images. These prompts cover both Chinese and English, span range of fonts and layouts (for example, posters, title cards, UI elements, and product labels), and vary in difficulty from short slogans to longer multi-line phrases. By concentrating the budget on this slice of the space, the GRPO data directly targets one of the known weaknesses of diffusion models, namely accurate and legible text generation."
        },
        {
            "title": "4 Training",
            "content": "4.1 Training Infrastructure Our training framework is built with PyTorch, utilizing Hybrid Sharding Data Parallel (HSDP) for efficient data parallelism and parameter sharding. To address memory limitations in larger models, we incorporate gradient checkpointing and activation offloading (Korthikanti et al., 2023). Training is conducted with bfloat16 (BF16) mixed precision while maintaining FP32 master weights for accuracy. To optimize training efficiency, we employ Flash Attention (Dao et al., 2022; Dao, 2024) and regional compilation techniques. Additionally, distributed checkpointing is implemented to minimize the overhead of saving model state. 4.2 Training Procedure Ovis-Image is trained using four-stage pipeline: pretraining stage followed by three post-training stages. Each subsequent stage is initialized using the checkpoint from the previous stage. Stage 0: Pretraining. The MMDiT is initialized randomly and optimized throughout all four training stages, while the text encoder and VAE use pretrained weights and remain frozen during training. The training objective follows the standard noise-prediction loss commonly applied in flow-matching-style diffusion models (Esser et al., 2024). AdamW serves as the optimizer, paired with constant learning rate schedule and brief linear warmup period. Initially, the model is trained on 256 256 images, followed by training on images of varying resolutions and aspect ratios, ranging from 512 to 1024 pixels and 0.25 to 4.0, respectively. This process produces robust initial model, which is further refined in the later stages using supervised data and preference optimization. Stage 1: Supervised Fine-tuning. In the second stage, we move from generic caption data to instructionstyle supervision tailored to common text-to-image usage. Starting from the pretraining checkpoint, we finetune the MMDiT on mixture of open and proprietary datasets. This stage therefore teaches the model not only what to draw, but also how to interpret instruction-like descriptions, constraints, and text-rendering requirements. The training objective remains the same noise-prediction loss as in pretraining, applied to latent representations of images up to 1024 resolution with different aspect ratios, so that the model learns to handle variable input sizes and aspect ratios at inference time. We use small learning rate and shorter schedule, which helps preserve the general visual competence learned during pretraining while adapting to the instruction-style and text-rendering distributions. In practice, we find that this supervised tuning substantially improves faithfulness to user prompts and aesthetic quality of the generated images. Stage 2: DPO. In the second stage, we apply Direct Preference Optimization (DPO) (Wallace et al., 2024) directly to the diffusion model using mixture of human and model generated preference data. Each training example consists of prompt and two images (xw, xℓ), where xw is labeled as preferred (winner) and xℓ as dispreferred (loser). We keep frozen reference model pref at the end of the supervised stage and treat the current image decoder pθ initialized from pref as the policy model, which is trained to assign higher probability to the denoising trajectory leading to the preferred sample. For each pair, we compute DPO-style log-likelihood ratio log pθ(xw, xℓ c) = (cid:2)log pθ(xw c) log pθ(xℓ c)(cid:3) (cid:2)log pref(xw c) log pref(xℓ c)(cid:3), and minimize the standard Diffusion-DPO objective LDPO(θ) = (c,xw,xℓ) (cid:104) log σ(cid:0)β log pθ(xw, xℓ c)(cid:1)(cid:105) , (1) (2) where σ() is the logistic function and β is temperature hyperparameter. In practice, the log probabilities are instantiated via the diffusion reconstruction losses along the denoising trajectories, following Wallace et al. (2024). Following Diffusion-SDPO (Fu et al., 2025), we further incorporate winner-preserving safeguard that modifies how the winner and loser branches contribute to the update. Let Lw(θ) and Lℓ(θ) denote the per-pair diffusion reconstruction losses for the winner and loser, and let gw = oLw and gℓ = oLℓ be their output-space gradients. Diffusion-SDPO explicitly computes gradient scale factor λsafe to stabilize the optimization as follows: (cid:32) λsafe = clip µ gw, gℓ gℓ2 + ε (cid:33) , λmin, λmax , (3) where µ controls the overall strength of the loser branch, ε is small constant for numerical stability, and the clipping interval [λmin, λmax] is chosen so that the first-order change of the winner loss is nonpositive. Intuitively, the loser gradient is down-weighted whenever it conflicts with the winner gradient, which implicitly clips overly aggressive loser updates and preserves the quality of the preferred branch. Omitting this safeguard leads to models that frequently produce noisy or artifact-prone images, whereas SDPO-style control yields more stable optimization and systematically better visual quality. We retain the same learning rate as used in the SFT stage, but adopt large global batch size and β in order to obtain stable preference gradients and avoid drifting far from the supervised baseline. The DPO stage enhances the model by improving its helpfulness, harmlessness, adherence to prompts (including layout and text rendering), and by reducing noticeable artifacts. Stage 3: GRPO. After training with DPO, we refine the model using Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Liu et al., 2025), conducting on-policy sampling during training and evaluating with set of reward models. For each prompt, the model generates multiple candidate images as group, which are then scored by combination of reward models. Conditioned on text prompt c, the flow model predicts group of individual images {xi i=1 with their corresponding trajectorys {xi 0}G T, xi T1, ..., xi 0}G i=1 . The advantage of i-th image within the group can be formulated as: 0, c) mean({R(xi 0, c)}G 0, c)}G i=1) std({R(xi Ai = i=1) R(xi , (4) 5 where denotes the reward model. Consequently, the training objective of GRPO is: LGRPO(θ) = cD,{xi T,...,xi 0}G i=1 (r, A, θ, ϵ, β) πθ where (r, A, θ, ϵ, β) = ri t(θ) = min(ri t(θ)Ai, clip(ri t(θ), 1 ϵ, 1 + ϵ)Ai) βDKL(πθπref) (cid:16) T1 1 1 t=0 i=1 pθ(xi t1xi t1xi (xi pθold t, c) t, c) . (5) (cid:17) To accelerate training with minimizing the impact on performance, we sample each candidate image using fewer denoising steps. Furthermore, we introduce coefficients-preserving sampling (Wang & Yu, 2025) during the GRPO stage to further enhance performance. The training window adaptively learns the needs of different denoise stages. We retain the same learning rate from the DPO stage and run GRPO for approximately 500 steps. Throughout this process, the policy is optimized to maximize the expected reward, while applying KL penalty to constrain its divergence from the DPO model."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate Ovis-Image on the text-to-image task from two perspectives: text rendering capability and general text-to-image generation capability. To evaluate the text rendering capability, we conduct evaluations on LongText-Bench (Geng et al., 2025)and CVTG-2K (Du et al., 2025). To evaluate the general text-to-image generation capability, we present comprehensive evaluation across multiple public benchmarks, including DPG-Bench (Hu et al., 2024), GenEval (Ghosh et al., 2023) and OneIGBench (Chang et al., 2025). Despite its compact parameter size, Ovis-Image consistently outperforms significantly larger open-source baselines (highlighted in gray), proving that it delivers competitive generation quality with superior parameter efficiency. CVTG-2K Table 2 presents the results of the English rendering evaluation on CVTG-2K (Du et al., 2025). This benchmark comprises 2,000 prompts, each demanding the rendering of 2 to 5 English text regions on the generated image. It introduces Word Accuracy, NED and CLIPScore to evaluate the precision of the text rendering. As shown in the table, Ovis-Image achieves the highest overall word accuracy across all regions. Additionally, Ovis-Image obtains the highest NED and CLIPScore, further confirming its superior text rendering capability. LongText-Bench Table 3 presents the evaluation results on LongText-Bench (Geng et al., 2025), benchmark designed to examine the models capability to accurately render long texts in both English and Chinese. As illustrated in the table, Ovis-Image demonstrates superior performance on the Chinese text. Despite its relatively small model parameter, Ovis-Image still excels at generating long English text, achieving performance comparable against closed-source models and models with larger parameter counts. This result highlights the particular strength of Ovis-Image in long text generation capabilities. Table 2: Evaluation of text rendering ability on CVTG-2K. Model #Params. 2 regions Word Accuracy 4 regions 3 regions 5 regions Seedream 3.0 (Gao et al., 2025) GPT4o (OpenAI, 2025) - - SD3.5 Large (Esser et al., 2024) RAG-Diffusion (Chen et al., 2024) FLUX.1-dev (Labs, 2024) TextCrafter (Du et al., 2025) Qwen-Image (Wu et al., 2025a) Ovis-Image 11B+8B 11B+12B 11B+12B 11B+12B 7B+20B 2B+7B 0.6282 0.8779 0.7293 0.4388 0.6089 0.7628 0. 0.9248 0.5962 0.8659 0.6825 0.3316 0.5531 0.7628 0.8364 0.9239 0.6043 0.8731 0.6574 0.2116 0.4661 0.7406 0. 0.9180 0.5610 0.8218 0.5940 0.1910 0.4316 0.6977 0.8158 0.9166 average 0.5924 0. 0.6548 0.2648 0.4965 0.7370 0.8288 0.8537 0.9478 0.8470 0.4498 0.6879 0.8679 0.9116 0.9200 0.9695 0.7821 0. 0.7797 0.7797 0.7401 0.7868 0.8017 0.8368 NED CLIPScore DPG-Bench Table 4 reports the results on DPG-Bench (Hu et al., 2024), benchmark of 1,000 dense prompts intended to evaluate the alignment of text-to-image generation in various dimensions, allowing detailed inspection of prompt adherence from multiple perspectives. Overall, Ovis-Image delivers robust performance compared to both close-source models and open-source models with larger parameter counts. 6 Table 3: Evaluation of text rendering ability on LongText-Bench. Model #Params. LongText-Bench-EN LongText-Bench-ZN Kolors 2.0 (Team, 2025a) GPT4o (OpenAI, 2025) Seedream 3.0 (Gao et al., 2025) OmniGen2 (Wu et al., 2025b) Janus-Pro (Chen et al., 2025b) BLIP3-o (Chen et al., 2025a) FLUX.1-dev (Labs, 2024) BAGEL (Deng et al., 2025) HiDream-I1-Full (Cai et al., 2025) Qwen-Image (Wu et al., 2025a) Ovis-Image - - - 3B+4B 7B 7B+1B 11B+12B 7B+7B 11B+17B 7B+20B 2B+7B 0.258 0.956 0.896 0.561 0.019 0.021 0.607 0.373 0.543 0.943 0. 0.329 0.619 0.878 0.059 0.006 0.018 0.005 0.310 0.024 0.946 0.964 GenEval Table 5 summarizes the performance on GenEval (Ghosh et al., 2023) benchmark, which emphasizes object-centric text-to-image generation by employing compositional prompts with wide range of object attributes. These results exhibit the competitive controllable generation capabilities of Ovis-Image. OneIG-Bench Table 6 and Table 7 show the performance comparison on OneIG-Bench (Chang et al., 2025), comprehensive benchmark developed for detailed evaluation of T2I models across multiple dimensions. As shown in the table, Ovis-Image demonstrates exceptional bilingual performance, particularly distinguished by its performance in the text dimensions. Table 4: Evaluation of text-to-image generation ability on DPG-Bench. Model #Params. Global Entity Attribute Relation Other Overall Seedream 3.0 (Gao et al., 2025) GPT4o (OpenAI, 2025) - - Ovis-U1 (Wang et al., 2025) OmniGen2 (Wu et al., 2025b) Janus-Pro (Chen et al., 2025b) BAGEL (Deng et al., 2025) HiDream-I1-Full (Cai et al., 2025) UniWorld-V1 (Lin et al., 2025) Qwen-Image (Wu et al., 2025a) Ovis-Image 2B+1B 3B+4B 7B 7B+7B 11B+17B 7B+12B 7B+20B 2B+7B 94.31 88.89 82.37 88.81 86.90 88.94 76.44 83.64 91.32 82.37 92.65 88. 90.08 88.83 88.90 90.37 90.22 88.39 91.56 92.38 91.36 89.84 88.68 90.18 89.40 91.29 89.48 88.44 92.02 90.42 92.78 92. 93.35 89.37 89.32 90.82 93.74 89.27 94.31 93.98 88.24 90.96 85.20 90.27 89.48 88.67 91.83 87.22 92.73 91.20 88.27 85. 83.72 83.57 84.19 85.07 85.89 81.38 88.32 86.59 Table 5: Evaluation of text-to-image generation ability on GenEval. Model #Params. Single object Two object Counting Colors Position Attribute binding Overall Seedream 3.0 (Gao et al., 2025) GPT4o (OpenAI, 2025) - - Ovis-U1 (Wang et al., 2025) OmniGen2 (Wu et al., 2025b) Janus-Pro (Chen et al., 2025b) BAGEL (Deng et al., 2025) HiDream-I1-Full (Cai et al., 2025) UniWorld-V1 (Lin et al., 2025) Qwen-Image (Wu et al., 2025a) Ovis-Image 2B+1B 3B+4B 7B 7B+7B 11B+17B 7B+12B 7B+20B 2B+7B 0.99 0.99 0.98 1.00 0.99 0.99 1.00 0.99 0.99 1.00 0.96 0.92 0.98 0.95 0.89 0.94 0.98 0.93 0.92 0. 0.91 0.85 0.90 0.64 0.59 0.81 0.79 0.79 0.89 0.76 0.93 0.92 0.92 0.88 0.90 0.88 0.91 0.89 0.88 0. 0.47 0.75 0.79 0.55 0.79 0.64 0.60 0.49 0.76 0.67 0.80 0.61 0.75 0.76 0.66 0.63 0.72 0.70 0.77 0. 0.84 0.84 0.89 0.80 0.80 0.82 0.83 0.80 0.87 0.84 Computational Overhead Table 8 presents comparative analysis of computational overhead, focusing on inference time and GPU memory utilization. Ovis-Image exhibits superior trade-off between resource efficiency and model performance. Most notably, Ovis-Image maintains significantly lower memory footprint compared to larger baselines. Furthermore, in terms of temporal efficiency, Ovis-Image achieves substantial speedup,thereby offering more practical solution for resource-constrained environments. 7 Table 6: Evaluation of text-to-image generation ability on OneIG-EN. Model #Params. Alignment Text Reasoning Style Diversity Overall Kolors 2.0 (Team, 2025a) Imagen4 (Google, 2025) Seedream 3.0 (Gao et al., 2025) GPT4o (OpenAI, 2025) Ovis-U1 (Wang et al., 2025) CogView4 (Z.ai., 2025) Janus-Pro (Chen et al., 2025b) OmniGen2 (Wu et al., 2025b) BLIP3-o (Chen et al., 2025a) FLUX.1-dev (Labs, 2024) BAGEL (Deng et al., 2025) BAGEL+CoT (Deng et al., 2025) HiDream-I1-Full (Cai et al., 2025) HunyuanImage-2.1 (Team, 2025b) Qwen-Image (Wu et al., 2025a) Ovis-Image - - - - 2B+1B 6B 7B 3B+4B 7B+1B 11B+12B 7B+7B 7B+7B 11B+17B 7B+17B 7B+20B 2B+7B 0.820 0.857 0.818 0. 0.816 0.786 0.553 0.804 0.711 0.786 0.769 0.793 0.829 0.835 0.882 0.858 0.427 0.805 0.865 0.857 0.034 0.641 0.001 0.680 0.013 0.523 0.244 0.020 0.707 0.816 0.891 0.914 0.262 0.338 0.275 0. 0.226 0.246 0.139 0.271 0.223 0.253 0.173 0.206 0.317 0.299 0.306 0.308 0.360 0.377 0.413 0.462 0.443 0.353 0.276 0.377 0.361 0.368 0.367 0.390 0.347 0.355 0.418 0.386 0.300 0.199 0.277 0. 0.191 0.205 0.365 0.242 0.229 0.238 0.251 0.209 0.186 0.127 0.197 0.186 0.434 0.515 0.530 0.533 0.342 0.446 0.267 0.475 0.307 0.434 0.361 0.324 0.477 0.486 0.539 0.530 Table 7: Evaluation of text-to-image generation ability on OneIG-ZN. Model #Params. Alignment Text Reasoning Style Diversity Overall Kolors 2.0 (Team, 2025a) Seedream 3.0 (Gao et al., 2025) GPT4o (OpenAI, 2025) CogView4 (Z.ai., 2025) Janus-Pro (Chen et al., 2025b) BLIP3-o (Chen et al., 2025a) BAGEL (Deng et al., 2025) BAGEL+CoT (Deng et al., 2025) HiDream-I1-Full (Cai et al., 2025) HunyuanImage-2.1 (Team, 2025b) Qwen-Image (Wu et al., 2025a) Ovis-Image - - - 6B 7B 7B+1B 7B+7B 7B+7B 11B+17B 7B+17B 7B+20B 2B+7B 0.738 0.793 0. 0.700 0.324 0.608 0.672 0.719 0.620 0.775 0.825 0.805 0.502 0.928 0.650 0.193 0.148 0.092 0.365 0.127 0.205 0.896 0.963 0.961 0.226 0.281 0. 0.236 0.104 0.213 0.186 0.219 0.256 0.271 0.267 0.273 0.331 0.397 0.449 0.348 0.264 0.369 0.357 0.385 0.304 0.348 0.405 0.368 0.333 0.243 0. 0.214 0.358 0.233 0.268 0.197 0.300 0.114 0.279 0.198 0.426 0.528 0.474 0.338 0.240 0.303 0.370 0.329 0.337 0.481 0.548 0.521 Table 8: Comparison of model inference time and GPU memory usage (10241024 images, 50-step sampling, BF16 inference) Model #Params. Accelerate A100 H100 Memory (MB) Time (s) Memory (MB) Time (s) Flux.1-dev Qwen-Image Ovis-U1 Ovis-Image 11B+12B 7B+20B 2B+1B 2B+7B guidance dis. - - - 34637 59329 10528 24959 23.51 45.16 8.41 30.56 34661 59354 11937 11.03 20.27 4.29 13."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Ovis-Image, 7B text-to-image model designed to reconcile strong in-image text rendering with practical deployment cost. By pairing diffusion-based visual decoder with the Ovis 2.5 multimodal backbone and training it through text-centric pipeline, Ovis-Image attains text rendering quality comparable to much larger open models and approaches leading closed-source systems, while preserving solid general-purpose generation and fitting on single high-end GPU. Beyond the empirical gains, Ovis-Image illustrates more general design principle: frontier-like text-aware generation can emerge from moderate-scale models when architectural choices, data curation, and alignment objectives are explicitly organized around the demands of in-image text, rather than treated as byproduct of generic image synthesis."
        },
        {
            "title": "7 Contributors",
            "content": "Guo-Hua Wang1, Liangfu Cao, Tianyu Cui, Minghao Fu2, Xiaohao Chen, Pengxin Zhan, Jianshan Zhao, Lan Li2, Bowen Fu2, Jiaqi Liu, Qing-Guo Chen"
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. HiDream-I1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. OneIG-Bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. BLIP3-o: family of fully open unified multimodal modelsarchitecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-Pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memoryIn Advances in Neural Information Processing Systems efficient exact attention with IO-awareness. (NeurIPS), 2022. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933941. PMLR, 2017. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying arXiv preprint Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv:2503.23461, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for highresolution image synthesis. In Forty-first international conference on machine learning, 2024. Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Diffusion-sdpo: Safeguarded direct preference optimization for diffusion models. arXiv preprint arXiv:2511.03317, 2025. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. GENEVAL: An object-focused framework for evaluating text-to-image alignment. In Advances in Neural Information Processing Systems, 2023. 1Correspondence to Guo-Hua Wang <wangguohua@alibaba-inc.com> 2Work done during the internship at Alibaba Group Google. Imagen. https://deepmind.google/models/imagen/, 2025. Google DeepMind. Gemini 3 Pro Image: Model card. https://deepmind.google/models/model-cards/g emini-3-pro-image/, 2025. Model card, accessed Nov. 25, 2025. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. EllA: Equip diffusion models with LLM for enhanced semantic alignment. arXiv:2403.05135, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-Pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems, pp. 36652 36663, 2023. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353, 2023. Black Forest Labs. FLUX. https://github.com/black-forest-labs/flux, 2024. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. UniWorld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, arXiv preprint and Wanli Ouyang. Flow-GRPO: Training flow matching models via online rl. arXiv:2505.05470, 2025. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2.5 technical report. arXiv preprint arXiv:2508.11737, 2025. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. HPSv3: Towards wide-spectrum human preference score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15086 15095, 2025. OpenAI. Introducing 4o image generation. https://openai.com/index/introducing-4o-image-generat ion/, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763, 2021. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Kuaishou Kolors Team. Kolors 2.0. https://app.klingai.com/cn/, 2025a. Tencent Hunyuan Team. HunyuanImage 2.1: An efficient diffusion model for high-resolution (2k) text-to-image generation. https://github.com/Tencent-Hunyuan/HunyuanImage-2.1, 2025b. Daniel Verd and Javier Martın. Flux.1 Lite: Distilling flux.1-dev for efficient text-to-image generation, 2024. Freepik Research. Contact: dverdu@freepik.com, javier.martin@freepik.com. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Feng Wang and Zihao Yu. Coefficients-preserving sampling for reinforcement learning with flow matching. arXiv preprint arXiv:2509.05952, 2025. 10 Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-U1 technical report. arXiv preprint arXiv:2506.23044, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-Image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. OmniGen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. THUKEG Z.ai. Cogview4. https://github.com/THUDM/CogView4, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}