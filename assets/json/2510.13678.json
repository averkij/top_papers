{
    "paper_title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "authors": [
        "Xinyang Li",
        "Tengfei Wang",
        "Zixiao Gu",
        "Shengchuan Zhang",
        "Chunchao Guo",
        "Liujuan Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 7 6 3 1 . 0 1 5 2 : r Preprint Paper FLASHWORLD: HIGH-QUALITY 3D SCENE GENERATION WITHIN SECONDS Xinyang Li1, Tengfei Wang2, Zixiao Gu3, Shengchuan Zhang1, Chunchao Guo2, Liujuan Cao1 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 2 Tencent, 3 Yes Lab, Fudan University Project Page: https://imlixinyang.github.io/FlashWorld-Project-Page/"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose FlashWorld, generative model that produces 3D scenes from single image or text prompt in seconds, 10 100 faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes dual-mode pre-training phase followed by cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from video diffusion model, we first pre-train dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose strategy to leverage massive single-view images and text prompts during this process to enhance the models generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method. Figure 1: FlashWorld enables fast and high-quality 3D scene generation across diverse scenes."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D generation shows great promise for applications in gaming, robotics, and VR/AR. However, generating full 3D scenes remains significant challenge for both quality and efficiency, compared to Work done during an internship at Tencent. Corresponding Authors. 1 Preprint Paper Input CAT3D Bolt3D Wonderland 77 min (A100 GPU) 15 sec (A100 GPU) 5 min (A100 GPU) Ours w/ MV-Diff Ours w/ 3D-Diff Ours w/ MV-Dist Ours 1 min (H20 GPU) 1 min (H20 GPU) 7 sec (H20 GPU) 9 sec (H20 GPU) Figure 2: brief comparison of different 3D scene generation methods. MV-oriented diffusion methods (i.e., CAT3D (Gao et al., 2024), Bolt3D (Szymanowicz et al., 2025), Wonderland (Liang et al., 2025), and Ours w/ MV-Diff) suffer from noisy textures due to multi-view inconsistency. MV-oriented distillation further exacerbates this flaw (i.e., Ours w/ MV-Dist). 3D-oriented diffusion methods (i.e., Ours w/ 3D-Diff) suffer from blurry visual effect. Our cross-mode distillation model (i.e., Ours) simultaneously solves these, making the quality of the novel view close to the input view. The time cost per scene, tested on single GPU, is presented at the bottom of each method. generating individual 3D objects. These challenges stem from two core obstacles: the scarcity of high-quality 3D scene data and the exponential complexity of modeling real-world scenes. Early methods typically relied on assembling pre-existing 3D assets (Xu et al., 2002; Yu et al., 2011; Wu et al., 2018; Feng et al., 2023; elen et al., 2024; Yang et al., 2024c; Deng et al., 2025) or iteratively reconstructing scenes from inpainted images and depth maps (Cai et al., 2023; Fridman et al., 2023; Hollein et al., 2023; Lei et al., 2023; Yu et al., 2024; Zhang et al., 2024b;c; Chung et al., 2023; Yu et al., 2025; Shriram et al., 2025; Ni et al., 2025). Yet, without holistic scene-level understanding or multi-view consistency constraints, these approaches often struggle to produce semantically coherent and visually realistic scenes. To address this, scalable data-driven approaches have emerged. The dominant paradigm is two-stage, multi-view-oriented (MV-oriented) pipeline (Gao et al., 2024; Sun et al., 2024; Wallingford et al., 2024; Zhao et al., 2025; Szymanowicz et al., 2025; Yang et al., 2025; Go et al., 2025a;b): diffusion model first generates multiple views from text or reference images, and then 3D reconstruction is performed. However, the lack of explicit 3D constraints during view synthesis often causes geometric and semantic inconsistencies in generated views, leading to noticeable visual quality gap between synthesized views and the reconstructed 3D scene. Moreover, the considerable computational overhead of both the diffusion and reconstruction stages leads to generation latencies of several minutes to hours, as shown in Fig. 2. These limitations compromise the effectiveness and efficiency of current 3D scene generation methods, blocking their applications. One promising but relatively less explored direction is the 3D-oriented scene generation pipeline (Xu et al., 2023; Li et al., 2024a;b; Tang et al., 2025; Cai et al., 2024). These methods combine differentiable rendering (Mildenhall et al., 2020; Wang et al., 2021; Kerbl et al., 2023) with diffusion models, allowing for direct 3D scene generation without an additional reconstruction stage. However, these generated 3D scenes often suffer from visual artifacts and blurry content. Consequently, they often require an additional refinement stage, which significantly degrades generation efficiency. Preprint Paper To enhance efficiency of diffusion models, post-training distillation techniques, such as consistency model distillation (Song et al., 2023) and distribution matching distillation (Yin et al., 2024b;a; Xie et al., 2024), are often used. However, directly applying distillation amplifies each frameworks inherent limitations: e.g., it exacerbates multi-view inconsistency in the MV-oriented pipeline. In this work, we introduce novel framework that combines the strengths of both paradigms through distillation, achieving substantial gains in 3D consistency and visual fidelity while significantly accelerating inference speed. Our contributions are briefly summarized as follows: We introduce dual mode pretraining strategy built on video diffusion model to train multi-view diffusion model capable of operating in both MV-oriented and 3D-oriented modes. We propose cross-mode post-training strategy, where the MV-oriented mode serves as the teacher to improve visual quality, while the 3D-oriented mode acts as the student to ensure 3D consistency. To improve out-of-distribution generalization ability, we introduce novel strategy that can leverage massive unlabeled image data and text prompts with randomly simulated camera trajectories during post-training, enhancing the models adaptability to diverse inputs, as shown in Fig. 1."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Diffusion models (Ho et al., 2020) generate data by progressively transforming samples from standard Gaussian distribution p(xT ) (0, I) into samples from target data distribution p(x), which have been widely applied across multiple domains, including image synthesis (Rombach et al., 2022), multi-view generation (Shi et al., 2023; Tang et al., 2023), video generation (Blattmann et al., 2023; Wan et al., 2025), and panoramic 3D scenes (HunyuanWorld, 2025). The core methodology involves training denoising network with optimizable parameters to reconstruct the original data by removing the injected Gaussian noise ϵ from according to predefined noise schedule. The forward process is formulated as: xt = (x, t) = αtx + σtϵ, where αt and σt jointly control the signal-to-noise ratio at each timestep t. The denoising network can be trained to predict clean data ˆx from noisy input xt by minimizing the following objective: = Ex,t,ϵ (cid:104) ˆxθ(xt, t)2(cid:105) . (1) Alternative training objectives include predicting noise ϵ (Ho et al., 2020) or linear combination of x0 and ϵ, known as v-prediction (Salimans & Ho, 2022). All predictions can be converted to the denoised estimate µ(xt, t) and represent the gradient of the log probability of the distribution: s(xt, t) = xt log pt(xt) = xt αtµ(xt, t) σ2 . (2) Distribution matching distillation (DMD) (Yin et al., 2024b;a) is an advanced technique designed to distill slow, multi-step teacher diffusion model into fast, few-step student model with comparable generation capabilities. The key component is to minimize the approximate KL divergence across randomly sampled timesteps and noise inputs between the smoothed real data distribution preal(xt) and the student generators output distribution pfake(xt) by: LDMD = Et (cid:18)(cid:90) (sreal(F (Gθ(z), t), t) sfake(F (Gθ(z), t), t)) dGθ(z) dθ (cid:19) dz , (3) where sreal and sfake are approximated scores using diffusion models µreal and µfake trained on their respective distributions (Eq. 1). DMD uses frozen pre-trained diffusion model µreal as the teacher, and dynamically updates µfake while training Gθ, using diffusion loss on samples from the generator."
        },
        {
            "title": "3 METHOD",
            "content": "The core of our framework lies in leveraging DMD to transfer knowledge from MV-oriented multi-view diffusion model, one well-established for high visual quality, to 3D-oriented few-step multi-view generator, which is inherently endowed with 3D consistency. However, this paradigm introduces two key challenges: First, for open-world 3D scene generation, the 3D-oriented few-step 3 Preprint Paper Figure 3: Method overview. We first pre-train dual-mode multi-view latent diffusion model using multi-view datasets, and then employ an cross-mode distillation post-training strategy to accelerate generation while enhancing visual quality and inheriting 3D consistency. generator requires sufficiently robust prior and strong generative capacity from the start. Without this, the training process is prone to collapse. Second, due to the limited quantity and diversity of high-quality multi-view datasets, it becomes critical to develop strategy that effectively handles scenarios with diverse styles, object categories, and camera trajectories. Specifically, To address these challenges, we first design dual-mode pre-training strategy as detailed in Sec. 3.1. This strategy yields multi-view diffusion model that operates in two distinct modes: MV-oriented mode for high visual fidelity and 3D-oriented mode for inherent 3D consistency. Subsequently, in Sec. 3.2, we present cross-mode post-training framework to bridge these two modes: the MV-oriented mode acts as the teacher, supplying score distillation gradients to ensure visual quality; the 3D-oriented mode serves as the student, learning to inherit the teachers distribution while preserving 3D consistency. Furthermore, to explicitly tackle out-of-distribution generalization, in Sec. 3.3, we introduce strategy that can leverage single-view image data, text prompts, and pre-defined camera trajectories, boosting the models adaptability to diverse scenarios. 3.1 DUAL-MODE PRE-TRAINING In this stage, we pre-train dual-mode multi-view latent diffusion model using multi-view datasets, as illustrated in Fig. 3 (left). For each training iteration, we sample batch containing multi-view images , their corresponding camera parameters C, and additional conditioning information (such as text prompt or single-view image). The multi-view images are first encoded into the latent space to obtain multi-view latents = E(X ). forward diffusion process is then applied to produce noisy multi-view latents Zt = αtZ + σtϵ at randomly sampled timestep t. The noisy latents Zt, together with the camera parameters and conditioning y, are input to the denoising network for reverse denoising training. We represent cameras using Reference-Point Plucker Coordinates (Cai et al., 2024) raymaps. The denoising network is Diffusion Transformer (DiT) (Peebles & Xie, 2023) enhanced with 3D attention blocks, and outputs both denoised estimate ˆZMV and an auxiliary multi-view feature F. For the MV-oriented mode, we optimize: LMV = EX ,t,ϵ,y,C (cid:20)(cid:13) (cid:13)Z ˆZMV (cid:13) 2(cid:21) (cid:13) (cid:13) (cid:13) . (4) To enable 3D-oriented generation, we decode 3D Gaussian parameters from the multi-view feature using 3DGS decoder: {τ, q, s, α, c} = DG(F), where τ , q, s, α, and represent the depth, rotation quaternion, scale, opacity, and spherical harmonics coefficients of the 3D Gaussians, respectively. The 3DGS decoder DG is initialized from the original latent decoder D, with its first and last convolutional layers re-initialized to accommodate the additional features and output channels required for the Gaussian parameters. The predicted depth is then converted to pixel-aligned Gaussian points via µ = + τ d, where and denote the camera origin and ray direction, respectively. For the 4 Preprint Paper 3D-oriented mode, we optimize the following loss: L3D = EX ,t,ϵ,y,C Xnovel R(G, Cnovel)2(cid:105) (cid:104) , (5) where denotes the rendering operation, = {µ, q, s, α, c} is the set of 3D Gaussians, and Xnovel, Cnovel are the ground-truth novel-view images and their associated cameras, respectively. During inference, both MV-oriented and 3D-oriented modes can be used for denoising (Li et al., 2024a;b). In particular, for the 3D-oriented mode, the model predicts the estimated clean multi-view latents as ˆZ3D = E(R(G, C)). In contrast to previous methods (Li et al., 2024a;b) that are initialized from image diffusion models (Rombach et al., 2022), we initialize our framework with video diffusion model (Wan et al., 2025). We observe that this video model not only converges more rapidly, but also features powerful VAE with higher compression rate, enabling support for larger number of views (i.e., 24) and higher output resolutions (i.e., 480P). 3.2 CROSS-MODE POST-TRAINING After pre-training, we employ an asymmetric distillation strategy to accelerate generation while enhancing visual quality and inheriting 3D consistency, as shown in Fig 3 (right). Specifically, we observe that while the MV-oriented mode exhibits poor consistency, it can generate multi-view images with high visual quality; thus, we leverage the MV-oriented mode of our dual-mode multi-view latent diffusion model as the real teacher µreal: this teacher model is frozen, tasked with computing the real score gradient. Another copy of the model µfake is dynamically updated to estimate the fake score corresponding to the current distribution of the distilled generator. Meanwhile, our few-step student model is initialized with the 3D-oriented mode of our dual-mode multi-view latent diffusion model. Specifically, to generate 3D scene, the 3D-oriented multi-view generation process alternates between denoising and noise injection steps to enhance sample quality (Luo et al., 2023). We first define schedule of timesteps, denoted as {t1, t2, , tN }, where is typically small (e.g., 4). Starting from randomly sampled noise Zt1 = (0, I), we alternate between 3D-oriented denoising updates ˆZti = E(R(Gθ,3D(Zti, ti, y, C), C)) and forward diffusion steps Zti+1 = αti+1 ˆZti + σti+1ϵ where Gθ,3D is the 3DGS generator and ϵ (0, I), until obtaining the 3D Gaussians at the final step (i.e., Gθ,3D(ZtN , tN , y, C)). At each step, the multi-view denoising update is performed based on rendering, thereby ensuring that 3D consistency is maintained throughout the process. During distillation training, we adopt the DMD2 algorithm (Yin et al., 2024a), which includes DMD objective (i.e., Eq. 3) and standard non-saturating GAN objective (Goodfellow et al., 2020), where the logits value required by the GAN loss is obtained by adding an extra classification branch with several convolutional layers at the end of the fake score network. We adopt the estimated R1 regularization (Lin et al., 2025a) to stabilize the GAN training. The DMD objective and the GAN objective are employed to optimize both the input and novel views. We also observe that relying solely on the above strategy can lead to the generation of scenes with unstable floating artifacts. We hypothesize that this instability arises from the challenges in optimizing with noisy gradients introduced by Gaussian rendering and latent encoding. To address this, during post-training, we additionally update an MV-oriented student model at lower frequency. This model shares the same DiT backbone as the 3D-oriented student model. To encourage alignment between the two modes, we introduce cross-mode consistency loss: E(R(Gθ,3D(Zti, ti, y, C), C)) Gθ,MV(Zti, ti, y, C)2(cid:105) (cid:104) LCMC = Ez,t,ϵ,y,C,i (6) , where λ is small weighting factor (i.e., 0.1). Because the MV-oriented mode prediction are less affected by unstable rendering gradients, this consistency loss regularizes the 3D-oriented mode to produce more stable and reliable generations. 3.3 OUT-OF-DISTRIBUTION DATA CO-TRAINING. During pre-training, it is common to jointly train on image and video generation tasks to enhance the models generalization ability. While this approach benefits the DiT backbone, it does not optimize the 3DGS decoder, potentially limiting the range of inputs the 3DGS decoder can effectively process. 5 Preprint Paper u n s CAT3D Bolt3D Wonderland Figure 4: Image-to-3D scene generation results of different methods. To address this, in the post-training phase, we introduce strategy to broaden the models input distribution and improve generalization to diverse scenes, even when multi-view data is limited in quantity and variety. Specifically, we combine image or text conditions sampled from image datasets with random camera trajectories, which can be drawn either from multi-view sequences or from set of predefined trajectories. Importantly, we omit the GAN loss during this co-training process to prevent distribution mismatches. This approach not only enhances the models generalization to wide range of input images and text prompts, but also increases its robustness when encountering out-of-distribution camera trajectories. The details of this strategy are provided in Appendix ."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate the performance of our method on various benchmarks, including imageto-3D scene generation, text-to-3D scene generation, and WorldScore benchmark. For implementation details, please refer to Appendix A. 4.1 COMPARISON ON IMAGE-TO-3D SCENE GENERATION We present qualitative comparison with state-of-the-art image-to-3D scene generation methods in Fig. 4. These baselines are MV-oriented, including: CAT3D (Gao et al., 2024), which generates novel views via multi-view diffusion followed by optimization-based 3D reconstruction; Bolt3D (Szymanowicz et al., 2025), which synthesizes both appearance and geometry for novel views and then applies feed-forward 3D reconstruction; and Wonderland (Liang et al., 2025), leading approach that leverages powerful video diffusion model and latent-based feed-forward 3D reconstruction. As these methods are not open-sourced, we utilize the video results provided in their respective project pages for visualization. We employ ViPE (Huang et al., 2025a) to estimate camera poses and intrinsics from the baseline videos. CAT3D struggles to generate complex scenes, resulting in blurry outputs and missing geometric details. Bolt3D also exhibits inaccurate geometric details, such as imprecise tree branches and needle-like leaves. Wonderland suffers from repeated and distorted Gaussian artifacts, especially under large camera pose changes. Overall, these MV-oriented methods fail to generate complex scenes, primarily due to insufficient multi-view consistency. In contrast, our model produces high-fidelity, detailed scenes and successfully recovers intricate structures (e.g., leaves, iron fences, and tentacles), highlighting the advantages of our 3D-oriented pipeline. 4.2 COMPARISON ON TEXT-TO-3D SCENE GENERATION We compare our method against several state-of-the-art text-to-3D scene generation approaches, including Director3D (Li et al., 2024b), Prometheus (Yang et al., 2025), SplatFlow (Go et al., 2025a), and VideoRFSplat (Go et al., 2025a). qualitative comparison is presented in Fig. 5. Director3D relies on per-scene refinement, which frequently introduces blurry and wave-like artifacts in the Preprint Paper fluffy, orange cat. spacious kitchen with wooden floors, white countertops, and an island in the center. traditional wooden gate with red lanterns. large stone fountain surrounded by lush greenery and clear blue sky. Director3D Prometheus SplatFlow VideoRFSplat n u n s Figure 5: Text-to-3D scene generation results of different methods. Method Q-Align IQA Director3D Promtheus Ours 3.24 2.34 4.12 T3Bench-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 1.95 1.92 2.26 0.43 0.34 0.54 4.70 4.76 4.49 CLIP Score 27.84 24.85 27.68 Q-Align IQA 2.51 2.07 3.96 DL3DV-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 1.78 1.99 2.27 0.34 0.35 0.50 4.55 4.69 4.77 CLIP Score 26.12 23.49 27.63 Q-Align IQA 2.55 2.45 3.76 WorldScore-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 2.47 2.94 2.55 0.35 0.37 0. 5.32 5.65 5.08 CLIP Score 29.05 28.07 29.13 Time Cost 7 min 15 sec 9 sec Table 1: Quantitative comparison on text-to-3D scene generation. Cell background colors indicate the method is the best , second best , or third best on this metric. generated results. In contrast, our model produces accurate objects with fine-grained details, such as animal fur, while preserving realistic backgrounds. Prometheus does not utilize refinement, and due to the inherent inconsistency of its MV-oriented pipeline, the generated scenes are often blurry and may exhibit incorrect object geometries (e.g., chair legs). Our approach, however, is capable of generating structurally rich and precise objects in complex scenes, even under large camera movements. SplatFlow and VideoRFSplat also suffer from blurry artifacts and have difficulty reproducing fine details, such as those found in floors and grass. In comparison, our model generates realistic details while maintaining semantic consistency with the input text prompt. We further perform comprehensive quantitative evaluation for this task. Specifically, we sample 600 text prompts from T3Bench (He et al., 2023), DL3DV (Ling et al., 2024), and WorldScore (Duan et al., 2025), covering object-centric and general scenes. As all compared methods are based on 3D Gaussian representations, metrics related to camera control and 3D consistency are not applicable in this setting. Accordingly, we concentrate on the quality evaluation metrics utilized, including CLIP IQA+ (Wang et al., 2023), CLIP Aesthetic (Schuhmann, 2022), the text-image alignment score (CLIP Score) (Hessel et al., 2021), as well as the latest LMM-based Q-Align (Wu et al., 2024) image quality metric. The quantitative results are summarized in Tab. 1. It is evident that our model achieves superior performance on the majority of quality evaluation metrics. For CLIP-Aesthetic, we note that this metric sometimes favor smooth outputs, which may not always align with the detailed and 7 Preprint Paper Ours WonderWorld LucidDreamer WonderJourney Figure 6: 3D scene generation results of different methods on WorldScore benchmark. Method WonderJourney LucidDreamer WonderWorld Ours 3D Consistency Photometric Consistency Object Control Content Alignment Style Consistency Subjective Quality Average 80.60 90.37 86.91 85.87 79.03 90.20 85.56 86.72 34.81 43.48 52.09 49.61 38.37 59.41 56.82 53.96 67.52 66.41 75.92 81.52 61.49 48.02 41.28 54. 60.30 66.32 66.43 68.72 Time Cost 6 min 6 min 10 sec 9 sec Table 2: Quantitative comparison on WorldScore benchmark. Note that the time cost of the baselines is tested on 1 H100 GPU, while our time cost is tested on 1 H20 GPU. realistic results produced by our method. Our method also attains the highest CLIP Score for two subsets, demonstrating the strong text alignment ability of our method. In addition, we report the average time required to generate single scene for each method on single H20 GPU. Our method demonstrates substantial speed advantage over other approaches. Remarkably, this efficiency is maintained even when our method produces results with higher resolution and greater number of frames. In addition, our approach leverages unified model that seamlessly handles both image-to-3D and text-to-3D tasks without requiring separate training processes. This unified framework not only simplifies the overall workflow but also substantially reduces the training cost. 4.3 COMPARISON ON WORLDSCORE BENCHMARK We further conduct comprehensive evaluation on the recent WorldScore (Duan et al., 2025) benchmark. The static subset of WorldScore comprises 2,000 test examples, encompassing diverse array of worlds with varying styles, scenarios, and objects. Each test case provides an input image, text prompt, and camera trajectory as conditions for generation. The evaluation protocol is designed to assess two primary aspects of world generation: controllability and quality. For baselines, we select three state-of-the-art 3D generation methods: WonderJourney (Yu et al., 2024), which iteratively completes novel-view images and depth maps based on point clouds; LucidDreamer (Chung et al., 2023), which also performs iterative novel view completion but utilizes 3DGS for rendering; and WonderWorld (Yu et al., 2025), which improves generation quality through the use of layered Gaussian surfels. Since our comparison focuses exclusively on 3D generation methods, the Camera Control metric primarily reflects the robustness of the evaluation protocol for each method, and is thus less informative in this context. Accordingly, we omit this metric from our comparison. Additionally, the original WorldScore benchmark evaluates most metrics only on anchor frames, which is suboptimal for 3D world generation tasks that require novel view synthesis. To ensure 8 Preprint Paper Q-Align IQA 3.11 2.61 3.46 4.12 3.98 4.12 D T3Bench-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 2.03 1.68 2.12 2.31 2.50 2.26 0.41 0.37 0.45 0.52 0.53 0.54 4.36 4.11 4.42 4.52 4.58 4. CLIP Score 25.34 22.92 26.95 27.59 27.04 27.68 Q-Align IQA DL3DV-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 2.64 2.71 2.99 4.02 3.89 3.96 2.09 1.96 2.05 2.35 2.35 2.27 0.39 0.40 0.42 0.51 0.50 0.50 4.60 4.54 4.57 4.80 4.82 4.77 CLIP Score 24.49 22.71 26.41 27.90 27.45 27. Q-Align IQA 2.48 2.74 3.06 3.90 3.66 3.76 WorldScore-200 CLIP IQA+ Q-Align IAA CLIP Aesthetic 2.10 2.16 2.18 2.71 2.56 2. 0.35 0.33 0.42 0.51 0.47 0.49 4.78 4.83 4.92 5.12 4.95 5.08 CLIP Score 27.40 26.11 28.71 29.09 28.76 29.13 Table 3: Quantitative ablation studies. The letters AF correspond to different model variants: (A) w/ MV-Diff, (B) w/ 3D-Diff, (C) w/ MV-Dist, (D) w/o CMC, (E) w/o OOD, and (F) Full model. w/ MV-Diff w/ 3D-Diff w/ MV-Dist w/o CMC w/o OOD Full model Figure 7: Qualitative ablation studies. Prompts: (Top) vintage clock hanging on brick wall; (Bottom) bright sunflower in field. fairer comparison, we re-evaluate these metrics by randomly sampled frames within specific intervals. Qualitative and quantitative comparisons are shown in Fig. 6 and Tab. 2, respectively. Our method achieves the highest average score and the fastest inference speed among all compared approaches. In particular, our model achieves the best Style Consistency and secures the second place in Photometric Consistency, Object Control, and Subjective Quality, reflecting well-balanced and robust capability across controllability and quality. While our method yields relatively lower scores in 3D Consistency and Content Alignment, these results can be attributed to methodological differences: for 3D Consistency, all baselines utilize monocular depth estimation models that are closely aligned with the evaluation protocol, whereas our approach relies solely on RGB supervision without explicit depth guidance; for Content Alignment, our method does not directly manipulate the anchor frame content, in contrast to the baselines. Qualitative analysis further reveals that baseline methods frequently exhibit unnatural transitions, discontinuous content, and visible holes in the generated scenes, which may not be fully reflected by the current metrics. Overall, our approach demonstrates superior consistency and faithful generation over existing methods. 4.4 ABLATION STUDY In Fig. 2, we show the generation results of various ablation models for image-to-3D scene generation. The outcomes align well with our expectations: both the MV-oriented diffusion model (w/ MV-Diff) and the MV-oriented distillation model (w/ MV-Dist) exhibit noisy 3D reconstruction due to multiview inconsistency, while the 3D-oriented diffusion model (w/ 3D-Diff) produces blurry visual results. To further validate the effectiveness of each proposed strategy, we conduct more comprehensive ablation studies on text-to-3D scene generation. Quantitative and qualitative results are summarized in Tab. 3 and Fig. 7, respectively. Consistently, the first three ablation models continue to demonstrate worse visual quality and weaker text alignment. The model without cross-mode consistency loss (w/o CMC) achieves competitive, and in some cases superior, scores on most quantitative metrics compared to our full model. However, qualitative analysis reveals that this model is susceptible to floating and duplicated artifacts. The model without out-of-distribution data (w/o OOD) is more prone to semantic misalignment (e.g., field) and exhibits drop in quantitative text alignment metrics. This issue is exacerbated on T3Bench and WorldScore, which differ in distribution from the original multi-view data, highlighting the importance of incorporating OOD data to improve generalization. 9 Preprint Paper"
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose an efficient yet powerful model for 3D scene generation, named FlashWorld. At the core of our approach is novel distillation strategy, which transfers high visual fidelity from multi-view-oriented diffusion model to 3D-oriented multi-view generative model endowed with perfect 3D consistency. To achieve this, we design dual-mode pre-training phase and cross-mode post-training phase, and introduce an out-of-distribution data co-training strategy to boost the models generalization. Our method achieves state-of-the-art performance on multiple tasks, while offering significant advantages in inference speed. The efficiency and effectiveness of our approach are well-positioned to advance applications of 3D scene generation. Future work includes incorporating autoregressive generation (Zhang et al., 2024d; 2025; Huang et al., 2025b; Chen et al., 2024) and extending our framework to dynamic 4D scene generation tasks (Wu et al., 2023; Zhang et al., 2024a)."
        },
        {
            "title": "REFERENCES",
            "content": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, and Gordon Wetzstein. Diffdreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 21392150, 2023. Yuanhao Cai, He Zhang, Kai Zhang, Yixun Liang, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, et al. Baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation. arXiv e-prints, pp. arXiv2411, 2024. Ata elen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, and Xi Wang. I-design: Personalized llm interior designer. In European Conference on Computer Vision, pp. 217234. Springer, 2024. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2283122840, 2025. Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. Wei Deng, Mengshi Qi, and Huadong Ma. Global-local tree search in vlms for 3d indoor scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 89758984, 2025. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36:3989739914, 2023. Ruiqi Gao, Aleksander Hołynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: create anything in 3d with multi-view diffusion models. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pp. 7546875494, 2024. 10 Preprint Paper Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, Soonwoo Kwon, and Changick Kim. Splatflow: Multi-view rectified flow model for 3d gaussian splatting synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2152421536, 2025a. Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, and Changick Kim. Videorfsplat: Direct scene-level text-to-3d gaussian splatting generation with flexible pose and multi-view joint modeling. arXiv preprint arXiv:2503.15855, 2025b. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. COMMUNICATIONS OF THE ACM, 63(11), 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong-Jin Liu. T3bench: Benchmarking current progress in text-to-3d generation. arXiv preprint arXiv:2310.02977, 2023. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: referencefree evaluation metric for image captioning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances In Neural Information Processing Systems (NeurIPS), 33:68406851, 2020. Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 79097920, 2023. Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025a. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025b. Team HunyuanWorld. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint, 2025. Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. arXiv preprint arXiv:2505.23716, 2025. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH), 42(4), July 2023. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Jiabao Lei, Jiapeng Tang, and Kui Jia. Rgbd2: Generative scene synthesis via incremental view inpainting using rgbd diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 84228434, 2023. Xinyang Li, Zhangyu Lai, Linning Xu, Jianfei Guo, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Dual3d: Efficient and consistent text-to-3d generation with dual-mode multi-view latent diffusion. arXiv preprint arXiv:2405.09874, 2024a. Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. Advances in neural information processing systems, 37:7512575151, 2024b. Preprint Paper Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 798810, 2025. Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025a. Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025b. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2216022169, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In 11th International Conference on Learning Representations, ICLR 2023, 2023. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, and Wenjun Mei. Wonderturbo: Generating interactive 3d world in 0.72 seconds. arXiv preprint arXiv:2504.02261, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024a. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024b. Christoph Schuhmann. Clip+ mlp aesthetic score predictor. [Online]. Available: https:// github.com/christophschuhmann/improved-aesthetic-predictor, 2022. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 12 Preprint Paper Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. In International Conference on 3D Vision 2025, 2025. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, pp. 3221132252, 2023. Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3d: Generating 3d scenes in seconds. arXiv preprint arXiv:2503.14445, 2025. Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint arxiv:2307.01097, 2023. Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generationreconstruction cycle. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 73207328, 2025. Matthew Wallingford, Anand Bhattad, Aditya Kusupati, Vivek Ramanujan, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, and Ali Farhadi. From an image to scene: Learning to imagine the world from million 360 videos. Advances in Neural Information Processing Systems, 37:1774317760, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 25552563, 2023. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In 35th Conference on Neural Information Processing Systems, pp. 2717127183. Curran Assoicates, Inc., 2021. Yiming Wang, Lucy Chai, Xuan Luo, Michael Niemeyer, Manuel Lagunas, Stephen Lombardi, Siyu Tang, and Tiancheng Sun. Splatvoxel: History-aware novel view streaming without temporal training. arXiv preprint arXiv:2503.14698, 2025. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. International Conference on Machine Learning (ICML), 2024. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. Wenming Wu, Lubin Fan, Ligang Liu, and Peter Wonka. Miqp-based layout design for building interiors. In Computer Graphics Forum, volume 37, pp. 511521. Wiley Online Library, 2018. Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2237822389, 2024. 13 Preprint Paper Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. Advances in Neural Information Processing Systems, 37:4507345104, 2024. Ken Xu, James Stewart, and Eugene Fiume. Constraint-based automatic placement for scene composition. In Graphics Interface, volume 2, pp. 2534, 2002. Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1037110381, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024b. Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, and Yiyi Liao. Prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 28572869, 2025. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1622716237, 2024c. Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024b. Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66586667, 2024. Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 59165926, 2025. Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony Chan, and Stanley Osher. Make it home: automatic optimization of furniture arrangement. ACM Trans. Graph., 30 (4):86, 2011. Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 91509161, 2023. Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. Advances in Neural Information Processing Systems, 37:1527215295, 2024a. 14 Preprint Paper Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):77497762, 2024b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. Songchun Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, and Changqing Zou. 3d-scenedreamer: Text-driven 3d-consistent scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1017010180, 2024c. Xuying Zhang, Yutong Liu, Yangguang Li, Renrui Zhang, Yufei Liu, Kai Wang, Wanli Ouyang, Zhiwei Xiong, Peng Gao, Qibin Hou, et al. Tar3d: Creating high-quality 3d assets via next-part prediction. arXiv preprint arXiv:2412.16919, 2024d. Xuying Zhang, Yupeng Zhou, Kai Wang, Yikai Wang, Zhen Li, Shaohui Jiao, Daquan Zhou, Qibin Hou, and Ming-Ming Cheng. Ar-1-to-3: Single image to consistent 3d object generation via next-view prediction. arXiv preprint arXiv:2503.12929, 2025. Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, and Lijuan Wang. Genxd: Generating any 3d and 4d scenes. In The Thirteenth International Conference on Learning Representations, 2025. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In ACM Transactions on Graphics (SIGGRAPH), 2018."
        },
        {
            "title": "A TRAINING DETAILS",
            "content": "Architecture configuration. Our dual-mode multi-view latent diffusion model is initialized with WAN2.2-5B-IT2V (Wan et al., 2025). For both pre-training and post-training, we adopt 24 key frames as input views. The spatial downsampling factor from image space to latent space is set to 16. The auxiliary multi-view feature has channel dimension of 1024. The discriminator head is simple CNN with several residual convolutional blocks (He et al., 2016). Pre-training configuration. We set the learning rates for both the transformer and 3DGS decoder to 2 106. We use weight decay of 1 106 and Adam (Kingma, 2014) optimizer parameters β1 = 0.9 and β2 = 0.95. The training schedule includes warm-up phase of 1,000 steps, followed by learning rate decay over 10,000 steps, with total of 20,000 training steps. The training takes around 3 days. Post-training configuration. During post-training, the timestep schedule for the few-step generator is set to {1000, 900, 750, 500}. For each generator update, the fake score network is updated 4 times. The learning rates are set to 1 106 for the generator and 5 107 for the discriminator, both with weight decay of 1 106. We use the Adam optimizer with β1 = 0.9 and β2 = 0.95. The training schedule consists of 1,000 step warm-up, followed by learning rate decay over 5,001 steps, and total of 10,000 training steps. The GAN loss weights for both the generator and discriminator are set to 5 103. The training takes around 2 days. The frequency of different tasks is controlled as follows: the probability ratio for training on MV-oriented mode, input views of 3D-oriented mode, and novel views of 3D-oriented mode tasks is 1:3:1. The ratio for sampling multi-view data versus out-of-distribution data is 2:1. We use bf16 precision for both training phases. The batch size is 64, using 64 NVIDIA H20 GPUs. For distributed training, we adopt the FSDP (Fully Sharded Data Parallel) strategy and activation checkpointing to improve training efficiency and memory utilization. The prediction of MV-oriented mode is actually v-prediction, following the original video diffusion model (Wan et al., 2025). We use the flow matching schedule (Lipman et al., 2023) for both training phases. Dataset configuration. For both pre-training and post-training, we utilize the following multi-view datasets: (1) MVImgNet (Yu et al., 2023): an object-centric dataset with resolution of 480704; (2) Preprint Paper RealEstate10K (Zhou et al., 2018): an indoor scene dataset with resolution of 704480 and frame stride [5, 6, 7, 8, 9, 10, 11, 12]; (3) DL3DV10K (Ling et al., 2024): general-purpose scene dataset with resolution of 704480 and frame stride [2, 3, 4]. For out-of-distribution data during post-training, we employ: (1) Arbitrary image and text data paired with RealEstate10K and WorldScore camera trajectories: general dataset with resolution of 704480. The images and texts are sampled from proprietary video dataset. (2) Echo4O (Ye et al., 2025) images with WildRGBD (Xia et al., 2024) camera trajectories: stylized, object-centric dataset with resolution of 480704."
        },
        {
            "title": "B RELATED WORKS",
            "content": "Iterative 3D scene generation. Recent advances in diffusion models (Rombach et al., 2022; Podell et al., 2023; Zhang et al., 2023) have enabled iterative generation of 3D scenes. DiffDreamer (Cai et al., 2023) improves multi-view consistency by conditioning on both past and future frames. SceneScape (Fridman et al., 2023), Text2Room (Hollein et al., 2023), and RGBD2 (Lei et al., 2023) refine mesh-based representations through depth-conditioned diffusion. WonderJourney (Yu et al., 2024) leverages point clouds with VLM-guided re-generation. Text2NeRF (Zhang et al., 2024b) and 3D-SceneDreamer (Zhang et al., 2024c) address error accumulation by utilizing NeRF (Mildenhall et al., 2020) representations. LucidDreamer (Chung et al., 2023), WonderWorld (Yu et al., 2025), RealmDreamer (Shriram et al., 2025), and WonderTurbo (Ni et al., 2025) accelerate generation and enhance fidelity using 3DGS (Kerbl et al., 2023). While iterative generation methods have made significant progress, they often suffer from cross-view semantic inconsistency. In contrast, data-driven approaches leverage rich cross-view priors to better maintain semantic coherence. Multi-view-oriented 3D scene generation. major class of data-driven methods adopts two-stage pipeline: generate multi-view images first, then reconstruct. CAT3D (Gao et al., 2024) synthesizes novel views via multi-view diffusion, followed by 3D reconstruction. DimensionX (Sun et al., 2024) generates temporally coherent videos, expands viewpoints through video diffusion, and reconstructs 3D scenes from frames. ODIN (Wallingford et al., 2024) produces trajectory-conditioned novel views for subsequent reconstruction. GenXD (Zhao et al., 2025) decouples multi-view and temporal features to jointly generate static and dynamic scenes. Bolt3D (Szymanowicz et al., 2025) outputs colored 3D Gaussians from images and point maps generated by multi-view diffusion. Prometheus (Yang et al., 2025) leverages the training paradigm of RGBD latent diffusion models. SplatFlow (Go et al., 2025a) jointly learns camera poses and multi-view image distributions from text. Wonderland (Liang et al., 2025) generates continues multi-view latents via video diffusion, then reconstructs scenes using latent-based reconstruction models. 3D-oriented 3D scene generation. Another line of work adopts 3D-oriented pipeline, employing rendering during denoising steps. DMV3D (Xu et al., 2023) introduces large reconstruction-based denoising model based on triplane NeRF representation, performing denoising through NeRF-based reconstruction and rendering. Dual3D (Li et al., 2024a) proposes dual-mode multi-view latent diffusion model based on pre-trained image diffusion models and neural surface rendering to reduce training and rendering costs. Director3D (Li et al., 2024b) synthesizes pixel-aligned 3D Gaussians directly from latent space using trajectory-conditioned multi-view diffusion, followed by SDS++ refinement. DiffusionGS (Cai et al., 2024) presents diffusion model that outputs pixel-aligned 3DGS at each timestep to ensure 3D consistency. Cycle3D (Tang et al., 2025) proposes unified generation-reconstruction framework, where the 3D reconstruction module is integrated into the multi-step denoising process to further guarantee 3D consistency. Distillation for diffusion models. Distillation techniques for diffusion models focus on transferring knowledge from pretrained teacher model to more compact and efficient student model. Denoising Student (Luhman & Luhman, 2021) achieves this by training single-step generator to minimize the RMSE between the outputs of the teacher and student models. Consistency Model (Song et al., 2023) enables trajectory distillation, allowing the student to mimic the teachers denoising process across multiple steps. Adversarial Diffusion Distillation (ADD) (Sauer et al., 2024b), Latent Adversarial Diffusion Distillation (LADD) (Sauer et al., 2024a), Adversarial Post-Training (APT) (Lin et al., 2025a), and Autoregressive Adversarial Post-Training (AAPT) (Lin et al., 2025b) further enhance distillation by introducing adversarial objectives to improve the student performance. Distribution Matching Distillation (DMD) (Yin et al., 2024b) formulates the distillation objective as optimizing 16 Preprint Paper the reverse KL-divergence between the student and teacher distributions. DMD2 (Yin et al., 2024a) extends this framework by incorporating GAN-based objective and supporting for multi-step generators, further improving the flexibility and effectiveness of the distillation process."
        },
        {
            "title": "C LIMITATIONS",
            "content": "While the proposed FlashWorld demonstrates strong capabilities in generating high-fidelity and efficient 3D scenes, several limitations remain. First, despite increasing the number of views, the diversity and scale of generated scenes are still constrained by the coverage of existing datasets. Second, the model currently struggles with accurately generating fine-grained geometry, mirror reflections, and articulated objects. These issues may be alleviated by incorporating depth priors (Yang et al., 2024a;b; Chen et al., 2025) and more 3D-aware structural information (Jiang et al., 2025; Wang et al., 2025) to further enhance the quality of our pixel-aligned 3D Gaussians."
        },
        {
            "title": "D RGBD RENDERING RESULTS",
            "content": "While FlashWorld does not explicitly incorporate depth supervision, the 3DGS outputs inherently enable the export of depth maps. In this regard, we present several RGBD rendering results in Fig. 8. This serves to demonstrate that our model is capable of learning meaningful depth geometric information solely via image supervision."
        },
        {
            "title": "E MORE RESULTS",
            "content": "We provide more generation results in Fig. 9, including object-centric, indoor, outdoor, realistic, and stylized scenes, to demonstrate the strong and generalizable generation ability of our model. For video rendering results, please kindly refer to our project page. 17 18 Figure 8: RGBD rendering results. Figure 9: More generation results. All images are rendered with generated 3DGS."
        }
    ],
    "affiliations": [
        "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University",
        "Tencent",
        "Yes Lab, Fudan University"
    ]
}