{
    "paper_title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables",
    "authors": [
        "Jie Zhang",
        "Changzai Pan",
        "Kaiwen Wei",
        "Sishi Xiong",
        "Yu Zhao",
        "Xiangyu Li",
        "Jiaxin Peng",
        "Xiaoyan Gu",
        "Jian Yang",
        "Wenhan Chang",
        "Zhenhe Wu",
        "Jiang Zhong",
        "Shuangyong Song",
        "Yongxiang Li",
        "Xuelong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance."
        },
        {
            "title": "Start",
            "content": "T2R-bench: Benchmark for Generating Article-Level Reports from Real World Industrial Tables Jie Zhang1,*, Changzai Pan1,*, Kaiwen Wei2,*, Sishi Xiong1,*, Yu Zhao1, Xiangyu Li1, Jiaxin Peng1, Xiaoyan Gu 1, Jian Yang3, Wenhan Chang1, Zhenhe Wu3, Jiang Zhong2, Shuangyong Song1, Yongxiang Li1, Xuelong Li1, 1 Institute of Artificial Intelligence (TeleAI), China Telecom, 2 Chongqing University, 3 Beihang University 5 2 0 2 7 2 ] . [ 1 3 1 8 9 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-ofthe-art models like Deepseek-R1 only achieves performance with 62.71% overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance."
        },
        {
            "title": "Introduction",
            "content": "The rapid development of large language models (LLMs) has significantly advanced research progress in table reasoning (Lu et al., 2024). Traditional research has primarily focused on tasks such as table-to-text generation (Parikh et al., 2020), table question answering (Pasupat and Liang, 2015a; Hu et al., 2024b; Xiong et al., 2025a,b), fact verification (Chen et al., 2019), text2sql (Li et al., 2024c; Wu et al., 2025b,a) and table data analysis (Chen, 2023). Figure 1: The illustration of table-to-report task. The goal of this task is to analyze numerical data from table to generate comprehensive, coherent and accurate report, including descriptions, analysis and conclusions. However, the automation of report generation from tables is far more widely adopted in industrial applications, such as industrial data analysis systems (Ma et al., 2023), business intelligence (BI), table analysis tools and enterprise reporting tools. Notably, systematic research in this field remains largely unexplored and urgently requires further in-depth investigation. In addition, industrial tables commonly exhibit high complexity and diversity, creating significant gap between existing academic benchmarks and industrial demands, which poses new challenges for related research. In light of the above considerations, we propose * These authors contributed equally to this work. Corresponding author: xuelong_li@chinatelecom.cn Typical industrial table applications include Microsoft Power BI, SAP BusinessObjects, IBM Cognos, MicroStrategy, Smartbi, etc. table-to-report, novel task designed to convert structured tabular data into natural language reports, aiming to present data, trends, and insights to enhance table information flow efficiency (Shao and Li, 2025) as illustrated in Figure 1. As an emerging task, it still faces several key challenges: (1) Lack of high-quality benchmark: current table benchmarks, such as Text2Analysis (He et al., 2024), TableBench (Wu et al., 2024) and MiMoTable (Li et al., 2024b) primarily evaluate LLMs on table question answering tasks, each with distinct focus. However, these benchmarks are not designed to assess table-to-report task. Besides, the table datasets used in most benchmarks predominantly consist of open-source academic data, failing to fully capture the main features and types of industrial tabular data, such as multiple tables, complex structure, and extremely large-size tables. The data volume and scale remain significantly constrained for extremely large-size tables (Sui et al., 2024). (2) Lack of targeted evaluation criteria: existing criteria like BLEU (Papineni et al., 2002a) and ROUGE (Lin, 2004) designed for summarization tasks, are unsuitable for table-to-report task due to non-unique gold standards. While general LLM-as-a-judge (Li et al., 2024a) method performs excel in text quality assessment,it neglects to evaluate numerical accuracy and table topic coverage, therefore limiting its applicability. To address the aforementioned issues, we introduce T2R-bench, high-quality benchmark designed to evaluate the reasoning capabilities of LLMs in the table-to-report task. T2R-bench encompasses Chinese and English tables from realworld industrial scenarios, covering 6 domains and 19 secondary industry categories. Compared to existing table benchmarks, as shown in Table 1, our benchmark features comprehensive and diverse collection of single tables, multiple tables, complex structured tables, and extremely large-size tables, enhancing the benchmarks practicality and challenge. We also craft the designed approaches for table question annotation and report reference annotation. Furthermore, we develop an evaluation system that incorporates three criteria of numerical accuracy, information coverage, and general quality to comprehensively assess report quality. In the experiment, we select 25 widely-used methods for evaluation, the results demonstrate that strongest models struggle to achieve satisfactory performance on the table-to-report task. Our contributions are summarized as follows: Task and Benchmark TableQA WikiSQL (Zhong et al., 2017) WTQ (Pasupat and Liang, 2015b) TAT-QA (Zhu et al., 2021) FeTaQA (Nan et al., 2021) AIT (Katsis et al., 2021) TabFact (Chen et al., 2020) TableBench (Wu et al., 2024) HiTab (Cheng et al., 2022) DataBench (Grijalba et al., 2024) Mimo (Li et al., 2024b) Spider (Yu et al., 2018) Table2Text ToTTo (Parikh et al., 2020) DAE-val (Hu et al., 2024a) DataTales (Yang et al., 2024b) Text2Analysis (He et al., 2024) Table2Report T2R-Bench (ours) Multiple Table Complex Structure Table Extremely Large-Size Table Answer Lengths 1.9 10.39 20.3 18.9 1.1 18.3 8.5 12.9 3.2 44.2 35.5 17.4 3.6 108.0 / 950.2 Table 1: Comparison with existing datasets on table types and answer lengths. Since Text2Analysis benchmark dose not provide the publicly accessible download links, the average length could not be calculated. We introduce T2R-bench, the first real world industrial benchmark for the table-to-report task. It encompasses 457 real-world tables across 19 domains, covering 4 industrially relevant types, including single tables, multiple tables, complex structured tables, and extremely large-size tables. We propose an evaluation system for table-toreport generation, incorporating 3 carefully designed criteria to assess report accuracy and reliability. Extensive validation demonstrates that the evaluation system achieves strong alignment with human judgment. We evaluate the ability of 25 strong methods on T2R-Bench. The experiments show that the best performed model Deepseek-R1 achieves only 62.71% overall score, which suggests great challenges in satisfying realworld table-based report generation needs."
        },
        {
            "title": "2 Related Work",
            "content": "Tabular Benchmarks. With the development of deep learning (Wei et al., 2021, 2023b,a; Xing et al., 2025; Wang et al.; Zhao et al., 2025; Wu et al., 2025c; Wang et al., 2024a, 2025a; Dai et al., 2025a,b), recent advances in table reasoning research have driven the development of diverse benchmarks covering TableQA, Table2Text, and advanced data analysis tasks, incorporating various table types including large-size tables, multiple tables, and complex structures. TableQA benchmarks (Zhong et al., 2017; Chen et al., 2020; Nan et al., 2021; Osés Grijalba et al., 2024) dominate the landscape, with TableBench (Wu et al., 2024) emerging as representative benchmark that captures real-world tabular reasoning challenges. For Table2Text tasks (Lebret et al., 2016), ToTTo (Parikh et al., 2020) constructs tabledescription pairs from Wikipedia snippets, while DATATALES (Yang et al., 2024b) generates financial narratives from tabular data. Advanced analysis benchmarks like DAEval (Hu et al., 2024a) and Text2Analysis (He et al., 2024) focus on programmatic table manipulation. However, as evidenced in Table 1, current solutions remain limited in their coverage of diverse table types (including largescale, multi-table, and complex layouts) and are constrained to sentence-level outputs that fail to meet industrial requirements for comprehensive report generation. Recent research has placed growing emphasis on complex table structure understanding (Cheng et al., 2022; Katsis et al., 2021; Tang et al., 2024; Mathur et al., 2024), yielding specialized benchmarks like MiMoTable (Li et al., 2024b) for multidimensional spreadsheets, DataBench (Grijalba et al., 2024) for containing limited number of large-size tables, and SPREADSHEETBENCH (Ma et al., 2024) for multiple tables manipulation. However, these works focused on TableQA and manipulation tasks, overlooking comprehensive report generation needs. Text quality Evaluation. Established metrics like ROUGE (Lin, 2004), BLEU (Papineni et al., 2002b), and BERTScore (Zhang et al., 2020) have been widely adopted, complemented by emerging LLM-as-judge approaches (Li et al., 2024a). For Table2Text tasks, Text2Analysis employs code generation metrics, while (Wiseman et al., 2017) designs three new dataset-adapted evaluation metrics for text generation. ToTTo (Li et al., 2024b) adapts PARENT (Dhingra et al., 2019) alongside BLEU. DATATALES introduces domain-specific criteria including factual accuracy, insightfulness, and stylistic quality, demonstrating the necessity for task-aligned evaluation frameworks. However, those methods typically neglect to evaluate numerical accuracy and table topic coverage (Szymanski et al., 2025), hindering the evaluation applicability."
        },
        {
            "title": "3 Construction of T2R-bench",
            "content": "Table-to-report is the task of automatically converting structured table into fluent article-level report R. To evaluate existing approaches, we introduce T2R-bench, whose construction pipeline consists of three key components: table data collection, table question annotation, and report reference annotation, as detailed in Figure 2."
        },
        {
            "title": "3.1 Table Data Collection",
            "content": "The tables of T2R-bench are collected from publicly available internet resources. The primary sources encompass municipal open data platforms, the National Bureau of Statistics, industrial association official websites and open-source tabular datasets (refer to Appendix B.7 for details). We collect tables to cover as many real-world scenarios as possible, including single table with individual and multiple sheets, multiple tables, extremely largesize tables, tables with simple and complex header structures. Specifically, we leverage two-stage selecting method. Firstly, tables are pre-screened based on industry-specific topics to ensure domain relevance. Subsequently, to ensure each table has sufficient information density for statistical analysis, we remove files with obvious garbled text or cell blank values exceeding 60%. To ensure the quality and legality of the collected tables, we manually review each table and anonymize any potential private and sensitive information. Ultimately, we collect 252 Chinese tables and 205 English tables across 6 distinct domains and 19 secondary classes based on topics to fit diverse industrial fields."
        },
        {
            "title": "3.2 Table Question Annotation",
            "content": "We adopt semi-automatic heuristic method to efficiently generate diverse and high-quality questions. The specific steps are shown as follows: Seed Question and Prompt Preparing. To improve the precision and relevance of the generated questions, we employ 24 annotators with expertise in data analysis and report writing in diverse domains (see Appendix B.1 for annotator qualifications). They carefully curate 10 seed questions, and meticulously design the prompt template library with 5 diverse prompt templates (prompt templates and seed questions are provided in Appendix B.4). Self-Instruct to Generate Questions. We employ self-instruct(Wang et al., 2023) by using GPT-4o to efficiently generate pool of questions. Two prompt templates are randomly selected from the prompt template library for each table. Each template incorporates 2-5 seed questions as in-context demonstrations, with instructions to generate 3 relFigure 2: An overview of the construction pipeline for T2R-bench. evant questions. Human Annotation and Filtering. We randomly assign each question to two annotators, whose selection criteria and qualifications are detailed in Appendix B.1. Annotators evaluate question candidates based on three criteria: 1) tabular answer ability, where questions must be answerable solely using table data without external knowledge; 2) focused conclusions, where questions should target single analytical dimensions for definitive conclusions; and 3) complementary uniqueness, where questions from the same table must address distinct aspects. In cases where the evaluation results of the two annotators are inconsistent, the results will be handed over to third senior annotator with extensive domain expertise and experience for the final judgment (For detailed annotation procedure, please refer to Appendix B.2). Through this rigorous quality assurance procedure, we obtain 910 high-quality, comprehensive questions."
        },
        {
            "title": "3.3 Report Reference Annotation",
            "content": "Unlike summarization tasks, which often yield single optimal summary, table-to-report tasks exhibit significant variability due to differences in expression, stylistic preferences, structural choices among annotators, and the inherent complexity of tabular data. Consequently, using entire reports as reference standards proves impractical. To this end, we observe that professionally authored reports on the same tabular content and report topic consistently share core elements, including central viewpoints, analytical conclusions, recommendations, critical supporting data, despite variations in phrasing or presentation. This consistency motivates our introduction of report keypoints: distilled representations of reports essential content, encompassing its analytical backbone and evidentiary support (See Figure 2 for keypoint examples). These invariant keypoints provide robust basis for evaluating generated reports. Based on this finding, we design the report reference annotation process, which consists: 1) Report Generation. We leverage three distinct LLMs to generate different reports for each <table, report question> pair, resulting in three distinct reports (see Appendix D.1 for the prompt template). 2) Keypoints Extraction. Then, we prompt GPT-4o to distill the most crucial information from each report, extracting 5-10 keypoints, resulting three groups of keypoints for each <table, question> pair (see Appendix B.5 for the prompt template). 3) Human Annotation. Mirroring the question annotation procedure, we implement rigorous dualannotator verification protocol for key point refinement, with discrepancies resolved by senior annotators with data analysis and domain-specific report writing experience. Please see Appendix B.1 and B.3 for full qualifications and annotation procedure details."
        },
        {
            "title": "3.4 Dataset Statistics",
            "content": "Through the construction process, T2R-bench comprises 910 high-quality questions originating from 457 unique tables, along with 4,320 annotated report keypoints. These meticulously annotated keypoints of the report will serve as the gold reference to evaluate report in Section 4.2. Table Statistics. Table 2 and Figure 3 show the key statistics and distribution of tables in T2R-bench. (b) (c) (e) (g) (a) (d) (f) Figure 3: Distribution of different types of tables in T2R-bench. (a) Domain distribution. (b) Proportion of Chinese and English tables. (c) Proportion of complex header tables. (d-e) The row and cell size distribution for all tables in T2R-bench. (f) Number of table files for single tables and multiple tables. (g) Distribution of report reference keypoints for each report question. Specifically, the global statistics reveal that T2Rbench contains over 8.3% extremely large-size tables containing more than 50K cells; 28.9% complex structured tables with hierarchical indexing, merged cells, and non-uniform cell structures; and 23.6% multiple tables comprising interdependent files or sheets. key feature distinguishing our benchmark is its substantial number of extremely large-size tables. Domain Distribution. As shown in Figure 3a, T2R-bench covers six main industry domains, which can be further divided into 19 more specific sub-domains, including engineering, manufacturing, finance, education, healthcare, telecommunications, transportation (detailed sub-categories refers to Table 9 of Appendix B.6), ensuring that abundant types of tables in the dataset encompass as many real-world scenarios as possible. Questions and Report Keypoints. Following the human annotation process, T2R-bench comprises total of 910 questions. Notably, the number of questions is pruned from an initial range of 3.00 to 1.99 per table during the expert annotation phase. As illustrated in Figure 2, the number of report keyProperty Number of Tables Avg Table Files or Sheets for Multi-Tables Avg Rows per Table Avg Cells per Table Number of Extremely Large-size Tables Avg Rows for Extremely Large-size Tables Avg Cells for Extremely Large-size Tables Number of Questions Avg Questions per Table Avg Report Reference Keypoints per Question Value 457 5.04 30,183 490,308 38 721,882 11,895,814 910 1.99 4.75 Table 2: Key Statistics of T2R-bench. points per question is reduced to an average of 4.75 after expert verification and filtering. These rigorous annotation and verification processes enhance the quality of the benchmark."
        },
        {
            "title": "4 Evaluation Criteria",
            "content": "To address the challenges encountered in automated evaluation for the table-to-report task, we propose comprehensive evaluation system from three aspects: numerical accuracy, information coverage, and general quality."
        },
        {
            "title": "4.1 Numerical Accuracy Criterion",
            "content": "Generated reports frequently incorporate numerical values, some directly extracted from source tables and others derived through data synthesis (e.g., aggregations like column averages). To ensure the fidelity of such numerical claims, we propose the Numerical Accuracy Criterion (NAC), self-consistency mechanism for validating numerical facts against their tabular sources. Specifically, we first segment sentences in the target report using NLTK (for English) and Jieba (for Chinese). We then apply regular expressions to extract clusters of sentences containing numerical statements (integers or floating-point numbers). For each cluster, we generate corresponding verification questions, treating the extracted numerical values as ground-truth answers (see Appendix C.1 for prompt). To resolve these questions robustly, we employ three specialized code-generation LLMs (i.e., Qwen2.5-32B-Coder-Instruct, Deepseek-Coder, and CodeLlama-70B-Instruct), capable of interpreting and executing numerical operations (see Appendix C.1 for details). NAC enforces consensus by requiring agreement from at least two models; discordant results (including execution failures) are discarded to minimize noise. The final NAC https://www.nltk.org/ https://github.com/fxsjy/jieba score is computed by systematically comparing the validated solutions against the original numerical assertions in each sentence cluster. 4."
        },
        {
            "title": "Information Coverage Criterion",
            "content": "To address the challenges of incomplete coverage and irrelevant content in LLM-generated reports, we propose the Information Coverage Criterion (ICC), quantitative measure of semantic alignment between generated reports and reference keypoints. Inspired by the successful application of mutual information (MI) in machine translation for evaluating alignment quality, ICC assesses how effectively report preserves essential information from the source table. Specifically, for each generated report, we define = {k1, k2, . . . , kM } as the set of annotated keypoints, where represents the total keypoint number. Then, the generated report is segmented into multiple sentence clusters = {s1, s2, . . . , sN } by NLTK toolkit (English reports) and Jieba toolkit (Chinese reports). After that, we construct semantic similarity matrix S, where each element Sij represents the semantic similarity of keypoints-sentence pair(ki, sj) calculated by BERTScore(Zhang et al., 2020): Sij = BERT Score(ki, sj) Given the similarity matrix S, the ICC is defined as normalized MI: (cid:80)M i=1 ICC = (cid:80)N j=1 (ki, sj) log (ki,sj ) (ki)P (sj ) i=1 (ki) log (ki) (cid:80)M (1) where the joint and marginal probabilities are derived from similarity matrix as follows: (ki, sj) = (ki) = (cid:80)M i=1 (cid:80)N (cid:80)M i=1 (cid:80)M S(ki, sj) (cid:80)N j=1 S(ki, sj) j=1 S(ki, sj) (cid:80)N j=1 S(ki, sj) i=1 S(ki, sj) (cid:80)N j=1 S(ki, sj) (sj) = (cid:80)M i=1 Eq. (1) provides an information-theoretic measure scaled to [0,1] by dividing the keypoint entropy H(K), enabling consistent comparison across reports with varying numbers of keypoints. The final evaluation aggregates ICC scores across all reports, with higher values indicating better preservation of critical information in the generated outputs."
        },
        {
            "title": "4.3 General Evaluation Criterion",
            "content": "Inspired by evaluation methodologies for longcontext generation (Lee et al., 2024), we propose the General Evaluation Criterion (GEC) to holistically assess report quality using LLMs as judges. GEC focuses on five key dimensions that most effectively discriminate report quality: reasoning depth, human-like style, practicality, content completeness and logical coherence. The final GEC score is computed as the average across these dimensions. Detailed evaluation criteria and prompts are provided in Appendix C.3."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Baselines and Evaluation. We evaluate 25 strong methods on T2R-Bench, including both opensource and closed-source foundation models. The open-source models comprise TableGPT2 (Su et al., 2024), Qwen series (Bai et al., 2023; Yang et al., 2024a; Qwen et al., 2025; Hui et al., 2024), Llama family (Dubey et al., 2024; Roziere et al., 2023), Mistral (Jiang et al., 2023), Deepseek models (DeepSeek-AI et al., 2024; Guo et al., 2024; DeepSeek-AI et al., 2025), and TeleChat (Wang et al., 2024b,c, 2025b), while the closed-source models include GPT series (OpenAI, 2023), OpenAI o1-mini, Claude-3.5-Sonnet2, Doubao, and Moonshot. The evaluation covers 4 practical industrial scenarios: single tables, multiple tables, complex structured tables, and extremely large-size tables. We assess all models using the proposed metrics: Numerical Accuracy Criterion (NAC), Information Coverage Criterion (ICC), and General Evaluation Criterion (GEC), and we report both overall and average performance scores. Implementation Details. We design uniform style prompt template to ensure the fairness of the evaluation. Input tables are in Markdown format, and the single LLM directly uses them for generation. For tables whose content exceeds the LLMs context length limit, the content will be truncated. For closed-source models, we utilize official APIs to generate complete reports, with detailed website information provided in Table 13 from Appendix D.4. For open-source models, we use 16 A100 40G GPUs for inference. All models use the official default parameters. The uniform style prompt template can be found in Appendix D.1. Model Open-Source Models TableGPT2-7B (Su et al., 2024) Qwen1.5-14B-Chat (Bai et al., 2023) Qwen2.5-72B-Instruct (Qwen et al., 2025) Qwen2-72B (Yang et al., 2024a) Qwen2.5-32B (Qwen et al., 2025) Qwen2.5-Coder-32B-Instruct (Hui et al., 2024) Qwen3-30B-A3B (Qwen et al., 2025) Qwen3-32B (Qwen et al., 2025) Qwen3-8B (Qwen et al., 2025) CodeLlama-70B-Instruct (Roziere et al., 2023) Deepseek-Chat-V3 (DeepSeek-AI et al., 2024) Deepseek-Coder (Guo et al., 2024) Deepseek-R1 (DeepSeek-AI et al., 2025) Llama3.1-70B (Dubey et al., 2024) Llama3.1-8B (Dubey et al., 2024) Llama3.3-70B (Dubey et al., 2024) Mistral-Large-Instruct-2407 (Jiang et al., 2023) Qwen2.5-7B-instruct (Qwen et al., 2025) Telechat2-35B (Wang et al., 2024b) Closed-Source Modelsa Moonshot-V1-32K Claude-3.5-Sonnet Doubao-Pro-128K Doubao-Pro-32K GPT-4o (OpenAI, 2023) OpenAI o1-mini Overall NAC ICC GEC AVG Single NAC ICC GEC Multiple NAC ICC GEC Complex Structure NAC ICC GEC Extremely Large-Size NAC ICC GEC 34.24 36.03 47.82 44.64 42.91 43.82 49.46 53.01 36.60 40.04 51.47 50.96 53.51 40.33 34.09 42.25 44.28 35.52 34.75 42.41 47.62 49.14 44.58 49.35 51.59 25.70 26.29 42.28 33.76 35.85 32.33 42.27 45.01 31.65 29.72 42.26 40.93 45.12 34.40 28.61 31.19 35.86 30.43 25. 36.05 36.31 31.28 31.47 41.91 41.19 81.28 83.83 88.68 87.76 84.54 86.25 88.02 89.12 78.38 80.80 89.63 87.07 89.51 76.52 72.82 78.07 79.86 75.73 83.32 87.11 88.61 82.98 81.21 88.72 89.07 47.07 48.72 59.59 55.39 54.43 54.13 59.90 61.37 48.87 50.19 61.12 59.65 62.71 50.42 45.17 50.50 53.33 46.45 47.94 55.19 57.51 54.47 52.42 59.29 60.62 49.54 50.67 67.29 66.23 54.36 61.97 70.32 73.21 51.26 47.34 68.58 71.52 74.58 54.05 49.26 56.05 59.15 50.63 51. 60.25 62.18 65.01 61.83 73.35 69.41 35.91 46.62 58.23 50.36 44.45 46.36 56.46 59.34 42.36 36.82 59.64 55.51 60.64 52.36 40.36 49.26 51.36 41.63 40.81 46.55 44.36 29.07 34.18 54.91 53.36 83.09 82.61 87.82 88.55 79.45 86.18 90.35 91.21 77.27 85.64 90.18 87.45 90.18 81.82 72.18 82.32 86.23 76.84 84.24 84.36 88.43 82.91 79.64 87.82 88.36 40.99 38.31 54.15 46.96 50.03 44.57 54.35 58.24 40.56 50.93 55.64 56.32 57.64 43.56 38.84 46.56 53.26 39.62 44. 50.35 54.28 56.04 51.02 54.10 60.94 31.29 27.06 46.23 39.35 46.53 40.82 47.35 50.53 34.55 42.18 49.47 47.06 48.47 32.71 30.35 31.23 37.72 33.25 27.39 42.24 48.53 39.41 44.29 56.35 66.29 81.76 82.72 89.65 88.71 86.82 86.24 88.36 90.23 81.46 79.53 89.18 88.35 89.18 75.76 77.29 78.31 82.63 79.36 82.76 88.35 87.59 84.47 82.59 88.47 89.53 30.12 40.02 43.58 39.92 45.84 46.23 47.82 51.24 39.27 42.67 52.25 50.17 53.39 46.12 36.01 48.62 49.42 39.27 32. 39.72 47.64 50.57 48.80 42.27 47.98 27.40 25.67 47.40 29.93 40.21 28.53 45.65 48.82 35.13 34.07 39.31 46.53 50.32 40.32 33.53 31.13 43.12 34.45 25.43 40.20 41.13 43.80 37.53 49.53 35.27 79.87 85.17 90.42 88.53 88.64 86.13 87.02 88.53 76.54 80.81 89.42 88.21 89.07 77.25 67.33 78.23 78.25 74.36 84.87 87.20 89.60 84.40 83.37 89.32 90.17 16.33 15.12 26.18 25.47 21.41 22.52 25.36 29.35 15.34 19.22 29.43 25.83 28.43 17.57 12.25 18.57 15.27 13.25 10. 19.33 26.39 24.94 16.67 27.69 28.04 8.20 5.81 17.24 15.40 12.21 13.62 19.63 21.25 14.54 5.80 20.63 14.62 21.05 12.20 10.20 13.12 11.24 14.21 9.42 15.21 11.24 12.83 9.86 16.83 18.84 80.42 84.82 86.84 85.25 83.26 86.43 86.24 88.82 78.23 77.21 89.72 84.28 89.62 71.23 74.46 73.42 72.32 76.32 81.41 88.54 88.83 80.13 79.25 89.26 88.21 Table 3: Overall performance of LLMs on T2R-bench. For each criterion, the best result is marked in bold, and the second best result is underlined. casing exceptional numerical computation abilities and outperforming even the larger Qwen2.5-72BInstruct model. (3) While the GPT series maintains strong performance with an ICC score of 66.29% on multiple table tasks, we observe significant performance degradation across most models when transitioning from single to multiple table tasks, suggesting limitations in cross-table comprehension. (4) The benchmark proves particularly challenging for extremely large-size tables, where all models show substantially reduced performance across all evaluation criteria. The top-performing Deepseek-R1 achieves an average overall score of 62.71%, highlighting the considerable room for improvement in current approaches for comprehensive table understanding tasks. Analysis of Table Cell Count. We conduct experiment to investigate how the number of cells in input tables affects the performance. As shown in the Figure 4 , we can see that as table size increases, all evaluated LLMs exhibit sharp performance decline, particularly when processing extremely largesize tables. This finding provides the first empirical evidence in table-related benchmarks that current models face fundamental limitations in comprehending large-scale tabular data, mirroring known challenges in long-text understanding. Analysis of Bilingual Capability. We conduct the English and Chinese experiment on T2R-Bench, have them processed by the five LLMs for report generation, and subsequently assess using averaged Figure 4: The performance of different LLMs on NAC and ICC criteria across varying numbers of table cell. Model Languages Chinese English Qwen3-32B Qwen2.5-72B-Instruct Deepseek-R1 Llama3.3-70B GPT-4o 62.43 60.43 63.74 48.26 59. 60.07 58.56 61.45 53.24 60.48 Table 4: Performance of LLMs on bilingual tables. The indicators in the table are based on the average values of NAC, ICC, and GEC."
        },
        {
            "title": "5.2 Main Results",
            "content": "Overall Performance. As shown in the Table 3, we conduct comparative analysis of advanced LLMs on the proposed T2R-Bench. We could find: (1) The Deepseek series demonstrates superior performance across single table, multiple table, and complex table tasks, establishing its leading capability in Table-to-Report applications. (2) Notably, Qwen3-32B achieves the highest NAC score, showMarkdown Json Html Models Our Evaluation Criteria Human Evaluation Qwen2.5-72B-Instruct Deepseek-R1 OpenAI-o1-mini 59.59 62.71 60.62 55.82 58.12 59.43 54.91 60.02 59.67 Table 5: Average performance of NAC, ICC and GEC of three different models across markdown, json and html table input formats. score of the proposed automated evaluation criteria. The Table 4 shows that nearly all models exhibit similar performance in both languages, highlighting their consistent ability to generate bilingual reports. However, Llama-3.3-70Bs performance in generating Chinese reports lags significantly behind its English capabilities, indicating need for further fine-tuning. Analysis of Input Formatting. Table 5 demonstrates that among the three most representative table input formats (Markdown, HTML, and JSON), the Markdown format achieves the highest average performance, followed by HTML, while JSON exhibits the lowest performance."
        },
        {
            "title": "5.3 Human Evaluation",
            "content": "As table-to-report is newly formulated task, we establish human baseline for comparison. Given the substantial time commitment required for human report generation, we randomly select subset of 50 questions (denoted as Dval) from the dataset by stratified sampling, covering single tables, multiple tables, complex-structure tables, and extremely large-size tables. To mitigate confirmation bias, six independent expert annotators with substantial data analysis experience (and no prior involvement in dataset creation) were recruited to generate reference reports, ensuring unbiased evaluations. We conducted rigorous validation studies to assess the correlation between our proposed metrics and human evaluation. Another six independent annotators evaluated reports generated by five representative models (Qwen2.5-72B-Instruct, Llama3.3-70B, GPT-4o, DeepSeek-R1, Qwen332B-Instruct) alongside human-written reports on Dval. Evaluations followed criteria (NAC, ICC, GEC from Section 4), achieving excellent interrater reliability (Fleiss = 0.85 (Fleiss and Cohen, 1973)). As shown in Table 6, while systematically more stringent, those metrics demonstrated strong correlation with human judgments (Pearsons = 0.908 (Cohen et al., 2009)), validating the frameworks reliability despite absolute score differences. Qwen2.5-72B-Instruct Deepseek-R1 Llama3.3-70B GPT-4o Qwen3-32B-Instruct Human baseline 59.59 62.71 50.50 59.29 61.37 89.32 61.06 65.58 55.09 62.56 63.02 96.52 Table 6: consistency test of evaluation methods between the proposed evaluation criteria and human evaluation on average performance of NAC, ICC and GEC. Figure 5: An example illustrating an original table and its corresponding report generated by DeepSeek-R1, with critical error highlighting."
        },
        {
            "title": "5.4 Case Study",
            "content": "Our manual analysis of 50 randomly selected error cases from T2R-Bench reveals persistent challenges in LLMs table-to-report capabilities. As shown in Figure 5, even the top-performing Deepseek-R1 model exhibits critical failures when processing multiple tables, such as numerical hallucinations (e.g., incorrect summation of \"Tag Price\" in Table 1) and table selection errors (e.g., mistakenly referencing \"Gross Sales\" from Table 1 instead of Table 2). These errors, along with challenges posed by complex table structures, descriptive hallucinations, and variable misinterpretations, reveal fundamental reasoning limitations despite the models ability to generate superficially fluent, humanlike reports. Comprehensive case study and error analysis are provided in Appendices D.2 and D.3."
        },
        {
            "title": "6 Conclusion",
            "content": "To meet practical industrial requirements, we introduce the table-to-report task and present T2Rbench, which requires models to generate articlelevel reports from tabular data. T2R-bench comprises 457 real-world tables spanning 19 diverse domains, with coverage of 4 industrial table types. In addition, we develop an adapted framework to rigorously evaluate model performance and conduct experiments on 25 state-of-the-art LLMs. Experimental results demonstrate that the top-performing model, Deepseek-R1, achieves suboptimal performance, revealing great room for advancing LLMs capabilities in table-to-report generation."
        },
        {
            "title": "Acknowledgments",
            "content": "The work is supported by the National Natural Science Foundation of China (62176029, 62506050), China Postdoctoral Science Foundation Funded Project (2024M763867), Chongqing Higher Education Teaching Reform Research Project (No. 242009)."
        },
        {
            "title": "References",
            "content": "While our benchmark represents significant step forward, several challenges remain. The current best-performing open-source model (Deepseek-R1) achieves suboptimal performance, with both Numerical Accuracy (NAC) and Information Coverage (ICC) scores below 65% on the proposed evaluation framework. This performance gap highlights two critical needs: (1) the expansion of our benchmark dataset to cover more diverse table types and domains, and (2) the development of specialized models specifically designed for the table-to-report task. These limitations underscore the pressing demand for methodological innovations that can bridge the gap between current capabilities and real-world application requirements."
        },
        {
            "title": "Ethics Statement",
            "content": "In the construction and evaluation of the T2RBench, we rigorously adhered to established ethical guidelines for responsible AI research. Data Collection and Privacy. All datasets utilized in this study were sourced from publicly available repositories with potential private and sensitive information eliminated. Annotator Compensation and Instruction. Our annotation team comprises 24 annotators, with 12 native English speakers and 12 native Chinese speakers, selected from individuals with extensive experience in analyzing tabular data and demonstrated proficiency in writing analytical reports in relevant fields. We ensure fair compensation for all human annotators, paying each annotator compensation of $40 per day, with specialized experts receiving an additional 20% premium in recognition of their advanced skills. All annotation work is conducted voluntarily with informed consent, and participants were fully aware of the research objectives and data usage policies. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, and 29 others. 2023. Qwen technical report. CoRR, abs/2309.16609. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055. Wenhu Chen. 2023. Large language models are few(1)- shot table reasoners. Preprint, arXiv:2210.06710. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019. Tabfact: large-scale dataset for table-based fact verification. CoRR. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. Tabfact: large-scale dataset for table-based fact verification. Preprint, arXiv:1909.02164. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. 2022. Hitab: hierarchical table dataset for question answering and natural language generation. Preprint, arXiv:2108.06712. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. Noise reduction in speech processing, pages 14. Muzhi Dai, Shixuan Liu, Zhiyuan Zhao, Junyu Gao, Hao Sun, and Xuelong Li. 2025a. Secure tug-of-war (sectow): Iterative defense-attack training with reinforcement learning for multimodal model security. arXiv preprint arXiv:2507.22037. Muzhi Dai, Jiashuo Sun, Zhiyuan Zhao, Shixuan Liu, Rui Li, Junyu Gao, and Xuelong Li. 2025b. From captions to rewards (carevl): Leveraging large language model experts for enhanced reward modeling in large vision-language models. arXiv preprint arXiv:2503.06260. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 19 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 37 others. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, MingWei Chang, Dipanjan Das, and William W. Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. Preprint, arXiv:1906.01081. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 15 others. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Joseph Fleiss and Jacob Cohen. 1973. The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. Educational and psychological measurement, 33(3):613619. Jorge Osés Grijalba, Alfonso Urena Lopez, Eugenio Martínez-Cámara, and Jose Camacho-Collados. 2024. Question answering over tabular data with databench: large-scale empirical evaluation of llms. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1347113488. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Xinyi He, Mengyu Zhou, Xinrun Xu, Xiaojun Ma, Rui Ding, Lun Du, Yan Gao, Ran Jia, Xu Chen, Shi Han, Zejian Yuan, and Dongmei Zhang. 2024. Text2analysis: benchmark of table question answering with advanced data analysis and unclear queries. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18206 18215. AAAI Press. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024a. Infiagent-dabench: Evaluating agents on data analysis tasks. Preprint, arXiv:2401.05507. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024b. Infiagentdabench: Evaluating agents on data analysis tasks. CoRR, abs/2401.05507. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, and 5 others. 2024. Qwen2.5-coder technical report. Preprint, arXiv:2409.12186. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, and 1 others. 2023. Mistral 7b. CoRR. Yannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, and Soumen Chakrabarti. 2021. Ait-qa: Question answering dataset over complex tables in the airline industry. Preprint, arXiv:2106.12944. Remi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. Preprint, arXiv:1603.07771. Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, and Jaewoo Kang. 2024. ETHIC: evaluating large language models on long-context tasks with high information coverage. CoRR, abs/2410.16848. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. 2024a. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv: 2411.16594. Zheng Li, Yang Du, Mao Zheng, and Mingyang Song. 2024b. Mimotable: multi-scale spreadsheet benchmark with meta operations for table reasoning. Zhongqiu Li, Zhenhe Wu, Mengxiang Li, Zhongjiang He, Ruiyu Fang, Jie Zhang, Yu Zhao, Yongxiang Li, Zhoujun Li, and Shuangyong Song. 2024c. Scalable database-driven kgs can help text-to-sql. In Proceedings of the ISWC 2024 Posters, Demos and Industry Tracks: From Novel Ideas to Industrial Practice co-located with 23nd International Semantic Web Conference (ISWC 2024), Hanover, Maryland, USA, November 11-15, 2024, volume 3828 of CEUR Workshop Proceedings. CEUR-WS.org. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Weizheng Lu, Jiaming Zhang, Jing Zhang, and Yueguo Chen. 2024. Large language model for table processing: survey. CoRR, abs/2402.05121. Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, and Dongmei Zhang. 2023. InsightPilot: An LLMempowered automated data exploration system. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 346352. Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, and Jie Tang. 2024. Spreadsheetbench: Towards challenging real world spreadsheet manipulation. Preprint, arXiv:2406.14991. Puneet Mathur, Alexa Siu, Nedim Lipka, and Tong Sun. 2024. MATSA: Multi-agent table structure attribuIn Proceedings of the 2024 Conference on tion. Empirical Methods in Natural Language Processing: System Demonstrations, pages 250258, Miami, Florida, USA. Association for Computational Linguistics. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir Radev. 2021. Fetaqa: Free-form table question answering. Preprint, arXiv:2104.00369. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Jorge Osés Grijalba, L. Alfonso Ureña-López, Eugenio Martínez Cámara, and Jose Camacho-Collados. 2024. Question answering over tabular data with DataBench: large-scale empirical evaluation of the 2024 Joint InLLMs. ternational Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1347113488, Torino, Italia. ELRA and ICCL."
        },
        {
            "title": "In Proceedings of",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002a. Bleu: method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002b. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 11731186. Association for Computational Linguistics. Panupong Pasupat and Percy Liang. 2015a. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1470 1480. The Association for Computer Linguistics. Panupong Pasupat and Percy Liang. 2015b. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1470 1480, Beijing, China. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, and 1 others. 2023. Code llama: Open foundation models for code. CORR. Jiawei Shao and Xuelong Li. 2025. Ai flow at the network edge. IEEE Network. Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, and 14 others. 2024. Tablegpt2: large multimodal model with tabular data integration. Preprint, arXiv:2411.02059. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table meets LLM: can large language models understand structured table data? benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM 2024, Merida, Mexico, March 4-8, 2024, pages 645654. ACM. Annalisa Szymanski, Noah Ziems, Heather A. EicherMiller, Toby Jia-Jun Li, Meng Jiang, and Ronald A. Metoyer. 2025. Limitations of the llm-as-a-judge approach for evaluating LLM outputs in expert knowledge tasks. In Proceedings of the 30th International Conference on Intelligent User Interfaces, IUI 2025, Cagliari, Italy, March 24-27, 2025, pages 952966. ACM. Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2024. Struc-bench: Are large language models good at generating complex structured tabular data? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 1234, Mexico City, Mexico. Association for Computational Linguistics. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. 2024a. Boosting LLM agents with recursive contemplation for effective deception handling. In Findings of the Association for Computational Linguistics: ACL 2024, pages 99099953, Bangkok, Thailand. Association for Computational Linguistics. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025a. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Shiquan Wang, Ruiyu Fang, Mengxiang Li, Zhongjiang He, and Shuangyong Song. When less is more: Minimal prompts with lora for llm text detection. In The 14th CCF International Conference on Natural Language Processing and Chinese Computing. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, and 17 others. 2024b. Telechat technical report. Computing Research Repository. Zihan Wang, XinZhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Mengxiang Li, Zhongjiang He, Yongxiang Li, Luwen Pu, Huinan Xu, Chao Wang, and Shuangyong Song. 2024c. Telechat: An open-source billingual large language model. In Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10). Jiaxin Peng, Yuyao Huang, and 1 others. 2025b. Technical report of telechat2, telechat2. 5 and t1. arXiv preprint arXiv:2507.18013. Kaiwen Wei, Xian Sun, Zequn Zhang, Li Jin, Jingyuan Zhang, Jianwei Lv, and Zhi Guo. 2023a. Implicit event argument extraction with argument-argument IEEE Trans. Knowl. Data relational knowledge. Eng., 35(9):88658879. Kaiwen Wei, Xian Sun, Zequn Zhang, Jingyuan Zhang, Zhi Guo, and Li Jin. 2021. Trigger is not sufficient: Exploiting frame-aware knowledge for implicit event argument extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 46724682. Association for Computational Linguistics. Kaiwen Wei, Yiran Yang, Li Jin, Xian Sun, Zequn Zhang, Jingyuan Zhang, Xiao Li, Linhao Zhang, Jintao Liu, and Zhi Guo. 2023b. Guide the many-toone assignment: Open information extraction via iou-aware optimal transport. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4971 4984. Association for Computational Linguistics. Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document generation. Preprint, arXiv:1707.08052. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, and 1 others. 2024. Tablebench: comprehensive and complex benchmark for table question answering. arXiv preprint arXiv:2408.09174. Zhenhe Wu, Zhongqiu Li, Mengxiang Li, Jie Zhang, Zhongjiang He, Jian Yang, Yu Zhao, Ruiyu Fang, Yongxiang Li, Zhoujun Li, and Shuangyong Song. 2025a. MR-SQL: multi-level retrieval enhances inference for llm in text-to-sql. DASFAA. Zhenhe Wu, Zhongqiu Li, Jie Zhang, Zhongjiang He, Jian Yang, Yu Zhao, Ruiyu Fang, Bing Wang, Hongyan Xie, Shuangyong Song, and Zhoujun Li. 2025b. UCS-SQL: uniting content and structure for enhanced semantic bridging in text-to-sql. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 81568168. Association for Computational Linguistics. Zhenhe Wu, Jian Yang, Jiaheng Liu, Xianjie Wu, Changzai Pan, Jie Zhang, Yu Zhao, Shuangyong Song, Yongxiang Li, and Zhoujun Li. 2025c. Tabler1: Region-based reinforcement learning for table understanding. arXiv preprint arXiv:2505.12415. Zihan Wang, Xinzhang Liu, Yitong Yao, Chao Wang, Yu Zhao, Zhihao Yang, Wenmin Deng, Kaipeng Jia, Hongrui Xing, Xinzhang Liu, Zhuo Jiang, Zhihao Yang, Yitong Yao, Zihan Wang, Wenmin Deng, Chao Wang, Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. Preprint, arXiv:1709.00103. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and TatSeng Chua. 2021. TAT-QA: question answering benchmark on hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 32773287, Online. Association for Computational Linguistics. Shuangyong Song, Wang Yang, and 1 others. 2025. Llmsr@ xllm25: language model-based pipeline for structured reasoning data construction. In Proceedings of the 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM 2025), pages 342350. Sishi Xiong, Mengxiang Li, Dakai Wang, Yu Zhao, Jie Zhang, Changzai Pan, Haowei He, Xiangyu Li, Wenhan Chang, Zhongjiang He, and 1 others. 2025a. Teleai at semeval-2025 task 8: Advancing table reasoning framework with large language models. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 1828 1841. Sishi Xiong, Dakai Wang, Yu Zhao, Jie Zhang, Changzai Pan, Haowei He, Xiangyu Li, Wenhan Chang, Zhongjiang He, Shuangyong Song, and 1 others. 2025b. Tablereasoner: Advancing table reasoning framework with large language models. arXiv preprint arXiv:2507.08046. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 43 others. 2024a. Qwen2 technical report. CoRR, abs/2407.10671. Yajing Yang, Qian Liu, and Min-Yen Kan. 2024b. DataTales: benchmark for real-world intelligent data narration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1076410788, Miami, Florida, USA. Association for Computational Linguistics. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39113921, Brussels, Belgium. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Deji Zhao, Donghong Han, Jia Wu, Zhongjiang He, Bo Ning, Ye Yuan, Yongxiang Li, Chao Wang, and Shuangyong Song. 2025. Enhancing math reasoning ability of large language models via computation logic graphs. Knowledge-Based Systems, page 113905. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Examples of T2R-bench A.1 English Table Example with Report Generated by the Single LLM This subsection shows an example of <question, table, report keypoints> triple, with report generated through the single LLM method of Qwen2.572B-Instruct. The incorrect parts in the report have been highlighted in red. Question Formulate an insightful report entitled Handler Effect Evaluation, examining the efficiency of various handlers in managing stock entries and departures as recorded in January 2020s data. Table Serial Number Date Product Name ID Stock In Stock Out Real-time Inventory Handler 1 2 3 4 5 6 7 1/10/20 Name1 1/11/20 Name2 1/12/20 Name3 1/13/20 Name4 1/14/20 Name5 1/15/20 Name6 1/16/20 Name7 mg-022 mg-023 mg-024 mg-025 mg-026 mg-027 mg-028 Quantity 16 62 23 126 103 85 Supplier Supplier01 Supplier02 Supplier03 Supplier04 Supplier05 Supplier06 Supplier07 Quantity 8 51 15 85 55 27 38 Supplier Supplier01 Supplier02 Supplier03 Supplier04 Supplier05 Supplier06 Supplier07 8 11 8 41 48 58 16 Wang Jing Wang Jing Wang Jing Li Kaihua Li Kaihua Li Kaihua Li Kaihua Table 7: Inventory Management Table Report Reference Keypoints Keypoint 1: Wang Jings average incoming quantity is 33.67, and the average outgoing quantity is 24.67, indicating gradual increase in inventory, but the final inventory level remains stable. Keypoint 2: Li Kaihuas average incoming quantity is 92.0, and the average outgoing quantity is 51.25. Despite the significantly higher incoming quantity compared to the outgoing quantity, the actual inventory level has decreased in practice. Keypoint 3: Wang Jings inventory management is more volatile, with an average real-time inventory of 9.0, indicating lower and relatively stable inventory level. Keypoint 4: Li Kaihuas inventory activities are more active, with an average real-time inventory of 40.75, indicating higher inventory level. Keypoint 5: Wang Jings real-time inventory data for the earliest and latest dates processed is 8, indicating that the inventory level remained stable during the processing period with no significant changes. Keypoint 6: Li Kaihuas initial inventory was 41.0, and the final inventory decreased to 16.0, indicating reduction in actual inventory levels, possibly due to unrecorded outgoing activities or other factors. Keypoint 7: Wang Jings inventory management is relatively stable, but the inventory level is low, indicating more conservative management strategy. Keypoint 8: Li Kaihuas inventory management strategy is more active, with higher average incoming quantity, but the actual inventory level has decreased, indicating potentially more aggressive management strategy. Generated Report Title: Handler Effect Evaluation Macro Description This report aims to evaluate the efficiency of different handlers in managing inventory in and out during the period from January 10, 2020 to January 16, 2020. The data covers 7 inventory records involving 7 different products, and each record includes information such as date, product name, product ID, inbound quantity, outbound quantity, real-time inventory, and handler. By analyzing these data, we can understand the performance of different handlers in inventory management, thereby providing basis for optimizing the inventory management process. Overall Situation of Inbound and Outbound During the period from January 10, 2020 to January 16, 2020, total of 7 inventory operations were recorded, involving 7 different products. Overall, in these 7 operations, the total inbound quantity was 460 pieces, the total outbound quantity was 274 pieces, and the final real-time inventory was 186 pieces. Specifically, the inbound quantity on January 10 was 16 pieces, and the outbound quantity was 8 pieces; the inbound quantity on January 11 was 62 pieces, and the outbound quantity was 51 pieces; the inbound quantity on January 12 was 23 pieces, and the outbound quantity was 15 pieces; the inbound quantity on January 13 was 126 pieces, and the outbound quantity was 85 pieces; the inbound quantity on January 14 was 103 pieces, and the outbound quantity was 55 pieces; the inbound quantity on January 15 was 85 pieces, and the outbound quantity was 27 pieces; the inbound quantity on January 16 was 54 pieces, and the outbound quantity was 38 pieces. These data indicate that, overall, the inbound quantity is slightly higher than the outbound quantity, and the inventory has increased. Handler Efficiency Analysis In the 7 inventory operations, the handlers are mainly divided into two: Wang Jing and Li Kaihua. Wang Jing handled 3 operations, and Li Kaihua handled 4 operations. Specifically, in the 3 operations handled by Wang Jing, the total inbound quantity was 91 pieces, the total outbound quantity was 74 pieces, and the final real-time inventory was 17 pieces. In the 4 operations handled by Li Kaihua, the total the total inbound quantity was 369 pieces, outbound quantity was 200 pieces, and the final real-time inventory was 169 pieces. From these data, it can be seen that the inbound and outbound quantities handled by Li Kaihua are much higher than those handled by Wang Jing, and the final real-time inventory is also significantly higher than that of Wang Jing. This indicates that Li Kaihua is more efficient in handling large number of inventory operations. Specific Performance of Handlers Further analysis of the specific performance of Wang Jing and Li Kaihua reveals some interesting phenomena. In the 3 operations handled by Wang Jing, the inbound and outbound quantities of each operation are relatively small, and the real-time inventory changes after each operation are not significant. For example, the inbound the outbound quantity on January 10 was 16 pieces, quantity was 8 pieces, and the real-time inventory was 8 pieces; the inbound quantity on January 11 was 62 pieces, the outbound quantity was 51 pieces, and the real-time inventory was 11 pieces; the inbound quantity on January 12 was 23 pieces, the outbound quantity was 15 pieces, and the real-time inventory was 8 pieces. This indicates that Wang Jing performs stably when handling small-batch inventory operations, but has relatively weak ability to handle large-batch inventory operations. In contrast, in the 4 operations handled by Li Kaihua, the inbound and outbound quantities of each operation are relatively large, and the real-time inventory changes after each operation are obvious. For example, the inbound quantity on January 13 was 126 pieces, the outbound quantity was 85 pieces, and the real-time inventory was 41 pieces; the inbound quantity on January 14 was 103 pieces, the outbound quantity was 55 pieces, and the real-time inventory was 48 pieces; the inbound quantity on January 15 was 85 pieces, the outbound quantity was 27 pieces, and the real-time inventory was 58 pieces; the inbound quantity on January 16 was 54 pieces, the outbound quantity was 38 pieces, and the real-time inventory was 16 pieces. This indicates that Li Kaihua is more efficient in handling large-batch inventory operations and can complete tasks quickly and accurately. Comparison of Handler Efficiency In order to more intuitively compare the efficiency of Wang Jing and Li Kaihua, we can calculate the processing time of each operation. Assuming that the processing time of each operation is the same, the average inbound quantity of Wang Jing in handling 3 operations is 30.33 pieces, and the average outbound quantity is 24.67 pieces; the average inbound quantity of Li Kaihua in handling 4 operations is 92.25 pieces, and the average outbound quantity is 50 pieces. From these data, it can be seen that the average inbound and outbound quantities of Li Kaihua in each operation are significantly higher than those of Wang Jing, which further confirms the efficiency of Li Kaihua in handling large-batch inventory operations. In addition, we can also calculate the real-time inventory change after each operation. In the 3 operations handled by Wang Jing, the real-time inventory changes after each operation are 8 pieces, -41 pieces, and -7 pieces, respectively, with an average change of -13.33 pieces; in the 4 operations handled by Li Kaihua, the real-time inventory changes after each operation are 41 pieces, -7 pieces, 31 pieces, and -22 pieces, respectively, with an average change of 10.75 pieces. This indicates that Li Kaihua can better maintain the stability of inventory after each operation, while Wang Jing, although performing stably when handling small-batch inventory operations, may cause greater inventory fluctuations when handling large-batch inventory operations. Summary and Recommendations In summary, through the analysis of inventory management data from January 10, 2020 to January 16, 2020, we can draw the following conclusions: Li Kaihua is more efficient in handling large-batch inventory operations, can complete tasks quickly and accurately, and can better maintain the stability of inventory after each operation. In contrast, Wang Jing performs stably when handling small-batch inventory operations, but may cause greater inventory fluctuations when handling large-batch inventory operations. Based on the above analysis, we make the following recommendations: 1. **Optimize personnel allocation**: When handling large-batch inventory operations, prioritize Li Kaihua for operation to improve overall efficiency and inventory stability. For small-batch inventory operations, Wang Jing can continue to be responsible to maintain the stability and accuracy of the operation. 2. **Training and improvement**: In view of the shortcomings of Wang Jing in handling large-batch inventory operations, special training can be arranged to improve her ability to handle large-batch inventory operations. At the same time, regular performance evaluations can also be used to motivate handlers to continuously improve their skills. 3. **Process optimization**: In the inventory management process, more automated tools and technologies can be introduced to reduce the error of manual operations and improve overall efficiency. For example, an inventory management system can be used to automatically record and update inventory data to reduce the burden of manual operations. Through the above measures, the inventory management process can be further optimized, the overall efficiency and inventory stability can be improved, and better support can be provided for the operation of the enterprise. A.2 Chinese Table Example with Report Generated by the Single LLM This subsection shows an example of <question, table, report keypoints> triple, with report generated through the single LLM of Qwen2.5-72BInstruct. The incorrect parts in the report have been highlighted in red. Question 统计不同地区农村家庭卫生设施差异形成农村卫 生设备区域对比重点探讨水冲式厕所在各区域的 普及程度 Table 饮用水情况 饮用水情况 饮用水情况 饮用水情况 饮用水情况 单位% 单位% 中部地区 全国 获取饮用水困难住户的比重 10.3 9.6 使用管道水住户的比重 28.5 48.6 净化处理过的饮用水 8.8 23.1 深井水 53.4 41.8 浅井水 31.7 27.8 江河湖水 2.0 2.8 池塘水 1.6 1.4 雨水 1.1 1.4 其他水 1.4 1.7 单位% 东部地区 2.8 71.1 44.2 37.6 15.3 1.4 0.4 0.1 1. 单位% 西部地区 22.2 42.9 13.3 27.5 43.6 6.0 2.6 3.7 3.3 单位% 东北地区 1.3 41.3 15.0 75.9 9.0 0.1 0.0 0.0 0.0 Table 8: 农村生活条件与设备普及率统计 Report Reference Keypoints 关键点1全国农村家庭中仅有12.8%的家庭拥有 水 冲 式 厕 所 旱 厕 和 简 易 厕 所 或 无 厕 所 的 家 庭 占 比87.2% 关 键 点2 东 部 地 区 水 冲 式 厕 所 普 及 率 最 高 为26.0%而东北地区最低仅为1.3% 关键点3中部地区旱厕最为普遍占54.1% 关键点4西部地区农村家庭中简易厕所或无厕 所 的 比 例 最 高 达 到53.2% 水 冲 式 厕 所 比 例 仅 为6.2% Generated Report 农村卫生设备区域对比 宏观描述 本报告基于全国及各地区农村家庭卫生设施的数据 重点探讨了水冲式厕所在不同区域的普及程度数据 显示全国范围内水冲式厕所的普及率仅为12.8% 而不同地区的普及率差异显著东部地区的普及率 最 高 达 到26.0% 而 东 北 地 区 的 普 及 率 最 低 仅 为1.3%这一差异反映了各地区在卫生设施建设和管 理上的不同水平也揭示了农村卫生设施改善的迫切 需求 东部地区水冲式厕所普及率最高 东部地区的水冲式厕所普及率达到了26.0%远高于 全国平均水平这一高普及率的背后是东部地区相 对发达的经济条件和较高的居民生活水平东部地区 的农村家庭不仅在卫生设施上投入更多而且在其他 生活设施方面也表现出较高的普及率例如东部 地区的彩电普及率为97.5台/百户固定电话普及率 为68.2部/百户手机普及率为86.1部/百户电脑普及 率为4.8台/百户摩托车普及率为50.9辆/百户生活 用汽车普及率为5.1辆/百户这些数据表明东部地 区的农村家庭在基础设施和生活条件方面整体较好 为水冲式厕所的普及提供了良好的基础 此 外 东 部 地 区 的 饮 用 水 条 件 也 较 为 优 越 使 用 管 道 水 的 住 户 比 例 为71.1% 远 高 于 全 国 平 均 水 平 的48.6%净化处理过的饮用水的使用比例为44.2% 同样高于全国平均水平的23.1%这些数据进一步说 明东部地区的农村家庭在卫生和生活条件方面具有 明显优势为水冲式厕所的普及提供了有力支持 中部地区水冲式厕所普及率较低 中部地区的水冲式厕所普及率仅为6.0%远低于全 国平均水平这一低普及率反映了中部地区在卫生 设施建设和管理上的不足尽管中部地区的经济条 件相对较好但卫生设施的普及率仍然较低中部 地区的彩电普及率为85.1台/百户固定电话普及率 为45.9部/百户手机普及率为68.7部/百户电脑普及 率为0.9台/百户摩托车普及率为36辆/百户生活用 汽车普及率为2.6辆/百户这些数据表明中部地区 的农村家庭在基础设施和生活条件方面虽然有所改 善但仍存在较大差距 中部地区的饮用水条件也较为一般使用管道水的住 户比例为28.5%低于全国平均水平的48.6%净化处 理过的饮用水的使用比例为8.8%远低于全国平均水 平的23.1%这些数据说明中部地区的农村家庭在 卫生和生活条件方面仍需进一步提升特别是在水冲 式厕所的普及方面 西部地区水冲式厕所普及率略高于中部 西部地区的水冲式厕所普及率为6.2%略高于中部 地区的6.0%但仍低于全国平均水平西部地区的 经济条件相对较为落后基础设施建设相对滞后西 部地区的彩电普及率为74.5台/百户固定电话普及率 为35.1部/百户手机普及率为52.3部/百户电脑普及 率为0.6台/百户摩托车普及率为25.6辆/百户生活 用汽车普及率为2.2辆/百户这些数据表明西部地 区的农村家庭在基础设施和生活条件方面存在较大差 距特别是在卫生设施方面 西部地区的饮用水条件也较为落后使用管道水的住 户比例为42.9%低于全国平均水平的48.6%净化处 理过的饮用水的使用比例为13.3%略高于中部地区 的8.8%但仍低于全国平均水平的23.1%这些数据 说明西部地区的农村家庭在卫生和生活条件方面仍 需进一步改善特别是在水冲式厕所的普及方面 东北地区水冲式厕所普及率最低 东北地区的水冲式厕所普及率仅为1.3%是全国最 低的这一低普及率反映了东北地区在卫生设施建设 和管理上的严重不足尽管东北地区的经济条件相对 较好但卫生设施的普及率仍然较低东北地区的彩 电普及率为97.1台/百户固定电话普及率为64.4部/百 户手机普及率为63.7部/百户电脑普及率为1台/百 户摩托车普及率为34.3辆/百户生活用汽车普及率 为2.6辆/百户这些数据表明东北地区的农村家庭 在基础设施和生活条件方面虽然有所改善但仍存在 较大差距 东北地区的饮用水条件也较为一般使用管道水的住 户比例为41.3%略低于全国平均水平的48.6%净化 处理过的饮用水的使用比例为15.0%略高于全国平 均水平的23.1%这些数据说明东北地区的农村家 庭在卫生和生活条件方面仍需进一步提升特别是在 水冲式厕所的普及方面"
        },
        {
            "title": "Construction",
            "content": "B.1 Details for Annotation Team Composition We recruit total of 24 annotators in three batches, evenly split between native Chinese and English speakers. All annotators hold Masters degrees and have at least one year of experience in data analysis and report writing. The first group of 12 annotators focuses on dataset construction and annotation introduced in Section 3, including six senior specialists with domain-specific report writing experience across six distinct fields in the dataset. These senior members serve as quality control reviewers, conducting final verification of annotations to ensure accuracy and consistency throughout the dataset development process. The second group comprises six evaluators responsible for human evaluation of generated reports introduced in Section 5.3. This team receive comprehensive training through virtual meetings to establish unified evaluation criteria, enabling them to systematically annotate and score reports based on predefined quality metrics while maintaining inter-rater reliability. The third group contains six independent report writers who manually create reference reports serving as the human baseline introduced in Section 5.3. This isolated team operates without exposure to the dataset construction details or evaluation protocols, ensuring an objective performance baseline by preventing any potential information leakage that might influence their writing outputs. All annotators work eight hours day and earned wage of $40 per day on average, with specialized experts receiving an additional 20% premium. All annotators are trained through videos or online meetings provided with annotation guidelines that explains the data usage for academic research purposes. B.2 Details of Procedure for Question"
        },
        {
            "title": "Annotation",
            "content": "We randomly assign each question to two annotators, whose selection criteria and qualifications are detailed in Section B.1. Each annotator assesses the quality of question candidate based on the following aspects: a) scope compliance: the question must be answerable using tabular data, without requiring any extraneous domain knowledge. Temporal and spatial references must be strictly confined within the boundaries of the dataset. b) thematic focus: the question should concentrate on single analytical dimension to derive evidence-bound conclusions, rather than enabling the generation of multi-thematic reports across divergent analytical directions. c) conceptual distinctiveness: multiple questions derived from the same table must address non-overlapping thematic aspects with clearly differentiated analytical objectives. In cases where the evaluation results of the two annotators are inconsistent, the results will be handed over to third annotator for the final judgment. Through this rigorous quality assurance procedure, we obtained 910 high-quality, comprehensive questions. B.3 Details of Procedure for Keypoints"
        },
        {
            "title": "Annotation",
            "content": "Similarly to question annotation, each <table, question> pair and corresponding three groups of extracted report key points is assigned to two independent annotators for revision. However, more complicated than binary validity judgments in question annotation, key point annotation requires multidimensional modifications including summarization, deletion, insertion and polishing based on AI-generated report keypoints. The annotation of key points adheres to the following criteria: 1) Factual Accuracy: The keypoints must be derived from and accurately reflect the data presented in the tables. 2) Relevance: The keypoints must align with the question of the report generation. 3) Essentiality: The key points should encompass the core content necessary to address the reports objectives. 4) Consistency: The key points should be logically coherent, non-repetitive, and form cohesive narrative. The results of two annotators are assigned to the third annotator for justification. If the third annotator finds the two annotations to be consistent or very similar, they will make minor adjustments and approve it as the final core point. However, if the third annotator identifies significant discrepancies between the two annotations, the issue will be documented and discussed during the daily meeting to reach consensus with the other two annotators. B.4 Prompts Library and Seed Questions for"
        },
        {
            "title": "Question Generation",
            "content": "The five prompt templates in the prompt library for question generation are shown below: As an expert with extensive experience in data analysis and report writing, you are required to propose questions for generating reports from multiple different perspectives along with specific requirements, based on the table description uploaded. The questions must be detailed enough and ensure there is sufficient differentiation among the questions. ## Response Format: Question 1:... Question 2:... Question 3:... ## Input Table Descriptions: [TABLE DESCRIPTION] Please directly output the generated 3 questions, do not include any additional explanations or comments. As an expert in table structure comprehension and narrative synthesis, develop three questions that cover different investigative angles, such as ratio and share analysis, comparative growth rates, and anomaly flaggingdetailing the fields involved, the analytical approach. ## Response Format: Question 1:... Question 2:... Question 3:... ## Input Table Descriptions: [TABLE DESCRIPTION] Please directly output the generated 3 questions, do not include any additional explanations or comments. Drawing on your ability to unpick multidimensional tables and deliver actionable insights, craft three report questions that each emphasize unique focuslayered segmentation, extreme-value exploration, and temporal dynamicswhile clarifying which metrics to calculate, over what time or dimension range, and the expected outcome for managerial decision support. ## Response Format: Question 1:... Question 2:... Question 3:... ## Input Table Descriptions: [TABLE DESCRIPTION] Please directly output the generated 3 questions, do not include any additional explanations or comments. As specialist skilled in dissecting complex tables and translating data into clear narratives, design three probing questions that focus respectively on trend analysis, distributional characteristics, and comparative the benchmarking; for each, comparison group or baseline, the required data granularity, and the decision-making context. indicate the key metric, ## Response Format: Question 1:... Question 2:... Question 3:... ## Input Table Descriptions: [TABLE DESCRIPTION] Please directly output the generated 3 questions, do not include any additional explanations or comments. From the perspective of an analyst with deep expertise in interpreting tabular datasets and crafting concise reports, propose three questions that each target different analytical dimension, such as time series trends, category comparisons, or geographic breakdownswhile specifying the exact fields to use, the calculations or aggregations required. ## Response Format: Question 1:... Question 2:... Question 3:... ## Input Table Descriptions: [TABLE DESCRIPTION] Please directly output the generated 3 questions, do not include any additional explanations or comments. The 10 Seed Questions are shown below: trading volume and transaction amounts. Seed Question 2:Develop Q3 2008 Metals and Fuel Oil Market Dynamics Report, investigating annual trading value fluctuations and market trends for copper, aluminum, and zinc contracts on SHFE. Seed Question 3:Analyze September 2023 food and alcohol price variations across Chinas major cities, with particular focus on how grain and vegetable price movements impact composite indices. Seed Question 4:Prepare an in-depth report on Model Differentiation Analysis for Shenbei Avenue 4S Stores (July), evaluating sales performance and customer preference across vehicle models. Seed Question 5:Generate trend analysis report on township hospital bed utilization rates from 2014 to 2022, utilizing comprehensive tabular data. Seed Question 6:Conduct Historical Analysis of Healthcare Personnel Structure (2014-2023), tracking growth patterns across medical staff categories with special attention to licensed physicians and registered nurses. Seed Question 7:Complete Human Resource Efficiency in Small and Micro Enterprises, examining workforce allocation and revenue efficiency across industries using employment and income data. Seed Question 8:Produce Study on Melon Cultivation Structure Transformation (2014-2022), detailing area changes for watermelon, muskmelon, strawberry and related crops. Seed Question 9:Investigate productivity trends in petroleum and natural gas extraction, creating detailed change analysis report for August 2023 through March 2024. Seed Question 10:Compile report titled Seasonal Fluctuation Analysis of Beijing Secondary Housing Prices, conducting year-round data dissection with emphasis on seasonal influencing factors. B.5 Prompt for Report Keypoints Extraction As an expert with extensive experience in information extraction, you are required to summarize 5-10 report reference keypoints based on the report provide. ## Response Format: Keypoint 1:... Keypoint 2:... ## Reference Reports: [REPORTS] Please directly output the generated 5-10 report reference keypoints, do not include any additional explanations or comments. Seed Question 1:Produce report entitled Analysis of Stock Market Trading Trends in May 2006, providing comprehensive examination of monthly fluctuations in both B.6 Domain and Sub-domain of T2R-bench The 6 domains and 19 sub-domains in T2R-bench are shown in Table 9. Domains Sub-domains Engineering Science Environmental Stewardship Transportation Logistics Social Policy Administration Electronics and Automation Manufacturing; Chemical Engineering and Advanced Materials; Energy Production and Power Systems; Automotive Manufacturing and Mobility Solutions Environmental Protection; Agricultural Production and Forestry Management; Marine Resources and Fisheries Management As an expert in language logic analysis and data recognition, you need to transform the given sentence into question addressing the numerical parts. The questions should inquire about all numerical values appearing in the paragraph, clearly specifying the objects and criteria based on the context. Telecommunications and IT Infrastructure; Transportation Networks and Logistics Management ## Example: Education and Scientific Research; Government Administration and Public Sector Services; Healthcare Systems and Public Health; Demographics and Social Development Input: In July 2023, the Kangming Road 4S store sold 20 Accord, 116 Odyssey, 35 Vezel, 123 CR-V, 43 Lingpai, 163 Fit, and 39 Odyssey units. Consumer Lifestyle Retail Trade and E-commerce Platforms; Tourism and Hospitality Services; Food and Beverage Services; Business Management Financial Economics Economic Development and International Trade; Banking and Financial Services Table 9: The 6 domains and 19 sub-domains in T2Rbench B.7 Data Source of T2R-bench The sources of tabular data in T2R-bench are shown in Table 10. Sources Websites Open-source data platform Wolrd Bank Group https://datacatalog.worldbank.org/ National Bureau of Statistics of China https://www.stats.gov.cn/sj/ Kaggle https://www.kaggle.com/datasets China Association of Automobile Manufactures http://www.caam.org.cn/ Beijing Public Data Open Platform https://data.beijing.gov.cn/ The United States Governments Open Data Site China Securities Regulatory Commission Data Platform https://catalog.data.gov/dataset http://www.csrc.gov.cn/csrc/tjsj/index.shtml Shanghai Public Data Open Platform https://data.sh.gov.cn/view/data-resource/index.html CelesTrak Tabular dataset https://celestrak.org/ MiMoTable(Li et al., 2024b) https://github.com/jasonNLP/MiMoTable Table 10: The data sources of T2R-bench Tables"
        },
        {
            "title": "Criteria",
            "content": "C.1 Prompts for Numerical Accuracy"
        },
        {
            "title": "Criterion",
            "content": "This subsection introduce the details of evaluating numerical accuracy criterion. Firstly, given the report to be evaluate, we extract clusters of sentences with numerical values through using regular expressions. Secondly, we transfer the extracted sentence clusters with numerical statements to inversely generate questions which take these sentence clusters as answers, following the prompt below: Output: How many units of Accord, Odyssey, Vezel, CR-V, Lingpai, Fit, and Odyssey were sold by the Kangming Road 4S store in July 2023? Input Sentence: [SENTENCE] Please directly output the question, do not include any additional explanations or comments. Thirdly, we get the answer of each question by prompting three different LLMs coder versions (Qwen2.5-32B-Coder-Instruct, Deepseek-Coder and CodeLlama-70B-Instruct) to generate python code and extract relative data through Python programming, following the ideas of previous research proposed for Table QA task. If the code execution fails, it will not be included in the final score. The code generation prompt is shown below: You are data analysis assistant. Based on the users provided analysis question, analytical approach, and file path, generate an efficient and robust Python code snippet to read the file from the specified path and perform data extraction. ## Data Description (Input): [DATA] ## Specified File Path: [FILE PATH] ## User Query: [QUERY] ## Requirements: File Reading: Efficiently read data from the specified path according to the file format and size (supporting CSV, Excel, etc.) and load it into pandas DataFrame. For larger datasets, choose appropriate reading methods to ensure performance. Data Processing: 1. Process data based on the users requirements and analytical approach, including column selection, conditional filtering, group calculations, etc., ensuring the code results meet the users query. All computed results should retain two decimal places for precise representation. 2. Analyze the users query and the execution process of the Python code in the Chain-of-Thought (COT) manner. 3. Limit the number of keys in the output dictionary to within 10, avoiding tuple data types in the output. 4. Avoid using object types and numpy operations; ensure correct computation types during calculations. 5. Ensure all keys and values in the answer conform to dictionary format requirements, with keys being string types and values being strings or dictionaries, not lists or tuples, and convert types as necessary. 6. The generated code should be robust, including error handling and file format compatibility. It should strictly match column names mentioned in the users query, avoiding irrelevant or mismatched columns. 7. Return variable format: The final result should only include the answer variable in dictionary format, without any other outputs. ## Example: [PYTHON CODE EXAMPLES] ## Note: Ensure outputs are formatted compactly and effectively, allowing successful loading by Python scripts, and avoid explanatory content. After obtaining the three sets of answers from Qwen2.5-32B-Coder-Instruct, Deepseek-Coder, and CodeLlama-70B-Instruct, we apply majorityvoting mechanism to aggregate these outputs into the single most reliable result, using the prompt below: You are model evaluator who rigorously applies consistency based assessment principles. You excel at analyzing, synthesizing, and summarizing multiple large language model outputs, and under majority voting scheme to deriving the most coherent and reliable final answer. ## Task Data Description (Input): [ANSWER 1], [ANSWER 2], [ANSWER 3] ## Requirements: Answer Grouping: 1. Extract the core response from each models output, then categorize the models answers into groups of equivalent meaning. Ensure that stylistic or formatting differences (e.g., \"the RPN range is 24 to 24\" versus \"24.024.0\") do not lead to separate groupings when the semantic content is identical. 2. Accurately identify semantically consistent answers to prevent fragmentation due to superficial wording variations. 3. You may paraphrase or consolidate responses when summarizing each group. 4. Numeric equivalence rule: If two numeric answers round to the same value, consider them identical; adopt the value with the highest precision as the representative. Final Answer Determination: 1.Apply majority voting rule: select the answer supported by the greatest number of models as the final result. 2. If there is tie for highest votes, output \"Unable to Infer\" without subjective judgment. 3.If the selected answer is empty, \"nan\", \"0\", \"0.0\", an empty list or empty dictionary, or otherwise noninformative, also output \"Unable to Infer\". ## Response Format: The response must strictly follow JSON format, as shown below: { \"Answer Groups\": { \"Answer 1\": [\"LLMA\", \"LLMB\"] \"Answer 2\": [\"LLMC\", \"LLMD\"] ... }, \"Final Answer\": \"The consolidated answer, or Unable to Infer\" } ## Example: [TASK ANSWER EXAMPLES] ## Note: Ensure outputs are formatted compactly and effectively, fully understand the requirements. Finally, by comparing the derived answers with numerical statements within each sentence cluster, we obtain the final NAC score, using the prompt below: As an expert in fact verification and logical analysis, your task is to compare given factual statement against standard factual answer to determine if there is any contradiction in their numerical data. You should provide score between 0 and 1, where 1 indicates complete agreement and 0 indicates complete contradiction. ## Note: Focus solely on the numerical portions of the statements. Only output the final score without any intermediate steps or explanations. ## Response Format: { \"score\": 1.0, \"reason\": \"Reason for the assigned score\" } ## Factual Statement to Verify: [SENTENCE] ## Standard Factual Answer: [ANSWER] Please ensure that your response is strictly formatted as valid JSON object and can be directly parsed by json.load(). Do not include any additional characters, comments, or text outside of the JSON structure. C.2 Report Evaluation Aspects for General"
        },
        {
            "title": "Evaluation Criterion",
            "content": "Existing evaluation criteria for reports or long texts typically encompass multiple aspects, including relevance, logical coherence, clarity, human-like style, Aspect Description Reasoning Depth Does the report demonstrate deep and multi-layered reasoning behind its claims? Does the analysis go beyond surface-level observations to reveal underlying mechanisms or causes? Human-like Style Practicality Does the writing style of the report resemble natural human expression rather than overly structured or mechanical language generated by machines? Do you think it even slightly resembles machine-generated content, or human written content? Are the analyses and recommendations provided in the report practically feasible? Can they offer valuable references to readers? Does the report demonstrate profound industry insights? Content Completeness Does the report provide comprehensive overview of both current status and future opportunities? Are there areas where the reports depth of coverage is insufficient? Where could additional data or examples strengthen the reports coverage? Logical Coherence Is the report structured so that each point builds logically on the previous one? Are there any gaps in reasoning or sudden jumps between topics? Do all conclusions follow clearly from the evidence or analysis presented? Table 11: Evaluation Aspects for General Evaluation Criterion. innovation, and structural rationality, when using LLMs as judge(Bai et al., 2024; Zheng et al., 2023; Li et al., 2024a). However, some evaluation aspects, such as linguistic standardization and logical coherence, don not show significant difference across various methods. Therefore, we concentrate on those aspects that can effectively distinguish the quality of different reports, as shown in Table 11. C.3 Prompt for General Evaluation Criterion You are professional large-model evaluation expert specializing in assessing the quality of AI-generated reports. We will provide you with user instruction and the AI-generated report. Your task is to evaluate the AI-generated report according to the following evaluation criteria and scoring rules. ## Evaluation Aspects: [EVALUATION ASPECTS] ## Scoring Criteria: The score ranges from 0 to 10. The intermediate ranges are defined as follows: - 10 points: Fully meets requirements, outstanding performance, comprehensive content, no obvious defects. - 89 points: Strong performance, meets most requirements with only minor flaws, very close to perfect overall. - 67 points: Some shortcomings or areas that need improvement, yet still generally meets the requirements and provides valuable information or analysis. - 45 points: Noticeable flaws or omissions; certain requirements are not adequately addressed, negatively affecting overall quality. - 03 points: Poor quality; fails to satisfy core requirements. Contains serious errors, omissions, or logical confusion that prevent effective communication of information. ## Response Format: The response must strictly follow JSON format, as shown below: { \"Evaluation Aspect 1\": { \"Reason\": \"Explanation for this aspects rating\", \"Score\": <numeric score> }, ... \"Evaluation Aspect N\": { \"Reason\": \"Explanation for this aspects rat- \"Score\": <numeric score> ing\", } } ## Evaluation Steps: 1.Understand the User Question: Carefully read the users request. Identify each requirement and how they interrelate. 2.Analyze the AI-Generated Report: Thoroughly review the report to ensure you understand its content and topic. 3.Evaluate Each Dimension: Check the report against every dimension in the list. Your evaluation should be both strict and fair, to enable comparison across different models. 4.Assign Scores and Provide Explanations: Give each dimension score (010) and clearly state the reasons that justify your score. 5.Output the Final Evaluation: Present your results in JSON format, double-checking for any formatting or syntax errors. ## Example: [EXAMPLES] ## Input User Question: [QUESTION] ## Input AI-Generated Report: [REPORT] Please ensure that your response is strictly formatted as valid JSON object and can be directly parsed by json.load(). Do not include any additional explanations, comments, or extraneous characters outside of the JSON structure."
        },
        {
            "title": "Experiments",
            "content": "D.1 Prompt Template for Generating Report by LLMs As an expert with extensive experience in data analysis and report writing, Please generate comprehensive report based on the provided data and analysis perspectives, and follow these guidelines: ## Report Standards: Objectivity: Ensure that the analysis is grounded in the actual data provided by the user. Avoid subjective judgments and ensure accuracy. Precision: Each conclusion should be supported by data. Ensure numbers and results are accurate and based solely on the data provided. Logic: The structure of the analysis should be clear and logically connected from problem definition to conclusions and recommendations. Readability: Present the analysis in simple and straightforward manner, avoiding overly complex terminology for better understanding by non-specialists. Action-Oriented: Beyond just reviewing the data, provide specific suggestions or strategies to support decision-making. Variety and Pacing: Use varied language to maintain reader interest and enhance the professionalism and appeal of the analysis. ## Requirements: 1. The output report should be complete and wellstructured, with minimum length of 1000 words. 2. Ensure content is appropriately detailed, avoiding repetition and vague descriptions. 3. Adjust the analysis perspective if there are gaps or incomplete data, rather than mentioning \"insufficient data.\" 4. Follow the analysis standards closely, avoid directly applying template references, and tailor the content according to the actual data for proper derivation and summary. ## Input Question for generating report: [QUESTION] ## Input Table Data for generating report: [TABLE DATA] Please directly output include any additional explanations or comments. the generated report, do not D.2 Analysis of Detailed Case Study This subsection shows examples of <question, table, report keypoints, case study> combination to display detailed case study , with report generated through the single LLM of Deepseek-R1 (i.e., it is the best-performing model in our benchmark.). The incorrect parts in the report have been highlighted in red. Since the generated report is quite lengthy, the part of report has been omitted, and only the content that requires case analysis is displayed. Case Study of English Extremely Large-size Table. As indicated by the highlighted text in red in Figure 6, the number of countries in the sentence \"this report analyzes the distribution of income brackets across global regions using verified data from 102 countries\" is indeed 217. This may be due to truncation when inputting extremely large tabular data into LLM. Moreover, the second paragraph doesnt cover Keypoint 4 when analyzing high income concentration and be lack of correct supporting data. This directly reduces the ICC evaluation metric of the report. Case Study of Chinese Complex Structured Table. For the aforementioned complex structured table shown in Figure 7, which features complicated header and describes the comparison of sales between this year and last year from January to May, there have been numerous numerical hallucinations and incorrect conclusions. For example: \"Data shows that Guangzhou (including Zone 1 and Zone 2) had total sales of 5.788 million, Shenzhen 4.529 million, and Nanning 158,000. The overall discount rates exhibited higher in the south, lower in the north pattern: Nannings average discount rate was 0.77, Shenzhen 0.78, and Guangzhou 0.63.\" Here, Shenzhens total sales figure was taken from the \"January to May cumulative data\" rather than the summation of the first quarters, and the discount rates were also incorrect. These errors, along with challenges posed by complex table structures, descriptive hallucinations, and variable misinterpretations, reveal fundamental reasoning limitations. D.3 Error Analysis of Samples As described in the case study section, we conduct an error analysis by randomly selecting 50 samples (with each set of 10 samples representing the typical characteristics of specific table type). The primary error types identified are as follows: First, there are hallucination errors, which include numerical factual errors (such as incorrect numerical calculations or hallucinations of numbers from the table in the report), generation errors (such as generating content unrelated to the table, or producing incorrect or insufficiently supported conclusions or descriptions), and table structure understanding errors (e.g., column selection errors resulting from misinterpretation of table structures, Figure 6: Case study of English extremely large-size table Figure 7: Case study of Chinese complex structured table such as selecting wrong column names due to incorrect recognition of complex table headers and structures; cross-table selection errors where the model retrieves data from incorrect tables). Second, there is the issue of missing key information, where the generated reports do not fully cover the key points, directly leading to low ICC evaluation metric. Third, there are columns truncation errors when the table content exceeds the context window length (e.g., for an extremely large-size table, miscalculating columns mean value), which directly results in low NAC evaluation metric. The statistics of the sampling error analysis are shown in the Table 12. D.4 URLs of Closed-source Models D.5 Analysis of Input Formatting Table 14 demonstrates that among the three most representative table input formats (Markdown, HTML, and JSON), the Markdown format achieves"
        },
        {
            "title": "NAC",
            "content": "22%"
        },
        {
            "title": "Table Structure\nUnderstanding Errors",
            "content": "NAC, ICC"
        },
        {
            "title": "Truncation Errors",
            "content": "NAC, ICC NAC, ICC 16% 17% 20% 25% Table 12: Error type distribution Model URL Moonshot-V1-32k https://kimi.moonshot.cn Claude-3.5-Sonnet https://www.anthropic.comt Doubao-Pro-128k https://www.volcengine.com Doubao-Pro-32k https://www.volcengine.com GPT-4o https://openai.com OepnAI o1-mini https://openai.com Table 13: The URLs of closed-source models we used the highest average performance, followed by HTML, while JSON exhibits the lowest performance. Markdown JSON HTML Qwen2.5-72B-Instruct Deepseek-R1 OpenAI-o1-1217 59.59 62.71 62.76 55.82 58.12 59.43 54.91 60.02 59.67 Table 14: Average performance of NAC, ICC and GEC of three different models across different input formats."
        },
        {
            "title": "E Details for payment and GPU hours",
            "content": "We pay each annotator daily remuneration of $40. We paid total of $2500 for calling various LLMs API interfaces. We use 16 A100 40G GPUs for inference, which took total of 25 hours."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Chongqing University",
        "Institute of Artificial Intelligence (TeleAI), China Telecom"
    ]
}