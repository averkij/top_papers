{
    "paper_title": "ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding",
    "authors": [
        "Junyi Hu",
        "Tian Bai",
        "Fengyi Wu",
        "Wenyan Li",
        "Zhenming Peng",
        "Yi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient."
        },
        {
            "title": "Start",
            "content": "ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Junyi Hu 1 Tian Bai 2 3 Fengyi Wu 2 Wenyan Li 4 Zhenming Peng 2 Yi Zhang"
        },
        {
            "title": "Abstract",
            "content": "Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, theoretically grounded vision-language alignment framework built on principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attentionbased soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including Top-K multi-positive contrastive objective and Geometry-Aware Consistency Objective derived from Lagrangianconstrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 APr on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inferenceefficient. 6 2 0 2 0 3 ] . [ 1 6 6 6 2 2 . 1 0 6 2 : r 1. Introduction Large vision-language models (VLMs) enable powerful zero-shot transfer by aligning images and texts in shared embedding space (Radford et al., 2021; Li et al., 2023; Jia et al., 2021). Despite significant progress, precise spatial grounding, which involves localizing free-form textual 1Department of Automation, Tsinghua University, China 2University of Electronic Science and Technology of China, China 3Lingsu Lab, China 4University of Copenhagen, Denmark. Correspondence to: Yi Zhang <zhyi@mail.tsinghua.edu.cn>. Preprint. February 2, 2026. concepts within images, remains key challenge in dense prediction tasks such as open-vocabulary detection and segmentation (Kamath et al., 2021; Cai et al., 2022). Recent open-vocabulary methods (Liu et al., 2024; Cheng et al., 2024; Wang et al., 2025; Fu et al., 2025) alleviate vocabulary constraints but often struggle with complex linguistic phenomena, including negation, relations, and compositional descriptions, when fine-grained localization is required. Recent theoretical analysis reveals an inherent limitation of CLIP-style joint embeddings: collapsing prompt into single global representation cannot simultaneously encode attribute binding, spatial relations, and negation under cosine similarity (Kang et al., 2025b). This geometric bottleneck motivates token-level vision-language alignment, where informative tokens are selectively emphasized rather than uniformly aggregated. However, incorporating tokenlevel reasoning into dense grounding remains nontrivial due to weak supervision and optimization instability. We propose ExpAlign, an expectation-guided visionlanguage alignment framework for open-vocabulary grounding. At its core is the Expectation Alignment Head (EAH), which produces prompt-conditioned spatial alignment maps by aggregating token-wise similarities through soft expectation mechanism. By treating spatial locations as latent instances and textual tokens as competing hypotheses, EAH performs implicit token selection without instance-level annotations, admitting natural interpretation as attentionbased soft pooling in multiple instance learning (MIL) (Ilse et al., 2018). To further improve discriminability and spatial coherence, we introduce two auxiliary objectives. multi-positive InfoNCE loss enforces prompt-level semantic separation under weak supervision, while Geometry-Aware Consistency Objective (GACO) regularizes alignment maps by emphasizing relatively consistent regions within each ground-truth mask. Together, they stabilize optimization and support both positive and negative prompts. Experiments on LVIS (Gupta et al., 2019), ODinW (Li et al., 2022), and RefCOCO/+/g (Yu et al., 2016) show that ExpAlign delivers strong open-vocabulary detection and segmentation performance under similar pre-training ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding scale and model capacity to recent baselines. It achieves competitive or superior results on LVIS rare categories and ODinW subsets, while on RefCOCO/+/g it outperforms detection-focused methods such as YOLOE but trails specialized grounding models like Grounding DINO-T due to CLIPs limited spatial reasoning. dition, geometry-aware regularization has been explored in segmentation and structured prediction (Liang et al., 2021), but existing approaches typically rely on absolute geometric cues. In contrast, our geometry-aware consistency objective operates on relative instance statistics, encouraging coherent alignment without rigid spatial targets. 2. Related Work Sentence-level vision-language Alignment. visionlanguage pretraining methods such as CLIP (Radford et al., 2021) and BLIP (Li et al., 2023) align whole images with global text embeddings using contrastive objectives. These sentence-level alignment techniques have enabled strong zero-shot transfer for retrieval and classification, and have been adapted to open-vocabulary detection by using prompt embeddings as classifier proxies (Zhou et al., 2022). However, collapsing prompt into single vector can lose internal structure and limits fine-grained localization, motivating methods that exploit richer textual and visual interactions. Token-level and Phrase-level Alignment. To capture fine-grained semantics between language and vision, several works explicitly model interactions at the token or phrase level. For instance, GLIP and its variants unify localization and grounding by introducing region-word contrastive alignment and phrase grounding objectives that align phrases with corresponding image regions (Zhang et al., 2022), enabling the model to learn regiontoken correspondences beyond global text embeddings. Methods like X-VLM perform multi-grained visionlanguage pretraining that aligns text with visual concepts at varying granularities, leveraging patch-level or concept-level representations (Zeng et al., 2021). Works in temporal grounding also observe that treating all tokens uniformly under cross-modal attention fails to exploit word-level signals crucial for fine-grained alignment (Kang et al., 2025a). Unlike these approaches that rely on explicit cross-attention structures or phrase annotations, ExpAlign uses expectation-based aggregation to softly pool token similarities into spatial alignment maps, preserving token discriminability under weak supervision. Alignment Regularization and Objectives. Contrastive learning remains core tool for visionlanguage alignment, with InfoNCE-style objectives widely adopted for separating positive and negative pairs in multimodal settings (Oord et al., 2018; Li et al., 2023). Region-level contrastive losses have been proposed to improve localization quality (Zhang et al., 2022; Zhong et al., 2022), and dense alignment objectives have been incorporated into grounding frameworks to better capture spatial semantics. Our multi-positive InfoNCE adapts these ideas to multi-prompt supervision, focusing on the most informative regions. In adRL-Inspired Regularization in VLMs Several works leverage reinforcement learning (RL)-inspired techniques or loss functions as regularizers to improve robustness and generalization in vision-language models. For instance, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) introduces an efficient PPO variant that computes advantages via group-relative ranking, inspiring advantage-weighted alignment mechanisms. VARP (Singh et al., 2025) uses agent-regularized preferences in RL from VLM feedback to better align rewards and mitigate inaccuracies. PRLL (Zheng et al.) applies LLM-assisted policy regularization for reward shaping, enabling adaptation in unfamiliar environments. In VLM fine-tuning, RL4VLM (Zhai et al., 2024) directly optimizes VLMs with regularization for decisionmaking, while VLM-RL (Huang et al., 2025) incorporates contrastive language goals as regularized rewards in autonomous driving. These works illustrate the increasing adoption of RL-based regularization to enforce consistency and reduce overfitting in multimodal settings. ExpAlign advances visionlanguage grounding by combining soft token-level aggregation with principled regularization, balancing expressiveness and optimization stability. It situates itself between sentence-level and structured alignment methods by enabling fine-grained, supervisionefficient alignment without reliance on heavy cross-attention or explicit token annotations. 3. Method 3.1. Overview We study open-vocabulary grounding, where model aligns visual regions with flexible language prompts and produces region-level predictions for detection or segmentation. Given an image and set of textual prompts {Tk}K k=1, our goal is to compute prompt-conditioned spatial alignment maps that support robust localization under ambiguous and weak supervision. We propose ExpAlign, an expectation-guided visionlanguage alignment framework. As illustrated in Fig. 1, ExpAlign consists of three key components: (i) an Expectation Alignment Head (EAH) that performs soft prompt region alignment via token-level expectation, producing an Expectation Alignment Map (EAM); (ii) Consistency Regularization Module that enforces cross-scale coherence of alignment maps; and (iii) auxiliary objectives that im2 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Figure 1. Overview of the proposed ExpAlign framework. Top: the overall pipeline, where prompt-conditioned Expectation Alignment Maps (EAMs) are computed at multiple feature scales and injected into visual features for open-vocabulary grounding and segmentation. Bottom-left: the Expectation Alignment Head, which aggregates token-level vision-language similarities into prompt-specific spatial alignment maps via expectation-based token weighting. Bottom-right: the Consistency Regularization Module, which applies semantic and geometric constraints to regularize the alignment maps. Best viewed in color. pose semantic and geometric constraints on the alignment distribution. We then form token posterior distribution via softmax over non-pad tokens: This design enables implicit instance selection over both prompt tokens and spatial locations, while remaining fully differentiable and compatible with standard detection and segmentation heads. 3.2. Expectation Alignment Head For scale {3, 4, 5} the backbone produces feature map RCHsWs , and each prompt {1, , } (in imFs age {1, , B}) is represented by token embeddings Tb,p RLC. We compute the token-wise similarity at every spatial location (x, y): b,p(x, y, l) = Fs Ss b(x, y), Tb,p(l), = 1, . . . , L. (1) To estimate the global relevance of each token, we aggregate spatial evidence by average pooling: exp(cid:0) Ss exp(cid:0) Ss where τt is temperature parameter. πs b,p(l) = (cid:80) b,p(l)/τt (cid:1) b,p(l)/τt (cid:1) , (3) Finally, we compute the expectation alignment map (EAM) by marginalizing over tokens: Ss b,p(x, y) = (cid:88) l=1 b,p(l) Ss πs b,p(x, y, l). (4) The resulting map Ss b,p RHsWs is referred to as the Expectation Alignment Map (EAM) at scale s. This formulation performs implicit token selection by assigning higher weights to globally informative tokens, while suppressing noisy or irrelevant ones, and yields spatial alignment score suitable for downstream grounding. Ss b,p(l) = 1 HsWs Hs(cid:88) Ws(cid:88) x=1 y=1 Ss b,p(x, y, l). (2) Given the expectation alignment maps (EAMs) produced at multiple feature scales, we impose consistency constraints 3.3. Consistency Regularization Module 3 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding to stabilize vision-language alignment across scales while respecting their distinct computational and geometric properties. Specifically, EAMs from different scales are used in scale-aware manner: low-resolution maps are employed for efficient semantic alignment aggregation, whereas highresolution maps are preserved for geometry-sensitive consistency regularization."
        },
        {
            "title": "SEMANTIC CONSTRAINT",
            "content": "To aggregate semantic evidence across scales while maintaining efficiency, we first unify multi-scale expectation alignment maps (EAMs) at the coarsest resolution. Specifically, EAMs from all feature levels are progressively downsampled to the spatial resolution of the smallest scale (e.g., P5) and summed to obtain unified alignment map: Sdw b,p = (cid:16)"
        },
        {
            "title": "Down",
            "content": "(cid:16) (Down( S3 b,p) + S4 b,p)/2 (cid:17) + S5 b,p (cid:17) /2. (5) where Down() denotes resolution-aligned downsampling. For each image and prompt p, we select the top-1% highest responses from the unified map: (cid:16) Sdw Ib,p = TopK b,p , H3W3/ (6) (cid:17) . We then define the pooled prompt-level logit as the average alignment score over these selected locations: ℓb,p = 1 Ib,p (cid:88) iIb,p Sdw b,p (i). (7) Let Pb denote the set of positive prompts for image b. The multi-positive InfoNCE objective with temperature τ is formulated as Lsem = 1 (cid:88) (cid:88) b=1 pPb 1 Pb log exp(ℓb,p/τ ) p=1 exp(ℓb,p/τ ) (cid:80)P . (8) GEOMETRY CONSTRAINT We introduce Geometry-Aware Consistency Objective (GACO) to regularize the spatial structure of the energy field produced by the Consistency Regularization Module. Instead of enforcing absolute geometric targets, GACO shapes the energy landscape through relative, instance-wise consistency within the ground-truth region. we construct unified high-resolution Expectation Alignment Map by progressively aggregating EAMs from coarse to fine levels. Starting from the coarsest scale (P5), we apply top-down fusion strategy: b,p = (cid:0)Up (cid:0)(Up(S5 Sup b,p) + S4 where Up() denotes resolution-aligned upsampling. This aggregation preserves fine-grained geometry while incorporating multi-scale semantic evidence. b,p)/2(cid:1) + S3 (cid:1) /2. (9) b,p Given the aggregated alignment map, we define normalized distribution over promptpatch pairs by applying softmax over all prompts and spatial locations: Pb(p, i) = exp(cid:0) Sup b,p(i)(cid:1) exp(cid:0) Sup b,p(i)(cid:1) (cid:80) p=1 (cid:80) iΩ , (10) where indexes spatial locations at the P3 resolution. This distribution assigns higher probability mass to patches that are more strongly aligned with given prompt. Let Mb,p(i) {0, 1} denote the binary ground-truth mask associated with prompt p, resized to the P3 resolution, and define the positive region Mb,p = { Mb,p(i) = 1 }. We introduce bounded local alignment confidence Rb(p, i) = σ(cid:0) Sup b,p(i)(cid:1), (11) and compute its mean and standard deviation over the positive region: µb,p = 1 Mb,p (cid:88) iMb,p Rb(p, i), σb,p = (cid:118) (cid:117) (cid:117) (cid:116) 1 Mb,p (cid:88) (cid:0)Rb(p, i) µb,p (cid:1)2 + ε. iMb,p Based on these statistics, we define relative consistency score Ab,p(i) = clip (cid:18) Rb(p, i) µb,p σb,p (cid:19) , c, , (12) which measures how well each patch agrees with the dominant spatial structure of its corresponding ground-truth instance. Importantly, this score depends only on intrainstance relative statistics and does not impose absolute alignment targets. The final geometry-aware consistency loss is defined as Lgeo = 1 Mb,p (cid:80) (cid:80) (cid:88) (cid:88) (cid:88) Ab,p(i) logPb(p, i). b=1 p=1 iMb,p (13) This objective redistributes probability mass within each ground-truth region according to relative geometric consistency, encouraging spatially coherent alignment maps while remaining invariant to monotonic transformations of alignment scores. Rather than collapsing responses via pointwise regression, GACO sculpts the geometry of alignment maps and naturally supports implicit instance selection. Notably, under the Gibbs energy minimization framework with the three assumptions detailed in Appendix B, the two loss terms in this module are mathematically derived as principled regularizers for cross-modal alignment. The full variational derivation is provided in Appendix. 4 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding 3.4. Full Training Objective The proposed consistency regularization serves as auxiliary supervision during training, complementing the standard detection or segmentation objective. Specifically, the semantic consistency loss Lsem promotes instance-level prompt patch alignment, while the geometry-aware consistency loss Lgeo encourages spatially coherent responses within each ground-truth region. Let Ldet/seg denote the task-specific loss (including classification, regression, and mask prediction terms when applicable). The overall training objective is = Ldet/seg + λsem Lsem + λgeo Lgeo. Both consistency losses are used only during training and are discarded at inference time, preserving standard prediction behavior while improving vision-language alignment and spatial consistency. 3.5. Connection to Multiple Instance Learning Our Expectation Alignment Map (EAM) is mathematically equivalent to attention-based soft pooling in multiple instance learning (Ilse et al., 2018). Concretely, by treating each spatial location as an instance and each textual prompt as bag, the EAM implements soft-MIL pooling over token hypotheses, which grants permutation invariance and expressive pooling power. The multi-positive InfoNCE loss and the Geometry-Aware Consistency Objective (GACO) further enforce discriminative bag semantics and regularize instance relationships within positive regions. formal proof is provided in Appendix A. 4. Experiment 4.1. Implementation Details Model. ExpAlign is implemented as lightweight visionlanguage alignment module that can be seamlessly integrated into standard multi-scale detection and segmentation architectures. Unless otherwise specified, we adopt frozen DINOv3 (Simeoni et al., 2025) ConvNeXt-T image encoder as the visual backbone. Following the encoder, we employ the same YOLOv8 (Varghese & Sambath, 2024) FPN-style feature enhancement module to produce multi-scale feature maps. The detection and segmentation heads, along with their corresponding loss functions, strictly follow the standard YOLOv8 formulation without modification. Text prompts are encoded using frozen CLIP (Radford et al., 2021) ViT-L/14 text encoder, where we retain all token-level representations before the end-of-text (EOT) token, rather than collapsing the prompt into single global embedding. To map textual tokens into the same feature space as visual representations, we append lightweight Residual SwiGLU feed-forward network (SwiGLUFFN) (Shazeer, 2020) after the CLIP text encoder. The second linear layer of the SwiGLUFFN is initialized to zero, such that the module initially behaves as an identity mapping. This design stabilizes early training and ensures that token-level alignment is learned progressively without disrupting the pretrained CLIP geometry. The Expectation Alignment Head (EAH) is attached to each feature level and computes prompt-conditioned alignment maps via token-wise similarity aggregation. The Consistency Regularization Module operates solely on the resulting alignment maps and introduces no additional learnable parameters. Data. We adopt the same data protocol as Cheng et al. (2024) and train ExpAlign on combination of detection and grounding datasets. Specifically, we use Objects365 (Shao et al., 2019) for large-scale object detection and GoldG (Kamath et al., 2021), which aggregates GQA (Hudson & Manning, 2019) and Flickr30k (Plummer et al., 2015), for visionlanguage grounding. To avoid data leakage, all images overlapping with COCO (Lin et al., 2014) are excluded from the training set. Since pixel-level annotations are not available for most training images, we generate pseudo instance masks for segmentation by applying the SAM-2.1 model (Ravi et al., 2024) to ground-truth bounding boxes from the detection and grounding datasets. Training. All experiments employed the AdamW optimizer combined with cosine learning rate scheduler across two-stage training procedure. Both training and evaluation were carried out on dedicated machine featuring eight NVIDIA RTX Pro 6000 GPUs, each with 96 GB of memory. Both the image encoder and the text encoder remain frozen throughout training. In the first stage, the model is trained for 30 epochs using only the standard YOLOv8 detection and segmentation losses, with an initial learning rate lr0 = 0.002, final learning rate ratio lrf = 0.1, and warmup of 3 epochs. In the second stage, we enable the multipositive InfoNCE loss and the Geometry-Aware Consistency Objective (GACO), and continue training for another 20 epochs with reduced initial learning rate lr0 = 0.001, lrf = 0.2, and no warmup. The loss weights are fixed to λsem = 0.5 and λgeo = 1.0 across all experiments. The semantic contrastive loss is applied at the lowest feature resolution for efficiency, while GACO is computed on highresolution alignment maps to preserve spatial structure. 4.2. Zero-shot Detection and Segmentation Performance ExpAlign exhibits competitive zero-shot open-vocabulary detection performance under fair pre-training and inference conditions. We perform zero-shot evaluation on the val and minival splits of LVIS with fixed AP protocol. LVIS features 1203 classes with long-tail distribution, while ODinW spans 35 diverse real-world datasets, testing generalization to varied domains and vocabularies. 5 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Table 1. Zero-shot detection performance. Metrics on LVIS val (Gupta et al., 2019) and minival (Kamath et al., 2021) are fixed AP (Achal Dave et al., 2022). All models use an input resolution of 640640, except for those with Swin-Tiny as the backbone, which employ 8001333. For training data, OG indicates Objects365 (Shao et al., 2019) and GoldG (Kamath et al., 2021). RefC indicates RefCOCO/g/+ (Yu et al., 2016). Method Backbone Pre-train Data Swin-T Swin-T Swin-T LW-DETR-L ConvNeXt-B GLIP-T (Li et al., 2022) DetCLIP (Yao et al., 2022) GDINO-T (Liu et al., 2024) OVLW-DETR-L (Wang et al., 2024) OmDet-Turbo-B (Zhao et al., 2024) YOLO-Worldv2-L (Cheng et al., 2024) YOLOv8-L GDINO 1.5 Edge (Ren et al., 2024) YOLOE-8-L (Wang et al., 2025) ExpAlign (Ours) ExpAlign (Ours) EfficientViT-L1 Grounding-20M YOLOv8-L ConvNeXt-T ConvNeXt-T OG OG OG,RefC OG OG OG, Cap4M OG OG OG #Params - AP 232M 24.9 34.4 172M 27.4 47M 33.5 175M 34.7 35.4 48M 33.5 - 35.9 45M 37.2 60M 37.1 60M LVISminival APr APc APf 31.0 19.5 17.7 36.3 33.9 26.9 32.7 23.3 18.1 34.4 33.9 26.5 - - - 38.0 34.1 27.6 33.9 34.3 28.0 37.3 34.8 33.2 37.6 37.2 35.8 36.2 37.4 37. LVIS ODinW13 ODinW35 AP 16.5 - - - - 26.8 27.3 - 30.3 29.5 APr APc APf 26.1 11.6 7.5 - - - - - - - - - - - - 33.4 23.6 19.8 29.6 25.7 26.3 - - - 33.7 29.8 26.5 33.4 28.0 24.8 AP - - 49.7 - - 38.4 - - 48.0 47.7 AP - - 22.3 - - 17.1 - - 22.6 22. Table 2. Zero-shot instance segmentation performance on LVIS val set using standard mask APm. ExpAlign and YOLOE are evaluated purely zero-shot without any LVIS images or annotations during training. In contrast, YOLO-Worldv2-L is fine-tuned on LVIS-Base data for the segmentation head. Model YOLO-Worldv2-L OpenSeeD (Zhang et al., 2023) YOLOE-v8-L YOLOE-11-L ExpAlign ExpAlign ( + RefC) APm APm APm APm 19.8 21.0 23.5 22.6 29.9 29.8 17.2 - 21.9 19.3 29.0 29. 17.5 - 21.6 20.9 30.9 30.8 23.6 - 26.4 26.0 29.1 28.9 As shown in Table 1, ExpAlign with OG and RefC achieves 37.1 AP on LVIS minival and leads in rare-category performance with APr of 36.2. On full LVIS val, it reaches 29.5 AP and 24.8 APr, benefiting from RefCOCO referring expression supervision for improved long-tail handling. On ODinW, it attains 47.7 AP on ODinW13 and 22.4 AP on ODinW35, substantially outperforming GLIP-T and closely matching or exceeding Grounding DINO-T, using lightweight design with only 60M total parameters (26M frozen), which enables superior efficiency under comparable backbone scale. All models use 640640 input resolution except those with Swin-Tiny backbone, which use 8001333. These results demonstrate the effectiveness of referring expression data for enhancing rare-object detection and real-world robustness without relying on massive extra pre-training data. Furthermore, as shown in Table 2, ExpAlign achieves strong zero-shot instance segmentation on the LVIS val set using the standard APm metric. It attains 29.9 APm overall and 29.0 APm on rare categories without any exposure to LVIS images during training. This performance far surpasses YOLO-Worldv2-L fine-tuned on LVIS-Base at 19.8 APm and YOLOE variants ranging from 22.6 to 23.5 APm. The substantial improvement of 6 to 10 APm is largely attributed to the GACO regularization term introduced during pre-training, which significantly enhances mask precision Table 3. Downstream fine-tuning performance on COCO. ExpAlign is fine-tuned on the COCO train2017 set and evaluated on val2017 using standard bounding-box APb and mask APm metrics, including AP at IoU thresholds 0.50 and 0.75. We compare two practical strategies: linear probing with the backbone frozen for 10 epochs and full tuning with all parameters trainable for 80 epochs. Training-from-scratch baselines are included for reference. Epochs APb APb 50 APb 75 APm APm 50 APm 300 600 Model Training from scratch YOLOv8-L YOLO11-L Linear probing YOLOE-v8-L YOLOE-11-L ExpAlign (Ours) Full tuning YOLOE-v8-L YOLOE-11-L ExpAlign (Ours) 10 10 10 80 80 80 52.4 53.3 45.4 45.1 47. 53.0 52.6 53.5 69.3 70.1 63.3 62.8 65.3 69.8 69.7 70.7 57.2 58.2 50.0 49.5 51. 57.9 57.5 57.8 42.3 42.8 38.3 38.0 39.2 42.7 42.4 42.9 66.0 66.8 59.6 59.2 61. 66.5 66.2 67.0 44.9 45.5 40.8 40.6 41.8 45.6 45.2 45.5 and boundary alignment across long-tail categories in openvocabulary settings. 4.3. Downstream Transferring We evaluate ExpAligns downstream transferability on the COCO dataset through fine-tuning for object detection and instance segmentation, as shown in Table 3. Under linear probing (backbone frozen, 10 epochs), ExpAlign outperforms YOLOE-v8-L and YOLOE-11-L in both boundingbox APb and mask APm. In full tuning (80 epochs, all parameters trainable), ExpAlign further surpasses these baselines across most metrics, including APb 50, and APm 75. These consistent gains across both strategies highlight that ExpAligns pre-training design enables more efficient and effective adaptation to standard supervised tasks compared to recent open-vocabulary baselines. 50, APm 4.4. Referring Expression Comprehension Performance As shown in Table 4, ExpAlign underperforms significantly on referring expression comprehension tasks compared to 6 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Table 4. Performance on common referring expression comprehension datasets. The evaluation metric for RefCOCO, RefCOCO+, and RefCOCOg is the Top-1 accuracy. * indicates removed mosaic, flip, and HSV augmentations in phase-2 training. Method Pre-Train Data YOLOE-8-L GDINO-T GDINO-T ExpAlign ExpAlign* OG OG OG, RefC OG OG, RefC RefCOCO RefCOCO+ val 32.8 50.4 74.0 42.2 51.6 testA testB 45.7 43.7 43.2 57.2 59.3 74.9 46.7 48.2 47.7 59.3 val 28.5 51.4 66.8 37.62 48.9 testA testB 45.8 31.1 45.8 57.6 56.1 69.9 45.2 35.3 45.5 47.5 RefCOCOg test val 23.1 22.6 67.1 67.5 72.1 71.1 59.67 60.0 64.0 65. Table 5. Ablation study on token-level alignment versus pooled token representations on the LVIS dataset. Alignment Strategy Mean pooling over tokens Global pooled token (EOT) Token-level alignment (EAH) AP 31.9 34.4 37.1 APr 27.3 33.2 36.2 APc 30.5 34.5 37.1 APf 32.2 35.4 37.4 Grounding DINO-T, even when pre-trained with the same RefC data. For example, ExpAlign with RefC achieves only 51.6/59.3/47.7 on RefCOCO splits and 65.6/64.0 on RefCOCOg, far below Grounding DINO-Ts 74.0/74.9/59.3 and 71.1/72.1. We acknowledge this limitation openly. The primary cause is likely the CLIP text encoders inherent weakness in understanding positional and relational language (e.g., left of, behind, next to), which is crucial for many referring expressions, especially on RefCOCO+ and RefCOCOg. In contrast, Grounding DINO benefits from more specialized text encoder and fusion mechanism better suited for spatial reasoning. This highlights key area for future improvement in ExpAligns design. 4.5. Ablation Study We further provide extensive analyses for the effectiveness of designs in our ExpAlign. Experiments are conducted on fixed AP (Achal Dave et al., 2022) is reported on LVIS minival splits set for zero-shot evaluation, by default. As shown in Table 5, the token-level alignment strategy (EAH) significantly outperforms simpler representations. Compared to mean pooling and global pooled token (EOT), EAH improves 5.2 and 2.7 absolute AP points, respectively. More specifically, on rare categories, applying EAH reaches 36.2 APr, achieving the largest gain of 8.9 APr points compared to mean pooling. This demonstrates that explicit alignment at the token level captures finer-grained crossmodal correspondence, leading to better generalization on long-tail distributions. Table 6 ablates the loss weights for semantic contrastive loss (λsem) and geometry-aware consistency (λgeo). Using λsem = 0.5 alone reaches 37.0 AP and strong rare performance (35.8 APr). Adding λgeo (especially at 0.5 or 1.0) consistently improves overall AP and frequent/common categories, with the best results at λsem = 0.5 + λgeo = 0.5 Table 6. Ablation study on the loss weights λsem (semantic contrastive loss) and λgeo (geometry-aware consistency objective) on the LVIS dataset. λsem λgeo 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.0 0. 0.5 0.5 0.5 1.0 1.0 1.0 1.5 1.5 1.5 AP 35.0 37.0 36.1 37.6 37.8 36. 37.1 37.2 36.9 36.6 36.7 35.1 APr 32.0 35.8 30.2 31.7 32.1 29.2 35.6 35.9 32. 34.5 35.0 31.9 APc 35.8 35.9 33.1 37.7 37.6 26.1 37.6 37.2 36.3 36.2 36.6 35. APf 35.1 38.1 36.7 38.6 38.3 37.6 37.2 37.6 37.7 37.4 37.2 36.0 Table 7. Backbone comparison on the LVIS dataset. All settings use the same detection and segmentation heads. Backbone YOLOv8 DINOv3 DINOv3 (frozen) AP 35.6 N/A 37.2 APr 33.9 N/A 35. APc 35.8 N/A 37.2 APf 37.3 N/A 37.6 (37.8 AP) or λsem = 0.0 + λgeo = 1.0 (37.1 AP, 35.6 APr). Excessive λsem tends to hurt rare-category performance when combined with high λgeo, indicating necessary balance. These results confirm that the geometry-aware term complements semantic alignment by enforcing spatial consistency across the long tail. Table 7 compares different backbones under identical detection and segmentation heads on LVISminival. The YOLOv8 backbone achieves 35.6 AP overall, with 33.9 APr on rare categories. In contrast, using the DINOv3 backbone without freezing leads to training collapse (indicated by N/A), resulting in no meaningful convergence. However, when the DINOv3 backbone is frozen during pre-training, ExpAlign reaches 37.2 AP overall and 35.9 APr, outperforming YOLOv8 by 1.6 AP and showing particular gains on rare categories. This suggests that preserving the rich, highquality pre-trained features from strong frozen vision foundation model is crucial for ExpAligns cross-modal alignment objectives, whereas detection-oriented backbone like YOLOv8 or unfrozen DINOv3 hinders effective learning of the alignment signals. 5. Visualization Figure 2 illustrates the intuition behind the proposed EAH. Instead of collapsing text prompt into single global embedding, EAH preserves all token-level representations before the EOT token and computes spatial alignment map for each token. These token-wise maps are then combined 7 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Figure 2. Expectation alignment map calculation diagram. Spatial alignment maps are first computed for individual text tokens. All maps are then aggregated with their importance weight (displayed below each map) to form prompt-conditioned expectation alignment map. (a) (b) (c) (d) Figure 3. Qualitative examples of detection and segmentation results. (a) prompts: laptop, cellphone, watch, cup, mouse, long arm desk lamp, pen, mouse pad, touchpad, screen, keyboard. (b) prompts: paper cutting, cabinet, exit sign. (c) prompts: capybara, monkey on the back of capybara. (d) prompts: person wearing helmet, pliers, gloves, goggles. Zoom in for better visual effect. through soft expectation mechanism, where each token contributes with learned importance weight. As result, the EOT token remains the dominant alignment signal inherited from CLIP pre-training, while informative non-EOT tokens (e.g., knee, high, socks) provide complementary finegrained cues that refine the spatial structure of the alignment map. Rather than suppressing the EOT token, Expectation Alignment enhances it with token-level semantic details, enabling fine-grained vision-language alignment without introducing hard token selection or additional supervision. We present qualitative results of ExpAlign in Figure 3. Subfigures 3a and 3b present correctly detected objects, where all corresponding prompt phrases are completely absent from the training data. This clearly demonstrates the robust zero-shot generalization capability of ExpAlign. Furthermore, the results in subfigures 3c and 3d reveal that the model exhibits non-trivial level of referring expression comprehension (REC) ability, successfully grounding complex and novel expressions even in unseen scenarios. Notably, ExpAlign delivers exceptionally high-quality segmentation masks, with particularly impressive performance at object boundaries and under partial occlusion, highlighting its strong capability in precise instance delineation. See more examples in Appendix H. 6. Conclusion In this paper, we presented ExpAlign, an expectationguided vision-language alignment framework for openvocabulary grounding under weak and ambiguous supervision. By introducing the Expectation Alignment Head (EAH), our method aggregates token-level vision-language similarities through principled expectation mechanism, enabling implicit token selection and soft region alignment without relying on explicit instance-level annotations. Furthermore, we proposed multi-scale consistency regularization strategy, including Top-K multi-positive contrastive objective and Geometry-Aware Consistency Objective, to jointly enhance semantic discriminability and spatial coherence of alignment maps during training. Extensive experiments on open-vocabulary detection and instance segmentation benchmarks demonstrate that ExpAlign consistently improves performance, particularly on long-tail categories and zero-shot segmentation quality, while remaining lightweight and fully compatible with standard detection and segmentation pipelines. We believe this work offers practical and theoretically grounded step toward more expressive and robust vision-language alignment for open-world visual understanding. 8 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding"
        },
        {
            "title": "References",
            "content": "Achal Dave, P., Ramanan, D., Kirillov, A., and Girshick, R. Evaluating large-vocabulary object detectors: The devil is in the details. CVPR, 2022. Cai, Z., Kwon, G., Ravichandran, A., Bas, E., Tu, Z., Bhotika, R., and Soatto, S. X-detr: versatile architecture for instance-wise vision-language tasks. In European Conference on Computer Vision, pp. 290308. Springer, 2022. Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., and Shan, Y. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1690116911, 2024. Fu, S., Su, Y., Rao, F., Lyu, J., Xie, X., and Zheng, W.- S. Wedetect: Fast open-vocabulary object detection as retrieval. arXiv preprint arXiv:2512.12309, 2025. Kang, R., Song, Y., Gkioxari, G., and Perona, P."
        },
        {
            "title": "Is\narXiv preprint",
            "content": "clip ideal? no. can we fix it? yes! arXiv:2503.08723, 2025b. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Li, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.-N., et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1096510975, 2022. Liang, H., Jiang, C., Feng, D., Chen, X., Xu, H., Liang, X., Zhang, W., Li, Z., and Van Gool, L. Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 32933302, 2021. Gupta, A., Dollar, P., and Girshick, R. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 53565364, 2019. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Huang, Z., Sheng, Z., Qu, Y., You, J., and Chen, S. Vlmrl: unified vision language models and reinforcement learning framework for safe autonomous driving. Transportation Research Part C: Emerging Technologies, 180: 105321, 2025. Hudson, D. A. and Manning, C. D. Gqa: new dataset for compositional question answering over real-world images. arXiv preprint arXiv:1902.09506, 3(8):1, 2019. Ilse, M., Tomczak, J., and Welling, M. Attention-based deep multiple instance learning. In International conference on machine learning, pp. 21272136. PMLR, 2018. Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and Carion, N. Mdetr-modulated detection for end-toend multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 17801790, 2021. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pp. 3855. Springer, 2024. Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 2641 2649, 2015. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Kang, M., Lee, M., Kim, M., Kim, D., and Lee, S. Empower words: Dualground for structured phrase and sentence-level temporal grounding. arXiv preprint arXiv:2510.20244, 2025a. Ren, T., Jiang, Q., Liu, S., Zeng, Z., Liu, W., Gao, H., Huang, H., Ma, Z., Jiang, X., Chen, Y., et al. Grounding dino 1.5: Advance the edge of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. 9 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024. Zhang, H., Zhang, P., Hu, X., Chen, Y.-C., Li, L., Dai, X., Wang, L., Yuan, L., Hwang, J.-N., and Gao, J. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems, 35: 3606736080, 2022. Zhang, H., Li, F., Zou, X., Liu, S., Li, C., Yang, J., and Zhang, L. simple framework for open-vocabulary segmentation and detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1020 1031, 2023. Zhao, T., Liu, P., and Lee, K. Omdet: Large-scale visionlanguage multi-dataset pre-training with multimodal detection network. IET Computer Vision, 2024. Zheng, Q., Luo, X., and Wang, T. Prll: Policy regularization and reward shaping assisted by large language models. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L. H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1679316803, 2022. Zhou, X., Girdhar, R., Joulin, A., Krahenbuhl, P., and Misra, I. Detecting twenty-thousand classes using image-level supervision. In European conference on computer vision, pp. 350368. Springer, 2022. Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., and Sun, J. Objects365: large-scale, highquality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 84308439, 2019. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Simeoni, O., Vo, H. V., Seitzer, M., Baldassarre, F., Oquab, M., Jose, C., Khalidov, V., Szafraniec, M., Yi, S., arXiv preprint Ramamonjisoa, M., et al. Dinov3. arXiv:2508.10104, 2025. Singh, A., Bhaskar, A., Yu, P., Chakraborty, S., Dasyam, R., Bedi, A., and Tokekar, P. Varp: Reinforcement learning from vision-language model feedback with agent regularized preferences. arXiv preprint arXiv:2503.13817, 2025. Varghese, R. and Sambath, M. Yolov8: novel object detection algorithm with enhanced performance and robustness. In 2024 International conference on advances in data engineering and intelligent computing systems (ADICS), pp. 16. IEEE, 2024. Wang, A., Liu, L., Chen, H., Lin, Z., Han, J., and Ding, G. Yoloe: Real-time seeing anything. arXiv preprint arXiv:2503.07465, 2025. Wang, Y., Su, X., Chen, Q., Zhang, X., Xi, T., Yao, K., Ding, E., Zhang, G., and Wang, J. Ovlw-detr: Openvocabulary light-weighted detection transformer. arXiv preprint arXiv:2407.10655, 2024. Yao, L., Han, J., Wen, Y., Liang, X., Xu, D., Zhang, W., Li, Z., Xu, C., and Xu, H. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. Advances in Neural Information Processing Systems, 35:91259138, 2022. Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. In EuroModeling context in referring expressions. pean conference on computer vision, pp. 6985. Springer, 2016. Zeng, Y., Zhang, X., and Li, H. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276, 2021. Zhai, S., Bai, H., Lin, Z., Pan, J., Tong, P., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al. Fine-tuning large vision-language models as decision-making agents via 10 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding A. Connection to Multiple Instance Learning Although ExpAlign is presented as vision-language alignment module rather than canonical MIL algorithm, it admits an exact interpretation and equivalence to attention-based soft pooling in the MIL framework. Below we give concise mapping and proof sketch that justifies the claim in Section 3.5. Notation. Fix textual prompt and feature scale s. Let Ω = {1, . . . , } index spatial locations in the feature map (N = HsWs), and let denote the number of valid text tokens. For each spatial location Ω and token {1, . . . , L}, define the tokenpatch affinity where is the flattened index of (x, y). The spatially averaged response of token is S(i, l) = Fs(x, y), Tb,p(l), and the token posterior is given by S(l) ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) iΩ S(i, l), π(l) = exp( S(l)/τt) l=1 exp( S(l)/τt) (cid:80)L . The EAM assigns to each spatial location the score (cid:101)S(i) = (cid:88) l=1 π(l) S(i, l). Reformulation as instance-wise linear pooling. For each instance i, define the token-affinity vector vi = (S(i, 1), . . . , S(i, L)) RL, and collect the token posteriors into π RL. With this notation, the EAM score can be written compactly as (cid:101)S(i) = πvi, which shows that each instance score is obtained by applying the same linear functional to its token-affinity vector. MIL interpretation and bag-level aggregation. From the MIL perspective, the prompt defines bag whose instances are the unordered set {vi}iΩ. The mapping vi (cid:55) (cid:101)S(i) is permutation equivariant, and the subsequent aggregation used by ExpAlign, ℓ = 1 TopK( (cid:101)S) (cid:88) (cid:101)S(i), iTopK( (cid:101)S) is permutation invariant. Such construction satisfies the defining requirement of MIL pooling operators and corresponds to Top-K variant of attention-based soft pooling, where discriminative instances dominate the bag-level response. Equivalence to attention-based MIL pooling. Attention-based MIL methods (Ilse et al., 2018) compute scalar score for each instance via an attention mechanism and aggregate these scores using permutation-invariant operator. In ExpAlign, attention is factorized into token-level posterior π, shared across instances, followed by instance-level pooling over (cid:101)S(i). Algebraically, both formulations reduce to computing instance scores g(vi) and applying soft or Top-K aggregation over instances. The difference lies only in how attention weights are parameterized, not in the form of the pooling operator. Permutation invariance and expressiveness. Because π depends only on the set {vi} through the averaged statistics { S(l)}, the overall operator from {vi} to ℓ is permutation invariant. Moreover, by adjusting the temperature τt and the Top-K ratio, the pooling behavior interpolates between mean, max, and soft-attention pooling, matching the expressive family of attention-based MIL operators. Discussion. This equivalence clarifies that ExpAlign performs principled MIL-style soft selection over instances while allowing uncertainty at both the token and spatial levels. The auxiliary multi-positive InfoNCE loss and the Geometry-Aware Consistency Objective can thus be viewed as bag-level discriminative losses and intra-bag energy shaping, respectively, consistent with standard MIL training principles. 11 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Remarks. For completeness, the variational derivation in Appendix further shows that the geometry-aware consistency term yields Gibbs reweighting of promptpatch probabilities under Lagrangian-constrained free-energy, which reshapes intra-instance mass without requiring explicit instance labels. B. Variational Derivation of Gibbs Reweighting in Energy-Based Consistency Regularization We consider finite collection of promptpatch pairs indexed by (p, i), with {1, . . . , } and Ω (Ω finite). For compactness we sometimes write generic index α to denote pair (p, i). Assumption B.1 (Energy field). There is real-valued alignment score field Sp(i) R. We define the associated energy by E(p, i) = Sp(i). (14) We assume E(p, i) is uniformly bounded on the finite domain. Assumption B.2 (Instance-wise geometry score). For each image and prompt we are given bounded geometry score Ab,p(i) defined for Ω such that: 1. Ab,p depends only on intra-instance relative statistics (e.g. mean and standard deviation computed over the ground-truth mask Mb,p), and is therefore invariant to adding constant to (affine invariance in the additive sense) and to monotone affine rescaling when appropriately adjusting normalization; 2. Ab,p is bounded and (locally) Lipschitz in (so gradient bounds exist and empirical gradients are well-defined). Assumption B.3 (Regularization parameters). Let τ > 0 be the temperature (entropy weight) and λ be the geometry weight (we will take λ 0 in most discussion). We denote by the probability simplex over all promptpatch indices: (cid:110) : Q(p, i) 0, = Q(p, i) = 1 (cid:111) . (cid:88) p,i Let denote the uniform distribution on the finite set of pairs (p, i), i.e. U(p, i) = 1/(P Ω). Theorem B.4 (Variational optimality and induced Gibbs form). Under Assumptions B1B3, consider the variational free-energy functional F[Q] = EQ (cid:2)E(p, i)(cid:3) λ EQ (cid:2)Ab,p(i)(cid:3) + τ KL(cid:0)Q U(cid:1), P. (15) Then: 1. The functional is strictly convex on and admits unique minimizer P. 2. The minimizer has the explicit Gibbs (exponential-family) form Q(p, i) = (cid:80) exp(cid:0) 1 p,i exp(cid:0) 1 τ τ (cid:0)E(p, i) λAb,p(i)(cid:1)(cid:1) (cid:0)E(p, i) λAb,p(i)(cid:1)(cid:1) . (16) 3. Equivalently, substituting E(p, i) = Sp(i), the optimal distribution can be written Q(p, i) = exp(cid:0) 1 p,i exp(cid:0) 1 τ (cid:0) Sp(i) + λAb,p(i)(cid:1)(cid:1) (cid:0) Sp(i) + λAb,p(i)(cid:1)(cid:1) . τ (cid:80) 4. Moreover, minimizing the cross-entropy (or KL divergence) from an empirical target distribution Qtarget(p, i) iMb,p wb,p(i) to yields the geometry-aware loss of the form Lgeo = (cid:88) (cid:88) b,p iMb,p cb,p,i log Q(p, i), which is equivalent up to normalization constants to the loss reported in the main text. 12 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Remarks. The KL term provides strict convexity and enforces positive entropy, preventing collapse to point mass; the linear terms (expectation of and A) are affine in and therefore preserve convexity. The parameter τ controls the trade-off between fidelity to energy and entropy (stability vs. selectivity); λ controls the strength of instance-local geometric shaping. Additive shifts of (i.e. (cid:55) + c) do not change Q; multiplicative rescaling of can be absorbed into τ (i.e. aE/τ = (E)/(τ /a)). Proof. We supply complete and explicit derivation in several carefully enumerated steps. Work on the finite index set = {(p, i)}. Any can be represented as vector RI with nonnegative entries summing to one. On this finite dimensional simplex all functions below are well-defined and differentiable on the interior. Convexity and existence/uniqueness. Observe that F[Q] in (15) can be written as F[Q] = (cid:88) Q(p, i)(cid:0)E(p, i) λAb,p(i)(cid:1) + τ (p,i)I Q(p, i) log Q(p, i) U(p, i) . (cid:88) (p,i) The first term is linear in Q; the second term is τ times the relative entropy (KL), which is strictly convex in on the interior of the simplex. Hence is strictly convex. Because is compact and is continuous, unique minimizer exists. First-order optimality (variational derivative). To find the minimizer, form the Lagrangian for the constrained minimization (constraint: (cid:80) (p,i) Q(p, i) = 1): L(Q, η) = (cid:88) (p,i) Q(p, i)(cid:0)E(p, i) λAb,p(i)(cid:1) + τ Q(p, i) log Q(p, i) U(p, i) (cid:88) (p,i) + η(cid:0) (cid:88) Q(p, i) 1(cid:1), (p,i) where η is the Lagrange multiplier enforcing normalization. Take partial derivative with respect to Q(p,i) (interior point) and set to zero: 0 = Q(p,i) = E(p,i) λAb, p(i) + τ (cid:16) log Q(p,i) U(p,i) (cid:17) + 1 + η. Rearrange to isolate the log term: log Q(p,i) U(p,i) = Exponentiating both sides yields (cid:16) Q(p,i) = U(p,i) exp 1 τ 1 τ (cid:0)E(p,i) λAb, p(i)(cid:1) 1 η τ . (cid:0)E(p,i) λAb, p(i)(cid:1)(cid:17) (cid:16) exp 1 (cid:17) . η τ Since exp(1 η/τ ) is global scalar independent of (p,i), normalization enforces that this scalar equals the reciprocal of the partition sum. Using the explicit form of U(p, i) (uniform), we obtain the normalized Gibbs form Q(p, i) = (cid:80) exp(cid:0) 1 p,i exp(cid:0) 1 τ τ (cid:0)E(p, i) λAb,p(i)(cid:1)(cid:1) (cid:0)E(p, i) λAb,p(i)(cid:1)(cid:1) . This completes the derivation of (16) and establishes both necessity and sufficiency of this form for optimality (sufficiency follows from strict convexity). 13 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Substitution = and alternative form. Using E(p, i) = Sp(i), rewrite (16) as Q(p, i) = exp(cid:0) 1 p,i exp(cid:0) 1 τ (cid:0) Sp(i) + λAb,p(i)(cid:1)(cid:1) (cid:0) Sp(i) + λAb,p(i)(cid:1)(cid:1) . τ (cid:80) This shows that the geometry term Ab,p(i) directly enters the logits of the Gibbs distribution and hence modifies the model posterior in multiplicative exponential manner. Equivalence to cross-entropy style training loss. Suppose we define target empirical distribution on (p, i) for training, Qtarget(p, i) = {i Mb,p} wb,p(i) p,i {i Mb,p} wb,p(i) (cid:80) , where wb,p(i) is nonnegative weight (e.g. wb,p(i) = Ab,p(i) or another monotone transform). The standard cross-entropy (expected negative log-likelihood) of this target under model is CE(QtargetQ) = (cid:88) p,i Qtarget(p, i) log Q(p, i). Minimizing this CE over model parameters (i.e. making approximate Qtarget) is equivalent to minimizing KL(QtargetQ) up to an additive entropy constant H(Qtarget) independent of model. When the model is constrained to the Gibbs family as in (16), minimizing CE corresponds to adjusting free-energy parameters (and indirectly logits and geometry weight λ) so that matches Qtarget. Thus the training objective Lgeo = (cid:88) (cid:88) cb,p,i log Q(p, i) b,p iMb,p is precisely the empirical counterpart of the variational optimization (15) when choosing Qtarget proportional to instancelocal geometry weights. Additional properties (invariance and non-collapse). Additive invariance. If (cid:55) + (for constant c), then the numerator of (16) acquires factor exp(c/τ ) independent of (p, i) and cancels with the denominator; hence is invariant to additive shifts of energy, equivalently to adding constants to S. Scaling and temperature. If is multiplied by positive scalar > 0, then τ /a E(cid:1), τ aE(cid:1) = exp(cid:0) 1 exp(cid:0) 1 so multiplicative rescaling of can be absorbed into reparametrization of τ (temperature). Non-collapse (positive entropy). Because τ > 0 and the KL term penalizes zero entropy, the minimizer has strictly positive entropy (unless the energy differences are arbitrarily large compared to τ ). In particular is not point mass unless the limit τ 0 is taken. Instance-local perturbation. If Ab,p(i) is supported only on indices belonging to ground-truth instance Mb,p, then the additive perturbation λAb,p(i) only affects relative probabilities within that instance; it does not change ordering of energies between different instances except insofar as their partition sums change, and thus constitutes conditional (instance-wise) energy shaping. Limit cases and interpretation. As τ , the KL penalty dominates and tends to the uniform distribution (max-entropy limit). 14 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding As τ 0+, concentrates on the minimizers of E(p, i) λAb,p(i) (hard selection / argmax). As λ 0, one recovers the standard Gibbs posterior based solely on (i.e. the semantic-only reweighting). Intermediate (τ, λ) trade off stability (entropy), semantic fidelity (alignment to S), and geometric consistency. This completes the derivation and justification of the Gibbs reweighting and conditional energy shaping regularizers used in the main text. C. Comparative Evaluation of ExpAlign, Grounding DINO, and GLIP on Diverse Real-World"
        },
        {
            "title": "Datasets in ODinW",
            "content": "In our comparison of ExpAlign, Grounding DINO, and GLIP across the diverse real-world datasets in the ODinW benchmark, as presented in Table 8, ExpAlign demonstrates competitive overall performance with slightly higher average score (22.4) than Grounding DINO (22.3) and clear advantage over GLIP (19.6), while also showing strong gains on several challenging and domain-specific subsets. For instance, ExpAlign substantially outperforms both baselines on datasets involving uncommon or underrepresented scenarios, such as MountainDewCommercial (45.46 vs. 25.46 for Grounding DINO and 21.60 for GLIP), ShellfishOpenImages* (42.63 vs. 29.56 and 25.90), MaskWearing (7.83 vs. 0.25 and 1.10), and PKLot 640 (5.23 vs. 0.06 and 0.00). These improvements are likely attributable to ExpAligns training on RefCOCO, which emphasizes referring expression comprehension and finer-grained grounding of objects in complex or natural-language contexts, helping the model better handle rare categories, occluded objects, or domain shifts not well covered in the O365 + GoldG + Cap4M pre-training corpus shared by Grounding DINO and GLIP. Notably, on the ODinW-13 benchmark subset (marked with *), ExpAlign also achieves leading results in several cases (e.g., CottontailRabbits, EgoHands generic, pistols, VehiclesOpenImages), underscoring its enhanced generalization in high-quality, diverse open-world evaluation settings. These observations highlight the value of incorporating referring expression data during pre-training to boost robustness on out-of-distribution and long-tail categories in real-world object detection. D. GACO Pseudocode This section provides the pseudocode of the Geometry-Aware Consistency Objective (GACO) used in all experiments. The objective operates on prompt-conditioned patch-level similarity maps and enforces consistency by reshaping the distribution of alignment scores within positive regions. Specifically, GACO treats the normalized similarity scores as an energy field over spatial locations and derives Gibbs-style reweighting through log-softmax normalization. Within each positive region, instance-level responses are standardized using region statistics, yielding an advantage signal that emphasizes relatively confident locations while preserving uncertainty. The final loss is computed as an advantage-weighted log-likelihood over masked locations, which corresponds to constrained reweighting of spatial energies rather than explicit instance-level supervision. Algorithm 1 summarizes the exact implementation used in our method. E. Hyper Paramerter Settings ExpAlign is trained using two-stage protocol with frozen image and text encoders in both stages. Stage 1 focuses on semantic alignment with moderate learning rate schedule and standard augmentations, while Stage 2 introduces geometry-aware consistency (GACO) and multi-positive contrastive weighting with reduced augmentation strength. Detailed hyperparameters are provided in Table 9. F. EAM Heatmap Visualizations for Negative Prompts Figure 4 presents the visualization of Explainable Attention Map (EAM) for negative sample prompt words on an example image of girl in sailor uniform. The left subfigure shows the original image, while the middle and right subfigures display EAM heatmaps for the positive prompt sailor uniform and negative prompt black sailor uniform, respectively. As observed, for negative prompt, the EAM activations are more uniformly distributed across the background rather than 15 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding"
        },
        {
            "title": "Average Score\nMedian Score",
            "content": "GLIP-T Grounding DINO ExpAlign 19.6 5.1 22.3 11.9 22.4 10.4 AerialMaritimeDrone large* AerialMaritimeDrone tiled AmericanSignLanguageLetters Aquarium* BCCD ChessPieces CottontailRabbits* DroneControl EgoHands generic* EgoHands specific HardHatWorkers MaskWearing MountainDewCommercial NorthAmericaMushrooms* OxfordPets by-breed OxfordPets by-species PKLot 640 Packages* PascalVOC* Raccoon* ShellfishOpenImages* ThermalCheetah UnoCards VehiclesOpenImages* WildfireSmoke boggleBoards brackishUnderwater dice openPoetryVision pistols* plantdoc pothole* selfdrivingCar thermalDogsAndPeople* websiteScreenshots 13.70 12.60 2.50 18.30 1.00 10.00 69.70 5.10 50.00 0.80 3.00 1.10 21.60 75.10 0.40 1.10 0.00 72.30 56.10 57.80 25.90 2.70 0.20 56.00 2.30 0.00 3.70 1.10 0.00 49.80 1.10 17.20 8.00 43.70 0.50 Table 8. Comparison of ExpAlign, Grounding DINO, and GLIP on the ODinW benchmark. Grounding DINO and GLIP are trained on Objects365, GoldG, and Cap4M using Swin-Tiny backbones.ExpAlign is trained on Objects365, GoldG, and RefCOCO using ConvNeXt-Tiny backbones. *denotes results belonging to the ODinW-13 benchmark. 10.30 17.50 0.78 18.64 11.96 15.62 67.61 4.99 57.64 0.69 4.05 0.25 25.46 68.18 0.21 1.30 0.06 60.53 55.65 60.07 29.56 17.72 0.81 58.49 20.04 0.29 1.47 0.33 0.05 66.99 0.36 25.21 9.95 67.89 1.30 11.15 23.66 2.34 19.88 13.98 7.87 81.34 0.37 61.50 0.91 3.30 7.83 45.46 25.38 2.01 7.20 5.23 71.65 59.00 46.29 42.63 12.65 0.99 61.40 10.38 1.33 3.70 0.36 0.16 75.69 2.37 7.30 8.21 57.04 2.64 16 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Algorithm 1 Geometry-Aware Consistency Objective (GACO) Input: similarity map sim [1, 1] of shape [B, K, H, ], binary mask {0, 1} of shape [B, K, H, ], hyperparameters β, adv clip, ϵ Output: geometry consistency loss Lgeo Normalize sim sim/(simmax + ϵ) Compute logits sim.view(B, ) Compute log log softmax(logits, dim = 1) Flatten to Mflat [B, ] Compute probability probflat σ(sim).view(B, ) Initialize advantage-weighted loss Ladv 0, denominator denom 0 for each batch = 0 to 1 do Find positive pixel indices pos idx where Mflat[b] > 0.5 if pos idx is not empty then Rpos probflat[b, pos idx] µ mean(Rpos), σ std(Rpos) + ϵ Advantage (Rpos µ)/σ Clamp clamp(A, adv clip, adv clip) Accumulate Ladv Ladv (A log p[b, pos idx]).sum() denom denom + pos idx end if end for Ladv Ladv/denom if denom > 0 else 0 Lgeo β Ladv Hyper Paramerter Stage 1 Training Stage 2 Training image size batch size epochs warmup epochs weight decay initial learning rate final learning rate fraction bias learning rate warmup momentum AMP training freeze image encoder freeze text encoder multi-positive InfoNCE weight GACO weight mosaic close mosaic hsv hsv hsv fliplr 640x640 512 30 3 0.025 0.002 0.001 0.0 0.9 True True True 0.0 0.0 1.0 2 0.015 0.7 0.4 0.5 Table 9. ExpAlign training hyper-parameters. 640x640 512 20 0 0.025 0.01 0.1 0.0 0.9 True True True 0.5 1.0 1.0 5 0.005 0.05 0.05 0. concentrating on the foreground object (the uniform). This pattern suggests that the model suppresses the detection of negative prompts by diffusing attention, reducing false positives in irrelevant regions and enhancing overall robustness in prompt-guided tasks. 17 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding (a) original image (b) positive prompt: sailor uniform (c) negative prompt: black sailor uniform Figure 4. EAM heatmaps for positive prompt sailor uniform and negative prompt black sailor uniform. Background-dominant activations indicate effective suppression of unseen negative prompts. G. Impact of Global Negative Vocabulary During training, we observe that the composition and quality of the global negative vocabulary have noticeable impact on performance, particularly for rare categories (APr). Varying the negative prompt setthrough different sampling strategies, vocabulary sizes, or semantic distributionsresults in fluctuations of approximately 0.8% in APr on the LVIS minival split. In contrast, the effect on overall AP as well as APc and APf is relatively limited, with variations within 0.2%. This behavior suggests that rare-category representations in the CLIP embedding space are inherently more fragile and sensitive to interference from negative prompts. When negative samples are semantically close to rare positives or occupy nearby regions in the embedding space, they can induce stronger gradient conflicts during contrastive alignment, disproportionately impairing the models ability to discriminate long-tail classes. Frequent and common categories, which are more densely covered during visionlanguage pre-training, exhibit greater robustness to such perturbations. We hypothesize that an effective negative vocabulary should occupy sweet spot in the CLIP feature space: sufficiently separated from the positive (LVIS) distribution to suppress false activations, yet not so distant that the negatives become uninformative and yield weak or noisy gradients. Negative sets that are overly similar to positives may lead to excessive suppression and hinder rare-category learning, while overly distant negatives may fail to provide meaningful discriminative supervision. Identifying such balanced negative distribution could further improve performance on LVIS, particularly for long-tail categories. At present, however, there is no standardized metric or principled methodology to quantify the quality or difficulty of global negative vocabulary in open-vocabulary detection. Developing reliable criteria or adaptive strategies for negative vocabulary constructionsuch as embedding-aware sampling, online hard-negative mining, or dynamic vocabulary curationremains an open challenge and promising direction for future work. H. More Visualization Examples Figure 5 and 6 shows additional zero-shot detection and segmentation results of ExpAlign on diverse scenes with multiobject and detailed text prompts. The model demonstrates strong open-vocabulary grounding and precise instance masks across novel categories and complex compositions. 18 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding (a) (b) (c) (d) Figure 5. (a) prompts: girl, sailor uniform, the right loafer, bow-knot, knee-high socks, pleated skirt. (b) prompts: snowboard, ski goggles, gondola lift, gloves, bunny ears headband. (c) prompts: minion, ballons, kid in white shirt, woman wear sunglasses. (d) prompts: black cat, panda toy, round panda toy. Zoom in for better visual effect. 19 ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding (a) (b) (c) (d) Figure 6. (a) prompts: lantern, lantern hold by hand, hand, headdress, embroidery, long skirt. (b) prompts: cat, popcorn, blacket, plush toys. (c) prompts: drawing child, woman. (d) prompts: diploma, person holding babies, woman in red dress, infant, sunglasses. Zoom in for better visual effect."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University, China",
        "Lingsu Lab, China",
        "University of Copenhagen, Denmark",
        "University of Electronic Science and Technology of China, China"
    ]
}