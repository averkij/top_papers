{
    "paper_title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models",
    "authors": [
        "Quan-Sheng Zeng",
        "Yunheng Li",
        "Qilong Wang",
        "Peng-Tao Jiang",
        "Zuxuan Wu",
        "Ming-Ming Cheng",
        "Qibin Hou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 4 5 1 0 . 8 0 5 2 : r Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models Quan-Sheng Zeng1,2, Yunheng Li1, Qilong Wang3, Peng-Tao Jiang4, Zuxuan Wu2, Ming-Ming Cheng1, Qibin Hou1 1VCIP, CS, Nankai University 2Shanghai Innovation Institute 3Tianjin University 4vivo Mobile Communication Co., Ltd Corresponding author. Abstract: Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes data-driven glimpse and prunes irrelevant visual tokens in single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining similarly high pruning rate. Our work paves new way for building more powerful and efficient LVLMs. Project Page: https://github.com/HVision-NKU/GlimpsePrune HVision@Nankai"
        },
        {
            "title": "Introduction",
            "content": "The rise of Large Language Models (LLMs) has spurred the development of large vision language models (LVLMs). Standard LVLMs encode visual inputs into tokens, which are then projected into the textual embedding space and concatenated with text instructions for an LLM to process. recent trend towards fixed patch size, rather than fixed number of patches, means that higher-resolution inputs often generate large number of visual tokens. This proliferation of visual tokens creates significant computational burden, as the number of visual tokens often dwarfs the textual query tokens. Usually, only small, querydependent subset of these visual tokens is relevant for answering given question. This highlights the need for dynamic, instruction-guided visual token pruning. Existing methods either fix the compression ratio in the architectural design [1, 2] or use hand-crafted compression metric design [3, 4, 5]. As shown in Fig. 1, when setting the same upper limit on the retention rate of visual tokens, existing methods still retain high degree of redundancy among the selected tokens. This motivates our central research question: How can we learn data-driven metric for dynamic and efficient visual token pruning? Beyond their inflexible designs, some existing token pruning methods exhibit critical limitation in real-world, freeform response scenarios. This is because they operate on the flawed assumption that the cross-attention from the first generated token to the visual tokens reliably indicates visual token importance [3, 6]. We find this assumption to be fragFigure 1 Compared to previous methods with fixed retention rates [5], our method can dynamically select effective visual tokens while being more accurate. ile. Tab. 1 provides quantitative evidence, showing that the performance of prior methods drops sharply when they are not prompted for concise answers. The reason is visually evident in Fig. 2: when generating free-form responses, the cross-attention initially focuses on irrelevant regions, leading to imprecise pruning and degraded final output. While dynamic pruning at each decoding step appears to be straightforward solution, its practical utility is limited. The critical information for pruning the cross-attention maps that best indicate visual token importance has been shown to emerge only at deeper layers of the LLM decoder [3, 7, 5]. per-step pruning strategy would thus have to operate 1 mance on free-form VQA tasks. Leveraging this efficiency, we show that under reinforcement learning framework, our fine-tuned version, GlimpsePrune+, reaches 110% of the original performance while maintaining high pruning rate, highlighting powerful synergy between our pruning strategy and advanced fine-tuning."
        },
        {
            "title": "2 Related Work",
            "content": "Large vision-language models. Pioneered by models like BLIP-2 [8] and LLaVA [9], LVLMs combine vision encoder, projector, and an LLM to achieve visual understanding. Recent models, including the InternVL [10, 11, 12, 13, 14] and Qwen-VL [15, 16, 17] series, support dynamic, highresolution inputs. While enabling fine-grained analysis, this capability dramatically increases visual tokens, creating computational bottleneck that motivates token compression research. Visual token compression. To mitigate the high computational cost of LVLMs, various token reduction strategies have been proposed. Some methods embed compression directly into the model architecture [1, 2]. However, these approaches are inflexible, suffering from fixed compression ratios tied to the architecture and requiring costly, full-scale retraining. Consequently, training-free, post-hoc methods that prune or merge tokens have gained prominence, which typically rely on two types of metrics: importance-based metrics and similarity-based metrics. For importance-based metrics, prior works typically compute importance scores using the cross-attention from global query representation to all visual tokens. This query representation can be derived from the instruction token [18, 19, 7, 20, 5], or dedicated global visual token [4, 21, 22]. However, such hand-crafted importance metric can be unreliable for free-form generation tasks, where the users core objective is not explicitly stated in the query, or the target information does not appear at the beginning of the generated response. Similarity-based metrics are also used by some prior works to prune and merge visual tokens, which typically use cosine distance or self-attention scores within the visual features [4, 23, 20, 24, 25]. However, these techniques incur substantial memory overhead from the computation of dense similarity matrices, potentially negating the efficiency gains from optimizations like FlashAttention [26, 27]. Our work presents data-driven pruning metric to enable dynamic compression ratio and addresses the limitations of hand-crafted ones. Although some recent works also share the motivation for dynamic token compression [28, 29, 30, 31], our contribution is distinct in several key aspects. (i) We identify and address the inadequacy of prior methods for free-form VQA tasks; (ii) Our proposed framework employs glimpse token and lightweight predictor, which are supported by tailored training methodology. (iii) Our pruning strategy is fundamentally more efficient, performing single full-depth KV cache prune per response. This is in stark contrast to the Figure 2 Without brief prompt, the cross-attention map may initially fail to focus on relevant visual tokens, and only begins to shift its focus once the generated text aligns with those visual elements. Method Qwen2.5-VL-7B PDrop VScan GlimpsePrune w/ brief w/o brief 0.936 0. 0.753 0.406 0.845 0.781 0.929 0.939 Table 1 On TextVQA, previous methods (PDrop, VScan) show high sensitivity to the brief prompt, while the base model (Qwen2.5-VL7B) and our method remain robust. after the forward pass through these initial layers, which necessitates retaining the full KV cache for all visual tokens up to that point. Consequently, this strategy would offer limited savings on computation and memory. To address these challenges, we introduce GlimpsePrune, dynamic visual token pruning framework inspired by the human cognitive process of first glancing at relevant areas before responding. During the prefilling stage, we temporarily insert learnable glimpse token after the instruction tokens. We leverage the forward pass through the initial decoder layers to extract the cross-attention scores between this glimpse token and all visual tokens, and feed them into lightweight predictor. Both the glimpse token and the predictor are trained on small amount of Grounded Question Answering (GQA) data to learn data-driven pruning metric. Based on this, we perform one-time pruning of irrelevant visual tokens and their corresponding KV cache entries across all model layers. This strategy reduces the computational and memory footprint for the remaining prefill layers and the entire subsequent decoding phase. Extensive experiments demonstrate our predictors strong generalization capabilities. Integrated with Qwen2.5-VL-7B, GlimpsePrune achieves an average visual token pruning rate of 92.6% while retaining 100% of the original models perforFigure 3 Overview of GlimpsePrune. (a) At the prefilling stage, we prune irrelevant visual tokens after layers of the LLM decoder. (b) The architecture of visual token importance predictor (VIP). (c) The details of conditional self-attention with 2D RoPE. (d) At the decoding stage, the answer generation relies on the pruned KV cache, saving memory and I/O bandwidth. incremental pruning [3] or partial [7, 5] pruning in prior art, thus maximizing memory savings during decoding."
        },
        {
            "title": "3 Preliminaries",
            "content": "Inference in LVLMs is two-phase process: prefilling and decoding, facilitated by KV cache. To mitigate the high memory cost of attention, techniques like FlashAttention [26, 27] are now standard. We denote the visual and text token sequences by lengths Nv and Nt, respectively (where typically Nv Nt), and consider an LLM with decoder layers of hidden dimension D. The prefilling phase is computeintensive. It processes the initial Nv + Nt tokens via causal self-attention, with computational cost of O((Nv +Nt)2D), to populate KV cache of size 2L(Nv + Nt)D. Conversely, the decoding phase is memory-bandwidth-intensive. It generates tokens autoregressively, where the attention cost is reduced to O((Nv + Nt)D) per token by leveraging the cache. The bottleneck thus shifts to the I/O cost of loading this large cache from memory. While optimizations like FlashAttention reduce prefilling memory by avoiding the materialization of the full O((Nv + Nt)2) attention matrix, this efficiency gain is lost by pruning methods that require this exact matrix for token similarity assessment [20, 5]. In contrast, computing cross-attention from single query to all visual tokens is linear-cost operation compatible with FlashAttentions memory efficiency. Our approach, therefore, intervenes mid-prefilling. After designated layer K, we employ lightweight selection module, informed by linear-cost glimpse attention and precomputed visual features, to prune the visual token set from Nv down to v. Critically, this action simultaneously prunes the KV cache for all preceding layers (1 to K). This single intervention reduces the computational load for the remaining prefill layers and, more importantly, slashes both memory and I/O costs for the entire subsequent decoding phase. More details are presented in Sec. 4."
        },
        {
            "title": "4 Method",
            "content": "Hierarchical visual features and glimpse attention extraction. When the visual encoder processes patched image into Nv visual tokens, we extract C-dimensional visual features, denoted as RM NvC, from of its intermediate layers. These hierarchical features provide multi-scale visual priors that are crucial for the subsequent visual token importance prediction. For the prefilling stage, we introduce an matrix of learnable embeddings as glimpse token. The first embedding is concatenated to the end of the input sequence, and then at each decoder layer, its respective embedding is added to the hidden state at that newly appended position. The purpose of appending them at the final position is to leverage the LLM decoders causal attention mechanism. This allows the glimpse token to fully interact with both visual and textual tokens across all layers, thereby capturing early signals about the importance of visual tokens relevant to the users instruction, all without perturbing the original processing of the visual and textual tokens themselves. We posit that by an intermediate layer K, the glimpse token has already aggregated sufficiently clear importance information. Our subsequent experiments demonstrate that setting = 2/3 strikes favorable trade-off between performance and the computational overhead of the prefilling 3 Figure 4 Overview of the training process: (a) Training glimpse token and VIP for data-driven pruning metric. (b) RL fine-tuning LLM for GlimpsePrune+. stage. We then extract the glimpse attention from the glimpse token to all visual tokens at layer K, denoted as RNvH , where is the number of attention heads. Visual importance prediction and one-shot pruning. Subsequently, the extracted glimpse attention and the multilevel visual features are fed into the selection module, named Visual Importance Predictor (VIP). As illustrated in Fig. 3(b), the VIP is composed of Self-Attention Blocks, equipped with 2D Rotary Position Embeddings (RoPE) [32]. The glimpse attention is first projected into new embedding space RNvE, where is the VIPs hidden dimension. As illustrated in Fig. 3(c), within the m-th self-attention block, we project the corresponding m-th visual feature map RNvF and concatenate it to the query and key Vm to vectors. This allows the relationship within the visual tokens of to act as additional conditioning, influencing the final importance prediction. By setting the dimensions D, (where is the LLMs hidden dimension and is the visual feature dimension), and implementing memory-efficient attention mechanism, the computational overhead of the entire VIP remains negligible compared to single LLM decoder layer. The VIPs output, an importance map , dynamically and precisely identifies visual tokens relevant to the users instruction, adapting to different inputs and visual targets. Furthermore, maximum retention ratio can be configured to sparsify the visual tokens even when focuses on large-scale visual objects. Based on the importance map, we perform one-shot pruning at layer K. Unimportant visual tokens are removed from the layer-K hidden state and their corresponding KV cache entries in all preceding layers, permanently reducing the sequence length to + Nt for all subsequent computations. The glimpse token is also discarded so that the answer generation will not be influenced by it. Importance metric learning by glimpse token and VIP. The glimpse tokens and the VIP are trained while keeping the parameters of the base VLM frozen, using only small amount of Grounded Question Answering (GQA) data. Each data sample consists of question, its corresponding answer, and the bounding boxes of the visual regions relevant to the answer. Our training objectives are two-fold. On one hand, we apply language modeling loss that encourages the model to generate the correct answer immediately after the position of the glimpse token. On the other hand, we employ localization loss to optimize the VIPs output. This loss is weighted sum of DiceLoss and Binary Cross-Entropy (BCE) Loss. Given that bounding box annotations are often coarse, we prioritize recalling foreground visual tokens over achieving precise boundary segmentation. Therefore, we set the weight ratio for DiceLoss to BCE Loss at 10:1. The language loss and the localization loss are balanced with 1:1 weight ratio. Since the LVLMs original attention mechanism already possesses an inherent localization capability, our training primarily serves to calibrate the newly introduced parameters and amplify this ability. Consequently, our method exhibits strong generalization, claim we will substantiate in the experimental section. Reinforcement learning fine-tuning. RL fine-tuning on dense visual inputs is computationally expensive and memoryintensive. Combining with our pruning mechanism mitigates this cost by first employing pruning mechanism to create sparse visual context. We then fine-tune the LLM decoder using the Group-wise Ranking Policy Optimization (GRPO) framework [33]. As illustrated in Fig. 4(b), the fine-tuning process begins after pruning at layer K. The policy model generates multiple candidate responses, which are evaluated by reward model to compute policy loss. The loss is regularized with KL divergence term against reference model. The new model, named GlimpsePrune+, is trained iteratively. We first update the LLM decoder via GRPO, and then train our glimpse token 4 and predictor on the updated decoder. This process allows GlimpsePrune+ to achieve superior performance over the baseline model even at high pruning rate. for all methods. As our proposed method, GlimpsePrune, features dynamic retention rate, we will report both its performance under this constraint and its average retention rate on each dataset."
        },
        {
            "title": "5.1 Experimental setup",
            "content": "Model architectures. Our primary experiments are conducted on Qwen2.5-VL-7B [17], an open-source architecture that supports dynamically high-resolution inputs (from 4 to 16, 384 visual tokens). To demonstrate the generalizability of our approach, we also adapt and evaluate it on LLaVA-1.57B [34], model designed for fixed-resolution inputs (576 visual tokens). In these architectures, we set the VIPs hidden dimension to = 256 and the visual conditioning dimension to = 512, with layers of visual features. The pruning position is set to approximately = 2/3 of the original LLM decoder. For detailed settings on different model sizes, please refer to the Appendix C. Training setups. To demonstrate the generalization capability of GlimpsePrune, we train the VIP and glimpse token using only 20K randomly sampled data from the GQA dataset [35]. We only train for one epoch, which can be completed in about 0.5 hours on single NVIDIA A100 GPU. For the RL finetuning in GlimpsePrune+, we use 240K samples randomly selected from the VisCoT dataset [36], which provides diverse set of visual question-answering scenarios. For the GRPO algorithm, we set the group size to 4, the number of policy update iteration to 1, and the KL divergence coefficient to 0.04, respectively. We apply LoRA [37] with rank of = 64 to fine-tune the LLM decoder. More details can be found in the Appendix B. Evaluation setups. For free-form VQA, we utilize the comprehensive benchmark from VisCoT [36], which comprises 12 diverse VQA datasets spanning multiple domains and featuring wide range of target object scales. Crucially, evaluation is performed by powerful LLM rather than rule-based string matching, providing more holistic assessment of generative quality. We use Qwen2.5-32B-Instruct-GPTQ-Int8 [38] as the evaluator LLM. For short-form VQA, we select 10 widely-used VQA benchmarks and evaluate them under the LMMs-Eval framework [39]. These benchmarks typically require concise answers, such as short phrase or single option from list, and are evaluated using exact-match or rulebased string comparison. Further details on these datasets are provided in the Appendix A. Comparison methods. We compare our method against several state-of-the-art and representative visual token compression techniques: PDrop [7], VisionZip [4], DivPrune [23], CDPruner [20] and VScan [5]. Since these methods operate with fixed compression ratio and apply compression at different stages, we set an upper bound on the average retention rate of visual tokens KV cache during the decoding phase"
        },
        {
            "title": "5.2 Main results",
            "content": "GlimpsePrune for free-form VQA. As detailed in Tab. 2, we evaluate GlimpsePrune on 12 free-form VQA datasets under three distinct average retention rate constraints: 11.1%, 22.2%, and 33.3%. With an average retention rate as low as 11.1%, our method achieves mean accuracy of 0.761, remarkably retaining 100% of the performance of the original Qwen2.5-VL-7B model. The dynamic nature of GlimpsePrune is particularly evident in its adaptability to varying target object scales. On datasets with small target objects, such as DocVQA, GlimpsePrune dynamically adjusts to much lower retention rate (3.6-3.8%) while exhibiting only marginal performance drop (from 0.964 to 0.962). Conversely, on datasets featuring large-scale visual targets like VSR, even when constrained by stringent upper bound, GlimpsePrune effectively sparsifies the visual context. It reduces the retention rate from 39.4% (unconstrained) to 10.3% with similar performance (0.618 vs. 0.620), These results strongly indicate that GlimpsePrune not only accurately identifies critical visual tokens but also intelligently sparsifies them when operating under tight computational budgets. We provide qualitative examples in the Appendix D. On high-resolution inputs, methods like VisionZip [4] and VScan [5] face practical limitations. Their reliance on computing dense similarity matrices leads to prohibitive memory usage that negates FlashAttention2 [27] benefits, often causing Out-of-Memory (OOM) errors. In contrast, our method is designed to remain efficient under these conditions. GlimpsePrune for short-form VQA. As shown in Tab. 3, GlimpsePrune also demonstrates strong performance on shortform VQA tasks. With an average retention rate capped at 11.1%, GlimpsePrune achieves an average accuracy of 70.0% across 10 benchmarks, which is 99.6% of the original Qwen2.5-VL-7B models performance. GlimpsePrune for other architectures. As shown in Tab. 4, GlimpsePrune also demonstrates strong performance on the LLaVA-1.5-7B model. Due to the fixed number of 576 visual tokens used as input in LLaVA-1.5-7B, the dynamic range for compression is smaller than that of the Qwen2.5-VL-7B model. Nevertheless, GlimpsePrune still achieves an average accuracy of 0.490 with an average retention rate capped at 11.1%, which is 97.6% of the original LLaVA-1.5-7B models performance. More results on other architectures can be found in the Appendix D. General VQA Relation Reasoning Fine grained Doc/Text Chart Method Flickr30k V7W GQA OpenImages VSR CUB DocVQA TextCaps TextVQA DUDE SROIE InfoVQA Average Qwen2.5-VL-7B 0. 0.615 0.567 0.425 0.642 0.724 0.964 0.868 0. 0.801 0.977 0.860 0.761 PDrop VisionZip VScan GlimpsePrune PDrop VisionZip VScan GlimpsePrune PDrop VisionZip VScan GlimpsePrune GlimpsePrune GlimpsePrune+ 0.301 0.248 0.373 0.520 0.505 0.657 0.635 0.518 0.475 8.9% 8.8% 8.7% 0.636 0.584 0. Average retention rate 11.1 % 0.293 oom oom 8.6% 0.458 0.263 0.177 0.315 oom 0.565 0.661 0.484 0.627 oom 10.3% 10.5% 3.6% 0.962 0.620 0.700 0.358 0.660 0.678 6.7% 0.861 Average retention rate 22.2 % 0.235 0.238 0.102 0.406 oom oom oom 0.674 0.781 oom oom oom 7.1% 4.4% 6.7% 4.5% 0.854 0.786 0.978 0. 0.276 7.4% 0.761 0.380 0.318 0.561 0.534 0.568 0.511 0.398 0.354 0.465 oom oom 0.716 0.690 oom oom 18.6% 15.0% 14.5% 13.8% 18.4% 18.6% 3.8% 0.962 0.461 0.757 0.263 0.486 0.598 0.713 0.582 0.645 0.621 0.717 0.639 0. 0.473 0.788 0.775 8.5% 0.861 Average retention rate 33.3 % 0.352 0.176 oom oom oom oom 0.545 0.840 0.856 9.2% 5.3% 7.1% 5.2% 11.5% 0.764 0.941 0.288 oom oom 0.375 0.792 0.978 0.857 0.636 0.587 0.427 0.366 0.590 0.550 0.580 0.515 0.336 0.552 0.619 0.718 0.590 0.662 0.505 0.392 0.537 oom oom 0.731 0.714 oom oom 18.3% 24.7% 18.4% 17.2% 24.7% 22.4% 3.8% 0.963 0.464 0.757 Unlimited constraint 26.8% 31.2% 25.0% 23.1% 39.4% 25.5% 3.8% 0.963 0.759 0.460 28.7% 31.9% 24.9% 23.2% 45.9% 23.7% 6.9% 0.719 0.776 0. 0.618 0.722 0.620 0.723 0.800 0.830 0.635 0.583 0.753 0.722 0.562 0.839 0.812 9.3% 0. 0.420 0.248 oom oom oom oom 0.637 0.897 0.892 10.1% 5.6% 7.2% 5.5% 13.9% 0.765 0.940 0.392 oom oom 0.448 0.792 0.978 0. 10.1% 11.1% 6.0% 7.2% 5.5% 17.9% 0.795 0.978 0.860 0.764 12.1% 11.5% 9.9% 13.8% 8.4% 20.1% 0.838 0.822 0.977 0.876 0.860 0.941 0.953 0.869 Table 2 Performance in free-form VQA tasks, oom indicates out-of-memory errors."
        },
        {
            "title": "5.3 Ablation studies",
            "content": "We conduct ablation studies on the Qwen2.5-VL-3B model, which has an LLM decoder with = 36 layers and achieves an average performance of 0.746 on free-form VQA tasks. We will analyze the impact of design choices and training components in GlimpsePrune under fixed retention rate upper limit of 11.1%. The importance of glimpse tokens. We first implement baseline without training, as shown in the first row of Tab. 5: using the attention weights from the first generated token to the visual tokens at layer = 24 as the pruning metric. This baseline achieves an average accuracy of only 0.546. When we add glimpse tokens and subsequently add language and location losses, the performance gradually improves to 0.684 and 0.702, respectively. This demonstrates that glimpse tokens can provide good visual importance prediction before the model generates free-form answers. The importance of visual conditions. The last two rows of Tab. 5 highlight the necessity of conditioning our importance map optimization on visual features. Without them, merely adding 4-layer self-attention VIP only improves the performance from 0.702 to 0.716. Conversely, incorporating visual features as conditions in VIP boosts performance to 0.736. Notably, this performance gain predominantly stems from Doc/Text data, whose distribution significantly differs from the training set. This suggests that visual conditioning mitigates the models tendency towards shortcut learning. The selection of pruning layer. The choice of the pruning layer, K, presents trade-off between performance and efficiency. While deeper layers provide more interpretable attention maps, they also increase prefill computation. Based on our analysis in Tab. 6, we found = 24 (two-thirds of the total layers) to be an optimal balance. We therefore adopt this 2/3 ratio across all tested architectures. Notably, pruning at = 36 (the last layer) leads to performance drop to 0.556, as the glimpse attention of the final layer tends to focus less on visual information."
        },
        {
            "title": "5.4 Efficiency analysis",
            "content": "Efficent inference. As shown in Tab. 7, our method significantly enhances inference efficiency. Benchmarked on single A100 GPU with the Qwen2.5-VL-7B model on 100 DocVQA samples (with KV Cache and FlashAttention2), GlimpsePrune reduces the computation-intensive prefill cost to 69.1% of the baseline. More critically, for the memoryintensive decoding phase, it slashes the initial KV Cache length from 5,073.9 to 202.5 tokens. This leads to reduction in peak memory usage across the entire generation process to 6 Method VQAv2 vallite GQA VizWizval SQAimg POPE MME MMBen test MMBcn test SEEDimg V* Average Qwen2.5-VL-7B 79.5 61.4 70.6 PDrop VisionZip VScan GlimpsePrune 53.3 68.4 74.2 78.0 41.8 54.9 57.1 59.5 53.3 66.4 67.5 69.4 GlimpsePrune 79.1 60. 70.5 87.5 2327.9 87.5 Average retention rate 11.1 % 68.4 77.0 82.9 82.7 86.1 82.2 87.2 87.5 Unlimited constraint 87.6 87.3 1642.2 2031.6 2129.8 2348.0 2323. 83.2 42.9 75.6 74.1 82.6 82.9 82.8 38.7 72.6 71.5 81.7 81. 77.5 71.7 70.3 49.4 77.4 42.4 60.2 59.7 72.8 46.8 70. 77.4 72.8 70.0 Table 3 Performance in short-form VQA tasks, the maximum number of visual tokens is set to 2048 here. General VQA Relation Reasoning Fine grained Doc/Text Chart Method Flickr30k V7W GQA OpenImages VSR CUB DocVQA TextCaps TextVQA DUDE SROIE InfoVQA Average LLaVA-1.5-7B 0.694 0.658 0.635 0.519 0.613 0.560 0.252 0. 0.650 0.293 0.125 0.393 0.502 Average retention rate 11.1 % PDrop VisionZip DivPrune CDPruner VScan GlimpsePrune 0.498 0.454 0.493 0.577 0.574 0.643 0.616 0.592 0.656 0.616 0.602 0.670 0.619 0.598 0.663 9.2% 9.5% 8.9% 0.666 0.654 0.690 0.501 0.511 0.522 0.531 0.540 9.8% 0.526 0.134 0.462 0.457 0.207 0.577 0.502 0.210 0.562 0.483 0.206 0.560 0.506 0.224 0.584 0.516 10.7% 10.3% 8.4% 0.236 0.615 0.547 Unlimited constraint 0.335 0.595 0.533 0.551 0.610 8.5% 0.584 0.356 0.148 0.022 0.314 0.349 0.242 0.107 0.613 0.382 0.230 0.077 0.551 0.368 0.217 0.064 0.575 0.624 0.370 0.244 0.101 8.7% 8.8% 9.3% 6.3% 0.273 0.124 0.381 0. 0.348 0.458 0.451 0.455 0.474 9.0% 0.490 22.9% 30.0% 22.8% 29.2% 41.7% 18.1% 13.2% 14.8% 16.2% 16.1% 10.9% 8.8% 20.4% 0.495 0.247 0.693 0.606 0.551 0.272 0.119 0.664 0.649 0. 0.384 0.532 0.605 Table 4 Performance in free-form VQA tasks using LLaVA-1.5-7B. GlimpsePrune 72.8%. Efficient RL fine-tuning. Our fine-tuning experiments on the Qwen2.5-VL-7B model (Tab. 8) demonstrate that our pruning method effectively addresses Out-of-Memory (OOM) errors encountered when training with token lengths over 3000. By applying this technique, we not only supported token length to 6000 but also reduced the average iteration time and GPU memory usage to 81% and 70% of the original, respectively. Crucially, this efficiency gain was achieved with negligible impact on performance (0.835 vs. 0.841 compared to the dense-token baseline)."
        },
        {
            "title": "6 Conclusions",
            "content": "This work demonstrates the failure of static token compression methods and introduces GlimpsePrune, dynamic visual token pruning framework. By learning data-driven pruning metric, GlimpsePrune prunes 92.6% of visual tokens while on average retaining 100% of the baseline performance. An enhanced version, GlimpsePrune+, further boosts performance to 110% using GRPO fine-tuning. Our method presents practical and robust solution for building powerful and efficient LVLMs. 7 References [1] Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Efficient large multi-modal models via visual context compression. Advances in Neural Information Processing Systems, 37:7398674007, 2024. 1, 2 [2] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 1, 2 [3] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 1, 3 [4] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1979219802, 2025. 1, 2, [5] Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, and Dong Yu. Vscan: Rethinking visual token reduction for efficient large vision-language models. arXiv preprint arXiv:2505.22654, 2025. 1, 2, 3, 5 Glimpse Llang Lloc VIP Visual Condition General VQA Relation Reasoning Fine Grained Doc/Text Chart Average Qwen2.5-VL-3B 0.664 0.585 0.647 0.641 0.639 0.642 0.531 0.515 0.537 0.521 0.528 0.524 0. 0.762 0.760 0.763 0.764 0.760 0.892 0.506 0.771 0.802 0.835 0.882 0.816 0.542 0.693 0.802 0.788 0.804 0. 0.546 0.684 0.702 0.716 0.736 Table 5 Ablation study of training components and design choices in GlimpsePrune. Prune Layer (K) Performance Prefilling FLOPs(T) Pruned Time (s/iter) Memory (GB) Performance Qwen2.5-VL-3B 18 24 30 36 0.746 0.690 0.736 0.739 0.556 33. 17.4 23.1 28.5 34.1 37.1 29.9 80 4 56 4 0.841 0.835 Table 8 GlimpsePrune reduces the cost of RL fine-tuning. Experiments are conducted on 4 NVIDIA A100 GPUs. Table 6 The trade-off between performance and efficiency with different pruning layers. Method Qwen2.5 VL-7B Glimpse Prune Glimpse Prune+ Prefill FLOPs (T) Prefill Time (ms/token) Length of KV Cache Decode FLOPs (B) Decode Time (ms/token) #Generate tokens Max GPU Memory (GB) Average Score 77.8 423.3 5073.9 333.4 29.5 22.0 33.5 0.761 53.8 306.9 202.5 317.4 28.3 24.2 24.4 0.761 54.2 309.9 291.1 256.6 28.4 19.5 23.7 0.835 Table 7 Efficiency analysis of GlimpsePrune on Qwen2.5-VL-7B model with 100 samples from DocVQA. [6] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. MLLMs know where to look: Training-free perception of small visual details with multimodal LLMs. In International Conference on Learning Representations, 2025. 1 [7] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large visionlanguage models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 1, 2, 3, [8] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, pages 1973019742. PMLR, 2023. 2 [9] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36:3489234916, 2023. 2 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2 [11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 2 [12] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 2 [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. [14] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 2 [15] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 2 [16] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of 8 the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [17] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 5, [18] Kai Han, Jianyuan Guo, Yehui Tang, Wei He, Enhua Wu, and Yunhe Wang. Free video-llm: Prompt-guided visual perception for efficient training-free video llms. arXiv preprint arXiv:2410.10441, 2024. 2 [19] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Retake: Reducing temporal and knowledge redundancy for long video understanding. arXiv preprint arXiv:2412.20504, 2024. 2 [20] Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, and Shanghang Zhang. Beyond attention or similarity: Maximizing conditional diversity for token pruning in mllms. arXiv preprint arXiv:2506.10967, 2025. 2, 3, 5 [21] Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. Beyond text-visual attention: Exploiting visual cues for effective token pruning in vlms. arXiv preprint arXiv:2412.01818, 2025. 2 [22] Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, and Aymen Shabou. Pact: Pruning and clustering-based token reduction for faster visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1458214592, 2025. 2 [23] Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversity-based visual token pruning for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93929401, 2025. 2, [24] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, et al. Topv: Compatible token pruning with inference time optimization for fast and low-memory multimodal vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19803 19813, 2025. 2 [25] Boyuan Sun, Jiaxing Zhao, Xihan Wei, and Qibin Hou. Llavascissor: Token compression with semantic connected components for video llms. arXiv preprint arXiv:2506.21862, 2025. 2 [26] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. 2, 3 [27] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations, 2024. 2, 3, 5 [28] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370013710, 2024. 2 [29] Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, and Shaohui Lin. Dynamic-LLaVA: Efficient multimodal large language models via dynamic vision-language context sparsification. In International Conference on Learning Representations, 2025. [30] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18992 19001, 2025. 2 [31] Xiaoyu Liang, Chaofeng Guan, Jiaying Lu, Huiyao Chen, Huan Wang, and Haoji Hu. Dynamic token reduction during generation for vision language models. arXiv preprint arXiv:2501.14204, 2025. 2 [32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4, 11 [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 5, 12 [35] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6700 6709, 2019. 5, [36] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:8612 8642, 2024. 5, 11 [37] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 5 [38] Yang Qwen, Baosong Yang, Zhang, Hui, Zheng, Yu, Chengpeng Li, Liu, Huang, Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 5 [39] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 5, 12 [40] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69046913, 2017. 11 [41] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36083617, 2018. 11 [42] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 11 [43] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 11 [44] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:/2306.13394, 2023. [45] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2024. 11 [46] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 11 [47] Penghao Wu and Saining Xie. V*: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Free-form VQA datasets The free-form VQA tasks are evaluated on the VisCoT benchmark [36], which consists of 12 datasets. Tab. 9 lists the number of samples and the average ratio of target objects in each dataset. Dataset #Samples Ratio Dataset #Samples Ratio Flickr30k V7W GQA OpenImages VSR CUB 1546 1000 978 945 404 18.2% DocVQA 22.8% TextCaps 20.2% TextVQA 24.3% DUDE 41.6% SROIE 42.5% InfoVQA 888 853 526 603 686 360 6.8% 4.9% 5.6% 1.7% 10.1% 12.3% Table 9 Datasets used for free-form VQA evaluation. Ratio refers to the average ratio of target objects in the images. A.2 Short-form VQA datasets Tab. 10 lists the number of samples and answer types for the datasets used in short-form VQA evaluation, including VQAv2 [40], GQA [35], VizWiz [41], ScienceQA [42], POPE [43], MME [44], MMBench [45], SEEDBench [46], and V* bench [47]. Dataset #Samples Type Dataset #Samples Type vallite VQAv2 GQA VizWizval SQAimg POPE 500 12578 4319 2017 Phrase MME Phrase MMBcn test Phrase MMBen test Selection SEEDimg Judgment V* 2374 6666 6666 17990 191 Phrase Selection Selection Selection Selection Table 10 Datasets used for short-form VQA evaluation. The Type column indicates the type of answers in the dataset. Phrase means the answer is word or phrase, Selection means the answer is selected from set of options, and Judgment means the answer is judgment (e.g., yes/no). A.3 Datasets for training GlimpsePrune We train GlimpsePrune using 20,000 randomly sampled samples from the GQA dataset [35]. Each sample contains an image, question, short answer, and several bounding boxes of visual targets. We find that GlimpsePrune generalizes well to various VQA datasets after training on GQA, allowing our method to operate without the need for further expansion of the training dataset. A.4 Datasets for RL fine-tuning Since fine-tuning datasets for LLMs often require domain richness, we use the training sets of 12 datasets from VisCoT [36] for fine-tuning the LLM. Specifically, we randomly sample 20,000 samples from each dataset, with 10,000 samples constructed as short-form VQA tasks and 10,000 samples as free-form VQA tasks. Due to the high image resolution or long answers in some data, which can easily lead to out-ofmemory error during training, we limit the number of input tokens to 6000, and only compute the GRPO loss [33] for the first 128 generated tokens."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Training details. Tab. 11 lists the hyperparameters used for training GlimpsePrune, and Tab. 12 lists the hyperparameters used for RL fine-tuning. Key Value Key batch / device grad acc steps num gpus learning rate warmup ratio optimizer scheduler dtype 1 8 max grad norm 2 train data size 1e-4 num train epochs 0.1 weight of Llanguage AdamW weight of Ldice cosine weight of Lbce Value bfloat16 1.0 20,000 1 1.0 1.0 0.1 Table 11 Training arguments for GlimpsePrune. Key Value Key Value batch / device grad acc steps num gpus num generations num iterations lora rank lora alpha lora dropout learning rate reward model optimizer 1 8 scheduler 4 warmup ratio 4 1 max grad norm 32 64 dtype train data size num train epochs 0.05 weight of Lpolicy 1e-4 weight of LKL Qwen2.5-32B-Instruct-GPTQ-Int8 AdamW cosine 0.1 bfloat16 1.0 240,000 1 1.0 0.04 Table 12 Training arguments for RL fine-tuning. VQA LVLM Evaluator LLM temperature dtype use cache max new tokens attn impl bfloat16 top 0.0 temperature 0.7 0.8 True repetition penalty 1.05 512 1024 max new tokens FlashAttention2 Table 13 Evaluation details for free-form VQA tasks. B.2 Evaluation details. For free-form VQA tasks, we use Qwen2.5-32B-InstructGPTQ-Int8 as the evaluator LLM. The evaluation template is 11 You are responsible for proofreading the answers, you need to give score to the models answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the form score: <score>. The evaluation criteria require that the closer the models answer is to the standard answer, the higher the score. question: {} standard answer: {} models answer: {} Figure 5 Input template for the evaluator LLM. Architecture Qwen2.5VL-3B Qwen2.5VL-7B LLaVA-1.5-7B LLaVA-1.5-13B Num visual tokens Num visual layers Visual feature size (C) Num LLM layers (L) Num LLM attention heads (H) LLM hidden size (D) Selected visual layers Glimpse token shape Prune Layer (K) VIP hidden size (E) VIP condition size (F ) VIP attention heads 416384 32 1280 36 16 2048 [31, 23, 15, 7] 36 2048 24 256 512 416384 32 1280 28 28 3584 [31, 23, 15, 7] 28 3584 19 256 512 4 576 24 1024 32 32 4096 576 24 1024 40 40 5120 [23, 17, 11, 5] 32 4096 22 256 512 4 [23, 17, 11, 5] 40 5120 27 256 512 Table 14 Details of the model architectures. shown in Fig. 5. The evaluation settings for the LVLMs and the evaluator LLM are shown in Tab. 13. For short-form VQA tasks, we use the default configuration of the LMMs-Eval framework [39]."
        },
        {
            "title": "C Model Architecture Details",
            "content": "As demonstrated in the main paper, we validated the effectiveness of our method, GlimpsePrune, on several baseline models, including Qwen2.5-VL [17] and LLaVA-1.5 [34]. The detailed architectural parameters for these models are presented in Tab. 14. Our approach introduces two primary new components: learnable glimpse tokens and VIP module composed of selfattention blocks. Specifically, the glimpse tokens are integrated into every layer of the LLM decoder. However, during inference, only the tokens in the first layers are utilized. The position of pruning, K, is determined based on our ablation studies (Sec. 5.3), following the rule = 2/3 L, where is the total number of decoder layers. Furthermore, the VIP module is conditioned on = 4 visual features that are uniformly sampled from the visual encoder."
        },
        {
            "title": "D Additional Results and Analysis",
            "content": "D.1 Results on other architectures. In addition to the results presented in Tab. 2 and Tab. 4 of the main paper, Tab. 15 and Tab. 16 present the performance of GlimpsePrune on the free-form VQA task, based on the Qwen2.5-VL-3B and LLaVA-1.5-13B models, respectively. Compared to previous methods, our approach demonstrates superior ability to dynamically adjust the retention rate while achieving better performance. This highlights the adaptability and efficiency of GlimpsePrune across different model scales and architectures. D.2 Case studies. Although Tab. 2 shows that GlimpsePrune (based on Qwen2.5VL-7B) matches the baselines average score with only 7.4% retention rate, its performance varies across different cases. Here, we present both successful and unsuccessful examples to facilitate future research. Fig. 6, Fig. 7 and Fig. 8 display successful cases where GlimpsePrune answers more accurately because it prunes question-irrelevant visual information. In contrast, Fig. 9 and Fig. 10 show two types of failures. In the first type  (Fig. 9)  , the retained visual information is insufficient. In the second type  (Fig. 10)  , the information is sufficient, but the model still answers incorrectly. We hope these examples provide valuable insights for future studies. General VQA Relation Reasoning Fine grained Doc/Text Chart Method Flickr30k V7W GQA OpenImages VSR CUB DocVQA TextCaps TextVQA DUDE SROIE InfoVQA Average Qwen2.5VL-3B 0.703 0.626 0. 0.438 0.573 0.748 0.944 0.840 0.927 0.793 0. 0.816 0.746 Average retention rate 11.1% PDrop VisionZip VScan GlimpsePrune GlimpsePrune 0.378 oom oom 9.2% 0.444 0.298 0.267 0.401 0.478 0.465 0.546 0.492 0.444 0.402 8.9% 8.9% 8.8% 0.609 0.567 0.675 0.254 0.222 0.502 oom 0.426 0.713 0.367 0.717 oom 10.6% 10.4% 5.2% 0.934 0.561 0.760 Unlimited constraint 25.4% 31.8% 25.9% 27.0% 44.1% 23.0% 4.9% 0.931 0.437 0.695 0.560 0.767 0.624 0.578 0.428 0.600 0.564 7.6% 0. 0.320 0.255 0.125 0.463 oom oom oom 0.603 0.618 oom oom oom 7.7% 7.1% 7.3% 6.9% 0.804 0.774 0.964 0.907 0.326 8.2% 0.736 11.7% 13.3% 9.3% 6.5% 7.3% 19.2% 0.741 0.776 0.965 0.828 0.916 0.813 Table 15 Performance in free-form VQA tasks using Qwen2.5VL-3B. General VQA Relation Reasoning Fine grained Doc/Text Chart Method Flickr30k V7W GQA OpenImages VSR CUB DocVQA TextCaps TextVQA DUDE SROIE InfoVQA Average LLaVA-1.5-13B 0.719 0.674 0.656 0.533 0.643 0. 0.273 0.660 0.674 0.301 0.153 0.407 0. PDrop VisionZip DivPrune CDPruner VScan GlimpsePrune GlimpsePrune Average retention rate 11.1% 0.581 0.564 0.604 0.593 0.581 0.662 0.620 0.601 0.662 0.682 0.639 0.609 0.673 0.625 0.620 9.5% 9.5% 9.0% 0.691 0.673 0.677 0.163 0.551 0.539 0.556 0.240 0.580 0.602 0.517 0.219 0.582 0.562 0.513 0.585 0.630 0.218 0.514 0.530 0.247 0.600 0.604 10.0% 10.6% 9.4% 10.2% 0.273 0.623 0.595 0.523 Unlimited constraint 0.477 0.622 0.543 0.583 0.625 9.3% 0.631 0.402 0.173 0.032 0.440 0.387 0.243 0.109 0.633 0.394 0.236 0.071 0.586 0.381 0.244 0.069 0.602 0.643 0.397 0.267 0.111 9.4% 10.6% 10.0% 9.7% 0.418 0.283 0.148 0.655 0.423 0.481 0.466 0.480 0.495 9.8% 0.516 27.0% 32.8% 23.8% 31.9% 42.4% 14.3% 21.8% 17.2% 17.4% 28.9% 17.4% 26.0% 25.1% 0.521 0.273 0.687 0.625 0.605 0.302 0. 0.684 0.673 0.663 0.423 0.521 0.636 Table 16 Performance in free-form VQA tasks using LLaVA-1.5-13B. Question: What is in the sky? Standard answer: Kites Qwen2.5-VL-7B: In the sky, there are several kites . The kites appear to be of different shapes and colors, and they are flying at varying heights. The sky is clear, suggesting good day for kite flying. Score: 0.8 Retention ratio: 4.5% GlimpsePrune: In the sky, there are kites. There are at least three kites visible, each with distinct shape and color. Score: 1. Figure 6 Case 1: With only the relevant visual tokens retained, GlimpsePrune can focus more on the target object rather than other irrelevant information while answering the question correctly. 13 Question: What is on top of the building? Standard answer: clock. Qwen2.5-VL-7B: The image shows building with statue on top of it. The statue appears to be of person, possibly historical figure, given its prominent placement and the formal setting. The building itself has classical architectural style, with columns and pediment, which is typical of neoclassical architecture. Score: 0.0 Retention ratio: 1.5% GlimpsePrune: The image shows building with triangular pediment on top. On the pediment, there is clock. The clock appears to be decorative element, likely used to tell time for the buildings occupants or visitors. The pediment itself is classical architectural feature. Score: 0.7 Figure 7 Case 2: By retaining the visual tokens at the correct position, GlimpsePrune can accurately answer that there is clock on top of the building and provide relevant descriptions. Question: In 1960, what is the population of females in the age group 0-4 (millions)? Standard answer: 10 Qwen2.5-VL-7B: In 1960, the population of females in the age group 0-4 is approximately 5 million. Score: 0. Retention ratio: 3.6% GlimpsePrune: In 1960, the population of females in the age group 0-4 is approximately 10 million. Score: 1.0 Figure 8 Case 3: With the correct visual tokens retained, GlimpsePrune can answer the question related to the chart. 14 Question: What has been the greatest concern for non-smokers in 1990? Standard answer: Getting enough exercise Qwen2.5-VL-7B: The greatest concern for non-smokers in 1990, according to the table, is Getting enough exercise , with 45% of non-smokers being concerned about it. Score: 1.0 Retention ratio: 3.7% GlimpsePrune: The greatest concern for non-smokers in 1990, according to the chart, is Maintaining right weight, with 43% of non-smokers indicating this as their primary concern. Score: 0.0 Figure 9 Failure case 1: The retained visual tokens is insufficient to answer the question correctly. Question: How many categories of income mentioned in this infographic? Standard answer: 6 Qwen2.5-VL-7B: The infographic mentions five categories of income: 1. Under $25k 2. $25-$35k 3. $35-$50k 4. $50-$75k 5. Over $75-$100k 6 . Over $100k Score: 1.0 Retention ratio: 4.4% GlimpsePrune: The infographic mentions five categories of income: 1. Under $25k 2. $25-$35k 3. $35-$50k 4. $50-$75k 5 . Over $100k Score: 0.8 Figure 10 Failure case 2: The retained visual tokens is sufficient but the model fails to answer the question correctly."
        }
    ],
    "affiliations": [
        "Shanghai Innovation Institute",
        "Tianjin University",
        "VCIP, CS, Nankai University",
        "vivo Mobile Communication Co., Ltd"
    ]
}