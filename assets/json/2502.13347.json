{
    "paper_title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
    "authors": [
        "Shi Yu",
        "Zhiyuan Liu",
        "Chenyan Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM."
        },
        {
            "title": "Start",
            "content": "CRAW4LLM: Efficient Web Crawling for LLM Pretraining Shi Yu12* Zhiyuan Liu1 Chenyan Xiong2 1Department of Computer Science and Technology, Tsinghua University 2School of Computer Science, Carnegie Mellon University yus21@mails.tsinghua.edu.cn; liuzy@tsinghua.edu.cn; cx@cs.cmu.edu 5 2 0 2 9 1 ] . [ 1 7 4 3 3 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Web crawl is main source of large language models (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents CRAW4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of webpage in LLM pretraining as the priority score of the web crawlers scheduler, replacing the standard graph-connectivity-based priority. Our experiments on web graph containing 900 million webpages from commercial search engines index demonstrate the efficiency of CRAW4LLM in obtaining highquality pretraining data. With just 21% URLs crawled, LLMs pretrained on CRAW4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM."
        },
        {
            "title": "Introduction",
            "content": "Massive in size and diverse in topics, web data usually serve as the primary source of pretraining data for large language models (LLMs), providing an extensive and heterogeneous corpus that captures wide spectrum of human knowledge and real-world information (Baack, 2024; Dubey et al., 2024; Penedo et al., 2024). Pretraining datasets are typically built from large-scale web crawls such as Common Crawl (CommonCrawl, 2007), which may contain TBs of data spanning billions of webpages (Penedo et al., 2024; Weber et al., 2024). Despite their vast scale, most of the data collected from web crawls are not used in the pretraining of LLMs. Existing work often discards over 90% of the raw data collected from the web (Li et al., 2024; Penedo et al., 2024; Tang et al., *Work done while visiting Carnegie Mellon University. 1 Figure 1: Graph traverse process of traditional graphconnectivity-based crawler (green) and CRAW4LLM (red) starting from same seed URL (star). 2024), highlighting the inefficiency of current web crawlers in collecting LLM pretraining data. Common web crawlers like Common Crawl prioritize pages based on graph connectivity metrics like PageRank (Page et al., 1999; Cho et al., 1998) or harmonic centrality (Boldi and Vigna, 2014; Baack, 2024), which favor documents with high number of inlinks (indegree) (Fortunato et al., 2008) rather than those most relevant for pretraining. This misalignment not only leads to waste in computational resources during excessive data processing for LLM developers, but also incentivizes overcrawling, which burdens website operators with redundant traffic and increases ethical and legal risks related to fair use of data and copyright (Longpre et al., 2024; New York Times, 2023). To bridge this gap, we propose Web Crawling for LLM Pretraining (CRAW4LLM). Instead of relying on traditional graph-connectivity-based signals, CRAW4LLM improves crawling efficiency by prioritizing webpages based on their influence on LLM pretraining. Specifically, during each crawling iteration, all newly discovered documents are scored with pretraining influence scorer derived from data-filtering pipelines for pretraining (Li et al., 2024; Penedo et al., 2024), and docuAlgorithm 1 CRAW4LLM Algorithm Input: Seed URLs Useed, number of pages to be crawled , number of pages to be crawled in each iteration n, pretraining influence scorer M() Pc FETCHPAGES(Uc) Output: Crawled page set 1: Initialize URL and score priority queue 2: Initialize crawled page set 3: Initialize visited URL set Useed 4: Uc Useed 5: while do 6: 7: Merge Pc into 8: 9: 10: 11: 12: 13: 14: 15: 16: end while 17: return Uout EXTRACTURLS(Pc) for all Uout do if / then end for Uc DEQUEUE(Q, n) ENQUEUE(Q, v, SCORE_URL(v; M)) ADD(V, v) end if ments influence for pretraining. can be derived from data classification models for pretraining data, which have been used to decide whether document should be retained in or filtered out from the raw dataset (Li et al., 2024; Penedo et al., 2024). Formally, given pretraining influence scorer M, the score of URL is calculated as SCORE_URL(u; M) = M(FETCHPAGE(u)), (1) where FETCHPAGE(u) gets the page content of and M() returns the score. Once all outlinks have been scored, following the standard procedures of existing crawlers, they are inserted into priority queue, which automatically orders them based on their scores. The top highest-scoring URLs are then dequeued for pretraining and serve as the sources for the next round of crawling. This process repeats until documents have been collected, forming the final pretraining dataset P. In contrast, traditional crawlers typically rely on graph connectivity metrics, such as PageRank (Cho et al., 1998) and harmonic centrality (Baack, 2024), which basically assign higher priority to pages with higher indegrees (Fortunato et al., 2008). As shown in Figure 2(a), the indegrees of webpages exhibit poor correlation with the scores assigned by the DCLM fastText classifier, pretraining influence scorer for identifying high-quality pretraining data (Li et al., 2024). This confirms that graph connectivity-based crawlers are inefficient in crawling pretraining data. By incorporating pretraining influence scorer, CRAW4LLM traverses the web graph in way 2 (a) Pretraining (-0.11). (b) PageRank (0.88). Figure 2: Correlations between pretraining influence scores from DCLM fastText (Li et al., 2024) and PageRank to indegrees, on randomly sampled ClueWeb22-B documents (Overwijk et al., 2022). Spearman correlation coefficients are reported in parentheses. ments with the highest scores are used to discover new documents. By prioritizing webpages with high influence scores, as illustrated in Figure 1, CRAW4LLM explores the web graph in fundamentally different manner from traditional graphconnectivity-based crawlers, uncovering distinct subset of the web more useful for pretraining. We conduct large-scale crawling simulations on ClueWeb22-A (Overwijk et al., 2022), snapshot of the web containing 900 million English webpages obtained from the central index of commercial search engine. Results show that, by crawling only 1 of the pretraining dataset size, CRAW4LLM can outperform traditional crawlers which collect 1, 2, and 4 more data followed by data selection. Compared to the baseline crawler that achieves the same performance, CRAW4LLM crawls only 21% of the webpages. Further analysis reveals that during crawling, CRAW4LLM quickly discovers documents that align with the oracle selection, which crawls the full web graph. As result, it achieves 95% of the oracle performance while crawling only 2.2% of the data."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we introduce Web Data Crawling for LLM Pretraining (CRAW4LLM), an efficient crawling method that integrates LLM pretraining preference into the crawler. The algorithm of CRAW4LLM is presented in Algorithm 1. Similar to traditional crawlers (Cho et al., 1998), CRAW4LLM starts with set of seed URLs. For each unvisited outlink of them, CRAW4LLM assigns score using pretraining-oriented URL scoring function SCORE_URL(; M), where is pretraining influence scorer which rates docuCrawling Method Selection Pool Size Commonsense Reasoning (4 tasks) Language Understanding (6 tasks) Reading Comprehension (3 tasks) Symbolic Problem Solving (5 tasks) World Knowledge (5 tasks) Core % of (23 tasks) Oracle Oracle Selection (Upper Bound): Random sample from the top 10% rated data from ClueWeb22 using DCLM fastText for pretraining n.a. 45 0.2438 0.2209 0.1483 0.2039 0. 0.2239 100% Crawl-then-Select: Crawl 1 and 2 more data from ClueWeb22 and select top-rated 1 data using DCLM fastText for pretraining Random Indegree 1 2 1 2 0.1906 0.1896 0.1730 0.1845 0.1890 0.1967 0.1680 0.1856 0.0244 0.1260 0.0326 0.0970 0.1834 0.2000 0.1616 0.1958 0.1930 0.2024 0.1668 0.1953 0.1748 0.1964 0.1556 0. 78.1% 87.7% 69.5% 83.3% Ours: Crawl 1 data using CRAW4LLM for pretraining CRAW4LLM 1 0.2116 0.2311 0. 0.1979 0.2486 0.2133 95.3% Table 1: Downstream LLM performance. All models are pretrained on 1 data, which corresponds to 20M documents and 32.9B tokens. The evaluation metric is centered accuracy (0 = random guess) (Li et al., 2024). Best/2nd best in the last two groups are bolded/underlined. See Appendix for detailed results. that prioritizes high-quality pretraining documents. This makes the crawling more efficient and enables the discovery of documents dramatically different with connectivity-based crawlers."
        },
        {
            "title": "3 Experimental Methodology",
            "content": "In this section, we introduce our experimental setup, with details on the crawler implementation and LLM training provided in Appendix and B. CRAW4LLM. To run experiments in our limited computational budget, we run simulation of CRAW4LLM on the ClueWeb22 dataset (Overwijk et al., 2022), snapshot of the web with graph information from commercial crawler. We use the English subset of ClueWeb22-A, which is web graph containing 900M webpages with links. We randomly sampled 10K URLs as our seed URLs. We set the number of total crawled documents to 20M and crawled documents each iteration to 10K. We use the DCLM fastText classifier (Li et al., 2024) as the pretraining influence scorer M(). emulate traditional Baselines. We graphconnectivity-based crawlers by replacing the LLM-oriented URL scoring function (Eq. 1) with function that returns the indegree for given URL, since nodes indegree closely correlates with PageRank, common graph connectivity metric, as shown in Figure 2(b) and previous findings (Fortunato et al., 2008). We also introduce random crawling baseline, where the scorer assigns random scores. We run both of them in crawl-then-select setting, first crawling 1 or 2 more documents and then selecting the top 1 (20M) documents based on scores assigned by the DCLM fastText classifier. This process mimics existing data-filtering pipelines, which begin with crawled documents and then apply filtering (Li et al., 2024; Penedo et al., 2024). Oracle. We also introduce an oracle selection run in which we directly apply the DCLM fastText classifier to the entire ClueWeb22-A document set and select the top 10% documents for pretraining, serving as the upper bound. LLM Training and Evaluation. For all runs, we use the final set of 20M crawled or selected documents to pretrain 411M Transformer on 4 Chinchilla-optimal tokens (Hoffmann et al., 2022), totaling 32.9B tokens. The pretraining is conducted using the DCLM codebase (Li et al., 2024). To evaluate the pretrained LLMs, we follow the DCLM evaluation recipe, assessing performance on 23 (22 unique) core tasks."
        },
        {
            "title": "4 Evaluation Results",
            "content": "In this section, we first present the overall performance of CRAW4LLM (Sec. 4.1), followed by further analysis (Sec. 4.2)."
        },
        {
            "title": "4.1 Overall Performance",
            "content": "In this experiment, we compare the performance of CRAW4LLM with baseline crawlers by evaluating LLMs trained on their respective crawled data. As shown in Table 1, when all methods crawl the same amount of training data (1), CRAW4LLM significantly outperforms random crawling and indegree crawling. In the crawl-then-select setting, where traditional crawlers are allowed to collect twice as much data (2) for later selection, they still underperform compared to CRAW4LLM. This suggests that incorporating pretraining-oriented signals early in the crawling process is more beneficial than relying on post-selection. With only 1 of the data, CRAW4LLM retains 95% of the performance 3 (a) Extended crawling. (b) Visited documents. Figure 3: Efficiency of crawlers. (a) shows the performance of LLMs trained on selected data crawled by CRAW4LLM and extended baseline crawlers. (b) presents the number of crawled (P) and visited (V) documents for CRAW4LLM, along with the estimated number of crawled documents required for indegreebased crawler to match CRAW4LLMs performance. achieved by the oracle run, which directly selects from substantially larger 45 data pool. In Section 4.2, we further analyze the efficiency of CRAW4LLM compared to traditional crawlers and explore the reasons behind it."
        },
        {
            "title": "4.2 Analysis",
            "content": "Crawling Efficiency. We evaluate the efficiency of CRAW4LLM by comparing the number of documents it crawls or visits against baseline crawlers. As shown in Figure 3(a), even when the baselines crawl 4 the required pretraining data for selection, they still underperform compared to CRAW4LLM. Extrapolation suggests that the indegree-based crawler would need to process 4.8 more documents (96M) to match CRAW4LLMs performance. Figure 3(b) further illustrates that CRAW4LLM achieves the same performance while crawling only 21% of the documents required by the indegreebased crawler, or 48% when considering all visited documents. These results highlight the efficiency of CRAW4LLM, demonstrating its potential to reduce website burdens and mitigate over-crawling. Document Coverage. In this experiment, we plot the precision and recall of the oracle-selected documents among those crawled by CRAW4LLM and baseline crawlers throughout the crawling process. As shown in Figure 4, the precision quickly reaches 1.0, while the recall increases linearly, aligning with the theoretical upper bound. The saturated performance remains until 13 million documents have been crawled, after which the performance starts to decline, likely due to the lack of connectivity of the ClueWeb22 subgraph. In contrast, baseFigure 4: Precision (left) and recall (right) of the oracle documents among the documents crawled by CRAW4LLM, indegree, and random crawler. The upper bound represents always crawling the oracle documents. (a) 1-hop (0.61). (b) 2-hop (0.60). Figure 5: Correlations between the pretraining influence scores of the documents themselves and the average scores of their 1and 2-hop outlink documents. Spearman correlation coefficients are reported in parentheses. line crawlers exhibit minimal overlap with oracleselected data, verifying that most of their crawled content is misaligned with pretraining needs and should be filtered (Li et al., 2024; Penedo et al., 2024). These results emphasize the importance of targeted crawling strategies for pretraining. Score Correlations Across Links. CRAW4LLM tracks the outlinks of the highest-scored documents in the current iteration to enrich the queue for future crawls. As shown in Figure 5, we plot the correlations between the pretraining influence scores of current documents and their 1and 2-hop outlinks. The results indicate correlation in influence scores across link hops, suggesting that highly-rated documents are interconnected and can be discovered through previously crawled documents."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents CRAW4LLM, step toward more efficient and responsible web crawling for LLM pretraining. By prioritizing documents based on the pretraining needs, our method improves 4 crawling efficiency and reduces unnecessary crawling, easing the burden on web hosts. While fair use of web data remains critical challenge, we hope that CRAW4LLM can help mitigate these concerns and promote more compliant and sustainable practices in obtaining pretraining data for LLMs."
        },
        {
            "title": "Limitations",
            "content": "Web crawling raises important concerns regarding copyright and the fair use of web data (Longpre et al., 2024), necessitating better solution from the entire LLM community, such as sharing benefits with website owners. In this paper, we propose more efficient crawling method that mitigates these challenges by reducing crawling, though it does not fully resolve them. Our experiments are conducted on web graph dataset ClueWeb22 (Overwijk et al., 2022), thereby avoiding issues associated with actual web crawling. We hope that future advancements in web crawling will better align with ethical and legal standards. While our crawling simulation is sufficient research setup, further validation is required to assess the effectiveness of CRAW4LLM in realworld crawling scenarios. Our CRAW4LLM and baseline crawlers implement only the selection policy (Cho et al., 1998) of crawler, which determines which pages to crawl. Although we try to mimic real-world crawling procedures used in systems like Apache Nutch1, we do not implement other web crawling policies in industriallevel crawlers, such as the re-visit policy (Cho and Garcia-Molina, 2003a), politeness policy (Cho and Garcia-Molina, 2003b), and parallelization policy (Cho and Garcia-Molina, 2002). We leave the integration of CRAW4LLM into real-world crawling engines like Nutch and comprehensive comparison between CRAW4LLM and traditional crawling methods in real-world crawling scenarios for future work."
        },
        {
            "title": "References",
            "content": "Stefan Baack. 2024. critical analysis of the largest source for generative AI training data: Common crawl. In FAccT, pages 21992208. ACM. Paolo Boldi and Sebastiano Vigna. 2014. Axioms for centrality. Internet Math., 10(3-4):222262. Junghoo Cho and Hector Garcia-Molina. 2002. Parallel crawlers. In WWW. 1https://nutch.apache.org/ Junghoo Cho and Hector Garcia-Molina. 2003a. Effective page refresh policies for web crawlers. ACM Trans. Database Syst., 28(4):390426. Junghoo Cho and Hector Garcia-Molina. 2003b. Estimating frequency of change. ACM Trans. Internet Techn., 3(3):256290. Junghoo Cho, Hector Garcia-Molina, and Lawrence Page. 1998. Efficient crawling through URL ordering. Comput. Networks, 30(1-7):161172. CommonCrawl. 2007. Common crawl. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Santo Fortunato, Marián Boguñá, Alessandro Flammini, and Filippo Menczer. 2008. Approximating pagerank from in-degree. In WAW. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training computeoptimal large language models. In NeurIPS. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee F. Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. 2024. Datacomp-lm: In search of the next generation of training sets for language models. In NeurIPS. Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Shamiso Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad A. Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kushagra Tiwary, Lester James V. Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Minh Chien Vu, Xuhui 5 Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, and Alex Pentland. 2024. Consent in crisis: The rapid decline of the AI data commons. In NeurIPS. New York Times. 2023. Complaint, the new york times company v. microsoft corporation, openai, inc., openai lp, openai gp, llc, openai llc, openai opco llc, openai global llc, oai corporation, llc, and openai holdings, llc. Case 1:23-cv-11195, United States District Court, Southern District of New York. Arnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, and Jamie Callan. 2022. Clueweb22: 10 billion web documents with visual and semantic information. arXiv preprint arXiv:2211.15848. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking : Bringing order to the web. In The Web Conference. Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin A. Raffel, Leandro von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. In NeurIPS. Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, and Eric P. Xing. 2024. Txt360: top-quality llm pre-training dataset requires the perfect blend. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. In NeurIPS."
        },
        {
            "title": "A Details on Crawling",
            "content": "Our implementation of the indegree-based crawler employs static URL scoring function, which directly returns the indegree of given URL based on the full ClueWeb22 graph (Sec. 3). For realworld crawlers, as the true indegree value of URL cannot be known in advance, local graph must be maintained to track the known inlinks of discovered URLs. This local graph is updated iteratively as the discovered portion of the web expands during the crawling process (Cho et al., 1998). Maintaining such local graph during crawling introduces significant computational overhead. For simplicity, we instead implement the static simulation, where we directly return the global indegree Hyper-parameter Value nlayers nheads dmodel dhead Warmup Learning Rate Weight Decay z-loss Global Batch Size Sequence Length 24 8 1,024 128 2,000 3e-3 0.033 1e-4 512 2048 Table 2: Model and training hyper-parameters. nlayers, nlayers, dmodel, and dhead denote the number of layers, attention heads, width, and width per attention head, respectively. of each URL. We believe that this simplified implementation does not underperform compared to realworld implementations, as our approach leverages global information from the entire graph, which should be better than the partial information from the local graph. We run our simulated crawlers on Linux server equipped with two Intel(R) Xeon(R) E5-2630 v3 CPUs (8 cores per socket, 16 cores in total, 1 thread per core), 125GiB of memory, and an SSD. crawl of 20 million documents takes approximately one day to complete."
        },
        {
            "title": "Evaluation",
            "content": "We pretrain 411M-parameter2 decoder-only Transformer model using the DCLM training recipe (Li et al., 2024)3. The hyper-parameters are presented in Tabel 2. To enhance training stability, we extend the original 411M-1x setting to 411M4x, meaning the model is trained on 4 times the Chinchilla-optimal number of tokens (Hoffmann et al., 2022), which amounts to 32.9B tokens. The training process takes 1 day and 12 hours on 8 NVIDIA L40S GPUs. For further details, please refer to the DCLM paper (Li et al., 2024). Due to computational constraints, each pretraining experiment is conducted only once. We use the DCLM evaluation recipe (Li et al., 2024) to evaluate model performance on 23 (22 unique) core tasks. 2Sometimes referred to as 400M in the DCLM paper (Li et al., 2024). 3https://github.com/mlfoundations/dclm"
        },
        {
            "title": "C Detailed Results",
            "content": "The raw (uncentered) accuracy of all evaluation tasks is presented in Table 3, 4, 5, 6, and 7. Please refer to Li et al. (2024) for more details on the evaluation tasks. The ClueWeb22 Dataset ClueWeb22 (Overwijk et al., 2022) is distributed under TREC-style license for research purpose. The dataset can be obtained by signing data license agreement with Carnegie Mellon University4. We use ClueWeb22 only for research purpose."
        },
        {
            "title": "E Use of AI Assistants",
            "content": "We use GitHub Copilot5 to assist with coding and ChatGPT6 (powered by GPT-4o) to enhance the writing of this paper. 4https://lemurproject.org/clueweb22/obtain.php 5https://github.com/features/copilot 6https://chatgpt.com/"
        },
        {
            "title": "Method Pool Size CommonsenseQA COPA OpenBookQA PIQA",
            "content": "Oracle Selection (Upper Bound) n.a. 45 Crawl-then-Select Random 1 Random 2 Random 4 1 Indegree 2 Indegree 4 Indegree Ours CRAW4LLM 1 0.2850 0. 0.3300 0.6812 0.2072 0.2588 0.2326 0.3219 0.1966 0.2088 0.6700 0.6200 0.6400 0.6000 0.6600 0.6400 0.2980 0.3160 0.3380 0.2780 0.3040 0.3400 0.6746 0.6785 0.6757 0.6513 0.6752 0. 0.2277 0.6600 0.3300 0.6926 Table 3: Results for commonsense reasoning tasks."
        },
        {
            "title": "Language Understanding",
            "content": "Method Pool Size BIG-Bench Lang. Id. HellaSwag (zero-shot) HellaSwag LAMBADA Winograd Winogrande Oracle Selection (Upper Bound) n.a. 45 Crawl-then-Select Random 1 Random 2 Random 4 1 Indegree 2 Indegree 4 Indegree Ours CRAW4LLM 1 0. 0.2490 0.2468 0.2521 0.2566 0.2547 0.2562 0.2544 0.3856 0.3709 0.3882 0.4011 0.3515 0.3749 0.3994 0.4035 0. 0.4432 0.6557 0.5130 0.3716 0.3925 0.4019 0.3519 0.3771 0.4008 0.3990 0.4073 0.4390 0.3596 0.3773 0.4159 0.6044 0.6007 0.6154 0.5971 0.5861 0. 0.5146 0.5130 0.5130 0.5004 0.5241 0.5178 0.4048 0.4196 0.6593 0.5288 Table 4: Results for language understanding tasks."
        },
        {
            "title": "Crawling Selection Reading Comprehension\nMethod Pool Size BoolQ CoQA SQuAD",
            "content": "Oracle Selection (Upper Bound) n.a. 45 Crawl-then-Select Random 1 Random 2 Random 4 1 Indegree 2 Indegree 4 Indegree Ours CRAW4LLM 1 0.5755 0. 0.3139 0.5080 0.5807 0.5911 0.5324 0.5697 0.5765 0.1799 0.2053 0.2361 0.1666 0.1843 0.2147 0.1882 0.2759 0.2951 0.1616 0.2390 0.2736 0.5440 0. 0.2215 Table 5: Results for reading comprehension tasks. 8 Crawling Selection Symbolic Problem Solving Method Pool Size AGI Eval LSAT-AR BIG-Bench CS Algorithms BIG-Bench Dyck Lang. BIG-Bench Operators BIG-Bench Repeat Copy Logic Oracle Selection (Upper Bound) n.a. 45 Crawl-then-Select Random 1 Random 2 Random 4 1 Indegree 2 Indegree 4 Indegree Ours CRAW4LLM 1 0.2739 0.2391 0.2696 0.1957 0.2304 0.2609 0. 0.2696 0.4341 0.4568 0.4538 0.4568 0.4371 0.4235 0.4538 0.4371 0.2160 0.1970 0.2520 0.2600 0.1900 0.2340 0. 0.1620 0.2143 0.2143 0.1762 0.1857 0.1429 0.2143 0.1667 0.2095 0.0625 0.0000 0.0313 0.0625 0.0000 0.0313 0. 0.0938 Table 6: Results for symbolic problem solving tasks."
        },
        {
            "title": "World Knowledge",
            "content": "Method Pool Size ARC Easy ARC Challenge BIG-Bench-Bench QA Wikidata"
        },
        {
            "title": "Jeopardy MMLU",
            "content": "Oracle Selection (Upper Bound) n.a. 45 Crawl-then-Select Random 1 Random 2 Random 4 1 Indegree 2 Indegree 4 Indegree Ours CRAW4LLM 1 0.5951 0. 0.5152 0.5425 0.5577 0.4857 0.5248 0.5749 0.2799 0.2807 0.2867 0.2509 0.2790 0.2935 0.6103 0.3208 0.4945 0.5186 0.5081 0.5126 0.4888 0.5205 0. 0.5143 0.1176 0.2805 0.0461 0.0648 0.0970 0.0138 0.0555 0.0959 0.2552 0.2561 0.2543 0.2618 0.2464 0.2430 0. 0.2661 Table 7: Results for world knowledge tasks."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University",
        "School of Computer Science, Carnegie Mellon University"
    ]
}