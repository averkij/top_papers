{
    "paper_title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
    "authors": [
        "Chia-Yu Hung",
        "Navonil Majumder",
        "Zhifeng Kong",
        "Ambuj Mehrish",
        "Rafael Valle",
        "Bryan Catanzaro",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 7 3 0 1 2 . 2 1 4 2 : r TANGOFLUX: SUPER FAST AND FAITHFUL TEXT TO AUDIO GENERATION WITH FLOW MATCHING AND CLAP-RANKED PREFERENCE OPTIMIZATION Chia-Yu Hung1 Navonil Majumder1 Zhifeng Kong2 Ambuj Mehrish1 Rafael Valle2 Bryan Catanzaro Soujanya Poria1 1Singapore University of Technology and Design (SUTD) 2NVIDIA {chiayu hung, navonil majumder, ambuj mehrish, sporia}@sutd.edu.sg {zkong, rafaelvalle, bcatanzaro}@nvidia.com"
        },
        {
            "title": "TANGOFLUX Resources",
            "content": "Website https://tangoflux.github.io Code Repository https://github.com/declare-lab/TangoFlux Pretrained Model https://huggingface.co/declare-lab/TangoFlux Dataset Fork https://huggingface.co/datasets/declare-lab/CRPO Interactive Demo https://huggingface.co/spaces/declare-lab/TangoFlux"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce TANGOFLUX, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on single A40 GPU. key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), novel framework that iteratively generates and optimizes 1 preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TANGOFLUX achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Audio plays vital role in daily life and creative industries, from enhancing communication and storytelling to enriching experiences in music, sound effects, and podcasts. However, creating highquality audio, such as foley effects or music compositions, demands significant effort, expertise, and time. Recent advancements in text-to-audio (TTA) generation (Majumder et al., 2024; Ghosal et al., 2023; Liu et al., 2023; 2024b; Xue et al., 2024; Vyas et al., 2023; Huang et al., 2023b;a) and offer transformative approach, enabling the automatic creation of diverse and expressive audio content directly from textual descriptions. This technology holds immense potential to streamline audio production workflows and unlock new possibilities in multimedia content creation. However, many existing models face challenges with controllability, occasionally struggling to fully capture the details in the input prompts, especially when the prompts are complex. This can sometimes result in generated audio that omits certain events or diverges from the user intent. At times, the generated audio may even contain input-adjacent, but unmentioned and unintended, events, that could be characterized as hallucinations. In contrast, the recent advancements in Large Language Models (LLMs) (Ouyang et al., 2022) have been significantly driven by the alignment stage after pre-training and supervised fine-tuning. This alignment stage, often leveraging reinforcement learning from human feedback (RLHF) or other reward-based optimization methods, endows the generated outputs with human preferences, ethical considerations, and task-specific requirements (Ouyang et al., 2022). Despite the rapid progress in TTA models, until recently (Majumder et al., 2024) alignment, that could mitigate the aforementioned issues with audio outputs, has not been mainstay in TTA model training. One critical challenge in implementing alignment for TTA models lies in the creation of preference pairs. Unlike LLM alignment, where off-the-shelf reward models (Lambert et al., 2024a;b) and human feedback data or verifiable gold answers are available, TTA domain as yet lacks such tooling. For instance, in general, LLM alignment settings, such as safety or instruction following, tools exist for categorizing specific safety risks (Inan et al., 2023). Frontier LLMs like GPT-4 (OpenAI et al., 2024) are often used directly to judge the candidate outputs (Zheng et al., 2023). While audio language models (Chu et al., 2024; 2023; Tang et al., 2024) can process audio inputs and generate textual outputs, they often produce noisy feedback, unfit for preference pair creation for audio. BATON (Liao et al., 2024) employs human annotators to assign binary score of 0 or 1 to each audio sample based on its alignment with given prompt. However, such labor-intensive manual approach is often economically not viable at large scale. To address these issues, we propose CLAP-Ranked Preference Optimization (CRPO), simple yet effective approach to generate audio preference data and perform preference optimization on rectified flows. As shown in Fig. 1, CRPO consists of iterative cycles of data sampling, generating preference pairs, and performing preference optimization, resembling self-improvement algorithm. We first demonstrate that the CLAP model (Wu* et al., 2023) can serve as proxy reward model for ranking generated audios by alignment with the text description. Using this ranking, we construct an audio preference dataset that yields superior performance after preference optimization, as compared to other audio preference datasets, such as, BATON and Audio-Alpaca (Majumder et al., 2024). Finally, we demonstrate the effectiveness of this iterative optimization, emphasizing the importance of each component, including the modified loss function compared to conventional preference optimization loss. Additionally, many TTA models are trained on proprietary data (Evans et al., 2024b;a; Copet et al., 2024), with their weights often unavailable to the public or accessible only through private APIs, posing challenges for public use and foundational research. Moreover, the diffusion-based TTA models (Ghosal et al., 2023; Majumder et al., 2024; Liu et al., 2024b) are known to require too many denoising steps to generate decent output, consuming much GPU compute and time. 2 Figure 1: depiction of the overall training pipeline of TANGOFLUX. To address this, we introduce TANGOFLUX, trained on completely non-proprietary dataset, achieving state-of-the-art performance on benchmarks and out-of-distribution prompts, despite its smaller size. Unlike conventional TTA models, TANGOFLUX supports variable-duration audio generation up to 30 seconds with blazing inference speed of 3.7 seconds on single A40 GPU. This is achieved by the use of transformer (Vaswani et al., 2023) backbone that undergoes pretraining, finetuning, and preference optimization with rectified flow matching as training objectiveenabling good quality audio output guided by much fewer sampling steps tracing the almost straight path between the noise and output audio. Our contributions: (i) We introduce TANGOFLUX, small and fast TTA model based on rectified flow that achieves state-of-the-art performance for fully non-proprietary training data. (ii) We propose CRPO, simple and effective strategy to generate audio preference data and align rectified flow, demonstrating its superior performance over other audio preference datasets. (iii) We conduct extensive experiments and highlight the importance of each component of CRPO in aligning rectified flows for improving scores on benchmarks. (iv) We publicly release the code and model weights to foster research on text-to-audio generation."
        },
        {
            "title": "2 METHOD",
            "content": "TANGOFLUX consists of FluxTransformer blocks which are Diffusion Transformer (DiT) (Peebles & Xie, 2023) and Multimodal Diffusion Transformer (MMDiT) (Esser et al., 2024), conditioned on textual prompt and duration embedding to generate audio at 44.1kHz up to 30 seconds. TANGOFLUX learns rectified flow trajectory from audio latent representation encoded by variational autoencoder (VAE) (Kingma & Welling, 2022). TANGOFLUX training pipeline consists of three stages: pre-training, fine-tuning then preference optimization. TANGOFLUX is aligned via CRPO which iteratively generates new synthetic data and constructs preference pairs to perform preference optimization. The overall pipeline is depicted in Fig. 1. 2.1 AUDIO ENCODING We use the VAE from Stable Audio Open (Evans et al., 2024c), which is capable of encoding stereo audio waveforms at 44.1kHz into audio latent representations. Given stereo audio R2dsr with as the duration and sr as the sampling rate, the VAE encodes into latent representation RLC, with L, being the latent sequence length and channel size, respectively. The VAE decodes the latent representation back into the original stereo audio X. The entire VAE is kept frozen during TANGOFLUX training."
        },
        {
            "title": "2.2 MODEL CONDITIONING",
            "content": "To enable the controllable generation of audio of varying lengths, we employ textual conditioning and duration conditioning. Textual conditioning controls the event present of the generated audio based on provided description, while duration conditioning specifies the desired audio length, up to maximum of 30 seconds. Textual Conditioning. Given the textual description of an audio, we obtain the text encoding ctext from pretrained text-encoder. Given the strong performance of FLAN-T5 (Chung et al., 2022; Raffel et al., 2023) as conditioning in text-to-audio generation (Majumder et al., 2024; Ghosal et al., 2023), we select FLAN-T5 as our text encoder. Duration Encoding. Inspired by the recent works (Evans et al., 2024c;a;b), to generate audios with variable length, we firstly use small neural network to encode the audio duration into duration embedding cdur. This is concatenated with the text encoding ctext and fed into TANGOFLUX to control the duration of audio output."
        },
        {
            "title": "2.3 MODEL ARCHITECTURE",
            "content": "Following the recent success of FLUX models in image generation 1, we adopt hybrid MMDiT and DiT architecture as the backbone for TANGOFLUX. While MMDiT blocks demonstrated strong performance, simplifying some of them into single DiT block improved scalability and parameter efficiency 2. These lead us to select model architecture consisting of 6 blocks of MMDiT, followed by 18 blocks of DiT. Each block uses 8 attention heads, with each attention head dimension of 128, resulting in width of 1024. This configuration results in model with 515M parameters. 2.4 FLOW MATCHING Several generative models have been successfully trained under the diffusion framework (Ho et al., 2020; Song et al., 2022; Liu et al., 2022). However, this approach is known to be sensitive to the choice of noise scheduler, which may significantly affect performance. In contrast, the flow matching (FM) framework (Lipman et al., 2023; Albergo & Vanden-Eijnden, 2023) has been shown to be more robust to the choice of noise scheduler, making it preferred choice in many applications, including text-to-audio (TTA) and text-to-speech (TTS) tasks (Liu et al., 2024a; Le et al., 2023; Vyas et al., 2023). Flow matching builds upon the continuous normalizing flows framework (Onken et al., 2021). It generates samples from target distribution by learning time-dependent vector field that maps samples from simple prior distribution (e.g., Gaussian) to complex target distribution. Prior work in TTA, such as AudioBox (Vyas et al., 2023) and Voicebox (Le et al., 2023), has predominantly adopted the Optimal Transport conditional path proposed by (Lipman et al., 2023). However, in our approach, we utilize rectified flows (Liu et al., 2022) instead, which is straight line path from noise to distribution, corresponding to the shortest path. Rectified Flows. Given latent representation of an audio sample x1, noise sample x0 (0, I), time-step [0, 1], we can construct training sample xt where the model learns to predict velocity vt = dxt dt that guides xt to x1. While there exist several methods of constructing transport path xt , we used rectified flows (RFs) (Liu et al., 2022), in which the forward process are straight paths between target distribution and noise distribution, defined in Eq. (1). It was empirically demonstrated that rectified flows are sample efficient and degrade less compared to other formulations when reducing lesser number of sampling steps (Esser et al., 2024). We use θ to denote the model us parameter. The model directly regresses the predicted velocity u(xt, t; θ) against the ground truth velocity vt where the loss is shown in Eq. (2). xt = (1 t)x1 + tx0, vt = dxt dt = x0 x1, (1) 1https://blackforestlabs.ai/ 2https://blog.fal.ai/auraflow/ 4 LFM = Ex1,x0,t u(xt, t; θ) vt2 . (2) Inference. During inference, we sample noise from prior distribution x0 (0, I) and use an ordinary differential equation solver to compute x1, based on the model-predicted velocity vt at each time step t. We use the Euler solver for this process."
        },
        {
            "title": "2.5 CLAP-RANKED PREFERENCE OPTIMIZATION (CRPO)",
            "content": "CLAP-Ranked Preference Optimization (CRPO) leverages text-audio joint-embedding model like CLAP (Wu* et al., 2023) as proxy reward model to rank the generated audios by similarity with the input description and subsequently construct the preference pairs. We firstly set pre-trained checkpoint of TANGOFLUX architecture as the base model to align, denoted by π0. Thereafter, CRPO iteratively aligns checkpoint πk := u(; θk) into checkpoint πk+1, starting from = 0. Each of such alignment iterations consists of three steps: (i) batched online data generation, (ii) reward estimation and preference dataset creation, and (iii) fine-tuning πk into πk+1 via direct preference optimization. This approach to aligning rectified flow is inspired by few LLM alignment approaches (Zelikman et al., 2022; Kim et al., 2024a; Yuan et al., 2024; Pang et al., 2024). However, there are key distinctions to our work: (i) we focus on aligning rectified flows for audio generation, rather than autoregressive language models; (ii) while LLM alignment benefits from numerous off-the-shelf reward models (Lambert et al., 2024b), which facilitate the construction of preference datasets based on reward scores, LLM judged outputs, or programmatically verifiable answers, the audio domain lacks such models or method for evaluating audio. We demonstrate that the CLAP model can serve as an effective proxy audio reward model, enabling the creation of preference datasets (see Section 4.3). Finally, we highlight the necessity of generating online data at every iteration, as iterative optimization on offline data leads to quicker performance saturation and subsequent degradation. 2.5.1 CLAP AS REWARD MODEL CLAP reward score is calculated as the cosine similarity between textual and audio embeddings encoded by the model. Thus, we assume that CLAP can serve as reasonable proxy reward model for evaluating audio outputs against the textual description. In Section 4.3, we demonstrate that using CLAP as judge to choose the best-of-N inferred policies improves performance in terms of objective metrics. 2.5.2 BATCHED ONLINE DATA GENERATION To construct preference dataset at iteration k, we first sample set of prompts Mk from larger pool B. Subsequently, we generate audios for each prompt yi Mk using πk and use CLAP3 (Wu* et al., 2023) to rank those audios by similarity with yi. For each prompt yi, we select the highest-rewarded or -ranking audio xw as the loser, yielding preference dataset Dk = {(xw as the winner and the lowest-rewarded audio xl i, yi) yi Mk}. , xl 2.5.3 PREFERENCE OPTIMIZATION Direct preference optimization (DPO) (Rafailov et al., 2024c) is shown to be effective at instilling human preferences in LLMs (Ouyang et al., 2022). Consequently, DPO is successfully translated into DPO-Diffusion (Wallace et al., 2023) for alignment of diffusion models. The DPO-diffusion loss is defined as LDPO-Diff = (xw 0 ,xl 0)D log σ (cid:32) βE 1:T pθ (xw xw 1:T xw 0 ),xl 1:T pθ (xl 1:T xl 0) (cid:34) log pθ(xw pref(xw 0:T ) 0:T ) log pθ(xl pref(xl 0:T ) 0:T ) (cid:35)(cid:33) (3) 3https://huggingface.co/lukewys/laion_clap/blob/main/ 630k-audioset-best.pt 5 = En,ϵw,ϵl log σ(cid:0)β(ϵw ϵθ(xw )2 2 ϵw ϵref(xw )2 2 (ϵl ϵθ(xl n)2 2 ϵl ϵref(xl n)2 2))(cid:1) (4) After some algebraic simplification of Eq. (3), as shown by Wallace et al. (2023), LDPO-Diff reduces to tractable term shown in Eq. (4). Here, denotes the diffusion timestep (0, ), xl and xw the losing and winning audio respectively, and ϵ (0, I). Following Esser et al. (2024), DPO-Diffusion loss is applicable to rectified flow through the equivalence (Lipman et al., 2023) between ϵθ and u(; θ), thereby the noise matching loss terms can be substituted with flow matching terms: LDPO-FM = EtU (0,1),xw,xl log σ β (cid:32) (cid:32) u(xw (cid:124) , t; θ) vw (cid:123)(cid:122) Winning loss 2 2 (cid:125) u(xl (cid:124) t, t; θ) vl (cid:123)(cid:122) Losing loss t2 2 (cid:125) (cid:16) u(xw (cid:124) , t; θref) vw (cid:123)(cid:122) Winning reference loss 2 2 (cid:125) u(xl (cid:124) t, t; θref) vl (cid:123)(cid:122) Losing reference loss t2 2 (cid:125) (cid:17) (cid:33)(cid:33) , (5) where is the flow matching timestep and xl tively. and xw represent losing and winning audio, respecThe DPO loss for LLMs models the relative likelihood of the winner and loser responses, allowing minimization of the loss by increasing their margin, even if both log-likelihoods decrease (Pal et al., 2024). As DPO optimizes the relative likelihood of the winning responses over the losing ones, not their absolute values, convergence actually requires both likelihoods to decrease despite being counterintuitive (Rafailov et al., 2024b). The decrease in likelihood does not necessarily decrease performance, but required for improvement (Rafailov et al., 2024a). However, in the context of rectified flows, this behavior is less clear due to the challenges in estimating the likelihood of generating samples with classifier-free guidance (CFG). closer look at LDPO-FM (Eq. (5)) reveals that it can similarly be minimized by increasing the margin between the winning and losing losses, even if both losses increase. In Section 4.5, we demonstrate that preference optimization of rectified flows via LDPO-FM suffer from this phenomenon as well. To remedy this, we incorporate the winning loss directly into the optimization objective to prevent winning loss from increasing. Our loss is denoted as LCRPO := LDPO-FM + LFM, where LFM is the flow matching loss computed on the winning audio as shown in Eq. (2). While the DPO loss is effective at improving preference rankings between chosen and rejected audio, relying on it alone can lead to overoptimization. This can distort the semantic and structural fidelity of the winning audio, causing the models outputs to drift from the desired distribution. Adding the LFM component mitigates this risk by anchoring the model to the high-quality attributes of the chosen data. This regularization stabilizes training and preserves the essential properties of the winning examples, ensuring balanced and robust optimization process. Our empirical results demonstrates LCRPO outperform LDPO-FM as shown in Section 4.5."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 MODEL TRAINING We pretrained TANGOFLUX on Wavcaps (Mei et al., 2024) for 80 epochs with the AdamW (Loshchilov & Hutter, 2019), β1 = 0.9, β2 = 0.95, max learning rate of 5 104. We used linear learning rate scheduler for 2000 steps. We used five A40 GPUs with batch size of 16 on each device, resulting in an overall batch size of 80. After pretraining, TANGOFLUX was finetuned on the AudioCaps training set for 65 additional epochs. Several works find that sampling timesteps from the middle of its range [0, 1] leads to superior results (Hang et al., 2024; Kim et al., 2024b; Karras et al., 2022), thus, we sampled from logit-normal distribution with mean 6 of 0 and variance of 1, following the approach in (Esser et al., 2024). We name this version as TANGOFLUX-base. During the alignment phase, we used the same optimizer, but an overall batch size of 48, maximum learning rate of 105, and linear warmup of 100 steps. For each iteration of CRPO, we train for 8 epochs and select the last epoch checkpoint to perform batched online data generation. We performed 5 iterations of CRPO due to the manifestation of performance saturation."
        },
        {
            "title": "3.2 DATASETS",
            "content": "Training dataset. We use complete open source data which consists of approximately 400k audios from Wavcaps (Mei et al., 2024) and 45k audios from the training set of AudioCaps. (Kim et al., 2019). For audios shorter than 30 seconds, we pad the remaining audio with silence. For audios longer than 30 seconds, we perform center cropping of 30 seconds. Since the audio files are all mono, we duplicated the channel to create stereo audio for compatibility with our model. CRPO dataset. We initialize the prompt bank as the prompts of AudioCaps training set, with total of 45k prompts. At the start of each iteration of CRPO, we randomly sample 20k prompts from the prompt bank and generate 5 audios per prompt, and use the CLAP model to construct 20k preference pairs. Evaluation dataset. For the main results, we evaluated TANGOFLUX on the AudioCaps test set, using the same 886-sample split as (Majumder et al., 2024). Objective metrics are reported on this subset. Additionally, we categorized AudioCaps prompts using GPT-4 to identify those with multiple distinct events, such as Birds chirping and thunder strikes, which includes sound of birds chirping and sound of thunder. Metrics for these multi-event captions are reported separately. Subjective evaluation was conducted on an out-of-distribution dataset with 50 challenging prompts. 3.3 OBJECTIVE EVALUATION Baselines. We compare TANGOFLUX to four existing strong baselines for text-to-audio generation: Tango 2, AudioLDM 2, and Stable Audio Open, including the previous SOTA models. For all of our baseline evaluations, we use the default recommended classifier free guidance (CFG) scale (Ho & Salimans, 2022) and number of steps. For TANGOFLUX, we use CFG scale of 4.5 and 50 steps for inference. Since TANGOFLUX and Stable Audio Open allow variable audio generation length, we set the duration conditioning to 10 seconds and use the first 10 seconds of generated audio to perform the evaluation. We also report the effect of CFG scale in the appendix A.1. Evaluation metrics. We evaluate TANGOFLUX using both objective and subjective metrics. For objective metrics, we report the 4 metrics: Frechet Distance (FDopenl3) (Cramer et al., 2019), KullbackLeibler divergence (KLpasst) , CLAPscore and Inception Score (IS) (Salimans et al., 2016). We chose this set of metrics proposed by (Evans et al., 2024a) due to the capabilities to perform highquality audio evaluation up to 48kHz. FDopenl3 evaluates the similarity between the statistics of generated audio set and another reference audio set in the feature level space. low FDopenl3 indicates that the generated audio is realistic and closely resembles the reference audio. KLpasst computes the KL divergence over the probabilities of the labels between the generated and the reference audio given the state-of-the-art audio tagger PaSST. low KLpasst signifies the generated and reference audio share similar semantics tags. CLAPscore is reference-free metric that measures the cosine similarity between the audio as well as the text prompt. High CLAPscore score denotes the generated audio aligns with the textual prompt. IS measures the specificity and coverage for set of samples. high IS score represents the diversity of the generated audio. We use stable-audio-metrics (Evans et al., 2024a) to compute FDopenl3, KLpasst, CLAPscore and AudioLDM evaluation toolkit (Liu et al., 2023) to compute IS. Note that we use different CLAP checkpoints to create our preference dataset (630k-audioset-best) and to perform the evaluation (630k-audioset-fusion-best). 4https://huggingface.co/lukewys/laion_clap/blob/main/ 630k-audioset-fusion-best"
        },
        {
            "title": "3.4 HUMAN EVALUATION",
            "content": "To evaluate the instruction-following capabilities and robustness of TTA models, we created 50 outof-distribution complex captions, such as pile of coins spills onto wooden table with metallic clatter, followed by the hushed murmur of tavern crowd and the creak of swinging door. These captions describe multiple events (ranging from 3 to 6 per caption) and go beyond conventional or overused sounds, such as simple animal noises, footsteps, or city ambiance. Events were identified using GPT4o to evaluate the captions generated. Each of the generated prompts contains multiple events including several where the temporal order of the events must be maintained. Details of our caption generation template and samples of generated captions can be found in the Appendix A.2. Following prior studies (Ghosal et al., 2023; Majumder et al., 2024), our subjective evaluation focuses on two primary attributes of the generated audio: overall audio quality (OVL) and relevance to the text input (REL). The OVL metric evaluates the general sound quality, including clarity and naturalness, irrespective of the alignment with the input prompt. In contrast, the REL metric specifically measures the alignment of the generated audio with the provided text input. Annotators rated each audio sample on scale from 0 (worst) to 100 (best) for both OVL and REL. This evaluation was performed on 50 GPT4o-generated prompts, with each sample independently assessed by at least four annotators. Additional details on the evaluation instructions and annotators can be found in Appendix A.2. 3.4.1 METRICS We report three key metrics for subjective evaluation: Scores: The average of the scores assigned by individual annotators. Due to the subjective nature of these scores and the significant variance observed in the annotator scoring patterns, the ratings were normalized to z-scores at the annotator level: zij = (sij µi)/σi. zij: The z-score for annotator is score of model Mj. This is the score after applying z-score normalization. sij: The raw score assigned by annotator to model j. This is the original score before normalization. µi: The mean score assigned by annotator across all models. It represents the central tendency of the annotators scoring pattern. σi: The standard deviation of annotator is scores across all models. This measures the variability or spread in the annotators ratings. This normalization procedure adjusts the raw scores, centering them around the annotators mean score and scaling by the annotators score spread (standard deviation). This ensures that scores from different annotators are comparable, helping to mitigate individual scoring biases. Ranking: Despite z-score normalization, the variability in annotator scoring can still introduce noise into the evaluation process. To address this, models are also ranked based on their absolute scores. We utilize the mean (average rank of model), and mode (the most common rank of model) as metrics for evaluating these rankings. Elo: Elo-based evaluation, widely adopted method in language model assessment, involves pairwise model comparisons. We first normalized the absolute scores of the models using zscore normalization and then derived Elo scores from these pairwise comparisons. Elo score mitigates the noise and inconsistencies observed in scoring and ranking techniques. Specifically, Elo considers the relative performance between models rather than relying solely on absolute or averaged scores, providing more robust measure of model quality under subjective evaluation. While ranking-based evaluation provides an ordinal comparison of models, determining the order of performance (e.g., Model ranks first, Model ranks second), it does not capture the magnitude of differences between ranks. For instance, if the difference between the first and second rankers is minimal, this is not evident from ranks alone. Elo scoring addresses this limitation by integrating both ranking and pairwise performance data. In rankingbased systems, the rank Ri of model Mi is determined purely by its position relative to others: Ri = position of Mi in the sorted list of models based on performance.. However, this approach fails to quantify: 1) The gap in performance between consecutive ranks. 2) The consistency of relative performance across different pairwise comparisons. Elo scoring provides probabilistic measure of model performance based on pairwise comparisons. By leveraging annotator scores, Elo assigns continuous score Ei to each model Mi, capturing its relative strength. 8 Model #Params. Duration Steps FDopenl3 KLpasst CLAPscore IS AudioLDM 2-large Stable Audio Open Tango 2 TANGOFLUX-base TANGOFLUX 712M 1056M 866M 515M 515M 10 sec 47 sec 10 sec 30 sec 30 sec 200 100 200 50 50 108.3 89.2 108.4 80.2 75.1 1.81 2.58 1.11 1.22 1.15 0.419 0.291 0.447 0.431 0.480 7.9 9.9 9.0 11.7 12.2 Inference Time (s) 24.8 8.6 22.8 3.7 3.7 Table 1: Comparison of audio generation models across various metrics. Output length represents the duration of the generated audio. Metrics include FDopenl3 for Frechet Distance, KLpasst for KL divergence, and CLAPscore for alignment. All inference time is computed on the same A40 GPU. We report the trainable parameters in the #Params column. Model AudioLDM 2-large Stable Audio Open Tango 2 TANGOFLUX-base TANGOFLUX #Params. Duration 712M 1056M 866M 515M 515M 10 sec 47 sec 10 sec 30 sec 30 sec FDopenl3 KLpasst CLAPscore 1.83 2.67 1.14 1.23 1.20 0.415 0.286 0.452 0.438 0.488 107.9 88.5 108.3 79.7 75.2 IS 7.3 9.3 8.4 10.7 11.1 Table 2: Comparison of text-to-audio models on multi-event inputs."
        },
        {
            "title": "4 RESULTS",
            "content": "4.1 MAIN RESULTS Table 1 compares TANGOFLUX with prior text-to-audio generation models on AudioCaps in terms of the objective metrics. Model performance on the prompts with more than one event, namely multi-event prompts, are reported in Table 2. The results suggest that TANGOFLUX consistently outperforms the prior works on all objective metrics, except Tango 2 on KLpasst. Interestingly, the margin on CLAPscore between TANGOFLUX and baselines is higher when evaluated on multi-event prompts. This suggests that TANGOFLUX excels at understanding and generating audio for complex instructions involving multiple events, effectively capturing nuanced details and relationships within the text compared to the baselines. 4.2 BATCHED ONLINE DATA GENERATION IS NECESSARY To show the impact of generating new samples at each iteration, in Fig. 2 we present the results of 5 training iterations of CRPO, both with and without generating new data at each iteration. Our findings suggest that training on the same dataset over multiple iterations leads to quick performance saturation and eventual degradation. Specifically, for offline CRPO, the CLAP score decreases after the second iteration, while the KL increases significantly. By the final iteration, performance degradation is evident, with both the CLAP score and KL worse than the first iteration, emphasizing the limitations of using offline data. In contrast, the online CRPO with data generation at the beginning of each iteration consistently outperforms the offline CRPO in terms of both CLAP score and KL. possible explanation of this performance degradation could be reward over-optimization (Rafailov et al., 2024a; Gao et al., 2022). Previous work by (Kim et al., 2024a) demonstrated that the reference model serves as lower bound in DPO training for language models. Several iterations of updating the reference model (lower bound) with the same dataset cause the current model to excessively minimize the loss in unexpected ways. In Section 4.5, we identify unexpected phenomena in loss minimization that could explain over-optimization. This over-optimization ultimately leads to performance degradation as shown by the spike in KL and drop in CLAP score. 4.3 CLAP AS REWARD MODEL 9 e v K 1.3 1.2 CRPO CRPO (offline) 0.48 0.47 0.46 0.45 c L"
        },
        {
            "title": "3\nIteration",
            "content": "4 5"
        },
        {
            "title": "3\nIteration",
            "content": "4 5 Figure 2: The trajectory of CLAP score and KL divergence across the training iterations. This plot shows the stark difference between online and offline training. Offline training clearly peaks early, by the second iteration, indicated by the peaking CLAP score and increasing KL. In contrast, the CLAP score of online training continues to increase until iteration 4, while the KL divergence has clear downward trend throughout. Dataset BATON Audio Alpaca CRPO FDopenl3 KLpasst CLAPscore 1.20 1.20 1. We experiment to validate that the CLAP model can serve as proxy reward model for evaluating audio output. We experiment with evaluating TANGOFLUX with Best-of-N policy, with {1, 5, 10, 15}. We use the 630kaudioset-best.pt checkpoint to rank the generated audio. We report the result in Table 4. Results suggest that increasing yield better CLAPscore and KLpasst while FDopenl3 remains about the same. This indicates that the CLAP model effectively identifies and ranks higherquality audio outputs that better align with the textual descriptions, without sacrificing output diversity or quality as shown by the lower KLpasst and similar FDopenl3. Table 3: Comparison of difference preference dataset used for preference tuning. Metrics include FDopenl3 for Frechet Distance, KLpasst for KL divergence, and CLAPscore for alignment. 0.437 0.448 0.453 80.5 80.0 79.1 4.4 CRPO DATASET IS BETTER THAN OTHER AUDIO PREFERENCE DATASETS To validate the effectiveness of CRPO in constructing preference datasets, we compared the performance of CRPO with two other audio preference datasets: Audio-Alpaca (Majumder et al., 2024) and BATON (Liao et al., 2024). BATON: BATON collects human-annotated data by asking labelers to assign binary score of 0 or 1 to each audio sample based on its alignment with given prompt. score of 1 indicates alignment, while 0 indicates misalignment. From this data, we construct preference dataset by pairing audio samples scored as 1 (winners) with those scored as 0 (losers) for the same prompt, creating set of winner-loser pairs. Audio-Alpaca: Audio-Alpaca, in contrast, is already structured as preference dataset, requiring no further processing. We use the base model TANGOFLUX-base for preference optimization, conducting only one iteration since Audio-Alpaca and BATON are fixed datasets. Table 3 reports objective metrics FDopenl3, KLpasst, and CLAPscore, demonstrating that preference optimization with the CRPO dataset outperforms the other two audio preference datasets across all metrics. Despite its simplicity, CRPO proves highly effective for constructing audio preference datasets for optimization. 4.5 LCRPO VS LDPO-FM 10 1. 1.6 L 1.55"
        },
        {
            "title": "3\nIteration",
            "content": "4 5 Winning Loss (CRPO) Winning Loss (DPO-Diff) Losing Loss (CRPO) Losing Loss (DPO-Diff) Figure 3: Winning and Losing losses of LDPO-FM and LCRPO at each iteration. Winning and Losing losses increase each iteration, as well as their margin. We investigate whether using DPO to align rectified flow increases both winning and losing losses of Eq. (5) while increasing the margin of them simultaneously. To do so, we calculate the average winning and losing losses on the training set using the final checkpoint (epoch 8) from each iteration. We present the result in Fig. 3. We also present benchmark performance on AudioCaps training with LCRPO and LDPO-FM in Fig. 4. Here, we investigate only with offline data such that we use the fixed dataset generated by TANGOFLUX-base. 0.46 0.44 0.42 78 76 Model TANGOFLUX Tango 2 FDopenl3 KLpasst CLAPscore 1 5 10 15 0.480 0.494 0.499 0.502 75.0 74.3 75.8 75.1 1.15 1.14 1.08 1. 1 5 10 15 108.4 108.8 108.4 108.7 1.11 1.05 1.08 1.06 0.447 0.467 0.474 0.473 Table 4: Comparison of different preference datasets used for preference tuning. Metrics include FDopenl3 for Frechet Distance, KLpasst for KL divergence, and CLAPscore for alignment. LDPO-FM LCRPO 1.4 1.3 1.2 1 2 3 Iteration (a) CLAPscore 5 1 2 3 Iteration (b) FDopenl3 5 1 2 3 Iteration 5 (c) KLpasst Figure 4: Comparison of metrics across iterations for LDPO-FM and LCRPO. As shown in Fig. 3, the winning and losing losses for both LCRPO and LDPO-FM increase with each iteration, along with their margin. Despite the increase in losses, benchmark performance improves, with LCRPO achieving superior results in CLAPscore while maintaining similar KLpasst and FDopenl3 across all iterations. We observe notable acceleration in loss growth after iteration 3, which may In contrast, LCRPO exhibits more gradual and indicate performance saturation or degradation. stable increase in loss, maintaining smaller margin and more controlled growth, leading to less performance degradation compared to LDPO-FM. This highlights the role of the winning loss as regularizer in controlling the overall optimization dynamics. Specifically, adding winning loss helps to stabilize the training process by preventing the model from excessively focusing on increasing the margin through increasing both winning loss and losing loss. Figure 5: CLAP and FD Scores vs Inference Time for each model. We plot this for steps count of 10, 25, 50, 100, 150 and 200. Interestingly, our findings show that in aligning rectified flow with DPO, both winning and losing losses increase, while the margin between them widenssimilar to the behavior observed when aligning LLMs with DPO (Rafailov et al., 2024b). This behavior is consistent across both LCRPO and LDPO-FM, where performance improves despite the seemingly counterintuitive nature of this phenomenon, as also noted by (Rafailov et al., 2024a) in the context of LLMs. 4.6 INFERENCE TIME AND PERFORMANCE COMPARISON We compare inference times, CLAP scores, and FD scores across models for steps 10, 25, 50, 100, 150, and 200, as shown in Figure 5. TANGOFLUX demonstrates remarkable balance between efficiency and performance, consistently achieving higher CLAP scores and lower FD scores while requiring significantly less inference time compared to other models. For example, at 50 steps, TANGOFLUX achieves CLAP score of 0.480 and an FD score of 75.1 in just 3.7 seconds. In comparison, Stable Audio Open requires 4.5 seconds for the same step count but only achieves 12 CLAP score of 0.284 (41% lower than TANGOFLUX) and an FD score of 87.8 (17% worse than TANGOFLUX). This demonstrates that TANGOFLUX achieves superior performance metrics in less time. Additionally, at lower step count of 10, TANGOFLUX maintains strong performance with CLAP score of 0.465 and an FD score of 77.2 in just 1.1 seconds. In contrast, Audioldm2 at the same step count achieves lower CLAP score of 0.357 (23% lower) and significantly worse FD score of 131.7 (70% higher), while requiring 1.5 seconds (36% more time). We also observe that reducing the step count from 200 to 10 has minimal impact on TANGOFLUXs performance, highlighting its robustness. Specifically, TANGOFLUXs CLAP score decreases by only 3.2% (from 0.480 to 0.465), and its FD score increases by only 4.5% (from 73.9 to 77.2). In contrast, Tango 2 shows larger degradation, with its CLAP score decreasing by 16.0% (from 0.443 to 0.372) and its FD score increasing by 37.8% (from 108.4 to 158.6). These results highlight TANGOFLUXs effectiveness in delivering high-quality outputs with lower computational requirements, making it highly efficient choice for scenarios where inference time is critical. TL;DR 1. Model Comparison: TANGOFLUX outperforms prior works in almost all objective metrics on AudioCaps, especially for prompts with multiple events. It achieves superior performance in FDopenl3, CLAPscore, and Inception Score (IS), with notable efficiency gains (lowest inference time). Only Tango 2 marginally surpasses TANGOFLUX in KLpasst. 2. Multi-Event Prompts: The margin in CLAPscore between TANGOFLUX and baselines is larger for multi-event inputs, demonstrating its capability to handle complex and nuanced scenarios. 3. Training Strategies: Online batched data generation significantly outperforms offline strategies, preventing performance degradation caused by over-optimization. Online training maintains consistent improvement across CLAPscore and KLpasst over iterations. 4. Preference Optimization: CRPO dataset leads to better results than other preference datasets like BATON and Audio-Alpaca across all metrics. Larger in the Best-of-N policy enhances CLAPscore and KLpasst, validating CLAP as an effective reward model. 5. Optimization Techniques: LCRPO demonstrates more stable and effective optimization than LDPO-FM, with reduced performance saturation and better benchmark results. The controlled growth in optimization metrics with LCRPO highlights its robustness for rectified training processes. 6. Inference Time: While delivering superior performance, TANGOFLUX also boasts much lower inference time, resulting in greater efficiency compared to other models. TANGOFLUX shows less performance decline compared to other models when sampling at fewer steps. 4.7 HUMAN EVALUATION RESULTS The results of the human evaluation are presented in Table 5, with detailed comparisons of the models across the evaluated metrics: z-scores, rankings, and Elo scores for both overall audio quality (OVL) and relevance to the text input (REL). Below, we provide an analysis of the findings. Z-scores: The z-scores offer normalized perspective on annotator evaluations, mitigating individual biases by transforming the absolute variable into standard normal variable with zero mean and one standard deviation. Among the models, TANGOFLUX demonstrated the highest performance across both metrics, with z-scores of 0.2486 for OVL and 0.6919 for REL. This indicates its superior quality and strong alignment with the input prompts. Conversely, AudioLDM 2 scored the lowest with z-scores of -0.3020 (OVL) and -0.4936 (REL), suggesting both lower sound quality and weaker adherence to textual inputs compared to the other models. Ranking: Rankings provide an alternative ordinal measure of performance, capturing both the mean and mode of model rankings. Consistent with the z-score findings, TANGOFLUX achieved the best rankings with mean rank of 1.7 for OVL and 1.1 for REL, and mode rank of 2 (OVL) and 1 (REL). This reinforces the models superior position in the subjective evaluations. AudioLDM 2 consistently ranked the lowest, with mean rankings of 3.5 (OVL) and 3.7 (REL), and mode rankings of 4 for both metrics. Interestingly, StableAudio and Tango 2 showed competitive mean ranks for OVL, both at 2.4, but diverged on REL where Tango 2 outperformed StableAudio (1.9 vs. 3.3 in mean rank). However, curiously, StableAudio has two modes 1 and 3 for OVL, indicating polarized perception by the annotators. The lower mode could be explained by possible bias induced by the stark misalignment between the prompt and audio output, indicated by the mean and mode of 3.3 and 3, respectively, in terms of ranking by REL. Elo Scores: The Elo scores provide probabilistic and continuous measure of model performance, offering insights into the magnitude of differences in relative performance. Here, TANGOFLUX again excelled, achieving the highest Elo scores for both OVL (1,501) and REL (1,628). The Elo results highlight the robustness of TANGOFLUX, as it consistently outperformed other models in pairwise comparisons. Tango 2 emerged as the second-best performer, with Elo scores of 1,419 (OVL) and 1,507 (REL). StableAudio followed, showing competitive performance in OVL (1,444) but weaker REL score (1,268). As observed in other metrics, AudioLDM 2 ranked last with the lowest Elo scores (1,236 for OVL and 1,196 for REL). TL;DR 1. TANGOFLUX consistently demonstrated superior performance across all metrics, highlighting its strength in generating high-quality, text-relevant audio. This is particularly evident in its significant lead in the REL metrics, showcasing its robust capability to align with complex, multi-event prompts. 2. Tango 2 performed strongly in REL, reflecting its alignment capability. However, it slightly lagged behind TangoFlux in OVL, indicating potential room for improvement in audio clarity and naturalness. 3. Stable Audio Open displayed competitive performance in OVL, but its REL scores suggest limitations in accurately and faithfully representing complex text inputs. 4. AudioLDM2 consistently underperformed across all metrics, reflecting challenges in both audio quality and relevance to complex prompts. This positions it as the least preferred model in this evaluation. Model OVL REL OVL REL OVL REL z-scores Ranking Elo Mean Mode Mean Mode AudioLDM 2 Stable Audio Open Tango 2 TANGOFLUX -0.3020 0.0723 -0.019 0.2486 -0.4936 -0.3584 0.1602 0. 3.5 2.4 2.4 1.7 4 1, 3 2 2 3.7 3.3 1.9 1.1 4 3 2 1 1,236 1,444 1,419 1,501 1,196 1,268 1,507 1, Table 5: Human evaluation results on two attributes: OVL (overall quality) and REL (relevance). We report the z-scores, ranking, and Elo scores to mitigate individual annotator biases and present relative performance comparison."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Text-To-Audio Generation. TTA Generation has garnered attention lately due to models such as AudioLDM series model (Liu et al., 2024b; 2023), Tango series model (Majumder et al., 2024; Ghosal et al., 2023; Kong et al., 2024) and Stable Audio series model (Evans et al., 2024a;c;b). These models commonly adopt the diffusion framework (Song & Ermon, 2020; Rombach et al., 2022; Song et al., 2022; Ho et al., 2020), which trains latent diffusion model conditioned on either T5 embedding or CLAP embedding. However, another common framework for TTA generation is the flow matching framework which was employed in models such as VoiceBox (Le et al., 2023), AudioBox (Vyas et al., 2023), FlashAudio (Liu et al., 2024c). Alignment Method. Preference optimization is the standard approach for aligning LLMs, achieved either by training reward model to capture human preferences (Ouyang et al., 2022) or by using the LLM itself as the reward model (Rafailov et al., 2024c). Recent advances improve this process through iterative alignment, leveraging human annotators to construct preference pairs or utilizing pre-trained reward models. (Kim et al., 2024a; Chen et al., 2024; Gulcehre et al., 2023; Yuan et al., 2024). Verifiable answers can enhance the construction of preference pairs. For diffusion and flowbased models, Diffusion-DPO shows that these models can be aligned similarly (Wallace et al., 2023). However, constructing preference pairs for TTA remains challenging due to the lack of gold audio for given text prompts and the subjective nature of audio. Tango2 addresses this by using prompt perturbation, while BATON (Liao et al., 2024) relies on human annotation to construct preference pairs which is not scalable solution."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce TANGOFLUX, fast flow-based text-to-audio model aligned using synthetic preference data generated online during training. Objective and human evaluations show that TANGOFLUX produces audio more representative of user prompts than existing diffusion-based models, achieving state-of-the-art performance with significantly fewer parameters. Additionally, TANGOFLUX demonstrates greater robustness, maintaining performance even when sampling with fewer time steps. These advancements make TANGOFLUX practical and scalable solution for widespread adoption."
        },
        {
            "title": "REFERENCES",
            "content": "Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants, 2023. URL https://arxiv.org/abs/2209.15571. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models, 2024. URL https://arxiv. org/abs/2401.01335. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models, 2023. URL https://arxiv.org/abs/2311.07919. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report, 2024. URL https://arxiv.org/abs/2407.10759. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation, 2024. URL https://arxiv.org/ abs/2306.05284. 15 Aurora Linh Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello. Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 38523856, 2019. doi: 10.1109/ICASSP.2019.8682475. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/ 2403.03206. Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion, 2024a. URL https://arxiv.org/abs/2402.04825. Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Long-form music generation with latent diffusion, 2024b. URL https://arxiv.org/abs/2404. 10301. Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open, 2024c. URL https://arxiv.org/abs/2407.14358. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction-tuned llm and latent diffusion model, 2023. URL https://arxiv. org/abs/2304.13731. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling, 2023. URL https://arxiv.org/abs/2308.08998. Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy, 2024. URL https://arxiv. org/abs/2303.09556. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation, 2023a. URL https://arxiv.org/abs/2305.18474. Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models, 2023b. URL https://arxiv.org/abs/2301.12661. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llmbased input-output safeguard for human-ai conversations, 2023. URL https://arxiv.org/ abs/2312.06674. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models, 2022. URL https://arxiv.org/abs/2206.00364. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Generating captions for audios in the wild. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1011. URL https://aclanthology.org/N19-1011. 16 Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sdpo: Dont use your data all at once, 2024a. URL https://arxiv.org/abs/2403. 19270. Myunsoo Kim, Donghyeon Ki, Seong-Woong Shim, and Byung-Jun Lee. Adaptive non-uniform timestep sampling for diffusion model training, 2024b. URL https://arxiv.org/abs/ 2411.09998. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Zhifeng Kong, Sang gil Lee, Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Rafael Valle, Soujanya Poria, and Bryan Catanzaro. Improving text-to-audio models with synthetic captions, 2024. URL https://arxiv.org/abs/2406.15487. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2024a. URL https: //arxiv.org/abs/2411.15124. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024b. URL https://arxiv.org/abs/2403.13787. Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multilingual universal speech generation at scale, 2023. URL https://arxiv.org/abs/2306. 15687. Huan Liao, Haonan Han, Kai Yang, Tianjiao Du, Rui Yang, Zunnan Xu, Qinmei Xu, Jingquan Liu, Jiasheng Lu, and Xiu Li. Baton: Aligning text-to-audio model with human preference feedback, 2024. URL https://arxiv.org/abs/2402.00744. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, and Wei-Ning Hsu. Generative pre-training for speech with flow matching, 2024a. URL https://arxiv.org/abs/ 2310.16338. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models, 2023. URL https://arxiv.org/abs/2301.12503. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining, 2024b. URL https://arxiv.org/abs/2308.05734. Huadai Liu, Jialei Wang, Rongjie Huang, Yang Liu, Heng Lu, Wei Xue, and Zhou Zhao. Flashaudio: Rectified flows for fast and high-fidelity text-to-audio generation, 2024c. URL https: //arxiv.org/abs/2410.12266. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization, 2024. URL https://arxiv.org/abs/2404.09956. 17 Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang. WavCaps: ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 115, 2024. Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous normalizing flows via optimal transport, 2021. URL https://arxiv.org/abs/2006. 00104. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel18 ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive, 2024. URL https: //arxiv.org/abs/2402.13228. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Iterative reasoning preference optimization, 2024. URL https://arxiv.org/ Weston. abs/2404.19733. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms, 2024a. URL https://arxiv.org/abs/2406.02900. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to q: Your language model is secretly q-function, 2024b. URL https://arxiv.org/abs/2404.12358. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024c. URL https://arxiv.org/abs/2305.18290. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans, 2016. URL https://arxiv.org/abs/1606.03498. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution, 2020. URL https://arxiv.org/abs/1907.05600. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models, 2024. URL https://arxiv.org/abs/2310.13289. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, Jeff Wang, Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian Ellis, Rashel Moritz, Yael Yungster, Alice Rakotoarison, Liang Tan, Chris Summers, Carleigh Wood, Joshua Lane, Mary Williamson, and Wei-Ning Hsu. Audiobox: Unified audio generation with natural language prompts, 2023. URL https://arxiv.org/abs/ 2312.15821. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization, 2023. URL https://arxiv.org/abs/2311.12908. Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keywordIn IEEE International Conference on Acoustics, Speech and Signal to-caption augmentation. Processing, ICASSP, 2023. Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation, 2024. URL https://arxiv.org/ abs/2401.01044. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv.org/ abs/2401.10020. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685."
        },
        {
            "title": "Model",
            "content": "Steps CFG FDopenl3 KLpasst CLAPscore TANGOFLUX 50 50 50 50 50 3.0 3.5 4.0 4.5 5.0 77.7 76.1 74.9 75.1 74.6 1.14 1.14 1.15 1.15 1. 0.479 0.481 0.476 0.480 0.472 Table 6: TANGOFLUX with different classifier free guidance (CFG) values."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EFFECT OF CFG SCALE We conduct an ablation of the effect of CFG scale for TANGOFLUX and show the result in Table 6. It reveals trade-off: higher CFG values improve FD score (lower FD) but slightly reduce semantic alignment (CLAP score), which peaks at CFG=3.5. The results emphasize CFG=3.5 as the optimal balance between fidelity and semantic relevance. A.2 HUMAN EVALUATION The human evaluation was performed using web-based Gradio5 app. Each annotator was presented with 20 prompts, each having four audio samples generated by four distinct text-to-audio models, shuffled randomly, as shown in Fig. 6. Before the annotation process, the annotators were instructed with the following directive: Welcome username # Instructions for evaluating audio clips Please carefully read the instructions below. ## Task You are to evaluate four 10-second-long audio outputs to each of the 20 prompts below. These four outputs are from four different models. You are to judge each output with respect to two qualities: Overall Quality (OVL): The overall quality of the audio is to be judged on scale from 0 to 100: 0 being absolute noise with no discernible feature. Whereas, 100 is perfect. Overall fidelity, clarity, and noisiness of the audio are important here. Relevance (REL): The extent of audio alignment with the prompt is to be judged on scale from 0 to 100: with 0 being absolute irrelevance to the input description. Whereas, 100 is perfect representation of the input description. You are to judge if the concepts from the input prompt appear in the audio in the described temporal order. You may want to compare the audios of the same prompt with each other during the evaluation. ## Listening guide 1. Please use head/earphone to listen to minimize exposure to the external noise. 2. Please move to quiet place as well, if possible. ## UI guide 5https://www.gradio.app 21 1. Each audio clip has two attributes OVL and REL below. You may select the appropriate option from the dropdown list. 2. To save your judgments, please click on any of the save buttons. All the save buttons function identically. They are placed everywhere to avoid the need to scroll to save. Hope the instructions were clear. Please feel free to reach out to us for any queries. Figure 6: The Gradio-based human evaluation form created for the annotators to score the model generated audios with respect to the input prompts. A.2.1 PROMPTS USED IN THE EVALUATION Prompts Multiple Events Temporal Events robotic arm whirs frantically while an electric plasma arc crackles and metallic voice counts down ominously, interspersed with glass vials clinking to the floor. Unfamiliar chirps overlap with low, throbbing hum as bioluminescent plants audibly crackle and squelch with movement. Dripping water echoes sharply, distant growl reverberates through the cavern, and soft scraping metal suggests something lurking unseen. Alarms blare with rising urgency as fragments clatter against metallic hull, interrupted by faint hiss of escaping air. Hundreds of tiny wings buzz with chaotic pitch shift, joined by the faint clattering of mandibles and an organic squish as they collide. Jagged rocks crumble underfoot while distant ocean waves crash below, punctuated by the sudden snap of rope. 22 Digital beeps and chirps meld with overlapping chatter in multiple languages, as automated drones whiz past, scanning barcodes audibly. Rusted swings creak in rhythmic disarray, faint mechanical laugh stutters from distant speaker, and the sound of gravel crunches under unseen footsteps. Bubbling lava gurgles ominously, instruments beep irregularly, and faint crackling signals static from failing radio. Tiny pops and hisses of chemical reactions intermingle with the rhythmic pumping of centrifuge and the soft whirr of air filtration. The faint hiss of gas leak grows louder as metal chains rattle and single marble rolls across the floor. hand slaps table sharply, followed by the shuffle of playing cards and the hum of an overhead fan. train horn blares in the distance as bicycle bell chimes and soda can pops open with fizzy hiss. drawer creaks open, papers rustle wildly, and the sharp click of lock snapping shut echoes. burst of static interrupts soft typing sounds, followed by the distant chirp of pager and cough. heavy book thuds onto desk, accompanied by the faint buzz of fluorescent light and muffled sneeze. The sharp squeak of sneakers on gym floor blends with the rhythmic bounce of basketball and the screech of metal door. An elevator dings, its doors sliding open, as muffled voices overlap with the shuffle of heavy bags. clock ticks steadily, light switch clicks on, and the crackle of fire igniting briefly fills the silence. fork scrapes plate, water drips slowly into sink, and the faint hum of refrigerator lingers in the background. cat hisses sharply as glass shatters nearby, followed by hurried footsteps and the slam of closing door. parade marches through town square, with drumbeats pounding, children clapping, and horse neighing amidst the commotion. basketball bounces rhythmically on court, shoes squeak against the floor, and referees whistle cuts through the air. baby giggles uncontrollably, stack of blocks crashes to the ground, and the faint hum of lullaby toy plays in the background. The rumble of subway train grows louder, followed by the screech of brakes and muffled announcements over crackling speaker. beekeeper moves carefully as bees buzz intensely, smoker puffs softly, and wooden frames creak as theyre lifted. dog shakes off water with noisy splatter, bicycle bell rings, and distant lawnmower hums faintly in the background. Books fall off shelf with heavy thud, chair scrapes loudly across wooden floor, and surprised gasp echoes. soccer ball hits goalpost with metallic clang, followed by cheers, clapping, and the distant hum of commentators voice. hikers pole taps against rocks, mountain goat bleats sharply, and loose gravel tumbles noisily down steep slope. rooster crows loudly at dawn, joined by the rustle of feathers and the crunch of chicken feed scattered on the ground. carpenter saws through wood with steady strokes, hammer strikes nails rhythmically, and measuring tape snaps back with metallic zing. frog splashes into pond as dragonflies buzz nearby, accompanied by the distant croak of toads echoing through the marsh. The crack of whip startles herd of cattle, their hooves clatter against dirt path as rancher shouts commands. paper shredder whirs noisily, the rustle of documents being fed in grows louder, and stapler clicks shut in rapid succession. An elephant trumpets in the savanna as herd stomps through dry grass, accompanied by the buzz of flies and the distant roar of lion. mime claps silently as juggling act clinks glass balls together, and crowd bursts into laughter at the clatter of dropped prop. train conductor blows sharp whistle, metal wheels screech on the rails, and passengers murmur while settling into their seats. squirrel chitters nervously as acorns drop from tree, landing with dull thuds, while leaves rustle above in quick bursts of movement. blacksmith hammers molten iron with rhythmic clangs, bellows pumps air with whoosh, and sparks sizzle on stone floor. skateboard grinds loudly against metal rail, followed by the sharp slap of wheels hitting pavement and triumphant cheer from the rider. 24 An old typewriter clacks rapidly as paper rustles with each keystroke, interrupted by the sharp ding of the carriage return. pack of wolves howls in unison as dry leaves crunch underfoot, and the faint trickle of nearby stream echoes through the forest. Table 7: Prompts used in human evaluation and their characteristics."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Singapore University of Technology and Design (SUTD)"
    ]
}