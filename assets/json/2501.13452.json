{
    "paper_title": "EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion",
    "authors": [
        "Jiangchuan Wei",
        "Shiyue Yan",
        "Wenfeng Lin",
        "Boyuan Liu",
        "Renjie Chen",
        "Mingyu Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low similarity issues, primarily due to their reliance on low-level facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) a two-stage training strategy, incorporating a stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering a more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 5 4 3 1 . 1 0 5 2 : r ECHOVIDEO: IDENTITY-PRESERVING HUMAN VIDEO GENERATION BY MULTIMODAL FEATURE FUSION Jiangchuan Wei ByteDance weijiangchuan@bytedance.com Shiyue Yan ByteDance yanshiyue@bytedance.com Wenfeng Lin ByteDance linwenfeng.1008@bytedance.com Boyuan Liu ByteDance liuboyuan@bytedance.com Renjie Chen ByteDance chenrenjie.1998@bytedance.com Mingyu Guo ByteDance guomingyu.313@bytedance.com Figure 1: Sampling results of EchoVideo. (a) Facial feature preservation. (b) Full-body feature preservation. EchoVideo is capable of not only extracting human features but also resolving semantic conflicts between these features and the prompt, thereby generating coherent and consistent videos."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low similarity issues, primarily due to their reliance on lowlevel facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) two-stage training strategy, incorporating stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos."
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 2: Issues in IP character generation. (a) Semantic conflict. The input image depicts childs face, while the prompt specifies an adult male. Insufficient information interaction leads to inconsistent character traits in the models output. (b) Copy-paste. During training, the model overly relies on visual information from facial images, directly using the Variational Autoencoder(VAE)-encoded [1] face as the output for the generated face."
        },
        {
            "title": "Introduction",
            "content": "In the realm of text-to-video generation tasks, large-scale pre-trained models based on Diffusion Transformers (DiT) [2] have exhibited exceptional performance, driving significant advancements across various downstream applications [3, 4, 5, 6], particularly in identity-preserving text-to-video (IPT2V) generation. Users can input images containing facial portraits, which are subsequently utilized by text-to-image models to produce videos that feature their likeness. Recently, several methods have emerged within the open-source community [7, 8, 9, 10] targeting this domain. Notably, ConsisID [10] has achieved state-of-the-art (SOTA) verifiable results; However, it relies heavily on shallow information extracted from facial images, leading to artifacts such as rigid facial expressions and misalignment between the face and body in the generated outputs. Additionally, the generated faces are vulnerable to interference from extraneous information present in the input facial images, including occlusions. This issue is often associated with the \"copy-paste\" phenomenon. In contrast, closed-source methods such as Vidu [11], Pika [12], and HailuoAI [13] frequently encounter challenges related to low similarity between the generated faces and the input facial images. In light of the observed phenomena, we note that the process of decoupling facial identity information from portrait images often incorporates excessive irrelevant information inherent to the images themselves. For instance, the ConsisID [10] utilizes the original portrait images as strong supervisory signals, concatenating them with the initial noisy latent representations. While this approach facilitates the models rapid acquisition of high-fidelity facial information, it fails to effectively filter out other irrelevant details. This issue is illustrated in Figure 2. To address this issue, we propose EchoVideo, which integrates the IITF module specifically designed to capture high-level semantic information, including refined facial identity features. Unlike the dual facial guidance mechanisms [7, 10, 8], this structural approach facilitates pre-fusion integration that significantly simplifies the complexity of multimodal information fusion learning within the pre-trained DiT. Consequently, this enables the DiT to efficiently generate videos that reflect the target identity characteristics, guided by the pre-fusion features of IITF. Notably, this multimodal information fusion module is designed to be pluggable, facilitating seamless adaptation to other tasks and effectively addressing the challenges associated with information fusion learning in similar applications. Another existing challenge is that users often desire not only to preserve their facial identity but also to retain additional attributes such as clothing and hairstyle from the portrait images. Existing methods [14, 15, 16, 17, 18, 19] within the open-source community, require supplementary pose information for control, which considerably increases the usability barrier for users. To tackle this, we conducted experiments with EchoVideo aimed at this task, which validated that human control can be achieved solely through text prompts, demonstrating promising performance. This is illustrated in Figure 1. Our contributions can be summarized as follows:"
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 3: Overall architecture of EchoVideo. By employing meticulously designed IITF module and mitigating the over-reliance on input images, our model effectively unifies the semantic information between the input facial image and the textual prompt. This integration enables the generation of consistent characters with multi-view facial coherence, ensuring that the synthesized outputs maintain both visual and semantic fidelity across diverse perspectives. We introduce EchoVideo, an identity-preserving model based on DiT that effectively maintains high degree of identity similarity while addressing common \"copy-paste\" problem encountered in such contexts. Furthermore, EchoVideo not only preserves facial identity but also ensures consistency in full-body representations, including attributes such as clothing and hairstyle. We propose multi-modal fusion module, termed IITF, which integrates textual semantics, image semantics, and facial image identity. This module effectively extracts clean identity information and resolves semantic conflicts between modalities. To the best of our knowledge, this is the first approach that combines these three modalities for identity preservation in video generation. Additionally, this architecture is designed to be plug-and-play, allowing it to be applied to other pre-trained generative models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion for Video Generation The introduction of diffusion models has rapidly supplanted GANs [20, 21, 22] and auto-regressive models [23, 24, 25] in the fields of image and video generation due to their superior performance. Initially, video generation based on U-net models [26, 27, 28, 29, 30] primarily built upon text-to-image models [31]. These models incorporated temporal attention blocks to facilitate learning along the temporal dimension, achieving subtle video motion. However, they still fall short in terms of duration, realism, and dynamic quality compared to real videos. The emergence of SORA [32] has underscored the potential of DiT-based video generation models to significantly enhance video quality. Notably, Latte [33] marks the first application of DiT in the video generation domain, integrating temporal learning modules into text-to-video models like PixArt [34] to enable video-level transfer. Following this, there has been an explosive proliferation of DiT-based video generation works [35, 36, 37, 38, 39]. These emerging works, particularly the open-source initiatives, have accelerated the development of applications related to video generation, including image-to-video translation, video continuation, motion control [40, 41], and identity-preserving video generation. 2.2 Identity-preserving Video Generation Identity-preserving generation aims to retain distinct identity attributes in generated images or videos. Building on the remarkable success of diffusion models in text-to-image synthesis, numerous methods [42, 43, 44, 45, 46] have emerged that leverage these models for identity-preserving image generation. However, achieving identity preservation at the video level introduces additional complexity, as it requires maintaining identity consistency across frames while ensuring the naturalness of facial movements. Among the U-Net-based approaches, MagicMe [7] relies on fine-tuning with reference images, while ID-Animator [8] introduces face adapter to encode facial features, allowing for tune-free operation. However, both methods have limitations regarding facial similarity and overall video quality. In contrast, several open-source DiT-based methods, such as ConsisID [10] and Magic Mirror [9], leverage the robust CogVideoX"
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 4: Illustration of facial information injection methods. (a) Dual branch. Facial and textual information are independently injected through Cross Attention mechanisms, providing separate guidance for the generation process. (b) IITF. Facial and textual information are fused to ensure consistent guidance throughout the generation process. [37] model as foundation, integrating facial information through cross-attention mechanisms to achieve identity preservation. Validation of the ConsisID [10] model reveals that, despite generating high-quality videos and maintaining strong facial similarity, it suffers from significant \"copy-paste\" artifacts. This issuearises from the models structural design, which fails to adequately decouple facial features. To address these challenges, we propose EchoVideo, which incorporates multimodal feature fusion module named IITF. This module captures only the high-level semantic and identity information of the face while disregarding irrelevant factors such as occlusions, poses, and lighting variations. Additionally, the fused semantic features help resolve semantic conflicts across different modalities, resulting in the generation of more stable and coherent character representations in videos."
        },
        {
            "title": "3 Preliminary: Diffusion Model",
            "content": "The forward process of the diffusion model is process of progressively adding Gaussian noise to perturb the video through Markov chain, which can be expressed as zt = αtzvideo + 1 αtϵ, ϵ (0, I), (1) where zvideo is the VAE-encoded video, αt = (cid:81)t i=1(1 βi) determines the variance of the noise and controls the noise-to-signal ratio (NSR) of the noisy image. βt (0, 1) is parameter that increases monotonically over time defined in advance. When is very large, the noisy image in the forward process tends towards standard Gaussian noise. The inference (reverse) process starts from standard Gaussian noise and gradually transfers it to the target data distribution according to the condition through the Gaussian transition qθ (zt1 zt, y) = (zt1; µθ(zt, t; y), σt). [47] claims that this process can be non-Markovian, and gives the sampling formula in the reverse process as zt1 = 1 1 βt (cid:0)zt 1 αtϵθ(zt, t; y)(cid:1) + (cid:113) 1 αt1 σ2 ϵθ(zt, t; y) + σtϵ, (2) where represents the input image, σt = η βt; η [0, 1]; when η = 0, the sampling process is deterministic, known as Denosing Diffusion Implicit Model (DDIM), whereas at η = 1, the sampling process aligns with that of DDPM [48]; ϵθ(zt, t; y) is the noise estimated by using zt, t, through the denoising network, and its training loss function is L1 loss between the estimated noise and the real noise, shown as (cid:113) (1 αt1) (1 αt) = Et,ϵ[ϵ ϵθ(zt, t; y)2]. (3)"
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 5: Qualitative results. (a) Ours. (b) ConsisID [10]. (c) ID-Animator [8]. Our model can effectively overcome semantic conflicts and copy-paste phenomena while maintaining the face IP."
        },
        {
            "title": "4 Methodology",
            "content": "4.1 Overall Architecture The overall structure of EchoVideo is shown in the Figure 3. The model architecture is based on conditional DiT video generation model. Through innovative fusion of the target persons facial image and text prompt, we achieve precise preservation of personal identity characteristics to generate high-quality personalized video content. For image input with faces Iin RHW 3, we first use face extractor to locate the facial region If ace RH 3, If ace = ACE_EXT RACT OR(Iin) (4) To comprehensively capture facial feature information, we design dual-branch feature extraction architecture to obtain facial visual features Fvision and identity information Fid, considering both overall semantic information and local fine-grained features of the face to provide more complete identity information for subsequent video generation. Specifically, Fvision is obtained using the high-performance SigLip [49] encoder: Fvision = SigLip(If ace) (5) This feature contains high-level semantic information such as overall facial structure. Fid contains local features of facial details, ensuring precise modeling of key facial regions, extracted through arcface [50]: Fid = arcf ace(If ace) (6) For the obtained facial features, we use the Identity Image-Text Fusion (IITF) module to fuse them with textual description information, achieving alignment and integration between different modalities. This module can correct potential inconsistencies between text descriptions and actual facial attributes (such as age, gender, etc.), ensuring identity consistency in the final generated content through feature-level correction. Additionally, similar to previous works, we use VAE to encode the obtained facial region as conditional input for DiT, ensuring that the model can obtain correct low-dimensional visual information while maintaining facial semantic information."
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 6: Effect of the IITF module. (a) Without IITF. (b) With IITF. IITF can effectively extract facial semantic information and resolve conflicts with text information, generating consistent characters while maintaining the face IP. 4.2 Identity Image-Text Fusion Module In existing IP video generation methods, there are notable limitations in how textual and facial modality information is utilized. Specifically, these two modalities are injected through separate Cross Attention modules, design that prevents the model from effectively coordinating and integrating character feature information contained in different modalities. This problem becomes particularly evident when there are discrepancies between the input facial image and text description. As shown in the Figure 2(a), the facial image shows child while the text description refers to an adult male. Due to the lack of deep inter-modal interaction, the model often produces simple \"face swap\" effect rather than truly understanding and fusing feature information from both modalities. Therefore, in this paper, we propose IITF to fuse text and facial information, establishing semantic bridge between facial and textual information, coordinating the influence of different information on character features, thereby ensuring the consistency of generated characters. IITF consists of two core components: facial feature alignment and conditional feature alignment. Facial Feature Alignment. Since SigLip [49] is pretrained image-language model, Fvision can naturally align with the semantic space of text. However, Fid is obtained through pure vision model. Directly fusing it with Fvision would make it difficult for the model to effectively utilize both types of information due to feature space misalignment. Therefore, in IITF, we introduce learnable lightweight mapping module Prj [46] to map Fid, aligning it with Fvision in feature space: id = rj(Fid) Since Fid focuses more on detailed facial features while Fvision focuses more on overall facial information and is less affected by external changes like lighting and occlusion, fusing both types of information ensures accuracy of facial features in generated results: (7) Ff ace = LP (concat(Fvision, id)) (8)"
        },
        {
            "title": "Running Title for Header",
            "content": "Figure 7: Effect of using facial visual features encoded by VAE. (a) Without face visual features. (b) With face visual features. By using the facial visual information , the facial details in the generated video can be effectively supplemented. Conditional Feature Alignment. Text descriptions Fprompt typically contain key attribute information of the target person (such as gender, age, etc.), while facial features Ff ace also implicitly contain related visual features. Inconsistencies between these two types of information often lead to \"copy-paste\" phenomena in generated results. To avoid this, in IITF, we fuse facial features Ff ace with text features Fprompt. Through interaction between both types of information, we unify the guidance direction for character features in the generation process, ensuring consistency of character features in output results: Fcond = LP (concat(Fprompt, Ff ace)) (9) 4.3 Data and Training Although Ff ace can effectively extract high-level semantic information of faces, low-dimensional visual information about faces is often insufficient, leading to loss of high-frequency facial details in generated videos, producing blurry effects. Therefore, similar to previous works, we use VAE-encoded features zf ace of padded If ace as conditional input to the model. Since zf ace is strong condition, this would cause the model to attempt using zf aces face directly for all video frames, producing stiff expressions and fixed viewpoints, as shown in the figure. Therefore, to avoid excessive model dependence on zf ace, we adjusted our training data and strategies. Data. In the training data, we try to select faces outside of the training videos, avoiding facial information with the same lighting and angles as in the videos, driving the model to generate faces from different points of view based on the Ff ace information. Additionally, we randomly drop zf ace during training to ensure correct perception of Ff ace. Loss function. Besides using the L2 loss function shown in equation 2 to supervise noise prediction in the diffusion process, we added extra loss for facial regions to ensure model perception of faces. From equation 1, the original features can be calculated from model-predicted noise as: zpred = (zt 1 αt 1 αtϵθ) (10) where ϵθ is the predicted noise by model. In previous work, supervision of latent-space faces was often based on downsampled facial masks in pixel space, ensuring computational efficiency during loss calculation. However, due to spatiotemporal interactions during video VAE encoding, obtaining latent space masks directly through resizing may lead to misalignment issues. Therefore, we propose using MTCNN-extracted face detection boxes to supervise facial regions: Lbox = askboxzvideo zpred2 (11) (12) The final loss function used is: = Lddpm + λLbox"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Settings Dataset Preparation. We utilized text-to-video dataset collected from the Internet as our primary dataset and filtered it to select segments containing only single individual for training purposes. To establish image-video pairs, we first"
        },
        {
            "title": "Models",
            "content": "CLIPScore DynamicDegree AestheticQuality FID FaceSim ID-Animator [8] consisID [10] EchoVideo 28.921 30.610 30.567 0.280 0.871 0. 0.595 0.594 0.601 159.109 200.400 154.678 0.349 0.414 0.138 Table 1: Quantitative results. The best is in bold. Addressing the \"copy-paste\" issue that leads to reduced facial similarity. applied face detector on the raw text-to-video content, extracting one frame per second for face detection. We retained only those detected faces that exhibited high quality and moderate face sizes across the entire video. Subsequently, we employed face recognition model to extract facial features from all detected faces within the video. If the extracted facial features clustered into single identity, the video was preserved; otherwise, it was discarded due to the presence of multiple identities. In constructing the image-video pair dataset, we generated three distinct types of data. Paired Data. For each text-to-video pair, we uniformly sampled five frames containing faces to serve as the corresponding images for that video. This approach resulted in the construction of dataset comprising 3M paired data samples. Cross-Paired Data. To mitigate the risk of the model learning trivial copy-paste shortcut solution, we created cross-paired data by interleaving images of the same individual from different video. This strategy yielded an additional 0.3M cross-paired data samples. Generated Cross-Paired Data. To further enhance the diversity of non-identity information in the imagevideo pairs, we employed an identity-preserving text-to-image model [43], to generate faces with varying poses, lighting conditions, and expressions based on the faces detected in the video. This process resulted in the creation of another 0.3M cross-paired data samples. Training Details. We utilized CogVideoX-5B [37] as the foundational model, employing Siglip [49] to extract facial visual features and face recognition network [50] to obtain facial embeddings. The input consisted of video data at resolution of 480p, comprising 49 frames per video. The training process was divided into two phases: the pre-training phase of the IITF module and the subsequent training of the entire EchoVideo model, which incorporated facial visual features encoded by VAE. During the pre-training phase of the IITF module, we set the batch size to 256 and the learning rate for IITF to 2 104, with total of 20K iterations. In the complete training phase of the EchoVideo, we increased the batch size to 320 and adjusted the learning rate to 2 105, conducting total of 50K iterations. Additionally, the drop ratio for image latent, which serve as shallow facial information, was set to 0.1, while the weight for the mask loss was configured to 1.0. Evaluation. To mitigate the influence of visual feature extractors on celebrity faces, we carefully selected 23 images of ordinary individuals from the FFHQ dataset [21] as test subjects, ensuring these individuals were not part of the training set. The selection aimed for balanced representation across gender, age, and ethnicity, while also considering diversity in facial poses, lighting conditions, and occlusions. We designed 23 prompts based on the methodology from VBench [51], corresponding to various scenarios, activities, and facial poses. The evaluation of the IPT2V model emphasized human-annotated results. We selected baseline model and employed the Good Same Bad (GSB) metric for quantification. Evaluation metrics were categorized into two dimensions: video quality and identity preservation. The video quality dimension followed the established text-to-video evaluation framework, subdivided into sensory quality, adherence to instructions, physical simulation, and coverage quality. The identity preservation dimension involved annotators comparing which video maintained closer identity consistency with the provided inference face image. Additionally, we utilized various tools, including VBench [51], to assess video generation quality and facial recognition embedding similarity for evaluating identity preservation effectiveness. Models CLIPScore DynamicDegree AestheticQuality FID FaceSim w/o IITF w/o Two-Stage EchoVideo 31.418 29.480 30.567 0.722 0.606 0.771 0.579 0.587 0. 254.469 155.557 154.678 0.025 0.135 0.138 Table 2: Ablation study on IITF and training strategy. \"w/o IITF\" denotes the exclusion of IITF module. \"w/o Two-Stage\" indicates the omission of the second training stage."
        },
        {
            "title": "Overall Quality",
            "content": "-11.15% -9.42% -13.04% Table 3: User study between EchoVideo and state-of-the-art method[10]. Each column displays the net good rate (good%-bad%). 5.2 Results Qualitative Results. We present comparative analysis of EchoVideo against other methods in Figure 5. The IDAnimator [8] model, limited by parameter scale, exhibits noticeably inferior visual quality and realism compared to the other two approaches, and it shows lack of responsiveness to text prompts. Meanwhile, ConsisID [10] suffers from an over-reliance on shallow information, resulting in pronounced copy-paste effect in the generated faces. For instance, in case 1, the \"coffee cup\" from the reference image appears unchanged in the generated video. Additionally, ConsisID [10] fails to effectively integrate multimodal information, leading to semantic conflicts that manifest as \"head-body coordination\" issues, as shown in case 2. In contrast, our method not only achieves video quality and identity preservation comparable to ConsisID [10] but also effectively addresses the aforementioned challenges. Quantitative Results. We present the quantitative results of various methods across five dimensions in Table 1. The evaluation of generated videos is conducted using the dynamic degree and aesthetic quality metrics from VBench [51] to assess motion amplitude and visual appeal, respectively. Additionally, we employ the CLIPScore [52] to measure adherence to textual instructions. The Fréchet Inception Distance (FID) [53] is utilized to quantify the visual quality of the generated videos. Finally, we calculate the average cosine similarity of facial features between the reference image and the video frame image using FaceSim [50]. In terms of video quality, EchoVideo demonstrates superior advantage; however, it falls short of ConsisID [10] in the dimensions of identity preservation and video dynamism. The remaining dimensions show comparable performance to other methods. 5.3 User Studies We conducted user study on the 529 videos to evaluate human preferences between our proposed EchoVideo, and ConsisID [10], with the latter serving as the baseline model. Each video pair was assessed by five annotators, and the outcome was determined through voting mechanism. In non-tied cases, the majority decision prevailed. For tied votes, we used scoring system: \"good\" scored 1, \"same\" scored 0, and \"bad\" scored -1. The cumulative score classified the result as \"good\" (> 0), \"same\" (= 0), or \"worse\" (< 0). The statistical analysis of the metrics, differentiated by gender, is presented in Table 3, where it can be observed that the proposed EchoVideo performs slightly worse compared to the ConsisID [10]. Nevertheless, it is evident that our model achieves state-of-the-art performance in this task. 5.4 Ablation Studies In this section, we conduct ablation studies to demonstrate the effectiveness of our IITF module and training strategy. IITF. To demonstrate that the IITF module effectively integrates identity information for controlling the face generation in videos while decoupling irrelevant factors from the reference image, such as occlusion and lighting, we conducted an ablation study by replacing the IITF module with unimodal text encoder. As illustrated in Figure 6, the absence of the IITF module leads to loss of identity control in the generated videos. This finding is further supported by the significant decline in facial similarity observed in Table 2, where the FaceSim scores indicate marked decrease in identity preservation. Training Strategy. We compared the necessity of incorporating shallow facial information training in the second phase of our training strategy. As shown in Figure 6, the absence of VAE-encoded image latent leads to degradation of facial detail in the generated outputs compared to the complete EchoVideo results, particularly in the regions of the eyes and eyebrows. This observation is further supported by the FaceSim scores presented in Table 2, which indicate that the training conducted in the second phase effectively enhances facial similarity."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce EchoVideo, unified framework for achieving identity consistency in human video generation through multimodal feature fusion. This framework not only maintains consistency in facial features but also extends to full-body representations. The multimodal integration module, IITF, is specifically developed to effectively capture clean facial identity information while aligning semantic information across different modalities in pre-fusion manner. By"
        },
        {
            "title": "Running Title for Header",
            "content": "leveraging IITF, we effectively address the copy-paste issue commonly encountered in video generation. Furthermore, EchoVideo can be seamlessly integrated into existing DiT-based or U-Net-based text-to-video or text-to-image models. Extensive experiments demonstrate that EchoVideo achieves state-of-the-art performance in identity-preserving tasks."
        },
        {
            "title": "References",
            "content": "[1] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Structured causal disentanglement in variational autoencoder. arXiv preprint arXiv:2004.08697, 2020. [2] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [3] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [4] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224, 2024. [5] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generation-reconstruction cycle. arXiv preprint arXiv:2407.19548, 2024. [6] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate drag-based image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024. [7] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. [8] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. [9] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magic mirror: Id-preserved video generation in video diffusion transformers. arXiv preprint arXiv:2501.03931, 2025. [10] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [11] Shengshu Technology. Vidu. https://www.vidu.com/, 2024. [12] Pika Labs. Pika. https://pika.art/, 2025. [13] Minimax. Hailuo. https://hailuoai.video/, 2025. [14] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. [15] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [16] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [17] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [18] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. [19] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [21] Tero Karras. style-based generator architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2019. [22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020."
        },
        {
            "title": "Running Title for Header",
            "content": "[23] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [24] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [26] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [27] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [28] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. [29] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024. [30] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2025. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [33] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [34] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [35] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [36] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [38] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [39] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [40] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [41] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024."
        },
        {
            "title": "Running Title for Header",
            "content": "[42] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. [43] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [44] Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, and Yi Yang. Caphuman: Capture your moments in parallel universes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64006409, 2024. [45] Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. arXiv preprint arXiv:2404.16771, 2024. [46] Cheng Yu, Haoyu Xie, Lei Shang, Yang Liu, Jun Dan, Liefeng Bo, and Baigui Sun. Facechain-fact: Face adapter with decoupled training for identity-preserved personalization. arXiv preprint arXiv:2410.12312, 2024. [47] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [48] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [49] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [50] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [51] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [52] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}