{
    "paper_title": "Reasoning-Augmented Representations for Multimodal Retrieval",
    "authors": [
        "Jianrui Zhang",
        "Anirudh Sundara Rajan",
        "Brandon Han",
        "Soochahn Lee",
        "Sukanta Ganguly",
        "Yong Jae Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval."
        },
        {
            "title": "Start",
            "content": "Reasoning-Augmented Representations for Multimodal Retrieval Jianrui Zhang * 1 Anirudh Sundara Rajan * 1 Brandon Han 1 Soochahn Lee 2 Sukanta Ganguly 3 Yong Jae Lee 1 Abstract Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry silent evidence and queries leave key semantics implicit, single embedding pass must both reason and compress, encouraging spurious feature matching. We propose data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using strong VisionLanguage Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledgeintensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https: //github.com/AugmentedRetrieval/ ReasoningAugmentedRetrieval. 6 2 0 2 6 ] . [ 1 5 2 1 7 0 . 2 0 6 2 : r 1. Introduction Universal Multimodal Retrieval (UMR) aims to support information seeking across arbitrary combinations *Equal contribution 1University of Wisconsin-Madison 2Kookmin University 3NetApp, Inc. Correspondence to: Jianrui Zhang <harrisz@cs.wisc.edu>, Anirudh Sundara Rajan <asundararaj2@wisc.edu>. Preprint. February 10, 2026. 1 of modalitiestext-to-image, image-to-text, and imageto-imagewithin single retrieval system. dominant paradigm is to learn shared embedding space in which semantically corresponding inputs are nearby. Large-scale contrastive pretraining has yielded strong multimodal representations: CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023), and ImageBind (Girdhar et al., 2023) use modality-specific encoders and achieve impressive crossmodal retrieval performance. However, their designs are fundamentally strained by mixed-modality inputs (e.g., an image paired with textual qualifier), typically resorting to ad hoc fusion strategies (Wei et al., 2024) that can be brittle. An alternative is to leverage multimodal large language models (MLLMs) (Liu et al., 2023) as embedding models, which can natively ingest and jointly interpret imagetext inputs; recent work has demonstrated the promise of this direction (Jiang et al., 2024; 2025; Liu et al., 2025a). Despite this progress, recent benchmarks such as M-BEIR (Wei et al., 2024) highlight persistent gap between retrieval and information seeking. Many queries require nontrivial interpretation before they can be matched to corpus entry. For example, given an image of an unfamiliar building and the query Who designed this?, retriever must implicitly (i) recognize or disambiguate the depicted structure, (ii) infer that the desired attribute is the architect, and (iii) connect that attribute to the appropriate passage or entity in the corpus. When these intermediate steps remain implicit, existing models frequently fall back on superficial cuesmatching sky color, viewpoint, or low-level texturerather than the underlying intent, leading to failures driven by spurious correlations. These failure modes are not unique to any one architecture; they arise whenever the representation must simultaneously infer what to match and compress the input into single vector. This observation motivates simple but consequential hypothesis: expecting single embedding forward pass to perform both reasoning (deciding which latent aspects of the input are relevant) and compression (mapping the result into compact vector) is often too demanding, especially for underspecified queries. Queries in UMR commonly contain indirect references (this, the one on the left), missing attributes (the designer vs. the architect), or verbose instructions whose core constraints are buried in irrelevant Reasoning-Augmented Representations for Multimodal Retrieval text. When the semantics are not made explicit, the embedding model is forced to approximate the missing reasoning with heuristics, which amplifies spurious matching. We therefore propose to decouple reasoning from compression by explicitly materializing the latent reasoning steps as text annotations and then training the retriever to operate on this dense semantic form. Concretely, we introduce reasoning-augmented enhancement pipeline that shifts interpretive burden away from the embedding model and into separate, high-capacity Vision Language Model (VLM). The VLM is used to (i) caption salient visual details that are otherwise implicit, (ii) resolve ambiguous references in multimodal queries, and (iii) rewrite verbose instructions into concise, retrieval-oriented constraints. In effect, the pipeline externalizes the hidden chain of reasoning that retriever would otherwise need to perform internally. This turns difficult reasoning-thenretrieve problems into explicit semantic matching: once the relevant attributes and referents are stated plainly, standard embedding model can focus on faithful compression and similarity search rather than guessing what the query meant. key advantage of this approach is practicality. Corpusside enhancement is one-time offline cost, and query-side enhancement can be performed efficiently at test time with modest overhead. More importantly, we find that the benefit is not merely an inference-time artifact of higher-quality text. Training retrieval backbones on the enhanced distribution yields consistent and substantial improvements over strong baselines across M-BEIR (Wei et al., 2024). Crucially, our results indicate that models must be re-aligned to operate on these explicit, semantically dense inputs; simply enhancing queries at inference time is insufficient to eliminate spurious feature matching. Together, these findings suggest that robust UMR is less about ever more expressive embedding architectures and more about making the semantics matchable: by converting implicit reasoning into explicit, verifiable descriptions, we enable retrieval models to generalize beyond shallow correlations and better satisfy real information-seeking intent. 2. Related Work Foundational multimodal retrieval. Early multimodal retrieval systems largely relied on dual-encoder architectures trained with contrastive objectives to align image and text representations in shared embedding space. CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023) demonstrated that large-scale imagetext contrastive pretraining enables strong zero-shot transfer and cross-modal matching. Building on this paradigm, UniIR (Wei et al., 2024) introduced an instruction-guided retriever for universal multimodal retrieval and established the M-BEIR benchmark, which emphasizes realistic information-seeking queries 2 Figure 1. Overview of our reasoning-augmented multimodal representation. We use strong VLM to (i) inject descriptive text into image-only inputs and (ii) refine captions for imagetext pairs, making key visual semantics explicit. This externalizes implicit reasoning, allowing the retriever to focus on compression into robust embeddings rather than on-the-fly visual inference. Red highlights denote the matching components on both the query and corpus sides with the help of VLM-generated enhancements. Reasoning-Augmented Representations for Multimodal Retrieval across heterogeneous tasks. However, dual encoders typically process each modality independently and compress inputs into single vector, which can be brittle when queries require joint interpretation of mixed-modal inputs or latent reasoning (e.g., resolving references such as this or inferring the intended attribute in Who designed this?). This brittleness is increasingly exposed by M-BEIR-style settings, where shallow correlations can dominate when the underlying intent is underspecified. VLM-based universal embeddings. The rise of large visionlanguage models (VLMs) has shifted retrieval toward embedding models that can natively consume multimodal inputs and perform richer cross-modal interactions before compression. E5-V (Jiang et al., 2024) adapts VLM backbones for universal embeddings and argues that strong transfer can be obtained even with predominantly text-pair supervision. In contrast, VLM2Vec (Jiang et al., 2025) demonstrates benefits from explicitly multimodal supervision. Subsequent work has focused on training recipes and architectural adaptations that improve universal embedding quality, including specialized pooling or embedding tokens (Liu et al., 2025a), hard-negative strategies and reranker distillation (Li et al., 2025), and multi-stage pipelines that achieve strong performance on newer benchmarks (Li et al., 2026). These advances primarily improve how embeddings are learned (architecture, losses, distillation, mining). In contrast, our work targets what is being embedded: we identify data/representation bottleneck where queries and corpus entries lack sufficient semantic explicitness, forcing embedding models to simultaneously infer intent and compress, which encourages spurious matching. Rather than modifying the retriever architecture, we use VLM to externalize latent reasoning into explicit textual form, and show that training on this semantically denser distribution yields consistent gains. Data-centric and task-specialized retrieval. complementary line of research improves retrieval by tailoring representations and interaction mechanisms to specific modalities or by altering the training data. For document-centric visual retrieval, ColPali (Faysse et al., 2025) adopts late interaction (Khattab & Zaharia, 2020) to reduce reliance on brittle OCR pipelines and better capture token-level evidence. Extensions such as VLM2Vec-v2 (Meng et al., 2025) broaden universal embeddings to additional modalities (e.g., video) and domains (e.g., visual documents). On the data side, GME (Zhang et al., 2025) synthesizes large-scale fused-modal training pairs to mitigate modality imbalance and enrich supervision. While such approaches increase coverage by creating new training data or designing modality-specific mechanisms, our approach is orthogonal and lightweight: we refine the semantic density of the existing benchmark inputs by resolving ambiguities, explicitly captioning salient visual evidence, and rewriting verbose instructions into retrieval-oriented constraints. This shifts implicit reasoning from the embedding model to an external VLM, effectively converting reason-then-retrieve into explicit semantic matching, and complements prior improvements in architecture and training strategy. 3. Method Our goal is to improve universal multimodal retrieval by decoupling reasoning from compression. In standard embedding-based retrieval, single model must (i) infer the intended semantics of an underspecified multimodal input (e.g., resolve textual references to objects in an image or identify described visual attributes) and (ii) compress the result into compact vector for nearest-neighbor search. If these reasoning steps are implicitly learned, embeddings can overfit to shallow cues and spurious correlations. We instead explicitly externalize the intended semantics of inputs by using strong VLM (in our case, Qwen3-VL-8B (Bai et al., 2025)) to produce semantically dense, self-contained textual information that allows embedding models to focus on data compression. We implement this idea through reasoning-augmented enhancement pipeline with the following two components: (1) corpus enhancement, which unsilences visual evidence in database items by adding explicit textual descriptions of visual information; and (2) query enhancement, which resolves ambiguous references and rewrites verbose instructions into retrieval-oriented constraints. The enhanced data is then used to train (or fine-tune) standard retrieval backbone, aligning the embedding space to this semantically explicit distribution. 3.1. Problem Setup and Notation Let retrieval corpus be = {di}N i=1, where each item di may contain text ti, an image vi, or both. query similarly may contain text tq, an image vq, or both. An embedding retriever parameterized by θ maps an input to vector representation: zq = fθ(q), zi = fθ(di), and retrieves by similarity search between zq and zi. Our method introduces an enhancer (a VLM) that transforms inputs to have more explicit textual forms: di = E(di), = E(q), where di and are semantically denser, self-contained representations. The retriever is trained on these enhanced forms, i.e., zq = fθ(q), zi = fθ( di). Intuitively, performs the interpretation step (e.g., captioning, reference resolution, constraint extraction), while 3 Reasoning-Augmented Representations for Multimodal Retrieval fθ focuses on stable compression for retrieval. 3.2. Corpus Enhancement Real-world multimodal corpora typically contain three entry types: (I) text-only, (II) image-only, and (III) imagetext pairs. Our enhancement targets missing visual semantics; thus Category is left unchanged. Category II: image-only entries (unsilencing visual evidence). For image-only items, the decisive evidence is often entirely visual, making them silent to text-centric embedding model. Therefore, to effectively extract the image semantics and enrich the dataset item, we generate dense visual caption that enumerates salient objects, attributes, relations, and distinctive cues. We prompt the VLM to produce keyword-rich description (about 100 words) rather than fluent prose. This design prioritizes lexical coverage of discriminative attributes (e.g., arched bridge, gothic facade, two people holding surfboards), which empirically is more helpful for embedding-based matching than stylistic verbosity. Dense captions also reduce ambiguity by making implicit evidence explicit and directly matchable. Category III: imagetext entries (preserving metadata while adding visual grounding). In imagetext entries, the accompanying text often only provide contextual information like encyclopedic content. Textual descriptions of the visual data are either high-level at best. To enrich these entries, we append an image caption to the original query text, i.e., {original text}nVisual Context: {caption}. Appending our generated caption to the existing data enjoys two benefits over either entirely replacing the original text or interleaving within it. Firstly, the original text may include critical metadata (e.g., names, dates, definitions, or narrative content) that distinguishes our corpus item. Altering or removing this data would have significant negative impact, especially in cases where multiple corpus items have similar images. Secondly, it makes the augmentation robust to moderate caption noise, since the retriever can learn to rely on whichever span (i.e., original or caption) provides the most reliable evidence for the task. 3.3. Query Enhancement Queries follow the same three types as corpus entries. We do not modify Category (text-only) queries. For the remaining types, the objective is to convert underspecified multimodal queries into self-contained text that exposes the intended semantics. Category II: image-only queries, we generate dense caption using the same schema image-only queries. For as for corpus images, but with tighter length budget (about 50 words). We present captions that should capture the primary discriminative evidence while avoiding overly granular details that can introduce noise and dilute the query intent. Category III: imagetext queries. Mixed-modal queries commonly fall into two formats, which require different forms of externalized reasoning: (a) QA-style questions that require reference resolution and intent completion, and (b) modification requests that require constraint extraction and target specification. 3.3.1. QA-STYLE QUERIES QA-style queries often include ambiguous deictic references (e.g., When was this animal discovered?) whose referent is only identifiable from the image. To retrieve the correct document, system must first resolve the referent and make the question explicit. We prompt the VLM to rewrite such queries into explicit text by: (i) identifying the referent when it is recognizable (e.g., giant panda, Eiffel Tower), and (ii) substituting vague phrases (this animal, this building) with the resolved entity name. When canonical name is uncertain, the VLM emits concise visual descriptor (e.g., red noodle soup with shrimp; black-and-yellow wasp). The result is self-contained query that can be matched via standard text-based semantics, reducing the need for the retriever to perform implicit visual reasoning. 3.3.2. MODIFICATION REQUESTS Modification-style queries specify desired transformation of reference image (e.g., given an image of dog in field, change to cat), where the retrieval target should satisfy the modified constraints (e.g., cat running in field). These requests are frequently verbose, include superfluous phrasing, and bury the actionable constraints. We prompt the VLM to distill the request into concise, keyword-based constraint set (e.g., Unlike the reference image, want the target image to have shorter hair and more dogs shorter hair; more dogs). This representation is well-suited to embedding retrieval: it foregrounds the discriminative attributes the target should satisfy and improves compositional matching. Crucially, when rewriting modification requests, we exclude the reference image from the VLM input. VLMs trained with dense captioning priors exhibit strong tendency to describe the visible image, even when the instruction concerns target that differs from the reference. Including the image can cause contextual contamination, where the rewritten query is biased toward properties of the reference rather than the intended modification. Conditioning only on the instruction text encourages faithful constraint extraction and 4 Reasoning-Augmented Representations for Multimodal Retrieval yields more consistent retrieval-oriented rewrites. 3.4. Training on Enhanced Representations The enhancement pipeline changes the semantic density and surface statistics of both queries and corpus entries. Accordingly, we train the retrieval backbone on enhanced pairs so that its embedding space is aligned with the explicit semantics produced by E. This training step is conceptually simpleit uses standard retrieval objectives (e.g., contrastive learning) on (q, d+) pairsbut is essential for ensuring that the model leverages the enhanced signal rather than defaulting to shallow correlations learned from the original, underspecified distribution. 4. Experiments In this section, we conduct exhaustive experiments by training retrieval models on both the enhanced and unenhanced M-BEIR datasets (Wei et al., 2024). We use M-BEIR due to its vast coverage of multiple task types of multimodal retrieval. We first discuss our training recipe in 4.1, then discuss our main results in 4.2. We conduct ablation studies dissecting performance improvements in 4.3 and justifying the necessity of training on our enhanced data in 4.4. Finally, we provide qualitative results to more directly demonstrate the superiority of our method in 4.6. We also report results on composed image retrieval tasks from MVRB (Liu et al., 2025b) in Appendix and report results of variants trained with hard negatives in Appendix C. 4.1. Training Recipe We adopt the training recipe from LamRA-Ret (Liu et al., 2025a), which adapts VLM for retrieval tasks by introducing special embedding token, <emb>. Specifically, we prepend task-specific instructions and append the prompt Summarize the above into one word: <emb> to the input sequence and utilize the second-last hidden state corresponding to this token to compute contrastive losses between the query and the positive candidates from the corpus. We skip stage 1 text-only pretraining as we only focus on multimodal retrieval. We use Qwen3-VL-2B (Bai et al., 2025), an open-source state-of-the-art VLM chosen for its balance of performance and deployment efficiency, as our base model to finetune from for retrieval. Each model takes only 7 hours to train, and takes around 2-3 hours to evaluate on all reported benchmarks. For all query and corpus enhancements, we utilize Qwen3VL-8B (Bai et al., 2025). Our methodology requires that both the corpus and query sets undergo enhancement prior to training. To facilitate this, we provide scripts and prompts enabling Qwen to generate enhanced queries and captions for the target database. Utilizing 8A100-80GB GPUs, we can complete the enhancement of the entire M-BEIR corpus and query seta dataset exceeding 7 million entriesin approximately 8 hours. During inference, our code can easily be incorporated into the system as an online enhancement tool to refine user queries on the go with negligible overhead. All prompts utilized in this pipeline are provided in Appendix A. 4.2. Main Results The comprehensive results of our enhanced retrieval pipeline are presented in Table 1. By systematically addressing the semantic gaps in both queries and corpus entries, our method achieves consistent improvements across the majority of M-BEIR benchmarks compared to the baseline LamRA implementation. critical observation in our results is the substantial improvement in the WebQA and InfoSeek benchmarks (e.g., Infoseek-8 R@1 +2.84%, WebQA-1 R@1 +1.59%). Our query enhancementresolving ambiguous terms like this animal into named entities (e.g., Great Panda)directly benefits InfoSeek and OVEN-6, where precise entity matching is paramount. We even observe significant 1.6% improvement on WebQA, text-to-text retrieval task. It is important to highlight that our method does not alter text-to-text retrieval pathways; we strictly preserve original text-only queries and corpus entries. Despite this, we observe outsized gains in these tasks. This confirms that the primary bottleneck in previous baselines was not the textual modality itself, but the disconnect between textual queries and visual evidence. By augmenting the image-text corpus with our generated Visual Context, the modality gap between image and text is further reduced during training. We hypothesize that this has indirectly caused the improvement in the quality of text-only embeddings. For standard cross-modal tasks such as MSCOCO and VisualNews, our corpus enhancement strategy yields notable gains (e.g., VisNews-0 R@1 +1.03%). In these datasets, the raw images often contain visual details that are not captured in the original, often noisy, alt-text or captions. Our dense captioning pipeline extracts these latent features into keyword-rich text, providing the retrieval model with higher-resolution semantic signal during the contrastive learning phase. In the CIRR benchmark, which tests the models ability to modify visual query based on text instructions (e.g., change dog to cat), we achieve clear improvement over the baseline (R@1 +1.3%). This validates our Change Request query enhancement strategy. By distilling verbose natural language commands (e.g., would like the new image to show...) into concise, keyword-driven instructions, we reduce the cognitive load on the retrieval model, preventing it from being distracted by irrelevant linguistic 5 Reasoning-Augmented Representations for Multimodal Retrieval Table 1. Recall Scores Comparison (Baseline vs. Ours). Each task is named after the original benchmark name dash the task number. Tasks are grouped by retrieval category: General, Knowledge, and Composed. Each task type is labelled by query type (i means image and means text) corpus type. Improvements using Ours are bolded. We gray out the results for FashionIQ and Fashion200k due to unreliable ground-truth. Task Type Baseline (LamRA-Ret (Liu et al., 2025a)) Ours R@1 R@5 R@10 R@50 R@1 R@5 R@10 R@ MSCOCO-0 MSCOCO-3 VisNews-0 VisNews-3 WebQA-1 WebQA-2 InfoSeek-6 InfoSeek-8 OVEN-6 OVEN-8 EDIS-2 Nights-4 CIRR-7 FashIQ-7 Fash200k-3 qt ci qi ct qt ci qi ct qt ct qt ci, ct qi, qt ct qi, qt ci, ct qi, qt ct qi, qt ci, ct qt ci, ct qi ci qi ci qi, qt ci qi ct 54.19 69.90 13.95 12.08 57.31 49.54 21.64 25.50 24.82 47.65 26.97 8.40 21.41 10.48 4.91 79.36 89.66 28.47 26.16 83.58 77.78 41.43 49.72 47.96 68.11 53.38 30.85 49.35 23.55 13.11 87.18 94.42 35.70 33.62 90.14 86.62 51.51 60.81 57.79 75.24 64.33 49.01 60.77 30.67 18.82 97.96 99.36 53.89 52.00 97.07 96.26 72.84 80.34 76.22 87.37 82.81 92.88 82.81 50.07 37. 55.57 70.80 14.98 12.88 58.90 51.10 22.10 28.34 25.47 47.46 27.12 8.02 22.71 10.48 4.89 80.20 90.02 29.70 26.82 84.89 79.61 42.08 52.38 48.32 68.56 53.29 31.56 49.40 22.92 13.07 87.62 94.34 37.14 34.17 91.00 86.74 51.13 62.21 57.89 74.80 63.87 50.57 59.93 30.35 18.88 98.12 99.52 55.42 53.02 97.39 96.81 71.37 80.93 75.94 86.69 83.46 92.74 82.73 49.01 37.90 are penalized as false negatives. Thus, the flat or slightly regressed scores in these rows reflect limitation of the evaluation metric rather than degradation in model capability. An exceptional error case from Fashion200k-3 can be seen in Figure 2, where the enhanced query accurately describes the clothing in the image, allowing the retrieval of multiple results of similar meanings, while the ground truth is erroneous. 4.3. Ablation Study 1: Disentangling Enhancements To disentangle the contributions of our pipeline, we evaluate the retrieval performance (R@1) across four settings: the unenhanced baseline, query enhancement only (Q-Only), corpus enhancement only (C-Only), and the full pipeline (Full). The results are summarized in Table 2. Synergy of Components The results demonstrate that our full pipeline achieves the highest average recall (30.72), confirming that query and corpus enhancements are complementary. While individual modules provide gains, their combination is essential for MSCOCO-0, VisNews-0/3, WebQA2, and InfoSeek-6. This suggests that complex QA tasks require both clearer question (from Q-Enhancement) and semantically richer target (from C-Enhancement) to bridge the modality gap. Impact of Query Enhancement Surprisingly, Query Enhancement alone (Q-Only) drives the majority of the performance gains (Avg: 30.51), significantly outperforming Corpus Enhancement alone (Avg: 29.96). This gap is mostly contributed by InfoSeek-8, where Q-Only yields remarkFigure 2. In this entry of Fashion200K, the query asks for description matching the image, which Qwen accurately captioned with keywords such as black dress and flared sleeves. This allowed the retrieval of multiple correct results, indicating how the one-ground-truth design is flawed. Furthermore, the ground truths multicolored description is simply untrue, reinforcing our observation over these benchmarks low quality nature. noise. We explicitly gray out the results for FashionIQ and Fashion200k in Table 1. These benchmarks present unique evaluation challenge: while the corpus contains tens of thousands of items, the ground truth for given query is often restricted to single specific image ID. In the fashion domain, however, hundreds of items may share identical visual attributes (e.g., black leather jacket). Our enhanced model frequently retrieves these visually correct candidates, but because they do not match the specific ground truth ID, they 6 Reasoning-Augmented Representations for Multimodal Retrieval Table 2. Ablation of component contributions on Recall@1. QOnly: Training with enhanced queries but original corpus. C-Only: Training with enhanced corpus but original queries. We observe that corpus enhancement drives knowledge tasks, while query enhancement is critical for modification tasks. Best performances per row are bolded."
        },
        {
            "title": "Task",
            "content": "Baseline Q-Only C-Only"
        },
        {
            "title": "Full",
            "content": "MSCOCO-0 MSCOCO-3 VisNews-0 VisNews-3 WebQA-1 WebQA-2 InfoSeek-6 InfoSeek-8 OVEN-6 OVEN-8 EDIS-2 Nights-4 CIRR-7 FashIQ-7 Fash200k-"
        },
        {
            "title": "Average",
            "content": "54.19 69.90 13.95 12.08 57.31 49.54 21.64 25.50 24.82 47.65 26.97 8.40 21.41 10.48 4.91 29.92 54.77 71.16 13.80 12.78 56.17 50.85 21.61 30.37 25.42 47.43 26.81 8.49 22.76 10.21 5.03 30.51 54.89 70.20 14.69 12.47 56.17 49.14 19.89 25.98 24.62 47.55 27.89 7.36 22.73 10.86 4.91 29. 55.57 70.80 14.98 12.88 58.90 51.10 22.10 28.34 25.47 47.46 27.12 8.02 22.71 10.48 4.89 30.72 able improvement (+4.87) over the baseline, even surpassing the Full model by roughly 2 points (30.37 vs. 28.34). We hypothesize that for entity-centric questions, simply resolving ambiguous terms (e.g., replacing this building with The Empire State Building) is more effective than enriching the corpus, whose much longer text entry length (composed of both the original Wikipedia entry and the dense caption) makes exact entity matching more difficult. retrieval In text-to-text Task-Specific Analysis task WebQA-1, we realize that both Q-only and C-only reduces performance, meaning that without enhancing both sides of the retrieval, we do not achieve the indirect positive effect on text-only retrieval. In standard cross-modal tasks like MSCOCO-3 (ImageText), Q-Only achieves the highest individual score (71.16), suggesting that dense captioning of the visual query is highly effective for matching against standard text captions. For CIRR, both modules contribute equally to the improvement (both recalls being 22.7), indicating that either clarifying the modification instruction or enriching the target image with text description is sufficient to solve Image-TextImage retrieval tasks. An exception is that to the seemingly fact that Q-Only is generally superior, EDIS-2 favors C-Only (27.89), likely because event-centric retrieval benefits more from detailed contextual descriptions in the corpus than from query reformulation. 4.4. Ablation Study 2: The Necessity of Training natural question arises: do the performance gains stem solely from the information-rich content of our enhanced queries and corpus, or is the training process necessary to utilize this information? To investigate this, we conducted an Inference-Only experiment where we utilized the baseline model (trained on the original M-BEIR) but performed retrieval using our enhanced queries and corpus. Table 3. Analysis of Training Necessity using R@1. InferenceOnly denotes using the unenhanced baseline model to retrieve using enhanced data. The performance drop indicates significant distribution shift, confirming that the model must be explicitly trained to align with the dense semantic signals in our enhanced data. Task Baseline Inf-Only Full MSCOCO-0 MSCOCO-3 VisNews-0 VisNews-3 WebQA-1 WebQA-2 InfoSeek-6 InfoSeek-8 OVEN-6 OVEN-8 EDIS-2 Nights-4 CIRR-7 FashIQ-7 Fash200k-3 Average 54.19 69.90 13.95 12.08 57.31 49.54 21.64 25.50 24.82 47.65 26.97 8.40 21.41 10.48 4.91 29.92 45.37 70.06 9.91 13.68 57.31 44.84 20.37 22.77 22.30 45.87 22.53 5.99 17.41 9.21 4. 27.50 55.57 70.80 14.98 12.88 58.90 51.10 22.10 28.34 25.47 47.46 27.12 8.02 22.71 10.48 4.89 30.72 As shown in Table 3, directly applying enhanced data to the baseline model results in performance degradation even when compared to the unenhanced baseline. We attribute this to severe distribution shift. The baseline model, trained on sparse and often ambiguous text, fails to effectively process the dense, keyword-rich Visual Context provided by our method, likely treating the additional tokens as noise rather than signal. This finding underscores that high-quality data alone is insufficient; the retrieval model must be explicitly aligned via training to effectively leverage the augmented semantic information. 4.5. Comparison with Other Baselines In Table 4, we benchmark our approach against diverse set of strong baselines, including dual-encoders (CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023)), dense retrievers (DSE (Ma et al., 2024a)), and recent VLM-based embedding models (E5-V (Jiang et al., 2024), GME-2B (Zhang et al., 2025)). 7 Reasoning-Augmented Representations for Multimodal Retrieval Table 4. R@5 Scores Comparison (Incorporating Stronger Baselines). We include SigLIP, DSE (Ma et al., 2024b), E5-V, and GME-2B. Ours demonstrates competitive performance, particularly against SigLIP on Knowledge tasks and DSE on General/Composed tasks. Best performance per row is bolded and second-best performance is underlined. Task CLIP SigLIP DSE E5-V GME-2B Ours MSCOCO-0 MSCOCO-3 VisNews-0 VisNews-3 Nights-4 WebQA-1 WebQA-2 InfoSeek-6 InfoSeek-8 OVEN-6 OVEN-8 EDIS-2 CIRR-7 61.10 79.00 43.30 41.30 26.10 36.20 45.10 20.50 26.40 24.20 38.80 43.30 13.20 75.70 88.20 30.10 30.80 28.90 39.80 43.50 25.10 27.40 29.70 41.70 27.00 22.70 74.62 82.06 14.12 8.74 27.36 83.95 66.99 3.06 5.96 0.38 0.39 41.26 36.52 52.38 86.40 29.46 29.54 27.92 89.94 49.62 12.69 39.69 14.40 54.46 49.62 13.19 71.82 84.12 38.85 38.32 29.86 95.19 83.15 39.06 44.21 58.17 75.98 70.32 46. 80.20 90.02 29.70 26.82 31.56 84.89 79.61 42.08 52.38 48.32 68.56 53.29 49.40 Our data-centric pipeline achieves state-of-the-art performance (R@5) on 6 out of the 13 reported tasks and secures the second-best position on 4 others. This empirical evidence suggests that the ceiling of current retrieval models is often limited not by model capacity, but by the semantic sparsity of the training data. It is critical to contextualize the performance of GME-2B against our method. GME-2B benefits from massive training corpus of approximately 8 million image-text pairs, including 1M subset that remains closed-source. In contrast, our model is trained strictly on the standard MBEIR dataset (1M pairs). Despite this 8 data disparity, our method outperforms GME-2B on benchmarks such as MSCOCO, InfoSeek, and CIRR. This demonstrates that our performance gains stem from the efficacy of our method rather than data scaling, offering significantly more data-efficient solution. The advantages of our reasoning-augmented data are most pronounced in knowledge-intensive tasks. On InfoSeek-8, which requires complex entity linking and visual reasoning, our method achieves an R@5 of 52.38, outperforming the strongest competitor (GME-2B) by over 8 points and nearly doubling the performance of SigLIP (27.40). This validates our hypothesis that resolving ambiguous coreferences (e.g., this lake) into explicit entities during training enables standard models to perform reasoning tasks that typically stump visual-only encoders. On the CIRR modification task, we surpass both DSE (specialized for dense text) and GME-2B, achieving an R@5 of 49.40. This indicates that our query enhancement strategydistilling verbose instructions into keywordsis more effective for modification retrieval than simply scaling model size. Furthermore, on standard cross-modal tasks like MSCOCO, we maintain clear lead (90.02 on MSCOCO-3), proving that our enhancements improve complex reasoning capabilities without sacrificing general zero-shot retrieval performance. 4.6. Qualitative Analysis We present qualitative examples in Figures 3 and 4 in the Appendix to demonstrate the efficacy of our reasoningaugmented framework compared to the baseline. In Figure 3, we analyze query from CIRR-7 where the user instructs the system to retrieve an image that focuses on the wolfs head. As the corpus consists of image-only entries, the baseline model is forced to implicitly reason over the visual concept of focus. Consequently, it fails to capture the semantic shift, defaulting to spurious correlations by retrieving images of puppies in the same sleeping pose as the reference image. Conversely, our method enriches the corpus with detailed descriptions. Keywords such as closeup, profile view, and facial features in the enhanced captions provide direct semantic match for the simplified query focused head, enabling the model to retrieve the correct target with high precision. Figure 4 illustrates challenging example from InfoSeek-8. The query asks specific question about lake but relies on generic image without explicit textual naming. Since the original Wikipedia entries lack detailed descriptions of their accompanying visual data, the baseline model fails to ground the query this lake to specific entity, returning irrelevant lake-related articles. However, our pipeline injects dense visual cues into both the query and the corpus. In this instance, the explicit descriptor surrounded by green mountains was generated for both the query image and the target corpus images. This shared textual context acts as robust retrieval anchor, allowing the model to filter out generic matches and correctly identify the specific location. Collectively, these qualitative examples highlight fundamental shift in the retrieval mechanism. The baseline model frequently overfits to spurious visual correlationsmatching the sleeping pose of the wolf or the general color palette of the lake landscaperather than capturing the users intent. By incorporating these implicit visual attributes into text, our method allows the model to not conduct visual reasoning. This confirms that the performance gains reported in 4.2 are driven by the models improved ability to ground abstract concepts (e.g., focus, this lake) in concrete, textual definitions, thereby bypassing the ambiguity inherent in visual feature-level matching. 5. Conclusion In this work, we argue that the key bottleneck in universal multimodal retrieval is not model capacity but semantic ambiguity in raw multimodal inputs, which forces retrievers 8 Reasoning-Augmented Representations for Multimodal Retrieval Figure 3. Qualitative comparison on CIRR-7. The users instruction requests modification of more focus on its head. The baseline model, relying on implicit visual features, exhibits strong bias toward low-level visual similarity, retrieving images with matching poses (sleeping bodies) rather than the requested semantic change. In contrast, our enhanced model leverages dense corpus captions (e.g., close-up, profile view) to successfully align the modification instruction with the correct target. Figure 4. Qualitative comparison on InfoSeek-8. The baseline struggles with the ambiguity of the multimodal query, where the visual signal alone is insufficient to anchor the vague question about the lake, leading to unrelated retrieval results. Our method bridges this gap by explicating visual cues; the generated context surrounded by green mountains in both the query and corpus serves as crucial semantic bridge, enabling the precise retrieval of the correct Wikipedia entries. to collapse reasoning and compression into single embedding pass and encourages spurious matching. We introduce data-centric framework that uses strong VLM to externalize these latent reasoning steps, turning reasoning-thenretrieve into explicit semantic matching over reasoningaugmented text. On M-BEIR, this yields consistent, substantial gains across tasks ranging from knowledge-intensive entity linking to fine-grained composed image modification. We further show that inference-time enhancement alone is insufficient: models must be trained on the dense semantic distribution induced by augmentation to avoid distribution shift and fully benefit from explicit semantics. Overall, our results suggest practical route to more robust and interpretable multimodal retrieval by prioritizing scalable semantic augmentation and alignment over architectural complexity."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by NetApp Inc., NSF IIS2404180, and the Institute of Information & Commu9 Reasoning-Augmented Representations for Multimodal Retrieval nications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No.RS-202502219317, AI Star Fellowship (Kookmin University)), (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration), (No. RS2022-00187238, Development of Large Korean Language Model Technology for Efficient Pretraining), and (No. RS2025-2543949. Environment-Aware and Domain-Adaptive Multimodal Embodied AI for Real-World Interaction)."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., and Colombo, P. Colpali: Efficient document retrieval with vision language models. In ICLR, 2025. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., and Misra, I. Imagebind: One embedding space to bind them all. In CVPR, 2023. Jiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun, F., Zhang, Q., Wang, D., and Zhuang, F. E5-v: Universal embeddings with multimodal large language models, 2024. URL https://arxiv.org/abs/2407.12580. Jiang, Z., Meng, R., Yang, X., Yavuz, S., Zhou, Y., and Chen, W. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. In ICLR, 2025. Khattab, O. and Zaharia, M. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 3948, 2020. Li, M., Zhang, Y., Long, D., Chen, K., Song, S., Bai, S., Yang, Z., Xie, P., Yang, A., Liu, D., Zhou, J., and Lin, J. Qwen3-vl-embedding and qwen3-vl-reranker: unified framework for state-of-the-art multimodal retrieval and ranking, 2026. URL https://arxiv.org/abs/ 2601.04720. Li, X., Li, C., Chen, S.-Z., and Chen, X. U-marvel: Unveiling key factors for universal multimodal retrieval via 10 embedding learning with mllms, 2025. URL https: //arxiv.org/abs/2507.14902. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Liu, Y., Chen, P., Cai, J., Jiang, X., Hu, Y., Yao, J., Wang, Y., and Xie, W. Lamra: Large multimodal model as your advanced retrieval assistant. In CVPR, 2025a. Liu, Z., Liu, Z., Liang, Z., Zhou, J., Xiao, S., Gao, C., Zhang, C. J., and Lian, D. Any information is just worth one single screenshot: Unifying search with visualized information retrieval. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1923819261, 2025b. Ma, X., Lin, S.-C., Li, M., Chen, W., and Lin, J. Unifying multimodal retrieval via document screenshot embedding. arXiv preprint arXiv:2406.11251, 2024a. Ma, X., Lin, S.-C., Li, M., Chen, W., and Lin, J. Unifying multimodal retrieval via document screenshot embedding. In EMNLP, 2024b. Meng, R., Jiang, Z., Liu, Y., Su, M., Yang, X., Fu, Y., Qin, C., Chen, Z., Xu, R., Xiong, C., Zhou, Y., Chen, W., and Yavuz, S. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents, 2025. URL https://arxiv.org/abs/2507.04590. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, PMLR, 2021. Wei, C., Chen, Y., Chen, H., Hu, H., Zhang, G., Fu, J., Ritter, A., and Chen, W. Uniir: Training and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision, pp. 387404. Springer, 2024. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In International Conference on Computer Vision, 2023. Zhang, X., Zhang, Y., Xie, W., Li, M., Dai, Z., Long, D., Xie, P., Zhang, M., Li, W., and Zhang, M. Gme: Improving universal multimodal retrieval by multimodal llms. In CVPR, 2025. Reasoning-Augmented Representations for Multimodal Retrieval A. Prompts Used A.1. Corpus Enhancement The prompt we feed to Qwen for enhancing corpus entries is as follows: Task: Generate precise, keyword-rich text entry based on the [Image]. Instructions: 1. Subject First: Identify the main object, entity, or scene layout immediately. 2. Distinctive Features: List specific details: colors, materials, text/logos (if visible), and unique shapes. If detail doesnt exist, dont mention it (no stating no visible logos or text). 3. Entity Recognition: If the object is named entity (e.g., Eiffel Tower, Toyota Camry, Nike), state it. 4. Viewpoint: Mention the angle (e.g., close-up, aerial, profile) ONLY IF it distinguishes the image. 5. No Filler: Do not use aesthetic words (e.g., beautiful, cinematic). Focus on factual visual content. 6. Length: Maximum 100 words. Reference Image: <image> Output: A.2. QA Query Enhancement The prompt we feed to Qwen for enhancing QA format queries is as follows: Task: Rewrite the users question by integrating the visual subject. Goal: Create search query that matches text documents. Keep it extremely concise. Strict Constraint Rules: 1. Length Limit: The added visual description must be MAX 35 words. No long sentences. 2. The Specific vs. Generic Split: If Unique Entity (Landmark, Art, Car Model): Use the NAME only. Delete all visual adjectives. BAD: Who built this tall iron tower? GOOD: Who built the Eiffel Tower? If Generic Object (Food, Plant, Animal): Use [Dominant Color/Material] + [Broad Category]. BAD: What is this delicious spicy red soup with shrimp? (Too many distractors) GOOD: What is this red noodle soup with shrimp? (Anchors only) 3. No Filler Adjectives: Banned words: beautiful, large, small, generic, distinct, looking, shaped. 4. No Environment: Never mention background, weather, or lighting. 5. Zero-Leakage: NEVER answer the question yourself. YOU ARE ONLY REWRITING THE QUERY. Examples: Input: [Photo of Giant Panda] Query: When was it discovered? Output: When was the Giant Panda discovered? (Reason: Named entity. No adjectives needed.) Input: [Photo of Yellowjacket Wasp] Query: What species is this? 11 Reasoning-Augmented Representations for Multimodal Retrieval Output: What species is this black and yellow wasp? (Reason: Black and yellow distinguishes it. Insect is too broad, Wasp is better.) Input: [Photo of Red Laksa Soup] Query: What dish is this? Output: What dish is this red noodle soup with shrimp? (Reason: Red, Noodle, Shrimp are the only keys needed to find the recipe.) Input: [Photo of Blue Ford Focus] Query: What car is this? Output: What car is this blue hatchback? (Reason: Blue and Hatchback filter the candidates. Ford Focus might be hallucination, so we play it safe. We also dont want to leak the answer.) Input: [Photo of Melting Clock Painting] Query: Who painted this? Output: Who painted The Persistence of Memory? (Reason: Unique Art Specific Name.) Current Task: Query: {query txt} Input Image: <image> Output: A.3. Modification Request Enhancement The prompt we feed to Qwen for enhancing modification requests is as follows: Task: Extract the key semantic phrases describing the TARGET image. Remove conversational filler and grammar words. Input: User Query (describing change or target attribute). Strict Reduction Rules: 1. Delete Filler Verbs: Remove Is, Has, Make, Change, Show, Put, Be. 2. Delete Pronouns/Articles: Remove it, the, a, an, my, me, them, this. 3. Preserve Adjectives & Nouns: Keep ALL descriptors (colors, patterns, objects). If the user says Is white, output White. 4. Preserve Prepositions: Keep with, on, in, without to maintain spatial/compositional logic. Note: There are cases where the original query is concise enough, and you might not have to change anything. Examples: Input: Is shiny and silver with shorter sleeves. Output: Shiny silver with shorter sleeves Input: Is white in color with short sleeves and is more plain. Output: White, short sleeves, more plain Input: Remove the lemon. Output: Remove lemon 12 Reasoning-Augmented Representations for Multimodal Retrieval Table 5. Baseline vs Our method on MVRB Composed Image Retrieval. Our method outperforms the Baseline model on the Composed Image Retrieval Section of the MVRB benchmark which involve complex queries that requires reasoning. Task Baseline (LamRA-Ret (Liu et al., 2025a)) R@1 R@5 Ours R@1 R@"
        },
        {
            "title": "Knowledge Relation\nNews to Wiki\nProduct Discovery\nWiki to Product",
            "content": "35.00 52.48 52.34 70.00 71.00 85.15 91.59 97.00 46.00 55.45 50.47 73.00 78.00 83.17 92.52 96.00 Input: Make the needle upside down in the hand. Output: Needle upside down in hand Input: Human and one animal from different species. Output: Human and animal from different species Input: Is plain white feminine shirt and is tan shirt. Output: Plain white feminine t-shirt and tan shirt Input: Remove all cheetahs. Output: Remove all cheetahs Input: Remove one cheetah. Output: Remove one cheetah. Input: Remove green from the background. Output: Remove green from background. Current Input: {query txt} Output: B. Experiments on MVRB Composed Image Retrieval We further evaluate our approach on composed image retrieval tasks from the MVRB benchmark (Liu et al., 2025b). These tasks are particularly challenging, as they require multi-step reasoning over object relations (e.g., identifying the phone case corresponding to given phone). As shown in Fig. 5, our method consistently outperforms the baseline on several tasks as indicated by the recall scores. The performance gains are especially pronounced on knowledge-based retrieval tasks, further supporting the claims made in Section 4.5. C. Impact of Hard Negative Mining and Data Enrichment We evaluate the effectiveness of our Enriched (Full) data when integrated into robust Hard Negative training regime. Table 6 compares the performance of the Baseline model versus our Full model, both trained with M-BEIR hard negatives. C.1. Robustness of Semantic Enrichment The primary objective of this experiment was to stress-test our Enriched (Full) data against highly competitive baseline strengthened by hard negative mining. Despite the significantly elevated performance bar set by the hard-negative baseline, our Full model successfully provides additive performance gains across the majority of benchmarks. Notably, our findings of Reasoning-Augmented Representations for Multimodal Retrieval Table 6. Performance comparison of Baseline vs. Full data under Hard Negative training. While the Full model demonstrates consistent improvements across the majority of tasks (see Avg w/o outliers), specific regressions are observed on OVEN-8 and EDIS-2. We attribute these drops to task-specific interference introduced by the hard negative mining process, as discussed in Section C.2. Task Type Baseline + Hard Negatives Ours + Hard Negatives R@1 R@5 R@10 R@50 R@1 R@5 R@10 R@50 MSCOCO-0 MSCOCO-3 VisNews-0 VisNews-3 WebQA-1 WebQA-2 InfoSeek-6 InfoSeek-8 OVEN-6 OVEN-8 EDIS-2 Nights-4 CIRR-7 FashIQ-7 Fash200k-3 qt ci qi ct qt ci qi ct qt ct qt ci, ct qi, qt ct qi, qt ci, ct qi, qt ct qi, qt ci, ct qt ci, ct qi ci qi ci qi, qt ci qi ct 53.93 69.86 13.70 12.10 74.34 55.87 20.53 33.11 22.22 41.84 33.88 9.34 23.09 10.48 4.93 79.04 89.46 28.39 25.90 93.08 83.19 39.15 55.28 44.33 65.35 56.65 32.50 50.10 23.06 13.25 86.88 94.42 35.33 33.12 95.56 90.56 49.02 64.04 54.13 72.40 65.44 53.44 61.29 30.38 18. 97.93 99.54 53.65 51.99 98.86 97.25 69.97 83.07 73.63 85.52 83.61 90.94 82.78 48.59 37.35 55.09 70.54 14.76 12.85 74.18 56.59 21.37 33.64 22.46 40.88 30.17 9.48 23.65 10.31 4.95 79.87 89.98 29.28 26.56 93.12 83.59 40.10 56.62 44.28 63.89 53.10 33.35 49.81 23.49 12.62 87.48 94.64 36.41 34.04 95.89 91.20 48.70 65.80 53.88 71.21 62.67 54.20 59.93 30.25 18.08 97.99 99.40 55.18 52.51 98.78 97.45 68.87 82.49 73.30 84.68 82.65 90.80 82.09 49.06 36.78 improvement trends highly match our findings in the main experiments, confirming that the benefits of semantic enrichment are robust and persist even under rigorous discriminative training regimes. On general retrieval benchmarks such as MSCOCO, we observe clear improvements (e.g., +1.16% R@1 on MSCOCO-0), reaffirming that detailed captions enhance fundamental scene understanding. This positive trend extends to knowledgeintensive and multimodal tasks like VisualNews and InfoSeek, where our method consistently outperforms the baseline. The gains on InfoSeek-6 and InfoSeek-8 are particularly significant, as they demonstrate that our enriched entity descriptions and factual augmentations provide critical discriminative signals. These signals allow the model to resolve complex queries more effectively than negative mining alone, proving that our data enrichment strategy is not merely redundant but complementary to the structural improvements gained from hard negative training. C.2. Analysis of Task-Specific Deviations While the general trend is positive, we observe specific regressions on OVEN-8 and EDIS-2. We attribute these deviations to dataset-specific interference and bias introduction during the hard negative mining process. OVEN-8 and Cross-Task Interference The performance regression on OVEN-8 is most accurately understood by first examining the Baselines behavior. The introduction of hard negatives caused significant 6 point drop in the Baseline performance compared to the original training configuration. This explicitly isolates the source of the regression to biases induced by the hard negative training regime itself, rather than an intrinsic flaw in our data enrichment process. OVEN serves as coarser, less nuanced counterpart to InfoSeek, sharing the same underlying data distribution. However, while InfoSeek relies on explicit hard negatives to enforce fine-grained discrimination, OVENs training data does not. We hypothesize that training on the shared InfoSeek hard negatives introduces distribution bias that conflicts with the objectives of OVEN. The model likely overfits to the hyper-specific discriminative features required for InfoSeek, which do not generalize toand actively interfere withthe broader entity retrieval requirements of OVEN-8. EDIS-2 and Length-Induced Modality Bias The regression on EDIS-2 reveals critical interaction between data length and hard negative training. EDIS is characterized by concise text corpus entries, significantly shorter than the encyclopedic descriptions found in benchmarks like InfoSeek. However, our enrichment pipeline uniformly generates detailed captions (100 words) regardless of the native corpus density. Crucially, in our prior experiments without hard negatives, our Full model outperformed the Baseline on EDIS-2, confirming 14 Reasoning-Augmented Representations for Multimodal Retrieval that our generated data is intrinsically high-quality and informative. The performance reversal occurs only under hard negative training. We hypothesize that the stark contrast between our verbose generated captions and the concise ground truth creates modality imbalance. When forced to discriminate against hard negatives, the model may over-index on the abundant visual context provided by our long descriptions, while demonstrating diminished sensitivity to the sparse but critical textual cues present in the short ground truth. This suggests that while semantic expansion is beneficial generally, rigid length constraints can introduce signal-to-noise issues in low-density text domains. Future Work: Dynamic Length Curation To resolve the length mismatch observed in EDIS and the interference in OVEN, our future work will pivot from fixed-length generation to dynamic length curation. Rather than systematically generating 100-word descriptions, we aim to condition the generation pipeline to match the intrinsic information density and length distribution of the target domain. By adaptively sizing the enriched textproviding verbose descriptions for complex scenes (e.g., InfoSeek) and concise, sharp summaries for sparse domains (e.g., EDIS)we aim to retain the semantic benefits of enrichment while preventing the distribution shifts that currently hamper fine-grained discrimination."
        }
    ],
    "affiliations": [
        "Kookmin University",
        "NetApp, Inc.",
        "University of Wisconsin-Madison"
    ]
}