{
    "paper_title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection",
    "authors": [
        "Kai Hua",
        "Steven Wu",
        "Ge Zhang",
        "Ke Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 3 9 2 7 0 . 5 0 5 2 : r AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection Kai Hua, Steven Wu, Ge Zhang, Ke Shen"
        },
        {
            "title": "ByteDance Seed",
            "content": "Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domainspecific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, simple yet effective, training-free method without supervision signal. Our approach enables small pretrained language model to act as strong data selector through simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger modelsoffering promising and scalable path for reasoning-centric data selection. Date: May 13, 2025 Correspondence: Kai Hua at huakai.dev@bytedance.com, Ke Shen at shenke@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "The identification of high-quality pretraining data has been key factor enabling Large Language Models (LLMs) creation. Commonly recognized high-quality pretraining materials include academic papers (e.g., arXiv), books (e.g., Project Gutenberg), high-quality code (e.g., GitHub), and instruction datasets [25]. Existing approaches often rely on manually curated high-quality seed data to train classifiers for extracting additional high-quality pretraining data from massive web corpora. However, as the size and diversity of LLMs pretraining data requirements continue to grow, these carefully curated classifiers suffer from the high manual effort requirements and relatively low diversity of identified data. This raises critical research question: How can we continue to identify diverse high-quality pretraining data efficiently and scalably? Current mainstream methods[40] typically use supervised or weakly supervised data to train classifiers to identify high-quality data. For instance, LLaMA2[43] uses reference information of Wikipedia documents, which can be seen as weakly supervised data to train fasttext[21] classifier and then recognize Wikipedia1 Figure 1 (a) Performance evolution on comprehensive benchmark evaluations during pretraining. The first 750 billion tokens correspond to the pretraining phase, represented by solid lines, while the subsequent 250 billion tokens represent the learning rate annealing phase, represented by dashed lines, using the same dataset. After around 100 billion tokens, AttentionInfluence-1.3B consistently outperforms the baseline across wide range of tasks on average, including the annealing phase. (b) Training Loss during pretraining. AttentionInfluence-1.3B consistently achieves lower loss than the baseline. like documents. LLaMA3[13] and FineWeb-Edu[32] use LLM-generated responses to train classifier for educational value, which can be regarded as much sparser form of distillation from larger LLM(up to 70B dense parameters) than knowledge distillation[17]. While other approaches like DCLM aim to fit user preferences through utilizing signals of user behavior, these methods may introduce potential bias and do harm to diversity[25]. There are also efforts to train several domain classifiers and combine them for practical use [46]. However, we assume that these methods fail to capture the essence of what makes data reasoning-intensive, and as result, they can be labor-intensive and require significant data engineering efforts. Moreover, there exists risk that the classification results from small models distilled from larger models responses may not improve the final performance of larger models. Therefore, we propose AttentionInfluence, which leverages the intrinsic mechanism of existing LLMs attention heads for pretraining data selection to achieve weak-to-strong generalization. Existing research suggests that feedforward networks (FFNs) store atomic knowledge[12], while attention mechanisms execute algorithms and store procedural knowledge[31, 47]. These mechanistic interpretability insights inspire us to hypothesize that the data activating more important attention heads are high-quality and about procedural knowledge. To be specific, we select the data with relatively larger loss difference when small pretrained language models process them with and without masking retrieval heads. Compared with mainstream data selection methods [21, 25], our method is training-free and more generalizable. To validate AttentionInfluence, we adopt LLaMA2-alike-1.3B pretrained checkpoint for data selection from SmolLM-Corpus. We then pretrain 7B dense language modelSmolLM-7Bas our baseline on the SmolLM-Corpus, 241B-token curated dataset that already applies strong quality filtering with an education-focused classifier (FineWeb-Edu-Dedup) to retain high-quality data. As shown in (a) of Figure 1, despite this strong baseline, AttentionInfluence still yields consistent improvements, demonstrating its ability to further improve the overall data quality through better data selection beyond existing heuristics or classifiers. AttentionInfluence shows consistent improvement against SmolLM-7B across wide range of tasks, demonstrating the effectiveness of the selected data. We further compare AttentionInfluence with trained classifier (FineWeb-Edu Classifier) and find that it selects data that is more balanced and broadly distributed across content categories, and preferentially favors longer and more comprehensive samples. Despite being entirely supervision-free and training-free, AttentionInfluence also shows strong agreement with classifier-based patterns, validating its reliability and generalizability. In summary, our key contributions are as follows: 1. We propose AttentionInfluence, novel framework that leverages intrinsic model behaviorsspecifically 2 attention head mechanismsfor effective data selection without any supervision signal. 2. We show that data selected by AttentionInfluence is high-quality and well-distributed, yielding consistent improvements in downstream training. 3. We demonstrate that this approach exhibits weak-to-strong scaling property, where data selected by smaller models significantly improves the training of larger models, resulting in performance gains without relying on human-labeled data, LLM-generated data, or training any classifiers."
        },
        {
            "title": "2.1 Data Selection",
            "content": "Many works use heuristic filtering rules[35] or perplexity[2] of existing LLMs to assess the quality of pretraining data, which makes them training-free. Scaling filter[25] uses the perplexity difference between two LLMs trained on the same data to evaluate text quality. However, when two LLMs trained on the same data are unavailable, training LLMs incurs substantial computational cost. In contrast, methods that rely on training model with high-quality labeled data gain more attention owing to their higher accuracy and superior versatility across different data categories. For instance, LLaMA2 uses reference information from Wikipedia documents, which can be seen as weak supervision data to train fasttext[21] classifier and then recognize Wikipedia-like documents. LLaMA3[13] and FineWeb-Edu[32] use LLM-generated responses to train classifier for educational value, which can be regarded as much sparser form of distillation from larger LLM (up to 70B dense parameters) than knowledge distillation[17]. While other approaches like DCLM[24] aim to fit user preferences by utilizing user behavior signals. Some recent and more advanced approaches[33, 45, 52] instead train multi-class classifiers to make fine-grained distinctions among various content types and depend on labeled data generated by proprietary commercial large language models such as GPT-3.5-turbo, GPT-4, and GPT-4o. There are also efforts to train several domain classifiers and combine them for practical use [46]. AttentionInfluence can be seen as training-free method and is available at minimal cost without any training cost or human-labeled or LLM-labeled data."
        },
        {
            "title": "2.2 Data Mixture",
            "content": "There are also efforts to optimize the data mixture by either relying on human selection or using automatic frameworks. Ye et al. [49] propose Data Mixing Laws by introducing nested prediction framework that combines scaling laws of training steps, model sizes, and data mixtures, enabling efficient optimization of large-scale pretraining data using only small-scale experiments. Xie et al. [48] propose DoReMi (Domain Reweighting with Minimax Optimization), method that uses small proxy model and distributionally robust optimization to automatically learn optimal domain mixture weights for language model pretraining. Held et al. [14] conduct data mixing by designing lightweight method that leverages LLMs to estimate data utility from small samples, enabling compute-efficient optimization with comparable performance to ablation-based approaches. OLMo et al. [30] introduce OLMo and uses the existing data mixture designed for different model family. REGMIX [27] is regression-based framework for optimizing data mixtures in language model pretraining by training small proxy models on diverse mixtures and predicting performance using regression."
        },
        {
            "title": "2.3 Mechanistic Interpretability",
            "content": "Understanding the inner workings of LLMs is crucial for advancing artificial general intelligence safely. Consequently, studies in mechanistic interpretability [5, 11, 28, 31, 47, 53] are increasingly being conducted. Olsson et al. [31] primarily investigates the relationship between certain heads in large language models (LLMs) and their in-context learning capabilities. Bricken et al. [5] extracts large number of interpretable features from one-layer transformer with sparse autoencoder in order to analyze the behavior of the neural network. Lv et al. [28] investigates mechanisms for factual recall in Transformer-based LLMs, focusing on attention head extraction, MLP activation, and the collaborative mechanism between attention heads and MLPs. Wu et al. [47] identifies class of attention heads, termed retrieval heads, which retrieve relevant information in LLMs. These heads exhibit universal, sparse, and dynamically activated behavior, and they play crucial role in enabling chain-of-thought reasoning. Fu et al. [11] aims to estimate the importance 3 of different attention heads for contextual QA tasks that require both retrieval and reasoning capabilities, enabling efficient head-level KV cache compression for language model inference. Inspired by the findings of Qiu et al. [34], Wu et al. [47], AttentionInfluence adopts similar and simple proxy task to detect specific important heads, namely the retrieval heads in this paper. AttentionInfluence naturally extends the insights from Wu et al. [47], broadening their application beyond model analysis and inference acceleration to include effective and efficient data selection."
        },
        {
            "title": "2.4 Influence Measure",
            "content": "Ruis et al. [38] uses influence functions to recognize pretraining documents important for learning factual knowledge and mathematical reasoning separately. Mirror Influence[22] realizes an efficient data influence estimation to select high-quality data. MATES[50] continuously adapts data influence model to the evolving data preferences of the pretraining model and then selects the most effective data for the current pretraining progress. Our work is similar to Mirror Influence in that we use data influence estimation to select high-quality data. However, while Mirror Influence requires high-quality dataset to train strong reference model and create model pair with significant differences in capabilities to compute delta loss, our approach uses the attention mechanism to derive weaker reference model from the base model. This enables us to obtain two models with significant capability gap and compute delta loss to evaluate data quality."
        },
        {
            "title": "3 Preliminary",
            "content": "To estimate the impact of each pretraining data sample on LLMs intrinsic reasoning and retrieval capabilities, we adapt the retrieval score defined in [47] and model it as token-level recall rate based on the attention head behavior. We denote the current token being generated as at the decoding step while decoding the LLM. We further denote the attention scores of head as Rx where represents the vocabulary. Consequently, denotes the size of the vocabulary. We assume that an attention head performs copy-paste operation for corresponding content k, if and only if the following two conditions are met: Condition 1: The generated token appears in the corresponding content k: (1) Condition 2: The token receives the highest attention score among all positions visible to the current query token in this head: iq, where iq = {j < t} is the set of positions visible at decoding step j = arg max(a), xj = Let gh denote the set containing all tokens copied and pasted by given head h, we define: Retrieval score for head = gh k (2) (3) (4)"
        },
        {
            "title": "4 Method",
            "content": "Lin et al. [26] demonstrate that well-trained reference model can serve as proxy to fit the desired data distribution of the LLM pretraining by comparing the data loss gap between the base model and the reference model. By comparing the token-level data loss gap between the base model and the reference model, they can identify important tokens that align better with the target distribution. Inspired by recent work Ko et al. [22], Lin et al. [26], we propose AttentionInfluence to select high-quality pretraining data based on the data loss gap from <weak model, strong model> pair. However, while existing approaches [22, 26] focus on building stronger reference model as the strong model, AttentionInfluence points out that it is cheaper and more controllable to degrade the base model to weaker version, thus constructing <weak model, strong model> pair. 4 Existing studies [31, 47] point out that specific attention heads (i.e., retrieval heads) plays critical role in LLMs in-context learning, retrieval, and reasoning capabilities. We find that the language models retrieval heads emerge early in training, gradually strengthen, and eventually become entrenched in the middle to late stages, playing crucial role in the models performance, as shown in Figure 2. Inspired by this insight, AttentionInfluence identifies the specific attention heads that are important for targeted LLM capabilities and obtains degraded reference model by disabling them. Then, AttentionInfluence selects high-quality pretraining data based on the sample-level data loss gap from the constructed <weak model, strong model> pair. We detail the AttentionInfluence method in the following section. Figure 2 The evolution of retrieval heads in 1.3B dense model."
        },
        {
            "title": "4.1 Detecting Specific Important Heads",
            "content": "In this work, we detect the retrieval heads as specifically important heads for reasoning, because Wu et al. [47] reveals that retrieval heads are extremely relevant to LLMs retrieval and reasoning ability. Inspired by the Key-Passage Retrieval evaluation task proposed in CLongEval[34], we adopt similar and simple proxy task to evaluate the retrieval ability of LLMs in controlled setting, and identify attention heads that are strongly associated with retrieval and reasoning. To this end, we construct synthetic test dataset consisting of 800 samples. Each sample is formatted as 3-shot retrieval task in natural language, consisting of context, three in-context demonstrations, and query hash_key. The sample template is detailed in Appendix A. Each context is JSON object with key-value pairs, where each key is randomly generated 32-character alphanumeric string (hash_key), and each value (text_val)1 is natural language sentence sampled from corpus of web documents. The task requires the model to retrieve the text_val from the context and output the text_val corresponding to the given query hash_key. The inclusion of three in-context demonstrations (i.e., 3-shot) is designed to simulate few-shot learning scenario and help the model understand the task. Considering the context length limitation of existing pretrained models, we constrain the total length of each test sampleincluding both the input prompt and the answerto be close to but not exceeding 4,096 tokens. Next, we compute retrieval scores for each attention head across test samples, as described in Section 3. In this work, we use 1.3B-parameter model based on the LLaMA2-alike architecture as the small pretrained language model. We use the average score as the heads final retrieval score and sort them by it. Referring to Wu et al. [47], we select the heads ranked in the top 5% as specifically important heads."
        },
        {
            "title": "4.2 Calculating AttentionInfluence Score",
            "content": "We obtain reference model by masking the important heads of the base model detected in the first phase, and compute the AttentionInfluence score based on the base model and reference model. For details on the masking operation, refer to Appendix C. First, we use the base model to compute the mean token-level cross-entropy loss (Lbase) of each sample in the corpus. Subsequently, we compute the corresponding loss (Lref ) using the reference model. Finally, we use the relative delta between Lbase and Lref as an AttentionInfluence 1Each text_val is capped at maximum of 30 tokens. 5 Figure 3 The illustration of AttentionInfluence. Score to quantify the reasoning intensity of each sample, which can be denoted as: AttentionInfluence Score = Lref Lbase Lbase (5) Since the loss of language model for data from different domains (e.g., general/math/code) cannot be directly compared due to significant distribution differences, we restrict the AttentionInfluence Score to be compared only within the same domain (e.g., general/math/code). We consider that higher AttentionInfluence Score indicates higher reasoning intensity of the sample."
        },
        {
            "title": "5 Experiments and Results",
            "content": "In this section, we present experimental analyses to validate the effectiveness of reasoning-intensive data selected by AttentionInfluence."
        },
        {
            "title": "5.1 Experimental Details\nWe apply AttentionInfluence to a LLaMA2-alike-1.3B pretrained model to rank the SmolLM-Corpus dataset2 [3].\nThe specifications of the model are described in Appendix E. Specifically, we select the top 20% of samples\nbased on the AttentionInfluence score, yielding approximately 73.1B reasoning-intensive tokens for pretraining.\nTo evaluate the effectiveness of AttentionInfluence, we pretrain a 7B dense model using a combination of\nthe full SmolLM-Corpus and the selected 73.1B tokens. For comparison, we pretrain another model of\nidentical architecture and size using only the SmolLM-Corpus, serving as the baseline. The model architecture\nfollows that of LLaMA2, and detailed hyperparameters are listed in Table 9. Further information on the\nSmolLM-Corpus dataset and pretraining configurations is provided in Appendix E.",
            "content": "Following Grattafiori et al. [13], we adopt comprehensive set of benchmark evaluations across four major categories under the few-shot setting to holistically compare our model with the baseline: 1) Aggregate Benchmarks, including AGIEval-en [54], MMLU [15], MMLU-Pro [44], GPQA [37], and C-Eval [19]; 2) Mathematics, Code, and Reasoning, comprising GSM8K [9], MATH [16], HumanEval [7], ARC Challenge [8], DROP [10], and BBH [41]; 3) Commonsense Reasoning and Understanding, including HellaSwag [51], ARC-Easy [8], WinoGrande [39], CommonSenseQA [42], PiQA [4], OpenBookQA [29], and TriviaQA [20]; and 4) Reading Comprehension, represented by RACE [23]. Details of the evaluation setup are provided in Appendix E. 2https://github.com/huggingface/smollm/tree/main/text/pretraining 6 Model #Tokens Avg. Metrics SmolLM-1.7B 1T - Baseline w/o LRD 746B 40. Ours w/o LRD 746B 41.34 Baseline w/ LRD 1T 44. Ours w/ LRD 1T 45.26 ARC-C - PIQA 75.90 BBH - ARC-C 55.89 PIQA 79.27 BBH 30.35 ARC-C 56.66 PIQA 77.58 BBH 33. ARC-C 58.79 PIQA 80.63 BBH 35.36 ARC-C 59.98 PIQA 79.54 BBH 36.22 ARC-E - TriviaQA 13.14 GSM8K 4.62 ARC-E 81.69 TriviaQA 45.57 GSM8K 12.89 ARC-E 82.03 TriviaQA 45.68 GSM8K 15.77 ARC-E 83.92 TriviaQA 51.07 GSM8K 21. ARC-E 84.26 TriviaQA 51.20 GSM8K 23.73 ARC(C+E) 59.95 MMLU 39.35 MATH - ARC(C+E) 68.79 MMLU 41.76 MATH 6.15 ARC(C+E) 69.35 MMLU 45.10 MATH 7.25 ARC(C+E) 71.36 MMLU 50.05 MATH 9.00 ARC(C+E) 72.12 MMLU 51.48 MATH 10. Wino. 54.70 CSQA OpenBookQA 38.00 MMLU-Pro AGIEval-en RACE Hella. 62.83 42.60 DROP - 10.92 HumanEval - - C-Eval - - GPQA - Hella. 71.79 Wino. 66.22 CSQA OpenBookQA 49.14 MMLU-Pro AGIEval-en RACE 40.67 GPQA 21.93 13.80 HumanEval 20.70 21.10 C-Eval 26. 45.40 DROP 31.71 Hella. 71.90 Wino. 65.43 CSQA OpenBookQA 53.48 MMLU-Pro AGIEval-en RACE 41.72 GPQA 25.18 17.19 HumanEval 19.85 22.50 C-Eval 28. 43.60 DROP 32.03 Hella. 75.63 Wino. 70.24 CSQA OpenBookQA 59.62 MMLU-Pro AGIEval-en RACE 41.15 GPQA 24.77 19.36 HumanEval 23.02 24.50 C-Eval 33. 48.00 DROP 36.09 Hella. 75.49 Wino. 68.03 CSQA OpenBookQA 61.59 MMLU-Pro AGIEval-en RACE 42.30 GPQA 24.26 22.03 HumanEval 26.55 26.30 C-Eval 33. 46.60 DROP 36.52 Table 1 The main results on various benchmarks. The LRD denotes learning rate decay."
        },
        {
            "title": "5.2 Results\n1) Aggregate Benchmarks: On challenging aggregate benchmarks such as MMLU, MMLU-Pro, and AGIEval-en,\nAttentionInfluence consistently outperforms the baseline, indicating stronger general knowledge and reasoning\ncapabilities. These improvements—+1.4pp on MMLU, +2.7pp on MMLU-Pro, and +1.8pp on AGIEval-\nen—underscore the effectiveness of AttentionInfluence in identifying a diverse distribution of pretraining data\nthat supports both comprehensive knowledge acquisition and reasoning-intensive learning.\n2) Math, Code, and Reasoning: AttentionInfluence yields substantial improvements on complex multi-step\nreasoning tasks such as GSM8K (+2.7pp), HumanEval (+3.5pp), and BBH (+0.9pp), suggesting that the\nselected data distribution better facilitates problem-solving and advanced reasoning. Additional gains on ARC-\nChallenge, DROP, and MATH further demonstrate that AttentionInfluence enhances reasoning generalization\nacross a wide range of tasks.\n3) Commonsense Reasoning and Understanding: On benchmarks including CSQA, PiQA, and OpenBookQA,\nAttentionInfluence achieves competitive or superior results. Notably, its performance on ARC-Easy and\nTriviaQA indicates that AttentionInfluence maintains strong results on factual and commonsense tasks, despite\nbeing primarily designed to select reasoning-intensive data.",
            "content": "4) Reading Comprehension: On RACE, AttentionInfluence surpasses the baseline by +1.2pp, reflecting enhanced discourse-level understanding and reasoning capabilities. Performance Evolution During Pretraining: We evaluate the training dynamics of AttentionInfluence-1.3B on the specified tasks. As shown in Figure 1, our method consistently outperforms the baseline throughout the pretraining process. Full results for all evaluation tasks are provided in Figure 9 and Figure 10. The performance gap emerges earlywell before reaching 100B tokensand remains stable over time. After approximately 100 billion tokens, AttentionInfluence-1.3B demonstrates clear and consistent advantage over the baseline, on average, across multiple tasks. These improvements persist throughout all training phases, including both before and after the learning rate decay (LRD). Although the performance margin slightly narrows 7 following the LRD, this effect primarily results from saturation as training approaches 1T tokens on the 7B model, which is trained on the SmolLM-Corpus containing only 241B tokens. Nonetheless, AttentionInfluence maintains stable advantage without requiring any additional supervision signals. Moreover, on benchmarks that primarily require simple factual knowledge, the performance of LLMs trained with AttentionInfluence is comparable to that of the baseline (see Figure 9). In contrast, on reasoning-intensive benchmarks, our models achieve significantly better results than the baseline (see Figure 10). These findings demonstrate that AttentionInfluence is effective at selecting data with higher reasoning intensity. Mirror Effects in AttentionInfluence: For tasks with performance gainssuch as MMLU, MMLU-Pro, AGIEvalen, DROP, BBH and GSM8Kwe observe that masking retrieval heads in the pretrained 1.3B model leads to significant performance drop (see Appendix for details). This suggests mirror effect: when the performance of the 1.3B model significantly degrades on certain tasks due to masking the specific important heads, the data selected by AttentionInfluence-1.3B tends to improve performance on these tasks when used to train 7B model. This observation supports the insight discussed in subsection 4.1, demonstrating the interpretability of AttentionInfluence and its predictive power in identifying evaluation metrics likely to show improvement before any training. Increasing Parameter Size of AttentionInfluce: Furthermore, as shown in Table 2, LLMs pretrained on data selected by AttentionInfluence-7B exhibit superior performance on these challenging knowledge-intensive and reasoning-intensive benchmarks. This indicates that increasing the model size in AttentionInfluence enables the selection of samples with higher reasoning intensity. The comparison details between the 1.3B and 7B methods are provided in Table 10 of Appendix F. In conclusion: These results validate that AttentionInfluence effectively identifies high-quality pretraining data that enhances the knowledge and reasoning capabilities of LLMs, yielding particularly notable gains on benchmarks requiring comprehensive knowledge and complex reasoning, as shown in Table 1. Furthermore, AttentionInfluence can be combined with the FineWeb-Edu Classifier to achieve comprehensive improvements in LLM performance on tasks that require either simple factual knowledge, advanced reasoning, or both. Model MMLU GPQA MATH C-Eval AGIEval-en BBH 1.3B 7B 51.48 53.18 24.26 24.87 10.80 11. 33.06 36.85 26.30 26.85 36.22 36.80 Table 2 Comparison of results trained with AttentionInfluence-1.3B/7B on relatively difficult benchmarks."
        },
        {
            "title": "6.1 Reliability of AttentionInfluence",
            "content": "Domain FineWeb-Edu Classifier AttentionInfluence Edu Score Reasoning Score Token Len Edu Score Reasoning Score Token Len FineWeb-Edu-Dedup Cosmopedia-V2 Python-Edu OpenWebMath 0.99 1.0 0.98 0. 0.52 0.87 0.76 0.52 1610.12 825.46 414.15 1022.855 0.99 1.0 0.98 0.96 0.49 0.80 0.87 0.88 1629.73 893.805 820.71 2255.575 Table 3 The quality score of the data selected by AttentionInfluence and FineWeb-Edu Classifier. To validate the effectiveness of AttentionInfluence, we design two metricsEducation Score and Reasoning Scoreto quantify the quality of the selected data. Specifically, we randomly sample 200 examples from the top 20% ranked by AttentionInfluence and the FineWeb-Edu classifier, respectively, and employ GPT-4o as the evaluator. The detailed scoring criteria and prompt design for both metrics are provided in Appendix G. As shown in Table 3, both AttentionInfluence and FineWeb-Edu classifier yield comparable scores on educationrelated content. However, AttentionInfluence achieves substantially higher scores in reasoning, indicating that the samples selected by AttentionInfluence exhibit greater reasoning intensity. 8 Additionally, we analyze the length of the selected samples. In the Python-Edu and OpenWebMath domains, AttentionInfluence selects samples with an average length nearly twice that of those selected by the FineWebEdu classifier. qualitative inspection of these samples (see Appendix I) reveals that, in the Python-Edu domain, AttentionInfluence prefers documents containing not only more complex code but also richer textual context, such as detailed problem statements. In the OpenWebMath domain, the selected samples demonstrate more elaborate formula-based reasoning. These findings suggest that AttentionInfluence effectively identifies data with more comprehensive and complex reasoning structures."
        },
        {
            "title": "6.2.1 Word Frequency Analysis",
            "content": "Ranking (%) Static Method Data Source FineWeb-Edu-Dedup Cosmopedia-v2 Python-Edu OpenWebMath 10 50 TF TF-IDF TF TF-IDF TF TF-IDF 0.84 0.82 0.88 0.87 0.95 0.92 0.73 0.72 0.81 0.80 0.91 0.90 0.29 0.38 0.41 0.43 0.67 0.66 0.57 0.52 0.67 0.63 0.79 0. Table 4 Word overlap by ranking threshold and frequency-based statistical method We separately select the top 10%, 20%, and 50% of samples ranked by AttentionInfluence and the FineWeb-Edu classifier, and compute the overlap of high-frequency words using multiple statistical approaches. As shown in Table 4, we derive several key insights: 1) AttentionInfluence exhibits high degree of overlap with the FineWeb-Edu Classifier, highlighting the reliability of the samples selected by AttentionInfluence. 2) AttentionInfluence and the FineWeb-Edu Classifier demonstrate degree of complementarity. We observe notable domain-specific variations. Specifically, in the FineWeb-Edu-Dedup and Cosmopedia-v2 domains, the overlap exceeds 70%, whereas in the Python-Edu and OpenWebMath domains, it falls below 60%. To further examine the differences between AttentionInfluence and FineWeb-Edu Classifier in specific domains, we sample representative examples from the Python-Edu and OpenWebMath domains, as shown in Appendix I. These cases reveal that although the two methods display different preferences across domains, both yield reasonable selections.\" As shown in Table 11 of Appendix L, AttentionInfluence places greater emphasis on method-related terminology, while FineWeb-Edu Classifier is more sensitive to numerical expressions. We identify two distinctive high-frequency terms: 19th from subset selected by FineWeb-Edu Classifier and sklearn from AttentionInfluences subset. We then retrieve representative documents from the original corpus containing these terms. The sample containing 19th is related to historical topics, whereas the one with sklearn discusses K-Nearest Neighbors Classifier and Hyperparameter Tuning. This suggests that AttentionInfluence prefers samples containing hands-on coding or procedural mathematical reasoning. 6.2.2 Clustering-Based Distribution Analysis To better understand the distribution of samples selected by different methods (i.e., AttentionInfluence and the FineWeb-Edu Classifier), we cluster the selected subsets and employ GPT-4o to annotate the resulting clusters. The clustering procedure is detailed in Appendix H. We derive the following insights: 1) AttentionInfluence produces more balanced distribution across data categories. As illustrated in Figure 4, both methods achieve broad coverage of the top-level categories. However, the distribution resulting from AttentionInfluence is noticeably more balanced. 2) AttentionInfluence selects highly diverse set of samples. We examine two clusters from the AttentionInfluence subset that exhibit large embedding distances. As demonstrated by the examples from the 9 Figure 4 The statistics of clustering. The left is the clustering result of AttentionInfluence, the right part is that of FineWeb-Edu Classifier. Health Guidelines & Nutrition and Information Technology clusters in Appendix J, the selected samples differ substantially in both content and style. This lack of semantic overlap underscores the effectiveness of the clustering and enhances the interpretability of the annotated categories. 6.2.3 The Visualization of Data Distribution To provide an intuitive illustration of the relationship between the two selection methods, we apply Principal Component Analysis (PCA) to reduce the dimensionality of the document embeddings and visualize their distributions in two-dimensional space. As shown in Figure 5, AttentionInfluence selects samples with broader and more balanced coverage. By directly leveraging the attention mechanisms of pretrained language models, it facilitates more effective selection of general and diverse training data than the FineWeb-Edu classifier. In addition, the selected samples from the two methods exhibit complementary coverage. We further examine the distinctive regions identified by AttentionInfluence and FineWeb-Edu Classifier. For example, the samples in Zone1 are related to Health Education, while most samples in Zone2 fall under the theme of Emerging Technologies. This suggests that the samples selected by the two methods can be complementary. How to effectively integrate the strengths of both selection strategies could be promising direction for future exploration."
        },
        {
            "title": "6.3 Scalability of AttentionInfluence",
            "content": "Figure 5 Visualization of data selected by AttentionInfluence and FineWeb-Edu Classifier. We compare the samples selected by the AttentionInfluence method using 1.3B and 7B pretrained language models. We obtain the following insights: AttentionInfluence based on larger LLM selects higher quality data. Similar to the setting in the section 6.1, we use GPT4o to evaluate selected samples. As shown in Table 5, across all domains, the samples selected by the 7B model exhibit high edu scores that are comparable to those selected by the 1.3B model, with slight overall advantage. Regarding reasoning scores, the 7B model consistently outperforms the 1.3B model across all four domains, achieving particularly notable improvement of 9% in the FineWeb-Edu-Dedup domain. 10 Domain 1.3B 7B Edu Score Reasoning Score Token Len Edu Score Reasoning Score Token Len FineWeb-Edu-Dedup Cosmopedia-V2 Python-Edu OpenWebMath 0.99 1.0 0.97 0.96 0.49 0.80 0.87 0.88 1895.7 2774.6 909.3 2138. 0.97 1.0 0.98 0.96 0.58 0.82 0.91 0.93 3488.8 2984.1 1657.2 5550.4 Table 5 The quality score of the data selected by AttentionInfluence using 1.3B and 7B models, respectively. These results suggest that larger models are more effective at identifying reasoning-intensive samples. AttentionInfluence based on larger LLM is more generalizable. As shown in the Figure 6, we compare the sample distributions selected by the 1.3B and 7B models. We observe that the samples selected by the 7B model are more widely distributed in the space, covering many areas that the 1.3B model fails to reach. Notably, regions underrepresented by the 1.3B model are densely populated with specific categories of samples, which are predominantly captured by the 7B model. For instance, Zone1 corresponds to cooking, Zone2 is related to code, and Zone3 mainly focuses on the economy. This suggests that even without additional training, the samples selected by larger models are more balanced and diverse, capturing broader range of information. Moreover, we also trained 7B model on the data selected by AttentionInfluence-7B. As shown in the appendix (see Table 10 and Figure 7), this model achieves better performance than AttentionInfluence-1.3B in the middle and later stages of training, where the average accuracy excludes the result of the C-Eval[19] evaluation task. However, the gap narrows during the final learning rate annealing phase, which is likely due to saturation in the SmolLM corpus and training setup referring to the comparisons with SmolLM[3] and SmolLM2[1]. Importantly, the selected evaluation benchmarks may not fully capture the generalization benefits of AttentionInfluence-7B. For example, while the SmolLM corpus is predominantly English with minimal Chinese contents, we observe that AttentionInfluence-7B significantly outperforms AttentionInfluence-1.3B on the Chinese C-Eval benchmark which is shown in Figure 10, reflecting broader and more robust generalization ability that remains underexplored under the current evaluation settings. Figure 6 The visualization of the samples selected by AttentionInfluence using 1.3B and 7B models."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose AttentionInfluence, training-free method for selecting high-quality pretraining data by leveraging the activation patterns of attention heads in pretrained LLMs. Unlike traditional classifierbased approaches, our method exploits intrinsic model signals to identify reasoning-intensive data, requiring no additional supervision or manual curation. Experimental results on SmolLM-Corpus demonstrate that AttentionInfluence consistently improves downstream performance, selects longer and more diverse high-quality data, and aligns well with existing classifier-based patternswhile offering better weak-to-strong generalization. Our findings suggest that internal model mechanisms can serve as reliable indicators of data quality, offering scalable and efficient pathway for LLM pretraining."
        },
        {
            "title": "References",
            "content": "[1] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. [2] Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. [3] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 2024. URL https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [5] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, and et al. Towards monosemanticity: Decomposing language models with dictionary learning. https://transformer-circuits.pub/2023/ monosemantic-features/index.html, 2023. Accessed: 2023-10-04. [6] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807, 2020. [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. [11] Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. [12] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] William Held, Bhargavi Paranjape, Punit Singh Koura, Mike Lewis, Frank Zhang, and Todor Mihaylov. Optimizing pretraining data mixtures with llm-estimated utility. arXiv preprint arXiv:2501.11747, 2025. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 12 [18] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [19] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. [20] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [21] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. [22] Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, and Ruoxi Jia. The mirrored influence hypothesis: Efficient data influence estimation by harnessing forward passes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2628626295, 2024. [23] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations, 2017. URL https://arxiv.org/abs/1704.04683. [24] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. [25] Ruihang Li, Yixuan Wei, Miaosen Zhang, Nenghai Yu, Han Hu, and Houwen Peng. Scalingfilter: Assessing data quality through inverse utilization of scaling laws. arXiv preprint arXiv:2408.08310, 2024. [26] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. [27] Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. [28] Ang Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan. Interpreting key mechanisms of factual recall in transformer-based language models. arXiv preprint arXiv:2403.19521, 2024. [29] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [30] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. [31] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, In-context learning and induction heads. arXiv preprint Amanda Askell, Yuntao Bai, Anna Chen, et al. arXiv:2209.11895, 2022. [32] Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. [33] Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, and Junbo Zhao. Dataman: Data manager for pre-training large language models. arXiv preprint arXiv:2502.19363, 2025. [34] Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, and Irwin King. Clongeval: chinese benchmark for evaluating long-context large language models. arXiv preprint arXiv:2403.03514, 2024. [35] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. [37] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https: //arxiv.org/abs/2311.12022. 13 [38] Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning in large language models. arXiv preprint arXiv:2411.12580, 2024. [39] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [40] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. [41] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [42] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [44] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [45] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. arXiv preprint arXiv:2402.09739, 2024. [46] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the web: Constructing domains enhances pre-training data curation. arXiv preprint arXiv:2502.10341, 2025. [47] Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. [48] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023. [49] Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. [50] Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. Advances in Neural Information Processing Systems, 37:108735108759, 2024. [51] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [52] Ranchi Zhao, Zhen Leng Thai, Yifan Zhang, Shengding Hu, Yunqi Ba, Jie Zhou, Jie Cai, Zhiyuan Liu, and Maosong Sun. Decoratelm: Data engineering through corpus rating, tagging, and editing with language models. arXiv preprint arXiv:2410.05639, 2024. [53] Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, and Zhiyu Li. Attention heads of large language models: survey. arXiv preprint arXiv:2409.03752, 2024. [54] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [55] Youxiang Zhu, Ruochen Li, Danqing Wang, Daniel Haehn, and Xiaohui Liang. Focus directions make your language models pay more attention to relevant contexts. arXiv preprint arXiv:2503.23306, 2025."
        },
        {
            "title": "A Synthetic Test Sample",
            "content": "model input: Please extract the value corresponding to the specified key from the following JSON object. Output only the value of the corresponding key and nothing else. The JSON data is as follows: {context} {question-shot1} {answer-shot1} {question-shot2} {answer-shot2} {question-shot3} {answer-shot3} {question} answer: {answer}"
        },
        {
            "title": "B Evolution of Retrieval Heads in Pretrained Models",
            "content": "We apply the method described in Section 4.1 to identify retrieval heads at six checkpoints of the pretrained 1.3B-parameter model. These checkpoints correspond to training progress at 5B, 307B, 608B, 898B, 1200B, and 1499B tokens, respectively."
        },
        {
            "title": "C Masking Operation",
            "content": "The \"mask\" operation is to set the attention weights provided by the specific attention heads to equal weights. And if the length of the sequence is L, the attention weight of each token should be set to 1 . Effect of Masking Retrieval Heads vs. Random Non-Retrieval Heads Model 1.3B Hellaswag WinoGrande MMLU MMLU-Pro Benchmarks 0.5715 DROP 0.2344 0.6062 BBH 0.3166 0.1290 0.4258 GSM8K HumanEval Banking77-en-ICL 0.1820 0. 0.4148 Hellaswag WinoGrande MMLU MMLU-Pro 1.3B (Random Masked, Non-Retrieval Heads) 0.5518 DROP 0.2190 0.6069 BBH 0.3005 0. 0.4165 GSM8K HumanEval Banking77-en-ICL 0.1274 0.1159 0.3840 1.3B (Masked, Retrieval Heads) 0.5493 DROP 0.1141 0.5801 BBH 0. 0.0305 0.3089 GSM8K HumanEval Banking77-en-ICL 0.0068 0.1098 0.0001 Hellaswag WinoGrande MMLU MMLU-Pro AGIEval-en 0. AGIEval-en 0.2072 AGIEval-en 0.1298 GPQA 0.2203 GPQA 0.2071 GPQA 0.1827 Table 6 Effect of Masking Retrieval Heads vs. Random Non-Retrieval Heads on Reasoning and In-Context Learning Banking77-en-ICL is an internal evaluation task for assessing models in-context learning ability. It requires models to perform many-shot classification on the Banking77-en dataset[6]. Here, \"Masked, Retrieval Heads\" refers to masking attention heads ranked in the top 5% by retrieval score, while \"Random Masked, NonRetrieval Heads\" refers to randomly masking heads ranked between the top 5% and top 100% (i.e., the remaining 95%) by retrieval score. We conduct the experiments using the models shown in the Table 7 and 15 find that masking retrieval heads significantly impairs the models reasoning performance, while masking random non-retrieval heads has only minor effectconsistent with the findings of Wu et al. [47]. In addition, we find that retrieval heads also play an essential role in the models in-context learning ability."
        },
        {
            "title": "E Experiment Setting",
            "content": "Pretraining Data To ensure reproducibility, we use SmolLM-Corpus[3] as the pretraining dataset. The composition of the SmolLM-Corpus dataset is shown in the Table 8. We sample 100 million tokens from SmolLM-Corpus as the validation dataset. In this work, AttentionInfluence employs internal pretrained Pretrained models used by AttentionInfluence models based on the LLaMA2-alike architecture. The hyperparameters of the models are detailed in Table 7. model pretraining vocab hidden ffn size size inner heads num num shared layers q_head size 1.3B 7B tokens 1.5TB 9TB 155136 155136 2,560 4,096 10,240 16,384 20 32 16 32 2 seq len tie emb 4,096 true 8,192 true Table 7 Hyperparams of the Pretrained Models Used by AttentionInfluence. Model trained in the experiment The hyperparameters are presented in Table 9, and tokenizer used for training and computing token counts is the same as SmolLM3 with vocab size of 49,152. Pretraining setting Referring to SmolLM[3], our experiments are conducted with WSD learning rate scheduler [18] with 0.1% warmup steps, 75% stable phase, and final 25% decay phase. The amount of training tokens is 1 TB. The training is distributed across 32 machines, each equipped with eight H100-80GB GPUs. Dataset FineWeb-Edu-Dedup Cosmopedia-V2 Python-Edu OpenWebMath # Tokens (billions) 193.3 27.9 3. 13.3 Table 8 Composition of the SmolLM Corpus Dataset. model size 7B batch size 1, learning rate hidden size ffn inner num heads num layers shared q_head seq len tie emb total params 4e-4 4,096 8, 32 32 4 8,192 false 6.98B Table 9 Hyperparams of the Model Trained in the Experiment. Evaluation details To ensure that all demonstrations, along with the question and the generated prediction, fit within the 8192-token context window, we use different number of few-shot examples per evaluation task. Specifically, we use the following numbers of demonstrations (in parentheses): MATH (minerva_math) (4), DROP (3), BBH (3), and 5 for all other tasks. We report accuracy for most tasks, with the following exceptions: exact_match for MMLU-Pro, TriviaQA, and BBH; flexible-extract for GSM8K; math_verify for MATH; and F1 score for DROP. When available, we use the normalized accuracy (acc_norm) metric provided by the lm-evaluation-harness. ARC(C+E) denotes the average accuracy over ARC-Challenge (ARC-C) and ARC-Easy (ARC-E). For specific tasks, we adopt the following exceptions: For AGIEval, we conduct the official few-shot evaluation using the official AGIEval repository4. 3https://huggingface.co/HuggingFaceTB/cosmo2-tokenizer 4https://github.com/ruixiangcui/AGIEval/tree/main 16 Figure 7 Performance evolution on comprehensive benchmark evaluations during pretraining. The first 750 billion tokens correspond to the pretraining phase, represented by solid lines, while the subsequent 250 billion tokens represent the learning rate annealing phase, represented by dashed lines, using the same dataset. After around 100 billion tokens, AttentionInfluence-1.3B consistently outperforms the baseline across wide range of tasks on average, including the annealing phase. Figure 8 Training loss For HumanEval, we conduct zero-shot evaluation using the BigCode evaluation harness5 and report pass@1 using the following generation settings, which are the same as those used in SmolLM[3]: temperature = 0.2, top-p = 0.95, n_samples = 20, and max_length_generation = 1024. For DROP, we fix known bug in the lm-evaluation-harness implementation, following the discussion6."
        },
        {
            "title": "F Detailed Performance Evolution During Pretraining",
            "content": "As shown in Figure 7, Figure 9, and Figure 10, we illustrate how the performance of the baseline, the 1.3B method, and the 7B method evolves across different benchmarks as the number of training tokens increases. In addition, panel (b) of Figure 1 and Figure 8 present the training loss comparison among baseline, AttentionInfluence-1.3B, and AttentionInfluence-7B. Furthermore, we report the evaluation results of LLMs trained on data selected by AttentionInfluence-1.3B and AttentionInfluence-7B, as shown in Table 10. 5https://github.com/bigcode-project/bigcode-evaluation-harness 6https://github.com/EleutherAI/lm-evaluation-harness/issues/2137 17 Figure 9 The performance evolution during pretraining on relatively simple benchmarks (i.e., ARC-Challenge, ARC-Easy, WinoGrande, HellaSwag, CommonsenseQA, OpenBookQA, PIQA, TirvialQA). The first 750 billion tokens correspond to the standard pretraining phase (solid lines), followed by 250 billion tokens under learning rate annealing (dashed lines). Curves with the same color (solid and dashed) indicate training on the same dataset. After approximately 100 billion tokens, AttentionInfluence-1.3B consistently outperforms the baseline across broad range of tasks, including during the annealing phase. Figure 10 The performance evolution during pretraining on knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, C-Eval, GPQA, RACE, DROP, BBH, GSM8K, MATH, and HumanEval). The first 750 billion tokens correspond to the standard pretraining phase (solid lines), followed by 250 billion tokens under learning rate annealing (dashed lines). Curves with the same color (solid and dashed) indicate training on the same dataset. After around 100 billion tokens, AttentionInfluence-1.3B consistently outperforms the baseline across wide range of tasks on average, including the annealing phase. LLM-As-A-Judge Experiment Details We use GPT-4o to evaluate the performance of different data selection methods on the FineWeb-Edu-Dedup domain. On one hand, since most of the data in FineWeb-Edu-Dedup is related to education, we aim for the selected high-quality data to be highly relevant to this domain. Therefore, we design an Education Score based 18 Model #Tokens Avg. Metrics AttentionInfluence-1.3B w/o LRD 746B 41.34 AttentionInfluence-7B w/o LRD 746B 41.77 AttentionInfluence-1.3B w/ LRD 1T 45.26 AttentionInfluence-7B w/ LRD 1T 45.57 ARC-C 56.66 PIQA 77.58 BBH 33. ARC-C 55.80 PIQA 78.51 BBH 33.02 ARC-C 59.98 PIQA 79.54 BBH 36.22 ARC-C 56.31 PIQA 79.76 BBH 36.80 ARC-E 82.03 TriviaQA 45.68 GSM8K 15.77 ARC-E 83.25 TriviaQA 46.14 GSM8K 16.45 ARC-E 84.26 TriviaQA 51.20 GSM8K 23. ARC-E 84.05 TriviaQA 51.68 GSM8K 25.78 ARC(C+E) 69.35 MMLU 45.10 MATH 7.25 ARC(C+E) 69.53 MMLU 46.77 MATH 6.78 ARC(C+E) 72.12 MMLU 51.48 MATH 10.80 ARC(C+E) 70.18 MMLU 53.18 MATH 11.75 Hella. 71. Wino. 65.43 CSQA OpenBookQA 53.48 MMLU-Pro AGIEval-en RACE 41.72 GPQA 25.18 17.19 HumanEval 19.85 22.50 C-Eval 28.45 43.60 DROP 32.03 Hella. 71. Wino. 64.33 CSQA OpenBookQA 56.18 MMLU-Pro AGIEval-en RACE 40.29 GPQA 21.73 17.64 HumanEval 21.40 22.64 C-Eval 32.17 44.40 DROP 32.09 Hella. 75. Wino. 68.03 CSQA OpenBookQA 61.59 MMLU-Pro AGIEval-en RACE 42.30 GPQA 24.26 22.03 HumanEval 26.55 26.30 C-Eval 33.06 46.60 DROP 36.52 Hella. 75. Wino. 67.48 CSQA OpenBookQA 62.90 MMLU-Pro AGIEval-en RACE 42.39 GPQA 24.87 21.70 HumanEval 25.06 26.85 C-Eval 36.85 47.00 DROP 36.25 Table 10 The ablation results on various benchmarks. The LRD denotes learning rate decay. on whether the selected sample content is education-related. On the other hand, we want the selected samples to contain more complex, reasoning-intensive knowledge. Based on this criterion, we design Reasoning Score. In summary, we use the following prompt to have GPT-4o score the selected samples: LLM-As-A-Judge Prompt: Given piece of text: <Selected Sample>. Determine whether the text has educational value. If it does, respond with 1; if not, respond with 0. Then, determine whether the text is reasoning-intensive that is, whether it contains explicit or implicit logical reasoning chains. If it does, respond with 1; if not, respond with 0. Respond in the following format: ##Educational Value Score <educational value score> ##Reasoning Intensive Score <reasoning intensive score> Although GPT-4o can also be used for scoring pretraining data, different domains require specially designed prompts. Moreover, the computational cost of using GPT-4o for scoring is very high, whereas AttentionInfluence-1.3B has much lower computational overhead."
        },
        {
            "title": "H Details of Clustering",
            "content": "We obtain document embeddings using Sentence-BERT[36] and apply K-means clustering with = 100. For each cluster, we sample representative documents near the cluster center and use GPT-4o to generate descriptive fine-grained (i.e., secondary) category labels, such as EducationTeaching & Resources. We manually group these secondary labels into six primary categories and report the number of samples falling into each high-level category for both selection methods. 19 Method AttentionInfluence FineWeb-Edu Classifier Ranking 0%- 1% 1%- 10% 10%- 50% 50%- 100% 0%- 1% 1%- 10% Words frac, len, sklearn, append, pyplot, browser, pre, mathbf, 3d, employee, __init__ well, part, movement, children, appreciation, involve, remember, family growth, treatment, principles, business, b, long, work maximize, paintings, independence, therefore, expenses, regulatory, recall square, protocols, monitoring, integrity, consistent, channels, inspiring, width driver, flying, humble, fourier, smoother, longstanding, owl personnel, lawyers, entrenched, beach, brother, oils, wow, desk dimensional, student, 3d, 19th, eco, anti israelite, bmatrix, voter, socio, linspace creative, based, would, sources, do, system, compared, someone studies, delve, true, turn, only, elements, ultimately 10%- 50% argument, bright, rising, excessive, governments, friendships, complicated, discipline constitutes, hearing, consequences, institutional, match, meets, holocaust 50%- 100% peek, manifest, reciprocity, obligations, toilet, customized, olive validity, enriching, profits, presentations, twelve, originating, arithmetic, nazi Table 11 The high-frequency words of different methods. Figure 11 The sample in Python-Edu domain ranked within the top 20% according to AttentionInfluence-1.3B (left) an FineWeb-Edu Classifier (right)."
        },
        {
            "title": "I Case Study",
            "content": "In this section, we present the cases selected by FineWeb-Edu Classifier and AttentionInfluence-1.3B. 20 Figure 12 The sample in OpenWebMath domain ranked within the top 20% according to AttentionInfluence-1.3B (left) and FineWeb-Edu Classifier (right). 21 Figure 13 The samples of clustering in data in the Cosmopedia-V2 domain ranked within 20% according to AttentionInfluence."
        },
        {
            "title": "J Clustering Case",
            "content": "As shown in Figure 13, we present the two clustering cases in the Cosmopedia-V2 domain. 22 Figure 14 The cases of AttentionInfluence in Cosmopeida-V2 domain. Cases of AttentionInfluence based on 1.3B and 7B Models As shown in Figure 14, Figure 15, Figure 16, and Figure 17, we present some cases with different score levels. Figure 15 The cases of AttentionInfluence in FineWeb-Edu-Dedup domain. 24 Figure 16 The cases of AttentionInfluence in OpenWebMath domain. 25 Figure 17 The cases of AttentionInfluence in Python-Edu domain. Figure 18 The cloud maps of the data selected by AttentionInfluence and FineWeb-Edu Classifier, respectively. The left part is the cloud map of FineWeb-Edu Classifier, the right part is that of AttentionInfluence. Figure 19 The sample of doc containing the specific word selected by AttentionInfluence-1.3B (left) and FineWeb-Edu Classifier (right)."
        },
        {
            "title": "L High Frequency Words",
            "content": "As illustrated in Figure 18, we visualize the respective word clouds of AttentionInfluence-1.3B and the FineWeb-Edu Classifier after removing overlapping high-frequency words in the Cosmopeida-V2 domain. The resulting word clouds clearly highlight their distinct focal points, indicating complementary relationship between the two models. To gain deeper insights, we further examine representative samples corresponding to the key terms in each word cloud."
        },
        {
            "title": "M Limitations and Opportunities",
            "content": "While our experimental results demonstrate the effectiveness of AttentionInfluence, several important aspects warrant further investigation. We identify five key areas for future research: Our current experiments demonstrate the effectiveness of AttentionInfluence up to 7B parameters and 1,000B tokens of training budget. Extending this approach to long-horizon training and larger-scale models requires highly expensive computational cost, and we leave it for future research. Due to limited manpower, we do not investigate the effects of selected data by AttentionInfluence on the final performance of models, followed by post-training. We hypothesize that reinforcement learning will amplify the good effects of selected data by AttentionInfluence. While this work focuses on selecting data from short texts, AttentionInfluence can be readily extended to long texts to identify high-quality samples characterized by long-range dependencies. We conduct experiments with alternative approaches for identifying important attention heads, such as the method proposed by Fu et al. [11], which produces partially overlapping yet distinct set of heads compared to ours. Training LLMs based on the data selected by these heads can achieve comparable downstream evaluation performance. More recently, Zhu et al. [55] introduces another compatible method that can be incorporated into our framework. These results demonstrate that AttentionInfluence serves as flexible and general framework: by defining an appropriate proxy task, one can identify task-relevant attention heads and select associated data via masking. The entire pipeline operates without any supervision signals and is modular by design, allowing the proxy task to be easily replaced depending on the target domain or task. Moreover, the framework is effective even when applied to small pretrained language models, making it practical and scalable for wide range of data selection scenarios. More comprehensive proxy tasks can also be designed to better capture specific types of data within the AttentionInfluence framework, further expanding its applicability and customization potential. The combined effect of multiple heads remains unknown. Moreover, this work does not involve research on the MLP. Substantially more in-depth research endeavors are required to unearth the more fundamental and intrinsic mechanisms underpinning language models."
        }
    ],
    "affiliations": [
        "bytedance.com"
    ]
}