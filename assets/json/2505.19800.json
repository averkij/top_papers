{
    "paper_title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
    "authors": [
        "Zaid Alyafeai",
        "Maged S. Al-Shaibani",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets' scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: https://github.com/IVUL-KAUST/MOLE and dataset: https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community."
        },
        {
            "title": "Start",
            "content": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs Zaid Alyafeai1 Maged S. Al-Shaibani2 Bernard Ghanem1 1KAUST 2SDAIA-KFUPM Joint Research Center for AI, KFUPM 5 2 0 2 6 ] . [ 1 0 0 8 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al., 2021b) laid the groundwork for extracting wide range of metadata attributes from Arabic NLP datasets scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code1 and dataset2 for the research community."
        },
        {
            "title": "Introduction",
            "content": "Metadata is data about data The scientific community is experiencing an unprecedented data revolution, with researchers producing and sharing datasets at an extraordinary rate. However, the value of these datasets decreases when they are inadequately documented or difficult to discover. Extracting structured information that describes the characteristics, origins, and usage of datasets, is critical challenge (Borgman, 2012; Wilkinson et al., 2016), especially 1https://github.com/IVUL-KAUST/MOLE 2https://huggingface.co/datasets/IVUL-KAUST/ MOLE Figure 1: Sample metadata extracted from dummy paper with highlighted attributes. in vastly and rapidly growing domains such as natural language processing (NLP). These datasets vary widely in structure, size, format, purpose, and language. Without robust metadata extraction, valuable datasets remain underutilized, research efforts are duplicated, and reproducibility is compromised (Gebru et al., 2021; Dodge et al., 2019). With hundreds of thousands of new publications annually3, automating metadata extraction is essential to maintain the scalability of the scientific ecosystem. This work attempts to approach this problem utilizing LLMs to extract the metadata. We define metadata as JSON object that holds many attributes such as Year, License, and Paper and Dataset Links. Such attributes vary in constraints: some fixed options (License), some free form (Dataset Description), and some context-dependent (Paper & Dataset Links). While existing automated 3As of April 2025, there are more than 2.7 million articles on arXiv. Figure 2: MOLE pipeline. The paper text and Schema are used as input, and the output is the extracted metadata content. approaches typically extract around 5-10 attributes (Ahmad and Afzal, 2020; Tkaczyk et al., 2015), our work automatically extracts around 30 different attributes per paper, providing substantially more comprehensive metadata profile. Figure 1 overviews our work of extracting sample attributes from paper as simplified example. Current metadata extraction approaches typically rely on rule-based systems, supervised machine learning, or combinations thereof (Rodríguez Méndez et al., 2021; Ahmad and Afzal, 2020; Tkaczyk et al., 2015; Constantin et al., 2013; Granitzer et al., 2012; Councill et al., 2008). While effective for structured documents, these methods struggle with the heterogeneity of scientific papers and require domain knowledge and maintenance to accommodate evolving document structures (Rizvi, 2020). Recent advances in Large Language Models (LLMs) have opened new possibilities for information extraction (Team et al., 2023; OpenAI, 2023), with models showing promising results in extracting structured data, (Butcher et al., 2025; Li et al., 2024; Liu et al., 2024; Tam et al., 2024). prominent advantage of modern LLMs is their ability to handle long contexts (Peng et al., 2023; Zhang et al., 2023; Team et al., 2023), allowing our methodology to process entire papers. We summarize our contributions as follows: 1. generalized approach for dataset metadata extraction from scientific papers, capable of extracting more than 30 distinct attributes organized in structured schema hierarchy. ing datasets in multiple languages, covering Arabic, English, Russian, French, and Japanese, enabling systematic evaluation of performance across linguistic domains. 3. An examination of our approach on 7 LLMs, including proprietary and open-source models, analyzing the impact of long-context handling, few-shot learning, and constrained output generation."
        },
        {
            "title": "2 Methodology",
            "content": "Figure 2 illustrates our MOLE framework. This framework processes scientific papers in either LaTeX source or PDF format, leveraging the power of LLMs extraction capabilities to identify dataset metadata attributes. When processing LaTeX, the framework directly analyzes the source; for PDF, we study extracting text manually using the available PDF tools vs. prompting an LLM (with vision capabilities) to extract structured output from the paper. Then, an LLM identifies and structures the metadata according to our schema, which is subsequently validated before producing the final JSON output. This pipeline enables an automated, efficient, and reliable extraction of comprehensive dataset metadata from diverse document formats and across multiple languages."
        },
        {
            "title": "2.1 Metdata",
            "content": "We build on the work of Masader (Alyafeai et al., 2021b) by modifying the following attributes: HF Link we add the Hugging Face4 link to 2. benchmark for metadata extraction involv4Currently, https://huggingface.co/datasets conthe dataset. Access we change the Cost attribute to show the actual cost of the dataset and add Access to highlight the accessibility of the dataset, including Free, Upon-Request, or Paid. Derived From we replace Related datasets by this attribute to mention all datasets that are used as seed for given dataset. This attribute is critical for evaluation as it can indicate any contamination issues. Domain we use the following options to describe the attribute: social media, news articles, commentary, books, Wikipedia, web pages, public datasets, TV channels, captions, LLM, or other. The options are improved by including recent approaches for synthetic data generation using LLMs. Collection Style similar to Domain we use the following options to describe the attribute: crawling, human annotation, machine annotation, manual curation, LLM generated, or other. In total, we have 32 attributes that can be used to annotate Arabic datasets (See Figure 11). We also extend our approach to datasets in other languages by considering the more general attributes. In this study, we consider the following language categories: Arabic (ar), English (en), French (fr), Japanese (jp), Russian (ru), and multilingual (multi). Subsequently, we extend the metadata attributes to such categories. For example, the Script attribute in Japanese could be Kanji, Hiragana, Katakana, or mixed. In other languages, it is fixed, as it is Latin in English and French and Cyrillic in Russian. Note that other languages dont have dialects, so we remove the Dialect and Subset attributes for monolingual datasets. For the multilingual category, we remove the Script attribute and use the Subsets to indicate the languages in the dataset and their corresponding size."
        },
        {
            "title": "2.2 Schema",
            "content": "We define schema as JSON representing how an LLM should generate the metadata. Each metadata attribute is represented by key in the schema. Our metadata schema mainly consists of five keys: tains more than 300K datasets. 1. question specifies what metadata attribute to extract from the document. For example, we ask What is the license of the dataset? for the License attribute. 2. options list of string values to choose from. The LLM must choose an answer from this list. 3. option description dictionary that explains ambiguous options, for example, it might not be clear what low, medium, or high Ethical Risks mean. 4. answer type this field represents the output type for each metadata attribute. The complete list of output types is shown in Table 2. 5. validation group this field is used to collect similar attributes in group. Mainly we use this for evaluation. 6. answer min and max this field specifies the length of an answer for given question. As an example, take the Tasks attribute; then we assume that each dataset must have at least one task associated with it and at maximum 3 tasks. Hence we will have answer_min = 1, answer_max = 3. In general, if answer_min = 0, this attribute is optional. If the answer_max is not defined, then there are no constraints on the output max length. In the following example, we show schema for the License attribute. The answer min and max are set to 1 because dataset must have only one license. { } \" question \": \" What is the license of the dataset ?\" , \" options \": [ \" Apache -2.0 \" , \" MIT \" , ... ], \" answer_type \": \" str \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 1, \" answer_max \" : 1 Code 1: Example Schema for the License metadata attribute. Options are truncated for better visualization"
        },
        {
            "title": "2.3 Validation",
            "content": "We mainly use three types of validations to make sure the output is consistent with our schema: Table 1: Results of all models on the main categories Arabic (ar), English (en), Japanese (jp), French (fr), Russian (ru), Multilingual (multi) datasets. Average shows the weighted average of all categories. Maximum across category is bold and the second maximum is underlined. Model ar en jp fr ru multi Average Gemma 3 27B 56.69 Llama 4 Maverick 58.28 Qwen 2.5 72B 64.17 DeepSeek V3 64.17 Claude 3.5 Sonnet 60.54 GPT 4o 64.17 65.31 Gemini 2.5 Pro 60.00 66.67 62.22 70.00 66.67 71.11 72.22 66.32 68.42 64.21 65.26 71.58 69.47 74. 70.00 68.89 71.11 64.44 74.44 70.00 68.89 66.67 68.89 65.56 70.00 73.33 73.33 73.33 53.68 58.95 55.79 54.74 61.05 60.00 56.84 60.30 62.67 63.96 64.56 65.37 66.68 67.42 Table 2: Permissible data types in our schema. The data types are provided in the schema to force the model to generate specific data types. Table 3: Number of annotated papers and annotated metadata attributes for each category in the collected test dataset."
        },
        {
            "title": "Category",
            "content": "# papers # fields # annotations str url date[year] List[str] float bool List[Dict] string link year of the date list of strings floating point number true or false list of dictionaries ar en fr jp ru multi total 21 5 5 5 5 5 46 64 58 58 60 58 60 358 1,344 290 290 300 290 2,814 1. Type Validation if the output type for given question is not correct, then we either cast it or use the default value. For example, the volume can be casted to float if it is given as str. 2. Option Validation if there are options to answer the question, then if the answer does not belong to one of the options, we use similarity matching to choose the most similar option. 3. Length Validation the output length must be within the range [answer_min, answer_max], otherwise the model will have low score for length enforcing. 4. JSON Validation The generated JSON must be loadable using json.loads(...). To fix unloadable strings, we apply some regex rules. As an example, we remove json prefixes in some generated JSONs."
        },
        {
            "title": "3 Dataset",
            "content": "We manually annotated 52 papers covering datasets in different languages. We used 6 papers for validation and the rest of the papers for testing. We annotate each metadata with two values; the first one is the value of the metadata, and the second with binary value of 1 if the attribute exists in the paper; otherwise, it is 0. The binary annotation will help us in measuring which metadata exists in the papers and which ones require browsing the Web. For example, the License of given dataset might not exist in the paper itself, but most likely it can be accessed through the Link. The articles collected span six different categories: Arabic (ar), English (en), French (fr), Japanese (jp), Russian (ru), and Multilingual (multi) datasets. In Table 3, we highlight the annotated papers in each category in addition to the number of annotated metadata attributes in each category. Additionally, we create six schemata for the different language categories. Figure 3: Latex vs. PDF vs. Docling input formats results across all models. Table 4: Details of models used in our evaluation. Context refers to the maximum context window size in tokens. Underlined models are closed source."
        },
        {
            "title": "Model",
            "content": "Size (B) Context Gemini 2.5 Pro GPT-4o Claude 3.5 Sonnet DeepSeek V3 Qwen 2.5 Llama 4 Maverick Gemma 3 - - - 685 72 400 27 1M 128K 200K 164K 33K 1M 131K"
        },
        {
            "title": "4 Evaluation",
            "content": "We use the OpenRouter5 API to run all the experiments. The temperature is set to 0.0. We use the validation set to tune the system prompt. We repeat each inference maximum of 6 times until there is no error. If there is an error, we return random output. We evaluate diverse set of proprietary and open-source LLMsranging from around 30 to over 600 billion parametersto assess their capabilities on this task, as detailed in Table 4."
        },
        {
            "title": "4.1 Categories",
            "content": "We evaluated all models in the different language categories in Table 1. Generally speaking, Gemini 2.5 Pro achieves the highest average score. Across different categories, Gemini 2.5 Pro also achieves the highest score in 4 out of the 6 categories. Smaller LLMs like Gemma 3, with only 27B parameter,s still achieve decent results on the benchmark. We note that Claude Sonnet 3.5 results in many errors, which caused its score to be lower for some categories. 4."
        },
        {
            "title": "Input format",
            "content": "We experiment with three approaches, using LaTeX as source, PDF text as source, and structured PDF using Docling (Auer et al., 2024). We are interested in validating the performance on other input formats, as, in many scenarios, the LaTeX source may not necessarily be available. To extract the text content of PDF, we use Python pdfplumber6. Across all models, it is not straightforward to find which input format achieves the best results as shown in Figure 3. In general, we observe that most input formats achieve competitive results."
        },
        {
            "title": "4.3 Few-shot",
            "content": "Since our benchmarks requires structured formatting, we test with different n-shot examples. Since processing multiple papers in few-shot is expensive, we rely on synthetic examples creation and only evaluate the results using our top model, which is Gemini 2.5 Pro. We create the examples 5https://openrouter.ai 6https://pypi.org/project/pdfplumber/ Figure 4: Few-shot results with 0, 1, 3, and 5 -shot examples using the Gemini 2.5 Pro model. using template and fill the attributes randomly (see Appendix for more details). In Figure 4, we show that providing examples improve the results compared to zero-shot. In particular, 3-shot examples achieve the highest gain in results compared to zero-shot."
        },
        {
            "title": "4.4 Browsing",
            "content": "Some annotated attributes might not exist in the papers. For example, the license attribute is mostly extracted from the repository where the dataset is hosted. To allow all the models to browse, we use the extracted metadata from the non-browsing approach and the page where the dataset is hosted to predict the updated metadata attributes. For repositories that contain README.md file, like GitHub and HuggingFace, we fetch the file directly from the repository. In Figure 5, we show the results across all the models. We observe clear improvement when using browsing for all of the models. In Gemma 3 27B, we get +1.62 % increase in accuracy compared to non-browsing approach. Table 5: Model scores across three different constraints for the length of answer output (Low, Mid, High)."
        },
        {
            "title": "Low Mid High",
            "content": "Gemma 3 27B DeepSeek V3 Qwen 2.5 72B GPT 4o Llama 4 Maverick Claude 3.5 Sonnet Gemini 2.5 Pro 0.95 0.98 1.00 0.97 0.99 1.00 1.00 0.75 0.96 0.89 0.79 0.92 0.97 0.99 0.60 0.65 0.65 0.70 0.78 0.79 0.85 Figure 5: Browsing vs. no Browsing for all models in our evaluation benchmarks."
        },
        {
            "title": "4.5 Length Enforcing",
            "content": "We use the min and max of the answer to check if the answer from given LLM respects the required length from the schema. Additionally, to see the effect of increasing the length constraints, we define the following three granularities: 1. Low this is the standard type of constraint used in all previous experiments. It is considered more relaxed compared to others. 2. Mid medium constraint used to decrease the range of the following attributes: Name, Description, Provider, Derived From, and Tasks. 3. High similar to Mid, we use the same attributes but with more stricter range. As an example, the attribute Description, will have the answer_max as (50, 25, 12) for low, mid, and high, respectively. In Table 5, we highlight the results across all the models for low, mid, and high length constraints. Still, Gemini 2.5 Pro achieves the highest adherence to constraints."
        },
        {
            "title": "4.6 Context Length",
            "content": "In Figure 6, we show the effect of varying the context length on the results of all models. Interestingly, Gemini 2.5 Pro can still achieve competitive results by only selecting half or quarter of the Table 6: Model comparison across different validation groups. The average is calculated over the four groups. Model Diversity Accessibility Content Evaluation Average Gemma 3 27B Llama 4 Maverick Qwen 2.5 72B DeepSeek V3 Claude 3.5 Sonnet Gemini 2.5 Pro GPT 4o 77.54 72.10 76.45 77.17 74.28 80.07 81. 52.48 57.14 56.21 54.66 56.83 58.07 57.14 63.43 69.18 73.18 73.49 70.92 74.30 71.08 61.59 57.25 52.90 58.70 65.94 63.77 69.57 63.76 63.92 64.69 66.01 66.99 69.05 69.83 Figure 6: The effect of changing the context length on the results of all models. 1: full context, 1/2: half the context, 1/4: quarter of the context. context. This shows that most of the metadata can be extracted at the beginning of the paper. similar trend can also be seen for GPT-4o. For other models, the results are affected significantly, especially for Llama and Claude Sonnet where the results decrease dramatically. Also, we noticed the error frequency increased for such models when using smaller context."
        },
        {
            "title": "4.7 Validation Groups",
            "content": "Our metadata attributes are divided into four validation groups, which are diversity, accessibility, content, and evaluation (see Appendix and Figure 11). In Table 6, we compare the results of different models on the different validation groups. We realize that on the different groups, GPT-4o is more reliable compared to Gemini 2.5 Pro, which achieves worse score on the evaluation group. In general, we see all models struggle to achieve high scores for accessibility, which requires link to the dataset, license, and repository. On the other hand, diversity is the easiest group to extract attributes for. Figure 7 shows the results of 6 attributes across Figure 7: Results across 6 different metadata attributes (Link, Volume, License, Collection Style, Domain, and Tasks). different models. While Gemini 2.5 Pro achieves the highest average score, it doesnt achieve the highest scores across all attributes. Interestingly, smaller LLMs might achieve the highest scores on single attribute like Qwen on the Collection Style attribute."
        },
        {
            "title": "5 Related Work",
            "content": "The exponential growth of research data has made metadata extraction increasingly critical (Gebru et al., 2021; Mahadevkar et al., 2024; Yang et al., 2025). We examine the evolution and current state of metadata extraction research across three areas. Evolution of Metadata Extraction Approaches Early systems relied on rule-based and traditional machine learning methods. CERMINE (Tkaczyk et al., 2015) employed modular architectures for bibliographic extraction, building on methods like PDFX (Constantin et al., 2013). FLAG-PDFe (Ahmad and Afzal, 2020) introduced feature-oriented frameworks using SVMs for scientific publications. Deep learning marked paradigm shift in this field. (An et al., 2017) introduced neural sequence labeling for citation metadata extraction. Multimodal approaches (Boukhers and Bouabdallah, 2022; Liu et al., 2018) integrated NLP with computer vision to handle layout diversity in PDF documents. Recent work includes domain-specific applications in chemistry (Schilling-Wilhelmi et al., 2024; Zhu and Cole, 2022), HPC (Schembera, 2021), and cybersecurity (Pizzolante et al., 2024), with some exploring LLMs for metadata extraction. Cross-lingual approaches have addressed language-specific challenges, including Korean complexities (Kong et al., 2022), Persian (Rahnama et al., 2020), and Arabic NLP resource cataloging through Masader (Alyafeai et al., 2021b; Altaher et al., 2022). Standardization Efforts (Gebru et al., 2021) proposed standardized templates for ML dataset documentation, influencing practices across digital heritage (Alkemade et al., 2023), healthcare (Rostamzadeh et al., 2022), energy (Heintz, 2023), art (Srinivasan et al., 2021), and earth science (Connolly et al., 2025). DescribeML (Giner-Miguelez et al., 2022) provided domain-specific language with IDE integration for practical implementation7. Evaluation Benchmarks Several benchmarks exist for metadata extraction evaluation. PARDA (Fan et al., 2019) provides annotated samples across domains and formats. The unarXive corpus (Saier and Färber, 2020) represents one of the largest scholarly datasets with full-text publications and metadata links. DocBank (Li et al., 2020) and (Meuschke et al., 2023) offer additional evaluation frameworks. However, these focus on general paper attributes (title, authors, abstract) rather than detailed dataset characteristics like volume, license, and subsets that our work addresses."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduced methodology for using LLMs to extract and validate metadata from scientific papers. Through our framework, which extracts around 30 distinct metadata attributes, we offer more robust approach to automating metadata extraction. We experiment with multiple ap7https://code.visualstudio.com/ proaches, including different input formats, fewshot examples, and browsing. We also highlight the effect of length constraints and how LLMs still struggle to follow strict instructions. Throughout our experiments, we show recent advancements in processing long context, especially in flagship models like Gemini 2.5 Pro and GPT-4o that continue to achieve better results. We also release benchmark of 52 papers manually annotated to facilitate research in metadata extraction. This work contributes not only to the advancement of metadata extraction techniques but also to the broader goal of making scientific research more transparent, accessible, and reusable."
        },
        {
            "title": "Limitations",
            "content": "In this section, we provide some possible limitations and our proposed mitigation and possible future suggestions to improve our work: 1. Cost Processing thousands of tokens for given paper increases the cost. Interestingly, We showed in this paper that most metadata can be extracted using smaller context. One possible future direction is to make this process more cost-effective by reducing the context size. The context size can be reduced by using some kind of early skimming using lighter/less expensive LLM. 2. Length Enforcing Length constraining is difficult problem, and current LLMs are not capable of reliably predicting the exact number of tokens (Muennighoff et al., 2025). As possible direction, we can use precise controlling methods like (Butcher et al., 2025) to extract structured data with better adherence to length constraints. If LLMs become optimal at length enforcing, they can become more cost-effective as we can generate as many tokens as we need. 3. Source Code Availability Our approach mostly depends on the availability of LaTeX source code. To mitigate this, we also compare the results for other input formats like PDF and structured format using Docling. However, using such an approach might be difficult to scale, especially due to time constraints. As future direction, we will explore how to improve the skimming process and cleaning of PDF content. We can utilize this method to scale our approach to thousands of papers."
        },
        {
            "title": "Acknowledgments",
            "content": "We want to thank Ali Fadel and Amr Keleg for the useful discussions."
        },
        {
            "title": "References",
            "content": "Muhammad Waqas Ahmad and Muhammad Tanvir Afzal. 2020. FLAG-PDFe: Features oriented metadata extraction framework for scientific publications. IEEE Access, 8:9945899470. Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, and Steven Skiena. 2014. Polyglot-ner: Massive multilingual named entity recognition. arXiv preprint arXiv: 1410.3791. Ahmed Ali, Peter Bell, James Glass, Yacine Messaoui, Hamdy Mubarak, Steve Renals, and Yifan Zhang. 2016. The mgb-2 challenge: Arabic multi-dialect broadcast media recognition. arXiv preprint arXiv: 1609.05625. Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer Khurana, Sree Harsha Yella, James Glass, Peter Bell, and Steve Renals. 2015. Automatic dialect detection in arabic broadcast speech. arXiv preprint arXiv: 1509.06928. Ahmed Ali, Stephan Vogel, and Steve Renals. 2017. Speech recognition challenge in the wild: Arabic mgb-3. arXiv preprint arXiv: 1709.07276. Henk Alkemade, Steven Claeyssens, Giovanni Colavizza, Nuno Freire, Jörg Lehmann, Clemens Neudeker, Giulia Osti, and 1 others. 2023. Datasheets for digital cultural heritage datasets. Journal of open humanities data, 9(17):111. Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, and Chehir Dhaouadi. 2024. 101 billion arabic words dataset. arXiv preprint arXiv: 2405.01590. Ali Alshehri, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2020. Understanding and detecting dangerous speech in social media. arXiv preprint arXiv: 2005.06608. Yousef Altaher, Ali Fadel, Mazen Alotaibi, Mazen Alyazidi, Mishari Al-Mutairi, Mutlaq Aldhbuiub, Abdulrahman Mosaibah, Abdelrahman Rezk, Abdulrazzaq Alhendi, Mazen Abo Shal, and 1 others. 2022. Masader plus: new interface for exploring+ 500 arabic nlp datasets. arXiv preprint arXiv:2208.00932. Zaid Alyafeai, Maged S. Al-shaibani, Mustafa Ghaleb, and Yousif Ahmed Al-Wajih. 2021a. Calliar: An online handwritten dataset for arabic calligraphy. arXiv preprint arXiv: 2106.10745. Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, and Maged S. Al-Shaibani. 2024. Cidar: Culturally relevant instruction dataset for arabic. arXiv preprint arXiv: 2402.03177. Zaid Alyafeai, Maraim Masoud, Mustafa Ghaleb, and Maged Al-shaibani. 2021b. Masader: Metadata sourcing for arabic text and speech data resources. arXiv preprint arXiv:2110.06744. Mohamed Seghir Hadj Ameur, Farid Meziane, and Ahmed Guessoum. 2019. Anetac: Arabic named entity transliteration and classification dataset. arXiv preprint arXiv: 1907.03110. Dong An, Liangcai Gao, Zhuoren Jiang, Runtao Liu, and Zhi Tang. 2017. Citation metadata extraction via deep neural network-based segment sequence labeling. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 19671970. Mikel Artetxe and Holger Schwenk. 2018. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. arXiv preprint arXiv: 1812.10464. Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2020. Xor qa: Cross-lingual open-retrieval question answering. arXiv preprint arXiv: 2010.11856. Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, and 1 others. 2024. Docling technical report. arXiv preprint arXiv:2408.09869. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv: 2308.16884. Yonatan Belinkov, Alexander Magidow, Alberto BarrónCedeño, Avi Shmidman, and Maxim Romanov. 2018. Studying the history of the arabic language: Language technology and large-scale historical corpus. arXiv preprint arXiv: 1809.03891. Sai Saketh Aluru, Binny Mathew, Punyajoy Saha, and Animesh Mukherjee. 2020. Deep learning models for multilingual hate speech detection. arXiv preprint arXiv: 2004.06465. Yonatan Belinkov, Alexander Magidow, Maxim Romanov, Avi Shmidman, and Moshe Koppel. 2016. Shamela: large-scale historical arabic corpus. arXiv preprint arXiv: 1612.08989. Christine Borgman. 2012. The conundrum of sharing research data. Journal of the American Society for Information Science and Technology, 63(6):1059 1078. Zeyd Boukhers and Azeddine Bouabdallah. 2022. Vision and natural language for metadata extraction from scientific pdf documents: multimodal approach. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries, pages 15. Bradley Butcher, Michael OKeefe, and James Titchener. 2025. Precise length control for large language models. Natural Language Processing Journal, page 100143. Mauro Cettolo. 2016. An arabic-hebrew parallel corpus of ted talks. arXiv preprint arXiv: 1610.00572. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv: 2110.14168. Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crossarXiv preprint lingual sentence representations. arXiv: 1809.05053. Charlotte Connolly, Daniel Hueholt, and Melissa Burt. 2025. Datasheets for earth science datasets. Bulletin of the American Meteorological Society. Alexandru Constantin, Steve Pettifer, and Andrei Voronkov. 2013. PDFX: Fully-automated PDF-toXML conversion of scientific literature. In Proceedings of the 2013 ACM symposium on Document engineering, pages 177180. Isaac Councill, Lee Giles, and Min-Yen Kan. 2008. ParsCit: An open-source CRF reference string parsIn Proceedings of LREC, volume 8, ing package. pages 661667. Martin dHoffschmidt, Wacim Belblidia, Tom Brendlé, Quentin Heinrich, and Maxime Vidal. 2020. Fquad: French question answering dataset. arXiv preprint arXiv: 2002.06071. Shahd Dibas, Christian Khairallah, Nizar Habash, Omar Fayez Sadi, Tariq Sairafy, Karmel Sarabta, and Abrar Ardah. 2022. Maknuune: large open palestinian arabic lexicon. arXiv preprint arXiv: 2210.12985. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah Smith. 2019. Show your work: Improved reporting of experimental results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 21852194. Tiantian Fan, Junming Liu, Yeliang Qiu, Congfeng Jiang, Jilin Zhang, Wei Zhang, and Jian Wan. 2019. Parda: dataset for scholarly pdf document metadata extraction evaluation. In Collaborative Computing: Networking, Applications and Worksharing: 14th EAI International Conference, CollaborateCom 2018, Shanghai, China, December 1-3, 2018, Proceedings 14, pages 417431. Springer. Chayma Fourati, Abir Messaoudi, and Hatem Haddad. 2020. Tunizi: tunisian arabizi sentiment analysis dataset. arXiv preprint arXiv: 2004.14303. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM, 64(12):86 92. Joan Giner-Miguelez, Abel Gómez, and Jordi Cabot. 2022. Describeml: tool for describing machine learning datasets. In Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings, pages 2226. Taisia Glushkova, Alexey Machnev, Alena Fenogenova, Tatiana Shavrina, Ekaterina Artemova, and Dmitry I. Ignatov. 2020. Danetqa: yes/no question answering dataset for the russian language. arXiv preprint arXiv: 2010.02605. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzman, and Angela Fan. 2021. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. arXiv preprint arXiv: 2106.03193. Michael Granitzer, Maya Hristakeva, Rhiannon Knight, Kris Jack, and Roman Kern. 2012. comparison of layout based bibliographic metadata extraction techniques. In Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics, pages 18. Ahmed Heakl, Youssef Mohamed, and Ahmed B. Zaky. 2024. Araspider: Democratizing arabic-to-sql. arXiv preprint arXiv: 2402.07448. Ilana Heintz. 2023. Datasheets for energy datasets: An ethically-minded approach to documentation. In Companion Proceedings of the 14th ACM International Conference on Future Energy Systems, pages 4051. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv: 2009.03300. Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. 2023. Acegpt, localizing large language models in arabic. arXiv preprint arXiv: 2309.12053. Julie Hunter, Jérôme Louradour, Virgile Rennard, Ismaïl Harrando, Guokan Shang, and Jean-Pierre Lorré. 2023. The claire french dialogue dataset. arXiv preprint arXiv: 2311.16840. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv: 1705.03551. Nikolay Karpov, Alexander Denisenko, and Fedor Minkin. 2021. Golos: Russian dataset for speech research. arXiv preprint arXiv: 2106.10161. Rachel Keraron, Guillaume Lancrenon, Mathilde Bras, Frédéric Allary, Gilles Moyse, Thomas Scialom, Edmundo-Pavel Soriano-Morales, and Jacopo Staiano. 2020. Project piaf: Building native french question-answering dataset. arXiv preprint arXiv: 2007.00968. Phillip Keung, Yichao Lu, György Szarvas, and Noah A. Smith. 2020. The multilingual amazon reviews corpus. arXiv preprint arXiv: 2010.02573. Mohammed Khalil and Mohammed Sabry. 2024. Athar: high-quality and diverse dataset for classical arabic to english translation. arXiv preprint arXiv: 2407.19835. Hyesoo Kong, Hwamook Yoon, Jaewook Seol, Mihwan Hyun, Hyejin Lee, Soonyoung Kim, and Wonjun Choi. 2022. Annotated open corpus construction and bert-based approach for automatic metadata extracIEEE Access, tion from korean academic papers. 11:825838. Vladislav Korablinov and Pavel Braslavski. 2020. Rubq: russian dataset for question answering over wikidata. arXiv preprint arXiv: 2005.10659. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin. 2024. Arabicmmlu: Assessing massive multitask language understanding in arabic. arXiv preprint arXiv: 2402.12840. Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, and Pierre-Antoine Gourraud. 2023. Frenchmedmcqa: french multiple-choice question answering dataset for medical domain. arXiv preprint arXiv: 2304.04280. Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. 2020. Docbank: benchmark dataset for document layout analysis. arXiv preprint arXiv:2006.01038. Yinghao Li, Rampi Ramprasad, and Chao Zhang. 2024. simple but effective approach to improve structured language model output for information extraction. arXiv preprint arXiv:2402.13364. Michael Xieyang Liu, Frederick Liu, Alexander Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie Cai. 2024. \" we need structured output\": Towards user-centered constraints on large language model output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 19. Runtao Liu, Liangcai Gao, Dong An, Zhuoren Jiang, and Zhi Tang. 2018. Automatic document metaIn Natudata extraction based on deep networks. ral Language Processing and Chinese Computing: 6th CCF International Conference, NLPCC 2017, Dalian, China, November 812, 2017, Proceedings 6, pages 305317. Springer. Antoine Louis and Gerasimos Spanakis. 2021. statutory article retrieval dataset in french. arXiv preprint arXiv: 2108.11792. Natalia Loukachevitch, Ekaterina Artemova, Tatiana Batura, Pavel Braslavski, Ilia Denisov, Vladimir Ivanov, Suresh Manandhar, Alexander Pugachev, and Elena Tutubalina. 2021. Nerel: russian dataset with nested named entities, relations and events. arXiv preprint arXiv: 2108.13112. Supriya Mahadevkar, Shruti Patil, Ketan Kotecha, Lim Way Soong, and Tanupriya Choudhury. 2024. Exploring ai-driven approaches for unstructured document analysis and future horizons. Journal of Big Data, 11(1):92. Norman Meuschke, Apurva Jagdale, Timo Spinde, Jelena Mitrovic, and Bela Gipp. 2023. benchmark of pdf information extraction tools using multitask and multi-domain evaluation framework for academic documents. In International Conference on Information, pages 383405. Springer. Vladislav Mikhailov, Tatiana Shamardina, Max Ryabinin, Alena Pestova, Ivan Smurov, and Ekaterina Artemova. 2022. Rucola: Russian corpus of linguistic acceptability. arXiv preprint arXiv: 2210.12814. Makoto Morishita, Jun Suzuki, and Masaaki Nagata. 2019. Jparacrawl: large scale web-based englisharXiv preprint arXiv: japanese parallel corpus. 1911.10668. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. arXiv preprint arXiv: 1910.07475. Hamdy Mubarak, Sabit Hassan, and Shammur Absar Chowdhury. 2022. Emojis as anchors to detect arabic offensive language and hate speech. arXiv preprint arXiv: 2201.06723. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Moorosi, and Katherine Heller. 2022. Healthsheet: development of transparency artifact for health datasets. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 19431961. Mahmoud Nabil, Mohamed Aly, and Amir Atiya. 2014. Labr: large scale arabic sentiment analysis benchmark. arXiv preprint arXiv: 1411.6718. OpenAI. 2023. GPT-4 technical report. arXiv:2303.08774. Aissam Outchakoucht and Hamza Es-Samaali. 2021. arXiv Moroccan dialect -darijaopen dataset. preprint arXiv: 2103.09687. Bowen Peng, Sami Xie, Hao Yao, Karthik Gu, Skyler Reynolds, Tri Shen, and Denny Zhou. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, and 1090 others. 2025. arXiv preprint arXiv: Humanitys last exam. 2501.14249. Raffaele Pizzolante, Arcangelo Castiglione, and Francesco Palmieri. 2024. Unlocking insights: An extensible framework for automated metadata extracIn 2024 IEEE 23rd tion from online documents. International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), pages 15121521. IEEE. Tarek Saier and Michael Färber. 2020. unarxive: large scholarly data set with publications full-text, annotated in-text citations, and links to metadata. Scientometrics, 125(3):30853108. Björn Schembera. 2021. Like rainbow in the dark: metadata annotation for hpc applications in the age of dark data. The Journal of Supercomputing, 77(8):89468966. Marcel Schilling-Wilhelmi, Melissa Ríos-García, Sameena Shabih, Marcos V. Gil, Santiago Miret, Christoph T. Koch, José A. Márquez, and Kevin Maik Jablonka. 2024. From text to insight: large language models for materials science data extraction. arXiv preprint arXiv:2407.16867. ByungHoon So, Kyuhong Byun, Kyungwon Kang, and Seongjin Cho. 2022. Jaquad: Japanese question answering dataset for machine reading comprehension. arXiv preprint arXiv: 2202.01764. Ryosuke Sonobe, Shinnosuke Takamichi, and Hiroshi Saruwatari. 2017. free large-scale japanese speech corpus for end-to-end speech synthesis. arXiv preprint arXiv: 1711.00354. Jsut corpus: Ramya Srinivasan, Emily Denton, Jordan Famularo, Negar Rostamzadeh, Fernando Diaz, and Beth Coleman. 2021. Artsheets for art datasets. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2). Reid Pryzant, Yongjoo Chung, Dan Jurafsky, and Denny Britz. 2017. Jesc: Japanese-english subtitle corpus. arXiv preprint arXiv: 1710.10639. Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. arXiv preprint arXiv: 1906.00591. Mohadese Rahnama, Seyed Mohammad Hossein Hasheminejad, and Jalal Nasiri. 2020. Automatic metadata extraction from iranian theses and dissertations. In 2020 6th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS), pages 15. IEEE. Syed Toufeeq Ahmad Rizvi. 2020. hybrid approach for information extraction from biomedical text. Ph.D. thesis, University of Bedfordshire. Sergio Rodríguez Méndez, Pouya Omran, Armin Haller, and Kerry Taylor. 2021. MEL: Metadata extractor & loader. In European Semantic Web Conference, pages 159163. Springer. Mihaela Rosca and Thomas Breuel. 2016. Sequence-tosequence neural network models for transliteration. arXiv preprint arXiv: 1610.09565. Negar Rostamzadeh, Diana Mincu, Subhrajit Roy, Andrew Smart, Lauren Wilcox, Mahima Pushkarna, Jessica Schrouff, Razvan Amironesei, Nyalleng Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, ChiehYen Lin, Hung-yi Lee, and Yun-Nung Chen. 2024. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442. Kota Tanabe, Masahiro Suzuki, Hiroki Sakaji, and Itsuki Noda. 2024. Jafin: Japanese financial instruction dataset. arXiv preprint arXiv: 2404.09260. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Dominika Tkaczyk, Paweł Szostek, Mateusz Fedoryszak, Piotr Jan Dendek, and Łukasz Bolikowski. 2015. CERMINE: automatic extraction of structured International metadata from scientific literature. Journal on Document Analysis and Recognition (IJDAR), 18(4):317335. Mark Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip Bourne, and 1 others. 2016. The fair guiding principles for scientific data management and stewardship. Scientific data, 3(1):19. Wenli Yang, Rui Fu, Muhammad Bilal Amin, and Byeong Kang. 2025. Impact and influence of modern ai in metadata management. arXiv preprint arXiv:2501.16605. Wajdi Zaghouani and Anis Charfi. 2018. Arap-tweet: large multi-dialect twitter corpus for gender, age and language variety identification. arXiv preprint arXiv: 1808.07674. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv: 1905.07830. Shouyuan Zhang, Xiaohan Ding, and Alexander Rush. 2023. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Miao Zhu and Jacqueline Cole. 2022. Pdfdataextractor: tool for reading scientific text and interpreting metadata from the typeset literature in the portable document format. Journal of Chemical Information and Modeling, 62(7):16331643."
        },
        {
            "title": "A Datasets",
            "content": "Here is list of the papers we used in our study: RuCoLA (Mikhailov et al., 2022), RuBQ (Korablinov and Braslavski, 2020), NEREL (Loukachevitch et al., 2021), DaNetQA (Glushkova et al., 2020), Golos (Karpov et al., 2021), FQuAD1.1 (dHoffschmidt et al., 2020), FrenchMedMCQA (Labrak et al., 2023), CFDD (Hunter et al., 2023), PIAF (Keraron et al., 2020), BSARD (Louis and Spanakis, 2021), HellaSwag (Zellers et al., 2019), TriviaQA (Joshi et al., 2017), GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2020), HLE (Phan et al., 2025), TUNIZI (Fourati et al., 2020), AraDangspeech (Alshehri et al., 2020), Arabic-Hebrew TED Talks Parallel Corpus (Cettolo, 2016), Shamela (Belinkov et al., 2016), ACVA (Huang et al., 2023), Transliteration (Rosca and Breuel, 2016), LABR (Nabil et al., 2014), OpenITIproc (Belinkov et al., 2018), Calliar (Alyafeai et al., 2021a), Arap-Tweet (Zaghouani and Charfi, 2018), LASER (Artetxe and Schwenk, 2018), DODa (Outchakoucht and Es-Samaali, 2021), ADI5 (Ali et al., 2015), FLORES-101 (Goyal et al., 2021), MGB-3 (Ali et al., 2017), EmojisAnchors (Mubarak et al., 2022), Maknuune (Dibas Figure 8: Distribution of the number of errors per model. This graph only shows the models with at least one error. et al., 2022), ARASPIDER (Heakl et al., 2024), POLYGLOT-NER (Al-Rfou et al., 2014), ATHAR (Khalil and Sabry, 2024), ANETAC (Ameur et al., 2019), CIDAR (Alyafeai et al., 2024), 101 Billion Arabic Words Dataset (Aloui et al., 2024), Belebele (Bandarkar et al., 2023), ArabicMMLU (Koto et al., 2024), WinoMT (Stanovsky et al., 2019), MGB-2 (Ali et al., 2016), JaQuAD (So et al., 2022), JSUT (Sonobe et al., 2017), JESC (Pryzant et al., 2017), JaFIn (Tanabe et al., 2024), JParaCrawl (Morishita et al., 2019), XNLI (Conneau et al., 2018), MARC (Keung et al., 2020), XOR-TyDi (Asai et al., 2020), Multilingual Hate Speech Detection Dataset (Aluru et al., 2020), and MLQA (Lewis et al., 2019)."
        },
        {
            "title": "B Costs",
            "content": "In Table 8, we show the costs for each model in terms of input tokens, output tokens, and cost in one single experiment. In general, we observe that Claude is more costly while processing the least number of tokens. Gemini 2.5 Pro achieves the best cost vs performance results as it is cheaper than other models but achieves better results across different experiments."
        },
        {
            "title": "C Errors",
            "content": "In Figure 8, we show the number of errors for each model. The major proportion of errors comes from Llama 4 and Claude Sonnet 3.5. In general, we run more than 2K inference requests to the OpenRouter API, around 5% have some errors. Most of the errors occur due to failing to read the generated JSON file."
        },
        {
            "title": "D Input Format Processing Time",
            "content": "We analyze the processing time of each input format. Figure 9 shows the average of processing time where we applied each of these methods on the whole testset we have over all categories. The figure shows the varying differences in preprocessing time across these methods. LaTeX source processing is remarkably efficient (0.08s), while PDF extraction via pdfplumber is reasonably fast (2.03s). In contrast, Docling processing requires significantly more compute time (72.31s), highlighting important efficiency trade-offs when scaling metadata extraction to large document collections. Figure 9: Average processing time comparison between different input formats. LaTeX source processing (0.08s) is most efficient, followed by pdfplumber-based PDF extraction (2.03s), while Docling structured parsing (72.31s) requires substantially more computation time. Models Access In Table 7, we highlight the models used for evaluation. We use collection of closed and open models. We use the OpenRouter API to perform inference on all models."
        },
        {
            "title": "F Synthetic Template Generation",
            "content": "For few-shot experiments, we create synthetic examples, which are short paper templates that can be filled using the different attributes in each metadata schema. In Figure 10, we show an example of template. The language table is used for the multilingual schema, where we create table for the language subsets of the datasets. To add variance in the results, we sample different options for attributes like unit, task, collection style, and domain. We also sample different number each time for the volume. For each schema category in our dataset we generate five examples."
        },
        {
            "title": "G Validation Groups",
            "content": "We have four groups that gather similar metadata attributes. Figure 11 shows the different validation {name}: {task} dataset for {schema} {authors} {affs} {name}, is {schema} {task} dataset, that contains {volume} {unit}. {language_table} {provider_stmt} The dataset was collected from {collection_style} of {domain} in {year}. The dataset is publicly available through this link {link}. {license_stmt}. {hf_stmt}. Figure 10: Template-based few-shot example creation. groups in the Arabic subset of the dataset. The general group is not used for validation, as it shows easily extractable attributes. Each group covers attributes that are similar in terms of the grouping function. In-paper Annotations As we explained in our data annotation procedure, we also add binary value to indicate if the attribute is actually extracted from the paper or somewhere else. To test the models ability in only attributes that exist in papers, we show the results in Table 11. As expected, if we only include the annotations that exist in the paper, we get better results across all models. The highest difference happens with the DeepSeek model. Gemini 2.5 Pro still achieves the highest score in terms of Average and Average*."
        },
        {
            "title": "I System Prompt",
            "content": "In Figure 12, we show the system prompt to generate the metadata given the paper and the input schema."
        },
        {
            "title": "J Evaluation metrics",
            "content": "We use either exact match or list matching, depending on the answer type in the schema. For list matching, we use set intersection to compare the results. We use flexible matching where we allow at most one difference between the predicted and the ground truth. For dictionary matching, in addition to values, we also match the keys."
        },
        {
            "title": "K Individual Attribute Evaluation",
            "content": "In Table 9, we highlight the results on individual attributes of metadata. We observe that most models struggle with Links and License in general. This could be attributed to missing such attributes in most papers. To observe the effect of browsing, Table 7: Models used for evaluation and their respective links in OpenRouter. Model Link GPT 4o Claude 3.5 Sonnet Gemini 2.5 Pro DeepSeek V3 Llama 4 Maverick Gemma 3 27B Qwen2.5 72B https://openrouter.ai/openai/gpt-4o https://openrouter.ai/anthropic/claude-3.5-sonnet https://openrouter.ai/google/gemini-2.5-pro-preview-03-25 https://openrouter.ai/deepseek/deepseek-chat-v3-0324 https://openrouter.ai/meta-llama/llama-4-maverick https://openrouter.ai/google/gemma-3-27b-it https://openrouter.ai/qwen/qwen-2.5-72b-instruct Table 8: Input, output, and total tokens and cost in USD for running one experiment on all test sets using LaTeX as input. We use OpenRouter to estimate the number of input and output tokens. Model Input Tokens Output Tokens Total Tokens Cost (USD) Gemma 3 27B Qwen 2.5 72B Llama 4 Maverick DeepSeek V3 Gemini 2.5 Pro GPT 4o Claude 3.5 Sonnet 733902 733604 733734 733664 733733 733733 733734 32944 29689 26942 27897 31954 28205 766846 763293 760676 761561 765687 761938 760044 0.08 0.10 0.14 0.24 1.24 2.12 2.60 Figure 11: Schema validation groups and their associated attributes for the Arabic metadata. Table 9: Results across different attributes that exist in all categories. For better visualization, we remove the versions and naming conventions of each model. Attribute DeepSeek Claude Gemini Gemma Llama Qwen GPT Link HF Link License Language Domain Form Collection Style Volume Unit Ethical Risks Provider Derived From Tokenized Host Access Cost Test Split Tasks 45.65 23.91 17.39 84.78 56.52 95.65 60.87 52.17 67.39 91.30 36.96 56.52 89.13 63.04 95.65 100.00 76.09 43.48 43.48 23.91 26.09 78.26 52.17 86.96 58.70 69.57 50.00 89.13 45.65 60.87 86.96 69.57 89.13 100.00 80.43 56.52 43.48 23.91 30.43 86.96 63.04 97.83 65.22 65.22 58.70 80.43 41.30 52.17 84.78 69.57 97.83 100.00 82.61 56.52 36.96 21.74 15.22 84.78 39.13 89.13 56.52 65.22 56.52 32.61 45.65 67.39 93.48 56.52 93.48 97.83 82.61 34. 47.83 23.91 21.74 86.96 45.65 95.65 56.52 43.48 63.04 82.61 43.48 36.96 89.13 69.57 95.65 97.83 73.91 60.87 47.83 21.74 19.57 84.78 43.48 100.00 69.57 67.39 52.17 89.13 39.13 45.65 82.61 65.22 100.00 100.00 71.74 41.30 47.83 26.09 17.39 89.13 69.57 97.83 63.04 43.48 50.00 80.43 41.30 63.04 89.13 69.57 97.83 100.00 84.78 60.87 Figure 12: System prompt for generating the metadata. You are professional research paper reader. You will be provided Input schema and Paper Text and you must respond with an Output JSON. The Output Schema is JSON with the following format key:answer where the answer represents an answer to the question. The Input Schema has the following main fields for each key: question: question that needs to be answered. options : If the question has options then the question can be answered by choosing one or more options depending on answer_min and answer_max options_description: description of the options that might be unclear. Use the descriptions to understand the options. answer_type: the type of the answer to the question. The answer must follow the type of the answer. answer_min : If the answer_type is List[str], then it defines the minimum number of list items in the answer. Otherwise it defines the minimum number of words in the answer. answer_max : If the answer_type is List[str], then it defines the maximum number of list items in the answer. Otherwise it defines the maximum number of words in the answer. The answer must be the same type as answer_type and its length must be in the range [answer_min, answer_max]. If answer_min = answer_max then the length of answer MUST be answer_min. The Output JSON is JSON that can be parsed using Python json.load(). USE double quotes \"\" not single quotes for the keys and values. The Output JSON has ONLY the keys: columns. The value for each key is the answer to the question that represents the same key in the Input Schema. Table 10: Difference between Browsing and non-browsing models in all attributes. Scores less than 0 are shown in red , while positive scores have green color. Model names are shortened for better visualization. Attribute DeepSeek Claude Gemini Gemma Llama Qwen GPT Link HF Link License Language Domain Form Collection Style Volume Unit Ethical Risks Provider Derived From Tokenized Host Access Cost Test Split Tasks 0.00 0.00 10.87 0.00 0.00 0.00 0.00 -2.17 0.00 0.00 0.00 -2.17 0.00 0.00 -2.17 0.00 0.00 0.00 -4.35 2.17 32.61 0.00 2.17 0.00 0.00 -2.17 -2.17 0.00 -2.17 -4.35 -2.17 0.00 -2.17 0.00 -2.17 2.17 -6.52 2.17 28.26 2.17 0.00 0.00 0.00 -6.52 2.17 2.17 0.00 0.00 2.17 -2.17 -2.17 -2.17 2.17 0. 0.00 0.00 13.04 0.00 4.35 2.17 4.35 0.00 2.17 2.17 0.00 0.00 0.00 0.00 0.00 0.00 2.17 2.17 0.00 0.00 19.57 0.00 2.17 2.17 2.17 -2.17 0.00 0.00 -2.17 0.00 2.17 2.17 0.00 0.00 -2.17 2.17 -10.87 0.00 34.78 0.00 0.00 0.00 0.00 -4.35 2.17 0.00 0.00 2.17 0.00 0.00 -2.17 0.00 0.00 0.00 -8.70 -2.17 26.09 0.00 0.00 0.00 0.00 -6.52 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.17 0.00 Table 11: Comparison between models with and without annotations from the paper. Average is the default average score and Average* is the average score considering only attributes that exist in the paper."
        },
        {
            "title": "Model",
            "content": "Average Average* Difference Llama 4 Maverick Claude 3.5 Sonnet Qwen 2.5 72B Gemma 3 27B Gemini 2.5 Pro GPT 4o DeepSeek V3 62.67 65.37 63.96 60.30 67.42 66.68 64.56 68.31 71.32 69.85 66.38 73.62 73.34 71.25 5.64 5.95 5.89 6.08 6.20 6.66 6.69 we plot the difference between browsing and nonbrowsing models in Table 10. Generally, we see the most gain in extracting the license attribute, which is mostly reported in repositories. However, we see decrease in some other attributes. For example, the Volume is decreased because some papers report different numbers than what is actually released in the data. Additionally, the Link attribute is worse because it seems browsing forces the model to change the link based on what is in the repository. Interestingly, Gemmas results increase across multiple attributes without any decrease."
        },
        {
            "title": "L Schema",
            "content": "In Code 2, we show the full schema used for the Arabic datasets. Note in some experiments, we change the answer_max to reflect to different constraints. { \" Name \": { \" question \": \" What is the name of the dataset ?\" , \" answer_type \": \" str \" , \" answer_min \" : 1, \" answer_max \" : 5 }, \" Subsets \": { \" question \": \" What are the dialect subsets of this dataset ? The keys are Name , Volume , Unit , Dialect . Dialect must be one of the country name from the dialect options \" , \" answer_type \": \" List [ Dict [ Name , Volume , Unit , Dialect ]] \" , \" validation_group \":\" DIVERSITY \" , \" answer_min \" : 0, \" answer_max \" : 29 }, \" Link \": { \" question \": \" What is the link to access the dataset ? The link must contain the dataset . If the dataset is hosted on HuggingFace , use the HF Link .\" , \" answer_type \": \" url \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" HF Link \": { \" question \": \" What is the Huggingface link of the dataset ?\" , \" answer_type \": \" url \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 0, \" answer_max \" : 1 } , \" License \": { \" question \": \" What is the license of the dataset ?\" , \" options \": [ \" Apache -1.0 \" , \" Apache -2.0 \" , \" Non Commercial Use - ELRA END USER \" , \" BSD \" , \" CC BY 1.0 \" , \" CC BY 2.0 \" , \" CC BY 3.0 \" , \" CC BY 4.0 \" , \" CC BY - NC 1.0 \" , \" CC BY - NC 2.0 \" , \" CC BY - NC 3.0 \" , \" CC BY - NC 4.0 \" , \" CC BY -NC - ND 1.0 \" , \" CC BY -NC - ND 2.0 \" , \" CC BY -NC - ND 3.0 \" , \" CC BY -NC - ND 4.0 \" , \" CC BY - SA 1.0 \" , \" CC BY - SA 2.0 \" , \" CC BY - SA 3.0 \" , \" CC BY - SA 4.0 \" , \" CC BY - NC 1.0 \" , \" CC BY - NC 2.0 \" , \" CC BY - NC 3.0 \" , \" CC BY - NC 4.0 \" , \" CC BY -NC - SA 1.0 \" , \" CC BY -NC - SA 2.0 \" , \" CC BY -NC - SA 3.0 \" , \" CC BY -NC - SA 4.0 \" , \" CC0 \" , \" CDLA - Permissive -1.0 \" , \" CDLA - Permissive -2.0 \" , \" GPL -1.0 \" , \" GPL -2.0 \" , \" GPL -3.0 \" , \" LDC User Agreement \" , \" LGPL -2.0 \" , \" LGPL -3.0 \" , \" MIT License \" , \" ODbl -1.0 \" , \" MPL -1.0 \" , \" MPL -2.0 \" , \" ODC - By \" , \" unknown \" , \" custom \" ], \" answer_type \": \" str \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 1, \" answer_max \" : }, \" Year \": { \" question \": \" What year was the dataset published ?\" , \" answer_type \": \" date [ year ]\" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Language \": { \" question \": \" What languages are in the dataset ?\" , \" options \": [\" ar \" , \" multilingual \"], \" option_description \": { \" ar \": \" the dataset is purely in Arabic , there are no other languages involved \" , \" multilingual \": \" the dataset contains samples in other languages \" } , \" answer_type \": \" str \" , \" validation_group \":\" DIVERSITY \" , \" answer_min \" : 1, \" answer_max \" : 1 } , \" Dialect \": { \" question \": \" What is the dialect of the dataset ?\" , \" options \": [ \" Classical Arabic \" , \" Modern Standard Arabic \" , \" United Arab Emirates \" , \" Bahrain \" , \" Djibouti \" , \" Algeria \" , \" Egypt \" , \" Iraq \" , \" Jordan \" , \" Comoros \" , \" Kuwait \" , \" Lebanon \" , \" Libya \" , \" Morocco \" , \" Mauritania \" , \" Oman \" , \" Palestine \" , \" Qatar \" , \" Saudi Arabia \" , \" Sudan \" , \" Somalia \" , \" South Sudan \" , \" Syria \" , \" Tunisia \" , \" Yemen \" , \" Levant \" , \" North Africa \" , \" Gulf \" , \" mixed \" ], \" option_description \": { \" mixed \": \" the dataset contains samples in multiple dialects i.e. social media . Assume Modern Standard Arabic if not specified .\" }, \" answer_type \": \" str \" , \" validation_group \":\" DIVERSITY \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Domain \": { \" question \": \" What is the source of the dataset ?\" , \" options \": [ \" social media \" , \" news articles \" , \" reviews \" , \" commentary \" , \" books \" , \" wikipedia \" , \" web pages \" , \" public datasets \" , \" TV Channels \" , \" captions \" , \" LLM \" , \" other \" ], \" answer_type \": \" List [ str ]\" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 11 } , \" Form \" : { \" question \":\" What is the form of the data ?\" , \" options \": [\" text \" , \" spoken \" , \" images \"], \" answer_type \": \" str \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 1 } , \" Collection Style \": { \" question \": \" How was this dataset collected ?\" , \" options \": [ \" crawling \" , \" human annotation \" , \" machine annotation \" , \" manual curation \" , \" LLM generated \" , \" other \" ] , \" option_description \": { \" crawling \": \" the dataset was collected by crawling the web \" , \" human annotation \": \" the dataset was labelled by human annotators \" , \" machine annotation \": \" the dataset was collected / labelled by machine programs \" , \" manual curation \": \" the dataset was collected manually by human curators \" , \" LLM generated \": \" the dataset was generated by an LLM \" , \" other \": \" the dataset was collected in different way \" }, \" answer_type \": \" List [ str ]\" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 5 }, \" Description \": { \" question \": \" Write brief description about the dataset \" , \" answer_type \": \" str \" , \" answer_min \" : 0, \" answer_max \" : 50 }, \" Volume \": { \" question \": \" What is the size of the dataset ?. If the dataset is multilingual only use the size of the Arabic dataset \" , \" answer_type \": \" float \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1 }, \" Unit \": { \" question \": \" What kind of examples does the dataset include ?\" , \" options \": [\" tokens \" , \" sentences \" , \" documents \" , \" hours \" , \" images \"], \" option_description \": { \" tokens \": \" the dataset contains individual tokens / words \" , \" sentences \": \" the samples are sentences or short paragraphs \" , \" documents \": \" the samples are long documents .e. web pages or books \" , \" hours \": \" the samples are audio files \" , \" images \": \" the samples are images \" }, \" answer_type \": \" str \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Ethical Risks \": { \" question \": \" What is the level of the ethical risks of the dataset ?\" , \" options \": [\" Low \" , \" Medium \" , \" High \"], \" option_description \": { \" Low \": \" most likely no ethical risks associated with this dataset \" , \" Medium \": \" social media datasets \" , \" High \": \" hate / offensive datasets from social media , or web pages \" }, \" answer_type \": \" str \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 1 } , \" Provider \": { \" question \": \" What entity is the provider of the dataset ? Don use Team .\" , \" answer_type \": \" List [ str ]\" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 0, \" answer_max \" : 10 } , \" Derived From \": { \" question \": \" What datasets were used to create the dataset ?\" , \" answer_type \": \" List [ str ]\" , \" validation_group \":\" EVALUATION \" , \" answer_min \" : 0 } , \" Paper Title \": { \" question \": \" What is the title of the paper ?\" , \" answer_type \": \" str \" , \" answer_min \" : 3 } , \" Paper Link \": { \" question \": \" What is the link to the paper ?\" , \" answer_type \": \" str \" , \" answer_min \" : 1 }, \" Script \": { \" question \": \" What is the script of this dataset ?\" , \" options \": [\" Arab \" , \" Latin \" , \" Arab - Latin \"], \" option_description \": { \" Arab \": \" The script used is only in Arabic \" , \" Latin \": \" The script used is only in Latin i.e. it has samples written in Latin like Arabizi or transliteration \" , \" Arab - Latin \": \" The script used is mix of Arabic and Latin \" }, \" answer_type \": \" str \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Tokenized \": { \" question \": \" Is the dataset tokenized ?\" , \" options \": [ true , false ], \" option_description \": { \" true \": \" The dataset is tokenized . Tokenized means the words are split using morphological analyzer \" , \" false \": \" The dataset is not tokenized \" }, \" answer_type \": \" bool \" , \" validation_group \":\" CONTENT \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Host \": { \" question \": \" What is name of the repository that hosts the dataset ?\" , \" options \": [ \" CAMeL Resources \" , \" CodaLab \" , \" data . world \" , \" Dropbox \" , \" Gdrive \" , \" GitHub \" , \" GitLab \" , \" kaggle \" , \" LDC \" , \" MPDI \" , \" Mendeley Data \" , \" Mozilla \" , \" OneDrive \" , \" QCRI Resources \" , \" ResearchGate \" , \" sourceforge \" , \" zenodo \" , \" HuggingFace \" , \" ELRA \" , \" other \" ] , \" answer_type \": \" str \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 1, \" answer_max \" : } , \" Access \": { \" question \": \" What is the accessibility of the dataset ?\" , \" options \": [\" Free \" , \" Upon - Request \" , \" With - Fee \"], \" option_description \": { \" Free \": \" the dataset is public and free to access \" , \" Upon - Request \": \" the dataset is free to access but requires submitting request or filling out form \" , \" With - Fee \": \" the dataset is not free to access \" } , \" answer_type \": \" str \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 1, \" answer_max \" : }, \" Cost \": { \" question \": \" If the dataset is not free , what is the cost ?\" , \" answer_type \": \" str \" , \" validation_group \":\" ACCESSIBILITY \" , \" answer_min \" : 0 }, \" Test Split \": { \" question \": \" Does the dataset contain train / valid and test split ?\" , \" options \": [ true , false ], \" option_description \": { \" true \": \" The dataset contains train / valid and test split \" , \" false \": \" The dataset does not contain train / valid or test split \" }, \" answer_type \": \" bool \" , \" validation_group \":\" EVALUATION \" , \" answer_min \" : 1, \" answer_max \" : }, \" Tasks \": { \" question \": \" What NLP tasks is this dataset intended for ?\" , \" options \": [ \" machine translation \" , \" speech recognition \" , \" sentiment analysis \" , \" language modeling \" , \" topic classification \" , \" dialect identification \" , \" text generation \" , \" cross - lingual information retrieval \" , \" named entity recognition \" , \" question answering \" , \" multiple choice question answering \" , \" information retrieval \" , \" part of speech tagging \" , \" language identification \" , \" summarization \" , \" speaker identification \" , \" transliteration \" , \" morphological analysis \" , \" offensive language detection \" , \" review classification \" , \" gender identification \" , \" fake news detection \" , \" dependency parsing \" , \" irony detection \" , \" meter classification \" , \" natural language inference \" , \" instruction tuning \" , \" Linguistic acceptability \" , \" other \" ] , \" answer_type \": \" List [ str ]\" , \" validation_group \":\" EVALUATION \" , \" answer_min \" : 1, \" answer_max \" : 5 } , \" Venue Title \": { \" question \": \" What is the venue title of the published paper ?\" , \" answer_type \": \" str \" , \" answer_min \" : } , \" Venue Type \": { \" question \": \" What is the venue type ?\" , \" options \": [\" conference \" , \" workshop \" , \" journal \" , \" preprint \"], \" answer_type \": \" str \" , \" answer_min \" : 1, \" answer_max \" : 1 }, \" Venue Name \": { \" question \": \" What is the full name of the venue that published the paper ?\" , \" answer_type \": \" str \" , \" answer_min \" : 0 }, \" Authors \": { \" question \": \" Who are the authors of the paper ?\" , \" answer_type \": \" List [ str ]\" , \" answer_min \" : 1, \" answer_max \" : }, \" Affiliations \": { \" question \": \" What are the affiliations of the authors ?\" , \" answer_type \": \" List [ str ]\" , \" answer_min \" : 0, \" answer_max \" : 20 }, \" Abstract \": { \" question \": \" What is the abstract of the paper ? replace any double quotes in the abstract by single quotes \" , \" answer_type \": \" str \" , \" answer_min \" : } } Code 2: Example Schema for Arabic datasets metadata"
        }
    ],
    "affiliations": [
        "KAUST",
        "SDAIA-KFUPM Joint Research Center for AI, KFUPM"
    ]
}