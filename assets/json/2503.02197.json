{
    "paper_title": "ATLaS: Agent Tuning via Learning Critical Steps",
    "authors": [
        "Zhixun Chen",
        "Ming Li",
        "Yuxuan Huang",
        "Yali Du",
        "Meng Fang",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 9 1 2 0 . 3 0 5 2 : r ATLAS: Agent Tuning via Learning Critical Steps Zhixun Chen1*, Ming Li2*, Yuxuan Huang3, Yali Du4, Meng Fang3, Tianyi Zhou2 1University of Technology Sydney, 2University of Maryland, 3University of Liverpool, 4Kings College London, zhixun.chen@student.uts.edu.au, {minglii, tianyi}@umd.edu {yuxuan.huang, meng.fang}@liverpool.ac.uk, yali.du@kcl.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical stepssuch as planning, complex reasoning for intermediate subtasks, and strategic decision-makingare essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLAS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the trainings focus to few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLAS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLAS maintains and improves base LLM skills as generalist agents interacting with diverse environments."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: The proposed ATLAS identifies the critical steps in expert trajectories collected from diverse interactive environments and finetunes the agent on these steps only, where ai represents the expert action in stepi. ATLAS alleviates the potential overfitting to experts every-step behaviors and achieves better generalizability by training on much fewer steps (less is better). An intelligent agent can perceive its environment, process information, and take actions to achieve specific goals or objectives (Maes, 1995; Wooldridge and Jennings, 1995). Traditional AI agents, especially empowered by Monte Carlo Tree Search (MCTS) or Reinforcement learning, have shown great potential in complex and difficult tasks (Silver et al., 2017; Vinyals et al., 2019). However, these methods are task-specific and suffer from weak generalization across different task domains or environments. On the other hand, the recent rise of Large Language Models (LLMs) (Brown et al., 2020; Team, 2024a; Touvron et al., 2023a,b; Jiang et al., 2023) provide powerful foundation models with rich prior knowledge leading to remarkand reasoning capabilities, able generalization performance across various downstream applications (Zhao et al., 2023; Xu et al., 2024). Their unprecedented performance in different domains, including instruction following, reasoning, planning, and tool usage, makes LLMs ideal agents in multi-task settings (Wei et al., 2022; Kojima et al., 2022; Qin et al., 2023; Patil et al., 2023; Li et al., 2023, 2024a,b; Wang et al., 2024a). Despite their effectiveness, transferring capabilities across tasks remains challenging due to 1 overfitting to expert trajectories and training inefficiency. Most existing agent tuning methods rely on imitation of every step of expert trajectories (Chen et al., 2023; Zeng et al., 2023; Chen et al., 2024; Zhang et al., 2024; Xi et al., 2024; Song et al., 2024). For instance, FireAct (Chen et al., 2023) generates diverse expert trajectories to fine-tune LLMs with enhanced planning abilities, while AgentTuning (Zeng et al., 2023) combines expert trajectories from various interactive environments with general instruction datasets to improve agents generalization capability. However, fine-tuning on the entire expert trajectories can introduce expert bias, causing the model to overfit to specific behaviors and diminishing its ability to generalize to unseen environments (Ghosh et al., 2024). While experts in few critical steps indeed provide key supervised information to improve the agent, the agent may already excel in other non-critical steps. Additionally, fitting the full trajectories of one task may lead to declines in others due to the potential distribution gap and negative transfer between tasks (Song et al., 2024). Moreover, imitating expert trajectories on redundant steps or sub-optimal/replaceable actions incurs excessive training costs. To address the above challenges, we propose ATLAS, Agent Tuning via Learning Critical Steps, novel approach that identifies the critical steps within expert trajectories and only finetunes LLMs on those selected steps, which is illustrated in Figure 1. By steering the trainings focus to few critical steps, we not only reduce the backpropagation cost for training but also lower the overfitting risk to whole trajectories. Moreover, imitation on partial trajectories mitigates the expert bias, and encourages generalization across environments and tasks. In ATLAS, an oracle LLM (GPT4o by default) is utilized as the selector to identify the critical steps semantically based on four criteria: key observations, plan formulation, recalling prior information, and pivotal actions. These critical steps are essential to the success of downstream tasks, but they are usually challenging to be generated by the base LLMs. Our training focuses on finetuning base models on these selected steps. Our experiments provide extensive comparisons between ATLAS and other agent-tuning strategies or LLM agents. As indicated by the results in Figure 2, ATLAS-finetuned agent maintains and improves the generalization capabilities of base LLMs in diverse environments, excelling on both 2 Figure 2: Three base LLMs finetuned by ATLAS vs. full trajectories (100% of the steps), evaluated on held-in and held-out agentic tasks. ATLAS consistently outperforms full-trajectory finetuning, indicating better generalizability of ATLAS by training on fewer but critical steps. held-in and held-out tasks. Our main contributions can be summarized as: We introduce novel method ATLAS reducing the tokens for agent tuning to 30%, by selecting the critical steps in expert trajectories. Agents finetuned on 30% critical steps outperforms the baseline agents finetuned on 100% steps, especially in the multi-task learning scenario, mitigating the expert bias and negative transfer across tasks. ATLAS-finetuned agent achieves better generalization capability than baseline agents on not only held-in but also held-out tasks."
        },
        {
            "title": "2 Preliminaries",
            "content": "As multi-task learning framework, we define set of environments as E. For any particular environment and task prompt space I, the agents dynamics is formalized as Partially Observable Markov Decision Process (POMDP) (Sutton, 2018) ME,I (S, A, O, T, r). In this formalization, denotes the state space, denotes the action space, denotes the observation space, : represents the state transition function, and : defines the reward function. Upon receiving an instruction in environment E, the agent takes action at πθ(ot; E, i) according to its policy πθ, where θ represents the policy parameters. For LLM agents, πθ is an LLM, and its output is not limited to actions and depends on the prompt. The environment then transitions to new state st+1 based on the action, and the agent receives new observation ot+1 O. This interaction continues as the agent engages with the Figure 3: Overall of ATLAS. The selector identifies critical steps in expert trajectories collected in multiple environments, where and denote observation and action, respectively. Training loss is only computed on the critical steps. This encourages more exploration of non-critical steps, reduces the training cost, and improves the agents generalization performance. environment until the task is completed or the maximum step limit is reached. We adopt the ReAct framework (Yao et al., 2022b) for the LLM agent: at each step t, the agent first generates reasoning thought ht with think prompt prompth. Define the trajectory up to step-t as τ1:t (h1, a1, o1, . . . , ht, at, ot), (1)"
        },
        {
            "title": "The thought ht is sampled by",
            "content": "ht πθ(τ1:t1; prompth, E, i). (2) The action is then drawn from the same model with another prompt prompta, i.e., at πθ(τ1:t1, ht; prompta, E, i). (3) The return of trajectory τ = τ1:T , G(τ ) = (cid:80)T t=1 γtrt with discounting factor γ, reflects the agents performance in task within environment E. We collect dataset of expert trajectories for all environments in and tasks in I."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our ATLAS, which identifies critical steps of the expert trajectories and finetunes LLM agents solely on them."
        },
        {
            "title": "3.1 Definition of Critical Steps",
            "content": "Critical steps are key actions or states within an agents trajectory that have substantial impact on the return G(τ ). As depicted in Figure 3, these steps are essential for ensuring the success of the trajectory. We characterize critical steps as those requiring precise decision-making and flawless execution by the agent. In contrast, non-critical steps are more flexible, allowing for adjustments or rearrangements without compromising the overall trajectory. Typically, critical steps are challenging for the base models policy to generate, whereas non-critical steps are relatively easy for the base model. To identify these steps, we sample expert trajectories with dense rewards in available environments. We then empirically categorize four types of critical steps: Plan Creation, Critical Observation, Critical Action, and Self Correction."
        },
        {
            "title": "3.2 Selection of Critical Steps",
            "content": "Properly identifying and extracting critical steps from trajectory is essential for ensuring effective analysis and learning. However, unlike instancelevel data selection for LLMs, which has been explored (Li et al., 2024d,c), step-level selection is challenging since the task-related critical steps can differ dramatically by their semantic meaning. To address this issue, we leverage the selector to identify critical steps based on the critical step identification prompt, promptc. This approach ensures more nuanced and context-aware assessment, helping to accurately identify the most influential steps for optimal performance."
        },
        {
            "title": "The prompt initially defines the critical steps",
            "content": "with the four categories introduced above: Plan Creation: Steps where the LLM agent formulates sub-goals by analyzing previous 3 observations and considering the final objective, breaking down the larger goal into manageable tasks that guide the agents actions toward the overall outcome. Critical Observation: Steps where the LLM agent identifies and analyzes key information from the environment, which helps the agent understand the objective or state and refine its strategy and decision-making toward more effective outcomes. Critical Action: Steps where the LLM agent takes decisive and impactful actions based on prior observations, significantly advancing the process toward the final objectives. These actions are crucial in shaping the direction of the agents strategy and are often pivotal moments that determine progress or failure, ensuring that the agent remains on track to achieve the desired outcome. Self Correction: Steps where the LLM agent carefully recalls and assesses its previous actions or decisions, especially after encountering failure or suboptimal outcomes. During this process, the agent reflects on what went wrong, identifies areas for improvement, and adjusts its approach to enhance future performance, which helps the agent refine its decision-making and better align with the overall objective. After defining critical steps, promptc guides the selector to comprehend the given trajectory and summarize high-level plan with sub-goals of the trajectory to enhance its understanding of the trajectory. Based on the sub-goals, the selector identifies the critical steps and categorizes them into four predefined categories. Notably, we require the selector to select at most percentage of steps in the trajectory and return their indices as Cτ = selector(τ ; promptc, m) (4) ATLAS does not ignore all other steps in the trajectory during training. Instead, we keep the whole original trajectory, including the non-critical steps, as the input sequence but do not compute teacher-force loss on their tokens during training."
        },
        {
            "title": "Critical Step Identification Prompts",
            "content": "1. Generate high-level plan or strategy from the expert conversation, summarizing the key sub-goals required to complete the task. 2. Determine the most critical action steps in the expert conversation based on the plan and sub-goals, ensuring the number of selected steps does not exceed {m% length(τ )}. 3. Justify your selection of these critical steps, specifying the category to which each step belongs. The full prompts are provided in Appendix D.1."
        },
        {
            "title": "3.3 Agent Tuning on Critical Steps",
            "content": "Using the critical step dataset Dc, we optimize the model by calculating the loss exclusively on these critical steps. Training the agent on this reduced set of essential steps enhances both the efficiency and effectiveness of the fine-tuning process, ensuring the model focuses on the key actions that contribute to task success and gains knowledge rather than merely mimicking expert behavior. Thus, the objective function can be defined as: (θ) = τ (cid:34) (cid:88) log πθ(htτ1:t1; prompth, E, i)+ tCτ (cid:35) log πθ(atτ1:t1, ht; prompta, E, i) , where θ denotes the trainable parameter of the LLM agent. This expectation-based objective ensures that the model maximizes probabilities at the critical steps that most influence task success, following the same reasoning as in the full-trajectory approach but applied selectively to Dc."
        },
        {
            "title": "4.1 Environments",
            "content": "In our experimental framework, environments are categorized into held-in and held-out sets to evaluate the models performance. Held-in environments refer to the set of environments that are included in the training dataset. These are the environments in which the models can be fine-tuned, allowing them to learn and adapt their behavior based on the provided expert trajectories. The performance of the held-in environments measures how well the model has learned from the training data and its ability to replicate or improve upon expert behaviors within familiar settings. On the other hand, held-out environments consist of environments and tasks that are excluded from the training process. These environments are used to assess the models generalization capabilities, determining how effectively it can apply learned knowledge to novel and unseen scenarios. The performance of held-out environments indicates the models ability to generalize its decision-making and problemsolving skills beyond the specific examples it was trained on, highlighting its potential for adaptability in diverse and dynamic real-world applications. Specifically, our held-in environments include web navigation task Webshop (Yao et al., 2022a); textual digital games Textcraft (Prasad et al., 2023), Wordle and Maze (Abdulhai et al., 2023); embodied games Alfworld (Shridhar et al., 2020), Scienceworld (Wang et al., 2022) and Babyai (ChevalierBoisvert et al., 2018); tool using tasks Weather, Todo and Movie (Ma et al., 2024). It is important to note that some environments provide dense reward [0, 1], while others offer only sparse rewards {0, 1}. To maintain simplicity and consistency, we follow the evaluation matrix definition used in previous work (Singh et al., 2024; Xi et al., 2024) across all tasks. For the held-out environments, we apply two tool usage environments Sheet and Academic, and two game environments Jericho and PDDL (Ma et al., 2024). detailed description of these environments can be found in Appendix A. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We apply the open-sourced dataset AgentTraj-L 1 (Xi et al., 2024) as our unfiltered dataset Do, which contains the expert trajectories of all held-in tasks mentioned above. We then filter the dataset to obtain the critical step dataset Dc by prompting GPT-4o with the selection instruction Ic. For more details, please refer to Appendix D.1. We employ Llama-3.1-8B-Instruct (Dubey et al., 2024) as the backbone model for our agent unless specified. During training, we focus on the critical step Dc, computing loss exclusively at these pivotal steps in the trajectory. We set the maximum selection ratio for the critical step to be 30%. More information about the training setup is provided in Appendix B. The evaluation prompts are based on the React format (Yao et al., 2022b), consistent with the structure of the training data. 1https://huggingface.co/datasets/AgentGym/AgentTraj-L"
        },
        {
            "title": "4.3 Baselines",
            "content": "We compare our LLM agent against three types of baselines: closed-source models, open-source models, and other fine-tuned LLM agents. Specifically, for closed-source models, we include GPT-3.5-Turbo (Ouyang et al., 2022), GPT-4Turbo (Team, 2024a), Claude 3 (Anthropic, 2024), For and DeepSpeek-Chat (Liu et al., 2024). open-source models, we evaluate Llama3-8BInstruct, Llama3.1-8B-Instruct (Dubey et al., 2024), Gemma-2-9b-it (Gemma Team et al., 2024), Phi-3mini-128k-instruct, Phi-3.5-mini-instruct (Team, 2024b), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and Qwen2.5-7B-Instruct (Team, 2024c). For LLM agents fine-tuned on expert trajectories, we compare Xlam-7B-r (Zhang et al., 2024), AgentLM from AgentTuning(Zeng et al., 2023) and AgentEvol from AgentGym (Xi et al., 2024). We also fine-tune AgentTraj-L using the same base model, but with SFT on the unfiltered dataset Do as baseline for fair comparison. Additionally, we introduce another baseline where the base models perplexity on all steps of the training dataset is used to select critical steps. Specifically, we choose the top 30% of steps with the highest perplexity and fine-tune on those steps."
        },
        {
            "title": "4.4 Main Results",
            "content": "The results shown in Table 1 demonstrate the performance of various models on held-in and held-out tasks. For closed-source models, the reported outcomes follow those presented in AgentGym (Xi et al., 2024). On held-in tasks, our approach, finetuned on just 30% of the entire tokens of the expert trajectories, outperforms baseline methods across most tasks. It also achieves over 5% average improvement compared to models fine-tuned with unfiltered data. Furthermore, in certain tasks such as Alfowrld and Babyai, our model surpasses some advanced closed-source models. The perplexitybased selection performs poorly because it solely measures the token generation difficulty of one step by the base model, which is affected by both the steps importance and expert bias. Consequently, using perplexity for selection forces the model to mimic the experts distribution rather than effectively learning the task knowledge to solve the problem. The results on held-out tasks demonstrate that fine-tuning exclusively on critical steps enhances the generalization of agents to unseen tasks. Focus-"
        },
        {
            "title": "Model",
            "content": "Closed-Source Model DeepSeek-Chat Claude-3-Haiku Claude-3-Sonnet GPT-3.5-Turbo GPT-4-Turbo Open-Source Base Model Llama-2-7B-Instruct Llama-3-8B-Instruct Llama-3.1-8B-Instruct Phi-3-mini-128k-instruct Phi-3.5-mini-instruct Gemma2-9B-it Mistral-7B-Instruct-v0.3 Qwen2.5-7B-Instruct Fine-tuned Agents Xlam-7B-r AgentLM-7B AgentEvol-7B AgentTraj-L (100% steps) Perplexity Selection ATLAS (30% steps)"
        },
        {
            "title": "Jericho PDDL AVG",
            "content": "held-in held-out 51.00 0.00 13.00 26.00 67.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 17.50 71.00 88.00 83.00 78.50 84.50 45.67 1.93 79.25 71.36 72. 23.00 61.60 60.69 19.80 21.00 52.00 17.30 67.00 62.00 0.49 82.70 70.31 77.59 80.98 4.00 4.00 4.00 4.00 68.00 0.00 12.00 16.00 8.00 12.00 8.00 4.00 16.00 0.00 12.00 12.00 48.00 24.00 48.00 70.00 50.00 50.00 70.00 95. 0.00 60.00 65.00 50.00 60.00 55.00 0.00 75.00 20.00 5.00 60.00 75.00 65.00 90.00 16.80 0.83 2.78 7.64 14.38 7.20 70.82 19.57 19.10 18.20 72.60 48.00 15.30 12.00 1.63 38.00 37.92 53.54 42.02 23.00 0.00 38.00 47.00 77. 0.00 15.00 9.00 1.00 3.00 0.00 0.00 7.00 1.00 4.00 64.00 71.00 59.00 72.00 75.00 65.00 80.00 40.00 95.00 0.00 60.00 65.00 40.00 60.00 45.00 10.00 85.00 15.00 15.00 70.00 85.00 70.00 90.00 70.00 55.00 65.00 25.00 80. 0.00 30.00 40.00 20.00 30.00 0.00 5.00 30.00 10.00 0.00 25.00 55.00 30.00 60.00 11.00 5.50 1.50 12.50 15.50 0.00 13.00 2.00 1.00 2.00 0.00 0.00 3.00 37.50 36.50 76.50 68.00 66.50 71.50 24.00 16.00 36.00 20.00 88. 0.00 0.00 0.00 4.00 6.00 0.00 0.00 12.00 4.00 4.00 12.00 12.00 8.00 20.00 39.05 19.83 36.95 32.35 67.32 3.02 32.24 27.72 16.29 21.22 23.26 8.43 31.03 17.9 14.96 52.82 60.52 53.21 65.91 - - - 65.00 80. 0.00 40.00 40.00 25.00 30.00 30.00 0.00 55.00 35.00 10.00 25.00 75.00 30.00 70.00 - - - 38.34 75.83 0.00 35.40 37.31 30.21 33.62 34.46 9.69 39.00 28.40 13.30 26.20 42.83 36.48 49.39 - - - 19.93 52. 1.25 10.51 19.23 1.25 0.56 14.02 1.96 14.26 8.92 15.88 4.05 15.18 13.28 18.21 - - - 24.98 81.16 0.83 16.67 12.18 1.25 0.83 7.20 2.50 0.28 3.06 2.78 3.10 11.71 8.76 15.84 - - - 37.06 72. 0.52 25.65 27.18 14.43 16.25 21.42 3.54 27.14 18.85 10.49 14.59 36.18 22.13 38.36 Table 1: Performance of ATLAS-finetuned LLM agents vs. baseline LLMs and finetuned agents on held-in and held-out test tasks. Unfilter finetunes an LLM on the unfiltered dataset of complete expert trajectories. The best performance in each category is highlighted in bold. By finetuning LLMs only on the critical steps of expert trajectories, ATLAS achieves outstanding agent performance on both held-in and held-out tasks. ing solely on these steps reduces the need to fully replicate the expert LLMs distribution, allowing agents to learn the knowledge of how to solve problems effectively. Unlike other approaches, such as those described in (Zeng et al., 2023), which mix large amount of general instruction data with task-specific trajectory data to enhance generalization, our method achieves comparable performance without relying on such datasets. This underscores the effectiveness of critical step training in boosting agents performance on held-out tasks. To validate the effectiveness of our approach across various backbone models, we perform experiments using Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and Qwen2.5-7B-Instruct (Team, 2024c). The results, summarized in Table 2, demonstrate that our method consistently enhances performance across different models, yielding improvements on both held-in and held-out tasks. The detailed results are shown in Appendix C."
        },
        {
            "title": "4.5.1 Critical Steps vs. Non-Critical Steps\nTo assess the effectiveness of critical steps, we\ncompare ATLAS with finetuning on non-critical\nsteps not selected by the selector LLM in ATLAS.\nThe number of steps for the non-critical selection\ndataset remains identical to our fine-tuning dataset,\nwhich is 30% of the entire training dataset.",
            "content": "The experimental results, presented in Table 2, indicate that fine-tuning with critical steps positively impacts the performance on both held-in and held-out tasks. In contrast, fine-tuning with non-critical steps results in negative impact across all tasks. These findings suggest that certain steps contribute positively to the agents learning by providing useful knowledge, whereas other steps introduce negative biases, leading to decline in performance."
        },
        {
            "title": "4.5.2 Different Selection Ratio",
            "content": "The ratio of critical steps plays crucial role in the fine-tuning process. Therefore, we explore this hyperparameter and require the teacher model to select different proportions of steps from the expert trajectory as critical steps to identify the optimal ratio. As shown in Table 3, 30% ratio yields the best agent performance on both held-in and heldout tasks. We also explore values of greater than 30% but observed only marginal increase in the number of critical steps selected by the selector. Consequently, we did not evaluate fine-tuning results for larger than 30%. Motivated on these findings, we adopt = 30%. More detailed results can be found in Appendix C."
        },
        {
            "title": "4.5.3 Critical Steps vs. Random Steps",
            "content": "To further evaluate the efficacy of using selector LLM for selecting critical steps, we conduct comparisons to randomly selected steps of different ratios. Specifically, we randomly draw 30%, 20%, and 10% steps from the original trajectories and compare them with 30%, 20%, and 10% critical steps for agent tuning. The results are summarized in Table 3, with only the average performance across tasks presented due 6 Data Non-critical Steps (30%) Complete Trajectories (100%) by AgentTraj-L Critical Steps (30%) by ATLAS Alfworld Babyai Maze Movie Sciworld Textcraft Todo Weather Webshop Wordle AVG Academic Sheet 40.42 75.00 42.83 85.00 49.39 90. 12.00 12.00 20.00 68.00 68.00 71.50 57.00 71.00 72.00 39.02 37.92 42.01 75.00 75.00 90.00 55.00 55.00 60. 56.17 60.52 65.91 32.00 48.00 48.00 78.00 83.00 84.50 70.70 70.31 80.98 60.00 75.00 70.00 held-in held-out Jericho PDDL AVG 29.88 8.28 10.80 36.18 11.71 15.18 38.36 15.84 18.21 Table 2: Critical vs. Non-critical steps & complete trajectories for agent tuning (tested on held-in and held-out tasks). The agent trained on ATLAS-selected critical steps does not only outperform the agent trained on full trajectories but also surpasses the agent trained on non-critical steps by large margin. This indicates that the critical steps improve agent tuning while training on non-critical steps might be detrimental to the generalization."
        },
        {
            "title": "Steps",
            "content": "held-in held-out"
        },
        {
            "title": "Complete",
            "content": "100%"
        },
        {
            "title": "Critical\nCritical\nCritical",
            "content": "30% 20% 10% 30% 20% 10% 60.52 59.90 59.08 58.68 65.91 60.40 58.99 36. 38.04 34.73 29.04 38.36 36.96 31.18 Table 3: ATLAS vs. random selection of steps at different ratios (including 100% as reference). to space constraints. Detailed results are provided in Appendix C. Our results indicate that increasing the ratio of random selection improves performance of the fine-tuned LLM agent on both held-in and held-out tasks. However, the random-selections performance at all the three ratios remains inferior to that achieved on ATLASs critical steps, particularly at higher ratios, highlighting the effectiveness of the selector."
        },
        {
            "title": "4.5.4 Value Function defined Critical Steps",
            "content": "To further assess the accuracy of using GPT to identify critical steps, we perform experiments to compare datasets selected based on the estimated value function. Following Xiong et al. (2024), we estimate the value of expert actions by traversing all actions along the trajectory times. Specifically, the agent begins from specific action in the expert trajectory and performs rollouts from that action times. The final rewards from these rollouts are summed and averaged to estimate the value of the action. Subsequently, the differences in value between consecutive steps are calculated, and steps with differences exceeding predefined threshold are identified as critical. We set = 5 for this experiment and detailed methodology is provided in Appendix B.3. Notably, the estimated value function is designed for single-task training, so we compare the performance on the BabyAI and Weather tasks for evaluation These environments are chosen because some others randomly generate goals upon reset, making rollouts infeasible."
        },
        {
            "title": "Weather",
            "content": "ATLAS Value Func. ATLAS Value Func. 78.93 78.58 60.00 60.00 Table 4: ATLAS vs. Value function (estimated oracle) on critical step selection for the BabyAI and Weather tasks. The value function follows Xiong et al. (2024). It measures the importance of each step in MDP but it is computationally intractable in practice. This evaluation aims to verify the consistency between LLM-selected critical steps and their values in MDP. Additionally, environments with long horizons are impractical due to the extensive time and computational resources required. For instance, the rollout of dataset with 2000 trajectories, each averaging 25 steps, necessitates at least 6.5 105 inferences, which is highly inefficient. Table 4 shows that our method achieves performance comparable to the estimated value function approach. However, with limited rollouts, the estimates are often inaccurate, producing values of 0 at initial steps and 1 at later steps. As result, only few steps are identified as critical based on these variations. Despite this limitation, the comparable performance of the two methods suggests the potential for achieving better results with fewer critical steps, which we plan to explore in future work."
        },
        {
            "title": "4.5.5 Choices of Selector LLMs",
            "content": "In this section, we evaluate different models for selecting critical steps. Specifically, we use the open-source model Llama3.1-70B-Instruct2 as the comparison teacher model to identify critical steps on three tasks Alfworld, BabyAI and Weather. The results in Table 5 indicate that GPT4o selected dataset can better fine-tune the LLM agents on all three tasks, showing that the selector models capability significantly impacts the quality of the critical step dataset. The dataset selected 2https://huggingface.co/meta-llama/Llama-3. 1-70B-Instruct"
        },
        {
            "title": "Weather",
            "content": "GPT-4o Llama3.1-70B GPT-4o Llama3.1-70B GPT-4o Llama3.1-70B 83.00 78.50 78.93 67.23 60.00 55. Table 5: Ablation study of the selector LLM for critical step selection on Alfworld, BabyAI, and Weather tasks. Average performance over tasks is reported. fails to accurately by Llama3.1-70B-Instruct identify critical steps, instead including many non-critical steps, which in turn reduces the agents performance after fine-tuning."
        },
        {
            "title": "5.1 LLM Agents",
            "content": "LLMs have emerged as powerful agents capable of performing complex tasks by leveraging their abilities across diverse environments. (Huang et al., 2022a) explores the use of LLMs as zero-shot planners, demonstrating that agents can perform complex tasks directly from natural language without requiring task-specific training. Inner Monologue (Huang et al., 2022b) introduces method in which agents utilize internal monologues for reasoning, using LLM to guide decision-making and planning without the need for external training. ReST meets ReAct (Aksitov et al., 2023) introduces self-improvement mechanism for agents, enabling them to refine their multi-step reasoning skills autonomously through iterative reasoning and action-based feedback. TextGrad (Yuksekgonul et al., 2024) presents an approach where agents perform automatic differentiation through textual instructions, bypassing traditional training and relying on language models to derive solutions. AUTOACT (Qiao et al., 2024) shows how agents can autonomously learn to solve questionanswering tasks through self-planning, by generating and executing their own strategies. SOTOPIA (Wang et al., 2024b) focuses on agents learning socially intelligent behaviors through interactive communication with humans, adapting in real-time without task-specific training."
        },
        {
            "title": "5.2 Tuning Agents",
            "content": "Fine-tuning large language models (LLMs) to enhance their performance in agent-related tasks has emerged as key area of research, with various works proposing innovative frameworks, datasets, and methods. FireAct (Chen et al., 2023) finetunes models using various agent trajectories in the ReAct format, achieving substantial performance boosts. AgentTuning (Zeng et al., 2023) demonstrates the value of instruction tuning with datasets such as AgentInstruct, achieving superior task performance with its AgentLM models. AgentBank (Song et al., 2024) improves agent generalization and performance by fine-tuning LLMs with more than 50,000 diverse interaction trajectories. AgentOhana (Zhang et al., 2024) standardizes and aggregates agent trajectory datasets, facilitating efficient pipelines and providing state-of-the-art results with its xLAM-v0.1 model. AGENTGYM (Xi et al., 2024) enables agent self-evolution through the interactive AGENTEVOL framework, which reduces human supervision through environmental feedback. The IPR framework (Xiong et al., 2024) refines LLM agents with step-level feedback, demonstrating strong results on complex benchmarks. Above all, these works highlight the critical role of fine-tuning in advancing LLM agent capabilities, leveraging diverse data, iterative feedback mechanisms, and innovative frameworks to improve generalization, robustness, and task efficiency."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose novel method, ATLAS, for fine-tuning LLM agents by focusing on critical steps in expert trajectories. Using an oracle LLM as the selector, we first construct dataset of critical steps, which highlights the most impactful actions in the trajectories. We then optimize the LLMs by computing and minimizing the loss exclusively on these critical steps. Experimental results demonstrate that by leveraging only 30% of the steps, our method achieves superior performance in training environments compared to finetuning on 100% of the trajectories. Moreover, the improved generalization ability of our finetuned agents is also reflected in enhanced performance on held-out tasks. These findings underscore the importance of critical step training in maintaining and enhancing the strengths of LLM agents while significantly reducing training costs. This approach offers cost-effective solution for fine-tuning LLMs across diverse tasks and environments."
        },
        {
            "title": "7 Limitation",
            "content": "While our proposed method, ATLAS, demonstrates significant improvements in efficiency and generalization for fine-tuning LLM agents on multi-environments, some aspects could be further improved. Firstly, the current approach to selecting critical steps depends heavily on It is crucial to powerful closed-source models. explore methods that enable precise critical step selection while minimizing time and computatiional costs. Additionally, the existing selection process primarily focuses on semantic aspects; combining it with other metrics could enhance the accuracy of identifying critical steps and further reduce the tokens needed for fine-tuning."
        },
        {
            "title": "References",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. 2023. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232. Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. 2023. Rest meets react: Self-improvement Preprint, for multi-step reasoning llm agent. arXiv:2312.10003. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage models are few-shot learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2018. Babyai: platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Thomas Mesnard Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, and et al. 2024. Gemma. Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha, et al. 2024. closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Preprint, arXiv:2201.07207. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022b. Inner monologue: Embodied reasoning through planning with language models. Preprint, arXiv:2207.05608. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Diederik Kingma. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024a. Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. In Findings of the Association for Computational Linguistics ACL 2024, pages 16189 16211, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, and Tianyi Zhou. 2023. Reflection-tuning: Recycling data for better instruction-tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 9 Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, and Tianyi Zhou. 2024b. Mosaic-it: Free compositional data augmentation improves instruction tuning. arXiv preprint arXiv:2405.13326. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. Autoact: Automatic agent learning from scratch for qa via self-planning. Preprint, arXiv:2401.05268. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024c. Superfiltering: Weak-to-strong data filtering In Proceedings of the for fast instruction-tuning. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1425514273, Bangkok, Thailand. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024d. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 75957628, Mexico City, Mexico. Association for Computational Linguistics. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. arXiv preprint arXiv:2401.13178. Pattie Maes. 1995. Agents that reduce work and information overload. In Readings in humancomputer interaction, pages 811821. Elsevier. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of go without human knowledge. nature, 550(7676):354359. Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond human data: Scaling self-training for problem-solving with language models. Preprint, arXiv:2312.06585. Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. 2024. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories. arXiv preprint arXiv:2410.07706. Richard Sutton. 2018. Reinforcement learning: An introduction. Bradford Book. OpenAI Team. 2024a. Gpt-4 technical report. Preprint, arXiv:2303.08774. Phi-3 Team. 2024b. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2407.10671. Qwen2 Team. 2024c. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. 10 Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. 2024. Watch every step! llm agent learning via iterative step-level process refinement. arXiv preprint arXiv:2406.11176. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024. survey on knowledge distillation of large language models. ArXiv, abs/2402.13116. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. 2024. Textgrad: Automatic \"differentiation\" via text. Preprint, arXiv:2406.07496. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823. Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. survey of large language models. Preprint, arXiv:2303.18223. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. 2019. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. 2024b. Sotopia-π: Interactive learning of socially intelligent language agents. Preprint, arXiv:2403.08715. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than 5th grader? arXiv preprint arXiv:2203.07540. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Michael Wooldridge and Nicholas Jennings. 1995. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115152. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. 2024. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151."
        },
        {
            "title": "A Environment Details",
            "content": "ALFWorld (Shridhar et al., 2020): ALFWorld is simulated household environment developed upon the TextWorld framework, designed to require agents to perform tasks that involve spatial navigation and the application of common-sense knowledge. Within this environment, the action space includes interactions such as object manipulation (e.g., lifting, repositioning), environmental inspection, and the operational use of furniture. Agent actions are executed according to predefined logical rules, with the system generating contextually appropriate feedback. The principal evaluation criterion is the success rate, with each task sequence capped at 30 iterations to balance efficiency and feasibility. For the AGENTTRAJ-L dataset, total of 2,420 trajectories were aggregated, consisting of 1,920 instances generated by state-of-the-art computational models and 500 human-annotated examples to ensure diversity and grounding in real-world reasoning. BabyAI (Chevalier-Boisvert et al., 2018): BabyAI is grid-world simulation platform featuring 40 instruction-following tasks that necessitate agent-object interaction. Agents are constrained to 7x7 observational grid, permitting object manipulation only within immediate proximity. The original framework employs visual observations and primitive actions (e.g., \"move forward,\" \"turn left\"), while an adapted version substitutes visual inputs with textual instructions and integrates highlevel action commands (e.g., \"pick up green key 1,\" \"go through blue locked door 2\"), thereby broadening the action space. step-discounted reward is assigned upon successful task completion, with unsuccessful attempts yielding zero reward. For AGENTTRAJ-L, 810 trajectories were generated using state-of-the-art models. Performance is evaluated via task-specific rewards, capped at 20 rounds per task. MAZE (Abdulhai et al., 2023): Maze is textual environment where agents possess positional awareness, including their current location, target destination, and adjacent obstructions. Agents select movement in one of four directional axes (up, down, left, right) per step, with each action incurring penalty of -1 until goal attainment. AGENTTRAJ-L comprises 215 trajectories. Performance is evaluated via success rate, with task sequences capped at 15 iterations. Movie (Ma et al., 2024): Movie is structured environment enabling LLM agents to leverage specialized tool for accessing film-related metadata (e.g., cinematic details, personnel, production entities). The framework offers 16 discrete actions to execute task-specific objectives. It integrates The Movie Database (TMDB) API, incorporating its dataset and operational functions to facilitate agent interactions. Agents obtain binary reward (1 for correct output alignment with reference solutions, 0 otherwise). AGENTTRAJ-L comprises 215 annotated trajectories, with 20 questions reserved for evaluation and the remainder allocated to training. Success rate serves as the primary performance metric, with task sequences limited to 12 iterations. SciWorld (Wang et al., 2022): SciWorld functions as an empirical assessment framework for evaluating scientific reasoning capabilities in an interactive textual environment, structured to reflect elementary-level curricular standards. The platform encompasses 30 diverse task categories, operationalizing activities such as tool-based measurement and mechanical experimentation. It employs domain-specific action space, with simulator generating contextual feedback on action outcomes. The AGENTTRAJ-L dataset includes 2,120 trajectories synthesized for evaluation. Task performance is measured via reward accumulation, with each trial restricted to maximum of 30 iterations. TextCraft (Prasad et al., 2023): TextCraft is text-based environment modeled after WordCraft, designed for simulating item crafting through hierarchical framework of 544 nodes aligned with Minecrafts recipe hierarchy. Each task specifies target item and sequence of compositional crafting actions (e.g., craft <item> using <ingredients>, get <item>, inventory), with complexity ranging from 1 to 4 procedural steps. The environment provides real-time feedback on action validity and execution states, enabling agents to directly acquire non-craftable items. reward of 1 is granted exclusively upon successful synthesis of the target item. For evaluation, 100 tasks are partitioned from broader training set, with AGENTTRAJ-L comprising 374 trajectories (299 generated by state-of-the-art models, 75 humanannotated). Success rate is the primary metric, capped at 20 rounds per task for empirical validation. TODOList (Ma et al., 2024): Todolist is structured environment enabling LLM agents to manage personal agenda data through task-oriented interface with 11 discrete operational commands. The 12 tool integrates the TodoList API to operationalize task management functionalities. Agents receive binary reward allocation (1 for congruence with reference outputs, 0 otherwise). AGENTTRAJ-L comprises 135 trajectories, with 20 task instances reserved for evaluation and the remainder allocated to training. Performance is assessed via success rate, constrained to maximum of 15 iterations per task. Weather (Ma et al., 2024): Weather is structured environment enabling LLM agents to retrieve meteorological parameters (e.g., temperature, precipitation, air quality) across spatiotemporal contexts via specialized tool. The framework offers 18 discrete operational commands, integrating the Open-Meteo API through Python-based implementation to enable data querying functionalities. Agents receive binary reward (1 for output alignment with reference solutions, 0 otherwise). AGENTTRAJ-L includes 311 annotated trajectories, with 20 queries reserved for evaluation and the remainder allocated to training. Performance is assessed via success rate, constrained to maximum of 10 iterations per task. WebShop (Yao et al., 2022a): Webshop is simulated e-commerce platform where agents execute product procurement tasks adhering to predefined criteria through interface interactions (button-based navigation) or text-based search functionality. The environment integrates 12,000 structured instructions and leverages over one million real-world Amazon product listings, with 6,910 instructions selected for task execution. AGENTTRAJ-L includes 3,930 annotated trajectories. Performance is quantified via success rate, with task sequences limited to 10 rounds to balance efficiency and practical applicability. Wordle (Abdulhai et al., 2023): Wordle is lexical reasoning assessment framework where agents deduce target word from constrained fiveletter vocabulary. The environment operationalizes character-level feedback after each guess, indicating positional accuracy and presence of characters. Agents accumulate step penalty of -1 until successful identification or attempt exhaustion. AGENTTRAJ-L contains 955 trajectories. Performance is quantified via success rate, with trials capped at 8 rounds for empirical validation. Academia (Ma et al., 2024): Academia is structured environment enabling LLM agents to access computer science research resources (e.g., publications, author metadata) via seven discrete operational commands. The tool integrates the Citation Network Dataset to implement core functionalities. Agents receive binary rewards (1 for output alignment with reference solutions, 0 otherwise). Performance is assessed via success rate across 20 evaluation tasks, with trials capped at 12 rounds per task. Sheet (Ma et al., 2024): Sheet is structured environment enabling LLM agents to manipulate spreadsheet data via 20 discrete operational commands, leveraging the Google Sheets API. Reward is determined by structural and content congruence between the agent-modified spreadsheet and reference template, quantified on 01 scale. Evaluation employs 20 predefined tasks, with performance measured via reward accumulation and trials capped at 15 rounds per task. Jericho (Ma et al., 2024): Jericho is textbased game framework designed to evaluate agents capacity for interactive exploration and dynamic world modeling within fictional narratives. These tasks demand agents to infer contextual rules (e.g., magical systems) through iterative interaction rather than pre-existing commonsense knowledge. To address operational feasibility given LLM agents context window constraints, original game objectives (often requiring 50300 steps) are restructured into modular subtasks achievable within 15 steps. For example, the zork1 dungeon exploration is redefined as locate the secret passage entrance in the living room, reducing the sequence to 8 steps. This adaptation preserves core exploratory challenges while aligning task complexity with computational tractability. PDDL (Ma et al., 2024): The Planning Domain Definition Language (PDDL) serves as framework for evaluating strategic reasoning in symbolic planning tasks, with four benchmark domainsGripper, Barman, Blocksworld, and Tyreworlddesigned to test multi-step action sequencing under efficiency constraints. Agents must navigate domain-specific objectives (e.g., transporting objects, mixology, block stacking, tire installation) by optimizing action sequences to minimize redundant operations. For instance, in the Barman domain, agents strategically allocate containers to reduce cleaning cycles during cocktail preparation. To enable natural language interaction, symbolic PDDL predicates (e.g., ontable(shaker1)) and actions (e.g., clean-shaker) are translated into textual observations (e.g., Shaker1 is on the table) and instructions (e.g., Clean shaker1 step t, based on the historical trajectory τt1. The environment then gives final reward ro(i, τn, E) for the trajectory. The estimated reward can be calculated as: rs(st, at) = Eτnπs(τt:nτt1)[ro(i, τn, E)] (6) Due to the complexity of computing this expectation value directly, we utilize the Monte Carlo sampling technique for estimation. By drawing trajectories from step using πs, we create collection of trajectories: {τ (i)i = 1, . . . , } = MCπs(τt1; ) (7) The estimated reward is then calculated as: (cid:80)N rs(st, at) = (cid:40) 1 ro(i, τn, E), i=1 ro(i, τn, E), for < n, for = n. (8) Then the value function π(st) is estimated as the expectation over all possible trajectories starting from state st under policy π: π(st) = Eτt:nπ(τt:nst) (cid:34) (cid:88) (cid:35) γktrs(sk, ak) , k=t (9) where γ is the discount factor which we set to be 0.99 here. Finally, we compute the difference between consecutive steps. If this difference exceeds the threshold of 0.1, the step is identified as critical step. At this point, we designate the expert action corresponding to the same timestep as trainable and calculate the loss associated with that action."
        },
        {
            "title": "C Additional Results",
            "content": "Detailed results of other backbone models, random selection and different critical selection ratio are shown in Table 6. with hand1 while hand2 is empty). Each domain features 1020 curated multi-round problems, with progress measured via normalized matching score that quantifies alignment between the current state and the goal state. Full task completion (100% progress) requires satisfying all goal conditions, such as hierarchical block placement in Blocksworld. This adaptation bridges symbolic planning with language-agent interoperability while preserving strategic complexity."
        },
        {
            "title": "B More Implementation Details",
            "content": "B.1 Training Configuration We use the Adam optimizer (Kingma, 2014), with learning rate of 2e-5 and cosine scheduler for the agent fine-tuning. The models are trained with 3 epoches and warmup rate 0.03. The batch size is 128 and the max length of 8192. All experiments are conducted on 4 NVIDIA A100 80G GPUs and fine-tunes Llama3.1-8B-Instruct on ATLAS takes approximately 8 hours. We use PyTorch FSDP (Paszke et al., 2019) for efficient training. B.2 Perplexity Selection The perplexity-based selection method identifies critical steps by measuring the generation difficulty of each step within trajectory. We compute perplexity as the exponentiated average negative loglikelihood of steps tokens under the models distribution. Formally, the perplexity for step is defined as: PPL(x) = exp 1 x (cid:88) t=1 log p(xt x1:t1) (5) where is the token length of the step, and p(xt x1:t1) is the models probability for token xt given its context. Higher perplexity values indicate steps that are more challenging for the model to generate. Instead of using fixed threshold, we dynamically select the top 30% of steps with the highest perplexity within each trajectory. B.3 Value Estimation We follow the rollout setup of (Xiong et al., 2024), which defines the rollout estimate reward function as rs(st, at) as the estimated outcome reward from the exploration starting step t. The agent with policy πs is employed to rollout trajectory τt:n from Model Alfworld Babyai Maze Movie Sciworld Textcraft Todo Weather Webshop Wordle AVG Sheet Academic Jericho PDDL AVG held-in held-out Mistral-7B-Instruct-v0.3 AgentTraj-L Ours 79.50 85.00 Qwen2.5-7B-Instruct AgentTraj-L Ours 78.50 76.50 Random Selection 40% 30% 20% 10% 83.50 83.50 81.00 78.50 Critical Selection Ratio 84.50 82.50 78. 30% 20% 10% 76.43 77.73 44.00 40.00 75.00 85.00 73.48 80.90 32.00 40. 80.00 80.00 73.08 71.70 72.30 72.78 80.98 73.77 83.76 60.00 40.00 36.00 32.00 48.00 48.00 44.00 85.00 80.00 80.00 80. 90.00 80.00 75.00 47.32 56.59 39.51 41.32 37.51 38.32 39.98 41.02 42.01 34.68 36.13 76.00 69. 55.00 64.00 69.00 67.00 65.00 66.00 72.00 72.00 64.00 75.00 85.00 90.00 80.00 75.00 80.00 80.00 80. 90.00 80.00 80.00 25.00 30.00 40.00 40.00 50.00 50.00 60.00 55.00 60.00 50.00 50.00 72.00 72. 65.50 62.50 69.50 68.50 64.50 65.50 71.50 71.00 67.00 8.00 8.00 57.83 60.83 35.00 45. 8.00 12.00 56.20 57.72 55.00 55.00 16.00 20.00 12.00 16.00 20.00 12.00 12.00 61.86 59.90 59.08 58. 65.91 60.40 58.99 55.00 70.00 65.00 55.00 70.00 70.00 60.00 22.39 30.44 41.56 39.96 40.71 48.63 43.35 36. 49.39 43.90 35.63 6.55 6.32 14.52 16.76 18.97 14.80 13.93 9.80 18.21 25.01 13.01 2.50 2. 16.61 21.07 9.77 11.72 30.21 30.86 15.96 18.71 16.64 14.39 15.84 8.94 16.08 32.66 38.04 34.73 29. 38.36 36.96 31.18 Table 6: Detailed performance of other backbone models, random selection and different critical selection ratio m. The best performance in each section is highlighted in bold."
        },
        {
            "title": "D Prompts Details",
            "content": "D.1 Prompts of Critical Step Selection Critcal Selection Prompts critical step is defined as key action or decision that, if performed correctly, significantly increases the likelihood of successfully completing the task. It represents turning point in the process that influences the outcome of subsequent actions. More specifically, critical steps include: Plan Creation: Steps where the LLM agent formulates sub-goals by analyzing previous observations and considering the final objective, breaking down the larger goal into manageable tasks that guide the agents actions towards the overall outcome. Critical Observation: Steps where the LLM agent identifies and analyzes key information from the environment, which help agent understand the objective or state and refine its strategy and decision-making towards more effective outcomes. Critical Action: Steps where the LLM agent takes decisive and impactful actions based on prior observations, significantly advancing the process toward the final objectives. These actions are crucial in shaping the direction of the agents strategy and are often pivotal moments that determine progress or failure, ensuring that the agent remains on track to achieve the desired outcome. Self Correction: Steps where the LLM agent carefully recalls and assesses its previous actions or decisions, especially after encountering failure or suboptimal outcomes. During this process, the agent reflects on what went wrong, identifies areas for improvement, and adjusts its approach to enhance future performance, which helps the agent refine its decision-making and better align with the overall objective. Task Description: Your task is: 1. Induce high-level plan or strategy based on the expert conversation, summarizing the key steps needed to successfully complete the task. 2. Based on this high-level plan, identify the most critical action steps in the expert conversation. maximum of {} steps may be chosen from the conversation. 3. Provide detailed explanation for choosing these critical steps, specifying which category (e.g., key observation, planning, recall, pivotal action) they belong to and why mastering these steps ensures the success of the task. Answer Format: 1. The high-level plan is: [Summarize the strategy and key steps for task completion] 2. The critical steps are: conversation[...] 3. Reason: [Explain why these steps are critical, including which category they fall into (key observation, planning, recall, pivotal action) and how they enable the player to avoid mistakes in subsequent steps] D.2 Prompts for Task Inference We adopt the setup of AgentGym (Xi et al., 2024) and utilize the same prompts for task inference. For further details, please refer to Appendix G: Prompt Details of AgentGym."
        }
    ],
    "affiliations": [
        "Kings College London",
        "University of Liverpool",
        "University of Maryland",
        "University of Technology Sydney"
    ]
}