{
    "paper_title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
    "authors": [
        "Weiming Ren",
        "Wentao Ma",
        "Huan Yang",
        "Cong Wei",
        "Ge Zhang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640$\\times$360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks."
        },
        {
            "title": "Start",
            "content": "VAMBA: Understanding Hour-Long Videos with Hybrid Mamba-Transformers Weiming Ren1,4, Wentao Ma2, Huan Yang3, Cong Wei1,4, Ge Zhang1,5, Wenhu Chen1,4 1University of Waterloo, 2University of Toronto, 301.AI, 4Vector Institute, 5M-A-P {w2ren,wenhuchen}@uwaterloo.ca https://tiger-ai-lab.github.io/Vamba/ 5 2 0 2 4 1 ] . [ 1 9 7 5 1 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640360) on single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on broad spectrum of long and short video understanding tasks. 1. Introduction The field of large multimodal models (LMMs) has seen tremendous progress in recent years. The seminal work of LLaVA [43] successfully transferred the power of autoregressive next-token prediction from large language models (LLMs) [2, 5, 28, 48, 60] to the multimodal domain, establishing new standard for visual understanding by autoregressively modelling visual and language tokens through causal transformer layers. Since then, transformer-based LMMs have been widely studied and enhanced to tackle diverse range of tasks, such as high-resolution image understanding [32, 42, 62] and interleaved image-text reasoning [29, 33, 34]. Nevertheless, the quadratic complexity inherent in causal self-attention operations presents significant Figure 1. VAMBA achieves strong long video understanding performance (42.1% on LVBench [63]) while being more computationally efficient compared to transformer-based LMMs. challenges for long-context inputs, making it difficult for transformer-based LMMs to effectively handle the task of understanding extremely long videos. key challenge for advanced LMMs when processing long video inputs is that they tend to encode each frame into large number of vision tokens, which leads to substantial computational and memory costs during both training and inference. For example, Qwen2-VL [62] can only process 256 frames (360p) on single GPU, which is far from sufficient for hour-long video understanding. To reduce the computational/memory cost, previous efforts have primarily focused on reducing the vision tokens in the input sequence. Several approaches [20, 36, 38, 72] utilize Q-Former [35] to compress vision tokens. More recent methods, such as LongVU [54] and Video-XL [55], partition vision token sequences into chunks and employ adaptive token compression mechanisms to decrease the token count. Another line of work, such as InternVideo2.5 [65] 1 Table 1. Theoretical time and memory complexity for transformerbased LMMs and VAMBA in pre-filling. and represent the number of vision and text tokens. denotes the hidden dimension. Model Time Complexity Memory Complexity Transformer VAMBA O(d(M + )2) O(dM + d2M ) O((M + )2) O(M ) and Orxy-1.5 [46], employs various mechanisms to evaluate the vision tokens importance, and thus dropping or merging those less significant tokens [6] during training and inference. Despite these advancements, key challenges in LMMs for long video understanding still persist. First, aggressive token reduction can lead to critical information loss, particularly when processing extremely long videos. Second, these methods still suffer from quadratic computational complexity as the number of input frames increases. Third, token reduction-based methods require additional overhead, which might increase the actual wall-clock time. In this study, we investigate an orthogonal direction to previous approaches: instead of compressing the video tokens, we seek to develop an alternative model architecture that improves the efficiency of processing video tokens during training and pre-filling stage of inference. We propose VAMBA, hybrid Mamba-Transformer model for efficient hour-long video understanding. The key insight of our method is that we can design efficient modules to approximate the causal self-attention operation for both text and video tokens in transformer-based LMMs. In particular, we propose to (1) utilize cross-attentions to update text tokens based on video tokens, which is affordable due to the short length of text tokens, and (2) adopt Mamba-2 [17] to process the massive video tokens with linear complexity. Assuming combined input sequence of + tokens, where is the number of video tokens and is the number of text tokens, we find that could be at least 100 times larger than on many long-video tasks (M ). Our model can reduce the training/prefilling computational complexity from O(d(M + )2) to O(dM + d2M ), where is the hidden dimension, according to Table 1. In practice, this theoretical improvement may not be fully realized due to the hardware underoptimization for Mamba [24]. Nevertheless, we still observe more than 50% reduction in GPU memory usage and FLOPs/runtime during training and inference for long video inputs, as shown in Figure 1. VAMBA can be efficiently trained using 8XA100 GPUs, whereas other efficient video LMMs such as LongVU [54] and LongLLaVA [64] require 64 and 24 GPUs for training, respectively. By performing two-stage training, our VAMBA achieves 4.3% improvement over the best efficient video LMMs on the challenging hour-scale video understanding benchmark LVBench [63]. On other long video understanding datasets like Video-MME [21], MLVU [76] and LongVideoBench [66], VAMBA also achieves top-notch performance. Our contributions can be summarized as follows: 1. We propose VAMBA, hybrid Mamba-Transformer model for hour-long video understanding. VAMBAs design features efficient modules such as Mamba-2 blocks and cross-attention layers, effectively reducing the computational overhead of transformer-based LMMs. 2. We conduct comprehensive ablation study and show that employing Mamba-2 blocks and initializing crossattention weights from pretrained self-attention layers are crucial for achieving high performance in VAMBA. 3. Our extensive evaluations demonstrate that VAMBA achieves strong video understanding capabilities. Specifically, VAMBA achieves 4.3% improvement over stateof-the-art efficient video LMMs on the challenging hourlong benchmark LVBench. 2. Preliminaries 2.1. State Space Models and Mamba State space models (SSMs) are linear time-invariant systems for modelling continuous signals. continuous SSM can be expressed by ordinary differential equations (ODEs): h(t) = Ah(t) + Bx(t), y(t) = Ch(t), here, x(t) is the input signal, h(t) RN is the -dimensional hidden state and y(t) is the output signal. RN is the state transition matrix and RN 1, R1N are projection matrices. Mamba [24], and its predecessor S4 [26], are discretized SSMs for discrete sequence modelling. The discretization of the SSM is done by the zero-order hold (ZOH) technique: = exp(A), = (A)1(exp(A) I) B, where A, are discretized state variables and is the step size. The discretized SSM can thus be rewritten as: ht = Aht1 + Bxt, yt = Cht. Mamba proposes the selective scan algorithm that dynamically determines B, C, based on the sequence inputs. Following S4, the matrix is initialized as the diagonalized HiPPO matrix [25]. These design choices are proven to be efficient and effective for modelling extremely long sequences. Mamba-2 [17] further simplifies the formulation of to scalar times identity structure. Mamba-2 supports multi-head SSM and allows much larger state dimension than Mamba, while being faster during training. 2.2. Transformer-based LMMs We study how text and video tokens are being updated in transformers. As shown in Figure 2, we assume standard 2 Figure 2. Overview of our VAMBA model architecture. Compared to transformer-based LMMs (left), we replace the costly causal selfattention operations with the more efficient cross-attention layers and Mamba blocks to achieve better efficiency. case where video tokens are placed before text tokens and ignore the chat template tokens for simplicity. In each LMM decoder layer, the video and text tokens are updated as: All video tokens get updated by an input layer normalization (LN) layer, self-attention layer, post LN layer and final MLP layer. There are also residual connections for self-attention and MLP layers. The ith video token vi computes self-attention based on the first video tokens: ovi = (σ( qviK [v1:vi] )V[v1:vi])Wo, (1) where σ() denotes the softmax operation, qvi is the query vector for vi and K[v1:vi], V[v1:vi] are key and value matrices of the first video tokens. denotes the dimension of the query and key vectors and Wo is the out projection matrix. ovi represents the final output for token vi. The text tokens share similar update route to the video tokens. The self-attention updates are slightly different: for the jth text token tj, it computes self-attention based on all video tokens and the first text tokens: otj = (σ( qtj [Kv, K[t1:tj ]] )[Vv, V[t1:tj ]])Wo, (2) where [Kv, K[t1:tj ]] and [Vv, V[t1:tj ]] are the key and value matrices of the combination of all video tokens and text tokens up to tj. The main computation overhead in the transformerbased LMMs comes from the quadratic complexity of the self-attention in the video tokens. To overcome this issue, we design hybrid Mamba Transformer architecture to process text and video tokens differently. Our detailed model architecture designs are listed in the sections below. 3. Our Method: VAMBA Our goal is to devise model Θ that preserves the performance of transformer Θ, while being more efficient. We approach this problem by approximating the causal transformer layers in the pretrained video LMMs. 3.1. Updating Text Tokens via Cross-Attentions The key idea of our method is to split the expensive selfattention operation over the entire video and text token sequence into two more efficient components. Since video tokens typically dominate the sequence while text tokens remain few, we maintain the self-attention mechanism exclusively for the text tokens and eliminate it for the video tokens. Instead, we add cross-attention layers that use text tokens as queries and video tokens as keys and values. As shown in Figure 2, this design updates the text tokens during the attention layers in our model as follows: 3 otj =(1 α)(σ( qtj v )Vv)Wc (Cross-Attention) + α(σ( qtj [t1:tj ] )V[t1:tj ]])Ws o, (Self-Attention) and Ws (3) where Wc are the output projection matrices for the crossand self-attention layers. α [0, 1] is learnable weighting parameter that balances the crossand self-attention outputs. Assuming total of video tokens and text tokens, the self-attention operations in the transformer-based video LMMs have (pre-filling) computational complexity of O(d(M + )2). Our self+crossattention design has the complexity of O(dN 2) (selfattention) and O(dM ) (cross-attention), effectively reducing the pre-filling complexity to O(dN 2 + dM ) O(dM ), given that d. Our attention formulation can be viewed as an approximation of the causal self-attention operation (Equation 2), as each text token qtj can still attend to all video tokens and the first text tokens. As the video and text tokens share the same channel dimension, the projection matrices in the cross-attention layer will have identical dimensions as in the self-attention layers. This allows us to consider two model design choices: we can either randomly initialize the weights in the cross-attention layers, or we can initialize the cross-attention layers using the self-attention layer weights = Ws (i.e. Wc o, same applies to the query, key and value projection matrices Wc v). We add these two design choices to the VAMBA design space. k, Wc q, Wc 3.2. Updating Video Tokens with Mamba Blocks While the self+cross-attention design significantly reduces the models complexity, relying solely on cross-attention layers can compromise the models representational power. Specifically, rather than updating video tokens through selfattention and MLP layers (i.e., through the causal transformer block, c.f. Section 2.2), the video tokens remain unchanged after the cross-attention layers. To address this limitation, we seek an efficient architecture to approximate the effects of the transformer blocks. Motivated by the success of the Mamba [17, 24] architectures in image and video modelling [37, 45, 77], we propose to employ Mamba blocks to effectively process the video tokens. As shown in Figure 2, the video token updates can be formulated as: all preceding tokens. Crucially, Mamba reduces the complexity of updating video tokens from O(dM 2) in causal self-attentions to O(d2M ). Combining the Mamba blocks and the self+cross-attention design, our model achieves an overall complexity of O(dN 2 + dM + d2M ) O(dM + d2M ), which is substantially lower than the quadratic complexity O(d(M + )2) in the transformerbased video LMMs. In Section 3.3, we further detail how the Mamba layers are trained to approximate Equation 1. We consider employing Mamba or Mamba-2 blocks in Equation 4 in the VAMBA design space. 3.3. Training Paradigm We utilize two-stage training strategy to optimize VAMBA, including pretraining stage and an instruction-tuning stage. In the pretraining stage, the model is initialized with pretrained transformer-based LMM weights in all modules except for the newly introduced cross-attention and Mamba layers. We freeze the pretrained components and train only the new layers using image captioning data to restore the models visual understanding capabilities. We explore two types of training objectives during pretrain- (1) the standard language modelling loss LLM = ing: 1 t=1 log p(xtx<t), which is the cross-entropy loss for next-token (xt) prediction; (2) the distillation loss LDistill, where we extract the top 100 logits PΘ with the highest values from the transformer-based model Θ and compute KL-divergence loss with the logit outputs PΘ of our model Θ at the same indices, such that LDistill = DKL(PΘPΘ) (DKL() denotes KL-divergence). The final loss is formulated as = LLM + λLDistill, where we use λ to balance the weights of the two losses. We include λ = 0, 0.001, 0.1, 0.5, 1, 2 to the VAMBA design space. (cid:80)T In the instruction-tuning stage, we leverage both image and video instruction-following data to fully finetune VAMBA, thereby enhancing its instruction-following capability. When GPU memory limitations prevent full finetuning, we freeze the vision encoder and finetune only the LMM decoder. We only employ the language modelling loss during the instruct-tuning stage to ensure that the teacher model does not restrict the students performance. The full design space for VAMBA includes whether to initialize cross-attention weights from self-attention; whether to use Mamba or Mamba-2 blocks to update video tokens, and the choice of λ in the pretraining stage. 4. Experiments ovi = Mamba(LN(vi), hvi1, A, B, C), (4) In this section, we first ablate the VAMBA design choices and then adopt the best setup for full-scale training. where LN() is the layer normalization operator and hvi1 is the context (hidden states) at token vi1. As recurrent model, Mamba operates similarly to causal self-attention layers, enabling each video token to update itself based on 4.1. Ablation Study We explore the VAMBA design space to determine the effectiveness of our model components. We initialize all Table 2. Ablation study results for the VAMBA design space explorations. CA from SA? denotes whether to initialize crossattention weights from self-attention layers. VidMME represents the Video-MME benchmark. Table 4. Comparisons between baseline models and VAMBA on hour-long video understanding benchmarks. Bold: best results among efficient video LMMs. Underline: second-best. Model ID CA from SA? Mamba Block Type Stage Stage 2 G-VEval LVBench VidMME MVBench ref. free test w/o sub. D N/A N/A Mamba Mamba-2 75.7 81.0 81.8 82.2 23.7 34.2 34.2 35.3 47.6 51.7 53.4 54.1 test 40.9 51.8 53.5 53.5 Table 3. Ablation study results on pretraining VAMBA models with transformer-based LMM teacher and distillation loss. Models Size LVBench HourVideo HourEval Hour-Long Video Understanding test dev test Transformer-based LMMs Gemini-1.5-Pro [58] GPT-4o [49] Qwen2-VL [62] - - 7B 33.1 34.7 42.0 LLaVA-Mini [74] LongLLaVA [64] LongVU [54] Video-XL [55] Efficient LMMs 7B 9B 7B 7B 17.6 31.2 37.8 36.8 37.3 19.6 33.8 20.6 27.7 30.8 33. 33.6 - - 53.0 24.2 39.1 46.8 47.1 50.7 Model + Distill λ value in Stage 1 Training"
        },
        {
            "title": "VAMBA",
            "content": "10B 42.1 0 0.001 0.01 0. 1 2 G-VEval 82.19 81.05 80. 73.69 63.65 47.61 our models based on strong pretrained transformer-based LMM Qwen2-VL-7B [62]. For all ablation experiments, we randomly select one million images from CC12M [9] and employ the PixelProse [56] captions for pretraining. We use the LLaVA-Video-178K [75] dataset with total of 1.3M video-instruction pairs for instruction-tuning. Detailed hyperparameter settings can be found in Appendix 7. Evaluation Metrics For pretraining (stage 1) evaluation, we sample 500 images from the COCO-2017 [14] dataset and have the models generate captions for each image. We then apply the reference-free version of G-VEval [59], metric based on GPT-4o [49] to evaluate the quality of the captions. For instruction-tuning (stage 2), we evaluate model performances across three benchmarks: LVBench [63] for hour-long videos, Video-MME [21] for medium-tolong videos, and MVBench [38] for short videos. Ablation study results are shown in Table 2 and Table 3. Cross-Attention Layer Initialization Strategy According to Table 2, we find that it is crucial to initialize the cross-attention layer weights using the corresponding selfattention layer from the same decoder level. Compared to Model A, the dramatic performance boost observed in Model across all metrics underscores the critical impact of this weight initialization strategy. We believe this observation stems from the fact that initializing the crossattention layer weights from the self-attention layers allows our self+cross-attention operation (Equation 3) to more closely approximate the pretrained causal self-attention opv and Wc q, Wc eration (Equation 2): once Wc are set equal to Ws q, Ws and Ws o, the only discrepancy between the two equations becomes their attention score mak, Ws k, Wc trices. This difference can be mitigated by our two-stage training process, making it easier for Model to recover its multimodal understanding capabilities. Mamba Block Design Choices Comparing Model with Model and in Table 2, we observe that both Mamba and Mamba-2 blocks improve the model performances across all metrics, indicating that Mamba blocks bring extra representation power to the models visual understanding capabilities. Furthermore, when comparing Model and Model D, we find that Mamba-2 demonstrates superior image and video modelling capabilities. Despite having simplified matrix structure, Mamba-2 accommodates larger state dimension than Mamba (64 versus 16 in our case), which likely contributes to its improved performance. Pretraining Distillation Loss We employ the pretrained Qwen2-VL-7B model as the teacher model and perform series of pretraining experiments with the additional distillation loss based on our best model setting (Model in Table 2). The results are shown in Table 3. In contrast to previous findings from cross-attention-based LLMs (e.g. CEPE [69]), where distillation loss is reported to be beneficial, we observe that incorporating an additional distillation loss does not further improve our models performance, as the G-VEval score decreases for all λ > 0. Therefore, we exclude the distillation loss and only employ the language modelling loss in both stages in our final training setting. 4.2. Main Evaluation Results Implementation Details Based on our ablation study  (Table 2)  , we adopt Model as our final design and initialize VAMBA from Qwen2-VL-7B. We source 3M image caption data from CC12M and PixelProse for pretraining and use LLaVA-Video-178K [75], the training set for 5 Table 5. Comparisons between baseline models and VAMBA on medium-length or short video understanding benchmarks. Bold: best results among each section. Underline: second-best. * indicates the results based on our evaluation scripts. Models Size Video-MME MLVU LongVideoBench MVBench NExT-QA DREAM-1K Medium-Length Understanding Short Video Understanding Video Captioning w/o subtitles m-avg val GPT-4V [2] GPT-4o [49] Gemini-1.5-Pro [58] VideoChat2 [38] ShareGPT4Video [12] LongVA [73] Video-CCAM [20] Kangaroo [44] InternVL2 [15] LLaVA-OneVision [33] Qwen2-VL [62] Phi-4-Mini [1] LLaVA-Mini [74] LongLLaVA [64] LongVU [54] Video-XL [55]"
        },
        {
            "title": "VAMBA",
            "content": "- - - 7B 7B 7B 9B 8B 8B 7B 7B 5.6B 7B 9B 7B 7B 10B 59.9 71.9 75.0 39.5 39.9 52.4 50.3 56.0 54.0 58.2 63.3 55. 40.3 51.6 55.3* 55.5 57."
        },
        {
            "title": "Proprietary Models",
            "content": "49.2 64.6 62.9 59.1 66.7 64.0 Open-source Transformer-based LMMs 47.9 46.4 56.3 58.5 61.0 48.1* 62.6* 64.2* 60.1 39.3 39.7 51.8 - 54.8 51.8* 56.4 52.4* 46.7 Open-source Efficient LMMs 44.3 53.3* 65.4 64.9 65.9 19.3* 42.1* 53.5* 50.7 55.9 test 43.5 - 54. 51.9 51.2 49.2 64.6 61.1 66.4 56.7 67.0 60.4 44.5 54.6 66.9 55.3 60.4 mc 70.4 76.0 76.4 78.6 - 68.3 - - - 79.4 - - 47.6* 72.2* 78.0* 77.5* 78.1 F1 34.4 39.2 36.2 26.6 19.5 - - - 26.9 31.7 29.6 - 22.9 24.6 28.1 23. 28.1 NExT-QA [67], ActivityNet [70] and PerceptionTest [50] and 2M image-text data from LLaVA-OneVision [33] for instruction-tuning. The full implementation details can be found in Appendix 7. Hour-Long Video Understanding We evaluate our models ability to handle extremely long videos using two public benchmarks: LVBench [63] and HourVideo [8] (development set). We further compose new benchmark called HourEval by selecting all questions related to videos longer than 30 minutes from Video-MME [21], MLVU [76] development set, and LongVideoBench [66] validation set. The average video lengths for LVBench, HourVideo, and HourEval are 68.4, 47.2, and 54.7 minutes, respectively. We compare our model against four efficient video LMMs: LLaVA-Mini [74], LongLLaVA [64], Video-XL [55] and LongVU [54]. We also include the results from Qwen2-VL7B (our baseline transformer-based LMM) as reference. Experimental results are shown in Table 4. Our VAMBA consistently outperforms all efficient video LMMs across the three hour-long video benchmarks, highlighting its exceptional ability to understand and reason over hour-scale videos. Notably, our model surpasses the baseline Qwen2VL-7B on the LVBench benchmark, and its performance on HourVideo is also very close to Qwen2-VL-7B. These results underscore that our VAMBA is competitive with the best open-sourced transformer-based LMMs, while being significantly more efficient during training and inference. Medium-Length or Short Video Understanding To further demonstrate our models generalizability across various video durations, we test VAMBA on several mediumlength or short video understanding benchmarks. Specifically, we report multiple-choice question performance on Video-MME, MLVU, LongVideoBench, NExT-QA [67] and MVBench [38]. We also evaluate VAMBA on DREAM1K [61] for video captioning assessment. According to Table 5, our VAMBA demonstrates superior performance across three medium-length video understanding benchmarks (with average video durations between 1020 minutes), ranking first among efficient video LMMs on all metrics. Despite the newly integrated cross-attention and Mamba layers in VAMBA have only been trained on relatively smaller datasets, VAMBAs performance remains competitive with large-scale pretrained transformer-based Furthermore, all our evaluations can be conLMMs. ducted on single 80G GPU, while some transformer-based LMMs, such as Qwen2-VL-7B, require additional inference strategies like sliding window attention or ring attention over multiple GPUs to achieve optimal results. For short video understanding and video captioning benchmarks, VAMBA also achieves competitive perfor6 (a) Training GPU Memory Usage Comparison (a) Pre-filling GPU Memory Usage Comparison (b) Training Runtime Comparison (time for 1 training step) (b) Pre-filling FLOPs Comparison (measured using calflops) Figure 3. Comparison of training GPU memory usage and runtime per training step between Qwen2-VL-7B and VAMBA. Figure 4. Comparison of GPU memory usage and FLOPs between Qwen2-VL-7B and VAMBA during inference. mances, ranking first on NExT-QA and DREAM-1K and second on MVBench among efficient LMMs. Overall, our model delivers the best results on medium-length and long video benchmarks, demonstrating its strong ability to handle long-context video-language inputs. 4.3. Runtime Efficiency Analysis To understand our models runtime efficiency gains over the baseline transformer-based LMM (Qwen2-VL-7B), we conduct an efficiency analysis for both training and inference. For model training, we ensure consistent training environment by using 8 NVIDIA A800 80G GPUs for both models. All input videos are resized to resolution of 640 360, and batch size of 1 is maintained on each GPU. We apply standard optimization methods such as Flash-Attention 2 [16], DeepSpeed ZeRO-3 [51] and gradient checkpointing [13] for both models. Based on different numbers of input frames, we measure the average GPU memory across 8 GPUs and the runtime per training step during training. As shown in Figure 3a, although VAMBA and Qwen2-VL-7B share the same vision encoder design and generate vision tokens of identical sequence lengths, our model requires over 50% less training memory when processing videos with more than 16 frames. This efficiency gain allows us to handle larger number of video frames during training (512 vs. 128). Furthermore, as depicted in Figure 3b, our efficient design also accelerates model training, achieving nearly twice the speedup per training step when working with more than 64 frames. For model inference, we focus on the pre-fill stage and analyze GPU memory usage and FLOPs for both models. All measurements are conducted on an input video with resolution of 640 360. As shown in Figure 4a, VAMBA requires slightly more GPU memory than Qwen2-VL-7B when the number of input frames is low (fewer than 32 frames), due to the additional 3B parameters in its crossattention and Mamba layers. However, VAMBAs memory usage increases more slowly as the frame increases, allowing it to handle four times as many frames on single NVIDIA A800 80G GPU compared to Qwen2-VL-7B (256 vs. 1024). Regarding computational cost, VAMBA reduces FLOPs by 30% to 50% during inference (Figure 4b), demonstrating significantly lower complexity than its transformer-based LMM counterparts. 4.4. Case Study In this section, we perform case study for Video-XL and VAMBA using different video inputs and show the results in Figure 5. The cookie example focuses on object property identification in long video (36 minutes). Video-XL incorrectly identifies the cookie colors as green and white, possibly due to the video also containing scenes for making other desserts (e.g. white ice cream in the frame highlighted in red). On the other hand, VAMBA more accurately describes them as green and brown. In the magic trick example, Video-XL misinterprets the rubber band as green ring, whereas VAMBA correctly describes the performer manipulating the rubber band around their fingers, making it appear to pass through. These comparisons highlight how VAMBA is better at retrieving relevant events in long videos and capturing fine-grained visual details and actions. 7 Figure 5. Qualitative comparison between VAMBA and efficient LMM baselines. Red text denotes incorrect responses, while green text highlights the correct responses by our VAMBA. 5. Related Work Large Multimodal Models (LMMs) Large Multimodal Models (LMMs) have rapidly evolved in recent years. Recent research has focused on improving LMMs instructionfollowing ability through better data curation [11, 33, 38, 42, 52, 75], building better vision encoders that can process images at higher resolutions [32, 42, 62], and extending LMMs to interleaved image [29, 32, 34] or video [40, 47, 73] understanding. To enhance instruction-following capabilities, LLaVA-1.5 [41] and LLaVA-NeXT [42] leverage GPT-4-generated multimodal data for knowledge-enhanced tuning. To optimize visual feature extraction, Idefics2 [32] leverages the NaViT [18] strategy and the SigLIP [71] vision encoder to support image inputs with native resolution. Qwen2-VL [62] applied Multimodal RoPE (M-RoPE) to optimize feature extraction for varying input sizes. Despite significant advances, current state-of-the-art LMMs remain inefficient at handling long-context multimodal inputs due to the quadratic complexity of causal selfattention operations. To mitigate this issue, methods like Flamingo [3], OpenFlamingo [4], Idefics [31] and LLaMA 3.2 [19] interleave gated cross-attention layers with selfattention layers for vision-text modelling. mPlug-Owl3 [68] develops the hyper-attention layer that integrates selfattention with cross-attention to support multimodal understanding. However, these models generally underperform compared to LLaVA-like transformer-based LMMs. In this study, we identify that one possible reason is the lack of vision token updates for cross-attention-based models. Our VAMBA addresses this limitation by incorporating additional Mamba layers, which effectively update vision tokens and enhance multimodal integration. Efficient LLMs/LMMs To improve the efficiency of LMM reasoning, recent work [35, 36, 38, 54, 55, 72, 74] develops techniques to reduce the length of the vision token sequence. Video-LLaMA [72] and VideoChat [36, 38] use Q-Former [35] to compress dense visual tokens into more compact sequence. LLaVA-Mini [74] further reduces each image to only one token. Video-XL [55] introduces Visual Summarization Token to summarize the visual information, while LongVU [54] designs spatiotemporal adaptive compression mechanism to reduce the number of video tokens. Another research avenue, exemplified by the Mamba [17, 24] model series, focuses on developing linear-time sequence models to reduce the complexity of autoregressive next-token prediction. Mamba-based [78] and MambaTransformer hybrid models [22, 39] have achieved notable success in the realm of pure language modelling. However, these efficient architectures have received little attention in the multimodal understanding domain. LongLLaVA [64] took an initial approach by incorporating the hybrid Jamba [39] model as the LMM decoder, yet its performance in long video understanding remains suboptimal. This shortfall is likely due to the absence of strong hybrid LLM as the base model and the lack of large-scale training on multimodal instruction-following data. 6. Conclusion We presented VAMBA, hybrid Mamba-Transformer model for efficient hour-long video understanding. By integrating cross-attention for text tokens and Mamba-2 blocks for video token updates, our approach reduces computational complexity and GPU memory usage while achieving competitive performance across long, medium, and short video benchmarks. Extensive evaluations on datasets such as LVBench demonstrate VAMBA superiority over existing efficient video LMMs. For future work, since our model design is orthogonal to token reduction-based methods, we will focus on combining the strengths of both approaches to develop more efficient VAMBA variants."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, YenChun Chen, Yi ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, and Xiren Zhou. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. 6 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 6 [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 8 [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 8 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. [6] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In ICLR, 2023. 2 [7] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. 1 [8] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2025. 6, 2 [9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 5 [10] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024. 1 [11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. 8, [12] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2025. 6, 1 [13] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. arXiv Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016. 7, 1 [14] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 5, 1 [15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 6 [16] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 7, 1 [17] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state 9 space duality. arXiv preprint arXiv:2405.21060, 2024. 2, 4, 8 [18] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [20] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 1, 6 [21] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5, 6 [22] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024. 8 [23] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 2 [24] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2, 4, 8 [25] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:14741487, 2020. 2 [26] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 2 [27] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Efficient multiWang, Tiejun Huang, and Bo Zhao. modal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. 1 [28] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 1 [29] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 1, [30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. 1 [31] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023. 8 [32] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 1, 8 [33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 6, 8 [34] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1, 8 [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 8 [36] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1, [37] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In European Conference on Computer Vision, pages 237255. Springer, 2024. 4 [38] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1, 5, 6, 8, 2 [39] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Itay Dalmedigos, Erez Safahi, Shaked Meirom, Osin, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. 8 [40] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 8 [41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 8, 1 [42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next. https: 10 // llavavl.github . io / blog /20240130llava-next/, 2024. Accessed: 2025-02-14. 1, 8 [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [44] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 6 [45] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model 2024. arXiv preprint arXiv:2401.10166, 2024. 4 [46] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 2 [47] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 8, 1 [48] OpenAI. Chatgpt. https://openai.com/index/ chatgpt/, 2023. [49] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. 5, 6 [50] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 6 [51] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 7, 1 [52] Weiming Ren, Huan Yang, Jie Min, Cong Wei, and Wenhu Chen. Vista: Enhancing long-duration and high-resolution video understanding by video spatiotemporal augmentation. arXiv preprint arXiv:2412.00927, 2024. 8 [53] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [54] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 1, 2, 5, 6, 8 [55] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 1, 5, 6, 8 Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions. arXiv preprint arXiv:2406.10328, 2024. 5 [57] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 1 [58] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 5, 6 [59] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: versatile metric for evaluating imarXiv preprint age and video captions using gpt-4o. arXiv:2412.13647, 2024. [60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [61] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 6, 2 [62] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 5, 6, 8 [63] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 1, 2, 5, 6 [64] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 2, 5, 6, 8, 1 [65] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [66] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2025. 2, 6 [67] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 6, 2 [56] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, [68] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug11 owl3: Towards long image-sequence understanding in multiIn The Thirteenth Internamodal large language models. tional Conference on Learning Representations, 2024. 8 [69] Howard Yen. Long-context language modeling with parallel context encoding. Masters thesis, Princeton University, 2024. 5 [70] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. [71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 8 [72] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 543553, Singapore, 2023. Association for Computational Linguistics. 1, 8 [73] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 6, 8 [74] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large mularXiv preprint timodal models with one vision token. arXiv:2501.03895, 2025. 5, 6, 8, 1 [75] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 5, 8 [76] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 6 [77] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. [78] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim Hacid. Falcon mamba: The first competitive attention-free 7b language model. arXiv preprint arXiv:2410.05355, 2024. 8 12 VAMBA: Understanding Hour-Long Videos with Hybrid Mamba-Transformers"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Implementation Details We use 8 NVIDIA A800 80G GPUs to train our models for both ablation study and full-scale training. For ablation studies, the learning rate is set to 1e-5 for pretraining and 1e-7 for instruction-tuning. We further conduct hyperparameter search and find that setting the learning rate to 5e-6 during instruction-tuning works the best for VAMBA across multiple benchmarks. We therefore set the learning rate to 1e-5 for pretraining and 5e-6 for instruction tuning for fullscale training. We employ cosine learning rate schedule for all training stages in both ablation studies and full-scale training. The training batch size is set to 128. We employ training optimization methods such as Flash-Attention 2 [16], DeepSpeed ZeRO-3 [51] and gradient checkpointing [13] to reduce the training cost, and apply sequence parallelism to pack multiple samples into one sequence during training in both stages. 8. Model Evaluation Details In this section, we provide more details for benchmarking VAMBA and our selected baseline models. 8.1. Baseline Models Qwen2-VL [62] is an LMM that uses the Qwen2 LLM as its backbone and DFN-derived Vision Transformer with 2D RoPE positional embedding. It is pretrained on vast 1.4Ttoken multimodal corpus composed of image-text pairs, OCR text (images of text), interleaved imagetext web articles, visual QA data, video dialogues, and image-based knowledge datasets. The pre-training is staged: 600B tokens for vision-language alignment followed by 800B tokens mixing richer imagetext content and VQA/multitask data, alongside continued pure text to maintain language skills. Finally, Qwen2-VL is instruction-tuned via ChatMLformat dialogs that span multiple modalities, e.g. document parsing, comparisons of two images, long video understanding, and even agent-oriented visual tasks. LLaVA-Mini [74] is compact multimodal model built on 78B Vicuna LLM with CLIP ViT-based vision encoder. It uses the same training data as LLaVA-1.5 [41]: about 558K imagecaption pairs for initial visionlanguage pre-training and 665K image-grounded instruction examples for fine-tuning. The pre-training stage aligns visual features to text using caption datasets like COCO [14] and VisualGenome-based [30] captions, while the instructiontuning stage uses multimodal dialogues. An enhanced variant of LLaVA-Mini further incorporates 100K video-based instruction samples from Video-ChatGPT [47] and other open sources, combined with the original 665K image instructions (total 3M training instances) to extend its capability to video understanding. LongLLaVA [64] extends LLaVA [41] to handle very long visual contexts by using hybrid TransformerMamba architecture with Jamba-9B backbone for language. It follows three-stage training process, including single-image feature alignment on Allava-Caption [10] and ShareGPT4V [11], single-image instruction finetuning on LLava-1.5 and Mantis-Single [29], and multiimage instruction fine-tuning on VideoChat2 [36] and ShareGPT4Video [12]. By progressively increasing the number of images per sample, LongLLaVA learns temporal and spatial dependencies and can efficiently handle input sequences up to around 1000 images. LongVU [54] is multimodal model geared toward long video understanding. It first learns from 3.2 million imagetext pairs via single-image training stage using the LLaVA-OneVision dataset [33]. It then leverages subset of VideoChat2-IT [36] that contains around 0.55M videos, 1K video-classification clips from Kinetics-710 [7], and about 85K multimodal video instruction dialogues from the ShareGPT4Video [12]. Additionally, the MovieChat longvideo dialogue data [57] is used to provide hour-length conversational examples. This rich training mix enables LongVU to handle extended videos by adaptively compressing frames while preserving essential visual details. Video-XL [55] employs an LLaMA-based 7B language model and CLIP ViT-L vision encoder, and it is trained entirely on image-based data despite targeting long videos. Its two-stage training first performs projection-layer pretraining on 2M imagetext pairs from Laion-2M [53] to align visual features with the text space. It then undergoes visual instruction tuning on roughly 695K image-grounded instruction samples from Bunny-695k [27], where the model learns to follow image-based instructions. The training approach lets Video-XL handle hour-long videos in context by compressing visual tokens, achieving strong results on benchmarks for long video comprehension. 8.2. Evaluation Benchmarks LVBench [63] is benchmark designed to test the ability of video LMMs to comprehend extremely long videos. It contains 1,549 question-answer pairs, with an average video length of 4,101 seconds. The evaluation focuses on six fundamental aspects: temporal grounding, which involves identifying specific moments in video; video summarization, which assesses the models ability to condense key information; video reasoning, which tests logical infer1 ence from video content; entity recognition, which identifies people, objects, or places; event understanding, which captures the sequence and significance of events; and key information retrieval, which ensures the model extracts crucial details. The full test set is used for evaluation. HourVideo [8] is benchmark dataset for long-form videolanguage understanding, focusing on videos up to one hour in length. It consists of 500 carefully selected firstperson videos sourced from the Ego4D [23] dataset, with each video ranging from 20 to 120 minutes in duration. The dataset includes 12,976 human-annotated multiplechoice questions covering four major task categories: summarization, perception, visual reasoning, and navigation. HourVideo is designed to challenge models in long-context reasoning and multimodal comprehension across extended video timelines. Benchmark results reveal that existing multimodal models, such as GPT-4 and LLaVA-NeXT, perform only marginally better than random chance, while human experts achieve an accuracy of 85.0%. This highlights the datasets difficulty and the current gap in long-video understanding capabilities. Video-MME [21] is benchmark specifically designed to evaluate how well LMMs can analyze video content. It features dataset of 900 videos and 2700 questions, covering six different visual domains. The questions are grouped based on video length into short, medium, and long categories, with median durations of 26 seconds, 164.7 seconds, and 890.7 seconds, respectively. The benchmark supports two evaluation methods: (1) the w/ subtitle setting, where both subtitles and questions are provided as text inputs, and (2) the w/o subtitle setting, which relies only on raw video inputs alongside the questions. Our study primarily focuses on the w/o subtitle format to enhance long video comprehension by leveraging video-based augmentation rather than textual cues from subtitles. MLVU [76] is benchmark designed to assess long video understanding across various tasks and video genres. It includes two types of questions: multiple-choice and freeform generation. The evaluation framework measures LMM performance in three key aspects: (1) holistic video understanding, which requires comprehending the entire video for global context; (2) single-detail video understanding, which focuses on recognizing key moments or short segments; and (3) multi-detail video understanding, which involves drawing connections between multiple short clips within the video. Our paper specifically reports accuracy scores for multiple-choice questions from the MLVU development set. LongVideoBench [66] is question-answering benchmark designed for interleaved long video-text input. It includes 3,763 videos and 6,678 human-annotated multiple-choice questions covering 17 fine-grained categories. The benchmark supports two evaluation formats: (1) the standard format, where video tokens are processed first, followed by the question descriptions, and (2) the interleaved video-text format, where subtitles are inserted between video frames. We evaluate all baseline models and our VAMBA using the standard input format. The reported results are based on the validation split. NExT-QA [67] is video question-answering benchmark designed to evaluate reasoning-based video understanding. It consists of 5,440 videos and approximately 52,000 human-annotated question-answer pairs, covering diverse range of real-world activities. The dataset includes two types of question formats: multiple-choice questions and open-ended free-form questions. NExT-QA emphasizes causal and temporal reasoning, requiring models to understand event sequences, cause-effect relationships, and interactions within videos. The dataset is divided into three temporal, and descriptive questions. categories: causal, The dataset is split into training (3,870 videos), validation (570 videos), and test (1,000 videos), ensuring standardized benchmarking. MVBench [38] is comprehensive multimodal video understanding benchmark. The dataset introduces novel static-to-dynamic task transformation, converting existing static image tasks into video-based challenges, assessing models ability to perform both low-level perception and high-level cognitive reasoning over time. MVBench automatically converts annotations from 11 publicly available video datasets into unified multiple-choice question-answer pairs, covering diverse scenarios ranging from first-person to third-person perspectives and indoor to outdoor environments. Each question presents five answer choices, ensuring standardized evaluation through human-verified ground truth responses. DREAM-1K [61] is video description dataset designed for fine-grained event and motion understanding. It contains 1,000 short videos, each averaging 9 seconds, and covers diverse set of real-world and cinematic scenarios. Unlike question-answering datasets, DREAM-1K requires models to generate detailed multi-sentence descriptions that capture all key actions, interactions, and temporal sequences within each video. The dataset includes videos from five different sourceslive-action movies, animated films, stock footage, long YouTube videos, and shortform social media clipsensuring broad coverage of visual styles. DREAM-1K prioritizes event-based reasoning, expecting models to understand sequential actions, motion cues, and interactions rather than just static descriptions. Evaluation is conducted using AutoDQ (Automatic Description Quality), which measures how well generated descriptions align with reference descriptions by comparing extracted events. 2 Figure 6. Additional qualitative results for VAMBA. 9. Additional Qualitative Results In this section, we showcase more qualitative results from our VAMBA for detailed video captioning and video event understanding. The results are shown in Figure 6."
        }
    ],
    "affiliations": [
        "1.AI",
        "M-A-P",
        "University of Toronto",
        "University of Waterloo",
        "Vector Institute"
    ]
}