{
    "paper_title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications",
    "authors": [
        "Aleksandr Algazinov",
        "Matt Laing",
        "Paul Laban"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE."
        },
        {
            "title": "Start",
            "content": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications Aleksandr Algazinov Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China algazinovalexandr@gmail.com Matt Laing Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China matthieu.laing@gmail.com Paul Laban Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China plaban.pro@gmail.com 5 2 0 2 J 4 2 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 2 0 5 9 1 . 6 0 5 2 : r AbstractAccessibility remains critical concern in todays society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, multimodal accessibility MAS, which performs the modality conversions based on the users needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to wide range of domains, industries, and areas, such as healthcare, and can become useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE. Index TermsMulti-Agent Systems (MAS), AI for Accessibility, Natural Language Processing (NLP), Modality Conversion, Assistive Technologies, Human-Computer Interaction (HCI), Cross-Modal Learning I. INTRODUCTION Multi-Agent Systems (MAS) are increasingly applied across diverse domains such as autonomous driving, healthcare, and finance [44]. For instance, in autonomous vehicles [4], MAS facilitate decentralized traffic coordination, improving the safety and efficiency of urban traffic light control and vehicular flow in real-time [52]. In healthcare, agents collaborate on patient monitoring, diagnostics, and resource management. Within smart grids [39], MAS manages demandresponse operations and optimizes energy distribution. In - Equal contribution logistics and search and rescue [35], Agents divide search, rescue, and coordination tasks, make quick decisions about route planning, resource allocation, or mission priorities, and react to new problems like blocked paths or accidents, enhancing operational resilience. [46]. The advantages of MAS include having no single point of control, being able to keep working even if one part fails, and easily adding more or adapting existing agents if and when needed, making them suitable for dynamic, real-world environments [46]. Each agent is capable of both autonomous decision-making and collaboration, resulting in more effective problem-solving. These features are important in systems that are required to adapt to uncertainty, like noisy data, changing surroundings, or missing information. Hence, MAS are crucial for enhancing complex systems such as autonomous transportation networks, smart grids, and financial trading systems [29]. People with disabilities often face challenges when using technology, especially when their specific needs are not being met. Disabilities can affect how people move, see, hear, or think, so they may require different ways to interact with digital tools. However, many everyday technologies, especially MAS, are not flexible enough to satisfy their needs, making basic tasks, communication, or daily life harder and more uncomfortable for them. Although MAS are effective in numerous applications, their use in accessibility is still limited. Most MAS implementations for accessibility are typically implemented for specific tasks, such as voice control tools or screen readers for blind users, and cannot be generalized over various accessibility use cases [46]. Moreover, most of the existing solutions are not open source and, therefore, not customizable and widely accessible [29]. These limitations highlight the necessity for general-purpose, open-source, and lightweight MAS architecture that allows for real-time, multi-modal adaptation to different peoples accessibility needs and requirements. To address these challenges, we introduce MATE, comprehensive multi-agent AI accessibility system that uses advanced AI tools and complex code structure to assist users in need. Our contributions are summarized as follows: We introduce MATE, comprehensive, flexible, and open-source MAS that understands and solves multiple modality conversion problems based on the users needs. To the best of our knowledge, this is the first opensource and lightweight MAS built specifically for modality adaptation tasks. The most closely related open-source solutions include general-purpose MAS [23] [16], and Multimodal Large Language Models (MLLMs) designed to effectively handle different modalities [1] [51] [61]. We created the ModConTT (Modality Conversion Task Type) dataset. The dataset was AI-generated and humanverified. Verifications confirmed the correctness, completeness, and diversity of the dataset, making the dataset suitable benchmark for further research within the AI for accessibility domain. We introduce ModCon-Task-Identifier, model designed for recognizing the modality conversion task type based on the user prompt. Furthermore, our experimental findings show that the model significantly outperforms other existing LLMs (e.g., Llama-3.1-70B-Instruct), as well as machine learning classifiers (e.g., CatBoost [32]) trained on the ModConTT dataset. The models were evaluated using standard classification metrics, such as accuracy and F1-score. II. RELATED WORK A. Multi-Agent Systems In recent years, MAS has proven to be powerful tool for solving problems for which single-agent AI solutions are inefficient [42]. Because each agent in the system can be specialized for single task, those systems are more relevant and have more adaptability when in complex environments. Numerous studies showed that the use of MAS was effective on wide variety of tasks [48] [50], including decisionmaking, reasoning, etc. Furthermore, the efficiency and saved time of dividing work across multiple agents are essential in applications such as numerical accessibility. Recent advances in multimodal learning [24] have made MAS more effective than single-agent systems in dynamic environments. However, agents require efficient communication as well as precise prompts in the context of LLMs. Studies [6] have shown that MAS lacking those elements are inferior to single-agent models in problem-solving skills. The same studies proposed multiple strategies to improve MAS, such as establishing standardized communication protocol and reinforcing clear role specifications by defining conversation patterns and setting termination conditions. Another drawback of multiagent system context is the scale needed for the whole solution, particularly when using LLMs. This issue is addressed by new frameworks aiming to improve small language models and their reasoning skills [26] [34] [56], thus reducing the necessary size needed in MAS. Recent studies integrate MAS with Large Language Models (LLMs), boosting their cognitive flexibility, reasoning, planning, and adaptability. Frameworks like AutoGen [50] and LangChain [7] demonstrate how LLMpowered agents can engage in collaborative multi-agent dialogues and tool use [34], simulating expert interaction. These integrations mark shift from rule-based coordination to adaptive, knowledge-driven agent collaboration [46]. These developments underscore MAS as fundamental architectures for scalable, intelligent, and multimodal AI ecosystems that support emergent intelligence in human-AI collaboration [44]. B. AI for Accessibility"
        },
        {
            "title": "Numerous AI tools were built",
            "content": "to improve accessibility for users with disabilities (e.g., real-time captioning, content description, etc.). The primary objective is to ensure that people with disabilities can exploit AI capabilities to enhance their learning or social interactions. AI is becoming more and more important in enhancing cognitive support, mobility, and medical diagnostics, allowing for user-specific and flexible solutions [2] [15]. Thus, many tools are developed to fit different types of conditions, such as natural language processing (NLP) for speech recognition and translation, open models for text-to-speech, or even sign language transcription to support interaction innovation [20] [19] [13] [8]. In addition, smart homes and tools are produced to help users (mostly elderly people), assisting them daily. In this paper, we will compare our work to research based on multi-agent interaction [30] that reaches multiple limitations since it is only focused on visually impaired disabilities with single camera that captures information to be treated by the multi-agent. Thus, our research targets larger audience with diversified disabilities that can be treated and handled by MAS. C. Modality Conversion Modality conversion is defined as mapping data from one form to another. This is an important problem in the AI for accessibility domain, since the goal is to represent the particular data in way that is convenient for user. Some of the modality conversion tasks can be summarized as follows: Text-to-Sign Language (TTSL). As of 2021, sign language (SL) was used by 466 million people as the main source of communication [11]. Techniques for solving the problem include Text-to-Gloss-to-Pose-to-Video [43] (a combination of Text-to-Gloss, Gloss-to-Pose, and Pose-to-Video models) and gloss-free approaches [33]. While numerous models for different sign languages exist [45] [28] [58], many of the modern and promising approaches, such as SignDiff [12], are not open-source. Text-to-Speech (TTS). TTS is well-explored research area. Hence, multiple open-source models are available to be used, such as Coqui TTS (XTTS-v2) [5], Parler-TTS [21] [25], and MeloTTS [60]. Speech-to-Text (STT, also known as ASR). This problem can be used both for offline mapping of speech to text and real-time captioning. Similar to TTS, the problem is well explored. Hence, multiple models are available to be used, such as Whisper [37] and wav2vec 2.0 [3]. Image-to-Audio (ITA). This problem can be decomposed into two subtasks: Image-to-Text (ITT) and TTS. The classical models for image captioning are CLIP [36] and BLIP [22]. Fig. 1: MATE Workflow TABLE I: Agent Overview Agent Name Interpreter Agent TTS Expert TTI Expert Functionality Given prompt, the agent determines the type of modality conversion desired by the user Produces an audio transcription of the text Generates an image based on the text input STT Expert Creates text transcription of the audio file ITT Expert ATI Expert ITA Expert VTT Expert Generates text caption of the image Combines STT and TTI agents to generate an image based on the audio Combines ITT and TTS agents to produce an audio description of the image Produces the text transcription of videos audio component Audio-to-Video (ATV). The ATV problem can be decomposed into two subtasks: SST and Text-to-Video (TTV). Converting text to video is currently an active research area, and we plan to try the following (but we are not limited to them) models: CogVideoX [55], ModelScopeTTV [47]. Video-to-Audio (VTA). This problem can be decomposed into two subtasks: Video-to-Text (VTT) and TTS. VTT is the inverse of the TTV problem, yet it is not as popular. Hence, less research is done in this area. Nevertheless, several opensource models can be used, such as cogvlm2-llama3-caption [55], and Video-LLaMA [59]. Audio-to-Image (ATI). In this scenario, audio is converted Input StdIn Output Category Default Model ModCon-Task-Identifier .txt, .pdf, .docx, StdIn .txt, .pdf, .docx, StdIn .mpeg, .wav, .mpeg, .wav, .m4a, .mp4, .mp3, .mpga, .webm .png, .jpeg, .jpg .mp3, .mp4, .mpga, .webm .png, .jpeg, .jpg .m4a, .mp4, .webm .wav .png .txt .txt .png .wav .txt Tacotron 2 [41] Stable Diffusion V1-4 [38] Whisper [37] BLIP [22] Whisper + BLIP BLIP + Tacotron 2 Whisper to text, and then an image is created from the textual description. One of the effective ways to solve the Text-toImage (TTI) problem is to apply stable diffusion [38] models. While the separate conversion problems (for example, TTS) do not require the involvement of agents, more from the use of Collective complex system can benefit AI [9]. Firstly, that can solve the described modality conversion problems. Hence, all agents with different roles and registered tools are useful for this purpose. Secondly, user might not explicitly provide In this case, multi-agent the expected output there is no unified model format. intelligent system will be capable of identifying the optimal output format based on the collective reasoning. Meanwhile, system not involving agents will face difficulties with this task. D. Multi-Agent and Multimodal Model Evaluation systems Assessing the quality of communicative multi-agent systems research area, particularly for has become an important developing agents capable of understanding and exchanging multimodal signals. COMMA [31], for example, provides benchmark that evaluates the communicative effectiveness through coordinated of multimodal multi-agent visual and textual tasks. In contrast, BattleAgentBench [49] introduces mixed cooperative-adversarial environment, allowing for the evaluation of both collaboration and competition among LLMs in multi-agent scenarios. For cooperative agents, CH-MARL [40] introduces benchmark involving agents with heterogeneous sensors and action spaces, designed to emulate the diversity and complexity enables of the evaluation of agents coordination, adaptability, and communication across varied modalities and capabilities. Similarly, VS-Bench [54] focuses on vision-language models testing their strategic reasoning and long-term (VLMs), planning skills within shared tasks, thereby pushing the limits of what VLMs can achieve in collaborative agent settings. real-world multi-agent environments. This Recent evaluation frameworks have emphasized core components of agent rationality, including coherence in decisionmaking, concise communication, and utility-optimized interaction strategies [18]. broader review of multimodal language model evaluation identifies persistent challenges such as aligning information across modalities, generalizing to novel tasks, and maintaining response consistency in dynamic interaction contexts [17]. LLaVA-Critic [53] introduces modelbased evaluation framework that incorporates feedback loops to assess multimodal output quality in an interactive and supervised manner. Collectively, these emerging benchmarks and evaluation strategies signal shift toward more nuanced, standardized, and context-sensitive assessments reflecting the increasing complexity and interactivity of modern multimodal multi-agent systems. III. METHODOLOGY Fig. 1 shows the proposed MATE architecture. user sends request to the interpreter agent, describing the problem that needs to be solved (e.g., convert text to audio, or generate an audio description of the image). Based on the prompt, the agent identifies the type of task and redirects this task to the appropriate agents. These agents are capable of solving specific modality conversion tasks, such as VTT, SST, etc. The output is file of desired format, and the path to that file is returned to the user. A. Agents and User Interface Table provides summary of the agents used in the MATE MAS. In total, eight agents were created using Microsofts Autogen [50] framework. The main agent (interpreter agent) is an LLM agent responsible for communicating with the user. An LLM used for the agent initialization can be both stored locally and called through the API. Upon receiving prompt, it identifies the exact task that needs to be executed. Following this, the interpreter agent assigns the task to one of the seven experts for further execution. These experts use pre-defined models and functions to perform the modality adaptation tasks. Overall, agents are capable of solving the following modality conversion problems: TTS, TTI, ATI, STT, ITT, ITA, and VTT. In addition to agents functionality description, Table shows their input and output format and the default models the agents are built on. During initialization, the framework identifies suitable folder for saving output files. This is achieved by searching for directories with specific names, such as agents output, data, etc. In case of the absence of such folders, MATE creates new folder and returns its path to the user. After initialization, the system is capable of processing user requests. If the prompt is identified as irrelevant (UNK category), the system asks the user to re-enter the query so that the concrete modality conversion task can be specified. After the requirement is recognized, the user is asked to provide path to the file that needs to be processed. In the cases of unknown, unexpected, or unrecognized file types, the user is asked to modify the file path accordingly. Next, the relevant agent completes the task and saves new file at the pre-defined output directory. Finally, the path to this file is returned to the user, and the system is ready for further assistance. B. ModConTT Dataset Creation Given the absence of existing datasets appropriate for training and evaluating models in modality conversion task type recognition, we constructed diverse dataset (ModConTT) by leveraging LLMs to generate prompts aligned with various task types. These tasks include TTS, STT, ITT, ITA, VTT, TTI, ATI, TTV, ATV, and unknown (UNK). Prompts labeled with UNK refer to confusing, unrelated, or irrelevant requests from which the specific prompt type cannot be extracted. The dataset was made in two versions. The first version was created to identify the most accurate of the selected LLMs. These LLMs are available through API calls and are used for interpreter agent initialization. This version consists of 230 samples (20 for each category, except for the UNK class, which has 50 statements). The second version was used to train model-based task type classifiers from scratch. In total, there are 150 observations for the UNK category and 50 unique examples for each of the remaining task types, resulting in 600 prompts overall. Prompts in these two versions do not overlap. In ModConTT, each example includes user prompt and the correct task type associated with this prompt. Existing large-scale LLMs were used to generate the prompts consistently and diversely so that the data is comprehensive for both training and evaluation. The dataset was designed to be applicable for various modality conversion use cases. Fig. 2: Comparison of Model Performance and Failure Distribution Across Task Types TABLE II: Comparison of LLM-Based Interpreter Agents LLM Name GLM-4-Flash [14] Llama-3.1-70B-Instruct [27] GPT-3.5-Turbo [57] Accuracy 0.774 0.835 0.865 Precision 0.797 0.846 0.880 Recall 0.774 0.835 0.865 F1-Score 0.774 0.833 0. Failure Rate 4/230 9/230 1/230 Hence, the set of labels is larger than the number of modality adaptation tasks that the MATE system is capable of solving. For instance, MATE does not solve TTV tasks yet; however, TTV examples are present in ModConTT. C. Model Training and Evaluation In addition to using existing LLMs through API calls, five different classifiers (logistic regression, random forest, SVM, KNN, and CatBoost) were trained on the second version of ModConTT to identify the most suitable model. Prompt embeddings were created using the TF-IDF method and the BERT [10] model. Each of the classifiers was trained on both types of embeddings, resulting in 10 models overall. The data set was divided into train and test (90% and 10%, respectively) for all classifiers except CatBoost, where an additional 10% of the train set observations were used as validation. All classifiers were trained with default hyperparameters, ensuring fair comparison. Besides training classical machine learning models, we fine-tuned BERT model on our train set. The number of early stopping rounds was set to be 5, and the total number of fine-tuning epochs was 6. The finetuned model (ModCon-Task-Identifier) was publicly released on Hugging Face for reproducibility and further research, and all the project repository. Models were evaluated using standard classification metrics (accuracy, precision, recall, and F1-score). Precision, recall, and F1-score were averaged (using weighted average strategy) across different classes to get unique score for each model. In addition, the failure rate was calculated for LLMpowered interpreter agents, since they are not guaranteed to the trained classifiers are available at output valid label. The failure rate is defined as the number of prompts where an agent failed to output valid label from the pre-defined set, divided by the total number of prompts processed. IV. RESULTS AND ANALYSIS A. Evaluation of Interpreter Agents The performance of several existing LLMs (GLM-4-Flash [14], Llama-3.1-70B-Instruct [27], and GPT-3.5-Turbo [57]) was tested. GLM-4-Flash provides free API access, making it widely accessible for zero or low-budget experimentation. Llama3.1-70B, as an open-source model, is relatively cheap for API calls while delivering strong performance, providing an adequate balance between resource consumption and output quality. The GPT models are widely regarded as high standard in LLM quality, with their robust instructionfollowing and reliability for general NLP tasks. Hence, GPT-3.5-Turbo was selected as the third model. Importantly, the most advanced closed-source models, such as GPT-4 or Llama3.1-405B, were avoided because their API usage is expensive, and the marginal performance gains might not justify the extra costs. Thus, our focus was on models that offer strong overall performance while remaining financially sustainable. Three separate interpreter agents were initialized under the configurations of these models, and their performance was evaluated based on the first version of the ModConTT dataset. Table II shows the comparison of the performance of these three models on the first version of the ModConTT dataset. GPT-3.5-Turbo outperformed both GLM-4-Flash and Llama3.1-70B on all the metrics, achieving an accuracy of 0.865, precision of 0.880, recall of 0.865, F1-score of 0.865, and failure rate of 1/230 ( 0.4%). Hence, the GPT-3.5Turbo model is recommended for initializing the external LLM-powered interpreter agents. Fig. 2 (d) displays the distribution of prediction failures by task type. The highest failure rate was observed for the UNK class (32%), since this category is the most common in the dataset. The second highest failure rates were detected for STT, ATV, and VTT categories (16% for each one). Tasks involved in converting text and audio files to images were the simplest to recognize, accounting for 20% of the overall failures. B. Evaluation of Task Classification Models (ModCon-Task-Identifier) agent attained the best overall classification results (accuracy 0.917 and F1-score 0.916), tested methods on all significantly outperforming other the metrics. This performance gap highlights the capacity of LLM-based agents to effectively capture complex task semantics. Hence, the use of MAS is justified for the modality conversion problem, as MAS architectures proved to be effective against ML models. the results demonstrate that Fig. 2 shows confusion matrices for the three bestperforming models: the ModCon-Task-Identifier agent (subfigure (a)), the logistic regression trained on BERT embeddings (subfigure (c)), and the GPT-3.5-Turbo agent (subfigure (b)). The ModCon-Task-Identifier agent is the most robust, consistently achieving an accuracy of 80% or higher across task types (with the only exception for VTT, where the accuracy is 60%). The overall performance of the GPT-3.5-Turbo model, conversely, is damaged by substantial errors in several classes (such as TTI and STT), making it less reliable. The logistic regression model displays more diverse error patterns across the classes; however, its overall performance remains relatively strong, establishing high-quality baseline that can be achieved with simpler machine learning architecture. V. LIMITATIONS AND FUTURE WORK"
        },
        {
            "title": "While MATE represents an advancement in the AI for the",
            "content": "accessibility domain, our research faces several limitations. External General Purpose Models. While the choices of underlying models are flexible, our system relies on external tools for modality conversions. Hence, the MATEs performance can be compromised due to the model errors. In addition, most of the modality conversion models are trained for general-purpose applications and are not specifically designed for the accessibility domain, which could potentially lead to additional issues. Lack of Video Generation Capabilities. The current version of the framework does not support video generation models. Computational complexity and resource requirements of these models make them unsuitable for our real-time, efficient modality translation paradigm. Hence, although MATE offers extensive assistance for users in need, currently it is not comprehensive solution for every possible scenario in the modality adaptation domain. For instance, TTSL and TTV cases are not handled by MATE. Table III summarizes the performance of variety of task classification models on the second version of the ModConTT dataset. Within the TF-IDF embeddings, random forest outperformed other ML-based classifiers, achieving an accuracy of 0.650 and an F1-score of 0.629. When using BERT representations, logistic regression achieved the best quality among non-LLM methods with an accuracy of 0.783 and an F1-score of 0.779. However, two of the top-3 performing systems are MAS-based interpreter agents. The GPT-3.5-Turbo agent achieved an accuracy of 0.750 and an F1-score of 0.720, while the fine-tuned BERT model In addition to limitations, several research directions are suggested for further enhancing MATEs capabilities. Industrial Applications. MATE can be directly integrated with digital hospital assistants to help improve patient care. MAS can support patients who have problems understanding medical information. For instance, using real-time TTS, MATE can turn medical document into an audio representation, supporting people with vision impairment. In addition, MATE can be applied to the academic industry, helping disabled students get high-quality education. Other possible application areas include transportation, retail, and entertainment. TF-IDF BERT TABLE III: Comparison of Task Classification Models Model Name Logistic Regression Random Forest SVM KNN CatBoost Logistic Regression Random Forest SVM KNN CatBoost Interpreter Agent (GPT-3.5-Turbo) Interpreter Agent (ModCon-Task-Identifier) Accuracy 0.617 0.650 0.583 0.517 0.617 0.783 0.600 0.600 0.583 0.667 0.750 0. Precision 0.640 0.659 0.605 0.524 0.600 0.809 0.623 0.662 0.626 0.664 0.756 0.924 Recall 0.617 0.650 0.583 0.517 0.617 0.783 0.600 0.600 0.583 0.667 0.750 0.917 F1-Score 0.611 0.629 0.580 0.514 0.582 0.779 0.584 0.588 0.584 0.646 0.720 0.916 Optimized TTV Models Integration. As more efficient and lightweight TTV models are explored in the research community, the adaptability and scalability of MATE can be significantly enhanced, making it viable solution for widespread use in resource-limited environments and across wide range of industries. Hence, the future research can focus on optimizing MATE for domains where representing text and audio files as video is crucial. For instance, person with hearing difficulties can use ATV to follow cooking instructions through slow, visual demonstrations. Another example could be an individual using ATV to understand public transport directions as animated travel guides. VI. CONCLUSION AI for accessibility remains promising research direction, as existing technologies are usually designed for generalpurpose use and fail to fully address the specific needs and preferences of users with disabilities. Even though MAS have proven to be powerful and effective in solving complex problems, their usage in the accessibility domain is limited. In this paper, we introduced MATE, the first open-source multi-agent framework created to offer comprehensive assistance to individuals with disabilities by performing modality conversions based on their needs. The core function of the framework is to convert data into format that is accessible and understandable to the user. Additionally, using our custom dataset, we developed ModCon-Task-Identifier, fine-tuned BERT model for modality task type recognition. Experiments show that the model achieves state-of-the-art performance on the ModConTT dataset, outperforming both existing LLMs and machine learning models trained from scratch. MATEs simplicity, flexibility, and light weight enable local system usage, ensuring privacy and convenience. With many potential applications, including healthcare, education, and transportation, MATE is expected to be useful digital assistant to wide range of users."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, and Zhengyu He. Ming-omni: unified multimodal model for perception and generation, 2025. [2] Vignesh Amarnath and Research Pub. Ai-driven multimodal cognitive support: Advancing digital accessibility through adaptive technoloINTERNATIONAL JOURNAL OF COMPUTER ENGINEERING gies. & TECHNOLOGY, 16:23802393, 02 2025. [3] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations, 2020. [4] Christoph Bartneck, Christoph Lutge, Alan Wagner, and Sean Welsh. Autonomous Vehicles, pages 8392. Springer International Publishing, Cham, 2021. [5] Edresson Casanova, Kelly Davis, Eren Golge, Gorkem Goknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi, and Julian Weber. Xtts: massively multilingual zero-shot text-to-speech model, 2024. [6] Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Why do multi-agent llm systems fail?, 2025. [7] Harrison Chase. LangChain, October 2022. [8] Khansa Chemnad and Achraf Othman. Digital accessibility in the era of artificial intelligencebibliometric analysis and systematic review. Frontiers in Artificial Intelligence, 7:1349668, 2024. [9] Hao Cui and Taha Yasseri. Ai-enhanced collective intelligence. Patterns, 5(11):101074, November 2024. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [11] Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Giro Nieto. How2sign: large-scale multimodal dataset for continuous american sign language, 2021. [12] Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, Yapeng Tian, and Chen Chen. Signdiff: Diffusion model for american sign language production, 2025. [13] Daniele Giansanti and Antonia Pirrera. Integrating ai and assistive technologies in healthcare: Insights from narrative review of reviews. Healthcare, 13(5), 2025. [14] Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm4 all tools, 2024. [15] Yu Hao, Fan Yang, Hao Huang, Shuaihang Yuan, Sundeep Rangan, JohnRoss Rizzo, Yao Wang, and Yi Fang. multi-modal foundation model to assist people with blindness and low vision in environmental interaction, 2024. [16] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent assistance in realworld task automation. arXiv preprint arXiv:2505.23885, 2025. [17] Jiaxing Huang and Jingyi Zhang. survey on evaluation of multimodal large language models, 2024. [18] Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Yuan Yuan, Zhuoqun Hao, Xinyi Bai, Weijie J. Su, Camillo J. Taylor, and Tanwi Mallick. Towards rationality in language and multimodal agents: survey, 2025. [19] Antonia Karamolegkou, Malvina Nikandrou, Georgios Pantazopoulos, Danae Sanchez Villegas, Phillip Rust, Ruchira Dhar, Daniel Hershcovich, and Anders SÃ¸gaard. Evaluating multimodal language models as visual assistants for visually impaired users, 2025. [20] Laxminarayana Korada, Vijay Sikha, and Dayakar Siramgari. Ai & accessibility: conceptual framework for inclusive technology. 12, 08 2024. [21] Yoach Lacombe, Vaibhav Srivastav, and Sanchit Gandhi. Parler-tts. https://github.com/huggingface/parler-tts, 2024. [22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. [23] Xinbin Liang, Jinyu Xiang, Zhaoyang Yu, Jiayi Zhang, Sirui Hong, Sheng Fan, and Xiao Tang. Openmanus: An open-source framework for building general ai agents, 2025. [24] Rui Liu, Yu Shen, Peng Gao, Pratap Tokekar, and Ming Lin. Caml: Collaborative auxiliary modality learning for multi-agent systems, 2025. [25] Dan Lyth and Simon King. Natural language guidance of high-fidelity text-to-speech with synthetic annotations, 2024. [26] Graziano A. Manduzio, Federico A. Galatolo, Mario G. C. A. Cimino, Enzo Pasquale Scilingo, and Lorenzo Cominelli. Improving small-scale large language models function calling for reasoning tasks, 2024. [27] Meta AI. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/blog/meta-llama-3-1/, July 2024. [28] Amit Moryossef, Mathias Muller, Anne Gohring, Zifan Jiang, Yoav Goldberg, and Sarah Ebling. An open-source gloss-based baseline for spoken to signed language translation, 2023. [29] Nasim Nezamoddini and Amirhosein Gholami. survey of adaptive multi-agent networks and their applications in smart cities. Smart Cities, 5(1):318347, 2022. [30] Juliana Damasio Oliveira, Debora C. Engelmann, Davi Kniest, Renata Vieira, and Rafael H. Bordini. Multi-agent interaction to assist visuallyInternational Journal of Environmental impaired and elderly people. Research and Public Health, 19(15):8945, 2022. [31] Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu. Comma: communicative multimodal multiagent benchmark, 2025. [32] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: Unbiased boosting with categorical features. In Advances in Neural Information Processing Systems 31, pages 66396649. Curran Associates, Inc., 2018. [33] Fan Qi, Yu Duan, Huaiwen Zhang, and Changsheng Xu. Signgen: Endto-end sign language video generation with&nbsp;latent diffusion. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LIII, page 252270, Berlin, Heidelberg, 2024. Springer-Verlag. [34] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. [35] Jorge Pena Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan Sarker, Tuan Nguyen Gia, Hannu Tenhunen, Moncef Gabbouj, Jenni Raitoharju, and Tomi Westerlund. Collaborative multi-robot search and IEEE rescue: Planning, coordination, perception, and active vision. Access, 8:191617191643, 2020. [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via largescale weak supervision, 2022. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models . In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, Los Alamitos, CA, USA, June 2022. IEEE Computer Society. [39] Vikas K. Saini, Rajesh Kumar, Sujil A., Ramesh C. Bansal, Chaouki Ghenai, Maamar Bettayeb, Vladimir Terzija, Elena Gryazina, and Petr Vorobev. Multi agent framework for consumer demand response in electricity market: Applications and recent advancement. Sustainable Energy, Grids and Networks, 40:101550, 2024. [40] Vasu Sharma, Prasoon Goyal, Kaixiang Lin, Govind Thattai, Qiaozi Gao, and Gaurav S. Sukhatme. Ch-marl: multimodal benchmark for cooperative, heterogeneous multi-agent reinforcement learning, 2022. [41] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions, 2018. [42] Karthik Sreedhar and Lydia Chilton. Simulating strategic reasoning: Comparing the ability of single llms and multi-agent systems to replicate human behavior, 2024. [43] Stephanie Stoll, Necati Camgoz, Simon Hadfield, and Richard Bowden. Sign language production using neural machine translation and generative adversarial networks. 08 2018. [44] Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, and Yang Shen. Multi-agent coordination across diverse applications: survey, 2025. [45] Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei, Feng Li, and Richang Hong. Sign-idd: Iconicity disentangled diffusion for sign language production, 2024. [46] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: survey of llms, 2025. [47] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. [48] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. [49] Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, and Jie Tang. Battleagentbench: benchmark for evaluating cooperation and competition capabilities of language models in multi-agent systems, 2024. [50] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. [51] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Nextgpt: any-to-any multimodal llm. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [52] Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu. Multi-agent deep reinforcement learning for urban traffic light control in vehicular networks. IEEE Transactions on Vehicular Technology, 69(8):82438256, 2020. [53] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models, 2025. [54] Zelai Xu, Zhexuan Xu, Xiangmin Yi, Huining Yuan, Xinlei Chen, Yi Wu, Chao Yu, and Yu Wang. Vs-bench: Evaluating vlms for strategic reasoning and decision-making in multi-agent environments, 2025. [55] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2025. [56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. [57] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. comprehensive capability analysis of gpt-3 and gpt-3.5 series models, 2023. [58] Biao Zhang, Mathias Muller, and Rico Sennrich. Sltunet: simple unified model for sign language translation, 2023. [59] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instructiontuned audio-visual language model for video understanding, 2023. [60] Wenliang Zhao, Xumin Yu, and Zengyi Qin. Melotts: High-quality multi-lingual multi-accent text-to-speech, 2023. [61] Bin Zhu, Munan Ning, Peng Jin, Bin Lin, Jinfa Huang, Qi Song, Junwu Zhang, Zhenyu Tang, Mingjun Pan, Xing Zhou, and Li Yuan. Llmbind: unified modality-task integration framework, 2024."
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China",
        "Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China"
    ]
}