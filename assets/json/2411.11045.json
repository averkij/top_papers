{
    "paper_title": "StableV2V: Stablizing Shape Consistency in Video-to-Video Editing",
    "authors": [
        "Chang Liu",
        "Rui Li",
        "Kaidong Zhang",
        "Yunwei Lan",
        "Dong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present a shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate a testing benchmark, namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual consistency, and inference efficiency of our method compared to existing state-of-the-art studies."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 5 4 0 1 1 . 1 1 4 2 : r STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing Chang Liu1, Rui Li1, Kaidong Zhang1, Yunwei Lan1, Dong Liu1* 1University of Science and Technology of China {lc980413, liruid, richu, ywlan}@mail.ustc.edu.cn, dongeliu@ustc.edu.cn Project page: https://alonzoleeeooo.github.io/StableV2V Figure 1. Qualitative comparison (left) and results on different editing tasks by STABLEV2V (right). Herein, we highlight the words that depict the main edited contents and the modalities of external prompts in red and blue, respectively, and present the visualizations of several prompts (i.e., reference image and hand-drawn sketch) at the right-bottom corner of the corresponding first edited frames. Notably, AnyV2V [20] uses the same first edited frames as ours, where both results are highlighted in green and red bounding boxes, respectively."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate testing benchmark, namely DAVIS-Edit, for comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual 1 consistency, and inference efficiency of our method compared to existing state-of-the-art studies.1 1. Introduction Video editing aims to modify the source video contents according to user demands. With the prosper of diffusion models [17, 33] that demonstrates superior generative capabilities, recent studies have adopted this astonishing technique for video editing, making it possible for end users to interact with various types of external prompts, e.g., text [28, 50], instruction [40, 48], image [10, 30], sketches [24], and etc. They achieve significant success on this topic, bringing video editing to prominent attractive research direction for the community of visual content generation. To perform video editing, recent studies manage to transfer the motion patterns from the original video and adapt them to the editing process. In doing so, prevailing methods can be categorized into four main types, i.e., DDIM inversion-, one-shot tuning-, learning-, and firstframe-based methods. Specifically, DDIM inversion-based methods [28, 47] leverage DDIM inversion to store the motion patterns of videos in forms of latent features, which are then injected into the diffusion models when editing, thus enforcing the consistency between edited frames and the original ones. One-shot tuning-based solutions [25, 42] aim to tailor the motion patterns of each video through learning video-specific model weights. These two types of methods, however, often produce results that are inconsistent to the shapes that user prompts require, especially the ones with significant shape differences, e.g., the cases illustrated in Fig. 1. Learning-based methods [30, 50, 52] provide more general solution for video editing by finetuning temporal-enhanced diffusion models on large-scale video-text datasets [8, 27], but these studies are highly restricted due to their inpainting paradigms. They normally require mask annotations to precisely localize the edited regions, thus becoming tough for users to interact with. Also, the inpainting paradigms limit them to regional editing scenarios, where the applications of global ones (e.g., video style transfer [21]) are neglected. First-frame-based methods [20, 29] offer more flexible solution for video editing, where this paradigm decomposes video editing into image editing and motion transfer, enabling the potentials to perform both global and local editing with the same solution. Nevertheless, they suffer from similar limitations to the aforementioned studies due to their requirements of DDIM inversion [20] and video-specific tuning [29]. Recently, DMT [47], which proposes space-time feature loss to constrain the motion consistency, serves as the most rel1We open-source our codebase at https : / / github . com / AlonzoLeeeooo/StableV2V, and release the model weights and testing benchmark DAVIS-EDIT at https://huggingface.co/ AlonzoLeeeooo/StableV2V and https://huggingface.co/ datasets/AlonzoLeeeooo/DAVIS-Edit, respectively. evant study to address such misalignment, but even so, inferior condition-following ability and detail loss of backgrounds are often observed in its results like the ones in Fig. 1, where effective paradigm is thus expected to ensure the consistency between delivered motions and user prompts. Therefore in this paper, we propose STABLEV2V to perform video editing in shape-consistent manner, with our method built based on the first-frame-based paradigm. In doing so, our method performs video editing with three main components, i.e., Prompted First-frame Editor (PFE), Iterative Shape Aligner (ISA), and Conditional Image-tovideo Generator (CIG). PFE serves as the first-frame image editor that converts external prompts into edited contents, which are then propagated to other frames in later processes to construct the entire edited video. To offer precise guidance that are well aligned with shapes required by user prompts, especially in scenarios that comprise complicated shape differences, we assume that the edited contents share the same motions with the ones of source video. Based on the assumption, we propose ISA, which manages to iteratively propagate the average motions, shapes, and depths from core elements (e.g., main objects) of each original video frame to the edited one, resulting in the simulated optical flow and depth map of all edited frames, along with shape-guided depth refinement network to further calibrate the obtained depth map and ensure its preciseness. Eventually, we leverage the depth map as an intermediate vehicle to deliver precise motions from the source video, and utilize it to guide the image-to-video generation process of CIG, obtaining the final edited video. Furthermore, we collect testing benchmark based on DAVIS [31], namely DAVISEDIT, to conduct comprehensive evaluation for textand image-based video editing. Experimental results compared to existing state-of-the-art studies demonstrate that STABLEV2V outperforms others from various perspectives, including visual quality, consistency, and inference efficiency. 2. Related Works Video Synthesis. Modeling the high-dimensional distribution of video data is challenging task for video generation. Early-proposed methods [37] mainly address this problem via Generative Adversarial Network (GAN), but suffering from inferior visual quality and training instability. Recent advancements of diffusion models [17, 33] have greatly promoted the development of various visual generation tasks, e.g., text-to-image and conditional generation [7, 13, 43], where this effective paradigm is also adopted for video generation [39, 44]. Particularly, existing studies leverage various model architectures upon the video modeling task, including U-net [12] and Diffusion Transformer [44]. These studies demonstrate outstanding generative abilities in producing photo-realistic videos with text prompts, and serve as strong foundation models for wide 2 Figure 2. Illustration of the overall pipeline of STABLEV2V, with three main components, i.e., Prompted First-frame Editor (PFE), Iterative Shape Aligner (ISA), and Conditional Image-to-video Generator (CIG), whose backgrounds are highlighted in red, yellow, and gray, respectively. Herein, the green bounding boxes refer to the first video frames; the blue bounding boxes represent the k-th optical flow, segmentation mask, and depth map in ISA. For simplicity, we only showcase the k-th to + 1-th iteration process of ISA in this figure. range of down-stream applications, e.g., text-to-video generation [46], image-to-video generation [2, 11, 15, 34] as well as video editing [3, 9, 20, 25, 28, 42, 47, 52]. Video Editing. Recently, the research direction of video editing has attracted great attention. In performing this task, conventional works normally introduce external conditions to assist video editing, e.g., optical flow [9], Neural Layered Atlas (NLA) [4, 22], and etc., where limitations are usually observed due to the inherent problems of the used techniques. With the prosper of diffusion models, such task is significantly facilitated by their strong generative abilities, where we summarize existing methods into four categories, i.e., DDIM inversion-, one-shot tuning-, learning-, and firstframe-based methods. Specifically, DDIM inversion-based methods offer way to represent the motion patterns of videos through inverted latent features, where these features are then utilized to enforce the temporal consistency in the generated video frames [28]. One-shot tuning-based methods [25, 42] mainly learn video-specific model weights to model the motion patterns, where diversified results can be then generated through adjusting the text prompts. Learning-based methods [30, 50, 52] solve the task via training particular networks on large datasets, where they integrate motion modules into pre-trained image diffusion models [5, 33], and optimize the enhanced model architectures with video-text data, enabling these networks to edit video contents in local regions. First-frame-based methods [20, 29] start with editing the first video frame, and propagate the results to all other frames through transferring the motions from the source video. Nevertheless, these studies obtain inferior performance since their delivered motions are inconsistent with user prompts. AnyV2V [20] and DMT [47] are the most relevant studies to our method. However, the former struggles to handle challenging scenarios with significant shape differences, and the latter presents inferior capability of background preservation, where all issues above motivate STABLEV2V in this paper. 3. Methods STABLEV2V comprises three main components to perform video editing, i.e., Prompted First-frame Editor (PFE), Iterative Shape Aligner (ISA), and Conditional Image-to-video Generator (CIG), where the overall pipeline is shown in Fig. 2. Given an input video = {X1, . . . , XN } with video frames in total, PFE edits the first video frame X1 into (cid:98)X1 according to an external prompt P. Then, ISA extracts the depth maps D, optical flows F, and segmentation masks from , and simulates the depth maps (cid:98)Dr of edited video based on D, F, M, and (cid:99)M1 of (cid:98)X1. Eventually, CIG serves as depth-guided image-to-video generator, and leverages (cid:98)Dr and (cid:98)X1 to produce the entire edited video (cid:98)X , where the overall process of STABLEV2V is formulated by: (cid:98)X = fCIG (cid:16) fP (X1, P) , fISA (cid:16) D, F, M, (cid:99)M1 (cid:17)(cid:17) , (1) where fP (), fISA (), and fCIG () denote PFE, ISA, and CIG, respectively. In the following texts, we illustrate the details of each aforementioned component following the pipeline sequence of STABLEV2V. 3.1. Prompted First-frame Editor Since STABLEV2V is built based on first-frame-based methods that decompose video editing into image editing and controlled image-to-video generation, the first step of STABLEV2V is to convert the external prompt into edited contents in the first video frame, with PFE serving as the core component in this step. Given an input video = {X1, . . . , XN }, we send its first frame X1 and the external prompt into PFE, where we formulate this process by: (cid:98)X1 = fP (X1, P) , (2) where (cid:98)X1 refers to the first edited video frame of (cid:98)X . Herein, we consider various categories of prompt inputs P, e.g., text descriptions, user instructions, reference images, and etc., where we adopt off-the-shelf image editors to process these prompts accordingly. For example, we utilize textguided editors, e.g., SD Inpaint [33] and InstructPix2Pix [36], to process text inputs, and adopt models like Paintby-Example [5] to integrate reference image prompts. Afterward in the subsequent processes, we build the alignment between motion controls and edited contents based on (cid:98)X1. 3.2. Iterative Shape Aligner Once we obtain the first edited frame (cid:98)X1, the next step is to propagate the edited contents to the remaining video frames. To conduct this step, we observe that existing studies often produce inferior results through directly propagation of motions from the source video, where the delivered motions in such case struggle to be consistent with contents that users expect, especially in the cases that user prompts may cause significant shape changes, as is shown in Fig. 1, thereby leading to artifacts in the edited video. Therefore, it is pivotal to propose an effective design to address such misalignment, so as to ensure the consistency in video editing. In doing so, we propose ISA, which establishes the alignment between delivered motions and user prompts, and later offers precise guidance for CIG to produce the final video. Specifically, we assume that the edited and original contents share the same motion and depth information, and consider depth map as the intermediate media to deliver the motion information. Based on the assumption, ISA sequentially simulates the motion and depth information of all edited video frames, and leverages an additional refinement network to obtain precise motion guidance for CIG. Motion Simulation. To simulate the motion information of the edited video, we use optical flows to represent its motions. Given the source video input = {X1, . . . , XN } with frames, we utilize an off-the-shelf flow extractor (i.e., RAFT [35]) to annotate the optical flows = {F12, . . . , FN 1N } from . Besides, we use an image segmenter (i.e., SAM [19]) to obtain the segmentation masks of all frames in , as well as the one of (cid:98)X1, resulting in = {M1, . . . , MN } and (cid:99)M1, respectively. Considering that the edited contents and the original ones share the same motion information, we firstly compute the mean value of the k-th optical flow Fkk+1 within Mk to represent the average motion, with the process formulated by: (cid:88) Fkk+1 (i, j) , (3) Fkk+1 = (i,j)Mk where (i, j) represents the pixel at the i-th row and the jth column of Mk. Then, we simulate the flow within the regions of edited contents through performing the motion pasting operation on (cid:99)Mk, where it is written as: 1 Mk 4 Figure 3. Visualizations of the intermediate results by ISA. (cid:98)F mp (x, y) = (cid:40) Fkk+1, 0, (x, y) fd otherwise (cid:17) (cid:16) (cid:99)Mk Finally, we obtain (cid:98)Fkk+1 of the k-th edited frame via: (cid:16) (cid:17)(cid:17) (cid:16) (cid:98)Fkk+1 = Fkk+1 1 fd (cid:99)Mk + (cid:98)F mp . (4) (5) Herein, fd () and refer to the binary dilation and the Hadamard production operations, respectively, where we apply them on (cid:99)Mk to ensure that the simulated motion covers all regions of the edited contents. Once (cid:98)Fkk+1 is simulated, we obtain (cid:99)Mk+1 via warping (cid:99)Mk, written as: (cid:16) (cid:17) (cid:99)Mk+1 = fw (cid:99)Mk, (cid:98)Fkk+ , (6) where fw () denotes the warping operation. By iteratively simulating the optical flows from = 1 to = 1, we eventually obtain the optical flows (cid:98)F = { (cid:98)F12, . . . , (cid:98)FN 1N } of all edited frames. Depth Simulation. Once we simulate the motion information of the edited video, the next step is to obtain the guidance for the image-to-video generator, i.e., the depth maps. In doing so, we conduct procedures similar to that in motion simulation. Specifically, we firstly adopt an offthe-shelf depth estimator (i.e., MiDaS [32]) to extract the depth maps = {D1, . . . , DN } from . Given the k-th (k {1 . . . }) depth map Dk, we compute the average depth similar to the process of Eq. (3), formulated by: Dk = 1 Mk (cid:88) Dk (i, j) , (7) (i,j)Mk where (i, j) represents the pixel at the i-th row and j-th column. Then, we conduct the depth pasting operation on (cid:99)Mk to propagate the depth information, where the average depth DAVIS-EDIT-S / DAVIS-EDIT-C(=CS)"
        },
        {
            "title": "Method",
            "content": "DOVER FVD WE CLIP-Temporal CLIP Score TokenFlow [28] 66.36 / 67.47(1.11) 17.33 / 17.45(0.12) FLATTEN [9] 63.86 / 61.18(2.68) 19.17 / 21.65(2.48) Tune-A-Video [42] 28.54 / 34.63(6.09) 25.89 / 26.76(0.87) Video-P2P [25] 55.10 / 51.22(3.88) 17.22 / 17.87(0.65) CoCoCo [52] 66.81 / 66.12(0.69) 18.13 / 18.41(0.28) AnyV2V [20] 66.82 / 65.01(1.72) 14.87 / 17.83(2.96) DMT [47] 59.27 / 57.45(1.82) 19.53 / 21.64(2.11) STABLEV2V 67.78 / 70.80(3.02) 13.77 / 17.18(3.41) 18.58 / 18.60(0.02) 17.29 / 17.75(0.46) 89.63 / 81.44(8.19) 19.95 / 18.82(1.13) 16.24 / 18.47(2.23) 15.35 / 18.26(2.91) 16.65 / 19.89(3.24) 15.95 / 15.27(0.68) 95.84 / 95.61(0.23) 95.39 / 94.51(0.88) 91.82 / 90.91(0.91) 94.37 / 93.51(0.86) 96.07 / 94.97(1.10) 95.66 / 94.36(1.30) 94.11 / 93.58(0.53) 96.34 / 96.83(0.49) 24.89 / 24.12(0.77) 24.07 / 23.24(0.83) 24.67 / 24.89(0.22) 24.72 / 24.11(0.61) 24.36 / 23.24(1.12) 25.09 / 24.32(0.77) 24.91 / 24.51(0.40) 25.46 / 25.68(0.22) AnyV2V [20] STABLEV2V 65.83 / 64.56(1.27) 12.97 / 15.25(2.28) 67.58 / 68.42(0.84) 12.36 / 14.87(2.51) 24.47 / 25.61(1.14) 22.17 / 21.23(0.94) 95.89 / 96.13(0.24) 96.51 / 96.71(0.20) 25.41 / 24.79(0.62) 26.24 / 26.55(0.31) 5.81 4.23 20.23 21.17 1.55 8.28 8.88 3.14 8.43 3.23 Table 1. Quantitative results of STABLEV2V on text- (top) and image-based (bottom) evaluation settings of DAVIS-EDIT, compared to existing methods [9, 20, 25, 28, 42, 47, 52] with respect to DOVER [41], FVD [38], Warping Error (WE), CLIP-Temporal [30], CLIP scores [16], and averaged inference time (termed , in units of minutes), where the best and second best results are boldfaced and underlined. Results on DOVER, FVD, WE, CLIP-Temporal, and CLIP scores are scaled by 102, 102, 105, 102, and 102, respectively. Herein, performance gain and drop by comparing DAVIS-EDIT-C to DAVIS-EDIT-S are highlighted in blue and red, correspondingly. (x, y) = Dk if (x, y) (cid:99)Mk otherwise 0. Finally, we (cid:98)Ddp construct the k-th simulated depth map (cid:98)Dk via composing: 3.3. Conditional Image-to-video Generator (cid:98)Dk = Dk 1 (cid:99)Mk (cid:16) (cid:17) + (cid:98)Ddp . (8) By iterating all depth maps = {D1, . . . , DN 1}, we are able to obtain the simulated depth map (cid:98)D = { (cid:98)D1, . . . , (cid:98)DN } of all edited video frames. Since the simulated depth maps (cid:98)D are obtained via composing, we observe that (cid:98)D often contains unnecessary depth information in the regions of the original contents, as is shown in Fig. 3, indicating that (cid:98)D needs to be further refined to ensure its preciseness. Shape-guided Depth Refinement. To refine (cid:98)D, we draw inspirations from existing video inpainting methods [51] that adopt completion networks to repair optical flows, and propose depth refinement network based on such paradigm.2 Furthermore, we integrate the first-frame shape mask (cid:99)M1 into it to ensure the shape consistency of refinement. Given and (cid:99)M, the mask regions Mr and the masked depth maps (cid:98)Dm are obtained through: Mr = fd (cid:16)(cid:16) (cid:17) 1 (cid:99)M (cid:17) , (cid:98)Dm = Mr (cid:98)D. (9) Then, we send the concatenation of (cid:98)Dm, Mr, and (cid:99)M1 into the shape-guided refinement network fr (), resulting in the final depth maps (cid:98)Dr, where the process is written as: (cid:16) (cid:17) (cid:98)Dr = fr (cid:98)Dm, Mr, (cid:99)M1 . (10) In this way, ISA is able to obtain the accurately simulated depth maps (cid:98)Dr of the edited video, where (cid:98)Dr later play pivotal role in offering precise guidance for CIG. Once we obtain (cid:98)Dr, the final goal of CIG is to generate the edited video (cid:98)X . Specifically, CIG consists of two components, i.e., the controller model and the image-to-video generator, where we use Ctrl-Adapter [23] as controller to inject (cid:98)Dr, and leverage I2VGen-XL [34] to propagate the edited contents from (cid:98)X1 to all other frames in (cid:98)X , respectively. Given the corresponding text prompt Pt and (cid:98)Dr, CIG produces the final edited video (cid:98)X through: (cid:98)X1, Pt, Ec (cid:98)X = { (cid:98)X1, . . . , (cid:98)XN } = fCIG (11) (cid:98)Dr (cid:17)(cid:17) (cid:16) (cid:16) . 4. Experimental Settings In this section, we illustrate our experimental settings from aspects of evaluation setup, testing benchmark, baselines, and metrics, whose details are presented as follows. Evaluation Setup. In our experiments, we summarize and evaluate existing video editing studies based on two mainstream setups, i.e., textand image-based evaluation. For text-based evaluation, we adopt captions with only their object words modified to generate the edited videos. For image-based evaluation, we utilize reference images as external prompts to produce the edited videos. Testing Benchmark. For evaluation, we construct testing benchmark, namely DAVIS-EDIT, based on DAVIS [31]. DAVIS-EDIT contains two subsets DAVIS-EDIT-S and DAVIS-EDIT-C, which address the scenarios with similar (S) and changing (C) shapes, respectively. Specifically, we select 26 videos from DAVIS, and annotate the captions and images for them, obtaining 100 cases eventually.3 Baselines. We compare STABLEV2V with several state-ofthe-art video editing methods, including TokenFlow [28], 2We illustrate the implementation details of the shape-guided depth re3We illustrate more details of the proposed testing benchmark DAVISfinement network in Sec. of our supplementary materials. EDIT in Sec. of our supplementary materials. 5 Figure 4. Qualitative comparison of textand image-based editing, with their backgrounds highlighted in green and yellow, respectively. Note that results of AnyV2V [20] (green bounding boxes) use the same first edited frames as ours (red bounding boxes). FLATTEN [9], Tune-A-Video [42], Video-P2P [25], CoCoCo [52], AnyV2V [20], and DMT [47]. Notably, we use the same first edited frames in comparison with other first-frame-based methods such as AnyV2V.4 Metrics. We evaluate all compared methods from four aspects, i.e., visual quality, temporal consistency, alignment, 4Since we have no access to AVID [50], VASE [30], and I2VEdit [29], we qualitatively compare STABLEV2V with them based on their demo videos, with details presented in Sec. of our supplementary materials. and efficiency. For visual quality, we utilize DOVER [41] and FVD [38] for evaluation. For temporal consistency, we compute the Warping Error (WE) of adjacent frames in the edited video, and adopt CLIP-Temporal following VASE [30]. For alignment, we leverage CLIP score [16] to measure the feature similarities of generated frames with the text prompts. For efficiency, we evaluate based on averaged inference time, where results are tested on the same A100 GPU with torch.float16 precision. Besides, we conMethod D.-E.-S D.-E.-C Avg. TokenFlow [28] FLATTEN [9] Tune-A-Video [42] Video-P2P [25] CoCoCo [52] AnyV2V [20] DMT [47] STABLEV2V 10.92% 7.49% 14.71% 2.52% 1.60% 3.53% 3.08% 5.88% 0.00% 4.77% 2.14% 7.65% 8.56% 9.52% 10.58% 23.53% 20.45% 17.06% 23.53% 22.41% 21.18% 25.29% 27.27% 26.33% Table 2. Human evaluation results on DAVIS-EDIT-S (D.- E.-S) and DAVIS-EDIT-C (D.-E.-C). duct user study to analyze with human evaluation. 5. Results and Applications Performance Comparison and Human Evaluation. Tab. 1, Fig. 4, and Tab. 2 report the quantitative, qualitative comparisons, and human evaluation on DAVIS-EDIT, respectively, compared to several existing methods [9, 20, 25, 28, 42, 47, 52]. Specifically, TokenFlow [28] and FLATTEN [9] produce videos that are inconsistent with user prompts, and obtain inferior performance on most metrics, proving our motivation to address the shape inconsistency issue. Similar trends are observed in Tune-A-Video [42] and Video-P2P [25], with the video quality severely deteriorated, due to their incapabilities of modeling consistent motions with user prompts. Although CoCoCo [52] and AnyV2V [20] improve the aforementioned methods to some extents, they struggle to handle challenging cases with significant shape change, especially when AnyV2V uses the same edited frame as ours, suggesting the deficiencies in these methods. DMT [47] is the most related study to ours, where it fails to follow the edited text prompts in some scenarios, and tends to produce contents with information loss in the backgrounds. STABLEV2V consistently outperforms others with promising performance and video quality, where its results are also overwhelmingly preferred by users. Notably, we observe that most methods obtain worse performance on DAVIS-EDIT-C, whose cases comprise more complicated shape changes and are thus more challenging, however, STABLEV2V still obtains promising results and even gets improvements, owing to the fact that it ensures the consistency between the delivered motions and user prompts, thus will not be confused by misaligned motions when producing the final videos as others do.7 5In this paper, D., WE, C.-T, and C.S. denote the abbreviations of DOVER [41], Warping Error, CLIP-Temporal [30], and CLIP score [1] unless otherwise specified. Besides, DOVER, FVD, WE, CLIP-Temporal, and CLIP scores are scaled by 102, 102, 105, 102, and 102. 6We recruit 17 users, and show them with the inputs, prompts, and results, with 10 and 11 cases from DAVIS-EDIT-S and DAVIS-EDIT-C, respectively. Each user is asked to choose the videos with best quality without knowing the corresponding methods. Then, we compute the averaged top-1 preference percentage of all cases for comparison. 7We present more results in Sec. of our supplementary materials. Figure 5. More applications performed by STABLEV2V, where the source video frames are shown in the first row. Efficiency Comparison. In our experiments, we observe that STABLEV2V demonstrates outstanding efficiency compared to other methods, as is reported in Tab. 1. One can see that one-shot tuning-based methods [25, 42] take the most time (more than 20 minutes) to edit video due to their requirements of video-specific training, but the corresponding performance is not satisfying. DDIM inversion-based methods [20, 28, 47] also require massive time (around 6 to 8 minutes) to perform an complete editing process, where they need to prepare CNN features and attention maps via the inversion process. FLATTEN [9] presents as an improved method that uses more efficient strategy to sample trajectories of optical flows, where STABLEV2V surpasses it with approximate 1.09 minutes. Eventually, CoCoCo serves as the best method in the comparison, however, it is worth noting that it also needs to train on Web10M [27] for one epoch in advance, while STABLEV2V plays as training-free solution for video editing. Applications. Despite of the aforementioned results, STABLEV2V also support other applications as is demonstrated in Fig. 5. Herein, we adjust PFE according to the conducted application, where STABLEV2V consistently handles different tasks, especially the ones that are susceptible to cause shape differences (e.g., instructions and sketches). Notably in Fig. 1 and 5, sketch-based editing offers way for users to customize the shapes of edited contents, indicating the great potentials of applying STABLEV2V for realworld cases. Notably, video inpainting represents an extreme scenario of shape differences in STABLEV2V, with 7 Figure 6. Text-guided results under different settings of PFE. Figure 7. Results under different depth simulation strategies. Method D. FVD WE C.-T C.S. 46.03 21.06 17.69 92.22 19.72 SD SD + Con. (Canny) 61.16 19.90 16.67 94.24 21.55 SD + Con. (scribble) 64.08 14.70 16.69 95.66 24.75 67.78 13.77 15.95 96.34 25.46 SD + Con. (depth) Table 3. Evaluation scores under different settings of PFE, evaluated on text-based editing of DAVIS-EDIT-S. the foreground object completely removed from the source video. Particularly in ISA, (cid:99)M becomes all-zero maps since there is no foreground, and the pasting processes are subsequently skipped, where the shape-guided depth refinement network fr () in such case aims to fully remove and obtains depth maps of backgrounds to guide CIG. 6. Ablation Studies To further analyze STABLEV2V, we ablate its different components through conducting experiments under different settings of PFE and the depth simulation strategies, where details are presented in the following texts. Effect of PFE on Text-based Editing. We evaluate the effect of PFE using various types of text-guided editors, with the corresponding results shown in Tab. 3 and Fig. 6. Specifically, we use SD and SD + Con., referring to the SD inpaint model [33] and the integrated framework that uses the ControlNet [26] to guide the inpainting process with conditions from the source video, respectively, where the condition types are illustrated in the parentheses. We observe that SD often produces unstable edited contents like the ones in Fig. 6, which later misguides the image-tovideo generator, and produces video with inferior quality. Using conditions significantly improves such limitation by Depth Simulation D. FVD WE C.-T C.S. Using 62.00 22.93 17.25 94.73 22.55 Using (cid:98)D 66.46 16.62 16.36 95.94 24.55 Warping (cid:98)D1 with 64.54 19.14 16.83 95.33 23.71 Using (cid:98)Dr (Ours) 67.78 13.77 15.95 96.34 25.46 Table 4. Evaluation scores under different depth simulation strategies, evaluated on text-based editing of DAVIS-EDIT-S. enforcing the consistency, however, artifacts are observed due to the over-control by some conditions like Canny edge [6], with this situation alleviated in SD + Con. (scribble) and SD + Con. (depth) to some extents. This experiments highlight the vitalness of the first edited frame, which offers superior flexibility on one hand, while on the other hand, it also determines how subsequent processes perform. Effect of the Depth Simulation Strategies. In STABLEV2V, depth map plays vital role in transporting motions and guiding CIG, where we explore its effects via different simulation strategies, as is reported in Tab. 4 and Fig. 7. Directly using of source video suffers from issues similar to existing studies, where such depth maps misalign with the user prompts, so that incorrect motions are used for editing, thus leading to artifacts in results of CIG. Similar results are shown when using (cid:98)D (w/o depth refinement), since (cid:98)D contain redundant regions like the ones in Fig. 3, indicating that depth refinement significantly boosts the accuracy of CIG guidance, thus ensuring that the edited video is consistent with user prompts. Warping-based solution produces results with varying shapes due to the lack of motion pasting, where fail to fully cover (cid:98)D1, especially when edited objects comprise larger sizes than the original ones, e.g., the case of editing black swan to bag in Fig. 1. 8 7. Conclusion In this work, we present STABLEV2V, shape-consistent video editing method that sequentially edits the first video frame, aligns the motions with user prompts, and finally produces the edited video with such consistent motions, with superior performance demonstrated on challenging applications. Even so, STABLEV2V comprises several limitations due to the intrinsic problems of its paradigm, especially leading to potential working boundaries in cases with complicated motion patterns. In our future work, we expect to address such issue, and propose an improved paradigm with more fine-grained motion modeling for video editing."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual In ICML, Models From Natural Language Supervision. pages 87488763, 2021. 7 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. arXiv, 2023. 3 [3] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. UniEdit: Unified TuningFree Framework for Video Motion and Appearance Editing. arXiv, 2024. 3 [4] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2LIVE: Text-Driven Layered Image and Video Editing. In ECCV, pages 707723, 2022. 3 [5] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by Example: Exemplar-based Image Editing with Diffusion Models. In CVPR, pages 1838118391, 2023. 3, 4, 14 [6] John Canny. Computational Approach to Edge Detection. TPAMI, (6):679698, 1986. 8 [7] Chang Liu, Rui Li, Kaidong Zhang, Xin Luo, and Dong Liu. LaCon: Late-Constraint Diffusion for Steerable Guided Image Synthesis. arXiv, 2024. 2 [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70M: Captioning 70M Videos In CVPR, pages with Multiple Cross-Modality Teachers. 1332013331, 2024. [9] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. FLATTEN: Optical FLow-guided ATTENtion for Consistent Text-to-Video Editing. In ICLR, 2024. 3, 5, 6, 7 8We analyze and discuss the limitations of our proposed method in Sec. of our supplementary materials. [10] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fredo Durand, and Song Han. FastComposer: Tuning-Free MultiSubject Image Generation with Localized Attention. arXiv, 2023. 2 [11] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2VAdapter: General Image-to-Video Adapter for Diffusion Models. In SIGGRAPH, page 112, 2024. 3 [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-toImage Diffusion Models without Specific Tuning. In ICLR, 2024. [13] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. In CVPR, pages 24162425, 2022. 2 [14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. 14 [15] Li Hu. Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. In CVPR, pages 81538163, 2024. 3 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. In EMNLP, pages 75147528, 2021. 5, 6 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, 2020. [18] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. In ICLR, 2015. 12 [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross B. Girshick. Segment Anything. In ICCV, pages 3992 4003, 2023. 4 [20] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. AnyV2V: Tuning-Free Framework For Any Videoto-Video Editing Tasks. arXiv, 2024. 1, 2, 3, 5, 6, 7, 15 [21] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning Blind Video Temporal Consistency. In ECCV, pages 179195, 2018. 2 [22] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-Aware Text-Driven Layered Video Editing. In CVPR, pages 1431714326, 2023. 3 [23] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. CtrlAdapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model. arXiv, 2024. 5, [24] Chang Liu, Shunxin Xu, Jialun Peng, Kaidong Zhang, and Dong Liu. Towards interactive image inpainting via robust sketch refinement. TMM, pages 99739987, 2024. 2 [25] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-P2P: Video Editing with Cross-Attention Control. In CVPR, pages 85998608, 2024. 2, 3, 5, 6, 7 9 [26] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In ICCV, pages 38133824, 2023. 8 [27] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in Time: Joint Video and Image Encoder for End-to-End Retrieval. In ICCV, pages 17081718, 2021. 2, 7 [28] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. TokenFlow: Consistent Diffusion Features for Consistent Video Editing. In ICLR, pages 113, 2024. 2, 3, 5, 7 [29] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2VEdit: First-Frame-Guided Video Editing via Imageto-Video Diffusion Models. arXiv, 2024. 2, 3, 6, 11, 12, 13, 14, 15 [30] Elia Peruzzo, Vidit Goel, Dejia Xu, Xingqian Xu, Yifan Jiang, Zhangyang Wang, Humphrey Shi, and Nicu Sebe. VASE: Object-Centric Appearance and Shape Manipulation of Real Videos. arXiv, 2024. 2, 3, 5, 6, 7, 11, 12, 13, 14 [31] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS Challenge on Video Object Segmentation. arXiv, 2018. 2, [32] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot CrossDataset Transfer. TPAMI, 44(03):16231637, 2022. 4, 12 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image In CVPR, pages Synthesis with Latent Diffusion Models. 1067410685, 2022. 2, 3, 4, 8, 13, 15 [34] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models. arXiv, 2023. 3, 5, 15 [35] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field In ECCV, pages 402419, Transforms for Optical Flow. 2020. 4 [36] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In CVPR, pages 1839218402, 2023. [37] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing Motion and Content for Video Generation. In CVPR, pages 15261535, 2018. 2 [38] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: New Metric for Video Generation. In ICLR Workshop, 2019. 5, 6 [39] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. Recipe for Scaling up Text-to-Video Generation with Text-free Videos. In CVPR, pages 65726582, 2024. 2 [40] Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, and Peter Vajda. Fairy: Fast Parallelized InstructionIn CVPR, pages 8261 Guided Video-to-Video Synthesis. 8270, 2024. 2 10 [41] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives. In ICCV, pages 2008720097, 2023. 5, 6, 7 [42] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. In ICCV, pages 75897599, 2023. 2, 3, 5, 6, [43] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More Control for Free! Image Synthesis with Semantic Diffusion Guidance. In WACV, pages 289299, 2023. 2 [44] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent Diffusion Transformer for Video Generation. arXiv, 2024. 2 [45] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas S. Huang. YouTubeVOS: Large-Scale Video Object Segmentation Benchmark. CoRR, abs/1809.03327, 2018. 12 [46] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models. arXiv, 2023. 3 [47] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-Time Diffusion Features for ZeroShot Text-Driven Motion Transfer. In CVPR, pages 8466 8476, 2024. 2, 3, 5, 6, 7, 15 [48] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. InstructVideo: Instructing Video Diffusion Models with Human Feedback. In CVPR, pages 6463 6474, 2024. [49] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-Guided Transformer for Video Inpainting. In ECCV, pages 7490, 2022. 12 [50] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris N. Metaxas, and Licheng Yu. AVID: Any-Length Video Inpainting with Diffusion Model. In CVPR, pages 71627172, 2024. 2, 3, 6, 11, 12, 13, 14 [51] Shangchen Zhou, Chongyi Li, Kelvin C. K. Chan, and Chen Change Loy. ProPainter: Improving Propagation and Transformer for Video Inpainting. In ICCV, pages 10443 10452, 2023. 5, 11 [52] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility. arXiv, 2024. 2, 3, 5, 6, 7 Figure 8. Selected data samples and the corresponding annotations from DAVIS-EDIT, with visualizations of (a) source video frames, (b) text descriptions, and (c) reference images highlighted in orange, green, and red, respectively. Specifically in (b), represents the original (O) text description of the source video; and refers to the annotated captions indicating similar (S) and changing (C) shapes in the edited contents, respectively. Besides, we highlight the words depicting the main edited contents in red. In (c), we show the annotated images indicating similar and changing shapes on the left and right sides, respectively. Overview In our supplementary materials, we provide more details and results of STABLEV2V, so as to offer more insights into the proposed method, where we construct the contents following the structures below: Implementation Details of Shape-guided Depth Refinement Network. The proposed depth refinement network plays pivotal role in ensuring preciseness of depth guidance for STABLEV2V, where we illustrate its detailed implementation details from perspectives of the motivation, network architecture, and training in Sec. A. Implementation Details of the DAVIS-EDIT. DAVISEDIT serves as the testing benchmark for the evaluation of STABLEV2V, where we report its implementation details in Sec. B, illustrating the annotation process of different prompts and some samples for demonstration. More Qualitative Comparison. In Sec. C, we conduct the qualitative comparison with more video editing methods, especially the ones that are not open-sourced, including AVID [50], VASE [30], and I2VEdit [29]. More Results. In Sec. D, we demonstrate more qualitative results generated by STABLEV2V, from aspects of text-, image-based editing, and applications. Limitations. Although STABLEV2V achieves promising performance in various editing tasks, it also comprises several limitations due to its inherent problems, i.e., the paradigm based on pre-trained models and depth maps, where we discuss its working boundaries in Sec. E. Notably, we offer the video format of all results (both main paper and this document) at https : //alonzoleeeooo.github.io/StableV2V, and highly encourage readers to refer to them for more intuitive experience of STABLEV2V. A. Implementation Details of Shape-guided"
        },
        {
            "title": "Depth Refinement Network",
            "content": "In this section, we introduce the implementation details of shape-guided depth refinement network from various aspects, including its motivation, network architecture, and training details, as is presented in the following texts. Motivation and Network Architecture. The depth refinement network serves as pivotal component in STABLEV2V, where it is highly associated with the preciseness of depth guidance for CIG, thus subsequently affecting the consisetncy of the edited video. The final goal of such network is to calibrate the input depth maps by removing its redundant regions, meanwhile ensuring the consistency of the refined depth map with the corresponding edited first frame. To build such network, we draw inspirations from the task of video inpainting [51], where optical flows, similar to the depth maps in STABLEV2V, normally serve as pivotal guidance for the inpainting process. Recently, VASE [30] borrows the same network architecture of the flow completion network in ProPainter [51], and adds an additional channel to the input layer to integrate the shape guidance, where the resulting network is used to offer flow guidance for reference-guided video editing. Enlightened 11 Figure 9. More qualitative comparison of STABLEV2V, compared to (a) AVID [50], (b) VASE [30], and (c) I2VEdit [29]. Note that we use the same first frame as the ones of I2VEdit [29] for comparison. Figure 10. More visualizations of intermediate results in ISA, where we show the reference images at the right-bottom corners. by the aforementioned studies, we adopt the same architecture as VASE does, and utilize the segmentation mask of the first edited frame as guidance for the refinement process. Training Details. We train the shape-guided depth refinement network on YouTube-VOS [45] dataset, whose training set consists of 3, 471 videos and the corresponding mask annotations in total. To obtain the depth maps of all videos, we use an off-the-shelf depth estimator, i.e., MiDaS [32], to automatically annotate depth maps for all videos. Once the data are pre-processed, we train the shape-guided depth refinement network for 50, 000 iterations, along with batch size of 8. Specifically in each training step, we randomly sample 10 frames of depth maps, and adopt the random mask generation algorithm in Flow-guided Transformer [49]. We use AdamW [18] optimizer to update the model parameters, with the learning rate set to 0.99. B. Implementation Details of the DAVIS-EDIT In this section, we illustrate more implementation details of our testing benchmark DAVIS-EDIT. DAVIS-EDIT 12 Figure 11. More text-based results generated by STABLEV2V, where we show the first frame of the source video in the first row. plays crucial role in evaluating the performance of STABLEV2V, where we curate this testing benchmark to offer standard to promote further studies in addressing the shape misalignment problem for video editing. Fig. 8 demonstrates some samples selected from DAVIS-EDIT, along with the example text prompts and reference images that we manually annotate. To obtain the text prompts, we only modify specific words that describe the main elements of videos, e.g., objects and foregrounds, and put emphasis on embodying the shape difference problem during annotation. For example, we use duck to replace blackswan to represent the setting with similar shapes of edited contents, and edit duck into rabbit for the scenario with changing shape. For the annotation of reference images, we follow the similar principles, considering the variety of shape Figure 12. More image-based results generated by STABLEV2V, where we show the first frame of the source video in the first row for simplicity. Note that reference images are shown at the right-bottom corners of the first row. differences. On top of that, we focus on collecting reference images that are tough for texts to illustrate, e.g., the Transformer truck in Fig. 8, so as to highlight the impacts of image guidance in such setting. C. More Qualitative Comparison In this section, we showcase more qualitative comparison with more methods, especially the ones that are not opensourced yet, including AVID [50], VASE [30], and I2VEdit [29]. Specifically, both AVID and VASE serve as learningbased solution for video editing, where AVID is textguided video inpainting framework initialized from SD Inpaint [33]; VASE is fine-tuned based on image-guided 13 Figure 13. More results of applications conducted by STABLEV2V, including instruction-based editing, sketch-based editing, video style transfer, and video inpainting, whose backgrounds are highlighted in green, blue, red and yellow, respectively. editor, i.e., Paint-by-Example [5], and mainly puts emphasis on object-centric video editing. I2VEdit serves as first-frame-based video editing method that trains videospecific LoRA [14] to model the motion patterns of the source video. Since we do not have access to their code and model weights, we mainly compare STABLEV2V to their released demo video, with results presented in Fig. 9. For fair comparison, we use the same reference images provided by VASE [30] in their demonstrated videos, and adopt the same first frame as the ones of I2VEdit [29]. Analyses. By comparing STABLEV2V to learning-based methods, i.e., AVID [50] and VASE [30], it is observed that AVID [50] has possibilities in producing results with inconsistent textures, e.g., the case of editing swan into duck, suggesting its deficiencies in maintaining the temporal consistency. VASE [30] produces results that merely transfer the textures of reference images into the edited videos, e.g., the cow-shape zebra in Fig. 9, since it is highly restricted by the input masks used in its inpainting paradigm. The aforementioned results illustrate the typical issues in learning-based methods, where they are limited to editing scenarios with little shape changes due to the inpainting 14 E. Limitations Although outperforming performance and applications are demonstrated by STABLEV2V, we observe that our proposed method also comprises several limitations due to the inherent problems that are caused by its paradigm, especially leading to potential working boundaries in cases that contain complicated motion patterns. Therefore in this section, we analyze the limitations and working boundaries of STABLEV2V, with some failure cases shown in Fig. 14, and discuss several potential solutions. Details of the aforementioned analyses are illustrated in the following texts. inherent Problems of Pre-trained Models. Since STABLEV2V presents training-free solution in addressing the misalignment problem between the motion controls and the edited contents, it relies on the use of pre-trained models and also suffers from severl inherent problems of them. Specifically, this limitation occurs mostly in two components, i.e., PFE and CIG, where the former normally leverages off-the-shelf image editing methods; the latter is mainly designed based on conditional generation paradigm for image-to-video generation, i.e., Ctrl-Adapter [23], since few studies are available in the existing literature. For PFE, as is analyzed in Sec. 6 in our main paper, it comprises certain degree of randomness in some textguided editors such as SD Inpaint [33], where edited contents with undesired orientations might be produced, and then subsequently mis-guide the CIG module to produce inferior results. For CIG, we observe that Ctrl-Adapter might lead to slight color discrepancy in several cases, especially when the edited contents are biased to certain colors, e.g., the case of editing the car into Transformer truck in Fig. 14. Such color bias might be caused by the limited diversity and quality in the training data of Ctrl-Adapter, since its fine-tuning process may not require as much data as that used for its foundation model, i.e., I2VGen-XL [34]. Meanwhile, we observe that the generated textures are much more consistent than other studies, especially compared to the ones that also leverage I2VGen-XL, e.g., AnyV2V [20], since ISA ensures the alignment between the edited contents and the delivered motions to CIG. This finding indicates potential solution to the above issue by considering ISA as plug-and-play plugin, where we can integrate it into more powerful methods in the future once available. Working Boundaries in Complicated Motion Patterns. Another problem that STABLEV2V might suffer from is its limited capabilities in modeling motion patterns that are too complicated, e.g., the case of man doing break dancing in Fig. 14. Similar results are observed in other studies like DMT [47] and AnyV2V [20], where it is also tough for these methods to produce consistent results. Such scenario serves as the challenging case that most existing methods struggle to handle, where the task of modeling fine-grained motions for video editing deserves studying in future works. Figure 14. Failure cases of STABLEV2V illustrating the limitations of inherent problems of pre-trained models (top) and complicated motion patterns (bottom). paradigm of their foundation models, i.e., SD Inpaint and Paint-by-Example, where such issues are significantly alleviated in STABLEV2V, since our first-frame-based scheme offers more flexiblity. By comparing STABLEV2V to other first-frame-based method, i.e., I2VEdit [29], two limitations are observed, where I2VEdit either produces results with information loss in the backgrounds, e.g., the case of editing the blackswan into flamingo, or generates edited contents with simple motions like the case of rising rocket. Conversely, results generated by STABLEV2V comprise more detailed textures such as the waves on the river and the smoke emitted by the rocket, indicating that STABLEV2V not only offers robust consistency in the edited videos, but also ensures its video quality in details. D. More Results In this section, we illustrate more results generated by STABLEV2V. Specifically, we offer more visualizations of the intermediate results of ISA in Fig. 10. Besides, we show several results on textand image-based editing scenarios in Fig. 11 and Fig. 12, respectively. Also, we present more applications performed by STABLEV2V in Fig. 13."
        }
    ],
    "affiliations": [
        "University of Science and Technology of China"
    ]
}