{
    "paper_title": "SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios",
    "authors": [
        "Kai Li",
        "Wendi Sang",
        "Chang Zeng",
        "Runxuan Yang",
        "Guo Chen",
        "Xiaolin Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet the training and evaluation requirements of models. Although synthetic datasets offer a larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, a synthetic toolkit de-designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and source-level, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed a moving sound source benchmark dataset, SonicSet, using the Librispeech, the Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to validate the differences between synthetic data and real-world data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record a real-world speech separation dataset, which was then compared with the corresponding synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between other synthetic datasets and the SonicSet dataset for speech enhancement. The results indicate that the synthetic data generated by SonicSim can effectively generalize to real-world scenarios. Demo and code are publicly available at https://cslikai.cn/SonicSim/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 1 8 4 1 0 . 0 1 4 2 : r Technical report SONICSIM: CUSTOMIZABLE SIMULATION PLATFORM FOR SPEECH PROCESSING IN MOVING SOUND SOURCE SCENARIOS Kai Li1, Wendi Sang1, Chang Zeng2, Runxuan Yang1, Guo Chen1 & Xiaolin Hu1,3,4 1. Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China 2. National Institute of Informatics, Tokyo, Japan 3. Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China 4. Chinese Institute for Brain Research (CIBR), Beijing 100010, China tsinghua.kaili@gmail.com xlhu@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet models training and evaluation requirements. Although synthetic datasets offer larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, synthetic toolkit designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and sourcelevel, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed moving sound source benchmark dataset, SonicSet, using LibriSpeech, Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to investigate the differences between synthetic and realworld data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record real-world speech separation dataset, which set reference for comparing SonicSet and other synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between the SonicSet dataset and other synthetic datasets for speech enhancement. The results indicate that models trained on SonicSet generalized better to real-world scenarios compared with other synthetic datasets. Demo and code are publicly available at https://cslikai.cn/SonicSim/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Speech separation (Cherry, 1953) and enhancement (Loizou, 2007) are classic tasks in speech signal processing, with the goal of extracting the target speech signal from noisy audio. In recent years, the release of large-scale synthetic datasets, such as LibriMix (Cosentino et al., 2020) and the DNS Challenge (Reddy et al., 2020), has contributed to the improvement of speech separation and enhancement models. Many methods Wang et al. (2023); Subakan et al. (2021); Chen et al. (2020a); Luo & Yu (2023b); Li et al. (2022e); Li & Luo (2023) have demonstrated the ability to transfer across different test environments in zero-shot settings. Applications like online meetings (Rao et al., 2021) and human-computer interaction (Yu & Chen, 2017) have greatly benefited from advancements in these state-of-the-art (SOTA) speech separation and enhancement methods. However, despite the Corresponding author. 1 Technical report remarkable progress, there remains significant acoustic mismatch between synthetic data and realworld data (Yang et al., 2024). This discrepancy often results in suboptimal model performance in real-world scenarios when trained on synthetic data (Sivaraman & Kim, 2022). In previous studies, efforts to reduce the gap between synthetic and real-world data have primarily focused on more accurately modeling room reverberation. Current datasets, such as WHAMR! (Maciejewski et al., 2020) and the DNS Challenge (Reddy et al., 2020), simulate room impulse responses (RIRs) using methods like finite element analysis (Jarrett et al., 2012), ray tracing (LopezHernandez et al., 2000), or the image source method (Luo & Yu, 2023a). However, RIRs generated by these methods still exhibit significant acoustic discrepancies when compared to real-world reverberation. Specifically, from the perspective of RIR simulation, existing approaches face three major challenges: 1) Occlusion of obstacles. RIRs generated based on the image source method struggle to effectively handle varying quantities and shapes of obstacles within room (Aralikatti et al., 2022). This issue becomes particularly pronounced when objects obstruct the sound path between the source and the microphone, making it challenging to accurately reproduce their acoustic effects. 2) Complex room geometry. Simulated RIRs typically assume that rooms have cuboid-shaped geometries (Southern et al., 2011), which is significant deviation from real-world complex environments, such as lecture halls with tiered seating. This discrepancy leads to less realistic simulation outcomes. 3) Complicated room surfaces and object materials. Different materials have distinct sound absorption, reflection, and scattering properties, and when multiple materials are present in scene, accurately replicating such complex acoustic environments in simulations becomes difficult. For example, hard walls generally reflect more sound, while soft materials like carpets or furniture absorb more. Therefore, an accurate simulation of RIRs remains challenging problem. Additionally, notable characteristic of these datasets is that they are primarily collected from static sound sources (Cosentino et al., 2020; Maciejewski et al., 2020; Reddy et al., 2020), which makes these datasets more suitable for non-moving scenarios such as online meetings. In moving sound source scenarios, speakers often move around within space while static microphones capture their speech, setup more relevant to applications such as robotic navigation (Wang et al., 2004). Compared to the collection of static speech datasets, capturing large-scale moving sound source datasets is more time-consuming (Yang et al., 2024). As result, there is relative scarcity of data on moving sound source scenarios, which limits the development and evaluation of speech separation and enhancement models in such tasks. Even if the cost of data acquisition can be justified and the data is sufficiently accurate, real-world datasets are still constrained by the availability of physical environments. For example, for specific indoor layouts, it is necessary to collect data directly from real-world scenes. Once collected, the distribution of real-world datasets is often fixed, making them difficult to adjust flexibly. This limits researchers ability to conduct personalized experiments and increases the risk of models overfitting to specific dataset, thereby rendering benchmarks outdated. Thus, despite the high research value of moving sound source scenarios, the lack of relevant datasets significantly hinders deeper exploration in this area. Figure 1: Overview of SonicSim, our toolkit for speech research. SonicSim provides customizable data generator based on Habitat-sim, allowing users to generate realistic and physically plausible audio data in controlled manner. To advance research in moving sound sources, particularly in speech separation and enhancement tasks, there is pressing need for high-fidelity, low-cost, and comprehensive synthetic toolkit and data asset for moving sound sources. To address these issues, we developed SonicSim, synthetic toolkit capable of accurately simulating RIRs for moving sound sources, as shown in Figure 1. This toolkit is built upon the embodied AI simulation platform, Habitat-sim (Savva et al., 2019), which can import variety of 3D environments and accurately simulate the acoustic characteristics of rooms. The core strength of SonicSim lies in its precise simulation of RIRs (Chen et al., 2022a) and 2 Technical report Figure 2: Overview of the SonicSet dataset. It covers wide range of scenarios, speakers, and noise. highly customizable simulation of moving sound sources, which significantly expands the scale of data collection for moving sound source research. Based on SonicSim, we constructed multi-scene, large-scale, and high-quality moving sound source dataset called SonicSet (see Figure 2): 1) Multi-scene: SonicSet utilizes 90 scenes from the Matterport3D dataset (Chang et al., 2017), covering wide range of real-world environments, such as homes, offices, and churches; 2) Large-scale: SonicSet integrates 360 hours of speech audio from the LibriSpeech (Panayotov et al., 2015), combined with environmental noise from FSD50K (Fonseca et al., 2021) and musical noise from the FMA dataset (Defferrard et al., 2017); 3) Highquality: The RIRs of the synthetic audio closely resemble real-world environments by simulating reflection and diffraction across various materials, resulting in higher-quality reverberated audio. On the SonicSet dataset, we conducted extensive quantitative experiments on 11 speech separation methods and 9 speech enhancement methods, thoroughly analyzing the performance of each approach. To evaluate the gap between SonicSet and real-world environments, we randomly selected some raw audio without reverberation from the SonicSet validation set and recorded it in real-life scenarios, constructing speech separation dataset consisting of 10 scenes with total duration of 5 hours. Additionally, for the speech enhancement task, we utilized the RealMAN test set (Yang et al., 2024), which includes recordings of moving sound sources from real-world environments, to evaluate the gap between the synthetic dataset and real environments. The experimental results demonstrated that models trained on the SonicSet dataset generalized to real-world environments, validating the effectiveness and potential of SonicSim for speech research."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "In this section, we systematically compare the SonicSim toolkit and the generated SonicSet dataset with existing real-world datasets, synthetic datasets, and simulation toolkits (see Table 1). Real-world datasets: Currently, there are very few speech separation and enhancement datasets recorded in real-world environments (Yang et al., 2024; Watanabe et al., 2020). This is primarily because most speech datasets are designed for speech recognition evaluation and only provide transcription annotations, lacking the ground-truth labels needed for supervised learning tasks such as speech separation and enhancement. Recently, the RealMAN dataset introduced real-world microphone array recordings of speech and noise (Yang et al., 2024). Although the gap between the RealMAN dataset and real-world application scenarios is relatively small, its high annotation cost and limited scalability hinder model adaptation to diverse scenarios. Our proposed SonicSim addresses these limitations by offering highly customizable and realistic synthetic data generation method, filling the gap in scale and scene diversity present in existing datasets. Synthetic datasets: Synthetic datasets (Reddy et al., 2020; Cosentino et al., 2020; Maciejewski et al., 2020; Chen et al., 2020b) provide an alternative solution for speech separation and enhancement tasks, typically generating synthetic data by convolving RIR with multiple source signals or noise signals to ensure realism. These simulated RIRs are often generated using the image source methods Allen & Berkley (1979). Datasets such as WHAMR! Maciejewski et al. (2020) introduce synthetic reverberation and noise environments, increasing the complexity of the data. However, synthetic datasets are often created under tightly controlled simulation conditions, which reduces the authenticity of the data and cannot reflect the performance of speech separation and enhancement 3 Technical report Datasets Geometry Occlusion Material Scalability Cost Tools Src Type Tasks WHAMR! 2020 LibriCSS 2020b DNS Challenge 2020 Chime6 2020 LRS2 2023 RealMan 2024 SonicSet (ours) Cuboid Cuboid Cuboid Variable Variable Variable Variable Low High Low High High High Low Static Static Static Static Static Dynamic SS/SE SS SE SS SS SE Dynamic SS/SE Table 1: Comparison of different datasets for speech separation and enhancement. Geometry refers to the room geometry. Occlusion indicates whether objects obstruct the sound source and microphone. Material shows whether the material properties of objects are considered. Scalability refers to the ability to increase the amount of data. Cost represents the difficulty of simulating or collecting data. Tools indicates whether there are toolkits for data collection. Src Type indicates the source type. SS and SE denote the speech separation and enhancement tasks. methods in real-world environments. In contrast, SonicSet not only ensures the acoustic plausibility of simulated audio (e.g., occlusions and material properties) but also supports broader range of customizable options (e.g., varying geometric structures and multiple microphone array configurations). This provides more challenging and realistic environments for speech separation and enhancement methods. Reverberation simulation tools: In recent years, significant progress has been made in the development of reverberation simulation tools. FRAM-RIR (Luo & Yu, 2023a) offers rapid RIR simulation method using stochastic approximation of the image source method, which improves simulation speed. RIR-Generator (Tang et al., 2020) is an open-source tool based on the image source method, allowing users to set parameters. Pyroomacoustics (Scheibler et al., 2018) provides flexible Python library that enables fast simulation of source-microphone setups in 2D or 3D rooms. For more complex acoustic environments, COMSOL Multiphysics (Multiphysics, 1998) combines finite element methods to provide detailed modeling of rooms acoustic characteristics. Compared to these tools, SonicSim based on Habitat-sim (Savva et al., 2019) surpasses existing simulation tools in acoustic realism, making synthetic audio closely resembling recordings in real-world environments. Additionally, SonicSim provides range of practical features that allow users to efficiently create diverse acoustic scenes tailored to specific needscapabilities that most existing reverberation simulation tools lack."
        },
        {
            "title": "3 MOVING SOUND SOURCE SUITE",
            "content": "The moving sound source suite consists of two core components: the customizable data generator SonicSim and the moving sound source dataset SonicSet. SonicSim leverages existing datasets to create data tailored for speech separation and enhancement. 3.1 CUSTOMIZABLE DATA GENERATOR: SONICSIM The customizable dataset generator, SonicSim, is specifically designed to generate datasets for speech tasks. Built on Habitat-sim (Savva et al., 2019), SonicSim leverages its highly realistic audio renderer (demonstrated in (Chen et al., 2022a)) and high-performance 3D simulator to produce high-quality audio data adapted to various acoustic environments. SonicSim provides rich set of annotations, including source and microphone position maps, clean audio, and audio with reverberation and noise, without incurring additional data collection costs. More importantly, SonicSim provides users with extensive control over the dataset generation process, allowing customization of scene layouts, scene materials, source and microphone positions, and microphone types, while ensuring physical realism through its physics engine. By adjusting these configurations, SonicSim can flexibly generate diverse acoustic environment data to meet the specific requirements of various tasks. The following main functions are included in SonicSim, as shown in Figure 1. 3D scene import: Through Habitat-sim (Savva et al., 2019), SonicSim can import various realistic 3D assets generated through simulations or scans, such as those from the Matterport3D 4 Technical report dataset (Chang et al., 2017). SonicSim leverages Habitat-sim1 to interpret metadata and structural information from external datasets and convert them into Habitat-sims native scene format. Geometric data, material properties, and semantic annotations are transformed to ensure that the imported scenes maintain their original high fidelity. This ensures consistency within the simulation environment. This workflow not only simplifies the process but also makes the generation of complex and realistic acoustic environments more efficient and scalable, enabling researchers to conduct experiments in environments that closely resemble real-world scenarios. Acoustic environment simulation: SonicSim uses Habitat-sim to simulate various acoustic features in 3D environments2. Its capabilities include: 1) accurately simulating sound reflections within room geometries using indoor acoustic modeling and bidirectional path tracing algorithms; 2) mapping the semantic labels of 3D scenes to material properties to set the acoustic features of different surfaces, such as absorption, scattering, and transmission coefficients; 3) extended Habitat-sim acoustic simulation capabilities enable the synthesis of moving sound source data based on sound source paths. SonicSim can generate audio that closely resembles real-world conditions, ensuring the physical realism of acoustic simulations. Microphone types: Habitat-sim offers comprehensive microphone configurations, supporting various audio formats such as mono, binaural, and ambisonics3. Additionally, we have integrated common linear and circular microphone arrays, allowing users to customize the shape of microphone arrays to meet different experimental requirements. Specifically, this functionality is realized by arranging multiple mono audio sensors within 3D space. We provide flexible API that enables researchers to represent the desired array layout by inputting custom functions or data structures. The simulation engine then instantiates microphones in the environment based on the input, ensuring precise acoustic modeling of the array geometry. Source and microphone positions: SonicSim allows users to customize or randomly set the positions of sound sources and microphones, which can be defined within the global coordinate system of the environment. Apart from static positioning, SonicSim also supports generating motion trajectories for moving sound sources and microphones based on specified start and endpoints. This feature is implemented by interpolating between defined points using navigable paths and leveraging Habitat-sims physics engine to update the positions of moving entities over time. As the sound sources and microphones move along their trajectories, the system dynamically calculates the acoustic response in real-time. SonicSim accounts for dynamic changes in distance, occlusion, and environmental interactions, continuously updating the sound propagation paths to simulate evolving reverberation. By integrating these capabilities, SonicSim provides highly realistic simulation environment for scenarios involving moving sound sources. 3.2 MOVING SOUND SOURCE DATASET: SONICSET Figure 3: An automatic simulation pipeline for moving sound sources. The positions of the sound sources, noises and microphones are all randomly generated. The five RIRs shown in the figure are just for demonstration purposes; the actual process involves more RIRs. We used SonicSim to construct moving sound source dataset, SonicSet, specifically designed for research on moving speech separation and enhancement tasks. This dataset supports diverse acoustic scenarios by simulating microphones, sound sources, and noise sources randomly placed in the environment. detailed data analysis can be found in Appendix A. As shown in Figure 3, the 1https://aihabitat.org/docs/habitat-sim/habitat_sim.sim.SimulatorConfiguration.html 2https://aihabitat.org/docs/habitat-sim/habitat_sim.sensor.AudioSensor.html 3https://github.com/facebookresearch/habitat-sim/blob/main/docs/AUDIO.md 5 Technical report entire data generation process is outlined, ensuring the validity of the sound source and microphone configurations under varying positions and dynamic changes. 3.2.1 DATA SOURCE The data we used consists of two main components: 3D assets and audio data. For the 3D assets, we utilized the Matterport3D dataset (Chang et al., 2017), large-scale indoor RGB-D dataset containing 90 building-level scenes, representing wide range of real-world environments. The semantic segmentation annotations from Matterport3D were used to assign acoustic material properties, enabling the creation of more realistic acoustic environments. In SonicSet, the training set includes 62 scenes, the validation set contains 19 scenes, and the test set includes 9 scenes. For the audio data, we primarily used speech data from the LibriSpeech dataset (Panayotov et al., 2015), and noise data from the FSD50K (Fonseca et al., 2021) and the FMA datasets (Defferrard et al., 2017). LibriSpeech is an English speech dataset consisting of approximately 1,000 hours of 16 kHz sampled read speech, sourced from audiobooks in the LibriVox project. We selected trainclean-360 as the training set for speech content, dev-clean as the validation set, and test-clean for testing. FSD50K is an open dataset of manually labeled sound events, containing 51,197 audio clips with total duration of over 100 hours, covering 200 sound categories. We downsampled them to 16 kHz using the Librosa (McFee et al., 2015). The dataset was divided into training, validation, and test sets in 7:2:1 ratio. For the FMA dataset, an open music dataset designed to support variety of tasks in the music domain, we employed pre-trained BSRNN model (Luo & Yu, 2023b) on music separation tasks to remove vocals from the tracks, ensuring that the speech separation process would not be affected during data synthesis. Subsequently, we downsampled the music data to 16 kHz using Librosa. Similar to FSD50K, the FMA dataset was divided into training, validation, and test sets following 7:2:1 ratio. 3.2.2 DATASET CONSTRUCTION The audio simulation pipeline in the SonicSet dataset, as illustrated in Figure 3, could be divided into the following steps. First, we selected 3D scene from the Matterport3D dataset and imported it into SonicSim to initialize the acoustic environment. Second, we randomly selected the placement positions for the microphone and the sound source from certain layer of the scene. Once the microphones position was determined, the sound sources starting position and the noise sources position were randomly chosen within 1-8 meter radius from the microphone. Third, based on the initial position of the sound source, we selected sound sources endpoint within 1-8 meter range from both the microphone and the sound sources initial position. We then employed SonicSims trajectory function to calculate and generate movement path for the sound source. This trajectory function calculated the distance between consecutive points and converts them into sampling rate, dividing the total number of samples for the dynamic audio based on these distances, and subsequently calculating interpolation indices and weights. Fourth, SonicSim calculated the corresponding RIRs for the different positions along the path and convolved the source audio with these RIRs. Finally, based on the precomputed indices and weights, we extracted audio segments corresponding to the start and end positions of each section from the convolution output and mixed them according to the interpolation weights. Through this process, temporally coherent audio signal was generated, accurately reflecting the movement of the sound source within the spatial environment. To create realistic synthetic mixed audio, we set the length of each mixed audio clip to 60 seconds. Each speech segment was composed of 3-5 full audio clips with the same speaker randomly selected from LibriSpeech, arranged into 60-second audio sequence by randomly selecting start positions within 0-8 seconds for each clip. This approach ensured various overlap rates in the mixed audio while utilizing SonicSim for moving sound source synthesis. For noise data, 6-8 segments of environmental noise were randomly selected from the divided FSD50K dataset, and 6-8 segments of musical noise were randomly chosen from the divided FMA dataset. These noise segments were arranged into 60-second audio clips by choosing random start positions within 0-4 seconds. We used Loudness Units relative to Full Scale (LUFS) to adjust volume levels, setting the speech at -17 LUFS, environmental noise at -21 LUFS, and musical noise at -24 LUFS. When generating the mixed audio, different speakers were randomly chosen from the speech data, and segment of environmental noise was randomly selected to create mixed audio clip with background noise, while segment of musical noise was randomly chosen to create mixed audio clip with musical noise. Technical report These mixed audio samples were used to test the robustness of speech separation and enhancement models in various noisy environments. For each dataset group (three speech clips + one environmental noise clip + one musical noise clip), we simulated within the same scene, where the speech clips were simulated as moving sources and the noise was simulated as static sources. We present an example of SonicSet in Figure 5 in Appendix and provide the corresponding audio metadata in the JSON file 1 in Appendix B."
        },
        {
            "title": "4 BENCHMARK I: SPEECH SEPARATION",
            "content": "4.1 PROBLEM DEFINITION Speech separation aims to isolate individual speech signals from mixture containing multiple speakers, as shown in Figure 4(a). This task is critical for real-world applications such as meetings and telephone conversations. detailed explanation of the task pipeline is provided in Appendix C.1. Figure 4: Overall pipeline for speech separation and enhancement. 4.2 BENCHMARK MODELS We selected several popular models that have demonstrated excellent performance in speech separation as benchmarks, including Conv-TasNet (Luo & Mesgarani, 2019), DPRNN (Luo et al., 2020), DPTNet (Chen et al., 2020a), SuDORM-RF (Tzinis et al., 2020), A-FRCNN (Hu et al., 2021), SKIM (Li et al., 2022d), TDANet (Li et al., 2023), BSRNN (Luo & Yu, 2023b), TF-GridNet (Wang et al., 2023), Mossformer (Zhao & Ma, 2023), and Mossformer2 (Zhao et al., 2024). For detailed information about the benchmark models, see Appendix C.2. The training object of the models is available in Appendix C.3. The hyperparameter configuration and all training settings are available in Appendix C.4. The PyTorch implementations and pre-trained weights for all benchmark models are publicly available4. 4.3 DATASET DETAILS We report the performance of trained baseline models using the LRS2-2Mix (Li et al., 2023), Libri2Mix (Cosentino et al., 2020), and SonicSet datasets, which we tested on real-world moving speech separation dataset collected by us. We used the test sets from LRS2-2Mix and Libri2Mix to test the models generalization and transfer performance across different scenarios. Please refer to Appendix C.5 for detailed descriptions of these datasets. 4.4 EVALUATION METRICS We employed series of evaluation metrics to assess the performance of the speech separation benchmark models comprehensively. These metrics cover multiple dimensions of audio quality, including signal quality (SI-SNR (Le Roux et al., 2019) and SDR (Vincent et al., 2006)), speech intelligibility (STOI (Taal et al., 2011) and WER), and subjective quality perception (PESQ (Rix et al., 2001)). Details are given in Appendix C.6. For these metrics, higher values are preferable for all indicators except for WER, where lower value is better. From the perspective of practical applications, WER, speech recognition metric, is the most important metric because speech separation is usually preprocessing step for speech recognition. 7 Technical report Table 2: Comparative performance evaluation of models trained on different datasets using realrecorded audio with environmental noise. The results are reported separately for trained on LRS22Mix, trained on Libri2Mix and trained on SonicSet, distinguished by slash. The relative length is indicated below the value by horizontal bars. Table 3: Comparative performance evaluation of models trained on different datasets using realrecorded audio with musical noise. The results are reported separately for trained on LRS2-2Mix, trained on Libri2Mix and trained on SonicSet, distinguished by slash. 4.5 RESULTS AND ANALYSIS Comparison on real-recorded datasets. To validate the acoustic simulation gap between synthetic datasets and real data, we constructed real-world moving sound source dataset consisting of 5 hours of recorded audio (details in Appendix C.5), encompassing various complex acoustic environments and different types of background noise (environmental and musical noise). In Tables 2 and 3, we trained models using the LRS2-2Mix and Libri2Mix datasets to compare their performance with models trained on the SonicSet dataset. The LRS2-2Mix dataset (Li et al., 2023) contains realworld noise and reverberation. The Libri2Mix dataset (Cosentino et al., 2020) is currently the largest synthetic speech separation dataset, and its data scale is closest to SonicSet among public speech separation datasets. We trained different models on these datasets and tested them on the real-world recorded datasets. The results demonstrated that models trained on the SonicSet dataset achieved overall the best separation performance on the real-world recorded datasets. This finding indicates that the SonicSet dataset excels in simulating more realistic acoustic scenes, making it effective for model training and evaluation. Comparison on the SonicSet dataset. The results of the speech separation are shown in Table 4. Different models exhibit varying performance under environmental and musical noise conditions. Among the RNN-based models, DPRNN and BSRNN performed relatively poorly when handling musical noise. In contrast, CNN-based models (SuDoRM-RF and A-FRCNN) demonstrated more balanced performance across both environmental and musical noise scenarios. Among Transformerbased models, Mossformer2 showed the SOTA performance, especially in handling complex noise. This superiority can be attributed to incorporating the multi-head attention mechanism, which effectively captures long-term dependencies and complex frequency variations (Vaswani et al., 2017). 4https://github.com/JusperLee/SonicSim/tree/main/separation 8 Technical report Table 4: Comparison of existing speech separation methods on the SonicSet dataset. The performance of each model is listed separately for results under environmental noise and musical noise, distinguished by slash."
        },
        {
            "title": "5 BENCHMARK II: SPEECH ENHANCEMENT",
            "content": "5.1 PROBLEM DEFINITION The speech enhancement task aims to extract high-quality target speech from noisy signal, reducing or eliminating background noise, as shown in Figure 4(b). This task is crucial in applications such as speech recognition, speech communication, and hearing aids. detailed explanation of this task pipeline is available in Appendix D.1. 5.2 BENCHMARK MODELS In the speech enhancement experiments, we selected several popular models: DCCRN (Hu et al., 2020), Fullband (Hao et al., 2021), FullSubNet (Hao et al., 2021), Fast-FullSubNet (Hao & Li, 2022), FullSubNet+ (Chen et al., 2022b), TaylorSENet (Li et al., 2022a), GaGNet (Li et al., 2022c), G2Net (Li et al., 2022b) and Inter-SubNet (Chen et al., 2023). The details of the models are available in Appendix D.2. The training object of the model is available in Appendix D.3. The hyperparameter configuration and all training settings are available in Appendix D.4. 5.3 EVALUATION METRICS In the speech enhancement task, we use series of evaluation metrics to comprehensively evaluate the performance of the speech separation baseline model, including signal quality (SI-SNR (Le Roux et al., 2019) and SDR (Vincent et al., 2006)), speech intelligibility (STOI (Taal et al., 2011) and CER), and subjective quality perception (PESQ (Rix et al., 2001), DNSMOS (Reddy et al., 2021) and SigMOS (Ristea et al., 2024)). Since the real-world dataset, RealMAN is Chinese dataset, we use CER as the evaluation metric for speech recognition. For these metrics, except for CER, the higher the value, the better, while for CER, the lower the value, the better. From an application perspective, CER is the most important metric, as speech enhancement is usually pre-processing step for speech recognition. The PyTorch implementations and pre-trained weights for all baseline models have been made publicly available5. 5.4 DATASET DETAILS Unlike the speech separation task, in the speech enhancement task, we select the audio of single speaker from the training set as the ground truth label for each iteration, mixing it with various types of noise using the same mixing method as in the speech separation task. We trained baseline models using the training sets of VoiceBank-DEMAND (Valentini-Botinhao et al., 2016), the DNS Challenge (Reddy et al., 2020) and SonicSet, and tested them on the test set of the real-world dataset (RealMAN (Yang et al., 2024)) to evaluate the effectiveness of different datasets in simulating real acoustic environments. To assess the generalization ability of models trained on SonicSet, we also tested the baseline models on VoiceBank-DEMAND and the DNS Challenge test sets. 5https://github.com/JusperLee/SonicSim/tree/main/enhancement 9 Technical report 5.5 RESULTS AND ANALYSIS Comparison on real-recorded dataset. We utilized the real-world recorded moving source dataset, RealMAN (test set), to evaluate trained speech enhancement models on various speech enhancement datasets, including VoiceBank-DEMAND, DNS Challenge, and SonicSet. VoiceBank-DEMAND is smaller dataset, while the DNS Challenge dataset comprises approximately 700 hours of data, with data scale comparable to that of SonicSet. We employed the DNSMOS (Reddy et al., 2021) tool to calculate subjective metrics, while Character Error Rate (CER) results were evaluated using the same speech recognition model6 as used with RealMAN. The results presented in Table 5 indicated that models trained on the SonicSet dataset achieved overall the best results across multiple metrics. Please note that for most practical applications, CER is the most important metric. Based on these findings, we infer that the synthetic dataset SonicSet effectively simulates the acoustic environments of real moving source scenarios. This provides significant flexibility and cost-effectiveness for constructing large-scale datasets, positioning synthetic datasets as an effective alternative for addressing complex environments. Table 5: Comparative performance evaluation of models trained on different datasets using the RealMAN dataset. The results are reported separately for trained on VoiceBank-DEMAND, trained on DNS Challenge and trained on SonicSet, distinguished by slash. Table 6: Comparison of speech enhancement methods using the SonicSet test set. The metrics are listed separately under environmental noise and musical noise, distinguished by slash. Comparison on the SonicSet dataset. By comparing the performance of different speech enhancement models on the SonicSet dataset, we analyzed their enhancement effects under various types of noise. Table 6 presents the performance of the benchmark models. Sub-band splitting methods (such as FullSubNet, Fast-FullSubNet, FullSubNet+, and Inter-SubNet) performed well across multiple metrics. Their interaction processing mechanism between sub-bands significantly improved the models ability to handle frequency band dependencies, resulting in better speech clarity and noise suppression. In contrast, although TaylorSENet and GaGNet showed relatively similar performance on SI-SNR and SDR, they fell slightly behind in the MOS series scores."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "We introduce simulation tool named SonicSim, and large-scale synthetic dataset named SonicSet, designed to study speech separation and enhancement tasks involving moving sound sources. By integrating the Habitat-sim platform, we developed tool capable of simulating complex acoustic environments, supporting moving sound sources and multi-scene audio generation. Baseline 6https://huggingface.co/espnet/pengcheng_guo_wenetspeech_asr_train_asr_raw_zh_char 10 Technical report experiments demonstrate that models pre-trained on the SonicSet dataset exhibit strong generalization abilities across various public benchmark datasets and real-world recorded datasets, effectively narrowing the gap between simulation and real-world scenarios. Through continuous improvements in the simulation tool and optimization of model algorithms, future work could further advance the deployment of speech tasks in complex environments. Limitation: The realism of SonicSims audio simulation is constrained by the level of detail in 3D scene modeling. When there are gaps or incomplete structures in the imported 3D scenes, the system cannot accurately simulate the reverberation effects in the current environment."
        },
        {
            "title": "REFERENCES",
            "content": "Jont Allen and David Berkley. Image method for efficiently simulating small-room acoustics. The Journal of the Acoustical Society of America, 65(4):943950, 1979. Rohith Aralikatti, Zhenyu Tang, and Dinesh Manocha. Synthetic wave-geometric impulse responses for improved speech dereverberation. arXiv preprint arXiv:2212.05360, 2022. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In International Conference on 3D Vision, pp. 667676. IEEE, 2017. Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip Robinson, and Kristen Grauman. Soundspaces 2.0: simulation platform for visual-acoustic learning. In Advances in Neural Information Processing Systems, pp. 8896 8911, 2022a. Jingjing Chen, Qirong Mao, and Dong Liu. Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation. In Conference of the International Speech Communication Association, 2020a. Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, and Helen Meng. Fullsubnet+: ChanIn International nel attention fullsubnet with complex spectrograms for speech enhancement. Conference on Acoustics, Speech and Signal Processing, pp. 78577861. IEEE, 2022b. Jun Chen, Wei Rao, Zilin Wang, Jiuxin Lin, Zhiyong Wu, Yannan Wang, Shidong Shang, and Helen Meng. Inter-subnet: Speech enhancement with subband interaction. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 15. IEEE, 2023. Zhuo Chen, Takuya Yoshioka, Liang Lu, Tianyan Zhou, Zhong Meng, Yi Luo, Jian Wu, Xiong Xiao, and Jinyu Li. Continuous speech separation: Dataset and analysis. In International Conference on Acoustics, Speech and Signal Processing, pp. 72847288. IEEE, 2020b. Colin Cherry. Some experiments on the recognition of speech, with one and with two ears. The Journal of the Acoustical Society of America, 25(5):975979, 1953. Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge, and Emmanuel Vincent. Librimix: An open-source dataset for generalizable speech separation. arXiv preprint arXiv:2005.11262, 2020. Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. Fma: dataset for music analysis. In International Society for Music Information Retrieval Conference, 2017. Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. Pierre-Amaury Grumiaux, Srjan Kitic, Laurent Girin, and Alexandre Guerin. survey of sound source localization with deep learning methods. The Journal of the Acoustical Society of America, 152(1):107151, 2022. Xiang Hao and Xiaofei Li. Fast fullsubnet: Accelerate full-band and sub-band fusion model for single-channel speech enhancement. arXiv preprint arXiv:2212.09019, 2022. 11 Technical report Xiang Hao, Xiangdong Su, Radu Horaud, and Xiaofei Li. Fullsubnet: full-band and sub-band fusion model for real-time single-channel speech enhancement. In International Conference on Acoustics, Speech and Signal Processing, pp. 66336637. IEEE, 2021. John Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. Deep clustering: Discriminative embeddings for segmentation and separation. In International Conference on Acoustics, Speech and Signal Processing, pp. 3135. IEEE, 2016. Xiaolin Hu, Kai Li, Weiyi Zhang, Yi Luo, Jean-Marie Lemercier, and Timo Gerkmann. Speech separation using an asynchronous fully recurrent convolutional neural network. In Advances in Neural Information Processing Systems, volume 34, pp. 2250922522, 2021. Yanxin Hu, Yun Liu, Shubo Lv, Mengtao Xing, Shimin Zhang, Yihui Fu, Jian Wu, Bihong Zhang, and Lei Xie. Dccrn: Deep complex convolution recurrent network for phase-aware speech enhancement. In Conference of the International Speech Communication Association, 2020. Daniel Jarrett, Emanuel AP Habets, Mark RP Thomas, and Patrick Naylor. Rigid sphere room impulse response simulation: Algorithm and applications. The Journal of the Acoustical Society of America, 132(3):14621472, 2012. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John Hershey. Sdrhalf-baked or well done? In International Conference on Acoustics, Speech and Signal Processing, pp. 626630. IEEE, 2019. Andong Li, Shan You, Guochen Yu, Chengshi Zheng, and Xiaodong Li. Taylor, can you hear me now? taylor-unfolding framework for monaural speech enhancement. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pp. 41934200, 2022a. Andong Li, Chengshi Zheng, Guochen Yu, Juanjuan Cai, and Xiaodong Li. Filtering and refining: collaborative-style framework for single-channel speech enhancement. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:21562172, 2022b. Andong Li, Chengshi Zheng, Lu Zhang, and Xiaodong Li. Glance and gaze: collaborative learning framework for single-channel speech enhancement. Applied Acoustics, 187:108499, 2022c. Chenda Li, Lei Yang, Weiqin Wang, and Yanmin Qian. Skim: Skipping memory lstm for lowlatency real-time continuous speech separation. In International Conference on Acoustics, Speech and Signal Processing, pp. 681685. IEEE, 2022d. Kai Li and Yi Luo. On the design and training strategies for rnn-based online neural speech separation systems. In International Conference on Acoustics, Speech and Signal Processing, pp. 15. IEEE, 2023. Kai Li, Xiaolin Hu, and Yi Luo. On the use of deep mask estimation module for neural source separation systems. In Conference of the International Speech Communication Association, 2022e. Kai Li, Runxuan Yang, and Xiaolin Hu. An efficient encoder-decoder architecture with top-down attention for speech separation. In The Eleventh International Conference on Learning Representations, 2023. PC Loizou. Speech enhancement: theory and practice, 2007. Francisco Lopez-Hernandez, Rafael Perez-Jimenez, and Asuncion Santamaria. Ray-tracing algorithms for fast calculation of the channel impulse response on diffuse ir wireless indoor channels. Optical Engineering, 39(10):27752780, 2000. Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(8): 12561266, 2019. 12 Technical report Yi Luo and Jianwei Yu. Fra-rir: Fast random approximation of the image-source method. In Conference of the International Speech Communication Association, 2023a. Yi Luo and Jianwei Yu. Music source separation with band-split rnn. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:18931901, 2023b. Yi Luo, Zhuo Chen, and Takuya Yoshioka. Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation. In International Conference on Acoustics, Speech and Signal Processing, pp. 4650. IEEE, 2020. Matthew Maciejewski, Gordon Wichern, Emmett McQuinn, and Jonathan Le Roux. Whamr!: Noisy In International Conference on Acoustics, and reverberant single-channel speech separation. Speech and Signal Processing, pp. 696700. IEEE, 2020. Mishaim Malik, Muhammad Kamran Malik, Khawar Mehmood, and Imran Makhdoom. Automatic speech recognition: survey. Multimedia Tools and Applications, 80:94119457, 2021. Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. In SciPy, pp. 1824, 2015. COMSOL Multiphysics. Introduction to comsol multiphysics. COMSOL Multiphysics, Burlington, MA, accessed Feb, 9(2018):32, 1998. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus In International Conference on Acoustics, Speech and based on public domain audio books. Signal Processing, pp. 52065210. IEEE, 2015. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, volume 32, 2019. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pp. 2849228518. PMLR, 2023. Wei Rao, Yihui Fu, Yanxin Hu, Xin Xu, Yvkai Jv, Jiangyu Han, Zhongjie Jiang, Lei Xie, Yannan Wang, Shinji Watanabe, et al. Conferencingspeech challenge: Towards far-field multi-channel speech enhancement for video conferencing. In Automatic Speech Recognition and Understanding Workshop, pp. 679686. IEEE, 2021. Chandan KA Reddy, Vishak Gopal, Ross Cutler, Ebrahim Beyrami, Roger Cheng, Harishchandra Dubey, Sergiy Matusevych, Robert Aichner, Ashkan Aazami, Sebastian Braun, et al. The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results. In Conference of the International Speech Communication Association, 2020. Chandan KA Reddy, Vishak Gopal, and Ross Cutler. Dnsmos: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 64936497. IEEE, 2021. Nicolae-Catalin Ristea, Ando Saabas, Ross Cutler, Babak Naderi, Sebastian Braun, and Solomiya In International Conference on Branets. Acoustics, Speech and Signal Processing Workshops, pp. 1516. IEEE, 2024. Icassp 2024 speech signal improvement challenge. Antony Rix, John Beerends, Michael Hollier, and Andries Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In International Conference on Acoustics, Speech and Signal Processing, pp. 749752. IEEE, 2001. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 93399347, 2019. 13 Technical report Robin Scheibler, Eric Bezzam, and Ivan Dokmanic. Pyroomacoustics: python package for audio In International Conference on Acoustics, room simulation and array processing algorithms. Speech and Signal Processing, pp. 351355. IEEE, 2018. Shilpa Sharma, Punam Rattan, and Anurag Sharma. Recent developments, challenges, and future scope of voice activity detection schemesa review. Information and Communication Technology for Competitive Strategies (ICTCS 2020) Intelligent Strategies for ICT, pp. 457464, 2021. Aswin Sivaraman and Minje Kim. Efficient personalized speech enhancement through selfIEEE Journal of Selected Topics in Signal Processing, 16(6):13421356, supervised learning. 2022. Alexander Southern, Samuel Siltanen, and Lauri Savioja. Spatial room impulse responses with hybrid modeling method. In Audio Engineering Society Convention 130. Audio Engineering Society, 2011. Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. Attention is all you need in speech separation. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 2125. IEEE, 2021. Cees Taal, Richard Hendriks, Richard Heusdens, and Jesper Jensen. An algorithm for intelligibility prediction of timefrequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):21252136, 2011. Zhenyu Tang, Lianwu Chen, Bo Wu, Dong Yu, and Dinesh Manocha. Improving reverberant speech training using diffuse acoustic simulation. In International Conference on Acoustics, Speech and Signal Processing, pp. 69696973. IEEE, 2020. Efthymios Tzinis, Zhepei Wang, and Paris Smaragdis. Sudo rm-rf: Efficient networks for universal audio source separation. In IEEE 30th International Workshop on Machine Learning for Signal Processing, pp. 16. IEEE, 2020. Cassia Valentini-Botinhao, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigating rnnbased speech enhancement methods for noise-robust text-to-speech. In SSW, pp. 146152, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. Emmanuel Vincent, Remi Gribonval, and Cedric Fevotte. Performance measurement in blind audio source separation. IEEE Transactions on Audio, Speech, and Language Processing, 14(4):1462 1469, 2006. DeLiang Wang and Jitong Chen. Supervised speech separation based on deep learning: An overview. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(10):17021726, 2018. Qing Hua Wang, Teodor Ivanov, and Parham Aarabi. Acoustic robot navigation using distributed microphone arrays. Information Fusion, 5(2):131140, 2004. Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, and Shinji Watanabe. Tf-gridnet: Integrating full-and sub-band modeling for speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, et al. Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings. In International Workshop on Speech Processing in Everyday Environments. ISCA, 2020. Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. Wham!: Extending speech separation to noisy environments. In Conference of the International Speech Communication Association, 2019. Technical report Bing Yang, Changsheng Quan, Yabo Wang, Pengyu Wang, Yujie Yang, Ying Fang, Nian Shao, Hui Bu, Xin Xu, and Xiaofei Li. Realman: real-recorded and annotated microphone array dataset for dynamic speech enhancement and localization. arXiv preprint arXiv:2406.19959, 2024. Jun Yu and Chang Wen Chen. From talking head to singing head: significant enhancement for more natural human computer interaction. In International Conference on Multimedia and Expo, pp. 511516. IEEE, 2017. Shengkui Zhao and Bin Ma. Mossformer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions. In International Conference on Acoustics, Speech and Signal Processing, pp. 15. IEEE, 2023. Shengkui Zhao, Yukun Ma, Chongjia Ni, Chong Zhang, Hao Wang, Trung Hieu Nguyen, Kun Zhou, Jia Qi Yip, Dianwen Ng, and Bin Ma. Mossformer2: Combining transformer and rnnfree recurrent network for enhanced time-domain monaural speech separation. In International Conference on Acoustics, Speech and Signal Processing, pp. 1035610360. IEEE, 2024. Victor Zue, Stephanie Seneff, and James Glass. Speech database development at mit: Timit and beyond. Speech Communication, 9(4):351356, 1990."
        },
        {
            "title": "A DATASET ANALYSIS",
            "content": "The dataset statistics are shown in Table 7. SonicSet dataset offers ground-truth labels for speech separation and enhancement for supervised learning, which captures different audio samples for data synthesis in the same scene, with acoustic environments that closely resemble real-world conditions. SonicSet contains 57,596 speech moving trajectories across 90 scenes. The training set includes 57,102 trajectories, while the validation and test sets each contain 247 trajectories, covering most possible positions within indoor scenes. The dataset comprises 1,001 speakers, with 921 speakers in the training set, 40 in the validation set, and 40 in the test set, ensuring that no speaker appears in different subsets, which maintains speaker-independent training. Through our data construction process, SonicSet generates approximately 952 hours of training data, 4 hours of validation data, and 4 hours of test data. SonicSet provides researchers with high-quality and complex moving sound source dataset for speech separation and enhancement methods. Datasets Speakers Utterances Duration (h) Noise Reverb Dynamic TIMIT (1990) VoiceBank-DEMAND (2016) DNS Challenge (2020) RealMan (2024) WSJ0 (2016) Libri2Mix (2020) LibriCSS (2020b) LRS2-2Mix (2023) Speech enhancement 630 110 11k 55 191 1001 40 100 6,300 400 100k - Speech separation 28,000 56,800 1000 48, 5 44 760 81 43 232 10 50 Speech separation and enhancement WHAM! (2019) WHAMR! (2020) SonicSet (ours) 191 191 1001 28,000 28,000 57, 43 43 960 Table 7: Comparison of speech datasets and their characteristics. Speakers refer to the number of unique speakers. Utterances stands for the total number of utterances, and Duration represents the total duration of the dataset in hours. Noise, Reverb, and Dynamic indicate whether the dataset contains noisy conditions, reverberation, and dynamic acoustic environments, respectively. Technical report Figure 5: An example from the SonicSet data set. Each set of data includes three moving speaker audio files, two types of noise, and the trajectories of the sound source movement."
        },
        {
            "title": "B SONICSET DEMO",
            "content": "We provide detailed description of the composition and structure of the SonicSet dataset. Each data group consists of audio files from three mobile sound sources, two types of noise, and trajectory information regarding the movement of these sources. Specifically, each sound source audio file comprises multiple audio segments in FLAC format, with each segment associated with precise start and end times along with corresponding textual content. In addition to speech data, SonicSet includes two categories of interference: background noise and musical noise. The background noise data captures various non-speech sounds in the environment, while the musical noise data consists of music audio recorded within relatively consistent time intervals. Each data group is also accompanied by detailed information on sound source trajectories, microphone locations, and noise positions, illustrating the movement patterns of sound sources in different environments. This data 16 Technical report format, combining speech, noise, and motion information, provides valuable training data for tasks such as sound source localization (Grumiaux et al., 2022), voice activity detection (Sharma et al., 2021), and speech recognition (Malik et al., 2021). With its rich audio and trajectory information, the SonicSet dataset serves as an ideal testing platform for variety of speech processing tasks, including noisy speech recognition, sound source separation, and source tracking, thereby enhancing the performance of speech and audio-related technologies. 17 Technical report JSON file 1: The raw information corresponding to the audio in Figure 5 is stored in file in JSON format. It can be used to train voice activity detection and speech recognition. {\"source1\": { \"audio\": [ \"672-122797-0033.flac\", \"672-122797-0070.flac\", \"672-122797-0045.flac\", \"672-122797-0054.flac\", \"672-122797-0029.flac\", \"672-122797-0053.flac\" ], \"start_end_points\": [ [83071, 103631], [118366, 218686], [360049, 583409], [632894, 700894], [772607, 821407], [871263, 918543] ], \"words\": [ \"A STORY\", \"THE GOLDEN STAR OF TINSEL WAS STILL ON THE TOP OF THE TREE AND GLITTERED IN THE SUNSHINE\", \"TIME ENOUGH HAD HE TOO FOR HIS REFLECTIONS FOR DAYS AND NIGHTS PASSED ON AND NOBODY CAME UP AND WHEN AT LAST SOMEBODY DID COME IT WAS ONLY TO PUT SOME GREAT TRUNKS IN CORNER OUT OF THE WAY\", \"I KNOW NO SUCH PLACE SAID THE TREE\", \"HOW IT WILL SHINE THIS EVENING\", \"THEY WERE SO EXTREMELY CURIOUS\" ] }, \"source2\": { \"audio\": [ \"908-31957-0014.flac\", \"908-157963-0007.flac\" ], \"start_end_points\": [ [91122, 212082], [268394, 792714] ], \"words\": [ \"A RING OF AMETHYST COULD NOT WEAR HERE PLAINER TO MY SIGHT THAN THAT FIRST KISS\", \"THE LILLY OF THE VALLEY BREATHING IN THE HUMBLE GRASS ANSWERD THE LOVELY MAID AND SAID AM WATRY WEED AND AM VERY SMALL AND LOVE TO DWELL IN LOWLY VALES SO WEAK THE GILDED BUTTERFLY SCARCE PERCHES ON MY HEAD YET AM VISITED FROM HEAVEN AND HE THAT SMILES ON ALL WALKS IN THE VALLEY AND EACH MORN OVER ME SPREADS HIS HAND SAYING REJOICE THOU HUMBLE GRASS THOU NEW BORN LILY FLOWER\" ] }, \"source3\": { \"audio\": [ \"61-70968-0021.flac\", \"61-70968-0052.flac\", \"61-70968-0030.flac\", \"61-70968-0050.flac\", \"61-70970-0006.flac\", \"61-70968-0013.flac\" ], \"start_end_points\": [ [63669, 106709], 18 Technical report [127764, 170164], [245602, 336562], [408129, 497409], [578830, 616190], [754238, 825438] ], \"words\": [ \"SURELY WE CAN SUBMIT WITH GOOD GRACE\", \"BUT WHO IS THIS FELLOW PLUCKING AT YOUR SLEEVE\", \"NOW BE SILENT ON YOUR LIVES HE BEGAN BUT THE CAPTURED APPRENTICE SET UP AN INSTANT SHOUT\", \"HE MADE AN EFFORT TO HIDE HIS CONDITION FROM THEM ALL AND ROBIN FELT HIS FINGERS TIGHTEN UPON HIS ARM\", \"NEVER THAT SIR HE HAD SAID\", \"BEFORE THEM FLED THE STROLLER AND HIS THREE SONS CAPLESS AND TERRIFIED\" ] }, \"noise\": { \"audio\": [], \"start_end_points\": [ [26850, 940228] ] }, \"music\": { \"audio\": [], \"start_end_points\": [ [13482, 918705] ] } } BENCHMARK I: SPEECH SEPARATION C.1 PROBLEM DEFINITION Given an input signal yyy = (cid:80)C c=0 xxxi + nnn, yyy R1T that contains audio xxxi R1T from speakers and noise nnn R1T , the objective of speech separation is to extract each speakers speech xxxi R1T and assign it to different output channels, where denotes the length of audio. Currently, most speech separation methods adopt an encoder-separator-decoder framework (Wang & Chen, 2018). In time-domain approaches, the encoder converts yyy into high-dimensional features EEEt RN Tt through 1D convolutional layers, where and Tt denote the feature dimension and length, respectively. In time-frequency domain approaches, the encoder applies Short-Time Fourier Transform (STFT) to convert yyy into time-frequency features EEEf CF Tf , where and Tf represent the frequency and the time dimensions, respectively. Next, EEEt or EEEf is passed to the separator to obtain the feature representations of individual speakers, EEEt,i RN Tt or EEEf,i CF Tf . Finally, the decoder reconstructs the target waveform xxxi from EEEt,i or EEEf,i using transposed convolutional layers or inverse STFT, thereby achieving the final separated speech sources. C.2 BENCHMARK MODELS Conv-TasNet (Luo & Mesgarani, 2019): As the first time-domain speech separation model, it employs dilated convolutional network, surpassing traditional frequency-domain methods in separation performance. This model processes audio directly in the time domain, enhancing clarity compared to frequency-based methods. DPRNN (Luo et al., 2020): time-domain model based on bidirectional LSTM (BLSTM), it introduced dual-path architecture for intraand inter-block modeling, significantly improving the ability to capture long-term dependencies, laying the foundation for subsequent speech separation 19 Technical report models. The dual-path approach enhances the models resolution of speaker characteristics over longer time sequences, optimizing it for complex auditory scenes. DPTNet (Chen et al., 2020a): Built on DPRNN, it incorporates multi-head attention mechanism, further enhancing the models ability to capture long-term contextual information and improving separation performance in complex speech scenarios. As one of the earliest works to introduce the concept of attention into the speech separation, it explores the application of attention in learning complex contextual correlations under the long-term characteristics of the speech domain. SuDORM-RF (Tzinis et al., 2020): time-domain speech separation model formed by stacking multiple UNet networks, it uses refinement strategy to progressively optimize separation performance, improving the models ability to handle complex signals. This model uses UNet to model different resolutions, and each additional UNet layer refines the separation detail, offering better clarity and fidelity in the final audio output. A-FRCNN (Hu et al., 2021): Building on SuDORM-RF, it introduces cross-layer connections and parameter-sharing mechanisms, improving the models capacity and parameter efficiency, leading to more precise speech separation. As an asynchronous update scheme, its top-down mechanism enables the model to better model multi-scale temporal information and achieve better results with fewer parameters. SKiM (Li et al., 2022d): Incorporates the reuse of hidden states and cell states in DPRNNs interblock modeling, strengthening contextual modeling ability and enabling the model to better handle long-term dependencies. In addition, SKiM also maintains the causal modeling ability of the model well, enabling it to handle real-time tasks with low latency and good performance. TDANet (Li et al., 2023): Simplifies redundant connections in A-FRCNN and introduces topdown attention mechanism to selectively extract multi-level acoustic features, further improving processing efficiency. The introduction of attention module further enhances the models ability to extract complex correlations of temporal information at different scales. BSRNN (Luo & Yu, 2023b): Proposes frequency-band splitting method, modeling both within frequency bands and across the time dimension using BLSTM, marking the first application of band-splitting methods in speech separation. This unique approach allows for more granular control over the frequency components, with better performance in tasks that rely more on fine frequency features such as music separation tasks. TF-GridNet (Wang et al., 2023): Extends DPRNN by modeling in both the time and frequency dimensions and integrates information from both dimensions using multi-head attention module, leading to more refined and comprehensive speech separation. Compared to previous attention based speech separation works, this work models in both time and frequency domains, and reduces the length load of the attention module, making it more conducive to capturing contextual relationships. Mossformer (Zhao & Ma, 2023): Combines convolution and self-attention in gated, single-head Transformer architecture, utilizing full self-attention within local blocks and linearized, low-cost self-attention mechanism for global modeling, significantly improving separation performance. The model also utilizes an attention gathering mechanism with simplified single head self attention. The model achieved good performance by integrating multiple modules. Mossformer2 (Zhao et al., 2024): Enhances Mossformer by introducing feed-forward sequential memory network (FSMN) recurrent module, improving long-term dependency modeling and further boosting speech separation effectiveness. This FSMN blocks are mainly enhanced by using gated convolution units (GCUs) and dense connections. In addition, bottleneck layers and output layers have been added to control the flow of information. Based on the above content, the model integrates the loop module into the MossFormer framework, providing the ability to model remote, coarse scale dependencies, and fine scale loop patterns. C.3 TRAINING OBJECT For all benchmark models in the speech separation, we employed the permutation invariant training (PIT) method (Hershey et al., 2016) to select the best output permutation PM (where PM is the set of all permutations) to minimize the negative SNR adaptively. Specifically, SNR loss LSNR 20 Technical report can be defined as negative SNR: LSNR(xxxi, xxxi) = 10 log10 (cid:18) xxxi2 xxxi xxxi2 (cid:19) , (1) In this way, the final loss function of the PIT method under the SNR optimization objective can be expressed as: LPIT-SNR = min PM (cid:88) i=1 LSNR(xxxi, xxxi) (2) C.4 IMPLEMENTATION DETAILS During training, since all model hyperparameters were originally configured for the WSJ0-2Mix dataset (Hershey et al., 2016) and suited for audio with an 8 kHz sampling rate, we doubled the window length and window shift of the encoder and decoder to adapt to the 16 kHz sampling rate datasets. Aside from this adjustment, all other hyperparameters remained unchanged. For training, we randomly sampled 3-second audio clips from the training set. The batch size was set to 1, and the Adam optimizer (Kingma, 2014) was used with an initial learning rate of 1 103. The learning rate was halved whenever the validation loss did not decrease for five consecutive epochs. We applied gradient clipping to limit the maximum L2 norm of the gradients to 5. The training ran for maximum of 500 epochs, with early stopping applied if the validation loss did not improve for 10 consecutive epochs. For each benchmark model, we selected the best model based on its performance on the validation set for testing. In the SonicSet evaluation, we used the full test set and employed 6-second inference window with 3-second sliding window to process long audio sequences. For the LRS2-2Mix dataset, we used 2-second audio segments for both training and testing. For the Libri2Mix dataset, we trained and tested the models using 3-second audio segments. All experiments were conducted on server equipped with 8 NVIDIA 4090 GPUs. C.5 REAL-WORLD DATASET DETAILS Figure 6: Recording audio using realistic scene images. These images show the layout of the actual physical environment used to record audio. In the speech separation experiments, we used SonicSet, public datasets, and real-world datasets. Each mixed audio contained two different speakers, with sampling rate of 16 kHz. The baseline models pre-trained on the SonicSet training set were trained using dynamic augmentation strategy. Specifically, during training, we randomly selected set of data, and two speakers were randomly chosen from three available audio tracks for mixing. Depending on the task requirements, either environmental noise or musical noise was then mixed accordingly. Finally, 3-second segment was randomly extracted from the 60-second audio for model training. To evaluate the gap between the SonicSet dataset and real-world conditions, as well as the models transferability to real-world scenarios, we collected small-scale real-world moving sound source 21 Technical report dataset and repeated the relevant experiments. First, we randomly selected 30 audio samples from 10 scenes in the SonicSet validation set, totaling 5 hours of audio. All data was clean, without reverberation or noise. During recording, one participant used the 2023 MacBook Pros speakers to play audio while moving around randomly within the same scene to obtain the ground-truth audio. In addition, we used the same steps to play environmental and music noise from fixed positions to obtain noise data. The audio and noise were recorded using Logitech Blue Yeti Nano omnidirectional microphone fixed in position, with recording sample rate of 16 kHz and 32-bit depth. After recording, we clipped the audio and noise based on the recorded start and stop positions to ensure alignment with the original files. We then recorded another real-world dataset in 10 similar real-world scenes using the corresponding original audio. Finally, we mixed the data using the same method as SonicSet to construct dataset for testing model performance. The layout of the real-world scenes is illustrated in Figure 6. To evaluate the generalization performance of the models trained on SonicSet, we used the commonly used speech separation test datasets LibriMix (Cosentino et al., 2020) and LRS2 (Li et al., 2023). For the LibriMix dataset, we used the two-speaker mixed test set with 16 kHz sampling rate. For the LRS2 dataset, we used the test set consistent with the (Li et al., 2023). C.6 EVALUATION METRICS DETAILS Scale-invariant signal-to-noise Ratio (SI-SNR) (Le Roux et al., 2019) and Signal-to-distortion ratio (SDR) (Vincent et al., 2006) are standard metrics for evaluating the ratio of speech to noise and distortion, effectively measuring models ability to suppress noise while preserving the original signal. NB-PESQ and WB-PESQ (Rix et al., 2001) are used to assess the perceptual quality of wideband signals, respectively, based on human auditory models, predicting subjective speech quality, which is particularly suitable for evaluating signals processed through separation and enhancement. Additionally, STOI (Taal et al., 2011) measures speech intelligibility in noisy environments, helping to assess the loss of speech clarity in complex conditions. Furthermore, Word Error Rate (WER), standard in automatic speech recognition (ASR) systems, measures the error rate between the recognized words and the ground truth, reflecting the intelligibility of the speech signal. In this study, we use Whisper-medium-en (Radford et al., 2023) as the ASR model to recognize the content of separated audio. By employing these multi-dimensional evaluation metrics, we can comprehensively assess the models performance in real-world applications. BENCHMARK II: SPEECH ENHANCEMENT D.1 PROBLEM DEFINITION Typically, the speech enhancement problem can be expressed as follows: given an observed noisy speech signal sss R1T , where sss = xxx + nnn, with xxx R1T being the target speech signal and nnn R1T representing background noise, the goal of speech enhancement is to recover an estimate xxx of the target speech signal from sss. Specifically, the process begins by applying the STFT to map the time-domain signal sss into time-frequency domain features EEE CF . Then, speech enhancement network extracts the target speech from the noisy mixed signal, producing estimated time-frequency domain features EEE = (EEE). Finally, the Inverse STFT is applied to reconstruct EEE CF back into the time-domain waveform xxx R1T , thus completing the recovery of the target speech signal. D.2 BENCHMARK MODELS DCCRN (Hu et al., 2020) is speech enhancement model designed for complex-domain processing. It operates in the frequency domain by handling both the real and imaginary components of speech signals, leveraging the characteristics of complex signals. Additionally, DCCRN employs recurrent neural network to capture temporal information, making it particularly effective in processing speech in complex noisy environments. The models innovation lies in its combination of complex22 Technical report domain processing and temporal feature extraction, providing robust support for improving speech clarity and intelligibility. Fullband (Hao et al., 2021) processes the entire frequency band of the speech signal, typically operating directly on either the full spectrum in the time or frequency domain. These approaches are particularly well-suited for speech enhancement tasks that address full-band noise. Leveraging complete frequency information effectively improves speech intelligibility and quality, making them strong choice for applications in diverse noisy environments. Their comprehensive processing capability ensures that both spectral and temporal features are preserved, leading to more natural and clear enhanced speech outputs. FullSubNet (Hao et al., 2021) is speech enhancement model that effectively combines full-band and sub-band information. It begins by extracting global features at the full-band level, subsequently refining these features at the sub-band level. This dual-layer processing approach enhances the overall performance of speech enhancement, allowing for more precise noise reduction and improved clarity. By integrating information from both frequency scales, FullSubNet successfully addresses various noise conditions, leading to more intelligible and natural-sounding speech outputs. Fast-FullSubNet (Hao & Li, 2022) is an accelerated version of FullSubNet, specifically optimized for real-time speech enhancement. This model reduces computational complexity and improves inference speed, allowing faster speech processing while maintaining high enhancement quality. By streamlining the architecture and leveraging efficient algorithms, Fast-FullSubNet addresses the demands of real-time applications, making it suitable for scenarios where quick responses are critical. FullSubNet+ (Chen et al., 2022b) is an improved version of FullSubNet that further optimizes the model architecture and parameters for more efficient handling of sub-band information. It incorporates advanced techniques, such as channel attention mechanisms, to boost performance, particularly in complex noise environments. By refining the processing strategies and utilizing complex spectrograms, FullSubNet+ achieves superior noise reduction and clarity compared to its predecessor. TaylorSENet (Li et al., 2022a) is speech enhancement model that utilizes the Taylor series expansion to represent speech signal features. By expanding these features as Taylor series, the model captures subtle variations in the signal, which enhances its robustness when processing complex audio signals. This innovative approach allows TaylorSENet to improve the overall quality and intelligibility of speech in challenging acoustic environments. GaGNet (Li et al., 2022c) introduces novel approach that combines gating mechanisms and guided learning to enhance speech separation tasks. The gating mechanism plays crucial role in controlling the flow of information within the model, allowing for more effective differentiation between target speech and background noise. Simultaneously, the guided learning mechanism leverages external prior knowledge to assist the model in improving its separation capabilities. This dual approach not only enhances the models performance in challenging acoustic environments but also facilitates better intelligibility and clarity of the separated speech, making GaGNet significant advancement in the field of speech processing. G2Net (Li et al., 2022b) enhances computational efficiency and performance by incorporating both gating mechanisms and group convolution. The use of group convolution effectively reduces the number of parameters in the model, leading to lower computational costs. Meanwhile, the gating mechanism plays critical role in helping the model selectively process and prioritize key speech features. This combination not only optimizes the performance model but also makes it suitable for real-time speech enhancement tasks, ensuring that it delivers high-quality outputs even in demanding acoustic environments. Inter-SubNet (Chen et al., 2023) is speech enhancement model that focuses on interaction processing among sub-bands. By decomposing sub-band features, it facilitates information exchange between different sub-bands, effectively addressing inter-band dependencies. This innovative approach enhances the models ability to capture the complex relationships between sub-bands, leading to improved speech enhancement outcomes. Through this interaction, Inter-SubNet significantly boosts the quality and intelligibility of the enhanced speech, making it particularly effective in challenging acoustic environments where traditional methods may struggle. 23 Technical report D.3 TRAINING OBJECT For all benchmark models used in the speech enhancement tasks, we applied various objective functions as outlined in the original papers to optimize model performance. Specifically, the SI-SNR loss function was employed in the DCCRN model (Hu et al., 2020). For the Fullband Hao et al. (2021), FullSubNet Hao et al. (2021), FullSubNet+ Chen et al. (2022b), FullSubNet-Fast Hao & Li (2022), and Inter-SubNet Chen et al. (2023) models, we utilized the complex ratio mask (cRM) error in the frequency domain as the core loss metric. By computing the mean square error (MSE) between the complex mask of the enhanced speech and the ideal mask, we can accurately quantify the models performance in the time-frequency domain. This method not only restores the magnitude of the speech signal but also effectively preserves its phase information. The loss function is expressed as: LFullband = MSE( MMM cRM, MMM cIRM), (3) where MMM cRM CF complex ratio mask. is the estimated complex ratio mask, and MMM cIRM CF is the ideal For the TaylorSENet (Li et al., 2022a) and GaGNet (Li et al., 2022c) models, we used an Euclidean loss function based on complex magnitude. This loss function not only considers the differences in the complex space but also incorporates magnitude information, enabling the model to capture subtle changes in the input speech signal more precisely. The objective function is given as: LTaylorSENet = α Lc + (1 α) Lm, (4) where α is the weighting factor that controls the trade-off between the complex error Lc and the magnitude error Lm. Lc is the loss of the complex part, which is defined as: Lc(xxx, xxx) = xxxr xxxr2 + xxxi xxxi2 , where xxxr and xxxi represent the real and imaginary parts of the predicted STFT, respectively, and xxxr and xxxi are the real and imaginary parts of the ground-truth STFT. Lm is the loss of the amplitude part, which is defined as: (5) Lm(xxx, xxx) = (xxx xxx)2 . (6) In G2Net (Li et al., 2022b), we employed the stagewise complex magnitude Euclidean loss. This loss function applies different weights to the estimated signals at various stages, allowing the model to converge quickly in the early stages while fine-tuning the output signal quality in the later stages. The formula is as follows: LG2Net = (cid:88) i=1 αi (Lc(xxxi, xxx) + Lm(xxxi, xxx)) , (7) where αi is the weight for each stage, and is the number of stages in the model. D.4 IMPLEMENTATION DETAILS The hyperparameter settings for all baseline models remain consistent with the original papers. We trained the models using the PyTorch framework (Paszke et al., 2019), with the Adam optimizer (Kingma, 2014) and an initial learning rate of 0.001. If the validation loss does not decrease for five consecutive epochs, the learning rate is automatically adjusted with decay factor of 0.5. Additionally, all audio samples are processed at 16 kHz sampling rate. The final model selection follows an early stopping strategy, where training stops if there is no improvement in validation loss for 10 consecutive epochs. During the testing phase, we used the same inference strategy as in the speech separation task to handle long audio samples. All experiments were conducted on server equipped with 8 NVIDIA 4090 GPUs."
        }
    ],
    "affiliations": [
        "Chinese Institute for Brain Research (CIBR), Beijing 100010, China",
        "Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China",
        "National Institute of Informatics, Tokyo, Japan",
        "Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China"
    ]
}