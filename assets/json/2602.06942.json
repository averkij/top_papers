{
    "paper_title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
    "authors": [
        "Duygu Altinok"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 2 4 9 6 0 . 2 0 6 2 : r Cambridge Default Journal (), , 146 Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of DataVocabularyMorphology Interplay Duygu Altinok* Independent Researcher, Berlin, Germany *Corresponding author. Email: duygu@turkish-nlp-suite.com Abstract Tokenization is pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes, but they typically (i) vary vocabulary without systematically controlling the tokenizers training corpus, (ii) provide limited intrinsic diagnostics of segmentation quality, and (iii) evaluate narrow slice of downstream tasks. We present the first and only comprehensive, principled study of Turkish subword tokenization subwords manifest that jointly varies vocabulary size and tokenizer training corpus size (datavocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology-level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro-F1 over gold morpheme boundaries, decoupled lemma atomicity vssurface boundary hits, over-/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) systematic investigation of the vocabularycorpussuccess triad for Turkish, including larger data regimes than prior work; (ii) unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) extensive, controlled comparisons that identify when character-level and morphology-level tokenization pay off; and (iv) full open-source release of evaluation code, tokenizer training pipelines, and interim Transformer checkpoints for reproducibility. As the first and only work of its kind, this subwords manifest delivers actionable, prescriptive guidance for building effective tokenizers in MRLs and establishes reproducible foundation for future research and deployment."
        },
        {
            "title": "Introduction",
            "content": "1. Since the advent of Transformer architecturesfrom encoder-only models like BERT (Devlin et al. 2019) to decoder-only large language models(Minaee et al. 2024), tokenizers have drawn sustained attention, especially their underlying algorithms. For non-English and, in particular, morphologically rich languages (MRLs), tokenization becomes even more consequential: how vocabularies are compressed, how subwords align with linguistic units (morphemes), and how compression choices affect downstream task performance are central design questions. Tokenization is the firstand often most consequentialinterface between raw text and neural language models. Its design determines how linguistic structure is exposed to the model, how parameters are allocated, and how efficiently sequences are represented. For MRLs such as Turkish, where productive agglutination creates long and sparse surface forms (Oflazer 1994), tokenization is not merely preprocessing choice but core modeling decision. Word-level tokenization explodes the vocabulary and invites out-of-vocabulary failures; characteror byte-level tokenization lengthens sequences and obscures morpheme boundaries; and off-the-shelf subword tokenizers (e.g. BPE"
        },
        {
            "title": "Duygu Altinok",
            "content": "(Sennrich, Haddow, and Birch 2016), WordPiece (Devlin et al. 2019), Unigram (Kudo 2018)) often fragment stems or smear affixes, weakening the models access to syntactic and morphological cues. In Turkish specifically, the central practical question is how to represent and leverage morphology. Naturally Transformer type tokenizers such as WordPiece and BPE attracted attention from research side. Recent work has examined aspects of this problem in Turkish: suffix-preserving tokenizers have shown modest, consistent gains on select tasks; RoBERTa-scale comparisons have tied vocabulary size to downstream performance; and WordPiece/BPE have remained strong baselines. Yet, these studies largely vary vocabulary size without systematically controlling or scaling the tokenizers training corpus, provide limited intrinsic diagnostics of segmentation quality, and evaluate narrower slice of tasksoften omitting morphologyand syntax-sensitive settings where boundary fidelity matters most. As result, practitioners lack prescriptive guidance on when to favor larger vocabularies, morphology-aware tokenizers, or byte/character regimes, and how tokenizer training data interacts with these choices. Motivated by these gaps, we argue that Turkish subword modeling and morphological structure have not been dissected in sufficient depth. This paper advances tokenization for Turkish from ad hoc exploration to principled design. We present the first comprehensive study that jointly varies vocabulary size and tokenizer training corpus size (datavocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology-level, and character baselines), and evaluates across broad suite of downstream tasks spanning semantic (NLI, STS, NER, sentiment analysis), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce morphology-aware diagnostic toolkit that moves beyond coarse aggregates (e.g., fertility) to boundary-level micro/macro-F1 over gold morpheme boundaries, decoupled lemma atomicity etcsurface boundary hits, over-/undersegmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) systematic investigation of the vocabularycorpussuccess triad for Turkish tokenization, including larger data regimes than prior work. (ii) unified, morphology-aware evaluation framework that links intrinsic segmentation diagnostics to extrinsic task outcomes, enabling causal interpretation rather than post hoc correlation. (iii) Extensive, controlled comparisons across tokenizer families and parameter allocations, including conditions where character-level tokenization is competitive (e.g., NER) and when morphology-level tokenization pays off. (iv) Open-source release of all evaluation code, tokenizer training pipelines, and interim Transformer checkpoints to ensure full reproducibility and to facilitate further research and deployment in Turkish NLP. Positioned as manifest in Turkish subwords, our study is the first and only of its kind to integrate large-scale tokenizer data sweeps, fine-grained morphological diagnostics, and the widest coverage of morphologyand syntax-sensitive tasks. We transform fragmented observations into actionable, prescriptive rules for building tokenizers that actually work for morphologically rich languagesgrounded in evidence, reproducible by design, and immediately useful to both researchers and practitioners. We offer all our work freely, all evaluation and training scripts under our Github repository 1 and Transformer models under our Hugging Face repository2. 2. Related Work In this section, we present works directly related to our study and compare them to our contributions. (Erkaya and Güngör 2023) investigates how subword tokenization interacts with Turkish morphology and whether morphology-aware tokenization improves modeling and downstream tasks. The study compares BPE, WordPiece, and Unigram across corpus and vocabulary sizes on Turkish 1. https://github.com/turkish-nlp-suite/Turkish-subwords-research 2. https://huggingface.co/collections/turkish-nlp-suite/turkish-subwords-research"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "3 data (OSCAR Turkish split (Ortiz Suarez, Romary, and Sagot 2020)), and introduces suffixpreserving tokenizer (morphosubwords) that keeps affixes atomic while letting roots be learned. With ELECTRA (Clark et al. 2020) pretraining, morphosubwords reduces pseudo-perplexity and delivers small but consistent gains on POS, NER, QA, and sentiment versus WordPiece baseline, with faster convergence. Intrinsically, they report trends with data and vocabulary size (fertility increases, average token length decreases, single-word rate decreases, morphology-compatibility increases, and diminishing returns at larger scales). Their morphology metrics include fertility, average token length, single-word token rate, word-level morphology-compatible segmentation, suffix precision/recall, and root tokens rate. Common concepts with our work: both target Turkish MRL tokenization; both analyze corpusand vocabulary-size effects; both compare standard subword tokenizers to morphology-aware variant; both evaluate on POS, NER, QA, and sentiment; and both use morphology-sensitive metrics alongside intrinsic/extrinsic evaluation. Where we differ: our evaluation is more granular and diagnostic (boundary-level micro/macro-F1 over all morpheme boundaries; decoupled lemma atomicity vs. surface boundary hits; over-/under-segmentation indices; CER/WER sequence edits; continuation rate; affix-type coverage and token-level atomicity). We empirically connect vocabulary size and tokenizer training corpus size to downstream success (not just intrinsic metrics), extend to larger-data regimes (e.g., 80 GB), and include pre-transformer analyses and interpretability diagnostics, yielding broader and more explanatory treatment. (Toraman et al. 2023) analyze tokenization granularity for Turkish by pretraining RoBERTamedium (Zhuang et al. 2021) on OSCAR-TR with five tokenizers (character, BPE, WordPiece, morphological, word) and evaluating six downstream tasks (news, hate speech, sentiment, NER, STS, NLI), while sweeping vocabulary size via embedding-parameter allocation. Results show WordPiece/BPE strongest overall, morphology-level tokenizer competitive but slightly behind, word-level hurt by UNKs, and character-level underperforming at this scale. Increasing vocabulary size yields monotonic gains with quicker saturation for BPE/WordPiece and larger relative gains for morphology/word-level; they recommend allocating 20% of parameters to embeddings for de facto tokenizers and 40% for morphology/word-level. Similar to our work, (Toraman et al. 2023) (i) targets Turkish and agglutinative morphology; (ii) compares de facto subword tokenizers against morphology-level alternative; and (iii) systematically studies vocabulary size and ties it to downstream performance across multiple tasks. Where we go further, our study (i) adds syntactic evaluations (POS, dependency parsing) that are particularly sensitive to morpheme boundaries; (ii) provides explainability via rich morphology-aware metrics (boundary micro/macro-F1 over gold morpheme boundaries, lemma boundary hits and lemma single-token rates, over-/under-segmentation indices, CER/WER, continuation rate, affix coverage/atomicity) rather than only extrinsic scores; (iii) examines the vocabularycorpussuccess triad by varying tokenizer training corpus size (including larger data regimes), which (Toraman et al. 2023) does not; and (iv) offers nuanced view on character/byte models (e.g., character-level can be competitive on NER under some settings), whereas (Toraman et al. 2023) reports broadly negative conclusion at their scale. (Kaya and Tantuğ 2024) study tokenization granularity for Turkish by pretraining Turkish BERT variants with WordPiece tokenizers at multiple vocabulary sizes (32k256k) on the BERTurk corpus (Schweter 2020) and evaluating on NER, QA, and sentiment. They also test normalization and simple morphology-injection schemes (tags and inflectional groups). Their results show that larger vocabularies steadily improve tokenization success for token-level tasks (NER, QA), with saturation around 128k256k, while sentimentdriven by the [CLS] representationdoes not benefit from larger vocabularies. Morphology-tag tokenization increases sequence length and generally harms downstream performance under 512-token limit. Overall, they confirm task-dependent trade-off between vocabulary size and granularity for Turkish."
        },
        {
            "title": "Duygu Altinok",
            "content": "This study is similar to our study in spirit, but our study is substantially broader and more diagnostic. First, we expand task coverage beyond NER/QA/SA to include NLI and syntax-sensitive evaluations (POS, dependency parsing), which are critical for assessing morpheme-boundary fidelity. Second, we sweep vocabulary sizes much more finely, including very small regimes (e.g., 18k) where segmentation behavior and sequence-length pressure are most revealing in agglutinative languages, and we compare multiple tokenizer families (WordPiece, BPE, morphology-level, and character/byte baselines) under matched parameter budgets. Third, unlike (Kaya and Tantuğ 2024)who train tokenizers on fixed large corpuswe explicitly vary tokenizer training corpus size and domain to study datavocabulary coupling. Finally, we add intrinsic, morphology-aware diagnostics (boundary F1, lemma boundary hits, affix coverage/atomicity, over-/under-segmentation indices, continuation rates) and analyze parameter-allocation trade-offs, yielding prescriptive guidance on when larger vocabularies or morphology-level tokenizers pay off and why. In summary, earlier studies illuminate pieces of the puzzletokenizer choice, vocabulary size, or individual tasksbut stop short of holistic, data-coupled, diagnostic account for Turkish. Our work is the first and only of its kind, manifest in Turkish subwords: we systematically couple vocabulary with tokenizer training data across larger regimes, evaluate the broadest suite of semantic, syntactic, and morphology-sensitive tasks, and contribute the most comprehensive morphologyaware diagnostics to date. This manifest transforms scattered observations into actionable, prescriptive rules for building effective tokenizers in morphologically rich languages. 3. Datasets We evaluate Turkish subword strategies across diverse corpora and tasks that map to our three axes: data scale (for tokenizer training), vocabulary size (via controlled tokenizer variants), and morphology-aware evaluation (via syntactic and morphological benchmarks). This section details the datasets used for benchmarking and, where relevant, how they interface with our diagnostics."
        },
        {
            "title": "3.1 Benchmarking\nWe adopt a broad evaluation suite covering semantic, named entity recognition, and syntax/morphology-\nsensitive tasks:",
            "content": "Semantic (TrGLUE) We use the newly released TrGLUE (Altinok 2025; Wang et al. 2018) benchmark for Turkish, which aggregates multiple semantic tasks to probe representation quality across inference, similarity, and classification. TrGLUE provides standardized splits and metrics, enabling consistent model comparison under matched conditions. We evaluate model success on focused yet linguistically diverse suite chosen to probe morphologysubword interactions without excessive redundancy. For syntax, we include POS tagging and NER, which are directly sensitive to morpheme segmentation, affix boundaries, and lemma integrity. TrCoLA (Warstadt, Singh, and Bowman 2018; Altinok 2025) serves as complementary stress test for morphosyntactic well-formedness, capturing acceptability phenomena beyond token-level labels. For sentence-level semantics, TrMNLI (Williams, Nangia, and Bowman 2018; Altinok 2025) provides broad inference coverage across genres, TrMRPC (Dolan and Brockett 2005; Altinok 2025) tests paraphrase recognition under varying lexical overlap, TrSST-2 (Socher et al. 2013; Altinok 2025) captures polarity composition in single sentences, and STS-B (Cer et al. 2017; Altinok 2025) measures graded semantic similarity with continuous correlations. This mix balances syntax-heavy supervision with semantic understanding and uses metric diversity (accuracy/F1, MCC, correlation) to reveal how subword choices trade off sequence length, OOV handling, and morphological fidelity across task families. Table 1 gives the sizes and evaluation metrics per evaluation set. Table 1. Sizes of TrGLUE datasets, task type, size, eval metric and BERTurk performance on them."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "5 Task acceptability Size 9.9K Eval metric BERTurk success Matthews corr. Dataset TrCoLA TrMNLI TrMRPC TrSST-2 NLI 202K matched/mismatched acc. paraphrase 5.18K sentiment 78K acc./F1 acc./F1 TrSTS-B sentence similarity 3.06K Pearson/Spearman corr. 42 87.9/90.8 74.4/72.7 87.4/91.3 71.3/69. For brevity, we omit the Trprefix in the remainder of the paper (e.g., TrCoLA CoLA, TrMRPC MRPC) and refer to the Turkish variants by their task names. Named Entity Recognition (NER) We evaluate on the Turkish WikiNER dataset introduced by Altinok (2023a). The corpus uses 19 entity tags, covering traditional categories (e.g., PERSON, LOC) and finer-grained types (e.g., TITLE, EVENT, TIME). The dataset comprises approximately 20k sentences, with 18k for training and 1k each for development and test. As reference point, BERTurk (Schweter 2020) baseline attains an F1-score of 0.77 on the official test split. Syntax and Morphology (BOUN Treebank) For POS tagging, dependency parsing (Marneffe et al. 2021), and morphology-sensitive evaluation, we use the BOUN Turkish Treebank (Marşan et al. 2022) in UD format. The training set contains roughly 7.8k sentences, with development and test sets of about 1k sentences each. The treebank provides rich Turkish morphological annotation, enabling fine-grained diagnostics beyond POS and dependencies. As baseline, we evaluate BERTurk-based model on BOUN across tasks. The model achieves 92.63 UPOS accuracy, 81.51 UAS, and 74.59 LAS. For morphology, overall micro-accuracy is 30.76, with substantial variation across features: higher accuracies for Reflex (56.20), Typo (50.81), VerbForm (49.12), Abbr (46.57), and Number[psor] (43.48); and lower performance for Voice (2.01), Mood (6.54), Aspect (11.41), Case (11.96), Tense (21.49), and PronType (15.35). These results suggest that while the baseline captures several orthographic and inflectional cues, categories with finer-grained semantics and skewed label distributions remain challengingunderscoring the value of morphology-aware tokenization and diagnostics."
        },
        {
            "title": "3.2 Morphological segmentation\nWe curated a Turkish morphology evaluation set compiled specifically for tokenizer assessment. Each\ninstance provides a surface word form alongside its gold morphological analysis: a lemma (base form)\nand a “+”-separated suffix sequence reflecting the inflectional or derivational chain. The collection is\norganized into five validation-only subsets to probe different usage regimes: Çekimli (general suffixed\nwords), Common Nouns (suffixed nouns), Common Verbs (suffixed verbs), Lemma (lemma-only\nforms for lemma integrity), and Common Lemmas (frequent lemmas). Together, these splits cover a\nbroad range of productive suffixation patterns in Turkish, enabling evaluation of whether subword\nsegmentations align with morpheme boundaries and preserve lemma integrity. We collected analyses\nstrings from a previous work, the collection of analyses called Turkish morph dictionaries (Altinok\n2023b).\nThe dataset is distributed as JSON Lines (one analysis per line) with fields word (surface), lemma, and\nsuffixes (a “+”-joined string; empty for lemma-only items). It emphasizes clean morphological\nstructure over sentence context, focusing evaluation on subword behavior at the word level. This\nmakes it well-suited for comparing tokenizers (e.g., BPE, WordPiece, Unigram) by boundary",
            "content": ""
        },
        {
            "title": "Duygu Altinok",
            "content": "precision/recall/F1, subwords-per-word, and lemma atomicity, and for diagnosing overand undersegmentation on nouns and verbs. The resource is intended for test-only use; no training split is provided. word: kitaplarımızda lemma: kitap suffixes: lar+ımız+da word: evlerden lemma: ev suffixes: ler+den word: koşuyordum lemma: koş suffixes: uyor+du+m word: güzelleştirmek lemma: güzel suffixes: leş+tir+mek word: bilgisayarcılardan lemma: bilgisayar suffixes: cı+lar+dan word: gelmeyecekti lemma: gel suffixes: me+yecek+ti word: çocuk lemma: çocuk suffixes: word: çalıştırdık lemma: çalış suffixes: tır+dı+k This dataset can be found on its Hugging Face repository3. 4. Tokenization Metrics 4.1 Tokenization Granularity and Fragmentation Metrics In this subsection we formalize two complementary metricsfertility and token continuation ratethat together characterize how subword tokenizer fragments text. These measures are model-agnostic yet diagnostic for downstream behavior: they govern pretraining sequence lengths, the visibility of morpheme boundaries, and the trade-off between compression (short sequences) and compositionality (interpretable subword structure)."
        },
        {
            "title": "4.1.1\nFertility is basically the average number of subwords per word. Let the corpus contain words {wi\nand let t(wi",
            "content": ") be the multiset of subword tokens for wi . Define i=1, }N Fertility = 1 N (cid:88) (cid:12) (cid:12)t(wi )(cid:12) (cid:12). i=1 Lower values indicate stronger compression (more words realized as single tokens); higher values indicate greater fragmentation. In morphologically rich languages, very high fertility often reflects near character-level behavior; very low fertility can reflect whole-word memorization that may obscure productive morphology."
        },
        {
            "title": "4.1.2 Token continuation rate (cross-token span frequency)\nMany subword schemes mark continuations (e.g., WordPiece ##). Let {uj\nand ⊮cont(uj\nword). Define",
            "content": ") indicate whether uj j=1 be the subword stream }M is continuation (i.e., not the first subword aligned to its source Continuation rate = 1 (cid:88) j=1 cont(uj ). High continuation rates imply long intra-word chains; low rates indicate that most tokens start new spans, consistent with whole-word coverage or morpheme-sized chunks. Fertility captures sequence-length inflation; continuation rate captures the shape of segmentations within words. 3. https://huggingface.co/datasets/turkish-nlp-suite/turkish-morph-analysis Heres an example: Morpheme-aligned mid granularity"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "7 Sentence: Evlerimizden ayrıldık ve Ankaraya döndük. evlerimizden [ev, ##ler, ##imiz, ##den] (4) ayrıldık [ayrıl, ##dık] (2) Ankaraya [Ankara, ##, ##ya] (3) döndük [dön, ##dük] (2) Fertility: (4 + 2 + 1 + 3 + 2 + 1)/6 = 13/6 2.17. Continuation rate: continuations are ##ler, ##imiz, ##den, ##dık, ##dük (5) out of total 13 subwords (period excluded), so 5/13 0.38. ve [ve] (1) . [.] (1)"
        },
        {
            "title": "Interpreting the metric pair",
            "content": "4.1.3 - Moderate fertility with moderate continuation rate often signals morpheme-aligned segmentations. - Very high fertility with very high continuation rate indicates over-fragmentation (near characterlevel). - Very low fertility with very low continuation rate suggests whole-word memorization and potential loss of compositionality."
        },
        {
            "title": "4.2.1 Setup and Notation\nLet the dataset contain N surface words. For item i ∈ {1, . . . , N}:",
            "content": "(i) with character length The surface string is The gold analysis consists of lemma ℓ(i) and gold morpheme sequence (i). where (i) (i) (i) 1 = ℓ(i) and 2 , . . . , ki (i) to subwords tokenizer maps are suffixes. (i) 1:ni = [t (i) (i) 1 , . . . , ni ]. (i) 1:ki = [m (i) (i) 1 , . . . , ki ], We interpret subword ends as predicted morpheme boundaries and compare them against gold morpheme ends. Using 1-indexed character offsets measured from the start of (i): (i) gold = (cid:88) (cid:12) (i) (cid:12) (cid:12)m (cid:12) (cid:12) (cid:12) : = 1, . . . , ki , j=1 (cid:88) j= (i) pred = (cid:12) (cid:12) (cid:12)t (i) (cid:12) (cid:12) (cid:12) : = 1, . . . , ni . For WordPiece-like tokenizers, continuation markers (e.g., ## are removed before computing lengths."
        },
        {
            "title": "Illustrative gold analyses and tokenizations",
            "content": "We use three running examples: kitaplarımızda = kitap + lar + ımız + da güzelleştirmek = güzel + leş + tir + mek koşuyordum = koş + uyor + du + We will compare different predicted tokenizations (good, over-segmentation, undersegmentation, and lemma splits) against these gold segmentations."
        },
        {
            "title": "4.2.2 Core Metrics\n1) Mean subwords per word Average tokenization granularity:",
            "content": "Subwords/Word = 1 (cid:88) i=1 . ni (1) High values imply fine-grained segmentations (potential over-segmentation and longer sequences); low values imply coarse segmentations (potential under-segmentation). Example: Mean subwords/word For kitaplarımızda: Tokenizer A: [kitap][lar][ımız][da] = 4. Tokenizer B: [kita][p][lar][ımız][da] = 5 (more granular). 2) Boundary precision, recall, and micro-F1 Define true/false positives and false negatives at the character-boundary level: TP = FP = FN = (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 (cid:12) (cid:12) (cid:12)B (i) pred (i) gold (cid:12) (cid:12) (cid:12) , (cid:12) (cid:12) (cid:12)B (i) pred (i) gold (cid:12) (cid:12) (cid:12)B (i) gold (i) pred (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) , . Then micro-averaged precision, recall, and F1 are Pµ = TP TP + FP , Rµ = TP TP + FN , F1µ = 2PµRµ Pµ + Rµ . Example: Boundary micro-F1 (2) (3) (4) (5) Gold boundaries in kitap+lar+ımız+da occur at offsets {5, 8, 12, 14}. Pred 1: [kitap][lar][ımız][da] Bpred = {5, 8, 12, 14}, so = = 1. Pred 2: [ki][tap][lar][ımız][da] adds an extra boundary at offset 2 FP , < 1, while recall may remain high. 3) Boundary macro-F1 Per-item precision/recall/F1:"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "9 (i) = (i) = F1(i) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) , , (i) gold + ε (cid:12) (i) (cid:12) pred (cid:12)B (cid:12) (cid:12) (i) (cid:12) (cid:12) (cid:12)B (cid:12) pred (cid:12) (i) (cid:12) pred (cid:12)B (cid:12) (cid:12) (i) (cid:12) (cid:12) (cid:12)B (cid:12) gold (i) (i) 2P (i) + ε (i) + + ε (i) gold , with ε > 0 tiny constant to avoid division by zero.4 The macro-F1 is F1macro = 1 (cid:88) i=1 F1(i). (6) (7) (8) (9) 4) Lemma boundary hit rate Let The lemma boundary hit indicator is (i) = ℓ(i) denote the lemma span length in characters within (i). and the corpus-level rate is (cid:104) (i) = (i) (i) pred (cid:105) , LemmaHit = 1 (cid:88) i=1 (i). (10) (11) Example: Lemma boundary hit Gold: koş+yor+du+m. Pred A: [koş][uyor][dum] has boundary at koş = 1. Pred B: [ko][şuyor][dum] splits inside the lemma = 0. 5) Lemma single-token rate Tokenize each lemma in isolation; let τ() be the tokenizer applied to standalone string and τ(ℓ) its subword count. Define Lemma1Tok = 1 (cid:88) (cid:104) τ(ℓ(i)) = 1 (cid:105) . i= (12) This reflects lemma atomicity in the vocabulary independent of surface affixation. 4. E.g., single-morpheme items where (i) gold = 0."
        },
        {
            "title": "Duygu Altinok",
            "content": "6) Over-/Under-segmentation indices Let ki of predicted subwords for item i. We define: be the number of gold morphemes and ni the number OverSeg = UnderSeg = 1 (cid:88) i=1 (cid:88) i= ni + ε , ki ki + ε . ni (13) (14) Values > 1 indicate, respectively, tendency to split morphemes further (over-segmentation) or to fuse multiple morphemes (under-segmentation). Examples: Over/Under-segmentation güzelleştirmek (güzel+leş+tir+mek) Good: [güzel][leş][tir][mek] = 4, = 4, balanced. Over-seg: [gü][zel][leş][tir][mek] = 5, = 4, OverSeg > 1. kitaplarımızda (kitap+lar+ımız+da) Under-seg: [kitaplarımızda] = 1, = 4, UnderSeg > 1. 7) Sequence agreement: CER and WER Represent the gold morph sequence as token string (i) , where + is gold literal separator. Let Edit(, ) be the Levenshtein distance. and the predicted subword sequence as (i) (i) 1 + +t ni (i) 1 + +m (i) pred = = (i) ki CER = WER = (cid:80)N (cid:80)N (i) gold chars(S (cid:12) (cid:12) (cid:12) i=1 Edit(cid:0) chars(S (cid:80)N i=1 i=1 Edit(cid:0) tokens(S (cid:80)N i=1 (cid:12) (cid:12) (cid:12) (i) gold tokens(S (i) pred ) (cid:1) , ), chars(S (cid:12) (i) ) (cid:12) (cid:12) gold (i) pred ) (cid:1) , ), tokens(S (cid:12) (i) (cid:12) (cid:12) gold ) (15) (16) where chars() returns character sequence and tokens() returns the +-separated units. Examples: CER/WER Gold (koşuyordum): koş+uyor+du+m. Pred: koş+uyor+dum small character edit (CER low), one token substitution (WER higher)."
        },
        {
            "title": "4.2.3 Supporting Metrics\nAffix coverage and atomicity Let A be a set of frequent suffix types (e.g., plural, case, possessive).\nFor type-level coverage:",
            "content": "AffixCov = 1 (cid:88) aA (cid:2) subword type s.t. = (cid:3) . (17) For token-level atomicity, let C(a) be the number of occurrences of affix in the corpus and A(a) the number of those realized as standalone predicted subword (a predicted boundary on both sides"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "11 AffixAtom = (cid:80) (cid:80) aA A(a) aA C(a) . (18) of in the surface word):"
        },
        {
            "title": "4.2.4 Worked examples",
            "content": "Example 1: kitaplarımızda Gold: kitap+lar+ımız+da. Offsets after gold morphemes: {5, 8, 12, 14}. Pred (good): [kitap][lar][ımız][da] Bpred = {5, 8, 12, 14}. TP = 4, FP = 0, FN = 0; Pµ = Rµ = F1µ = 1. LemmaHit: boundary at kitap = 5 exists 1. OverSeg = n/k = 4/4 = 1; UnderSeg = 4/4 = 1; WER/CER = 0. Pred (over-seg): [ki][tap][lar][ımız][da] Bpred = {2, 5, 8, 12, 14}. Intersections {5, 8, 12, 14} TP = 4, FP = 1, FN = 0; Pµ = 4/5, Rµ = 1, F1µ = 2(4/5)1 LemmaHit: boundary at 5 exists 1. OverSeg = 5/4 = 1.25; UnderSeg = 4/5 = 0.8. 9 0.889. (4/5)+1 = 8 Example 2: güzelleştirmek Gold: güzel+leş+tir+mek. Suppose offsets are {5, 8, 11, 14} (illustrative). Pred (good): [güzel][leş][tir][mek] perfect boundary match. Pred (lemma split + over-seg): [gü][zel][leş][tir][mek] Bpred = {2, 5, 8, 11, 14}; gold {5, 8, 11, 14}. TP = 4, FP = 1, FN = 0; F1µ 0.889. LemmaHit: boundary at güzel = 5 exists 1 (even though lemma was internally split). Example 3: koşuyordum Gold: koş+uyor+du+m. Let offsets be {3, 7, 9, 10}. Pred (good-ish): [koş][uyor][dum] Assume subword char lengths [3, 4, 3] on the surface, so Bpred = {3, 7, 10}; TP = 3, FP = 0, FN = 1; Pµ = 1.00, Rµ = 0.75, F1µ 0.857. LemmaHit: boundary at koş = 3 exists 1. Sequence strings: Gold koş+uyor+du+m vs Pred koş+uyor+dum: CER= 0, WER= 0.25. High F1µ and LemmaHit indicate that subword splits align with morphological seams; Lemma1Tok reflects vocabulary-level lemma atomicity. OverSeg/UnderSeg explain whether gains come from finer or coarser granularity. CER/WER summarize end-to-end sequence agreement, with CER sensitive to near-misses and WER stricter at the unit level. Affix coverage and atomicity connect the subword inventory to the languages productive suffix system. 5. Pre-Transformer Tokenization Benchmarks We benchmark character-level tokenization, word-Level tokenization, and morphology-aware subwords on three task families: TrGLUE, NER, and POSDEPMorph. Each tokenizer has its own subsection with results and explainability; architectures are kept comparable across tokenizers for fair evaluation."
        },
        {
            "title": "Tokenization schemes",
            "content": "Character-level tokenization: words are segmented into characters; the first character is standalone token and subsequent characters are prefixed with ## to denote continuation. Word-level tokenization: words remain intact as tokens. Morphology-aware subwords: words are segmented using Zeyrek (Akin and Akin 2007) and spaCy Turkish (Altinok 2023a); each subword is prefixed with ## to mark continuation. {Tokenization of the example word gittim under different schemes.} begin{tabular}{@{}ll@{}} Character-level: & ##i ##t ##t ##i ##m Word-level: & gittim Morphological subwords: & git ##ti ##m end{tabular}"
        },
        {
            "title": "Modeling overview",
            "content": "TrGLUE: Character-level: CNN encoder over character embeddings (no external pretrained embeddings). Word-level and morphology-aware subwords: BiLSTM encoders over embeddings initialized with word2vec (Google); embeddings are fine-tuned. NER: BiLSTM encoder with token-classification head (BIO). For word-level predictions, we compute one representation per word (direct word token or pooled subword/char states). In subword setups, only the first subword of each word can take B-/Itags (others are masked or forced to O) to ensure consistent word-level span reconstruction. In character-level setups, BIO is predicted per character and contiguous labeled spans are merged back to words for F1. POSDEPMorph: joint multi-task model over shared BiLSTM. UPOS is predicted with linear head; dependency parsing uses deep-biaffine scorer for arcs and relations; morphology uses parallel per-attribute classifiers (including an explicit NONE). With subword/character inputs, word-level representations are formed via pooling (e.g., first-subword or mean) or by composing character states; all predictions and evaluation remain at the word level. In the following subsections, we report results and provide tokenizer-specific explainability analyses for character-level tokenization, word-Level tokenization, and morphology-aware subwords, followed by key takeaways."
        },
        {
            "title": "5.1.1 TrGLUE\nThe character model achieves competitive performance on single-sentence sentiment (SST-2: 84.3%\naccuracy) and a respectable baseline on natural language inference (MNLI: 67.1% accuracy). In\ncontrast, it underperforms on grammatical acceptability (CoLA: MCC = 0.08) and semantic textual\nsimilarity (STS-B: ρ = 0.12 Pearson), with moderate results on paraphrase (MRPC: 62.1% accuracy).",
            "content": "We observe three consistent phenomena:"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "13 Tasks dominated by surface sentiment cues (e.g., polarity markers, intensifiers) are well captured by local character n-grams, yielding strong SST-2 performance without explicit lexical segmentation. For inference (MNLI), the model recovers non-trivial decision boundary from local sublexical patterns, suggesting that Turkish morphology conveys discriminative information even at the character level; however, the absence of longer-range compositional structure limits ceiling performance. Tasks requiring fine-grained acceptability judgments or graded semantic similarity (CoLA, STSB) degrade substantially. These tasks depend on syntactic well-formedness and precise lexical semantics, which are not reliably recoverable from short receptive fields and tokenization-free representations."
        },
        {
            "title": "5.1.2 NER\nCharacter-level NER achieved 0.70 F1 with our architecture, indicating that fine-grained subword\nrepresentations and orthographic cues are sufficiently expressive to recover entity spans without\nreliance on pre-tokenized inputs. Despite the lack of explicit word boundaries and the longer effective\nsequence lengths characteristic of character-level processing, the encoder learns consistent patterns in\nmorphology, affixation, and character-shape features that correlate with entity categories, yielding\na balanced precision–recall profile. Typical error modes include boundary drift at span edges and\nconfusions among semantically proximate labels (e.g., organization vs. location for institution names),\nreflecting the diffuse, local nature of character-level evidence. Nevertheless, this result constitutes a\nstrong baseline for settings with noisy text, rich morphology, or ambiguous tokenization.",
            "content": "Relative to English counterparts, character-level baselines on standard English NER benchmarks typically exceed this score when coupled with strong contextual encoders (e.g., BERT/SpanBERT) and token-level conditioning, often reporting F1 in the 0.900.95 range under well-tokenized, highresource conditions. Purely character-driven models without large pretrained context generally trail those token-based systems in English by substantial margin, as English benefits from comparatively stable tokenization, abundant labeled data, and capitalization cues. In contrast, for languages with richer inflection, noisier orthography, or less reliable tokenization, the gap narrows: character-level modeling better captures morphological regularities and exhibits robustness to OOV forms and spelling variation. Thus, while 0.70 F1 would be considered below state-of-the-art for English token-based pipelines, it is competitive for purely character-level architectures and is particularly promising in non-English or low-resource scenarios where subword granularity confers larger advantage."
        },
        {
            "title": "5.1.3 POS-DEP-Morph\nCharacter-level baseline results on the BOUN treebank show strong performance on POS, morphol-\nogy, and moderate performance on dependency parsing. The model attains 91.56 POS accuracy,\n65.19 UAS, and 57.15 LAS. For morphological tagging, the overall micro-accuracy is 96.19, with\nuniformly high accuracies across individual features: Abbr (99.98), Echo (100.00), Typo (99.99),\nReflex (99.72), NumType (99.48), Polarity (95.38), PronType (98.21), Evident (98.59), Mood (97.26),\nTense (96.66), VerbForm (97.10), Voice (97.11), Person (91.04), Case (88.48), Number (89.25), and\nNumber[psor]/Person[psor] around 94–95. These results indicate that, at the character level, the\nencoder effectively leverages orthographic regularities and inflectional morphology to recover rich\nfeature structures, yielding near-ceiling performance on several categories that are often brittle in\ntoken-based setups. The comparatively lower UAS/LAS suggests that while local morphological cues\nare captured reliably, learning long-distance syntactic relations remains more challenging without\nstrong contextual token-level priors.",
            "content": ""
        },
        {
            "title": "Duygu Altinok",
            "content": "Compared to BERTurk on the same dataset, the character-level baseline trades off syntactic attachment quality for morphological fidelity. Whereas the BERTurk baseline achieves higher UAS/LAS (81.51/74.59) and slightly better POS accuracy (92.63 vs. 91.56), its morphological micro-accuracy is substantially lower (30.76 vs. 96.19), with large gaps across many features (e.g., Voice: 2.01 vs. 97.11; Mood: 6.54 vs. 97.26; Case: 11.96 vs. 88.48). This divergence underscores different inductive biases: character-level modeling excels at capturing affixal and orthographic signals that directly encode morphological categories, while contextualized token encoders better model headdependent structure and span-level semantics that support dependency parsing. In settings where accurate morphological annotation is paramount (e.g., downstream agreement, lemmatization, or generation), the character baseline establishes strong reference point; conversely, for syntactic structure, pretrained token-based models retain an advantage."
        },
        {
            "title": "5.1.4 Key Findings\nCharacter-level models serve as tokenizer-free baselines that operate directly over raw characters,\noffering robustness to out-of-vocabulary (OOV) morphology, misspellings, and tokenization errors.\nAcross TrGLUE, NER, and POS–DEP–Morph evaluations, they reveal where sublexical cues suffice\nand where explicit segmentation, longer-range composition, and lexicalization remain necessary.",
            "content": "Here are our key findings from this subsection: Strong tokenizer-free baselines on surface-driven tasks. Character CNNs are competitive on single-sentence sentiment (e.g., SST-2) and deliver serviceable accuracy on MNLI, indicating that local character n-grams and morphotactics carry substantial signal without explicit tokenization. Clear limits on structureand meaning-sensitive tasks. Large gaps on grammatical acceptability (CoLA) and graded semantic similarity (STS-B) highlight the need for longer-range composition and precise lexical semantics that character-only encoders struggle to capture. Near-ceiling morphological fidelity; weaker long-distance syntax. On BOUN, characterlevel baselines achieve very high morphological micro-accuracy with uniformly strong perattribute scores, yet substantially lower UAS/LAS, indicating difficulty modeling headdependent structure and long-distance attachments without strong token-level context. Viable NER without tokenization. Character-level NER attains an F1 of 0.70, showing that entity spans can be recovered from subword signals; primary errors involve boundary drift and confusions among closely related types. Complementary to pretrained token/subword models. Relative to BERTurk on BOUN, character models excel in morphological fidelity but trail in POS and dependency parsing, reflecting complementary inductive biases: local morpho-orthographic cues vs. broader contextual and lexical knowledge. Practical advantages for agglutinative and low-resource settings. Character models are robust to OOV forms and noisy orthography, avoid vocabulary explosion, and simplify preprocessingbenefits that are particularly salient for languages with rich morphology. Methodological value as clean floor. Using characters establishes clear baseline for attributing subsequent gains to subword segmentation and lexicalization, disentangling tokenizer coverage from genuine modeling improvements. In summary, character-level modeling provides robust, tokenizer-free baseline that excels at morphology and surface-driven classification, while exposing headroom on syntax and semantics. These complementary strengths motivate hybrid designs and systematic exploration of subword vocabularies to close the gaps on structureand meaning-sensitive tasks."
        },
        {
            "title": "Cambridge Default Journal",
            "content": ""
        },
        {
            "title": "5.2 Word-Level Tokenization\nWe next consider the opposite extreme to subwording: tokens are whole words. In Turkish, rich\ninflection means many surface forms per lemma, so exact word forms in the test set are often unseen\neven when related inflections occur in training. This drives a substantial train–test mismatch via\nOOV items and motivates a careful measurement of vocabulary coverage and its impact on task\nperformance.",
            "content": "For each task below, let denote the full word vocabulary extracted from training and sorted by frequency, and let be the size of the retained prefix (top-K types). We report: (i) training coverage: the fraction of training tokens accounted for by the top-K types; (ii) test coverage: the fraction of test tokens covered by the same top-K types learned from training; (iii) the relationship between coverage (or K) and final task scores. This protocol quantifies how rapidly coverage saturates on train, how it decays on test, and how OOV exposure correlates with accuracy/F1 across TrGLUE, NER, and POSDEPMorph. It also provides controlled knob (K) to compare word-level tokenization against character-level and morphology-aware subword baselines in subsequent analyses."
        },
        {
            "title": "5.2.1 TrGLUE\nWe evaluate word-level tokenization task-by-task on TrGLUE, relating performance to training/test\ncoverage, OOV rates, and the chosen top-K vocabulary size.",
            "content": "CoLA We examine CoLA under word-level vocabularies, relating token coverage to Matthews correlation (MCC). As shown in Figure 1 despite increasing coverage, the models acceptability judgments remain below chance, indicating that lexical coverage alone is not sufficient for this task. On CoLA, the word-level model never gets off the ground: MCC is negative across all coverages and drifts downward as we retain more of the vocabulary. The coverageefficiency plot shows that train coverage grows slowly with K/V while test coverage saturates earlier, but the success curve makes clear that coverage isnt the bottleneckrepresentation is. Grammatical acceptability depends on abstract well-formedness cues (long-distance constraints, subcategorization, function words in context) that arent captured by sparse, surface-form word inventory. The heavy morphological/orthographic tail further fragments evidence across many low-count types, so adding more words mostly adds rare variants without improving generalization. In short, the model is effectively memorizing lexical patterns and label priors; as the vocabulary grows, this memorization amplifies noise and overfits train idiosyncrasies, yielding worse MCC. This aligns with the strong transformer gains on CoLA: contextualized subword representations encode the syntactic regularities that word-level model misses. SST-2 Here we offer results for movie sentiment analysis binary task SST-2. Figure 2 shows the success vs vocabulary size. For SST-2, word-level modeling shows quick payoff and then long plateau: accuracy climbs from 69% at 50% train coverage to 85% by roughly 7580%, after which larger vocabularies do not help. The early gains occur once the top-K list includes frequent polarity markers and phrasese.g., hiç, çok kötü, berbat, beğenmedim on the negative side, and mükemmel, harika, bayıldım on the positive side. The coverage plot shows test coverage saturates earlier than train, but beyond this elbow, adding rarer types contributes little, consistent with sentiment being driven by compact lexicon of strong cues. For MNLI, MRPC, and STS-B, vocabulary coverage behaves like CoLA (test coverage saturates with smaller than train), while performance vs. coverage mirrors SST-2 (early elbow followed by broad plateau). Across coverage levels, scores fluctuate around roughly 0.70 accuracy for MNLI, 0.60 accuracy for MRPC, and 0.25 Pearson/Spearman for STS-B, with no consistent gains"
        },
        {
            "title": "Duygu Altinok",
            "content": "(a) Token coverage vs. required vocabulary fraction (K/V) for CoLA (Train/Test). Test coverage accumulates faster than train up to 95% and then saturates, while train coverage increases gradually, indicating that many frequent train types do not transfer cleanly to the test distribution. (b) Train token coverage vs. CoLA score (MCC). Each point uses the smallest top-K achieving the indicated train coverage. Performance is negative across the range and degrades slightly as coverage grows ( -10 at 50% to -36 at 100%), suggesting the word-level model fails to capture grammatical acceptability even with full coverage. Figure 1. CoLA with word-level vocabularies: coverage efficiency and success. Left: achieving high token coverage requires retaining large fractions of the word list on train, with different accumulation pattern on test. Right: increasing train token coverage does not improve CoLA performance and in fact trends downward, pointing to representation limits rather than coverage as the bottleneck. near full coverage. Compared to the character baseline (MNLI 67.1, MRPC 62.1, STS-B 0.12), word-level modeling is slightly better on MNLI, roughly comparable on MRPC, and clearly stronger on STS-Bbut all three tasks exhibit the same diminishing-returns pattern once the top-K includes the frequent pairs and cues that drive each task."
        },
        {
            "title": "5.2.2 NER\nWe analyze how efficiently a word-level vocabulary captures token mass in our data and whether\nincreasing coverage translates into better NER performance. Figure 3 relates coverage to both the\nrequired vocabulary fraction and downstream F1 to diagnose efficiency and generalization.",
            "content": "Across the training data, token coverage increases only gradually as we retain larger fractions of the word vocabulary, with no early head wins. This unusually slow accumulation is consistent with heavy-tailed type distribution driven by morphology: many surface forms (inflections, casing, clitics, hyphenation) split single lemma into numerous distinct word tokens, each occurring sparsely and counted as separate vocabulary items. As result, even large expansions of primarily mop up rare forms rather than consolidating mass under frequent entries, making the word vocabulary inefficient on both train and test (where coverage lags further due to domain shift and unseen variants). This inefficiency carries through to downstream performance: increasing training token coverage from 75% toward 100% yields only modest, unstable gains, with NER F1 saturating around 0.5well below character/subword models ( 0.70) and Transformer baselines ( 0.77). In short,"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "17 (a) Token coverage vs. required vocabulary fraction (K/V) for SST-2 (Train/Test). Test coverage rises sharply and saturates around 9395% with relatively small K, while train coverage increases gradually, indicating mismatch in how the ranked word list accumulates coverage across splits. (b) Train token coverage vs. SST-2 accuracy. Each point uses the smallest top-K achieving the shown train coverage. Accuracy jumps from 69% at 50% coverage to 85% by 7580%, then remains flat near 85% through 100% coverage. Figure 2. SST-2 with word-level vocabularies: coverage efficiency and success. Left: coverage accumulates differently on train vs. test, with test saturating earlier. Right: performance exhibits an early elbow, reaching 85% accuracy by 80% train coverage and showing no gains with larger vocabularies. rich morphology fragments the word-level representation, so chasing higher word coverage adds vocabulary without commensurate improvements in success."
        },
        {
            "title": "5.2.3 POS-DEP-Morph\nWe evaluate word-level tokenization on the BOUN treebank across POS tagging, dependency\nparsing (LAS), and morphological tagging, relating success to how much of the training token mass\nthe vocabulary covers. Given BOUN’s rich morphology, we expect heavy-tailed surface forms\nto fragment the word vocabulary; Figure 4 examine coverage efficiency and its impact on task\nperformance.",
            "content": "The success curves are strikingly flat after 75% train coverage: increasing the kept word fraction does not translate into better POS, LAS, or Morph F1; which stall at 60/19/12, far below the character-level (91/65/96). This indicates that the limiting factor is not token mass coverage but the representation itself: in BOUNs morphology-heavy setting, many inflected surface forms split each lemma into sparse word types, producing heavy tail that the word list cannot generalize over. The traintest coverage mismatch reinforces this: coverage accumulated on train does not reflect what the model needs at test time. Practically, these results argue against word-level vocabularies for POS/DEP/Morph on BOUN; character/subword models with explicit morphological supervision are necessary to capture inflectional variation and deliver competitive accuracy."
        },
        {
            "title": "Duygu Altinok",
            "content": "(a) Token coverage vs. required vocabulary fraction (K/V) for train and test. Curves show how much of the ranked word vocabulary must be retained to reach given coverage on each split; test coverage lags train, indicating distribution shift. (b) Train token coverage vs. NER F1. Each point uses the smallest top-K word list that achieves the indicated training token coverage; despite increasing coverage, F1 remains around 0.5. Figure 3. Word-level vocabulary efficiency and downstream NER performance. Left: achieving high coverage requires large fractions of the vocabulary, and test coverage accumulates more slowly than train, underscoring inefficiency and domain mismatch. Right: raising training token coverage from 75% to 100% yields only modest and unstable gains, with F1 saturating around 0.5."
        },
        {
            "title": "5.2.4 Explainability\nWe probe what the word-level model attends to by visualizing token-level attributions as heatmaps.\nUnless stated otherwise, weights are nonnegative “intensity-only” scores normalized per input (sum\n= 1), so darker shades indicate tokens the model relies on most for its decision.",
            "content": "The contrast between CoLA and SST-2 is stark as exhibited in Figure 5. For CoLA (top-80% vocab), weights are low-contrast and scattered: the three OOV forms (Cümlede, kullanmabilen, öğeye) receive only mild emphasis because many rare types are OOV at this coverage, while domain words like biçimbirime/biçimbirimler get slightly higher but still modest intensityno coherent morphosyntactic cue emerges. For SST-2, the model concentrates on sentiment-bearing cues and verbs: memnun etmedi, klişe, ağır, Sıkıldığım, tavsiye edemeyeceğim, and decision verbs like tercih/edebilir dominate, illustrating why sentiment reaches performance plateau once these frequent polarity markers are covered. We further probe sequence labeling models (NER and POS) using token-level intensity heatmaps, where nonnegative weights (normalized per input) indicate which tokens the model relies on when assigning tags. As exhibited in Figure 6, across sequence labeling tasks, intensity patterns mirror coverage constraints. For NER, the model leans on unambiguous cues like numeric dates while underweighting OOV entities, which helps explain its modest F1. For POS, the model defaults to the few in-vocab function words as anchors and allocates nearly uniform, low weights to OOV content tokens, signaling poor morphosyntactic generalization under subword-sparse, word-level vocabularies. Together, these heatmaps show that when coverage is capped at 80%, models prioritize frequent,"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "19 (a) Token coverage vs. required vocabulary fraction (K/V) for BOUN, train and test. Train coverage increases only gradually with larger K/V, while the test split accumulates coverage differently, indicating mismatch between the ranked word list and the test distribution. (b) Train token coverage vs. POS, LAS, and Morph F1. Each point uses the smallest top-K word list achieving the indicated train coverage. All three metrics reach their plateau by 75% coverage and remain essentially flat thereafter (POS 60, LAS 19, Morph 12). Figure 4. BOUN word-level vocabulary: coverage efficiency and downstream success. Left: achieving higher coverage requires retaining large fractions of the word list, and traintest coverage accumulates differently, evidencing distribution shift. Right: increasing train token coverage beyond 75% does not translate into meaningful gains for POS, dependency (LAS), or morphology; performance plateaus at low levels, highlighting the inefficiency of word-level vocabularies on this morphologically rich dataset. (a) CoLA (word-level, top-80% vocab). Low-contrast, diffuse attributions: OOVs are only mildly weighted; biçimbirime/biçimbirimler receive modest intensity; no clear morphosyntactic signal. (b) SST-2 (word-level, top-80% vocab). Attribution concentrates on polarity cues and verbs, e.g., memnun etmedi, klişe, ağır, Sıkıldığım, tavsiye edemeyeceğim, tercih/edebilir. Figure 5. Explainability heatmaps for CoLA vs. SST-2 with word-level vocabularies. CoLA reveals weak, scattered cues consistent with poor grammatical acceptability performance, whereas SST-2 focuses on compact set of sentiment-bearing tokens, explaining the early performance gains and subsequent plateau once these cues are covered. surface-level signals and fail to exploit richer structure in OOV segments."
        },
        {
            "title": "5.2.5 Key Findings\nWe distill general lessons from word-level modeling of Turkish and other morphologically rich\nlanguages:",
            "content": "Coverage-driven brittleness. Fixed word vocabularies inevitably leave many inflected and derived forms out-of-vocabulary. When large portions of morphology collapse to <unk>, being"
        },
        {
            "title": "Duygu Altinok",
            "content": "(a) NER example (top-80% vocab). Dates (1949da, 1 Ekim 1949da) receive the clearest emphasis; OOV entities (Mao, Cumhuriyetini, Pekini) are de-emphasized, consistent with F1 0.50. (b) POS example (top-80% vocab). Emphasis shifts to in-vocab function words (Bir, yandan, da), while OOV content words receive flat, low weights, indicating limited morphological awareness. Figure 6. Explainability for sequence labeling with word-level vocabularies. Left: NER focuses on easily recognizable spans (dates) and struggles with OOV named entities, yielding low, diffuse attention elsewhere. Right: POS relies on frequent function words available in the vocabulary and spreads weak intensity across OOV content words, reflecting limited generalization to unseen morphology under 80% coverage. OOV becomes commonplace and ceases to be informative, attenuating access to grammatical evidence. Shallow cues dominate when structure is hidden. On structure-sensitive tasks (e.g., acceptability judgments, agreement checking), models struggle to capture suffix order, case, and lemma relations if these cues live primarily in unseen variants. Explanations mirror this with low-contrast, diffuse attributions. Sentiment is the exception, not the rule. Tasks driven by frequent lexical markers (e.g., sentiment classification) remain comparatively robust. Explanations cluster around polaritybearing words and common verbal constructions, suggesting reliance on high-frequency lexical cues rather than deeper composition. Entity-centric extraction degrades under OOV pressure. For NER and related extraction tasks (e.g., slot filling, event arguments), proper names, titles, and multiword expressions are disproportionately OOV. Models over-weight easy numeric/date spans and under-weight rare names, yielding unstable boundaries and modest precision/recallpatterns clearly visible in attribution maps. From POS to syntax: limited generalization. Beyond POS tagging, syntax-oriented tasks such as dependency parsing, chunking and morph-feature prediction; suffer when content words are unseen. Models fall back to frequent function words as anchors while assigning flat, low intensity to OOV content tokens. This limits recovery of long-distance dependencies, agreement, and attachment decisions that hinge on morphology. Explainability mirrors learning dynamics. Flat, low-contrast heatmaps indicate saturation on surface statistics without access to morphological signals hidden behind <unk>. Simply scaling data yields diminishing returns unless the representational units change. Error profile tracks the long tail. Missed derivations, unstable case/possessive handling, and inconsistent proper-name treatment cluster in the long tailexactly where word-level vocabularies provide the weakest support. Implication: re-think the units. For morphologically rich languages, morphology-aware subwording (e.g., FST/analyzer-guided morpheme splits) is more suitable inductive bias. It restores access to grammatical signals needed by syntax-oriented tasks while preserving the frequent lexical cues that already sustain sentiment and other surface-lexical phenomena."
        },
        {
            "title": "Cambridge Default Journal",
            "content": ""
        },
        {
            "title": "5.3.1 TrGLUE\nOverall, morphology-aware subwords close much of the gap to pretrained transformers on structure-\nand morphology-sensitive tasks while yielding consistent gains over word- and character-level\nbaselines. Figure 7 exhibits the model performance insights.",
            "content": "(a) CoLA: Token cov. vs. vocab. frac. (b) MNLI: Token cov. vs. vocab. frac. (c) SST-2: Token cov. vs. vocab. frac. (d) CoLA: Train cov. vs. Matthews (e) MNLI: Train cov. vs. Acc (f) SST-2: Train cov. vs. Acc Figure 7. Morphology-aware subwords: coverage and performance across CoLA, MNLI, and SST-2. For each task, the coverage curve (above) precedes the performance-vs.-coverage plot (below), enabling direct reading from vocabulary coverage to downstream effectiveness. Full-size per-task plots are provided in the appendix. Across CoLA, MNLI, and SST-2, morphology-aware subwords achieve high token coverage with markedly compact vocabulary, and this efficiency translates into stable downstream performance. The coverage curves rise steeplyparticularly for MNLI and SST-2reaching near-complete train/test coverage after retaining only small fraction of the morpheme inventory, indicating that limited set of productive stems and suffixes accounts for most running tokens. In contrast to word-level tokenization, which requires an order-of-magnitude larger lexicon to obtain comparable coverage (and still suffers severe OOV under derivation and inflection), the morph-based inventory yields near-saturated coverage without ballooning the vocabulary. This compactness matters empirically: performance as function of training coverage plateaus early for MNLI and SST-2, suggesting diminishing returns once the core morphemes are included, while CoLA shows flatter but consistent trend, consistent with its sensitivity to agreement and case morphology rather than sheer lexical variety. Qualitatively, sentiment is carried by small set of polarity-bearing morphemes (e.g., -ma/-me), explaining why SST-2 saturates quickly; MNLI benefits from early"
        },
        {
            "title": "Duygu Altinok",
            "content": "capture of negation/modality and case markers that stabilize contradiction/neutral decisions; and CoLA leverages person/number/case morphemes to localize acceptability violations. Overall, relative to word-level tokenization, morphology-aware subwords provide superior coverage at far lower vocabulary size, reduce OOV-driven variance, and yield comparable or better accuracy at fraction of the lexical footprint, underscoring their efficiencyperformance trade-off for Turkish. For STS-B and MRPC, we observe broadly similar coverage and learning dynamics to MNLI: vocabulary saturates quickly with compact morpheme inventory, and performance plateaus early. Concretely, the morph-subword BiLSTM attains 0.45 on STS-B and 0.62 on MRPC, consistent with the notion that morphology helps stabilize lexical variation while the remaining gap is largely semantic/pretraining-driven. We now turn to sequence labeling, where morphology plays more central role. The next section examines NER with morphology-aware subwords, quantifying how morpheme-level cues (e.g., case and possessive markers) improve boundary and type decisions relative to character and word baselines."
        },
        {
            "title": "5.3.2 NER\nWe examine how pruning the morphology-aware subword vocabulary affects token coverage and\nNER performance, and compare these trends to a word-level baseline.",
            "content": "(a) Token coverage as function of retained morpheme vocabulary on train and test. Coverage saturates around 8890% of the full vocabulary, indicating that compact core yields near-perfect coverage on both splits. (b) NER span-F1 versus train token coverage. F1 rises rapidly up to 8085% coverage and then plateaus, showing diminishing returns from rare morphemes beyond this point. Figure 8. Effect of morpheme vocabulary pruning on coverage and NER performance. small subset of frequent morphemes achieves near-maximal token coverage and F1, enabling aggressive pruning without loss in accuracy. Across pruning levels, morphology-aware subwords maintain high token coverage with relatively small vocabulary and convert that coverage into early NER gains: moving from 55% to 85% coverage boosts F1 from the mid-50s to 75, after which performance stabilizes despite further increases in coverage. Compared to word-level model, the morph-subword system achieves com-"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "23 parable or higher F1 with far fewer lexical types and thus better data efficiency; in practice, retaining only 8890% of the morpheme inventory matches the near-maximal performance while keeping the vocabulary substantially smaller than the full word lexicon. This suggests that most NER signal resides in frequent morphemes (case, possessive, derivational cues), and that pruning rare morphemes yields parameter and speed benefits without sacrificing accuracy."
        },
        {
            "title": "5.3.3 POS-DEP-Morph\nWe analyze how pruning the morphology-aware subword inventory trades off token coverage\nwith POS/LAS/Morph performance, and compare these trends to a word-level vocabulary, given in\nFigure 9.",
            "content": "(a) Train token coverage vs. task performance. POS and Morph F1s climb steeply and saturate by 8588% coverage; LAS remains stable across pruning once coverage is high. (b) Token coverage as function of retained morpheme vocabulary. Test coverage reaches 100% by 275% of the vocabulary; train coverage hits 100% around 8385% Figure 9. POS-DEP-Morph under morpheme-vocabulary pruning. compact morpheme set attains near-max coverage and task performance, enabling aggressive pruning without accuracy loss. The curves show that morphology-aware subwords yield rapid gains as coverage increases: POS and Morph F1 improve sharply from low coverage to about 8085%, after which they plateau at their near-maximum values; LAS is comparatively flat once coverage is adequate, reflecting that core case/possessive markers needed for head selection are already retained early. Coverage itself is achieved with small inventoryon the test set, near-complete coverage arrives by roughly threequarters of the morpheme vocabulary, and the train split reaches 100% by about 8385%. Relative to word-level model, the morph-subword approach attains comparable or better POS/Morph/LAS with far fewer lexical types, meaning fewer parameters and better generalization on OOV stems; in practice, pruning to the 8085% range preserves performance while reducing vocabulary size dramatically compared to the full word lexicon."
        },
        {
            "title": "5.3.4 Explainability\nTo understand how morphology-aware subwords drive decisions beyond aggregate metrics, we\nvisualize per-(sub)token attributions across classification (TrGLUE) and token-level tasks (NER/POS),\nusing a unified color scale so contributions are comparable within and across panels.",
            "content": "(a) CoLA token/morpheme attributions. (b) SST-2: token/morpheme attributions. Figure 10. Explainability on GLUE tasks. Heatmaps show per-(sub)token attribution with consistent colormap and scale; darker indicates stronger positive contribution. As shown in Figure 10, across both TrGLUE examples, high positive mass concentrates on morphologically salient units that disambiguate polarity and entailment cues: negation morphemes, derivational markers shifting part-of-speech, and clause-level connectives receive strong weights, while function words without semantic load remain muted. We also see complementary evidence aggregation: stems contribute broad semantics, while suffixes tilt the decision (e.g., polarity or modality), indicating that morph-subwords capture compositional meaning rather than relying on surface word forms alone. (a) NER: morpheme-level attributions highlighting dates, apostrophes, and case markers. (b) POS: contributions of derivational/inflectional suffixes to tag decisions. Figure 11. Explainability on token-level tasks. We visualize how morphology-aware subwords guide boundary/type (NER) and category (POS) predictions. In NER, date numerals, month names, and apostrophe-plus-case sequences carry the highest weights, clarifying entity boundaries and types; ablative/locative and possessive markers reduce boundary fragmentation and improve attachment. In POS, inflectional endings for number, case, and tense (e.g., -ler, -de/-da, -yor, -du) consistently dominate attributions over stems, especially for nouns and verbs with ambiguous stems, showing that the model anchors syntactic category decisions in morphology rather than capitalization or context heuristics, as shown in Figure 11. Taken together, the visualizations show coherent strategy: stems provide coarse semantics, while compact set of frequent morphemes supplies decisive, task-specific cues (negation/modality for GLUE, case/possessive/TAM for NER/POS). This aligns with our pruning resultsonce the core morphemes are retained, models remain accurate and interpretable, suggesting that morphologyaware tokenization yields both efficiency and transparent, linguistically grounded behavior."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "25 Compared to word-level vocabulary, morph subwords reach comparable-or often betterperformance with dramatically fewer lexical types. Word vocabularies balloon with inflectional variants and rare forms, leading to sparse embeddings and brittle OOV handling; in contrast, morph subwords generalize across forms via shared affixes (case, possessive, derivation), yielding higher data efficiency and steadier performance on rare words and domains. In practice, pruning to the core morphemes offers the best trade-off: smaller models, faster inference, and equal or better accuracy than word-based tokenization. 6. WordPiece Tokenization In this section, we examine the three-way interaction among vocabulary size, tokenizer training data size, and morphological alignment. We ask: (i) does aligning WordPiece segments with morphological subwords improve downstream performance? and (ii) how large should the tokenizer training corpus bedoes more data always help? To make these aims concrete and evaluable across tasks, we frame the agenda as the following research questions: RQ1 How do tokenizer size and type (morphology-aware vs. WordPiece) affect downstream performance across syntax/morphology-sensitive tasks versus semantics/entity-oriented tasks? RQ2 Do tokenizers with stronger Turkish morphological alignment yield larger gains on POS/DEP/Morph than on NER/STS-B/sentiment? RQ3 How does tokenizer training corpus size (5/20/80 GB) interact with vocabulary size to trade off sequence length and and morphological fidelity? RQ4 What Pareto frontier emerges between minimal sequence length, maximal morphological alignment, and downstream accuracy? To operationalize these questions, the next subsection details the pretraining corpora used to train tokenizers and associated Transformers."
        },
        {
            "title": "6.1 Pretraining Corpus\nFor training the tokenizers and pretraining the transformers in this section, we use high-quality\nTurkish text from the BellaTurca collection (Altinok 2024), a large-scale corpus resource. We include\nthree genres in total: high-quality web data, books, and cleaned OSCAR web data.",
            "content": "High-quality web data comes from subcollection extensively filtered during crawl to prioritize the best-quality web content. This portion has total size of 4.6 GB, 1.3M documents, and 557M words. To reach approximately 5 GB, we add an academic/scientific web subcollection, contributing around 910 MB, 500K documents, and 90.6M words. The combined web total is then 5.5 GB, 1.8M documents, and 648M words. Next, we use data from the books subcollection. This portion is about 15 GB, 100M documents (each sentence treated as document), and 1.43B words. The genre is books and the data quality is high. Finally, we include cleaned OSCAR subcollection, derived from several Turkish OSCAR releases via intensive text cleaning and quality filtering. Its total size is 57 GB, 23.7M documents, and 7.15B words. In our experiments, we define three corpus scales: Minimal, Medium, and Alldata, with sizes around 5 GB, 20 GB, and 80 GB. Minimal consists of the filtered web data (plus the academic addition). Medium adds the books portion. Alldata includes web, books, and cleaned OSCAR. In the following, we use the terms Minimal, Medium, and Alldata to refer to these three pretraining corpora."
        },
        {
            "title": "6.2 Training WordPiece Tokenizers",
            "content": "We train WordPiece tokenizers across two axes: vocabulary size and corpus scale. For vocabulary size, we consider 2k, 5k, 10k, 20k, 32k, 52k, and 128k to span very small to very large inventories. For corpus scale, each vocabulary size is trained on three pretraining corpora: Minimal (5 GB), Medium (20 GB), and Alldata (80 GB) as defined above. Tokenizers are trained using the Hugging Face tooling (Tokenizers and Trainer API) (Wolf et al. 2020) and prepared with the Hugging Face libraries for downstream use. With these trained variants in hand, we analyze coverage, OOV rate, sequence length, and morphology alignment, and relate these properties to downstream performance in subsequent sections."
        },
        {
            "title": "6.3 Tokenization Behavior Across Corpora and Vocabulary Sizes",
            "content": "The empirical patterns in Turkish segmentation reveal clear progression as vocabulary size grows and as the training corpus diversifies. With very small vocabularies (210k) trained on large, heterogeneous corpora (e.g., Medium, Alldata), the tokenizer tends to operate close to the character level. Even highly frequent stems such as ev or oku are decomposed as [e, ##v] and [o, ##k, ##u, . . . ], respectively. This regime inflates sequence lengthfertility hovers around 6.5 and the proportion of continued subwords is near 0.98while dispersing the morpheme-level cues that are often decisive for downstream syntax-sensitive tasks. In other words, the model must infer morphological structure from many short fragments, strategy that is data-hungry and tends to dilute signal. As we move to mid-size vocabularies (2032k), segmentation stabilizes around stems and recurrent suffixes. Common locatives and ablatives (-de/-da, -den/-dan), plural and possessive series (-lar, - (i)miz), and clitic-like elements (-ki, -(y)a) begin to surface as consistent subword units. Typical forms such as ev, evde, and evden become whole tokens; moderately frequent inflections like okudum are often [oku, ##dum] or whole. Fertility drops into more interpretable band (roughly 1.41.7 depending on corpus), and the continued-subword rate settles between 0.30 and 0.48. This shift indicates healthier balance: sequences are shorter, and morphological boundaries remain visible enough to support generalization across productive paradigms. At large vocabularies (52128k), the tokenizer increasingly memorizes whole words and frequent inflected forms. Items such as okudum, okudular, görülebilirdi, evim, and evimiz are often single tokens. The resulting fertility pushes toward 1.151.18 with continued rates near 0.120.14. While this compression is attractive for efficiency, it sometimes fuses morphemes idiosyncraticallyfor instance, evleriniz realized as [evlerini, ##z]thereby weakening the models explicit access to morpheme boundaries in rarer or compositional variants. Syntactic tasks (POS, NER) that benefit from overt morphological cues can be disproportionately affected by such over-merging, whereas semantic tasks are generally more tolerant of it. Orthographic conventions mediate these effects. Proper-name constructions with apostrophes (İstanbulda, Ankaradan) reliably split at the apostrophe across sizes, which is correct. Numerals with apostrophes (1923te) likewise exhibit stable patternnumbers remain intact and the suffix attaches as separate unitpreserving the syntactic signal while controlling vocabulary growth."
        },
        {
            "title": "Cambridge Default Journal",
            "content": ""
        },
        {
            "title": "Illustrative tokenization snapshots",
            "content": "Small vocabulary (210k; medium/alldata) ev [e, ##v] evim [e, ##v, ##i, ##m] evlerimizden [e, ##v, ##l, ##e, ##r, ##i, ##m, ##i, ##z, ##d, ##e, ##n] okudum [o, ##k, ##u, ##d, ##u, ##m] 1923te character level sequence yazdırılmayacakmışsınız near character-level sequence Mid vocabulary (2032k) ev [ev] ev [ev, ##im] evlerimizden [ev, ##lerimiz, ##den] or okudum [ok, ##u, #du, #m] 1923te [192, ##3, , te] görülebilirdi [görülebilir, ##di] çalıştırılabilir [çalıştır, ##ılabilir] [ev, ##lerimizden] Large vocabulary (52128k) ev, evim, okudum, okudular, kapkara single tokens evleriniz [evlerini, ##z] (example of over-fusion) evlerimizden [evlerimiz, ##de] (another example of over-fusion) 1923te [1923, , te]"
        },
        {
            "title": "6.3.1 Corpus Size, Vocabulary Size, and Fragmentation\nThe interaction between corpus size and vocabulary size is not linear; more data is only beneficial if\nthe vocabulary scales accordingly. With small vocabularies on large, diverse corpora, the tokenizer\ncannot “afford” morphemes as reusable units and so defaults to short fragments, producing very\nhigh fertility (around 6.5) and almost entirely continued tokens, as quantified in Table 2. This\nconfiguration increases training cost and tends to obscure systematic morphology, pressuring the\nmodel to learn it implicitly from long sequences of micro-fragments.",
            "content": "A mid-size vocabulary establishes more favorable equilibrium. On smaller or cleaner corpora, fertility can quickly settle near 1.5 with continued rates around 0.30.4, as shown in Figure 12; region where suffixes are explicitly represented and stems remain stable. On larger and more heterogeneous corpora, achieving comparable fertility requires larger vocabulary; for instance, moving from 20k to 32k can substantially reduce over-fragmentation without collapsing morpheme boundaries. This regime offers pragmatic balance for both pretraining efficiency and downstream accuracy across syntactic and semantic tasks. At the high end, very large vocabularies minimize sequence length but risk eroding compositionality by baking frequent inflections into whole-word entries. The resulting token economy is efficient, yet the loss of morpheme visibility can degrade generalization to rarer inflectional combinations and attenuate the explicit signals that aid tagging and parsing. In practice, the impact is task-dependent: syntax-heavy tasks prefer the mid-range where morphology remains legible, whereas purely semantic tasks may benefit more from shorter sequences and tolerate morpheme fusion. These patterns together trace practical Pareto frontier. For Turkish, we consistently observe that fertility around 1.41.7 with continued-subword rates roughly 0.300.45 yields strong trade-off: sequences are compact enough for efficient training while preserving the morphological structure crucial to syntactic competence. On smaller ( 5 GB) and medium ( 20 GB) corpora, this frontier is typically achieved with 2032k vocabularies; on very large ( 80 GB) corpora, 3252k often hits the same target."
        },
        {
            "title": "Duygu Altinok",
            "content": "Table 2. Fertility and continuation rate across vocabulary sizes (mean sd). minimal medium alldata Vocab Fert. Cont. Fert. Cont. Fert. Cont. 2k 10k 20k 32k 52k 6.30.2 0.970. 6.50.2 0.980.01 6.60.2 0.980.01 3.60.1 0.690. 3.80.1 0.720.02 3.90.1 0.730.02 2.00.1 0.390. 2.10.1 0.420.02 2.30.1 0.450.02 1.50.1 0.310. 1.60.1 0.350.02 1.70.1 0.380.02 1.30.0 0.160. 1.30.0 0.180.01 1.40.0 0.200.01 128k 1.140. 0.120.01 1.150.0 0.130.01 1.180.0 0.140.01 (a) Fertility versus vocabulary size across corpus-size buckets. Points show mean fertility at each vocabulary size; colors denote corpus size (Minimal, Medium, Alldata). Lower values indicate fewer subwords per word. (b) Continuation rate versus vocabulary size across corpus-size buckets. Each point is model at given vocabulary size; colors denote corpus size. Continuation rate is the fraction of subwords that are continuations (01) Figure 12. Effect of WordPiece vocabulary size on fertility (left) and continuation rate (right) across Minimal, Medium, and Alldata corpora; mid-size vocabularies balance sequence length and morpheme visibility."
        },
        {
            "title": "6.3.2 Tokenizer Morphology Diagnostics: Results and Discussion\nWe instantiate the metrics from Section 4.2 on Turkish morphology testset and compare WordPiece\ntokenizers trained on multiple corpora and vocabulary sizes. We report segmentation granularity\n(Subwords/Word), boundary alignment (micro/macro PRF), lemma integrity (lemma_single_rate,\nlemma_boundary_rate), sequence agreement (exact_morph_sequence_match, WER metrics (WER,\nMER, CER, WIL, WIP), and affix signals (coverage/atomicity over top-200 suffixes), together with\nover/under-segmentation indices.",
            "content": "We summarize the implications for morphology-aware tokenization across our three corpora and practical vocabulary sizes (5k52k). The patterns synthesize boundary metrics, lemma integrity, and suffix diagnostics to indicate where segmentation helps or hurts interpretability. They also clarify the trade-offs introduced by vocabulary size and inventory quality, with regime-specific behavior for frequent items versus morphologically complex forms. Compact results appear in Tables 3 and 4;"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "29 more detailed tables are provided in Appendix 2. Table 3. Morphology-aware tokenization: compact summary by split. Averages across practical vocabulary sizes (5k52k) and corpora (Minimal, Medium, alldata). Degenerate regimes (2k/20k/128k) excluded. Split Çekimli Common Nouns Common Verbs Subw/Word 5.63 3.48 2.90 Pµ 0.616 0.801 0.782 Rµ F1µ LemmaBoundary 0.584 0.683 0.504 0.598 0.736 0. 0.570 0.616 0.182 Table 4. Key metrics by vocabulary size (averaged across splits and corpora). Degenerate regimes excluded. Vocab Subw/Word 5k 10k 32k 52k 3.08 4. 3.41 3.08 Pµ 0.777 0.658 0. 0.727 Rµ F1µ LemmaBoundary 0.535 0. 0.616 0.543 0.626 0.640 0.676 0. 0.396 0.575 0.578 0.525 Some key takeaways emerge from these results: Segmentation granularity increases when vocabulary is small or the learned inventory is noisy, which tends to raise boundary recall while depressing precision. Boundary alignment is highest on frequent, simpler paradigms (Common Nouns/Verbs) and lowest on Çekimli, which exhibits longer suffix chains. Lemma integrity is saturated on Common Lemmas for mid/large vocabularies, but remains modest on the long tail of rare lemmas. Suffix coverage is robust across 5k52k vocabularies, whereas suffix atomicity varies and does not monotonically correlate with boundary F1. Sequence-level exact matches remain low on Çekimli; jiwer distances confirm substantial structural deviations despite reasonable boundary F1. Several runs show extreme fragmentation (e.g., Medium/Alldata with 2k, 20k, 128k), where almost every character becomes subword: Subwords/Word 18, boundary recall 1.0, lemma_ boundary_rate 1.0, but very low precision and poor sequence agreement (CER > 0.5). These serve as stress tests and are not competitive. The extreme 2k/20k/128k regimes on Medium and Alldata collapse into near-character models, trivially achieving Rµ 1 and lemma_boundary_rate 1, but with poor precision and sequence fidelity. For practical ranges (5k52k), Çekimli boundary F1µ clusters around 0.560.59, Common Nouns often reaches 0.760.82, and Common Verbs spans 0.540.71 depending on granularity. Lemma integrity on Common Lemmas (not shown) is near-saturated (> 0.95 single-token) at 5k52k; for the full Lemma set, the same models achieve only 0.240.58 single-token, reflecting long-tail brittleness. Over/under-segmentation reveal the expected trade-off: stronger over-segmentation (e.g., 10kAlldata) raises recall and suffix atomicity but hurts exact match and CER. Suffix coverage remains high (0.740.88) at 5k52k, while moderate atomicity (0.180.33) correlates with better noun/verb F1µ than the extremes. practical operating point is to avoid highly fragmented vocabularies and instead use mid-sized inventories, roughly 5k32k, which yield about 2.55.5 subwords per word. Within this range, configurations that achieve F1 µ 0.60 on Çekimli and F1 µ 0.78 on Common Nounstypically the"
        },
        {
            "title": "Duygu Altinok",
            "content": "10k32k settings on the Minimal and Medium corporaoffer favorable precisionrecall balance for morphology-sensitive applications. When lemma preservation is priority (e.g., document retrieval or bootstrapping lemmatizers), models with higher lemma_single_rate and lemma_boundary_rate are preferable; in our experiments, 32k vocabularies on Minimal/Medium and on alldata consistently improved these indicators. We aligned predictions to gold morpheme annotations at character offsets within each whitespacedelimited token (no cross-token credit), after consistent NFKC + lowercasing; hyphens only count if marked in gold. Lemma_single_rate requires single contiguous subword span exactly covering the lemma surface; overlaps or gaps do not count. For multiple gold analyses, we use the corpuss primary analysis; for OOV morphemes, we match by longest contiguous offset overlap rather than string identity. We ignore boundaries at string start/end and punctuation-induced splits unless mirrored in gold; clitics/enclitics are evaluated within their host token as in the gold. Boundary metrics are micro-averaged over all decisions in the test set, with stability reported as means over three fixed-seed runs and 95% CIs from sentence-level bootstrap (1,000 resamples); degenerate vocabularies are excluded from compact summaries but retained in the appendix. Taken together, these results suggest that mid-sized vocabularies reliably balance segmentation fidelity with interpretability, especially on frequent paradigms, while still preserving lemma structure at rates useful for downstream tasks. The boundary and lemma-oriented diagnostics clarify where current tokenizers succeed (common paradigms, moderate affixation) and where they remain fragile (long suffix chains, rare lemmas). By consolidating the main findings into compact summaries and providing full tables for reproducibility, we aim to offer both an interpretable guide for practitioners and detailed record for future work. We hope this evaluation frameworkjointly reporting boundary alignment, lemma integrity, and sequence-level divergencecan serve as reference point for advancing tokenization in under-represented languages."
        },
        {
            "title": "6.4.1 Pretraining the Transformers\nFor each tokenizer configuration, we tokenized the pretraining corpus, converted it to BERT input\nformat, and trained on TPU (Jouppi et al. 2020). All models in this subsection were trained on the\nAlldata corpus with their respective tokenizers.",
            "content": "We broadly follow the BERT pretraining recipe. As in BERTurk, long-context position embeddings are exposed by mixing sequence lengths (BERTurk trained for 3M steps with 90% at length 128 and 10% at 512). To exploit TPU optimizations, tokenized text is chunked into fixed-length sequences (128/512). Our data preparation differs slightly: documents are segmented into sentences, and each 128token buffer is packed with as many consecutive sentences as fit, then padded. For sentences exceeding 128 tokens (which did not occur in practice), we fall back to splitting across multiple buffers. In our main runs, we train each model for 1M steps with sequence length 128. All experiments use Google TPU v2-8."
        },
        {
            "title": "6.4.2 Pretraining Times\nWord fragmentation directly affects the number of tokens after segmentation and, in turn, the total\npretraining time.",
            "content": "Pretraining wall-clock times on Google TPU v2-8 are given in Table 5."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "31 Table 5. Transformer training time (hours) by vocabulary size. Corpus Minimal 2k Medium 54 Alldata 74 5k 46 60 10k 20k 32k 52k 128k 43 48 50 41 45 38 42 42 36 37 33 34 34 key takeaway is that reduced fragmentation (larger vocabularies) shortens wall-clock substantially, especially on Alldata, so in the next section we examine whether these efficiency gains coincide with equal or better downstream accuracy."
        },
        {
            "title": "6.4.3 Downstream Results and Analysis\nTrGLUE First, we present TrGLUE results; for dataset sizes and evaluation metrics, see Table 1.\nFor all GLUE-style tasks, we use a simple scheme: single-sentence inputs are encoded and fed to a\nclassifier head, and two-sentence inputs are concatenated with a [SEP] token, encoded, and passed\nto the same head.",
            "content": "Table 6. results on CoLA Corpus Minimal 2k 0.0 Medium 0. Alldata 0.0 5k 0.0 0.0 0. 10k 0.0 0.0 0.0 20k 0. 0.09 0.00 32k 0.0 0.07 0. 52k 128k 0.11 0.12 0.11 0. 0.07 0.13 CoLA remains challenging across the board, with near-zero Matthews correlations for most vocabulary sizes and corpora, indicating limited grammatical acceptability generalization under current pretraining settings. Performance only lifts meaningfully at larger vocabularies and higher-data regimes: Minimal and Alldata show small gains at 52k128k (0.110.13), and Medium yields its best at 20k52k (0.090.12) before dropping at 128k. Overall, corpus scale helps slightly but is not sufficient on its own; improvements appear when paired with moderate-to-large vocabularies. The non-monotonic trend (e.g., Medium peaking at 52k, then declining) suggests optimization and tokenization interact in ways CoLA is sensitive to. These results recommend modestly larger vocabularies (52k128k) with richer corpora as default, alongside targeted tuning (longer pretraining or task-specific fine-tuning) to unlock further CoLA gains. Table 7. SST-2 accuracy across vocabulary sizes. Corpus 2k 5k 10k 20k 32k 52k 128k Minimal 83.63 82.83 84.57 82.98 84. 85.38 85.75 Medium 78.80 79.46 80.11 84. 84.45 84.97 84.67 Alldata 82.79 83. 84.08 84.08 84.71 85.67 85.47 SST-2 shows steady gains as vocabulary grows into the midlarge range, with the strongest accuracies clustering at 52k128k across corpora. Minimal improves from the low80s at 2k5k to 85.485.8 at 52k128k; Medium lags at very small vocabularies but catches up around 20k+, stabilizing near 84.785.0; Alldata is consistently strong once past 20k, reaching 85.585.7 at 52k128k. The pattern mirrors coverage effect: tiny vocabularies underfit sentiment cues, while mid-size and"
        },
        {
            "title": "Duygu Altinok",
            "content": "larger inventories recover polarity markers and idioms. Practically, 32k52k is safe default, with 52k offering small but reliable edge without clear gains from pushing to 128k. Table 8. MNLI (matched/mismatched accuracy) across vocabulary sizes. Corpus 2k 5k 10k 20k 32k 52k 128k Minimal 0.80/0. 0.77/0.77 0.81/0.82 0.79/0.80 0.74/0.73 0.83/0.85 0.82/0. Medium 0.74/0.74 0.76/0.76 Alldata 0.76/0.77 0.78/0.79 0.71/0.70 0.80/0.81 0.82/0.84 0.82/0.84 0.82/0.84 0.81/0.84 0.82/0.83 0.82/0. 0.83/0.85 0.82/0.84 MNLI is comparatively robust and clearly benefits from corpus scale once vocabularies reach mid-range. With Minimal data, accuracy is already solid (0.790.85 m/mm across 10k52k), and Medium/Alldata reliably land in the 0.820.85 band from 20k upward with only minor fluctuations. The differences between Medium and Alldata are small at 20k, suggesting diminishing returns from additional data once coverage is adequate. practical default for MNLI is 32k52k vocabulary with Medium or Alldata, which delivers consistently strong matched/mismatched accuracy. Table 9. MRPC (F1/Accuracy) across vocabulary sizes. Corpus 2k 5k 10k 20k 32k 52k 128k Minimal 0.58/0.41 0.58/0.42 0.61/0.50 0.62/0.55 0.60/0. 0.60/0.54 0.63/0.57 Medium 0.55/0.40 0.57/0.41 0.55/0.41 0.61/0. 0.65/0.57 0.62/0.51 0.62/0.54 Alldata 0.56/0.42 0.58/0. 0.59/0.46 0.59/0.47 0.61/0.49 0.63/0.55 0.63/0.54 MRPC exhibits clearer improvements than CoLA but remains optimization-sensitive. Minimal rises from roughly 0.58/0.41 at 2k to 0.63/0.57 at 128k, indicating steady gains with vocabulary size even under limited data. Medium and Alldata converge to the 0.610.65 F1 and 0.470.57 accuracy band once vocabulary 20k, with Medium peaking around 32k (0.65/0.57). The non-monotonicity across vocabulary sizes and corpora suggests interactions between tokenizer granularity and finetuning stability; in practice, 32k52k with Medium/Alldata is robust default, but modest tuning (smaller learning rate, slightly longer schedules, early stopping) is likely needed to consistently push MRPC toward its upper range. Table 10. STS-B (Pearson/Spearman) across vocabulary sizes. Corpus 2k 5k 10k 20k 32k 52k 128k Minimal 0.22/0.22 0.22/0. 0.63/0.68 0.63/0.68 0.63/0.68 0.68/0.66 0.22/0.21 Medium 0.66/0. 0.55/0.60 0.22/0.24 0.24/0.27 0.62/0.63 0.17/0.17 0.53/0. Alldata 0.64/0.66 0.59/0.64 0.62/0.62 0.68/0.67 0.52/0. 0.64/0.66 0.65/0.66 STS-B patterns similarly to MNLI in its responsiveness to corpus scale, but shows greater sensitivity at small vocabularies. Minimal and Medium are weak or unstable at 2k5k, while performance stabilizes and improves markedly by 20k52k, where Medium/Alldata achieve the best and most consistent Pearson/Spearman/combined scores. Differences between Medium and Alldata narrow in this mid/large range, indicating that sufficiently expressive tokenizer plus moderate-to-large pretraining data is the key driver. As with MNLI, 32k52k with Medium or Alldata is reliable default for sentence similarity."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "33 Key takeaways from the TrGLUE results include: Vocabulary size (overall pattern): Very small vocabularies (2k5k) yield weak and unstable outcomes across tasks. Performance becomes reliably strong from 20k upward, with 32k52k emerging as the most consistent sweet spot across corpora. Diminishing returns at high vocab: Beyond 52k, gains taper and can turn non-monotonic. The 128k setting occasionally helps specific setups but is not uniformly better; it often introduces sensitivity to optimization and dataset composition. Corpus scaling effect: The largest, most dependable improvement comes from scaling data from Minimal to Medium. Moving from Medium to Alldata adds gains, but these are typically smaller once the vocabulary is already 20k32k. Effective pairing (interaction): The benefits of larger vocabularies materialize only with sufficient pretraining data. Medium (32k52k) captures most of the attainable gains; upgrading to Alldata mainly reduces variance and nudges plateaus rather than transforming performance. Coverage vs. granularity trade-off: Small vocabularies under-cover morphology and rare forms, especially under Minimal data, compounding errors. Moderate vocabularies (32k52k) balance subword granularity and token coverage, enabling stable learning without fragmenting words excessively. Stability considerations: As vocabulary grows past 52k, tokenization becomes finer and sequences lengthen, which can increase optimization fragility (e.g., sensitivity to learning rate, warmup, and batch size). This is most visible when corpus size is not maximal. Budget-aware guidance: If compute/data are constrained, prioritize Medium-scale pretraining and choose 32k52k vocabulary; this configuration delivers strong, repeatable results with good efficiency. Alldata is best used to harden robustness and extract incremental gains. When to try 128k: Consider 128k only if you have Medium or Alldata and can afford tuning (slightly lower LR, longer schedules, more warmup). Expect uneven benefits across tasks; treat it as an exploration, not default. What to avoid: 5k vocabulary should be reserved for ablations or diagnostic runs. Pairing very small vocabularies with Minimal data is especially brittle due to sparse token coverage and poor compositional representations. Practical default: For most scenarios, set vocabulary to 32k52k and corpus to at least Medium. Promote to Alldata when chasing the last few points or seeking robustness, keeping hyperparameters mildly conservative at higher vocabulary sizes. To complement the task-wise results, we next distill how vocabulary size and corpus scale shape tokenization behavior and morphology alignment, and how these factors, in turn, relate to downstream success."
        },
        {
            "title": "Subword statistics and downstream success",
            "content": "Fragmentation harms GLUE scores: In small-vocab settings (2k10k on large corpora), fertility 3.66.6 and continuation 0.690.98 create long intra-word chains and inflate sequence length. These configurations co-occur with lower CoLA MCC and SST-2 accuracy and yield less faithful explanations (flatter deletion curves, lower saliency-on-morph spans). Efficient-but-legible band yields peaks: The most reliable GLUE performance appears where fertility 1.41.7 and continuation 0.300.45  (Table 2)  . Vocabularies of 20k32k (Minimal/Medium; 32k52k on very large corpora) achieve this band, producing shorter sequences while preserving morph seamsand correspondingly higher CoLA MCC and SST-2 accuracy. Over-merging degrades generalization: Very large vocabularies (52k128k) compress aggressively (fertility 1.141.4; continuation 0.120.20), often fusing frequent inflections into single tokens."
        },
        {
            "title": "Duygu Altinok",
            "content": "This reduces explicit morphology and is associated with weaker generalization on morph-heavy phenomena, lowering CoLA MCC and subtly flattening SST-2 gains despite good efficiency. Pareto guidance for GLUE: Match corpus size to vocabulary. On small/medium corpora, 20k 32k reaches the efficient/legible band and delivers the strongest CoLA/SST-2 results; on very large corpora, 32k52k is safer. Avoid extremeshigh-fragmentation (2k10k) and high-fusion (128k without careful tuning)as both correlate with drops in CoLA/SST-2 and poorer explanation faithfulness. Net effect: subword settings that keep fertility in [1.4, 1.7] and continuation in [0.30, 0.45] align with the highest CoLA MCC and SST-2 accuracy while preserving explanation quality. We now connect these subword-level patterns to explicit morphology alignment signals."
        },
        {
            "title": "Morphology alignment and downstream success",
            "content": "Boundary alignment predicts task gains: Across vocabularies (5k52k), higher boundary F1µ on frequent paradigms (Common Nouns 0.730.82) and tolerable alignment on long suffix chains (Çekimli 0.560.59) coincide with better GLUE results: CoLA MCC and SST-2 accuracy both peak where boundary F1µ is highest (typically 20k32k). Lemma integrity reduces label errors: Configurations with higher LemmaBoundary / lemma_sing le_rate produce cleaner rationales and fewer polarity/role mistakes, translating into higher SST-2 accuracy and fewer CoLA acceptability flips on agreement/negation cases. Balanced segmentation maximizes both: Over-segmentation (e.g., 10k on large corpora) improves suffix recall but hurts precision and sequence agreement (higher CER/WER), yielding lower CoLA MCC and more brittle SST-2 decisions; under-segmentation (52k) hides inflections and harms generalization on morph-heavy constructs. Mid-range vocabularies (10k32k) that achieve F1µ 0.60 (Çekimli) and F1µ 0.78 (Common Nouns) deliver the best precisionrecall balance and the strongest CoLA/SST-2 scores. Attribution alignment tracks correctness: Instances where saliency mass falls on gold morph spans (negation, case, evidential, degree) show higher accuracy and steeper deletion/insertion curves; this alignment improves with corpus scale under 20k32k vocabularies and coincides with the observed GLUE peaks. In short, stronger morphology alignment (boundary F1µ, lemma integrity, and saliency-onmorph spans) is positively associated with higher CoLA MCC and SST-2 accuracy, with the clearest gains in the 20k32k vocabulary range. TrGLUE Explainability This section we offer explainability of our example sentences from CoLA and SST-2. We exhibited these sentences for word-level and morphology-aware subwords as well. We chose CoLA corpora for their richness, CoLA include rich representations in morphology due to including morphological violations. In the Figure 13 we see explainability results for our example CoLA sentence. We now turn to SST-2 to see whether the morphology-aware patterns persist in sentiment. As shown in Figure 14, token-level explanations for representative sentence consistently foreground the core negative cues while de-emphasizing hedged endings. Having established that sentiment predictions concentrate on semantically diagnostic spans despite segmentation differences, we now pivot to sequence labeling. In NER, where boundary fidelity directly determines token-level supervision, we test whether the same vocabulary-size effects translate into entity boundary precision and stability."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "35 (a) 2k vocabulary -Minimal corpus combination. LIME highlights the kullan ##ma ##bil ##en span as the dominant driver, but the signal is fragmented across multiple continuation pieces. This diffusion creates mild spillover to nearby tokens and lowers attribution sharpness. Overall, the violation is detectable but less clean due to higher segmentation granularity. (b) 32k vocabulary size - minimal corpus combination. The explanation mass is sharply localized on kullanma ##bilen, yielding compact, high-contrast peak with minimal spillover. Reduced fragmentation improves attribution stability and interpretability, making the offending span visually unambiguous. This cleaner focus aligns with the identical split observed across larger vocabularies. 32k-medium, 52k-medium and 52k-alldata produced the same subwords and weights. Figure 13. Explainability of the sentence Cümlede tek başına kullanmabilen, başka bir öğeye ihtiyaç duymayan biçimbirime bağımsız biçimbirimler denir. and violation focus at kullanmabilen across tokenizers. In both subfigures the explanation mass is sharply localized on the offending span with minimal spillover. NER We use the same encoderclassifier architecture as in the pre-Transformer tokenization section: pretrained Transformer encoder with linear head, word-level supervision via the first subword, and token-level cross-entropy training. Performance is computed by mapping back to word-level BIO tags and reporting sequence F1. Table 11. NER macro F1 across vocabulary sizes. Corpus 2k 5k 10k 20k 32k 52k 128k Minimal 0.596 0.620 0. 0.589 0.633 0.620 0.542 Books 0. 0.610 0.664 0.558 0.539 0.625 0. Alldata 0.540 0.560 0.580 0.690 0. 0.602 0.602 NER results are presented in Table 11. Across vocabulary and corpora sizes, we see consistent pattern: once the tokenizer reaches roughly 1020k types, performance on GLUE-style tasks (MNLI, MRPC) stabilizes and remains high through 52k128k, with only small matchedmismatched gaps on MNLI and modest, smooth gains on MRPC. In contrast, NER is more sensitive to both corpus and size: the best NER scores occur not simply at the largest vocab, but when vocabulary granularity and training corpus align with the domain (e.g., strong results at 10k20k for Minimal and at 128k for Books), while some large-vocab/corpus pairings underperform smaller, better-aligned ones. Put simply, GLUE tasks reward crossing sufficiency threshold in vocabulary size and then plateau, whereas NER benefits from targeted segmentation that preserves morpho-lexical cuesso corpus choice and segmentation style can outweigh raw size. This divergence suggests using medium-tolarge vocabularies for sentence-level GLUE benchmarks, but tuning corpus-specific tokenization for"
        },
        {
            "title": "Duygu Altinok",
            "content": "(a) Token-weight heatmap using minimal wordpiece vocab. Saliency clusters around the core negative judgment memnun etmedi, with additional activation on the cliché/insufficient-action span (fazla kli.. ., istenilen. .. verem. .. ) and the pacing critique (ağır ilerleyen). Some fragmentation appears on subwords like ##eme/##ilen, but the pattern still foregrounds dissatisfaction. Closing hedge (edebilir) receives low weight, keeping the overall attribution negative. (b) With leaner vocabulary, negative evidence is still concentrated but slightly more diffuse across subword splits. The maxima remain on memnun etmedi, followed by verem.. ./##eyen and ağır/ilerleyen. The cliché cue (fazla klişe) is visible though softened by subword segmentation. Sentence scaffolding (e.g., olarak, da) is minimally weighted, suggesting the models attention is driven by evaluative content rather than syntax. (c) Medium-size wordpiece tokenizer yields crisper peaks on semantically pivotal words. Etmedi is the single strongest token, paired with elevated weight on memnun, veremeyen, and ağır. The cliché complaint (fazla klişe) is distinctly highlighted. Weights over connective and function words stay uniformly light, and the permissive ending (tercih/edebilir) remains downweighted, indicating model focus on the negative appraisal. (d) Higher-resolution wordpiece coverage cleanly isolates key negatives. Etmedi and memnun dominate, while veremeyen, ağır, and the boredom span (Sıkı ##ldığı) receive consistent emphasis. The recommendation clause (pek... edemeyeceğim) is present but moderate, and the final concessive allowance (izlemeyi tercih edebilir) is intentionally subdued, reflecting strong overall negative stance with light hedge. (e) Byte-level/large-vocab tokenization produces sharp, contiguous highlights on full words. The strongest weights fall on etmedi, memnun, and veremeyen, with sustained support on ağır ilerleyen and the cliché marker fazla klişe. Discomfort/boredom cues (Sıkı... kısımlar) are clearly captured. Tokens tied to curiosity or allowance near the end carry comparatively low weight, preserving the dominant negative interpretation. Figure 14. Token-weight heatmaps across five tokenizers consistently highlight the core negative assessmentmemnun etmedi, fazla klişe, veremeyen, ağır ilerleyenwith low emphasis on the hedged closing, showing stable negative attribution despite segmentation differences. token-level, morphology-heavy tasks like NER. NER Explainability Similar to GLUE tasks, we offer explainability results on an example sentence from validation set. Figure 15 exhibits NER explainability on this sentence. Across all configurations, the largest weights concentrate on entity-core tokens: 1949/1/Ekim /1949 for DATE, Mao for PERSON, and the nucleus of Çin Halk Cumhuriyeti for GPE. Boundary pieces (apostrophes, -da/-ni) receive small positive weights as span cues, while surrounding function words and verbs are near zero, with punctuation slightly negative for contrast. Finer-grained tokenization (2k) spreads mass over more subwords but still peaks at core pieces; mid-range tokenization (32k/52k) concentrates importance cleanly on whole entity tokens; very large vocabularies (128k) keep high saliency on intact entity chunks. This distribution correlates with NER success: models that assign sharp, stable weight to entity cores and moderate weight to boundaries show"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "37 (a) 2k-Minimal: Entity saliency is fragmented across many subwords: strong reds on DATE pieces (19 ##4 ##9 da), high on Ma ##o (PERSON), and moderate on GPE core (Ç ##in ... Cumhur ##iyet ##i). Apostrophes and case clitics get light positives; non-entity context remains near zero. (b) 32k-Minimal: Importance concentrates on full entity tokens: strong on 194 ##9 da and 1 Ekim 194 ##9 da (DATE), sharp on Ma ##o (PERSON), and highest within Çin Halk Cumhuriyeti ni (GPE). Context words are largely neutral; punctuation slightly negative. The tokenization and model weights are same for 32k-Medium and 52k-Alldata. (c) Saliency collapses onto intact whole-word entities: 1949 (DATE), Mao (PERSON), and Cumhuriyeti within the GPE show the strongest weights; clitics and punctuation are minimal, indicating reliance on large, unbroken entity chunks. Figure 15. 128k-Medium: Explainability overlays for NER on the sentence 1949da önce Pekini daha sonra diğer birçok önemli şehri eline geçiren Mao, 1 Ekim 1949da Çin Halk Cumhuriyetini kurar. across tokenizers (2k Minimal; 32k Minimal/Medium/52k Alldata; 128k Alldata). Warm reds highlight high positive importance on entity cores (DATE, GPE, PERSON), lighter tints mark boundaries and clitics, and near-zero/blue shades mark non-entity context and punctuation. stronger, more consistent F1, while diffuse or misplaced weights (e.g., overemphasis on clitics or non-entity context) align with weaker spans and boundary errors. POSDEPMorph We use the same architecture as in the pre-Transformer tokenization section for POS, dependency parsing, and morphology. Table 12 reports results across corpora and vocabulary sizes. Table 12. BOUN POS/LAS/Morph across corpora and vocabulary sizes. Corpus Vocab 2k 5k 10k 20k 32k 52k 128k Minimal Medium Alldata .91/.63/.28 .91/.59/.26 .92/.63/.29 .88/.53/.30 .83/.43/. .93/.68/.29 .92/.68/.27 .90/.62/.28 .90/.62/.28 .90/.62/.28 .92/.68/. .92/.67/.27 .92/.66/.27 .92/.66/.31 .91/.64/.27 .91/.65/.27 .92/.66/. .92/.65/.28 .92/.66/.28 .93/.68/.28 .92/.68/.26 Across corpora and vocabulary sizes, performance shows consistent gains from very small vocabularies toward midlarge ranges, with clear peaks around 52k for Minimal and Alldata, and broadly flat tops from 20k128k for Medium. Minimal benefits most from increasing vocab, culminating in strong POS and LAS at 52k128k despite slight morph micro-accuracy dip at the largest size."
        },
        {
            "title": "Duygu Altinok",
            "content": "Medium is stable across 20k128k, with LAS and POS clustered tightly and occasional morph upticks (e.g., 20k and 128k). Alldata delivers the best overall balance, with 52k yielding the strongest combined syntactic scores (high LAS alongside high POS) and only marginal trade-offs in morphology; 128k maintains high POS/LAS but shows small morphology decline. Overall, tokenizer granularity helps up to mediumlarge vocabulary, after which returns taper for POS/LAS and may slightly harm morphology, suggesting sweet spot near 52k for large or mixed-domain corpora. Trained on varying corpora and vocabularies, the POSDepMorph results sharpen the picture: labeled structure (LAS) and POS improve markedly as vocabulary grows into the 32k52k band, with Minimal and Alldata both peaking around 52k, while morphology micro-accuracy tends to plateau earlier and soften at very large vocabularies (128k). This structural trajectory parallels TrGLUE where it matters: configurations that maximize LAS in the midlarge range are precisely those that yield stable MNLI and strong STS-B, indicating that better labeled dependencies translate into more reliable sentence-level inference and similarity. The divergence emerges on morph-sensitive behavior: as vocabularies over-merge at the top end, Morph accuracy dips and CoLA stallsevidence that acceptability judgments depend on explicit access to inflectional cues that large subwords can obscure. Corpus scaling reinforces the pattern: moving from Minimal to Medium lifts both LAS and TrGLUE substantially, while MediumAlldata yields smaller, variance-reducing gains once the tokenizer sits in the 32k52k efficient-but-legible zone. In short, the POSDEPMorph suite points to Medium(32k52k) as the structural sweet spot that generalizes best to TrGLUEs NLI/similarity tasks, whereas pushing vocabulary higher risks eroding morphology and, with it, CoLA. POS-DEP-Morph Explainability To probe how our shared encoder distributes evidence across linguistic tasks, we derive single, task-aware attribution map for each sentencemerging POS, dependency, and morphology salienciesso we can visualize where the model grounds its decisions and how syntactic and morphological cues interact at the token level. We compute single, task-merged token attribution heatmap using Integrated Gradients (IG) over the shared encoder embeddings with loss-weighted fusion across heads. For each input token i, we define three scalar targets: the POS cross-entropy at i, the dependency joint loss at (arc loss to the gold head plus relation loss given that head), and the morphology multi-label loss at (sum over active features). We run IG from zero-embedding baseline to the actual embedding, take the L2 norm of the attribution vector per token, and normalize each heads saliency to sum to 1 over tokens. To produce the merged heatmap, we compute convex combination s_merged = w_pos s_pos + w_dep s_dep + w_morph s_morph, where weights are the per-example head losses normalized to sum to 1 (giving more influence to the head that matters most for that instance). For visualization, subword attributions are summed to the original word and then re-distributed uniformly across its subwords to keep the grid consistent. This yields single, faithful, task-aware saliency map while preserving interpretability across POS, dependency, and morphology. Taken together, Figure 16 shows how shared encoder concentrates evidence where the joint objective is most informativeon the finite predicatewhile still surfacing syntactically and morphologically diagnostic cues on arguments and case/possession markers when the tokenizer makes them legible. As vocabularies move into the midlarge range, attribution becomes both crisper and better aligned with linguistic structure, paralleling gains in POS/LAS. Beyond that range, undersegmentation reduces the visibility of non-verb morphology, echoing the small Morph dips we observe. Thus, explainability and accuracy trace the same curve: the 32k52k band yields the most faithful, task-consistent rationales, while very large vocabularies trade legibility for marginal returns."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "39 (a) 2k-Minimal: Attributions diffuse across tokens with peaks on the finite verb chain (kar + ##alı + ##yor + ##dum). Non-verb morphology (Loc, Plur) is visible but weaker, reflecting lower LAS and stronger reliance on overt inflectional cues. (b) 32k-Minimal: Sharper, more localized saliency: predicate morphemes dominate, with clearer emphasis on object öyküler and oblique çizgisi+nde, consistent with mid-vocab LAS gains. (c) 32k-Medium: Attributions are sharp and linguistically aligned: highest saliency on the finite predicate chain (kara + ##lıyordum), with clear emphasis on the object (öyküler) and the oblique (çizgisi + ##nde), while possessive/genitive cues (öykü ##cül ##üğü ##n) are moderately weightedmatching strong LAS/POS with stable morphology at this vocabulary size. (d) 52k-Alldata: Most balanced map: strong predicate focus alongside higher weights on possessive/genitive and case markers (öykücülüğün, çizgisi+nde) and plural object (öyküler), aligning with peak POS/LAS and stable morphology. (e) 128k-Medium: High confidence on the predicate (karalıyordum) but reduced visibility of non-verb morphology; larger subwords under-segment inflectional cues, consistent with slight Morph accuracy softening at very large vocabularies. Figure 16. Merged POSDepMorph token attributions for the sentence Bir yandan da benimsediğim modern öykücülüğün çizgisinde öyküler karalıyordum. We visualize single task-aware heatmap per tokenizer by fusing head-specific Integrated Gradients over the shared encoder (loss-weighted combination; subword scores aggregated to words for display). As vocabulary size and training data increase from 2k-Minimal to 52k-Alldata, attributions become more focused and linguistically aligned: the predicates inflectional chain remains the main evidence source while object, oblique, and possessive cues gain salience. At 128k, predicate salience persists but non-verb morphology attenuates, mirroring the observed plateau in POS/LAS and mild decline in Morph accuracy at very large vocabularies.. downstream accuracy. We keep scope to BERT-style models trained with our WP variants; later section compares word/char/subword families more broadly. RQ1 (Vocabulary size vs. task class). Midlarge WP vocabularies (32k52k) strike the best overall balance: they improve POS/LAS and maintain strong results on semantics-oriented benchmarks (e.g., SST-2/STS-like tasks). Very small vocabularies (5k) over-fragment inputs, inflate sequences, and depress parsing and acceptability; very large vocabularies (128k) under-segment morphology and yield diminishing or negative returns on morph-sensitive tasks. RQ2 (Morphological alignment helps syntax/morph more than semantics/NER). Higher boundary F1 and better lemma integrity correlate strongly with gains in POS/DEP/Morph and CoLA, while effects on sentiment/semantic tasks and NER are smaller or task-specific. NER benefits more from stable entity segmentation (reduced over-fragmentation of names/numbers) than from"
        },
        {
            "title": "Duygu Altinok",
            "content": "fine-grained morpheme boundaries, explaining slightly higher optimal vocab on NER. RQ3 (Training data scale vocabulary size Scaling tokenizer training data from 52080 GB improves robustness and reduces variance once vocabulary is in the 32k52k band. With tiny vocabularies, additional data does not overcome over-fragmentation; with huge vocabularies, more data cannot recover lost morphological legibility. The interaction is complementary: mixed-domain data plus midlarge vocabulary delivers the most reliable gains. RQ4 (Pareto region of tokenization). There is clear trade-off among (i) sequence length (favoring larger vocabularies), (ii) morphological fidelity (favoring smaller vocabularies), and (iii) downstream accuracy. The efficient operating region is 32k52k on mixed-domain data: fertility remains moderate (1.41.7), continuation sits in 0.300.45, sequences are compact, and POS/LAS and SST-like tasks are strong. Pushing beyond this band shortens sequences further but erodes morphology and hurts morph-sensitive tasks. Practical prescription for WordPiece tokenizers. Default for broad Turkish NLP: mixed-domain WP vocabulary in the 32k52k band. Morph/grammar-sensitive pipelines (parsing, CoLA, morph tagging): favor the lower end (20k32k) or use morphology-aware constraints if available. NER-heavy applications: lean toward the higher end of the band (32k52k) or protect entity stems via tokenizer customization. Explainability alignment. Attribution analyses mirror these trends: midlarge vocabularies concentrate saliency on predicate morphology and syntactically diagnostic spans while keeping entities intact; tiny vocabularies diffuse saliency across long subword chains, and huge vocabularies hide inflectional cues. The configurations that peak on POS/LAS also yield the most faithful, task-consistent rationales. In sum, midlarge WordPiece vocabularies paired with mixed-domain training data offer the most reliable accuracyinterpretability trade-off for Turkish Transformers, preserving inflectional cues that drive syntax and acceptability while maintaining compact sequences and strong entity handling. 7. Optimal Ways of Tokenizing Turkish This section synthesizes what character, word, morphology-aware subword, and WordPiece tokenization offer for Turkish, with an emphasis on how each balances sequence length, morphology visibility, robustness, and downstream accuracy. Character-level tokenization maximizes coverage by construction: there are no OOV tokens, and all inflectional material is explicit in the sequence. This yields strong recall of morphological cues and robustness to noise and spelling variants. The downside is severe fragmentationsequences are long, training is slower, and attribution tends to diffuse across many characters. When compute is ample, character models can perform well on morphology-sensitive tasks, but their efficiency and long-range semantic modeling are comparatively weaker. Word-level tokenization sits at the opposite extreme. Sequences are short and training/inference are fast; entity stability is excellent because names and numbers remain intact. However, for an agglutinative language like Turkish, the closed vocabulary leads to high OOV and sparse statistics on unseen inflections, making morphology opaque and hurting generalization across paradigms. In practice, word-level models can be competitive on NER with careful OOV handling, but they underperform on POS/DEP/CoLA where inflectional cues matter. Morphology-aware subword tokenization explicitly aligns units with stems and affixes using analyzers or rules. This alignment improves boundary precision/recall and lemma integrity, yielding interpretable rationales and consistent gains on POS, dependency parsing, morphological tagging, and acceptability judgments. The trade-off is dependency on linguistic resources and some risk of"
        },
        {
            "title": "Cambridge Default Journal",
            "content": "41 over-segmentation for very frequent forms if frequency signals are not incorporated. Effects on semantics and NER are smaller unless the scheme is adapted to preserve entity stems. WordPiece offers data-driven middle path that can be tuned via vocabulary size. In our experiments, midlarge vocabularies (roughly 32k52k on mixed-domain data) achieve the most reliable balance: sequences remain compact, fertility and continuation sit in favorable ranges, and accuracy is strong across syntax/morphology and semantic tasks. Very small vocabularies overfragment and inflate sequences, while very large vocabularies over-merge and obscure morphology, both of which degrade performance on morph-sensitive benchmarks. NER often prefers the higher end of the midlarge range, where entities remain more intact. Taken together, no single approach dominates across all tasks. Morphology-aware subwords provide the highest linguistic fidelity and the clearest explanations on morph-bearing spans. WordPiece, when kept in the midlarge regime and trained on mixed-domain data, delivers the most dependable end-to-end trade-off of accuracy, efficiency, and portability for Turkish, with small adjustmentstoward smaller units for parsing and acceptability, or toward larger units for NERdepending on application needs. 8. Conclusion We investigated how tokenizer design shapes Turkish Transformer performance, connecting subword granularity to both efficiency and linguistic fidelity. Across extensive experiments, consistent picture emerges: midlarge WordPiece vocabularies trained on mixed-domain data offer the most reliable accuracyefficiency trade-off, with fertility and continuation in morphology-visible ranges. This regime preserves inflectional cues critical for POS/DEP/morph tagging and acceptability, while maintaining strong results on semantic and NER benchmarks. Morphology-aware subword schemes can further boost morph-sensitive tasks and interpretability, though they require linguistic resources; character and word extremes provide robustness or speed, respectively, but underperform on one or more dimensions. Practically, we recommend WordPiece in the 32k52k band as default for broad Turkish NLP, nudging smaller for grammar-centric pipelines and larger for NER-heavy applications. Beyond these prescriptions, our analysis frameworklinking fragmentation metrics, morphology alignment, and attribution to downstream accuracyoffers general template for tokenization choices in other morphologically rich languages. Future work includes extending the comparison to alternative subword learners, longer-context pretraining, and cross-lingual validation in related agglutinative families. Acknowledgments We gratefully acknowledge support from the Google TPU Research Cloud program, which provided the compute resources used in this work. Data Availability All datasets used in this studyincluding the pretraining corpus and benchmarking suitesare openly available under permissive licenses suitable for commercial use. Ethical Standards This research complies with relevant ethical guidelines and legal requirements in the jurisdictions of data collection and use. No personally identifiable information was processed beyond what is permitted by the data licenses."
        },
        {
            "title": "Notes",
            "content": ""
        },
        {
            "title": "References",
            "content": "Akin, Ahmet Afşin, and Mehmet D. Akin. 2007. Zemberek, an open source nlp framework for turkic languages. In Proceedings of the 7th international conference on natural language processing (icon). Hyderabad, India. https://github.com/ahmetaa/zembereknlp. Altinok, Duygu. 2023a. diverse set of freely available linguistic resources for Turkish. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), edited by Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, 1373913750. Toronto, Canada: Association for Computational Linguistics, July. https: //doi.org/10.18653/v1/2023.acl-long.768. https://aclanthology.org/2023.acl-long.768/. . 2023b. statistical approach to analyzing turkish morphology. In Advances in automation, mechanical and design engineering, edited by Med Amine Laribi, Giuseppe Carbone, and Zhiyu Jiang, 165180. Cham: Springer International Publishing. ISBN: 978-3-031-09909-0. . 2024. Bella turca: large-scale dataset of diverse text sources for turkish language modeling. In Text, speech, and dialogue, edited by Elmar Nöth, Aleš Horák, and Petr Sojka, 196213. Cham: Springer Nature Switzerland. ISBN: 978-3-031-70563-2. . 2025. Introducing trglue and sentiturca: comprehensive benchmark for turkish general language understanding and sentiment analysis. arXiv: 2512.22100 [cs.CL]. https://arxiv.org/abs/2512.22100. Cer, Daniel, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017), edited by Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens, 114. Vancouver, Canada: Association for Computational Linguistics, August. https: //doi.org/10.18653/v1/S17-2001. https://aclanthology.org/S17-2001/. Clark, Kevin, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: pre-training text encoders as discriminators rather than generators. In Iclr. https://openreview.net/pdf ?id=r1xMH1BtvB. Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), edited by Jill Burstein, Christy Doran, and Thamar Solorio, 41714186. Minneapolis, Minnesota: Association for Computational Linguistics, June. https://doi.org/10.18653/v1/N19-1423. https://aclanthology.org/N19-1423/. Dolan, William B., and Chris Brockett. 2005. Automatically constructing corpus of sentential paraphrases. In Proceedings of the third international workshop on paraphrasing (IWP2005). https://aclanthology.org/I05-5002/. Erkaya, Erencan, and Tunga Güngör. 2023. Analysis of subword tokenization approaches for turkish language. In 2023 31st signal processing and communications applications conference (siu), 14. https://doi.org/10.1109/SIU59756.2023.10223973. Honnibal, Matthew, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python, https://doi.org/10.5281/zenodo.1212303. Jouppi, Norman P., Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David A. Patterson. 2020. domain-specific supercomputer for training deep neural networks. Communications of the ACM 63:6778. https://api.semanticscholar.org/CorpusID:219843141. Kaya, Yiğit Bekir, and A. Cüneyd Tantuğ. 2024. Effect of tokenization granularity for turkish large language models. Intelligent Systems with Applications 21:200335. ISSN: 2667-3053. https://doi.org/https://doi.org/10.1016/j.iswa.2024.200335. https://www.sciencedirect.com/science/article/pii/S2667305324000115. Kudo, Taku. 2018. Subword regularization: improving neural network translation models with multiple subword candidates. In Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: long papers), edited by Iryna Gurevych and Yusuke Miyao, 6675. Melbourne, Australia: Association for Computational Linguistics, July. https://doi.org/10.18653/v1/P18-1007. https://aclanthology.org/P18-1007/. Marneffe, Marie-Catherine de, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. Universal Dependencies. Computational Linguistics (Cambridge, MA) 47, no. 2 (June): 255308. https : / / doi . org / 10 . 1162 / coli _ _ 00402. https://aclanthology.org/2021.cl-2.11/. Marşan, Büşra, Salih Furkan Akkurt, Muhammet Şen, Merve Gürbüz, Onur Güngör, Şaziye Betül Özateş, Suzan Üsküdarlı, Arzucan Özgür, Tunga Güngör, and Balkız Öztürk. 2022. Enhancements to the boun treebank reflecting the agglutinative nature of turkish. arXiv preprint arXiv:2207.11782. Minaee, Shervin, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: survey. arXiv: 2402.06196 [cs.CL]. https://arxiv.org/abs/2402.06196."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "43 Oflazer, Kemal. 1994. Two-level description of turkish morphology. Literary and Linguistic Computing 9 (2): 137148. https://doi.org/10.1093/llc/9.2.137. Ortiz Suarez, Pedro Javier, Laurent Romary, and Benoit Sagot. 2020. monolingual approach to contextualized word embeddings for mid-resource languages. In Proceedings of the 58th annual meeting of the association for computational linguistics, edited by Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, 17031714. Online: Association for Computational Linguistics, July. https://doi.org/10.18653/v1/2020.acl-main.156. https://aclanthology.org/2020.aclmain.156. Ribeiro, Marco, Sameer Singh, and Carlos Guestrin. 2016. why should trust you?: explaining the predictions of any classifier. In Proceedings of the 2016 conference of the north American chapter of the association for computational linguistics: demonstrations, edited by John DeNero, Mark Finlayson, and Sravana Reddy, 97101. San Diego, California: Association for Computational Linguistics, June. https://doi.org/10.18653/v1/N16-3020. https://aclanthology.org/N16-3020/. Schweter, Stefan. 2020. Berturk - bert models for turkish. V. 1.0.0, April. https://doi.org/10.5281/zenodo.3770924. https: //doi.org/10.5281/zenodo.3770924. Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. arXiv: 1508.07909 [cs.CL]. https://arxiv.org/abs/1508.07909. Socher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, 16311642. Seattle, Washington, USA: Association for Computational Linguistics, October. https://www.aclweb.org/anthology/D13-1170. Toraman, Cagri, Eyup Halit Yilmaz, Furkan Şahi.nuç, and Oguzhan Ozcelik. 2023. Impact of tokenization on language models: an analysis for turkish. ACM Trans. Asian Low-Resour. Lang. Inf. Process. (New York, NY, USA) 22, no. 4 (March). ISSN: 2375-4699. https://doi.org/10.1145/3578707. https://doi.org/10.1145/3578707. Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP workshop BlackboxNLP: analyzing and interpreting neural networks for NLP, edited by Tal Linzen, Grzegorz Chrupała, and Afra Alishahi, 353355. Brussels, Belgium: Association for Computational Linguistics, November. https://doi.org/10.18653/ v1/W18-5446. https://aclanthology.org/W18-5446. Warstadt, Alex, Amanpreet Singh, and Samuel Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471. Williams, Adina, Nikita Nangia, and Samuel Bowman. 2018. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: human language technologies, volume 1 (long papers), edited by Marilyn Walker, Heng Ji, and Amanda Stent, 1112 1122. New Orleans, Louisiana: Association for Computational Linguistics, June. https://doi.org/10.18653/v1/N18-1101. https://aclanthology.org/N18-1101/. Wolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. Transformers: state-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, edited by Qun Liu and David Schlangen, 3845. Online: Association for Computational Linguistics, October. https : / / doi . org / 10 . 18653 / v1 / 2020 . emnlp - demos . 6. https : //aclanthology.org/2020.emnlp-demos.6/. Zhuang, Liu, Lin Wayne, Shi Ya, and Zhao Jun. 2021. robustly optimized BERT pre-training approach with post-training [in eng]. In Proceedings of the 20th chinese national conference on computational linguistics, edited by Sheng Li, Maosong Sun, Yang Liu, Hua Wu, Kang Liu, Wanxiang Che, Shizhu He, and Gaoqi Rao, 12181227. Huhhot, China: Chinese Information Processing Society of China, August. https://aclanthology.org/2021.ccl-1.108/. Appendix 1. Explainability Tools We used LIME Ribeiro, Singh, and Guestrin 2016 to produce local, model-agnostic explanations of tokenizer behavior by learning sparse linear surrogate around each instance via perturbed inputs. Concretely, we generate perturbations by masking or merging subword units while preserving character order, query the model on these variants, and weight them by proximity to the original example. LIME then fits simple interpretable model whose coefficients act as token-level attributions, indicating which subwords increase or decrease evidence for correct boundary alignment and related diagnostics (e.g., lemma consistency). We visualize the top-contributing subwords with signed"
        },
        {
            "title": "Duygu Altinok",
            "content": "weights to qualitatively inspect over-/under-segmentation and boundary errors across vocabulary sizes. Appendix 2. Comprehensive Morphology Diagnostics by Vocabulary Size Table 13. Minimal corpus: morphology-aware tokenization diagnostics across vocabulary sizes. C.Noun and C.Verb abbreviate Common Noun and Common Verb. Sw/W denotes Subwords per Word. LSingle, LBoun, and ExMatch denote LemmaSingle, LemmaBoundary, and ExactMatch. Vocab Split Sw/W Pµ Rµ F1µ PM RM F1M LSingle LBoun ExMatch OverSeg UnderSeg 2k 2k 2k 5k 5k 5k 10k 10k 10k 20k 20k 20k 32k 32k 32k 52k 52k 52k Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0. C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0.53 Çekimli 4.02 0.67 0.48 0.56 0.71 0.51 0.58 C.Noun 2.33 0.79 0.52 0.63 0.83 0.53 0.64 C.Verb 2.13 0.83 0.39 0.53 0.86 0.40 0.54 Çekimli 5.15 0.61 0.56 0.59 0.63 0.58 0.59 C.Noun 3.02 0.85 0.72 0.78 0.87 0.72 0.78 C.Verb 2.86 0.74 0.48 0.58 0.77 0.49 0.59 Çekimli 6.96 0.56 0.69 0.62 0.57 0.70 0.62 C.Noun 4.07 0.66 0.74 0.70 0.69 0.75 0.71 C.Verb 4.04 0.78 0.71 0.75 0.80 0.72 0.75 Çekimli 4.77 0.64 0.54 0.58 0.66 0.56 0.59 C.Noun 2.81 0.82 0.64 0.72 0.84 0.66 0.72 C.Verb 2.48 0.81 0.45 0.58 0.83 0.46 0.58 Çekimli 4.49 0.65 0.52 0.58 0.68 0.54 0.59 C.Noun 2.64 0.77 0.57 0.66 0.80 0.59 0.67 C.Verb 2.36 0.79 0.42 0.55 0.83 0.43 0.56 128k 128k 128k Çekimli 6.02 0.60 0.65 0.63 0.61 0.66 0.62 C.Noun 3.34 0.85 0.79 0.82 0.87 0.80 0.82 C.Verb 3.51 0.80 0.63 0.70 0.81 0.64 0. 0.00 0.00 0.00 0.58 0.96 0. 0.32 0.95 0.99 0.08 0.50 0. 0.39 0.96 0.99 0.46 0.96 0. 0.17 0.83 0.98 1.00 1.00 1. 0.49 0.19 0.00 0.58 0.83 0. 0.68 0.66 0.95 0.56 0.59 0. 0.54 0.41 0.02 0.63 0.88 0. 0.00 0.00 0.00 0.01 0.04 0. 0.01 0.16 0.02 0.01 0.12 0. 0.01 0.12 0.02 0.01 0.08 0. 0.01 0.22 0.04 3.37 3.11 2. 0.76 0.66 0.48 0.98 0.87 0. 1.32 1.15 0.92 0.90 0.80 0. 0.85 0.75 0.54 1.15 0.95 0. 0.31 0.33 0.36 1.44 1.63 2. 1.12 1.24 1.64 0.82 0.93 1. 1.21 1.34 1.90 1.29 1.43 2. 0.95 1.11 1."
        },
        {
            "title": "Cambridge Default Journal",
            "content": "45 Table 14. Medium corpus: morphology-aware tokenization diagnostics across vocabulary sizes. C.Noun and C.Verb abbreviate Common Noun and Common Verb. Sw/W denotes Subwords per Word. LSingle, LBoun, and ExMatch denote LemmaSingle, LemmaBoundary, and ExactMatch. Vocab Split Sw/W Pµ Rµ F1µ PM RM F1M LSingle LBoun ExMatch OverSeg UnderSeg 2k 2k 2k 5k 5k 5k 10k 10k 10k 20k 20k 20k 32k 32k 32k 52k 52k 52k Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0. C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0.53 Çekimli 3.91 0.68 0.48 0.56 0.71 0.50 0.57 C.Noun 2.36 0.80 0.52 0.63 0.82 0.54 0. C.Verb 2.06 0.84 0.39 0.53 0.87 0.40 0.54 Çekimli 5.46 0.60 0.58 0.59 0.62 0.60 0.60 C.Noun 3.16 0.85 0.75 0.80 0.87 0.76 0. C.Verb 3.14 0.77 0.54 0.64 0.78 0.55 0.64 Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0. Çekimli 4.84 0.63 0.54 0.58 0.65 0.56 0.59 C.Noun 2.83 0.86 0.68 0.76 0.87 0.69 0.76 C.Verb 2.69 0.72 0.43 0.54 0.75 0.45 0. Çekimli 4.47 0.64 0.51 0.57 0.67 0.53 0.58 C.Noun 2.65 0.80 0.59 0.68 0.82 0.61 0.69 C.Verb 2.41 0.79 0.42 0.55 0.81 0.44 0. 128k 128k 128k Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0.53 0.00 0.00 0.00 0.59 0. 0.99 0.25 0.83 0.99 0.00 0. 0.00 0.37 0.95 0.99 0.45 0. 0.99 0.00 0.00 0.00 1.00 1. 1.00 0.47 0.22 0.01 0.60 0. 0.30 1.00 1.00 1.00 0.56 0. 0.05 0.53 0.50 0.03 1.00 1. 1.00 0.00 0.00 0.00 0.01 0. 0.00 0.01 0.19 0.03 0.00 0. 0.00 0.01 0.14 0.02 0.01 0. 0.01 0.00 0.00 0.00 3.37 3. 2.88 0.74 0.67 0.47 1.04 0. 0.72 3.37 3.11 2.88 0.92 0. 0.61 0.85 0.76 0.55 3.37 3. 2.88 0.31 0.33 0.36 1.48 1. 2.27 1.05 1.17 1.48 0.31 0. 0.36 1.19 1.32 1.74 1.29 1. 1.95 0.31 0.33 0."
        },
        {
            "title": "Duygu Altinok",
            "content": "Table 15. alldata corpus: morphology-aware tokenization diagnostics across vocabulary sizes. C.Noun and C.Verb abbreviate Common Noun and Common Verb. Sw/W denotes Subwords per Word. LSingle, LBoun, and ExMatch denote LemmaSingle, LemmaBoundary, and ExactMatch. Vocab Split Sw/W Pµ Rµ F1µ PM RM F1M LSingle LBoun ExMatch OverSeg UnderSeg 2k 2k 2k 5k 5k 5k 10k 10k 10k 20k 20k 20k 32k 32k 32k 52k 52k 52k Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0.53 Çekimli 4.01 0.68 0.49 0.57 0.71 0.51 0.58 C.Noun 2.37 0.83 0.54 0.66 0.85 0.56 0.66 C.Verb 2.14 0.83 0.40 0.54 0.86 0.41 0.55 Çekimli 7.85 0.54 0.75 0.63 0.54 0.76 0.62 C.Noun 5.12 0.55 0.78 0.64 0.56 0.78 0.65 C.Verb 4.86 0.75 0.82 0.79 0.77 0.83 0.79 Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0.53 Çekimli 5.33 0.60 0.57 0.59 0.62 0.59 0.59 C.Noun 3.05 0.86 0.72 0.79 0.88 0.74 0.79 C.Verb 3.10 0.77 0.53 0.63 0.78 0.55 0.63 Çekimli 4.71 0.63 0.53 0.57 0.65 0.55 0.58 C.Noun 2.87 0.78 0.62 0.70 0.81 0.64 0.70 C.Verb 2.57 0.80 0.46 0.58 0.82 0.47 0.59 128k 128k 128k Çekimli 18.17 0.31 1.00 0.47 0.31 1.00 0.47 C.Noun 11.05 0.33 1.00 0.49 0.33 1.00 0.49 C.Verb 12.70 0.35 1.00 0.52 0.36 1.00 0. 0.00 0.00 0.00 0.56 0.95 0. 0.04 0.12 0.69 0.00 0.00 0. 0.28 0.95 0.99 0.40 0.95 0. 0.00 0.00 0.00 1.00 1.00 1. 0.48 0.22 0.01 0.70 0.69 0. 1.00 1.00 1.00 0.59 0.88 0. 0.55 0.55 0.04 1.00 1.00 1. 0.00 0.00 0.00 0.01 0.06 0. 0.00 0.02 0.18 0.00 0.00 0. 0.01 0.17 0.03 0.01 0.10 0. 0.00 0.00 0.00 3.37 3.11 2. 0.76 0.68 0.49 1.49 1.46 1. 3.37 3.11 2.88 1.01 0.87 0. 0.89 0.82 0.59 3.37 3.11 2. 0.31 0.33 0.36 1.44 1.59 2. 0.72 0.73 0.94 0.31 0.33 0. 1.08 1.23 1.51 1.23 1.31 1. 0.31 0.33 0."
        }
    ],
    "affiliations": [
        "Independent Researcher, Berlin, Germany"
    ]
}