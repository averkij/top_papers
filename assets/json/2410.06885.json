{
    "paper_title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
    "authors": [
        "Yushen Chen",
        "Zhikang Niu",
        "Ziyang Ma",
        "Keqi Deng",
        "Chunhui Wang",
        "Jian Zhao",
        "Kai Yu",
        "Xie Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . e [ 2 5 8 8 6 0 . 0 1 4 2 : r F5-TTS: Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching Yushen Chen1, Zhikang Niu1, Ziyang Ma1, Keqi Deng2 Chunhui Wang3, Jian Zhao3, Kai Yu1, Xie Chen1 1Shanghai Jiao Tong University, 2University of Cambridge, 3Geely Automobile Research Institute (Ningbo) Company Ltd. {swivid,chenxie95}@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces F5-TTS, fully non-autoregressive text-to-speech system based on ﬂow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with ﬁller tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we ﬁrst model the input with ConvNeXt to reﬁne the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which signiﬁcantly improves our models performance and efﬁciency. This sampling strategy for ﬂow step can be easily applied to existing ﬂow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efﬁciency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development2."
        },
        {
            "title": "1 Introduction",
            "content": "Recent research in Text-to-Speech (TTS) has experienced great advancement [1, 2, 3, 4, 5, 6, 7, 8]. With few seconds of audio prompt, current TTS models are able to synthesize speech for any given text and mimic the speaker of audio prompt [9, 10]. The synthesized speech can achieve high ﬁdelity and naturalness that they are almost indistinguishable from human speech [11, 12, 13, 14]. While autoregressive (AR) based TTS models exhibit an intuitive way of consecutively predicting the next token(s) and have achieved promising zero-shot TTS capability, the inherent limitations of AR modeling require extra efforts addressing issues such as inference latency and exposure bias [15, 16, 17, 18, 19]. Moreover, the quality of speech tokenizer is essential for AR models to achieve high-ﬁdelity synthesis [20, 21, 22, 23, 24, 25, 26]. Thus, there have been studies exploring direct modeling in continuous space [27, 28, 29] to enhance synthesized speech quality recently. Although AR models demonstrate impressive zero-shot performance as they perform implicit duration modeling and can leverage diverse sampling strategies, non-autoregressive (NAR) models Corresponding author 2https://github.com/SWivid/F5-TTS Preprint. Under review. Figure 1: An overview of F5-TTS training (left) and inference (right). The model is trained on the text-guided speech-inﬁlling task and condition ﬂow matching loss. The input text is converted to character sequence, padded with ﬁller tokens to the same length as input speech, and reﬁned by ConvNeXt blocks before concatenation with speech input. The inference leverages Sway Sampling for ﬂow steps, with the model and an ODE solver to generate speech from sampled noise. beneﬁt from fast inference through parallel processing, and effectively balance synthesis quality and latency. Notably, diffusion models [30, 31] contribute most to the success of current NAR speech models [11, 12]. In particular, Flow Matching with Optimal Transport path (FM-OT) [32] is widely used in recent research ﬁelds not only text-to-speech [14, 33, 34, 35, 36] but also image generation [37] and music generation [38]. Unlike AR-based models, the alignment modeling between input text and synthesized speech is crucial and challenging for NAR-based models. While NaturalSpeech 3 [12] and Voicebox [14] use frame-wise phoneme alignment; Matcha-TTS [34] adopts monotonic alignment search and relies on phoneme-level duration model; recent works ﬁnd that introducing such rigid and inﬂexible alignment between text and speech hinders the model from generating results with higher naturalness [36, 39]. E3 TTS [40] abandons phoneme-level duration and applies cross-attention on the input sequence but yields limited audio quality. DiTTo-TTS [35] uses Diffusion Transformer (DiT) [41] with crossattention conditioned on encoded text from pretrained language model. To further enhance alignment, it uses the pretrained language model to ﬁnetune the neural audio codec, infusing semantic information into the generated representations. In contrast, E2 TTS [36], based on Voicebox [14], adopts simpler way, which removes the phoneme and duration predictor and directly uses characters padded with ﬁller tokens to the length of mel spectrograms as input. This simple scheme also achieves very natural and realistic synthesized results. However, we found that robustness issues exist in E2 TTS for the text and speech alignment. Seed-TTS [39] employs similar strategy and achieves excellent results, though not elaborated in model details. In these ways of not explicitly modeling phoneme-level duration, models learn to assign the length of each word or phoneme according to the given total sequence length, resulting in improved prosody and rhythm. In this paper, we propose F5-TTS, Fairytaler that Fakes Fluent and Faithful speech with Flow matching. Maintaining the simplicity of pipeline without phoneme alignment, duration predictor, text encoder, and semantically infused codec model, F5-TTS leverages the Diffusion Transformer 2 with ConvNeXt V2 [42] to better tackle text-speech alignment during in-context learning. We stress the deep entanglement of semantic and acoustic features in the E2 TTS model design, which has inherent problems and will pose alignment failure issues that could not simply be solved with reranking. With in-depth ablation studies, our proposed F5-TTS demonstrates stronger robustness, in generating more faithful speech to the text prompt, while maintaining comparable speaker similarity. Additionally, we introduce an inference-time sampling strategy for ﬂow steps substantially improving naturalness, intelligibility, and speaker similarity of generation. This approach can be seamlessly integrated into existing ﬂow matching based models without retraining."
        },
        {
            "title": "2.1 Flow Matching",
            "content": "The Flow Matching (FM) objective is to match probability path pt from simple distribution p0, e.g., the standard normal distribution p(x) = (x0, I), to p1 approximating the data distribution q. In short, the FM loss regresses the vector ﬁeld ut with neural network vt as LF (θ) = Et,pt(x) kvt(x) ut(x)k2 , (1) where θ parameterizes the neural network, U[0, 1] and pt(x). The model vt is trained over the entire ﬂow step and data range, ensuring it learns to handle the entire transformation process from the initial distribution to the target distribution. As we have no prior knowledge of how to approximate pt and ut, conditional probability path pt(xx1) = (x µt(x1), σt(x1)2I) is considered in actual training, and the Conditional Flow Matching (CFM) loss is proved to have identical gradients w.r.t. θ [32]. x1 is the random variable corresponding to training data. µ and σ is the time-dependent mean and scalar standard deviation of Gaussian distribution. Remember that the goal is to construct target distribution (data samples) from initial simple distribution, e.g., Gaussian noise. With the conditional form, the ﬂow map ψt(x) = σt(x1)x + µt(x1) with µ0(x1) = 0 and σ0(x1) = 1, µ1(x1) = x1 and σ1(x1) = 0 is made to have all conditional probability paths converging to p0 and p1 at the start and end. The ﬂow thus provides vector ﬁeld dψt(x0)/dt = ut(ψt(x0)x1). Reparameterize pt(xx1) with x0, we have LCFM(θ) = Et,q(x1),p(x0)kvt(ψt(x0)) dt ψt(x0)k2. Further leveraging Optimal Transport form ψt(x) = (1 t)x + tx1, we have the OT-CFM loss, LCFM(θ) = Et,q(x1),p(x0)kvt((1 t)x0 + tx1) (x1 x0)k2. (2) (3) To view in more general way [43], if formulating the loss in terms of log signal-to-noise ratio (logSNR) λ instead of ﬂow step t, and parameterizing to predict x0 (ǫ, commonly stated in diffusion model) instead of predict x1 x0, the CFM loss is equivalent to the v-prediction [44] loss with cosine schedule. For inference, given sampled noise x0 from initial distribution p0, ﬂow step [0, 1] and condition with respect to generation task, the ordinary differential equation (ODE) solver [45] is used to evaluate ψ1(x0) the integration of dψt(x0)/dt with ψ0(x0) = x0. The number of function evaluations (NFE) is the times going through the neural network as we may provide multiple ﬂow step values from 0 to 1 as input to approximate the integration. Higher NFE will produce more accurate results and certainly take more calculation time."
        },
        {
            "title": "2.2 Classiﬁer-Free Guidance",
            "content": "Classiﬁer Guidance (CG) is proposed by [46], functions by adding the gradient of an additional classiﬁer, while such an explicit way to condition the generation process may have several problems. Extra training of the classiﬁer is required and the generation result is directly affected by the quality of the classiﬁer. Adversarial attacks might also occur as the guidance is introduced through the way of updating the gradient. Thus deceptive images with imperceptible details to human eyes may be generated, which are not conditional. 3 Classiﬁer-Free Guidance (CFG) [47] proposes to replace the explicit classiﬁer with an implicit classiﬁer without directly computing the explicit classiﬁer and its gradient. The gradient of classiﬁer can be expressed as combination of conditional generation probability and unconditional generation probability. By dropping the condition with certain rate during training, and linear extrapolating the inference outputs with and without condition c, the ﬁnal guided result is obtained. We could balance between ﬁdelity and diversity of the generated samples with vt,CF = vt(ψt(x0), c) + α(vt(ψt(x0), c) vt(ψt(x0))) (4) in CFM case, where α is the CFG strength."
        },
        {
            "title": "3 Method",
            "content": "This work aims to build high-level text-to-speech synthesis system. We trained our model on the text-guided speech-inﬁlling task [48, 14]. Based on recent research [35, 36, 49], it is promising to train without phoneme-level duration predictor and can achieve higher naturalness in zero-shot generation deprecating explicit phoneme-level alignment. We adopt similar pipeline as E2 TTS [36] and propose our advanced architecture F5-TTS, addressing the slow convergence (timbre learned well at an early stage but struggled to learn alignment) and robustness issues (failures on hard case generation) of E2 TTS. We also propose Sway Sampling strategy for ﬂow steps at inference, which signiﬁcantly improves our models performance in faithfulness to reference text and speaker similarity. 3.1 Pipeline Training The inﬁlling task is to predict segment of speech given its surrounding audio and full text (for both surrounding transcription and the part to generate). For simplicity, we reuse the symbol to denote an audio sample and the corresponding transcript for data pair (x, y). As shown in Fig.1 (left), the acoustic input for training is an extracted mel spectrogram features x1 RF from the audio sample x, where is mel dimension and is the sequence length. In the scope of CFM, we pass in the model the noisy speech (1 t)x0 + tx1 and the masked speech (1 m) x1, where x0 denotes sampled Gaussian noise, is sampled ﬂow step, and {0, 1}F represents binary temporal mask. Following E2 TTS, we directly use alphabets and symbols for English. We opt for full pinyin to facilitate Chinese zero-shot generation. By breaking the raw text into such character sequence and padding it with ﬁller tokens to the same length as mel frames, we form an extended sequence with ci denoting the i-th character: = (c1, c2, . . . , cM , hF i, . . . , hF ). } {z (N M) times (5) The model is trained to reconstruct x1 with (1 m) x1 and z, which equals to learn the target distribution p1 in form of (m x1(1 m) x1, z) approximating real data distribution q. Inference To generate speech with the desired content, we have the audio prompts mel spectrogram features xref , its transcription yref , and text prompt ygen. Audio prompt serves to provide speaker characteristics and text prompt is to guide the content of generated speech. The sequence length , or duration, has now become pivotal factor that necessitates informing the model of the desired length for sample generation. One could train separate model to predict and deliver the duration based on xref , yref and ygen. Here we simply estimate the duration based on the ratio of the number of characters in ygen and yref . We assume that the sum-up length of characters is no longer than mel length, thus padding with ﬁller tokens is done as during training. To sample from the learned distribution, the converted mel features xref , along with concatenated and extended character sequence zref gen serve as the condition in Eq.4. We have vt(ψt(x0), c) = vt((1 t)x0 + tx1xref , zref gen), (6) 3Note that the inference time will be doubled if CFG. Model vt will execute the forward process twice, once with condition, and once without. 4 See from Fig.1 (right), we start from sampled noise x0, and what we want is the other end of ﬂow x1. Thus we use the ODE solver to gradually integrate from ψ0(x0) = x0 to ψ1(x0) = x1, given dψt(x0)/dt = vt(ψt(x0), xref , zref gen). During inference, the ﬂow steps are provided in an ordered way, e.g., uniformly sampled certain number from 0 to 1 according to the NFE setting. After getting the generated mel with model vt and ODE solver, we discard the part of xref . Then we leverage vocoder to convert the mel back to speech signal."
        },
        {
            "title": "3.2 F5-TTS",
            "content": "E2 TTS directly concatenates the padded character sequence with input speech, thus deeply entangling semantic and acoustic features with large length gap of effective information, which is the underlying cause of hard training and poses several problems in zero-shot scenario (Sec.5.1). To alleviate the problem of slow convergence and low robustness, we propose F5-TTS which accelerates training and inference and shows strong robustness in generation. Also, an inference-time Sway Sampling is introduced, which allows inference faster (using less NFE) while maintaining performance. This sampling way of ﬂow step can be directly applied to other CFM models. Model As shown in Fig.1, we use latent Diffusion Transformer (DiT) [41] as backbone. To be speciﬁc, we use DiT blocks with zero-initialized adaptive Layer Norm (adaLN-zero). To enhance the models alignment ability, we also leverage ConvNeXt V2 blocks [42]. Its predecessor ConvNeXt V1 [50] is used in many works and shows strong temporal modeling capability in speech domain tasks [51, 52]. As described in Sec.3.1, the model input is character sequence, noisy speech, and masked speech. Before concatenation in the feature dimension, the character sequence ﬁrst goes through ConvNeXt blocks. Experiments have shown that this way of providing individual modeling space allows text input to better prepare itself before later in-context learning. Unlike the phoneme-level force alignment done in Voicebox, rigid boundary for text is not explicitly introduced. The semantic and acoustic features are jointly learned with the entire model. And unlike the way of feeding the model with inputs of signiﬁcant length difference (length with effective information) as E2 TTS does, our design mitigates such gap. The ﬂow step for CFM is provided as the condition of adaLN-zero rather than appended to the concatenated input sequence in Voicebox. We found that an additional mean pooled token of text sequence for adaLN condition is not essential for the TTS task, maybe because the TTS task requires more rigorously guided results and the mean pooled text token is more coarse. We adopt some position embedding settings in Voicebox. The ﬂow step is embedded with sinusoidal position. The concatenated input sequence is added with convolutional position embedding. We apply rotary position embedding (RoPE) [53] for self-attention rather than symmetric bi-directional ALiBi bias [54]. And for extended character sequence ˆy, we also add it with an absolute sinusoidal position embedding before feeding it into ConvNeXt blocks. Compared with Voicebox and E2 TTS, we abandoned the U-Net [55] style skip connection structure and switched to using DiT with adaLN-zero. Without phoneme-level duration predictor and explicit alignment process, and nor with extra text encoder and semantically infused neural codec model in DiTTo-TTS, we give the text input little freedom (individual modeling space) to let it prepare itself before concatenation and in-context learning with speech input. Sampling As stated in Sec.2.1, the CFM could be viewed as v-prediction with cosine schedule. For image synthesis, [37] propose to further schedule the ﬂow step with single-peak logitnormal [56] sampling, in order to give more weight to intermediate ﬂow steps by sampling them more frequently. We speculate that such sampling distributes the models learning difﬁculty more evenly over different ﬂow step [0, 1]. In contrast, we train our model with traditional uniformly sampled ﬂow step U[0, 1] but apply non-uniform sampling during inference. In speciﬁc, we deﬁne Sway Sampling function as fsway(u; s) = + (cos( π 2 u) 1 + u), (7) 2 π2 ]. We ﬁrst sample U[0, 1], then apply this which is monotonic with coefﬁcient [1, function to obtain sway sampled ﬂow step t. With < 0, the sampling is sway to left; with > 0, the 5 sampling is sway to right; and = 0 case equals to uniform sampling. Fig.3 shows the probability density function of Sway Sampling on ﬂow step t. Conceptually, CFM models focus more on sketching the contours of speech in the early stage (t 0) from pure noise and later focus more on the embellishment of ﬁne-grained details. Therefore, the alignment between speech and text will be determined based on the ﬁrst few generated results. With scale parameter < 0, we make model inference more with smaller t, thus providing the ODE solver with more startup information to evaluate more precisely in initial integration steps."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Datasets We utilize the in-the-wild multilingual speech dataset Emilia [57] to train our base models. After simply ﬁltering out transcription failure and misclassiﬁed language speech, we retain approximately 95K hours of English and Chinese data. We also trained small models for ablation study and architecture search on WenetSpeech4TTS [58] Premium subset, consisting of 945 hours Mandarin corpus. Base model conﬁgurations are introduced below, and small model conﬁgurations are in Appendix B.1. Three test sets are adopted for evaluation, which are LibriSpeech-PC testclean [59], Seed-TTS test-en [39] with 1088 samples from Common Voice [60], and Seed-TTS test-zh with 2020 samples from DiDiSpeech [61]4. Most of the previous English-only models are evaluated on different subsets of LibriSpeech test-clean while the used prompt list is not released, which makes fair comparison difﬁcult. Thus we build and release 4-to-10-second LibriSpeech-PC subset with 1127 samples to facilitate community comparisons. Training Our base models are trained to 1.2M updates with batch size of 307,200 audio frames (0.91 hours), for over one week on 8 NVIDIA A100 80G GPUs. The AdamW optimizer [62] is used with peak learning rate of 7.5e-5, linearly warmed up for 20K updates, and linearly decays over the rest of the training. We set 1 for the max gradient norm clip. The F5-TTS base model has 22 layers, 16 attention heads, 1024/2048 embedding/feed-forward network (FFN) dimension for DiT; and 4 layers, 512/1024 embedding/FFN dimension for ConvNeXt V2; in total 335.8M parameters. The reproduced E2 TTS, 333.2M ﬂat U-Net equipped Transformer, has 24 layers, 16 attention heads, and 1024/4096 embedding/FFN dimension. Both models use RoPE as mentioned in Sec.3.2, dropout rate of 0.1 for attention and FFN, the same convolutional position embedding as in Voicebox[14]. We directly use alphabets and symbols for English, use jieba5 and pypinyin6 to process raw Chinese characters to full pinyins. The character embedding vocabulary size is 2546, counting in the special ﬁller token and all other language characters exist in the Emilia dataset as there are many codeswitched sentences. For audio samples we use 100-dimensional log mel-ﬁlterbank features with 24 kHz sampling rate and hop length 256. random 70% to 100% of mel frames is masked for inﬁlling task training. For CFG (Sec.2.2) training, ﬁrst the masked speech input is dropped with rate of 0.3, then the masked speech again but with text input together is dropped with rate of 0.2. We assume that the two-stage control of CFG training may have the model learn more with text alignment. Inference The inference process is mainly elaborated in Sec.3.1. We use the Exponential Moving Averaged (EMA) [63] weights for inference, and the Euler ODE solver for F5-TTS (midpoint for E2 TTS as described in [36]). We use the pretrained vocoder Vocos [51] to convert generated log mel spectrograms to audio signals. Baselines We compare our models with leading TTS systems including, (mainly) autoregressive models: VALL-E 2 [13], MELLE [29], FireRedTTS [64] and CosyVoice [65]; non-autoregressive models: Voicebox [14], NaturalSpeech 3 [12], DiTTo-TTS [35], MaskGCT [66], Seed-TTSDiT [39] and our reproduced E2 TTS [36]. Details of compared models see Appendix A. Metrics We measure the performances under cross-sentence task. The model is given reference text, short speech prompt, and its transcription, and made to synthesize speech reading the reference text mimicking the speech prompt speaker. In speciﬁc, we report Word Error Rate (WER) and speaker Similarity between generated and the original target speeches (SIM-o) for objective evaluation. For WER, we employ Whisper-large-v3 [67] to transcribe English and Paraformer-zh [68] 4https://github.com/BytedanceSpeech/seed-tts-eval 5https://github.com/fxsjy/jieba 6https://github.com/mozillazg/python-pinyin 6 Table 1: Results on LibriSpeech test-clean and LibriSpeech-PC test-clean. The boldface indicates the best result, and * denotes the score reported in baseline papers with different subsets for evaluation. The Real-Time Factor (RTF) is computed with the inference time of 10s speech. #Param. stands for the number of learnable parameters and #Data refers to the used training dataset in hours."
        },
        {
            "title": "Model",
            "content": "#Param. #Data(hrs) WER(%) SIM-o RTF Ground Truth (2.2 hours subset) VALL-E 2 [13] MELLE [29] MELLE-R2 [29] Voicebox [14] DiTTo-TTS [35] LibriSpeech test-clean - - - - 330M 740M - 50K EN 50K EN 50K EN 60K EN 55K EN Ground Truth (40 samples subset) Voicebox [14] NaturalSpeech 3 [12] MaskGCT [66] - 60K EN 60K EN - 330M 500M 1048M 100K Multi. LibriSpeech-PC test-clean Ground Truth (1127 samples 2 hrs) Vocoder Resynthesized CosyVoice [65] FireRedTTS [64] E2 TTS (32 NFE) [36] F5-TTS (16 NFE) F5-TTS (32 NFE) - - - - 300M 170K Multi. 580M 248K Multi. 333M 100K Multi. 336M 100K Multi. 336M 100K Multi. 2.2* 2.44* 2.10* 2.14* 1.9* 2.56* 1.94* 2.03* 1.94* 2.634* 2.23 2.32 3.59 2.69 2.95 2.53 2.42 0.754* 0.643* 0.625* 0.608* 0.662* 0.627* 0.68* 0.64* 0.67* 0.687* 0.69 0.66 0.66 0.47 0.69 0.66 0.66 - 0.732* 0.549* 0.276* 0.64* 0.162* - 0.64* 0.296* - - - 0.92 0.84 0.68 0.15 0.31 for Chinese, following [39]. For SIM-o, we use WavLM-large-based [69] speaker veriﬁcation model to extract speaker embeddings for calculating the cosine similarity of synthesized and ground truth speeches. We use Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for subjective evaluation. For CMOS, human evaluators are given randomly ordered synthesized speech and ground truth, and are to decide how higher the naturalness of the better one surpasses the counterpart, w.r.t. prompt speech. For SMOS, human evaluators are to score the similarity between the synthesized and prompt."
        },
        {
            "title": "5 Experimental Results",
            "content": "Tab.1 and 2 show the main results of objective and subjective evaluations. We report the average score of three random seed generation results with our model and open-sourced baselines. We use by default CFG strength of 2 and Sway Sampling coefﬁcient of 1 for our F5-TTS. For English zero-shot evaluation, the previous works are hard to compare directly as they use different subsets of LibriSpeech test-clean [70]. Although most of them claim to ﬁlter out 4-to-10-second utterances as the generation target, the corresponding prompt audios used are not released. Therefore, we build 4-to-10-second sample test set based on LibriSpeech-PC [59] which is an extension of LibriSpeech with additional punctuation marks and casing. To facilitate future comparison, we release the 2-hour test set with 1,127 samples, sourced from 39 speakers (LibriSpeech-PC missing one speaker). F5-TTS achieves WER of 2.42 on LibriSpeech-PC test-clean with 32 NFE and Sway Sampling, demonstrating its robustness in zero-shot generation. Inference with 16 NFE, F5-TTS gains an RTF of 0.15 while still supporting high-quality generation with WER of 2.53. It is clear that the Sway Sampling strategy greatly improves performance. The reproduced E2 TTS shows an excellent speaker similarity (SIM) but much worse WER in the zero-shot scenario, indicating the inherent deﬁciency of alignment robustness. 7 Table 2: Results on two test sets, Seed-TTS test-en and test-zh. The boldface indicates the best result, the underline denotes the second best, and * denotes scores reported in baseline papers."
        },
        {
            "title": "Model",
            "content": "WER(%) SIM-o CMOS SMOS Ground Truth Vocoder Resynthesized CosyVoice [65] FireRedTTS [64] MaskGCT [66] Seed-TTSDiT [39] E2 TTS (32 NFE) [36] F5-TTS (16 NFE) F5-TTS (32 NFE) Seed-TTS test-en 0.73 2.06 0.70 2.09 0.64 3.39 0.46 3.82 0.717* 2.623* 0.790* 1.733* 0.71 2.19 0.67 1.89 1.83 0.67 Seed-TTS test-zh Ground Truth Vocoder Resynthesized CosyVoice [65] FireRedTTS [64] MaskGCT [66] Seed-TTSDiT [39] E2 TTS (32 NFE) [36] F5-TTS (16 NFE) F5-TTS (32 NFE) 1.26 1.27 3.10 1.51 2.273* 1.178* 1.97 1.74 1.56 0.76 0.72 0.75 0.63 0.774* 0.809* 0.73 0.75 0.76 0.00 - 0.02 -1.46 - - 0.06 0.16 0.31 0.00 - -0.06 -0.49 - - -0.04 0.02 0.21 3.91 - 3.64 2.94 - - 3.81 3.79 3. 3.72 - 3.54 3.28 - - 3.44 3.72 3.83 From the evaluation results on the Seed-TTS test sets, F5-TTS behaves similarly with close WER to ground truth and comparable SIM scores. It produces smooth and ﬂuent speech in zero-shot generation with CMOS of 0.31 (0.21) and SMOS of 3.89 (3.83) on Seed-TTS test-en (test-zh), and surpasses some baseline models trained with larger scales. It is worth mentioning that Seed-TTS with the best result is trained with orders of larger model size and dataset (several million hours) than ours. As stated in Sec.3.1, we simply estimate duration based on the ratio of the audio prompts transcript length and the text prompt length. If providing ground truth duration, F5-TTS with 32 NFE and Sway Sampling will have WER of 1.74 for test-en and 1.53 for test-zh while maintaining the same SIM, indicating high upper bound. robustness test on ELLA-V [15] hard sentences is further included in Appendix B.5. 5.1 Ablation of Model Architecture To clarify our F5-TTSs efﬁciency and stress the limitation of E2 TTS. We conduct in-depth ablation studies. We trained small models to 800K updates (each on 8 NVIDIA RTX 3090 GPUs for one week), all scaled to around 155M parameters, on the WenetSpeech4TTS Premium 945 hours Mandarin dataset with half the batch size and the same optimizer and scheduler as base models. Details of small model conﬁgurations see Appendix B.1. We ﬁrst experiment with pure adaLN DiT (F5-TTSConv2Text), which fails to learn alignment given simply padded character sequences. Based on the concept of reﬁning the input text representation to better align with speech modality, and keep the simplicity of system design, we propose to add jointly learned structure to the input context. Speciﬁcally, we leverage ConvNeXts capabilities of capturing local connections, multi-scale features, and spatial invariance for the input text, which is our F5-TTS. And we ablate with adding the same branch for input speech, denoted F5TTS+Conv2Audio. We further conduct experiments to ﬁgure out whether the long skip connection and the pre-reﬁnement of input text are beneﬁcial to the counterpart backbone, i.e. F5-TTS and E2 TTS, named F5-TTS+LongSkip and E2 TTS+Conv2Text respectively. We also tried with the Multi-Modal DiT (MMDiT) [37] double-stream joint-attention structure for the TTS task which learned fast and collapsed fast, resulting in severe repeated utterance with wild timbre and prosody. Figure 2: Ablation studies on model architecture. Seed-TTS test-zh evaluation results of 155M small models trained with WenetSpeech4TTS Premium 945 hours Mandarin Corpus. We assume that the pure MMDiT structure is far too ﬂexible for rigorous task e.g. TTS which needs more faithful generation following the prompt guidance. Fig.2 shows the overall trend of small models WER and SIM scores evaluated on Seed-TTS test-zh. Trained with only 945 hours of data, F5-TTS (32 NFE w/o SS) achieves WER of 4.17 and SIM of 0.54 at 800K updates, while E2 TTS is 9.63 and 0.53. F5-TTS+Conv2Audio trades much alignment robustness (+1.61 WER) with slightly higher speaker similarity (+0.01 SIM), which is not ideal for scaling up. We found that the long skip connection structure can not simply ﬁt into DiT to improve speaker similarity, while the ConvNeXt for input text reﬁnement can not directly apply to the ﬂat U-Net Transformer to improve WER as well, both showing signiﬁcant degradation of performance. To further analyze the unsatisfactory results with E2 TTS, we studied the consistent failure (unable to solve with re-ranking) on 7% of the test set (WER>50%) all along the training process. We found that E2 TTS typically struggles with around 140 samples which we speculate to have large distribution gap with the train set, while F5-TTS easily tackles this issue. We investigate the models behaviors with different input conditions to illustrate the advantages of F5-TTS further and disclose the possible reasons for E2 TTSs deﬁciency. See from Tab.4 in Appendix B.2, providing the ground truth duration allows more gains on WER for F5-TTS than E2 TTS, showing its robustness in alignment. By dropping the audio prompt, and synthesizing speech solely with the text prompt, E2 TTS is free of failures. This phenomenon implied deep entanglement of semantic and acoustic features within E2 TTSs model design. From Tab.3 GFLOPs statistics, F5-TTS carries out faster training and inference than E2 TTS. The aforementioned limitations of E2 TTS greatly hinder real-world application as the failed generation cannot be solved with re-ranking. Supervised ﬁne-tuning facing out-of-domain data or tremendous pretraining scale is mandatory for E2 TTS, which is inconvenient for industrial deployment. On the contrary, our F5-TTS better handles zero-shot generation, showing stronger robustness."
        },
        {
            "title": "5.2 Ablation of Sway Sampling",
            "content": "It is clear from Fig.3 that Sway Sampling with negative improves the generation results. Further with more negative s, models achieve lower WER and higher SIM scores. We additionally include comparing results on base models with and without Sway Sampling in Appendix B.4. As stated at the end of Sec.3.2, Sway Sampling with < 0 scales more ﬂow step toward early-stage inference (t 0), thus having CFM models capture more startup information to sketch the contours of target speech better. To be more concrete, we conduct leak and override\" experiment. We ﬁrst replace the Gaussian noise input x0 at inference time with ground-truth-information-leaked input (1 t)x0 + tx ref is duplicate of the audio prompt mel features. Then, we provide text prompt different from the duplicated audio transcript and let the model continue the subsequent inference (skip the ﬂow steps before t). The model succeeds in overriding leaked utterances and producing speech following the text prompt if Sway Sampling is used, and fails without. Uniformly sampled ﬂow steps will have the model producing speech dominated by ref , where = 0.1 and 9 Figure 3: Probability density function of Sway Sampling on ﬂow step with different coefﬁcient (left), and small models performance on Seed-TTS test-zh with Sway Sampling (right). leaked information, speaking the duplicated audio prompts context. Similarly, leaked timbre can be overridden with another speakers utterance as an audio prompt, leveraging Sway Sampling. The experiment result is shred of strong evidence proving that the early ﬂow steps are crucial for sketching the silhouette of target speech based on given prompts faithfully, the later steps focus more on formed intermediate noisy output, where our sway-to-left sampling (s < 0) ﬁnds the proﬁtable niche and takes advantage of it. We emphasize that our inference-time Sway Sampling can be easily applied to existing CFM-based models without retraining. And we will work in the future to combine it with training-time noise schedulers and distillation techniques to further boost efﬁciency."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces F5-TTS, fully non-autoregressive text-to-speech system based on ﬂow matching with diffusion transformer (DiT). With tidy pipeline, literally text in and speech out, F5-TTS achieves state-of-the-art zero-shot ability compared to existing works trained on industry-scale data. We adopt ConvNeXt for text modeling and propose the test-time Sway Sampling strategy to further improve the robustness of speech generation and inference efﬁciency. Our design allows faster training and inference, by achieving test-time RTF of 0.15, which is competitive with other heavily optimized TTS models of similar performance. We open-source our code, and models, to enhance transparency and facilitate reproducible research in this area. Acknowledgments The authors would like to express our gratitude to Tianrui Wang, Xiaofei Wang, Yakun Song, Yifan Yang, Yiwei Guo, and Yunchong Xiao for the valuable discussions."
        },
        {
            "title": "Ethics Statements",
            "content": "This work is purely research project. F5-TTS is trained on large-scale public multilingual speech data and could synthesize speech of high naturalness and speaker similarity. Given the potential risks in the misuse of the model, such as spooﬁng voice identiﬁcation, it should be imperative to implement watermarks and develop detection model to identify audio outputs."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Shen, Ruoming Pang, Ron Weiss, Mike Schuster, et al. Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions. In Proc. ICASSP, pages 47794783. IEEE, 2018. [2] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with transformer network. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages 67066713, 2019. [3] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020. [4] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-TTS: generative ﬂow for text-to-speech via monotonic alignment search. Advances in Neural Information Processing Systems, 33:80678077, 2020. [5] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pages 55305540. PMLR, 2021. [6] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Gradtts: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pages 85998608. PMLR, 2021. [7] Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. VioLA: Uniﬁed codec language models for speech recognition, synthesis, and translation. arXiv preprint arXiv:2305.16107, 2023. [8] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, et al. Naturalspeech: End-to-end text-to-speech synthesis with human-level quality. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [9] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. [10] Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, et al. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. arXiv preprint arXiv:2303.03926, 2023. [11] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, et al. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023. [12] Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. arXiv preprint arXiv:2403.03100, 2024. [13] Sanyuan Chen, Shujie Liu, Long Zhou, Yanqing Liu, Xu Tan, Jinyu Li, Sheng Zhao, Yao Qian, and Furu Wei. VALL-E 2: Neural codec language models are human parity zero-shot text to speech synthesizers. arXiv preprint arXiv:2406.05370, 2024. [14] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. [15] Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, and Xie Chen. ELLA-V: Stable neural codec language modeling with alignment-guided sequence reordering. arXiv preprint arXiv:2401.07333, 2024. [16] Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, Zhikang Niu, Shuai Wang, Hui Zhang, Xie Chen, and Kai Yu. VALL-T: Decoder-only generative transducer for robust and decodingcontrollable text-to-speech. arXiv preprint arXiv:2401.14321, 2024. [17] Bing Han, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Yanming Qian, Yanqing Liu, Sheng Zhao, Jinyu Li, and Furu Wei. VALL-E R: Robust and efﬁcient zero-shot text-to-speech synthesis via monotonic alignment. arXiv preprint arXiv:2406.07855, 2024. [18] Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, et al. Rall-e: Robust codec language modeling with chain-of-thought prompting for text-to-speech synthesis. arXiv preprint arXiv:2404.03204, 2024. 11 [19] Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, and David Harwath. arXiv preprint Voicecraft: Zero-shot speech editing and text-to-speech in the wild. arXiv:2403.16973, 2024. [20] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. [21] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High ﬁdelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [22] Yi-Chiao Wu, Israel Gebru, Dejan Markovic, and Alexander Richard. Audiodec: An opensource streaming high-ﬁdelity neural audio codec. In Proc. ICASSP, pages 15. IEEE, 2023. [23] Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. Hiﬁ-codec: Group-residual vector quantization for high ﬁdelity audio codec. arXiv preprint arXiv:2305.02765, 2023. [24] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Uniﬁed speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. [25] He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, and Navdeep Jaitly. dMel: Speech tokenization made simple. arXiv preprint arXiv:2407.15835, 2024. [26] Zhikang Niu, Sanyuan Chen, Long Zhou, Ziyang Ma, Xie Chen, and Shujie Liu. NDVQ: Robust neural audio codec with normal distribution-based vector quantization. arXiv preprint arXiv:2409.12717, 2024. [27] Zhijun Liu, Shuai Wang, Sho Inoue, Qibing Bai, and Haizhou Li. Autoregressive diffusion transformer for text-to-speech synthesis. arXiv preprint arXiv:2406.05551, 2024. [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [29] Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, et al. Autoregressive speech synthesis without vector quantization. arXiv preprint arXiv:2407.08551, 2024. [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [32] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [33] Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu. Voiceﬂow: Efﬁcient text-tospeech with rectiﬁed ﬂow matching. In Proc. ICASSP, pages 1112111125. IEEE, 2024. [34] Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, and Gustav Eje Henter. Matcha-TTS: fast TTS architecture with conditional ﬂow matching. In Proc. ICASSP, pages 1134111345. IEEE, 2024. [35] Keon Lee, Dong Won Kim, Jaehyeon Kim, and Jaewoong Cho. DiTTo-TTS: Efﬁcient and scalable zero-shot text-to-speech with diffusion transformer. arXiv preprint arXiv:2406.11427, 2024. [36] Seﬁk Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, et al. E2 TTS: Embarrassingly easy fully non-autoregressive zero-shot TTS. arXiv preprint arXiv:2406.18009, 2024. [37] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, et al. Scaling rectiﬁed ﬂow transformers for high-resolution image synthesis. In Proc. ICML, 2024. [38] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Flux that plays music. arXiv preprint arXiv:2409.00587, 2024. [39] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, et al. Seed-TTS: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. [40] Yuan Gao, Nobuyuki Morioka, Yu Zhang, and Nanxin Chen. E3 TTS: Easy end-to-end diffusion-based text to speech. In Proc. ASRU, pages 18. IEEE, 2023. 12 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [42] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1613316142, 2023. [43] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. [44] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [45] Ricky T. Q. Chen. torchdiffeq, 2018. [46] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [47] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [48] He Bai, Renjie Zheng, Junkun Chen, Mingbo Ma, Xintong Li, and Liang Huang. A3t: Alignment-aware acoustic and text pretraining for speech synthesis and editing. In International Conference on Machine Learning, pages 13991411. PMLR, 2022. [49] Zhijun Liu, Shuai Wang, Pengcheng Zhu, Mengxiao Bi, and Haizhou Li. E1 TTS: Simple and fast non-autoregressive TTS. arXiv preprint arXiv:2409.09351, 2024. [50] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. [51] Hubert Siuzdak. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. arXiv preprint arXiv:2306.00814, 2023. [52] Takuma Okamoto, Yamato Ohtani, Tomoki Toda, and Hisashi Kawai. Convnext-TTS and Convnext-VC: Convnext-based fast end-to-end sequence-to-sequence text-to-speech and voice conversion. In Proc. ICASSP, pages 1245612460. IEEE, 2024. [53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [54] Oﬁr Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. [55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Proc. MICCAI, pages 234241. Springer, 2015. [56] Jhon Atchison and Sheng Shen. Logistic-normal distributions: Some properties and uses. Biometrika, 67(2):261272, 1980. [57] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. arXiv preprint arXiv:2407.05361, 2024. [58] Linhan Ma, Dake Guo, Kun Song, Yuepeng Jiang, Shuai Wang, Liumeng Xue, Weiming Xu, Huan Zhao, Binbin Zhang, and Lei Xie. WenetSpeech4TTS: 12,800-hour Mandarin TTS corpus for large speech generation model benchmark. arXiv preprint arXiv:2406.05763, 2024. [59] Aleksandr Meister, Matvei Novikov, Nikolay Karpov, Evelina Bakhturina, Vitaly Lavrukhin, and Boris Ginsburg. LibriSpeech-PC: Benchmark for evaluation of punctuation and capitalization capabilities of end-to-end ASR models. In Proc. ASRU, pages 17. IEEE, 2023. [60] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. [61] Tingwei Guo, Cheng Wen, Dongwei Jiang, Ne Luo, et al. Didispeech: large scale Mandarin speech corpus. In Proc. ICASSP, pages 69686972. IEEE, 2021. [62] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 13 [63] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. [64] Hao-Han Guo, Kun Liu, Fei-Yu Shen, Yi-Chen Wu, Feng-Long Xie, Kun Xie, and Kai-Tuo Xu. FireRedTTS: foundation text-to-speech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283, 2024. [65] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [66] Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Shunsi Zhang, and Zhizheng Wu. MaskGCT: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024. [67] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [68] Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao Du, Zhangyu Xiao, et al. FunASR: fundamental end-to-end speech recognition toolkit. arXiv preprint arXiv:2305.11013, 2023. [69] Zhengyang Chen, Sanyuan Chen, Yu Wu, Yao Qian, Chengyi Wang, Shujie Liu, Yanmin Qian, and Michael Zeng. Large-scale self-supervised speech representation learning for automatic speaker veriﬁcation. In Proc. ICASSP, pages 61476151. IEEE, 2022. [70] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR corpus based on public domain audio books. In Proc. ICASSP, pages 52065210. IEEE, 2015. [71] Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, and Daniel Povey. Libriheavy: 50,000 hours asr corpus with punctuation casing and context. In Proc. ICASSP, pages 1099110995. IEEE, 2024. [72] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, et al. Libri-light: benchIn Proc. ICASSP, pages 76697673. IEEE, mark for ASR with limited or no supervision. 2020. [73] Yinghao Aaron Li, Cong Han, Vinay Raghavan, Gavin Mischler, and Nima Mesgarani. Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "A Baseline Details",
            "content": "VALL-E 2 [13] large-scale TTS model shares the same architecture as VALL-E [9] but employs repetition-aware sampling strategy that promotes more deliberate sampling choices, trained on Libriheavy [71] 50K hours English dataset. We compared with results reported in [29]. MELLE [29] An autoregressive large-scale model leverages continuous-valued tokens with variational inference for text-to-speech synthesis. Its variants allow to prediction of multiple melspectrogram frames at each time step, noted by MELLE-Rx with denotes reduction factor. The model is trained on Libriheavy [71] 50K hours English dataset. We compared with results reported in [29]. Voicebox [14] non-autoregressive large-scale model based on ﬂow matching trained with inﬁlling task. We compared with the 330M parameters trained on 60K hours dataset English-only models results reported in [14] and [12]. NaturalSpeech 3 [12] non-autoregressive large-scale TTS system leverages factorized neural codec to decouple speech representations and factorized diffusion model to generate speech based on disentangled attributes. The 500M base model is trained on Librilight [72] 60K hours English dataset. We compared with scores reported in [12]. DiTTo-TTS [35] large-scale non-autoregressive TTS model uses cross-attention Diffusion Transformer and leverages pretrained language model to enhance the alignment. We compare with DiTTo-en-XL, 740M model trained on 55K hours English-only dataset, using scores reported in [35]. FireRedTTS [64] foundation TTS framework for industry-level generative speech applications. The autoregressive text-to-semantic token model has 400M parameters and the token-to-waveform generation model has about half the parameters. The system is trained with 248K hours of labeled speech data. We use the ofﬁcial code and pre-trained checkpoint to evaluate7. MaskGCT [66] large-scale non-autoregressive TTS model without precise alignment information between text and speech following the mask-and-predict learning paradigm. The model is multi-stage, with 695M text-to-semantic model (T2S) and then 353M semantic-to-acoustic (S2A) model. The model is trained on Emilia [57] dataset with around 100K Chinese and English in-thewild speech data. We compare with results reported in [66]. Seed-TTS [39] family of high-quality versatile speech generation models trained on unknown tremendously large data that is of orders of magnitudes larger than the previously largest TTS systems [39]. Seed-TTSDiT is large-scale fully non-autoregressive model. We compare with results reported in [39]. E2 TTS [36] fully non-autoregressive TTS system proposes to model without the phoneme-level alignment in Voicebox, originally trained on Libriheavy [71] 50K English dataset. We compare with our reproduced 333M multilingual E2 TTS trained on Emilia [57] dataset with around 100K Chinese and English in-the-wild speech data. CosyVoice [65] two-stage large-scale TTS system, ﬁrst autoregressive text-to-token, then ﬂow matching diffusion model. The model is of around 300M parameters, trained on 170K hours of multilingual speech data. We obtain the evaluation result with the ofﬁcial code and pre-trained checkpoint8. 7https://github.com/FireRedTeam/FireRedTTS 8https://huggingface.co/model-scope/CosyVoice-300M"
        },
        {
            "title": "B Experimental Result Supplements",
            "content": "B.1 Small Model Conﬁguration The detailed conﬁguration of small models is shown in Tab.3. In the Transformer column, the numbers denote the Model Dimension, the Number of Layers, the Number of Heads, and the multiples of Hidden Size. In the ConvNeXt column, the numbers denote the Model Dimension, the Number of Layers, and the multiples of Hidden Size. GFLOPs are evaluated using the thop Python package. As mentioned in Sec.3.2, F5-TTS leverages an adaLN DiT backbone, while E2 TTS is ﬂat U-Net equipped Transformer. F5-TTS+LongSkip adds an additional long skip structure connecting the ﬁrst to the last layer in the Transformer. For the Multi-Model Diffusion Transformer (MMDiT) [37], double stream transformer, the setting denotes one stream conﬁguration. Table 3: Details of small model conﬁgurations."
        },
        {
            "title": "Transformer ConvNeXt",
            "content": "#Param. GFLOPs F5-TTS F5-TTSConv2Text F5-TTS+Conv2Audio F5-TTS+LongSkip E2 TTS E2 TTS+Conv2Text MMDiT [37] 768,18,12,2 768,18,12,2 768,16,12,2 768,18,12,2 768,20,12,4 768,20,12,4 512,16,16,2 512,4,2 - 512,4,2 512,4,2 - 512,4,2 - 158M 153M 163M 159M 157M 161M 151M 173 164 181 175 293 301 B.2 Ablation study on Input Condition The ablation study on different input conditions is conducted with three settings: common input with text and audio prompts, providing ground truth duration information rather than an estimate, and retaining only text input dropping audio prompt. In Tab.4, all evaluations take the 155M small models checkpoints trained on WenetSpeech4TTS Premium at 800K updates. Table 4: Ablation study on different input conditions. The boldface indicates the best result, and the underline denotes the second best. All scores are the average of three random seed results. Model Common Input Ground Truth Dur. Drop Audio Prompt WER SIM WER SIM WER SIM F5-TTS F5-TTS+Conv2Audio F5-TTS+LongSkip E2 TTS E2 TTS+Conv2Text 4.17 5.78 5.17 9.63 18.10 0.54 0.55 0.53 0.53 0.49 3.87 5.28 5.03 9.48 17. 0.54 0.55 0.53 0.53 0.49 3.22 3.78 3.35 3.48 3.06 0.21 0.21 0.21 0.21 0.21 B.3 Comparison of ODE Solvers The comparison results of using the Euler or midpoint ODE solver during F5-TTS inference are shown in Tab.5. The Euler is inherently faster (ﬁrst-order) and performs slightly better typically for larger NFE inference with Sway Sampling (otherwise the Euler solver results in degradation). B.4 Sway Sampling Effectiveness on Base Models From Tab.6, it is clear that our Sway Sampling strategy for test-time ﬂow steps consistently improves the zero-shot generation performance in aspects of faithfulness to prompt text (WER) and speaker similarity (SIM). The gain of applying Sway Sampling to E2 TTS [36] proves that our Sway Sampling strategy is universally applicable to existing ﬂow matching based TTS models. 16 Table 5: Evaluation results of F5-TTS (F5) on LibriSpeech-PC test-clean, Seed-TTS test-en and Seed-TTS test-zh, employing the Euler or midpoint ODE solver, and with different Sway Sampling values. The Real-Time Factor (RTF) is computed with the inference time of 10s speech."
        },
        {
            "title": "Model",
            "content": "LibriSpeech-PC test-clean Seed-TTS test-en Seed-TTS test-zh WER(%) SIM-o WER SIM-o WER SIM-o RTF Ground Truth = 1 F5 (16 NFE Euler) F5 (16 NFE midpoint) F5 (32 NFE Euler) F5 (32 NFE midpoint) = 0.8 F5 (16 NFE Euler) F5 (16 NFE midpoint) F5 (32 NFE Euler) F5 (32 NFE midpoint) 2.23 0.69 2.06 0. 1.26 0.76 - 2.53 2.43 2.42 2.41 2.82 2.58 2.50 2.42 0.66 0.66 0.66 0. 0.65 0.65 0.66 0.66 1.89 1.88 1.83 1.87 2.14 1.86 1.81 1.84 0.67 0.66 0.67 0.66 0.65 0.65 0.67 0.66 1.74 1.61 1.56 1. 2.28 1.70 1.62 1.62 0.75 0.75 0.76 0.75 0.72 0.73 0.75 0.75 0.15 0.26 0.31 0.53 0.15 0.26 0.31 0.53 Table 6: Base model evaluation results on LibriSpeech-PC test-clean, Seed-TTS test-en and test-zh, with and without proposed test-time Sway Sampling (SS, with coefﬁcient = 1) strategy for ﬂow steps. All generations leverage the midpoint ODE solver for ease of ablation. Model WER(%) SIM-o RTF LibriSpeech-PC test-clean Ground Truth (1127 samples) Vocoder Resynthesized E2 TTS (16 NFE w/ SS) E2 TTS (32 NFE w/ SS) E2 TTS (32 NFE w/o SS) F5-TTS (16 NFE w/ SS) F5-TTS (32 NFE w/ SS) F5-TTS (32 NFE w/o SS) 2.23 2.32 2.86 2.84 2.95 2.43 2.41 2.84 Seed-TTS test-en Ground Truth (1088 samples) Vocoder Resynthesized E2 TTS (16 NFE w/ SS) E2 TTS (32 NFE w/ SS) E2 TTS (32 NFE w/o SS) F5-TTS (16 NFE w/ SS) F5-TTS (32 NFE w/ SS) F5-TTS (32 NFE w/o SS) 2.06 2.09 1.99 1.98 2.19 1.88 1.87 1.93 Seed-TTS test-zh Ground Truth (2020 samples) Vocoder Resynthesized E2 TTS (16 NFE w/ SS) E2 TTS (32 NFE w/ SS) E2 TTS (32 NFE w/o SS) F5-TTS (16 NFE w/ SS) F5-TTS (32 NFE w/ SS) F5-TTS (32 NFE w/o SS) 1.26 1.27 1.80 1.77 1.97 1.61 1.58 1.93 17 0.69 0.66 0.71 0.72 0.69 0.66 0.66 0. 0.73 0.70 0.72 0.73 0.71 0.66 0.66 0.63 0.76 0.72 0.78 0.78 0.73 0.75 0.75 0.69 - - 0.34 0.68 0.68 0.26 0.53 0.53 - - 0.34 0.68 0.68 0.26 0.53 0.53 - - 0.34 0.68 0.68 0.26 0.53 0.53 B.5 ELLA-V Hard Sentences Evaluation ELLA-V [15] proposed challenging set containing 100 difﬁcult textual patterns evaluating the robustness of the TTS model. Following previous works [13, 29, 36], we include generated samples in our demo page9. We additionally compare our model with the objective evaluation results reported in E1 TTS [49]. StyleTTS 2 is TTS model leveraging style diffusion and adversarial training with large speech language models. CosyVoice is two-stage large-scale TTS system, consisting of text-to-token AR model and token-to-speech ﬂow matching model. Concurrent with our work, E1 TTSDMD is diffusion-based NAR model with distribution matching distillation technique to achieve one-step TTS generation. Since the prompts used by E1 TTSDMD are not released, we randomly sample 3second-long speeches in our LibriSpeech-PC test-clean set as audio prompts. The evaluation result is in Tab.7. We evaluate the reproduced E2 TTS and our F5-TTS with 32 NFE and Sway Sampling and report the averaged score of three random seed results. Table 7: Results of zero-shot TTS WER on ELLA-V hard sentences. The asterisk * denotes the score reported in E1 TTS. Sub. for substitution, Del. for Deletion, and Ins. for Insertion."
        },
        {
            "title": "Model",
            "content": "WER(%)) Sub.(%) Del.(%) Ins.(%) StyleTTS 2 [73] CosyVoice [65] E1 TTSDMD [49] E2 TTS [36] F5-TTS 4.83* 8.30* 4.29* 8.58 4.40 2.17* 3.47* 1.89* 3.70 1. 2.03* 2.74* 1.62* 4.82 2.40 0.61* 1.93* 0.74* 0.06 0.18 We note that higher WER compared to the results on commonly used test sets is partially due to mispronunciation (yogis to yojus, cavorts to caverts, etc.). The high Deletion rate indicates wordskipping phenomenon when our model encounters stack of repeating words. The low Insertion rate makes it clear that our model is free of endless repetition. We further emphasize that prompts from different speakers will spell very distinct utterances, where the ASR model transcribes correctly for one, and fails for another (e.g. quokkas to Cocos). 9https://SWivid.github.io/F5-TTS"
        }
    ],
    "affiliations": [
        "Geely Automobile Research Institute (Ningbo) Company Ltd.",
        "Shanghai Jiao Tong University",
        "University of Cambridge"
    ]
}