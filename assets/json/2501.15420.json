{
    "paper_title": "Visual Generation Without Guidance",
    "authors": [
        "Huayu Chen",
        "Kai Jiang",
        "Kaiwen Zheng",
        "Jianfei Chen",
        "Hang Su",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT."
        },
        {
            "title": "Start",
            "content": "Huayu Chen* 1 Kai Jiang* 1 2 Kaiwen Zheng 1 Jianfei Chen 1 Hang Su 1 Jun Zhu 1 2 5 2 0 2 6 2 ] . [ 1 0 2 4 5 1 . 1 0 5 2 : r Abstract Classifier-Free Guidance (CFG) has been default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversityfidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT. 1. Introduction Low-temperature sampling is critical technique for enhancing generation quality by focusing only on the models high-likelihood areas. Visual models mainly achieved this through Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). CFG jointly optimizes both conditional and unconditional models during training and combines them to define the sampling process. By altering the guidance scale, it can flexibly trade off image fidelity and diversity at inference time, while significantly improving sample quality. Due to *Equal contribution 1Department of Computer Science & Technology, Tsinghua University 2ShengShu, Beijing, China. Preprint. 1 its effectiveness, CFG has been default technique for wide spectrum of visual generative models, including diffusion (Ho et al., 2020), autoregressive (AR) (Chen et al., 2020; Tian et al., 2024), and masked-prediction models (Chang et al., 2022; Li et al., 2023). Despite the wide application, CFG requires inferencing both conditional and unconditional models to achieve the sampling distribution. This not only doubles the sampling cost but also complicates the post-training of visual models: When distilling pretrained diffusion models (Meng et al., 2023; Luo et al., 2023; Yin et al., 2024b) for fast inference or applying RLHF techniques (Black et al., 2023; Chen et al., 2024b), CFG requires the extra unconditional model to be additionally considered in the algorithm design. Its also worth noting that for language models (LMs), single model is sufficient to represent sampling distributions across various temperatures. However, similarly following LMs approach to divide model output by constant temperature value has largely been found ineffective in visual sampling (Dhariwal & Nichol, 2021), even for visual AR models with similar architecture to LMs (Sun et al., 2024). This leaves us wondering, how can we control the sampling temperature for visual models using only one model? Existing attempts mainly include distillation methods for diffusion models (Meng et al., 2023; Luo et al., 2023; Yin et al., 2024b) and alignment methods for AR models (Chen et al., 2024b). These methods rely heavily on pretrained CFG networks for loss definition and do not support training guidance-free models from scratch. Their two-stage optimization pipeline may also lead to performance loss compared with CFG, even after extensive tuning. Generalizability is also concern. Current methods are tailored for either continuous diffusion models or discrete AR models, lacking the versatility to cover all domains. We propose Guidance-Free Training (GFT) as foundational algorithm for building guidance-free visual generative models. GFT matches CFG in performance while requiring only single model for temperature-controlled sampling (Figure 1), effectively halving sampling costs. It offers stable and efficient training with the same convergence rate as CFG, almost no extra memory usage, and only 1020% additional computation per training update. Notably, GFT is highly versatile, applicable in all visual domains within Visual Generation Without Guidance β = 1.0 β = 0.5 β = 0.25 β = 0. Figure 1: Impact of adjusting sampling temperature β for DiT-XL/2 after applying GFT. More examples are in Figure 11. CFGs scope, including diffusion, AR, and masked models. The core idea behind GFT is straightforward (Figure 2). GFT and CFG optimize the same conditional loss. CFG parameterizes an independent conditional model and combines it with another unconditional model for guided sampling. Contrastively, GFT does not explicitly define conditional model. Instead, it constructs the conditional model as linear interpolation between sampling model and the unconditional model. By training this implicit network, GFT directly optimizes the underlying sampling model, which is then employed for visual generation without guidance. In essence, one can consider GFT simply as conditional parameterization technique in CFG training. This perspective makes GFT extremely easy to implement based on existing codebases, requiring only few lines of modifications and with most design choices and hyperparameters inherited. We verify the effectiveness and efficiency of GFT in both class-to-image and text-to-image tasks, spanning 5 distinctive types of visual models: DiT (Peebles & Xie, 2023), VAR (Tian et al., 2024), LlamaGen (Sun et al., 2024), MAR (Li et al., 2024) and LDM (Rombach et al., 2022). Across all models, GFT enjoys almost lossless FID in fine-tuning existing CFG models into guidance-free models (Sec. 5.2). For instance, we achieve guidance-free FID of 1.99 for the DiT-XL model with only 2% of pretraining epochs, while the CFG performance is 2.11. This surpasses previous distillation and alignment methods in their respective application domains. GFT also demonstrates great superiority in building guidance-free models from scratch. With the same amount of training epochs, GFT models generally match or even outperform CFG models, despite being 50% cheaper in sampling (Sec. 5.3). By taking in temperature parameter as model input, GFT can achieve flexible diversity-fidelity trade-off similar to CFG (Sec. 5.4). Our main contribution is GFT as fundamental visual generative modeling objective. GFT enables effective lowtemperature visual generation without guided sampling. We hope it leads to new paradigm of visual generative training. Figure 2: Comparison of GFT and CFG method. GFT shares CFGs training objective but has different parameterization technique for the conditional model. This enables direct training of an explicit sampling model. 2. Background 2.1. Visual Generative Modeling Continuous diffusion models. Diffusion models (Ho et al., 2020) define forward process that gradually injects noises into clean images from data distribution p(x): xt = αtx + σtϵ, where [0, 1], and ϵ is standard Gaussian noise. αt, σt defines the denoising schedule. We have (cid:90) pt(xt) = (xtαtx, σ2 I)p(x)dx, where p0(x) = p(x) and p1 (0, 1). Given dataset p(x, c), we can train conditional diffusion models by predicting the Gaussian noise added to xt. Ep(x,c),t,ϵ (cid:2)ϵθ(xtc) ϵ 2 (cid:3) . min θ (1) 2 Visual Generation Without Guidance More formally, Song et al. (2021) proved that Eq. 1 is essentially performing maximum likelihood training with evidence lower bound (ELBO). Also, the denoising model ϵ θ eventually converges to the data score function: ϵ θ(xtc) = σtxt log pt(xtc) (2) Given condition c, ϵθ can be leveraged to generate images from pθ(xc) by denoising noises from p1 iteratively. Discrete AR & masked models. AR models (Chen et al., 2020) and masked-prediction models (Chang et al., 2022) function similarly. Both discretize images into token sequences x1:N and then perform token prediction. Their maximum likelihood training objective can be unified as min θ Ep(x1:N ,c) (cid:88) pθ(xnx<n, c). (3) For AR models, x<n represents the first tokens in predetermined order, and xi is the next token to be predicted. For masked models, xi represents all the unknown tokens that are randomized masked during training, while x<n are the unmasked ones. Due to discrete modeling, the data likelihood pθ in Eq. 3 can be easily calculated. 2.2. Classifier-Free Guidance In diffusion modeling, vanilla temperature sampling (dividing model output by constant value) is generally found ineffective in improving generation quality (Dhariwal & Nichol, 2021). Current methods typically employ CFG (Ho & Salimans, 2022), which redefines the sampling denoising function ϵs θ(xtc) using two denoising models: 3. Method Despite its effectiveness, CFG requires inferencing an extra unconditional model to guide the sampling process, directly doubling the computation cost. Moreover, CFG complicates the post-training of visual generative models because the unconditional model needs to be additionally considered in algorithm design (Meng et al., 2023; Black et al., 2023). We propose Guidance-Free Training (GFT) as an alternative method of CFG for improving sample quality in visual generation without guided sampling. GFT matches CFG in performance but only leverages single model ϵs θ(xc, s) to represent CFGs sampling distribution ps(xc). We derive GFTs training objective for diffusion models in Sec. 3.1, discuss its practical implementation in Sec. 3.2, and explain how it can be extended to discrete AR and masked models in Sec. 3.3. 3.1. Algorithm Derivation The key challenge in directly learning sampling model ϵs θ is the absence of dataset that aligns with distribution ps(xc) in Eq. 5. It is impractical to optimize maximum-likelihoodtraining objectives like Eps(x,c),t,ϵ (cid:2)ϵs θ(xtc) ϵ2 (cid:3) . min θ because we cannot draw samples from ps. In contrast, training ϵc θ (x) separately as in CFG is feasible because their corresponding datasets, {(x, c) p(x, c)} and {x p(x)}, can be easily obtained. θ(xc) and ϵu θ(xtc) := ϵc ϵs θ(xtc) + s[ϵc θ(xtc) ϵu θ(xt)], (4) To address this, we reformulate Eq. 4 by simple algebra: θ and ϵu where ϵc θ respectively model the conditional data distribution p(xc) and the unconditional data distribution p(x). θ, by randomly masking the conditioning data in Eq. 1 with some fixed probability. θ can be jointly trained with ϵc In practice, ϵu According to Eq. 2, CFGs sampling distribution ps(xc) has shifted from standard conditional distribution p(xc) to ps(xc) p(xc) (cid:20) p(xc) p(x) (cid:21)s . (5) CFG offers an effective approach for lowering sampling temperature in visual generation by simply increasing > 0, thereby substantially improving sample quality. Besides diffusion, CFG is also critical sampling technique in discrete visual modeling (Chang et al., 2022; Team, 2024; Tian et al., 2024; Xie et al., 2024). ϵc θ(xtc) (cid:124) (cid:123)(cid:122) (cid:125) p(xc) Learnable = 1 1 + + ϵs θ(xtc) (cid:124) (cid:123)(cid:122) (cid:125) ps(xc) Target sampling model 1 + + ϵu θ(xt) (cid:124) (cid:123)(cid:122) (cid:125) p(x) Learnable . (6) Although learning ϵs combined with an unconditional model ϵu standard conditional ϵc leverage the same conditional loss in Eq. 1 to train ϵs θ: θ directly is difficult, we note it can be θ to represent the θ, which is learnable. Thus, we can Ep(x,c),t,ϵ min θ (cid:20) 1 1 + ϵs θ(xtc) + θ (xt) ϵ2 ϵu 2 (cid:21) , 1 + (7) where ϵ is standard Gaussian noise, xt = αtx + σtϵ are diffused images. αt and σt define the forward process. To this end, we have practical algorithm for directly learning guidance-free models ϵs θ. However, unlike CFG which allows controlling sampling temperature by adjusting guidance scale to trade off fidelity and diversity, our method Algorithm 1 Guidance-Free Training (Diffusion) Visual Generation Without Guidance 1: Initialize θ from pretrained models or from scratch. 2: for each gradient step do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for // CFG training w/ pseudo-temperature β x, p(x, c) β (0, 1), (0, 1) ϵ (0, 2) xt = αtx + σtϵ = masked by with 10% probability Calculate ϵs // Additional to CFG Calculate ϵu ϵθ = βϵs // Standard Maximum Likelihood Training θ θ λθϵθ ϵ2 2 θ (xt, 1) in evaluation mode θ(xtc, β) + (1 β)sg[ϵu θ(xtc, β) in training mode θ (xt, 1)] (Eq. 9) still lacks similar inference-time flexibility as Eq. 7 is performed for specific s. To solve this problem, we define pseudo-temperature β := 1/(1 + s) and further condition our sampling model ϵs θ(xtc, β) on the extra β input. We can randomly sample β [0, 1] during training, corresponding to [0, +). The GFT objective in Eq. 7 now becomes: min θ (cid:2)βϵs Ep(x,c),t,ϵ,β θ(xtc, β) + (1 β)ϵu (cid:3) . θ (xt) ϵ2 2 (8) When β = 1, Eq. 8 reduces to conditional diffusion loss ϵs θ(xtc, β) ϵ2 2. When β = 0, Eq. 8 becomes an unθ (xt) ϵ2 conditional loss ϵu 2. This allows simultaneous training of both conditional and unconditional models. As pseudo-temperature β decreases 1 0, the modeling target for ϵs θ gradually shifts from conditional data distribution p(xc) to lower-temperature distribution ps(xc) as defined by Eq. 5. 3.2. Practical Implementation Eq. 9 presents practical loss function of GFT. The implementation is in Algorithm 1. Ldiff θ (x, c, t, ϵ, β) = βϵs θ(xtc, β) + (1 β)sg[ϵu θ (xt, 1)] ϵ2 2. (9) Stopping the unconditional gradient. The main difference between Eq. 9 and Eq. 8 is that ϵu θ is computed in evaluation mode, with model gradients stopped by the sg[] operation. To train the model unconditionally, we randomly mask conditions with when computing ϵs θ. We show this design does not affect the training convergence point: Theorem 1 (GFT Optimal Solution). Given unlimited model capacity and training data, the optimal ϵs θ for opti4 Figure 3: Comparison of computational efficiency between GFT and CFG. Estimated based on the DiT-XL model. mizing Eq. 9 and Eq. 8 are the same. Both satisfy ϵs θ (xtc, β) (cid:20) 1 β = σt xt log pt(xtc) ( 1)xt log pt(xt) (cid:21) 1 β Proof. In Appendix B. The stopping-gradient technique has the following benefits: (1) Alignment with CFG. The practical GFT algorithm (Eq. 9) differs from CFG training by single unconditional inference step. This allows us to implement GFT with only few lines of code based on existing codebases. (2) Computational efficiency. Since the extra unconditional calculation is gradient-free. GFT requires virtually no extra GPU memory and only 19% additional train time per update vs. CFG (Figure 3). This stands in contrast with the naive implementation without gradient stopping (Eq. 8), which is equivalent to doubling the batch size for CFG training. (3) Training stability. We empirically observe that stopping the gradient for the unconditional model could lead to better training stability and thus improved performance. Input of β. GFT requires an extra pseudo-temperature input in comparison with CFG. For this, we first process β using the similar Fourier embedding method for diffusion time (Dhariwal & Nichol, 2021; Meng et al., 2023). This is followed by some MLP layers. Finally, the temperature embedding is added to the models original time or class embedding. If fine-tuning, we apply zero initialization for the final MLP layer so that β would not affect model output at the start of training. Hyperparameters. Due to the high similarity between CFG and GFT training, we inherit most hyperparameter choices used for existing CFG models and mainly adjust parameters like learning rate during finetuning. When training from scratch, we find simply keeping all parameters the same with CFG is enough to yield good performance. Visual Generation Without Guidance Table 1: Comparison of GFT (ours) and other guidance-free methods. Numbers are reported based on experiments for the DiT-XL model or the VAR-d30 model. We use 880GB H100 GPU cards."
        },
        {
            "title": "Method",
            "content": "Guidance Distillation Condition Contrastive Alignment Guidance-Free Training"
        },
        {
            "title": "Loss form",
            "content": "ϕ + pu pc ϕ ps Diffusion θ(xc) (cid:13) (cid:13)ϵs θ [(1+s)ϵc ϕ sϵu ϕ](cid:13) (cid:13) 2 2 ϕ p(xc) θ + pc ps AR & Masked p(x) θ + pu ps θ p(xc)"
        },
        {
            "title": "All",
            "content": "logσ(rp θ )logσ(rn θ ); rθ = 1 log ps θ pc ϕ βf θ +(1β)f θ + Diff/AR Loss Train from scratch?"
        },
        {
            "title": "Not allowed",
            "content": "# Inference / Update Train time / Update"
        },
        {
            "title": "GPU memory usage",
            "content": "3 1.19 1.15 4 1.69 1."
        },
        {
            "title": "Allowed",
            "content": "2 1.00 1.00 Training epochs. When fine-tuning pretrained CFG models, we find 1% - 5% of pretraining epochs are sufficient to achieve nearly lossless FID performance. When from scratch, we always use the same training epochs compared with the CFG baseline. 3.3. GFT for AR and Masked Models CFG is also standard decoding method for discrete AR or masked visual models, critical to improving their sample quality (Chang et al., 2022; Li et al., 2023; Tian et al., 2024). Different from diffusion models which apply CFG in the score field, AR and masked models perform guided sampling on model logits: 4. Connection with Other Guidance-Free"
        },
        {
            "title": "Methods",
            "content": "Previous attempts to remove guided sampling from visual generation mainly include distillation methods for diffusion models and alignment methods for AR models. Alongside GFT, these methods all transform the sampling distribution ps into simpler, learnable forms, differing mainly in how they decompose the sampling distribution and set up modeling targets  (Table 1)  . Guidance Distillation (Meng et al., 2023) is quite straightforward, it simply learns single model to match the output of pretrained CFG targets using L2 loss: ℓs θ(xnx<n, c) = θ(xnx<n, c) + s[ℓc ℓc θ(xnx<n, c) ℓu θ(xnx<n)]. Here xn represents the i-th token of tokenized image x. ℓθ are unnormalized model logits. Similar to Sec. 3.1, we can derive the GFT objective for AR and masked models as standard cross-entropy loss: LAR θ (x, c, β) = = (cid:88) (cid:88) log ps θ(xnx<n, c, β) (10) θ(xnx<n,c,β) log (cid:80) eℓs wV eℓs θ(wx<n,c,β) where is token in the vocabulary V, and ℓs θ(wx<n, c, β) := βℓs θ(wx<n, c, β) + (1 β)sg[ℓu θ (wx<n)]. , (11) (12) In Sec. 5, we apply GFT to wide spectrum of visual generative models, including diffusion, AR, and masked models, demonstrating its versatility. θ = (cid:13) LGD (cid:13)ϵs θ(xtc, s) (cid:2)(1+s)ϵc ϕ(xtc)sϵu ϕ(xt)(cid:3)(cid:13) 2 , (cid:13) 2 (13) ϕ and ϵc where ϵu ϕ are pretrained models. LGD θ breaks down the sampling model into linear combination of conditional and unconditional models, which can be separately learned. Despite being effective, Guidance distillation relies on pretrained CFG models as teacher models, and cannot be leveraged for from-scratch training. This results in an indirect, two-stage pipeline for learning guidance-free models. In comparison, our method unifies guidance-free training in one singular loss, allowing learning in an end-to-end style. Besides, GFT no longer requires learning an explicit conditional model ϵc θ. This saves training computation and VRAM usage. detailed comparison is in Table 1. Condition Contrastive Alignment (Chen et al., 2024b) constructs preference pair for each image in the dataset and applies similar preference alignment techniques for language models (Rafailov et al., 2023; Chen et al., 2024a) to fine-tune visual AR models: LCCA θ = logσ [rθ(x, cp)]logσ [rθ(x, cn)] , (14) Table 2: Model comparisons on the class-conditional ImageNet 256 256 benchmark. Table 3: Zero-shot text-to-image generation performance comparison on COCO 2014 validation set. Visual Generation Without Guidance FID w/o Guidance w/ Guidance Text to Image Models FID w/o Guidance w/ Guidance Model Type Diffusion Models ADM (Dhariwal & Nichol, 2021) LDM-4 (Rombach et al., 2022) U-ViT-H/2 (Bao et al., 2023) MDTv2-XL/2 (Gao et al., 2023) DiT-XL/2 (Peebles & Xie, 2023) +Distillation (Meng et al., 2023) +GFT (Ours) Autoregressive Models VQGAN (Esser et al., 2021) ViT-VQGAN (Yu et al., 2021) RQ-Transformer (Lee et al., 2022) LlamaGen-3B (Sun et al., 2024) +CCA (Chen et al., 2024b) +GFT (Ours) VAR-d30 (Tian et al., 2024) +CCA (Chen et al., 2024b) +GFT (Ours) Masked Models MaskGIT (Chang et al., 2022) MAGVIT-v2 (Yu et al., 2023b) MAGE (Li et al., 2023) MAR-B (Li et al., 2024) +GFT (Ours) 7.49 10.56 5.06 9.34 2.11 1.99 15.78 4.17 7.55 9.44 2.69 2.21 5.26 2.54 1.91 6.18 3.65 6.93 4.17 2.39 3.94 3.60 2.29 1.58 2.11 5.20 3.04 3.80 2.22 1.92 1.78 2.27 where cp is the preferred positive condition corresponding to the image x, cn is negative condition randomly and independently sampled from the dataset. Given conditional reference model pc ϕ, the implicit reward rθ is defined as rθ(x, c) := 1 log ps θ(xc) pc ϕ(xc) . CCA proves the optimal solution for solving Eq. 14 is θ = log p(xc) θ(xc) also satisfies Eq. 5. p(x) , thus the convergence point for ps θ + (1 β)pu Both CCA and GFT train sampling model ps θ directly by combining it with another model to represent learnable distribution. GFT leverages βps θ to represent standard conditional distribution p(xc), while CCA combines ps θ and pretrained pϕ(xc) to represent the conditional residual log p(xc) p(x) . They also differ in applicable areas. CCA is based on language alignment losses, which requires calculating model likelihood log pθ during training. This forbids its direct application to diffusion models, where calculating exact likelihood is infeasible. 5. Experiments Our experiments aim to investigate: 1. GFTs effectiveness and efficiency in fine-tuning CFG 6 GLIDE (Nichol et al., 2021) LDM (Rombach et al., 2022) DALLE 2 (Ramesh et al., 2022) Stable Diffusion 1.5 (Rombach et al., 2022) +GFT (Ours) 22.55 8.32 12.24 12.63 10.39 8.30 models into guidance-free variants (Sec. 5.2) 2. GFTs ability in training guidance-free models from scratch, compared with classic CFG training (Sec. 5.3) 3. GFTs capacity in controlling diversity-fidelity tradeoff through temperature parameter β. (Sec. 5.4) 5.1. Experimental Setups Tasks & Models. We evaluate GFT in both class-to-image (C2I) and text-to-image (T2I) tasks. For C2I, we experiment with diverse architectures: DiT (Peebles & Xie, 2023) (transformer-based latent diffusion model), MAR (Li et al., 2024) (masked-token prediction model with diffusion heads), and autoregressive models: VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024). For T2I, we use Stable Diffusion 1.5 (Rombach et al., 2022), text-to-image model based on the U-Net architecture (Ronneberger et al., 2015), to provide comprehensive evaluation of GFTs performance across various conditioning modalities. All these models rely on guided sampling as critical component. Training & Evaluation. We train C2I models on ImageNet-256x256 (Deng et al., 2009). For T2I models, we use subset of the LAION-Aesthetic 5+ (Schuhmann et al., 2022), consisting of 12.8 million image-text pairs. Our codebases are directly modified from the official CFG implementation of each respective baseline, keeping most hyperparameters consistent with CFG training. We use official OPENAI evaluation scripts to evaluate our C2I models. For T2I models, we evaluate our model on zero-shot COCO 2014 (Lin et al., 2014). The training and evaluation details for each model can be found in Appendix D. 5.2. Make CFG Models Guidance-Free Method Effectiveness. In Table 2 and 3, we apply GFT to fine-tune wide spectrum of visual generative models. With less than 5% pretraining computation, the fine-tuned models achieve comparable FID scores to CFG while being 2 faster in sampling. Figure 4 visually demonstrates this quality improvement. Visual Generation Without Guidance w/o Guidance GFT (w/o Guidance) w/ CFG Guidance Figure 4: Qualitative T2I comparison between vanilla conditional generation, GFT, and CFG on Stable Diffusion 1.5 with the prompt Elegant crystal vase holding pink peonies, soft raindrops tracing paths down the window behind it. More examples are in Figure 12. Comparison with other guidance-free approaches. GFT achieves comparable performance to guidance distillation (designed for diffusion models) while outperforming AR alignment method in both performance  (Table 2)  and efficiency  (Table 1)  . We attribute this effectiveness to GFTs end-to-end training style, and to the nice convergence property of its maximum likelihood training objective. Finetuning Efficiency Figure 5 tracks the FID progression of DiT-XL/2 during fine-tuning. The guidance-free FID rapidly improves from 9.34 to 2.22 in the first epoch, followed by steady optimization. After three epochs, our model achieves better FID than the CFG baseline (2.05 vs 2.11). This computational is almost negligible compared with pretraining, demonstrating GFTs efficiency. Figure 5: Efficient convergence of FID scores for GFT using DiT-XL/2 model across training epochs. classic supervised training using CFG across various architectures, maintaining consistent training epochs. We mainly focus on smaller models due to computational constraints. Performance. Table 4 shows that GFT models trained from scratch outperform CFG baselines across DiT-B/2, MAR-B, and LlamaGen-L models, while reducing evaluation costs by 50%. Notably, these from-scratch models outperform their fine-tuned counterparts, demonstrating the advantages of direct guidance-free training. Training stability. An informative indicator of an algorithms stability and scalability is its loss convergence speed. With consistent hyperparameters, we find GFT convergences at least as fast as CFG for both diffusion and autoregressive modeling (Figure 6). Direct loss comparison is valid as both methods optimize the same objective: the conditional modeling loss for the dataset distribution. The only difference is that the conditional model for CFG is single end-to-end model, while for GFT it is constructed as linear interpolation of two model outputs. Based on the above observations, we believe that GFT is at least as stable and reliable as CFG algorithms, providing new training paradigm and viable alternative for visual generative models. 5.4. Sampling Temperature for Visual Generation 5.3. Building Guidance-Free Models from Scratch Training Guidance-Free Models from scratch is more tempting than the two-stage pipeline adopted by Sec 5.2. However, this is also more challenging due to higher requirements for the algorithms stability and convergence speed. We investigate this by comparing from-scratch GFT training with key advantage of CFG is its flexible sampling temperature for diversity-fidelity trade-offs. Our results demonstrate that GFT models share this capability. We evaluate diversity-fidelity trade-offs across various models, with FID-IS trade-off for c2i models and FID-CLIP trade-off for t2i models. Results for DiT-XL/2 (fine-tuning), Visual Generation Without Guidance Method Guidance Base +CFG +GFT GFT w/o w/ w/o w/o DiT-B/2 (Peebles & Xie, 2023) FID 44.8 9.72 10.9 9.04 IS 30.7 161.5 128.5 166.6 MAR-B (Li et al., 2024) IS FID 175.4 4.17 2.27 260.7 264.7 2.39 279.5 2.27 LlamaGen-L (Sun et al., 2024) FID 19.0 3.06 2.88 2.52 IS 64.7 257.1 238.4 270. VAR-d16 (Tian et al., 2024) FID 11.97 3.36 3.28 3.42 IS 105.5 280.1 251.3 254.8 Table 4: Performance comparison between GFT from-scratch training, CFG, and GFT fine-tuning variants across different model architectures. GFT and the base model are trained for the same number of epochs. Figure 6: Comparison of training dynamics between GFT and CFG on diffusion and autoregressive models. Figure 8: FID-CLIP trade-off comparison on COCO-2014 validation set. Methods compared using Stable Diffusion 1.5 (SD 1.5). Figure 1 shows how adjusting temperature β produces effects similar to adjusting CFGs scale s. This similarity results from both methods aiming to model the same distribution (Eq. 5). The key difference is that GFT directly learns series of sampling distributions controlled by β through training, while CFG modifies the sampling process to achieve comparable results. 6. Conclusion In this work, we proposed Guidance-Free Training (GFT) as an alternative to guided sampling in visual generative models, achieving comparable performance to ClassifierFree Guidance (CFG). GFT reduces sampling computational costs by 50%. The method is simple to implement, requiring minimal modifications to existing codebases. Unlike previous distillation-based methods, GFT enables direct training from scratch. Our extensive evaluation across multiple types of visual models demonstrates GFTs effectiveness. The approach maintains high sample quality while offering flexible control over the diversity-fidelity trade-off through temperature adjustment. GFT represents an advancement in making high-quality visual generation more efficient and accessible. Figure 7: FID-IS trade-off comparisons on ImageNet. Upper: DiT-XL/2 with GFT (fine-tuned), CFG, and Guidance Distillation. Lower: LlamaGen-L with GFT (trained from scratch) and CFG. LlamaGen-L (from-scratch training) and Stable Diffusion 1.5 (fine-tuning) are shown in Figures 7 and 8, with additional trade-off curves provided in Appendix C. For DiTXL/2, we also compare GFT with Guidance Distillation, showing that GFT achieves results comparable to both CFG and Guidance Distillation in the diversity-fidelity trade-off. Visual Generation Without Guidance"
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Huanran Chen, Xiaoshi Wu, Cheng Lu, Fan Bao, Chengdong Xiang, Zhengyi Wang, Chang Li and Peize Sun for the discussion."
        },
        {
            "title": "Impact Statement",
            "content": "Our Guidance-Free Training (GFT) method significantly reduces the computational costs of visual generative models by eliminating the need for dual inference during sampling, contributing to more sustainable AI development and reduced environmental impact. However, since our method accelerates the sampling process of generative models, it could potentially be misused to create harmful content more efficiently, emphasizing the importance of establishing appropriate safety measures and deploying these models responsibly with proper oversight mechanisms."
        },
        {
            "title": "References",
            "content": "Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K., Freeman, W. T., Rubinstein, M., et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Chen, H., He, G., Su, H., and Zhu, J. Noise contrastive alignment of language models with explicit rewards. Advances in neural information processing systems, 2024a. Chen, H., Su, H., Sun, P., and Zhu, J. Toward guidance-free ar visual generation via condition contrastive alignment. arXiv preprint arXiv:2410.09347, 2024b. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In International conference on machine learning, pp. 1691 1703. PMLR, 2020. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. Chung, H., Kim, J., Park, G. Y., Nam, H., and Ye, J. C. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis, 2024. Fan, L., Li, T., Qin, S., Li, Y., Sun, C., Rubinstein, M., Sun, D., He, K., and Tian, Y. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2316423173, 2023. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Ilharco, G., Wortsman, M., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. 9 Visual Generation Without Guidance Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124 10134, 2023. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Koulischer, F., Deleu, J., Raya, G., Demeester, T., and Ambrogioni, L. Dynamic negative guidance of diffusion models: Towards immediate content removal. In Neurips Safe Generative AI Workshop 2024. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Li, T., Chang, H., Mishra, S., Zhang, H., Katabi, D., and Krishnan, D. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21422152, 2023. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. Lu, C., Chen, H., Chen, J., Su, H., Li, C., and Zhu, J. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. In International Conference on Machine Learning, pp. 22825 22855. PMLR, 2023. Luo, S., Tan, Y., Huang, L., Li, J., and Zhao, H. Latent consistency models: Synthesizing high-resolution arXiv preprint images with few-step inference. arXiv:2310.04378, 2023. Ma, X., Zhou, M., Liang, T., Bai, Y., Zhao, T., Chen, H., and Jin, Y. Star: Scale-wise text-to-image generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1429714306, June 2023. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing arXiv preprint with text-guided diffusion models. arXiv:2112.10741, 2021. Parmar, G., Zhang, R., and Zhu, J.-Y. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1141011420, 2022. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot textIn International Conference on to-image generation. Machine Learning, pp. 88218831. PMLR, 2021. 10 Visual Generation Without Guidance Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 3647936494, 2022. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. Shenoy, R., Pan, Z., Balakrishnan, K., Cheng, Q., Jeon, Y., Yang, H., and Kim, J. Gradient-free classifier guidarXiv preprint ance for diffusion model sampling. arXiv:2411.15393, 2024. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Song, J., Zhang, Q., Yin, H., Mardani, M., Liu, M.-Y., Kautz, J., Chen, Y., and Vahdat, A. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pp. 3248332498. PMLR, 2023. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Song, Y., Durkan, C., Murray, I., and Ermon, S. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34: 14151428, 2021. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Tang, H., Wu, Y., Yang, S., Xie, E., Chen, J., Chen, J., Zhang, Z., Cai, H., Lu, Y., and Han, S. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, W. T. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024a. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024b. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023a. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023b. Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. Zhang, Q., Dai, X., Yang, N., An, X., Feng, Z., and Ren, X. Var-clip: Text-to-image generator with visual autoregressive modeling. arXiv preprint arXiv:2408.01181, 2024. Zhao, M., Bao, F., Li, C., and Zhu, J. Egsde: Unpaired image-to-image translation via energy-guided stochastic 11 Visual Generation Without Guidance differential equations. Advances in Neural Information Processing Systems, 35:36093623, 2022. Zhou, M., Wang, Z., Zheng, H., and Huang, H. Long and short guidance in score identity distillation for arXiv preprint one-step text-to-image generation. arXiv:2406.01561, 2024. 12 A. Related Work Visual Generation Without Guidance Visual generation model with guidance. Visual generative modeling has witnessed significant advancements in recent years. Recent explicit-likelihood approaches can be broadly categorized into diffusion-based models (Ho et al., 2020; Song et al., 2020b; Dhariwal & Nichol, 2021; Kingma et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Karras et al., 2022; Bao et al., 2023; Peebles & Xie, 2023; Esser et al., 2024; Xie et al., 2024), auto-regressive models (Chen et al., 2020; Esser et al., 2021; Ramesh et al., 2021; Yu et al., 2021; Tian et al., 2024; Team, 2024; Sun et al., 2024; Ma et al., 2024; Zhang et al., 2024; Tang et al., 2024), and masked-prediction models (Chang et al., 2022; Yu et al., 2023a; Chang et al., 2023; Li et al., 2024; Yu et al., 2024; Fan et al., 2024). The introduction of guidance techniques has substantially improved the capabilities of these models. These include classifier guidance (Dhariwal & Nichol, 2021), classifier-free guidance (Ho & Salimans, 2022), energy guidance (Chung et al., 2022; Zhao et al., 2022; Lu et al., 2023; Song et al., 2023), and various advanced guidance methods (Kynkaanniemi et al., 2024; Karras et al., 2024; Chung et al., 2024; Koulischer et al.; Shenoy et al., 2024). Guidance distillation. To address the computational overhead introduced by classifier-free guidance (CFG), One widely used approach to remove CFG is guidance distillation (Meng et al., 2023), where student model is trained to directly learn the output of pre-trained teacher model that incorporates guidance. This idea of guidance distillation has been widely adopted in methods aimed at accelerating diffusion models (Luo et al., 2023; Yin et al., 2024b;a; Zhou et al., 2024). By integrating the teacher models guided outputs into the training process, these approaches achieve efficient few-step generation without guidance. Condition Contrastive Alignment. Beyond guidance distillation, Condition Contrastive Alignment (CCA) (Chen et al., 2024b) could also significantly enhance the guidance-free performance of visual AR models through alignment (Rafailov et al., 2023; Chen et al., 2024a) in self-contrastive manner. 13 Visual Generation Without Guidance B. Proof of Theorem 1 We first copy the training objective in Eq. 9 and Eq. 8. θ = Ep(x,c),t,ϵ,β Lraw = Ep(x,c),t,ϵ,ββϵs θ(xtc, β) + (1 β)ϵu θ(xtc, β) + (1 β)sg[ϵu θ (xt) ϵ2 2 θ (xt, 1)] ϵ2 2. (cid:2)βϵs (cid:3) ."
        },
        {
            "title": "Lpractical",
            "content": "θ (15) (16) Theorem 1 (GFT Optimal Solution). Given unlimited model capacity and training data, the optimal ϵs 15 and Eq. 16 are the same. Both satisfy θ for optimizing Eq. ϵs θ (xtc, β) = σt (cid:20) 1 β xt log pt(xtc) ( (cid:21) 1)xt log pt(xt) 1 β Proof. The proof is quite straightforward. First consider the unconditional part of the model. Let β = 1 in Lraw θ , we have θ = Ep(x,c),t,ϵ,β=1 Lraw (cid:2)ϵu θ (xt) ϵ2 2 (cid:3) , which is standard unconditional diffusion loss. According to Eq. 2 we have Then we prove stopping the unconditional gradient does not change this optimal solution. Taking derivatives of Lpractical have: θ we ϵu θ (xt) = σtxt log pt(xt) (17) θLpractical θ (c = ) = Ep(x),t,ϵ,βθβϵs = Ep(x),t,ϵ,β2β[θϵs = Ep(x),t,ϵ,β2β[θϵs = [2Eβ]θEp(x,c),t,ϵ,β=1 = [2Eβ]θLraw θ (β = 1) θ (xt, 1)] ϵ2 2 θ(xt, 1) + (1 β)sg[ϵu θ(xt, 1)]βϵs θ(xt, 1)]ϵs (cid:2)ϵu θ(xt, 1) + (1 β)ϵu θ(xt, 1) ϵ2 (cid:3) θ (xt) ϵ2 θ (xt, 1) ϵ2 Since [2Eβ] is constant, this does not change the convergence point of Lraw Lpractical remains the same. θ θ . The optimal unconditional solution for For the conditional part of the model, since both Lraw θ and Lpractical θ are standard conditional diffusion loss, we have βϵs θ (xtc, β) + (1 β)ϵu θ (xt) = σtxt log pt(xtc) Combining Eq. 17, we have ϵs θ (xtc, β) = σt (cid:20) 1 β xt log pt(xtc) ( 1)xt log pt(xt) . (cid:21) 1 β Let = 1 β 1, we can see that GFT models the same sampling distribution as CFG (Eq. 4). 14 C. Additional Experiment Results. Visual Generation Without Guidance Figure 9: FID-IS trade-off comparison in fine-tuning experiments. Figure 10: FID-IS trade-off comparison in from-scratch-training experiments. 15 Visual Generation Without Guidance β = 1. β = 0.5 β = 0.25 β = 0.1 Figure 11: Additional results of temperature sampling (β) impact on DiT-XL/2 after applying GFT. 16 Visual Generation Without Guidance w/o Guidance GFT (w/o Guidance) w/ CFG Guidance Figure 12: Additional results of qualitative T2I comparison between vanilla conditional generation, GFT, and CFG on Stable Diffusion 1.5. 17 Visual Generation Without Guidance w/o Guidance GFT (w/o Guidance) w/ CFG Guidance Figure 13: Additional results of qualitative C2I comparison between vanilla conditional generation, GFT, and CFG on DiT-XL/2. 18 D. Implementation Details. Visual Generation Without Guidance For all models, we keep training hyperparameters and other design choices consistent with their official codebases if not otherwise stated. We employ mix of H100, A100 and A800 GPU cards for experimentation. DiT. We mainly apply GFT to fine-tune DiT-XL/2 (28 epochs, 2% of pretraining epochs) and train DiT-B/2 from scratch (80 epochs, following the original DiT papers settings (Peebles & Xie, 2023)). Since the DiT-B/2 pretraining checkpoint is not publicly available, we reproduce its pretraining experiment. For all experiments, we use batch size of 256 and learning rate of 1e 4. For DiT-XL/2 fine-tuning experiments, we employ cosine-decay learning rate scheduler. For comparison, we also fine-tune DiT-XL/2 using guidance distillation, with scale range from 1 to 5, while keeping all other hyperparameters aligned with GFT. The original DiT uses the old-fashioned DDPM (Ho et al., 2020) which learns both the mean and variance, while GFT is only concerned about the mean. We therefore abandon the variance output channels and related losses during training and switch to the Dpm-solver++ (Lu et al., 2022) sampler with 50 steps at inference. For reference, our baseline, DiT-XL/2 with CFG, achieves an FID of 2.11 using DPM-solver++, compared with 2.27 reported in the original paper. All results are evaluated with EMA models. The EMA decay rate is set to 0.9999. VAR. We mainly apply GFT to fine-tune VAR-d30 models (15 epochs) or train VAR-d16 models from scratch (200 epochs). Batch size is 768. The initial learning rate is 1e 5 in fine-tuning experiments and 1e 4 in pretraining experiments. Following VAR (Tian et al., 2024), we employ learning rate scheduler including warmup and linear decay process (minimal is 1% of the initial). VAR by default adopts pyramid CFG technique on predicted logits. The guidance scale 0 decreases linearly during the decoding process. Specifically, let be the current decoding step index, and be the total steps. The n-step guidance scale sn is sn = 1 s0. We find pyramid CFG is crucial to an ideal performance of VAR, and thus design similar pyramid β schedule during training: βn = (cid:20) ( )α( 1 β0 (cid:21)1 1) + 1 , where βn represents the token-specific β value applied in the GFT AR loss (Eq. 12). α 0 is hyperparameter to be tuned. When α = 0, we have βn = β0, standing for standard GFT. When α = 1.0, we have 1 1), βn corresponding to the default pyramid CFG technique applied by VAR. In practice, we set α = 1.5 in GFT training and find this slightly outperforms α = 1.0. 1 = ( 1 )( 1 β0 LlamaGen. We mainly apply GFT to fine-tune LlamaGen-3B models (15 epochs) or train LlamaGen-L models from scratch (300 epochs). For fine-tuning, the batch size is 256, and the learning rate is 2e 4. For pretraining, the batch size is 768, and the learning rate is 1e 4. We adopt cosine-decay learning rate scheduler in all experiments. MAR. We apply GFT to MAR-B, including both fine-tuning (10 epochs) and training from scratch (800 epochs). We find the batch size crucial for MAR and use 2048 following the original paper. For fine-tuning, we employ learning rate scheduler including 5-epoch linear warmup to 8e 4 and cosine decay process to 1e 4. For training from scratch, we employ 100-epoch linear lr warmup to 8e 4, followed by constant lr schedule, which is the same configuration as the original MAR pretraining. The original MAR follows the old-fashioned DDPM (Ho et al., 2020) which learns both the mean and variance, while GFT is only concerned about the mean. We therefore abandon the variance output channels and related losses during training and switch to the DDIM (Song et al., 2020a) sampler with 100 steps at inference. As the β condition may not precisely capture the effects of the guidance scale after training, we tune the inference β schedule to maximize the performance. Specifically, we adopt power-cosine schedule βn = (cid:20) 1 cos((n/(N 1))απ) 2 ( 1 β0 (cid:21)1 1) + 1 where we choose α = 0.4. Visual Generation Without Guidance Stable Diffusion 1.5. We apply GFT to fine-tune Stable Diffusion 1.5 (SD1.5) (Rombach et al., 2022) for 50,000 gradient updates with batch size of 256 and constant learning rate of 1e 5 with 1,000 warmup steps. We disable conditioning dropout as we find it improves CLIP score. For evaluation, following GigaGAN (Kang et al., 2023) and DMD (Yin et al., 2024b), we generate images using 30K prompts from the COCO2014 (Lin et al., 2014) validation set, downsample them to 256256, and compare with 40,504 real images from the same validation set. We use clean-FID (Parmar et al., 2022) to calculate FID and OpenCLIP-G (Ilharco et al., 2021; Cherti et al., 2023) to calculate CLIP score (Radford et al., 2021). All results are evaluated using 50 steps DPM-solver++ (Lu et al., 2022) with EMA models. The EMA decay rate is set to 0.999 for insufficient training steps. E. Prompts for Figure 12 We use the following prompts for Figure 12. vintage camera in park, autumn leaves scattered around it. Pristine snow globe showing winter village scene, sitting on frost-covered pine windowsill at dawn. Vibrant yellow rain boots standing by cottage door, fresh raindrops dripping from blooming hydrangeas. Rain-soaked Parisian streets at twilight."
        }
    ],
    "affiliations": [
        "Department of Computer Science & Technology, Tsinghua University"
    ]
}