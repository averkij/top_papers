{
    "paper_title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
    "authors": [
        "Meet Doshi",
        "Vishwajeet Kumar",
        "Yulong Li",
        "Jaydeep Sen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets."
        },
        {
            "title": "Start",
            "content": "Meet Doshi 1 Vishwajeet Kumar 1 Yulong Li 1 Jaydeep Sen"
        },
        {
            "title": "Abstract",
            "content": "General-purpose open-domain dense retrieval systems are usually trained with large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, novel reinforcement learningdriven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight w.r.t. GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on target development set. We evaluate the efficacy of our sampling strategy on wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5 4 cheaper in GPU compute. Our sampling strategy achieves 5.03 absolute NDCG@10 improvement while training multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on large pool of training datasets. 6 2 0 2 9 2 ] I . [ 1 9 5 7 1 2 . 1 0 6 2 : r 1. Introduction Text-to-embedding based dense retriever models have recently gained huge popularity with their strong results across various benchmarks (Karpukhin et al., 2020; Izacard et al., 2022; Wang et al., 2023; Chen et al., 2024; BehnamGhader et al., 2024). These models are prized for their general1IBM Research AI. Correspondence to: Meet Doshi <meet@ibm.com>. Figure 1: Trade-off between training time (GPU hours) and average NDCG@10 on the MLDR-13 test set when optimizing bge-m3-dense on the MLDR-13 dev set. izability across domains without domain-specific tuning, typically trained on vast, diverse datasets. For instance, the Sentence-Transformers project, which develops generic sentence embeddings, utilized billions of instances across multiple datasets, with domain-specific data varying widely in size. However, larger datasets dont inherently improve embedding quality, making it crucial to identify the most informative datasets and their optimal proportions for training. Effective sampling strategies are essential to prevent overfitting or underfitting, making dataset selection central challenge in developing robust, generalizable, or domainspecific embedding models. While random sampling is common default, it is limited by ignoring data source informativeness when sampling from large training datasets. Alternatives include temperature sampling and instance-based proportional sampling. more intensive approach involves creating ad-hoc sampling distributions via iterative experimentation, termed expert weights, requiring expert evaluation. However, these strategies are static and predefined, often suboptimal compared to the unknown ideal distribution for maximizing model performance. dynamic sampling approach, capable of adaptation, may better approximate this optimal distribution. There has consequently been substantial interest in mak1 Influence Guided Sampling for Domain Adaptation of Text Retrievers ing sampling adaptive. Gradient-based approaches such as DDS (Wang et al., 2020a) and its multi-target extension (Wang et al., 2020b) use gradient-derived rewards to adjust the training distribution online. DoGE (Fan et al., 2024) proposes generalization estimation function to approximate data influence, while methods like DoReMi (Xie et al., 2023a; Engstrom et al., 2024) rely on proxy models to estimate dataset utility. In practice, these dynamic methods face two main challenges: (1) instability and high variance introduced by stochastic gradients, which we empirically demonstrate in Section 5, and (2) substantial computational overhead when proxy models or expensive estimators are required. Together, these limitations motivate the design of an online, adaptive optimization strategy that can learn sampling weights efficiently and robustly while remaining computationally tractable. In this paper, we propose Influence-guided Dynamic Data Sampling strategy (Inf-DDS), computationally efficient novel algorithm that addresses the critical challenge of data sampling for domain adaptation, overcoming limitations of existing gradient-based methods. Inf-DDS iteratively takes small gradient-update steps on each domains data, monitoring the impact on the downstream metric. Domains demonstrating greater performance improvements are subsequently assigned higher rewards. This adaptive sampling strategy, inspired by recent influence-based methods (Koh & Liang, 2017; Bae et al., 2022; Fan et al., 2024; Yu et al., 2024; Xia et al., 2024) for sampling training data across multiple domains, focuses learning on the most informative subsets of data. Our algorithm offers three key advantages over prior work: (1) it eliminates the dependence on noisy gradient estimates for reward computation, (2) it efficiently reuses computations from updating the parameterized sampling distribution ψ parameters to also update the model parameters θ, making it much more computationally efficient and (3) it produces more reliable and interpretable sampling trajectories for better downstream gains. Our contributions in this work are as follows: a. We propose Inf-DDS, an influence-guided reinforcement learning approach that learns to adjust sampling probabilities across diverse training datasets, improving targetdomain retrieval performance while being much more computationally efficient. b. We validate Inf-DDS against robust benchmarks, including BEIR datasets (Thakur et al., 2021), SentenceTransformers all-MiniLM-L6-v2 (Reimers & Gurevych, 2019) and MLDR (Chen et al., 2024), demonstrating significant improvements while optimizing for target domain. 2. Related Work Recent work on training models with large, diverse data pools has explored both simple heuristic sampling and more adaptive, learned reweighting schemes. common practice is to sample languages uniformly or via temperature-scaled distribution that interpolates between uniform and sizeproportional sampling. For example in language pretraining, Cooldown (Li et al., 2024b) demonstrates improvements in multilingual training by oversampling high-resource languages during the initial phases of training, while shifting to uniform sampling across highand low-resource languages toward the end to enhance generalization. DoReMi leverages the loss gap between proxy models and the target model to optimize domain sampling weights for train set generalization. Early works on domainor task-specific adaptation select relevant subsets of data using either cross-entropy differences or simple classifiers (Moore & Lewis, 2010; Brown et al., 2020). DSIR (Xie et al., 2023b) employs importance sampling by assigning weights to training instances based on their hashed features for the target task, which then guide data selection. In contrast, CRISP (Grangier et al., 2025b) clusters the training data and assigns importance weights to clusters based on the frequency of source/target instances that fall into each cluster. Recent influence estimation approaches guide data selection by identifying and prioritizing examples that most impact models predictions or performance, ensuring the most influential data are included in training (Grosse et al., 2023; Nikdan et al., 2025; Zhou et al., 2025). Methods such as LESS (Xia et al., 2024) and Quad (Zhang et al., 2025) use first-order approximations of Influence estimates via Taylor expansions of the change in target loss after gradient update to select relevant instances. In contrast, we focus on learning domain weights online during training. Prior work shows that data models can accurately predict the influence of training samples on held-out target examples (Ilyas et al., 2022; Engstrom et al., 2024). MATES (Yu et al., 2024) similarly uses small proxy model to locally estimate oracle influence after single step, but small proxies may provide limited accuracy. Building on these insights, we propose leveraging online proxy models to compute exact influence scores as signals for learning sampling weights. DDS and DoGE move beyond fixed heuristics by learning scorer network through bi-level optimization, jointly updating the scorer and model parameters to prioritize training examples whose gradients align with held-out development set. Wang et al. (2020b) extend DDS to multiple targets (MultiDDS) by learning per-language scoring functions that optimize performance across development sets. However, in practice, gradient-based strategies suffer from significant variance in the reward signal. In contrast, our method builds 2 Influence Guided Sampling for Domain Adaptation of Text Retrievers on MultiDDS and DoGE by replacing noisy gradient-based rewards with online-computed influence scores, derived from updated model parameters. This simplifies and stabilizes training by adjusting dataset-level sampling weights and reusing intermediate computations. Extensive research has explored domain adaptation and universal generalization; providing comprehensive review is beyond the scope of this paper. We refer interested readers to (Grangier et al., 2025a; Choi et al., 2023; Chung et al., 2023; Cao et al., 2024; Pruthi et al., 2020; Park et al., 2023; Liu et al., 2025; Grosse et al., 2023) and related works for further insights. 3. Influence Guided Dynamic Data Sampling In this section, we propose reinforcement learningbased strategy for domain adaptation of text retrievers. We frame dataset sampling as bilevel optimization problem and learn an adaptive policy to maximize performance on target datasets. Our approach is illustrated in Figure 2 and elaborated in Algorithm 1. 3.1. Problem Formulation Efficient data sampling for training text retrieval models: Our goal is to devise an adaptive sampling policy that efficiently samples batches from large pool of training domains/datasets (M ) to maximize the model performance on set of target datasets (N ) a.k.a. development or dev sets. We treat each training and dev dataset as single homogeneous unit and optimize sampling at the dataset level. dev}N train}M Given set of training datasets {Di i=1 with initial sampling probability as PD(i); 1, . . . , , set of dev datasets {Dj j=1 on which we want our model to adapt, and initial model with parameters θ, our objective is to optimize the model parameter θ by learning dynamic sampling strategy for PD using parameterized policy ψ that maximizes performance on development sets. We formalize our objective through the following optimization problem: θ, ψ = arg min θ,ψ EiPD(i;ψ) (cid:2)J(θ, Di train)(cid:3) (1) where J(θ, Di train) is the empirical risk. The training datasets sampling probability distribution PD(i; ψ) is computed as: PD(i; ψ) = eψi k=1 eψk (cid:80)M (2) where is the total number of datasets in the pool. This formulation allows the sampling strategy to dynamically adjust the probabilities based on the learned parameters ( ψ ), guiding the selection of datasets in way that optimizes the model performance on the dev set. ( ψi ) represents the importance score associated with the training dataset i. We assume that the dev sets Ddev have distribution similar to the test set Dtest that is, Ddev Dtest. Assuming the existence of development datasets, equation 1 can be expanded as follows: ψ = arg min ψ θ = arg min θ (cid:88) (cid:16) θ(ψ), Dj dev (cid:17)"
        },
        {
            "title": "1\nN",
            "content": "j=1 EiPD(i;ψ) (cid:2)J(cid:0)θ, Di train (cid:1)(cid:3) (3) This formulation naturally leads to bi-level optimization problem involving θ and ψ, which can be effectively addressed by alternating optimization steps. Specifically, the model parameters θ are updated using the standard gradient descent algorithm, while the scorer parameters ψ, are optimized using the REINFORCE algorithm (Williams, 1992). 3.2. Influence based Rewards Intuitively, the reward signal should guide the parameterized scoring network (ψ) to up-sample training datasets most likely to improve model performance on the development sets. We therefore propose an influence-based reward mechanism that quantifies the contribution of each training dataset to performance on held-out development set. Existing influence-based methods (Xia et al., 2024; Fan et al., 2024; Yu et al., 2024) either rely on first-order approximations or simulate this contribution using smaller proxy models. In contrast, we employ online proxy models to obtain accurate influence scores. Specifically, at each time step t, we update the model parameters θt using dataset Di train for gradient steps, where is the minimum number of steps required to reach meaningful local minima that demonstrates the potential benefit of up-sampling Di train. During each of these steps, we estimate the influence of every training subset Di train on the current model θt. We do this by (1) taking gradient steps on Di t+1, (2) evaluating both θt and θi t+1 on each dev batch using an influence metric M, and (3) computing the change in performance: train to produce θi Mi = M(θi t+1; dj val) M(θi t; dj val) Iθ(i; Dval) (4) where Iθ(i; θ) is the influence estimate on Dval if Di upweighed. train is To ensure robustness and stability in our estimates, we normalize across the development datasets by taking mean over all influences Mi where Mi = 1 j=1 Mi j. We iterate the process times over all traindev pairs (i, j), accumulating the reward values Mi to compute the final (cid:80)N 3 Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 2: Overview of Inf-DDS. The trainable scorer ψ and model parameters θ are optimized by generating online proxy models to compute influence scores, which serve as rewards for updating the scorer ψ. Proxy model gradients are efficiently reused for weighted Reptile update on θ. influence i, which serves as reliable measure of the impact of the ith training dataset on the models performance on the development set. PD(i S; ψ) = PD(i; ψ) jS PD(j; ψ) (cid:80) 0, , S, / S, (6) 3.3. Optimization for compute and scalability To reuse gradients across datasets and reduce computation, we perform Reptile-style first-order meta-updates (Nichol et al., 2018). For each training dataset Di train we take inner steps from the current initialization θt (step size ηt), producing θi t+1, and compute an influence score i. We convert scores to sampling distribution via softmax, pi = exp(I i/τ )/ (cid:80) exp(I j/τ ), and form the weighted Reptile update θt+1 = (cid:88) i=1 pi θi t+ θt+1 = θt + α(cid:0)θt+1 θt (cid:1) (5) PD(i; ψ) = exp(ψi) j=1 exp(ψj) (cid:80)M . We then compute the scorer gradient on as dψ = (cid:88) iS PD(i; ψ) ψ log PD(i S; ψ). (7) This estimator is unbiased for the conditional objective over (by the policy-gradient identity) but biased with respect to the full-objective gradient over all datasets. In practice, choosing < yields large reductions in per-iteration compute and memory while still improving performance (see Figure 8 and Section 4). 4. Experimental Setup with Reptile rate α = ηt. Using this procedure we need only single copy of parameter gradients and optimizer states (first/second moments), which substantially reduces memory overheads. When is large (e.g., many domains or languages) computing for every each iteration is costly. We therefore update the scorer ψ on uniform random subsample {Di i=1 with = < . Restricting the policy to yields the conditional categorical train}M In this section, we detail the benchmarks and experimental setup used to test the domain adaptation of text retrievers under different sampling strategies. BEIR: We start with controlled setup where in-domain train, dev, and test retrieval datasets are available, though their sizes vary across domains. We train on seven BEIR-15 datasets: MSMarco, NQ, FEVER, FiQA, HotpotQA, SciFact, and NFCorpus, first optimizing on the FEVER dev set and then extending to other datasets with available dev and 4 Influence Guided Sampling for Domain Adaptation of Text Retrievers {gradient cache} j=1, influence metric M, i=1, {Dj 1: Input: {Di Algorithm 1 Pseudocode Inf-DDS val}N train}M inner steps per meta-update 2: Output: converged model θ 3: Initialize PD(i; ψ, τ ) Di Dj 4: while θt not converged (every steps) do 5: 0, 0 6: 7: 8: 9: Sample val batch {dj for = 1, . . . , do j=1 Dval val}N train1/τ train1/τ (cid:80) 10: 11: 12: 13: 14: 15: train t+1; dj M(θi (x, y) Di θi t+1 Step(θt, Optt; x, y) {do this for steps} Mi val) {compute influence} 1 + = {accumulate influence updates} t+1 θt), + = val) M(θt; dj Mi i(θi (cid:80)"
        },
        {
            "title": "I i",
            "content": "end for t/S θt+1 θt + α update} {reward normalized reptile 16: Optt+1 StateUpdate(Optt, t) 17: 18: 19: end while dψ (cid:80)M ψ GradientUpdate(ψ, dψ) i=1 PD(i; ψ) ψ log PD(i; ψ) {sampler update} test sets, including Quora, FiQA, HotpotQA, and DBpedia. NFCorpus is excluded from the target datasets due to noise, as many level-1 relevant passages are in fact irrelevant. Our biencoder models are initialized with pretrained robertabase model (Liu et al., 2019), and trained for 2 epochs with InfoNCE loss (van den Oord et al., 2018) as influence metric with cross-batch negatives. The scorer network is warmed up for 50 steps and updated every 50 training steps. Multilingual Long Document Retrieval (MLDR): We next examine the realistic setting of Multilingual Long Document Retrieval, using the BGE-M3 corpus originally employed to train the bge-m3-dense model. Adaptation is performed on the MLDR-13 development set, with evaluation on its corresponding test set. The biencoder model is initialized from 568M parameter bge-m3-unsupervised checkpoint1. We treat each language as domain and sample proportionally across all datasets in language. The scorer network is warmed up for 500 steps and updated every 250 training steps. We use the m3-kd-distill loss from BGE-M3, with cross-batch negatives and 8 hard negatives, as our influence metric. Sentence-Transformers Embedding Dataset: Finally, we consider more challenging scenario where train and test datasets span diverse domains, using the publicly available 1BAAI/bge-m3-unsupervised 5 sentence-transformers embedding dataset2, originally used to train the all-MiniLM-L6-v2 model. The training corpus contains 1 billion parallel sentences drawn from 32 datasets. Its data configuration includes carefully tuned sampling weights, referred to as Expert initialization, designed to optimize performance. To align with our target domain, we excluded the Reddit comments dataset due to its large size and the CodeSearchNet dataset, as it is unrelated to code-focused tasks. Our biencoder models are initialized with the pretrained MiniLM-L6-H384-uncased model, and we optimize performance jointly across all BEIR-5 dev sets. The scorer network is warmed up for 500 steps and updated every 250 steps. Following the toy setting, we use the standard InfoNCE loss as the influence metric with crossbatch negatives. Additional hyperparameters are listed in Appendix A. Baselines: We compare our sampling algorithm against four categories of baselines: (i) static sampling methods (Temperature, Cooldown), (ii) universal generalization approach (DoReMi), (iii) gradient-based task-adaptive sampling methods (MultiDDS, DoGE), and (iv) cluster-level, task-adaptive importance-sampling method (CRISP). For all baselines, we follow the training guidelines recommended in their respective papers, with hyperparameter details provided in Appendix A. We evaluate statistical significance using paired t-test comparing the two highest-performing models in each setting. For CRISP, we construct clusters in powers of 32x: 1, 2 for BEIR-train, 1, 2, 3 for Sentence-Transformers, and 1, 2 per language for BGE-M3. 5. Results And Analysis In this section, we aim to answer the following research questions: (1) Does learning dynamically evolving sampling distribution through influence measures lead to superior adaptation on the test set? (2) Does the influence-based scorer capture additional insights beyond domain similarity between training and dev sets? (3) In diverse domain setting, how reliable are influence-based approaches compared to gradient-based methods? 5.1. Main Results Domain adaptation on BEIR: Our initial findings demonstrates that mere domain similarity alone does not consistently lead to enhanced performance in text retrieval setting. Figure 4(a) illustrates this by showing the normalized performance correlation between train/test sets without adaptive sampling. Notably, target datasets such as FEVER, HotpotQA, and FiQA benefit not only from their corresponding training domains but also from MS MARCO and τ = 1 2sentence-transformers/embedding-model-datasets Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 3: FEVER training set sampling trajectories for different initialization temperatures using MultiDDS, Inf-DDS, and learned weights from baseline methods (optimized on FEVER dev set). (a) (b) Figure 4: (a) Heatmap showing Z-score (row) normalized performance correlations between train and test splits across BEIR datasets. (b) Domain weights learned by Inf-DDS during optimization for each target domain. sampling, indicating that effective retriever improvement requires going beyond domain similarity highlighting the need for adaptive sampling strategies designed to optimize downstream performance. To investigate this, we adapt sampling distribution to the FEVER development set. As Inf-DDS relies on single-shot optimization, it is sensitive to the initial distribution, prompting us to assess multiple initializations to ensure robustness. As shown in Figure 5(a), Inf-DDS consistently outperforms all baselines when initialized with τ = 0.3, and remains competitive with CRISP using 32 clusters. Figure 3 provides comparision of sampling trajectories, highlighting that MultiDDS exhibits unstable and inconsistent dynamics across varying temperatures, Inf-DDS produces stable behavior, consistently prioritizing FEVER and MS MARCO (Figure 4(b)), in line with CRISP and DoReMi. This stability underscores the reliability of Inf-DDS in effectively aligning sampling strategies with measurable downstream performance improvements. Full results, including statistical significance tests, are presented in Table 6 in the Appendix. To further validate our approach, we conduct joint optimization over the BEIR-5 dev sets for generalization. As shown in Figure 5(b), Inf-DDS outperforms static sampling in all temperature initializations and surpasses MultiDDS in 2 out of 3 scenarios. While MultiDDS shows more improvements when initialized with τ = 1, it underperforms Inf-DDS best score by 0.93 points. Table 8 in the Appendix presents the full results, including tests for statistical significance. MLDR: Multilingual retrieval introduces distinct challenges stemming from the substantial variability in language resources and heterogeneous domain distributions. We assess how Inf-DDS and MultiDDS tackle these challenges by implicitly harmonizing data from high and low resource languages while leveraging cross-lingual relatedness without the need for explicit supervision. Specifically, we investigate two key questions: (1) Can Inf-DDS automatically upsample underrepresented languages within shared multilingual corpus to improve performance? (2) How critical is sampling high-resource languages when optimizing for multiple languages? We optimize the bge-m3-unsupervised model and scorer on the full MLDR-13 development set to maximize performance across 13 languages. As shown in Figure 6, starting Influence Guided Sampling for Domain Adaptation of Text Retrievers (a) (b) Figure 5: Sampling probability initialization vs. Average NDCG@10 on the FEVER and BEIR-5 test set while training with BEIR-7 train set. (a) Scorer optimized only on the FEVER dev set. (b) Scorer optimized jointly on BEIR-5 dev set. data. This experiment addresses two key questions: (1) Does dynamic sampling remain effective with high domain diversity? (2) Can our algorithm further improve performance when strong initial sampling distribution is available? The extensive domain coverage significantly increases the complexity of the adaptation problem. Figure 6: Avg. NDCG@10 scores on the MLDR-13 language test collection using BGE-M3 training data. Optimization of the scorer is done jointly on the 13 development sets. from the same initial sampling weights as bge-m3-dense, InfDDS improves this baseline by +5.03 points in NDCG@10, while DoReMi achieves +4.76-point gain. Inf-DDS also achieves the highest individual-language performance in 8 out of 13 languages. Full results, including statistical significance tests, are presented in Table 10 in the Appendix. Sampling trajectories for each language are presented in Figure 17 (Appendix). Interestingly, the sampling weights for English and Chinese drop substantially from their high initial values, yet performance remains comparable to bgem3-dense, likely due to their dominant presence in bgem3-unsupervised training (66.4% of total). This indicates that high-resource languages need little supervised data, demonstrating dynamic samplings ability to upweight lowresource languages while avoiding overfitting on dominant ones. Sentence-Transformers Embedding Dataset: This diverse training corpus includes over 440 Million querypositive passage pairs spanning 32 domains. We use the same BEIR-5 development sets from the toy setting for optimization, as they have minimal overlap with the training (a) (b) Figure 7: Average NDCG@10 on the BEIR-5 test collection using Sentence-Transformers training data with uniform (a) and expert initialization (b). The scorer is optimized jointly on the development sets. Starting from uniform initialization, Inf-DDS achieves performance only 0.22 points below the off-the-shelf SentenceTransformers Expert model (all-MiniLM-L6-v2), nearly optimaland yields 1.83 point gain over the uniform baseline. In contrast, gradient-based methods such as MultiDDS and DoGE fail to make any gains. When we re-run the experiment starting from Expert weights, Inf-DDS still produces an additional 0.94 point improvement, demonstrating its ability to refine even strong initial distributions. DoReMi attains larger 1.25 point gain but requires 3 the compute  (Fig. 8)  . Figure 15 shows the evolving sampling distributions under Inf-DDS, illustrating that Expert weights can be further improved by dynamically adapting dataset sampling. 7 Influence Guided Sampling for Domain Adaptation of Text Retrievers 5.2. Discussion Computational Overheads: Inf-DDS computes exact influence scores for each training dataset using online proxy models. While this introduces additional overhead, retrieval models are typically small, making the tradeoff worthwhile given the performance gains. Figure 8 compares the training time of all-MiniLM-L6-v2 across different sampling strategies. Although Inf-DDS is slightly slower than CRISP, MultiDDS, and static sampling, it consistently achieves superior performance. To reduce memory usage, Inf-DDS stores only single set of intermediate gradients and optimizer states during influence computation, which are efficiently reused in the weighted Reptile update. Figure 8: Comparison of approximate GPU Hours for training on Sent-Trans embedding data. Effect of initialization: Choosing an effective initialization for sampling is challenging but can substantially impact InfDDSs performance (Figures 5(a), 5(b) and 7). While the algorithm does not always reach globally optimal sampling weights, starting from reasonable initialization and updating the weights consistently yields gains. We do not explore heuristics for selecting initial weights, but experiments with standard static initializations show that, although no single choice is universally best, reasonably good initializations generally perform well. Effect of Reptile Updates and Update Steps: We conduct an ablation study on BEIR to evaluate the contribution of the meta-learning component, with results reported in Table 11 (Appendix). Disabling Reptile updates leads to only marginal performance changes, suggesting that the primary gains stem from dynamic sampling rather than metalearning itself. Nevertheless, Reptile remains beneficial in practice, as it reduces computational overhead by enabling reuse of intermediate computations. We further analyze the effect of the number of update steps (l) required to obtain reliable influence estimates on the FEVER and FiQA datasets under the BEIR setting. As shown in Table 12, using 35 update steps provides good trade-off, yielding strong performance while avoiding unnecessary computation. Dev/Test Overlap Analysis: Several adaptive data selection and optimization methods, including DoGE, DDS, CRISP, and our approach Inf-DDS, rely on development set to 8 compute rewards or influence estimates, which can raise concerns about potential devtest leakage. While our main experiments follow the standard train, dev, and test splits provided by the original benchmarks, we also perform an analysis to verify that this protocol does not introduce test set bias. We first train models using the original training splits of MLDR and BEIR. We then perform repeated traindev resampling by merging training and development splits and sampling five training sets matching the original training size. All models are evaluated on the original test sets. This setup assesses whether exposure to development data during optimization affects test performance. Results in Tables 13 and 14 show models trained on mixed train and dev folds achieve performance comparable to those trained solely on the original training data. This indicates the development sets do not leak information to the test sets and that observed gains are not driven by devtest overlap. Relation between Influence and Gradients: Stochastic gradient updates move model parameters toward the local minimum of the loss L, whereas influence measures their impact on the target metric M. When M, influence directly reflects the benefit of an optimization step. In contrast, gradient-based rewards quantify the alignment between the gradient toward the dev set minimum (dev) and the gradient from given training instance (train). We posit that in high-dimensional landscapes, low alignment need not indicate convergence to poor minimum. Influence-based rewards, by evaluating instances according to the actual target metric, provide more direct and reliable estimate of which steps lead to the best attainable minima. 6. Conclusion In this work, we present comprehensive analysis for adapting text retrievers to target domains using influenceguided dynamic data sampling (Inf-DDS). Our approach parametrizes the sampling distribution with scorer parameters ψ and performs bi-level optimization, jointly updating both the model parameters θ and the scorer parameters ψ, using influence scores as rewards. Across multiple benchmarks and baselines spanning diverse domains, InfDDS produces more stable sampling trajectories and consistently comes close to or outperforms both proxy-model and gradient-based approaches, while remaining computationally efficient. Although the algorithm does not always converge to the global optimum, it reliably delivers substantial improvements from reasonable initializations. We further analyze why gradient-based signals can mislead optimization, demonstrating that influence-based rewards offer more robust estimate of the best attainable minima. For future work, we aim to investigate improved initialization strategies and more sophisticated optimization techniques for the parameterized scorer distribution ψ. Influence Guided Sampling for Domain Adaptation of Text Retrievers"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bae, J., Ng, N. H., Lo, A., Ghassemi, M., and Grosse, R. B. If influence functions are the answer, then what is the question? In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=hzbguA9zMJ. BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S. Llm2vec: Large language models are secretly powerful text encoders. CoRR, abs/2404.05961, 2024. doi: 10.48550/ARXIV. 2404.05961. URL https://doi.org/10.48550/arXiv. 2404.05961. Bonifacio, L. H., Campiotti, I., Lotufo, R. A., and Nogueira, R. mmarco: multilingual version of MS MARCO passage ranking dataset. CoRR, abs/2108.13897, 2021. URL https://arxiv.org/abs/2108.13897. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Cao, Y., Kang, Y., Wang, C., and Sun, L. Instruction mining: Instruction data selection for tuning large language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= wF6k0aWjAu. Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. BGE m3-embedding: Multi-lingual, multifunctionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. doi: 10.48550/ARXIV.2402.03216. URL https://doi. org/10.48550/arXiv.2402.03216. Choi, D., Xin, D., Dadkhahi, H., Gilmer, J., Garg, A., Firat, O., Yeh, C.-K., Dai, A. M., and Ghorbani, B. Order matters in the presence of dataset imbalance for multilingual learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=7RMGI4slcb. Chung, H. W., Garcia, X., Roberts, A., Tay, Y., Firat, O., Narang, S., and Constant, N. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/forum?id=kXwdL1cWOAi. Engstrom, L., Feldmann, A., and Madry, A. Dsdm: modelaware dataset selection with datamodels. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Fan, S., Pagliardini, M., and Jaggi, M. Doge: domain reweighting with generalization estimation. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Gao, T., Yao, X., and Chen, D. SimCSE: Simple contrastive learning of sentence embeddings. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 68946910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https: //aclanthology.org/2021.emnlp-main.552/. Grangier, D., Ablin, P., and Hannun, A. Adaptive training distributions with scalable online bilevel optimization. In Transactions on Machine Learning Research (TMLR), 2025a. URL https://arxiv.org/abs/2311.11973. Grangier, D., Fan, S., Seto, S., and Ablin, P. Task-adaptive pretrained language models via clustered-importance sampling. In The Thirteenth International Conference on Learning Representations, 2025b. URL https:// openreview.net/forum?id=p6ncr0eTKE. Grosse, R. B., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukosiute, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R. Studying large language model generalization with influence functions. CoRR, abs/2308.03296, 2023. doi: 10.48550/ARXIV.2308.03296. URL https://doi.org/ 10.48550/arXiv.2308.03296. He, W., Liu, K., Liu, J., Lyu, Y., Zhao, S., Xiao, X., Liu, Y., Wang, Y., Wu, H., She, Q., Liu, X., Wu, T., and Wang, H. DuReader: Chinese machine reading comprehension dataset from real-world applications. In Choi, E., Seo, M., Chen, D., Jia, R., and Berant, J. (eds.), Proceedings of the Workshop on Machine Reading Influence Guided Sampling for Domain Adaptation of Text Retrievers for Question Answering, pp. 3746, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2605. URL https: //aclanthology.org/W18-2605/. Ilyas, A., Park, S. M., Engstrom, L., Leclerc, G., and Madry, A. Datamodels: Predicting predictions from training data. CoRR, abs/2202.00622, 2022. URL https:// arxiv.org/abs/2202.00622. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview. net/forum?id=jKN1pXi7b0. Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X. PubMedQA: dataset for biomedical research question anIn Inui, K., Jiang, J., Ng, V., and Wan, X. swering. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 25672577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259/. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Barzilay, R. and Kan, M.-Y. (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology. org/P17-1147/. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https: //aclanthology.org/2020.emnlp-main.550/. Kim, M.-Y., Rabelo, J., Goebel, R., Yoshioka, M., Kano, Y., and Satoh, K. Coliee 2022 summary: Methods for legal document retrieval and entailment. In New Frontiers in Artificial Intelligence: JSAI-IsAI 2022 Workshop, JURISIN 2022, and JSAI 2022 International Session, Kyoto, Japan, June 1217, 2022, Revised Selected Papers, pp. 5167, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3031-29167-8. doi: 10.1007/978-3-031-29168-5 4. URL https://doi.org/10.1007/978-3-031-29168-5 4. Koh, P. W. and Liang, P. Understanding black-box predictions via influence functions. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 18851894. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/ v70/koh17a.html. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl 00276. URL https://aclanthology. org/Q19-1026/. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Li, H., Shao, Y., Wu, Y., Ai, Q., Ma, Y., and Liu, Y. Lecardv2: large-scale chinese legal case retrieval dataset. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, pp. 22512260, New York, NY, USA, 2024a. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657887. URL https://doi.org/10.1145/3626772.3657887. Li, T., Xu, H., Tan, W., Murray, K., and Khashabi, D. Upsample or upweight? balanced training on heavily imbalanced datasets. CoRR, abs/2410.04579, 2024b. doi: 10.48550/ARXIV.2410.04579. URL https://doi.org/ 10.48550/arXiv.2410.04579. Liu, Q., Zheng, X., Muennighoff, N., Zeng, G., Dou, L., Pang, T., Jiang, J., and Lin, M. Regmix: Data mixture as regression for language model pre-training. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=5BjQOUXq7i. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/ 1907.11692. Moore, R. C. and Lewis, W. Intelligent selection of language In Hajiˇc, J., Carberry, S., Clark, model training data. S., and Nivre, J. (eds.), Proceedings of the ACL 2010 Conference Short Papers, pp. 220224, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL https://aclanthology.org/P10-2041/. 10 Influence Guided Sampling for Domain Adaptation of Text Retrievers Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: human generated machine reading comprehension dataset. In Besold, T. R., Bordes, A., dAvila Garcez, A. S., and Wayne, G. (eds.), Proceedings of the Workshop Integrating neural and on Cognitive Computation: symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL https://ceur-ws.org/ Vol-1773/CoCoNIPS 2016 paper9.pdf. Nichol, A., Achiam, J., and Schulman, J. On first-order meta-learning algorithms. CoRR, abs/1803.02999, 2018. URL http://arxiv.org/abs/1803.02999. Nikdan, M., Cohen-Addad, V., Alistarh, D., and Mirrokni, V. Efficient data selection at scale via influence distillation. arXiv preprint arXiv:2505.19051, 2025. Park, S. M., Georgiev, K., Ilyas, A., Leclerc, G., and Madry, A. Trak: attributing model behavior at scale. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Pruthi, G., Liu, F., Kale, S., and Sundararajan, M. Estimating training data influence by tracing gradient descent. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ e6385d39ec9394f2f3a354d9d2b88eec-Abstract. html. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https: //aclanthology.org/D16-1264/. Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 39823992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology. org/D19-1410/. Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. BEIR: heterogenous benchmark for zeroshot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs/ 2104.08663. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/ 1807.03748. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. SimLM: Pretraining with representation bottleneck for dense pasIn Rogers, A., Boyd-Graber, J., and sage retrieval. Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 22442258, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.125. URL https://aclanthology.org/2023.acl-long.125/. Wang, X., Pham, H., Michel, P., Anastasopoulos, A., Carbonell, J. G., and Neubig, G. Optimizing data usage via differentiable rewards. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 9983 9995. PMLR, 2020a. URL http://proceedings.mlr. press/v119/wang20p.html. Wang, X., Tsvetkov, Y., and Neubig, G. Balancing training for multilingual neural machine translation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 85268537, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.754. URL https://aclanthology.org/2020.acl-main.754/. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8:229256, 1992. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696. Xia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D. LESS: selecting influential data for targeted inIn Forty-first International Conferstruction tuning. ence on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=PG5fV50maR. Xie, S. M., Pham, H., Dong, X., Du, N., Liu, H., Lu, Y., Liang, P., Le, Q. V., Ma, T., and Yu, A. W. Doremi: Optimizing data mixtures speeds up language model In Thirty-seventh Conference on Neural pretraining. 11 Influence Guided Sampling for Domain Adaptation of Text Retrievers Information Processing Systems, 2023a. URL https: //openreview.net/forum?id=lXuByUeHhd. v1/2021.mrl-1.12. URL https://aclanthology.org/ 2021.mrl-1.12/. Zhang, X., Thakur, N., Ogundepo, O., Kamalloo, E., Alfonso-Hermelo, D., Li, X., Liu, Q., Rezagholizadeh, M., and Lin, J. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11: 11141131, 2023. doi: 10.1162/tacl 00595. URL https://aclanthology.org/2023.tacl-1.63/. Zhou, H., Liu, T., Ma, Q., Zhang, Y., Yuan, J., Liu, P., You, Y., and Yang, H. DavIR: Data selection via implicit reward for large language models. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 92209237, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-889176-251-0. doi: 10.18653/v1/2025.acl-long.452. URL https://aclanthology.org/2025.acl-long.452/. Zhou, Z., Shi, J., Song, P., Yang, X., Jin, Y., Guo, L., and Li, Y. Lawgpt: chinese legal knowledge-enhanced large language model. CoRR, abs/2406.04614, 2024. doi: 10.48550/ARXIV.2406.04614. URL https://doi.org/ 10.48550/arXiv.2406.04614. Xie, S. M., Santurkar, S., Ma, T., and Liang, P. Data selection for language models via importance resampling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https://openreview. net/forum?id=uPSQv0leAu. Xie, X., Dong, Q., Wang, B., Lv, F., Yao, T., Gan, W., Wu, Z., Li, X., Li, H., Liu, Y., and Ma, J. T2ranking: large-scale chinese benchmark for passage ranking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, pp. 26812690, New York, NY, USA, 2023c. Association for Computing Machinery. ISBN 9781450394086. doi: 10.1145/3539618.3591874. URL https://doi.org/10.1145/3539618.3591874. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology. org/D18-1259/. Yu, Z., Das, S., and Xiong, C. MATES: Model-aware data selection for efficient pretraining with data influence models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=6gzPSMUAz2. Zhang, C., Zhong, H., Zhang, K., Chai, C., Wang, R., Zhuang, X., Bai, T., Jiantao, Q., Cao, L., Fan, J., Yuan, Y., Wang, G., and He, C. Harnessing diversity for important data selection in pretraining large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=bMC1t7eLRc. Zhang, S., Zhang, X., Wang, H., Guo, L., and Liu, S. Multiscale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:7406174071, 2018. doi: 10.1109/ACCESS.2018.2883637. URL https://doi.org/10.1109/ACCESS.2018.2883637. Zhang, X., Ma, X., Shi, P., and Lin, J. Mr. TyDi: multi-lingual benchmark for dense retrieval. In Ataman, D., Birch, A., Conneau, A., Firat, O., Ruder, S., and Sahin, G. G. (eds.), Proceedings of the 1st Workshop on Multilingual Representation Learning, pp. 127137, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ Influence Guided Sampling for Domain Adaptation of Text Retrievers A. Hyperparameters A.1. BEIR For our experiments, we initialize our bi-encoder with roberta-base model, we set the initial learning rate to 2e-5, and train with mixed-precision (BF16) enabled. We used an in-batch negative sampling strategy with temperature of 0.02, normalizing representations before contrastive scoring. Both training and evaluation batch sizes were 256 examples on single NVIDIA-A100 80GB GPU. For scorer updates, we also use batch size of 256. We use linear learning rate decay with warmup of 250 steps, and trained for total of 7,000 steps. We use the standard InfoNCE (van den Oord et al., 2018) as our loss and our influence metric M. Queries and passages were truncated to maximum lengths of 64 and 256 tokens, respectively, with CLS-token pooling for sentence embeddings. We further update our scorer every 50 steps after 50-step scorer warmup (we dont train the scorer during the warmup period). For reptile updates, we use α based on the current learning rate ηt of the learning rate schedule. We do checkpoint selection by selecting the best checkpoint on the dev set and report the corresponding numbers on the final test set. For DoReMi, we train our reference models using τ = 1 sampling and proxy models starting with τ = initialization. For DoGE, too, we train our proxy models starting with τ = initialization. For CRISP, we limit the clusters to 32x: 1, 2 since the size of the total dataset is only 800k instances and use all-MiniLM-L6-v2 embedding model for clustering. A.2. Sentence Transformers Embedding We initialize our bi-encoder with nreimers/MiniLM-L6-H384-uncased model, we set the initial learning rate to 2e-5, and train with mixed-precision (BF16) enabled. We used an in-batch and cross-device negative sampling strategy with temperature of 0.02, normalizing representations before contrastive scoring. Training batch sizes are set to 2000, and evaluation batch sizes are 256 examples per GPU and use 8 NVIDIA-A100 80GB GPUs. We use the standard InfoNCE (van den Oord et al., 2018) as our loss and our influence metric M. We also use 1-hard negative when using MSMarco in the training set. For scorer updates, we use batch size of 256 per dataset. We use linear learning rate decay with warmup of 1000 steps, and trained for total of 150k steps. Queries and passages were truncated to maximum lengths of 64 and 256 tokens, respectively, with CLS-token pooling for sentence embeddings. We further update our scorer every 250 steps after 500-step scorer warmup (we dont train the scorer during the warmup period). For reptile updates, we use α based on the current learning rate ηt of the learning rate schedule. For DoReMi, we train our reference models using τ = sampling when comparing for uniform initialization τ = expert for expert initialization and proxy models starting with τ = initialization. For DoGE, too, we train our proxy models starting with τ = initialization sampling when comparing for uniform initialization τ = expert for expert initialization. For CRISP, we limit the clusters to 32x: 1, 2, 3 since the size of the total dataset is only 440M instances, and use all-MiniLM-L6-v2 embedding model for clustering. A.3. Multilingual Long Document Retrieval We initialize our bi-encoder with BAAI/bge-m3-unsupervised model, we set the initial learning rate to 2e-5, and train with mixed-precision (BF16) enabled. We used an in-batch negative, cross-device negatives, and 8 hard negatives during training with temperature of 0.02, normalizing representations before contrastive scoring. We use the bge-m3-kd-distil loss by BGE-M3 as our training loss and InfoNCE with hard negatives as our influence metric M. Training batch sizes are set to 5, and evaluation batch sizes are 5 examples per GPU and use 8 NVIDIA-A100 80GB GPUs. For scorer updates, we use batch size of 4 per dataset. We use linear learning rate decay with warmup of 1000 steps, and trained for total of 10k steps. Queries and passages were truncated to maximum lengths of 512 and 8192 tokens, respectively, with CLS-token pooling for sentence embeddings. We further update our scorer every 250 steps after 500-step scorer warmup (we dont train the scorer during the warmup period). We employ subsampling as detailed in Section 3.3 with = 8. We experiment with both enabling and disabling Reptile updates, and observe better performance when Reptile updates are turned off in the MLDR-13 setting. We use reptile update α based on the current learning rate ηt of the learning rate schedule. For DoReMi, we train our reference models using τ = sampling and proxy models starting with τ = initialization. For DoGE, too, we train our proxy models starting with τ = initialization. For CRISP, we limit the clusters to 32x: 1, 2 per language since the size of the total dataset is only 1.5M instances and use paraphrase-multilingual-MiniLM-L12-v2 embedding model for clustering. 13 Influence Guided Sampling for Domain Adaptation of Text Retrievers B. Datasets B.1. BEIR Table 1: BEIR Train Datasets Table 2: BEIR Dev Datasets"
        },
        {
            "title": "Dataset",
            "content": "#Qrels MSMARCO 499,184 2,590 NFCorpus 100,231 NQ 85,000 HotpotQA 5,500 FiQA 109,810 Fever 809 SciFact Total 803,"
        },
        {
            "title": "Dataset",
            "content": "#Qrels HotpotQA 10,894 1,238 FiQA 7,626 Quora 5,673 DBpedia 8,079 Fever Total 40,947 B.2. Sentence Transformers Embedding Dataset The all-MiniLM-L6-v2 model was fine-tuned with self-supervised contrastive objective over concatenation of diverse, publicly available sentence-pair corpora. These include user-generated content such as paired Reddit comments and Q&A threads from Stack Exchange and Yahoo Answers; scientific citation pairs drawn from the S2ORC and SPECTER datasets; question answering benchmarks like PAQ, MSMARCO, Natural-Questions, SearchQA, SQuAD 2.0, and TriviaQA; paraphrase and duplicate-question collections from WikiAnswers, Quora Question Triplets, and the AllNLI (SNLI+MultiNLI) corpus; multimodal captions from COCO and Flickr30k; code-search examples; and specialized text-compression and instructional corpora such as Simple Wikipedia, Wikihow, Altlex, and explicit sentence-compression datasets. Full dataset statistics are provided in Table 3. B.3. BGE-m3 Multilingual For English, bge-m3 fine-tuning dataset includes 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2022), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SimCSE (Gao et al., 2021). For Chinese, it includes 7 datasets, DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), T2-Ranking (Xie et al., 2023c), LawGPT (Zhou et al., 2024), CMedQAv2 (Zhang et al., 2018), NLIzh2, and LeCaRDv2 (Li et al., 2024a). It also includes training data for other languages from Mr. Tydi (Zhang et al., 2021), MIRACL (Zhang et al., 2023) and train sets of MLDR. Full dataset statistics are provided in Table 5. C. Additional results C.1. BEIR To further validate our approach, we conduct individual optimizations on each of the BEIR-5 development sets and report the results in Table 7 Figure 9. As shown, Inf-DDS consistently matches or outperforms static sampling across all datasets. While it does not achieve the highest performance in every domain compared to MultiDDS, Inf-DDS surpasses proportional sampling by an average margin of 2.24 points and even outperforms MultiDDS on average. The corresponding sampling trajectories are provided in the Appendix D. C.2. Sentence Transformers Embedding Dataset Here in Table 7 we report Average NDCG@10 numbers on BEIR-5 test collection when training all-MiniLM-L6-v2 using Sentence-Transformers data, when optimization of scorer is done jointly on the BEIR-5 development sets. 14 Influence Guided Sampling for Domain Adaptation of Text Retrievers Dataset S2ORC Citation pairs (Abstracts) WikiAnswers Duplicate question pairs Amazon QA Pairs PAQ (Question, Answer) pairs S2ORC Citation pairs (Titles) S2ORC (Title, Abstract) Stack Exchange (Title, Body) pairs Stack Exchange (Title+Body, Answer) pairs Stack Exchange (Title, Answer) pairs Stack Exchange Math MS MARCO triplets GOOAQ: Open QA with Diverse Answer Types Yahoo Answers (Title, Answer) COCO Image captions SPECTER citation triplets Yahoo Answers (Question, Answer) Yahoo Answers (Title, Question) SearchQA Eli5 Flickr 30k Stack Exchange Duplicate questions (titles) AllNLI (SNLI and MultiNLI) Stack Exchange Duplicate questions (bodies) Stack Exchange Duplicate questions (titles+bodies) Sentence Compression Wikihow Altlex Quora Question Triplets Simple Wikipedia Natural Questions (NQ) SQuAD2.0 TriviaQA Total Count 116,288,806 77,427,422 2,448,839 64,371,441 52,603,982 41,769,185 25,316,456 21,396,559 21,396,559 2,218,989 9,144,553 3,012,496 1,198,260 828,395 684,100 681,164 659,896 582,261 325,475 317,695 304,525 277,230 250,519 250,460 180,000 128,542 112,696 103,663 102,225 100,231 87,599 73,346 440,096,511 Proportional Sampling % Weight Expert Sampling % 26.42% 17.60% 0.56% 14.63% 11.96% 9.49% 5.75% 4.86% 4.86% 0.50% 2.08% 0.68% 0.27% 0.19% 0.16% 0.15% 0.15% 0.13% 0.07% 0.07% 0.07% 0.06% 0.06% 0.06% 0.04% 0.03% 0.03% 0.02% 0.02% 0.02% 0.02% 0.02% 100.00% 123 123 247 123 123 123 565 17 373 166 247 247 247 1 84 169 163 144 81 1 26 69 21 21 45 32 28 26 26 25 22 19 4009 3.07% 3.07% 6.16% 3.07% 3.07% 3.07% 14.09% 0.42% 9.31% 4.14% 6.16% 6.16% 6.16% 0.02% 2.10% 4.21% 4.07% 3.59% 2.02% 0.02% 0.65% 1.72% 0.52% 0.52% 1.12% 0.80% 0.70% 0.65% 0.65% 0.62% 0.55% 0.47% 100.00% Table 3: Training data provided by sentence transformers all-MiniLM-L6-v2 showing dataset sizes, hand-picked weights, and normalized sampling percentages. C.3. MLDR Here in Table 10 we report Average NDCG@10 numbers on MLDR-13 test set when training with BGE-M3 data, when optimization of scorer is done jointly on the MLDR-13 development sets. C.4. Ablation Effect of reptile updates: We perform an ablation study to determine whether the observed performance gains arise solely from dynamically updating the sampling distribution or whether the meta-learning component of our updates also contributes. Table 11 compares downstream performance with Reptile updates enabled versus disabled. We observe only minor differences in performance, which suggests that most of the improvement can be attributed to dynamic sampling. However, because the Reptile meta-update reuses existing computations, it remains valuable for reducing overall computational overhead. D. Sampling trajectories 15 Influence Guided Sampling for Domain Adaptation of Text Retrievers Language Swahili Farsi Finnish Indonesian French German Korean Spanish Italian Portuguese Japanese Bengali Telugu Thai Russian Hindi Arabic Chinese English Sampling (%) 0.588 0.588 0.294 0.294 1.176 1.176 1.176 1.176 1.176 1.176 1.176 0.294 0.294 1.176 2.353 1.176 2.353 23.529 58.824 Table 4: Language wise initialization probabilities for bge-m3-dense training. Figure 9: Average NDCG@10 on the BEIR-5 test collection using BEIR-7 training data with τ = 1. The scorer is optimized individually on the development sets. 16 Influence Guided Sampling for Domain Adaptation of Text Retrievers Dataset MSMarco MIRACL/fr MIRACL/zh MIRACL/es MIRACL/ja MIRACL/te MIRACL/en MIRACL/id MIRACL/fa MIRACL/ko MIRACL/fi MIRACL/th MIRACL/bn MIRACL/ru MIRACL/hi MIRACL/ar MIRACL/sw HotpotQA mMARCO-zh/chinese NQ zh NLI/LCQMC zh NLI/BQ zh NLI/STS-B zh NLI/afqmc zh NLI/ATEC zh NLI/QBQTC v2 zh NLI/PAWSX DuReader cMedQAv2 TriviaQA Lines 485,905 1,143 1,312 2,162 3,477 3,452 2,863 4,071 2,107 868 2,897 2,972 1,631 4,683 1,169 3,495 1,901 84,516 100,000 58,568 10,000 12,599 249 10,534 11,325 10,000 10,000 80,416 50,000 60,315 Sampling % 30.95% 0.07% 0.08% 0.14% 0.22% 0.22% 0.18% 0.26% 0.13% 0.06% 0.18% 0.19% 0.10% 0.30% 0.07% 0.22% 0.12% 5.38% 6.37% 3.73% 0.64% 0.80% 0.02% 0.67% 0.72% 0.64% 0.64% 5.12% 3.18% 3.84% Dataset Mr.TyDi/finnish Mr.TyDi/bengali Mr.TyDi/russian Mr.TyDi/swahili Mr.TyDi/indonesian Mr.TyDi/arabic Mr.TyDi/english Mr.TyDi/korean Mr.TyDi/japanese Mr.TyDi/telugu Mr.TyDi/thai T2Ranking en NLI/nli for simcse Law-Medical/colliee Law-Medical/law gpt Law-Medical/lecardv2 Law-Medical/pubmed qa MLDR/hi MLDR/es MLDR/ru MLDR/de MLDR/ja MLDR/fr MLDR/ar MLDR/ko MLDR/en MLDR/zh MLDR/pt MLDR/it MLDR/th SQuAD Total Lines 6,561 1,713 5,366 2,072 4,902 12,377 3,547 1,295 3,697 3,880 3,319 90,467 274,951 463 500 591 500 1,618 2,254 1,864 1,847 2,262 1,608 1,817 2,198 10,000 10,000 1,845 2,151 1,970 87,599 1,569,864 Sampling % 0.42% 0.11% 0.34% 0.13% 0.31% 0.79% 0.23% 0.08% 0.24% 0.25% 0.21% 5.76% 17.51% 0.03% 0.03% 0.04% 0.03% 0.10% 0.14% 0.12% 0.12% 0.14% 0.10% 0.12% 0.14% 0.64% 0.64% 0.12% 0.14% 0.13% 5.58% 100.00% Table 5: Training data provided by bge-m3 showing dataset sizes and normalized sampling percentages. Init. τ = 0.3 τ = 1 τ = 5 τ = Cooldown DoReMi DoGE CRISP-32 CRISP-1024 Static Sampling 57.3 48.8 46.5 50.1 - - - - - MultiDDS Inf-DDS Others 54.5 53.9 53.2 55.8 - - - - - 64.9 54.7 55.2 57.5 - - - - - - - - - 50.5 48.6 54.7 59.0 62.9 Table 6: NDCG@10 comparison of sampling strategies with varying initialization temperatures during optimization on the FEVER dev set. and mark the two models compared in the paired significance test, and indicates statistically significant difference (p < 0.05). Additional results from other baselines are shown in the last column. 17 Influence Guided Sampling for Domain Adaptation of Text Retrievers Init. Sampling Dev Dataset - Train Dataset BEIR-7 BEIR-7 BEIR-1 BEIR-7 BEIR-1 BEIR-7 BEIR-7 BEIR-1 Cluster IS BEIR-7 BEIR-1 CRISP-1024 Cluster IS BEIR-7 BEIR-1 Static MultiDDS Inf-DDS DoReMi DoGE CRISP-32 τ = 1 τ = 1 τ = 1 Proxy Proxy - Static MultiDDS Inf-DDS τ = BEIR-7 τ = BEIR-7 BEIR-1 τ = BEIR-7 BEIR-1 - Average Test 48.7 50.4 51.0 49.0 50.0 50.8 51.1 48.9 49.8 50.3 DBpedia FEVER FiQA HotpotQA Quora 29.5 27.4 29.4 27.5 25.1 27.0 24.8 27.2 25.6 26.7 48.8 53.9 54.7 48.6 54.7 59.0 62.9 50.1 55.8 57.5 29.0 33.3 33.7 32.0 32.4 31.3 31.8 31.7 31.1 32.0 52.4 53.8 52.9 53.7 54.8 52.9 52.6 52.8 53.1 52.6 84.0 83.7 84.1 83.1 82.8 83.6 83.3 82.8 83.1 82.7 Table 7: Average NDCG@10 on BEIR-5 test collection, optimization of scorer is done individually on the development sets. Dev Dataset - Sampling Init Train Dataset - τ = BEIR-5 BEIR-5 BEIR-7 τ = 0.3 Static MultiDDS Inf-DDS Static MultiDDS Inf-DDS Static MultiDDS Inf-DDS Static MultiDDS Inf-DDS BEIR-7 DoReMi DoGE BEIR-7 BEIR-5 Cluster IS BEIR-7 BEIR-5 CRISP-32 CRISP-1024 Cluster IS BEIR-7 BEIR-5 Proxy Proxy BEIR-5 BEIR-5 τ = τ = 5 - - - Avg. Test 49.2 48.2 50.3 48.7 49.3 48.9 48.9 47.4 49.7 48.9 47.3 50.2 49.0 50.4 50.0 49.4 Dbpedia Fever Fiqa Hotpotqa Quora 28.6 25.2 27.7 29.5 25.9 29.5 27.6 24.4 28.8 27.2 22.7 26.8 27.5 26.7 26.2 23. 58.2 58.6 69.2 48.8 59.0 50.0 48.4 51.0 55.0 50.1 52.4 59.6 48.6 61.3 57.2 56.1 26.5 23.3 23.6 29.0 29.0 29.4 32.4 30.1 31.2 31.7 30.4 32.1 32.0 30.4 31.0 31.2 48.7 54.0 48.6 52.4 50.5 51.6 53.2 49.7 51.4 52.8 50.1 50.4 53.7 51.4 52.3 52.5 84.0 80.1 82.3 84.0 82.0 84.1 83.0 81.5 82.2 82.8 81.1 82.2 83.1 82.2 83.5 83.3 Table 8: Average NDCG@10 on the BEIR-5 test collections. Scorer optimization is performed jointly on the development sets. and denote the two models compared in the paired significance test, and indicates statistically significant difference (p < 0.05). Additional baselines are listed below. Sampling Init. Train Dataset Dev Dataset Static MultiDDS InfDDS Cooldown DoReMi DoGE CRISP (n=32) CRISP (n=322) CRISP (n=323) Static (all-MiniLM-L6-v2) MultiDDS InfDDS DoReMi DoGE τ = τ = τ = 5 1 Proxy Proxy Cluster IS Cluster IS Cluster IS Expert Expert Expert Expert Expert Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans Sent-Trans - BEIR-5 BEIR-5 - - BEIR-5 BEIR-5 BEIR-5 BEIR-5 - BEIR-5 BEIR-5 - BEIRAverage Test 49.0 40.2 50.8 47.6 51.4 49.5 47.4 45.6 48.5 51.0 42.5 52.0 52.3 50.5 DBpedia FEVER FiQA HotpotQA Quora 31.7 22.3 32.0 28.6 31.7 29.9 28.3 26.8 28.4 32.3 22.8 32.4 32.5 30.0 49.0 43.9 50.6 49.9 52.3 57.4 58.1 54.8 61.9 51.9 37.2 58.3 56.3 59.6 31.8 24.8 34.4 33.7 36.2 31.2 27.9 26.4 28.5 36.9 29.2 36.1 36.2 33. 44.6 26.5 46.6 38.1 49.1 42.6 38.0 35.5 39.5 46.5 28.3 45.7 48.7 43.1 49.0 83.9 88.0 87.6 87.6 86.3 84.9 84.7 84.2 87.6 86.3 87.4 87.7 86.4 Table 9: Average NDCG@10 on BEIR-5 test collection when training all-MiniLM-L6-v2 using Sentence-Transformers data, optimization of scorer is done jointly on BEIR-5 development sets. and mark the two models compared in the paired significance test, and indicates statistically significant difference (p < 0.05). 18 Influence Guided Sampling for Domain Adaptation of Text Retrievers Dev Dataset - Model/ Sampling bge-m3 unsup. bge-m3-dense bge-m3-dense MultiDDS Inf-DDS Cooldown DoReMi DoGE (Iter 2) - - MLDR-13 MLDR-13 - - MLDR-13 CRISP (n=32/lang) MLDR-13 CRISP (n=322/lang) MLDR-13 Avg. Test 37.0 52.5 52.5 56.7 57.5 52.7 57.2 49.6 57.1 56. ar de en es fr hi it ja ko pt ru th zh 30.8 38.4 34.2 61.4 53. 23.9 43.7 33.3 24.9 59.7 45. 18.6 13.7 47.6 48.4 56.0 58.3 51.5 54.8 44.9 56.7 53.1 46.1 46.7 53.9 54.4 50.2 54.4 47.0 53.3 53.9 48.9 46.7 50.0 48.5 48.5 50.7 47.1 52.6 52.0 74.8 76.3 79.1 80.6 75.6 79.1 72.7 79.5 79. 73.8 74.3 76.3 77.9 72.6 77.1 70.5 75.6 75.5 40.7 40.0 44.1 43.9 33.1 44.5 34.6 44.0 44.9 62.7 61.7 66.0 66.1 61.7 66.6 58.5 66.6 65.5 50.9 49.1 56.6 56.7 48.4 54.1 44.4 55.4 54.5 42.9 41.0 49.3 49.9 41.3 48.0 37.0 48.8 48.6 74.4 74.3 78.6 79.5 75.5 82.3 73.3 79.3 78. 59.5 60.8 62.3 65.7 61.4 64.5 56.7 64.7 62.8 33.6 36.0 37.3 39.0 37.8 39.9 33.2 36.7 38.5 26.0 26.7 27.3 26.6 27.7 27.8 25.0 28.9 28.5 Table 10: Average NDCG@10 scores on the MLDR-13 test set across 13 languages. denotes reproduced results, and mark the two models compared in the paired significance test, and indicates statistically significant difference (p < 0.05). Reptile Update On Off Train Dataset Dev Dataset BEIR-7 BEIR-1 Fever Test 54.7 56.5 Fiqa Test 33.7 28.8 HotpotQA Test 52.9 52.7 Table 11: Average NDCG@10 on FEVER, FiQA, and HotpotQA test collection, optimization of scorer is done individually on the development sets without reptile updates. Update steps FEVER FiQA 0 1 3 5 10 20 57.3 59.4 64.9 62.4 62.9 60.6 29.0 30.9 33.7 33.4 31.2 31. Table 12: Effect of the number of update steps on NDCG@10 when optimizing separately for FEVER and FiQA. Language mix1 mix2 mix3 mix4 mix5 Train Mix Avg. ar de en es fr hi it ja ko pt ru th zh 35.7 34.2 29.5 58.7 56.7 26.7 49.5 39.4 34.7 61.3 49.8 26.4 17.6 35.7 34.0 30.2 59.8 57.0 25.6 49.6 38.8 32.6 59.4 49.6 25.0 16.8 35.9 33.7 29.4 60.7 58.4 25.0 48.6 38.4 32.7 61.4 50.6 24.6 17. 35.2 34.4 30.7 59.4 58.2 23.7 49.5 39.8 32.9 60.8 49.8 25.2 17.4 34.7 34.1 29.3 59.5 56.6 23.8 48.2 39.0 31.7 60.0 49.3 24.3 17.8 36.2 34.5 29.6 60.3 57.2 25.1 49.1 38.5 32.6 61.7 51.4 25.7 17.5 35.44 0.5 34.08 0.3 29.82 0.6 59.62 0.7 57.38 0.9 24.96 1.3 49.08 0.6 39.08 0.5 32.92 1.1 60.58 0.9 49.82 0.5 25.10 0.8 17.42 0.4 Avg. 40. 39.5 39.8 39.8 39.1 40.0 39.6 0. Table 13: Testing Dev/Test leakage on bge-m3-retromae via repeated sampling over the mixture of train and dev splits of MLDR, with evaluation on the MLDR-13 test set. We report NDCG@10 across five mixed-data runs, along with the mix mean and standard deviation. 19 Influence Guided Sampling for Domain Adaptation of Text Retrievers Dataset mix1 mix2 mix3 mix4 mix5 Train Mix Avg. 78.1 FEVER FiQA 30.7 HotpotQA 30. 77.7 30.6 30.1 78.5 30.8 30.3 77.5 30.4 30.4 78.3 30.8 30.0 76.6 31.0 30.0 78.02 0.4 30.66 0.2 30.24 0. Avg. 46.4 46.1 46.5 46.1 46. 45.9 46.3 0.2 Table 14: Testing Dev/Test leakage on roberta-base via repeated sampling over the mixture of BEIR train and development splits, with evaluation on the BEIR test collections. We report NDCG@10 across five mixed-data runs, along with the mix mean and standard deviation. Figure 10: Sampling probability trajectories of MultiDDS and Inf-DDS with varying initialization temperatures during optimization on the FEVER development set. The orange curve denotes the FEVER training set sampling trajectory. Figure 11: Sampling probability trajectories of MultiDDS and Inf-DDS with varying initialization temperatures during joint optimization on the BEIR-5 development set (DBpedia, FEVER, FiQA, HotpotQA, Quora). The brown curve represents the sampling trajectory for the NFCorpus training set, which is aggressively upsampled by MultiDDS, resulting in degraded overall performance, as shown in Table 8. Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 12: Sampling probability trajectories of MultiDDS and Inf-DDS with varying initialization temperatures during optimization on the FiQA development set. The green curve denotes the FiQA training set sampling trajectory. Compared to MultiDDS, which upsamples FiQA due to gradient similarity, Inf-DDS upsamples datasets like MSMarco and FEVER that are more relevant for performance gains as seen in Table 7. Figure 13: Sampling probability trajectories of MultiDDS and Inf-DDS with varying initialization temperatures during joint optimization on the HotpotQA development set. The red curve represents the sampling trajectory for the HotpotQA training set, which is being upsampled more by MultiDDS, leading to better performance as seen in Table 7. Figure 14: Sampling probability trajectories of MultiDDS and Inf-DDS with varying initialization temperatures during joint optimization on the Dbpedia development set. Since DBpedia is not present in the training set, we see FEVER being upsampled more by Inf-DDS, which should be true since DBpedia and FEVER are very closely related datasets (Thakur et al., 2021). 21 Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 15: Sampling probability trajectories of Inf-DDS with Expert initialization during training of all-MiniLM-L6-v2 on Sentence Transformers data. Optimization is done jointly on the BEIR-5 development set (DBpedia, FEVER, FiQA, HotpotQA, and Quora). 22 Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 16: Sampling probability trajectories of Inf-DDS with τ = 1 initialization during training of all-MiniLM-L6-v2 on Sentence Transformers data. Optimization is done jointly on the BEIR-5 development set (DBpedia, FEVER, FiQA, HotpotQA, and Quora). 23 Influence Guided Sampling for Domain Adaptation of Text Retrievers Figure 17: Sampling probability trajectories of Inf-DDS during training of bge-m3-dense while jointly optimizing for MLDR-13 dev sets."
        }
    ],
    "affiliations": [
        "IBM Research AI"
    ]
}