{
    "paper_title": "Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences",
    "authors": [
        "Vaishnavi Shrivastava",
        "Ananya Kumar",
        "Percy Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence (\"Score your confidence from 0-1.\") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence (\"Which question are you more confident in answering correctly?\"). Treating each question as a \"player\" in a series of matchups against other questions and the model's preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model's confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 2 1 1 0 . 2 0 5 2 : r Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences"
        },
        {
            "title": "Percy Liang\nStanford University",
            "content": "vshrivas@cs.stanford.edu ananya@cs.stanford.edu pliang@cs.stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking language model to assess its confidence (Score your confidence from 0-1.) is natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence (Which question are you more confident in answering correctly?). Treating each question as player in series of matchups against other questions and the models preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the models confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMsGPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405Bacross 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets."
        },
        {
            "title": "1 Introduction",
            "content": "To ensure users can make informed decisions when interpreting outputs from language models (LMs), it is crucial to develop methods for accurately gauging their confidence. Language models are widely deployed, yet they remain prone to errors in their outputs. For instance, even state-of-the-art models like GPT-4o and Llama 3.1 405B struggle to solve challenging datasets such as GPQA (Rein et al., 2023) and MATH (Hendrycks et al., 2021b). To help users detect mistakes in their generations, models should provide reliable confidence estimates, signaling when their responses are more likely to be incorrect. By leveraging these estimates, users can disregard low-confidence answers or seek expert opinions. Since users primarily engage with chatbots like ChatGPT (OpenAI, 2022) through language, asking language models to gauge their confidence is natural tool. straightforward approach to this is absolute confidence estimationasking the model to directly rate its confidence without further context or grounding, e.g., How confident are you on scale of 0-1? However, Shrivastava et al. (2023) find that absolute confidences can be too coarse-grained and lack discriminative power. For example, GPT-4 produces the same confidence score of 0.9 for 50% of examples across 12 datasets, limiting its ability to distinguish between correct and 1 Figure 1: Relative Confidence Estimation. We first prompt models to elicit their answers to different questions. For each question qi, we match qi with other questions qj and generate confidence preference data. We ask the model to compare its level of confidence in the pair of questions and decide which question it is more confident in answering correctly. We treat the questions and answers as players in these matchups and the confidence preferences as match outcomes. Leveraging rank aggregation techniques used in competitive games, such as Elo rating, we translate the models confidence preferences into confidence scores. incorrect answers. This may be due to lack of realistic examples of confidence estimation in training data. For example, Zhou et al. (2023) find that many examples in the Pile dataset use hyperbolic terms like am 100% confident, rather than providing more nuanced estimates. We introduce relative confidence estimation, as an alternative to absolute confidence estimation. Rather than asking models to rate their confidence on an answer to single question, we ask them to compare confidence across different questions: Which question are you more confident in answering correctly?. Relative comparisons are used in many scenarios as an easier alternative to absolute judgments. For instance, in RLHF, annotators assess which generation is better, rather than assigning direct scores (Ouyang et al., 2022). Kadavath et al. (2022) also show that LMs are better at making relative judgments of correctness by comparing multiple sampled outputs, rather than verifying single generation. To the best of our knowledge, ours is the first study to explore confidence estimation through relative comparisons. Figure 1 illustrates our method. To estimate confidence for language models answers to questions q1, q2, ..., qn, we generate confidence preference data by pairing each question qi with another question qj and asking the model, Which question are you more confident in answering correctly, qi or qj? We repeat this times for each question to gather pairwise confidence preferences. We then convert these preferences into confidence scores, treating this as rank aggregation problemdetermining scores or rankings from set of partial and potentially inconsistent comparisons. Leveraging well-established solutions to rank aggregation like Elo rating (Elo, 1978), Bradley-Terry (Bradley & Terry, 1952), and TrueSkill (Herbrich et al., 2006), we translate these relative judgments of confidence into confidence scores. We compare relative confidence estimation to state-of-the-art absolute confidence estimation methods. For absolute confidence estimates, we study direct promptingeliciting model confidence through single promptand self-consistency promptingrepeatedly prompting the model for its confidence and aggregating the results into single score through post-processing (Xiong et al., 2023). Our goal is to produce reliable confidence estimates that can allow users to detect potentially incorrect answers from the model, so we study the selective classification AUC which measures how accurate the model is if it is allowed to abstain on some (low-confidence) examples. Additionally, we also report the AUROC  (Table 7)  to understand how well confidence scores can distinguish between correct and incorrect examples. We evaluate relative confidence estimation on five state-of-the-art modelsLlama 3.1 405B, GPT-4, Gemini 1.5 Pro, GPT-4o, and Claude 3.5 Sonneton 14 challenging multiple-choice question answering tasks (GPQA, MedQA, TruthfulQA, OpenbookQA, SIQA, and 8 diverse MMLU datasets). Our approach matches or outperforms both direct confidence estimation and self-consistency methods for 4 out of 5 of these models (for Claude 3.5 Sonnet relative confidences slightly underperform self-consistency methods). For GPT-4o, we see 3.2% and 1.8% improvements respectively in AUC. For Llama 3.1 405B, we observe 6.1% improvement in the selective classification AUC over direct prompting and 4.9% gain over 2 self-consistency. Similar improvements are observed with the other models (Section 5). Our findings highlight the efficacy of relative confidences and introduce new way of thinking about confidence estimation."
        },
        {
            "title": "2 Setup",
            "content": "Task. We follow the experimental setup described in (Shrivastava et al., 2023). For given input x, let ˆy(x) represent the models output and y(x) represent the gold label. R(ˆy, y) is the ground truth correctness of ˆy(x). Since we work with multiple choice tasks, R(ˆy, y) = 1{ˆy(x) = y(x)}. C(x, ˆy) [0, 1] is the models confidence in ˆy(x) being the correct output for x. Our goal is to derive reliable confidence estimates from language modelsi.e. higher C(x, ˆy) where R(ˆy, y) is 1 and lower C(x, ˆy) where R(ˆy, y) is 0. Reliable confidence estimates can help prioritize high-confidence outputs and defer low-confidence cases to human experts. Metrics. We measure the reliability of confidence estimates through selective classification and focus on studying the AUC (El-Yaniv & Wiener, 2010; Liang et al., 2022), area under the selective accuracycoverage curve. The AUC measures the accuracy of model if it is allowed to abstain on low-confidence inputs. The selective accuracy A(c) is the accuracy of the model on the top fraction of examples it is most confident about. AUC is computed by aggregating the selective accuracy A(c) across all c. We compute the AUC as described by (Shrivastava et al., 2023), adding small amount of Gaussian noise to each confidence score to allow for tie-breaking across different examples with the same confidence score. For model with reliable confidence estimates, accuracy on dataset should increase by abstaining on larger fraction of low-confidence examples. Figure 2: Direct Confidence Prompt Instruction. Asks the model to directly score its confidence in its answer to question. Additionally, we also report the AUROC (Hendrycks & Gimpel, 2017; Xiong et al., 2023), area under the receiver operating characteristic curve, in Appendix A.2. AUROC is standard classification metric used to measure how well model can separate correct and incorrect examples at different thresholds. In our setting, we use the outputted confidence scores as the thresholds for measuring AUROC. Figure 3: Relative Confidence Prompt. Asks model to compare its confidence in two questions. Expected calibration error (ECE) (Guo et al., 2017a; Naeini et al., 2015) is also standard metric to measure how closely models confidence matches its accuracy. However, ECE does not assess models ability to discriminate between correct and incorrect answersa model with accuracy 0.5 can achieve perfect ECE by outputting confidence of 0.5 for all of its answers. Therefore, we focus our results on the AUC. Datasets. We measure the quality of confidence estimates produced by the model on 14 challenging multiple-choice question answering datasets: GPQA (Rein et al., 2023), MedQA (Jin et al., 2021), TruthfulQA (TQA) (Lin et al., 2021), CommonsenseQA (CSQA) (Talmor et al., 2019), OpenbookQA (OBQA) (Mihaylov et al., 2018), SIQA (Sap et al., 2019), and eight diverse MMLU (Hendrycks et al., 2021a) datasetsprofessional law (Law), business ethics (Ethics), conceptual physics (Physics), econometrics (Econ), abstract algebra (Algebra), college chemistry (Chem), computer security (Security), and US Foreign Policy (Policy). We evaluate on 250 examples from the test set of each dataset. We tune the hyperparameters of our approach on small heldout set for each task, when available, or otherwise use fixed set of hyperparameters. See Appendix A.3 for more details. Models. We evaluate our approach on five state-of-the-art modelsLlama 3.1 405B (Dubey et al., 2024), GPT-4 (OpenAI, 2023), Gemini 1.5 Pro (Team et al., 2024), GPT-4o (OpenAI, 2024), and Claude 3.5 Sonnet (Anthropic, 2024)."
        },
        {
            "title": "3 Absolute Confidence Estimation",
            "content": "Confidence estimation is often done in an absolute setting, where model assesses its confidence C(x, ˆy) independently for each example x. Using models log probabilities as measure of its confidence is common absolute confidence estimation technique. We focus on linguistic confidence estimation, where user interacts with model in natural language to assess its confidence, without assuming access to models internal representations or outputted log probabilities. Linguistic confidence estimation is becoming increasingly important as users interact with language models through chat interfaces, and several state-of-the-art models such as Claude 3.5 Sonnet and Gemini 1.5 Pro provide only API-level access to users. We compare relative confidence estimation to two popular absolute linguistic confidence estimation methods: Direct Confidence Prompting. We zero-shot prompt the language model with an instruction to answer the question and provide confidence estimate for that answer. Both the answer and confidence are outputted in single generation, greedily with = 0. Shrivastava et al. (2023) study several different instructions for direct confidence prompting including asking the model to rate its confidence on different numerical scales, to reason about its confidence level with chain of thought, and describe its confidence in words (e.g. not sure, sure, and very sure). We use the direct confidence prompt from Shrivastava et al. (2023) resulting in the highest selective classification AUC across multiple language models. This prompt asks the model to rate its confidence on scale of 0-1 and provides fake few-shot examples to allow the model to better understand the task. See Figure 2 for the prompt instruction and Appendix A.4 for the full prompt. Self-Consistency Confidence Prompting. Xiong et al. (2023) present an extension to direct confidence prompting where motivated by work on self-consistency prompting (Wang et al., 2022), multiple answers and confidences are sampled for given question to get more robust confidence estimate. These answers and confidences are aggregated via post-processing procedure to produce single answer and confidence score from the samples. See Xiong et al. (2023) for more details on the aggregation procedure. We follow the same procedure as Xiong et al. (2023)prompting the model multiple times per question to sample different answers and confidences using the prompt in Figure 2 (full prompt in Appendix A.4), then aggregating these samples through their post processing technique. We sample at = 0.7 and report results for 15 samples."
        },
        {
            "title": "4 Relative Confidence Estimation",
            "content": "Linguistic confidence estimation, where model is prompted to assess its own confidence, is typically done through absolute estimation methods, in which the model independently gauges its confidence for each question. However, without clear training examples demonstrating how to estimate confidence, the model may struggle to distinguish between different confidence levels (e.g., 85% vs. 90%) and generate appropriate scores. In contrast, it may be easier for the model to compare its confidence across different questions, making simpler, binary judgment about whether it is more or less confident in answering one question versus another. This approach provides more grounding, as confidence is evaluated relative to another question, rather than globally assessed via direct score. By aggregating many such relative comparisons, we can still derive global confidence estimates (e.g., determining whether question is one the model is highly confident in answering correctly). We propose relative confidence estimation, where the model compares pairs of questions, along with its answers, and provides preference judgments on which question it is more confident in answering correctly. Given set of questions and their corresponding answers, our task is to elicit pairwise confidence preferences and use these preferences to derive meaningful confidence scores for each question. This process involves two stages: Confidence Preference Data Generation (Section 4.1) and Rank Aggregation (Section 4.2). 4.1 Confidence Preference Data Generation To generate confidence preference data, we employ the following procedure: for each question i, pair it with randomly selected question = i. The model compares the two questions, alongside its answers to the questions, and is then asked which one it feels more confident about answering correctly (Figure 3). The answer for each question is obtained using the same prompt used for direct confidence prompting (Figure 2). 4 Algorithm 1 Confidence Preference Data Generation 1: Input: = {q1, q2, . . . , qm} 2: pref_data 3: for each qi do 4: 5: 6: for = 1 to do Randomly select qj qi winner Model(prompt, qi, qj) if winner = qi then Append (i, j) to 7: pref_data else Append(j, i) to pref_data end if 8: 9: 10: 11: end for 12: Output: pref_data end for Algorithm 2 Elo Rating 1: Input: pref_data, K, 2: = [1000, 1000, ..., 1000] 3: for = 1 to num_iters do 4: 5: (w, l) for each in pref_data do 6: 7: 8: (w wins) (l wins) 1 1 + 10(S[l]S[w])/K 1 1 + 10(S[w]S[l])/K S[w] S[w] + (1 (w wins)) S[l] S[l] (l wins) end for 9: 10: 11: end for 12: Output: This process is repeated times for each question i, pairing it with different questions and recording the models preferences (Algorithm 1). The result is list of confidence preference judgments (i, j) indicating the model was more confident in answering question than question (or (j, i) if model preferred question to i). Once this data is gathered, we move to the next step: aggregating these preferences to rank questions by confidence and using this to produce confidence scores (Section 4.2). 4.2 Rank Aggregation Confidence preference data provides partial rankings of questions based on confidence. For instance, given questions 1, 2, and 3, the model may indicate 3 > 2 and 2 > 1. These partial rankings can be aggregated into total ordering of questions by confidence, enabling the derivation of question-level confidence scores. This process, known as rank aggregation, is well-studied in social choice theory for voting, consensus formation, and preference aggregation (Arrow, 1951; Tideman, 1987; Kemeny & Snell, 1978; Dwork et al., 2001). The ideal ranking would place all correctly answered questions above incorrectly answered ones, reflecting calibrated models confidences. With complete set of noiseless comparisonswhere correctly answered questions are consistently preferreda total ordering could be derived by straightforward sorting. However, our confidence preference data is noisy (e.g., incorrectly answered questions are sometimes preferred), inconsistent (e.g., occasional circular preferences among questions), and incomplete (limited to comparisons per question for tractability). Given these challenges, we aim to approximate the best total ordering that represents the confidence preference data while being robust to noise, inconsistency, and incompleteness. While finding the optimal total ordering (Kemeny-optimal solutions (Kemeny & Snell, 1978)) is NP-hard, efficient approximation algorithms can provide practical solutions. Algorithm 3 Bradley-Terry MLE 1: function bradley_terry_ll(θ, pref_data) 2: 3: 4: exp(θ) ℓ(θ; pref_data) = 0 for each in pref_data do 5: 6: (w, l) (w wins) S[w] S[w] + S[l] + log (w wins) ℓ(θ; pref_data) ℓ(θ; pref_data) end for Output: ℓ(θ; pref_data) + λ 7: 8: 9: 10: 11: end function 12: 13: Input: pref_data 14: θ = [0, 0, ..., 0] 15: minimize(bradley_terry_ll, θ, pref_data, BFGS) 16: exp(θ) 17: Output: i=1 θ2 Pn We explore three popular algorithms to perform rank aggregation and assign confidence scores based on our preference data: Elo rating, TrueSkill, and Bradley-Terry. These algorithms are typically used to score 5 player skill levels in tournament-style games based on matchup data. In this setting, each question is treated as player engaging in matchups with other questions, where the models confidence preferences dictate the outcomes of these matches. Elo Rating. Elo rating (Elo, 1978) is commonly used in games like chess and leverages matchup data between players to iteratively update player ratings in an online learning fashion. We start by assigning all questions identical scores. For any pair of questions and j, the probability of winning the matchup is modeled as logistic function of and js current scores si, sj. determines how sensitive the player scores are to match outcomes. (i wins) = 1 1 + 10(sisj )/K (1) After each matchup the scores are adjusted based on how significantly the estimated win probabilities deviated from the true outcome (i.e. the models preference)surprising outcomes (low-confidence wins) lead to more substantial score changes. We iterate over the confidence preference data multiple times to ensure score convergence. See Algorithm 2 for more details. TrueSkill. TrueSkill (Herbrich et al., 2006) is Bayesian model designed for ranking players in competitive games. It is an extension of the Elo rating system that represents each players skill score as normal distribution, with the mean (µ) indicating the best estimate of their current score and the variance (σ) reflecting the models uncertainty about that score. After each matchup between pair of questions, the mean and variance of each questions scores are updated based on the difference between the expected result and the true outcome. The TrueSkill model uses factor graphs to represent the probabilistic relationships between player skill levels. belief propagation algorithm is used on the factor graph to update beliefs about players skills based on match outcomes. As more matchup data is processed for each question, the uncertainty (σ) decreases, refining the estimate of the questions score over time. We leverage the trueskill Python package as the implementation of this technique. Bradley-Terry. The Bradley-Terry model (Bradley & Terry, 1952) is probabilistic framework for modeling pairwise comparisons, commonly used in ranking tasks. It provides an alternate means of modeling the probability of question winning matchup against question j, based on their underlying scores. Bradley-Terry estimates the probability that question wins over question as: (i wins) = si si + sj (2) where si and sj are the scores for question and j. These scores are optimized using maximum likelihood estimation (MLE), with L2 regularization applied to control for overfitting and mitigate the impact of noisy comparisons. Bradley-Terry uses different estimate of the player win probability than Elo rating. Additionally unlike Elo, which updates scores iteratively after each comparison, the Bradley-Terry model optimizes the scores holistically, taking all pairwise comparisons into account simultaneously. We use the BFGS algorithm to perform this optimization. See Algorithm 3 for more details. We optimize the rank aggregation hyperparameters using small held out set (Appendix A.3). Finally, we normalize the confidence scores to range of 0-1 using min-max normalization."
        },
        {
            "title": "5 Results",
            "content": "Relative confidences outperform absolute confidences. We compare relative confidence estimates with absolute confidence estimates using direct prompting and self-consistency prompting, and report gains over the best relative confidence approach for each model. Across 14 datasets, relative confidence estimates boost AUC over direct prompting by 6.1% for Llama 3.1 405B, 4.1% for GPT-4, 3.2% for Gemini 1.5 Pro, 3.2% for GPT-4o, and 1.1% for Claude 3.5 Sonnet (Figure 4). Compared to self-consistency prompting, relative confidence raises AUC by 4.9% for Llama 3.1 405B, 1.0% for GPT-4, 0.8% for Gemini 1.5 Pro, and 1.8% for GPT-4o (Figure 4). For Claude 3.5 Sonnet, relative confidences slightly underperform self-consistency prompting (by 0.1%). See Table 1 and Table 3 respectively for the dataset-level AUC results on Llama 6 Figure 4: Selective Classification AUC Across Models. For each model, we plot the selective classification AUC averaged across the 14 tasks for each confidence estimation method. The absolute confidence estimation baselinesdirect prompting (Direct) and self-consistency (Hybrid SC)are indicated in blue, while relative confidence estimation with different rank aggregation methods is in green (Elo Rating, TrueSkill, BradleyTerry). For Llama 3.1 405B, GPT-4, Gemini 1.5 Pro, and GPT-4o, relative confidence estimates outperform both the direct and hybrid SC absolute confidence baselines. For Claude 3.5 Sonnet, relative confidences outperform direct prompting but slightly underperform self-consistency prompting. 3.1 405B and GPT-4o, and Appendix A.1 for dataset-level results for the other models. Overall, relative confidence improves confidence estimation for 4 of the 5 state-of-the-art models, with Llama 3.1 405B seeing the largest gains, followed by the GPT-4 models and Gemini 1.5 Pro. While confidence estimates for Claude 3.5 Sonnet also improve over direct prompting, the gains are smaller due to Sonnets ability to make good absolute judgments of confidence. Does chain of thought improve confidence estimates? We experiment with augmenting relative confidence judgments in GPT-4o with chains of thought (CoTs). We update the relative confidence prompt for confidence preference data generation (Algorithm 1) by asking the model to reason about which question it is more confident in (Appendix A.4). We apply Elo rating, the best rank aggregation algorithm for GPT-4o, to the CoT confidence preference data to generate confidence scores. However, the CoT confidence estimates fail to improve performance and lead to worse outcomes when the model hallucinates evidence, becoming confident in both options. Overall, incorporating CoTs slightly decreases GPT-4os AUC averaged over datasets, from 87.2% to 86.8% while also requiring more inference-time compute. How important are answers in determining confidence? We investigate how important it is for model to see its own answer to question in order to gauge its confidence level in correctly answering the question. To assess this, we modify the relative confidence prompt, asking GPT-4o to judge which of the two questions is more difficult for it to answer correctly, without providing it access to its own answers to these questions. See Appendix A.4 for the exact prompt. We then apply the same rank aggregation methods to this difficulty preference data and produce confidence scores. This approach drops the average AUC for relative confidence estimation with Elo rating by 5.3% from 87.2% to 81.9%, emphasizing that access to its own answers significantly enhances the models relative confidence judgments. Nevertheless, even without answers, relative confidence judgments are only 2.1% less reliable than absolute confidence assessments with answers (81.9% vs 84%), suggesting that models are still reasonably good at judging questions difficulty, even before answering it."
        },
        {
            "title": "Category",
            "content": "Dataset Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry"
        },
        {
            "title": "Commonsense\nReasoning",
            "content": "GPQA 0.356 MedQA 0.864 0.926 OBQA 0.862 Physics 0.378 Algebra 0.585 Chem 0.861 Security"
        },
        {
            "title": "TQA\nCSQA\nSIQA",
            "content": "0.749 0.889 0.711 0.961 0.861 0.837 0.830 Average 0.762 0.293 0.859 0.959 0.793 0.448 0.486 0.899 0.747 0.963 0.703 0. 0.899 0.920 0.879 0.774 0.453 0.914 0.970 0.907 0.467 0.747 0.895 0.813 0.922 0.778 0.989 0.876 0.865 0.868 0. 0.454 0.918 0.969 0.934 0.466 0.751 0.910 0.834 0.922 0.770 0.987 0.874 0.868 0.867 0.823 0.451 0.915 0.968 0.938 0.476 0.746 0.908 0.825 0.917 0.748 0. 0.877 0.870 0.871 0.821 Table 1: Llama 3.1 405B AUCs All Methods. We show the dataset-level results for Llama 3.1 405B, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidences for all STEM datasets, whereas absolute confidences with self-consistency (Hybrid SC) work best for commonsense reasoning tasks. Overall, relative confidences with TrueSkill rank aggregation lead to 6.1% improvement over direct prompting and 4.9% improvement over self-consistency prompting. 5 10 15 0.9% 1.8% 1.8% # Model Calls % Gains GPT-4o % Gains Llama 3. Does scaling up comparisons help? We hypothesize that increasing the number of relative confidence comparisons per question would lead to better ranking of questions by confidence, and more reliable confidence scores. To test this, we scale up the number of judgments, going from 5 to 10 to 15 model calls per question. To ensure fair comparison based on compute, we use self-consistency baseline with the same number of model calls per confidence estimate (Section 3). We report improvements based on the best rank aggregation method for each model in Table 2. Even for small number of model calls, relative confidences show improvements over self-consistency prompting. Further scaling up the number of relative confidence comparisons per question increases the improvements of relative confidence estimation over self-consistency prompting. However, as seen with GPT-4o, for some models further scaling model calls may show diminishing returns due to inherent noise in the models confidence preferences. Table 2: Gains by scaling up comparisons. We report the gains of relative confidence estimation over selfconsistency across different numbers of model calls. 2.2% 3.2% 4.9% Different methods for rank aggregation. We evaluate multiple rank aggregation methods for converting relative confidence preferences into scalar scores. Relative confidence estimation with any rank aggregation method outperforms direct and self-consistency prompting (Figure 4) (except for slightly underperforming self-consistency prompting with Claude 3.5 Sonnet). While differences in the performance of the rank aggregation methods is small, TrueSkill is the best method for most models, except for Gemini 1.5 Pro where Bradley-Terry performs best and GPT-4o where Elo rating performs best. TrueSkill explicitly models player skill levels as probability distributions instead of single point estimates, as in Elo rating and Bradley-Terry. This allows it to capture uncertainty in each players skill rating and update it as they participate in more games, which may allow this method to be more robust to the noise in the relative comparison data. In general, for relative confidence estimation with new model, we would recommend"
        },
        {
            "title": "Category",
            "content": "Dataset Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry"
        },
        {
            "title": "Commonsense\nReasoning",
            "content": "GPQA 0.480 MedQA 0.923 0.971 OBQA 0.898 Physics 0.655 Algebra 0.741 Chem 0.880 Security"
        },
        {
            "title": "TQA\nCSQA\nSIQA",
            "content": "0.859 0.960 0.799 0.962 0.906 0.864 0.855 Average 0.840 0.421 0.931 0.983 0.914 0.743 0.700 0.913 0.872 0.969 0.824 0. 0.935 0.900 0.884 0.854 0.530 0.944 0.987 0.940 0.722 0.795 0.930 0.872 0.962 0.837 0.983 0.908 0.886 0.905 0. 0.528 0.943 0.986 0.944 0.710 0.806 0.927 0.867 0.962 0.833 0.983 0.911 0.887 0.905 0.871 0.522 0.943 0.987 0.946 0.694 0.802 0.922 0.867 0.959 0.833 0. 0.911 0.884 0.908 0.868 Table 3: GPT-4o AUCs All Methods. We show the dataset-level results for GPT-4o, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidences for the majority of STEM and social science datasets, while absolute confidences with self-consistency tend to work better for commonsense reasoning tasks. starting with TrueSkill rank aggregation. The online learning paradigm of Elo rating and TrueSkill may also be particularly suited to environments where confidence judgments accumulate over time, leading to more refined confidence estimates (i.e. confidences of medical chatbot improving as it helps more patients), in contrast to Bradley-Terry where confidence scores are optimized over the full dataset of judgments at once."
        },
        {
            "title": "6 Related Work",
            "content": "Confidence Estimation. Recent studies have explored confidence estimation in language models. Kadavath et al. (2022) measure the calibration of outputted log probabilities from language models and find that models generally demonstrate good calibration on true/false and multiple-choice tasks. They also show that models can better estimate their confidence in an answer by comparing multiple answers for given question. Our approach instead asks models to compare their confidence across different questions and finds this leads to reliable confidence estimates. Shrivastava et al. (2023) show that absolute linguistic confidence estimation (e.g. Score your confidence from 0-1) is hard problem for closed models, and confidences for closed models can instead be estimated by transferring log probabilities from open models. Our work instead focuses on linguistic confidence estimates, without needing access to models log probabilities. Other works on linguistic confidence estimation use self-consistency-like methods to sample multiple answers and corresponding confidences from models and aggregate them (Xiong et al., 2023). We compare relative confidence estimation with the best performing self-consistency technique from Xiong et al. (2023) and find that relative confidences tend to outperform self-consistency based estimates. Other approaches fine-tune language models to improve confidence estimation (Lin et al., 2022), while our method elicits better estimates without requiring further training. LMs as Evaluators. Several works also use language models to evaluate the quality of models responses. GPTScore (Fu et al., 2023) and LLM-as-a-judge (Zheng et al., 2023) use LMs to provide automated scoring or feedback on different aspects of text quality as an alternative to traditional text evaluation metrics such as ROUGE and BLEU. These approaches are similar to absolute linguistic confidence estimation (Score your confidence from 0-1). Other works use LMs to evaluate their responses through either numerical score or natural language feedback to improve their own generations. This can occur through search at decoding 9 time (Yao et al., 2023), prompting the model to self-correct its responses using its feedback (Madaan et al., 2023; Bai et al., 2022), or by aligning model using its own reward signals (Yuan et al., 2024). Linguistic confidence estimation relates to self-evaluation with LMs, since we ask models to evaluate their own confidence levels. Learning from Human Preference Data. Several approaches have improved language models across diverse attributes (safety, fluency, etc.) by deriving reward signal from human preferences. These preferences are typically framed as relative judgments by asking annotators to select their preferred output from pair or set of responses for given input, instead of asking them to directly score the quality of single response (Ouyang et al., 2022; Ziegler et al., 2019; Christiano et al., 2017). Motivated by this framing, we elicit relative confidence judgments from LMs and use these to produce more reliable confidence scores. Rank Aggregation. There is rich body of work studying the problem of rank aggregationconverting partial orderings over set into better total ordering (Arrow, 1951; Tideman, 1987; Kemeny & Snell, 1978; Dwork et al., 2001). This problem is common in domains such as sports and competitive games, election voting, and product recommendations. Our work leverages popular rank aggregation algorithms such as Elo rating (Elo, 1978), TrueSkill (Herbrich et al., 2006), and Bradley-Terry (Bradley & Terry, 1952) to convert the pairwise confidence preferences from model into total ordering of questions and corresponding answers by confidence. Other approaches such as Rank Centrality (Negahban et al., 2012) model rank aggregation through Markov Chain and use the stationary distribution to determine the rank of each item. Calibration and Selective Classification. The quality of confidence estimates is often measured through calibrationby determining how grounded the confidences are in true correctness (Murphy & Winkler, 1977; DeGroot & Fienberg, 1983; Naeini et al., 2014; Guo et al., 2017b), typically through the expected calibration error (ECE). However, the ECE cannot capture how well confidences distinguish between correct and incorrect examples: outputting the same confidence for all examples can lead to perfect ECE if the confidence matches the average model accuracy. This leads us to focus on selective classification (El-Yaniv & Wiener, 2010; Khani et al., 2016; Feng et al., 2019; Jones et al., 2021) which measures if the model knows what it doesnt know and can achieve high accuracy by abstaining on examples where it is uncertain."
        },
        {
            "title": "7 Discussion",
            "content": "As users increasingly interact with language models through chat interfaces, estimating linguistic confidences by asking the model about its confidence in natural language has become increasingly important. Most current approaches rely on absolute confidence estimates, where the model is asked to judge its confidence for question in isolation, e.g., rate your confidence on scale of 0-1. However, prior work shows that models struggle with absolute confidence estimation, as they are not specifically trained to produce such estimates (Zhou et al., 2023). As result, they tend to default to narrow range of coarse-grained confidences for most questions (e.g., 0.9, 0.95), which fail to convey meaningful distinctions in certainty to users (Shrivastava et al., 2023). In contrast, relative preferences are ubiquitous in real life, from ranking players in games to conducting A/B testing for products. Relative preferences are also highly effective in machine learning. For example, relative annotations of generation quality lead to better reward estimates in RLHF, and models are shown to be better calibrated on multiple-choice questions (Kadavath et al., 2022), which also involve relative judgments. Given the challenges with absolute confidence estimation, we propose shift towards relative confidence estimation. Rather than asking models to directly generate confidence scores, we ask them to instead provide confidence preferences by comparing their confidence levels across pairs of questions. These preferences can then be converted into confidence scores using rank aggregation methods, such as Elo rating (Elo, 1978) and the Bradley-Terry model (Bradley & Terry, 1952). By framing confidence estimation as simpler binary decisionmore confident or less confidentwe reduce the complexity of the task and eliminate the need for models to generate fine-grained confidence scores in isolation. To our best knowledge, we are the first work to approach confidence estimation through the lens of relative comparisons. Our method is further motivated by the notion that, for any given task, questions can be ranked along spectrum of difficulty for given model. Harder questions, which the model is more likely to answer incorrectly, should correspond to lower confidence scores. Relative confidence estimation leverages this principle, using 10 pairwise confidence comparisons and rank aggregation to approximate ranking of questions by difficulty, thereby producing more meaningful confidence estimates. We show the effectiveness of relative confidence estimation over absolute confidence estimation across broad range of question answering tasks, demonstrating improved confidence estimates for five state-of-the-art language models."
        },
        {
            "title": "8 Future Work",
            "content": "Eliciting Confidence Preference Data. There can be several different ways of eliciting relative confidence judgments. Prompts could allow for ties in confidence or compare confidence across more than two questions. Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024) for LM alignment achieves DPO (Rafailov et al., 2023) levels of performance by using binary signals of desirability for generations. We can apply KTO to confidence preference data generation by asking for binary signalsconfident or notand then converting these into relative judgments, ranking not confident answers below confident ones. In this work, we explore the most popular rank aggregation methods like Rank Aggregation. Elo rating (Elo, 1978), Bradley-Terry (Bradley & Terry, 1952), and TrueSkill (Herbrich et al., 2006). Another approach to rank aggregation is to represent preference data as graph, with nodes as questions and directed edges reflecting match outcomes between questions. Since the outcome of some of these matchups can be inconsistent and non-transitive, algorithms like Rank Centrality (Negahban et al., 2012), PageRank (Page et al., 1999), and Minimum Feedback Arc Set (Vahidi & Koutis, 2024) could be used to reduce cycles in the graph and better manage these inconsistencies. Confidence Estimation for Longform Generations. While we benchmark on multiple-choice tasks, relative confidence estimation can also extend to longform generation. Log probabilities on answer tokens are commonly used for confidence estimation in multiple-choice tasks, but token-level uncertainty doesnt translate well to longform sequences. Moreover, there may be different levels of uncertainty associated with different aspects of longform generation, e.g. how complete generation, vs how factual it is, etc. Relative confidence estimation could produce fine-grained confidence scores for different attributes of longform response by adjusting the prompt for confidence preferences accordingly. Alignment with Relative Confidence. Works like Tian et al. (2023) explore using absolute confidence scores to align language models for different attributes such as factuality, without human annotations (RLAIF). Since relative confidences are more calibrated than absolute confidences, we can instead use relative confidences to construct preference pairs for aligning models on different attributes. Curriculum Learning with Difficulty Estimates. We also explore generating relative confidence judgments without revealing model answers (Section 5). These scores correspond to difficulty ratings, which could inform curriculum learning by first training on lower-difficulty examples."
        },
        {
            "title": "9 Acknowledgments",
            "content": "We sincerely thank Tushar Khot for his insightful discussions and guidance on this work during our time collaborating with the Allen Institute of AI (AI2). His feedback was invaluable in shaping the early aspects of this work."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 Sonnet. 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet. Kenneth J. Arrow. Social Choice and Individual Values. Yale University Press, 1951. URL https:// yalebooks.yale.edu/book/9780300179316/social-choice-and-individual-values/. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. URL https://api.semanticscholar.org/CorpusID:125209808. Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. ArXiv, abs/1706.03741, 2017. URL https: //api.semanticscholar.org/CorpusID:4787508. Morris H. DeGroot and Stephen E. Fienberg. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society. Series (The Statistician), 32:1222, 1983. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for the web. In The Web Conference, 2001. URL https://api.semanticscholar.org/CorpusID:8393813. Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research (JMLR), 11, 2010. Arpad Elo. The Rating of Chessplayers, Past and Present. Arco Pub., New York, 1978. URL https: //www.amazon.com/Rating-Chess-Players-Past-Present/dp/0668047216. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. URL https://api.semanticscholar. org/CorpusID:267406810. Jean Feng, Arjun Sondhi, Jessica Perry, and Noah Simon. Selective prediction-set models with coverage guarantees. arXiv preprint arXiv:1906.05473, 2019. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. In North American Chapter of the Association for Computational Linguistics, 2023. URL https://api.semanticscholar. org/CorpusID:256662188. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. ArXiv, abs/1706.04599, 2017a. URL https://api.semanticscholar.org/CorpusID:28671436. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning (ICML), pp. 13211330, 2017b. Dan Hendrycks and Kevin Gimpel. baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations (ICLR), 2017. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv, abs/2103.03874, 2021b. URL https://api.semanticscholar.org/CorpusID:232134851. 12 Ralf Herbrich, Thomas P. Minka, and Thore Graepel. Trueskilltm: bayesian skill rating system. In Neural Information Processing Systems, 2006. URL https://api.semanticscholar.org/CorpusID:9744799. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. In arXiv preprint arXiv:2009.13081, 2021. Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification can magnify disparities across groups. In International Conference on Learning Representations (ICLR), 2021. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. John G. Kemeny and J. Laurie Snell. Mathematical Models in the Social Sciences. The MIT Press, 1978. URL https://mitpress.mit.edu/9780262610308/mathematical-models-in-the-social-sciences/. Fereshte Khani, Martin Rinard, and Percy Liang. Unanimous prediction for 100% precision with application to learning semantic mappings. In Association for Computational Linguistics (ACL), 2016. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, D. Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, S. Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/ forum?id=8s8K2UZGTZ. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. ArXiv, abs/2303.17651, 2023. URL https://api.semanticscholar.org/CorpusID:257900871. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Empirical Methods in Natural Language Processing (EMNLP), 2018. Allan H. Murphy and Robert L. Winkler. Reliability of subjective probability forecasts of precipitation and temperature. Journal of the Royal Statistical Society. Series (Applied Statistics), 26:4147, 1977. Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Binary classifier calibration: Nonparametric approach. arXiv, 2014. Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. Proceedings of the AAAI Conference on Artificial Intelligence., 2015:29012907, 2015. URL https://api.semanticscholar.org/CorpusID:6292807. 13 Sahand N. Negahban, Sewoong Oh, and Devavrat Shah. Rank centrality: Ranking from pairwise comparisons. Oper. Res., 65:266287, 2012. URL https://api.semanticscholar.org/CorpusID:3602049. OpenAI. Introducing chatgpt. 2022. URL https://openai.com/index/chatgpt/. OpenAI. GPT-4. 2023. URL https://openai.com/index/gpt-4-research/. OpenAI. GPT-4o. 2024. URL https://openai.com/index/hello-gpt-4o/. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/CorpusID:246426909. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking : Bringing order to the web. In The Web Conference, 1999. URL https://api.semanticscholar.org/CorpusID: 1508503. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290, 2023. URL https://api.semanticscholar.org/CorpusID:258959321. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. ArXiv, abs/2311.12022, 2023. URL https://api.semanticscholar.org/CorpusID:265295009. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. Llamas know what gpts dont show: Surrogate models for confidence estimation. ArXiv, abs/2311.08877, 2023. URL https://api.semanticscholar. org/CorpusID:265213392. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In North American Association for Computational Linguistics (NAACL), 2019. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-tuning language models for factuality. ArXiv, abs/2311.08401, 2023. URL https://api.semanticscholar.org/CorpusID: 265158181. T. Nicolaus Tideman. Independence of clones as criterion for voting rules. Social Choice and Welfare, 4: 185206, 1987. URL https://api.semanticscholar.org/CorpusID:122758840. Soroush Vahidi and Ioannis Koutis. Minimum weighted feedback arc sets for ranking from pairwise comparisons. ArXiv, abs/2412.16181, 2024. URL https://api.semanticscholar.org/CorpusID:274982240. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022. URL https: //api.semanticscholar.org/CorpusID:247595263. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. URL https://arxiv.org/pdf/2306.13063.pdf. 14 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023. URL https://api.semanticscholar.org/CorpusID:258762525. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. ArXiv, abs/2401.10020, 2024. URL https://api.semanticscholar.org/ CorpusID:267035293. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL https://api. semanticscholar.org/CorpusID:259129398. Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: Expressions of overconfidence and uncertainty in language models, 2023. Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019. URL https://api.semanticscholar.org/CorpusID:202660943."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Full AUC Results"
        },
        {
            "title": "Category",
            "content": "Dataset Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry"
        },
        {
            "title": "Social Sciences",
            "content": "Commonsense Reasoning GPQA 0.441 MedQA 0.904 0.979 OBQA 0.909 Physics 0.802 Algebra 0.832 Chem 0.920 Security"
        },
        {
            "title": "Law\nEthics\nEcon\nPolicy",
            "content": "TQA CSQA SIQA 0.789 0.956 0.798 0.991 0.880 0.885 0.861 Average 0.853 0.457 0.920 0.984 0.911 0.811 0.840 0. 0.809 0.964 0.822 0.995 0.889 0.887 0.897 0.865 0.454 0.893 0.984 0.925 0.806 0.864 0.934 0.799 0.971 0.825 0.982 0.918 0.873 0. 0.863 0.460 0.900 0.984 0.928 0.805 0.854 0.930 0.815 0.970 0.829 0.980 0.917 0.869 0.856 0.864 0.466 0.901 0.985 0.927 0.804 0.850 0. 0.816 0.966 0.819 0.982 0.917 0.872 0.863 0.863 Table 4: Claude 3.5 Sonnet AUCs All Methods. We show the dataset-level results for Claude 3.5 Sonnet, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidence baselines for 9 out of 14 datasets across STEM, social science, and commonsense reasoning. On average, relative confidences closely match the performance of the best absolute confidence methods (only 0.1% lower AUC than self-consistency prompting). Category Dataset Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry STEM Social Sciences Commonsense Reasoning GPQA 0.395 MedQA 0.794 0.955 OBQA 0.878 Physics 0.603 Algebra 0.717 Chem 0.868 Security Law Ethics Econ Policy TQA CSQA SIQA 0.695 0.903 0.684 0.961 0.876 0.835 0.854 Average 0.787 0.424 0.831 0.959 0.922 0.612 0.762 0.863 0.727 0.910 0.734 0. 0.891 0.889 0.874 0.811 0.410 0.725 0.979 0.927 0.629 0.806 0.850 0.766 0.949 0.747 0.980 0.861 0.860 0.840 0. 0.409 0.786 0.985 0.936 0.651 0.851 0.824 0.776 0.954 0.732 0.979 0.853 0.869 0.854 0.818 0.413 0.792 0.985 0.934 0.640 0.842 0.837 0.778 0.957 0.736 0. 0.854 0.872 0.848 0.819 Table 5: Gemini 1.5 Pro AUCs All Methods. We show the dataset-level AUC results for Gemini 1.5 Pro. On average, relative confidence estimation with Bradley-Terry leads to the best AUC with 3.2% improvement over direct prompting and 0.8% improvement over self-consistency prompting."
        },
        {
            "title": "Category",
            "content": "Dataset Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry"
        },
        {
            "title": "Commonsense\nReasoning",
            "content": "GPQA 0.393 MedQA 0.841 0.966 OBQA 0.818 Physics 0.587 Algebra 0.682 Chem 0.911 Security"
        },
        {
            "title": "TQA\nCSQA\nSIQA",
            "content": "0.716 0.870 0.634 0.959 0.892 0.831 0.851 Average 0.782 0.383 0.893 0.979 0.851 0.650 0.774 0.916 0.741 0.915 0.638 0. 0.926 0.868 0.872 0.813 0.377 0.870 0.990 0.908 0.642 0.797 0.933 0.722 0.908 0.717 0.970 0.865 0.835 0.886 0. 0.404 0.875 0.990 0.918 0.651 0.805 0.927 0.753 0.914 0.714 0.971 0.869 0.841 0.888 0.823 0.394 0.864 0.989 0.917 0.663 0.795 0.922 0.754 0.911 0.725 0. 0.872 0.837 0.887 0.821 Table 6: GPT-4 AUCs All Methods. For GPT-4, relative confidences with TrueSkill lead to the best average AUC with 4.1% improvement over direct prompting and 1.0% improvement over self-consistency. A.2 Average AUROC Results Model Direct Hybrid SC Elo Rating TrueSkill Bradley-Terry Llama 3.1 405B GPT-4 Gemini 1.5 Pro GPT-4o Claude 3.5 Sonnet 0.575 0.642 0.627 0.698 0.685 Average Across Models 0.645 0.774 0.730 0.700 0.774 0.726 0. 0.849 0.708 0.689 0.762 0.711 0.744 0.856 0.719 0.713 0.763 0.713 0.753 0.852 0.713 0.712 0.758 0.713 0. Table 7: Model AUROCs. Relative confidences with TrueSkill lead to the best average AUROC for 2 out of 5 models, and 10.8% gain over direct prompting and 1.2% gain over self-consistency across all models. A.3 Hyperparameters Following are the hyperparameters involved for each rank aggregation method of relative confidence estimation. Elo rating. initial scores, K, # iterations TrueSkill. µ, σ, β, τ Bradley-Terry. maximum # iterations, λ for regularization We use the following fixed set of hyperparameters for datasets which do not have sufficient validation set for hyperparameter tuning of hundred examples or more beyond their test set. Elo Rating Initial Score # iterations µ 1000 1 25.0 TrueSkill σ µ 3.0 β µ 6.0 τ µ 300. Bradley-Terry max # iterations λ 5 0.01 Table 8: Rank Aggregation Hyperparameter Values. 17 For the datasets which have hundred or more examples in their train or validation sets, we select hundred examples to use for tuning the following hyperparameters to achieve the best AUC on this held-out set."
        },
        {
            "title": "Elo Rating",
            "content": "# iters"
        },
        {
            "title": "TrueSkill",
            "content": "σ β τ [1-20] 2.5 , µ 5.0 , µ [ µ 3.0 , µ 6.0 , µ [ µ [ µ µ 250.0 , 300.0 , 2.2 , µ 2.0 ] 4.0 , µ 3.0 ] µ 200.0 , µ 150.0 ] Bradley-Terry max # iters [1-20] Table 9: Rank Aggregation Hyperparameter Ranges. A.4 Prompts Linguistic Confidence Prompt Answer the following question to the best of your ability, and provide score between 0 and 1 to indicate the confidence you have in your answer. Confidence scores closer to 0 indicate you have less confidence in your answer, while scores closer to 1 indicate you have more confidence in your answer. You must answer the question with one of the valid choices. You must provide only single answer. Question: This is question (A) first answer (B) second answer (C) third answer (D) fourth answer (E) fifth answer Answer: (D) Confidence: 0.4 Question: This is another question (A) first answer (B) second answer (C) third answer (D) fourth answer (E) fifth answer Answer: (A) Confidence: 0. CoT Relative Confidence Prompt Here are two questions and your answers to those questions. Which question are you more confident in answering correctly and why? Respond in the following format: am more confident that correctly answered question <your selected question>, because <your reasoning>. Difficulty Prompt Here are two questions. Which question is more difficult? Respond in the following format: <your selected question> is more difficult."
        }
    ],
    "affiliations": [
        "cs.stanford.edu"
    ]
}