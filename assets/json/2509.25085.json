{
    "paper_title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
    "authors": [
        "Feng Wang",
        "Yuqing Li",
        "Han Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 8 0 5 2 . 9 0 5 2 : r jina-reranker-v3: Last but Not Late Interaction for Document Reranking Feng Wang1 Yuqing Li1,2 Han Xiao 1Jina AI GmbH 2University of Pittsburgh Prinzessinnenstraße 19, 10969, Berlin, Germany research@jina.ai"
        },
        {
            "title": "Abstract",
            "content": "jina-reranker-v3 is 0.6B parameter multilingual document reranker that introduces novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being 10 smaller than generative listwise rerankers."
        },
        {
            "title": "Introduction",
            "content": "Neural document reranking faces fundamental efficiency-effectiveness tradeoff. Cross-encoders achieve strong performance through joint query-document processing but require separate forward passes for each pair, while embedding models enable efficient similarity computation but lose finegrained interaction signals. Recent innovations have attempted to bridge this gap through different interaction approaches. Late interaction models like ColBERT [Khattab and Zaharia, 2020] and their variants [Liu et al., 2024, Chaffin and Sourty, 2025] separately encode queries and documents into multi-vector representations, then perform interaction through token-level similarity operations. We introduce jina-reranker-v3, which proposes novel last but not late interaction that fundamentally differs from existing approaches. While late interaction models delay attention interactions until after separate encoding, our method performs causal query-document self-attention within the transformer context window first, enabling documents to attend to each other and establish cross-document relationships before extracting contextual embeddings from the last token of each document. This cross-document interaction capability is unavailable in separate encoding approaches and represents the core innovation of our method. Unlike late interaction models that perform interactions after encoding, we enable interactions during encoding, making our approach not late. Our evaluation shows significant improvements across diverse benchmarks. jina-reranker-v3 achieves 61.94 nDCG@10 on BEIR, representing the highest score among all evaluated rerankers and 4.88% improvement over our previous jina-reranker-v2. The model excels particularly in multi-hop retrieval with HotpotQA reaching 78.56, fact verification achieving 93.95 on FEVER, competitive multilingual performance across 18 languages at 66.50 on MIRACL, and specialized domain coverage reaching 63.28 on code retrieval."
        },
        {
            "title": "2 Related Work",
            "content": "Document reranking approaches can be categorized by their interactions and learning objectives. Traditional learning-to-rank methods [Bruch et al., 2023] include pointwise approaches that predict Preprint. relevance scores independently, pairwise methods like RankNet [Burges et al., 2005] that compare document pairs, and listwise techniques that optimize global ranking objectives. Cross-encoders like BERT-based rerankers [Nogueira and Cho, 2019] achieve strong performance through full query-document interaction but require separate forward passes for each pair, creating computational bottlenecks for large-scale retrieval. Recent comparative studies [Déjean et al., 2024] demonstrate that while LLM-based rerankers show impressive zero-shot capabilities, traditional cross-encoders remain highly competitive across diverse retrieval scenarios. Late interaction models represent significant approach that balances efficiency with expressiveness. ColBERT [Khattab and Zaharia, 2020] exemplifies this approach by independently encoding queries and documents into multi-vector representations, then computing similarity through MaxSim operations over token-level embeddings. This design enables pre-computation of document representations while preserving fine-grained matching signals. Recent developments have expanded this approach: analysis of matching mechanisms and token pruning strategies [Liu et al., 2024] provides theoretical foundations, LITE [Ji et al., 2024] introduces learnable late interactions, and Jina-ColBERT-v2 [Zhang et al., 2024] extends the approach to multilingual settings. PyLate [Chaffin and Sourty, 2025] provides flexible frameworks for training and deployment of such models. The late chunking method [Günther et al., 2024] processes complete documents through transformers before applying chunking boundaries, extracting chunk-level embeddings that preserve contextual relationships. This approach demonstrates how leveraging broader document context can improve embedding quality, though it focuses primarily on retrieval rather than reranking applications. LLM-powered reranker has emerged as powerful family with diverse implementations. These approaches can be categorized into discriminative and generative methods. Generative approaches like RankGPT [Qin et al., 2023] prompt LLMs to generate ranked lists, leveraging their reasoning capabilities for relevance assessment, but typically require large models for competitive performance. Fine-tuning methods like RankVicuna [Pradeep et al., 2023] adapt existing models for relevance scoring tasks. Efficiency-focused innovations include FIRST [Reddy et al., 2024], which accelerates inference through single-token decoding, and PE-Rank [Qin et al., 2024], which leverages passage embeddings to reduce computational latency by 4.5. Recent advances in training methodology include ERank [Cai et al., 2025], which combines supervised fine-tuning with reinforcement learning for improved ranking quality, and the Qwen3 Embedding series [Zhang et al., 2025], which demonstrates sophisticated multi-stage training pipelines. DeAR [Abdallah et al., 2025] introduces dual-stage reasoning with LLM distillation for enhanced cross-document analysis."
        },
        {
            "title": "3 Model Architecture",
            "content": "3.1 Architecture jina-reranker-v3 implements an interaction architecture that fundamentally differs from existing approaches. Built upon Qwen3-0.6B [Yang et al., 2025] with 28 transformer layers, 1024 hidden dimensions, 16 attention heads, and 131K token context capacity, our approach processes queries and multiple documents simultaneously within shared context windows. We add lightweight MLP projector (1024512256 dimensions) to transform contextual representations into rankingoptimized embeddings. Table 5 provides complete architectural specifications. Our architecture addresses fundamental limitations in existing interactions. ColBERT [Khattab and Zaharia, 2020] achieves efficiency through separate encoding followed by multi-vector interaction, but cannot capture early query-document interactions during encoding or enable cross-document interactions within the attention mechanism. Our approach enables causal self-attention interaction within the transformer architecture, allowing documents to attend to each other and the query simultaneously, then extracts contextually-informed embeddings from designated token positions. The core innovation lies in enabling cross-document interactions during encoding: instead of delaying interaction until after separate encoding as in late interaction models, we process all documents and the query simultaneously within shared context windows. This allows each document to attend to other documents and observe their content, enabling contextual embeddings that capture not just query-document relevance but also inter-document relationships and comparative context. Such crossdocument interactions are impossible in separate encoding approaches and represent fundamental advancement in reranking architecture. 2 Figure 1: Architecture of jina-reranker-v3 showing the transformer backbone with special token positions for embedding extraction. The model processes multiple documents and query in one context window, extracting contextual embeddings at designated token positions for similarity computation. We extract contextual embeddings at designated special token positions: = Htq and di = Hti where tq and ti are positions of the special tokens and represents the transformers final layer hidden states after causal self-attention. These embeddings capture both local document semantics and global cross-document context through the shared attention mechanism, enabling rich inter-document interactions unavailable in separate encoding approaches. two-layer projection network with ReLU activation maps the 1024-dimensional hidden states to 256-dimensional embedding space: = Pϕ(q) and di = Pϕ(di). Relevance scores are computed via cosine similarity: si = cos(q, di). This architecture combines the expressiveness of joint encoding with efficient similarity computation. For document collections exceeding the 131K token context limit, we process documents in batches of up to 64 documents per forward pass, with query embeddings maintained consistently across batches to ensure ranking coherence. 3.2 Prompt Template jina-reranker-v3 processes structured prompts following Qwen3s instruction format with system/user/assistant roles to leverage existing instruction-following capabilities. As shown in Table 1, the system prompt establishes search relevance expert persona, while the user prompt provides clear ranking instructions with dual query placement. The template strategically places the query both at the beginning for instructions and at the end for final attention, sandwiching all documents in between. This design enables the final query position to attend to all preceding documents through causal attention while maintaining clear task instructions. Special tokens <doc_emb> after each document and <query_emb> after the final query mark specific positions for embedding extraction from transformer hidden states. While Qwen3 supports thinking capabilities, our implementation uses an empty <think></think> placeholder since the model is not trained with reasoning traces for document reranking tasks. 3 Prompt Template <im_start>system You are search relevance expert who can determine ranking of passages based on their relevance to the query. <im_end> <im_start>user will provide you with passages, each indicated by numerical identifier. Rank the passages based on their relevance to query: [QUERY] <passage id=\"0\"> [DOCUMENT_1]<doc_emb> </passage> <passage id=\"1\"> [DOCUMENT_2]<doc_emb> </passage> ... <passage id=\"k-1\"> [DOCUMENT_k]<doc_emb> </passage> <query> [QUERY]<query_emb> </query> <im_end> <im_start>assistant <think></think> Table 1: Complete prompt template structure used by jina-reranker-v3. Special tokens <doc_emb> and <query_emb> mark positions for embedding extraction from transformer hidden states. The empty <think></think> tag serves as placeholder without reasoning content."
        },
        {
            "title": "4 Training",
            "content": "jina-reranker-v3 employs comprehensive multi-objective training approach combining InfoNCE loss with specialized auxiliary losses to optimize ranking performance across diverse domains. The core training objective integrates multiple loss components, each addressing distinct aspects of the ranking problem: ℓ = ℓrank + 0.45 ℓdisperse + 0.85 ℓdual + 0.85 ℓsimilar (1) The primary component is the InfoNCE loss ℓrank [van den Oord et al., 2019], which generates the core ranking signal through contrastive learning with hard negatives: ℓrank = 1 (cid:88) i=1 log )/τ es(qi,d+ Zi where Zi = es(qi,d+ )/τ + (cid:88) k=1 es(qi,d i,k)/τ (2) Here, qi denotes the query embedding, d+ i,k denotes one of negative document embeddings, s(, ) is the cosine similarity function, τ is the temperature parameter, and is the batch size. represents the positive document embedding, To prevent representation collapse, we incorporate the dispersive loss ℓdisperse [Wang et al., 2024], which enhances embedding diversity by maximizing the average pairwise cosine distance between document embeddings: ℓdisperse = 1 log 1 + 1 (cid:32) es(d+ ,d i,k)/τ + K1 (cid:88) k=1 4 es(d i,k,d i,k+1)/τ (cid:33) (3) 1 follows the same formulation as Eq. 4 but computes the query embedThe dual matching loss ℓdual ding from the query tokens at the sequence start. This enforces bidirectional consistency between query-to-document and document-to-query similarity scores, enhancing ranking robustness. Finally, the similarity loss ℓsimilar [Huang et al., 2024] maintains semantic coherence at the document level. For each document in the input set, we create an augmented duplicate through text augmentation techniques. The loss then treats the original document and its augmented version as positive pair, while other documents serve as negatives. This encourages consistent embedding representations for semantically equivalent documents, even when their surface forms differ due to augmentation. The training methodology follows progressive three-stage approach designed for systematic complexity scaling: Stage 1: Foundation Specialization. Starting from pretrained Qwen3-0.6B, we simultaneously train domain-specific configurations using LoRA fine-tuning with r=16 and α=32 targeting all attention and FFN layers while freezing the backbone. The model processes training sequences containing 16 documents per query (one positive and 15 negative examples), with each document truncated or padded to 768 tokens, yielding maximal total sequence length of 12,288 tokens. Training data is drawn from diverse datasets including BGE-M3 [Chen et al., 2024] for multilingual coverage across 15 languages, Cornstack [Suresh et al., 2025] for code retrieval, as well as specialized datasets for biomedical [Xu et al., 2024] and instruction following [Weller et al., 2024] configurations. Stage 2: Context and Hard Negative Mining. This stage combines context scaling and comprehensive robustness optimization. Context scaling extends sequence lengths to 8,192 tokens through configurations like mldr-8192 for long documents using multilingual MLDR datasets, jinacrosslingual for enhanced multilingual capabilities, and biomed for medical domains. Simultaneously, cross-system hard negative mining ensures robustness through specialized optimizations including jina-en-v2 for English performance, miracl-v2 for multilingual retrieval, cornstack-v2 for code understanding, and context-chunk-v3 for long-document processing. Training systematically mines hard negatives across multiple retrieval systems including BGE, Jina, GTE, and E5-Large with up to 25 negatives per query and very low temperature of 0.05, using key datasets including MSMARCO Campos et al. [2016], mMARCO Bonifacio et al. [2021], and domain-specific synthetic question-answer pairs. Stage 3: Model Ensemble and Optimization. The final stage combines multiple specialized models trained in previous stages through linear model merging. Each domain-specific model contributes weighted expertise, with merge weights ranging from 0.25 to 0.65 based on domain importance and performance. This approach enables the final model to leverage diverse domain knowledge while maintaining architectural efficiency. Detailed hyperparameter evolution across stages demonstrates multi-objective optimization with stage-tailored configurations (see Appendix 6). Foundation stages use aggressive learning rates of 5e-5 with substantial negative sampling of 15 negatives. Context scaling stages reduce batch sizes dramatically from 60 to 6 to accommodate 8K sequences while employing conservative learning rates of 6e-6. Loss weight adaptation varies across different domain specializations, with dispersive loss typically set to 0.45, dual-matching loss ranging from 0.65 to 0.85, and similarity loss stabilizing around 0.75 to 0.85 depending on the specific domain requirements."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate jina-reranker-v3 across diverse multilingual retrieval benchmarks to demonstrate its architectural innovations and training efficiency. Our comprehensive assessment reveals how the dual embedding architecture with contextual late interaction enables superior performance density, achieving state-of-the-art results with remarkable parameter efficiency. 1During training, the special token <query_emb> is inserted at the end of the query at the beginning of the input sequence. 5 5.1 Experimental Setup Our evaluation spans four challenging benchmarks that test different aspects of ranking capability. BEIR Thakur et al. [2021] represents the gold standard for English retrieval evaluation, encompassing 13 heterogeneous tasks from question answering on Natural Questions to fact verification on FEVER, testing the models ability to generalize across domains without task-specific optimization. MIRACL Zhang et al. [2023] pushes multilingual boundaries with 18 languages spanning diverse linguistic families, from Arabic and Chinese to Finnish and Thai, requiring deep cross-lingual understanding. MKQA Longpre et al. [2021] specifically challenges cross-lingual question answering capabilities, while CoIR Li et al. [2025] focuses on the specialized domain of code retrieval, where semantic understanding of programming constructs becomes crucial. We establish rigorous baselines spanning both retrieval approaches. First-stage dense retrievers include jina-embeddings-v3, providing the foundation top-100 candidates that all rerankers process. Second-stage rerankers encompass our previous jina-reranker-v2, the multilingual bge-reranker-v2-m3, the mxbai-rerank variants at different scales, and Qwen3-Reranker-0.6B and Qwen3-Reranker-4B models. This comprehensive comparison enables analysis across both architectural approaches and parameter scaling effects. All evaluations follow identical protocols using nDCG@10 as the primary metric, with our own controlled runs ensuring fair comparison. The consistent use of jina-embeddings-v3 retrieval candidates eliminates retrieval variance and isolates reranking effectiveness. 5.2 Overall Performance Across Benchmarks Table 2 demonstrates jina-reranker-v3s exceptional performance density across diverse evaluation scenarios. On BEIR, our model achieves the highest score among all rerankers at 61.94, establishing new state-of-the-art performance for English retrieval. This represents 4.88% improvement over our previous jina-reranker-v2 at 57.06, directly attributable to our contextual late interaction architecture where query and document embeddings are extracted from shared forward passes rather than separate encoding pipelines. Parameter efficiency analysis reveals striking advantages compared to larger alternatives. Against the 1.5B parameter mxbai-rerank-large-v2, jina-reranker-v3 achieves superior BEIR performance with 61.94 versus 61.44 using 2.5 fewer parameters, while providing specialized domain coverage unavailable in competing models reaching 63.28 on CoIR. This efficiency derives from architectural innovations: Qwen3s optimized transformer backbone combined with our specialized 256-dimensional projector network that concentrates ranking signals without requiring massive parameter scaling. Multilingual evaluation reveals strong cross-lingual capabilities despite the models compact architecture. The 66.50 score on MIRACL, while 2.82 points below the multilingual-specialized bge-reranker-v2-m3 at 69.32, demonstrates effective knowledge transfer from our progressive training methodology. The 67.84 MKQA performance closely approaches jina-reranker-v2s 67.90, indicating that architectural sophistication can partially offset parameter differences in multilingual scenarios. 5.3 English Retrieval Performance on BEIR Table 3 provides granular analysis across BEIRs heterogeneous tasks, revealing specific architectural advantages. The model achieves consistent excellence across diverse reasoning tasks, with particularly strong performance on complex multi-hop reasoning reaching 78.56 on HotpotQA and fact verification achieving 93.95 on FEVER. These results highlight how contextual late interaction enables sophisticated query-document self-attention during encoding, capturing evidence relationships that separate encoding approaches miss. Within the same scale category, jina-reranker-v3 reveals significant advantages. Against bge-reranker-v2-m3 with the same 0.6B parameters, jina-reranker-v3 delivers substantial 5.43% improvement from 56.51 to 61.94, demonstrating architectural innovation over simple parameter scaling. The specialized 256-dimensional projector network effectively concentrates ranking signals while preserving contextual representations from the Qwen3 backbone. Remark6 Models # Param BEIR MIRACL MKQA CoIR jina-embeddings-v3 jina-embeddings-v2-base-code 0.5B - 55.81 - First-stage Retriever jina-reranker-v3 jina-reranker-v2 jina-reranker-m0 bge-reranker-v2-m3 mxbai-rerank-base-v2 mxbai-rerank-large-v2 Qwen3-Reranker-0.6B Qwen3-Reranker-4B Second-stage Reranker 0.6B 0.3B 2.4B 0.6B 0.5B 1.5B 0.6B 4.0B 61.94 57.06 58.95 56.51 58.40 61.44 56.28 61.16 58.90 - 66.50 63.65 66.75 69.32 55.32 57.94 57.70 67. 65.63 - 67.84 67.90 68.19 67.88 64.24 67.06 65.34 - - 56.90 63.28 56.14 63.55 35.97 - - - - Table 2: Evaluation results (nDCG@10 in percent) for reranking models. All scores are our runs based on the retrieval top-100 results from the first row. ably, our model surpasses mxbai-rerank-large-v2s 61.44 performance while using 2.5 fewer parameters, establishing that sophisticated architecture can surpass brute-force scaling approaches. Since jina-reranker-v3 processes all documents simultaneously in listwise manner within shared context windows, we investigate the sensitivity to document ordering. We evaluate three variants: documents ordered by descending relevance scores (D), ascending scores (A), and random permutation (R). The results show modest variations across orderings, with random ordering (R) achieving the highest average of 62.54, followed by descending (D) at 61.94 and ascending (A) at 61.52. While the differences are not conclusive, this analysis reveals that our contextual late interaction architecture maintains relatively stable performance across different input orderings, suggesting robust self-attention mechanisms that can effectively process documents regardless of their initial arrangement. The models dominance extends particularly to question-answering scenarios, where Natural Questions achieves 74.30 and argumentative retrieval on ArguAna reaches 77.39, showcasing the benefit of contextual embeddings. These tasks require understanding complex query intent and matching it against nuanced document semantics, precisely the scenario where our dual embedding extraction approach provides maximum advantage over traditional cross-encoder scoring. Models Size Avg. TC NFC NQ HQA FQA AA TCH DBP SD FVR CFV SF QRA First-stage Retriever jina-embeddings-v3 0.5B 55.81 77.81 36.65 64.31 64.63 47.47 54.31 26.55 41.07 19.91 89.00 42.33 72.4 89.06 Second-stage Reranker 0.6B 0.6B 0.6B 2.4B 0.6B 0.3B 0.6B 0.5B 1.5B 0.6B 4.0B 61.94 61.52 62.54 58.95 54.49 57.06 56.51 58.40 61.44 56.28 61. jina-reranker-v3 (D) jina-reranker-v3 (A) jina-reranker-v3 (R) jina-reranker-m0 jina-colbert-v2 jina-reranker-v2 bge-reranker-v2-m3 mxbai-rerank-base-v2 mxbai-rerank-large-v2 Qwen3-Reranker-0.6B Qwen3-Reranker-4B 84.32 37.59 74.30 78.56 48.90 77.39 32.11 47.70 23.08 93.95 40.77 75.98 90.60 85.78 39.53 72.50 77.39 51.15 69.09 31.46 48.14 23.90 93.40 40.83 76.53 90.10 86.39 39.66 73.14 77.91 51.79 76.02 31.29 48.22 24.27 93.69 42.83 77.55 90.31 84.17 41.03 72.25 76.99 51.62 40.69 31.79 49.34 22.91 91.14 36.42 79.94 88.01 81.94 35.88 66.01 74.36 43.62 35.46 29.11 47.14 19.40 87.92 29.20 70.13 88.25 80.53 37.17 67.39 76.17 46.48 39.28 32.35 47.81 20.03 93.02 37.17 76.50 87.83 82.19 34.33 69.52 77.89 45.45 36.21 33.12 46.72 17.79 91.03 38.69 72.64 89.10 82.75 37.57 67.74 77.35 47.33 47.33 30.71 48.00 18.09 93.30 42.93 77.76 88.33 81.51 37.76 72.46 78.10 52.75 74.55 29.81 49.07 18.58 93.94 42.03 78.86 89.36 87.08 38.37 56.54 74.41 43.45 56.53 27.26 43.54 20.98 86.19 44.11 74.89 78.32 87.08 41.56 69.06 77.03 52.29 58.82 33.73 50.81 26.01 87.80 47.59 78.41 84.83 Table 3: Performances of different rerankers (nDCG@10 in %) on BEIR. Top-100 retrieval results from jina-embeddings-v3 are passed as input. The best results are marked in bold. Avg. represents the averaged result of the 13 BEIR datasets. For jina-reranker-v3, (D)/(A)/(R) denote document ordering variants: Descending, Ascending, and Random relevance score ordering, respectively. 7 5.4 Multilingual Performance on MIRACL MIRACL evaluation across 18 diverse languages demonstrates jina-reranker-v3s cross-lingual consistency despite its compact architecture. The 66.50 average performance reveals sophisticated multilingual understanding, with particularly strong results in morphologically complex languages like Arabic achieving 78.69 and challenging contexts like Thai reaching 81.06. These results reflect the effectiveness of our progressive multilingual training strategy, where architectural advantages help compensate for reduced multilingual specialization. Perhaps most significantly, jina-reranker-v3 exhibits minimal performance degradation across linguistic families, from Indo-European languages like Russian at 65.20 to Sino-Tibetan languages like Thai at 81.06. This consistency stems from our progressive multilingual training strategy that incorporates diverse datasets including MIRACL, mMARCO, and domain-specific multilingual corpora during the three-stage training progression. The architectural advantage becomes particularly evident in Korean achieving 73.83, where the models contextual late interaction enables effective handling of complex agglutinative morphology that traditional cross-encoders struggle to process efficiently. Compared to bge-reranker-v2-m3s dedicated multilingual optimization averaging 69.32, jina-reranker-v3 accepts 2.82-point gap while achieving superior English performance and maintaining architectural efficiency. This trade-off reflects our design philosophy: contextual embedding extraction provides competitive multilingual capabilities without massive multilingual scaling, creating an optimal balance for applications requiring both English excellence and cross-lingual competency. Models Avg. AR BN EN ES FA FI FR HI ID JA KO RU SW TE TH ZH DE YO First-stage Retriever jina-embeddings-v 58.90 71.53 69.86 48.37 46.91 54.13 71.15 50.90 55.05 47.83 56.46 64.76 55.63 54.07 70.48 73.56 55.29 49.18 65.01 Second-stage Reranker 66.50 78.69 79.28 59.46 53.69 56.91 75.54 55.58 60.71 57.48 65.80 73.83 65.20 64.09 74.46 81.06 65.30 56.55 73.31 jina-reranker-v3 66.75 79.78 78.01 59.21 53.55 58.90 70.00 56.66 62.83 54.92 66.51 72.86 67.37 59.04 70.19 80.37 64.51 58.50 80.44 jina-reranker-m0 63.65 72.50 79.42 46.66 51.54 57.81 73.05 50.90 60.94 56.66 59.15 72.60 53.43 66.47 74.62 77.75 62.49 53.06 76.69 jina-reranker-v2 69.32 80.51 81.85 57.67 57.64 61.92 80.38 59.60 67.66 58.86 67.37 75.14 67.61 68.92 76.69 82.29 64.46 58.32 80.85 bge-reranker-v2-m3 mxbai-rerank-base-v2 55.32 71.08 58.21 56.61 48.89 46.59 64.92 50.47 44.75 49.48 57.99 64.88 54.16 48.40 55.15 72.71 58.44 20.33 72.66 mxbai-rerank-large-v2 57.94 71.38 63.48 57.55 49.14 48.38 66.70 51.61 45.12 49.05 56.61 64.98 54.80 51.79 62.41 74.51 62.29 38.66 74.42 56.16 67.44 66.67 50.91 45.77 52.07 65.50 43.28 60.36 49.66 51.56 61.03 48.88 46.72 69.86 72.95 45.14 43.00 70.04 Qwen3-Reranker-0.6B 67.52 78.32 81.51 59.37 53.07 61.63 78.70 55.02 68.71 54.90 65.32 71.80 64.66 66.50 75.60 82.00 59.35 57.56 81.39 Qwen3-Reranker-4B Table 4: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10)."
        },
        {
            "title": "6 Conclusion",
            "content": "We present jina-reranker-v3, 0.6B parameter multilingual document reranker that introduces last but not late interaction for efficient document reranking. Our approach enables cross-document interactions during encoding by processing queries and multiple documents simultaneously within shared context windows, then extracting contextual embeddings from designated special token positions. By effectively adapting modern long-context reranking, jina-reranker-v3 demonstrates that discriminative methods can bridge the efficiency-effectiveness gap while maintaining significant computational advantages over generative alternatives. The results show strong performance across diverse benchmarks, with particular excellence in complex retrieval tasks and multilingual scenarios. language models for document"
        },
        {
            "title": "References",
            "content": "Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, and Adam Jatowt. DeAR: DualarXiv preprint Stage Document Reranking with Reasoning Agents via LLM Distillation. arXiv:2508.16998, 2025. URL https://arxiv.org/abs/2508.16998. Accepted at EMNLP Findings 2025. 8 L. Bonifacio, Israel Campiotti, R. Lotufo, and Rodrigo Nogueira. mmarco: multilingual version of the ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897, 2021. URL https: //arxiv.org/abs/2108.13897. Sebastian Bruch, C. Lucchese, and F. M. Nardini. Efficient and effective tree-based and neural learning to rank. Foundations and Trends in Information Retrieval, 2023. doi: 10.1561/1500000071. Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. Learning to Rank Using Gradient Descent. In Proceedings of the 22nd International Conference on Machine Learning, pages 8996, 2005. doi: 10.1145/1102351.1102363. URL https://doi.org/10.1145/1102351.1102363. Yuzheng Cai, Yanzhao Zhang, Dingkun Long, Mingxin Li, Pengjun Xie, and Weiguo Zheng. ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking. arXiv preprint arXiv:2509.00520, 2025. URL https://arxiv.org/abs/2509. 00520. Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, L. Deng, and Bhaskar Mitra. Ms marco: human generated machine reading comprehension dataset. In CoCo@NIPS, 2016. Antoine Chaffin and Raphael Sourty. Pylate: Flexible training and retrieval for late interaction models. arXiv.org, 2025. doi: 10.48550/arXiv.2508.03555. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. Hervé Déjean, Stéphane Clinchant, and Thibault Formal. Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE. arXiv preprint arXiv:2403.10407, 2024. URL https: //doi.org/10.48550/arXiv.2403.10407. Michael Günther, Isabelle Mohr, Bo Wang, and Han Xiao. Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models. arXiv preprint arXiv:2409.04701, 2024. URL https://arxiv.org/abs/2409.04701. Submitted to ICLR 2025. Xiang Huang, Hao Peng, Dongcheng Zou, Zhiwei Liu, Jianxin Li, Kay Liu, Jia Wu, Jianlin Su, and Philip S. Yu. Cosent: Consistent sentence embedding via similarity ranking. IEEE/ACM Transactions on Audio Speech and Language Processing, 2024. doi: 10.1109/TASLP.2024. 3402087. Ziwei Ji, Himanshu Jain, Andreas Veit, Sashank J. Reddi, Sadeep Jayasumana, Ankit Singh Rawat, Aditya Krishna Menon, Felix X. Yu, and Sanjiv Kumar. Efficient Document Ranking with Learnable Late Interactions. arXiv preprint arXiv:2406.17968, 2024. URL https://arxiv.org/ abs/2406.17968. Omar Khattab and Matei Zaharia. ColBERT: Efficient and Effective Passage Search via ContexIn Proceedings of the 43rd International ACM SIGIR tualized Late Interaction over BERT. Conference on Research and Development in Information Retrieval, pages 3948, 2020. doi: 10.1145/3397271.3401075. URL https://arxiv.org/abs/2004.12832. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, and Ruiming Tang. Coir: comprehensive benchmark for code information retrieval models. In ACL, pages 2207422091, 2025. URL https://aclanthology.org/2025.acl-long.1072/. Qi Liu, Gang Guo, Jiaxin Mao, Zhicheng Dou, Ji-Rong Wen, Hao Jiang, Xinyu Zhang, and Zhao Cao. An analysis on matching mechanisms and token pruning for late-interaction models. ACM Trans. Inf. Syst., 2024. doi: 10.1145/3639818. Shayne Longpre, Yi Lu, and Joachim Daiber. Mkqa: linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:13891406, 2021. doi: 10.1162/TACL_A_00433. URL https://arxiv.org/ abs/2007.15207. 9 Rodrigo Nogueira and Kyunghyun Cho. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. arXiv preprint arXiv:2309.15088, 2023. URL https://arxiv.org/abs/2309.15088. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1305313074, 2023. URL https://arxiv.org/abs/2306.17563. Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xinyu Zhang, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models. arXiv preprint arXiv:2406.14848, 2024. URL https://arxiv.org/abs/2406.14848. Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md. Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. FIRST: Faster Improved Listwise Reranking with Single Token Decoding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 86428652, 2024. doi: 10.18653/v1/2024.emnlp-main.491. URL https://arxiv.org/abs/2406.15657. Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, and Heng Ji. Cornstack: High-quality contrastive data for better code retrieval and reranking, 2025. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In NeurIPS Datasets and Benchmarks, 2021. URL https://arxiv.org/abs/2104.08663. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. arXiv preprint arXiv:1807.03748, 2019. URL https://arxiv.org/abs/ 1807.03748. Lu Wang, Chao Du, Pu Zhao, Chuan Luo, Zhangchi Zhu, Bo Qiao, Wei Zhang, Qingwei Lin, S. Rajmohan, Dongmei Zhang, and Qi Zhang. Contrastive learning with negative sampling correction. arXiv preprint arXiv:2401.08690, 2024. doi: 10.48550/arXiv.2401.08690. URL https://arxiv.org/abs/2401.08690. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions, 2024. Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May D. Wang, Joyce C. Ho, Chao Zhang, and Carl Yang. Bmretriever: Tuning large language models as better biomedical text retrievers. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. An Yang, Anfeng Li, Baosong Yang, et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. Xinyu Crystina Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David AlfonsoHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy J. Lin. Miracl: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 2023. doi: 10.1162/tacl_a_00595. Yanzhao Zhang, Dingkun Long, Guangwei Xu, and Pengjun Xie. Jina-ColBERT-v2: GeneralPurpose Multilingual Late Interaction Retriever. arXiv preprint arXiv:2408.16672, 2024. URL https://arxiv.org/abs/2408.16672. Yanzhao Zhang, Mingxin Li, Dingkun Long, Guangwei Xu, and Pengjun Xie. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176, 2025. URL https://arxiv.org/abs/2506.05176."
        },
        {
            "title": "A Model Configuration and Training Details",
            "content": "10 Parameter Total Parameters Non-Embedding Parameters Hidden Size Number of Layers Attention Heads (Q/KV) Context Length Effective Sequence Length Projector Architecture Projector Activation Value 0.6B 0.44B 1,024 28 16/8 (GQA) 131,072 8,192 1024512256 ReLU Table 5: Model architecture configuration for jina-reranker-v3. Hyperparameter Stage 1 Stage 2 Stage 3 Foundation Context & Hard Mining Model Ensemble Learning Rate Batch Size (per device) Max Sequence Length Max Query Length Max Doc Length Number of Negatives In-batch Negatives Temperature Training Mode LoRA Rank Word Embeddings Backbone Dispersive Loss α Dual Matching α Similarity Loss α 5e-5 60 [768, 2048] - - 15 3 0.25 LoRA 16 Tuned Frozen 0.45 0.85 0.85 [5e-5, 6e-6] [6, 60] [2048, 8192] [256, 512] [512, 2048] [9, 25] [0, 3] [0.05, 0.25] LoRA/Full 16 Frozen/Tuned Frozen/Tuned [0.25, 0.45] [0.65, 0.85] [0.75, 0.85] - - - - - - - - Linear Merging - - - - - - Table 6: Multi-stage supervised fine-tuning hyperparameters showing ranges across 47 training configurations."
        }
    ],
    "affiliations": [
        "Jina AI GmbH",
        "University of Pittsburgh"
    ]
}