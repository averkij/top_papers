{
    "paper_title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning",
    "authors": [
        "Liang Hu",
        "Jianpeng Jiao",
        "Jiashuo Liu",
        "Yanle Ren",
        "Zhoufutu Wen",
        "Kaiyuan Zhang",
        "Xuanliang Zhang",
        "Xiang Gao",
        "Tianci He",
        "Fei Hu",
        "Yali Liao",
        "Zaiyuan Wang",
        "Chenghao Yang",
        "Qianyu Yang",
        "Mingren Yin",
        "Zhiyuan Zeng",
        "Ge Zhang",
        "Xinyi Zhang",
        "Xiying Zhao",
        "Zhenwei Zhu",
        "Hongseok Namkoong",
        "Wenhao Huang",
        "Yuwen Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is a particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and time-sensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation -- closely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement a rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly.By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers a professional, high-difficulty testbed for complex financial search and reasoning."
        },
        {
            "title": "Start",
            "content": "FinSearchComp: Towards Realistic, Expert-Level Evaluation of Financial Search and Reasoning 1ByteDance Seed, 2Columbia Business School Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and timesensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasksTime-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigationclosely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly. By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers professional, high-difficulty testbed for complex financial search and reasoning. Github: https://randomtutu.github.io/FinSearchComp/ Huggingface: https://huggingface.co/ByteSeedXpert/FinSearchComp/ 5 2 0 2 6 1 ] . [ 1 0 6 1 3 1 . 9 0 5 2 : r Figure 1 The performance of web-based products on the global subset (left) and the Greater China subset (right) of FinSearchComp. Note that the performance of human experts is 75.0 and 88.3 on the Global and Greater China subsets, respectively."
        },
        {
            "title": "Introduction",
            "content": "Search is cornerstone capability for intelligent systems: beyond static knowledge recall, many real-world reasoning problems require agents to acquire, validate, and integrate information from diverse and timesensitive sources. Effective decision making hinges on identifying relevant signals, reconciling conflicting evidence, and synthesizing heterogeneous data into coherent judgment. These processes represent the core intellectual skills that large language model (LLM) agents must masterinformation gathering, coordination, and grounded reasoning. Yet current benchmarks provide only partial view of these skills. General-purpose browsing benchmarks such as BrowseComp [23] evaluate whether agents can persist through multi-step navigation to uncover obscure facts with short, easily verifiable answers. By design, they avoid long-form synthesis and ambiguity resolution and do not assess integration of domain knowledge or multi-source evidence, capabilities that are essential for domain-intensive decision support. As result, they fall short of capturing the multi-step evidence acquisition and reconciliation required in high-stakes, decision-relevant settings. natural question, then, is what environment best reveals whether agents truly possess these skills. Finance provides uniquely demanding answer. Analysts, regulators, and investors routinely engage in searches that combine real-time signals (e.g., market prices, exchange rates) with structured historical disclosures (e.g., annual filings, quarterly reports) and unstructured context (e.g., news events, commentaries). These tasks are time-sensitive, domain-grounded, and decision-relevant: errors in freshness, unit alignment, or source reconciliation directly affect valuations, risk management, and compliance [9, 13, 17]. Crucially, financial search is not only high-value application but also stress test of general cognitive skills for LLM-based agents. For example, checking the latest close price of IBM requires rapid retrieval and verification under freshness constraints; retrieving Starbucks total assets as of September 2020 requires timestamping and accounting alignment; identifying the month since 2010 with the largest single-month increase in the S&P 500 requires multi-period synthesis, provenance reconciliation, and error-tolerant reasoning. These examples illustrate how financial search combines timeliness, precision, and evidence integration, making it natural proving ground for assessing whether LLMs can support realistic, high-stakes decision making. To address this need, we introduce FinSearchComp, the first open-domain benchmark for realistic, analyst-style financial search, comprising 635 questions that require time-sensitive acquisition and multi-source evidence integration. To mirror the day-to-day needs of professional analysts, we design three task familiesTimeSensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation  (Table 2)  . These tasks correspond to core analyst workflows. Time-Sensitive Data Fetching tasks capture situations where the correct response changes rapidly (e.g., stock prices, exchange rates, and gold prices), emphasizing rapid retrieval and verification under tight time constraints. Simple Historical Lookup tasks reflect frequent point-in-time lookups. For example, an analyst may ask What was Apples iPhone revenue in Q4 2018? or How many employees did Google report in 2015? These questions require going back to the correct historical disclosure and aligning with the companys reporting calendar. Complex Historical Investigation tasks involve building multi-period views that integrate different sources. For instance, Over the past ten years, which quarter showed the fastest growth in Teslas vehicle deliveries? or Across 20202022, how did Microsofts cloud revenue trend compared to Amazons? Such queries demand stitching together multiple reports, checking consistency across sources, and ensuring that values are comparable across time. During construction, we engaged 70 professional financial experts for data annotation, conducted multi-stage verification of prompts and reference answers, and specified detailed, rubric-based scoring guidelines. To account for differences in data sources and reporting conventions, FinSearchComp covers two subsets: Global and Greater China. Further, since answers of different LLMs may have different formats and exhibit legitimate minor fluctuations (e.g., revisions or rounding across various sources), we adopt an LLM-based evaluation with rubric-guided judging and explicit tolerance bands, complemented by expert spot checks to ensure the overall correctness. We evaluate 21 models (products) on FinSearchComp, spanning web-enabled products and API endpoints. As shown in Figure 1, on the Global subset Grok 4 (web) attains the highest overall score (68.9%), outperforming the runner-up, GPT-5-Thinking (web), by 5.0 percentage points (pp), yet still trailing human experts by 6.1 pp. On the Greater China subset, Chinese models perform markedly better: DouBao (web) leads the leaderboard, followed closely by YuanBao-DeepSeek-R1 (web) and Grok 4 (web); nevertheless, all remain Benchmark FinQA [5] ConvFinQA [6] FLUE [20] MultiFinBen [18] FinanceQA [15] BizFinBench [14] FinEval [11] CPA-QKA [12] Finance Agent Benchmark [3] FinSearchComp (Ours) Open-domain Tool Time-sensitive Use Search Data End-to-end Holistic Agent Evaluation Evaluation Table 1 Comparison of FinSearchComp with existing financial benchmarks. more than 34 pp below human performance. Further analyses indicate that equipping agents with web search capabilities and financial plugins improves their performance on FinSearchComp. case study further reveals that poor performance often stems from insufficient search depth and the retrieval of outdated information. Our contributions are as follows: 1. We introduce FinSearchComp, the first fully open-source, end-to-end agent benchmark for open-domain financial data search. FinSearchComp comprises 635 expert-curated queries spanning global and Greater China markets and three analyst-style task families (Time-Sensitive, Simple Historical, Complex Historical), with multi-stage quality control. 2. We release carefully curated benchmark dataset with deterministic gold answers and an fully open-source evaluation harness. 3. We conduct comprehensive study of 21 models (web-enabled products and API endpoints), showing that equipping agents with web search and financial plugins consistently improves performance. Our analysis further identifies recurring failure modes: shallow search, stale or mis-timestamped evidence, cross-unit/currency aggregation, and report-calendar misalignment, offering concrete targets for future improvement. For example, common failures include neglecting to call specialized data plugins in favor of less reliable web searches, extracting incorrect data from valid source (e.g., confusing opening vs. closing prices), and unnecessarily over-complicating simple queries like market cap into multiple steps. Taken together, FinSearchComp enables us to measure, for the first time, how close LLM agents are to expert-level competence in realistic financial search. Our results show that models such as Grok 4 and GPT-5-Thinking can already approach human accuracy in certain subsets, demonstrating the remarkable progress of web-enabled LLMs. At the same time, persistent gaps in freshness awareness, multi-source reconciliation, and temporal reasoning indicate that current systems remain fragile when confronted with the full complexity of analyst-style tasks. In this way, FinSearchComp not only benchmarks performance in critical domain, but also highlights the broader aspects of intelligence that todays LLMs are beginning to approximatewhile still falling short of the robustness, adaptability, and judgment required for reliable decision support. Related Works. Recently, many benchmarks are proposed for evaluating browsing capabilities, while they fall short along two axes: (i ) General-purpose browsing benchmarks, like BrowseComp [23], BrowseComp-ZH [27], and BrowseComp-Plus [7], are intentionally domain-agnostic and center on lookup-oriented tasks with short, verifiable targets. They optimize for findability rather than analysis: temporal validity, unit/denomination normalization, reporting-calendar alignment (e.g., TTM vs. FY vs. quarterly1), and provenance reconciliation across sources are not required, leaving them weakly diagnostic for finance-grade decision support. (ii ) Financial QA-style benchmarks (e.g., FinQA [5], FinanceQA [15]) pre-collect relevant context and bypass open-domain search and tool use, thereby under-assessing agents search competence and diverging from analyst workflows [22, 25] (see Table 1). The Finance Agent Benchmark [3] offers an end-to-end evaluation but is confined to 1Trailing Twelve Months, Fiscal Year, and Quarterly self-constructed system, which is base model augmented with retrieval module. We advocate for holistic evaluation that assesses the performance of web-based products. The unrestricted evaluation, which allows any search tool or source, better reflects the models practical utility. Moreover, the benchmarks use of only historical data permits success via memorization, not necessarily real-time information retrieval."
        },
        {
            "title": "2 FinSearchComp",
            "content": "We begin by outlining our design principles and choices. We then describe FinSearchComp construction, quality-control measures, and some descriptive statistics."
        },
        {
            "title": "2.1 Design Principles",
            "content": "Before detailing FinSearchComp, we set out the desiderata for high-quality financial search benchmark and explain how FinSearchComp addresses each. 1. Task professionalism & diversity. Financial data retrieval encompasses diverse task types that vary significantly in complexity and time sensitivity. These range from real-time market data queries requiring immediate responses to complex multi-period analytical investigations spanning historical datasets. Given the intricate nature of financial metricswith nuanced distinctions in reporting standards, calculation methodologies (TTM/FY), and temporal specifications,task design and validation require careful oversight by domain experts to ensure professional accuracy and relevance. Our design: Engaging with various professional financial analysts, we carefully design three tasks that mirror analysts daily workflow, namely time-sensitive data fetching, simple historical lookup, and complex historical investigation. Details are demonstrated in Section 2.2. 2. High quality of questions. Financial figures vary across sources, vintages, and definitions; ambiguous prompts can admit multiple correct answers. Without precise definitions, provenance, and reproducible grading criteria (including tolerance for legitimate minor drift due to rounding/revisions), scores reflect dataset noise rather than model ability, harming reliability and fairness. High-quality items with unambiguous targets and auditable references are therefore prerequisite for credible evaluation. Our design: We perform set of quality-control processes to ensure the quality of each question, including reliable data source selection, mitigating ambiguity, and multi-expert answer verification. By integrating professional financial expertise. Details are in Section 2.4. 3. Broad market coverage. External validity in finance depends on robustness across markets, languages, and regulatory/reporting conventions. Cross-market coverage stresses generalization under heterogeneous tickers, filing formats, calendars/time zones, and currency/denomination regimes, and surfaces failure modes that single-market tests systematically miss. This breadth is essential to assess readiness for real-world deployment rather than narrow sandbox. Our design: FinSearchComp comprises two subsets, Global (Western markets) and Greater China, with questions in both English and Chinese; see Section 2.5 for details. To enable fair cross-market comparison, we mirror task templates across subsets and balance entity coverage by sector and size. Bilingual questions are involved to support cross-lingual evaluation."
        },
        {
            "title": "2.2 Task Design of FinSearchComp",
            "content": "We define three task types aligned with daily analyst workflows. Each requires at least one external tool call, and each question has single, fully objective answer. These tasks test core search & reasoning skills that are not only central to financial analysis but also broadly important to knowledge work in many disciplines, such as journalism, policy research, and scientific data analysis. In all domains, workers must fetch fresh information, verify point-in-time facts, and synthesize evidence across long horizons before drawing conclusions. T1 Time-Sensitive Data Fetching. This task type asks for data that changes daily or intraday, such as the latest close, new filing, or guidance update. It fits trading, monitoring, and event reactions where decisions depend on the newest number. Example: What was Nvidias closing price yesterday? Latest 4 Table 2 Examples of the three tasks in FinSearchComp, with retrieval depth, temporal span, reasoning complexity, and typical data types. Task Example T1. Time Sensitive Data Fetching T2. Simple Historical Lookup T3. Complex Historical Investigation IBM latest close price. Obtained from real-time query of IBM What was the total assets of Starbucks as of September 27, 2020? (Answer: $29374.5 million, rounding errors allowed.) From Jan 2010 to Apr 2025, in which month did the S&P 500 index experience the largest single-month increase? (Answer: Apr 2020 (12.68%), error 0.1% allowed.) Retrieval Depth Temporal Span Reasoning Complexity Typical Data Types / Examples 1 1 day Easy 1 day Medium >1 184 months Hard Stock prices, FX rates, gold prices (real-time quotes) YoY (Year-over-Year), HoH (Half-on-Half), TTM (Trailing Twelve Months), FY (Fiscal Year), quarterly reports Multi-period currency/unit malization, action data provenance views, norcorporate adjustments, change in Dow Jones Industrial index (based on the closing price of current and previous trading day) This type stresses freshness management, calendar handling, ticker aliasing, and conflict resolution across sources. Similar challenges arise in real-time journalism, monitoring policy updates, or tracking clinical trial results. T2 Simple Historical Lookup. This task type asks for fixed point fact, such as an issuers FY2024 R&D expense2 or TTM revenue3 on given date. It supports baselining, YoY4 or HoH5 comparisons, event studies, and backtests that rely on exact values. Example: What was Teslas reported revenue in Q2 2023? The key challenges are aligning reporting conventions (FY, TTM, quarterly), handling restatements, and ensuring unit and currency fidelity. Comparable skills are crucial in policy research (e.g., retrieving census data), medicine (e.g., comparing trial endpoints), or academic meta-analysis. T3 Complex Historical Investigation. This task type asks for multi-period aggregation or synthesis, such as identifying the month with the largest single-month gain for major index over long window. It underpins trend analysis, factor research, valuation comps, and risk monitoring. Example: Over the last 30 years, which month had the steepest decline in the S&P 500? The challenges include retrieving across long horizons, adjusting for corporate actions, normalizing units, and composing multi-step reasoning without error. Such synthesis is equally relevant in climate science (long-horizon weather anomalies), history (identifying peak conflict years), or epidemiology (largest single-month case surges). Detailed comparison is shown in Table 2. Together these task types cover three critical capabilities: freshness management, point-in-time fidelity, and multi-period synthesis. They mirror the actual workflows of analysts, are grounded in real data and conventions, and scale in difficulty from T1 to T3. This progression enables fine-grained error analysis and highlights how benchmarking these skills matters not just for finance, but for intelligence in knowledge work broadly. Time Cost of Financial Analysts on These Three Tasks. Financial analysts worldwide dedicate substantial resources to these core information retrieval activities. There are approximately 370,000 financial professionals in the US (based on Bureau of Labor Statistics) and probably over 1 million globallyincluding equity researchers, portfolio managers, risk analysts, and investment bankerswho regularly perform these tasks as part of their daily workflow. For T1 (Time-Sensitive Data Fetching), analysts typically spend around 1-2 minutes per query for quick references. This task is also commonly performed by non-expert investors in scenarios such as checking the 2R&D expense refers to research and development spending, reported in companys financial statements. FY2024 means fiscal year 2024. 3TTM (Trailing Twelve Months) revenue is the sum of revenue over the most recent 12-month period. 4YoY (Year-over-Year) compares financial metric with the same period in the previous year. 5HoH (Half-over-Half) compares metric with the previous half-year period. 5 Figure 2 The overview of the construction process. The construction of this benchmark involves three distinct tasks. The data for each task originate from different sources and undergo separate processing pipelines. uniform quality control procedure is applied across all tasks. current price of their investments. For T2 (Simple Historical Lookup) represents the most frequent task type, with individual analysts performing 10-30 such queries daily for financial analysis, peer comparisons, and modeling. Each lookup averages 5-10 minutes, accounting for data validation (faster for standard financials that can be retrieved from filings, slower for less common financial data such as operational, macroeconomic, and industry data). For T3 (Complex Historical Investigation) demands the highest time investment per-query, often requiring 15-60 minutes for comprehensive data retrieval and calculation, based on the complexity of the data retrieval and calculation steps. While less frequent (fewer than 20 queries per analyst monthly), these investigations and calculations are important building blocks in financial analysis and reports. While standardized templates and automated tools already exist to facilitate these taskssuch as comparable company analysis frameworks that can be efficiently updatedapproximately half of these information retrieval activities still inevitably require manual data collection and custom analytical framework development. If AI models could accurately perform such tasks, analysts could further automate these processes and significantly enhance overall productivity."
        },
        {
            "title": "2.3 Construction of FinSearchComp",
            "content": "To accommodate the unique characteristics of different tasks, we employ variety of question-and-answer construction strategies to ensure both diversity and quality, as shown in Figure 2 (left side). We now detail the data collection and processing procedures for the three tasks. For Time-Sensitive Data Fetching. Time sensitive data includes real-time stock prices, index levels, exchange rate, metal prices etc. Some of the example questions include: Latest closing price of Bloom Energy(NYSE) The latest opening price of Starbucks 6 The latest percentage change of Rivian (NASDAQ), based on the latest closing price and the previous closing price USD/THB price today Latest price of S&P 500 Financial experts first mannually design questions asking for time-sensitive data that can be verified through API. Time-sensitive data changes over time, so we actually prepare the code for API calls that obtain real-time data points for each question. Finally, financial experts check each API result against the real-time data to ensure the retrieved result is correct and on-time, for accurate evaluation. We establish permissible error margins for evaluation based on the specific volatility characteristics of different assets (e.g., equities, forex) For Simple Historical Lookup. Simple historical data includes historical market data (stock price, oil price etc.), corporate financials, macro economic statistics that can be directly obtained from official sources without data processing. Some of the example questions include: What was the additional paid-in capital of Lands End as of the end of the fiscal year 2020? (answer in thousand dollars, rounded to the nearest integer) What was the closing value of the VIX on April 25, 2022? (rounded to two decimal places) For the fiscal year 2023, what was Planet Labs Net cash provided by investing activities? (please answer in thousands of dollars, rounded to the nearest integer) What was the U.S. Housing Market Index (HMI) in November 2014? (answer rounded to the nearest integer) In April 2015, what was the CPI of Russia? (base year 2015=100, rounded to two decimal places) We collect data for this task from two sources to enrich the diversity. (i) Financial experts select documents from official sources (e.g., listed company filings, regulatory authority websites, statistics bureau) and extract key data points to formulate questions and their corresponding answers. (ii) We also design questions using historical data with consistent definitions from professional financial databases. To mitigate the risk of data revisions for certain official statistics (e.g., macroeconomic indicators), we address potential ambiguity by setting reasonable answer range or by explicitly specifying the reference time point in the question. For Complex Historical Investigation. Complex historical data includes financial data that needs to be derived based on multiple historical financial data points, and requires calculation and reasoning to solve. Some of the example questions include: What were the specific dates from January 1, 2020, to December 31, 2024, when London Gold (XAUUSD) dropped by more than $80 in single day? Please list these dates and the corresponding daily drop in USD (rounded to the nearest integer), presented in table sorted by date in ascending order. During April 2025, did the daily changes (compared to the previous day) in the central parity rates of EUR/CNY, HKD/CNY, and USD/CNY always occur in the same direction (i.e., all rates increasing together or all rates decreasing together)? Among the days when these rates did not move in unison, find the exact date when the USD/CNY central parity rate experienced its greatest single increase. List the three central parity rates (EUR/CNY, HKD/CNY, USD/CNY) for that specific day, rounded to four decimal places. Which constituent stock of the Nasdaq 100 Index (NDX), with Price-to-Earnings Trailing Twelve Months (PE-TTM) greater than 0 and less than 20 as of the market close on September 30, 2024, reported the largest operating revenue (consolidated financial statements, in billions of USD) in its 2024 interim report? Please provide the stock name and its operating revenue. Unit: billion USD, rounded to two decimal places. What is the year-over-year change in the proportion of Johnson & Johnsons revenue from international markets (excluding the United States) for each of the past three years (20222024)? Please provide the change in percentage points, rounded to two decimal places. 7 The construction process involves two primary methods. (i) Financial experts design questions based on their real-world professional scenarios and get answers using reliable financial data sources. (ii) Financial experts download tables from verified and reliable financial database and annotate questions based on the tables. Furthermore, financial experts screen and refine these candidates, finalizing 2 to 5 high-quality questions on each table and recording their definitive answers."
        },
        {
            "title": "2.4 Quality Control",
            "content": "To ensure FinSearchComps quality, we implemented rigorous quality-control process throughout construction and summarize the key measures below. Proficient Financial Experts. Our 70-person expert cohort is comprised of 50-expert annotation panel and 20-expert senior review panel. We assembled the former as distinguished group of 50 financial experts to conduct benchmark annotations. All panel members possess advanced degrees in finance (minimum Masters level) and maintain active professional standing within the financial services industry. Panel selection follows stringent qualification protocol, whereby candidates undergo comprehensive domain-specific assessments to ensure annotation quality and inter-annotator reliability. The latter, our senior review panel, consists of 20 senior financial experts who handle discrepancies arising from blind review. The entire expert cohort includes practitioners from prestigious institutions such as Citadel, J.P. Morgan, Deutsche Bank, Nomura, Citigroup, Credit Suisse, Harvest Fund, Zhongou Asset Management, CITIC Securities, and Huatai Securities. Reliable Data Source Selection. To ensure the authority and accuracy of our data, all the answers are sourced from highly reliable channels, including official filings of listed companies, government and regulatory authority websites, and professional financial databases. We employ multi-source cross-validation method to ensure data reliability and eliminate ambiguity. For instance, we cross-reference data from two different official websites or validate data from professional financial database against an official website. Through this process, we identify that even some widely recognized professional financial databases contain inconsistencies in definitional standards or calculation errors and avoid them. Mitigating Ambiguity. To address ambiguities arising from inconsistent calculation methods for the same metric across different institutions, we avoid questions prone to such variation. For instance, the methodologies for calculating forward-adjusted stock prices can differ significantly across data providers, and the precise definition of Earnings in Price-to-Earnings (PE) Trailing Twelve Months (TTM) ratios often varies. more comprehensive list of such cases is detailed in Appendix A.2. Furthermore, to ensure the unambiguity of our questions, we implement the following measures. We also summarize the detailed guide in Table 4. (i ) We explicitly state definitional standards within the question itself (e.g., specifying Static PE vs. PE TTM, or Nominal GDP vs. Real GDP) and avoid metrics with ambiguous time-points, such as the prices of assets with 24-hour trading cycles. (ii ) We set answers as numerical ranges or define tolerance for precision to accommodate minor discrepancies that may arise from different calculation tools or rounding methods. (iii ) We mitigate risks from data revisions by avoiding, where possible, metrics prone to retrospective adjustments (e.g., GDP, Non-Farm Payrolls). If their inclusion is necessary, we formulate the answer as range. Multi-Expert Answer Verification. The answer verification mechanism utilizes blind review module. After obtaining question and its answer, one or two other financial experts solve the question independently without access to the answer key. If discrepancies arise in the results or if an expert deems question to be ambiguous, senior expert arbitrates the matter. Based on the final judgment, the question or answer will be modified, or the question will be discarded entirely. The dataset construction process encompassed approximately 180 hours of contributions from financial experts and 60 hours from senior financial experts, thereby ensuring comprehensive professional oversight throughout development."
        },
        {
            "title": "2.5 Data of FinSearchComp",
            "content": "In this subsection, we present the statistics of FinSearchComp. We summarize the basic statistics in Figure 3. Most evaluation metrics in FinSearchComp rely on problem-specific, expert-annotated ranges of 8 (a) Distribution of the answer type and evaluation rubrics. (b) Average length of prompts and answers on each subset. Figure 3 Data statistics of FinSearchComp. acceptable values, rather than single ground truth. Furthermore, we illustrate the distribution of topics in Figure 4, where experts annotated the topic of each question. The distribution demonstrates the diversity of FinSearchComp, which covers 10 distinct topics."
        },
        {
            "title": "2.6 Evaluation of FinSearchComp\nEvaluation Protocol. Considering the dynamic nature of answers and the need for numerical tolerance in\nFinSearchComp, we adopt LLM-as-a-Judge [26] for evaluation. The specific evaluation methods for the\nthree tasks are detailed below, with the corresponding prompts provided in the Appendix C.",
            "content": "For Time-Sensitive Data Fetching, we address several challenges: (i ) time lags between the model response and the evaluation, (ii ) potential data latency from some financial APIs, and (iii ) the inability of most APIs to query prices at specific second. To mitigate these time-sensitivity issues, we initiate the evaluation process uniformly after the relevant markets have closed. To ensure fairness and accuracy, we establish differentiated evaluation rubrics based on the characteristics of various asset classes: Mainstream Market Stocks and Indices (e.g., U.S., A-shares, H-shares): Evaluation is conducted during non-trading hours. Only minor discrepancies attributable to rounding are permitted. Other Regional Indices: An answer is considered correct if its value falls within the days high-low price range. Foreign Exchange (FX) Rates: To account for potential discrepancies across different data providers, the valid range for an answer is defined as the high-low range of the day, augmented by an additional buffer. For Simple Historical Lookup and Complex Historical Investigation, which feature static and deterministic answers, we also annotate rubrics, such as predefined error margin. In FinSearchComp, we employ the 0-1 error as the metric. For questions in TimeEvaluation Metrics. Sensitive Data Fetching, we first obtain the real-time answer by executing the API commands. For questions in other tasks, the answers are static. After getting the answer, we adopt the LLM to judge referring to the rubrics. The LLMs assessment is modeled as judgment function, , which maps candidate answer and set of rubrics to binary outcome. Let be the space of all possible answers and be the space of all possible rubric sets. The judgment function : {1, 0} returns 1 if answer satisfies the criteria specified in rubrics R, and 0 otherwise. The final evaluation score, S, is derived directly from the output of this judgment function. We define the score using the indicator function I(), which formally connects the LLMs logical evaluation to numerical score. The score is therefore defined as follows: S(A, R) = (A, R). (1) Evaluation Accuracy. To validate the reliability of LLM-as-a-Judge, we conducted human evaluation on representative subset. For each benchmark dataset, we selected 45 models and verified their complete (a) Topic distribution of Global subset. (b) Topic distribution of Greater China subset. Figure 4 Topic distributions in FinSearchComp. Task Subset Accuracy (%) Time-Sensitive Retrieval Simple Historical Lookup Overseas Domestic Overseas Domestic Complex Historical Investigation Overseas Domestic 91.5 91.7 96.8 95.5 97.4 99.8 Table 3 Accuracy of LLM-as-a-Judge compared with human evaluation. evaluation sets, resulting in roughly 400 instances per dataset. On this sample, the judgments of LLM-as-aJudge reached 95% agreement with human-verified labels, confirming the robustness of our evaluation protocol. Detailed results are summarized in Table 3."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we present the main results for FinSearchComp. We evaluate the performance of 22 mainstream models (products) on FinSearchComp from August 1st to 20th, and to ensure temporal comparability across models, all T1 evaluations were conducted after the official market close (local market time) on each evaluation day; these models are categorized into two groups: Web-based products (12 models): Grok 4 (web) [24], GPT-5-Thinking (web)6, Gemini-2.5-pro (web) [10]7, Qwen3-235B-A22B-2507 (web)8 [19], DeepSeek-R1 (web) [8], DouBao (web) [4], DouBao-Thinking (web)9, YuanBao-HunYuan-T1-Thinking (web)10 [21], YuanBao-DeepSeek-V3 (web)11, YuanBao-DeepSeek-R1 (web)12, Ernie-X1 (web) [2], and Kimi k2 (web) [16]. APIs (9 models): Gemini2.5-pro (API)13, Gemini-2.5-pro (Google Search) (API)14, DouBao (API), 6abbr. GPT-5-T (web) 7abbr. Gemini (web) 8abbr. Qwen3 (web) 9abbr. DouBao-T (web) 10abbr. HunYuan-T1 (web) 11abbr. YuanBao-V3 (web) 12abbr. YuanBao-R1 (web) 13abbr. Gemini (API) 14abbr. Gemini-G (API) 10 DouBao-Thinking (API)15, Qwen3-235B-A22B-2507 (API)16, DeepSeek-R1 (API), HunYuan-T1 (API), Ernie-X1 (API), and Kimi k2 (API). To establish human performance baseline, we engage another 50 financial experts who are not involved in the data construction. These experts perform the benchmark tasks while utilizing search tools. Their average score is taken as the human baseline."
        },
        {
            "title": "3.1 Main Results",
            "content": "We report the overall performance of the strongest model from each provider in Figure 1, with the whole performance in Appendix B. The models form clear performance hierarchy, with sizable gap to human experts remaining. Rankings differ between the global and Greater China subsets, likely reflecting differences in training-corpus coverage, language/domain alignment, and retrieval infrastructure. On the global subset, Grok-4 (web) and GPT-5-Thinking form clear leading tier, with Grok-4 (web) securing the top score and approaching expert-level accuracy. While Gemini-2.5-pro (web) decline when moving from the global to the Greater China subset, Grok 4 (web) remains competitive. On the Greater China subset, DouBao (web) and YuanBao-HunYuan-T1-Thinking (web) are strong on the Greater China subset, though they still trail human experts by substantial margin. Detailed results and analyses are shown in Section 3.2 and Section 4."
        },
        {
            "title": "3.2 Results Across Different Tasks",
            "content": "We show the results of each tasks respectively in Figure 5. Our main findings are as follows. Finding 1. Task difficulty increases from T1 to T3. Across models, performance declines monotonically from T1 (time-sensitive data fetching) to T2 (simple historical lookup) to T3 (complex historical investigation), indicating that our task design mirrors the escalating demands of professional financial-analysis workflows. This pattern demonstrates that we are probing complex search and reasoning: T3 requires multi-hop retrieval across heterogeneous sources and time periods; temporal reasoning (event dating, fiscalcalendar alignment, handling revisions/restatements); fine-grained entity resolution (issuer/ticker/subsidiary/renamed entities); and reconciliation of partial or conflicting evidence, forcing systems to plan, verify, and synthesize rather than merely retrieve. Moreover, success on T2T3 hinges on finance-specific expertise, including interpreting primary filings and disclosures (10-K/10-Q/8-K), earnings releases and footnotes, distinguishing GAAP vs. non-GAAP metrics, and understanding corporate actions (splits, spin-offs, mergers), without which methods commonly fail via stale or misaligned time windows, misread accounting terminology, or incorrect consolidation across corporate structures. Finding 2. US models lead on the global set; Chinese models lead on the Greater China subset. We attribute this pattern mainly to corpus geography (English/SEC/multinational coverage vs. CN/HK/TW disclosures and regulator texts), linguistic and market conventions (domain terminology, tokenization, date and identifier formats) that ease in-region entity resolution, and alignment/recency effects, collectively boosting home-field performance without implying leakage. Finding 3. Despite broad underperformance versus human experts, Grok-4 and GPT-5-Thinking approaches expert-level results on the global subset. On the global subset,The outperformance of Grok-4 (web) and GPT-5-Thinking (web) over other systems becomes more pronounced on more difficult tasks (T1 T2 T3), with its largest margin on T3, indicating it goes beyond retrieval by performing multi-step reasoning, aligning timelines (event dating and fiscal/calendar consistency), and carefully disambiguating entities; on the Greater China subset, while aggregate accuracy still trails experts, Grok 4 (web) attains the top score on the most difficult task (T3), reinforcing that its gains reflect genuine reasoning strength rather than surface-level search."
        },
        {
            "title": "4 Case Study",
            "content": "In this section, we conduct case studies to analyze the performances in detail. 15abbr. DouBao-T (API) 16abbr. Qwen3 (API) 11 (a) Task 1. Time-sensitive data fetching. (b) Task 2. Simple historical data lookup. (c) Task 3. Complex historical investigation Figure 5 The performance of various models across the three tasks on FinSearchComp. Models with 0 scores are all APIs."
        },
        {
            "title": "4.1 How much do search capabilities impact performance on FinSearchComp?\nAs shown in Figure 6, models without search uniformly score 0 on T1, as they cannot retrieve current financial\ndata. Without search, they still obtain non-zero but low scores on T2 and T3; we attribute this to parametric\nmemory from pre-training (e.g., annual reports and statistical-agency releases), which surfaces approximate\nfacts that are often outdated or misaligned, yielding higher error rates. With search enabled, average gains of\n40.8, 29.0, and 8.1 points are observed on T1, T2, and T3, respectively—largest for time-sensitive tasks but\nstill material for complex historical investigations.",
            "content": "These patterns indicate that FinSearchComp stresses complex search and reasoning, where success requires planning multi-step queries, aligning timelines and identifiers across sources, and resolving conflicting evidence. In turn, performance reflects not only access to documents but also the ability to verify, synthesize, and reason."
        },
        {
            "title": "4.2 How do financial plugins improve the performance on top of general search tools?",
            "content": "A comparative analysis of the performance of Deepseek R1 on the DeepSeek and YuanBao web interfaces suggests that the integration of financial plugins on the YuanBao platform significantly enhances performance on certain financial tasks, as shown in Figure 7. For T1, the financial plugin appears particularly advantageous 12 Figure 6 Performance improvement of search-augmented models over models without search capabilities. Figure 7 Average performance change on DeepSeek R1 induced by financial plugins. Figure 8 comparison of US and Chinese models on the ratio of Global to Chinese scores. with 31.9 pp improvement. Standard web search functionalities could yield outdated financial data or fail to retrieve the most current information, potentially causing the model to erroneously report asset prices from previous days. dedicated financial plugin provides direct access to simple and real-time data, which allows the model to generate more accurate summaries and reduces the likelihood of such errors. The performance on T2, also indicates improvement. The ability of financial plugin to access historical datasets, such as the income statements of publicly listed companies, contributes to more robust and informed model output. However, the performance variation arises from both the inherent differences in the search tools employed by each platform and the specific functionalities afforded by the financial plugin on YuanBao. However, the performance of YuanBao-R1 (web) remains suboptimal even when augmented with financial plugins, as it fails to achieve nearly 100% success rate. Therefore, the intrinsic capability of the model is also critical for searching financial data."
        },
        {
            "title": "4.3 How does model origin impact the performance?",
            "content": "We compare the scores of the US models and the Chinese models on the Global and Greater China subsets in Figure 8. We define asset origin ratio = Global subset scores / Greater China subset scores. Higher ratio means better performance on tasks related to global assets than tasks related to Chinese assets, and vice versa. We observe that models from US tend to show stronger data search performance for global assets, while models of Chinese origin appear more proficient with Chinese assets. This pattern suggests underlying differences in search tool integration or core model capabilities between the two groups. Specially, for tasks T1 and T2, US models consistently yield an asset origin ratio exceeding 100%. In contrast, Chinese models register ratio below 100%. However, for task T3, majority of models achieve ratio greater than 100%. This implies that the global asset challenges within T3 are less demanding than the Chinese asset challenges. Additionally, among the Chinese models, Doubao and Kimi k2 achieve the highest asset origin ratios. The rank suggests their capabilities are more balanced and less skewed toward domestic assets when compared to other models from the same region. 13 Figure 9 Performance changes from the reasoning capability across web-based products in the same series."
        },
        {
            "title": "4.4 Where do models excel and falter on FinSearchComp?\nT1. Time-Sensitive Data Fetching. Products augmented with financial plugins, including GPT-5-Thinking\n(web), HunYuan-T1 (web), and DouBao (web), achieve over average 70% accuracy, demonstrating superior\nperformance over relying solely on their underlying LLMs. We show all the corresponding cases in Appendix D.\nNotably, Deepseek-R1 (web) on the official Deepseek website scored only 28.8, a 12.4% reduction compared to\nits performance when integrated with YuanBao, underscoring the critical role of specialized data retrieval\ntools. Common failure modes includes non-activation of plugins, retrieval of outdated web content, and an\ninability to select the correct information when presented with multiple, sometimes conflicting, sources.",
            "content": "T2. Simple Historical Lookup. Grok 4 (web) achieves the highest rank because of its utilization of diverse reliable search sources. Some products attempt to generate responses from parametric memory without employing search tools, frequently resulting in factual inaccuracies. Moreover, majority of products source information from news reports rather than official filings, which often lack granular details such as prepaid expense. T3. Complex Historical Investigation. No product surpasses score of 30 except Grok 4 (web) and GPT5-Thinking (web). This difficulty stems from the task requirement for structured data retrieval via API or SQL, capability largely absent in products limited to web search. The few successful attempts are confined to queries necessitating fewer than five data points (such as, calculating the difference between two weekly closing prices for given stock)."
        },
        {
            "title": "4.5 Does reasoning ability enhance performance on FinSearchComp?",
            "content": "An evaluation of models within the same series is detailed in Figure 9. We observe an average decline of 7.0 points for T1 for reasoning capacity, which is likely due to the low complexity of the task and potential overthinking of reasoning models [1]. For T2 and T3, the change for adding reasoning capacity is negligible."
        },
        {
            "title": "5 Related Work",
            "content": "Financial Benchmarks. Early efforts such as FinQA [5] and its conversational extension ConvFinQA [6] target numerical reasoning over annual reports by requiring models to compose multi-step programs that combine text and tabular evidence. Subsequent suites widened both task type and language coverage: FLUE [20] aggregated classic tasks in finance, while FinEval [11] and MultiFinBen [18] introduced largescale Chinese and multilingual collections spanning classification, extraction, generation and multimodality. Moving closer to real-world practice, FinanceQA [15] gathered zero-tolerance questions written by buy-side professionals, and BizFinBench [14] distilled 6.7K genuine queries from popular investment-search app to probe long-context and noisy scenarios. Collectively, existing financial datasets advance the measurement of domain knowledge, quantitative reasoning and robustness. However, these datasets provide relevant financial data by default, which substantially mitigates the challenge of financial data search from open-domain sources [23]. While Finance Agent Benchmark [3] incorporates open-domain financial data search, it is limited to searching static historical data. This design introduces the possibility of data memorization by models, thus failing to adequately evaluate their financial data search capabilities. 14 Agentic Benchmarks. To assess end-to-end decision-making, several works frame evaluation as goal-directed interaction with external tools. In finance, the FinEval [11] agentic track scores models on planning, API use and long-horizon reasoning across tasks such as financial question and answering, financial text classification . Beyond finance, BrowseComp [23], BrowseComp-ZH [27], and BrowseComp-Plus [7] poses questions that require persistent web navigation and creative search strategies, offering simple yet challenging yardstick for browsing agents. These studies highlight the gap between token-level metrics and practical autonomy, motivating an evaluation that couples financial expertise with realistic tool use."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we address the critical lack of an end-to-end benchmark for evaluating LLM-based agents in financial data search, as prior work failed to assess agent capabilities in realistic, context-free scenarios. To fill this gap, we introduce FinSearchComp, the first fully publicly available benchmark designed for this purpose. It comprises 635 questions curated by experts across three demanding tasks that require agents to orchestrate various tools, such as SQL, APIs, and web search, to procure verifiable answers. Our holistic evaluation reveals that even state-of-the-art agents significantly underperform humans, often failing due to insufficient search depth and the use of outdated information. We release FinSearchComp as vital resource to drive the development of more robust and reliable financial agents."
        },
        {
            "title": "7 Contributions",
            "content": "Core Contributors (α-β order) Liang Hu, Jianpeng Jiao, Jiashuo Liu, Yanle Ren, Zhoufutu Wen, Kaiyuan Zhang, Xuanliang Zhang ({huliang.will, liniuniu}@bytedance.com) Contributors (α-β order) Xiang Gao, Tianci He, Fei Hu, Yali Liao, Zaiyuan Wang, Chenghao Yang, Qianyu Yang, Mingren Yin, Zhiyuan Zeng, Ge Zhang, Xinyi Zhang, Xiying Zhao, Zhenwei Zhu Advisors Hongseok Namkoong (Columbia Business School, namkoong@gsb.columbia.edu) Wenhao Huang (huang.wenhao@bytedance.com) Yuwen Tang (tangyuwen.thomas@bytedance.com) denotes corresponding authors. Contributors without explicit affiliations are from ByteDance Seed. During the work, Xuanliang and Yanle are interns at ByteDance Seed."
        },
        {
            "title": "8.1 What is Xpert Platform",
            "content": "Xpert is an expert-level data service platform under ByteDance, committed to becoming the industrys leading specialized training data and evaluation solution provider. Our vision is to transform the deep knowledge and rich experience of experts across various industries into high-quality data, providing critical momentum for AGI and unlocking greater commercial and social value. The platform brings together approximately 3,000 rigorously selected experts, including masters and doctoral scholars from Chinas top-tier 985/211 universities as well as industry professionals with 2-10 years of rich practical experience in finance, law, healthcare, education, and others. Link:https://xpert.bytedance.com/"
        },
        {
            "title": "8.2 Xpert Leaderboard Intro",
            "content": "Unlike mainstream exam-oriented evaluations, Xpert Leaderboard focuses on assessing AIs ability to solve expert-level complex tasks in the real world, dedicated to driving AI to create greater economic value. Link:https://xpert.bytedance.com/leaderboard"
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. Optimalthinkingbench: Evaluating over and underthinking in llms, 2025. URL https://arxiv.org/abs/ 2508.13141. [2] Baidu. https://ernie.baidu.com/, 2025. [3] Antoine Bigeard, Langston Nashold, Rayan Krishnan, and Shirley Wu. Finance agent benchmark: Benchmarking llms on real-world financial research tasks, 2025. URL https://arxiv.org/abs/2508.00828. [4] ByteDance. https://www.doubao.com/chat/, 2025. [5] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. FinQA: dataset of numerical reasoning over financial data. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.300. URL https://aclanthology.org/2021.emnlp-main.300/. [6] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 62796292, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.421. URL https://aclanthology.org/2022.emnlp-main.421/. [7] Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent, 2025. URL https://arxiv.org/abs/2508.06600. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. URL http://arxiv.org/abs/2501.12948. Accessed: May 2025. [9] Han Ding, Yinheng Li, Junhao Wang, and Hang Chen. Large language model agent in financial trading: survey, 2024. URL https://arxiv.org/abs/2408.06361. [10] Google. Gemini-2.5-Pro(preview 05-06): large language model. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-5-pro, 2025. Accessed: May 2025. 17 [11] Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, and Liwen Zhang. FinEval: Chinese financial domain knowledge evaluation benchmark for large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62586292, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.318. URL https://aclanthology.org/ 2025.naacl-long.318/. [12] Ziyan Kuang, Feiyu Zhu, Maowei Jiang, Yanzhao Lai, Zelin Wang, Zhitong Wang, Meikang Qiu, Jiajia Huang, Min Peng, Qianqian Xie, and Sophia Ananiadou. From scores to skills: cognitive diagnosis framework for evaluating financial large language models, 2025. URL https://arxiv.org/abs/2508.13491. [13] Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, K.p. Subbalakshmi, Jimin Huang, Lingfei Qian, Xueqing Peng, Jordan W. Suchow, and Qianqian Xie. INVESTORBENCH: benchmark for financial decision-making tasks with LLM-based agent. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25092525, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https: //aclanthology.org/2025.acl-long.126/. [14] Guilong Lu, Xuntao Guo, Rongjunchen Zhang, Wenqiao Zhu, and Ji Liu. Bizfinbench: business-driven real-world financial benchmark for evaluating llms, 2025. URL https://arxiv.org/abs/2505.19457. [15] Spencer Mateega, Carlos Georgescu, and Danny Tang. Financeqa: benchmark for evaluating financial analysis capabilities of large language models, 2025. URL https://arxiv.org/abs/2501.18062. [16] MoonShot_AI. https://kimi.moonshot.cn/, 2025. [17] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. survey of large language models for financial applications: Progress, prospects and challenges, 2024. URL https://arxiv.org/abs/2406.11903. [18] Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, and Qianqian Xie. Multifinben: multilingual, multimodal, and difficulty-aware benchmark for financial llm evaluation, 2025. URL https: //arxiv.org/abs/2506.14028. [19] Qwen Team. Qwen3-235B: large language model, 2025. URL https://huggingface.co/Qwen/Qwen3-235B-A22B. Accessed: May 2025. [20] Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 23222335, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.148. URL https://aclanthology.org/2022.emnlp-main.148/. [21] Tencent. https://yuanbao.tencent.com/chat/, 2025. [22] Dingzirui Wang, Longxu Dou, and Wanxiang Che. survey on table-and-text hybridqa: Concepts, methods, challenges and future directions, 2023. URL https://arxiv.org/abs/2212.13465. [23] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516. [24] xAI. Grok4: large language model. https://docs.x.ai/docs/models#models-and-pricing, 2025. Accessed: August 2025. 18 [25] Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, and Christina Dan Wang. Finrobot: An open-source ai agent platform for financial applications using large language models, 2024. URL https://arxiv.org/abs/2405.14767. [26] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao. [27] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Yining Hua. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese, 2025. URL https://arxiv.org/abs/2504.19314."
        },
        {
            "title": "A Details of FinSearchComp",
            "content": "A.1 Release Format The dataset is distributed as set of JSONL files containing {id, tier, question, tool_template, answer, trace} plus an evaluation harness that replays traces in sandboxed environment. Detailed documentation and citation files accompany the release. A.2 Illustration of Inconsistent Calculation Methods for the Same Metric Stock Price Adjustment: Due to significant discrepancies in the calculation of forward-adjusted and backward-adjusted prices across different databases, we uniformly query for non-adjusted prices only. PE (TTM): The definition of \"Earnings\" can vary among different institutions. Market Capitalization of Dual-Listed Companies: We either specify the calculation method (e.g., \"Price 1 Share Class 1 + Price 2 Share Class 2\" vs. \"Price 1 Total Shares\") or avoid such questions. Futures Contracts: The timing for switching the main contract and the algorithm for constructing continuous contracts differ across institutions. Cryptocurrency: Prices vary across different exchanges. A.3 Guide for Mitigating Ambiguity Table 4 Consolidated guide for annotation in FinSearchComp for mitigating ambiguity. Category Topic Calendar vs. Fiscal Year Corporate Fundamentals Timing Description for Financial Statements Financial Item Naming Description Questions must differentiate between calendar and fiscal years. Many companies (e.g., NVIDIA) do not align their fiscal year with the calendar year. Default to fiscal year for consistency. Use precise language for time periods. Income statement and cash flow items occur over period, while balance sheet items are snapshot at point in time. The variable name in the question must match the terminology used in the financial statements to avoid ambiguity (e.g., operating income vs. operating revenue). Bad Example What was NVIDIAs revenue in 2024? Good Example What was NVIDIAs revenue for fiscal year 2024? What were the companys assets in fiscal year 2023? What were the companys total assets as of the end of fiscal year 2023? What was the companys operating income? What was the companys operating revenue? (when the report lists operating income) Continued on next page 20 Category Topic Description Bad Example Good Example Table 4 -- Continued from previous page GAAP vs. Non-GAAP Market Data To prevent evaluation mismatches, questions must specify the standard (GAAP or Non-GAAP). This ensures the ground truth and the answer are based on the same accounting principles. What was the companys income? (Ambiguous; the ground truth might be GAAP while the answer is Non-GAAP) What was the companys net income, based on U.S. GAAP standards? What was the companys revenue? What was the companys revenue in millions of USD? What industry is the company in? To prevent evaluation mismatches, questions must specify the currency (e.g., USD, CNY). This ensures the answer can be directly compared to the ground truth. If companys industry is mentioned, specify the classification standard (e.g., specific level of Shenwan or CSRC industry codes) to ensure consistency. Currency Industry Classification Market Capitalization For multi-listed companies, specify the exact calculation method. simple Market Cap = Total Shares is ambiguous; sum of market values from each listing is precise. What is the total market cap of company dual-listed in Hong Kong and Shanghai? Futures Quote Notation Fixed Income & Macro Currency Exchange Rates Answer Precision General Rules Futures quotes can use special hexadecimal notation (e.g., 113085). The answer should accept both this format and the standard decimal equivalent to be robust. Specify the type of RMB exchange rate: onshore (CNY), offshore (CNH), or interbank, as their values differ. Questions must specify the required precision for numerical answers (e.g., number of decimal places, rounding to nearest integer). Answer requires 113.265625 only, but 113085 is also acceptable. What is the USD to RMB exchange rate? What is the profit margin? What is the companys industry classification according to the Shenwan Level 1 standard? What is the total market cap of the dual-listed company, calculated as (A-share price A-share count) + (H-share price H-share count)? The reference answer is 113.265625, but 113085 is also acceptable. What is the onshore USD to CNY exchange rate as of [Date]? What is the profit margin in percentage, rounded to two decimal places? Continued on next page 21 Model Reasoning Search Global T2 T3 Avg. Greater China T1 T2 T3 Avg. Avg. T1 Human Performance Grok 4 GPT-5-Thinking Gemini2.5-pro DouBao DouBao-Thinking YuanBao-HunYuan-T1-Thinking YuanBao-DeepSeek-V3 YuanBao-DeepSeek-R1 Kimi k2 Qwen3-235B-A22B-2507 DeepSeek-R1 ERNIE-X1 Gemini2.5-pro (Google Search) Gemini2.5-pro Hunyuan-T1-latest Kimi k2 DeepSeek-R1 ERNIE-X1 DouBao DouBao-Thinking Qwen3-235B-A22B-2507 - 100.0 73.3 51.4 75.0 100.0 88.1 76.7 88.3 81.6 Web-based products 87.3 68.1 51.2 68.9 76.9 67.2 47.6 63.9 56.0 44.5 27.4 42.6 61.2 33.6 22.6 39.1 34.3 33.6 21.4 29.8 59.0 18.5 11.9 29.8 53.0 24.4 14.3 30.5 53.7 22.7 13.1 29.8 30.6 47.1 10.7 29.5 60.2 37.8 14.3 37.4 17.9 19.3 14.3 17.2 23.9 15.1 10.7 16.6 64.9 67.0 23.9 51.9 60.4 63.0 15.9 46.4 51.9 46.0 12.5 36.8 88.3 63.0 11.4 54.2 62.2 61.0 23.9 49.0 82.0 58.0 11.5 50.5 81.1 55.0 10.2 48.8 84.7 58.0 14.8 52.5 38.3 35.1 73.0 6.8 18.1 42.0 5.7 21.9 56.8 51.0 13.6 40.5 62.2 49.0 11.4 40.8 APIs 42.5 47.1 25.0 38.2 0.0 22.7 13.1 11.9 14.2 19.3 13.1 15.5 23.5 10.7 11.4 0.0 7.9 14.3 9.5 0.0 6.1 10.1 8.3 0.0 6.9 16.0 4.8 0.0 7.9 11.8 11.9 0.0 1.2 1.2 2.5 0.0 28.8 52.0 13.6 31.5 8.1 2.2 12.0 10.2 26.6 19.8 52.0 8.0 7.3 14.0 8.0 0.0 4.6 7.0 0.0 6.8 5.9 11.0 6.8 0.0 4.7 8.0 6.0 0.0 3.1 2.3 7.0 0.0 0.4 1.1 0.0 0.0 60.4 55.2 39.7 46.7 39.4 40.1 39.7 41.2 33.9 29.7 28.8 28. 34.8 10.0 21.1 9.4 6.3 6.0 5.8 5.5 0.8 Table 5 Performance of various models and human on FinSearchComp. Table 4 -- Continued from previous page Category Topic Description Bad Example Good Example Unit Specification Clearly state the unit for the answer (e.g., million, billion, USD, %). What was the revenue? What was the revenue in billions of USD, rounded to the nearest integer?"
        },
        {
            "title": "B Detailed Scores on FinSearchComp",
            "content": "We show the detailed scores of various models on FinSearchComp in Table 5."
        },
        {
            "title": "C Prompt",
            "content": "This section primarily describes the judge system prompts on three subtasks. Judge for Time-Sensitive Data Retrieval You are strict judge. Your task is to score students response to financial question based on the question itself, the Real-time Authentic Information provide, and the Scoring Criteria. score of 1 means the students response meets the requirements, and 0 means it does not. Please provide your analysis first, then give the final score. If the final score is 1, output {\"score\":1}; if it is 0, output {\"score\":0}. You must output strict JSON. The specific rules are as follows: - If the Student Answer is empty, score 0. 22 - If the Student Answer is not empty, but the data in the Real-time Authentic Information is empty (contains no numbers), output {\"score\":\"null\"}. - The \"Scoring Criteria - Required Content\" is the minimum information the student must provide. If the students response does not meet the Required Content (e.g., is missing content), score 0. Providing extra information is not an error and will not result in deduction. You only need to focus on the information itself, not the time. You should not deduct points because the question asks for the \"latest price\" and the student only provides the \"price on month x, day x.\" - The \"Real-time Authentic Information\" is the **correct response information** for the users question at the time of inquiry, and you should completely trust it. If the students response meets the Required Content, use the Real-time Authentic Information as the ground truth to verify the correctness of the students answer. Score it according to the \"Scoring Criteria - Accuracy Requirements\": score 1 if it meets the requirements, and 0 if it does not. - If the \"Scoring Criteria - Accuracy Requirements\" is \"Only rounding errors are allowed,\" it means difference between the students number and the real-time information is permissible if its caused by rounding to different number of decimal places. For example, 12.26 vs. 12.256 is considered match, scoring 1. - If the \"Scoring Criteria - Accuracy Requirements\" is \"Any answer between the high and low price is considered correct,\" it means the students number is correct if it falls between the maximum and minimum values provided in the Real-time Authentic Information. Rounding errors are also allowed. For example, if the Real-time Authentic Information gives high of 12.256 and low of 12.152, student answer of 12.26 is considered within the correct range and scores 1, while an answer of 12.27 is not and scores 0. - If the \"Scoring Criteria - Accuracy Requirements\" is \"The allowable error range is an absolute value of xx,\" it means the students number is correct if it falls within the range of the Real-time Authentic Informations number xx. For example, if the requirement is \"The allowable error range is an absolute value of 0.01%,\" and the number in the Real-time Authentic Information is 13.56%, then any student answer within the range [13.55%, 13.57%] is correct and scores 1; otherwise, it scores 0. - If the \"Scoring Criteria - Accuracy Requirements\" is \"Expand the high-low price range by an absolute value of a,\" the correct range is from \"minimum value - a\" to \"maximum value + a\" based on the Real-time Authentic Information. Note: - You only need to verify numerical values like stock prices/indices, not dates. As long as the numerical values are correct, the students response is considered correct. You should not deduct points if the students response mentions \"latest/yesterday\" while the reference information specifies particular date (e.g., \"Month X, Day Y\"), nor should you deduct points if the dates do not align (due to potential issues with time zones, trading days vs. calendar days, etc.). The following examples use fictional dates and numbers. So do not refer to the specific figures. Example 1: <Question>: How is NVDAs stock market performance today? <Scoring Criteria>: Required Content: NVIDIAs latest opening price, high price, low price, and change percentage. Accuracy Requirements: Only rounding errors are allowed <Real-time Authentic Information>: { \"data\": { \"symbol\": \"NVDA\", \"open\": \"112.905\", \"high\": \"114.665\", \"low\": \"112.660\", \"price\": \"113.820\", \"latest_trading_day\": \"20250426\", \"previous_close\": \"114.500\", \"change\": \"-0.680\", \"change_percent\": \"-0.59%\" } } <Student Answer>: According to the latest news as of 17:47 Beijing time on April 27, at the close of the most recent U.S. trading day, NVIDIAs (NVDA) stock price fell by 0.59%. On that day, NVIDIAs opening price was $112.91, the high was $114.67, the low was $112.66, the volume was 130 million, and the turnover was $15.15 billion. The after-hours price was $113.28, down $0.54 from the closing price, decrease of 0.47%. <Your Output>: Analysis: Does the students answer meet the Required Content: Yes; Are the numbers consistent (allowing for rounding errors): The opening, high, and low prices are consistent, with 23 only rounding errors; The students answer about the change percentage (-0.47%) is not consistent with the real-time informations \"change_percent\": \"-0.59%\". Not all requirements are met, final score is 0; Final score: {\"score\":0} Example 2: <Question>: USD/CNY onshore exchange rate <Scoring Criteria>: Required Content: The latest onshore USD/CNY exchange rate Accuracy Requirements: Any answer between the high and low price is considered correct <Real-time Authentic Information>: { \"currency_pair\": \"USD/CNY\", \"exchange_per\": \"-0.0403\", \"exchange_range\": \"-0.0029\", \"exchange_rate\": \"7.1871\", \"exchange_rate_hi\": \"7.1934\", \"exchange_rate_lo\": \"7.1855\", \"open_exchange_rate\": \"7.1900\", \"pre_close_exchange_rate\": \"7.1900\", \"trading_date\": \"20250605\" } <Student Answer>: As of 03:00 Beijing time on June 5, 2025, the onshore yuan (CNY) closed at 7.1905 against the US dollar, down 26 points from the previous trading days night session close, with trading volume of $36.094 billion. Additionally, on June 5, 2025, the central parity rate of the RMB in the inter-bank foreign exchange market was 7.1729 yuan per US dollar, an appreciation of 32 basis points from the previous trading day. <Your Output>: Analysis: Did the student answer the Required Content: Yes; Does the number meet the accuracy requirement: The threshold range from the real-time information is [7.1855, 7.1934]. The students answer, 7.1905, falls within this range and meets the requirement. Final score: {\"score\":1} Example 3: <Question>: Latest COMEX gold price <Scoring Criteria>: Required Content: Latest COMEX gold price Accuracy Requirements: The allowable error range is an absolute value of 0.6 <Real-time Authentic Information>: { \"res\": { \"request\": \"w.wsq(\"GC.CMX\",\"rt_date,rt_time,rt_last\")\", \"code\": 0, \"data\": { \"GC.CMX\": { \"RT_DATE\": 20250619, \"RT_TIME\": 90628, \"RT_LAST\": 3383.2000 } } } } <Student Answer>: According to Hithink RoyalFlush Futures data, as of 23:22 on June 18, 2025, the price of New York gold (COMEX Gold Futures) was $3382.7 / ounce, down $25.4 from the previous trading day, decrease of 0.75%, with high of $3405.2 / ounce and low of $3363.6 / ounce. <Your Output>: Analysis: Students answer meets the Required Content: Yes; Does the number meet the accuracy requirement: The threshold from the real-time information is 3383.2000 0.6, which is [3382.6000, 3383.8000]. The students answer, 3382.7, falls within this range and meets the requirement. Final score: {\"score\":1} ========End of examples, this is your task======== 24 Judge for Single Historical Data Point Retrieval and Complex Data Retrieval You are an intelligent judge and scorer for answers to financial questions. You will receive <Question>, its <Reference Answer>, and <Student Answer>. Some <Reference Answer>s may be supplemented with \"Scoring Criteria\". You need to evaluate the <Student Answer> and complete the following tasks: 1. Based on the content of the <Student Answer>, accurately identify its final answer (identification only, no need to output). You can identify the position and content of the final answer by analyzing the <Student Answer> or by searching for keywords, including but not limited to \"the answer is,\" \"the final result is,\" \"the correct option is,\" etc. If the <Student Answer> is empty, meaning it contains no content, assign score of 0 directly and skip steps 2 and 3 below. 2. Separately list the final answer from the <Reference Answer> and the final answer you identified from the <Student Answer>, and compare the two (no need to output the listing and comparison process or results). 3. Based on the result of the comparison and any Scoring Criteria that may be provided with the <Reference Answer>, judge whether the <Student Answer> is correct and assign score. The score can only be 1 or 0; 1 indicates the <Student Answer> is correct, and 0 indicates it is incorrect. No scores other than 0 and 1 are permitted. **Notes:** 1. You do not need to and should not answer or solve the question yourself. Your sole task is to judge and score. 2. The <Reference Answer> is accurate and correct. You can fully trust it. 3. If the <Reference Answer> contains 2 or more key points, such as persons name and number, an item and time, or multiple parallel locations, the <Student Answer> can only receive 1 point if it provides all the key points and all of them are correct. If only portion of the key points are provided or are correct, the score is 0. 4. If the <Student Answer> is empty or an abnormal error message, please assign score of 0. 5. Numerical values of the same magnitude written in different formats are considered the same, for example, 12.45% vs 0.1245, or 120,400,000 vs 120.4 million, or 2/5 vs 0.4. If the <Reference Answer> and the <Student Answer> use different formats but the underlying numbers represent the same value, they are considered consistent. 6. An answer is deemed correct if its numerical component matches the <Reference Answer>, even if the unit is omitted. For example, if the <Question> asks for \"xx revenue in billions of yuan\", <Student Answer> of \"12.98\" is valid match for the <Reference Answer> \"12.98 billion yuan\". Similarly, if the required unit is specified as \"%\" and the <Reference Answer> is \"12.98%\", <Student Answer> of \"12.98\" is also valid match. **Regarding your output format:** You need to first output the evaluation result, then provide the scoring basis and the score. Your output format is: - Scoring Basis: (Briefly state your reason for the score, not exceeding 150 words or 150 tokens) - JSON: {\"answer_score\": your score} ### **Example 1** <Question>: What is the average number of years it took for the C9 League university presidents who spoke at the 2024 graduation ceremonies to become president of university for the first time after receiving their PhD? (Calculate in years, round to one decimal place) <Reference Answer>: 27.9 years Scoring Criteria: The number in the Student Answer must be strictly identical to the number in the Reference Answer. <Student Answer>: Based on known information, the average time for the current presidents of C9 League universities to become president for the first time after their PhD is approximately 27.3 years. Here is the specific calculation: 1. Peking University: President Gong Qihuang, PhD in 1992, became president in 2022, gap of 30 years. 2. Tsinghua University: President Wang Xiqin, PhD in 1996, became president in 2022, gap of 26 years. 3. Fudan University: President Jin Li, PhD in 1994, became president in 2021, gap of 27 years. 4. Shanghai Jiao Tong University: President Ding Kuiling, PhD in 1990, became president in 2021, gap of 31 years. 5. Nanjing University: President Lü Jian, PhD in 1993, became president in 2018, gap of 25 years. 6. University of Science and Technology of China: President Bao Xinhe, PhD in 1991, became president in 2017, gap of 26 years. 7. Xian Jiaotong University: President Wang Shuguo, PhD in 1986, became president in 2014, gap of 28 years. 8. Zhejiang University: President Wu Zhaohui, PhD in 1993, became president in 2015, gap of 22 years. 9. Harbin Institute of Technology: President Han Jiecai, PhD in 1991, became president in 2022, gap of 31 years. Summing the years: 25 30 + 26 + 27 + 31 + 25 + 26 + 28 + 22 + 31 = 246 Average time: 246 9 = 27.3 years Therefore, on average, it took these presidents approximately 27.3 years to become university president for the first time after their PhD. - Scoring Basis: This question has Scoring Criteria, which require the number in the Student Answer to be strictly identical to the number in the Reference Answer. Here, the number in the <Student Answer> is not strictly identical to the number in the <Reference Answer>. Therefore, the <Student Answer> is incorrect, and the score is 0. - JSON: {\"answer_score\": 0} ### **Example 2** <Question>: An interesting aspect of the United States is that states political center is often not its economic center. For example, the capital of California is the relatively lesser-known Sacramento, not the more economically developed and famous San Francisco or Los Angeles. In contrast, in China, the provincial capital of each province (excluding autonomous regions) is often the city with the highest GDP in that province. Using the 2022 GDP as the standard, please list the provinces in China where the city with the highest GDP within the province is not the provincial capital. <Reference Answer>: Hebei Province, Liaoning Province, Jiangsu Province, Shandong Province, Guangdong Province Scoring Criteria: The students answer must list all the provinces from the reference answer and no other provinces. <Student Answer>: According to 2022 GDP data, the provinces in China (excluding autonomous regions) where the city with the highest GDP is not the provincial capital include: 1. Hebei Province: The city with the highest GDP is Tangshan, while the capital is Shijiazhuang. 2. Liaoning Province: The city with the highest GDP is Dalian, while the capital is Shenyang. 3. Heilongjiang Province: The city with the highest GDP is Daqing, while the capital is Harbin. 4. Jiangsu Province: The city with the highest GDP is Suzhou, while the capital is Nanjing. 5. Fujian Province: The city with the highest GDP is Quanzhou, while the capital is Fuzhou. 6. Shandong Province: The city with the highest GDP is Qingdao, while the capital is Jinan. 7. Guangdong Province: The city with the highest GDP is Shenzhen, while the capital is Guangzhou. These provinces economic centers are often concentrated in non-capital cities, reflecting different regional economic development patterns. - Scoring Basis: This question has Scoring Criteria, which require the Student Answer to list all the provinces from the Reference Answer and no other provinces. Here, the <Student Answer> lists all the provinces from the <Reference Answer> but also includes provinces not in the <Reference Answer> (Heilongjiang Province, Fujian Province). Therefore, the <Student Answer> is incorrect, and the score is 0. - JSON: {\"answer_score\": 0} ### **Example 3** <Question>: How many Michelin three-star restaurants in San Francisco specialize in Asian cuisine, and what are their names? <Reference Answer>: 1, Benu <Student Answer>: Among San Franciscos Michelin three-star restaurants, Benu is the one that specializes in Asian-inspired cuisine. Helmed by renowned chef Corey Lee, Benu offers modern Asian cuisine, fully showcasing Lees creativity and skill. The dishes blend Korean, Chinese, and other Asian elements with high degree of innovation and refinement. According to current information, there is 1 Michelin three-star restaurant in San Francisco specializing in Asian cuisine. - Scoring Basis: This question does not have Scoring Criteria. Here, the <Student Answer> states that there is 1 Michelin three-star restaurant specializing in Asian cuisine in San Francisco, and its name is Benu. Although the wording is different from the <Reference Answer>, the meaning is the same. Therefore, the <Student Answer> is correct, and the score is 1. - JSON: {\"answer_score\": 1} 26 ### **Example 4** <Question>: In terms of area, which provincial capital city in China (mainland provinces) is closest in size to Slovenia? What is the difference in square kilometers? <Reference Answer>: It is closest to Kunming, with difference of about 740 square kilometers. <Student Answer>: The total area of Slovenia is 20,273 square kilometers. Among Chinas provincial capitals, the one closest in area is Kunming, with an area of 22,100 square kilometers. The difference between the two is: 22,100 - 20,273 = 1827 square kilometers. - Scoring Basis: This question does not have Scoring Criteria. Here, the <Student Answer> correctly identifies Kunming as being closest in area to Slovenia, which is consistent with the <Reference Answer>. However, the area difference of \"1827 square kilometers\" given in the <Student Answer> is inconsistent with \"about 740 square kilometers\" in the <Reference Answer>. Therefore, the <Student Answer> is incorrect, and the score is 0. - JSON: {\"answer_score\": 0} ### hope you are competent for this job, as it is very important to me. If you perform well, will reward you accordingly. Otherwise, may penalize you."
        },
        {
            "title": "D Case",
            "content": "In this section, we present typical cases identified during evaluation on FinSearchComp. web-based product successfully addresses question of Time-Sensitive Data Fetching in FinSearchComp by leveraging financial plugins. Question walmart current stock price Response As of the U.S. market close on August 27, 2025, the stock price of Walmart Inc. (Ticker: WMT) was $96.08, marking daily gain of 0.03% with trading turnover of $1.241 billion. The price represented slight increase from the previous days close of $96.05. During the session, the stock reached an intraday high of $96.32 and low of $95.60. In terms of recent performance, Walmarts stock has declined by 6.33% over the past five trading days, yet it still maintains year-to-date (YTD) gain of 7.13%. Analysts note that despite facing short-term pressure from tariff costs, the company has sustained its market competitiveness through supply chain optimization and its omnichannel strategy. Search Sources 1. tradersunion.com Walmart Inc. (WMT) Stock Price Forecast for 30 days: Today, Tomorrow, Next Week August 27, 2025 - The current price of Walmart Inc. (WMT) is US$96.02. The analytical forecast indicates that the price of WMT, expected to reach US$96.12 by the end of the week, could rise/fall to US$99.35 by the end of the month. 2. tradersunion.com 3. Walmart Inc. (WMT) Stock Price Forecast for 30 days: Today, Tomorrow, Next Week August 27, 2025 - The current price of Walmart Inc. (WMT) is US$96.02. The analytical forecast indicates that the price of WMT is expected to reach US$96.12 by the end of the week, and could rise/fall to US$99.35 by the end of the month. in.benzinga.com Walmarts Options: Look at What the Big Money is Thinking - Walmart (NYSE:WMT) - Benzinga August 26, 2025 - Whales with lot of money to spend have taken noticeably bearish stance on Walmart. Looking at options history for Walmart WMT we detected 15 trades. If we consider the specifics of each trade, it is accurate to state that 33% of the investors opened trades with bullish expectations and 46% with bearish. 4. Tencent Financial Industry Database Walmart WMT.N August 28, 2025 - Real-time Stock Quote for Walmart (Code: WMT.N) in US Stocks: Data Updated: 2025-08-27 16:04:58; Latest Price: $96.08; Previous Close: $96.05; Todays Open: $95.90; Days High: $96.32; Days Low: $95.60; Days Change: 0.03%; Days Amplitude: 0.74%; Turnover Rate: 0.16%; Volume (shares): 12,925,656; Turnover (USD): 1,241,094,879.00; Volume Ratio: 0.45; 5-Day Change: -6.33%; YTD Change: 7.13%; P/E Ratio (TTM): 36.26; P/B Ratio: 8.51; Dividend Yield: 0.98%; 20-Day Change: -1.32%; Total Market Cap (billion USD): 766.76. 5. tradersunion.com Walmart Inc. (WMT) Stock Price Forecast (30 days): Today, Tomorrow, Next Week August 27, 2025 - The current price of Walmart Inc. (WMT) is $96.02. According to analytical forecasts, the price of WMT is expected to reach $96.12 by the end of this week, and could rise/fall to $99.35 by the end of this month. 6. marketbeat.com Walmart (NYSE:WMT) Price Target Raised to $114.00 August 24, 2025 - Walmart (NYSE:WMT) Price Target Raised to $114.00 Key Points - The Goldman Sachs Group has raised Walmarts price target from $101.00 to $114.00, indicating potential upside of 17.25% from its previous close. 7. Tonghuashun Finance Walmarts turnover on August 27 was $1.241 billion, decrease of 41.91% from the previous trading day. August 28, 2025 - On August 27, 2025, consumer staples retailer Walmart (WMT) had turnover of $1.241 billion, ranking 44th in US stocks for the day. The turnover decreased by 41.91% from the previous day, and the trading volume was 12.9257 million shares. 8. WeChat Official Account CC Finance Walmart, with Unimpressive Revenue and Profits, Has Stock Price Stronger Than the Magnificent Seven July 29, 2025 - The continuous rise of Walmarts stock price (around $95.67/share in July 2025, market cap of $760 billion, up over 300% from its 2020 low), despite single-digit growth in revenue and profit... 9. WeChat Official Account Haishi Shibei Walmart (WMT) Stock Price Under Pressure! Profit Misses Expectations for the First Time in Three Years, Soaring Tariff Costs Drag Down Profit Outlook August 22, 2025 - Transcript from the conference call: \"Good morning/afternoon, everyone. Welcome to Walmarts Fiscal Year 2026 Second Quarter Earnings Conference Call. All participants will now be in listen-only mode...\" web-based product fails Single Historical Data Retrieval question in FinSearchComp due to the absence of search tool utilization. Question How much cash was used in investing activities by Apple in the 2021 fiscal year? (Answer in millions of dollars, rounded to nearest integer) Reasoning The user needs to know the amount of cash used in investing activities by Apple in the 2021 fiscal year, in millions of dollars, rounded to the nearest integer. First, need to recall Apples financial reports, specifically the cash flow statement for the 2021 fiscal year. Apples fiscal year typically ends in September, so the 2021 fiscal year would be the year ending September 2021. According to public financial data, in Apples 2021 cash flow statement, the net cash flow from investing activities is usually negative because investment expenditures exceed income. The \"cash used\" here refers to the net amount of total cash outflows from investing activities minus inflows, but typically \"cash used\" in question refers to the net cash outflow from investing activities. Consulting Apples 2021 10-K report, the investing activities section includes capital expenditures (like PP&E), acquisitions, investment securities, etc. From memory, was the net cash outflow from investing activities for Apples 2021 fiscal year approximately 144,266 million dollars? Or is more accurate data needed. Confirming again, in Apples 2021 fiscal year cash flow statement, the net cash flow from investing activities was -144,266 million dollars. This means the cash used for investing activities was 144,266 million dollars. So the answer should be 144266. Response 144266 Search Sources No search sources were used. 28 web-based product successfully solves Complex Historical Investigation question by scoping its search to reliable source (nasdaq.com) and then cross-verifying the retrieved data through reverse searches. Question What was the change in opening price compared to the previous closing price (in dollars, unadjusted) on the effective date of Apple Inc.s (AAPL) most recent stock split? What was the split ratio? (Round to two decimal places) Reasoning ... Now, lets try searching NASDAQ for \"AAPL closing price August 28 2020\" to confirm the exact figure. Searched for AAPL closing price August 28 2020 site:nasdaq.com nasdaq.com (x10) Searched for AAPL opening price August 31 2020 site:nasdaq.com nasdaq.com (x10) ... Confirming closing price Im searching for \"AAPL August 28 2020 close 499.23\" to verify the exact figure. Searched for AAPL August 28 2020 close 499. finance.yahoo.com statmuse.com stockanalysis.com investor.apple.com financecharts.com fool.com Searched for AAPL August 31 2020 open 127.58 ... Response The most recent stock split for Apple Inc. (AAPL) was effective on August 31, 2020, with split ratio of 4-for-1. On that date, the opening price was $127.58, compared to the previous closing price of $499.23 on August 28, 2020. The change was -$371.65."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Columbia Business School"
    ]
}