{
    "paper_title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
    "authors": [
        "Leheng Sheng",
        "Yongtao Zhang",
        "Wenchang Ma",
        "Yaorui Shi",
        "Ting Huang",
        "Xiang Wang",
        "An Zhang",
        "Ke Shen",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\\text{update}}$ and $r^{\\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 0 6 5 0 1 . 2 0 6 2 : r When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning Leheng Sheng1,2,, Yongtao Zhang1, Wenchang Ma1, Yaorui Shi3, Ting Huang1, Xiang Wang3, An Zhang3,, Ke Shen1,, Tat-Seng Chua 1Bytedance Seed, 2National University of Singapore, 3University of Science and Technology of China Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals rupdate and rexit within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400% times inference speed acceleration. Date: February 12, 2026 Correspondence: An Zhang at an_zhang@ustc.edu.cn, Ke Shen at shenke@bytedance.com Project Page: https://alphalab-ustc.github.io/grumem-alphalab/"
        },
        {
            "title": "Introduction",
            "content": "Reasoning over extremely long contexts is crucial capability of large language models (LLMs) for real-world applications [7, 23, 31], such as reading an entire book or processing large-scale memories in agentic systems [4, 15, 24]. While important, this capability remains challenging: LLMs typically experience dramatic performance degradation as the context length grows [1, 12], and they struggle to handle corpora that exceed their maximum context window [36]. To address these limitations, recent work MemAgent [36] has explored recurrent memory paradigm for long context reasoning in an RNN-like manner [15, 22, 36]. As illustrated in Figure 1, instead of encoding the entire context in single forward pass [30, 33], this method reformulates long-context reasoning as sequential, recurrent, and chunk-by-chunk memorization process. Concretely, the entire long context is 1 Figure 1 MemAgent and its limitations. The MemAgent reads long context chunk-by-chunk in an RNN-like manner, recurrently updating textual memory and answering from the final memory. It faces two crucial risks: memory explosion by over-accumulating irrelevant memories and lacking an exit mechanism when collected sufficient evidence. divided into fixed-size chunks {C1 CT } first. At step t, memory agent ϕθ reads the question Q, chunk Ct, and the previous memory Mt1 to recurrently update textual memory Mt. After processing all chunks, an answer agent ψθ predicts the answer ˆA conditioned on the final memory MT and the question Q. In practice, the memory agent ϕθ and the answer agent ψθ share the same parameterized policy model θ, with their behaviors differentiated by the prompt assignment. Trained end-to-end with reinforcement learning (RL) [36, 37], this recurrent memory paradigm shows potential in mitigating the performance degradation from ingesting an overly long context at once, and ideally breaks the context window size limit. However, the vanilla approach inherits several drawbacks of naive RNN-style memory updates [3, 11]. Here, we identify two key limitations that may hinder the memory stability and efficiency of reasoning in practice: Risk of Memory Explosion. As shown in Figure 1, when updating on evidence-free chunks, the memory agent may accumulate irrelevant or noisy content over time. This drift progressively inflates the memory, potentially exceeding the allotted budget and triggering memory explosion. Once already exploded, the accumulated noise can further impede subsequent updates, making it harder to incorporate new key evidence from later chunks [36]. Additionally, inefficiently regenerating the already-exploded memory at each step also increases the inference cost. As result, such risks of memory explosion undermine both long-horizon stability and runtime efficiency for long-term reasoning. Lack of Exit Mechanism. As shown in Figure 1, the vanilla workflow is hard-coded to process all chunks and provides no early-exit mechanism when the collected evidence is sufficient. That is, even after sufficient evidence has been collected already (e.g., the last necessary evidence for the answer has appeared), the model must still process the remaining chunks until the end, incurring avoidable computation. This inefficiency is amplified when evidence is unevenly distributed (e.g., when reranking brings key evidence early [8, 40]) This inefficiency becomes even more severe when evidence is unevenly distributed across the context, where in some cases key evidence may appear very early (e.g., when the long context is reordered by specific reranking techniques) [8, 40]. To this end, we propose GRU-Mem, gated recurrent memory framework for stable and efficient long-context reasoning, inspired by the effectiveness of gating in GRUs [6] for addressing long-term sequence modeling challenges in RNNs (e.g., gradient explosion [11]). The key idea is to augment the recurrent workflow with two text-controlled gates an update gate (UG) and an exit gate (EG) which decide (i) whether the memory should be updated on the current chunk and (ii) whether the model can terminate early once sufficient evidence has been collected. Specifically, at step t, the memory agent ϕθ generates three pieces of key information: the update gate status Ut, the candidate memory ˆMt, and the exit gate status Et, which can be formulated as Ut, ˆMt, Et = ϕθ(Q, Ct, Mt1). Only when the update gate status ˆUt is True, the memory Mt will be updated with the candidate memory ˆMt, otherwise adopting the previous memory Mt1. Additionally, when the exit gate status Et is True, indicating the last evidence required occurs in this chunk, the workflow can end immediately for answering the question. Finally, similar to the vanilla design, the answer agent ψθ 2 provides the answer ˆA based on the terminal memory Mt. To endow the model with such capabilities, we introduce two reward signals rupdate and rexit within end-to-end RL, rewarding the correct updating and exiting behaviors, respectively. The update gate enables selective memory updates on only few informative chunks, mitigating memory explosion, while the exit gate provides flexible early-termination mechanism that reduces unnecessary computation; together, they yield more stable and efficient long-context reasoning. We further conduct extensive experiments to verify the effectiveness of GRU-Mem. First, we show that GRU-Mem generally outperforms the vanilla MemAgent across diverse tasks and among different model sizes, and generally achieves up to 400% times inference speed acceleration (cf. Section 4.1). Second, we validate the effectiveness of the two introduced gating mechanisms, where GRU-Mem can reduce the risks of memory explosion and provide meaningful exit mechanism (cf. Section 4.2). Third, we systematically analyze the role of components in GRU-Mem with the ablation study, including the training dynamics (cf. Section 4.3). We highlight that GRU-Mem provides stable and efficient long context reasoning paradigm with the introduction of two gating mechanisms."
        },
        {
            "title": "2 Preliminary",
            "content": "We briefly introduce the task formulation of reasoning over long contexts in Section 2.1 first. After that, we present the recurrent memory paradigm, as introduced in previous work MemAgent [36], in Section 2.2."
        },
        {
            "title": "2.1 Task Formulation\nLong-context QA. In this paper, we primarily focus on the widely adopted long-context question answering\n(QA) tasks [12, 36]. In these tasks, each example consists of a question Q, a long context C, and a ground-truth\nanswer A, which can be denoted as a triplet (Q, C, A). Here, the context length |Ci| can be very large, such as\nmillions of tokens.",
            "content": "Evidence Sparsity. To answer the question Ai, LLMs are required to accurately locate and then reason over few evidence pieces {ek}K , which are sparsely located within the long context Ci. The sparse evidence distribution in long context reasoning makes LLMs struggle to answer accurately, which is also commonly known as the needle in haystack (NIAH) [13] problem. k="
        },
        {
            "title": "2.2 Recurrent Memory for Long-context Reasoning",
            "content": "In the recurrent memory for long-context reasoning paradigm, the long context is split as Context chunks. set of fixed-size context chunks = {C1 CT } first. After splitting into chunks, the evidence is sparsely distributed in only few chunks. That is, most of the chunks do not contain any information for answering the question Q. This splitting strategy avoids directly feeding the entire context into LLMs at once. Workflow. The recurrent memory paradigm for long-context reasoning adopts an RNN-like workflow with two agents: memory agent ϕθ for recurrently updating textual memory and an answer agent ψθ for answering the question based on the final memory. Specifically, as shown in Figure 1, at each step the memory agent ϕθ takes the question Q, the current chunk Ct, and the previous memory Mt1 as input, and generate an updated memory Mt in recurrent manner, which can be formulated as: After reading all the chunks, an answer agent ψθ generates the answer ˆA to the question based on the final memory MT , as formulated as follows: Mt = ϕθ(Q, Ct, Mt1). (1) ˆA = ψθ(Q, MT ). (2) Here, the memory agent ϕθ and the answer agent ψθ are the same parameterized policy model θ differentiated by the prompt assignment. Workflow optimization with end-to-end RL. This agent workflow can be optimized with the Multi-Conv DAPO algorithm as proposed in MemAgent [36], which is an extension of the group relative policy optimization 3 (GRPO) algorithm [27] in the multi-turn scenario. The key idea lies in treating each conversation (i.e., one individual memory or answer turn) as an independent optimization target, and then optimizing these independent targets with their corresponding advantages. Specifically, one workflow trajectory in group can be expressed as (og,1, og,2, ..., og,Tg ), where Tg denotes the total turn number of generated conversations. denotes the answer agent turn while (og,1, og,2, ..., og,Tg1) denote the memory agent turns. Each Here og,Tg conversation output og,t at step consists of sequence of tokens (og,t,1, og,t,2, ..., og,t,og,t), where og,t is the number of output tokens in this conversation. Then the overall loss can be formulated as follows: (θ) = (Q,A)D,{og,t}G g=1πθold (Q, og,t1) (cid:34) 1 (cid:80)Tg t=1 og,t (cid:80)G g= (cid:88) Tg (cid:88) og,t (cid:88) g=1 t=1 i= (cid:16) (cid:17) ℓclip g,t,i β DKL(πθπref) (cid:35) , (3) where ℓclip g,t,i = min (cid:16) ρg,t,i(θ) ˆAg,t,i, clip(cid:0)ρg,t,i(θ), 1 εlow, 1 + εhigh (cid:1) ˆAg,t,i (cid:17) . Here πθ and πref denote the policy model and reference model, εlow and εhigh denote the lower and higher clipping factors as introduced in DAPO [37], and ρg,t,i(θ) refers to the importance sampling weight: ρg,t,i(θ) = πθ(og,t,i Q, og,t,<i) πθold(og,t,i Q, og,t,<i) . (4) In this vanilla approach, all the conversations within one group share the same advantage, and the advantage without the normalization of standard deviation is calculated as [20]: The reward for the final answer correctness is calculated as: ˆAg,t,i = routcome mean({routcome }G g=1). routcome = I(is_equiv(A, ˆA)), (5) (6) where ˆA is the predicted answer, and I() denotes the indicator function. This assigns reward of 1 if the answer ˆA produced by the answer agent ψθ matches the ground truth answer A, and 0 otherwise. With such end-to-end RL training, the long-context reasoning capability of this agent workflow is substantially enhanced, even enabling small-sized models to outperform big models that ingest the entire context in single pass. However, indiscriminately updating on all chunks and the lack of an early exit mechanism still largely limit the memory stability and efficiency of the vanilla approach."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce the gated recurrent memory (GRU-Mem), recurrent memory workflow equipped with two gating mechanisms, for stable and efficient long-context reasoning. We first introduce the workflow of GRU-Mem in Section 3.1. After that, we then detail how to train the workflow with GRU-Mem using end-to-end reinforcement learning, including the tailored reward design and advantage calculation, in Section 3.2. Finally, we briefly introduce the inference strategies in Section 3.3."
        },
        {
            "title": "3.1 Gated Recurrent Memory Workflow",
            "content": "To address the risk of memory explosion and lack of an exit mechanism, we introduce two gating mechanisms in the recurrent workflow, the update gate (UG) and the exit gate (EG), inspired by the success of gating in GRUs [6] for addressing long-term sequence modeling. Specifically, GRU-Mem mainly changes the memory agent ϕθ while keeping the answer agent ψθ unchanged, extending the vanilla ϕθ with two extra actions for gate control [36]. It outputs candidate memory ˆMt and two binary gating signals, Ut and Et, indicating whether to update the memory and whether to terminate the loop, respectively. This process is formulated as follows: Ut, ˆMt, Et = ϕθ(Q, Ct, Mt1). (7) 4 Figure 2 The memory updating process with the gated recurrent memory (GRU-Mem). At each time step t, the memory agent ϕθ decides: (1) whether to update the memory Mt with the candidate memory ˆMt or to keep the previous memory Mt1 unchanged, based on the update gate status Ut (True for updating and False for retaining Mt1); and (2) whether to stop scanning further chunks, based on the exit gate status Et (True for exiting and False for continuing processing). You should reason about whether the new section contains useful information, what to update, and what to do next first between <think> and </think>. If the new section contains useful information about the problem, you should first generate <check>yes</check>. After that, update the new memory between <update> and </update>. If the new section does not contain useful information about the problem, you should first generate <check>no</check>. between <update> and </update>. In the end, if you havent collected enough information for the problem, return <next>continue</next>. ONLY when enough information is collected, return <next>end</next>. After that, you should keep the previous memory unchanged Figure 3 Prompt of GRU-Mem (partial). To achieve this, the memory agent ϕθ responds by following structured output format (see prompts in Figure 3 and Appendix C, with case studies in Appendix E). The process of one single step of ϕθ in GRU-Mem is illustrated in Figure 2. It first produces intermediate reasoning enclosed by <think> and </think>, and then emits an update decision within <check> and </check>, where \"yes\" (i.e., Ut == True) triggers memory update and \"no\" (i.e., Ut == False) skips it. Next, it outputs the candidate memory ˆMt within <update> and </update>. If Ut == True, the memory will be updated with the candidate memory (i.e., Mt ˆMt). If Ut == False, the previous memory Mt1 will be adopted while the candidate memory ˆMt will be discarded (i.e., Mt Mt1). Finally, it decides whether to continue collecting evidence between <next> and </next>, where \"continue\" means continuing the recurrent loop and \"end\" means terminating the loop. Once terminated, the final memory Mt will be sent into the answer agent ψθ immediately for answering the question. The detailed process of GRU-Mem is described in Algorithm 1."
        },
        {
            "title": "3.2 Workflow Optimization with End-to-End RL\nTo teach the memory agent ϕθ when to correctly activate the update gate and the exit gate, we explicitly\nreward correct gate status generation behaviors, beyond merely rewarding the final answer correctness. In this\nsection, we first introduce the reward design in Section 3.2.1. After that, we discuss how to combine these\nrewards for the advantage calculation in Section 3.2.2. More training details can be found in the Appendix B.",
            "content": "5 Algorithm 1: Long Context Reasoning with GRU-Mem Input: Question Q, the chunk size s, the whole context C, and use_exit_gate: bool. Output: Answer to the question ˆA. Initialize: initialize maximum turns len(C)//s, divide the whole context into chunks {C1, , CT }, 1, M0 None. while do //get the update status, candidate memory, and exit status Ut, ˆMt, Et = ϕθ(Q, Ct, Mt1) if Ut == True then Mt ˆMt else Mt Mt1 end if //(optional) exit based on the exit status if use_exit_gate == True and Et == True then break end if + 1 end while //answer the question based on the final memory ˆA = ψθ(Q, Mt) return ˆA. // update the memory // adopt the previous memory GRU-Mem"
        },
        {
            "title": "3.2.1 Reward Design",
            "content": "Outcome reward. For the outcome reward, we adopt the same reward as introduced in Equation 6. We assign the identical outcome reward routcome for all the conversations (og,1, og,2, ..., og,Tg ) within the whole trajectory in group g. Update reward. To learn correctly activating the update gate, for each conversation at step t, we reward for generating the correct update gate status Ut. Specifically, for chunks that contain evidence, the memory agent is rewarded when generating <check>yes</check> (i.e., Ut == True), whereas for chunks without evidence, it is rewarded when generating <check>no</check> (i.e., Ut == False). This reward at step can be formulated as follows: (cid:40) rupdate ="
        },
        {
            "title": "Ut is correct",
            "content": "1, 1, Ut is incorrect. (8) Exit reward. To learn the exit gate, we reward the whole trajectory when exiting at the correct position (i.e., the turn that contains the last evidence required for answering the question, namely tlast evidence). This means the memory agent is required to generate <next>end</next> (i.e., Et == True) when recognizing the occurrence of the last evidence, and generate <next>continue</next> (i.e., Et == False). In other cases, all the conversations within the trajectory get punished. This reward is formulated as follows: rexit = 0.75, 0, 0.5, texit < tlast evidence texit = tlast evidence texit > tlast evidence. (9) Here, the texit denotes the turn that the memory agent ϕθ decides to exit the workflow. This reward design means the correct exit gate status Et generation behavior does not get punished, and an early exit behavior gets more punishment than late exit behavior due to the evidence insufficiency. Format reward. To ensure that the generation og,t of the memory agent ϕθ can be parsed correctly, we introduce an additional format reward rformat. We check whether the format meets the requirement of enclosed sequence of <think> </think>, <check> </check>, <update> </update>, and <next> </next>. Additionally, Figure 4 The advantage calculation process. The trajectory-level advantage ˆAtraj are calculated separately. They are combined into the total advantage with α (i.e., ˆAg,t,i = α ˆAtraj g,t and the turn-level advantage ˆAturn g,t g,t,i). g,t,i + (1 α) ˆAturn the content between <check> and </check> must be \"yes\" or \"no\", and the content between <next> and </next> must be \"continue\" or \"end\". We make this reward strict so that only when all the generation output (og,1, og,2, ..., og,Tg ) meets the format correctness, they will get reward of 1. Otherwise, all the generation outputs get reward of 0. This strict design is because we can not infer whether the incorrect format is caused by the previous erroneous parsing. This reward is formulated as: (cid:40) rformat = 1, The format of all turns is correct 0, Otherwise. (10) The overall trajectory-level reward. Since all the generation outputs (og,1, og,2, ..., og,Tg ) in the group share the same outcome reward, exit reward, and format reward, they can be combined as one total trajectory-level reward rtraj as follows: rtraj = routcome + rexit + rformat . (11)"
        },
        {
            "title": "3.2.2 Advantage Calculation",
            "content": "We calculate the trajectory-level advantage ˆAtraj respectively [28]. After g,t,i that, we combine them with hyperparameter α for more flexible control. This is inspired by recent works of disentangling advantage calculation for stabilizing the training with different rewards [19, 28]. Here, the two advantages are calculated as follows: and the turn-level advantage ˆAturn g,t,i ˆAtraj g,t,i = rtraj"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) g=1 rtraj , ˆAturn g,t,i = rupdate g,t"
        },
        {
            "title": "1\nGt",
            "content": "Gt(cid:88) g=1 rupdate g,t . (12) As shown in Figure 4, the trajectory-level advantage ˆAtraj is calculated between the trajectories of different g,t,i groups. And the turn-level advantage ˆAturn is calculated between turns at step from different groups. For g,t,i the turn-level advantage ˆAturn calculation, the group size Gt at step can be different from the trajectory g,t,i group size G, since the workflow may exit at different step Tg under the control of the exit gate. Therefore, these early exited groups have smaller Tg than others. Finally, the total advantage ˆAg,t,i in Equation (3) is combination of these two terms: where α is hyperparameter for balancing two terms. The policy model will be optimized with the loss (θ), which is based on the calculated advantage ˆAg,t,i. ˆAg,t,i = α ˆAtraj g,t,i + (1 α) ˆAturn g,t,i, (13) 7 Table 1 The performance comparison across diverse long-context tasks. Scale Method MemAgent 7B GRU-Mem (w/o EG) GRU-Mem (w EG) MemAgent 3B GRU-Mem (w/o EG) GRU-Mem (w EG) Avg. Metric Perf. % Time Perf. % Time Perf. % Time Perf. % Time Perf. % Time Perf. % Time HQA 76.07 463. 75.59 284.41 76.37 209.33 63.87 218.60 69.04 211.77 65.33 162.31 SQuAD 79.56 162. 80.73 85.03 80.47 64.32 67.58 68.81 69.92 60.71 69.66 45.30 SK-1 99.78 378. 100.00 135.60 100.00 126.00 96.76 122.31 94.42 114.49 95.31 104.38 Tasks SK-2 95.54 420.38 95.43 171.57 96.65 113.89 88.73 176.48 88.84 120.89 88.28 84. SK-3 97.66 419.74 95.98 154.03 95.20 107.64 86.72 182.72 89.40 118. 90.85 82.13 MK-1 97.21 412.93 98.10 168.54 98.55 102.46 79.46 146.77 MK-2 75.78 349. 67.52 258.40 84.15 123.53 35.05 118.16 MK-3 95.98 403.60 93.53 242. 95.54 136.02 44.42 165.48 91.52 116.92 67.08 140.82 91.41 122.88 89.84 74. 58.15 59.60 90.85 68.46 MQ 88.37 419.53 96.43 156.62 84.12 108.62 77.26 177. 73.91 121.32 63.84 80.29 MV 81.70 407.89 95.23 153.08 - - 36.27 154.78 59.46 118."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we explore the effectiveness of GRU-Mem by answering the following questions. RQ1: (Performance and Efficiency) Can GRU-Mem achieve better performance compared to the vanilla MemAgent with higher inference efficiency? RQ2: (Gating Mechanism) How do the introduced two gates impact the workflow behaviors? RQ3: (Ablation Study) How does the hyperparameter α affect the training dynamics of GRU-Mem? How does the RL training contribute to the final performance? LLMs. We conduct experiments on two open-source LLM backbones with different sizes: Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct [34]. We train these LLMs on the same data as introduced in MemAgent. Evaluation Benchmarks. We strictly follow the evaluation setup as introduced in MemAgent [36]. Specifically, we evaluate methods on one multi-hop QA task HotpotQA (HQA) [35], one single-hop QA task SQuAD [26], three single-key NIAH tasks (i.e., SK-1, SK-2, SK-3), three multi-key NIAH tasks (i.e., MK-1, MK-2, MK-3), one multi-queries NIAH task MQ, and one multi-values task MV. Here, the number -N indicates the difficulty level, with larger values corresponding to greater difficulty. HQA is one in-distribution task similar to the training data, while the remaining tasks are out-of-distribution tasks. The benchmark preparation and evaluation protocols strictly follows those in the MemAgent [36]. These tasks are constructed with different context lengths, ranging from 7K to 896K. More details about the training and evaluation can be found in the Appendix B. 8 4.1 (RQ1) Performance and Efficiency Comparison We report the averaged performance and inference time cost across different context sizes in Table 1, and the averaged performance and time across varying context sizes for representative single task in Figure 5. Additional results under different context length settings are provided in Appendix D.1. Overall, our results consistently demonstrate that GRUMem not only achieves stronger task performance than the vanilla MemAgent, but also offers substantially improved inference efficiency across wide range of benchmarks, inference modes, and backbone model sizes. Specifically, we summarize our observations as follows: Figure 5 Performance and efficiency across diverse context lengths on the MV task. GRU-Mem generally outperforms the vanilla MemAgent across diverse datasets. As shown in Table 1, under both w/o EG and EG inference modes, GRU-Mem outperforms MemAgent on most of the tasks. Moreover, compared to MemAgent, GRU-Mem excels at out-of-distribution tasks, as it performs much better on NIAH tasks. Additionally, under the backbone LLM size of 3B, GRU-Mem even gains more. For example, GRU-Mem consistently maintains high performance on MK task series, while MemAgent shows sharp performance drop. We attribute the success of GRU-Mem to its more stable memory updating with the introduced update gate. GRU-Mem exhibits superior inference efficiency compared to the vanilla MemAgent. As shown in the row of Time in Table 1, under both w/o EG and EG inference modes, the GRU-Mem exhibits significant inference efficiency improvements compared to MemAgent. Under the w/o EG mode, GRU-Mem generally achieves around 200% inference acceleration. Under the EG mode, when the early exit mechanism is activated, the inference acceleration can be even faster, achieving maximum 400% times faster in several cases, such as MK-1, without harming the performance. 4.2 (RQ2) Study of Gating Mechanisms In this section, we study how the two introduced gating mechanisms (i.e., update gate and exit gate) affect the behaviors of the recurrent workflow. Update Gate. We study how the update gate reduces the risk of memory explosion by tracking the memory size dynamics during the long-term inference. As shown in Figure 6, GRU-Mem shows much lower memory size increasing speed, while MemAgent quickly encounters the memory explosion when the size of memory reaches the maximum memory size of 1024 tokens. This phenomenon is because GRU-Mem only updates the memory on few critical chunks which contain the evidence for answering, while MemAgent may indiscriminately update the memory. As result, in MemAgent, the ever-growing memory hurts its performance, and always generating overly long memory also significantly increases inference overhead. Figure 6 Memory size dynamics on MV task (512K context size). Exit Gate. For the exit gate, we especially focus on how it can benefit the inference when the last evidence may occur very early, with possible reranking techniques. To simulate this, we manually construct an unbalanced evidence occurrence setting, where the last evidence must occur at the top 20% documents. We report the performance and efficiency comparison of the 7B-sized model under this setting in Table 2. As shown in this table, GRU-Mem (w EG) reduces the inference time to 1/4 of the vanilla MemAgent. We also additionally calculate the ratio of early stopping, exact stopping, and late stopping in Figure 7. As shown, GRU-Mem can 9 Table 2 Performance when evidence occurs at top 20% positions. Method Metric 112K Context Length 224K 448K MemAgent GRU-Mem (w EG) Perf. % Time Perf. % Time 79.69 171.65 78.91 60.81 78.91 358.60 82.03 111.67 78.12 804. 80.47 213.04 896K 80.47 1691.93 78.12 454.72 Figure 7 The ratio of early, exact, and late exit. (a) Acc (evidence-present). (b) Acc (evidence-free). (c) Ratio of exactly exiting. (d) Validation Reward. Figure 8 Training dynamics of update and exit gates. identify the last evidence position and exit accordingly in most cases. Such property makes GRU-Mem work more flexibly under evidence-unbalanced scenarios. More results are in Appendix D.3. 4.3 (RQ3) Ablation Study In this section, we conduct ablation studies on two aspects: the impact of α selection on training dynamics, and RL trainings improvement to workflow performance. Impact of α. We visualize the training dynamics under different selection of α (i.e., 1.0, 0.9, and 0.5) in Figures 8. Figures (8a) and (8b) reflect the update accuracy on evidence-present (8a) and evidence-free (8b) chunks. Figure 8c shows the ratio of exactly stopping at the last evidence turn, and Figure 8d shows the reward dynamics on the validation set. We have the following observations: higher α improves accuracy on evidence-present chunks, but also increases the risk of unnecessary updating on evidence-free chunks. As shown in Figures (8a) and (8b), when α = 1 (i.e., no reward for the update gate), the accuracy on evidence-free chunks drops dramatically, indicating that without the update reward, LLMs tend to update the memory indiscriminately. mild α leads to balanced update accuracy on both evidence-present and evidence-free chunks. Additionally, with an α of 0.9, the performance on the validation set shows more stable trend, outperforming experiments with α values of 1.0 and 0.5. Moreover, across all α settings, the LLM effectively learns the exit behavior, achieving an exit accuracy over 0.8, as shown in Figure 8c. Based on the above, we adopt the α of 0.9 as the default setting due to the relatively high reward and balanced update gate accuracy. More results about the training dynamics can be found in Appendix D.2. Effectiveness of RL training. We further report the effectiveness of RL training in Figure 9. Here, we report the performance of GRU-Mem with RL (i.e., RL) training and without RL training (i.e., w/o RL). We conduct experiments under the 7B size. For w/o RL, we adopt the same workflow but use the Qwen2.5-7B-Instruct. As shown in this figure, RL training generally brings performance gain across diverse tasks. Specifically, the RL training benefits more on harder tasks such as HQA, SQuAD, and MK series. 10 Figure 9 Effectiveness of RL training."
        },
        {
            "title": "5 Limitations",
            "content": "Several limitations remain in this paper. On one hand, it is limited to the QA domain, with other tasks (e.g., summarization) largely underexplored. On the other hand, the extra rewards in GRU-Mem reduce training stability, requiring smaller off-policy degree and longer convergence time."
        },
        {
            "title": "6 Conclusion",
            "content": "While recent recurrent memory work MemAgent [36] provided chunk-by-chunk paradigm for addressing longcontext reasoning, it suffers from memory explosion due to indiscriminate updates and wasted computation from lacking an early exit mechanism. To address these issues, we proposed GRU-Mem, which equips the recurrent loop with two text-controlled gates: an update gate that updates memory only when necessary and an exit gate that terminates once sufficient evidence is collected, which are trained end-to-end with two reward signals rupdate and rexit. Experiments across diverse long-context reasoning tasks showed GRU-Mem outperforms vanilla MemAgent while achieving up to 400% inference speed acceleration."
        },
        {
            "title": "References",
            "content": "[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding. In ACL. Association for Computational Linguistics, 2024. [2] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. [3] Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Trans. Neural Networks, 5(2):157166, 1994. [4] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready AI agents with scalable long-term memory. CoRR, abs/2504.19413, 2025. [5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. [6] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP, pages 103111. Association for Computational Linguistics, 2014. [7] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [8] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on RAG meeting llms: Towards retrieval-augmented large language models. In KDD, pages 64916501. ACM, 2024. [9] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. [10] Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, and Yonghui Yang. Visplay: Self-evolving visionlanguage models from images. arXiv preprint arXiv:2511.15661, 2025. [11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, 1997. [12] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: whats the real context size of your long-context language models? CoRR, abs/2404.06654, 2024. [13] Gregory Kamradt. Needle in haystack: Pressure testing llms. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack/tree/main, 2023. GitHub repository. [14] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 2020. [15] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhi-Qin John Xu, and Feiyu Xiong. Memos: An operating system for memory-augmented generation (MAG) in large language models. CoRR, abs/2505.22101, 2025. 12 [16] Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, and Jason Weston. SPICE: self-play in corpus environments improves reasoning. CoRR, abs/2510.24684, 2025. [17] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, and Zhaoxiang Zhang. comprehensive survey on long context language modeling. CoRR, abs/2503.17407, 2025. [18] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157173, 2024. [19] Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, YuChiang Frank Wang, Kwang-Ting Cheng, et al. Gdpo: Group reward-decoupled normalization policy optimization for multi-reward rl optimization. arXiv preprint arXiv:2601.05242, 2026. [20] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. CoRR, abs/2503.20783, 2025. [21] Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, and Guanjun Jiang. Search self-play: Pushing the frontier of agent capability without supervision. CoRR, abs/2510.18821, 2025. [22] Ibomoiye Domor Mienye, Theo G. Swart, and George Obaido. Recurrent neural networks: comprehensive review of architectures, variants, and applications. Inf., 15(9):517, 2024. [23] OpenAI. GPT-4 technical report. CoRR, 2023. [24] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. CoRR, abs/2310.08560, 2023. [25] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In ICLR. OpenReview.net, 2024. [26] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, pages 23832392. The Association for Computational Linguistics, 2016. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [28] Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, and An Zhang. Look back to reason forward: Revisitable memory for long-context LLM agents. CoRR, abs/2509.23040, 2025. [29] Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. Search and refine during think: Facilitating knowledge refinement for improved retrieval-augmented reasoning. In NeurIPS, 2025. [30] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [31] Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, and Ming Yan. Qwenlong-l1: Towards long-context large reasoning models with reinforcement learning. CoRR, abs/2505.17667, 2025. [32] Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian J. McAuley, and Xiaojian Wu. Mem-α: Learning memory construction via reinforcement learning. CoRR, abs/2509.25911, 2025. [33] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. In NAACL-HLT, pages 46434663. Association for Computational Linguistics, 2024. 13 [34] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. [35] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In EMNLP, pages 23692380. Association for Computational Linguistics, 2018. [36] Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, and Hao Zhou. Memagent: Reshaping long-context LLM with multi-conv rl-based memory agent. CoRR, abs/2507.02259, 2025. [37] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. In NeurIPS, 2025. [38] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In NeurIPS, 2020. [39] Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, and Hongyuan Zhan. The alignment waltz: Jointly training agents to collaborate for safety. CoRR, abs/2510.08240, 2025. [40] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. CoRR, abs/2506.05176, 2025. [41] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. CoRR, abs/2505.03335, 2025."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Long-context Reasoning Reasoning over long contexts remains challenging for large language models (LLMs) [1, 12], while this capability is important for various downstream tasks such as reading whole book [36]. Existing works show that LLM performance often degrades when relevant evidence is dispersed across long sequences, particularly when key information appears in the middle of the input, which is commonly referred to as lost in the middle [17, 18]. Traditional methods for long-context reasoning can be broadly categorized into architectural modifications [5, 9, 14] and context extension [25, 30] techniques. For architectures, sparse attention mechanisms restrict attention patterns using predefined structures such as sliding windows or global tokens to reduce computational cost [2, 38], while linear attention methods approximate softmax attention to achieve linear-time complexity [14]. For context extension methods, they typically focus on positional embedding extrapolation, such as RoPE [30] and YaRN [25]. While showing effectiveness in addressing long-context reasoning, such traditional methods still suffer from performance degradation in overly long contexts. A.2 LLM Memory To address the limitations that LLMs can only process limited information within their context window size, recent works have begun to explore augmenting the LLM with memory mechanisms [15, 24, 28, 32]. Such memory mechanisms have also been utilized for long-context reasoning recently [15, 36]. The key idea is to read the long context chunk-by-chunk, and recurrently maintain textual memory, and answer the question with the final memory. With end-to-end reinforcement learning, such techniques enable the LLM to significantly outperform the feed-in-once long-context reasoning paradigm [1, 33]. While effective, such methods face risks of memory explosion and lack of exiting mechanism. A.3 Reinforcement Learning with Multiple Tasks Recent efforts have demonstrated that LLMs can learn multiple objectives [16, 21, 29, 41] by rewarding different learning targets. On the one hand, single LLM can learn to play multiple roles that compete with one another, such as acting as both reasoner and data generator [10, 41], where the reward of one role is the loss of another. On the other hand, single LLM can also learn to collaborate across roles, for example, by completing safety alignment task [39] or performing long-context reasoning [28, 36], where the reward comes from the total task completion. These advances inspire us to design role-specific rewards for single policy model, enabling it to learn both update and exit behaviors for long-context reasoning."
        },
        {
            "title": "B Implementation Details",
            "content": "We conduct all the experiments based on the verl 1 framework. For the RL training, we train all models with chunk size of 5,000 tokens, maximum prompt length of 8,192 tokens, and maximum response length of 2,048 tokens. We adopt clip ratio of 0.2 and set the learning rate to 1 106. During training, we sample responses with temperature 1.0 and top-p 1.0, while for validation we use temperature 1.0 and top-p 0.7. The training batch size is set to 128, with rollout number = 16 and PPO mini-batch size 128. We apply learning rate warmup for 20 steps. We stop training until we observe the convergence of reward on the validation set. All the evaluations are conducted on an 8-GPU node. Hyperparameter Chunk size Max prompt length Max response length Clip ratio Learning rate Sampling temperature (train) Top_p (train) Sampling temperature (val) Top_p (val) Train batch size Rollout number (N , train) Mini batch size LR warmup steps Value 5000 8192 2048 0.20 1 106 1.0 1.0 1.0 0.7 128 16 128 20 Table 3 Key training hyperparameters used in RL training. 1https://github.com/volcengine/verl"
        },
        {
            "title": "C Prompts",
            "content": "We present the prompts adopted for the memory agent ϕθ and answer agent ψθ in GRU-Mem in Figure 10a and Figure 10b, respectively."
        },
        {
            "title": "You should also carefully judge whether you have collected enough",
            "content": "Prompt: You are presented with problem, section of an article that may contain the answer to the problem, and previous memory. Please read the provided section carefully. You should reason about whether the new section contains useful information about the problem, and then update the memory with the new information that helps to answer the problem. Be sure to retain all relevant details from the previous memory while adding any new, useful information. information to answer the problem. You should reason about whether the new section contains useful information, what to update, and what to do next first between <think> and </think>. If the new section contains useful information about the problem, you should first generate <check>yes</check>. </update>. If the new section does not contain useful information about the problem, you should first generate <check>no</check>. unchanged between <update> and </update>. In the end, if you havent collected enough information for the problem, return <next>continue</next>. <next>end</next>. <problem> {prompt} </problem> <memory> {memory} </memory> <section> {chunk} </section> After that, update the new memory between <update> and ONLY when enough information is collected, return After that, you should keep the previous memory (a) Prompt for memory agent ϕθ in GRU-Mem. Prompt: You are presented with problem and previous memory. on the previous memory and put the answer in boxed . <problem> {prompt} </problem> <memory> {memory} </memory> Your answer:"
        },
        {
            "title": "Please answer the problem based",
            "content": "(b) Prompt for answer agent ψθ in GRU-Mem. Figure 10 Prompts for memory and answer agents."
        },
        {
            "title": "D Experiments",
            "content": "D.1 Performance Under Varying Context Lengths We present the models performance and efficiency across range of context lengths in Figures 11 to 20. In these figures, numerical values represent performance metrics, while color shading indicates the acceleration ratio, where deeper hues corresponding to higher acceleration ratio. As shown, GRU-Mem consistently yields higher inference speed, with the acceleration more obvious as the context length increases. (a) 7B. (b) 3B. Figure 11 Performance and efficiency across varying context lengths on HQA. (a) 7B. (b) 3B. Figure 12 Performance and efficiency across varying context lengths on SQuAD. (a) 7B. (b) 3B. Figure 13 Performance and efficiency across varying context lengths on SK-1. (a) 7B. (b) 3B. Figure 14 Performance and efficiency across varying context lengths on SK-2. (a) 7B. (b) 3B. Figure 15 Performance and efficiency across varying context lengths on SK-3. (a) 7B. (b) 3B. Figure 16 Performance and efficiency across varying context lengths on MK-1. (a) 7B. (b) 3B. Figure 17 Performance and efficiency across varying context lengths on MK-2. (a) 7B. (b) 3B. Figure 18 Performance and efficiency across varying context lengths on MK-3. (a) 7B. (b) 3B. Figure 19 Performance and efficiency across varying context lengths on MQ. (a) 7B. (b) 3B. Figure 20 Performance and efficiency across varying context lengths on MV. 19 D.2 Training Dynamics We report more details about the training dynamics under different α in this section, including the format correctness (Figure 21a), the average response length (Figure 21b), the absolute exit deviation (Figure 21c), the exit deviation (Figure 21d), the ratio of early exit (Figure 22b), and the ratio of late exit (Figure 22c). We have the following observations: Correct formatting is rapidly acquired. As shown in Figure 21a, under all the settings of α, the format correctness quickly reaches around 100% accuracy with only few steps. The introduction of update gate reduces the increase of response length. As shown in Figure 21b, higher α exhibits sharper response length increasing trend. This is because the introduction of the update gate encourage the memory agent only to update on evidence-present chunks, which reduces unnecessary updates on evidence-free chunks and thus lowers the average response length. The correct exit behavior is gradually learned. As shown in Figure 21c and Figure 21d, the average absolute exit deviation (i.e., texit tlast evidence) decreases stably across different settings of α, and the exit deviation (i.e., texit tlast evidence) also converges to near zero. Additionally, the ratio of exactly exiting at the correct place also increases stably (i.e., Figure 22a), while the ratio of early exit and late exit decreases accordingly (i.e., Figure 22b and Figure 22c). (a) The format correctness. (b) Response length. (c) The abs. exit deviation. (d) The exit deviation. Figure 21 Training dynamics (part 1). (a) The ratio of exact exiting. (b) The ratio of early exiting. (c) The ratio of late exiting. Figure 22 Training dynamics (part 2). 20 D.3 Performance Under Unbalanced Evidence Distribution We further test the performance and exit accuracy when evidence only occurs at the top 10% position. We report the performance and efficiency in Table 4. We also report the ratio of early exit, exact exit, and late exit in Figure 23. As shown, under the unbalanced evidence occurrence distribution, where the last evidence must occur at the top 10% documents, GRU-Mem also shows an accurate ratio of exiting at the correct place (i.e., around 80%). With this accurate exiting ratio, GRU-Mem exhibits much faster inference time acceleration, while maintaining the same performance. Table 4 Performance when evidence occurs at top 10% positions. Method Metric 112K Context Length 224K 448K MemAgent GRU-Mem (w EG) Perf. % Time Perf. % Time 78.12 178. 78.12 62.65 76.56 373.26 81.25 102.48 79.69 799.81 80.47 205.15 896K 78.91 1652.31 78.12 405.54 Figure 23 The ratio of early, exact, and late exit."
        },
        {
            "title": "E Case Study",
            "content": "In this section, we present several cases about how the memory agent in GRU-Mem works, together with the memory updating dynamics. Case 1: What science fantasy young adult series, told in first person, has set of companion books narrating the stories of enslaved worlds and alien species? Ground Truth: Animorphs. Prediction: Animorphs. <think>The provided section does not contain any information about science fantasy young adult series or companion books narrating the stories of enslaved worlds and alien species. Therefore, there is no new useful information to add to the existing memory.</think> <check>no</check> <update>No previous memory</update> <next>continue</next> Figure 24 Response at turn 65 (evidence-free chunk)."
        },
        {
            "title": "We need to check if this series has companion books that narrate the",
            "content": "<think>The new section mentions Änimorphs,which is described as science fantasy series for young adults. It is told in first person and involves multiple narrators. The text also describes the core themes of the series, but does not mention anything about companion books or enslaved worlds. stories of enslaved worlds and alien species.</think> <check>yes</check> <update>Animorphs is science fantasy series of young adult books written by Katherine Applegate and her husband Michael Grant, writing together under the name K. A. Applegate, and published by Scholastic. taking turns narrating the books through their own perspectives. dehumanization, sanity, morality, innocence, leadership, freedom and growing up are the core themes of the series.</update> <next>continue</next> It is told in first person, with all six main characters Horror, war, Figure 25 Response at turn 66 (evidence-present chunk). 22 <think>The new section provides information about The Hork-Bajir Chronicles, which is companion book to the Änimorphsseries. It mentions that The Hork-Bajir Chroniclestakes place before The Pretenderänd narrates the story of the Hork-Bajir and their enslavement. This seems relevant to the question, as it suggests that the Änimorphsseries has companion books that tell the story of an enslaved species, which could potentially answer the problem.</think> <check>yes</check> <update>Animorphs is science fantasy series of young adult books written by Katherine Applegate and her husband Michael Grant, writing together under the name K. A. Applegate, and published by Scholastic. taking turns narrating the books through their own perspectives. Chroniclesïs the second companion book to the nimorphs series, written by K. A. Applegate. With respect to continuity within the series, it takes place before book #23, The Pretender, although the events told in the story occur between the time of The Ellimist Chroniclesänd The Andalite Chronicles. The book is introduced by Tobias, who flies to the valley of the free Hork-Bajir, where Jara Hamee tells him the story of how the Yeerks enslaved the Hork-Bajir, and how Aldrea, an Andalite, and her companion, Dak Hamee, Hork-Bajir, tried to save their world from the invasion. view of Aldrea, Dak Hamee, and Esplin 9466, alternating in similar fashion to the Megamorphsbooks.</update> <next>end</next> It is told in first person, with all six main characters Jara Hamees story is narrated from the points of The Hork-Bajir Figure 26 Response at turn 74 (evidence-present & last evidence chunk). 23 Case 2: What is the inhabitant of the city where 122nd SS-Standarte was formed in 2014? Ground Truth: 276,170 inhabitants. Prediction: 276,170. <think>The provided section does not mention anything related to the 122nd SS-Standarte or the city where it was formed in 2014. address the problem.</think> <check>no</check> <update>No previous memory</update> <next>continue</next> Therefore, there is no useful information here to Figure 27 Response at turn 15 (evidence-free chunk). <think>The new section contains information about the 122nd SS-Standarte and its formation in Strasbourg. This directly relates to the problem which asks about the inhabitant of the city where the 122nd SS-Standarte was formed in 2014. The new information is relevant.</think> <check>yes</check> <update>The 122nd SS-Standarte was regimental command of the Allgemeine-SS that was formed in the city of Strasbourg during World War II. The Standarte was activated on November 12, 1940, and reached battalion strength by the end of the year. the previously disbanded 121st SS-Standarte, also situated in Strasbourg.</update> <next>continue</next>"
        },
        {
            "title": "The command was a successor to",
            "content": "Figure 28 Response at turn 16 (evidence-present chunk). 24 <think>The new section provides detailed information about Strasbourg, stating that in 2014, the city proper had 276,170 inhabitants and that both the Eurométropole de Strasbourg and the Arrondissement of Strasbourg had 484,157 inhabitants. The section also mentions that Strasbourg is the capital and largest city of the Grand Est region of France. Given that the original passage stated that the 122nd SS-Standarte was formed in Strasbourg during World War II, this new information directly addresses the problem by identifying the city and its current population.</think> <check>yes</check> <update>Strasbourg ( , ] ; Alsatian: Strossburi; German: largest city of the Grand Est region of France and is the official seat of the European Parliament. Located close to the border with Germany in the historic region of Alsace, it is the capital of the Bas-Rhin département. and both the Eurométropole de Strasbourg (Greater Strasbourg) and the Arrondissement of Strasbourg had 484,157 inhabitants. Strasbourgs metropolitan area had population of 773,347 in 2013 (not counting the section across the border in Germany), making it the ninth largest metro area in France and home to 13% of the Grand Est regions inhabitants. The transnational Eurodistrict Strasbourg-Ortenau had population of 915,000 inhabitants in 2014. in the city of Strasbourg during World War II. The Standarte was activated on November 12, 1940, and reached battalion strength by the end of the year. the previously disbanded 121st SS-Standarte, also situated in Strasbourg.</update> <next>end</next> The 122nd SS-Standarte was regimental command of the Allgemeine-SS that was formed In 2014, the city proper had 276,170 inhabitants Straßburg] ) is the capital and"
        },
        {
            "title": "The command was a successor to",
            "content": "Figure 29 Response at turn 16 (evidence-present chunk). 25 Case 3: special magic uuid is hidden within the following text. Make sure to memorize it. What is the special magic uuid for belligerent-councilperson mentioned in the provided text? Ground Truth: ac5f891e-ac5d-4d3b-b830-a7efe7cf807a. Prediction: ac5f891e-ac5d-4d3b-b830-a7efe7cf807a. <think>The new section does not mention any belligerent-councilpersonör special magic UUID. It discusses topics related to addiction, technology, and Apples App Store approval process. Therefore, the new section does not provide useful information to identify the special magic UUID for the belligerent-councilperson.</think> <check>no</check> <update>No previous memory</update> <next>continue</next> Figure 30 Response at turn 1 (evidence-free chunk). <think>The provided section does not mention any special magic UUID associated with belligerent-councilperson. The text is about various topics including essay writing, venture capital, and some historical context related to startup. referenced.</think> <check>no</check> <update>No previous memory</update> <next>continue</next>"
        },
        {
            "title": "No specific UUID is",
            "content": "Figure 31 Response at turn 22 (evidence-free chunk). <think>The new section does not contain any information related to special magic uuidör the context of belligerent-councilperson. startup culture, and Paul Grahams experiences. information to solve the problem.</think> <check>no</check> <update>No previous memory</update> <next>continue</next> It discusses various aspects of essay writing, Therefore, it does not provide useful Figure 32 Response at turn 28 (evidence-free chunk). <think>The provided section does not mention belligerent-councilpersonör any special magic UUID. The content discusses startups, the internet economy, and the changing dynamics between employers, investors, and young talent. problem is present.</think> <check>no</check> <update>No previous memory</update> <next>continue</next>"
        },
        {
            "title": "No information related to the specific",
            "content": "Figure 33 Response at turn 66 (evidence-free chunk). <think>The new section contains information about special magic UUID for belligerent-councilperson, which is relevant to the problem. It provides the specific UUID: ac5f891e-ac5d-4d3b-b830-a7efe7cf807a.</think> <check>yes</check> <update>One of the special magic uuids for belligerent-councilperson is: ac5f891e-ac5d-4d3b-b830-a7efe7cf807a. <next>end</next> </update> Figure 34 Response at turn 100 (evidence-present & last evidence chunk)."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "National University of Singapore",
        "University of Science and Technology of China"
    ]
}