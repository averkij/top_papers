{
    "paper_title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective",
    "authors": [
        "Ming Li",
        "Yanhong Li",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 3 4 7 3 2 . 0 1 4 2 : r What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: Gradient Perspective Ming Li University of Maryland minglii@umd.edu Yanhong Li University of Chicago yanhongli@uchicago.edu Tianyi Zhou University of Maryland tianyi@umd.edu"
        },
        {
            "title": "Abstract",
            "content": "What makes difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chainof-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/ Layer_Gradient."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) excel at various complex tasks (Zhao et al., 2023b; Xu et al., 2024). But their complexity notoriously makes them black-box whose inner mechanisms and training behaviors remain mysterious (Zhao et al., 2023a; Singh et al., 2024). How do they acquire reasoning capabilities or knowledge? When do 1 they make mistakes, and why? What change was made to each layer during training? This lack of transparency extends to issues like unintentional generation of harmful or biased content (Huang et al., 2024; Li et al., 2024a) or hallucinations (Huang et al., 2023) and might hinder further understanding and mitigation of them. Interpretable machine learning either develops models that are inherently interpretable (Rudin et al., 2022) or adopts post-hoc interpretability methods (Krishna et al., 2024), which do not alter the underlying model architecture but analyze models after training completes (Gurnee et al., 2023; Zou et al., 2023; Wang et al., 2023a; Wu et al., 2024). Despite the broad advancements in interpreting static models, gradients on dynamic training patterns of LLMs remains underexplored, especially on how these gradients scale and distribute across different layers. Such an understanding is crucial as it directly reflects how the LLMs perceive training data and align with them. Recently, there has been growing trend for layer-related methods for LLMs: Gao et al. (2024) propose that higher layers of LLM need more LoRA; Li et al. (2024e) identify some of the layers in LLM related to safety, etc (Men et al., 2024; Chen et al., 2024). However, for the layer-wise analysis of LLMs, current research mainly employs probing methods (Alain and Bengio, 2017; Ju et al., 2024; Jin et al., 2024; Ye et al., 2024) that assess model behavior by observing changes in performance when certain layers are modified or removed (Wang et al., 2023a; Fan et al., 2024). These studies have been instrumental in illustrating how different layers capture and process various types of information while they often do not provide direct insights into the gradients that drive the learning process. Hence, we are motivated to move step forward by directly investigating the layer-wise gradients inside LLMs. Our study focuses on the post-training gradient Task Type Dataset Correct Irrelevant None CoT Simplified CoT Detailed CoT (GPT4o) Base LLMs Instructed LLMs Math Commonsense Wiki Knowledge AQuA GSM8K MATH-Algebra MATH-Counting MATH-Geometry StrategyQA ECQA CREAK Sensemaking Popular (Length 100) Popular (Length 500) Popular (Length 1000) Unpopular Table 1: The scope of our study. We compare the gradient patterns across different layers when training pretrained base LLMs vs. instruction-finetuned LLMs using correct vs. irrelevant responses, slow vs. fast thinking (None CoT, Simplified CoT, and Detailed CoT generated by GPT-4o) responses, on three types of tasks: Math, Commonsense Reasoning, and Wiki Knowledge Learning. The comparison of slow vs. fast thinking only applies to the first two types of tasks, and it is replaced by the comparison between different lengths of responses on the third type of task. Our study is conducted on 5 pretrained base LLMs and 5 instruction-finetuned LLMs. of LLMs for instruction-tuning on instructionresponse pairs (Mishra et al., 2021; Wei et al., 2022; Wang et al., 2023b; Taori et al., 2023; Xu et al., 2023; Li et al., 2024d,c,b; Zhang et al., 2023). Instead of finetuning LLMs, we investigate the layer-wise gradients of 5 base LLMs and 5 instruction-finetuned LLMs on different data, including (1) three types of tasks including Math, Commonsense Reasoning, and Knowledge Extraction, with several datasets per task type; (2) correct vs. irrelevant responses; and (3) fast vs. slow thinking, which corresponds to different levels of Chain of Thought (CoT) (Wei et al., 2023) reasoning paths. Table 1 summarizes the scope of our study. Our study is based on comparisons of layerwise gradients in terms of their spectral properties achieved by Singular Value Decomposition (SVD), focusing particularly on the projection layers for Query, Key, Value, and Output in transformer architectures (Vaswani et al., 2017). Specifically, we measure the gradient by its nuclear norm, compare the gradient norms across layers, and measure the sensitivity of gradients to different training data or initial models by the difference in gradient norm. These metrics serve as quantitative tools for examining the training behaviors and shed novel insights that could inform more efficient training strategies and analyses of model stability. Main Contribution.1 This paper investigates 1In addition to the observations and analysis included in this paper, all the gradient statistics (that cost thousands of GPU hours) within our experimental scope will be released in our GitHub repository. At the sample level, the instructionresponse pair, and the corresponding loss value are included. At the layer level, the mean, maximum, and minimum values, the Frobenius norm, and Nuclear norm, and the maximum the behaviors of the gradient across different layers of LLMs through spectral analysis of the layer-wise gradients. We compare the gradients of slow vs. fast thinking rationals when training different initial models using correct vs. irrelevant responses on different tasks. The difference in gradients reflects how these factors affect training dynamics, and, reversely, how sensitive LLM training is to these factors. Our observations reveal previously unrecognized patterns and shed novel insights for improving the stability and efficiency of LLM training. Our key findings are highlighted in the following: 1. Training LLMs for slow thinking (Detailed CoT) leads to similar gradient norms of different layers, while fast thinking (Simplified/None CoT) results in larger gradients (fast forgetting) of earlier layers and drastic differences across layers. 2. The gradient of slow thinking (Detailed CoT) helps distinguish correct responses from irrelevant responses. Without CoT, the gradient patterns of the two types of responses are similar. 3. The instruction-finetuned LLMs do not show superior capability over pre-trained base LLMs in identifying incorrect reasoning paths. 4. The above observations on reasoning tasks (math and commonsense) cannot be extended to knowledge learning tasks, where simply increasing response length does not show similar gradient patterns as slow thinking. and minimum singular values of each gradient matrix are included. We hope these gradient statistics can contribute to the communitys understanding of the gradient behaviors for different settings."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "In our experiments, we utilize the most commonly used instruction tuning settings to investigate the gradients for LLM fine-tuning. Given an instruction-tuning dataset D, each data sample is represented by tuple (ins, res), where ins represents the instruction and res represents the corresponding response. Let pθ denote the LLM with parameters θ. In the instruction tuning setting, pθ is typically fine-tuned by minimizing the following loss on each sample (ins, res), in which resj represents the jth token of response res, res<j represents the tokens before resj, and represents the token length of res: Lθ = 1 (cid:88) j=1 log pθ (resjins, res<j) , (1)"
        },
        {
            "title": "2.2 Gradient Representation",
            "content": "The attention mechanism (Vaswani et al., 2017) is one of the most critical parts of modern LLMs, and it dominates the behavior of LLMs. Thus in this paper, we mainly focus on the gradients of the layers related to the attention mechanism, including the Query (Q), Key (K), Value (V) projection layers and later output projection layer denoted as Output (O). Considering the LLM contains attention layers in total, after the loss calculation and Backpropagation, the resulting gradient for each projection layer is matrix with the same size as its weights, which can be notated as GQ,i, GK,i, GV,i, and GO,i for the corresponding projection layers, where [0, 1] represents the index of each layer in the LLM. Due to the dramatically large number of layers and parameters of the modern Large language models, it is unrealistic to directly investigate these large gradient matrices. Motivated by the advanced Singular Value Decomposition (SVD) technology used in this area (Biderman et al., 2024; Carlini et al., 2024), we utilize Nuclear Norm (the ℓ1 norm of singular values) to represent the characteristics for the gradient of each layer, especially its strength. SVD is factorization of real or complex matrix that generalizes the eigendecomposition of square normal matrix to any matrix via an extension of the polar decomposition. Specifically, e.g., for our gradient matrix GX,i Rmn, {Q, K, V, O}, it can be decomposed as: GX,i = ΣV (2) where Rmm is an orthogonal matrix containing the left singular vectors; Σ Rmn is diagonal matrix with singular values σ1, σ2, . . . , σmin{m,n}; Rnn is an orthogonal matrix containing the right singular vectors. For simplicity, the subscript for these intermediate matrices is omitted. Nuclear Norm: The nuclear norm of is defined as the ℓ1 norm of the singular values, which reflects the sparsity of the spectrum and serves as convex surrogate of the matrix rank. Hence, it does not only quantify the gradient magnitude but also the concentration of the spectrum on its top few singular values, which is vital to understand the gradient patterns in each layer, i.e., sX,i = GX,i = min{m,n} (cid:88) j=1 σj (3) σ1 Ratio: We define the σ1 ratio as the ratio of the largest singular value to the nuclear norm, which indicates the concentration of gradient effects, showing how much of the gradients magnitude is captured by its most principal component, which to some extent represents the diversity of directions of current gradient 2: rX,i = σ1 sX,i = σ1 GX,i (4)"
        },
        {
            "title": "2.3 Metrics of Gradient Difference",
            "content": "In our experimental analysis, the nuclear norm sX,i of each layer will not be investigated individually, but to investigate the overall dynamic characteristics across every layer of the LLM. For simplicity, we notate the nuclear norm value, sX,i, of specific metric across all the layers as curve notated as sX . To analyze these gradient results, the visualization of the layer-wise curves is one of the most important tools to get qualitative understanding of how the gradients change across the layers. However, quantitative analysis is still required for better understanding of gradient representations. Gradient-Norm Difference between Layers. Considering that both the fluctuation and scale are important for gradient curve, we utilize the Mean 2Due to the page limits, we do not include analysis based on the σ1 ratio in our main content, but the curves are all presented in the appendix. These statistics can also be found in our GitHub repository. 3 Absolute Difference, MAD, to represent gradient curve. Specifically, the MAD of the curve sX is notated as MADsX , which is calculated as: MADsX ="
        },
        {
            "title": "1\nN − 1",
            "content": "N 1 (cid:88) i=1 sX,i+1 sX,i (5) where is the total number of layers of the target LLM. MAD is measure of the average magnitude of change between consecutive points. Unlike standard deviation, MAD focuses on the direct differences between successive values without squaring or taking into account the direction (positive or negative change). It is useful for quantifying the overall variability of the data, especially when detecting fluctuations or local changes, which are more important than global trends. Gradient-Norm Difference between Initialmodels or Training-responses. In addition to the value presentation for each individual curve, the pair-wise comparison between two curves, across different layers, is also important for our analysis. For this purpose, we use the layer-wise Relative Difference, RD, as our metric. At each layer, the RD between 2 values s(1) X,i is calculated as: X,i and s(2) X,i RDX,i = X,i s(1) s(2) s(1) X,i X,i is utilized as the reference value. For this metric, the projection layer and the layer index should be kept the same to ensure this value is meaningful. where s(1) (6)"
        },
        {
            "title": "3.1 Models",
            "content": "We investigate the gradients for 10 models including 5 base pre-trained models, Qwen2-1.5B (Yang et al., 2024), gemma-2-2b (Team et al., 2024), Llama-3.1-8B (Dubey et al., 2024), gemma2-9b (Team et al., 2024), Llama-2-7b-hf (Touvron et al., 2023) and their instruction-tuned version, Qwen2-1.5B-Instruct, gemma-2-2b-it, Llama-3.18B-Instruct, gemma-2-9b-it, Llama-2-7b-chat-hf. The main illustrations in our paper will be based on the results of Qwen2-1.5B models, results for other models will be in the appendix. The Math and Commonsense Reasoning datasets are utilized to explore the gradients when LLMs are fine-tuned to learn the reasoning process (slow or fast thinking) and the Wiki Knowledge datasets are utilized to explore the gradients when LLMs are fine-tuned to learn pure knowledge. Due to the slow process of calculating gradients, we randomly sample 500 data instances for each task for our extensive experiments. The scope of our experiments is shown in Table 1. Detailed descriptions of these datasets and corresponding examples can be found in Appendix A."
        },
        {
            "title": "3.2.1 Math Reasoning",
            "content": "For the category of math, AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021) are utilzied. The original ground truth format for AQuA is options from to E, and for GSM8K is the resulting digits, and additional CoT reasoning paths are provided as well. During our experiments, both the process of learning the original ground truth (fast thinking) and learning the CoT plus ground truth are investigated. The ground truth for MATH is the complete solution for the question, which can be regarded as answers with CoT. We select the question types Algebra, Counting, and Geometry in MATH for our experiments. Moreover, to further explore the effects of more detailed reasoning paths (slow thinking), GPT4o is utilized to obtain detailed version of CoT paths."
        },
        {
            "title": "3.2.2 Commonsense Reasoning",
            "content": "For the category of commonsense reasoning, four datasets are utilized including StrategyQA (Geva et al., 2021), ECQA (Aggarwal et al., 2021), CREAK (Onoe et al., 2021), and Sensemaking, obtained from the FLAN collection (Longpre et al., 2023). The original ground truth format for StrategyQA is options of yes and no, for ECQA is word options, for CREAK is options of yes and no, and for Sensemaking is options of Sentence or Sentence B. For all these datasets, the corresponding human-annotated CoT reasoning paths are provided, and both the process of learning the original ground truth and learning the CoT plus ground truth are investigated in our experiments. Similarly, further GPT4o-generated CoTs are also investigated."
        },
        {
            "title": "3.2.3 Wiki Knowledge Learning",
            "content": "The datasets we use include three categories: Math, Commonsense Reasoning, and Wiki Knowledge. This group of tasks represents LLMs learning on pure knowledge-intensive responses without any 4 Figure 1: The nuclear norm of gradients across different layers (x-axis) when trained with fast to slow reasoning paths (left to right columns), on (a) AQuA and (b) ECQA datasets. When detailed CoT is utilized for training, the gradient norm tends to be similar across layers (on both math and commonsense reasoning tasks). Note the y-axis scale for None CoT is larger, and the scale for Simplified CoT and Detailed CoT are the same. reasoning process. For Wikipedia knowledge, we categorize Wiki articles into two groups: popular and unpopular based on their total page views in 2021 using the Pageviews Analysis tool3 provided by Wikimedias RESTBase API. After selection, we extract the first 100, 500, and 1000 tokens (respecting paragraph boundaries) from each article for the controlled experiments on the effects of response lengths. For the unpopular wiki pages, we use the least viewed articles in 20214. As the length of unpopular wiki pages is generally short, we take the full articles; if the length exceeds 1024 tokens, we truncate them with respect to sentence boundaries."
        },
        {
            "title": "4.1 Math and Commonsense Reasoning Tasks",
            "content": "This section focuses on tasks related to CoT reasoning, which includes the datasets within the Math and Commonsense Reasoning task type."
        },
        {
            "title": "4.1.1 Slow vs. Fast Thinking\nIn this section, we investigate the gradient\nbehaviors when LLMs learn the responses with the\nreasoning process. i.e. the CoT paths. For samples\nin the MATH dataset, the original responses already\ncontain the necessary steps to solve the question,\nwhich we notate as Simplified CoT setting. For the",
            "content": "3https://pageviews.wmcloud.org 4https://en.wikipedia.org/wiki/User:Colin_M/ Least_viewed_articles_in_2021 Figure 2: The nuclear norm of gradients across different layers (x-axis) when trained with Correct vs. Irrelevant responses (a) without CoT (fast thinking); (b) with detailed CoT (slow thinking), on the AQuA dataset. Gradient norm can help identify correct responses when provided Detailed CoT. But this does not extend to gradients without CoT. remaining datasets in these two task types, both the pure answer (resulting digits or options), and short CoT paths are provided, which we notate as None CoT and Simplified Cot settings. These configurations can help us to understand the gradients when LLMs learn response with or without CoT, probably revealing the advantages of CoT training. Moreover, the provided CoT paths are all too simplified, which might still be hard for LLMs to build the connections from question to CoT path to final answers. Thus we further prompt GPT4o to gen5 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All AQuA ECQA sQ sK sV sO sQ sK sV sO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.76 0.89 0.23 7.20 1.01 0.22 37.29 5.08 1. 23.79 3.31 0.82 8.00 1.11 0.30 11.51 1.34 0.26 59.33 8.53 1.56 39.20 5.56 1.00 4.13 0.52 0. 6.29 0.56 0.21 16.12 2.14 0.62 14.35 2.18 0.75 7.01 0.70 0.37 11.07 1.24 0.29 24.83 3.55 0. 19.50 3.33 0.97 3.49 0.77 0.29 8.40 1.11 0.42 3.94 0.86 0.33 3.04 0.63 0.29 5.01 0.86 0. 13.32 1.01 0.54 7.46 1.66 0.48 5.12 1.41 0.52 4.42 0.69 0.28 7.06 0.81 0.27 17.32 2.36 0. 12.91 1.97 0.64 6.53 0.85 0.35 11.11 1.13 0.34 27.40 4.01 0.82 19.38 3.22 0.85 Table 2: The mean absolute differences (MAD) of gradients nuclear norm for K, Q, V, projection layers. Early, Middle, Last, and All represent the MAD scores calculated across the early, middle, last, and all the layers across the LLM. consistent decrease is observed in all layers when LLMs are trained to produce more detailed reasoning paths (slow thinking). Dataset CoT Curve RD Average Top 5 Different Layer Idx 4 1 2 5 AQuA Detailed ECQA Detailed sQ sK sV sO sQ sK sV sO 0.81 0.90 0.81 0.72 0.46 0.50 0.47 0.41 3 3 3 0 0 0 3 0 0 4 1 3 3 1 1 1 1 4 4 2 1 2 2 4 0 2 2 1 2 4 3 2 7 0 4 4 0 4 Table 3: The average relative difference (RD) and the indexes of the top-5 layers that have the greatest gap between the curves of learning the correct and irrelevant responses. It shows that the earlier layers change more sensitively to irrelevant responses. erate more detailed version of reasoning paths to compare the effect of different CoT paths, i.e. slow or fast thinking, which is notated as Detailed CoT. The detailed statistical values for the gradient curves on AQuA and ECQA with different CoT settings are provided in Table 2, and the visualization of the gradient curves on AQuA and ECQA is shown in Figure 1, all the results in this table are based on the Qwen2-1.5B model more results can be found in the appendix. When no CoT reasoning paths are provided for LLMs to learn, fast thinking, the mean absolute differences (MADs) are the largest on all the curves of different projection layers, representing severe fluctuation of the gra6 dient scales across all the layers of the LLM, which might cause instability for training (Glorot and Bengio, 2010). However, when CoT paths are given, the MADs drop accordingly, especially when the detailed CoT paths are given, slow thinking, as visualized in Figure 1. The large scale indicates that the response distributions that LLMs are going to learn have large discrepancies with what it has learned from the pretraining phase, which might harm the performances of the original pre-trained models (Ghosh et al., 2024; Biderman et al., 2024). Our findings are aligned with the current success of utilizing more detailed CoT reasoning paths or responses for training (Mitra et al., 2023; Li et al., 2023) and provide another perspective on understanding the effectiveness of slow thinking."
        },
        {
            "title": "4.1.2 Effect of Response Correctness",
            "content": "In this section, we investigate the gradient behaviors when LLMs are learning the correct or irrelevant responses with different reasoning paths. Similarly, for datasets that pure ground truths are given, we investigate the gradient behaviors on three settings: None CoT, Simplified Cot, and Detailed CoT, otherwise only the last two settings can be investigated. In the None CoT setting, we directly shuffle the answers across the dataset and make sure every question has the incorrect answer; In the Simplified Cot and Detailed CoT settings, we split every CoT path into individual sentences, then shuffle the sentences across the dataset. Under this circumstance, each sentence in the response is still complete, while the relation across sentences will be logically wrong, simulating the irrelevant CoT reasoning paths. In these experiments, we try to investigate if the LLMs are able to identify the irrelevant responses during training with slow or fast thinking, reflected by gradient behaviors. The visualizations of LLMs learning on correct and irrelevant responses are presented in Figure 2, which contains 2 settings including (a) None CoT and (b) Detailed CoT. It is widely accepted that LLMs have learned all the knowledge in the pretraining phase (Zhou et al., 2023), so when LLMs are forced to learn responses that conflict with their internal knowledge, more efforts should be made (larger gradient) for this false alignment. However, from the visualizations, it can be observed that when no CoT paths are given, the gradient behaviors between learning the correct and nonsense responses are almost identical and their relative difference values on all the projection Figure 3: The nuclear norm of gradients across different layers (x-axis) on (a) pre-trained base LLM vs. (b) instruction-finetuned LLM. On both models, training using detailed CoT (slow thinking) reduces the gradient norm and difference across layers. However, the two models gradient patterns differ when training with fast thinking (Simplified/None CoT). The y-axis scale of None CoT is greater than that of Simplified CoT and Detailed CoT. layers are less than 0.01. Thus this phenomenon indicates that LLMs can not build the necessary mapping relations from the question to the answer, without explicit reasoning paths being given. On the contrary, when the detailed CoT reasoning paths are provided in the responses, the gradient behaviors will be different, mainly reflected by the larger scale of the gradient. This phenomenon indicates that LLMs can to some extent identify that the responses to be learned have potential conflicts with their internal knowledge, thus requiring more energy to adapt to the new nonsense responses. We further investigate if there are specific layers that are directly related to LLMs capability to perceive irrelevant knowledge, reflected by the larger differences between gradients of correct and irrelevant responses. As shown in Table 3, the relative difference values of the nuclear norm curves, and the indexes of the top 5 layers that have the greatest gap are presented. The results on the nuclear norm curves show that the earlier layers are more sensitive to nonsense responses, which might indicate the potential effects of earlier layers."
        },
        {
            "title": "4.1.3 Effect of Initial Models",
            "content": "In this section, we compare the gradient behaviors between pre-trained base LLMs and aligned instructed LLMs. For each instructed LLM, we utilize the conversation templates officially provided, avoiding the potential misalignment. In these experiments, we observe that the instructed LLMs do not have much better performance in identifying the irrelevant responses, evidenced by the minor relative differences between gradient curves obtained from base LLMs and aligned LLMs. However, as shown in Figure 3, although the tendencies are consistent on both types of LLMs that detailed CoT reasoning paths make the scale and fluctuation of gradient smaller and smoother, the gradients on simplified CoT show large discrepancy between the two types of LLMs. This discrepancy means that the instructed LLMs need more energy than the pre-trained LLMs to learn the simplified CoT paths. The phenomenon shows that (1) the distribution of the simplified CoT responses might have non-negligible discrepancies with the instruction datasets used for training this LLM; (2) the behaviors on gradient curves might be used as measurement of how specific data sample aligned with the internal knowledge of LLMs, which might be useful for the continued training settings."
        },
        {
            "title": "4.2.1 Effect of Response Length & Popularity",
            "content": "In this section, we investigate the effect on response length and popularity for the knowledge-intensive task. As shown in Figure 4, the left 3 columns represent the scenarios when LLMs are learning popular knowledge with different response lengths and the right-most column represents the scenario when 7 Figure 4: The nuclear norm of gradients across different layers (x-axis) when trained with responses of different lengths (left 3 columns) and unpopular knowledge (rightmost column) on the Wiki knowledge learning (knowledgeintensive) task. Comparing Short vs. Long, Popular vs. Unpopular, and Correct vs. Irrelevant on the two types of models indicates: (1) longer response=slower thinking. Unlike Figure 1, solely increasing the response length does not affect the gradient patterns; (2) unpopular knowledge triggers larger gradients; (3) Unlike Figure 2, gradient norm cannot help judge the responses correctness on knowledge-intensive tasks. LLMs are learning unpopular knowledge. By comparing the left 3 columns of the figure, it is observed that for the knowledge-intensive task, the lengths of responses do not affect the gradient scales and fluctuations. This phenomenon is largely different from the findings observed in the reasoning tasks, where detailed CoTs can largely reduce the gradient scale and fluctuation. This comparison can further verify that the effects of the detailed CoT (slow thinking) in the responses are not caused by the increase in token length but by the detailed reasoning process. On the contrary, as shown in the right-most figures, when LLMs are learning unpopular knowledge, the scales and fluctuation increase dramatically, representing LLMs need more effort to learn this unpopular knowledge. This phenomenon is reasonable as popular knowledge occurs frequently from diverse corpus sources, representing an augmentation of this knowledge."
        },
        {
            "title": "Models",
            "content": "Then we compare the curve differences when LLMs are learning the correct or nonsense responses as shown in Figure 4. The almost identical curves between (a) and (c) show that the pre-trained LLM is not able to identify the nonsense knowledge that it is learning, and the curves between (b) and (d) show that the instructed LLM also lacks this capability. This phenomenon is also different from the findings observed in the reasoning tasks, where LLMs are able to identify the nonsense reasoning paths reflected by the increase of gradient scales and fluctuation. As for the effect of the instructed LLMs, comparing between (a) (b), and (c) (d), consistent increase in the gradient scales and fluctuation is observed especially on the unpopular knowledge. This phenomenon indicates that it is harder for in8 structed LLMs to learn knowledge that is new or not well-learned during the pre-training phase."
        },
        {
            "title": "5 Conclusion",
            "content": "Our study reveals the significant differences in gradient behaviors between fast and slow thinking training methodologies in LLMs, offering insights into how training dynamics can influence these models. Specifically, we observe that slow thinking leads to stable gradient norms of different layers, while fast thinking results in larger gradients and fluctuation across layers. Moreover, the gradient of slow thinking helps distinguish correct responses from irrelevant responses, while without CoT, the gradient patterns of the two types of responses are similar. The above observations on reasoning tasks cannot be extended to knowledge-learning tasks, where simply increasing response length does not show similar gradient patterns as slow thinking."
        },
        {
            "title": "Limitations",
            "content": "Due to the page limits, only small proportion of results can be presented, which might weaken the findings of the paper. However, we try to include as many results as possible in the appendix through visualization and statistical results, hoping to provide further insights for the community. Moreover, the analysis in this paper focuses mainly on the strength of the layer-wise gradients, maybe more metrics can be used."
        },
        {
            "title": "References",
            "content": "Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for CommonsenseQA: New Dataset and Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 30503065, Online. Association for Computational Linguistics. Guillaume Alain and Yoshua Bengio. 2017. Understanding intermediate layers using linear classifier probes. Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John Patrick Cunningham. 2024. LoRA learns less and forgets less. Transactions on Machine Learning Research. Featured Certification. Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, et al. 2024. Stealing part of production language model. arXiv preprint arXiv:2403.06634. Xiaodong Chen, Yuxuan Hu, Jing Zhang, Yanling Wang, Cuiping Li, and Hong Chen. 2024. Streamlining redundant layers to compress large language models. Preprint, arXiv:2403.19135. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. 2024. Not all layers of Preprint, llms are necessary during inference. arXiv:2403.02181. Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, and VS Subrahmanian. 2024. Higher layers need more lora experts. Preprint, arXiv:2402.08562. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346 361. Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, and Dinesh Manocha. 2024. closer look at the limitations of instruction tuning. Preprint, arXiv:2402.05119. Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international 9 conference on artificial intelligence and statistics, pages 249256. JMLR Workshop and Conference Proceedings. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in haystack: Case studies with sparse probing. Transactions on Machine Learning Research. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Preprint, arXiv:2311.05232. Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. 2024. Trustllm: Trustworthiness in large language models. Preprint, arXiv:2401.05561. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, and Yongfeng Zhang. 2024. Exploring concept depth: How large language models acquire knowledge at different layers? Preprint, arXiv:2404.07066. Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, and Gongshen Liu. 2024. How large language models encode context knowledge? layerwise probing study. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 82358246, Torino, Italia. ELRA and ICCL. Ming Li, Jiuhai Chen, Lichang Chen, and Tianyi Zhou. 2024a. Can LLMs speak for diverse people? tuning LLMs via debate to generate controllable controversial statements. In Findings of the Association for Computational Linguistics ACL 2024, pages 16160 16176, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024b. Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. In Findings of the Association for Computational Linguistics ACL 2024, pages 16189 16211, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, and Tianyi Zhou. 2023. Reflection-tuning: Recycling data for better instruction-tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024c. Superfiltering: Weak-to-strong data filtering In Proceedings of the for fast instruction-tuning. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1425514273, Bangkok, Thailand. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024d. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 75957628, Mexico City, Mexico. Association for Computational Linguistics. Shen Li, Liuyi Yao, Lan Zhang, and Yaliang Li. 2024e. Safety layers in aligned large language models: The key to llm security. Preprint, arXiv:2408.17003. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, Vancouver, Canada. Association for Computational Linguistics. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. Preprint, arXiv:2301.13688. Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju. 2024. Post hoc explanations of language models can improve language models. Advances in Neural Information Processing Systems, 36. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language models are more redundant than you expect. Preprint, arXiv:2403.03853. 10 Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773. Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason. Preprint, arXiv:2311.11045. Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: dataset for commonsense reasoning over entity knowledge. Preprint, arXiv:2109.01653. Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys, 16:185. Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large language models. arXiv preprint arXiv:2402.01761. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023a. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 98409855, Singapore. Association for Computational Linguistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2024. Interpretability at scale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing Systems, 36. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. Preprint, arXiv:2304.12244. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng 11 Tao, and Tianyi Zhou. 2024. survey on knowledge distillation of large language models. ArXiv, abs/2402.13116. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan AllenZhu. 2024. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Preprint, arXiv:2407.20311. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruction tuning for large language models: survey. Preprint, arXiv:2308.10792. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2023a. Explainability for large language models: survey. Preprint, arXiv:2309.01029. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b. survey of large language models. Preprint, arXiv:2303.18223. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. Preprint, arXiv:2305.11206. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. 2023. Representation engineering: top-down approach to ai transparency. Preprint, arXiv:2310.01405."
        },
        {
            "title": "A Data Examples",
            "content": "B Results on gemma-2-2b . . . . . . . . . . . . . . . . . . . . . . B.1 Pre-trained LLM on Correct Re- . . . sponses . . . . . B.1.1 Reasoning Tasks . . . . . B.1.2 Wiki Tasks B.2 Pre-trained LLM on Irrelevant Re- . . . sponses . . . . . B.2.1 Reasoning Tasks . . . . . B.2.2 Wiki Tasks B.3 Instructed LLM on Correct Re- . . . sponses . . . . . B.3.1 Reasoning Tasks . . . . . B.3.2 Wiki Tasks B.4 Instructed LLM on Irrelevant Re- . . . sponses . . . . . B.4.1 Reasoning Tasks . . . . . B.4.2 Wiki Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Results on Llama-3.1-8B . . . . . . . . . . . . . . . . . . . . . . C.1 Pre-trained LLM on Correct Re- . . . sponses . . . . . C.1.1 Reasoning Tasks . . . . . C.1.2 Wiki Tasks C.2 Pre-trained LLM on Irrelevant Re- . . . sponses . . . . . C.2.1 Reasoning Tasks . . . . . C.2.2 Wiki Tasks C.3 Instructed LLM on Correct Re- . . . sponses . . . . . C.3.1 Reasoning Tasks . . . . . C.3.2 Wiki Tasks C.4 Instructed LLM on Irrelevant Re- . . . sponses . . . . . C.4.1 Reasoning Tasks . . . . . C.4.2 Wiki Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Results on Qwen2-1.5B . . . . . . . . . . . . . . D.1 Pre-trained LLM on Correct Re- . . . sponses . . . . . D.1.1 Reasoning Tasks . . . . . D.1.2 Wiki Tasks D.2 Pre-trained LLM on Irrelevant Re- . . . sponses . . . . . D.2.1 Reasoning Tasks . . . . . D.2.2 Wiki Tasks D.3 Instructed LLM on Correct Re- . . . sponses . . . . . D.3.1 Reasoning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3.2 Wiki Tasks . . . D.4 Instructed LLM on Irrelevant Re- . . . . . . . . . sponses . . . . . . . . . . . D.4.1 Reasoning Tasks . . . . . . . D.4.2 Wiki Tasks Results on Llama-2-7B-hf E.1 Pre-trained LLM on Correct Re96 96 96 96 . . . . . . . . . . . sponses E.1.1 Reasoning Tasks . . . . . . . E.1.2 Wiki Tasks . . . 109 . . . 109 . . . 110 E.2 Pre-trained LLM on Irrelevant Responses . . . . . . . . . . . E.2.1 Reasoning Tasks . . . . . . . E.2.2 Wiki Tasks . . . 110 . . . 110 . . . 110 E.3 Instructed LLM on Correct Responses . . . . . . . . . . . E.3.1 Reasoning Tasks . . . . . . . E.3.2 Wiki Tasks . . . 123 . . . 123 . . . 124 E.4 Instructed LLM on Irrelevant Responses . . . . . . . . . . . E.4.1 Reasoning Tasks . . . . . . . E.4.2 Wiki Tasks . . . 124 . . . 124 . . . 124 26 26 26 27 27 27 27 39 39 40 40 40 40 53 53 54 54 54 54 67 67 68 68 68 68 81 81 81 82 82 82 95"
        },
        {
            "title": "A Data Examples",
            "content": "In this section we provide examples for each of our datasets: AQuA (Figure 5), CREAK (Figure 6), ECQA (Figure 7), GSM8K (Figure 8), Sensemaking (Figure 9), StrategyQA (Figure 10), MATHAlgebra (Figure 11), MATH-Counting (Figure 12), MATH-Geometry (Figure 13), Wiki Popular (Figure 14), and Wiki Unpopular (Figure 15). 14 Question: 600 meter long train crosses signal post in 40 seconds. How long will it take to cross 3 kilometer long bridge, at the same speed? Options: (A) 4 min, (B) 2 min, (C) 8 min, (D) 9 min, (E) 5 min [None CoT] (A) [Simplified CoT] = 600/40 = 15 mps = 3600/15 = 240 sec = 4 min The final answer is (A) [Detailed CoT] To find the time it will take for 600-meter-long train to cross 3-kilometer-long bridge, we need to follow these steps: Step 1: Determine the speed of the train - The train crosses signal post (which can be treated as point) in 40 seconds. - This means the train travels its own length (600 meters) in 40 seconds. Calculate the speed of the train: Speed = Speed = Distance Time 600 meters 40 seconds Speed = 15 meters/second Step 2: Determine the total distance to be covered when crossing the bridge - The length of the bridge is 3 kilometers, which is 3000 meters. - When crossing the bridge, the train needs to cover the length of the bridge plus its own length. - Total distance to be covered = Length of the bridge + Length of the train Total distance = 3000 meters + 600 meters Total distance = 3600 meters Step 3: Calculate the time to cross the bridge - Using the speed calculated earlier, we now find the time to cover 3600 meters. Time ="
        },
        {
            "title": "Distance\nSpeed",
            "content": "Time = 3600 meters 15 meters/second Time = 240 seconds Step 4: Convert the time into minutes - There are 60 seconds in minute. Time in minutes = 240 seconds 60 seconds/minute Time in minutes = 4 minutes Thus, it will take the train 4 minutes to cross the 3-kilometer-long bridge. Conclusion: The correct answer is (A) 4 minutes. Figure 5: An example from AQuA dataset. 15 Question: Which of the following is not leap year ? Options: (A) 700; (B) 800; (C) 900; (D) 1000; (E) 1200 [None CoT] (A) [Simplified CoT] The century divisible by 400 is The final answer is (A) [Detailed CoT] To determine which of the given years is not leap year, we need to understand the rules for leap years. Heres step-by-step breakdown: Leap Year Rules: 1. year is leap year if it is divisible by 4. 2. However, if the year is also divisible by 100, it is not leap year, unless: 3. The year is divisible by 400, in which case it is leap year. We will apply these rules to each of the given years. Year 700 1. Check divisibility by 4: 700 4 = 175 (no remainder, so divisible by 4). 2. Check divisibility by 100: 700 100 = 7 (no remainder, so divisible by 100). 3. Check divisibility by 400: 700 400 = 1.75 (remainder exists, so not divisible by 400). - Since 700 is divisible by 100 but not by 400, it is **not leap year**. Year 800 1. Check divisibility by 4: 800 4 = 200 (no remainder, so divisible by 4). 2. Check divisibility by 100: 800 100 = 8 (no remainder, so divisible by 100). 3. Check divisibility by 400: 800 400 = 2 (no remainder, so divisible by 400). - Since 800 is divisible by 400, it is **a leap year**. Year 900 1. Check divisibility by 4: 900 4 = 225 (no remainder, so divisible by 4). 2. Check divisibility by 100: 900 100 = 9 (no remainder, so divisible by 100). 3. Check divisibility by 400: 900 400 = 2.25 (remainder exists, so not divisible by 400). - Since 900 is divisible by 100 but not by 400, it is **not leap year**. Year 1000 1. Check divisibility by 4: 1000 4 = 250 (no remainder, so divisible by 4). 2. Check divisibility by 100: 1000 100 = 10 (no remainder, so divisible by 100). 3. Check divisibility by 400: 1000 400 = 2.5 (remainder exists, so not divisible by 400). - Since 1000 is divisible by 100 but not by 400, it is **not leap year**. Year 1200 1. Check divisibility by 4: 1200 4 = 300 (no remainder, so divisible by 4). 2. Check divisibility by 100: 1200 100 = 12 (no remainder, so divisible by 100). 3. Check divisibility by 400: 1200 400 = 3 (no remainder, so divisible by 400). - Since 1200 is divisible by 400, it is **a leap year**. Conclusion From the analysis above, the years that are not leap years are: - 700 - 900 - 1000 Among the options provided, the incorrect assumption was identifying which specific year is being asked about in question format that expects singular correct answer. However, strictly adhering to the problem formatting where typically one answer is sought: - Since several options fit correctly and given the options stated as format; (A), (C), (D) are non-leap years. Proceeding usually with first in order and options mentioned logically (A)700 provided correct inference. Therefore: **Option (A) 700 is not leap year.** Figure 6: An example from CREAK dataset. 16 Question: The ability to remember things is sign of what? Options: - neuronal network - memories - intelligence - recall - learning about [None CoT] intelligence [Simplified CoT] Intelligence is an ability. The ability to remember things is sign of intelligence. The final answer is intelligence [Detailed CoT] To determine what the ability to remember things is most closely sign of, its helpful to analyze each option: 1. **Neuronal Network**: - The neuronal network refers to the structure and connections between neurons in the brain. While crucial for processing and storing information, the term itself does not directly signify the ability to remember. Rather, it describes the brains architecture which facilitates functions including memory. 2. **Memories**: - Memories are the end result or the content of what is remembered. While they are directly linked to remembrance, the ability to have memories is an outcome of the memory process, not an indicator of the capacity to remember. 3. **Intelligence**: - Intelligence involves broader range of cognitive abilities including problemsolving, reasoning, and understanding. Although memory is component of intelligence, particularly in areas like working memory and long-term recall, intelligence encompasses far more than just the ability to remember. 4. **Recall**: - Recall is the process of retrieving information from memory. The ability to remember is direct manifestation of recall, as it involves the capacity to retrieve stored information when needed. 5. **Learning About**: - Learning about refers to the process of acquiring new information or skills. While learning is related to memory, having the ability to remember indicates that the learning process has been successful, but it does not directly signify the capacity to remember. Given this analysis, the most accurate answer is: **Recall**. The ability to remember things is sign of recall because recall specifically refers to the cognitive process of retrieving information from memory. Figure 7: An example from ECQA dataset. 17 Question: Linda was going to pass out homemade cookies to each of her 24 classmates on the last day of school. She wanted to give each student 10 cookies and wanted to make chocolate chip cookies and oatmeal raisin cookies. Each cookie recipe made exactly 4 dozen cookies. She was able to make 2 batches of chocolate chip cookies before running out of chocolate chips. She made 1 batch of oatmeal raisin cookies. How many more batches of cookies does Linda need to bake? [None CoT] [Simplified CoT] She has 24 classmates and wants to give each 10 cookies, so she needs to bake 24 * 10 = 240 cookies. Each batch makes 4 dozen cookies so that means each batch has 4 * 12 = 48 cookies. She baked 2 batches of chocolate chip and one batch of oatmeal raisin so 2 + 1 = 3 batches. We know that each batch has 48 cookies so 3 * 48 = 144 cookies have been baked. She needs to have 240 cookies and has baked 144 so 240 - 144 = 96 cookies are needed. Again, each batch makes 48 cookies and we need 96 more cookies so 96 / 48 = 2 more batches are needed. The final answer is 2 [Detailed CoT] To determine how many more batches of cookies Linda needs to bake, lets proceed with the following steps: 1. **Calculate the total number of cookies Linda needs:** Linda wants to give each of her 24 classmates 10 cookies. Total cookies needed = 24 classmates 10 cookies per classmate = 240 cookies 2. **Determine the number of cookies Linda has already made:** - Each batch of cookies makes exactly 4 dozen cookies. - 1 dozen = 12 cookies, so 4 dozen = 4 12 = 48 cookies per batch. - Linda made 2 batches of chocolate chip cookies: Cookies from chocolate chip cookies = 2 48 = 96 cookies - Linda made 1 batch of oatmeal raisin cookies: Cookies from oatmeal raisin cookies = 1 48 = 48 cookies - Therefore, the total number of cookies she has made is: Total cookies made = 96 chocolate chip cookies + 48 oatmeal raisin cookies = 144 cookies 3. **Determine the additional number of cookies needed:** - Total cookies needed are 240. - Cookies already made are 144. - Thus, the additional cookies needed are: Additional cookies needed = 240 144 = 96 cookies 4. **Calculate the number of additional batches needed:** - Each batch makes 48 cookies. - To find out how many more batches are needed to produce 96 cookies: Number of additional batches = 96 cookies 48 cookies per batch = 2 batches Therefore, Linda needs to bake **2 more batches** of cookies to have enough for her classmates. Figure 8: An example from GSM8K dataset. Question: Choose the sentence that goes against common sense. Options: - Sentence A: \"Christopher Columbus sailed to North America with three ships: the Toyota, the Pontiac, and the Mazda.\" - Sentence B: \"Christopher Columbus sailed to North America with three ships: the Santa Maria, the Pinta, and the Nina.\" [None CoT] Sentence [Simplified CoT] The Santa Maria, the Pinta, and the Nina are sailing ships, while the Toyota, the Pontiac, and the Mazda are cars. The final answer is Sentence [Detailed CoT] To determine which sentence goes against common sense, well evaluate the accuracy and historical correctness of the information provided in each option. **Sentence A: \"Christopher Columbus sailed to North America with three ships: the Toyota, the Pontiac, and the Mazda.\"** 1. **Names of the Ships**: The names mentioned are \"Toyota,\" \"Pontiac,\" and \"Mazda.\" - **Analysis**: Toyota and Mazda are names of modern car manufacturing companies, and Pontiac is also associated with an automobile brand. These names have no historical connection to the period of Christopher Columbus or the names of ships used during his voyages. 2. **Historical Accuracy**: Christopher Columbuss voyages took place in the 15th century (1492 specifically for his first voyage) when such modern terms and companies did not exist. **Sentence B: \"Christopher Columbus sailed to North America with three ships: the Santa Maria, the Pinta, and the Nina.\"** 1. **Names of the Ships**: The names mentioned are \"Santa Maria,\" \"Pinta,\" and \"Nina.\" - **Analysis**: These are historically accurate names of the three ships that Christopher Columbus used on his first voyage across the Atlantic Ocean in 1492. 2. **Historical Accuracy**: This information is consistent with the well-documented historical facts about Columbuss voyage. Conclusion: **Sentence A** goes against common sense because it lists modern car brands (\"Toyota,\" \"Pontiac,\" and \"Mazda\") as the names of the ships used by Christopher Columbus in the 15th century, which is factually incorrect and anachronistic. **Reasoning Steps Summarized**: 1. Evaluate the historical context and time period relevant to Christopher Columbuss voyages. 2. Identify the historically accurate ship names used by Columbus. 3. Compare the provided ship names in each sentence against the known historical facts. 4. Determine that modern car brand names in Sentence are out of place and nonsensical in the given context. Figure 9: An example from Sensemaking dataset. Question: Could Durian cause someones stomach to feel unwell? [None CoT] yes [Simplified CoT] Durian has pungent odor that many people describe as being similar to feet and onions. Unpleasant smells can make people feel nauseous. The final answer is yes [Detailed CoT] Yes, durian can potentially cause someones stomach to feel unwell. Here are some detailed reasoning steps to understand why this might happen: 1. **Digestive Sensitivity** Different people have varying levels of sensitivity to foods. Durian, being rich and complex fruit, might be harder to digest for some individuals. **Reasoning:** - Durian has high fiber content which, while beneficial in moderate amounts, can cause gas, bloating, or uncomfortable bowel movements in those not accustomed to high-fiber foods. - High-fat content can also be an issue for individuals with sensitivities or specific digestive conditions, as it might slow down the digestive process. 2. **High Sulfur Content** Durian contains sulfur compounds, which contribute to its strong smell and unique flavor. **Reasoning:** - Sulfur compounds can be challenging for some people to digest and might lead to stomach discomfort, including bloating and gas. - People with sulfur sensitivities or related conditions may experience more pronounced gastrointestinal distress. 3. **Fermentation and Bloating** Durian contains sugars that can ferment in the gut. **Reasoning:** - The process of fermentation can produce gas, which might lead to bloating and discomfort. - If someone has an imbalance in their gut bacteria, the fermentation of these sugars could exacerbate feelings of unwellness in the stomach. 4. **Overconsumption** Eating too much of any food, including durian, can cause digestive discomfort. **Reasoning:** - When consumed in large amounts, the high fiber and fat content in durian can overwhelm the digestive system, leading to symptoms such as stomachache, bloating, and gas. - Overeating durian could also result in high caloric intake, leading to feeling of fullness and discomfort. 5. **Preexisting Conditions** Individuals with certain preexisting health conditions might be more susceptible to stomach discomfort from durian. **Reasoning:** - Conditions like Irritable Bowel Syndrome (IBS), gastritis, or other gastrointestinal disorders can make the stomach more sensitive to rich, complex foods like durian. - Specific allergies or intolerances to compounds in durian can trigger adverse reactions leading to stomach distress. 6. **Interaction with Other Foods** Combining durian with other foods, particularly certain types that dont mix well with rich, fatty, or high-fiber foods, might cause problems. **Reasoning:** - Consuming durian alongside alcohol, dairy, or other high-fat foods can stress the digestive system, leading to nausea or stomach pain. - Certain food combinations might inhibit proper digestion, causing discomfort. Conclusion While durian is nutritious and delicious fruit for many people, its complex composition can lead to stomach discomfort for others. Factors like individual digestive sensitivity, high sulfur and fiber content, fermentation processes, overeating, preexisting conditions, and interaction with other foods all contribute to this potential issue. To minimize discomfort, it is advisable for individuals to consume durian in moderation and pay attention to their bodys response. Figure 10: An example from StrategyQA dataset. Question: Solve for > 0 in the following arithmetic sequence: 12, x2, 32, . . .. [Simplified CoT] The term x2 is simply the average of 12 = 1 and 32 = 9, so x2 = (1 + 9)/2 = 5. Because > 0, = 5 . [Detailed CoT] To solve for in the arithmetic sequence 12, x2, 32, . . . with > 0, we will follow these detailed steps: 1. **Define the Terms of the Sequence**: The given sequence is 12, x2, 32, . . .. These are the first three terms of the sequence, where 12 = 1, x2, and 32 = 9. 2. **Arithmetic Sequence Property**: In an arithmetic sequence, the difference between consecutive terms is constant. Let this common difference be d. 3. **Set Up Equations for the Common Difference**: From the sequence, we have: x2 12 = and 32 x2 = Thus, the two equations based on the common difference are: x2 1 = and 9 x2 = 4. **Equating the Common Differences**: Since both expressions equal d, we can set them equal to each other: 5. **Solve the Equation**: Combine like terms to solve for x2: x2 1 = 9 x2 Add x2 to both sides: Add 1 to both sides: Divide by 2: x2 1 = 9 2x2 1 = 9 2x2 = 10 x2 = 5 Since > 0, take the positive square root of both sides to find x: = Therefore, the solution for in the given arithmetic sequence is: 5 = Figure 11: An example from MATH-Algebra dataset. 21 Question: How many positive three-digit integers with each digit greater than 4 are divisible by 6? [Simplified CoT] To be divisible by 6, number must have its digits add up to multiple of 3, and be even. Therefore, for the hundreds place, the possible digits are {5, 6, 7, 8, 9}, for the tens place the possible digits also are {5, 6, 7, 8, 9}, and for the ones digit, you can only choose from {6, 8}. First, let us choose 6 for the ones place. The other two digits must add up to multiple of 3, making total of 8 pairs that satisfy that condition: {5, 7}, {6, 6}, {6, 9}, {7, 5}, {7, 8}, {8, 7}, {9, 6}, {9, 9}. Next, let us choose 8 for the ones place. The other two digits must be congruent to 1 mod 3, making total of 8 pairs that satisfy that condition: {5, 5}, {5, 8}, {6, 7}, {7, 6}, {7, 9}, {8, 5}, {8, 8}, {9, 7}. This makes total of 16 numbers. [Detailed CoT] To determine how many positive three-digit integers with each digit greater than 4 are divisible by 6, we need to consider the criteria that an integer must meet to be divisible by 6. Specifically, number is divisible by 6 if it is divisible by both 2 and 3. Step-by-step Reasoning 1. **Divisibility by 2:** - number is divisible by 2 if its last digit is even. - The possible digits for three-digit number where each digit is greater than 4 are 5, 6, 7, 8, 9. - Among these digits, the even digits are 6 and 8. 2. **Divisibility by 3:** - number is divisible by 3 if the sum of its digits is divisible by 3. - We need to ensure the sum of the digits of the number fits this criterion. Enumeration of Valid Numbers Given the constraints (digits greater than 4 and the number must meet divisibility criteria for both 2 and 3), we can construct our solution: **Step 1: Form digits** The three-digit number is represented as abc, where a, b, and are its digits. **Step 2: Identify valid choices for (last digit, must be even):** Potential values for can be 6 or 8. Lets assume = 6: Now we find valid pairs (a, b) such that the sum + + 6 is divisible by 3. - Valid digits for and are 5, 6, 7, 8, and 9. Lets check all pairs (a, b): For = 6: - 6 = + 0 (mod 3) - Possible pairs (a, b) where + must be divisible by 3: - (5, 7), (5, 9) - (6, 6), (6, 9) - (7, 5), (7, 8) - (8, 7) - (9, 5), (9, 6) For = 8: - 8 = + 1 (mod 3) - Possible pairs (a, b) where + + 8 results in sum modulo 3 as: - (a, b) needs to sum up to 1 (mod 3), this means: - (5, 5) - (5, 8) - (6, 7) - (7, 6), (7, 9) - (8 , 5), (8, 8) - (9, 9) So combining both : - Pairs with = 6: 10 pairs. - Pairs with = 8: 8 pairs. **Step 3: Combine and count the numbers of valid solutions:** Hence, total such numbers = Total Pairs= 10 + 8 = 18. Thus, there are 18 largernumber possible solutions that satisfy the conditions: three-digit positive integers, each digit greater than 4, and divisible by 6. Figure 12: An example from MATH-Counting dataset. 22 Question: Square ABCD has side length 1 unit. Points and are on sides AB and CB, respectively, with AE = CF . When the square is folded along the lines DE and DF , sides AD and CD coincide and lie on diagonal BD. The length of segment AE can be expressed in the form units. What is the integer value of + m? [Simplified CoT] We start by drawing diagram. When the paper is folded, sides AD and CD coincide on the longer dashed line, and points and meet at G, as you can see below. [asy] draw((0,0)(1,0)(1,1)(0,1)cycle); draw((0,0)(1,.4)); draw((0,0)(.4,1)); draw((1,.4) (.4,1),dashed); draw((0,0)(.7,.7),dashed); label(\"A\",(0,1), NW); label(\"B\",(1,1), NE); label(\"C\",(1,0), SE); label(\"D\",(0,0), SW); label(\"F \",(1,.4), E); label(\"E\",(.4,1), N); label(\"G\",(.7,.7), NE); [/asy] Now, we assign variables. We are looking for the length of AE, so let AE = x. Then, BE = 1 x. Because of the symmetry of the square and the fold, everything to the left of line BD is mirror image of everything to the right of BD. Thus, BEF is an isosceles right triangle (45-45-90), so 2(1 x). Also, EGB and GB are congruent 45-45-90 triangles, so GB = EB EF = 2 Also, notice that because the way the paper is folded (its original position versus its final position), we have more congruent triangles, AED = GED. This means that AD = GD = 1. Lastly, notice that since is on BD, we have BD = BG + GD. BD is diagonal of the square, so it has side length GD = 1, and GB = (1x) 2 . Thus, our equation becomes = (1x) 2 2EB = 2, . Multiplying both sides by we see that + = 2 + 1 = 3 . 2 yields 2 = 2 = 1 + (1 x) 2 . 2 + 1 x; solving for yields = 2 1. Thus, AE = 2 1 = m, and [Detailed CoT] First, we label the points on square ABCD as follows: with A(0, 1), B(0, 0), C(1, 0), and D(1, 1). Let be point on AB with coordinate (0, y), where = AE, and let be point on CB with coordinate (x, 0), where = CF . Since AE = CF , it follows that = x. When the square is folded along DE and DF , the folded sides AD and CD align and coincide along diagonal BD. We know AD will lie on BD, so point will overlap somewhere on line BD. The diagonal BD of the square can be parameterized as the line from B(0, 0) to D(1, 1). The equation of line BD is: = Considering points on AB, having coordinates (0, y), and points on CB, having coordinates (x, 0): When folding AD onto BD, point A(0, 1) will fold along some line segment DE. To understand where falls upon folding onto BD, we calculate the perpendicular distance from to BD. The distance from point (x1, y1) to line ax + by + = 0 is given by: = ax1 + by1 + a2 + b2 In our case, the equation of the line BD is = and can also be written as: = 0 Thus, we have = 1, = 1, and = 0. The distance from point A(0, 1) to line = 0 is: = 1 0 + (1) 1 + 0 (cid:112)12 + (1)2 = 0 1 1 + 1 = 1 2 = 2 2 Since lies directly on the y-axis at = AE, and Es distance should match the perpendicular distance from to BD, we solve for = AE: = = 2 Thus, = = 2 2 = Here AE = ="
        },
        {
            "title": "Setting",
            "content": "2 2 . However, the problem requires the expression in the form 2 1 2 1 2 1, we get = 2 and = 1. Thus: 4 4 2 = 2 = m: The integer value of + is: + = 2 + 1 = 3 3 Figure 13: An example from MATH-Geometry dataset. 23 Question: \"Tell me about Charles, Prince of Wales.\" [Length 100] Charles was born at 21:14 (GMT) on 14 November 1948, during the reign of his maternal grandfather, King George VI, as the first child of Princess Elizabeth, Duchess of Edinburgh (later Queen Elizabeth II), and Philip, Duke of Edinburgh. He was delivered by Caesarean section at Buckingham Palace. His parents had three more children, Anne (born 1950), Andrew (born 1960) and Edward (born 1964). [Length 500] Charles was born at 21:14 (GMT) on 14 November 1948, during the reign of his maternal grandfather, King George VI, as the first child of Princess Elizabeth, Duchess of Edinburgh (later Queen Elizabeth II), and Philip, Duke of Edinburgh. He was delivered by Caesarean section at Buckingham Palace. His parents had three more children, Anne (born 1950), Andrew (born 1960) and Edward (born 1964). He was christened Charles Philip Arthur George on 15 December 1948 in the Music Room of Buckingham Palace by the Archbishop of Canterbury, Geoffrey Fisher. George VI died on 6 February 1952 and Charless mother acceded to the throne as Elizabeth II; Charles immediately became the heir apparent. Under charter of Edward III in 1337, and as the monarchs eldest son, he automatically assumed the traditional titles of Duke of Cornwall and, in the Scottish peerage, the titles Duke of Rothesay, Earl of Carrick, Baron of Renfrew, Lord of the Isles, and Prince and Great Steward of Scotland. The following year, Charles attended his mothers coronation at Westminster Abbey. When Charles turned five, Catherine Peebles was appointed as governess to oversee his education at Buckingham Palace. He then commenced classes at Hill House School in west London in November 1956. Charles was the first heir apparent to attend school, rather than be educated by private tutor. He did not receive preferential treatment from the schools founder and headmaster, Stuart Townend, who advised the Queen to have Charles train in football, because the boys were never deferential to anyone on the football field. Charles subsequently attended two of his fathers former schools: Cheam School in Hampshire, from 1958, followed by Gordonstoun, in the north-east of Scotland, beginning classes there in April 1962. He later became patron of Gordonstoun in May 2024. In his 1994 authorised biography by Jonathan Dimbleby, Charless parents were described as physically and emotionally distant and Philip was blamed for his disregard of Charless sensitive nature, including forcing him to attend Gordonstoun, where he was bullied. [Length 1000] Charles was born at 21:14 (GMT) on 14 November 1948, during the reign of his maternal grandfather, King George VI, as the first child of Princess Elizabeth, Duchess of Edinburgh (later Queen Elizabeth II), and Philip, Duke of Edinburgh. He was delivered by Caesarean section at Buckingham Palace. His parents had three more children, Anne (born 1950), Andrew (born 1960) and Edward (born 1964). He was christened Charles Philip Arthur George on 15 December 1948 in the Music Room of Buckingham Palace by the Archbishop of Canterbury, Geoffrey Fisher. George VI died on 6 February 1952 and Charless mother acceded to the throne as Elizabeth II; Charles immediately became the heir apparent. Under charter of Edward III in 1337, and as the monarchs eldest son, he automatically assumed the traditional titles of Duke of Cornwall and, in the Scottish peerage, the titles Duke of Rothesay, Earl of Carrick, Baron of Renfrew, Lord of the Isles, and Prince and Great Steward of Scotland. The following year, Charles attended his mothers coronation at Westminster Abbey. When Charles turned five, Catherine Peebles was appointed as governess to oversee his education at Buckingham Palace. He then commenced classes at Hill House School in west London in November 1956. Charles was the first heir apparent to attend school, rather than be educated by private tutor. He did not receive preferential treatment from the schools founder and headmaster, Stuart Townend, who advised the Queen to have Charles train in football, because the boys were never deferential to anyone on the football field. Charles subsequently attended two of his fathers former schools: Cheam School in Hampshire, from 1958, followed by Gordonstoun, in the north-east of Scotland, beginning classes there in April 1962. He later became patron of Gordonstoun in May 2024. In his 1994 authorised biography by Jonathan Dimbleby, Charless parents were described as physically and emotionally distant and Philip was blamed for his disregard of Charless sensitive nature, including forcing him to attend Gordonstoun, where he was bullied. Though Charles reportedly described Gordonstoun, noted for its especially rigorous curriculum, as \"Colditz in kilts\", he later praised the school, stating it had taught him \"a great deal about myself and my own abilities and disabilities\". He said in 1975 interview he was \"glad\" he had attended Gordonstoun and that the \"toughness of the place\" was \"much exaggerated\". In 1966 Charles spent two terms at the Timbertop campus of Geelong Grammar School in Victoria, Australia, during which time he visited Papua New Guinea on school trip with his history tutor, Michael Collins Persse. In 1973 Charles described his time at Timbertop as the most enjoyable part of his whole education. Upon his return to Gordonstoun, he emulated his father in becoming head boy, and left in 1967 with six GCE O-levels and two A-levels in history and French, at grades and respectively. On his education, Charles later remarked, \"I didnt enjoy school as much as might have; but, that was only because Im happier at home than anywhere else\". Charles broke royal tradition when he proceeded straight to university after his A-levels, rather than joining the British Armed Forces. In October 1967, he was admitted to Trinity College, Cambridge, where he studied archaeology and anthropology for the first part of the Tripos and then switched to history for the second part. During his second year, he attended the University College of Wales in Aberystwyth, studying Welsh history and the Welsh language for one term. Charles became the first British heir apparent to earn university degree, graduating in June 1970 from the University of Cambridge with 2:2 Bachelor of Arts (BA) degree. Following standard practice, in August 1975, his Bachelor of Arts was promoted to Master of Arts (MA Cantab) degree. Charles served in the Royal Air Force (RAF) and the Royal Navy. Figure 14: Three examples from Wiki Knowledge dataset. 24 Question: \"Tell me about Eucosmophora atlantis.\" [Unpopular] Eucosmophora atlantis is moth of the family Gracillariidae. It is known from Costa Rica. The length of the forewings is 3.64.5 mm for males and 4-4.8 mm. for females. The larvae probably feed on Sapotaceae species and probably mine the leaves of their host plant. Figure 15: An example from Wiki Knowledge dataset. 25 Results on gemma-2-2b Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All B.1 Pre-trained LLM on Correct Responses B.1.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 135, Table 4), MATH-Counting (Figure 136, Table 5), MATHGeometry (Figure 137, Table 6). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.61 0.45 0.51 0.38 2.11 1.25 1.91 1.01 0.02 0.02 0.03 0. 0.03 0.02 0.01 0.01 0.53 0.37 0.45 0.35 2.05 1.39 1.48 1. 0.02 0.02 0.01 0.02 0.04 0.03 0.02 0.02 0.49 0.41 0.48 0. 0.70 0.59 0.80 0.45 0.02 0.02 0.03 0.03 0.05 0.03 0.04 0. 0.53 0.39 0.48 0.38 1.82 1.20 1.42 0.87 0.02 0.02 0.02 0. 0.03 0.03 0.02 0.02 Table 4: Statistical results for MATH-Algebra using gemma-2-2b on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.72 0.56 0.58 0.46 2.40 1.57 2.20 1.27 0.02 0. 0.03 0.03 0.03 0.03 0.01 0.01 0.71 0.52 0.61 0.47 2.10 1. 1.54 1.23 0.02 0.02 0.01 0.02 0.03 0.03 0.03 0.02 0.54 0. 0.52 0.47 0.91 0.84 0.80 0.48 0.02 0.02 0.02 0.03 0.04 0. 0.03 0.02 0.65 0.50 0.59 0.47 1.97 1.45 1.53 1.04 0.02 0. 0.02 0.02 0.03 0.03 0.02 0.02 Table 5: Statistical results for MATH-Counting using gemma-2-2b on correct responses. Figure 17: Visualization for MATH-Counting using gemma-2-2b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.70 0.66 0.59 0.55 2.12 1. 1.87 1.35 0.02 0.02 0.03 0.03 0.03 0.02 0.01 0.01 0.64 0. 0.54 0.54 1.92 1.88 1.44 1.34 0.02 0.02 0.01 0.02 0.03 0. 0.02 0.02 0.49 0.45 0.47 0.49 0.51 0.89 0.68 0.51 0.01 0. 0.03 0.03 0.04 0.03 0.03 0.03 0.62 0.55 0.54 0.53 1.71 1. 1.36 1.13 0.02 0.02 0.02 0.03 0.03 0.03 0.02 0.02 Table 6: Statistical results for MATH-Geometry using gemma-2-2b on correct responses. Figure 16: Visualization for MATH-Algebra using gemma-2-2b on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 138, Table 7), GSM8K (Figure 139, Table 8), StrategyQA (Figure 140, Table 9), ECQA (Figure 141, Table 10), CREAK (Figure 142, Table 11), Sensemaking (Figure 143, Table 12). 26 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 2.55 0.93 0.48 2.37 0.81 0.42 9.17 2.48 1.40 9.14 2.13 1.20 0.03 0.02 0. 0.04 0.04 0.03 0.07 0.05 0.04 0.05 0.02 0.01 1.15 0.86 0.47 2.30 0.92 0.55 9.09 2.15 1. 3.86 1.54 1.19 0.03 0.03 0.02 0.04 0.02 0.02 0.05 0.04 0.04 0.04 0.04 0.03 3.91 0.76 0. 4.75 0.69 0.49 3.56 0.87 0.82 2.05 0.57 0.45 0.08 0.02 0.02 0.07 0.02 0.02 0.04 0.04 0. 0.07 0.04 0.03 1.97 0.84 0.46 2.65 0.86 0.50 8.21 1.98 1.39 4.61 1.43 1.00 0.05 0.02 0. 0.04 0.02 0.02 0.05 0.04 0.04 0.05 0.03 0.02 Table 8: Statistical results for GSM8K using gemma-22b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 10.57 1.35 0.61 10.24 0.99 0.47 34.59 3.80 2.06 27.75 3.25 1. 0.03 0.03 0.03 0.02 0.04 0.03 0.07 0.04 0.02 0.05 0.02 0.01 6.09 0.62 0.66 9.84 1.00 0. 42.47 3.19 2.02 12.40 1.72 1.63 0.06 0.03 0.02 0.03 0.02 0.02 0.09 0.06 0.04 0.03 0.04 0. 4.57 1.26 0.53 4.23 1.11 0.47 20.23 1.85 0.90 4.63 1.19 0.84 0.08 0.03 0.02 0.04 0.03 0. 0.04 0.05 0.04 0.07 0.07 0.03 7.07 0.95 0.60 9.08 0.98 0.53 36.18 3.12 1.80 13.80 1.95 1. 0.06 0.03 0.02 0.03 0.02 0.02 0.07 0.05 0.03 0.04 0.04 0.02 Table 9: Statistical results for StrategyQA using gemma2-2b on correct responses. reasoning tasks: AQuA (Figure 148, Table 17), StrategyQA (Figure 150, Table 18), ECQA (Figure 151, Table 19), CREAK (Figure 152, Table 20), Sensemaking (Figure 153, Table 21). B.2.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 154 and Table 22. Figure 18: Visualization for MATH-Geometry using gemma-2-2b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.29 1.65 0. 4.59 1.77 0.47 9.37 4.19 1.46 10.04 3.87 1.23 0.03 0.02 0.02 0.04 0.03 0.03 0.04 0.03 0. 0.04 0.01 0.01 2.07 1.06 0.46 2.75 1.11 0.50 9.72 3.19 1.69 4.36 2.19 1.25 0.09 0.03 0. 0.05 0.02 0.02 0.07 0.04 0.03 0.05 0.03 0.02 3.44 0.90 0.46 3.43 0.88 0.50 2.28 1.00 0. 1.91 0.94 0.55 0.10 0.02 0.02 0.09 0.03 0.03 0.03 0.05 0.03 0.07 0.04 0.03 2.77 1.17 0. 3.38 1.22 0.49 8.31 3.07 1.45 5.17 2.26 1.06 0.08 0.02 0.02 0.05 0.02 0.02 0.05 0.04 0. 0.05 0.03 0.02 Table 7: Statistical results for AQuA using gemma-2-2b on correct responses. B.1.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 144 and Table 13. B.2 Pre-trained LLM on Irrelevant Responses B.2.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 145, Table 14), MATH-Counting (Figure 146, Table 15), MATHGeometry (Figure 147, Table 16)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "27 Figure 19: Visualization for AQuA using gemma-2-2b on correct responses. Figure 20: Visualization for GSM8K using gemma-2-2b on correct responses. 28 Figure 21: Visualization for StrategyQA using gemma-2-2b on correct responses. Figure 22: Visualization for ECQA using gemma-2-2b on correct responses. 29 Figure 23: Visualization for CREAK using gemma-2-2b on correct responses. Figure 24: Visualization for Sensemaking using gemma-2-2b on correct responses. 30 Figure 25: Visualization for Wiki tasks using gemma-2-2b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 17.07 1.90 0.72 16.58 1.52 0.54 46.14 6.01 2. 42.31 5.41 2.08 0.02 0.02 0.02 0.03 0.03 0.03 0.05 0.05 0.03 0.03 0.02 0.01 8.82 1.34 0. 13.87 2.08 0.77 51.57 5.32 2.32 18.79 2.82 1.92 0.07 0.04 0.02 0.03 0.02 0.02 0.07 0.06 0. 0.04 0.04 0.03 7.27 1.48 0.56 14.35 2.22 0.63 24.02 3.23 1.52 3.24 1.33 0.75 0.10 0.03 0. 0.07 0.03 0.02 0.06 0.04 0.03 0.09 0.05 0.02 10.75 1.47 0.69 14.48 1.82 0.65 45.96 5.19 2. 20.68 3.04 1.68 0.06 0.03 0.02 0.04 0.03 0.02 0.06 0.05 0.03 0.05 0.04 0.02 Table 10: Statistical results for ECQA using gemma-22b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 9.01 2.05 0.69 7.59 1.55 0. 23.08 6.63 2.05 24.96 6.47 1.73 0.02 0.03 0.03 0.01 0.03 0.04 0.05 0.05 0.03 0.04 0.03 0. 4.68 1.53 0.69 8.43 2.23 0.73 25.63 5.80 2.02 9.49 2.87 1.74 0.06 0.03 0.02 0.04 0.02 0. 0.07 0.05 0.04 0.05 0.04 0.03 6.31 2.12 0.53 10.70 2.03 0.62 12.47 3.85 1.21 4.32 1.89 0. 0.07 0.05 0.02 0.09 0.04 0.03 0.04 0.03 0.04 0.10 0.05 0.02 6.48 1.74 0.64 8.79 1.93 0. 23.21 5.84 1.86 11.66 3.50 1.47 0.06 0.03 0.02 0.04 0.03 0.02 0.06 0.04 0.03 0.05 0.03 0. Table 12: Statistical results for Sensemaking using gemma-2-2b on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 15.91 1.89 0.75 18.49 1.50 0.60 41.16 5.76 2. 40.93 5.45 2.03 0.02 0.02 0.03 0.03 0.04 0.03 0.05 0.04 0.03 0.04 0.02 0.01 7.63 1.12 0. 11.02 1.76 0.73 41.56 4.92 2.32 15.01 2.46 1.81 0.08 0.03 0.02 0.03 0.02 0.02 0.07 0.06 0. 0.05 0.04 0.03 9.89 1.59 0.56 12.53 1.67 0.56 15.30 3.16 1.09 4.05 1.60 0.91 0.11 0.05 0. 0.07 0.04 0.02 0.03 0.03 0.04 0.10 0.06 0.03 10.00 1.38 0.67 13.18 1.58 0.63 37.29 4.93 2. 18.38 2.97 1.64 0.07 0.03 0.02 0.04 0.03 0.02 0.05 0.05 0.04 0.06 0.04 0.02 Table 11: Statistical results for CREAK using gemma2-2b on correct responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 1.05 0.71 0.65 1. 0.76 0.53 0.50 1.60 3.02 2.38 2.05 3.96 2.63 1.97 1.76 3.26 0.03 0.02 0.02 0.03 0.04 0.03 0.03 0.03 0.03 0.02 0.02 0. 0.02 0.01 0.01 0.02 0.57 0.49 0.48 0.86 0.77 0.46 0.37 1.17 2.84 2.07 1.73 3.47 1.76 1.50 1.32 1.88 0.02 0.02 0.02 0. 0.02 0.01 0.01 0.02 0.05 0.04 0.03 0.05 0.04 0.02 0.02 0.04 1.09 0.69 0.54 1.20 0.94 0.56 0.46 0.95 1.42 1.03 0.89 1. 1.87 1.45 1.23 1.96 0.02 0.01 0.01 0.03 0.02 0.01 0.01 0.03 0.08 0.05 0.04 0.09 0.08 0.04 0.03 0.08 0.81 0.57 0.52 1. 0.77 0.49 0.42 1.21 2.58 1.94 1.65 3.27 1.94 1.57 1.38 2.15 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.05 0.03 0.03 0. 0.04 0.02 0.02 0.04 Table 13: Statistical results for Wiki using gemma-2-2b on correct responses. 32 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.93 0.68 0.77 0.56 3.40 2.18 3.09 1.74 0.03 0.03 0.03 0. 0.03 0.03 0.01 0.01 0.69 0.42 0.54 0.38 2.78 1.84 1.70 1. 0.03 0.02 0.02 0.02 0.04 0.03 0.02 0.01 0.63 0.53 0.67 0. 0.92 0.76 1.33 0.84 0.02 0.02 0.03 0.03 0.05 0.04 0.04 0. 0.71 0.50 0.60 0.46 2.63 1.75 1.97 1.26 0.03 0.02 0.02 0. 0.04 0.03 0.02 0.02 Table 14: Statistical results for MATH-Algebra using gemma-2-2b on irrelevant responses. Figure 26: Visualization for MATH-Algebra using gemma-2-2b on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.96 0.90 0.74 0.73 3.60 2.73 3.25 2. 0.03 0.03 0.04 0.04 0.03 0.03 0.01 0.01 0.77 0.55 0.63 0. 2.74 1.99 1.75 1.40 0.03 0.03 0.02 0.02 0.04 0.03 0.02 0. 0.78 0.53 0.58 0.49 1.05 0.98 1.34 0.89 0.02 0.02 0.03 0. 0.05 0.04 0.03 0.02 0.80 0.63 0.63 0.54 2.67 2.01 2.02 1. 0.03 0.03 0.03 0.03 0.04 0.03 0.02 0.02 Table 15: Statistical results for MATH-Counting using gemma-2-2b on irrelevant responses. Figure 27: Visualization for MATH-Counting using gemma-2-2b on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.88 0. 0.74 0.76 2.95 2.78 2.61 2.10 0.03 0.03 0.03 0.04 0.03 0. 0.01 0.01 0.70 0.39 0.50 0.52 2.44 2.32 1.59 1.54 0.03 0. 0.02 0.02 0.04 0.03 0.02 0.02 0.67 0.75 0.55 0.75 0.55 0. 1.03 1.08 0.02 0.04 0.03 0.04 0.05 0.04 0.03 0.03 0.73 0. 0.57 0.64 2.23 2.14 1.71 1.53 0.03 0.03 0.02 0.03 0.03 0. 0.02 0.02 Table 16: Statistical results for MATH-Geometry using gemma-2-2b on irrelevant responses. Dataset Curve Cot sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed Mean Absolute Difference (MAD) Early Middle 111371.56 2.75 1.54 131198.87 2.24 1.18 683220.41 6.23 4.16 311685.32 5.32 3.14 29719.68 1.19 0.70 34747.63 1.65 0. 208522.69 4.76 2.68 92564.91 2.29 1.84 0.02 0.03 0.04 0.02 0.04 0.05 0.07 0.04 0.03 0.05 0.02 0. 0.06 0.03 0.02 0.03 0.02 0.02 0.09 0.06 0.04 0.03 0.03 0.02 Last 9.10 1.52 0. 13.72 1.48 0.68 69.15 5.15 1.97 17.60 2.00 1.54 0.08 0.03 0.03 0.05 0.03 0.03 0.04 0.06 0. 0.07 0.06 0.02 All 48455.22 1.67 0.91 53897.70 1.72 0.84 289494.88 5.23 2.95 124625.79 2.98 2. 0.06 0.03 0.03 0.03 0.03 0.03 0.07 0.05 0.03 0.04 0.03 0.02 Figure 28: Visualization for MATH-Geometry using gemma-2-2b on irrelevant responses. Table 18: Statistical results for StrategyQA using gemma-2-2b on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.41 1.65 0.95 4.80 1.77 0.85 9.50 4.19 2.93 10.25 3.87 2.46 0.03 0.02 0. 0.04 0.03 0.03 0.04 0.03 0.03 0.04 0.01 0.01 2.11 1.06 0.53 2.75 1.11 0.72 9.88 3.19 2. 4.43 2.19 1.58 0.09 0.03 0.03 0.05 0.02 0.02 0.07 0.04 0.03 0.05 0.03 0.02 3.47 0.90 0. 3.41 0.88 0.72 2.35 1.00 1.13 1.92 0.94 1.14 0.10 0.02 0.02 0.09 0.03 0.03 0.03 0.05 0. 0.07 0.04 0.03 2.83 1.17 0.65 3.44 1.22 0.72 8.45 3.07 2.33 5.27 2.26 1.68 0.08 0.02 0. 0.05 0.02 0.03 0.05 0.04 0.03 0.05 0.03 0.02 sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 14.09 2.51 1.29 13.60 2.03 1.02 40.86 7.26 4.19 38.03 6.74 3. 0.02 0.03 0.03 0.03 0.03 0.04 0.05 0.05 0.04 0.03 0.02 0.01 5.80 1.18 0.81 10.84 2.11 0. 44.16 6.21 2.79 16.51 2.85 1.98 0.05 0.03 0.03 0.03 0.02 0.03 0.07 0.06 0.04 0.04 0.03 0. 9.27 1.55 0.71 12.96 1.23 0.67 41.30 6.16 2.39 4.59 2.00 1.17 0.08 0.02 0.03 0.07 0.03 0. 0.06 0.04 0.04 0.06 0.05 0.02 8.31 1.58 0.89 11.64 1.85 0.89 43.71 6.61 3.11 18.87 3.50 2. 0.05 0.02 0.03 0.04 0.02 0.03 0.06 0.05 0.03 0.04 0.03 0.02 Table 17: Statistical results for AQuA using gemma-22b on irrelevant responses. Table 19: Statistical results for ECQA using gemma-22b on irrelevant responses. 34 Figure 29: Visualization for AQuA using gemma-2-2b on irrelevant responses. Figure 30: Visualization for StrategyQA using gemma-2-2b on irrelevant responses. 35 Figure 31: Visualization for ECQA using gemma-2-2b on irrelevant responses. Figure 32: Visualization for CREAK using gemma-2-2b on irrelevant responses. 36 Figure 33: Visualization for Sensemaking using gemma-2-2b on irrelevant responses. Figure 34: Visualization for Wiki tasks using gemma-2-2b on irrelevant responses. 37 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 16.17 2.55 1.43 18.79 2.19 1.16 41.27 7.35 4.24 41.08 6.97 3. 0.02 0.02 0.03 0.03 0.03 0.04 0.05 0.05 0.03 0.04 0.02 0.02 7.68 1.20 0.76 11.05 1.97 0. 41.45 6.05 2.80 15.00 2.68 1.95 0.08 0.03 0.03 0.03 0.02 0.02 0.07 0.06 0.04 0.05 0.04 0. 10.07 1.40 0.84 12.76 1.18 0.58 15.38 5.39 2.02 4.02 2.17 1.37 0.11 0.03 0.02 0.07 0.03 0. 0.03 0.05 0.04 0.10 0.05 0.03 10.13 1.59 0.92 13.34 1.81 0.90 37.26 6.39 3.03 18.42 3.55 2. 0.08 0.03 0.03 0.04 0.02 0.03 0.05 0.05 0.03 0.06 0.03 0.02 Table 20: Statistical results for CREAK using gemma2-2b on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 9.03 2.64 1.31 7.64 1.96 1.07 23.14 8.12 3. 24.94 7.80 2.94 0.02 0.03 0.03 0.01 0.03 0.04 0.05 0.05 0.03 0.04 0.03 0.01 4.66 1.72 1. 8.40 2.21 1.10 25.62 7.29 2.75 9.50 3.16 2.05 0.06 0.04 0.04 0.04 0.02 0.03 0.07 0.05 0. 0.05 0.03 0.02 6.26 2.34 0.73 10.64 2.12 0.80 12.45 5.94 2.44 4.30 2.30 1.20 0.07 0.08 0. 0.09 0.06 0.04 0.04 0.04 0.03 0.10 0.05 0.02 6.48 2.06 1.04 8.79 2.09 1.01 23.23 7.49 2. 11.66 4.04 2.07 0.06 0.05 0.04 0.04 0.03 0.03 0.06 0.04 0.03 0.05 0.03 0.02 Table 21: Statistical results for Sensemaking using gemma-2-2b on irrelevant responses. 38 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 2.37 1.63 1.37 3.46 1.93 1.32 1.11 2. 4.86 3.76 3.29 5.82 4.18 3.06 2.70 5.14 0.04 0.04 0.04 0.03 0.04 0.05 0.05 0.04 0.04 0.03 0.02 0.04 0.02 0.02 0.02 0. 1.03 0.66 0.53 1.27 1.26 0.71 0.55 1.75 4.01 2.93 2.51 4.63 2.28 1.86 1.59 2.49 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0. 0.06 0.04 0.03 0.06 0.03 0.02 0.02 0.03 1.23 0.91 0.79 1.31 1.19 0.83 0.73 1.20 2.83 1.94 1.62 3.93 2.43 2.23 2.07 3. 0.02 0.03 0.03 0.02 0.03 0.03 0.03 0.02 0.07 0.05 0.04 0.08 0.07 0.04 0.03 0.07 1.42 0.96 0.80 1.84 1.39 0.87 0.72 1. 3.95 2.93 2.53 4.76 2.73 2.22 1.97 3.15 0.03 0.03 0.03 0.02 0.03 0.03 0.03 0.02 0.05 0.04 0.03 0.05 0.03 0.02 0.02 0. Table 22: Statistical results for Wiki using gemma-2-2b on irrelevant responses. B."
        },
        {
            "title": "Instructed LLM on Correct Responses",
            "content": "Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All B.3.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 155, Table 23), MATH-Counting (Figure 156, Table 24), MATHGeometry (Figure 157, Table 25). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.56 0. 0.57 0.35 2.47 1.35 1.99 1.01 0.02 0.02 0.03 0.04 0.03 0. 0.01 0.01 0.82 0.53 0.67 0.52 2.87 1.70 1.89 1.17 0.02 0. 0.02 0.03 0.05 0.04 0.02 0.02 1.02 0.73 0.74 0.59 0.73 0. 0.76 0.43 0.07 0.08 0.05 0.05 0.05 0.04 0.04 0.04 0.84 0. 0.67 0.51 2.36 1.40 1.68 0.97 0.04 0.05 0.03 0.04 0.04 0. 0.02 0.02 Table 23: Statistical results for MATH-Algebra using gemma-2-2b-it on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.55 0.40 0.51 0.36 2.52 1.52 2.10 1.17 0.02 0.02 0.03 0. 0.02 0.02 0.01 0.01 0.75 0.62 0.66 0.55 2.61 1.72 1.71 1. 0.02 0.03 0.02 0.03 0.05 0.05 0.03 0.02 0.67 0.55 0.54 0. 0.76 0.67 0.75 0.44 0.05 0.07 0.04 0.05 0.05 0.04 0.04 0. 0.70 0.57 0.60 0.49 2.22 1.45 1.60 1.02 0.03 0.04 0.03 0. 0.04 0.04 0.02 0.02 Table 24: Statistical results for MATH-Counting using gemma-2-2b-it on correct responses. Figure 36: Visualization for MATH-Counting using gemma-2-2b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.55 0.41 0.47 0.36 2.11 1.53 1.72 1. 0.02 0.02 0.03 0.04 0.02 0.02 0.01 0.01 0.76 0.57 0.53 0. 2.00 1.65 1.53 1.16 0.02 0.02 0.02 0.02 0.04 0.04 0.02 0. 0.94 0.60 0.72 0.49 0.48 0.57 0.66 0.51 0.05 0.07 0.04 0. 0.05 0.04 0.03 0.04 0.78 0.56 0.58 0.49 1.74 1.40 1.39 1. 0.03 0.04 0.03 0.04 0.04 0.04 0.02 0.02 Table 25: Statistical results for MATH-Geometry using gemma-2-2b-it on correct responses. Figure 35: Visualization for MATH-Algebra using gemma-2-2b-it on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 158, Table 26), GSM8K (Figure 159, Table 27), StrategyQA (Figure 160, Table 28), ECQA (Figure 161, Table 29), CREAK (Figure 162, Table 30), Sensemaking (Figure 163, Table 31). 39 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.64 0.72 0.41 4.28 0.69 0.36 25.54 3.04 1.79 19.49 2.35 1.39 0.03 0.02 0.02 0.04 0.03 0. 0.03 0.03 0.03 0.02 0.01 0.01 6.94 1.24 0.89 10.24 1.60 1.04 33.15 3.49 2.38 14.00 2.36 1. 0.04 0.03 0.05 0.03 0.03 0.04 0.06 0.05 0.06 0.06 0.04 0.03 11.69 1.44 1.11 15.36 0.84 0. 13.66 0.77 0.96 10.25 0.38 0.35 0.05 0.07 0.09 0.05 0.04 0.06 0.10 0.05 0.04 0.07 0.04 0. 7.45 1.25 0.91 9.16 1.22 0.81 26.66 2.79 1.91 14.57 1.93 1.27 0.04 0.05 0.06 0.04 0.03 0. 0.06 0.04 0.04 0.05 0.03 0.03 Table 27: Statistical results for GSM8K using gemma2-2b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 11.01 1.47 0.44 8.98 1.00 0.36 48.02 5.55 2.19 33.59 4.11 1.68 0.02 0.03 0. 0.03 0.04 0.03 0.05 0.04 0.03 0.03 0.02 0.01 9.56 1.48 0.78 10.89 2.82 0.64 63.95 6.20 2. 22.10 2.94 1.59 0.06 0.04 0.02 0.02 0.04 0.02 0.05 0.07 0.05 0.06 0.04 0.03 4.80 2.23 0. 9.19 1.73 0.44 29.91 1.57 0.44 11.38 1.62 0.78 0.07 0.04 0.04 0.05 0.03 0.02 0.07 0.07 0. 0.07 0.06 0.03 9.15 1.69 0.68 10.78 2.00 0.52 52.33 5.05 1.84 23.64 2.99 1.42 0.05 0.04 0. 0.03 0.03 0.03 0.05 0.06 0.04 0.05 0.04 0.02 Table 28: Statistical results for StrategyQA using gemma-2-2b-it on correct responses. reasoning tasks: AQuA (Figure 168, Table 36), GSM8K (Figure 169, Table 37), StrategyQA (Figure 170, Table 38), ECQA (Figure 171, Table 39), CREAK (Figure 172, Table 40), Sensemaking (Figure 173, Table 41). B.4.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 174 and Table 42. Figure 37: Visualization for MATH-Geometry using gemma-2-2b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.94 1.18 0.43 3.77 1.18 0. 16.91 4.70 1.65 12.87 3.52 1.25 0.03 0.02 0.02 0.04 0.04 0.03 0.02 0.02 0.02 0.01 0.01 0. 5.18 1.42 0.74 8.15 1.94 0.80 24.85 5.12 2.11 11.21 3.00 1.36 0.05 0.03 0.04 0.03 0.02 0. 0.05 0.05 0.05 0.05 0.04 0.03 12.30 2.39 0.94 19.98 2.05 0.71 8.20 1.32 0.87 5.43 1.04 0. 0.10 0.08 0.09 0.04 0.04 0.06 0.16 0.06 0.04 0.08 0.04 0.04 6.55 1.68 0.77 8.94 1.82 0. 19.14 4.20 1.73 10.38 2.72 1.14 0.05 0.04 0.05 0.04 0.03 0.04 0.07 0.04 0.04 0.04 0.03 0. Table 26: Statistical results for AQuA using gemma-22b-it on correct responses. B.3.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 164 and Table ??. B."
        },
        {
            "title": "Instructed LLM on Irrelevant Responses",
            "content": "B.4.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 165, Table 33), MATH-Counting (Figure 166, Table 34), MATHGeometry (Figure 167, Table 35)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "40 Figure 38: Visualization for AQuA using gemma-2-2b-it on correct responses. Figure 39: Visualization for GSM8K using gemma-2-2b-it on correct responses. 41 Figure 40: Visualization for StrategyQA using gemma-2-2b-it on correct responses. Figure 41: Visualization for ECQA using gemma-2-2b-it on correct responses. 42 Figure 42: Visualization for CREAK using gemma-2-2b-it on correct responses. Figure 43: Visualization for Sensemaking using gemma-2-2b-it on correct responses. 43 Figure 44: Visualization for Wiki tasks using gemma-2-2b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 8.08 1.85 0.55 6.91 1.39 0.47 39.01 8.46 2. 29.11 6.28 2.10 0.02 0.02 0.02 0.03 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.01 13.75 3.70 0. 19.80 5.91 1.07 47.62 9.71 3.04 19.93 5.20 2.22 0.08 0.05 0.04 0.03 0.05 0.04 0.06 0.06 0. 0.07 0.05 0.03 11.11 2.91 0.92 10.42 1.71 0.59 22.02 2.91 0.90 5.20 1.27 0.70 0.10 0.07 0. 0.07 0.05 0.05 0.07 0.05 0.03 0.11 0.04 0.03 11.72 3.27 0.90 13.94 3.69 0.81 39.91 7.90 2. 19.57 4.71 1.85 0.07 0.05 0.05 0.04 0.05 0.04 0.05 0.05 0.04 0.06 0.04 0.02 Table 29: Statistical results for ECQA using gemma-22b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.07 2.03 0.51 6.14 1.53 0. 34.29 9.30 2.29 24.30 6.91 1.68 0.02 0.02 0.02 0.04 0.03 0.04 0.02 0.03 0.02 0.02 0.02 0. 12.93 2.84 0.99 17.32 5.10 0.97 44.41 9.10 2.79 19.65 5.03 2.06 0.08 0.04 0.04 0.02 0.04 0. 0.05 0.05 0.05 0.08 0.04 0.03 10.41 2.93 1.00 13.03 3.22 0.67 9.37 3.85 0.84 3.97 1.18 0. 0.07 0.08 0.07 0.05 0.05 0.05 0.07 0.05 0.03 0.08 0.04 0.03 10.52 2.81 0.92 12.55 3.56 0. 35.20 8.18 2.24 18.15 4.86 1.63 0.06 0.05 0.05 0.03 0.04 0.04 0.05 0.05 0.04 0.06 0.03 0. Table 31: Statistical results for Sensemaking using gemma-2-2b-it on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 19.40 2.17 0.52 15.51 1.52 0.42 97.03 9.55 2. 67.77 7.08 1.96 0.02 0.02 0.02 0.03 0.03 0.03 0.03 0.04 0.03 0.02 0.02 0.01 21.48 3.38 0. 28.99 5.41 0.96 109.60 10.54 2.84 37.98 4.32 1.90 0.07 0.05 0.03 0.04 0.04 0.03 0.05 0.06 0. 0.06 0.04 0.03 18.64 3.35 1.08 28.47 2.04 0.70 26.16 3.15 0.70 9.63 1.76 0.81 0.09 0.09 0. 0.08 0.06 0.04 0.07 0.05 0.04 0.08 0.04 0.03 20.16 3.24 0.92 26.59 3.46 0.77 90.55 8.84 2. 41.16 4.69 1.67 0.07 0.05 0.04 0.04 0.04 0.03 0.05 0.05 0.04 0.05 0.03 0.02 Table 30: Statistical results for CREAK using gemma2-2b-it on correct responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.96 0.60 0.55 1. 0.71 0.42 0.34 1.52 3.44 2.59 2.20 4.56 2.80 2.09 1.85 3.18 0.03 0.02 0.02 0.03 0.04 0.03 0.04 0.03 0.04 0.03 0.03 0. 0.02 0.01 0.01 0.02 0.89 0.50 0.46 1.09 1.39 0.78 0.57 2.14 3.72 2.52 2.06 5.46 2.03 1.58 1.37 2.27 0.03 0.03 0.03 0. 0.03 0.04 0.04 0.03 0.07 0.05 0.05 0.08 0.04 0.03 0.02 0.03 1.41 0.78 0.64 1.08 1.11 0.66 0.53 0.79 1.28 1.07 0.86 1. 2.23 1.57 1.30 2.28 0.03 0.03 0.03 0.02 0.03 0.03 0.03 0.03 0.09 0.06 0.05 0.08 0.07 0.04 0.03 0.06 1.03 0.58 0.52 1. 1.10 0.65 0.50 1.58 3.09 2.20 1.82 4.42 2.28 1.69 1.46 2.46 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.06 0.05 0.04 0. 0.04 0.03 0.02 0.03 Table 32: Statistical results for Wiki using gemma-2-2bit on correct responses. 45 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.63 0.47 0.58 0.44 3.42 2.10 2.76 1.55 0.02 0.03 0.03 0. 0.03 0.02 0.01 0.01 0.75 0.47 0.70 0.48 3.02 1.81 1.89 1. 0.03 0.03 0.03 0.03 0.05 0.04 0.02 0.01 1.32 0.83 1.03 0. 1.28 0.84 1.30 0.83 0.09 0.09 0.08 0.08 0.06 0.05 0.04 0. 0.89 0.59 0.75 0.52 2.83 1.72 2.02 1.25 0.04 0.05 0.04 0. 0.05 0.04 0.02 0.02 Table 33: Statistical results for MATH-Algebra using gemma-2-2b-it on irrelevant responses. Figure 45: Visualization for MATH-Algebra using gemma-2-2b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.63 0.56 0.58 0.50 3.47 2.47 2.87 1. 0.03 0.04 0.04 0.05 0.03 0.02 0.01 0.01 0.64 0.49 0.69 0. 3.00 1.78 1.78 1.22 0.03 0.03 0.03 0.03 0.05 0.04 0.02 0. 1.22 0.80 0.99 0.63 1.20 0.80 1.22 0.82 0.08 0.09 0.08 0. 0.06 0.05 0.03 0.03 0.80 0.62 0.74 0.54 2.76 1.76 1.94 1. 0.04 0.05 0.04 0.05 0.04 0.04 0.02 0.02 Table 34: Statistical results for MATH-Counting using gemma-2-2b-it on irrelevant responses. Figure 46: Visualization for MATH-Counting using gemma-2-2b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.60 0. 0.50 0.51 2.67 2.60 2.22 1.81 0.02 0.04 0.03 0.05 0.03 0. 0.01 0.01 0.65 0.44 0.51 0.55 2.31 2.25 1.53 1.34 0.02 0. 0.03 0.03 0.04 0.05 0.02 0.02 1.36 1.23 1.07 1.26 0.81 1. 0.98 1.13 0.08 0.10 0.08 0.10 0.06 0.05 0.03 0.04 0.84 0. 0.67 0.75 2.12 2.06 1.60 1.40 0.04 0.05 0.04 0.05 0.04 0. 0.02 0.02 Table 35: Statistical results for MATH-Geometry using gemma-2-2b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.30 0.96 0.70 3.94 0.95 0.62 24.31 4.61 3.06 18.70 3.54 2.19 0.03 0.03 0.04 0.04 0.05 0. 0.03 0.04 0.03 0.02 0.01 0.01 6.74 0.88 0.61 10.22 1.10 0.88 32.27 3.78 2.26 13.57 2.31 1. 0.04 0.02 0.03 0.04 0.02 0.03 0.06 0.06 0.05 0.06 0.03 0.02 11.48 1.83 1.22 15.59 1.60 0. 13.78 1.43 0.98 10.20 1.04 0.80 0.06 0.07 0.10 0.05 0.06 0.07 0.10 0.06 0.04 0.07 0.04 0. 7.24 1.17 0.84 9.07 1.19 0.81 25.89 3.47 2.20 14.13 2.34 1.58 0.04 0.04 0.06 0.04 0.04 0. 0.06 0.05 0.04 0.05 0.03 0.02 Figure 47: Visualization for MATH-Geometry using gemma-2-2b-it on irrelevant responses. Table 37: Statistical results for GSM8K using gemma2-2b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.97 1.22 0.69 3.84 1.23 0.62 17.02 5.30 3.11 12.88 4.15 2.21 0.03 0.03 0.04 0.04 0.04 0. 0.02 0.03 0.03 0.01 0.01 0.01 5.15 1.15 0.58 8.17 1.49 0.86 24.77 5.01 2.46 11.19 2.75 1. 0.05 0.02 0.03 0.03 0.02 0.03 0.05 0.05 0.05 0.05 0.03 0.02 12.37 2.35 1.15 20.16 2.02 0. 8.19 1.99 1.02 5.43 1.61 1.03 0.10 0.08 0.09 0.04 0.06 0.07 0.16 0.06 0.04 0.08 0.04 0. 6.55 1.49 0.80 9.03 1.56 0.81 19.12 4.45 2.34 10.37 2.87 1.62 0.05 0.04 0.05 0.04 0.04 0. 0.07 0.05 0.04 0.04 0.02 0.02 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 11.79 2.20 1.26 9.75 1.74 0.97 50.93 6.11 3.78 35.52 4.47 2.71 0.02 0.04 0. 0.03 0.04 0.05 0.05 0.05 0.03 0.03 0.02 0.02 10.10 0.96 0.44 12.02 1.62 0.65 67.72 4.74 2. 23.60 2.37 1.70 0.06 0.03 0.03 0.02 0.03 0.03 0.05 0.07 0.05 0.06 0.03 0.02 4.95 1.81 1. 9.80 1.69 0.97 30.54 3.01 1.26 11.71 2.34 1.47 0.07 0.04 0.06 0.05 0.04 0.06 0.07 0.08 0. 0.07 0.05 0.02 9.63 1.51 0.79 11.68 1.61 0.79 55.22 4.66 2.54 25.04 2.99 1.93 0.05 0.03 0. 0.03 0.03 0.04 0.05 0.06 0.04 0.05 0.03 0.02 Table 36: Statistical results for AQuA using gemma-22b-it on irrelevant responses. Table 38: Statistical results for StrategyQA using gemma-2-2b-it on irrelevant responses. Figure 48: Visualization for AQuA using gemma-2-2b-it on irrelevant responses. Figure 49: Visualization for GSM8K using gemma-2-2b-it on irrelevant responses. 48 Figure 50: Visualization for StrategyQA using gemma-2-2b-it on irrelevant responses. Figure 51: Visualization for ECQA using gemma-2-2b-it on irrelevant responses. Figure 52: Visualization for CREAK using gemma-2-2b-it on irrelevant responses. Figure 53: Visualization for Sensemaking using gemma-2-2b-it on irrelevant responses. 50 Figure 54: Visualization for Wiki tasks using gemma-2-2b-it on irrelevant responses. 51 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.22 1.84 1.07 5.62 1.55 0.77 32.47 7.71 4.24 24.35 5.84 3. 0.02 0.02 0.04 0.03 0.04 0.05 0.04 0.04 0.02 0.02 0.02 0.02 8.86 1.87 0.83 14.70 3.28 1. 32.21 6.52 2.95 13.34 3.09 1.92 0.05 0.04 0.04 0.04 0.04 0.04 0.06 0.07 0.05 0.06 0.03 0. 12.56 2.27 1.28 12.56 1.82 0.97 31.67 4.89 1.67 7.35 1.52 1.12 0.09 0.08 0.09 0.06 0.06 0. 0.07 0.05 0.04 0.06 0.05 0.02 8.62 2.01 1.03 11.71 2.39 0.99 31.90 6.46 3.00 15.24 3.52 2. 0.05 0.04 0.05 0.04 0.04 0.05 0.05 0.05 0.04 0.05 0.03 0.02 Table 39: Statistical results for ECQA using gemma-22b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.68 2.20 0.84 7.41 1.79 0.74 41.81 9.20 3. 29.03 6.87 2.47 0.02 0.02 0.03 0.04 0.04 0.04 0.03 0.03 0.02 0.02 0.02 0.01 13.57 1.91 0. 17.48 3.61 1.21 46.33 7.94 2.89 21.86 3.71 1.96 0.06 0.03 0.04 0.02 0.03 0.04 0.05 0.06 0. 0.07 0.04 0.02 12.50 1.91 1.29 14.33 2.98 0.88 10.97 5.11 1.55 3.94 1.61 0.92 0.08 0.07 0. 0.04 0.06 0.07 0.06 0.04 0.03 0.08 0.04 0.02 11.79 2.01 1.05 13.10 2.79 1.05 38.74 7.75 2. 20.63 4.22 1.87 0.06 0.04 0.06 0.03 0.04 0.05 0.05 0.05 0.04 0.06 0.03 0.02 Table 41: Statistical results for Sensemaking using gemma-2-2b-it on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 20.78 2.42 1.10 16.37 1.90 0.76 99.45 8.94 3.94 70.14 6.66 2. 0.02 0.03 0.03 0.03 0.04 0.04 0.03 0.04 0.02 0.02 0.02 0.02 22.32 2.21 0.74 29.50 3.55 1. 112.28 8.00 3.03 39.66 3.32 1.93 0.07 0.04 0.03 0.03 0.04 0.04 0.05 0.06 0.05 0.06 0.04 0. 16.61 2.19 1.49 25.20 2.29 1.20 26.10 4.34 1.37 10.86 1.95 1.32 0.09 0.08 0.07 0.07 0.06 0. 0.07 0.06 0.04 0.08 0.04 0.02 20.27 2.30 1.03 26.23 2.70 1.02 92.60 7.52 2.88 43.13 4.05 2. 0.07 0.05 0.04 0.04 0.04 0.04 0.05 0.05 0.04 0.05 0.03 0.02 Table 40: Statistical results for CREAK using gemma2-2b-it on irrelevant responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 1.97 1.36 1.15 1.96 1.63 0.99 0.80 1. 4.96 3.73 3.17 5.69 3.77 2.84 2.57 4.17 0.04 0.04 0.03 0.02 0.05 0.05 0.05 0.03 0.05 0.03 0.03 0.05 0.02 0.02 0.02 0. 0.93 0.52 0.43 1.04 1.36 0.72 0.55 1.84 3.75 2.66 2.29 4.80 2.09 1.68 1.46 2.31 0.03 0.03 0.02 0.03 0.02 0.03 0.03 0. 0.07 0.05 0.05 0.07 0.02 0.02 0.02 0.03 1.44 0.91 0.78 1.41 1.33 0.79 0.67 1.29 1.90 1.56 1.43 2.96 2.69 2.23 2.00 3. 0.03 0.04 0.04 0.03 0.04 0.04 0.05 0.04 0.08 0.06 0.06 0.09 0.06 0.03 0.03 0.05 1.32 0.81 0.68 1.33 1.38 0.76 0.60 1. 3.62 2.64 2.29 4.53 2.68 2.10 1.87 2.93 0.03 0.03 0.03 0.03 0.03 0.03 0.04 0.03 0.06 0.05 0.04 0.06 0.03 0.02 0.02 0. Table 42: Statistical results for Wiki using gemma-2-2bit on irrelevant responses. 52 Results on Llama-3.1-8B Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All C.1 Pre-trained LLM on Correct Responses C.1.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 135, Table 43), MATH-Counting (Figure 136, Table 44), MATHGeometry (Figure 137, Table 45). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.76 0.42 0.28 0.17 2.10 0. 1.06 0.61 0.01 0.01 0.02 0.02 0.02 0.01 0.01 0.01 0.76 0. 0.28 0.10 1.24 0.56 1.31 0.70 0.01 0.01 0.01 0.01 0.01 0. 0.01 0.01 0.35 0.17 0.12 0.07 0.51 0.27 0.61 0.33 0.04 0. 0.01 0.02 0.02 0.02 0.01 0.01 0.58 0.29 0.22 0.11 1.20 0. 0.93 0.51 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.01 Table 43: Statistical results for MATH-Algebra using Llama-3.1-8B on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.12 0.54 0.40 0. 2.91 1.11 1.41 0.84 0.01 0.01 0.02 0.02 0.02 0.02 0.01 0. 0.94 0.42 0.37 0.12 1.49 0.62 1.62 0.84 0.01 0.01 0.01 0. 0.01 0.01 0.01 0.01 0.56 0.24 0.17 0.08 0.57 0.35 0.72 0. 0.05 0.06 0.02 0.02 0.01 0.01 0.01 0.01 0.82 0.37 0.30 0. 1.52 0.63 1.14 0.66 0.03 0.03 0.01 0.01 0.02 0.01 0.01 0. Table 44: Statistical results for MATH-Counting using Llama-3.1-8B on correct responses. Figure 56: Visualization for MATH-Counting using Llama-3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.14 0.61 0.44 0.24 3.17 1.40 1.70 0.90 0.01 0.01 0.02 0. 0.02 0.01 0.01 0.01 1.10 0.56 0.41 0.16 1.85 0.80 1.93 1. 0.01 0.01 0.00 0.00 0.01 0.01 0.01 0.01 0.61 0.32 0.22 0. 0.70 0.38 0.89 0.50 0.05 0.06 0.01 0.02 0.02 0.01 0.01 0. 0.88 0.46 0.33 0.16 1.77 0.79 1.40 0.75 0.03 0.03 0.01 0. 0.02 0.01 0.01 0.01 Table 45: Statistical results for MATH-Geometry using Llama-3.1-8B on correct responses. Figure 55: Visualization for MATH-Algebra using Llama-3.1-8B on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 138, Table 46), GSM8K (Figure 139, Table 47), StrategyQA (Figure 140, Table 48), ECQA (Figure 141, Table 49), CREAK (Figure 142, Table 50), Sensemaking (Figure 143, Table 51). Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.91 0.79 0.47 2.38 0.29 0. 19.12 1.63 1.13 5.29 1.26 0.73 0.02 0.01 0.01 0.02 0.02 0.02 0.03 0.02 0.02 0.03 0.01 0. 6.38 0.67 0.37 1.98 0.22 0.11 6.14 1.07 0.60 5.43 1.31 0.79 0.08 0.01 0.01 0.05 0.01 0. 0.03 0.01 0.01 0.03 0.02 0.02 2.62 0.23 0.18 1.67 0.16 0.08 5.86 0.59 0.35 2.82 0.58 0. 0.11 0.03 0.05 0.03 0.01 0.02 0.02 0.02 0.02 0.07 0.02 0.02 4.37 0.53 0.31 2.02 0.21 0. 9.85 1.07 0.64 4.56 0.99 0.61 0.07 0.02 0.02 0.03 0.01 0.02 0.03 0.02 0.02 0.05 0.02 0. Table 47: Statistical results for GSM8K using Llama3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.71 0.84 0. 2.80 0.40 0.23 26.10 3.55 1.25 10.88 1.56 0.95 0.02 0.01 0.01 0.02 0.01 0.02 0.04 0.04 0. 0.03 0.01 0.01 2.43 0.76 0.50 1.25 0.24 0.13 6.82 1.13 0.77 5.17 1.26 1.10 0.05 0.01 0. 0.03 0.01 0.01 0.03 0.02 0.01 0.03 0.03 0.02 0.93 0.40 0.22 1.17 0.19 0.11 2.59 0.81 0. 3.06 1.06 0.79 0.08 0.03 0.06 0.06 0.02 0.02 0.04 0.02 0.01 0.03 0.02 0.02 2.65 0.64 0. 1.77 0.27 0.15 10.73 1.66 0.80 5.94 1.22 0.90 0.05 0.02 0.03 0.04 0.01 0.01 0.04 0.02 0. 0.03 0.02 0.02 Table 48: Statistical results for StrategyQA using Llama3.1-8B on correct responses. reasoning tasks: AQuA (Figure 148, Table 56), GSM8K (Figure 149, Table 57), StrategyQA (Figure 150, Table 58), ECQA (Figure 151, Table 59), CREAK (Figure 152, Table 60), Sensemaking (Figure 153, Table 61). C.2.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 154 and Table 62. Figure 57: Visualization for MATH-Geometry using Llama-3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.61 1.42 0.56 2.51 0.53 0.23 21.85 4.59 1.27 6.22 1.95 0. 0.02 0.01 0.01 0.03 0.02 0.02 0.03 0.02 0.02 0.02 0.01 0.01 5.39 1.03 0.44 1.81 0.35 0. 6.80 1.80 0.68 5.67 2.09 0.92 0.09 0.01 0.01 0.04 0.01 0.00 0.05 0.02 0.01 0.04 0.02 0. 7.22 0.57 0.25 3.74 0.26 0.09 3.35 0.83 0.38 2.69 0.78 0.46 0.14 0.05 0.06 0.13 0.03 0. 0.04 0.01 0.02 0.09 0.01 0.01 5.67 0.93 0.39 2.83 0.36 0.15 9.85 2.24 0.71 4.60 1.48 0. 0.09 0.03 0.03 0.07 0.02 0.02 0.04 0.02 0.02 0.05 0.02 0.01 Table 46: Statistical results for AQuA using Llama-3.18B on correct responses. C.1.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 144 and Table 52. C.2 Pre-trained LLM on Irrelevant Responses C.2.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 145, Table 53), MATH-Counting (Figure 146, Table 54), MATHGeometry (Figure 147, Table 55)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "54 Figure 58: Visualization for AQuA using Llama-3.1-8B on correct responses. Figure 59: Visualization for GSM8K using Llama-3.1-8B on correct responses. 55 Figure 60: Visualization for StrategyQA using Llama-3.1-8B on correct responses. Figure 61: Visualization for ECQA using Llama-3.1-8B on correct responses. 56 Figure 62: Visualization for CREAK using Llama-3.1-8B on correct responses. Figure 63: Visualization for Sensemaking using Llama-3.1-8B on correct responses. 57 Figure 64: Visualization for Wiki tasks using Llama-3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.39 1.46 0.83 3.23 0.65 0.34 31.64 5.53 1. 8.18 1.42 1.17 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.01 6.32 1.12 0. 3.17 0.30 0.15 12.53 1.53 0.94 9.24 1.78 1.35 0.08 0.01 0.01 0.05 0.01 0.01 0.03 0.02 0. 0.04 0.02 0.02 2.14 0.37 0.38 2.97 0.33 0.16 4.07 0.97 0.69 2.77 0.99 0.88 0.11 0.03 0. 0.05 0.02 0.03 0.03 0.02 0.01 0.07 0.02 0.02 4.94 0.92 0.56 3.05 0.42 0.21 14.50 2.45 1. 6.16 1.30 1.06 0.07 0.02 0.04 0.04 0.02 0.02 0.03 0.02 0.02 0.05 0.02 0.02 Table 49: Statistical results for ECQA using Llama-3.18B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.61 1.46 0.68 2.22 0.58 0. 25.94 5.68 1.56 8.32 1.97 1.10 0.02 0.01 0.01 0.03 0.02 0.02 0.03 0.03 0.02 0.02 0.02 0. 6.54 1.46 0.53 2.63 0.48 0.14 8.37 2.02 0.77 7.22 2.34 1.20 0.08 0.02 0.00 0.03 0.02 0. 0.04 0.02 0.01 0.04 0.03 0.02 3.41 0.54 0.26 2.83 0.37 0.13 3.03 1.03 0.57 2.38 1.06 0. 0.13 0.03 0.06 0.09 0.03 0.02 0.03 0.02 0.01 0.05 0.02 0.02 4.65 1.09 0.46 2.55 0.48 0. 11.13 2.70 0.89 5.61 1.68 0.94 0.08 0.02 0.03 0.05 0.02 0.02 0.03 0.02 0.01 0.04 0.02 0. Table 51: Statistical results for Sensemaking using Llama-3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.05 1.05 0. 2.74 0.50 0.30 26.92 4.74 1.83 9.03 1.44 1.08 0.02 0.01 0.01 0.02 0.02 0.02 0.03 0.03 0. 0.02 0.01 0.01 5.06 1.06 0.56 2.39 0.29 0.16 10.35 1.48 0.84 8.36 1.65 1.28 0.06 0.01 0. 0.03 0.01 0.01 0.04 0.02 0.01 0.04 0.03 0.02 3.79 0.44 0.31 4.00 0.26 0.12 4.35 0.89 0. 3.81 1.05 0.87 0.14 0.03 0.07 0.08 0.02 0.02 0.04 0.02 0.01 0.05 0.02 0.02 4.40 0.80 0. 2.97 0.34 0.19 12.72 2.19 1.02 6.62 1.29 1.01 0.07 0.02 0.03 0.04 0.02 0.02 0.04 0.02 0. 0.04 0.02 0.01 Table 50: Statistical results for CREAK using Llama3.1-8B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.88 0.71 0.62 1. 0.36 0.28 0.24 0.45 4.19 2.55 1.96 10.07 1.88 1.64 1.53 3.14 0.02 0.01 0.01 0.02 0.01 0.01 0.01 0.02 0.03 0.02 0.02 0. 0.01 0.01 0.01 0.01 0.76 0.71 0.65 0.79 0.23 0.22 0.18 0.25 0.97 1.09 1.00 1.41 1.18 1.34 1.27 1.12 0.01 0.01 0.01 0. 0.01 0.01 0.00 0.02 0.02 0.02 0.01 0.02 0.03 0.02 0.01 0.04 1.12 0.78 0.59 0.72 0.30 0.25 0.20 0.21 0.89 0.87 0.76 0. 1.04 1.06 0.98 0.85 0.06 0.05 0.05 0.04 0.01 0.01 0.01 0.03 0.01 0.01 0.01 0.03 0.02 0.02 0.01 0.02 0.92 0.71 0.59 0. 0.31 0.25 0.20 0.32 1.89 1.41 1.17 3.68 1.34 1.29 1.19 1.63 0.03 0.03 0.02 0.03 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0. 0.02 0.01 0.01 0.02 Table 52: Statistical results for Wiki using Llama-3.18B on correct responses. 59 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.31 0.68 0.47 0.24 3.38 1.48 1.75 0.82 0.01 0.01 0.01 0. 0.02 0.01 0.01 0.01 1.14 0.55 0.39 0.16 1.55 0.79 1.74 1. 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.82 0.30 0.24 0. 0.71 0.41 0.89 0.52 0.08 0.06 0.03 0.02 0.01 0.01 0.01 0. 1.03 0.48 0.36 0.16 1.70 0.82 1.36 0.74 0.04 0.03 0.02 0. 0.02 0.01 0.01 0.01 Table 53: Statistical results for MATH-Algebra using Llama-3.1-8B on irrelevant responses. Figure 65: Visualization for MATH-Algebra using Llama-3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.67 0.83 0.56 0.30 4.09 1.77 1.99 0. 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.01 1.26 0.54 0.44 0. 1.77 0.79 1.81 1.05 0.03 0.02 0.01 0.01 0.01 0.01 0.01 0. 1.12 0.36 0.33 0.15 0.69 0.47 0.94 0.63 0.10 0.07 0.03 0. 0.01 0.01 0.01 0.01 1.28 0.54 0.43 0.20 1.97 0.93 1.48 0. 0.05 0.03 0.02 0.01 0.02 0.01 0.01 0.01 Table 54: Statistical results for MATH-Counting using Llama-3.1-8B on irrelevant responses. Figure 66: Visualization for MATH-Counting using Llama-3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.50 0. 0.52 0.27 3.97 2.19 2.29 1.21 0.01 0.01 0.02 0.01 0.02 0. 0.01 0.01 1.43 0.52 0.47 0.17 2.00 0.85 2.12 1.16 0.02 0. 0.01 0.01 0.01 0.01 0.01 0.01 1.20 0.47 0.36 0.23 0.83 0. 1.10 0.72 0.09 0.06 0.02 0.03 0.01 0.02 0.01 0.01 1.29 0. 0.43 0.22 2.08 1.08 1.68 0.96 0.04 0.03 0.02 0.02 0.02 0. 0.01 0.01 Table 55: Statistical results for MATH-Geometry using Llama-3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.95 1.50 0.84 2.66 0.45 0.30 21.03 3.16 1.88 5.37 1.50 0.88 0.02 0.01 0.01 0.02 0.02 0. 0.02 0.02 0.01 0.03 0.01 0.01 7.50 1.00 0.52 2.20 0.35 0.17 5.28 1.43 0.73 5.16 1.52 1. 0.07 0.02 0.01 0.05 0.01 0.01 0.03 0.01 0.01 0.03 0.02 0.01 3.86 0.72 0.32 2.29 0.23 0. 5.89 0.68 0.49 2.62 0.89 0.67 0.10 0.08 0.06 0.03 0.02 0.02 0.02 0.01 0.01 0.07 0.02 0. 5.43 0.99 0.52 2.40 0.33 0.19 10.00 1.60 0.95 4.25 1.22 0.80 0.07 0.04 0.03 0.03 0.02 0. 0.03 0.02 0.01 0.04 0.02 0.01 Figure 67: Visualization for MATH-Geometry using Llama-3.1-8B on irrelevant responses. Table 57: Statistical results for GSM8K using Llama3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.98 1.90 0.95 2.50 0.68 0.36 23.13 6.41 2.42 5.71 1.95 1.04 0.02 0.01 0.01 0.02 0.01 0. 0.02 0.02 0.01 0.02 0.01 0.01 4.67 1.47 0.61 1.54 0.47 0.19 5.89 1.96 0.86 5.32 2.08 1. 0.08 0.02 0.01 0.04 0.01 0.01 0.05 0.02 0.01 0.04 0.02 0.01 5.91 1.07 0.52 3.45 0.34 0. 3.67 0.93 0.54 2.75 1.14 0.72 0.12 0.07 0.07 0.13 0.03 0.02 0.04 0.01 0.01 0.08 0.01 0. 5.20 1.40 0.66 2.72 0.48 0.24 10.02 2.81 1.17 4.42 1.63 0.92 0.07 0.03 0.03 0.07 0.02 0. 0.04 0.02 0.01 0.05 0.01 0.01 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.11 1.35 0.90 2.53 0.68 0.36 24.23 5.56 2.46 9.31 2.32 1.43 0.02 0.01 0. 0.02 0.01 0.01 0.04 0.03 0.02 0.03 0.01 0.01 3.77 0.92 0.50 1.38 0.37 0.21 6.14 1.39 0. 4.86 1.22 1.01 0.06 0.02 0.01 0.03 0.02 0.01 0.03 0.02 0.01 0.03 0.02 0.01 1.10 0.74 0. 1.11 0.29 0.16 2.28 0.94 0.70 2.60 1.22 0.97 0.08 0.04 0.06 0.06 0.02 0.02 0.04 0.02 0. 0.04 0.02 0.01 3.22 0.96 0.56 1.73 0.43 0.24 9.87 2.41 1.24 5.28 1.53 1.09 0.05 0.03 0. 0.04 0.01 0.01 0.04 0.03 0.01 0.03 0.02 0.01 Table 56: Statistical results for AQuA using Llama-3.18B on irrelevant responses. Table 58: Statistical results for StrategyQA using Llama3.1-8B on irrelevant responses. Figure 68: Visualization for AQuA using Llama-3.1-8B on irrelevant responses. Figure 69: Visualization for GSM8K using Llama-3.1-8B on irrelevant responses. 62 Figure 70: Visualization for StrategyQA using Llama-3.1-8B on irrelevant responses. Figure 71: Visualization for ECQA using Llama-3.1-8B on irrelevant responses. Figure 72: Visualization for CREAK using Llama-3.1-8B on irrelevant responses. Figure 73: Visualization for Sensemaking using Llama-3.1-8B on irrelevant responses. 64 Figure 74: Visualization for Wiki tasks using Llama-3.1-8B on irrelevant responses. 65 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.06 1.68 1.09 2.57 0.65 0.38 24.55 5.98 2.33 5.68 1.55 1. 0.02 0.01 0.01 0.02 0.02 0.01 0.02 0.02 0.01 0.03 0.01 0.01 3.84 1.13 0.52 0.95 0.36 0. 4.13 1.41 0.92 4.69 1.35 1.13 0.04 0.02 0.01 0.03 0.02 0.01 0.03 0.02 0.01 0.04 0.02 0. 3.92 0.86 0.53 1.93 0.33 0.23 2.53 1.04 0.79 2.78 1.23 1.02 0.08 0.06 0.09 0.06 0.02 0. 0.03 0.01 0.01 0.05 0.02 0.01 4.40 1.16 0.68 1.82 0.44 0.28 9.50 2.55 1.27 4.29 1.32 1. 0.05 0.03 0.04 0.04 0.02 0.02 0.03 0.02 0.01 0.04 0.02 0.01 Table 59: Statistical results for ECQA using Llama-3.18B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.73 2.06 1.00 2.29 0.84 0.35 26.46 7.69 1. 7.81 2.37 1.02 0.02 0.02 0.01 0.03 0.02 0.01 0.03 0.02 0.01 0.02 0.02 0.01 6.45 1.95 0. 2.57 0.76 0.24 8.03 2.40 0.88 6.87 2.30 1.22 0.06 0.02 0.02 0.05 0.03 0.01 0.04 0.02 0. 0.04 0.03 0.01 3.71 0.76 0.57 2.73 0.32 0.21 3.20 1.11 0.73 2.25 1.35 0.93 0.12 0.05 0. 0.11 0.04 0.04 0.03 0.02 0.01 0.06 0.02 0.01 4.75 1.52 0.69 2.51 0.64 0.26 11.28 3.41 1. 5.46 1.94 1.01 0.07 0.03 0.05 0.06 0.03 0.02 0.04 0.02 0.01 0.04 0.02 0.01 Table 61: Statistical results for Sensemaking using Llama-3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.75 1.52 0.96 3.16 0.67 0. 31.92 6.64 2.65 9.59 2.17 1.35 0.02 0.02 0.01 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.01 0. 5.46 1.27 0.51 2.03 0.52 0.21 9.46 1.63 0.85 8.89 1.58 1.05 0.07 0.01 0.01 0.03 0.02 0. 0.04 0.02 0.01 0.04 0.02 0.01 4.91 0.80 0.42 3.97 0.32 0.22 4.11 0.94 0.67 3.67 1.29 0. 0.11 0.05 0.06 0.06 0.02 0.03 0.04 0.02 0.01 0.06 0.02 0.01 5.54 1.15 0.60 3.10 0.49 0. 13.71 2.76 1.29 7.20 1.60 1.07 0.07 0.03 0.03 0.04 0.02 0.02 0.04 0.02 0.01 0.05 0.02 0. Table 60: Statistical results for CREAK using Llama3.1-8B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 1.21 0.99 0.81 1.53 0.57 0.44 0.37 0. 6.03 3.84 3.06 11.89 2.61 2.25 2.11 4.04 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.02 0.03 0.02 0.02 0.04 0.01 0.01 0.01 0. 0.85 0.61 0.53 1.00 0.27 0.20 0.17 0.30 1.13 0.97 0.86 1.57 1.20 1.11 1.00 1.44 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0. 0.02 0.01 0.01 0.02 0.02 0.01 0.01 0.03 1.02 0.65 0.48 1.08 0.32 0.24 0.19 0.41 0.84 0.76 0.67 0.88 1.03 0.97 0.89 1. 0.06 0.06 0.05 0.05 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.03 0.02 0.01 0.01 0.01 1.02 0.73 0.59 1.18 0.39 0.29 0.24 0. 2.47 1.71 1.41 4.36 1.58 1.40 1.29 2.18 0.03 0.03 0.03 0.03 0.01 0.01 0.01 0.01 0.02 0.02 0.01 0.03 0.02 0.01 0.01 0. Table 62: Statistical results for Wiki using Llama-3.18B on irrelevant responses. 66 C."
        },
        {
            "title": "Instructed LLM on Correct Responses",
            "content": "Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All C.3.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 155, Table 63), MATH-Counting (Figure 156, Table 64), MATHGeometry (Figure 157, Table 65). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.74 0. 0.36 0.16 2.20 0.77 1.15 0.51 0.01 0.01 0.02 0.02 0.02 0. 0.01 0.01 0.89 0.43 0.33 0.12 1.30 0.55 1.50 0.70 0.01 0. 0.01 0.00 0.01 0.01 0.01 0.01 0.33 0.20 0.11 0.07 0.49 0. 0.55 0.32 0.05 0.07 0.02 0.03 0.02 0.02 0.01 0.01 0.61 0. 0.27 0.11 1.22 0.50 0.98 0.48 0.03 0.03 0.02 0.02 0.02 0. 0.01 0.01 Table 63: Statistical results for MATH-Algebra using Llama-3.1-8B-Instruct on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.91 0.41 0.42 0.18 2.32 0.85 1.21 0.56 0.01 0.01 0.02 0. 0.02 0.02 0.01 0.01 1.00 0.48 0.36 0.14 1.43 0.63 1.73 0. 0.01 0.01 0.01 0.00 0.01 0.01 0.01 0.01 0.40 0.21 0.10 0. 0.54 0.34 0.58 0.40 0.06 0.08 0.02 0.02 0.01 0.02 0.01 0. 0.72 0.34 0.29 0.12 1.30 0.57 1.08 0.56 0.03 0.03 0.02 0. 0.02 0.02 0.01 0.01 Table 64: Statistical results for MATH-Counting using Llama-3.1-8B-Instruct on correct responses. Figure 76: Visualization for MATH-Counting using Llama-3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.07 0.45 0.48 0.20 2.84 0.97 1.51 0. 0.01 0.01 0.02 0.01 0.02 0.02 0.01 0.01 1.09 0.55 0.36 0. 1.63 0.70 1.77 0.88 0.01 0.01 0.00 0.00 0.01 0.01 0.01 0. 0.69 0.20 0.25 0.07 0.64 0.34 0.75 0.43 0.07 0.06 0.02 0. 0.02 0.01 0.01 0.01 0.89 0.37 0.34 0.13 1.57 0.63 1.22 0. 0.03 0.03 0.01 0.01 0.02 0.01 0.01 0.01 Table 65: Statistical results for MATH-Geometry using Llama-3.1-8B-Instruct on correct responses. Figure 75: Visualization for MATH-Algebra using Llama-3.1-8B-Instruct on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 158, Table 66), GSM8K (Figure 159, Table 67), StrategyQA (Figure 160, Table 68), ECQA (Figure 161, Table 69), CREAK (Figure 162, Table 70), Sensemaking (Figure 163, Table 71). 67 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 16.19 1.30 0.42 9.68 0.68 0.19 62.84 3.26 1.00 27.31 1.75 0.59 0.01 0.01 0.01 0.03 0.03 0. 0.02 0.02 0.02 0.02 0.01 0.01 19.18 1.53 0.51 11.73 0.65 0.17 37.06 2.12 0.67 42.87 2.78 1. 0.03 0.03 0.02 0.04 0.04 0.02 0.03 0.02 0.02 0.04 0.02 0.02 15.89 1.04 0.34 5.44 0.30 0. 13.72 0.73 0.38 11.38 0.78 0.42 0.08 0.10 0.10 0.05 0.03 0.03 0.03 0.03 0.02 0.08 0.03 0. 16.45 1.20 0.40 8.58 0.51 0.14 34.67 1.89 0.64 25.32 1.61 0.63 0.04 0.05 0.05 0.04 0.03 0. 0.03 0.03 0.02 0.05 0.02 0.01 Table 67: Statistical results for GSM8K using Llama3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 13.26 1.85 0.49 7.14 0.91 0.20 58.20 6.78 1.15 23.40 3.26 0.76 0.01 0.01 0. 0.02 0.02 0.02 0.03 0.02 0.02 0.02 0.02 0.01 18.96 3.19 0.68 7.75 1.33 0.18 27.11 3.32 0. 27.11 3.89 1.14 0.04 0.04 0.01 0.02 0.04 0.01 0.03 0.02 0.01 0.04 0.02 0.01 5.16 1.54 0. 4.61 0.39 0.09 11.06 0.95 0.56 7.11 1.32 0.73 0.09 0.08 0.09 0.03 0.02 0.02 0.03 0.02 0. 0.08 0.02 0.01 11.85 2.05 0.45 6.34 0.82 0.15 29.17 3.36 0.80 17.76 2.60 0.82 0.05 0.05 0. 0.02 0.03 0.01 0.03 0.02 0.02 0.05 0.02 0.01 Table 68: Statistical results for StrategyQA using Llama3.1-8B-Instruct on correct responses. reasoning tasks: AQuA (Figure 168, Table 76), GSM8K (Figure 169, Table 77), StrategyQA (Figure 170, Table 78), ECQA (Figure 171, Table 79), CREAK (Figure 172, Table 80), Sensemaking (Figure 173, Table 81). C.4.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 174 and Table 82. Figure 77: Visualization for MATH-Geometry using Llama-3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 19.05 2.64 0.43 10.39 1.40 0. 63.48 8.26 1.04 25.90 3.62 0.63 0.02 0.01 0.01 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0. 15.54 2.78 0.52 11.00 1.26 0.16 30.54 4.05 0.65 35.39 4.87 0.94 0.07 0.03 0.01 0.04 0.03 0. 0.03 0.02 0.02 0.04 0.02 0.01 10.80 1.42 0.29 3.13 0.65 0.09 8.67 1.24 0.36 7.55 1.26 0. 0.10 0.07 0.09 0.06 0.03 0.03 0.03 0.02 0.02 0.08 0.02 0.01 14.54 2.12 0.38 7.80 1.05 0. 30.72 4.07 0.65 21.70 3.00 0.62 0.06 0.04 0.04 0.04 0.03 0.02 0.03 0.02 0.02 0.05 0.02 0. Table 66: Statistical results for AQuA using Llama-3.18B-Instruct on correct responses. C.3.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 164 and Table 72. C."
        },
        {
            "title": "Instructed LLM on Irrelevant Responses",
            "content": "C.4.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 165, Table 73), MATH-Counting (Figure 166, Table 74), MATHGeometry (Figure 167, Table 75)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "68 Figure 78: Visualization for AQuA using Llama-3.1-8B-Instruct on correct responses. Figure 79: Visualization for GSM8K using Llama-3.1-8B-Instruct on correct responses. 69 Figure 80: Visualization for StrategyQA using Llama-3.1-8B-Instruct on correct responses. Figure 81: Visualization for ECQA using Llama-3.1-8B-Instruct on correct responses. 70 Figure 82: Visualization for CREAK using Llama-3.1-8B-Instruct on correct responses. Figure 83: Visualization for Sensemaking using Llama-3.1-8B-Instruct on correct responses. 71 Figure 84: Visualization for Wiki tasks using Llama-3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 16.08 3.02 0.67 7.86 1.47 0.28 48.82 7.47 1.40 20.81 3.46 0. 0.01 0.01 0.01 0.02 0.03 0.02 0.02 0.02 0.01 0.02 0.01 0.01 16.53 3.43 0.81 8.50 1.54 0. 32.86 4.70 1.00 34.23 5.65 1.40 0.05 0.04 0.01 0.02 0.03 0.01 0.02 0.02 0.01 0.04 0.02 0. 5.36 1.70 0.46 6.50 0.77 0.11 11.34 1.48 0.62 6.08 1.38 0.74 0.11 0.09 0.12 0.05 0.03 0. 0.02 0.02 0.02 0.07 0.03 0.02 11.91 2.55 0.60 7.53 1.21 0.20 28.05 4.19 0.96 19.08 3.24 0. 0.06 0.05 0.05 0.03 0.03 0.02 0.02 0.02 0.02 0.04 0.02 0.01 Table 69: Statistical results for ECQA using Llama-3.18B-Instruct on correct responses. sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.00 3.50 0.62 1.54 1.78 0. 8.05 8.79 1.29 4.33 4.32 0.82 0.01 0.01 0.01 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0. 3.45 3.96 0.76 1.18 1.69 0.22 4.41 5.79 0.88 6.16 7.21 1.33 0.06 0.03 0.01 0.03 0.04 0. 0.03 0.02 0.01 0.06 0.02 0.01 1.11 2.41 0.34 1.46 1.10 0.10 3.69 2.31 0.57 2.23 1.83 0. 0.12 0.09 0.10 0.06 0.04 0.02 0.04 0.02 0.02 0.09 0.03 0.02 2.35 3.14 0.53 1.43 1.48 0. 5.14 5.28 0.87 4.04 4.20 0.88 0.07 0.04 0.04 0.04 0.03 0.02 0.03 0.02 0.02 0.05 0.02 0. Table 71: Statistical results for Sensemaking using Llama-3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 21.99 3.10 0. 12.46 1.48 0.24 95.66 9.71 1.45 36.06 4.36 0.85 0.01 0.01 0.01 0.03 0.02 0.02 0.02 0.02 0. 0.02 0.02 0.01 25.06 4.30 0.82 15.76 1.73 0.23 49.82 4.98 0.97 50.58 6.26 1.35 0.04 0.04 0. 0.03 0.03 0.01 0.02 0.02 0.01 0.04 0.02 0.01 9.00 2.72 0.42 9.76 1.17 0.11 28.15 1.85 0. 14.81 1.66 0.75 0.10 0.10 0.10 0.06 0.03 0.02 0.03 0.02 0.02 0.08 0.03 0.01 17.79 3.21 0. 12.24 1.41 0.18 53.01 5.06 0.96 31.75 3.79 0.91 0.05 0.05 0.04 0.04 0.02 0.02 0.02 0.02 0. 0.05 0.02 0.01 Table 70: Statistical results for CREAK using Llama3.1-8B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 1.03 0.83 0.72 1. 0.47 0.37 0.33 0.60 4.98 3.01 2.38 12.54 2.44 1.80 1.52 4.38 0.02 0.02 0.02 0.03 0.02 0.02 0.02 0.03 0.03 0.03 0.02 0. 0.01 0.01 0.01 0.01 1.69 1.10 0.90 1.65 0.51 0.32 0.26 0.65 1.52 1.17 0.98 2.64 2.12 1.49 1.26 2.57 0.02 0.02 0.02 0. 0.02 0.02 0.01 0.03 0.02 0.01 0.01 0.02 0.03 0.02 0.01 0.03 1.25 0.59 0.38 0.99 0.29 0.15 0.10 0.34 0.81 0.64 0.55 0. 1.12 0.87 0.77 0.99 0.08 0.07 0.06 0.05 0.01 0.01 0.01 0.02 0.02 0.01 0.01 0.03 0.02 0.02 0.01 0.02 1.28 0.79 0.62 1. 0.43 0.28 0.23 0.55 2.23 1.46 1.19 4.80 1.82 1.31 1.11 2.50 0.04 0.04 0.03 0.03 0.02 0.02 0.02 0.03 0.02 0.02 0.02 0. 0.02 0.01 0.01 0.02 Table 72: Statistical results for Wiki using Llama-3.18B-Instruct on correct responses. 73 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.46 0.83 0.61 0.34 3.80 1.67 2.11 0.98 0.01 0.01 0.02 0. 0.02 0.02 0.01 0.01 1.62 0.85 0.58 0.29 2.13 1.10 2.45 1. 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.75 0.30 0.21 0. 0.80 0.45 0.90 0.53 0.09 0.07 0.03 0.02 0.02 0.01 0.02 0. 1.20 0.61 0.46 0.24 2.03 0.98 1.72 0.89 0.04 0.03 0.02 0. 0.02 0.02 0.01 0.01 Table 73: Statistical results for MATH-Algebra using Llama-3.1-8B-Instruct on irrelevant responses. Figure 85: Visualization for MATH-Algebra using Llama-3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.62 0.88 0.61 0.34 3.70 1.61 2.10 0. 0.01 0.01 0.02 0.02 0.02 0.02 0.01 0.01 1.72 0.85 0.63 0. 2.20 1.04 2.53 1.26 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0. 0.84 0.30 0.22 0.12 0.81 0.45 0.87 0.59 0.10 0.08 0.03 0. 0.02 0.01 0.01 0.01 1.32 0.62 0.47 0.24 2.03 0.94 1.74 0. 0.05 0.04 0.02 0.01 0.02 0.01 0.01 0.01 Table 74: Statistical results for MATH-Counting using Llama-3.1-8B-Instruct on irrelevant responses. Figure 86: Visualization for MATH-Counting using Llama-3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 1.48 0. 0.60 0.27 3.59 1.54 2.20 1.05 0.01 0.01 0.02 0.02 0.02 0. 0.01 0.01 1.52 0.68 0.47 0.19 1.88 0.79 2.19 1.09 0.02 0. 0.01 0.01 0.01 0.01 0.01 0.01 1.04 0.32 0.33 0.19 0.79 0. 0.94 0.64 0.09 0.05 0.03 0.03 0.02 0.02 0.01 0.01 1.26 0. 0.45 0.22 1.89 0.86 1.64 0.88 0.04 0.02 0.02 0.02 0.02 0. 0.01 0.01 Table 75: Statistical results for MATH-Geometry using Llama-3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 15.97 2.50 1.14 9.54 0.95 0.47 61.59 4.89 2.19 26.92 2.70 1.21 0.01 0.01 0.01 0.03 0.02 0. 0.02 0.02 0.02 0.02 0.01 0.01 18.81 2.44 1.14 11.64 1.11 0.45 36.45 3.93 1.62 42.07 4.31 1. 0.03 0.01 0.01 0.04 0.01 0.02 0.03 0.01 0.01 0.04 0.01 0.01 16.46 1.29 0.65 5.46 0.36 0. 13.56 0.94 0.61 11.22 1.13 0.79 0.08 0.10 0.11 0.05 0.03 0.03 0.03 0.01 0.01 0.08 0.02 0. 16.48 1.93 0.90 8.52 0.75 0.35 34.03 2.93 1.33 24.89 2.51 1.22 0.04 0.04 0.05 0.04 0.02 0. 0.03 0.02 0.02 0.05 0.01 0.01 Figure 87: Visualization for MATH-Geometry using Llama-3.1-8B-Instruct on irrelevant responses. Table 77: Statistical results for GSM8K using Llama3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 19.27 3.21 1.26 10.46 1.35 0.50 64.14 9.00 2.63 25.95 3.41 1.22 0.02 0.01 0.01 0.03 0.02 0. 0.02 0.02 0.02 0.02 0.01 0.01 15.45 3.18 1.17 10.93 1.33 0.45 30.36 4.50 1.69 35.16 5.21 1. 0.07 0.01 0.01 0.04 0.02 0.01 0.03 0.02 0.01 0.04 0.02 0.01 10.85 1.62 0.60 3.14 0.52 0. 8.73 1.13 0.57 7.56 1.29 0.73 0.10 0.09 0.09 0.06 0.03 0.03 0.03 0.02 0.01 0.08 0.02 0. 14.60 2.50 0.94 7.81 1.01 0.36 30.88 4.35 1.48 21.66 3.14 1.23 0.06 0.04 0.04 0.04 0.02 0. 0.03 0.02 0.02 0.05 0.01 0.01 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 14.08 1.97 0.93 7.59 0.81 0.34 61.77 8.18 2.21 24.75 3.66 1.36 0.01 0.01 0. 0.02 0.02 0.01 0.03 0.03 0.02 0.02 0.01 0.01 20.47 2.88 0.92 8.34 1.06 0.36 28.52 3.55 1. 28.78 3.90 1.40 0.04 0.02 0.01 0.02 0.01 0.01 0.03 0.02 0.01 0.04 0.02 0.01 5.98 1.63 0. 4.67 0.53 0.19 11.64 1.25 0.64 7.41 1.54 0.88 0.09 0.08 0.06 0.03 0.02 0.02 0.03 0.02 0. 0.08 0.02 0.01 12.84 2.06 0.69 6.68 0.77 0.28 30.85 3.91 1.25 18.74 2.86 1.16 0.05 0.04 0. 0.02 0.02 0.02 0.03 0.02 0.02 0.05 0.02 0.01 Table 76: Statistical results for AQuA using Llama-3.18B-Instruct on irrelevant responses. Table 78: Statistical results for StrategyQA using Llama3.1-8B-Instruct on irrelevant responses. Figure 88: Visualization for AQuA using Llama-3.1-8B-Instruct on irrelevant responses. Figure 89: Visualization for GSM8K using Llama-3.1-8B-Instruct on irrelevant responses. 76 Figure 90: Visualization for StrategyQA using Llama-3.1-8B-Instruct on irrelevant responses. Figure 91: Visualization for ECQA using Llama-3.1-8B-Instruct on irrelevant responses. Figure 92: Visualization for CREAK using Llama-3.1-8B-Instruct on irrelevant responses. Figure 93: Visualization for Sensemaking using Llama-3.1-8B-Instruct on irrelevant responses. 78 Figure 94: Visualization for Wiki tasks using Llama-3.1-8B-Instruct on irrelevant responses. 79 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 12.78 2.73 1.11 5.51 1.08 0.41 41.83 7.75 2.10 16.65 3.12 1. 0.01 0.01 0.01 0.02 0.02 0.01 0.02 0.02 0.01 0.02 0.01 0.01 12.97 3.31 1.09 6.16 1.33 0. 21.02 4.50 1.58 25.65 5.13 1.88 0.02 0.02 0.02 0.02 0.01 0.02 0.02 0.02 0.01 0.03 0.02 0. 8.86 2.05 0.55 4.31 0.83 0.29 9.15 1.56 0.76 6.44 1.64 0.94 0.07 0.10 0.07 0.04 0.03 0. 0.02 0.02 0.01 0.05 0.02 0.01 11.03 2.57 0.85 5.22 1.04 0.35 21.64 4.14 1.36 15.35 3.11 1. 0.04 0.05 0.04 0.03 0.02 0.02 0.02 0.02 0.01 0.03 0.02 0.01 Table 79: Statistical results for ECQA using Llama-3.18B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 8.17 3.69 1.04 3.67 1.55 0.45 19.95 9.01 1. 10.11 4.06 1.10 0.02 0.01 0.01 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.01 9.72 4.24 1. 4.53 1.94 0.41 13.96 6.34 1.52 14.51 7.06 1.84 0.06 0.01 0.01 0.02 0.01 0.02 0.03 0.02 0. 0.04 0.02 0.01 2.41 1.97 0.59 2.74 0.73 0.24 5.37 2.01 0.74 3.31 1.73 0.93 0.09 0.08 0. 0.04 0.03 0.02 0.03 0.02 0.01 0.07 0.03 0.02 6.35 3.13 0.84 3.56 1.34 0.35 12.18 5.26 1. 8.78 4.08 1.24 0.05 0.04 0.04 0.03 0.02 0.02 0.03 0.02 0.01 0.04 0.02 0.01 Table 81: Statistical results for Sensemaking using Llama-3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 23.94 3.05 1.01 12.65 1.21 0. 99.38 11.51 2.59 38.19 4.58 1.50 0.01 0.01 0.01 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0. 27.74 4.03 1.03 16.29 1.61 0.39 52.11 5.29 1.47 54.19 6.11 1.76 0.03 0.02 0.02 0.03 0.01 0. 0.02 0.02 0.01 0.04 0.02 0.01 9.32 2.41 0.59 10.48 1.04 0.32 28.59 1.54 0.66 15.59 1.63 0. 0.11 0.09 0.07 0.05 0.03 0.03 0.03 0.02 0.02 0.08 0.03 0.01 19.33 3.02 0.82 12.68 1.25 0. 54.88 5.46 1.42 33.78 3.84 1.31 0.05 0.04 0.03 0.04 0.02 0.02 0.02 0.02 0.02 0.05 0.02 0. Table 80: Statistical results for CREAK using Llama3.1-8B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 1.61 1.04 0.86 1.94 0.59 0.36 0.32 0. 7.75 4.06 3.21 16.50 3.52 2.10 1.74 6.20 0.02 0.02 0.03 0.03 0.01 0.02 0.03 0.02 0.03 0.03 0.03 0.05 0.01 0.01 0.01 0. 2.07 1.01 0.76 2.58 0.77 0.35 0.26 0.89 2.36 1.22 0.96 3.04 2.80 1.45 1.15 3.38 0.02 0.02 0.01 0.02 0.01 0.01 0.01 0. 0.02 0.01 0.01 0.02 0.02 0.01 0.01 0.02 1.37 0.53 0.36 1.72 0.41 0.18 0.13 0.65 0.96 0.67 0.58 0.92 1.25 0.92 0.82 1. 0.08 0.06 0.05 0.08 0.02 0.01 0.01 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.01 0.01 1.63 0.82 0.62 2.01 0.59 0.30 0.23 0. 3.37 1.82 1.45 6.14 2.43 1.45 1.21 3.51 0.04 0.03 0.03 0.04 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.03 0.02 0.01 0.01 0. Table 82: Statistical results for Wiki using Llama-3.18B-Instruct on irrelevant responses. 80 Results on Qwen2-1.5B Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All D.1 Pre-trained LLM on Correct Responses D.1.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 135, Table 83), MATH-Counting (Figure 136, Table 84), MATHGeometry (Figure 137, Table 85). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.32 0.19 0.32 0.17 1.61 0. 1.15 0.66 0.01 0.01 0.02 0.02 0.03 0.03 0.01 0.01 0.32 0. 0.34 0.21 1.15 0.54 1.24 0.66 0.01 0.01 0.03 0.03 0.02 0. 0.02 0.01 0.51 0.30 0.54 0.40 0.42 0.27 0.37 0.24 0.03 0. 0.01 0.02 0.04 0.03 0.07 0.06 0.38 0.25 0.39 0.25 1.02 0. 0.97 0.54 0.01 0.01 0.02 0.03 0.03 0.02 0.03 0.03 Table 83: Statistical results for MATH-Algebra using Qwen2-1.5B on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.37 0.23 0.36 0. 1.81 1.06 1.26 0.77 0.01 0.01 0.02 0.02 0.03 0.02 0.02 0. 0.34 0.29 0.30 0.22 1.18 0.56 1.25 0.73 0.01 0.01 0.03 0. 0.02 0.02 0.02 0.02 0.49 0.32 0.55 0.47 0.48 0.30 0.46 0. 0.02 0.02 0.02 0.02 0.04 0.03 0.07 0.06 0.41 0.30 0.38 0. 1.09 0.58 1.03 0.62 0.01 0.01 0.02 0.03 0.03 0.02 0.03 0. Table 84: Statistical results for MATH-Counting using Qwen2-1.5B on correct responses. Figure 96: Visualization for MATH-Counting using Qwen2-1.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.36 0.26 0.32 0.23 1.78 1.27 1.42 0.95 0.01 0.01 0.01 0. 0.03 0.02 0.01 0.01 0.40 0.32 0.30 0.23 1.06 0.60 1.35 0. 0.01 0.01 0.03 0.03 0.02 0.02 0.01 0.01 0.61 0.40 0.74 0. 0.53 0.34 0.40 0.30 0.02 0.03 0.01 0.02 0.03 0.03 0.05 0. 0.46 0.34 0.41 0.30 1.03 0.65 1.08 0.70 0.01 0.01 0.02 0. 0.02 0.02 0.02 0.02 Table 85: Statistical results for MATH-Geometry using Qwen2-1.5B on correct responses. Figure 95: Visualization for MATH-Algebra using Qwen2-1.5B on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 138, Table 86), GSM8K (Figure 139, Table 87), StrategyQA (Figure 140, Table 88), ECQA (Figure 141, Table 89), CREAK (Figure 142, Table 90), Sensemaking (Figure 143, Table 91). Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.39 0.29 0.22 4.47 0.34 0. 23.57 1.57 1.14 14.85 0.99 0.77 0.02 0.02 0.02 0.04 0.02 0.02 0.03 0.03 0.03 0.03 0.02 0. 2.78 0.27 0.23 5.53 0.38 0.25 14.68 1.12 0.69 14.41 1.12 0.79 0.04 0.01 0.01 0.03 0.03 0. 0.06 0.03 0.02 0.05 0.02 0.02 6.73 0.32 0.26 9.31 0.57 0.45 5.20 0.37 0.28 3.40 0.32 0. 0.13 0.02 0.03 0.04 0.03 0.02 0.04 0.03 0.03 0.10 0.08 0.06 3.95 0.32 0.25 6.02 0.42 0. 13.66 0.99 0.67 11.45 0.88 0.64 0.06 0.02 0.01 0.04 0.03 0.03 0.05 0.03 0.03 0.06 0.04 0. Table 87: Statistical results for GSM8K using Qwen21.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.03 0.58 0. 5.11 0.73 0.23 28.52 4.51 1.57 20.65 3.06 1.00 0.04 0.01 0.01 0.05 0.03 0.02 0.06 0.04 0. 0.04 0.01 0.01 1.74 0.32 0.33 3.58 0.39 0.22 10.02 1.53 0.63 6.22 1.46 0.90 0.07 0.01 0. 0.04 0.03 0.03 0.10 0.06 0.02 0.07 0.04 0.02 0.90 0.40 0.27 1.88 0.45 0.49 3.49 1.05 0. 4.33 0.94 0.52 0.07 0.02 0.01 0.09 0.03 0.01 0.08 0.10 0.03 0.05 0.08 0.05 1.87 0.41 0. 3.57 0.48 0.28 12.07 2.04 0.77 8.83 1.65 0.81 0.06 0.01 0.01 0.05 0.03 0.02 0.08 0.06 0. 0.05 0.04 0.02 Table 88: Statistical results for StrategyQA using Qwen2-1.5B on correct responses. reasoning tasks: AQuA (Figure 148, Table 96), GSM8K (Figure 149, Table 97), StrategyQA (Figure 150, Table 98), ECQA (Figure 151, Table 99), CREAK (Figure 152, Table 100), Sensemaking (Figure 153, Table 101). D.2.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 154 and Table 102. Figure 97: Visualization for MATH-Geometry using Qwen2-1.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.76 0.89 0.23 7.20 1.01 0.22 37.29 5.08 1.15 23.79 3.31 0. 0.03 0.02 0.01 0.04 0.03 0.02 0.04 0.04 0.03 0.02 0.02 0.01 4.13 0.52 0.28 6.29 0.56 0. 16.12 2.14 0.62 14.35 2.18 0.75 0.06 0.01 0.01 0.04 0.03 0.03 0.06 0.03 0.02 0.04 0.02 0. 3.49 0.77 0.29 8.40 1.11 0.42 3.94 0.86 0.33 3.04 0.63 0.29 0.21 0.02 0.02 0.14 0.02 0. 0.03 0.03 0.03 0.09 0.08 0.06 4.42 0.69 0.28 7.06 0.81 0.27 17.32 2.36 0.64 12.91 1.97 0. 0.09 0.02 0.01 0.06 0.03 0.02 0.05 0.03 0.02 0.05 0.04 0.03 Table 86: Statistical results for AQuA using Qwen21.5B on correct responses. D.1.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 144 and Table 92. D.2 Pre-trained LLM on Irrelevant Responses D.2.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 145, Table 93), MATH-Counting (Figure 146, Table 94), MATHGeometry (Figure 147, Table 95)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "82 Figure 98: Visualization for AQuA using Qwen2-1.5B on correct responses. Figure 99: Visualization for GSM8K using Qwen2-1.5B on correct responses. 83 Figure 100: Visualization for StrategyQA using Qwen2-1.5B on correct responses. Figure 101: Visualization for ECQA using Qwen2-1.5B on correct responses. 84 Figure 102: Visualization for CREAK using Qwen2-1.5B on correct responses. Figure 103: Visualization for Sensemaking using Qwen2-1.5B on correct responses. 85 Figure 104: Visualization for Wiki tasks using Qwen2-1.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 8.00 1.11 0.30 11.51 1.34 0.26 59.33 8.53 1. 39.20 5.56 1.00 0.02 0.02 0.01 0.06 0.03 0.02 0.04 0.03 0.02 0.02 0.02 0.01 7.01 0.70 0. 11.07 1.24 0.29 24.83 3.55 0.74 19.50 3.33 0.97 0.07 0.02 0.01 0.04 0.04 0.03 0.07 0.05 0. 0.05 0.03 0.02 5.01 0.86 0.26 13.32 1.01 0.54 7.46 1.66 0.48 5.12 1.41 0.52 0.14 0.05 0. 0.06 0.04 0.02 0.05 0.05 0.03 0.11 0.07 0.05 6.53 0.85 0.35 11.11 1.13 0.34 27.40 4.01 0. 19.38 3.22 0.85 0.08 0.03 0.01 0.05 0.04 0.02 0.05 0.04 0.02 0.06 0.04 0.02 Table 89: Statistical results for ECQA using Qwen21.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.76 2.22 0.31 6.84 2.90 0. 38.73 15.87 1.52 25.32 10.40 1.06 0.04 0.02 0.01 0.03 0.03 0.02 0.04 0.04 0.03 0.03 0.01 0. 3.42 1.50 0.35 4.71 2.06 0.35 13.42 7.58 0.75 12.11 6.41 0.88 0.06 0.02 0.01 0.07 0.05 0. 0.06 0.05 0.02 0.05 0.03 0.02 5.92 2.73 0.38 11.16 2.50 0.71 5.15 3.19 0.40 2.65 2.45 0. 0.20 0.10 0.02 0.12 0.02 0.03 0.07 0.08 0.03 0.11 0.08 0.05 4.28 1.98 0.37 6.59 2.27 0. 16.68 7.95 0.80 12.44 6.16 0.80 0.09 0.04 0.01 0.07 0.04 0.03 0.05 0.05 0.02 0.06 0.04 0. Table 91: Statistical results for Sensemaking using Qwen2-1.5B on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 13.91 1.77 0.31 20.31 2.37 0.28 111.18 14.29 1. 71.70 9.03 1.14 0.02 0.02 0.01 0.04 0.04 0.02 0.04 0.03 0.02 0.02 0.02 0.01 9.73 1.26 0. 15.99 1.89 0.26 40.86 6.89 0.81 36.08 6.26 1.07 0.06 0.03 0.01 0.05 0.04 0.03 0.07 0.06 0. 0.06 0.03 0.02 8.08 2.74 0.25 17.98 2.54 0.51 12.03 3.59 0.54 4.27 2.93 0.55 0.12 0.10 0. 0.05 0.03 0.01 0.05 0.09 0.04 0.08 0.08 0.05 9.73 1.78 0.33 16.92 2.08 0.32 47.98 7.41 0. 35.01 5.88 0.93 0.07 0.04 0.01 0.05 0.04 0.03 0.06 0.06 0.03 0.05 0.04 0.02 Table 90: Statistical results for CREAK using Qwen21.5B on correct responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.40 0.31 0.28 0. 0.47 0.29 0.23 0.76 3.26 2.10 1.67 5.49 2.21 1.48 1.24 3.58 0.01 0.01 0.01 0.03 0.03 0.02 0.02 0.04 0.03 0.02 0.02 0. 0.02 0.01 0.01 0.02 0.28 0.26 0.25 0.44 0.20 0.11 0.10 0.48 1.30 0.87 0.65 1.90 1.28 1.06 0.93 1.74 0.01 0.01 0.01 0. 0.03 0.02 0.02 0.02 0.05 0.03 0.03 0.08 0.03 0.02 0.02 0.05 0.30 0.21 0.18 0.19 0.33 0.29 0.31 0.41 0.89 0.67 0.52 0. 0.73 0.61 0.52 0.73 0.03 0.01 0.01 0.06 0.03 0.02 0.01 0.03 0.09 0.06 0.04 0.07 0.07 0.05 0.05 0.08 0.32 0.27 0.25 0. 0.31 0.19 0.17 0.52 1.63 1.08 0.84 2.41 1.34 1.01 0.87 1.86 0.02 0.01 0.01 0.03 0.03 0.02 0.02 0.03 0.06 0.03 0.03 0. 0.04 0.03 0.02 0.05 Table 92: Statistical results for Wiki using Qwen2-1.5B on correct responses. 87 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.65 0.39 0.63 0.36 3.16 1.66 2.26 1.24 0.01 0.01 0.01 0. 0.03 0.03 0.01 0.01 0.67 0.39 0.53 0.25 2.22 0.98 2.45 1. 0.02 0.02 0.03 0.03 0.04 0.03 0.02 0.01 1.16 0.48 1.16 0. 0.94 0.49 0.68 0.37 0.05 0.03 0.02 0.03 0.04 0.04 0.06 0. 0.80 0.42 0.67 0.34 2.02 0.96 1.87 0.96 0.02 0.02 0.03 0. 0.03 0.03 0.03 0.02 Table 93: Statistical results for MATH-Algebra using Qwen2-1.5B on irrelevant responses. Figure 105: Visualization for MATH-Algebra using Qwen2-1.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.67 0.46 0.64 0.41 3.20 1.94 2.36 1. 0.01 0.01 0.01 0.02 0.03 0.03 0.02 0.01 0.66 0.41 0.42 0. 2.08 0.93 2.25 1.21 0.02 0.02 0.03 0.03 0.04 0.03 0.02 0. 0.89 0.38 0.92 0.50 0.98 0.52 0.77 0.45 0.04 0.03 0.02 0. 0.04 0.03 0.06 0.05 0.73 0.42 0.56 0.34 1.98 1.02 1.81 1. 0.02 0.02 0.02 0.03 0.04 0.03 0.03 0.02 Table 94: Statistical results for MATH-Counting using Qwen2-1.5B on irrelevant responses. Figure 106: Visualization for MATH-Counting using Qwen2-1.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.64 0. 0.58 0.46 3.22 2.43 2.53 1.80 0.01 0.01 0.01 0.02 0.02 0. 0.01 0.01 0.64 0.32 0.45 0.18 1.91 1.00 2.40 1.40 0.01 0. 0.03 0.03 0.03 0.03 0.01 0.01 1.28 0.57 1.32 0.70 0.97 0. 0.65 0.47 0.04 0.02 0.02 0.03 0.03 0.04 0.05 0.04 0.80 0. 0.65 0.37 1.88 1.18 1.87 1.19 0.02 0.02 0.02 0.03 0.03 0. 0.02 0.02 Table 95: Statistical results for MATH-Geometry using Qwen2-1.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.34 0.85 0.47 4.41 0.91 0.43 23.08 4.56 2.31 14.64 3.12 1.54 0.02 0.02 0.01 0.04 0.03 0. 0.03 0.03 0.02 0.03 0.02 0.01 2.96 0.62 0.37 5.64 0.84 0.30 15.01 3.05 1.26 14.57 3.09 1. 0.04 0.02 0.02 0.04 0.03 0.04 0.06 0.05 0.04 0.05 0.02 0.01 7.10 1.12 0.40 9.89 1.29 0. 5.51 1.30 0.64 3.60 0.96 0.47 0.13 0.04 0.04 0.04 0.02 0.02 0.04 0.05 0.03 0.09 0.07 0. 4.14 0.85 0.42 6.21 0.94 0.38 13.78 2.87 1.30 11.53 2.47 1.16 0.05 0.02 0.02 0.04 0.03 0. 0.05 0.04 0.03 0.05 0.03 0.02 Figure 107: Visualization for MATH-Geometry using Qwen2-1.5B on irrelevant responses. Table 97: Statistical results for GSM8K using Qwen21.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.79 1.09 0.53 7.28 1.12 0.50 37.62 6.56 2.53 23.99 4.42 1.81 0.03 0.02 0.01 0.04 0.03 0. 0.04 0.03 0.03 0.02 0.02 0.01 4.22 0.67 0.40 6.31 0.69 0.28 16.30 3.14 1.26 14.51 3.22 1. 0.06 0.02 0.01 0.04 0.03 0.03 0.06 0.05 0.04 0.04 0.02 0.01 3.47 1.33 0.43 8.41 1.58 0. 4.00 1.47 0.73 3.06 1.07 0.54 0.21 0.03 0.03 0.14 0.02 0.02 0.03 0.05 0.04 0.09 0.08 0. 4.46 0.95 0.45 7.10 0.97 0.39 17.49 3.36 1.35 13.04 2.83 1.25 0.09 0.02 0.02 0.06 0.03 0. 0.05 0.04 0.03 0.05 0.04 0.02 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.15 0.81 0.69 5.22 1.04 0.55 28.90 6.18 3.17 21.00 4.29 2.29 0.04 0.01 0. 0.05 0.04 0.02 0.06 0.04 0.03 0.04 0.02 0.01 1.80 0.49 0.35 3.66 0.52 0.30 10.12 1.80 0. 6.26 1.92 1.41 0.07 0.02 0.02 0.04 0.03 0.03 0.10 0.08 0.04 0.07 0.03 0.01 0.89 0.42 0. 1.87 0.44 0.40 3.51 1.39 0.80 4.35 1.19 0.75 0.06 0.02 0.01 0.09 0.02 0.02 0.08 0.13 0. 0.05 0.08 0.05 1.93 0.56 0.40 3.64 0.66 0.37 12.21 2.67 1.40 8.94 2.24 1.40 0.06 0.02 0. 0.05 0.03 0.03 0.08 0.08 0.04 0.05 0.04 0.02 Table 96: Statistical results for AQuA using Qwen21.5B on irrelevant responses. Table 98: Statistical results for StrategyQA using Qwen2-1.5B on irrelevant responses. Figure 108: Visualization for AQuA using Qwen2-1.5B on irrelevant responses. Figure 109: Visualization for GSM8K using Qwen2-1.5B on irrelevant responses. 90 Figure 110: Visualization for StrategyQA using Qwen2-1.5B on irrelevant responses. Figure 111: Visualization for ECQA using Qwen2-1.5B on irrelevant responses. Figure 112: Visualization for CREAK using Qwen2-1.5B on irrelevant responses. Figure 113: Visualization for Sensemaking using Qwen2-1.5B on irrelevant responses. 92 Figure 114: Visualization for Wiki tasks using Qwen2-1.5B on irrelevant responses. 93 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.61 1.34 0.64 8.65 1.58 0.56 45.35 10.01 3.07 29.13 6.62 2. 0.02 0.01 0.01 0.04 0.03 0.02 0.04 0.03 0.02 0.02 0.02 0.02 4.74 0.90 0.42 6.92 1.25 0. 16.32 4.28 1.13 14.21 4.35 1.51 0.04 0.01 0.02 0.04 0.04 0.03 0.07 0.07 0.03 0.05 0.03 0. 3.44 1.70 0.29 6.53 1.92 0.56 6.82 2.30 0.79 5.63 2.41 0.80 0.07 0.12 0.02 0.05 0.04 0. 0.04 0.08 0.04 0.07 0.07 0.04 4.48 1.20 0.45 6.98 1.41 0.43 20.02 4.90 1.44 14.91 4.21 1. 0.04 0.04 0.02 0.04 0.04 0.03 0.05 0.06 0.03 0.05 0.04 0.02 Table 99: Statistical results for ECQA using Qwen21.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.14 1.86 0.67 7.37 2.35 0.55 39.58 13.80 2. 26.23 9.13 2.14 0.03 0.02 0.01 0.03 0.03 0.02 0.04 0.03 0.02 0.03 0.01 0.01 3.36 1.29 0. 4.08 1.62 0.50 14.36 5.90 1.12 12.84 5.58 1.48 0.05 0.02 0.02 0.06 0.03 0.05 0.05 0.06 0. 0.04 0.03 0.02 5.88 2.41 0.41 10.96 2.64 0.78 5.81 2.85 0.68 2.81 2.46 0.60 0.19 0.10 0. 0.11 0.07 0.02 0.06 0.08 0.03 0.10 0.08 0.04 4.39 1.73 0.57 6.43 1.93 0.54 17.47 6.67 1. 13.07 5.46 1.36 0.07 0.04 0.02 0.06 0.04 0.04 0.05 0.06 0.03 0.05 0.04 0.02 Table 101: Statistical results for Sensemaking using Qwen2-1.5B on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 13.97 1.97 0.70 20.17 2.36 0.61 110.47 15.38 3.76 70.90 9.95 2. 0.02 0.01 0.01 0.04 0.04 0.02 0.04 0.03 0.02 0.02 0.02 0.02 9.47 1.63 0.37 15.47 2.12 0. 41.19 6.33 1.38 36.52 6.47 1.76 0.06 0.01 0.01 0.05 0.04 0.03 0.07 0.08 0.04 0.06 0.05 0. 8.58 4.16 0.20 18.95 4.41 0.50 12.41 4.18 0.91 4.43 3.95 0.82 0.12 0.11 0.03 0.05 0.05 0. 0.05 0.12 0.05 0.08 0.09 0.05 9.75 2.30 0.42 16.82 2.56 0.42 47.98 7.61 1.75 35.11 6.43 1. 0.07 0.03 0.02 0.05 0.04 0.03 0.06 0.07 0.04 0.05 0.05 0.02 Table 100: Statistical results for CREAK using Qwen21.5B on irrelevant responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.65 0.56 0.49 0.63 0.76 0.48 0.38 0. 4.83 3.35 2.75 6.99 3.36 2.49 2.16 4.71 0.02 0.02 0.01 0.01 0.03 0.03 0.02 0.04 0.03 0.02 0.02 0.04 0.02 0.02 0.01 0. 0.34 0.26 0.23 0.46 0.32 0.19 0.14 0.45 1.52 1.06 0.83 1.80 1.57 1.36 1.22 1.71 0.02 0.01 0.01 0.02 0.03 0.02 0.02 0. 0.07 0.05 0.05 0.08 0.03 0.01 0.01 0.04 0.33 0.27 0.25 0.30 0.36 0.21 0.17 0.47 1.18 0.94 0.79 1.14 0.84 0.77 0.70 0. 0.03 0.01 0.01 0.03 0.02 0.01 0.01 0.02 0.11 0.09 0.08 0.11 0.07 0.06 0.05 0.08 0.43 0.34 0.29 0.45 0.46 0.28 0.21 0. 2.17 1.54 1.25 2.78 1.78 1.44 1.28 2.18 0.02 0.02 0.01 0.02 0.03 0.02 0.02 0.03 0.07 0.05 0.04 0.07 0.03 0.02 0.02 0. Table 102: Statistical results for Wiki using Qwen21.5B on irrelevant responses. 94 D."
        },
        {
            "title": "Instructed LLM on Correct Responses",
            "content": "Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All D.3.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 155, Table 103), MATH-Counting (Figure 156, Table 104), MATHGeometry (Figure 157, Table 105). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.40 0. 0.40 0.17 1.93 0.88 1.40 0.66 0.02 0.01 0.03 0.02 0.03 0. 0.02 0.01 0.38 0.27 0.35 0.20 1.25 0.54 1.34 0.67 0.01 0. 0.03 0.03 0.03 0.02 0.02 0.01 0.52 0.27 0.59 0.39 0.48 0. 0.43 0.28 0.03 0.03 0.02 0.02 0.04 0.03 0.07 0.07 0.43 0. 0.42 0.24 1.16 0.53 1.09 0.55 0.02 0.01 0.03 0.03 0.03 0. 0.03 0.03 Table 103: Statistical results for MATH-Algebra using Qwen2-1.5B-Instruct on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.44 0.22 0.42 0.20 2.05 1.07 1.49 0.79 0.02 0.01 0.02 0. 0.03 0.02 0.02 0.01 0.40 0.32 0.30 0.21 1.28 0.57 1.36 0. 0.01 0.01 0.03 0.03 0.03 0.02 0.02 0.02 0.51 0.30 0.59 0. 0.55 0.35 0.52 0.37 0.02 0.02 0.02 0.01 0.04 0.03 0.07 0. 0.46 0.31 0.40 0.27 1.21 0.60 1.14 0.64 0.01 0.01 0.02 0. 0.03 0.02 0.03 0.02 Table 104: Statistical results for MATH-Counting using Qwen2-1.5B-Instruct on correct responses. Figure 116: Visualization for MATH-Counting using Qwen2-1.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.40 0.26 0.35 0.23 1.89 1.28 1.52 0. 0.02 0.01 0.01 0.02 0.03 0.02 0.01 0.01 0.44 0.36 0.28 0. 1.07 0.62 1.35 0.86 0.01 0.01 0.03 0.03 0.02 0.02 0.01 0. 0.57 0.40 0.72 0.51 0.54 0.39 0.42 0.36 0.01 0.03 0.01 0. 0.03 0.03 0.06 0.06 0.47 0.36 0.40 0.30 1.06 0.68 1.11 0. 0.01 0.01 0.02 0.03 0.02 0.02 0.02 0.02 Table 105: Statistical results for MATH-Geometry using Qwen2-1.5B-Instruct on correct responses. Figure 115: Visualization for MATH-Algebra using Qwen2-1.5B-Instruct on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 158, Table 106), GSM8K (Figure 159, Table 107), StrategyQA (Figure 160, Table 108), ECQA (Figure 161, Table 109), CREAK (Figure 162, Table 110), Sensemaking (Figure 163, Table 111). 95 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.44 0.68 0.21 10.73 0.81 0.22 39.35 3.20 1.05 27.77 2.25 0.73 0.03 0.02 0.01 0.03 0.02 0. 0.03 0.02 0.02 0.02 0.01 0.01 4.82 0.55 0.26 8.05 0.73 0.22 23.28 2.42 0.65 22.56 2.44 0. 0.03 0.01 0.01 0.04 0.04 0.03 0.05 0.03 0.02 0.04 0.02 0.02 10.79 1.04 0.20 12.32 0.98 0. 7.91 1.23 0.33 4.89 0.70 0.30 0.12 0.04 0.01 0.04 0.05 0.02 0.05 0.04 0.03 0.08 0.08 0. 6.77 0.74 0.25 9.89 0.83 0.28 22.41 2.19 0.64 18.65 1.92 0.62 0.05 0.02 0.01 0.04 0.04 0. 0.04 0.03 0.02 0.05 0.03 0.03 Table 107: Statistical results for GSM8K using Qwen21.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.76 0.74 0.28 8.61 1.18 0.27 31.00 5.58 1.59 22.21 3.85 1.03 0.05 0.02 0. 0.06 0.05 0.03 0.04 0.03 0.02 0.03 0.01 0.01 2.78 0.51 0.35 6.27 0.70 0.20 16.26 2.28 0. 9.87 2.11 0.97 0.06 0.01 0.01 0.04 0.03 0.03 0.08 0.06 0.03 0.06 0.04 0.02 0.71 0.32 0. 2.09 0.47 0.42 3.75 1.44 0.56 3.95 1.15 0.60 0.09 0.02 0.01 0.06 0.03 0.01 0.07 0.08 0. 0.05 0.08 0.05 2.57 0.56 0.32 5.67 0.74 0.27 15.97 2.71 0.82 10.76 2.18 0.86 0.07 0.02 0. 0.05 0.04 0.02 0.06 0.05 0.03 0.05 0.04 0.02 Table 108: Statistical results for StrategyQA using Qwen2-1.5B-Instruct on correct responses. reasoning tasks: AQuA (Figure 168, Table 116), GSM8K (Figure 169, Table 117), StrategyQA (Figure 170, Table 118), ECQA (Figure 171, Table 119), CREAK (Figure 172, Table 120), Sensemaking (Figure 173, Table 121). D.4.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 174 and Table 122. Figure 117: Visualization for MATH-Geometry using Qwen2-1.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 12.15 0.82 0.21 15.61 1.09 0. 64.92 4.49 1.04 44.30 3.22 0.76 0.04 0.04 0.01 0.05 0.03 0.02 0.04 0.03 0.03 0.03 0.02 0. 6.73 0.72 0.30 11.96 0.50 0.20 30.95 2.32 0.56 24.03 2.45 0.69 0.08 0.01 0.01 0.05 0.04 0. 0.05 0.03 0.02 0.06 0.03 0.02 3.11 1.04 0.23 6.78 1.46 0.39 4.82 0.98 0.35 3.70 0.71 0. 0.18 0.02 0.01 0.13 0.01 0.01 0.09 0.03 0.03 0.11 0.08 0.06 7.40 0.84 0.28 12.92 0.90 0. 31.89 2.39 0.59 22.69 2.10 0.60 0.10 0.02 0.01 0.07 0.03 0.02 0.05 0.03 0.02 0.06 0.04 0. Table 106: Statistical results for AQuA using Qwen21.5B-Instruct on correct responses. D.3.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 164 and Table ??. D."
        },
        {
            "title": "Instructed LLM on Irrelevant Responses",
            "content": "D.4.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 165, Table 113), MATH-Counting (Figure 166, Table 114), MATHGeometry (Figure 167, Table 115)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "96 Figure 118: Visualization for AQuA using Qwen2-1.5B-Instruct on correct responses. Figure 119: Visualization for GSM8K using Qwen2-1.5B-Instruct on correct responses. 97 Figure 120: Visualization for StrategyQA using Qwen2-1.5B-Instruct on correct responses. Figure 121: Visualization for ECQA using Qwen2-1.5B-Instruct on correct responses. 98 Figure 122: Visualization for CREAK using Qwen2-1.5B-Instruct on correct responses. Figure 123: Visualization for Sensemaking using Qwen2-1.5B-Instruct on correct responses. 99 Figure 124: Visualization for Wiki tasks using Qwen2-1.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.09 1.34 0.33 9.03 1.75 0.30 42.20 8.69 1. 30.30 6.03 1.09 0.02 0.02 0.01 0.04 0.03 0.02 0.04 0.03 0.03 0.02 0.02 0.01 4.87 0.85 0. 6.21 1.19 0.27 21.30 3.82 0.81 19.50 3.86 1.07 0.07 0.01 0.01 0.06 0.05 0.03 0.04 0.04 0. 0.07 0.03 0.02 3.16 1.01 0.25 12.09 1.75 0.51 4.66 1.97 0.58 3.53 1.23 0.60 0.13 0.04 0. 0.08 0.02 0.01 0.07 0.03 0.03 0.12 0.06 0.05 5.15 1.07 0.38 7.84 1.42 0.33 21.30 4.27 0. 17.11 3.59 0.94 0.07 0.02 0.01 0.06 0.04 0.02 0.05 0.03 0.02 0.07 0.04 0.03 Table 109: Statistical results for ECQA using Qwen21.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 0.70 2.08 0.31 0.91 2.65 0. 3.89 12.02 1.54 2.88 8.04 1.08 0.03 0.02 0.01 0.03 0.03 0.02 0.03 0.03 0.03 0.02 0.02 0. 0.72 1.35 0.39 0.51 2.32 0.31 1.78 6.20 0.77 1.62 5.68 0.91 0.05 0.02 0.01 0.05 0.05 0. 0.04 0.04 0.02 0.06 0.03 0.02 0.40 2.43 0.34 0.83 2.27 0.60 0.32 3.13 0.48 0.38 1.98 0. 0.18 0.08 0.01 0.13 0.03 0.02 0.05 0.04 0.03 0.10 0.07 0.05 0.66 1.86 0.38 0.71 2.38 0. 1.82 6.45 0.83 1.56 5.11 0.83 0.08 0.03 0.01 0.06 0.04 0.03 0.04 0.04 0.02 0.06 0.04 0. Table 111: Statistical results for Sensemaking using Qwen2-1.5B-Instruct on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 9.33 1.79 0.32 12.71 2.27 0.31 64.10 11.90 1. 42.64 8.02 1.17 0.03 0.02 0.01 0.03 0.03 0.03 0.04 0.03 0.02 0.02 0.02 0.01 8.89 1.14 0. 19.58 1.97 0.25 28.13 5.69 0.85 26.70 5.24 1.09 0.08 0.02 0.01 0.07 0.04 0.02 0.06 0.05 0. 0.06 0.03 0.02 5.25 1.94 0.22 15.92 2.13 0.47 7.38 2.98 0.60 3.97 1.66 0.59 0.15 0.06 0. 0.08 0.03 0.01 0.08 0.05 0.04 0.10 0.07 0.05 7.94 1.57 0.36 15.92 2.07 0.31 30.21 6.11 0. 23.80 4.80 0.96 0.08 0.03 0.01 0.06 0.03 0.02 0.06 0.04 0.03 0.06 0.04 0.02 Table 110: Statistical results for CREAK using Qwen21.5B-Instruct on correct responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.54 0.37 0.32 0. 0.66 0.36 0.27 0.74 3.94 2.45 1.91 6.00 2.68 1.76 1.44 4.09 0.02 0.01 0.01 0.02 0.03 0.02 0.02 0.05 0.03 0.02 0.02 0. 0.02 0.01 0.01 0.01 0.37 0.33 0.31 0.47 0.28 0.13 0.12 0.60 1.70 1.01 0.74 1.94 1.66 1.24 1.04 1.76 0.01 0.01 0.01 0. 0.02 0.02 0.02 0.03 0.06 0.03 0.03 0.06 0.03 0.02 0.01 0.04 0.34 0.24 0.22 0.24 0.37 0.29 0.32 0.34 1.13 0.78 0.59 1. 0.86 0.68 0.57 0.80 0.03 0.01 0.01 0.03 0.02 0.02 0.02 0.04 0.09 0.05 0.04 0.07 0.07 0.05 0.05 0.09 0.41 0.32 0.30 0. 0.40 0.22 0.19 0.54 2.03 1.26 0.95 2.57 1.66 1.18 0.98 1.98 0.02 0.01 0.01 0.02 0.03 0.02 0.02 0.04 0.06 0.03 0.03 0. 0.04 0.03 0.02 0.05 Table 112: Statistical results for Wiki using Qwen21.5B-Instruct on correct responses. 101 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.71 0.42 0.71 0.40 3.35 1.74 2.48 1.34 0.01 0.01 0.02 0. 0.03 0.03 0.01 0.01 0.78 0.51 0.52 0.30 2.29 1.02 2.53 1. 0.02 0.02 0.03 0.04 0.04 0.03 0.02 0.01 1.27 0.59 1.27 0. 1.01 0.54 0.72 0.42 0.05 0.04 0.03 0.03 0.04 0.03 0.06 0. 0.90 0.51 0.71 0.40 2.10 1.01 1.97 1.02 0.03 0.02 0.03 0. 0.03 0.03 0.03 0.02 Table 113: Statistical results for MATH-Algebra using Qwen2-1.5B-Instruct on irrelevant responses. Figure 125: Visualization for MATH-Algebra using Qwen2-1.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.73 0.47 0.72 0.43 3.50 1.96 2.66 1. 0.01 0.01 0.02 0.02 0.02 0.03 0.01 0.01 0.79 0.52 0.43 0. 2.25 0.96 2.43 1.24 0.02 0.02 0.03 0.04 0.04 0.03 0.02 0. 1.05 0.47 1.06 0.55 1.08 0.58 0.84 0.50 0.05 0.04 0.02 0. 0.04 0.03 0.06 0.05 0.85 0.50 0.61 0.38 2.14 1.04 1.98 1. 0.02 0.02 0.03 0.03 0.04 0.03 0.03 0.02 Table 114: Statistical results for MATH-Counting using Qwen2-1.5B-Instruct on irrelevant responses. Figure 126: Visualization for MATH-Counting using Qwen2-1.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.62 0. 0.58 0.49 3.09 2.41 2.43 1.89 0.01 0.01 0.01 0.02 0.02 0. 0.01 0.01 0.69 0.48 0.41 0.25 1.82 1.00 2.26 1.48 0.01 0. 0.03 0.03 0.03 0.03 0.01 0.01 1.20 0.66 1.26 0.85 0.94 0. 0.64 0.50 0.04 0.02 0.02 0.05 0.03 0.03 0.05 0.05 0.80 0. 0.63 0.45 1.81 1.19 1.79 1.26 0.02 0.02 0.03 0.03 0.03 0. 0.02 0.02 Table 115: Statistical results for MATH-Geometry using Qwen2-1.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.27 1.29 0.52 10.22 1.44 0.51 39.15 6.56 2.38 27.74 4.80 1.65 0.03 0.01 0.01 0.03 0.03 0. 0.03 0.02 0.02 0.02 0.02 0.01 4.88 1.03 0.49 8.58 1.26 0.36 24.33 4.21 1.30 23.41 4.48 1. 0.03 0.01 0.02 0.04 0.04 0.05 0.05 0.05 0.03 0.04 0.02 0.01 12.46 2.17 0.47 12.97 2.16 0. 8.64 1.80 0.72 5.31 1.20 0.55 0.11 0.07 0.04 0.04 0.01 0.02 0.05 0.05 0.03 0.07 0.07 0. 7.22 1.42 0.51 10.05 1.48 0.44 22.97 3.98 1.35 19.20 3.61 1.24 0.05 0.03 0.02 0.03 0.03 0. 0.04 0.04 0.03 0.04 0.03 0.02 Figure 127: Visualization for MATH-Geometry using Qwen2-1.5B-Instruct on irrelevant responses. Table 117: Statistical results for GSM8K using Qwen21.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 12.33 1.13 0.54 15.89 1.37 0.54 66.05 6.27 2.42 45.04 4.58 1.77 0.04 0.02 0.01 0.05 0.03 0. 0.04 0.03 0.03 0.03 0.01 0.01 6.91 1.01 0.53 12.20 0.72 0.34 31.50 3.21 1.21 24.42 3.30 1. 0.08 0.01 0.02 0.05 0.04 0.04 0.04 0.05 0.04 0.06 0.02 0.01 3.27 1.55 0.47 7.05 1.82 0. 4.98 1.60 0.76 3.74 1.09 0.58 0.18 0.04 0.03 0.13 0.03 0.03 0.09 0.05 0.04 0.11 0.08 0. 7.57 1.19 0.53 13.20 1.10 0.43 32.48 3.35 1.32 23.06 2.90 1.24 0.10 0.02 0.02 0.07 0.03 0. 0.05 0.04 0.03 0.06 0.03 0.02 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.80 0.97 0.66 8.60 1.39 0.52 31.41 7.36 3.06 22.50 5.21 2.24 0.05 0.01 0. 0.06 0.04 0.02 0.04 0.03 0.02 0.03 0.01 0.01 2.86 0.70 0.46 6.48 0.79 0.35 16.63 2.47 1. 10.10 2.66 1.56 0.06 0.02 0.02 0.04 0.03 0.03 0.08 0.07 0.04 0.06 0.03 0.01 0.75 0.42 0. 2.08 0.73 0.42 3.81 1.64 0.88 3.98 1.21 0.79 0.09 0.02 0.01 0.06 0.02 0.02 0.07 0.10 0. 0.05 0.07 0.05 2.64 0.72 0.47 5.77 0.94 0.39 16.24 3.31 1.43 10.96 2.82 1.47 0.07 0.02 0. 0.05 0.03 0.03 0.06 0.06 0.03 0.05 0.03 0.02 Table 116: Statistical results for AQuA using Qwen21.5B-Instruct on irrelevant responses. Table 118: Statistical results for StrategyQA using Qwen2-1.5B-Instruct on irrelevant responses. Figure 128: Visualization for AQuA using Qwen2-1.5B-Instruct on irrelevant responses. Figure 129: Visualization for GSM8K using Qwen2-1.5B-Instruct on irrelevant responses. 104 Figure 130: Visualization for StrategyQA using Qwen2-1.5B-Instruct on irrelevant responses. Figure 131: Visualization for ECQA using Qwen2-1.5B-Instruct on irrelevant responses. Figure 132: Visualization for CREAK using Qwen2-1.5B-Instruct on irrelevant responses. Figure 133: Visualization for Sensemaking using Qwen2-1.5B-Instruct on irrelevant responses. 106 Figure 134: Visualization for Wiki tasks using Qwen2-1.5B-Instruct on irrelevant responses. 107 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 5.61 1.70 0.71 7.41 1.96 0.65 37.75 11.34 3.13 25.53 8.06 2. 0.02 0.02 0.01 0.04 0.03 0.02 0.03 0.03 0.02 0.02 0.02 0.02 3.42 1.11 0.65 4.11 1.64 0. 13.94 4.99 1.24 14.51 5.22 1.73 0.05 0.01 0.02 0.05 0.04 0.04 0.05 0.07 0.03 0.06 0.02 0. 3.16 2.04 0.37 9.45 3.10 0.74 5.35 2.53 0.84 4.79 2.11 0.80 0.08 0.12 0.03 0.04 0.06 0. 0.06 0.06 0.04 0.07 0.07 0.04 3.93 1.51 0.61 5.85 1.93 0.56 16.54 5.60 1.52 13.90 4.86 1. 0.05 0.04 0.02 0.05 0.04 0.04 0.04 0.05 0.03 0.05 0.03 0.02 Table 119: Statistical results for ECQA using Qwen21.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.04 1.58 0.71 3.70 2.08 0.59 17.71 10.88 2. 13.38 7.24 2.18 0.03 0.02 0.01 0.03 0.03 0.02 0.04 0.03 0.03 0.02 0.02 0.01 3.16 1.31 0. 2.41 1.45 0.50 7.85 5.10 1.18 7.51 4.89 1.53 0.05 0.02 0.02 0.03 0.04 0.05 0.03 0.06 0. 0.05 0.03 0.02 4.11 1.26 0.42 6.81 2.24 0.78 2.32 2.28 0.74 1.74 1.73 0.62 0.15 0.05 0. 0.11 0.01 0.02 0.07 0.05 0.03 0.09 0.07 0.04 3.41 1.43 0.62 3.76 1.67 0.56 8.34 5.45 1. 7.23 4.47 1.40 0.07 0.03 0.02 0.05 0.03 0.04 0.04 0.05 0.03 0.05 0.04 0.02 Table 121: Statistical results for Sensemaking using Qwen2-1.5B-Instruct on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 10.37 2.27 0.68 14.60 2.58 0.58 74.28 16.19 3.52 48.66 11.07 2. 0.02 0.01 0.01 0.03 0.03 0.02 0.04 0.03 0.02 0.02 0.02 0.02 10.21 1.89 0.53 18.67 3.23 0. 33.01 7.40 1.35 30.51 7.13 1.79 0.07 0.03 0.01 0.06 0.05 0.03 0.05 0.07 0.04 0.06 0.03 0. 6.14 4.72 0.33 16.06 5.85 0.69 8.01 3.77 0.89 4.55 2.50 0.74 0.14 0.16 0.03 0.07 0.08 0. 0.08 0.09 0.04 0.10 0.08 0.04 9.12 2.70 0.53 15.90 3.54 0.48 34.90 8.12 1.68 27.25 6.58 1. 0.08 0.06 0.02 0.05 0.05 0.03 0.06 0.06 0.03 0.05 0.04 0.02 Table 120: Statistical results for CREAK using Qwen21.5B-Instruct on irrelevant responses. Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.83 0.73 0.65 0.86 0.98 0.60 0.49 1. 5.79 3.77 3.07 8.64 4.06 2.84 2.45 5.89 0.02 0.02 0.03 0.01 0.03 0.02 0.02 0.04 0.03 0.02 0.02 0.03 0.02 0.01 0.01 0. 0.51 0.33 0.28 0.60 0.53 0.27 0.20 0.65 2.01 1.20 0.91 2.27 2.07 1.56 1.35 2.10 0.02 0.02 0.01 0.02 0.02 0.03 0.03 0. 0.06 0.05 0.04 0.07 0.02 0.01 0.01 0.03 0.40 0.29 0.26 0.45 0.54 0.27 0.21 0.72 1.42 1.07 0.89 1.33 0.98 0.82 0.76 0. 0.02 0.01 0.01 0.02 0.02 0.02 0.01 0.02 0.09 0.08 0.07 0.09 0.07 0.06 0.05 0.08 0.58 0.42 0.36 0.61 0.67 0.35 0.27 0. 2.70 1.74 1.39 3.42 2.22 1.63 1.42 2.65 0.02 0.02 0.02 0.02 0.03 0.02 0.02 0.03 0.06 0.05 0.04 0.06 0.03 0.02 0.02 0. Table 122: Statistical results for Wiki using Qwen21.5B-Instruct on irrelevant responses. 108 Results on Llama-2-7B-hf Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All E.1 Pre-trained LLM on Correct Responses E.1.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 135, Table 123), MATH-Counting (Figure 136, Table 124), MATHGeometry (Figure 137, Table 125). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.27 0.17 0.26 0.17 1.24 0. 0.66 0.44 0.03 0.04 0.03 0.03 0.04 0.04 0.02 0.02 0.19 0. 0.19 0.13 0.62 0.40 0.59 0.41 0.01 0.00 0.01 0.01 0.01 0. 0.00 0.00 0.08 0.05 0.12 0.08 0.18 0.09 0.17 0.10 0.01 0. 0.01 0.01 0.01 0.01 0.01 0.01 0.18 0.11 0.19 0.12 0.65 0. 0.45 0.30 0.01 0.02 0.02 0.01 0.02 0.02 0.01 0.01 Table 123: Statistical results for MATH-Algebra using Llama-2-7b-hf on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.26 0.17 0.25 0. 1.25 0.74 0.73 0.50 0.04 0.04 0.03 0.03 0.04 0.04 0.02 0. 0.16 0.11 0.17 0.12 0.56 0.37 0.59 0.39 0.00 0.01 0.01 0. 0.01 0.01 0.00 0.00 0.11 0.07 0.14 0.09 0.20 0.11 0.18 0. 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.18 0.11 0.19 0. 0.64 0.39 0.47 0.31 0.01 0.02 0.01 0.01 0.02 0.02 0.01 0. Table 124: Statistical results for MATH-Counting using Llama-2-7b-hf on correct responses. Figure 136: Visualization for MATH-Counting using Llama-2-7b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.25 0.18 0.25 0.17 1.16 0.79 0.66 0.53 0.04 0.04 0.02 0. 0.04 0.04 0.02 0.01 0.14 0.11 0.15 0.12 0.50 0.40 0.52 0. 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.00 0.09 0.07 0.12 0. 0.16 0.12 0.15 0.12 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0. 0.17 0.12 0.18 0.13 0.58 0.42 0.42 0.33 0.02 0.02 0.01 0. 0.02 0.02 0.01 0.01 Table 125: Statistical results for MATH-Geometry using Llama-2-7b-hf on correct responses. Figure 135: Visualization for MATH-Algebra using Llama-2-7b-hf on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 138, Table 126), GSM8K (Figure 139, Table 127), StrategyQA (Figure 140, Table 128), ECQA (Figure 141, Table 129), CREAK (Figure 142, Table 130), Sensemaking (Figure 143, Table 131). Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.17 0.29 0.14 0.97 0.28 0. 7.00 1.38 0.74 2.52 0.66 0.43 0.03 0.03 0.04 0.04 0.03 0.04 0.06 0.05 0.04 0.05 0.03 0. 0.57 0.19 0.09 0.65 0.18 0.10 1.77 0.59 0.31 1.27 0.54 0.32 0.03 0.01 0.01 0.02 0.01 0. 0.01 0.01 0.01 0.02 0.01 0.00 0.69 0.10 0.06 1.09 0.14 0.08 1.23 0.16 0.08 0.87 0.13 0. 0.06 0.01 0.02 0.05 0.01 0.02 0.03 0.01 0.01 0.06 0.02 0.01 0.81 0.19 0.10 0.92 0.20 0. 3.04 0.67 0.36 1.50 0.42 0.26 0.04 0.02 0.02 0.04 0.02 0.02 0.03 0.02 0.02 0.04 0.02 0. Table 127: Statistical results for GSM8K using Llama2-7b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.84 0.34 0. 1.85 0.33 0.19 15.66 2.19 0.90 5.49 0.98 0.58 0.04 0.02 0.03 0.02 0.03 0.03 0.06 0.05 0. 0.05 0.03 0.02 1.74 0.17 0.10 2.14 0.14 0.10 3.58 0.52 0.36 1.91 0.38 0.39 0.08 0.00 0. 0.07 0.00 0.01 0.04 0.01 0.01 0.04 0.01 0.01 0.82 0.13 0.08 2.49 0.14 0.07 3.69 0.31 0. 4.27 0.29 0.19 0.08 0.01 0.01 0.06 0.01 0.01 0.07 0.02 0.01 0.07 0.01 0.01 1.39 0.22 0. 2.05 0.20 0.12 7.06 0.94 0.46 3.86 0.52 0.37 0.07 0.01 0.01 0.05 0.01 0.02 0.06 0.03 0. 0.05 0.02 0.01 Table 128: Statistical results for StrategyQA using Llama-2-7b-hf on correct responses. reasoning tasks: AQuA (Figure 148, Table 136), GSM8K (Figure 149, Table 137), StrategyQA (Figure 150, Table 138), ECQA (Figure 151, Table 139), CREAK (Figure 152, Table 140), Sensemaking (Figure 153, Table 141). E.2.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 154 and Table 142. Figure 137: Visualization for MATH-Geometry using Llama-2-7b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 0.86 0.48 0.17 1.16 0.45 0.16 13.84 2.43 0.75 4.38 1.37 0. 0.07 0.03 0.03 0.04 0.03 0.03 0.07 0.06 0.04 0.04 0.02 0.02 0.77 0.33 0.12 0.95 0.33 0. 1.70 0.91 0.36 1.26 0.78 0.37 0.05 0.01 0.00 0.02 0.01 0.01 0.02 0.01 0.01 0.03 0.01 0. 1.00 0.18 0.06 1.77 0.28 0.08 0.64 0.27 0.09 0.42 0.21 0.09 0.13 0.01 0.01 0.08 0.01 0. 0.03 0.01 0.01 0.08 0.02 0.01 0.88 0.33 0.11 1.28 0.35 0.12 4.81 1.11 0.38 1.89 0.73 0. 0.08 0.02 0.02 0.04 0.02 0.01 0.04 0.02 0.02 0.05 0.02 0.01 Table 126: Statistical results for AQuA using Llama-27b-hf on correct responses. E.1.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 144 and Table 132. E.2 Pre-trained LLM on Irrelevant Responses E.2.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 145, Table 133), MATH-Counting (Figure 146, Table 134), MATHGeometry (Figure 147, Table 135)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "110 Figure 138: Visualization for AQuA using Llama-2-7b-hf on correct responses. Figure 139: Visualization for GSM8K using Llama-2-7b-hf on correct responses. 111 Figure 140: Visualization for StrategyQA using Llama-2-7b-hf on correct responses. Figure 141: Visualization for ECQA using Llama-2-7b-hf on correct responses. 112 Figure 142: Visualization for CREAK using Llama-2-7b-hf on correct responses. Figure 143: Visualization for Sensemaking using Llama-2-7b-hf on correct responses. 113 Figure 144: Visualization for Wiki tasks using Llama-2-7b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.03 0.66 0.27 1.34 0.60 0.25 11.00 3.86 1.18 4.11 1.89 0. 0.05 0.03 0.03 0.03 0.03 0.03 0.07 0.05 0.04 0.03 0.04 0.02 0.70 0.31 0.14 0.70 0.29 0. 1.60 1.04 0.51 1.24 0.76 0.49 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0. 0.84 0.20 0.11 2.12 0.27 0.11 1.88 0.49 0.26 1.45 0.40 0.25 0.06 0.02 0.01 0.03 0.02 0. 0.03 0.02 0.01 0.07 0.02 0.01 0.86 0.39 0.17 1.39 0.37 0.16 4.40 1.64 0.62 2.17 0.93 0. 0.04 0.02 0.02 0.03 0.02 0.02 0.04 0.02 0.02 0.04 0.02 0.01 Table 129: Statistical results for ECQA using Llama-27b-hf on correct responses. sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 0.42 0.54 0.22 0.54 0.49 0. 5.18 2.87 0.99 1.86 1.76 0.69 0.05 0.03 0.04 0.04 0.03 0.03 0.06 0.05 0.04 0.03 0.03 0. 0.44 0.34 0.12 0.44 0.27 0.13 0.90 1.01 0.45 0.70 0.77 0.43 0.04 0.01 0.00 0.02 0.01 0. 0.02 0.01 0.01 0.02 0.01 0.00 0.55 0.30 0.07 0.96 0.46 0.07 0.50 0.63 0.17 0.60 0.46 0. 0.12 0.04 0.01 0.07 0.03 0.01 0.04 0.03 0.01 0.06 0.03 0.01 0.48 0.40 0.13 0.65 0.41 0. 1.99 1.39 0.51 1.02 0.93 0.40 0.07 0.03 0.02 0.04 0.02 0.02 0.04 0.03 0.02 0.04 0.02 0. Table 131: Statistical results for Sensemaking using Llama-2-7b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.36 0.27 0.23 0.36 0.35 0.25 0.22 0. 2.19 1.35 1.05 3.83 1.15 0.86 0.74 1.89 0.02 0.03 0.04 0.03 0.03 0.02 0.02 0.04 0.05 0.04 0.04 0.06 0.03 0.02 0.02 0. 0.17 0.13 0.12 0.21 0.17 0.12 0.10 0.25 0.49 0.42 0.40 0.44 0.44 0.46 0.44 0.43 0.01 0.01 0.01 0.02 0.02 0.01 0.01 0. 0.02 0.01 0.01 0.01 0.01 0.01 0.00 0.02 0.24 0.19 0.16 0.29 0.27 0.22 0.17 0.32 0.29 0.29 0.28 0.45 0.28 0.28 0.27 0. 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.02 0.25 0.19 0.16 0.30 0.26 0.19 0.16 0. 0.91 0.64 0.55 1.42 0.58 0.50 0.46 0.88 0.01 0.02 0.02 0.02 0.02 0.01 0.01 0.02 0.02 0.02 0.02 0.03 0.02 0.01 0.01 0. Table 132: Statistical results for Wiki using Llama-27b-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.45 0.51 0. 2.07 0.45 0.25 19.35 3.00 1.20 6.51 1.48 0.71 0.05 0.03 0.03 0.03 0.03 0.03 0.07 0.05 0. 0.04 0.03 0.02 1.02 0.26 0.13 1.27 0.23 0.14 2.92 0.82 0.47 2.33 0.57 0.43 0.03 0.01 0. 0.02 0.01 0.01 0.02 0.01 0.01 0.02 0.01 0.01 1.87 0.19 0.09 3.70 0.25 0.08 3.63 0.49 0. 3.32 0.40 0.20 0.12 0.01 0.01 0.07 0.01 0.01 0.05 0.02 0.01 0.07 0.02 0.01 1.48 0.32 0. 2.37 0.31 0.15 7.88 1.32 0.59 3.97 0.76 0.42 0.07 0.01 0.02 0.04 0.01 0.02 0.05 0.03 0. 0.04 0.02 0.01 Table 130: Statistical results for CREAK using Llama2-7b-hf on correct responses. 115 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.38 0.23 0.39 0.24 1.83 1.01 0.97 0.62 0.03 0.04 0.02 0. 0.04 0.04 0.02 0.02 0.24 0.14 0.24 0.16 0.72 0.49 0.70 0. 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.00 0.14 0.09 0.17 0. 0.27 0.16 0.25 0.16 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0. 0.26 0.16 0.27 0.17 0.89 0.52 0.61 0.39 0.02 0.02 0.02 0. 0.02 0.02 0.01 0.01 Table 133: Statistical results for MATH-Algebra using Llama-2-7b-hf on irrelevant responses. Figure 145: Visualization for MATH-Algebra using Llama-2-7b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.32 0.25 0.33 0.26 1.62 1.13 0.91 0. 0.03 0.04 0.02 0.03 0.04 0.04 0.02 0.02 0.20 0.13 0.20 0. 0.62 0.46 0.63 0.46 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0. 0.14 0.10 0.16 0.12 0.32 0.20 0.29 0.18 0.00 0.01 0.01 0. 0.01 0.01 0.01 0.01 0.23 0.16 0.23 0.17 0.80 0.56 0.57 0. 0.01 0.02 0.01 0.01 0.02 0.02 0.01 0.01 Table 134: Statistical results for MATH-Counting using Llama-2-7b-hf on irrelevant responses. Figure 146: Visualization for MATH-Counting using Llama-2-7b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.32 0. 0.33 0.29 1.50 1.18 0.84 0.84 0.04 0.04 0.02 0.02 0.04 0. 0.02 0.02 0.16 0.11 0.17 0.12 0.57 0.44 0.59 0.47 0.01 0. 0.01 0.01 0.01 0.01 0.00 0.00 0.14 0.14 0.15 0.18 0.25 0. 0.23 0.23 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.21 0. 0.22 0.20 0.73 0.57 0.52 0.48 0.02 0.02 0.01 0.01 0.02 0. 0.01 0.01 Table 135: Statistical results for MATH-Geometry using Llama-2-7b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.29 0.41 0.26 1.06 0.39 0.25 7.23 2.24 1.35 2.64 1.02 0.74 0.03 0.03 0.04 0.04 0.03 0. 0.06 0.05 0.04 0.05 0.03 0.02 0.60 0.28 0.15 0.64 0.27 0.16 1.79 0.71 0.47 1.31 0.65 0. 0.03 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.00 0.80 0.17 0.12 1.26 0.21 0. 1.27 0.38 0.23 0.89 0.35 0.23 0.07 0.01 0.01 0.06 0.01 0.01 0.03 0.02 0.01 0.06 0.01 0. 0.90 0.29 0.18 1.00 0.29 0.18 3.13 1.01 0.63 1.55 0.64 0.44 0.04 0.02 0.02 0.04 0.02 0. 0.03 0.02 0.02 0.04 0.02 0.01 Figure 147: Visualization for MATH-Geometry using Llama-2-7b-hf on irrelevant responses. Table 137: Statistical results for GSM8K using Llama2-7b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 0.86 0.54 0.28 1.16 0.50 0.29 13.76 2.93 1.35 4.38 1.58 0.79 0.07 0.04 0.04 0.04 0.03 0. 0.07 0.06 0.04 0.04 0.02 0.02 0.78 0.35 0.16 0.95 0.34 0.18 1.71 0.89 0.52 1.28 0.82 0. 0.05 0.01 0.01 0.02 0.01 0.01 0.02 0.01 0.01 0.03 0.01 0.00 1.01 0.23 0.10 1.78 0.31 0. 0.64 0.43 0.22 0.43 0.34 0.21 0.13 0.01 0.01 0.08 0.01 0.01 0.03 0.01 0.01 0.08 0.02 0. 0.88 0.38 0.18 1.29 0.39 0.19 4.79 1.28 0.64 1.90 0.86 0.47 0.08 0.02 0.02 0.04 0.02 0. 0.04 0.03 0.02 0.05 0.02 0.01 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.74 0.52 0.29 1.93 0.53 0.29 16.91 3.33 1.44 5.70 1.60 0.91 0.05 0.02 0. 0.02 0.03 0.02 0.06 0.05 0.04 0.05 0.03 0.02 1.59 0.25 0.14 2.01 0.23 0.14 3.74 0.65 0. 1.92 0.42 0.39 0.08 0.01 0.01 0.06 0.01 0.01 0.04 0.01 0.01 0.04 0.01 0.00 0.85 0.17 0. 2.16 0.18 0.08 3.63 0.52 0.32 4.24 0.44 0.33 0.08 0.01 0.01 0.05 0.01 0.01 0.07 0.02 0. 0.07 0.01 0.01 1.35 0.31 0.17 1.94 0.30 0.16 7.44 1.35 0.68 3.92 0.78 0.51 0.07 0.01 0. 0.04 0.02 0.01 0.06 0.03 0.02 0.05 0.02 0.01 Table 136: Statistical results for AQuA using Llama-27b-hf on irrelevant responses. Table 138: Statistical results for StrategyQA using Llama-2-7b-hf on irrelevant responses. Figure 148: Visualization for AQuA using Llama-2-7b-hf on irrelevant responses. Figure 149: Visualization for GSM8K using Llama-2-7b-hf on irrelevant responses. 118 Figure 150: Visualization for StrategyQA using Llama-2-7b-hf on irrelevant responses. Figure 151: Visualization for ECQA using Llama-2-7b-hf on irrelevant responses. Figure 152: Visualization for CREAK using Llama-2-7b-hf on irrelevant responses. Figure 153: Visualization for Sensemaking using Llama-2-7b-hf on irrelevant responses. 120 Figure 154: Visualization for Wiki tasks using Llama-2-7b-hf on irrelevant responses. 121 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.45 0.64 0.35 1.58 0.57 0.34 12.22 3.70 1.60 6.02 1.80 0.98 0.05 0.03 0. 0.03 0.03 0.02 0.06 0.05 0.04 0.04 0.03 0.02 0.71 0.29 0.18 0.70 0.26 0.18 1.92 0.85 0. 1.47 0.59 0.49 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.83 0.20 0. 1.97 0.26 0.11 2.34 0.63 0.39 1.81 0.54 0.38 0.06 0.01 0.01 0.04 0.01 0.02 0.03 0.02 0. 0.04 0.03 0.01 0.98 0.37 0.21 1.39 0.34 0.20 5.00 1.56 0.79 2.95 0.91 0.58 0.04 0.01 0. 0.03 0.02 0.02 0.03 0.02 0.02 0.04 0.02 0.01 Table 139: Statistical results for ECQA using Llama-27b-hf on irrelevant responses. sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 0.44 0.69 0.32 0.56 0.66 0.34 5.22 3.68 1. 1.94 2.12 0.99 0.05 0.04 0.03 0.04 0.03 0.02 0.06 0.05 0.04 0.03 0.03 0.02 0.46 0.51 0. 0.45 0.55 0.20 0.95 1.17 0.57 0.72 0.82 0.50 0.03 0.01 0.01 0.02 0.01 0.01 0.02 0.01 0. 0.02 0.01 0.00 0.56 0.33 0.10 0.97 0.48 0.09 0.50 0.81 0.32 0.60 0.58 0.32 0.12 0.04 0. 0.07 0.03 0.02 0.04 0.03 0.01 0.06 0.02 0.01 0.50 0.52 0.19 0.67 0.56 0.20 2.02 1.71 0. 1.05 1.11 0.57 0.07 0.03 0.02 0.04 0.02 0.02 0.04 0.03 0.02 0.04 0.02 0.01 Table 141: Statistical results for Sensemaking using Llama-2-7b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.51 0.31 0.29 0.54 0.50 0.30 0.28 0.53 3.20 2.01 1.72 5. 1.62 1.29 1.17 2.86 0.02 0.03 0.04 0.02 0.03 0.02 0.02 0.03 0.05 0.04 0.04 0.05 0.03 0.02 0.02 0.03 0.22 0.14 0.12 0. 0.22 0.12 0.11 0.26 0.64 0.47 0.45 0.80 0.49 0.43 0.42 0.67 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.02 0.02 0.01 0.01 0. 0.01 0.01 0.00 0.01 0.23 0.18 0.18 0.25 0.26 0.20 0.20 0.27 0.37 0.37 0.37 0.47 0.33 0.35 0.35 0.39 0.01 0.01 0.01 0. 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.00 0.01 0.31 0.21 0.19 0.35 0.31 0.20 0.19 0.35 1.26 0.86 0.77 2. 0.76 0.64 0.60 1.22 0.01 0.02 0.02 0.01 0.02 0.01 0.01 0.02 0.03 0.02 0.02 0.03 0.02 0.01 0.01 0.01 Table 142: Statistical results for Wiki using Llama-27b-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 1.54 0.62 0.34 2.14 0.57 0. 18.53 3.81 1.53 6.50 2.01 0.91 0.05 0.03 0.03 0.03 0.03 0.02 0.07 0.05 0.04 0.04 0.03 0. 1.12 0.37 0.15 1.39 0.36 0.16 3.19 0.96 0.50 2.43 0.61 0.43 0.03 0.01 0.01 0.02 0.01 0. 0.02 0.01 0.01 0.02 0.01 0.01 1.87 0.23 0.11 3.62 0.30 0.11 3.71 0.65 0.33 3.36 0.54 0. 0.12 0.01 0.01 0.08 0.01 0.01 0.05 0.02 0.01 0.07 0.02 0.01 1.54 0.41 0.20 2.42 0.40 0. 7.75 1.63 0.72 4.02 0.99 0.53 0.07 0.01 0.01 0.04 0.02 0.02 0.04 0.03 0.02 0.04 0.02 0. Table 140: Statistical results for CREAK using Llama2-7b-hf on irrelevant responses. 122 E."
        },
        {
            "title": "Instructed LLM on Correct Responses",
            "content": "Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All E.3.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 155, Table 143), MATH-Counting (Figure 156, Table 144), MATHGeometry (Figure 157, Table 145). Counting Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.85 0. 0.83 0.43 4.11 1.98 2.66 1.41 0.03 0.04 0.02 0.02 0.05 0. 0.02 0.02 0.74 0.38 0.78 0.40 1.93 1.04 1.84 1.08 0.02 0. 0.02 0.02 0.01 0.01 0.01 0.00 0.53 0.34 0.63 0.38 0.60 0. 0.53 0.30 0.04 0.04 0.04 0.05 0.03 0.03 0.02 0.02 0.70 0. 0.73 0.40 2.10 1.07 1.56 0.87 0.03 0.03 0.03 0.03 0.03 0. 0.01 0.01 Table 143: Statistical results for MATH-Algebra using Llama-2-7b-chat-hf on correct responses. sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.80 0.47 0.78 0.45 4.05 2.17 2.80 1.62 0.03 0.04 0.02 0. 0.05 0.05 0.02 0.02 0.65 0.39 0.72 0.40 1.82 1.03 1.83 1. 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.00 0.44 0.36 0.53 0. 0.65 0.44 0.51 0.33 0.04 0.04 0.04 0.05 0.03 0.03 0.01 0. 0.62 0.40 0.65 0.42 2.05 1.16 1.58 0.95 0.03 0.03 0.03 0. 0.03 0.03 0.01 0.01 Table 144: Statistical results for MATH-Counting using Llama-2-7b-chat-hf on correct responses. Figure 156: Visualization for MATH-Counting using Llama-2-7b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.71 0.48 0.69 0.47 3.51 2.23 2.39 1. 0.04 0.04 0.02 0.02 0.04 0.04 0.02 0.02 0.57 0.38 0.65 0. 1.58 1.08 1.58 1.11 0.02 0.02 0.02 0.02 0.01 0.01 0.00 0. 0.52 0.39 0.62 0.43 0.59 0.43 0.51 0.35 0.04 0.05 0.04 0. 0.03 0.03 0.01 0.02 0.60 0.41 0.65 0.44 1.78 1.18 1.39 0. 0.03 0.03 0.03 0.03 0.03 0.03 0.01 0.01 Table 145: Statistical results for MATH-Geometry using Llama-2-7b-chat-hf on correct responses. Figure 155: Visualization for MATH-Algebra using Llama-2-7b-chat-hf on correct responses. The visualizations and statistical results on other reasoning tasks: AQuA (Figure 158, Table 146), GSM8K (Figure 159, Table 147), StrategyQA (Figure 160, Table 148), ECQA (Figure 161, Table 149), CREAK (Figure 162, Table 150), Sensemaking (Figure 163, Table 151). 123 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.79 0.88 0.45 5.31 0.86 0.41 31.34 4.46 2.18 19.29 2.95 1.65 0.03 0.03 0.04 0.03 0.03 0. 0.07 0.06 0.05 0.03 0.02 0.02 3.74 0.71 0.38 5.10 0.85 0.36 11.51 1.95 0.94 9.66 1.79 1. 0.04 0.02 0.02 0.03 0.02 0.02 0.03 0.01 0.01 0.03 0.01 0.01 2.69 0.42 0.39 3.77 0.53 0. 7.86 0.67 0.48 3.91 0.39 0.32 0.07 0.04 0.04 0.06 0.04 0.05 0.05 0.03 0.03 0.08 0.02 0. 3.51 0.65 0.40 4.42 0.72 0.40 15.28 2.20 1.14 10.13 1.58 0.93 0.04 0.03 0.03 0.04 0.03 0. 0.04 0.03 0.03 0.05 0.02 0.01 Table 147: Statistical results for GSM8K using Llama2-7b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.24 0.91 0.43 6.17 0.87 0.40 47.27 5.43 2.10 28.77 3.46 1.58 0.03 0.03 0. 0.03 0.03 0.02 0.07 0.05 0.04 0.04 0.03 0.02 5.38 0.56 0.29 7.25 0.71 0.30 17.66 1.60 0. 14.81 1.38 0.90 0.05 0.02 0.02 0.01 0.02 0.02 0.04 0.02 0.01 0.05 0.01 0.00 6.40 0.68 0. 8.80 0.88 0.53 17.60 1.03 0.56 9.00 0.57 0.40 0.09 0.02 0.04 0.04 0.03 0.05 0.08 0.04 0. 0.10 0.01 0.01 5.71 0.70 0.37 7.08 0.79 0.41 25.66 2.49 1.10 16.46 1.67 0.89 0.06 0.02 0. 0.03 0.03 0.03 0.06 0.03 0.03 0.06 0.02 0.01 Table 148: Statistical results for StrategyQA using Llama-2-7b-chat-hf on correct responses. reasoning tasks: AQuA (Figure 168, Table 156), GSM8K (Figure 169, Table 157), StrategyQA (Figure 170, Table 158), ECQA (Figure 171, Table 159), CREAK (Figure 172, Table 160), Sensemaking (Figure 173, Table 161). E.4.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 174 and Table 162. Figure 157: Visualization for MATH-Geometry using Llama-2-7b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 9.84 1.38 0.47 11.77 1.37 0. 82.67 7.31 2.17 32.51 4.50 1.63 0.08 0.03 0.04 0.05 0.03 0.02 0.09 0.06 0.05 0.05 0.02 0. 5.39 1.00 0.39 7.22 1.17 0.38 13.02 2.77 1.00 8.05 2.62 1.10 0.05 0.02 0.02 0.01 0.02 0. 0.03 0.01 0.01 0.03 0.01 0.01 4.01 0.63 0.40 7.84 0.77 0.43 6.96 0.98 0.45 4.15 0.72 0. 0.11 0.04 0.04 0.03 0.04 0.05 0.04 0.03 0.03 0.06 0.02 0.01 6.13 0.98 0.41 8.70 1.07 0. 31.47 3.42 1.15 14.13 2.42 0.95 0.08 0.03 0.03 0.03 0.03 0.03 0.05 0.03 0.03 0.05 0.02 0. Table 146: Statistical results for AQuA using Llama-27b-chat-hf on correct responses. E.3.2 Wiki Tasks The visualizations and statistical results on Wiki tasks are shown in Figure 164 and Table 152. E."
        },
        {
            "title": "Instructed LLM on Irrelevant Responses",
            "content": "E.4.1 Reasoning Tasks The visualizations and statistical results on MATH tasks: MATH-Algebra (Figure 165, Table 153), MATH-Counting (Figure 166, Table 154), MATHGeometry (Figure 167, Table 155)."
        },
        {
            "title": "The visualizations and statistical results on other",
            "content": "124 Figure 158: Visualization for AQuA using Llama-2-7b-chat-hf on correct responses. Figure 159: Visualization for GSM8K using Llama-2-7b-chat-hf on correct responses. 125 Figure 160: Visualization for StrategyQA using Llama-2-7b-chat-hf on correct responses. Figure 161: Visualization for ECQA using Llama-2-7b-chat-hf on correct responses. 126 Figure 162: Visualization for CREAK using Llama-2-7b-chat-hf on correct responses. Figure 163: Visualization for Sensemaking using Llama-2-7b-chat-hf on correct responses. 127 Figure 164: Visualization for Wiki tasks using Llama-2-7b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.62 1.41 0.58 3.91 1.42 0.54 27.04 8.06 2. 15.08 5.14 1.92 0.04 0.03 0.04 0.03 0.03 0.02 0.07 0.05 0.04 0.04 0.03 0.02 2.46 0.86 0. 3.97 1.04 0.41 8.65 2.94 1.11 6.91 2.53 1.21 0.04 0.02 0.02 0.01 0.02 0.03 0.03 0.02 0. 0.05 0.01 0.01 3.58 0.83 0.50 4.79 1.24 0.69 8.79 1.71 0.76 4.56 1.03 0.49 0.12 0.03 0. 0.08 0.03 0.06 0.09 0.05 0.04 0.11 0.02 0.01 3.18 1.01 0.49 4.15 1.19 0.55 13.76 3.93 1. 8.33 2.67 1.13 0.06 0.03 0.03 0.04 0.02 0.04 0.06 0.04 0.03 0.07 0.02 0.01 Table 149: Statistical results for ECQA using Llama-27b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.53 1.85 0.47 3.85 1.94 0. 23.30 10.36 2.17 14.40 6.65 1.71 0.03 0.03 0.04 0.04 0.03 0.02 0.05 0.05 0.04 0.03 0.03 0. 2.69 1.45 0.34 3.34 1.81 0.35 7.57 3.63 0.92 6.56 3.25 1.01 0.03 0.02 0.02 0.01 0.03 0. 0.03 0.02 0.01 0.03 0.02 0.01 3.06 1.51 0.38 4.61 2.14 0.51 6.10 2.60 0.58 3.24 1.42 0. 0.08 0.03 0.04 0.06 0.02 0.05 0.06 0.05 0.04 0.09 0.03 0.01 3.07 1.58 0.39 3.93 1.93 0. 11.41 5.05 1.17 7.64 3.51 0.96 0.05 0.03 0.03 0.04 0.03 0.03 0.05 0.04 0.03 0.05 0.02 0. Table 151: Statistical results for Sensemaking using Llama-2-7b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.67 0.48 0.41 0.96 0.65 0.45 0.38 0. 3.75 2.33 1.85 7.65 2.29 1.64 1.37 3.88 0.03 0.03 0.04 0.03 0.03 0.02 0.02 0.04 0.05 0.04 0.04 0.05 0.02 0.02 0.01 0. 0.28 0.23 0.20 0.78 0.35 0.23 0.20 0.92 0.81 0.73 0.65 1.50 0.87 0.76 0.70 1.49 0.01 0.01 0.01 0.03 0.01 0.01 0.02 0. 0.02 0.01 0.01 0.02 0.01 0.01 0.01 0.02 0.58 0.43 0.36 0.77 0.67 0.51 0.43 0.87 0.47 0.40 0.38 0.66 0.43 0.38 0.35 0. 0.02 0.02 0.03 0.02 0.02 0.02 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.50 0.38 0.32 0.83 0.54 0.40 0.34 0. 1.56 1.09 0.91 2.95 1.12 0.88 0.77 1.93 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.03 0.02 0.02 0.03 0.02 0.01 0.01 0. Table 152: Statistical results for Wiki using Llama-27b-chat-hf on correct responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.65 1.53 0. 8.34 1.57 0.50 55.83 9.31 2.58 33.10 5.72 1.86 0.03 0.03 0.04 0.03 0.03 0.02 0.06 0.05 0. 0.04 0.03 0.02 5.59 1.14 0.38 7.09 1.45 0.40 19.94 3.19 0.98 15.36 2.62 1.02 0.05 0.03 0. 0.02 0.03 0.02 0.04 0.02 0.01 0.05 0.01 0.00 6.68 1.05 0.44 9.21 1.53 0.57 16.12 2.07 0. 8.03 1.18 0.42 0.11 0.03 0.04 0.07 0.03 0.05 0.07 0.05 0.04 0.12 0.02 0.01 6.42 1.21 0. 8.02 1.48 0.48 28.78 4.45 1.33 18.01 2.97 1.03 0.07 0.03 0.03 0.04 0.03 0.03 0.05 0.04 0. 0.07 0.02 0.01 Table 150: Statistical results for CREAK using Llama2-7b-chat-hf on correct responses. 129 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Algebra sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.88 0.52 0.86 0.50 4.54 2.39 2.73 1.51 0.03 0.03 0.02 0. 0.04 0.04 0.02 0.02 0.77 0.43 0.91 0.46 1.94 1.17 1.88 1. 0.03 0.02 0.02 0.02 0.02 0.01 0.01 0.00 0.62 0.37 0.78 0. 0.72 0.41 0.59 0.36 0.04 0.04 0.04 0.05 0.03 0.03 0.02 0. 0.75 0.43 0.83 0.46 2.26 1.26 1.62 0.94 0.03 0.03 0.03 0. 0.03 0.03 0.01 0.01 Table 153: Statistical results for MATH-Algebra using Llama-2-7b-chat-hf on irrelevant responses. Figure 165: Visualization for MATH-Algebra using Llama-2-7b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Counting sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.82 0.51 0.78 0.48 4.36 2.40 2.79 1. 0.04 0.03 0.02 0.02 0.05 0.04 0.02 0.02 0.64 0.37 0.76 0. 1.82 1.08 1.77 1.10 0.02 0.02 0.02 0.02 0.02 0.01 0.00 0. 0.50 0.34 0.63 0.43 0.79 0.48 0.61 0.39 0.04 0.04 0.04 0. 0.03 0.02 0.01 0.01 0.64 0.40 0.70 0.42 2.17 1.24 1.60 0. 0.03 0.03 0.03 0.03 0.03 0.02 0.01 0.01 Table 154: Statistical results for MATH-Counting using Llama-2-7b-chat-hf on irrelevant responses. Figure 166: Visualization for MATH-Counting using Llama-2-7b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Geometry sQ sK sV sO rQ rK rV rO Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed Simplified Detailed 0.73 0. 0.70 0.52 3.80 2.48 2.50 1.75 0.04 0.04 0.02 0.02 0.04 0. 0.02 0.02 0.54 0.33 0.64 0.40 1.55 1.06 1.57 1.11 0.02 0. 0.02 0.02 0.01 0.01 0.01 0.00 0.61 0.41 0.77 0.52 0.69 0. 0.61 0.48 0.04 0.04 0.04 0.04 0.03 0.02 0.01 0.01 0.63 0. 0.70 0.47 1.88 1.26 1.46 1.04 0.03 0.03 0.03 0.03 0.03 0. 0.01 0.01 Table 155: Statistical results for MATH-Geometry using Llama-2-7b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV GSM8K sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.90 1.02 0.59 5.42 0.97 0.55 31.91 5.40 2.86 19.67 3.29 2.05 0.03 0.03 0.04 0.03 0.02 0. 0.07 0.05 0.05 0.03 0.02 0.02 3.83 0.86 0.48 5.16 1.11 0.55 11.68 1.99 1.09 9.77 1.88 1. 0.03 0.02 0.02 0.03 0.02 0.03 0.03 0.01 0.01 0.03 0.01 0.00 2.67 0.53 0.45 3.73 0.69 0. 7.97 1.00 0.66 3.96 0.70 0.47 0.06 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.03 0.07 0.02 0. 3.56 0.78 0.49 4.45 0.88 0.55 15.55 2.58 1.45 10.29 1.81 1.13 0.04 0.03 0.03 0.04 0.03 0. 0.04 0.03 0.03 0.04 0.01 0.01 Figure 167: Visualization for MATH-Geometry using Llama-2-7b-chat-hf on irrelevant responses. Table 157: Statistical results for GSM8K using Llama2-7b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV AQuA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 9.67 1.46 0.66 11.48 1.40 0.62 81.42 7.57 3.17 32.04 4.53 2.11 0.07 0.04 0.04 0.05 0.03 0. 0.09 0.06 0.05 0.05 0.02 0.02 5.34 1.10 0.53 7.26 1.44 0.61 12.90 2.92 1.31 8.00 2.70 1. 0.05 0.02 0.02 0.01 0.02 0.02 0.03 0.02 0.01 0.03 0.01 0.00 3.97 0.62 0.47 7.76 0.79 0. 6.94 1.19 0.65 4.11 0.83 0.51 0.11 0.03 0.05 0.03 0.04 0.05 0.04 0.02 0.03 0.06 0.02 0. 6.05 1.03 0.54 8.60 1.16 0.57 31.05 3.57 1.60 13.96 2.48 1.22 0.08 0.03 0.03 0.03 0.03 0. 0.05 0.03 0.03 0.05 0.02 0.01 sQ sK sV StrategyQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 6.22 1.05 0.50 6.13 0.95 0.47 46.97 6.61 2.58 28.58 3.79 1.88 0.03 0.03 0. 0.03 0.03 0.02 0.07 0.05 0.04 0.04 0.03 0.02 5.35 0.59 0.33 7.26 0.69 0.37 17.60 1.55 0. 14.76 1.32 0.80 0.05 0.01 0.01 0.01 0.01 0.02 0.04 0.02 0.01 0.05 0.01 0.01 6.41 0.69 0. 8.85 0.88 0.52 17.59 1.37 0.66 9.00 0.80 0.51 0.09 0.02 0.03 0.04 0.03 0.03 0.08 0.03 0. 0.10 0.01 0.01 5.71 0.76 0.39 7.09 0.81 0.45 25.55 2.90 1.28 16.38 1.84 0.99 0.06 0.02 0. 0.03 0.02 0.02 0.06 0.03 0.03 0.06 0.01 0.01 Table 156: Statistical results for AQuA using Llama-27b-chat-hf on irrelevant responses. Table 158: Statistical results for StrategyQA using Llama-2-7b-chat-hf on irrelevant responses. Figure 168: Visualization for AQuA using Llama-2-7b-chat-hf on irrelevant responses. Figure 169: Visualization for GSM8K using Llama-2-7b-chat-hf on irrelevant responses. 132 Figure 170: Visualization for StrategyQA using Llama-2-7b-chat-hf on irrelevant responses. Figure 171: Visualization for ECQA using Llama-2-7b-chat-hf on irrelevant responses. Figure 172: Visualization for CREAK using Llama-2-7b-chat-hf on irrelevant responses. Figure 173: Visualization for Sensemaking using Llama-2-7b-chat-hf on irrelevant responses. 134 Figure 174: Visualization for Wiki tasks using Llama-2-7b-chat-hf on irrelevant responses. 135 Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV ECQA sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 4.99 1.45 0.63 5.30 1.42 0.60 35.81 8.51 2.86 19.70 5.10 2. 0.04 0.04 0.03 0.02 0.03 0.02 0.06 0.05 0.04 0.03 0.02 0.02 3.02 0.88 0.44 4.70 1.24 0. 10.43 2.51 1.17 8.54 2.19 1.15 0.02 0.02 0.02 0.01 0.02 0.03 0.03 0.02 0.01 0.03 0.01 0. 3.68 0.83 0.44 5.35 1.26 0.64 10.68 1.99 0.89 5.41 1.18 0.62 0.06 0.03 0.04 0.05 0.03 0. 0.07 0.04 0.03 0.06 0.01 0.01 3.78 1.03 0.48 4.95 1.25 0.56 17.50 3.95 1.54 10.47 2.61 1. 0.04 0.03 0.03 0.03 0.03 0.03 0.05 0.04 0.03 0.04 0.01 0.01 Table 159: Statistical results for ECQA using Llama-27b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV Sensemaking sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 3.98 1.69 0.61 4.30 1.78 0.59 25.58 10.10 2. 15.93 6.08 2.00 0.03 0.03 0.03 0.04 0.03 0.02 0.05 0.05 0.04 0.03 0.03 0.02 3.08 1.21 0. 3.92 1.67 0.55 8.46 3.08 1.15 7.37 2.75 1.10 0.02 0.02 0.02 0.01 0.02 0.02 0.03 0.02 0. 0.03 0.01 0.01 3.24 1.31 0.43 4.94 1.86 0.59 6.55 2.73 0.77 3.43 1.54 0.56 0.07 0.03 0. 0.05 0.03 0.04 0.06 0.05 0.03 0.09 0.02 0.01 3.39 1.39 0.48 4.38 1.73 0.55 12.49 4.82 1. 8.40 3.23 1.14 0.04 0.03 0.03 0.03 0.02 0.03 0.04 0.04 0.03 0.05 0.02 0.01 Table 161: Statistical results for Sensemaking using Llama-2-7b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All Wiki sQ sK sV sO rQ rK rV rO Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular Len 100 Len 500 Len 1000 Unpopular 0.87 0.58 0.48 1.00 0.84 0.53 0.45 0.98 5.33 3.13 2.55 9. 2.93 2.08 1.81 5.34 0.03 0.03 0.04 0.03 0.03 0.02 0.02 0.03 0.05 0.04 0.04 0.06 0.02 0.02 0.02 0.02 0.50 0.30 0.25 0. 0.51 0.30 0.25 0.60 1.19 0.87 0.75 1.51 0.98 0.76 0.69 1.37 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0. 0.01 0.01 0.01 0.01 0.61 0.51 0.46 0.66 0.74 0.64 0.59 0.77 0.55 0.51 0.49 0.79 0.45 0.44 0.43 0.69 0.02 0.03 0.03 0. 0.02 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0.00 0.00 0.01 0.64 0.44 0.38 0.74 0.67 0.48 0.42 0.76 2.14 1.38 1.16 3. 1.36 1.01 0.91 2.28 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.03 0.02 0.02 0.03 0.01 0.01 0.01 0.01 Table 162: Statistical results for Wiki using Llama-27b-chat-hf on irrelevant responses. Dataset Curve Cot Mean Absolute Difference (MAD) Early Middle Last All sQ sK sV CREAK sO rQ rK rV rO None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed None Simplified Detailed 7.78 1.60 0.61 8.44 1.61 0. 56.30 10.10 2.95 33.37 5.86 2.02 0.03 0.03 0.04 0.03 0.02 0.02 0.06 0.05 0.04 0.04 0.03 0. 5.74 1.16 0.43 7.41 1.52 0.48 20.20 2.93 1.02 15.48 2.53 1.01 0.05 0.02 0.02 0.02 0.02 0. 0.04 0.02 0.01 0.05 0.01 0.01 6.80 1.03 0.48 9.30 1.55 0.67 16.14 2.23 0.82 8.03 1.24 0. 0.12 0.03 0.04 0.07 0.03 0.04 0.07 0.04 0.03 0.12 0.02 0.01 6.54 1.23 0.49 8.17 1.50 0. 28.98 4.61 1.49 18.13 3.00 1.13 0.07 0.03 0.03 0.04 0.02 0.03 0.05 0.03 0.03 0.07 0.02 0. Table 160: Statistical results for CREAK using Llama2-7b-chat-hf on irrelevant responses."
        }
    ],
    "affiliations": [
        "University of Chicago",
        "University of Maryland"
    ]
}