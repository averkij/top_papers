{
    "paper_title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?",
    "authors": [
        "Martin Spitznagel",
        "Jan Vaillant",
        "Janis Keuper"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 3 3 5 0 . 3 0 5 2 : r PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations? Martin Spitznagel1, Jan Vaillant1,2, Janis Keuper1,3 1 Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany 2 Herrenknecht AG 3 University of Mannheim, Germany firstname.lastname@hs-offenburg.de Accepted at IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
        },
        {
            "title": "Abstract",
            "content": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code: http://www.physics-gen.org."
        },
        {
            "title": "1 Introduction",
            "content": "The numerical simulation of physical processes and relations is crucial tool in wide range of scientific and engineering tasks. Traditionally depending on the solution of various types of differential equations, research of physical simulation methods is increasingly shifting towards datadriven and learnable approaches [9]. In this work, we focus on narrow but practically important sub-field: the inference of physical relations from images. Extending Tenenbaums seminal idea of intuitive Figure 1: Overview of the physical problems, baseline generative models and their evaluation. A: We introduce three complex physical simulation tasks with 100k input-output image pairs each, providing ground truth simulations based on differential equations with varying complexity. B: We evaluate all tasks on independently trained image translation models; only results for the sound propagation task are visualized in this figure. C: While the evaluation of the baseline models shows general ability of generative image models to learn physical relations from images, we observe significant performance drops for tasks that require higher order term in the differential equations of their simulation. 1 physics engine [2], where machine learning models have been trained to predict coarse physical properties from images, we investigate the capabilities of modern generative models to learn detailed, complex and physically correct mappings from image pairs. Given such abilities, generative models like GANs [15] and Diffusion models [18] could be utilized to introduce hard to formalize but observable relations into physical models. Due to the fast inference times, generative models could also be used to accelerate simulations [32]. Introducing the PhysicsGen Benchmark. first step towards such data-driven models is thorough analysis of the capabilities of current generative approaches. Hence, we introduce novel benchmark that allows to systematically investigate the inference of diverse set of physical problems from images (as visualized in fig. 1): We introduce PhysicsGen, novel Benchmark for the evaluation of generative image models learning to infer physical simulations tasks from images. PhysicsGen provides Dataset with total of 300k image-pairs, representing three diverse physical simulation problems: i) iterative wave propagation (section 3), ii) closed form lens distortion (section 4) and iii) time series prediction of motion dynamics (section 5). We provide baseline evaluation that measures speedups and physical accuracy compared to full physical simulations across multiple generative imageThe evaluated models include to-image models. various generative image-to-image models: Generative Adversarial Networks (GANs) [16], specifically Pix2Pix [20]; an U-Net [30]; Convolutional Autoencoders (convAE) and Variational Autoencoders (VAEs) [25]. We also assess diffusion models, including Denoising Diffusion Probabilistic Models (DDPMs) [19], Stable Diffusion [29], and Denoising (see apDiffusion Bridge Models (DDBMs) [36]. pendix for details of the model architectures and training - the model architectures and training procedures are identical for all three tasks). Our Analysis of the baseline results shows potentially high speed-ups at good accuracies for simple 1st order simulation tasks. However, our experiments also show that current generative models have fundamental problems to learn higher order physical relations."
        },
        {
            "title": "1.1 Related Work.",
            "content": "The integration of learnable methods into physical modeling is wide field with rich body of literature which 2 interactions [26]. is very hard to capture in the limited space of this paper. In the following, we try to refer to the most important works in direct relation to our benchmark and some more general key concepts. We refer the reader to [32] and [9] for comprehensive overview of the field. Generative models for physical simulations. The integration of physical principles within generative models is an active and burgeoning area of research. Recent models such as PUGAN [11] and FEM-GAN [1] have demonstrated the potential of combining GANs with physical modeling to enhance performance in environments governed by complex physical laws. Advances in fluid dynamics and structural system identification using physics-guided GANs have shown significant improvements in efficiency and precision by incorporating physics-based loss functions and simulations [24, 34]. Predicting physical relations. Also, significant progress has been made in understanding physics through machine learning. Initial research demonstrated that models could comprehend the dynamics of block towers, progressing beyond simple memorization to genuinely understanding Practical applications have physical been explored, including fall detection using body part tracking through machine learning [27]. Additionally, advancements in generating physically plausible human animations highlight the potential of incorporating physical principles into machine learning frameworks [35]. Grey-Box Models. Physics-guided approaches in AI have laid solid foundation for incorporating physical laws into model training. Prior research has demonstrated the effectiveness of embedding physical constraints within AI models to enhance their performance and reliability [22, 10]. In generative models, diffusion models have shown promise but often face challenges when generating complex geometrical structures due to issues like multi-modality and noise sensitivity. The development of Geometric Bayesian Flow Networks addresses these challenges by enhancing the robustness and efficiency of molecule geometry modeling [31]. Physical Simulations for Reinforcement Learning. Deep reinforcement learning settings benefit from substantial availability of physics-based environments and datasets, which facilitate the training of models to understand and predict physical phenomena [13]. Resources such as OpenAI Gym, Robosuite, and PyBullet provide robust platforms for developing and testing reinforcement learning models in simulated physical settings [37, 17, 6]. However, in the domain of generative AI, there is notable scarcity of physics-informed datasets and environments, limiting the ability to train models on complex physical systems."
        },
        {
            "title": "2 Benchmark Tasks Overview",
            "content": "Selecting an appropriate set of tasks for the PhysicsGen benchmark is crucial to comprehensively evaluate the capabilities of generative image models in inferring complex physical relations. Given the vast diversity of possible physical simulation tasks, it is impractical to encompass the entire spectrum. Therefore, we strategically selected three distinct tasks based on the following criteria: Diversity of Physical Problems: To cover different types of physical phenomena, ensuring that models are tested across various domains. Variety of Numerical Solution Strategies: Incorporating both iterative and closed-form (non-iterative) solvers to assess models adaptability to different computational approaches. Simplicity of Evaluation Metrics: Facilitating straightforward and effective model performance evaluation. Accessibility to the ML Community: Choosing intuitively understandable tasks without requiring specialized physics knowledge, thereby making the benchmark widely applicable to the machine learning community. Based on these objectives, we introduce the following three simulation tasks within the PhysicsGen benchmark: Urban Sound Propagation: This task models the behavior of sound waves in urban environments, accounting for phenomena such as diffraction and reflections (section 3). By providing 100,000 input-output image pairs based on differential equation-based simulations, it challenges generative models to accurately capture intricate sound distribution patterns influenced by urban structures. Lens Distortion: Focusing on optical aberrations, this task utilizes the Brown-Conrady distortion model to simulate lens-induced geometric distortions in images (section 4). With 100,000 image pairs depicting varying degrees of distortion, models are tasked with learning precise, closedform physical relations that alter image geometry based on lens parameters. Dynamics of Rolling and Bouncing Movements: This task involves simulating the motion of rolling or bouncing ball on an inclined surface, incorporating both linear and rotational dynamics (section 5). Providing 100,000 image pairs that capture the balls position and rotation over time, it assesses models ability to predict time-series dynamics and handle higher-order physical relations inherent in motion equations. Collectively, these tasks offer diverse framework for assessing the strengths and limitations of generative models in learning and replicating complex physical simulations from image data. The subsequent sections delve into each task in detail, outlining dataset generation, underlying physics, evaluation metrics, and baseline model performances."
        },
        {
            "title": "3 Urban Sound Propagation",
            "content": "The simulation of wave propagation in complex scenes, including diffraction and reflection at multiple objects is representative task for wide range of physical problems which are typically solved via (expensive) iterative solving of higher order differential equations. Due to its intuitive setting and accessible data, we investigate wave propagation problems by concrete example where sound is propagated in urban environments. Dataset. We sampled 25k geo-locations across 5 cities, leveraging open-source geo-data from the Overpass API1 and processed it with GeoPandas [21] to represent urban environments in 500m2 area with buildings and open spaces encoded in black and white pixels, respectively. Our selection criteria ensured diverse urban scenarios by mandating minimum of 10 buildings within 200 meters and 50meter clearance from buildings to the sample point. We utilized the NoiseModelling v4 framework [4], adhering to the CNOSSOS-EU standards [23], to simulate sound propagation maps, placing sound receivers on grid with stepsize of 5 meters and at building boundaries, excluding indoor placements (see appendix B.3 for details). The resulting simulation output was then interpolated into 512x512 or 256x256 pixel maps, representing decibel levels on [0, 255] scale. Four simulation tasks (Baseline, Diffraction, Reflection, Combined) were defined to model sound propagation under varying conditions, accumulating in dataset of 100, 000 samples. The Baseline task models constant 95 dB noise source at 500 Hz, excluding diffraction and reflection. The Diffraction task adds horizontal sound wave diffraction, the Reflection task simulates sound reflections with standard absorption coefficient, and the Combined task introduces variable sound levels (60-115 dB) and environmental factors, incorporating both reflection and diffraction to mimic realistic urban sound behavior. Refer to fig. 2 and appendix B.6 for detailed description and visualization of the dataset generation pipeline. Physics of Sound Propagation. Mathematically, the propagation of sound over time is described via partial differential wave equations. Due to space constraints and the practical nature of our problem setting, we will neglect the derivation from continuous wave equations and directly focus on the discrete and iterative implementations of sound propagation which have been applied for our ground-truth simulations. Following [33], for discrete set of receivers 1http://overpass-api.de/api/map 3 Figure 2: The sampling pipeline for the sound propagation dataset utilizes the NoiseModelling framework [4] to generate sound propagation maps based on specific urban layouts. The generators are then trained to replicate these sound propagation patterns for given locations and source parameters. Predictions are evaluated by specifically analyzing errors in relation to the line of sight (see appendix B.1 for details). Table 1: Quantitative evaluation across all tasks for all architectures with batch size of 16 during inference. The complete results containing the Combined task are given in appendix B.7. Best overall results in bold, second best underlined. Model Task Base Simulation Base convAE Base VAE [25] Base UNet [30] Base Pix2Pix [20] DDPM [19] Base SD(w.CA) [29] Base Base SD Base DDBM [36] Dif. Simulation Dif. convAE Dif. VAE Dif. UNet Dif. Pix2Pix Dif. DDPM Dif. SD(w.CA) Dif. SD Dif. DDBM Refl. Simulation Refl. convAE Refl. VAE Refl. UNet Refl. Pix2Pix Refl. DDPM Refl. SD(w.CA) Refl. SD Refl. DDBM MAE Runtime/ wMAPE LoS NLoS LoS NLoS Sample (ms) 0.00 3.67 3.92 2.29 1.73 2.42 3.76 2.12 1.61 0.00 3.59 3.92 0.94 0.91 1.59 2.46 1.33 1.35 0.00 3.83 4.15 2.29 2.14 2.74 3.81 2.53 1.93 204700 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 206000 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 251000 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 0.00 67.13 75.58 37.57 6.75 51.08 35.18 32.46 65.24 0.00 32.09 32.57 22.36 18.06 20.30 31.23 24.45 23.56 0.00 93.54 92.47 80.46 30.67 80.38 81.61 55.27 79. 0.00 20.24 21.33 12.91 9.36 15.57 17.42 13.23 17.50 0.00 13.77 14.46 4.22 3.51 8.25 10.14 8.15 11.22 0.00 20.67 21.57 12.75 11.30 17.85 19.78 15.04 18.34 0.00 2.74 2.84 1.73 1.19 3.26 3.34 1.08 2.17 0.00 8.04 8.22 3.27 3.36 3.27 7.72 5.07 3.35 0.00 6.56 6.32 5.72 4.79 7.93 6.82 5.26 6.38 simulations in processing speed, achieving up to 20k factor improvement in runtime (see table 9 in the Appendix for further analysis). Regarding the prediction quality, we observe that models encountering tasks with reflections or diffractions show an increase in MAE for NLoS regions across all tested architectures. For the Pix2Pix model this rise in NLoS error is moderate, while this effect is more pronounced in the UNet and Diffusion models. Models without skip connections, such as VAE and ConvAE, tend to blur most of the image and struggle to capture any reflection patterns accurately. This trend highlights how reflections and diffractions introduce complex higher-order dependencies (see eq. (12) and appendix B.4) that complicate the image-to-image sound propagation tasks. Although the models successfully replicate sound reflection patterns vi4 R, the amplitude Lj puted via iterative differences: Lj = Lj Aj AdivRk Rk Rk atmRk of receiver Rk at frequency is comAj difRk (j, N), (1) difRk models diffraction. Note that the captures the denotes the atmospheric abW represents the source level, AdivRk atmRk where Lj geometrical spreading, Aj sorption, and Aj ground effect Aj Additionally, the model accounts for reflections by adjusting the power level Lj based on the absorption coefficient αvert of the surfaces involved. This adjustment is performed using the equation is neglected in our study. grdRk L(nref ) = L(nref 1) + nref 10 log10(1 αvert) (2) where nref indicates the number of reflections considered. Specular reflections are modeled using the image receiver method, which provides computationally efficient way to account for the angle of incidence being equal to the angle of reflection [33]. The sound level at each receiver reflects the cumulative effect of direct, diffracted, and reflected sound paths. As the number of reflections increases, the complexity of calculating the power level Lj also increases, growing in O(N 2), where denotes the number of reflective surfaces considered in the computation. Evaluation. Our baseline evaluation focuses on the ability of standard image-to-image generative models to accurately predict sound propagations. Prediction quality is determined by the pixel-wise difference between actual and predicted sound distributions. We use Mean Absolute Error (MAE) to quantify the average magnitude of prediction errors. Additionally, we introduce Weighted Mean Absolute Percentage Error (wMAPE) to specifically penalize errors in predictions that inaccurately show high sound amplitudes in regions expected to have low amplitudes, such as areas behind buildings. To evaluate model performance in capturing sound reflections or diffractions, metrics were specifically calculated for areas both within and outside the direct line of sight (LoS and NLoS) from the sound source. This methodology allows us to assess model predictions for sound propagations through direct paths as well as reflections and diffractions. Analysis. The results in table 1 show that generative models significantly outperform traditional sound propagation sually (see fig. 3), there exists significant gap between its visual outputs and the actual acoustic accuracy. This discrepancy suggests that while visually detailed, the models do not fully capture the complexities of sound physics."
        },
        {
            "title": "4 Lens Distortion",
            "content": "Our second benchmark is designed to investigate class of physical problems for which solutions can be computed directly via closed equations, rather than iterative solvers. We find representative and intuitive task in the modeling of optical lens distortions [7, 14], common problem in computer vision. Generative models are trained to simulate the appearance of faces distorted by lens effects, using the Brown-Conrady [12] model. These distortions are computed using OpenCV [5], providing ground truth for training and evaluation of generative models. Dataset. We leverage subset of the CelebA dataset [28], comprising 50,000 facial images, which were cropped to center the faces within 256x256 pixel frame. The resulting images where split into two distinct groups to isolate the effects of tangential distortion: one focusing on horizontal distortion and the other on vertical distortion. This allows us to specifically measure the impact of each type of distortion on the accuracy of the generative models (see fig. 4 and Appendix for more details). Physics of Lens Distortions. The Brown-Conrady model is widely adopted method to describe and correct lens distortions. It provides mathematical framework to represent both radial and tangential distortions through series of coefficients. For our study, we concentrate exclusively on the tangential distortion components, utilizing relevant coefficients to model the distortion characteristics of specific lenses. The mathematical representation of tangential distortion is expressed through the following equation, where xdist and ydist denote the distorted coordinates: r2 + 2x2(cid:17)(cid:21) (cid:20) 2p1xy + p2 xdist = + (cid:16) , ydist = + (cid:20) p1 (cid:16) r2 + 2y2(cid:17) (cid:21) + 2p2xy , (3) where r2 = x2 + y2 represents the squared radial distance from the image center, and p1 and p2 are the coefficients that model the tangential distortion effects. By strategically adjusting p1 and p2 and isolating their effects via controlled nullification, we can identify how each parameter individually contributes to the image distortion: (cid:40) xdist = + 2p1xy + p2(r2 + 2x2) if p2 = 0, if p1 = 0, ydist = (cid:40) + p1(r2 + 2y2) + 2p2xy if p2 = 0, if p1 = 0. (4) These conditional equations illustrate the isolated impact of each coefficient, revealing that the correction complexity for ydist escalates with the term p1(r2 + 2y2) when p1 is non-zero and p2 is zero. Conversely, the complexity of xdist correction intensifies with p2(r2 + 2x2) under the condition that p2 is non-zero and p1 is zero. Evaluation. Given the challenges of accurately capturing lens distortion through traditional pixel-based metrics, our evaluation strategy is specifically designed to focus on accurately reproducing designated landmarks in the images. We detect facial landmarks using 2D facial landmark detection method based on the Facial Alignment Network (FAN) [8], applied to the faces in the CelebA dataset (see Appendix for details). We use these landmarks to assess the geometric distortions introduced by lens settings, represented by the coefficients p1 and p2. We then compute the Figure 3: Qualitative results comparing the ground-truth simulation with the prediction for single sample within the reflection task. Additional results can be found in appendix B.7 Figure 4: Sampling and evaluation pipeline for the lens distortion dataset. The Brown-Conrady distortion model generates the true distorted images based on parameters p1 and p2 (we depict chess pattern for visualization). The conditioned generators are then trained to replicate these distortions for given images and parameters. The models are evaluated by comparing predicted against true facial landmarks, using 2D facial landmark detection based on the Facial Alignment Network (FAN) [8]. 5 Table 2: Quantitative X-Y error and shift comparison across different models for varying activation of parameters p1 and p2 with batch size of 16 during inference. Best overall results in bold, second best underlined. Model Comb Err Err X-Y Shift Runtime/ Sample (ms) Sim. convAE VAE UNet Pix2Pix DDPM SD(w.CA) SD Sim. convAE VAE UNet Pix2Pix DDPM SD(w.CA) SD 0.00 11.93 11.53 2.82 2.00 1.93 3.09 2.79 0.00 10.56 10.40 2.36 1.77 2.13 2.85 2.44 p1 = 0, p2 = 0 0.00 8.13 7.83 2.15 1.43 1.39 2.21 2.01 0.00 1.38 1.28 0.87 0.44 0.45 0.62 0.60 p1 = 0, p2 = 0.00 4.77 4.62 1.60 1.14 1.23 1.94 1.64 0.00 2.21 3.64 0.27 0.13 0.16 0.34 0.26 0.00 6.75 6.55 1.28 0.99 0.94 1.59 1.41 0.00 8.35 8.26 1.33 1.02 1.39 1.60 1.38 153.205 0.110 0.122 0.118 0.122 3970.603 2991.678 2997.576 153.205 0.110 0.122 0.117 0.123 3970.603 2991.678 2997. ically, we define the task to predict the position and rotation of ball along an inclined surface for defined time interval after the input image. Only the bouncing case is discussed below; however, all results for the rolling ball can be found in appendix D.3. Fg = = y = y + (r y) = m = sin(β) along axis = cos(β) along axis (5) (6) (7) Figure 6: Physical equations describing the bouncing ball movement divided in 3 parts: free fall (green), impact & bounce of the ball on the ground (red dot) and oblique throw with initial velocity from ball bounce (orange). Dataset. We generated our data set with Pymunk [3], Python physics simulation toolbox. For the bouncing case we use 50k pairs of training images (256x256px) and perform the evaluation of the generative networks with 1600 images. Three parameters on the input images are variable: the ground inclination, the start height of the ball move6 Figure 5: Qualitative visualization of lens distortion predictions using different generative models on CelebA dataset sample. p2 distortion: Original image at top-left; subsequent panels show UNet, Pix2Pix, Diffusion models. Red dots mark actual landmark positions and blue dots for predictions. Euclidean distance between the actual position of landmark in the true distorted image and its predicted position in the image generated by the models as quality metric. This measurement is performed separately for the horizontal (X) and vertical (Y) coordinates, resulting in Error and Error respectively. This approach not only allows us to gauge the accuracy of the models in simulating the specific effects of lens distortion on facial features but also helps in understanding how different distortion coefficients impact the positional accuracy of landmarks along each axis. Analysis. Our experiments with the CelebA dataset under varying conditions of p1 and p2 showcase different patterns in error rates that align with the theoretical complexity of the distortion model. As shown in table 2 and fig. 5, the generative models exhibit differing accuracies in landmark prediction under the influence of lens distortion parameters p1 and p2. Notably, the increased complexity in xdist and ydist equations under separate conditions (p1 nonzero vs. p2 nonzero) slightly correlates with heightened error rates in respective dimensions. The models exhibit higher Error when only p1 is active, indicative of the complex correction challenge introduced by the higher order dependency on y. Similarly, slight increase in error and slight decrease in error are observed when p2 is exclusively nonzero."
        },
        {
            "title": "5 Dynamics of bouncing movements",
            "content": "The third physical problem under investigation is the movement of rolling or bouncing ball. The aim here is to evaluate the ability of generative models to map kinematic movements of physics - task which should generalize well to the investigation of various time series problems. Specifment and the ball position. Additionally, the time interval between the input and target images is also variable. All other physical properties of the problem (balls mass or friction coefficients) are kept constant. multi-coloured structure with randomly distributed lines is superimposed on the simulation images in order to obtain greater diversity in the training data and is also used to analyze consistency, e.g. how well the generative networks can map fine multicoloured patterns (see fig. 7). The total time required to create sample using the available physics simulation CPU code (with customized background) is 3.8 seconds, while the inference times for the generative models remain consistent with those reported in table 1. Physics of bouncing balls. To model the physics behind the movement of bouncing ball, the motion is divided into different sections which are represented with different colors in fig. 6. First, the ball is released from defined height without any velocity and falls down according to the Newtons law of the free fall (green part) (see equation 5). The impact of the ball on the ground and its rebound can be described in simplified terms by spring-damper model. The aim of this second part of the movement (red dot) is to calculate the rebound speed and angle. This is done with equation 6, which contains the constant of the elements of the simplified model (see details in Appendix D.1) and the angle corresponds to the ground inclination for the first impact. Finally, the orange part of the movement is assumed to be an oblique throw with the initial velocity v0 calculated with equation 6. The equations of motion describing this oblique throw along the and axes are also derived from the Newtons law (see equation 7). When the ball hits the ground for the second time, the principle angle of incidence = angle of reflection is used to determine the angle of rebound θ and the next part of the motion is again defined by equations 6 and 7. Evaluation. The generated images are examined according to 4 precise error criteria: the ball position split in and directions, where the error is defined as the number of pixels between the theoretical and the predicted ball center. The second error measure is the ball rotation describing its angle in degrees during the movement due to the inclined ground surface. The next quality indicator of the algorithms is the out-of-roundness of the ball also measured in pixels by taking all identified points of the ball contour and calculating the difference between the theoretical ball radius of 15px and the predicted one. The standard deviation of all radius errors is taken as the error measure. Finally, the correct representation of the ground inclination is checked: the angular error between simulation and prediction is also measured in degrees. In addition, there are three general error criteria that give preciser indications of the correct representation of the physics which are explained with the complete results in the Appendix D. The results of the seven examined generative algorithms are summarized in table 3 (only for the four most important error criteria). The last column of the table contains the average number of non-evaluable predicted images over the four error criteria. detailed analysis of the advantages and disadvantages of the individual methods with all the evaluation parameters can be found in the Appendix D, as well as the evaluations obtained for individual runs for the rolling and bouncing case. Position Position Rotation Roundness Error 4.24 3.9 6.08 5.9 12.2 8.6 1.06 0.0 99% convAE 4.69 6.1 6.25 6.9 31.0 40 0.90 0.1 95% VAE 15.2 23 0.74 0.2 5.53 7.5 10.8 12 28% UNet 0.56 0.1 17.2 21 11.7 13 6.28 8.0 11% Pix2Pix 32.9 34 0.61 0.2 5.7% DDPM 7.91 9.0 15.5 14 61.1 52 24.8 23 7.3% 2% 34.2 38 16.2 14 SD(w.CA) 40.0 49 8.55 12 0.53 0.2 0.47 0. SD Table 3: Prediction results for the bouncing ball experiment in meanstd and mean percentage of non-evaluable predictions for the four criteria (best overall results in bold).Average runtime of the simulation: 3.8s, generative models: same as intable 1 Overall, the GAN (Pix2Pix model) provides the most stable representation of the bouncing ball problem. It achieves the best results across all evaluation criteria, with the most reliable physics predictions and fewest failed samples. The UNet also represents the ball position and angle on the generated samples very well, but produces more error images that cannot be evaluated with mostly either no ball or only shredded shape. The greatest weakness of this algorithm lies in the clean representation of the ball roundness. The auto-encoder approaches (VAE and convAE) produce very blurred images in which the ball is often distorted and hardly recognizable. Therefore, the mean error values are not very representative, as over 90% of the predictions cannot be analyzed. The denoising diffusion approach (DDPM) produces slightly poorer results in absolute terms. Especially the prediction of the ball rotation is worse, with an error approximately twice as high compared to the two other approaches. Finally, to test another type of diffusion, two variations of stable diffusion were compared. The one with crossattention is generating more error pictures and show high error values. In contrast, the normal stable diffusion performs very well: the mapping of the position is almost as accurate as with the Pix2Pix approach and the representation of the ball roundness is the best. In addition, the network generates very few error images, so that it could almost be placed on the same level as the GAN but comes as second due to poorer angle mapping. As can be seen in the qualitative result images below (figure 8), GAN and diffusion (especially the SD) networks depict small details much more accurately. The background structure of the images with the different colors is com7 Figure 7: The sampling and evaluation pipeline for the rolling and bouncing ball movements trajectories. physics-based simulates ball model is used to predict the balls movement. Evaluations focus on the accuracy of predictions related to key movement metrics, including bounce height and horizontal displacement (see appendix for details). cases. In general, it can be concluded that the errors of the analyzed approaches increase with the order and complexity of the physical formulation."
        },
        {
            "title": "6 Discussion",
            "content": "With PhysicsGen, we present comprehensive benchmark for the estimation of complex physical relations by image-to-image generative models. The three different datasets contained in PhysicsGen cover wide range of diverse simulation scenarios and hence allow systematic evaluation of learning based image generation models. Key insights from the baseline-evaluation. Despite the diversity of the three given physical problems and the fundamentally different network types used the base-line evaluation experiments, we can report several consistent findings: Due to the unavailability of optimized GPU code for the used simulations it is impossible to provide theoretically sound runtime comparison. Since highly parallel implementation of the simulation algorithms is highly non trivial (if possible at all), our analysis focuses on the runtime analysis of the generative models. However, replacing physical simulations with GPU powered generative models allows high computational speed-ups in practice, especially when replacing highly iterative solvers like in the urban sound propagation with Pix2Pix or UNets (where we report speed-up factors of up to 20k). In terms of physical correctness and consistency, our evaluation shows good results for simple 0th and 1st order problems, but also indicates that generative models mostly fail to predict more complex physical relations (which are typically formulated by higher order terms). This finding needs further investigation whether this is an effect of the principal theoretical limitations of current models or just lack of sufficient training data. We also observe distinct error pattern for different model types which represents how the different model types are dealing with complexity and uncertainty. Please refer to Appendix D.2.1 for examples in the following exemplary discussion for the bouncing Figure 8: Qualitative results for predictions of the bouncing ball problem. Initial State and True label image at top-left; subsequent panels show results for VAE, UNet, Pix2Pix and Diffusion models. pletely blurred by the UNet but not by the GAN and the diffusion. The DDPM predictions are blurred because the If the images have been scaled up from 64px to 256px. output of the diffusion model is set to 256px, the target image is sharp again but several or no balls and many more artefacts appear on the net predictions (see figure 25 in the Appendix). To summarize, the Pix2Pix and Stable Diffusion approaches also are visually performing the best. Finally, slight correlations can be observed between the simulations parameters and the generated errors. For example the ball position in the direction is predicted worse for all nets when the time interval between input and prediction images becomes larger. More details on the correlations of each method can be found in the Appendix D.2.1. Analysis. When comparing the result in table 3 (and tables 11, 13 in the Appendix) with the physical motion equations, it shows that the errors of the generative networks increase with higher order terms. The second-order terms, such as the movement along the X-axis and the ball angle represent the largest errors for the rolling scenario. In the bouncing case, second-order derivative is also present to describe the ball movement in the Y-direction due to the balls impact on the ground. Therefore the calculation rule along the Y-direction consists of zero, first and second derivative terms, making it more complex than the equation in the Xdirection. This is reflected in the error values of all the investigated approaches: the Y-error clearly exceeds that in the X-direction, as indicated by the complexity of the motion equations, although the X-position and angular errors remain roughly the same between the rolling and bouncing 8 ball problem (similar effects can be observed in Appendix B.7 for the sound propagation tasks as well): while GANs often produce physically possible outputs, they still can fail to generate the correct output for given inputs. This results in consistent scenes with complete ball at the wrong position. UNets, VAE and convAE on the other hand, often fail to produce consistent scenes and tend to blur uncertain predictions, resulting in deformed balls and destructed backgrounds. Lastly, diffusion models often produce multiple candidate solutions (multiple balls in one scene) and tend to fail to estimate more complex relations like ball rotation. This phenomenon also needs further investigation. Limitations. The provided run-time analysis is based on the comparison of practically available implementations and is not fair from theoretical perspective as no efforts were made to optimize the simulation codes to GPUs. Also, even though we carefully selected the three proposed physical benchmark problems such that they are likely to transfer to wide range of other simulation tasks, our evaluation provides only small subset of possible applications of generative models in physical modeling. Hence, results from our benchmark might not transfer to all possible scenarios. Broader Impact Statement. The simulation of physical systems forms the backbone for wide range of scientific and engineering tasks. Hence, it is important to have benchmarks for the emerging field of neural enhanced simulations to compare and validate these novel approaches. This work provides such benchmark for the specific, but widely applicable case, where physical simulations are used to map between 2D input and output structures (visualized as images). Our analysis shows that while current generative models are promising significant speedups, there are still many open questions regarding their physical correctness."
        },
        {
            "title": "Funding Acknowledgement",
            "content": "The authors acknowledge the financial support by the German Federal Ministry of Education and Research (BMBF) in the program Forschung an Fachhochschulen in Kooperation mit Unternehmen (FH-Kooperativ) within the joint project KI-Bohrer under grant 13FH525KX1."
        },
        {
            "title": "References",
            "content": "[1] A. Argilaga. Fem-gan: physics-supervised deep learning generative model for elastic porous materials. Materials, 2023. [2] P. W. Battaglia, J. B. Hamrick, and J. B. Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. [3] V. Blomqvist. Pymunk pythonic 2d physics library, 2007 - 2024. [4] E. Bocher, G. Guillaume, J. Picaut, G. Petit, and N. Fortin. Noisemodelling: An open source gis based ISPRS tool to produce environmental noise maps. International Journal of Geo-Information, 8(3):130, 2019. [5] G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. [6] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. [7] D. Brown. Close-range camera calibration. 1971. [8] A. Bulat and G. Tzimiropoulos. How far are we from solving the 2d and 3d face alignment problem? (and dataset of 230,000 3d facial landmarks). In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, Oct. 2017. [9] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborova. Machine learning and the physical sciences. Reviews of Modern Physics, 91(4):045002, 2019. [10] Y. Chen, Q. Wu, Y. Sui, Y. Wang, and S.-C. Zhu. Physically grounded vision-language models for robotic arXiv preprint arXiv:2309.02561, manipulation. 2023. [11] R. Cong, W. Yang, C. Zhang, Weiand Li, C. Guo, and S. Huang, Qingmingand Kwong. Pugan: Physical model-guided underwater image enhancement using gan with dual-discriminators. IEEE Transactions on Image Processing, 2023. [12] A. E. Conrady. Decentred Lens-Systems. Monthly Notices of the Royal Astronomical Society, 79(5):384 390, 03 1919. [13] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control, 2016. [14] W. Faig. Calibration of close-range photogrammetric systems: Mathematical formulation. Photogrammetric Engineering and Remote Sensing, 41, 1975. [15] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks, 2014. 9 [16] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks, 2014. [27] W. Li, S. Azimi, A. Leonardis, and M. Fritz. To fall or not to fall: visual approach to physical stability prediction, 2016. [17] K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam, F. Golemo, C. Herrmann, T. Kipf, A. Kundu, D. Lagun, I. Laradji, Hsueh-Ti, Liu, H. Meyer, Y. Miao, D. Nowrouzezahrai, C. Oztireli, E. Pot, N. Radwan, D. Rebain, S. Sabour, M. S. M. Sajjadi, M. Sela, V. Sitzmann, A. Stone, D. Sun, S. Vora, Z. Wang, T. Wu, K. M. Yi, F. Zhong, and A. Tagliasacchi. Kubric: scalable dataset generator, 2022. [18] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models, 2020. [19] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. CoRR, abs/2006.11239, 2020. [28] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015. [29] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2022. [30] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. [31] Y. Song, J. Gong, Y. Qu, H. Zhou, M. Zheng, J. Liu, and W.-Y. Ma. Unified generative modeling of 3d molecules via bayesian flow networks, 2024. [20] P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Imageto-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016. [32] N. Thuerey, P. Holl, M. Mueller, P. Schnell, F. Trost, and K. Um. Physics-based Deep Learning. WWW, 2021. [33] M. Vorlander. Auralization: Fundamentals of acoustics, modelling, simulation, algorithms and acoustic virtual reality. Springer Science & Business Media, 2007. [34] Y. Yu and Y. Liu. Physics-guided generative adversarial network for probabilistic structural system Expert Systems with Applications, identification. 239:122339, 2024. [35] Y. Zhang, W. Li, Y. Gao, and S. Li. Physdiff: Physicsguided human motion diffusion model. arXiv preprint arXiv:2106.03704, 2021. [36] L. Zhou, A. Lou, S. Khanna, and S. Ermon. Denoising diffusion bridge models, 2023. [37] Y. Zhu, J. Wong, A. Mandlekar, R. Martın-Martın, A. Joshi, S. Nasiriany, and Y. Zhu. robosuite: modular simulation framework and benchmark for robot learning, 2022. [21] K. Jordahl, J. V. den Bossche, M. Fleischmann, J. Wasserman, J. McBride, J. Gerard, J. Tratner, M. Perry, A. G. Badaracco, C. Farmer, G. A. Hjelle, A. D. Snow, M. Cochran, S. Gillies, L. Culbertson, M. Bartos, N. Eubank, maxalbert, A. Bilogur, S. Rey, C. Ren, D. Arribas-Bel, L. Wasser, L. J. Wolf, M. Journois, J. Wilson, A. Greenhall, C. Holdgraf, Filipe, and F. Leblanc. geopandas/geopandas: v0.8.1, July 2020. [22] A. Karpatne, I. Ebert-Uphoff, S. Ravela, H. A. Babaie, and V. Kumar. Physics-guided neural networks (pgnn): An application in lake temperature modeling. arXiv preprint arXiv:2212.02500, 2022. [23] S. Kephalopoulos, M. Paviotti, and F. AnfossoLedee. Common noise assessment methods in europe (cnossos-eu). Technical Report EUR 25379 EN, Publications Office of the European Union, Luxembourg (Luxembourg), 2012. JRC72550. [24] B. Kim, V. C. Azevedo, N. Thurey, T. Kim, M. Gross, and B. Solenthaler. Deep fluids: generative network for parameterized fluid simulations. Computer Graphics Forum, 38, 2018. [25] D. P. Kingma and M. Welling. Auto-encoding variational bayes, 2022. [26] A. Lerer, S. Gross, and R. Fergus. Learning physical intuition of block towers by example, 2016. 10 PhysicsGen Supplementary Material Section D: Ball Appendix This Appendix provides additional details and supplementary material supporting the main content of the paper. Below are the sections included in this Appendix: Section A: Training Setup Provides detailed information about the configurations and parameters used during the training phase of the models. Section B: Sound Propagation Appendix B.1 Evaluation Metrics: Outlines the criteria and methods used to assess the performance and accuracy of sound propagation models within the simulation framework. B.2 Location Sampling: Describes the methods used to select and sample various urban locations. B.3 Receiver Placement: Details the strategies for placing receivers in the simulation environment. B.4 Sound Physics: Description of the physics behind the sound propagation. B.5 Runtime Analysis: Runtime Analysis of sound propagation tasks vs. simulation framework. B.6 Scalable Simulation Pipeline: Discusses the scalable simulation pipeline developed for processing sound propagation data. B.7 Additional Qualitative Results: Presents additional qualitative results, showcasing further analyses, visualizations, and interpretations of the sound propagation task. Section C: Lens Appendix C.1 Evaluation Metrics for Facial Landmark Detection: Outlines the metrics such as Euclidean distance and mean absolute errors to evaluate facial landmark detection accuracy. C.2 Lens Physics: Explains the mathematical models for simulating lens distortions, focusing on radial and tangential components. C.3 Lens Distortion Application Pipeline: Outlines the process and tools used to apply lens distortions to dataset images, highlighting the use of Python and OpenCV. C.4 Additional Qualitative Results: Visual results, comparing the models landmark predictions against actual distorted landmarks to assess accuracy. D.1 Ball physics and kinematics: Description of the physics behind the rolling and bouncing movement of the ball D.2 Bouncing ball: Further evaluation and analysis of the results of the three generative approaches for the bouncing case. Here the detailed result tables, some more predictions samples and some typical errors of the network can be found. D.3 Rolling ball: Further evaluation and analysis of the results of the three generative approaches for the rolling case. Here the detailed result tables, some more predictions samples and some typical errors of the network can be found. Datacard D.3.2."
        },
        {
            "title": "A Training Setup",
            "content": "This appendix provides detailed overview of the architecture, input specifications, hyperparameters, and computer resources used for the generative models utilized in this research. Each experiment was conducted on workstation equipped with single NVIDIA RTX 4090 GPU (24 GB VRAM) unless specified otherwise. The U-Net model architecture, adopted from [30], and the Pix2Pix setup, based on [20], are designed to process either grayscale or RGB image inputs. When necessary, the input is extended by appending additional parameters as separate dimension. We followed the methodology described in [19] for the Diffusion model while incorporating conditional inputs as separate dimensions alongside the noised input image. Each model was constructed upon unified U-Net backbone, scaling from 64 to 1028 channels and reconverging to 64, ensuring consistency in model design across all datasets. During training, different loss functions were employed to suit each models architecture: Mean Squared Error (MSE) loss for the U-Net and Diffusion models, and combination of Binary Cross Entropy (BCE) loss and L1 loss for the GAN. These architectures were applied consistently across all tasks introduced in this paper, with each task trained and evaluated independently. The ConvAE and VAE [25] models are both based on the UNet architecture, similar to Pix2Pix and the standard U-Net, but adapted for their respective tasks. The ConvAE uses the U-Net structure with skip connections disabled. The VAE employs similar hierarchical design. The Stable Diffusion models were trained following the training procedure outlined in [29], with variations in conditioning mechanisms. In the standard Stable Diffusion (SD) setup, the conditioned image and parameters were passed to 11 Table 4: UNet Training Hyperparameter Hyperparameter Value Batchsize Learning Rate Epochs Optimizer Adam Betas 18 1 104 50 Adam (0.5, 0.999) Table 5: Pix2Pix Training Hyperparameter Hyperparameter Batchsize Learning Rate Discriminator Learning Rate Generator Epochs L1 Lambda Lambda GP Optimizer Adam Betas Value 18 1 104 2 104 50 100 10 Adam (0.5, 0.999) the denoising model via cross-attention and as additional dimensions with the noised input image. In contrast, the Stable Diffusion with Cross Attention Only (SD wCA) model exclusively employed cross-attention for passing the conditioned image and parameters. The DDBMs were trained following the exact training procedure outlined in [36], ensuring adherence to the methods described in the original work. DDBMs were trained on four NVIDIA A100 GPUs (80 GB VRAM each) to accommodate the increased computational demand, while Stable Diffusion was trained on single A100 GPU. All other models, including U-Net, Pix2Pix, and standard Diffusion models, were trained on single NVIDIA RTX 4090 GPU. Table 6: DDPM Training Hyperparameter Hyperparameter Value Batchsize Learning Rate Epochs Noise Steps Optimizer Adam Betas 18 1 104 50 1000 Adam (0.5, 0.999) 12 Table 7: StableDiffusion Training Hyperparameters Hyperparameter Number of Timesteps Beta Start Beta End Down Channels Mid Channels Number of Heads LDM Batch Size Autoencoder Batch Size Discriminator Start LDM Learning Rate Autoencoder Learning Rate Codebook Weight Commitment Beta Perceptual Weight KL Weight LDM Epochs Value 1000 0.0015 0.0195 [256, 384, 512, 768] [768, 512] 16 16 4 5000 1 104 1 105 1 0.2 1 0.000005 Table 8: DDBM Training Hyperparameters Hyperparameter σmax σmin σdata Covariance (CXY ) Number of Channels Attention Levels Number of Residual Blocks Sampler Attention Type Learning Rate Dropout EMA Rate Number of Head Channels Value 80.0 0.002 0.5 0 256 [32, 16, 8] 2 real-uniform flash 0.0001 0.1 0."
        },
        {
            "title": "B Sound Propagation Appendix",
            "content": "B.1 Evaluation Metrics This appendix section details the evaluation metrics used, focusing on the Weighted Mean Absolute Percentage Error (wMAPE) and the implementation of ray tracing to determine line-of-sight (LoS) conditions between sound source and various points on sound propagation map. Weighted Mean Absolute Percentage Error: The calculation of the wMAPE is adjusted to address cases where the true value of the noise map is zero but the predicted value is non-zero. In such instances, the script sets the error for these specific pixels to 100%. This method accurately quantifies the total error, especially when the model predicts sound in areas where sound waves could not realistically reach according to the true data. Line of Sight: Ray tracing is employed to establish LoS conditions between sound source and various points on sound propagation map. The process initiates by establishing grid that represents the mapped area, with the sound source positioned at the center. For each point on this grid, direct line is drawn from the source to the point. The script then checks for any obstructions along this line. The ray-tracing function progresses by incrementally moving along the line from the source to the target point. It checks if any part of the line intersects with obstacles, represented by zero values on binary image map. If an obstruction is encountered before the line reaches the target point, that point is marked as not having line of sight to the source; otherwise, it is considered to have line of sight. B.2 Location Sampling In order to conduct sound propagation studies that are reflective of diverse urban environments, our location sampling was designed with specific criteria to ensure balanced and comprehensive dataset. The location sampling methodology for sound propagation studies required each selected location to contain at least ten buildings within 200-meter radius of the designated sound source. Additionally, to avoid anomalous acoustic results and to simulate realistic setting where sound sources are typically not positioned directly against structures, no buildings were allowed within 50-meter radius of the sound source. Figure 9 illustrates the urban areas selected for this study, showcasing the distribution of buildings relative to the sound source in the center. Locations were randomly sampled across 5 cities, providing broad geographical spread and variety of urban layouts. The locations sampled include: Hamburg, Hannover, Augsburg, Bonn and Munich. Figure 9: Satellite image of sampled location for sound propagation studies. Buildings are marked in red, illustrating the distribution within the area. The blue circle represents the 200-meter radius within which at least ten buildings are required, while the green circle indicates the 50-meter radius that must be free of any buildings to ensure clear propagation path from the sound source. For each city, 5,000 unique locations were selected, contributing to total of 25,000 data points for the study. The dataset was then divided into training, evaluation, and testing sets with split of 80%, 15%, and 5%. B.3 Receiver Placement The receiver placement for our sound propagation simulations is critical component that directly influences the accuracy and relevance of our results. To achieve an optimal setup, we utilized the NoiseModelling framework [4]. Our specific placement strategy required combination of precision and broad coverage, which was not fully supported by any single existing script within the framework. Therefore, we integrated two available scripts and further developed custom WPS (Web Processing Service) to meet our specific needs. The first script we used was Regular Grid, which calculates regular grid of receivers. This script uses single geometry or table of geometries to generate receivers evenly spaced by specified distance (delta) on the Cartesian plane, measured in meters. This method ensures systematic and uniform coverage across the studied area, providing comprehensive baseline for sound propagation assessments. The second script, Building Grid, is designed to place receivers specifically around building facades. This script generates receiver points approximately 2 meters from the building facades at given height, facilitating detailed analysis of sound interactions with building surfaces, which are critical for urban acoustic modeling. By combining these two approaches, we crafted receiver placement strategy that not only maintains uniform grid for broad coverage but also includes strategically 13 (a) Sattelite image (b) Receiver grid (c) Urban layout Figure 10: Starting with the selection of 500 m2 area (a), buildings are identified, followed by placing receiver grid (b). The urban layout (c) is then used for creating samples for the dataset. placed receivers around building facades and edges. This hybrid approach ensures that our simulations accurately reflect sound propagation both across open areas and in close proximity to structural barriers, thereby enhancing the precision of the simulation results at key locations. B.4 Sound Physics Following [33], for discrete set of receivers R, the amof receiver Rk at frequency is computed via plitude Lj Rk iterative differences: Lj Rk where = Lj AdivRk Aj Aj difRk grdRk Aj atmRk Lj AdivRk Aj atmRk Aj difRk models the source, captures the geometrical spreading, represents the atmospheric absorption, models diffraction, Aj grdRk the ground effect - which is neglected in our study. Several environmental factors influence the sound level at receiver: Geometrical Spreading: This factor accounts for the dispersion of sound waves as they propagate through the medium. The decrease in sound intensity due to geometrical spreading is given by: Table 9: Model vs. Simulation Performance Comparison for Single Sample Processing. The complex source is single test sample for more complex source with 28 descriptive sound signal sources for the simulation. This illustrates how the processing time increases significantly with more complex signal sources. It is important to note that this analysis may not provide completely fair assessment from theoretical perspective, as no efforts were made to optimize the simulation codes for GPU execution. Model - Condition convAE UNet Pix2Pix DDPM SD(w.CA) SD DDBM Simulation - Baseline Simulation - Diffraction Simulation - Reflection Simulation - Combined Simulation - Combined - 3rd Order Reflections Simulation - Combined - Complex Source Mean Runtime (ms) 0.128 0.126 0.138 3986.353 2961.027 2970.86 3732.21 20471.7 20602.7 25097.3 29239.5 186229. 540000 (8) where αair is the atmospheric absorption coefficient. Diffraction: Sound waves can bend around obstacles, phenomenon modeled by the diffraction factor: Aj difRk = (cid:40) 10 log10(3 + 40 0 λ δ) λ δ if 40 otherwise (11) where λ is the wavelength of the sound, is the diffraction coefficient, and δ is the path length difference expressed in between for direct and diffracted paths. Reflection Adjustments: Reflections off various surfaces can significantly modify the sound level. This factor adjusts the sound power level based on the number of reflections and the properties of the reflective surfaces: L(nref ) = L(nref 1) + nref 10 log10(1 αvert) (12) where nref is the number of reflections and αvert is the absorption coefficient of the reflecting surfaces. AdivRk = 20 log10(di) + 11 (9) B.5 Runtime Analysis where di is the distance between the sound source and the receiver. Atmospheric Absorption: This factor represents the loss of sound energy as it travels through the atmosphere, influenced by the atmospheric conditions such as humidity and temperature. It is calculated as follows: The runtime analysis, as detailed in Table 9, highlights the performance comparison between various models and simulation approaches for single sample processing. B.6 Scalable Simulation Pipeline Aj atmRk = αair di (10) The dataset generation pipeline visualized in Figure 11 is crucial component of our study, developed to effi14 ciently process sound propagation data in diverse urban settings. Utilizing the NoiseModelling framework [4], we have automated the data input and simulation processes within Docker-containerized environment. The pipeline commences with the automatic download of 500m² area map from OpenStreetMap for each location using the Overpass API, followed by their import into the NoiseModelling framework alongside the signal source. Figure 11: Detailed visualization of the dataset generation pipeline. Considering the computational intensity of this process, with an average duration of 30 seconds per sample, our pipeline is structured for scalability. It operates on Kubernetes cluster with 40 pods, enabling us to complete the generation of the entire dataset, encompassing 25,000 data points for each complexity level, in approximately 20 hours. B.7 Additional Qualitative Results Figure 13 visualizes the error patterns for Pix2Pix, Unet and DDPM, illustrating behavior that is similar across all the architectures evaluated. The Unet model tends to blur areas where it is uncertain about the sound propagation results. Instead of accurately replicating the complex reflection patterns found in the dataset, the GAN attempts to fill these uncertain areas with simplified, repetitive texture that does not match the true reflection dynamics. In contrast, the diffusion model attempts more direct replication of the reflection patterns visible in the training data. However, the error patterns observed with diffusion models, including DDPM, SD, and DDBMs, are highly similar. These models tend to overcompensate, introducing reflection patterns into areas where they are not physically plausible. This results in noisy and sometimes unrealistic simulation output, particularly in regions where sound reflections should be minimal or absent based on the physical layout. Figure 12: Comparison of true labels for baseline, reflection, diffraction, and combined tasks for two samples to visualize their differences. Figure 13: Visualization of unique error patterns for Pix2Pix, UNet, and DDPM models in sound propagation simulations. Each models approach to uncertain areas and its replication of reflection patterns are depicted. Figure 14: Comparing the output of the physical simulation with the predictions for single sample within the baseline task dataset. 15 Table 10: Quantitative evaluation across all tasks for all architectures with batch size of 16 during inference. Best overall results in bold, second best underlined. Model Task Base Simulation Base convAE Base VAE [25] Base UNet [30] Base Pix2Pix [20] DDPM [19] Base SD(w.CA) [29] Base Base SD Base DDBM [36] Dif. Simulation Dif. convAE Dif. VAE Dif. UNet Dif. Pix2Pix Dif. DDPM Dif. SD(w.CA) Dif. SD Dif. DDBM Refl. Simulation Refl. convAE Refl. VAE Refl. UNet Refl. Pix2Pix Refl. DDPM Refl. SD(w.CA) Refl. SD Refl. DDBM Comb. Simulation Comb. convAE Comb. VAE Comb. UNet Comb. Pix2Pix Comb. DDPM MAE Runtime/ wMAPE LoS NLoS LoS NLoS Sample (ms) 0.00 3.67 3.92 2.29 1.73 2.42 3.76 2.12 1.61 0.00 3.59 3.92 0.94 0.91 1.59 2.46 1.33 1.35 0.00 3.83 4.15 2.29 2.14 2.74 3.81 2.53 1.93 0.00 2.93 3.08 1.39 1.37 1.26 204700 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 206000 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 251000 0.128 0.124 0.138 0.138 3986.353 2961.027 2970.86 3732.21 251000 0.128 0.124 0.138 0.138 3986.353 0.00 67.13 75.58 37.57 6.75 51.08 35.18 32.46 65.24 0.00 32.09 32.57 22.36 18.06 20.30 31.23 24.45 23.56 0.00 93.54 92.47 80.46 30.67 80.38 81.61 55.27 79.13 0.00 48.92 50.24 45.15 40.68 40.38 0.00 20.24 21.33 12.91 9.36 15.57 17.42 13.23 17.50 0.00 13.77 14.46 4.22 3.51 8.25 10.14 8.15 11.22 0.00 20.67 21.57 12.75 11.30 17.85 19.78 15.04 18.34 0.00 18.61 19.59 10.10 9.80 13.07 0.00 2.74 2.84 1.73 1.19 3.26 3.34 1.08 2.17 0.00 8.04 8.22 3.27 3.36 3.27 7.72 5.07 3.35 0.00 6.56 6.32 5.72 4.79 7.93 6.82 5.26 6.38 0.00 5.19 5.35 2.63 2.67 2.21 Figure 16: Comparing the output of the physical simulation with the predictions for single sample within the reflection task dataset. (a) UNet (b) Pix2Pix (c) SD (d) DDBM Figure 15: Comparing the output of the physical simulation with the predictions for single sample within the diffraction task dataset. Figure 17: Comparing the output of the physical simulation with the prediction of UNet (a), Pix2Pix (b), stable diffusion model (c) and DDBM (d) for single sample within the reflection task dataset, distinguishing between the MAE in LoS and NLoS."
        },
        {
            "title": "C Lens Appendix",
            "content": "C.3 Lens Distortion Pipeline This appendix details the methodology for applying lens distortions to images within our dataset, using computational pipeline built on Python and OpenCV [5]. The process leverages pre-defined set of distortion parameters stored in CSV file, including tangential coefficients, to systematically alter each image. The dataset consists of 100k images and was divided into training, evaluation, and testing sets with split of 80%, 15%, and 5%. Distortion Computation: Each image is processed sequentially. The script calculates the distortion for each pixel using the specified parameters (radial coefficients k1, k2, k3 and tangential coefficients p1, p2) and the camera calibration data (focal lengths x, and principal point coordinates cx, cy). These calculations transform the original pixel coordinates to their new distorted positions. Image Remapping: Using OpenCVs remap function, the distorted pixel coordinates are mapped back onto the original image, creating the visually distorted output. This process ensures that each pixels new location reflects the simulated lens. C.4 Additional Qualitative Results This section provides visual evaluation of the generative models performance on the lens distortion task, showcasing some examples of model predictions and landmark detection. C.1 Evaluation Metrics for Facial Landmark Detection This appendix section details the evaluation metrics used to assess the accuracy of facial landmark detection, focusing on the Euclidean distance and mean absolute error calculations for both combined and separate and coordinates. Euclidean Distance: The primary metric for evaluating the accuracy of facial landmark predictions is the Euclidean distance between predicted and true landmarks. This measure computes the root mean square error across all landmarks, providing comprehensive gauge of overall prediction accuracy. Mean Absolute Error for and Coordinates: Additionally, the MAE for and coordinates is calculated separately. This breakdown allows for detailed analysis of the models performance along each axis, highlighting any directional biases or discrepancies in landmark prediction. These metrics provide dual approach to evaluating landmark detection performanceoffering both holistic measure via Euclidean distance and directional sensitivity via separate MAE calculations. C.2 Lens Physics The Brown-Conrady model is used in image processing to correct lens distortions by modeling both radial and tangential components [12]. While the paper focuses on the tangential aspects of distortion, here we provide detailed mathematical formulation of the entire BrownConrady model, including both radial and tangential distortion corrections. Radial distortion is typically dominant in optical systems and is characterized by its impact varying with the square of the distance from the optical center. This effect is modeled using three coefficients: k1, k2, and k3, which adjust the coordinates as function of radial distance r2 = x2 + y2. The full expressions for the distorted coordinates incorporating both radial and tangential distortions are as follows: xdist = x(1 + k1r2 + k2r4 + k3r6) r2 + 2x2(cid:17)(cid:21) (cid:20) 2p1xy + p2 +x + (cid:16) , ydist = y(1 + k1r2 + k2r4 + k3r6) (cid:21) r2 + 2y2(cid:17) + 2p2xy (cid:20) p1 +y + (cid:16) . (13) (14) (15) (16) These equations describe how each image point (x, y) is displaced to (xdist, ydist) due to lens distortions. Figure 18: Comparison of simulation data with predicted landmarks for single sample. The left image shows the input, while the right image overlays the predicted (blue dots) and true (red dots) landmark positions on the model output. For clarity, only every second landmark is visualized. Figure 19: Comparison of simulation data with predicted landmarks for single sample. The left image shows the input, while the right image overlays the predicted (blue dots) and true (red dots) landmark positions on the model output. For clarity, only every second landmark is visualized. 18 Figure 20: Comparing the input and output of the simulation with the predictions from Pix2Pix and DDPM for two random samples within the lens distortion task dataset. Green dots represent detected landmarks in the label image, red dots indicate the actual landmarks post-lens distortion, and blue dots denote the predicted landmarks by the models."
        },
        {
            "title": "D Ball Appendix",
            "content": "D.1 Ball physics and kinematics Rolling ball. The kinematic problem of the rolling ball can be relatively simply described using Newtons law and the angular momentum balance equation. The equations of motion for the and directions are derived from the force balance acting on the ball, where no movement occurs in the direction (see figure 21): Fall = with Fall = FG + FN + FHR Equation of motion along x: sin (β) FHR = (17) Equation of motion along (no movement): FN cos (β) = 0 (18) To describe the rotation and the ball angle, the angular momentum balance from rotational mechanics can be used: FHR = α > 0 FHR = φ > (19) In the description of this first simple case, the ball position along the X-axis and the ball angle are derived twice. Figure 22: Splitting the movement of the bouncing ball Subsequently, the contact with the ground (red point on figure 22) can be represented using simple spring-damper model (see figure 23). It is assumed that the Pymunk [3] physics simulation used calculates similarly, as an elasticity factor can be defined in the settings and not all calculations of the physics engine could be retraced. To be able to describe the whole ball movement, this section mainly focuses on determining the impact velocity of the ball. This is where the weight force, the spring force and the damping force with their respective constants (ball mass), (spring stiffness) and (damping constant) occur. Fg + Fc + Fd = y + (r y) = g (22) Figure 21: Forces overview for the rolling ball Bouncing ball. The movement of the bouncing ball is much more complex to describe than the rolling case. For better overview, the movement is divided into different sections (see different colors in figure 22). First, the ball is released from defined height without any initial velocity (green area). Until it hits the inclined ground surface, it is in free fall. Derived from Newtons law, the following equations applies: Fg = = y = (t) = y0 t2 (20) (21) 20 Figure 23: Spring-damper model to simulate the ball impact and bounce on the ground After the impact, the next motion section (the orange part in figure 22) is assumed to be an oblique throw with the initial velocity v0, which was calculated using equation 22 above. Additionally, the principle angle of incidence = angle of reflection is used to determine the angle of rebound θ from the ball; this corresponds to the ground slope β after the first ground contact. The following equations of motion for and are derived from Newtons law: = sin (β) = cos (β) (23) (24) The equations can also be expressed as functions of time by integrating them. v0 is the initial velocity of this third phase of motion, which was calculated above in the second part of the movement (impact on the ground). = = sin (β) 2 cos (β) 2 t2 + v0 sin (θ) + x0 t2 + v0 cos (θ) + y0 (25) (26) When the ball hits the ground for the second time after the oblique throw, the orange motion phase ends and the purple phase begins. In between, there is another bounce, where equation 22 helps to determine the second rebound velocity. The further movement is then described by the above equations 23 and 24. This requires the impact angle α, which can be calculated at the end of the orange phase as follows: tan (α) = vy vx = tAuf tref + vy,0 vx, (27) tAuf tref can be calculated using the equation of motion in the y-direction by setting it to zero. Finally the entire ball movement can be described step by step using the equations listed above. The rotation of the ball is determined as for the rolling case, by forming the angular momentum balance for each bounce, taking into account the friction force of the ground. In the bouncing case, all degrees of freedom (movements in the and directions, as well as the ball rotation) contain second-order terms. In addition, it becomes evident through the more complex equation 22 of the ball impact that the motion along the Y-axis is more complex here than along the ground. D.2 Bouncing ball D.2.1 Further evaluation and results Another result for the bouncing ball is shown in figure 24. Here it can be seen that the ball position varies slightly depending on the algorithm. The biggest error is made by the diffusion network along the vertical: the ball is drawn about 9px too low. In the table 11, the average results for all evaluation criteria and the three AI methods are shown. In order to supplement the main section with few more details on the respective AI networks, their strengths and weaknesses are now summarised. few typical error patterns are also listed for each network. 21 Figure 24: Predictions of the generative AI compared to the physics simulation for the rolling ball Pix2Pix (GAN architecture): Most stable mapping of the bouncing ball problem: best results achieved relative to the number of errors Most error cases for the ball rotation: 15% of the generated images are not interpretable regarding the error metric. For the valid 85%, there is an average error of 17,2 with standard deviation of 20,8, reflecting the high variance in the results. Only one ball appears on the generated images in 93% of cases, while in the remaining 7%, no proper ball is represented. In these cases, either no ball is present on the image or the depicted shape is too fragmented and has no roundness at all. The network most frequently places the target ball in the X-direction (i.e. along the ground slope) behind the start ball. In fact, the generated image must always show the ball with larger X-coordinate than on the input image, otherwise the correctness of the physics is not given. Only in 1% of the cases the target ball is in front of the start ball; otherwise all valid predictions correctly represent the rolling along the inclined surface. Good representation of the background: the line structures are most accurately represented for the horizontal and vertical lines. The diagonal segments are usually only partially drawn. Correlations between error and simulation parameters: Position error in the X-direction increases the most with larger time interval between input image and prediction, followed by an increasing ground inclination. Position error in the Y-direction: slight correlation is visible here between increasing start height of the ball and increasing error, although the ground inclination and the time interval have almost the same influence. Ball angle deteriorates when the time interval between the start and target images increases. No real correlation is observed for the roundness error. UNet: General: slightly better results in position and rotation compared to the Pix2Pix, if only the successfully analysable images are considered. Accuracy in the Xdirection is about 1px better and rotation about 2 better. Algorithm stability: much more unstable: 20% to 35% of the predictions are not evaluable. Many images are generated that contain no ball or shape that does not approximate ball. Target ball placement: the network also places very well the target ball behind the start ball in terms of the X-coordinate. All valid result images comply with the physical correctness of rolling along the ground slope. Where this is not fulfilled, the images are not evaluable. Background depiction: blurry representation of the background. As seen in the result images, both the ground and the sky are depicted almost monotonously. The UNet seems unable to represent fine line structures; only very attenuated vertical lines can be guessed. Similarly, the different colours of the lines are not shown: the ground appears uniformly gray and the sky light green. Correlations between error and simulation parameters: Position error in the X-direction increases most with larger time interval, followed by an increasing ground inclination (similar to the Pix2Pix) Position error in Y-direction increases mainly with greater starting height of the ball, although the correlation remains relatively weak (as with the Pix2Pix) Ball angle deteriorates when the time interval between the start and target image increases Diffusion: Three different models were examined under the category of diffusion: the DDPM, stable diffusion with cross-attention (SDw.CA) and without (SD). In general, the results of the DDPM and SD are very similar to each other, with the SD showing little more stability; the SDw.CA performs significantly worse except for the mapping of ball roundness, which is very good. For the DDPM network different approaches and settings were tested because the results of the first runs with 256px images were very poor. On one image, several balls appeared at randomly distributed positions or the prediction was noisy and contained artefacts (see figure 25). Since no Figure 25: Typical predictions for the DDPM model trained with 128 and 256px images approach or parameter set could be found to achieve better predictions with this algorithm, the image size was progressively reduced. This had very positive influence on the training, making the result images from the 64px network comparable to the Pix2Pix and UNet approaches. Therefore, only results for 64px images are shown in this paper for the DDPM, which explains the blurriness caused by upscaling the predictions to 256px. With the stable diffusion approach there were no such resolution problems, so that the 256px images could be used directly. In general, the error criteria of the diffusion images are worse than those of the other analysed networks. However, depending on the error measure, between 4 and 7% fewer non-evaluable images are produced compared to the Pix2Pix and significantly fewer than with the UNet, which has the highest error rate. In the position, the accuracy is between 25-40% worse. Rotation proves to be the biggest weakness of the approach, as the error here is twice as high as with Pix2Pix and UNet. The roundness is mapped similarly to the other methods and even bit better for the stable diffusion. This is the strength of this method, which delivers very high-quality images with round balls. The SD approach depicts the roundness the best without generating error images regarding this criterion. For roundness, unlike the Pix2Pix, there is dependence on the time interval; if this becomes larger, the error also increases. It is noticeable that the ground slope cannot be evaluated in approx. 3% of the images for the DDPM. Depending on the diffusion method, the mean error is also 22 different from zero, which was not the case either with Pix2Pix or with UNet. Regarding the number of balls in the result images, the diffusion approach performs best. For the DDPM in 97% of the images, only one ball is visible (which is not even achieved by GAN at 93%) and the remaining 3% is divided between images with multiple balls and error images. With stable diffusion, there are even no images with several or no balls. The approach is bit less reliable in terms of mapping the roll of the target ball along the ground slope. SDw.CA in particular performs worst with 12% of the cases where the target ball is located ahead the start ball along the X-axis, which is physically impossible. The DDPM and SD shows quite the same results with about 5 to 6% ahead target balls. Nevertheless, the predictions that correctly represent this relationship are not much worse with diffusion, as the error rate is reduced. The prediction images correctly represent the different colors as well as the line structures in the sky and ground. Due to the blurred image, the visual evaluation is worse for the DDPM, but all image elements are present. Both stable diffusion algorithms generate very high-quality images in which the colors, the background pattern and the ball itself are mapped very accurately. Correlations between errors and simulation parameters: Generally, there are hardly any recognizable correlations with the simulation settings: only for the position error along the axis and the ball rotation some relationships can be noticed. Position error in direction has slight dependency on the time interval between input and target image (a larger interval leads to greater inaccuracy) Ball angle deteriorates the further the ball of the input image has moved along the ground surface. For input images with ball already in the middle of the image, the rotation is slightly worse. This correlation is slightly stronger for the SD approach compared to the DDPM. No correlations are present for the position and ball roundness errors. Auto encoder: Two approaches were examined for the autoencoders: the variational AE (VAE) and the convolutional AE (convAE). However, no evaluable results could 23 be generated by the networks for the bouncing ball. Depending on the error criterion, between 89 and 99% of the predictions are evaluated as invalid (see results in Table 11). In general, it can be said that the obtained images are very blurred. In addition, the ball is rarely displayed as single point but is often drawn out as if it were taking up several positions in the image. The ground surface is also not displayed cleanly as single line; instead, it looks as if several slopes are depicted using different lines. There are also small white stripes in the ground at some images. These typical artefacts or errors of the autoencoder can be seen in Figure 26. Structurally, meaning in terms of colors and background pattern, the predictions look similar to the UNet results, as the networks are closely related in design. Since so many generated predictions are considered invalid when evaluating the error criteria, the results for the bouncing ball in terms of position, angle and roundness are hardly relevant. Therefore, no correlations between simulation parameters and error measures are provided here. Figure 26: Typical error patterns with auto encoders (VAE or convAE) for the bouncing case - no clean representation of the ground surface and very blurred ball In table 12 the detailed results of different training and evaluations are visible for the Pix2Pix and UNet approaches. Indeed for these two networks many runs were carried out to see how the models behave over multiple trainings. In the overview tables already shown in this paper, the mean values of all runs of one method are shown for each error criterion. D.2.2 Typical error patterns In this section, typical incorrect predictions of the analysed generative networks for the bouncing ball are presented to illustrate the above analysis and the error rate of the networks. Due to the poor results of the autoencoders, no error images are presented for this network structure (the typical appearance of the predictions has already been explained above). The visible errors or inaccuracies are described in the caption of the respective figures. Figure 27: Several balls represented on the predictions of the DDPM, the UNet and the Pix2Pix algorithms. Figure 28: Typical artefacts that occur for different trained model D.3 Rolling ball D.3.1 Further evaluation and results Result images of the predictions for the rolling ball are shown below on figure 32. For this simpler physics problem, there are only two variable parameters in the simulation: the ground slope and the position of the start ball on the image. Of course, predictions are also expected from the AI networks for any given time interval. Visually, the same advantages and disadvantages as with the bouncing ball can be recognized for the analysed AI algorithms (see section D.2.1). Here too, the DDPM network creates 64px image, which is then scaled up to 256px. That is because the network trains better with the 64px images and delivers more stable results. The up-scaling explains the blurriness of the diffusion image. In the table 13, the average results of several runs for the investigated AI networks for the rolling ball are shown. The first noticeable point in the results is that the autoencoders provide evaluable predictions for the rolling case, even though they remain the most unstable method. In fact, the error images drop to around 3% for the position and regarding the ball angle and roundness maximum of 34% invalid images are generated. Therefore, the results of this model structure can be discussed below. Otherwise it can be seen that the errors for the position are smaller compared to the bouncing case. This is especially evident for the Y-coordinate, where significant reduction of the error can be seen, as the ball here only rolls along the ground surface and no longer moves in the Y-direction. The third Figure 30: Position of the target ball ahead of the start ball along the X-axis (1st line: start ball; 2nd line: corresponding network prediction) Figure 31: Errors concerning the ball position and rotation (1st line: true images; 2nd line: corresponding net prediction) Figure 32: Predictions of the generative AI compared to the physics simulation for the rolling ball physical error measure, which describes whether the ball is lying on the ground surface, is approximately the same for all AI methods, ranging between 97-99%. Only the autoencoders have difficulties to represent the ball rolling down on the ground. The ball roundness is mapped similarly to the bouncing ball, although it is much better with the UNet. Finally, the ball angle is represented about 2 worse by each algorithm, except for the stable diffusion models which are improving by approx. 2-4. few special features of the analysed AI approaches and differences compared to the bouncing ball are listed below. Figure 29: Not-round ball and imperfect color separation between both ball halves (only Pix2Pix predictions) Pix2Pix (GAN architecture): 24 Significant reduction of error images created by the network that cannot be analysed regarding all error criteria except for the ball rotation. Here, the ball angle is still not recognisable for 13% of the images. Slight increase in the rotation error but strong increase in the standard deviation compared to the bouncing case. This is the error measure that is most poorly mapped by the Pix2Pix model. With 99% reliability, exactly one ball is represented on the predictions of the AI network (6% improvement over the bouncing case). Similarly, in 99% of cases, the target ball is placed behind the start ball in the Xdirection (7% improvement). Correlations between error and simulation parameters: Position errors in both directions (X and Y) increase the most with increasing ground slope, followed by larger time interval. Here, the and errors show the same dependencies on the physics simulation settings. Ball angle deteriorates with greater ground inclination and when the time interval between the start and target image increases. For the roundness error, unlike the bouncing case, there is slight dependence on the starting position of the ball: the further the ball of the input image has moved along the ground surface, the less round the ball is represented. UNet: The same insights and improvements apply to the UNet as to the Pix2Pix model concerning the position and rotation of the ball, the number of balls and the placement in X-direction of the target ball behind the input ball. With this approach, the error images have decreased the most compared to the bouncing case: the largest reduction of non-analysable images from 35% to 4% is observed for the ball roundness. The roundness, which is worse with the bouncing ball, achieves the best error measure for the rolling case. Overall, the UNet provides the best results for the rolling ball, slightly ahead of the Pix2Pix algorithm. However, the background of the predicted images (ground and sky) remains blurred, so that the GAN performs better in terms of overall appearance. Correlations between error and simulation parameters: Position errors in both directions increase as the time interval increases; there is significant dependence here. There is also smaller correlation with the increasing ground slope, but it is weaker. For ball rotation, the time interval is more dominant compared to the GAN, followed by the ground slope. As the parameters increase, the inaccuracy in the prediction of the ball angle also increases. For the roundness error, the same dependence as for the bouncing case is observed: larger time interval between input and target images results in poorer ball representation. Diffusion: Even in the rolling case, the best results for the DDPM model were obtained with 64px images, explaining the blurriness of the predictions due to up-scaling. Like the bouncing ball, the error criteria of the diffusion method are worse than for GAN and UNet. Here, stable diffusion with cross-attention delivers the worst results, with the network performing two or three times worse than the other diffusion methods, depending on the criterion. The DDPM and SD approaches differ only slightly in the results, with the SD model demonstrating greater stability with very low error rate and better representation of the ball roundness. Compared to the bouncing case, the position error in the Y-direction is mainly reduced, as with the other algorithms, while the other error metrics remain nearly identical. Both the ball position (in both directions) and the angle are represented with about twice the inaccuracy compared to Pix2Pix and UNet for the DDPM and the SD model. The roundness is about 35% worse. Regarding the mapping of the ground slope, the same is observed as for the bouncing case: very few error images and small standard deviation around the mean value of 0 are present for the DDPM, while the UNet and the GAN architecture draw the ground almost perfectly. The stable diffusion approaches even show mean error greater than zero. With 99% reliability going to almost 100% for the SD, exactly one ball is shown on the predictions of the diffusion networks and the few remaining predictions are not evaluable. Here, diffusion performs best, as about 1% of the Pix2Pix and 3% of the UNet predictions do not contain ball. Regarding the correct rolling of the ball on the ground surface, 3% of predictions represent the target ball 25 ahead of the input ball along the X-direction, which is physically impossible. Here, both other methods discussed above perform slightly better with almost 0% target balls ahead. However, the diffusion network, like the Pix2Pix and the UNet, correctly depicts the ball on the ground surface in 97 to 99% of cases. Overall the SD model shows the best results concerning the three criteria expected to verify the physical correctness. Correlation between error and simulation parameters: Position error in the direction most increases with larger time interval. There is also smaller correlation with the increasing ground slope. Position error in the direction increases the most with increasing ground slope, followed by larger time interval. Concerning the position errors along both axes the DDPM algorithm shows the strongest dependencies. Ball angle deteriorates the most when the time interval between the start and finish images increases. There is also slightly weaker correlation with the ground slope. No correlations are present for the roundness error. Autoencoder: For the rolling ball, the two examined autoencoder approaches generate evaluable results. With maximum of 34% invalid images for ball angle predictions and an error rate of 3% for the position, the quality of the predictions is significantly improved. Additionally, the average error metrics are close to those of the Pix2Pix model and even slightly better than the diffusion networks. Nonetheless, autoencoders remain the most unstable structure for modeling the ball problem, as they generate the highest number of non-evaluable images. The following observations can be made regarding the VAE and convAE approaches: The convAE network generally provides slightly better results than the VAE. The error rates of convAE are bit lower, as well as reduced average errors and standard deviations. However, the mapping of the ground and the representation of the ball angle are nearly identical between the two methods. As with the other models, the balls position is represented less accurately along the X-axis compared to the Y-direction for the autoencoders. The ball rotation is learned as effectively as with the Pix2Pix approach, but the error rate increases by 50% to more than double in the case of the VAE algorithm. notable observation is that the balls roundness is represented worse compared to all other approaches examined, with significantly more invalid images. Similarly, the ground slope is learned less accurately by both models. With the SDw.CA method, the autoencoders are the only models showing mean error greater than zero. Regarding physical correctness, these models generate the highest number of images without any ball present (3%). Additionally, in 12% (convAE) and 25% (VAE) of the cases, the ball is not depicted on the ground surface, although this should always be the case for rolling ball on an inclined surface. This error criterion stands out most significantly compared to the other models, where only 1 to 3% of the images show the ball not in contact with the ground. Correlation between error and simulation parameters: Position errors in both directions increase the most as the time interval increases. slight dependency can be observed to the ground slope, which generates more inaccuracy when getting greater (more pronounced for the Y-direction). Concerning the ball rotation the correctness of the model is worser when the time interval between start and target images increases. There is also slightly weaker correlation with the ground slope. For the roundness error, correlation to the position of the ball on the image can be observed. The further the ball of the input image has moved along the ground surface, the less round it is depicted. This is the only model that shows dependency between the roundness error and this simulation parameter. In table 14 the detailed results of different training and evaluations are visible for each of the three analysed AI approaches. In the overview tables already shown in this paper, the mean values of all runs of one method are shown for each error criterion. D.3.2 Typical error patterns In this section, typical incorrect predictions of the analysed networks for the rolling ball are presented to illustrate the above analysis and the error rate of the networks. The visible errors or inaccuracies are described in the caption of the respective figures. Finally, the evaluation of the rolling case can be concluded with brief look at the physics equations. As already 26 tion, the significant improvement in the position prediction can be explained by the fact that there is no movement along this axis and therefore the complex calculation parts of the bouncing case are omitted. Figure 33: Several or no balls are present in the predictions (left: several balls only occur with the diffusion network; right: no ball present) Figure 34: Typical artefacts that occur for different trained model Figure 35: Imperfect color separation between both ball halves (occurs mostly for the Pix2Pix model) Figure 36: Position of the target ball ahead of the start ball along the X-axis (mostly for diffusion). The predicted ball is slightly to the left of the starting ball position on the generated samples. Figure 37: Errors concerning the ball position and rotation (1st line: true images; 2nd line: corresponding network prediction) indicated in the main part of this paper, the largest errors can be traced back to the parts of the equation that contain double derivative. The prediction of the x-coordinate and the ball angle are the most error-prone and also occur in the equations with and ψ double derivatives. In the direc27 Table 11: Prediction results for the Pix2Pix, UNet, autoencoder and diffusion networks for the bouncing ball Model Metric Mean Standard deviation Error Number of balls 0 / >1 / Error Position to start ball Ahead / Error Pix2Pix UNet VAE convAE DDPM SD(w.CA) SD"
        },
        {
            "title": "Position X\nPosition Y\nRotation\nRoundness\nGround slope",
            "content": "Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope 6.28 11.7 17.2 0.56 5.53 10.8 15.2 0.74 0 4.69 6.25 31.0 0.90 1.00 4.24 6.08 12.2 1.06 1.00 7.91 15.5 32.9 0.61 0 40.0 24.8 61.1 0.53 1.80 8.55 16.2 34.2 0.47 0. 7% 7% 15% 10% 0% 20% 20% 30% 35% 0% 7% / 0% / 0% 1% / 7% 19% / 0% / 1% 0% / 20% 89% 89% 97% 20% / 0% / 68% 99% 25% 89% 97% 99% 11% / 0% / 86% 99.9% 26% 1% / 89% 0% / 97% 3% 3% 8% 6% 3% 0% 0% 21% 2% 0% 0% 0% 6% 0% 0% 0% / 1% / 2% 6% / 3% 0% / 0% / 0% 12% / 0% 0% / 0% / 0% 5% / 0% 7.98 12.8 20.8 0.14 0 7.48 12.2 22.9 0.21 0 6.13 6.94 39.8 0.14 0.04 3.85 5.93 8.61 0 0.05 9.04 13.7 33.8 0.17 0. 48.5 22.9 52.5 0.15 1.51 11.9 14.1 37.8 0.11 1.08 28 Table 12: Detailed results of each training run for the Pix2Pix (GAN) and UNet networks where multiple runs were carried out to get more representative mean values for the bouncing ball Rotation Mean / Std / Err Position Position Mean / Std Mean / Std Position Error Roundness Mean / Std / Err Pix2Pix UNet 15.9 / 18.8 / 58 (4%) 16.7 / 19.9 / 112 (7%) 14.5 / 20.5 / 587 (37%) 21.6 / 24.1 / 229 (14%) 15.1 / 23.4 / 495 (31%) 15.9 / 23.7 / 398 (25%) 15.2 / 24.1 / 458 (29%) 14.5 / 20.5 / 587 (37%) 6.46 / 8.27 6.31 / 8.17 5.71 / 7.25 6.65 / 8.23 5.30 / 7.44 5.55 / 7.62 5.54 / 7.62 5.71 / 7.25 12.2 / 12.9 12.1 / 13.0 10.3 / 11.9 12.4 / 13.3 10.8 / 11.8 10.9 / 12.1 11.3 / 12.9 10.3 / 11.9 6 (0%) 14 (1%) 433 (27%) 14 (1%) 310 (19%) 261 (16%) 274 (17%) 433 (27%) 0.50 / 0.11 / 16 (1%) 0.51 / 0.12 / 23 (1%) 0.73 / 0.20 / 612 (38%) 0.51 / 0.11 / 19 (1%) 0.73 / 0.22 / 542 (34%) 0.75 / 0.21 / 540 (34%) 0.74 / 0.22 / 520 (33%) 0.73 / 0.20 / 612 (38%) Ground slope Mean / Std / Error Number of balls 0 / >1 / Error Position to start ball Ahead / Error Pix2Pix UNet 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 3 (0%) / 1 (0%) / 0 (0%) 12 (1%) / 2 (0%) / 0 (0%) 416 (26%) / 0 (0%) / 17 (1%) 14 (1%) / 0 (0%) / 0 (0%) 304 (19%) / 0 (0%) / 4 (0%) 256 (16%) / 1 (0%) / 2 (0%) 263 (16%) / 0 (0%) / 8 (1%) 416 (26%) / 0 (0%) / 17 (1%) 9 (1%) / 6 (0%) 16 (1%) / 14 (1%) 0 (0%) / 433 (27%) 5 (0%) / 14 (1%) 3 (0%) / 310 (19%) 3 (0%) / 261 (16%) 3 (0%) / 274 (17%) 96 (6%) / 44 (3%) 29 Table 13: Prediction results for the Pix2Pix, UNet, autoencoder and diffusion networks for the rolling ball Model Metric Mean Standard deviation Error Number of balls 0 / >1 / Error Position to start ball Ahead / Error Distance to ground Error 1% / 0% / 0% 0% / 1% 2% 3% / 0% / 0% 0% / 3% 3% 3% / 0% / 0% 1% / 3% 25% 2% / 0% / 0% 0% / 2% 12% 0% / 0% / 1% 3% / 1% 3% 1% / 0% / 0% 3% / 1% 2% 0% / 0% / 0% 1% / 0% 1% Pix2Pix UNet VAE convAE DDPM SD(w.CA) SD Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope Position Position Rotation Roundness Ground slope 4.26 1.61 20.9 0.56 0 3.69 1.4 16.1 0.53 5.54 2.44 21.5 0.70 1.10 4.57 2.10 21.4 0.65 1.11 7.38 2.34 34.2 0.74 0 24.1 7.65 59.1 0.55 1.06 7.36 2.57 29.9 0.53 0.02 8.36 3.31 35.2 0.13 8.49 3.63 32.7 0.15 0 16.2 6.26 36.0 0.20 0.33 9.47 3.80 38.2 0.21 0.32 8.56 3.39 38.6 0.16 0.05 33.2 11.6 52.1 0.13 0.39 14.7 5.32 40.4 0.11 0. 1% 1% 13% 3% 0% 1% 1% 11% 4% 0% 3% 3% 34% 25% 0% 2% 2% 20% 12% 0% 1% 1% 10% 6% 1% 1% 1% 12% 2% 0% 0% 0% 4% 1% 0% 30 Table 14: Detailed results of each training run for the Pix2Pix and UNet networks where multiple runs were carried out to get more representative mean values for the rolling ball Rotation Mean / Std / Err Position Position Position Mean / Std Mean / Std Error Roundness Mean / Std / Err Pix2Pix UNet 21.7 / 36.0 / 199 (11%) 24.4 / 36.7 / 385 (21%) 21.8 / 35.4 / 377 (21%) 16.2 / 32.9 / 121 (7%) 20.2 / 34.8 / 98 (5%) 16.8 / 33.5 / 176 (10%) 15.7 / 32.1 / 194 (11%) 16.2 / 33.6 / 187 (10%) 15.6 / 31.5 / 203 (11%) 4.17 / 8.28 4.35 / 8.14 4.92 / 9.36 3.80 / 8.43 4.07 / 7. 3.69 / 8.33 3.59 / 8.35 3.77 / 8.69 3.71 / 8.59 1.58 / 3.24 1.67 / 3.32 1.91 / 3.82 1.38 / 3.26 1.49 / 2.92 1.30 / 3.18 1.33 / 3.26 1.40 / 3.46 1.57 / 4.61 16 (1%) 26 (1%) 21 (1%) 3 (0%) 14 (1%) 39 (2%) 40 (2%) 23 (1%) 9 (1%) 0.60 / 0.14 / 34 (2%) 0.62 / 0.15 / 85 (5%) 0.57 / 0.14 / 63 (4%) 0.50 / 0.11 / 16 (1%) 0.51 / 0.11 / 16 (1%) 0.52 / 0.15 / 62 (3%) 0.51 / 0.14 / 66 (4%) 0.53 / 0.15 / 62 (3%) 0.55 / 0.15 / 68 (4%) Ground slope Mean / Std / Err Number of balls 0 / >1 / Error 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 0 (0%) 0 / 0 / 11 (1%) 0 / 0 / 0 (0%) 0 / 0 / 1 (0%) 0 / 0 / 0 (0%) 16 (1%) / 0 (0%) / 0 (0%) 13 (1%) / 0 (0%) / 0 (0%) 21 (1%) / 0 (0%) / 0 (0%) 36 (2%) / 0 (0%) / 2 (0%) 14 (1%) / 0 (0%) / 0 (0%) 37 (2%) / 1 (0%) / 1 (0%) 36 (2%) / 0 (0%) / 4 (0%) 44 (2%) / 0 (0%) / 8 (0%) 57 (3%) / 0 (0%) / 7 (0%) Pix2Pix UNet Position to start ball Distance to ground Ahead / Error 5 (0%) / 16 (1%) 8 (0%) / 26 (1%) 0 (0%) / 21 (1%) 0 (0%) / 38 (2%) 2 (0%) / 14 (1%) 0 (0%) / 39 (2%) 0 (0%) / 40 (2%) 0 (0%) / 52 (3%) 0 (0%) / 64 (4%) Error 19 (1%) 34 (2%) 35 (2%) 38 (2%) 14 (1%) 40 (2%) 40 (2%) 52 (3%) 64 (4%)"
        },
        {
            "title": "E Datacard",
            "content": "32 33 34 35 36 38 39 40 41 42 44"
        }
    ],
    "affiliations": [
        "Herrenknecht AG",
        "Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany",
        "University of Mannheim, Germany"
    ]
}