{
    "paper_title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
    "authors": [
        "Xianhang Li",
        "Chen Huang",
        "Chun-Liang Li",
        "Eran Malach",
        "Josh Susskind",
        "Vimal Thilak",
        "Etai Littwin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable off-the-shelf video representation by predicting masked regions in latent space with an exponential moving average (EMA)-updated teacher. While EMA prevents representation collapse, it complicates scalable model selection and couples teacher and student architectures. We revisit masked-latent prediction and show that a frozen teacher suffices. Concretely, we (i) train a target encoder with a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze it and train a student to predict the teacher's latents on masked regions. This leads to a two-stage, unregularized scheme that we refer to as SALT (Static-teacher Asymmetric Latent Training). SALT decouples optimization into pixel reconstruction (teacher) and masked latent prediction (student), increasing transparency, efficiency, and scalability while preserving the ability of representation to generalize under frozen evaluation. Empirically, our student models outperform recently proposed V-JEPA 2 encoders under frozen backbone evaluation across diverse benchmarks. They are also more compute-optimal: at matched pretraining FLOPs, our method achieves higher probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs Pareto frontier. Finally, we find that student quality is remarkably robust to teacher quality: high-performing students emerge even with small, sub-optimal teachers. This points to a compute budget allocation that should overwhelmingly favor the student. These results position SALT as a simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 1 3 4 2 . 9 0 5 2 : r Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers Xianhang Li, Chen Huang, Chun-Liang Li, Eran Malach, Josh Susskind, Vimal Thilak, Etai Littwin Apple Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable off-the-shelf video representation by predicting masked regions in latent space with an exponential moving average (EMA)-updated teacher. While EMA prevents representation collapse, it complicates scalable model selection and couples teacher and student architectures. We revisit masked-latent prediction and show that frozen teacher suffices. Concretely, we (i) train target encoder with simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze it and train student to predict the teachers latents on masked regions. This leads to two-stage, unregularized scheme that we refer to as SALT (Static-teacher Asymmetric Latent Training). SALT decouples optimization into pixel reconstruction (teacher) and masked latent prediction (student), increasing transparency, efficiency, and scalability while preserving the ability of representation to generalize under frozen evaluation. Empirically, our student models outperform recently proposed V-JEPA 2 encoders under frozen backbone evaluation across diverse benchmarks. They are also more compute-optimal: at matched pretraining FLOPs, our method achieves higher probing accuracy, and its scaling curves dominate V-JEPAs accuracyFLOPs Pareto frontier. Finally, we find that student quality is remarkably robust to teacher quality: high-performing students emerge even with small, sub-optimal teachers. This points to compute budget allocation that should overwhelmingly favor the student. These results position SALT as simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning. Correspondence: Vimal Thilak: vthilak@apple.com; Etai Littwin: elittwin@apple.com Date: September 30,"
        },
        {
            "title": "Introduction",
            "content": "Self-supervised learning (SSL)-based methods have emerged as standard approach for representation learning in computer vision. These methods pretrain neural networks that use vast amounts of image (He et al., 2021; Assran et al., 2023; Caron et al., 2021; Oquab et al., 2024; El-Nouby et al., 2024) or video (Tong et al., 2022; Wang et al., 2023b; Bardes et al., 2024; Assran et al., 2025) data to learn backbones that have been shown to work well on many downstream tasks. Among these methods, Joint Embedding Predictive Architecture (JEPA)-based methods (LeCun, 2022) have demonstrated strong ability to learn powerful semantic features that perform well on downstream image (I-JEPA) (Assran et al., 2023) and video (V-JEPA) (Bardes et al., 2024; Assran et al., 2025) tasks. As concrete instantiations of the Joint Embedding Predictive Architecture (JEPA), I-JEPA (Assran et al., 2023) and V-JEPA (Bardes et al., 2024; Assran et al., 2025) are masking-based pretraining methods that learn powerful semantic representation by predicting masked-out portions of the input in learned embedding space. Specifically, these methods consist of context (student) encoder and predictor that are trained to make predictions that match the embeddings provided by target (teacher) encoder. While powerful, the JEPA family of models are often complex, hyperparameter-brittle, and use an uninformative loss metric that is poor proxy for representation quality, requiring practitioners to rely on other more downstream-predictive *Work done during an internship at Apple. 1 metrics (Agrawal et al., 2022; Garrido et al., 2023; Thilak et al., 2024). These issues stem from the core JEPA design: because student and teacher representation co-evolve, trivial collapsed solutions with near-zero loss exist, and must be avoided. To prevent representation collapse, these models are implicitly regularized using the self-distillation approach pioneered by BYOL (Grill et al., 2020). Namely, the stop-gradient operation is applied on the target encoder, and its weights are updated by an exponential moving average (EMA) copy of the student weights, according to some EMA scheduler. It is worth mentioning that other variants of joint-embedding SSL models utilize more explicit regularizers to prevent collapse (Zbontar et al., 2021; Bardes et al., 2021). In this paper, we challenge the common assumption that such involved collapse prevention mechanisms are required for learning high-quality semantic features. Specifically, we show that dynamic teacher is unnecessary, and that stable, high-quality targets needed to optimize the student model can be obtained in more efficient manner. We propose significantly simplified pretraining recipe that replaces the momentum teacher with frozen encoder. This design obviates the need for both the EMA update and the stop-gradient, streamlining the self-distillation process and reducing implementation complexity. We propose simple twostage pretraining scheme: (i) train target encoder with pixel-reconstruction objective under V-JEPAstyle masking; (ii) freeze this encoder and train student with the JEPA objective to predict the teachers latents on masked regions (Bardes et al., 2024). We dub this method as SALT (Static-teacher Asymmetric Latent Training). Prior work has explored using pretrained frozen encoders as masked-prediction targets (Wang et al., 2023c; Li et al., 2023), but typically assumes access to strong teachers and often relies on fine-tuning the student to realize the benefits. In contrast, we show: 1. Small, sub-optimal teachers suffice. High-quality semantic features competitive with state-of-theart under frozen evaluation protocols can be learned from much smaller and cheaper teachers. Using the strongest available pretrained encoders is unnecessary and yields at most marginal gains for the student. 2. Compute efficiency. Our two-stage design is more compute-efficient than EMA-based self-distillation (e.g., V-JEPA): at matched FLoating Point Operations (FLOPs) and wall-clock, and even when accounting for the cost of training the teacher, our method achieves better accuracyFLOPs trade-off1. 3. Interpretable model selection. Our design yields student loss that provides an informative, training-time metric that correlates strongly with downstream accuracy under the frozen-backbone protocol, in contrast to EMA-based methods that require proxy heuristics for model selection. Taken together, our results suggest that elaborate online student-teacher dynamics, and specifically EMA-based collapse prevention machinery may be unnecessary for learning high-quality representation."
        },
        {
            "title": "2 Method Overview",
            "content": "We first review video-based JEPA models that include both V-JEPA and V-JEPA 2, and then describe our simple approach named SALT for representation learning from videos. Note that V-JEPA 2 uses the same pretraining method described in V-JEPA but employs updated hyperparameters so our method review applies to both models."
        },
        {
            "title": "2.1 V-JEPA",
            "content": "V-JEPA employs masked prediction objective: the context encoderpredictor reconstructs masked regions from visible frames in learned representation space, while an EMA-updated target encoder supplies the supervision. Following the notation used by Bardes et al. (2024), the latent space prediction objective can be written: Ex,ygϕ(fθ(x), δy) stop_grad( fθ(y))1 min θ,ϕ (2.1) 1FLOPs and total number of training steps are used interchangeably to refer to compute. Section includes an explanation for this choice 2 Figure 1 (Left) SALT Stage 2: Frozen-teacher, learnable student and predictor. The frozen teacher encoder is obtained via Stage 1 (not pictured above) by training using pixel reconstruction objective. The student and predictor are jointly optimized to learn representation from video in Stage 2 using latent space prediction objective. (Right): SALTs compute-accuracy curve dominates V-JEPA 2. where and denote two disjoint regions of the input, , and denote the encoder, target encoder and predictor respectively, stop_grad denotes the stop-gradient operation; and δy denotes the spatio-temporal positions of missing regions in the input which serve as context for the predictor."
        },
        {
            "title": "Method",
            "content": "The V-JEPA method uses self-distillation approach incorporating stop-gradient and exponential moving average (EMA). This approach requires meticulous tuning of the associated hyperparameters to maintain training stability and prevent representation collapse. In this work, we advocate for an alternative solution, which we refer to as SALT, that does not require the use of EMA. Specifically, we simplify the architecture by breaking down video representation learning into two steps: Stage 1 - The teacher or target encoder is trained to optimize pixel reconstruction objective in Stage 1. The objective is identical to that used in VideoMAE (Tong et al., 2022). However, our Stage 1 method differs from VideoMAE as we use more efficient masking scheme, the details of which are described in Section 5.2. Stage 2 - The weights of the teacher from Stage 1 are frozen and used to train student and predictor network, as shown in Figure 1. The JEPA objective, described by Equation (2.1), is used to optimize the student and the predictor. The simplification described above results in two loss objectives that are proper loss functions which are easier to interpret in practice, and are immune to representation collapse by design. This stands in contrast to V-JEPAs objective in Equation (2.1), which is difficult to interpret due to its self-distillation nature that, in turn, necessitates the use of surrogate metrics (Garrido et al., 2023; Thilak et al., 2024). Moreover, our twostage approach completely decouples the teacher and student architectures, unlocking considerable compute efficiency gains through the use of small teachers to train larger students, as shown in Figure 1 and Table 1. We observe from Figure 1 that SALT shows significant improvement over V-JEPA 2 on Something-Something-v2 (SSv2) (Goyal et al., 2017), which is temporal understanding task. Furthermore Table 1 shows that smaller but noticeable improvement is observed on Kinetics-400 (Kay et al., 2017), an appearance understanding benchmark. We describe the experimental setup in Section 3 and discuss results in detail in Section 4. 3 2.2.1 SALT Design Principles SALT follows the contemporary trend toward simple, principled architectures and objectives, avoiding elaborate engineering. We provide simple recipe to train the teacher in Stage 1 with method that we call V-Pixel that uses pixel reconstruction objective 2 along with the multi-block masking method described in V-JEPA. The decoupled design of SALT allows us to study the role of architecture and dataset choices for training the teacher and student in granular manner. We uncover surprising finding that, high performing teacher, as measured by its downstream performance, is not necessary to train high-quality student. As we show in Section 5.1, Section 5.3 and Section 5.4, the students ultimate quality is surprisingly robust to suboptimal data mixture, teacher size and compute budget. Overall, our simplified design demonstrates superior efficiency, scalability, and interpretability over the baseline V-JEPA."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Training Our training data includes Kinetics-710 (K710) Kay et al. (2017), constructed by merging Kinetics400/600/700 and removing all validation samples, Something-Something V2 (SSV2) Goyal et al. (2017), and 2.8 million subset of the Panda70M Chen et al. (2024) resulting in approximately 3.6 million (3.6M) video dataset that we refer to as V-3.6M dataset in our work. Note that our training dataset differs from the training datasets used in V-JEPA and V-JEPA 2 as the latter methods use Howto100M (Miech et al., 2019) and YT-Temporal-1B datasets (Zellers et al., 2022) while we use subset of Panda70M in V-3.6M. Our models are standard Vision Transformers (ViT) (Dosovitskiy et al., 2020) with rotary positional embeddings (RoPE) (Su et al., 2024) which are identical to the architecture described in V-JEPA 2. Specifically, we use ViT-Large (ViT-L), ViT-Huge (ViT-H), ViT-giant (ViT-g), and ViT-G in our experiments, the details of which are described in Section B. All baseline models (V-JEPA and V-Pixel) are trained with the same batch size of 3072 using the AdamW optimizer with hyperparameters described in detail in Section C. To ensure fair comparisons, we keep the number of optimization steps fixed for SALT and the baseline methods. In other words, the total number of steps for Stage 1 and Stage 2 is identical to the number of steps used by baseline methods. The optimal number of steps to train teacher and student is obtained through an ablation detailed in Section 5.4. Evaluation We evaluate our models on variety of video and image tasks. For video classification, following, we use Kinetics-400 (K400), Something-Something-v2, COIN classification (Tang et al., 2019), Jester (Materzynska et al., 2019) and Diving-48 (Li et al., 2018) by freezing the backbone and training an attentive classifier to assess performance. For image classification, we adopt the same protocol on ImageNet-1K (Russakovsky et al., 2015), replicating each image 16 times to form the input sequence. Furthermore, we evaluate our models on intuitive physics understanding benchmarks, which measure performance by comparing the models surprise scores for possible versus impossible videos. Following (Garrido et al., 2025), we use the predictor to forecast future representation. We assess performance on the IntPhys (Riochet et al., 2018), GRASP (Jassim et al., 2023), and InfLevel (Weihs et al., 2022) datasets. All setup information and hyperparameters used for our evaluations are described in detail in Section D."
        },
        {
            "title": "4 Experimental Results",
            "content": "Systematic comparison of SALT with existing baselines Table 1 lists the performance of SALT and existing work that serve as strong baselines including V-JEPA 2, VideoPrism (Zhao et al., 2024), InternVideo2 (Wang et al., 2024), VideoMAEv2 (Wang et al., 2023b), Perception Encoder (Bolya et al., 2025) and image encoders that include DINOv2 (Oquab et al., 2024) and SigLIP2 (Tschannen et al., 2025). We use the Kinetics-400 (K400) and Something-Something-v2 (SSv2) as benchmark datasets and evaluate SALT following the same multiclip, multiview setting used in existing baseline. We observe from Table 1 that our largest models, ViT-g, and ViT-G, trained with SALT outperforms all of the baseline methods on SSv2, which tests the motion understanding ability of video models. On K400, which is an appearance understanding benchmark, the 2This method is implicitly described in Table 1 by Bardes et al. (2024). We name the method V-Pixel for clarity in presentation. 4 Table 1 Systematic comparison of state-of-the-art video encoders under frozen-backbone evaluation, using SSv2 (1623) and K400 (1623). The comparison includes several baselines including encoders trained with V-JEPA 2 method on our V-3.6M dataset. The V-JEPA 2 encoders trained on V3.6M and SALT encoders are evaluated using the protocol in Section 3 and Section D. The results for other models are copied from Table 4 in V-JEPA 2 (Assran et al., 2025). detailed description of FLOPs calculation is available in Section F. Method Param. Pretraining Dataset Teacher (Params) Total Compute # Seen Samples SSv2 K400 VideoMAEv2 (Tong et al., 2022) PEcoreG (Bolya et al., 2025) InternVideo2-1B (Wang et al., 2024) VideoPrism (Zhao et al., 2024) DINOv2 (Oquab et al., 2024) SigLIP2 (Tschannen et al., 2025) V-JEPA 2 ViT-L (Assran et al., 2025) V-JEPA 2 ViT-H (Assran et al., 2025) V-JEPA 2 ViT-g (Assran et al., 2025) V-JEPA 2 ViT-L V-JEPA 2 ViT-H SALT ViT-L SALT ViT-H SALT ViT-g SALT ViT-G 1B 1.9B 1B 1B 1.1B 1.2B 300M 600M 1B 300M 600M 300M 600M 1B 2B UnlabeledHybrid-1.4M MetaCLIP-5B (Xu et al., 2024) IV-25.5M VT-36M LVD-142M WebLI-10B (Chen et al., 2022) VM-22M VM-22M VM-22M N/A N/A InternVL-6B + VideoMAEv2-g (6B + 1.0B) Stage-1-ViT-g ( 1.0B) EMA teacher (1.1B) EMA teacher (1.2B) EMA teacher (300M) EMA teacher (600M) EMA teacher (1B) V-3.6M V-3.6M V-3.6M V-3.6M V-3.6M V-3.6M EMA teacher (300M) EMA teacher (600M) SALT-ViT-L (300M) 2.2 1.9 3.5 5.3 1.4 2.6 1.2 1.5 1.9 2. 1.6B 86B 2.0B 1.9B 40B 0.7B 0.7B 0.7B 0.7B 0.7B 0.7B 0.7B 0.7B 0.7B 56.1 55.4 67.3 68.5 50.7 49.9 73.7 74.0 75.3 68.2 73.4 74.9 75.4 76.2 76. 82.8 88.5 87.9 87.6 83.6 87.3 85.1 85.3 86.6 83.8 84.6 85.4 86.0 86.8 87.2 (a) Per-benchmark accuracy of ViT-L. (b) Scaling model-size. Figure 2 V-JEPA 2 vs. SALT at matched total steps on V-3.6M. Both methods are trained on the same V-3.6M dataset for an identical number of pretraining steps. SALT uses an 80k-step teacher and 160k-step student. We evaluate all models under the same frozen-backbone protocol across standard video/image benchmarks: K400 with 1611 input; SSv2 with 1611; Diving48 and Jester with 1643; and COIN 1683. Table 12 provides breakdown of downstream performance for each dataset used in this evaluation. encoders trained with our method exceeds the performance of V-JEPA 2 across all scales and remains highly competitive with other state-of-the-art methods including the recently proposed Perception Encoder. Static teacher improves representation quality key design choice of SALT is the use of static teacher which differs from the dynamic momentum-encoder teacher used in V-JEPA 2. To ascertain the differences between these two approaches, we use the same V-3.6M dataset and input resolution of 224 224 to train SALT and V-JEPA 2. We train both methods for total of 240k steps in this study. Figure 2a shows the downstream performance results of this study for ViT-L-based teacher and student model setup while Figure 2b shows the scaling behavior of the two methods as we scale up the student encoder while using teacher encoder that is the same size or smaller than the student. We observe from Figure 2a that SALT improves the average accuracy over the V-JEPA 2-based encoder by 2.3% with the average calculated over six benchmarks. Furthermore, we observe from Figure 2b that SALT displays improved performance as we scale up the student. Note that we use the same-sized teacher and student models for ViT-B and ViT-L while we use smaller ViT-L teacher model for training ViT-H student encoder. We refer the reader to Section for detailed model size and other architecture information. Together, Figure 2 suggests that the static teacher-based SALT learns higher quality features compared to V-JEPA 2 which uses an EMA-based teacher. Small teachers unlock compute efficiency Table 1 and figure 2 show that strong students can be trained from frozen teacher, which is considerably cheaper: fixed ViT-L teacher successfully trains same-size ViT-L 5 (a) SALT-teacher 80k. R2 = 0.951 (b) SALT-teacher 40k. R2 = 0. (c) SALT-teacher 20k. R2 = 0.984 Figure 3 Correlation between student loss and downstream accuracy. Observe that the student models training loss is predictive of downstream accuracy. students, and much larger ViT-H/g/G students. Consequently, SALT achieves lower total pretraining FLOPs than the EMA-based baseline across model sizes, even when accounting for the teacher pretraining stage. The savings stem from the simple, efficient teacher pretraining (e.g., ViT-L on V-3.6M) and grow with both model size and spatial resolution. FLOPs computation details are provided in Section F. SALT enables interpretable model selection key challenge with using joint-embedding methods including JEPA is that the training loss is typically uninformative of representation quality. Figure 3a shows student training loss versus student downstream accuracy plot for various student checkpoints that use the same SALT Stage 1 checkpoint as the teacher. This checkpoint is obtained by training teacher for 80K steps. The plot shows that the student loss is highly predictive of the downstream accuracy with an R2 value of 0.951, suggesting an almost linear relationship. This result implies that SALT significantly simplifies tracking the quality of representation during student pretraining, and provides clear signal for improvement via simple loss minimization. Similar observations can be made by teacher trained for 40K and 20K iterations in Figure 3. Lastly, we study whether teacher-related metrics such as teacher loss or RankMe (Garrido et al., 2023) are predictive of downstream performance in Figure 9. We find that neither the teachers loss nor embedding rank are predictive of the student encoders downstream performance. Intuitive physics evaluation Garrido et al. (2025) have shown that video models trained with the JEPA objective show an emergent understanding of intuitive physics. We follow the setup described in (Garrido et al., 2025) to measure the intuitive understanding ability of video models trained with SALT. The evaluation setup and results are discussed in detail in Section E.1 due to space limitations. The main finding is that intuitive physics understanding is observed on models trained via SALT as well as V-JEPA 2."
        },
        {
            "title": "5 Teacher Design Choice Ablation",
            "content": "The empirical analysis in Section 4 shows that SALT provides high-quality representation that outperforms several existing methods. key aspect of SALT is the static (frozen) teacher that provides high-quality prediction targets. We study the design choices involved in training the teacher model and student model that lead to optimal representation via series of ablations described in the following."
        },
        {
            "title": "5.1 Training Dataset",
            "content": "In this section, we study the role of pretraining data distribution on teacher model. Specifically, we train ViT-L teacher model with six training datasets: (i) Kinetics-710 (K710), (ii) Something-Something-V2 (SSv2), (iii) 2.8 million subset of Panda70M, (iv) ImageNet-1k, (v) data aggregated from K710 and SSv2, and (vi) V-3.6M which is data aggregated from K710, SSv2 and Panda70M subset. The exact details of the datasets are described in Section 3. The teacher model is trained using the Stage 1 approach (V-Pixel) described 6 Figure 4 Training data of static-teacher. We ablate the impact of training data of teacher, thus fixed the students training data as the whole data-mix by default. Table 13 provides detailed breakdown of the results show above. in Section 2.2. For each teacher model from Stage 1, we train ViT-L-based student model on the combined V-3.6M dataset using Stage 2 approach described in Section 2.2. Figure 4 shows the result of benchmarking the teacher and student models trained with the datasets described above. We observe that downstream performance of each student model described above improves over that of its corresponding teacher. Additionally, each student models downstream performance exceeds the performance of V-JEPA 2-based encoder with the exception of the student model trained on ImagetNet-1Kbased teacher model. Among the teacher models trained on video datasets considered in this study, we observe comparable performance with the notable exception of the teacher model trained on Something-Something-V2. Taken together, these results suggests that an effective teacher maybe trained with relatively small amount of data to build strong foundation models. Figure 5 Masking Strategy of static-teacher. We study the impact of random vs multi-block masking strategy influences students performance. Table 15 includes hyperparameters and results information."
        },
        {
            "title": "5.2 Teacher Masking Strategy",
            "content": "We study the role of masking strategy used to train the teacher that provides targets to optimize the student model. To this end, we train ViT-L using random masking used in VideoMAE, multi-block masking used in V-JEPA and modified method that we call multi-random tube where we adapt the short-range and 7 long-range masking idea from V-JEPA to random masking. We refer the reader to Table 15 for setup details used in this ablation. Figure 5 shows the results for this ablation. We observe that the multi-block masking approach works the best for V-Pixel model achieving an accuracy of 72.5%. This finding in and of itself is new empirical finding as VideoMAE models typically use random-tube masking. Furthermore, we observe from Figure 5 that the student trained with multi-block teacher achieves the highest accuracy while the other student models also show big improvement over their corresponding teachers. We conclude that multi-block masking strategy is effective with training our teacher and name the pixel reconstruction method with multi-block masking as V-Pixel."
        },
        {
            "title": "5.3 Teacher model size",
            "content": "Next, we study the impact of teacher models size on student models performance. We train ViT-B, ViT-L, ViT-H and ViT-G based V-Pixel models and use these models to supervise ViT-L and ViT-G based student models. Figure 6 shows the results of this ablation. We observe that the best performing ViT-L student has an average accuracy of 77.4% and is obtained by training with ViT-L teacher. This result is remarkable as this accuracy is better than the accuracy obtained with ViT-H and ViT-G based teachers that are larger than the student. similar observation can be made about the ViT-G student where the highest average accuracy of 78% is obtained with ViT-L teacher. Additionally we observe that all student models show improvement over their teachers which are of the same or smaller size. These observations suggest that the multi-stage training proposed in SALT allow the student to bootstrap from weaker teacher to learn high-quality representation. Figure 6 Teacher model size ablation. We train ViT-B, ViT-L, ViT-H and ViT-G based teacher and use the teacher to train ViT-L and ViT-G student. Observe that the best performing student is obtained from ViT-L teacher with modest performance. Table 14 provides detailed breakdown of results on downstream benchmarks."
        },
        {
            "title": "5.4 Impact of Teacher-Student Compute Allocation",
            "content": "Due to frozen teacher approach used in SALT, we are confronted with the problem of allocating compute between the teacher and the student. Training compute is function of the model size, the number of optimization steps as well as the number of FLOPs per step. In this ablation, we hold the model size, the total number of optimization steps, and the training dataset (V-3.6M) constant as that allows us to conduct fair comparison of SALT and V-JEPA 2 baseline. We use ViT-L based model for both teacher and student in this ablation where we vary the number of steps used to train the teacher and student for fixed total number of optimization steps. We observe from Figure 7 that SALT outperforms V-JEPA 2 baseline over range of optimization steps considered in our experiments. Furthermore, we observe that SALT exceeds V-JEPA 2s performance at approximately the same FLOPs level which in turn suggests that SALT is more compute efficient than V-JEPA 2. The best performing student is obtained by training on 240k total steps, as can be seen from Figure 7, that is supervised by teacher that is only trained for 40k steps. This finding underscores the effectiveness of our multi-stage training approach and demonstrates that the focus of pretraining in SALT should be on the student, and that aiming for high performing teacher may wasteful. Additional visualization and analysis that supports these claims are provided in Section E.2. 8 Figure 7 Comparison of compute allocation in SALT. We show average Top-1 accuracy across benchmarks against total training FLOPs. Our SALT curves dominate V-JEPA-2 at matched budgets. See Table 16 for additional details."
        },
        {
            "title": "6 Related Work",
            "content": "Video foundation models: Masking-based self-supervised learning (SSL) (Tong et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023a; Bardes et al., 2024; Assran et al., 2025; Wang et al., 2023c; Li et al., 2023; Zhao et al., 2024; Wang et al., 2022; Siméoni et al., 2025) is prominent approach used to learn representation from large-scale video datasets for building video foundation models. Several works (Tong et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023b) have extended image-based masked autoencoders (He et al., 2021) to video data by using random masking to learn representation via pixel-space reconstruction. An alternate approach for representation learning is to learn via latent-space predictions. These methods are known to learn features that differ in quality from those obtained via reconstruction-based methods (Littwin et al., 2024; Balestriero & Lecun, 2024). Prominent among latent-space prediction methods for video are V-JEPA (Bardes et al., 2024) and V-JEPA 2 (Assran et al., 2025) that use an online/momentum encoder to learn the teacher that provides prediction targets. SALT simplifies the JEPA pipeline by using frozen teacher that is reliable and efficient that leads to higher quality representation as shown in our analysis. Distillation from frozen teacher encoder: While many studies utilize frozen pretrained models as teachers for student encoder supervision, we limit our review to prior work that is directly relevant to our core method and refer the interested reader to (Balestriero et al., 2023) for comprehensive survey on SSL literature. MVD (Wang et al., 2023c) and InternVideo (Wang et al., 2022) use VideoMAE (Tong et al., 2022) while other works such as UnMasked Teacher (Li et al., 2023), VideoPrism Zhao et al. (2024), InternVideo2 Wang et al. (2024) and many more use vision-language model (Radford et al., 2021), as frozen teacher. PerceptionEncoder (Bolya et al., 2025) is recent vision foundation model that uses features from predefined layer from within the model as well as features from an external teacher SAM (Kirillov et al., 2023) to encourage feature locality. AM-RADIO (Ranzinger et al., 2024) proposes to learn student from multiple teacher models. Beyond prior work, which require access to powerful pretrained encoders, we uncover weak-teacher, strong-student effect: students supervised by much weaker frozen teachers consistently 9 outperform those trained with EMA-based teachers. Our method is purely self-supervised and unregularized, unlike self-training Xie et al. (2020), which relies on labeled+unlabeled data and explicit noise regularization. Related world-modeling approaches (Baldassarre et al., 2025b; Zhou et al., 2024) also fix encoders for stability but optimize for future-state prediction, while our aim is representation learning. Nevertheless, SALT strong video features make it promising backbone for world models. Masked video distillation: Wang et al. (2023c) propose two-stage method called masked video distillation (MVD) that first trains two separate encoders one each for image and video input using an MAE (He et al., 2021; Tong et al., 2022)-like approach. These teacher encoders then provide targets (latent features) used to optimize smaller student encoder. SALT resembles the approach taken by MVD but has several critical differences. The first difference is that we provide an improved method to train the video teacher encoder as result of careful empirical analysis in Figure 5. Additionally, we do not use separate encoder for image data but instead focus on learning representation from large-scale video datasets. The most critical difference is that we use teacher model whose size is the same or smaller than that of the student model. Furthermore, we conduct detailed ablations in Section 5 to show how to choose teacher model (checkpoint). Finally, SALT learns superior features as our benchmark results are based on frozen backbone evaluations while MVD uses fine-tuning for downstream evaluation."
        },
        {
            "title": "7 Limitations and Conclusion",
            "content": "We present SALT, simple, compute-efficient, and scalable framework for video representation learning. Across standard benchmarks, SALT consistently outperforms strong baselines, including V-JEPA-2 in frozenevaluation protocols. Strikingly, we find that sub-optimal, often smaller teachers can yield much stronger students, raising questions as to how the quality of the teacher should be assessed, and whether EMA-based machinery is necessary to learn highly-semantic representation. principled characterization of teacher quality and fuller study of SALT scaling behaviour is left for future work. While SALT improves compute efficiency and downstream performance over self-distillation, it has limitations. Our ablations (Section 5) suggest that simple V-Pixel recipe usually suffices to train an effective teacher and that compute is best allocated to the student; however, they do not fully explain what makes good teacher. We also observe modest gains from an additional student-training stage, but the mechanism remains unclear. Given the experiment volume, we focused compute on simple scalar diagnostics of teacher quality (Section 5 and Section E.3). Finally, performance plateaus as model size grows in our setting, likely reflecting data limits, and that larger pretraining sets may extend the scaling trend."
        },
        {
            "title": "References",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Kumar Krishna Agrawal, Arnab Kumar Mondal, Arna Ghosh, and Blake Aaron Richards. α-ReQ : Assessing Representation Quality in self-supervised learning by measuring eigenspectrum decay. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=ii9X4vtZGTZ. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael G. Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1561915629, 2023. URL https://api.semanticscholar.org/CorpusID:255999752. Mahmoud Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, and Piotr Bojanowski. Back to the features: Dino as foundation for video world models. arXiv preprint arXiv:2507.19468, 2025a. Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, and Piotr Bojanowski. Back to the features: Dino as foundation for video world models. arXiv preprint arXiv:2507.19468, 2025b. Randall Balestriero and Yann Lecun. How learning by reconstruction produces uninformative features for perception. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 25662585. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/balestriero24b.html. Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023. Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. ArXiv, abs/2105.04906, 2021. URL https://api.semanticscholar.org/CorpusID:234357520. Adrien Bardes, Quentin Garrido, Jean Ponce, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Florian Bordes, Quentin Garrido, Justine Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments. arXiv preprint arXiv:2506.09849, 2025. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 96309640, 2021. URL https://api.semanticscholar.org/CorpusID:233444273. João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv preprint arXiv:2412.15212, 2024. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024. URL https://arxiv.org/abs/2402.19479. 11 Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Ángel Bautista, Vaishaal Shankar, Alexander Toshev, Joshua M. Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. In Proceedings of the 41st International Conference on Machine Learning, pp. 1237112384, 2024. Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. Advances in neural information processing systems, 35:3594635958, 2022. Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. In Proceedings of the 40th International Conference on Machine Learning, 2023. Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from self-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 58425850, 2017. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2127121284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1597915988, 2021. URL https://api.semanticscholar.org/CorpusID:243985980. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni. Grasp: novel benchmark for evaluating language grounding and situated physics understanding in multimodal language models. arXiv preprint arXiv:2311.09048, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1994819960, 2023. Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Etai Littwin, Omid Saremi, Madhu Advani, Vimal Thilak, Preetum Nakkiran, Chen Huang, and Joshua Susskind. How jepa avoids noisy features: The implicit bias of deep linear self distillation networks. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 9130091336. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/ 2024/file/a600f0f740605205133553cb74a1c107-Paper-Conference.pdf. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Joanna Materzynska, Guillaume Berger, Ingo Bax, and Roland Memisevic. The jester dataset: large-scale video dataset of human gestures. In Proceedings of the IEEE/CVF international conference on computer vision workshops, pp. 00, 2019. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 26302640, 2019. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Featured Certification. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1249012500, June 2024. Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, and Emmanuel Dupoux. Intphys: framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothée Darcet, Théo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Hervé Jégou, Patrick Labatut, and Piotr Bojanowski. Dinov3. arXiv preprint arXiv: 2508.10104, 2025. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12071216, 2019. Vimal Thilak, Chen Huang, Omid Saremi, Laurent Dinh, Hanlin Goh, Preetum Nakkiran, Joshua M. Susskind, and Etai Littwin. LiDAR: Sensing linear probing performance in joint embedding SSL architectures. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=f3g5XpL9Kb. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 13 Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking, 2023a. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1454914560, June 2023b. Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In CVPR, 2023c. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pp. 396416. Springer, 2024. Luca Weihs, Amanda Yuile, Renée Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in AI. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=9NjqD9i48M. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687 10698, 2020. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. ArXiv, abs/2103.03230, 2021. URL https://api.semanticscholar.org/CorpusID:232110471. Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1637516387, 2022. Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217, 2024. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024."
        },
        {
            "title": "A Training Dataset",
            "content": "We describe the datasets used to train vision transformer (ViT) models with SALT. We form the Kinetics-710 (K710) dataset by combining training samples from Kinetics-400/600/700 (Kay et al., 2017) and removing duplicated samples as well as samples that are in the validation sets of the above datasets. We then add training samples from the Something-Something-v2 (SSv2) (Goyal et al., 2017) dataset. Finally, we add an approximately 2.8 million video clips subset of Panda70M (Chen et al., 2024) to form our dataset that we refer to as V-3.6M to train our models. We apply stratified sampling to select the subset of clips from Panda70M that enables us to have clips whose duration ranges from 4 seconds to 50 seconds. We do not apply any other form of filtering or curation to form our training dataset. Table 2 lists the sample count information for our V-3.6M dataset. Table 2 V-3.6M Training dataset details. Dataset Sample Count Kinetics-710 Something-Something-v2 Panda70M V-3.6M 657,257 168,913 2,799,959 3,626,"
        },
        {
            "title": "B Architecture Details",
            "content": "We use Vision Transformers (ViTs) (Dosovitskiy et al., 2020) to implement our video encoders and predictors. We use spatial patch size of 16 16 and temporal patch size of 2 in all of our models. Table 3 and Table 4 lists the model architecture in detail for our encoders and predictors respectively. We follow V-JEPA 2 (Assran et al., 2025) and use rotary position embedding (RoPE) (Su et al., 2024) to encode position information in all of our models. Note that our predictors last layer projects the embedding dimension to be compatible with that of the teacher encoder. This information is captured in the input and output dimension columns in Table 4. We use ViT-L teacher to train all encoders except the ViT-B model which uses ViT-B teacher. This information is captured in the output dimension column in Table 4. Table 3 Encoder model architecture details. indicates million and billion. Model Parameter Count Width Depth Heads ViT-B ViT-L ViT-H ViT-g ViT-G 86M 303M 632M 1.012B 1.843B 768 1024 1280 1408 12 24 32 40 48 12 16 16 16 16 Table 4 Predictor model architecture details. indicates million. Predictor & (Encoder) Input Output Parameter Width Depth Heads Dimension Dimension Count ViT-Predictor (ViT-B) ViT-Predictor (ViT-L) ViT-Predictor (ViT-H) ViT-Predictor (ViT-g) ViT-Predictor (ViT-G) 768 1024 1280 1408 1664 21.88M 22.08M 22.18M 22.23M 22.32M 384 384 384 384 384 12 12 12 12 12 16 16 16 16 16 768 1024 1024 1024"
        },
        {
            "title": "C Training Details",
            "content": "Table 5 Hyperparameter details used to train models with SALT. Note that \"indicates that Stage 2 uses the same hyperparameter value as listed in Stage 1. Parameter Stage 1 Stage 2 Input spatial resolution Tubelet size Patch size Number of frames Frame step Random resize aspect ratio Random resize scale Short-range Spatial mask scale Long-range Spatial mask scale Temporal mask scale Mask aspect ratio Batch size Number of Steps Steps per epoch scale Start learning rate learning rate Final learning rate Start Weight decay End Weight decay Clip grad Learning rate schedule Warmup steps AdamW β1 AdamW β2 224 224 2 16 16 2 16 4 [0.75, 1.35] [0.3, 1] 0.15 0.7 1 [0.75, 1.5] 3072 Variable 1 0.0002 0.000625 1e-6 0.04 0.4 0.02 Cosine 10000 0.9 0. \" \" \" \" \" \" \" \" \" \" \" \" Variable \" \" \" \" \" \" \" \" \" \" \" Recall from Section 2.2 SALT is multi-stage training approach in which the teacher is trained at first via V-Pixel method followed by student training using frozen or static teacher in the last stage. Table 5 lists hyperparameter information in detail that are used to train video encoders with SALT. Observe that we use multi-block masking method (Bardes et al., 2024) for V-Pixel. Note that the hyperparamters for setting up multi-block are copied over from those used in V-JEPA (Bardes et al., 2024). Table 5 also lists optimization-related hyperparametrs that we used to train video encoders. We use value of 240, 000 steps in total to show results in Table 1. We conduct ablations with the number of steps set to 120, 000, 160, 000, or 240, 000 for results shown in Figure 2 and discussed in Section 5. Observe that we use the standard cosine weight-decay strategy during training (Bardes et al., 2024). We use value of 0.95 for β2 in AdamW (Loshchilov & Hutter, 2017) as this value is used by Carreira et al. (2024) to train VideoMAE-like models both at scale but most importantly at large scale. We also opt not to use virtual early stopping approach adopted in V-JEPA (Bardes et al., 2024) and V-JEPA 2 (Assran et al., 2025) that scales the training steps by 25% to avoid training instabilities in the latter part of training. Empirically, we observe that frozen teacher provides stable representation that allows SALT to be stable throughout training."
        },
        {
            "title": "D Evaluation Details",
            "content": "We adopt the evaluation protocol used in V-JEPA 2 (Assran et al., 2025) that uses attentive probing to ensure fair comparison between SALT, V-JEPA 2 and several baselines reported by Assran et al. (2025). We use Kinetics-400 (Kay et al., 2017), Something-Something-v2 (SSv2) (Goyal et al., 2017) to systematically compare against state-of-the-art baselines the results of which are reported in Table 1. 16 Systematic evaluation setup for K400 and SSv2 We use inputs with 16 frames, 8 segments or clips per input and 3 spatial views per segment which is identical to the setting used in V-JEPA 2 (Assran et al., 2025) for this dataset. The probe consists of attentive pooling which is implemented via four Transformer blocks where the first three blocks are self-attention based blocks while the last layer uses cross-attention with learnable query token. This pooling operation is followed by standard linear layer where the number of outputs is set to the number of classes for classification dataset. This value is 400 for Kinetics-400 dataset and 174 for SSv2 dataset. The attentive probe is trained with AdamW for 20 epochs using learning and weight decay hyperparameters that are determined via grid search. Table 6 reports the hyperparametrs that are common to SALT and V-JEPA 2. The key difference between SALT and V-JEPA 2 is that we use spatial crop of 224 224 while V-JEPA 2 uses 256 256. This difference makes the results obtained with SALT even more remarkable as we spend much less compute during probing compared to V-JEPA 2 due to using smaller resolution. Table 6 Kinetics-400 and Something-Something-v2 evaluation hyperparameters that are common to SALT and V-JEPA 2. The results of this evaluation are shown in Table 1. Note that \"denotes the value is the same as the one used in K400 evaluation. Parameter K400 SSv2 Number of frames Segments / Clip Views / Segment Frame step Epochs Batch size (global) Classifier heads Classifier learning rates Classifier weight decay 16 8 3 4 20 256 20 [5e-3, 3e-3, 1e-3, 3e-4, 1e-4] [.8, .4, .1, .01] \" 2 \" \" \" \" \" \" \" Fast evaluation setup for K400 and SSv2 Due to the sheer volume of compute involved with training and probing our methods over range of downstream datasets, we use more efficient evaluation protocol for many ablations and results shown in the main paper. The main difference is the use of 1 clip and 1 view per frame while keeping the 16 frames per input clip as described above. The evaluation hyperparameters identical to the values used in V-JEPA (Bardes et al., 2024) and are described in Table 7 for completeness. The results of these evaluations are described in Figures 1 to 9. Table 7 Kinetics-400 and Something-Something-v2 evaluation hyperparameters that are common to SALT and V-JEPA 2. Parameter K400 & SSv2 Number of frames Segments / Clip Views / Segment Frame step Epochs Batch size (global) Classifier heads Classifier learning rates Classifier weight decay 16 1 1 4 20 256 1 1e-3 . COIN, Diving-48 and Jester Evaluations We report results using COIN classification (Tang et al., 2019), Diving48 (Li et al., 2018), Jester (Materzynska et al., 2019) and ImageNet-1K (Russakovsky et al., 2015) benchmarks 17 in addition to Kinetics-400 and Something-Something-v2 benchmarks. The number of classes in COIN, Diving48, Jester, and ImageNet-1K are 180, 48, 27 and 1000 respectively. Table 8 reports the hyperparameters used to evaluate frozen backbones with these benchmarks. The results of these evaluations are reported in Figures 1 to 9. Table 8 COIN (Tang et al., 2019), Jester (Materzynska et al., 2019), Diving-48 (Li et al., 2018) and ImageNet1K (Russakovsky et al., 2015) evaluation hyperparameters that are common to SALT and V-JEPA 2. Note that \"denotes the value is the same as the one used in COIN evaluation. Parameter COIN Jester/DivingImageNet-1K Number of frames Segments / Clip Views / Segment Frame step Epochs Batch size (global) Classifier heads Classifier learning rates Classifier weight decay 16 8 3 4 20 256 1 1e-3 .01 \" 4 \" 2 \" 128 \" \" \" \" 1 \" NA \" 1024 \" \" \" Intuitive physics We follow the protocol established by Garrido et al. (2025) and use the surprise score in our evaluations with IntPhys-2019 or IntPhys (Riochet et al., 2018), GRASP (Jassim et al., 2023) and InfLevel (Weihs et al., 2022) datasets. In the following, we reproduce the equations used by Garrido et al. (2025) to quantify surprise. We let be the context encoder, be the predictor or the decoder and be the target encoder. denotes frames of video clip, denotes the context frames count and denotes the number of future frames. The surprise at time is given by: St = gϕ (fθ (Vt:t+C)) hψ (Vt:t+C+M ) 1. (D.1) The surprise above can then be calculated over all windows to obtain the following global surprise score: Average Surprise = 1 (cid:88) t{1,1+s,...,T (C+M )} St or Maximum Surprise = max t{1,1+s,...,T (C+M )} St. (D.2) , where we set to 2 and use the average surprise score to quantify the surprise between pair of videos following the methodology used by Garrido et al. (2025). The scores are then converted to relative accuracy using label information for video pairs to obtain the relative accuracy values discussed in Table 9."
        },
        {
            "title": "E Additional Results",
            "content": "In this section, we include additional tables and results to support figures and tables in the main paper. E."
        },
        {
            "title": "Intuitive Physics Benchmarks",
            "content": "In this section, we evaluate the intuitive physics understanding of video models. We follow the protocol and datasets described in Garrido et al. (2025) to test video models understanding of intuitive physics in zero-shot setting. Following the protocol of Garrido et al. (2025) we calculate surprise metric that measures the deviations from expected physical behaviour. The benchmark probes the predictor or the decoder to test for physical attributes such as object permanence, spatio-temporal continuity, shape and color constancy, gravity, support, solidity, inertia and collision. We refer the interested reader to Garrido et al. (2025) for additional details on datasets and definition of the attributes mentioned above. 18 Table 9 Comparison on Intuitive physics benchmarks (IntPhys, GRASP, InfLevel). Method Encoder Predictor IntPhys GRASP InfLevel Avg Results reported in (Baldassarre et al., 2025a) VAE COSMOS-4B (Agarwal et al., 2025) ViT-L V-JEPA (Bardes et al., 2024) ViT-H V-JEPA (Bardes et al., 2024) Results reported in (Bordes et al., 2025) V-JEPA 2 Assran et al. (2025) ViT-H Our Eval VideoMAE V2 (Wang et al., 2023b) V-JEPA 2 (V-3.6M) V-JEPA 2 (V-3.6M) V-JEPA 2 (V-3.6M) SALT SALT SALT ViT-g ViT-B ViT-L ViT-H ViT-B ViT-L ViT-H 4B 22M 22M 22M 12M 22M 22M 22M 22M 22M 22M 99.5 92.2 89. 87.2 59.4 76.6 96.9 88.5 72.4 90.6 95.8 60.1 67.0 73.0 61.3 56.1 53.0 65. 56.4 53.5 58 44.8 58.9 59.9 68.1 72.7 74.1 54.4 52.4 57.4 60. 54.9 54.2 58.2 58.4 61.7 69.1 71.2 61.2 66.1 70.7 Table 9 shows the results of this benchmark for SALT as well as several baseline video models. Table 9 shows that SALTs average accuracy scales with model size. SALT compares favorably with published baselines including COSMOS-4B (Agarwal et al., 2025), V-JEPA (Bardes et al., 2024) and V-JEPA 2 (Assran et al., 2025) and VideoMAEv2 (Wang et al., 2023b). Finally, we consider setup where we train V-JEPA 2 and SALT models using the same dataset and optimization budget. We observe that V-JEPA 2 models trained under the conditions stated above compare favorably with SALT. These results suggests that emergent intuitive physics understanding behaviour observed in video models trained with V-JEPA objective (Garrido et al., 2025) is seen with SALT as well. E.2 Impact of Teacher-Student Compute Allocation Figure 8 SALT trained with compute budget of (a) 120K steps (b) 160K steps and (c) 240K steps. The X-axis shows the number of steps allocated to the teacher with the rest used to optimize the student. Observe that the optimal allocation favors training the student longer than the teacher. Figure 8 provides an alternative view of the plot shown in Figure 7. We train ViT-L model in this ablation. It is clear from Figure 8 that the teacher encoders downstream performance increases with the number of training steps across all values of total number of training steps. Remarkably, the student encoders improve over the teachers that they use to obtain predictions targets. The best performing model is obtained by training teacher for 40,000 steps and using the remaining steps on the student. This observation suggests that the optimal compute allocation should favor the student. E.3 How to choose Teacher checkpoint? question that arises naturally with SALT is whether there is principled way choose an optimal teacher checkpoint. By optimal, we here mean choosing checkpoint that maximizes the students performance. We study this question empirically by looking at the correlation between the students benchmark accuracy and 19 Figure 9 Teacher quality vs. student performance. We take all the teachers trained in SALT and measure the RankME(Garrido et al., 2023) of the embedding and pretraining loss and analyze the corelation between them and students downstream performance. Each point represents single SALT run. We control the total training budgets of 160k for both stages to the be same for all models in this comparison. teachers embedding rank, training loss and teacher models downstream accuracy. Figure 9a shows plot of embeddings rank where the embeddings are extracted from teacher model versus student accuracy. We use RankMe (Garrido et al., 2023) to estimate the embedding rank. Garrido et al. (2023) have shown that high embedding rank is necessary condition for good downstream performance in joint-embedding self-supervised learning (JE-SSL) models. We observe from Figure 9 that the the teachers embedding rank is not predictive of students downstream performance. similar trend can be observed from Figure 9b with the teachers pixel reconstruction or training loss. We see that neither the teacher loss nor its embedding rank are predictive of downstream students performance. Floating Point Operations (FLOPs) Estimation common approach to estimating the total training compute for Transformers, including ViTs, is by using the well-known 6N formula (Kaplan et al., 2020; Hoffmann et al., 2022). Here stands the for the model parameter count while represents the total number of tokens used to train the model. This simple approximation assumes that the backward pass during training costs twice as much as the forward pass. Consequently, we use 2N to approximate the training compute of teacher model that provides targets in distillation-based methods. The total number of input tokens observed by model during training is function of input resolution, spatial for image models and spatio-temporal for video models, the patch size, the batch size per step and the total number of optimization steps. Note that we include the embedding layer in our parameter count. With these preliminaries in place, we present the total compute estimate for models presented in Table 1. VideoMAEv2 (Wang et al., 2023b) We use masking ratio of 0.9 and 0.5 for VideoMAEv2 (Wang et al., 2023b) encoder and decoder respectively. The model considered here is ViT-g model with an input patch size of 14 that operates on spatial input of size 16 16. Our analysis uses 1200 epochs for calculating the number of tokens in the encoder and decoder. The other details used to estimate FLOPs is shown in Table 10. V-JEPA 2 (Assran et al., 2025) The spatial resolution is 256 256 with patch size of 16. We use masking ratio of 0.9 for the encoder on average following the recommendation made in V-JEPA (Bardes et al., 2024). The predictor operates on token count that is half of that seen by the encoder due to temporal stride being set to 2. In other words, the predictor sees the union of mask tokens used for missing regions and context tokens used for visible regions. We assume that the model is trained for 240,000 steps using batch size of 3072. Note that we need to account for the teacher forward call that we do in our analysis. SALT The spatial resolution is 224 224 with patch size of 16. We use masking ratio of 0.9 for all stages in our training as we use the same multi-block masking for training teacher and student encoders. The rest of the details are identical to those described above for V-JEPA 2. The main difference between our method and 20 V-JEPA 2 is the use of same-sized or smaller teacher encoder as well using smaller resolution for the inputs. Together, these significantly lower the training compute requirements for SALT over V-JEPA 2. Finally, we report the GPU-hours estimated by running training for 20 steps on single NVIDIA A100 GPU in Table 11. We observe strong correlation between the ordering of models provided by GPU-hours versus that obtained from 6ND FLOPs estimate. Table 10 FLOPs estimate for VideoMAEv2, V-JEPA 2 and SALT models. Model Input Resolution VideoMAEv2-g/14 16 224 V-JEPA 2 L/16 V-JEPA 2 H/16 V-JEPA 2 g/16 SALT-L/16 SALT-H/16 SALT-g/16 SALT-G/16 16 256 256 16 256 256 16 256 256 16 224 224 16 224 224 16 224 224 16 224 224 Ne (B) 1. 0.303 0.632 1.012 0.3 0.6 1.0 1.8 NT (B) Np (B) 0.012 De (109) 331. 0.303 0.632 1.012 0.303 0.303 0.303 0.303 0.022 0.022 0.022 0.022 0.022 0.022 0.022 302.0 302.0 302.0 154.1 154.1 154.1 154. Dp (109) 165.9 3019.9 3019.9 3019.9 1541.4 1541.4 1541.4 1541.4 Dt (109) 1510.2 1509.9 1509.9 770.7 770.7 770.7 770. # Samples Total FLOPs (B) 1.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 (1021) 2. 1.9 3.5 5.3 1.2 1.5 1.8 2.6 Table 11 Training compute and GPU hours. We evaluate our models on SSv2 with input of 1623 (*V-JEPA 2 uses spatial resolution of 256 256, and SALT utilizes 224 224.). We compute TFLOPs under the same batch size and masking strategy, and measure on one single A100 GPU for all methods to ensure fairness and we exclude data-loading overhead and GPU-communication load from all measurements to ensure they are CPU-agnostic. The results in this table are used in Figure 1. Method Teacher Params Student Params # Seen Samples (109) Total Compute (1021 FLOPs) GPU-hrs V-JEPA 2 ViT-L Assran et al. (2025) V-JEPA 2 ViT-H Assran et al. (2025) V-JEPA 2 ViT-g Assran et al. (2025) SALT-Stage-1 ViT-L SALT ViT-L SALT ViT-H SALT ViT-g* SALT ViT-G* 302M 631M 1B N/A 302M 302M 302M 302M 302M 631M 1B 302M 302M 631M 1B 2B 7.4 7.4 7.4 7.4 7.4 7.4 7.4 7.4 1.9 3.5 5. 0.24 1.2 1.5 1.9 2.6 9800 14377 18708 9062 8263 9574 10476 12379 SSv2 Top-1 (%) 1623 1611 73.7 74.0 75.3 - 74.9 75.4 76.2 76.1 69.6 69.6 72. 66.2 71.3 72.6 72.9 73."
        },
        {
            "title": "G Compute Budget specified via FLOPs and optimization steps",
            "content": "While we use FLOPs in our accuracy-compute trade-off analysis, we specify compute via the total number of optimization steps in our experiments. The use of latter quantity is natural in our setup as the teachers EMA update in V-JEPA 2 (Assran et al., 2025) depends on the student getting updated first which is similar to the nature of the update in our two-stage training as the teacher needs to be trained first followed by the student. The advantage with SALT is due from the fact that the teacher training is light-weight, and crucially, once teacher model is trained, it may be used to train multiple student models."
        },
        {
            "title": "H Additional Tables",
            "content": "Table 12 V-JEPA 2 vs. SALT on same pretraining set. Kinetics-400 uses 16 1 1 (number of frames in clip by temporal crops by spatial crops), Something-Something v2 (SSv2) uses 16 1 1 while COIN is run with 16 8 3. All models are evaluated using spatial resolution of 224 224 pixels. The results in this table are used in Figure 2. Method Teacher Student IN-1K K400 SSv2 COIN Diving-48 Jester Avg V-JEPA 2 (w/ our dataset) SALT Stage 1 (V-Pixel) SALT Stage 2 ViT-B ViT-L ViT-H N/A N/A ViT-B ViT-L ViT-B ViT-L ViT-H ViT-B ViT-L ViT-B ViT-L ViT-H ViT-g ViT-G 66.9 73.7 76.7 70.3 75.5 74.8 79.0 79.6 79.7 80. 66.9 73.3 73.6 65.2 70.4 70.9 76.0 77.2 78.0 78.9 61.4 68.4 68.9 60.9 66.2 66.1 71.3 72.6 72.9 73. 68.4 83.1 84.9 73.3 77.9 80.5 85.3 87.0 87.0 87.5 71.0 82.1 84.5 72.9 76.8 78.7 82.5 86.4 85.5 85. 95.9 97.0 97.1 95.7 96.9 96.7 97.2 97.3 97.4 97.4 71.8 79.6 81.0 73.1 77.3 78.0 81.9 83.4 83.4 83. Table 13 Ablation on teacher pretraining datasets. We test teacher and student model using frozen-backbone evaluation %). We report Top-1 accuracy on K400, SSv2, and ImageNet-1k, and COIN. This ablation is used in Figure 4. Dataset # Samples K400 SSv2 IN1k COIN Avg V-3.6M (default) K710 Panda2.8M SSv2 K710 + SSv2 ImageNet-1k V-3.6M (default) K710 SSv2 Panda2.8M K710 + SSv2 ImageNet-1k SALT-teacher 3,626,089 657,217 2,799,959 168,913 826,130 1,281,167 70.4 71.0 69.2 56.8 70.0 51.9 SALT-student V-3.6M (default) 75.5 75.7 72.9 75.3 75.1 72. 66.2 65.2 64.9 63.6 67.8 39.3 70.9 70.6 69.8 70.5 71.1 66.5 75.5 74.2 75.3 61.7 74.0 80.6 78.4 78.4 76.2 78.4 78.0 79.1 77.9 78.9 79.5 69.0 79.2 67.4 84.9 85.0 83.1 84.4 84.9 82. 72.5 72.3 72.2 62.8 72.8 59.8 77.4 77.4 75.5 77.2 77.3 74.9 22 Table 14 Teacher model size ablation. We report frozen-backbone Top-1 on K400, SSv2, IN1K, and COIN. The top block shows teachers evaluated directly. The lower blocks show students distilled from different teacher sizes (two student sizes: ViT-L and ViT-G). The data in this table is used in Figure 6."
        },
        {
            "title": "Model size",
            "content": "K400 SSv2 IN1k COIN Avg"
        },
        {
            "title": "Student",
            "content": "ViT-B ViT-L (default) ViT-H ViT-G 65.2 70.4 71.1 73.6 60.9 66.2 67.4 68.5 SALT-student ViT-B ViT-L (default) ViT-H ViT-G ViT-B ViT-L (default) ViT-H ViT-L ViT-G 74.4 75.5 75.6 75.5 76.9 77.6 77.5 69.5 70.9 70.7 71.5 71.8 71.9 72. 70.3 75.5 76.7 77.4 78.0 78.4 78.3 78.5 79.0 79.3 79.6 73.3 77.9 81.5 81.4 84.0 84.9 84.4 84.7 85.4 87.0 85. 67.4 72.5 74.2 75.2 76.5 77.4 77.3 77.6 78.3 79.0 78.7 Table 15 Masking strategy ablation. We report frozen-backbone Top-1 on K400, SSv2, IN1k, and COIN; Top block: teachers evaluated directly. Bottom block: students (fixed student recipe) trained from different teachers. The data in this table is used in Figure 5. Teacher masking strategy # masks Masking ratio Student Mmsking strategy K400 SSv IN1k COIN Avg Long-short block mask (default) Random tube Random tube Causal mask Long-short block mask (default) Random tube Random tube Causal mask 2 2 1 2 2 2 1 2 SALT-teacher 0.9 [0.9, 0.9] [0.9] [0.9, 0.9] SALT-student 0.9 [0.9, 0.9] [0.9] [0.9, 0.9] Long-short block mask 70.4 69.2 67.9 47. 75.5 75.0 75.0 71.7 66.2 64.7 63.3 34.3 70.9 70.3 70.4 66.3 75.5 74.1 72.6 57.3 78.4 78.1 78.0 75.0 77.9 74.9 73.5 58. 84.9 84.3 84.8 81.7 72.5 70.7 69.3 49.5 77.4 76.9 77.1 73.7 Table 16 Computeaccuracy tradeoffs at matched total steps. Each block fixes the total pretraining steps (budget) and compares V-JEPA,2 to our twostage schedule (teacher+student). FLOPs are reported as 1021. Metrics are frozenbackbone Top-1 on K400, SSv2, IN1k, and COIN. This table includes data presented in Figure 7. Teacher Steps Teacher FLOPs Student Steps Student FLOPs Total FLOPs Total Steps K400 SSv IN1k COIN Avg V-JEPA 2 (baseline) 80k+80k 40k+80k 80k 40k 20k V-JEPA 2 (baseline) 80k 40k 20k V-JEPA 2 (baseline) 80k 40k 20k 1.900 0.717 0.597 0.241 0.121 0.061 1.260 0.241 0.121 0. 0.951 0.241 0.121 0.061 80k 120k 160k 200k 220k 80k 120k 140k 40k 80k 100k Budget: 240k total steps 0.476 0.714 0.951 1.190 1. Budget: 160k total steps 0.476 0.714 0.833 Budget: 120k total steps 0.238 0.476 0.595 23 1.900 1.193 1.311 1.192 1.311 1. 1.260 0.717 0.835 0.894 0.951 0.479 0.597 0.656 240k 240k 240k 240k 240k 240k 160k 160k 160k 160k 120k 120k 120k 120k 73.3 76.5 76.8 76.0 76.3 75. 73.5 75.5 75.5 74.9 73.3 74.0 75.0 74.6 68.4 71.8 71.7 71.3 71.7 71.2 68.3 70.9 70.9 70.6 67.9 69.5 70.2 70.1 73.7 79.0 79.3 79.0 79.1 78. 74.8 78.4 78.4 78.0 74.7 77.7 78.0 77.5 83.1 86.8 86.7 85.3 85.6 84.9 82.9 84.9 85.4 84.6 81.5 84.5 84.9 84.5 74.6 78.5 78.6 77.9 78.2 77. 74.9 77.4 77.6 77.0 74.4 76.4 77.0 76."
        },
        {
            "title": "I Additional Figures",
            "content": "Figure 10 Correlation between RankME and downstream accuracy. We use the same SALT-Stage-1-80k teacher checkpoint. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}