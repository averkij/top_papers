{
    "paper_title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
    "authors": [
        "Hao-Shu Fang",
        "Branden Romero",
        "Yichen Xie",
        "Arthur Hu",
        "Bo-Ruei Huang",
        "Juan Alvarez",
        "Matthew Kim",
        "Gabriel Margolis",
        "Kavya Anbarasu",
        "Masayoshi Tomizuka",
        "Edward Adelson",
        "Pulkit Agrawal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 2 1 4 4 4 0 . 9 0 5 2 : r DEXOP: Device for Robotic Transfer of Dexterous Human Manipulation Hao-Shu Fang1,2, Branden Romero1,2, Yichen Xie3, Arthur Hu2, Bo-Ruei Huang2, Juan Alvarez2, Matthew Kim2, Gabriel Margolis1,2, Kavya Anbarasu2, Masayoshi Tomizuka3, Edward Adelson2 and Pulkit Agrawal1,2 1 Improbable AI Lab 2 Massachusetts Institute of Technology 3 UC Berkeley fhs@mit.edu, pulkitag@mit.edu Figure 1: (a) DEXOP is passive exoskeleton that links human hand movements to passive robotic hand movements through mechanical linkages. (b) DEXOP enables humans to collect task demonstrations of diverse and highly dexterous tasks. (c) Data collected by DEXOP can be used to train policies that transfer to robots. Abstract: We introduce perioperation, paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io. Keywords: Perioperation, Dexterous Manipulation, Exoskeleton, Data Collection"
        },
        {
            "title": "Introduction",
            "content": "Dexterous manipulation is among the most challenging problems in robotics. Machine learning has significantly advanced robotic manipulation, but its reliance on large data poses major bottleneck. * indicates equal contribution; the order of these authors is interchangeable. Bo-Ruei Huang was visiting student at MIT during his participation in this project. Prominent ways of data collection include simulation, human activity videos, and teleoperation. Each has its pros and cons. Simulation based training [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] allows largescale, lower-cost experimentation with precise environmental control, but often requires extensive engineering to set up the simulation, and overcoming the sim-to-real gap [11]. Human videos capture diverse environments and human expertise [12, 13, 14, 15, 16, 17, 18], but recovering finegrained interaction and forces is an open challenge. Real-world robot data collection via teleoperation [19, 20, 21, 22] provides robotic data that can directly be used for policy training without the challenges that come with sim2real or human video demonstrations. However, demonstrating dexterous manipulation is often unnatural for humans and remains costly to scale. More critically, providing humans with rich haptic feedback [23, 24] during teleoperation remains an open question. The absence of haptic feedback can substantially slow down the speed of teleoperation (and hence data collection) and limit demonstration of precise and fine manipulation tasks. In this paper, we advocate for new approach to data collection called perioperation: sensorizing human manipulation to capture rich multisensory data, including vision, proprioception, touch, and action, while maximizing the transferability of the demonstrated skills to robots. The primary distinguishing factor from teleoperation is the focus on constructing devices that human can wear and record data during their manipulation, instead of remotely controlling robot. Challenges in sensorizing human manipulation: In principle, we can sensorize human manipulation by having person wear glove that records hand pose and tactile data. However there are several pragmatic issues: (i) when the target robotic hand differs from the human handnot only in morphology but also in physical properties such as compliance, friction, and skin softnessthe collected data may not transfer well; (ii) most tactile gloves [25, 26] use resistive sensors with lower spatial and force resolution than vision-based or Hall-effect based tactile sensors. It is debatable whether resistive tactile sensors provide information rich enough for robot to reproduce humanlevel dexterity; and (iii) higher-fidelity tactile sensors (e.g., vision-based or Hall-effect) are usually rigid and bulky for humans to wear, impairing users natural dexterity during manipulation. While these sensors can be used to sensorize the human fingertip [27, 28], such information is insufficient for whole-hand manipulation. Following perioperation philosophy, we present DEXOP, novel hand exoskeleton system designed to sensorize human manipulation while user performs dexterous manipulation tasks in natural environments. Because we use high-resolution, but relatively bulky and rigid tactile sensors, DEXOP installs these sensors on passive robotic hand that is coupled through mechanical linkages to passive exoskeleton (see Figure 1(a)) that human can wear. By moving the exoskeleton, the human user can actuate the passive hand to demonstrate diverse tasks (see Figure 1(b)). Three key design objectives guided the development of DEXOP: Making Data Collection Natural: DEXOP makes data collection more natural and scalable by (i) maintaining high force transparency that enables users to perceive real-time and joint-level proprioceptive feedback through the robot hand, which addresses primary limitation of current teleoperation methods, allowing faster (Section 4.2) and more precise (Section 4.3) task demonstrations. (ii) Mapping the human hand pose to the passive robot hands pose (i.e., kinematic coupling) through mechanical design eliminates the need for unintuitive visual hand pose correction during teleoperation, which often arises from inaccurate pose retargeting or kinematics mismatch between the human and the robot hand. This makes the task operation intuitive and thus easier to scale. (iii) Data collection without the full robot, which is more affordable and scalable to diverse environments. High Transferability of Collected Data: Instead of using tactile glove with limitations we discussed above, DEXOP separates the human hand from the passive robotic hand, enabling the codesign of the passive and actual robotic hand to match their kinematic chain, shape, and sensors to maximize the transferability of collected data. Without this separation, it becomes difficult to replicate the same kinematic chain or sensors of the passive robotic hand on the human hand due to space constraints. For example, whole-hand tactile sensing is crucial for faithfully reproducing manipulation, because joint positions alone cannot guarantee the same interaction forces with objects. For instance, when grasping mug, matching the hand pose without matching the contact forces can 2 cause slip (too little force) or breakage (too much force). Since the object may contact the hand at multiple positions, densely capturing forces across the hand (i.e., whole-hand tactile sensing) allows us to preserve the action-relevant contact information needed for accurate and reliable replay on the robot. DEXOP incorporates whole-hand tactile sensing system (shown in Figure 2(b)) that captures detailed force and contact information during interaction, similar to the EyeSight Hand [29]. While DEXOP presents whole-hand tactile sensing, we choose not to focus on using whole-hand sensing to recover the interaction forces in this paper. Instead, we focus on developing DEXOP for demonstrating diverse tasks with the ability to capture whole-hand tactile sensing. We leave force recovery for future work, which will further improve the performance of policies trained with DEXOP data. Enhancing the diversity of accomplished tasks: While DEXOPs proprioceptive feedback can already enable users to do more tasks and be more efficient, we found that different mechanical enhancements can expand DEXOP capabilities. (i) Adding fingernails allows DEXOP to pick objects with shallow profile and manipulate small objects like an M2 screw cap. (ii) Abduction joints for the index/middle/ring fingers allow the fingertips to change the relative distance between each other, supporting better in-hand reorientation and manipulating objects of larger size range. (iii) Finally, padded palm can secure an object more firmly, making whole-hand manipulationssuch as holding seasoning bottle while opening the lid with the thumbmore stable. In Supplementary Section S4, we provide more details of these enhancements. These features have been explored in previous robotic hand designs [30, 31, 32, 33, 29], but not yet in data collection devices due to the challenges of adapting such designs while still allowing humans to manipulate objects effectively. Our approach of separating the human hand from the passive robotic hand makes their integration feasible. In Section 2.2, we summarize the comparison between DEXOP and prior and concurrent data collection devices. In this work, we developed three DEXOP variants with different degrees of freedom (DoF), showing that the design framework of DEXOP can easily be adapted to diverse robotic hands. We demonstrate the utility of DEXOP across variety of dexterous manipulation tasks, including drilling, lamp installation, box packaging, and bottle opening. Through comprehensive user studies, we show that DEXOP provides superior control compared to traditional teleoperation systems and significantly improves data collection throughput. Our work lays the foundation for scalable, real-world robotic data collection and pushes the boundary of whole-hand and precise dexterous manipulation."
        },
        {
            "title": "2.1 Teleoperation for Dexterous Manipulation",
            "content": "Teleoperation is widely used method for robotic data collection. Previous works used webcams, VR devices, or haptic gloves to teleoperate robotic systems [34, 19, 21, 22, 20, 35, 9]. However, most teleoperation approaches either lack haptic feedback or provide limited feedback, for instance, using vibration cues, which are unintuitive. Other haptic feedback systems [23, 24] provide force feedback only at the fingertips and only the normal force, which can be limiting [36]. In contrast, DEXOP provides haptic feedback with both normal and shear forces via the linkage system, and such feedback can optionally be expanded to each finger segment (implemented on DEXOP-7, with details given in Supplementary Section S2)."
        },
        {
            "title": "2.2 Data Collection Hardware for Robot Learning",
            "content": "Due to the growing demand for large-scale robotic data, hardware mechanisms for scalably collecting data have been of increasing interest in recent years. One line of work focuses on low-cost teleoperation systems [37, 38], typically composed of leader-follower setup where correspondence is achieved through joint mapping. Another line of work attempts to collect data without teleoperation by having human directly control passive robot gripper [39, 40, 41, 42] or by capturing human finger motions while performing object manipulation [18, 27]. Representative works like AirExo [39], UMI [40], and DobbE [42] use parallel-jaw grippers and capture only the position of 3 Figure 2: Hardware overview: (a) Variants of DEXOP: DEXOP-12 (4 fingers and 12 DOF), the most advanced DEXOP; DEXOP-9 without the ring finger (3 fingers and 9 DOF); and DEXOP-7 without abduction joints on index and middle fingers (3 fingers, 7 DOF, and co-designed with EyeSight hand). (b) Illustration of human hand controlling the DEXOP-9 to grasp ball, along with the tactile sensor readings and robotic hand pose. the two-finger gripper. Our work advances this direction by (i) enabling manipulation that requires relative motion between fingers and objects, such as in-hand reorientation; (ii) improving manipulation efficiency compared to bi-manual parallel jaw gripper systems that require more steps for task completion; (iii) manipulation of small objects (e.g., screws, nuts) in constrained spaces which is hard to achieve with bi-manual system, but can be achieved by multi-fingered hand; (iv) additional degrees of freedom aslo enable manipulation of articulated objects such as spray bottles. Concurrently, several works [43, 28, 44] explore passive exoskeletons for multi-fingered hands, but their device design does not emphasize fine/precise tasks nor whole-hand manipulation, resulting in the hand being largely used for basic openclose (flexion) motions that simple gripper could also excel at. In contrast, DEXOP unlocks wider variety of tasks (whole-hand and fine manipulation), makes data collection superior (by recording force information), and more intuitive (via force transparency)."
        },
        {
            "title": "2.3 Multi-Finger Hand Exoskeletons",
            "content": "Hand exoskeletons have been extensively studied in both robotics and medical domains, primarily for rehabilitation, force augmentation, and haptic feedback [45, 46, 47, 48, 49]. These systems feature motor systems to actively exert forces on human fingers, enhancing human motor control by amplifying movement and providing force feedback. In recent years, these systems have also been used for teleoperation and robot learning [23]. Our work, DEXOP, took the opposite approach: instead of building motorized exoskeleton to drive the human fingers, we developed passive exoskeleton that allows humans to drive passive robotic hand by moving their own fingers."
        },
        {
            "title": "3 Hardware Design",
            "content": "DEXOP consists of two main components: the passive robotic hand that interacts with objects and the wearable exoskeleton for the human hand. The passive robotic hand is mechanically linked to the exoskeleton through linkage system. Forces applied by the human fingers to the exoskeleton are transmitted to the robotic hand, driving its motion. Conversely, interaction forces experienced by the passive robot hand are fed back to the human user through the same linkage and exoskeleton. 4 In this work, we present three DEXOP variants: DEXOP12 with 4 fingers and 12 degrees of freedom, DEXOP-9 with 3 fingers and 9 degrees of freedom, and DEXOP7 with 3 fingers and 7 degrees of freedom. The first two are used to demonstrate the dexterous perioperation, while the third is co-designed with robotic hand [29] for skill transfer to an actual robot. In this section, we detail the design of DEXOP-12the most capable configurationand provide descriptions of the other versions in Supplementary Section S2. Figure 2(a) shows the different DEXOP versions, and Figure 2(b) shows an example of human controlling DEXOP-9 to grasp ball and the captured data."
        },
        {
            "title": "3.1 Kinematics of the DEXOP",
            "content": "Figure 3: depiction of joints in human hands kinematic chain. The blue joints are presented in DEXOP-12. For the passive robotic hand, our design goal is to closely match the human fingers kinematic chain so that perioperation is more intuitive. DEXOP-12 features 12 fully actuated degrees of freedom (DoF), with 3 DoF per finger. Figure 3 illustrates the kinematic chain of the human hand (colored in blue and orange) and shows which joints are implemented in DEXOP-12 (colored in blue). The 12-DOF of DEXOP-12 are chosen to maximize the diversity of tasks that can be performed: (i) Three fingers are required for single-hand in-hand manipulation, which is essential in constrained spaces and for tasks where the second hand is holding an object (e.g., holding paper while the first hand uses the scissor). The fourth ring finger provides additional support for whole-hand manipulation tasks (e.g., stabilizing the seasoning bottle when opening the lid, see Figure 7) (ii) The index, middle, and ring fingers each have 2-DoF MCP joint and PIP joint (refer to Figure 3 for these joint positions). The 2-DOF MCP joint is used to achieve abduction, which allows for changing the inter-finger distance and thereby increase/decrease the span of the hand for manipulating wider range of object sizes and also aids in-hand object manipulation. (iii) The thumb has 2-DoF TM joint and an IP joint. The TM joint enables flexion motion [50] of the thumb, which moves it to the opposite position of the index and middle fingers. This allows the hand to form antipodal grasps, aiding stable and precise manipulation. DEXOP-12 doesnt feature DIP joint on the index/middle/ring finger, nor the MCP joint on the thumb (shown in orange in Figure 3). While including these joints could increase the diversity of tasks that can be performed, in our view the increase in complexity of the overall design of DEXOP outweighs the potential increase in capabilities. For the wearable exoskeleton, the human finger tip rests in fingertip cot that connects to the palm through exoskeleton segments (see the gray part in Figure 4(a)). If the fingertip cot is not connected to the palm (i.e., free floating), it is mechanically infeasible to design linkage systems that can couple unconstrained human finger motions to the passive robotic hand, as no one-to-one mapping exists. To facilitate simple control of the passive hand, the kinematic design of the exoskeleton matches the passive robotic hands kinematic chain, which enables motion transmission from exoskeleton to passive hand through 4-bar linkages (Section 3.2). While in principle, the kinematic design can be different, doing so would complicate the linkage system without providing additional benefits. When implementing the wearable exoskeleton, two key challenges arise: For the index, middle, and ring fingers, collisions between the exoskeleton and the human fingers occur regardless of whether the exoskeleton is placed in front or behind the fingers, due to their close kinematic similarity and overlapping workspace during flexion (i.e., bending fingers towards the palm). To address this, the exoskeleton segments connecting the fingertip cot to the MCP joint are designed as thin sheet-metal links positioned alongside the fingers. 5 Figure 4: (a) Exploded view of the DEXOP-12 system. (b) Top: Annotated view of the 4-bar linkages coupling the index, middle and ring fingers of the robot hand and exoskeleton. Bottom: Annotated view of the rotary linkage system coupling the thumb of the robot hand and exoskeleton. For detailed explanation, please refer to text in Section 3.2. In the human hand, the two rotational axes of the thumb TM joint are positioned very closely. If the exoskeleton were designed with the same configuration, the limited spacing would cause the thumb exoskeleton to interfere with the users thumb, especially during flexion. To prevent this, we increase the perpendicular distance between the two TM joint axes, moving the abduction axis downward toward the wrist (see TM-1 and TM-2 at the bottom of Figure 4(b)). The thumb exoskeleton is then wrapped around the users thumb to avoid collisions during finger movement. Co-design with actual robotic hand: To maximize the transfer of demonstrated skills to the robot, the actual robotic hand needs to have the same kinematics chain as the passive robotic hand, thereby as the wearable exoskeleton. However, constructing an exoskeleton for the human hand that simply mimics the kinematic chain of an existing robotic hand can be uncomfortable for the user or be impossible to construct in manner that allows human fingers to move naturally. Therefore, we co-design the passive and the real-robotic hand to ensure maximum transfer of collected data and the comfort for the human operator. In Supplementary Section S3, we detail how we co-design DEXOP-7 by modifying the EyeSight Hand [29]."
        },
        {
            "title": "3.2 Linkage Design",
            "content": "The passive robotic hand is driven by the exoskeleton through linkage system (see Figure 4(a)). Since the wearable exoskeleton (shown in gray) shares the same kinematics as the passive robotic hand (shown in light blue), we use multiple 4-bar linkages to control the fingers. In 4-bar linkage system, ground framethe link that remains stationaryis required. To realize this, we use standoffs to connect the base of the passive robotic hand and wearable exoskeleton, thereby fixing their relative distance and serving as virtual ground frame. Reducing the standoff length can make DEXOP less bulky, but if the distance is too short, it constrains human motion due to potential collisions with the passive robotic hand when human moves their fingers. Therefore, the standoff length is set to the minimum distance that avoids such collisions. For the index, middle, and ring fingers, the linkage system is identical and illustrated on top of Figure 4(b). It consists of two 4-bar linkages that drive the two phalanges of each finger. In the first linkage, the fixed distance between the MCP joints of the exoskeleton and the passive robotic hand serves as the virtual ground frame. The exoskeletons proximal phalanx functions as the input link, while the passive hands proximal phalanx serves as the output. curved link (shown in 6 yellow) connects the two PIP joints of the wearable exoskeleton and the passive hand, acting as the coupler link. This 4-bar linkage system fixes the distance between the two PIP joints of the wearable exoskeleton and the passive hand, enabling the construction of second 4-bar linkage system to actuate the distal phalanx. In this configuration, the coupler link of the first 4-bar linkage serves as the ground frame for the second, while the input and output links are the short axes of the distal phalanges on the exoskeleton and the passive hand. The coupler link (shown in dark blue) is also curved to provide additional clearance during movement For the thumb, the linkage system is shown at the bottom of Figure 4(b). The TM joint features two perpendicular axes, with the abduction joint of the exoskeleton and the passive robotic hand aligned coaxially. This allows single coupler link (shown in dark blue) to drive the flexion and abduction axes of the TM joint, enabling control of two degrees of freedom. The IP joint, however, is not parallel to these axes. To control this additional degree of freedom, we introduce second spatial 4-bar linkage. The coupler (shown in yellow) is connected to the distal phalanx via two serially arranged perpendicular joints. This spatial 4-bar linkage system can control the bending of the IP joint given any configuration of the TM joint."
        },
        {
            "title": "3.3 Tactile Sensor",
            "content": "As mentioned in Section 1, replaying the joint position is insufficient for recreating the amount of the force the robot needs to exert. The missing information is the torque exerted by each joint. In an exoskeleton setup, torque cannot be directly captured because it is generated by the human hand. practical alternative is to capture force information at each phalanx along with joint positions, then compute joint torques using the Jacobian transpose method [51]. Without whole-hand force sensing, however, this torque recovery problem becomes ill-posed: external forces may act at multiple contact points across the hand, and without observing these forces, the resulting torques cannot be uniquely or reliably inferred. In this work, we adopt the whole-hand tactile sensing design from the EyeSight Hand [29], equipping the passive robotic hand with GelSim(ple), camera-based tactile sensor. The fingertips, palm, and proximal phalanges are all embedded with GelSim(ple) units. Each sensor uses one fisheye camera (or two in the case of the palm) with 220 field of view for capturing deformations on the entire sensors surface. Whole-hand sensing configuration significantly broadens the contact information in the collected data. For more details, we refer readers to the EyeSight Hand [29]. 3.4 In-the-wild Data Collection DEXOP is convenient tool for rapidly collecting dexterous manipulation data in natural environments. It can be integrated with an arm exoskeleton such as AirExo [39, 52], or use IMU/SLAMbased methods [42, 40] to capture the global position of the hand, allowing robot arm to replicate the motion of the hand. We also attach fisheye camera to the base of the palm (close to the wrist) to capture visual observations. Data collection records global position (can be translation/rotation from SLAM or joint angles captured by arm exoskeleton), hand joint angles, tactile sensor images, in-hand camera views, and/or global scene images."
        },
        {
            "title": "4 Experiments",
            "content": "In Sections 4.1 and 4.2, we evaluate the DEXOP system by measuring its hardware characteristics and comparing its data collection efficiency against teleoperation. Both evaluations require real robotic hand for meaningful comparison. Therefore, for these experimentss, we use the DEXOP-7 variant and its co-designed counterpartthe EyeSight Hand [29]. Section 4.3 provides qualitative analysis of diverse tasks made possible by DEXOP, where we use the DEXOP-9 and DEXOP-12 variants."
        },
        {
            "title": "4.1 Hardware Characteristics",
            "content": "7 good perioperation system should match the workspace, force, and speed of the robotic hand used to deploy the policy. Deficiencies in characteristics matching would indicate that the perioperation data cannot fully utilize the capabilities of the robotic system used for deploying policies, or are too hard for the robotic system to achieve. The performance of DEXOP7 across several critical metrics (force output, workspace, and finger speed) and comparison against the EyeSight hand [29] is summarized in Table 1, where we can see that DEXOP-7 hardware capability is comparable to the actual robotic hand. The experimental procedure and comparison of each metric is presented below. Table 1: Comparison of Force, Workspace, and Speed between DEXOP System and Robotic Hand Metric Category DEXOP-7 EyeSight Hand Max Force (N) Workspace (Degrees) Thumb Index Middle MCP Joint PIP Joint TM joint (flexion) TM joint (abduction) IP joint Max Speed (rad/s) MCP Joint PIP Joint TM joint (flexion) TM joint (abduction) IP 70 60 60 110 105 75 90 65 35 17 56 54 54 120 105 75 90 37 5 32 Force Output We wear DEXOP and press the fingertip of the passive robotic hand on force sensor while fixing the wrist on the table, to measure how much force can be applied at the fingertip. Our experiments provide evidence that DEXOP effectively transmits forces from the human hand to the robotic hand. We measured peak force of around 60 at the index and middle fingertips, and around 70 at the thumb fingertip, which is comparable to the force that the robotic hand can exert and similar to the maximum force that human fingertips can apply [53]. 35 12 9 Workspace Coverage When human subjects wear DEXOP, its workspace closely mirrors the workspace of the robotic hand. The MCP joint on both systems allows approximately 110120 degrees of rotation, while the PIP joints reach up to 105 degrees. The thumbs motion on the DEXOP system fully matches the robotic hands workspace across all three joints. Finger Speed Finger speed was measured to assess how quickly the DEXOP system responds to human input. DEXOPs MCP joint reaches maximum angular velocity of 35 rad/s, slightly below the 37 rad/s of the robotic hand. The PIP and IP joints (see Figure 3) on DEXOP achieve velocities of 15 rad/s and 9 rad/s, respectively, which is 2 to 3 times faster than those of the robotic hand. The TM joint on the DEXOP system is comparatively slower, but it is worth noting that users must exert significant effort to reach these peak speeds with DEXOP. In practice, such high speeds are rarely needed, as they may lead to unstable control in most manipulation tasks."
        },
        {
            "title": "4.2 Comparison with Teleoperation",
            "content": "To evaluate the performance and usability of the DEXOP system, we conducted user study with four participants. Each participant performed four manipulation tasks, as illustrated in Figure 5, using three different control schemes: 1. DEXOP System: Participants controlled the passive robotic hand using the DEXOP-7, enabling direct physical interaction through proprioceptive feedback. 2. Teleoperation: baseline teleoperation system composed of UR3 robotic arm, trakSTAR electromagnetic hand-tracking system, and an EyeSight hand was used. Participants manipulated the robotic hand via visual feedback, without haptic feedback. For more details on the tracking system, see [29]. 3. Direct Human Performance: As an upper-bound reference, participants completed the same tasks using their bare hands. 8 Figure 5: Illustration of evaluation tasks. Drilling: the user must pick up drill standing upright on table, then insert the drill bit into an M2 screw head and tighten it by actuating the drill. Bottle opening: with the bottle placed within the workspace of the hand, the user grasps the bottle and then uses the thumb to unscrew the cap. Box Packaging: the user approaches the an open box, and folds the side flaps before closing the top flap by folding the the securing flap into the box. Bulb installation: the task is composed of three parts, lamp base, light bulb, and light shade. The user picks and screws the light bulb into the lamp base before placing the light shade over the entire assembly. Figure 6: Comparison of task throughput of the drilling, bulb installation, box packaging and bottle opening tasks with TeleOP system, DEXOP and human hand. DEXOP achieves much higher throughput than TeleOP. Each participant completed five trials for each task using each control scheme, resulting in total of 240 trials across the four participants. Prior to task performance, participants were provided brief task explanation and 20-minute practice session. The primary performance metric was task throughput, calculated as the number of successful completions within one minute. Trials that exceeded three minutes were considered failures. The results of the user study are presented in Figure 6. Overall, DEXOP achieves much higher task throughput compared to the teleoperation system. For the drilling task, participants encountered substantial difficulties using teleoperation. No participant successfully completed the task even once. The main failure mode was the difficulty in grasping the drill while keeping it operational. We posit that difficulty in teleoperation is caused by the limited visual feedback, especially for determining whether the index finger engaged the drill trigger. Aligning the drill with the small screw was also challenging. In contrast, with the DEXOP system, participants completed the task an average of 6 times per minute. When using their own hands, participants achieved an average of 11 completions per minute. For the bulb installation task, performance improved slightly under teleoperation: 15 out of 20 trials were successful, with an average completion time of 86 seconds. With the DEXOP system, however, participants completed the task in just 11 seconds on averageeight times faster than with teleoperation. Using their own hands, participants completed the task in approximately 4 seconds. The box packaging task was also highly challenging. Only 3 out of 20 trials were successful under teleoperation, with successful attempts averaging around 80 seconds. Most failures occurred when participants tried to fold the flapsoften pushing the box away in the processor while attempting to insert the flap edge into the slot, which frequently caused the box to move or the flap to collapse. With the DEXOP system, participants completed the task an average of 5 times per minute7 times faster than teleoperation. Using their own hands, participants completed the task 16 times per minute. The bottle opening task was found to be relatively easy by the participants. Teleoperation resulted in an average throughput of 5 times per minute. Using the DEXOP system, participants achieved 12 completions per minute, 2.4 faster. When using their own hands, the average throughput reached 22 times per minute. In conclusion, DEXOP is much more efficient than teleoperation in accomplishing tasks that require proprioceptive feedback during manipulation, and is also better data collection option on simple tasks such as bottle opening, where teleoperation can also accomplish effectively. Nevertheless, our experiments also suggest that this type of device has the potential for improvement to further approach human performance."
        },
        {
            "title": "4.3 Dexterous Perioperation Ability",
            "content": "The focus of our work is showcasing fine-grained dexterity with focus on tasks that may not be achievable by parallel-jaw grippers. We categorize tasks into two types: (1) Precise finger manipulation, which primarily involves fine finger motions. Examples include object reorientation, syringe operation, and tiny screw and cap manipulationtasks that demand precision in fingertip control and pose adjustment of objects. (2) Whole-hand manipulation, which requires coordinated use of both the fingers and palm. It is pretty common during tool use [54, 55]. Representative tasks include opening the lid of seasoning bottle or depressing sprayer with the thumb while constraining the bottle/sprayer with index/middle/ring fingers and the palm, and squeezing the handle of paper cutter to release the blade and cut paper. These examples illustrate complex hand-environment interactions where force application and whole-hand contact are essential. Qualitative demonstrations of both categories are shown in Figure 7. For more comprehensive view of DEXOP capabilities, we invite readers to visit our website: https://dex-op.github.io. These results demonstrate DEXOPs unique dexterous manipulation capabilities, which are not easily achievable via teleoperated dexterous hand or parallel jaw grippers."
        },
        {
            "title": "5 Preliminary Policy Learning Experiments",
            "content": "To evaluate the effectiveness of DEXOP as data collection tool for robot learning, we trained and evaluated policies learned from this data. We first describe the robot platform and how DEXOP data is collected and aligned with the real robot. Then we describe the neural network architecture and tasks, and finally report the performance. As the corresponding robotic hands for DEXOP-9 and DEXOP-12 are still under development, to deploy the policy on the robot, we use DEXOP-7 and the co-designed version of EyeSight Hand [29]. This ensures compatibility in both kinematics and sensing, allowing us to directly transfer data collected via perioperation to the real robot platform."
        },
        {
            "title": "5.1 Robot Platform Setup",
            "content": "To evaluate policies trained with DEXOP-collected data, we build two EyeSight Hands [29] and mount them on Unitree H1 humanoid robot (see Figure 8(a)). Each EyeSight Hand has 3 fingers and 7 degrees of freedom. The Unitree H1 provides 4 degrees of freedom per arm, resulting in combined 22-DoF system. During evaluation, the H1 is suspended from gantry. fisheye camera is mounted on the base of the palm of each EyeSight Hand, near the wrist, to provide visual feedback. 10 Figure 7: Various tasks illustrating the dexterity that DEXOP can achieve. Arrows in the figure denote the motion of the object or its part being manipulated."
        },
        {
            "title": "5.2 Data Collection Method",
            "content": "DEXOP records finger joint positions, tactile images and wrist images. To capture global hand position, DEXOP is mounted onto an in-house built whole-arm exoskeleton (AirExo-2 [52]) customized to match the Unitree H1s kinematics (see Figure 8(b)). The AirExo-2 includes position encoders at each joint. 11 Figure 8: (a) Unitree H1 with EyeSight hands used for policy evaluation. (b) The DEXOP and AirExo setup, used for both teleop and DEXOP data collection. (c) The views from the left and right wrist cameras mounted on the exoskeleton system. Whole-hand tactile sensing for both hands would require 4 Raspberry Pis and 4 ArduCam CamArray HATs (2 per hand). To make the electronics manageable, we enable tactile sensors only at the distal phalanx of each hand for this task, reducing the number of required Raspberry Pis and camera HATs to two. The tactile images from each hand are concatenated into single super-image per hand. In total, we stream 22 joint angles, 2 wrist images (see Figure 8(c)), and 2 tactile super-images at 20 Hz. For comparison, we also collect teleoperation data. Since our DEXOP +AirExo2 system tracks all the joints for the robot hand and upper body of H1, we directly map the joint angles from the exoskeleton to the robot, without the need to solve inverse kinematics. Note that this teleoperation setup does not introduce any errors due to kinematic differences and is already better than most teleoperation setups that need kinematic retargetting."
        },
        {
            "title": "5.3 Data Post-Processing",
            "content": "Our DEXOP system shares the same tactile sensors, kinematics, and visual configuration as the EyeSight Hand. Similarly, the customized AirExo-2 shares its kinematic structure with the Unitree H1. As result, no further data processing is needed to close the embodiment gap; the data can be directly used to train the policy."
        },
        {
            "title": "5.4 Task Setup",
            "content": "To evaluate the transferability of DEXOP-collected data to real-world robot execution, we evaluated performance on complex bimanual manipulation task, bulb installation, modified from the lamp task from FurnitureBench [56]. This task is long-horizon, contact-rich, and requires both fine dexterous control and coordinated bimanual action. It integrates multiple low-level skills under partial tactile and visual observability. We decompose the task into six key stages, as shown in Figure 9, each targeting specific manipulation capability: (A) Grasp Base: The robot must reliably grasp the lamp base from resting position. This stage evaluates the robustness of basic grasping skills under positional variations in the environment. (B) Grasp Bulb: The lamp is spherical object, which makes grasping more sensitive to misalignment. The robot must orient its fingers precisely along the objects diameter to avoid slipping, making this step test of fine grasp alignment. (C) Bulb Insertion: This is high-precision phase requiring the lamp to be aligned and inserted into the base socket. Success depends on both accurate motion planning and closed-loop correction based on real-time vision and tactile feedback. If the insertion is not accurate initially, the robot must detect misalignment and adjust accordingly. (D) Bulb Installation: Once inserted, the robot must rotate the lamp to screw it into the base. This step requires coordinated multi-finger manipulation, along with perception of subtle torque or force changes to determine whether the bulb is securely installed. The robot must use tactile sensing to infer task state and trigger transition to the next stage. Figure 9: Illustration of our task for policy learning. Wrist camera images highlighted in green showcase successful actions for each stage, while images in red show different failure modes, such as misaligning the light bulb during insertion. Each of the six stages is shown in states after their respective actions have been performed successfully. (E) Grasp Lamp Shade: This stage revisits basic grasping, this time for the lamp cover. Success depends on recognizing object geometry and executing reliable power grasp. (F) Cover Bulb: Finally, the robot must place the cover over the assembled lamp, requiring coordinated motion between both arms and fine alignment to avoid collision or tilt. This evaluates bimanual coordination and spatial reasoning with large object. This complete sequence tests wide spectrum of dexterous capabilities, including grasp robustness, finger coordination, fine alignment, tactile-informed decision making, and bimanual coordination, making it strong benchmark for learning from DEXOP-collected demonstrations. 13 Figure 10: (a) Success rates of policies trained on mixed DEXOP and TeleOP data, broken down by task step. Note that these are cumulative success rates, where performance in earlier stages affects the outcomes of subsequent stages. (b) Speed comparison of data collection for DEXOP and TeleOP, averaged time for each trajectory by task step. Error bars denote the standard error of the mean."
        },
        {
            "title": "5.5 Learning Setup",
            "content": "To validate that data collected using DEXOP can be effectively used for training dexterous manipulation policies, we trained policies using behavior cloning. We use the Action Chunking Transformer (ACT) [37] as our policy architecture. The inputs to the policy include: Two wrist camera images (one per hand), providing visual context. Two tactile super-images (one per hand), each contains three concatenated tactile images from the thumb/index/middle distal phalanges. To emphasize contact changes, we use the delta image between the current and initial tactile readings. The current joint state, including hand and arm configurations. This input is used as query following the original ACT implementation. The network outputs: Delta joint positions for robot arms. This improves robustness to fabrication variability and calibration noise from the exoskeleton hardware, since relative motion is more transferable than absolute position in such systems. Absolute joint positions for the hands. Since our hand hardware is precisely machined and co-designed with DEXOP, absolute joint commands generalize well without significant error accumulation. To improve generalization and robustness during training, we apply several augmentations: Color jittering (brightness and hue randomized within 0.1) is applied to wrist camera images to account for lighting variation. Joint noise of up to 10 degrees is injected into the arm states with 10% chance, simulating variability in human arm posture or assembly imprecision. Dropout of 0.3 is applied to the wrist image stream, following prior work [29], to prevent overreliance on vision and encourage integration of tactile information."
        },
        {
            "title": "5.6 Results of DEXOP and TeleOP",
            "content": "We train different policies on different data sources collected by DEXOP and teleop and compare their performance. While in principle, we can directly use the data collected by DEXOP to train the policy, we observed accumulated joint errors in the arm exoskeleton data due to in-house fabrication variabilitye.g., errors in cutting carbon fiber tubes and assembling components. During data 14 collection, human operators may also introduce variability by bending their backs, which causes additional misalignment between the exoskeleton and the robot because we fix the robot in an upright position in this paper. While this issue could be addressed by mounting the arm exoskeleton on stationary base frame [52], or using IMU sensors to track torso orientation and compensate for it on the robot platform, doing so is beyond the scope of this work. To mitigate this challenge, we augment the DEXOP data with small amount of teleoperated robot demo and co-train the policy on mix of DEXOP and teleop data. The reason is that during teleoperation, the human can observe the motion of the robot and compensate for the errors of the arm exoskeleton, which wouldnt happen in DEXOP data collection. We trained four policies under different conditions: DEXOP + TeleOP (160 + 40 demos): 160 demonstrations collected using DEXOP combined with 40 demonstrations collected via teleoperation. TeleOP (200 demos): 200 demonstrations collected via teleoperation, matching the total number of demonstrations in the first condition. TeleOP (100 demos): 100 demonstrations collected via teleoperation, matching the total data collection time of the first condition, given DEXOP approximately 2.67 faster data collection rate on this task. TeleOP (40 demos): 40 demonstrations collected via teleoperation, serving as baseline without any DEXOP data compared to the first condition. . i=1 Si 6 Training is conducted on an NVIDIA H100 GPU with batch size of 110, and each model is trained for 800 epochs. For the model using combination of DEXOP and TeleOP data, training is performed on the mixed dataset for 500 epochs, followed by 300 epochs on the TeleOP subset to help calibrate errors in the arm exoskeleton assembly. For each policy, we repeat evaluations 40 times with small perturbations (2 cm translation, 5 rotation) on object location. The results are given in Figure 10(a). In Table 2, we report the total data collection time for each condition and their respective normalized cumulative success rate over all the 6 steps, 6 The 160 DEXOP + 40 teleop policy achieves the highest success rates across all stages, particularly excelling in challenging steps of bulb insertion and grasp lamp cover. In contrast, the 40 teleop policy shows the poorest performance, with rapid degradation in later stages. Although the data collection time for 100 teleop and 160 DEXOP + 40 teleop is the same, the performance of 100 teleop is considerably worse. This result highlights that DEXOP provides more efficient way to collect data for training robust policies. Table 2: Data collection time and normalized cumulative success. Method 160 DEXOP + 40 TeleOP 200 TeleOP 100 TeleOP 40 TeleOP 139.3 283.3 141.7 56.7 Time (mins) Success 0.5130.032 0.4250.032 0.3550.031 0.3500.031 We surprisingly observe that the 200 teleop policy is worse than 160 DEXOP + 40 teleop, despite teleoperation data generally being considered high quality. Specifically, two stages contribute most to this performance gap: the bulb insertion and grasp lamp cover steps. For bulb insertion, we hypothesize that exoskeleton-based data introduces greater variation (less precise) in object positioning during collection, because operators tend to be more relaxed when doing perioperation than teleoperation. For lamp cover grasping, we observe that the 200 teleop policy often fails by continuously rotating the bulb without transitioning to grasping the lamp cover. To better understand this behavior, we analyze the time spent in each stage during data collection and report the results in Figure 10(b). The results show that, during teleoperation, operators spend substantially more time (38 VS 6 seconds) than DEXOP in the screw bulb into base stage due to the lack of haptic feedback. Without shear force sensing, it is difficult for operators to judge if the bulb is fully tightened, often leading to over15 rotation. This overemphasis introduces bias in the dataset, causing the policy to prioritize rotation behaviors instead of transitioning to subsequent steps, leading to frequent failures in the grasp lamp shade stage. These findings suggest that teleoperation not only reduces data collection efficiency but also introduces systematic biases due to limited sensory feedback to operators, ultimately degrading policy performance. On the contrary, DEXOP provides rich force feedback, leading not only to faster demonstrations, but also to cleaner, less biased data that better reflect successful human strategies, ultimately improving policy generalization and performance in downstream tasks."
        },
        {
            "title": "6 Discussion",
            "content": "Perioperation for large-scale data collection. Perioperation systems are sensitive to manufacturing defects in the exoskeleton and to sensor calibration. Mismatches between the target robot platform and the exoskeleton can degrade performance to the extent that DEXOP data alone may not be sufficient to deploy learned policy. In this paper, we compensated for such errors by incorporating teleoperation data. One way to address this issue is to improve the calibration of the exoskeleton. Other exciting avenues include: (i) treating DEXOP as data-collection tool for pretraining foundation model, with small errors compensated by real robot data obtained through either real-world reinforcement learning or teleoperation; and (ii) developing more powerful manipulation models that are robust to minor errors in robot action labels. While perioperation systems require additional design effort, we believe that investing in their development and then distributing them offers unmatched scalability compared to teleoperation. Dexterity needs careful design. When designing DEXOP, we found it relatively easy to enable multi-finger hand to grasp large objects, but far from trivial to grasp small objects (with one dimension smaller than 5mm) or manipulate articulated objects using its redundant degrees of freedom. Releasing the potential of dexterity requires careful design, as mentioned in Supplementary Section S4. Without such attention to detail, we found that perioperation with multi-finger hand can be inferior to using simple parallel-jaw gripper, especially when grasping small objects. DEXOP incorporates many of these design considerations, though there is still room for improvement in the future such as adding DIP joint and optimizing the side and back finger shapes. Limitations and future work. Several limitations remain in our current paper. First, estimating joint torques from tactile and kinematic data still requires sensor calibration and real-time inference. Second, the current EyeSight hand does not have the degrees of freedom that the human hand does to allow us to use DEXOP data to perform very dexterous tasks like in-hand reorientation. Third, the current DEXOP system provides proprioceptive feedback but still misses the tactile feedback to humans. Fourth, we have only scratched the surface of what learning from perioperation data can achieve; future work will explore more learning methods, like how to combine tactile and vision that play different roles in different stages of manipulation, and how to enable precise force control during manipulation. Broader vision. As robot hands become more capable, the lack of high-quality data remains major bottleneck. We believe that systems like DEXOP fill crucial missing layer between raw human demonstrations and robotic generalization. By making it easier to capture rich, tactile, contact-driven data at scale, we hope this work will accelerate the co-evolution of better data, better hardware, and ultimately, more dexterous robots."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the members of the Improbable AI lab for the helpful discussions and feedback on the paper. We thank Lirui Wang for helping with the human study of perioperation versus teleoperation, Sameen Ahmad and Xavier Sanchez for helping with assembling the EyeSight hand and the DEXOP. This project benefits from the AAU program of Arducam. The authors sincerely thank their support 16 on multiple cameras data collection system, especially Ajax. This research was partly supported by Toyota Research Institute and Magna Inc. We acknowledge support from ONR MURI under grant number N00014-22-1-2740. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government."
        },
        {
            "title": "Author contributions",
            "content": "Project formulation: Hao-Shu Fang, Pulkit Agrawal; Paper writing: Hao-Shu Fang, Pulkit Agrawal; DEXOP design: Hao-Shu Fang, Arthur Hu, Juan Alvarez; DEXOP building: Hao-Shu Fang, Arthur Hu; EyeSight hand co-design: Branden Romero, Hao-Shu Fang; EyeSight hand building: Branden Romero; AirExo2 building: Matthew Kim; DEXOP driver: Bo-Ruei Huang; Teleoperation infrastructure: Bo-Ruei Huang, Hao-Shu Fang, Gabriel Margolis; Data collection: Hao-Shu Fang, Kavya Anbarasu, Matthew Kim; Learning and deployment: Hao-Shu Fang, Yichen Xie; Advice and support for hardware manufacturing: Edward Adelson, Pulkit Agrawal"
        },
        {
            "title": "References",
            "content": "[1] T. Chen, J. Xu, and P. Agrawal. system for general in-hand object re-orientation. In Conference on Robot Learning, pages 297307. PMLR, 2022. [2] T. Chen, M. Tippur, S. Wu, V. Kumar, E. Adelson, and P. Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. Science Robotics, 8(84):eadc9244, 2023. [3] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y. Ma, R. Calandra, and J. Malik. General in-hand object rotation with vision and touch. In Conference on Robot Learning, pages 25492564. PMLR, 2023. [4] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):320, 2020. [5] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubiks cube with robot hand. arXiv preprint arXiv:1910.07113, 2019. [6] H. Zhu, A. Gupta, A. Rajeswaran, S. Levine, and V. Kumar. Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost. In 2019 International Conference on Robotics and Automation (ICRA), pages 36513657. IEEE, 2019. [7] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal. Reconciling reality through simulation: real-to-sim-to-real approach for robust manipulation. arXiv preprint arXiv:2403.03949, 2024. [8] L. L. Ankile, A. Simeonov, I. Shenfeld, M. T. Villasevil, and P. Agrawal. From imitation to refinement residual rl for precise visual assembly. In CoRL 2024 Workshop on Mastering Robot Manipulation in World of Abundant Data. [9] Y. Park, J. S. Bhatia, L. Ankile, and P. Agrawal. Dexhub and dart: Towards internet scale robot data collection. arXiv preprint arXiv:2411.02214, 2024. [10] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak. Generalization in dexterous manipulation via geometryaware multi-task learning. arXiv preprint arXiv:2111.03062, 2021. [11] Y. Park, G. B. Margolis, and P. Agrawal. Position: Automatic environment shaping is the next frontier in rl. In Forty-first International Conference on Machine Learning. [12] D. Antotsiou, G. Garcia-Hernando, and T.-K. Kim. Task-oriented hand motion retargeting for dexterous manipulation imitation. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 00, 2018. [13] J. Orbik, S. Li, and D. Lee. Human hand motion retargeting for dexterous robotic hand. In 2021 18th International Conference on Ubiquitous Robots (UR), pages 264270. IEEE, 2021. [14] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu, and X. Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. arXiv preprint arXiv:2108.05877, 2021. 17 [15] K. Shaw, S. Bahl, and D. Pathak. Videodex: Learning dexterity from internet videos. In Conference on Robot Learning, pages 654665. PMLR, 2023. [16] P. Mandikal and K. Grauman. Dexvip: Learning dexterous grasping with human hand pose priors from video. In Conference on Robot Learning, pages 651661. PMLR, 2022. [17] I. Radosavovic, X. Wang, L. Pinto, and J. Malik. State-only imitation learning for dexterous manipulation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 78657871. IEEE, 2021. [18] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. Liu. Dexcap: Scalable and portable mocap data collection system for dexterous manipulation. arXiv preprint arXiv:2403.07788, 2024. [19] Y. Qin, H. Su, and X. Wang. From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation. arXiv preprint arXiv:2204.12490, 2022. [20] S. Yang, M. Liu, Y. Qin, R. Ding, J. Li, X. Cheng, R. Yang, S. Yi, and X. Wang. Ace: cross-platform visual-exoskeletons system for low-cost dexterous teleoperation. arXiv preprint arXiv:2408.11805, 2024. [21] R. Ding, Y. Qin, J. Zhu, C. Jia, S. Yang, R. Yang, X. Qi, and X. Wang. Bunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning. arXiv preprint arXiv:2407.03162, 2024. [22] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang. Open-television: teleoperation with immersive active visual feedback. arXiv preprint arXiv:2407.01512, 2024. [23] I. Sarakoglou, A. Brygo, D. Mazzanti, N. G. Hernandez, D. G. Caldwell, and N. G. Tsagarakis. Hexotrac: In 2016 IEEE/RSJ highly under-actuated hand exoskeleton for finger tracking and force feedback. International Conference on Intelligent Robots and Systems (IROS), pages 10331040. IEEE, 2016. [24] H. Zhang, S. Hu, Z. Yuan, and H. Xu. Doglove: Dexterous manipulation with low-cost open-source haptic force feedback glove. arXiv preprint arXiv:2502.07730, 2025. [25] S. Sundaram, P. Kellnhofer, Y. Li, J.-Y. Zhu, A. Torralba, and W. Matusik. Learning the signatures of the human grasp using scalable tactile glove. Nature, 569(7758), 2019. doi:10.1038/s41586-019-1234-z. [26] Y. Luo, C. Liu, Y. J. Lee, J. DelPreto, K. Wu, M. Foshey, D. Rus, T. Palacios, Y. Li, A. Torralba, et al. Adaptive tactile interaction transfer via digitally embroidered smart gloves. Nature communications, 15 (1):868, 2024. [27] C. Xing, H. Li, Y.-L. Wei, T.-A. Ren, T. Tu, Y. Lin, E. Schumann, W.-S. Zheng, and M. R. Cutkosky. Taccap: wearable fbg-based tactile sensor for seamless human-to-robot skill transfer. arXiv e-prints, pages arXiv2503, 2025. [28] M. Xu, H. Zhang, Y. Hou, Z. Xu, L. Fan, M. Veloso, and S. Song. Dexumi: Using human hand as the universal manipulation interface for dexterous manipulation. arXiv preprint arXiv:2505.21864, 2025. [29] B. Romero, H.-S. Fang, P. Agrawal, and E. Adelson. Eyesight hand: Design of fully-actuated dexterous robot hand with integrated vision-based tactile sensors and compliant actuation. arXiv preprint arXiv:2408.06265, 2024. [30] W. K. Do, A. K. Dhawan, M. Kitzmann, and M. K. III. Densetact-mini: An optical tactile sensor for grasping multi-scale objects from flat surfaces. 2023. arXiv preprint arXiv:2309.08860. [31] A. Laron, E. Sne, Y. Perets, and A. Sintov. Print-n-grip: disposable, compliant, scalable and one-shot 3d-printed multi-fingered robotic hand. 2024. arXiv preprint arXiv:2401.16463. [32] M. Burgess and E. H. Adelson. Grasp everything (get): 1-dof, 3-fingered gripper with tactile sensing for robust grasping. 2025. arXiv preprint arXiv:2505.09771. [33] K. Shaw, A. Agarwal, and D. Pathak. Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning. arXiv preprint arXiv:2309.06440, 2023. [34] U. Martinez-Hernandez, M. Szollosy, L. W. Boorman, H. Kerdegari, and T. J. Prescott. Towards wearable interface for immersive telepresence in robotics. In Interactivity, Game Creation, Design, Learning, and Innovation: 5th International Conference, ArtsIT 2016, and First International Conference, DLI 2016, Esbjerg, Denmark, May 23, 2016, Proceedings 5, pages 6573. Springer, 2017. [35] Z.-H. Yin, C. Wang, L. Pineda, F. Hogan, K. Bodduluri, A. Sharma, P. Lancaster, I. Prasad, M. Kalakrishnan, J. Malik, et al. Dexteritygen: Foundation controller for unprecedented dexterity. arXiv preprint arXiv:2502.04307, 2025. 18 [36] R. Luo, C. Wang, C. Keil, D. Nguyen, H. Mayne, S. Alt, E. Schwarm, E. Mendoza, T. Padr, and J. P. Whitney. Team northeasterns approach to ana xprize avatar final testing: holistic approach to telepresence and lessons learned. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 70547060. IEEE, 2023. [37] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. [38] P. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel. Gello: general, low-cost, and intuitive teleoperation framework for robot manipulators. arXiv preprint arXiv:2309.13037, 2023. [39] H. Fang, H.-S. Fang, Y. Wang, J. Ren, J. Chen, R. Zhang, W. Wang, and C. Lu. Airexo: Low-cost exoskeletons for learning whole-arm manipulation in the wild. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1503115038. IEEE, 2024. [40] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. arXiv preprint arXiv:2402.10329, 2024. [41] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3):49784985, 2020. [42] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chintala, and L. Pinto. On bringing robots home. arXiv preprint arXiv:2311.16098, 2023. [43] T. Tao, M. K. Srirama, J. J. Liu, K. Shaw, and D. Pathak. Dexwild: Dexterous human interactions for in-the-wild robot policies. arXiv preprint arXiv:2505.07813, 2025. [44] Z. Si, J. E. Chen, M. E. Karagozler, A. Bronars, J. Hutchinson, T. Lampe, N. Gileadi, T. Howell, S. Saliceti, L. Barczyk, et al. Exostart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations. arXiv preprint arXiv:2506.11775, 2025. [45] S.-S. Yun, B. B. Kang, and K.-J. Cho. Exo-glove pm: An easily customizable modularized pneumatic assistive glove. IEEE Robotics and Automation Letters, 2(3):17251732, 2017. [46] B. B. Kang, H. Choi, H. Lee, and K.-J. Cho. Exo-glove poly ii: polymer-based soft wearable robot for the hand with tendon-driven actuation system. Soft robotics, 6(2):214227, 2019. [47] Y. Yun, P. Agarwal, J. Fox, K. E. Madden, and A. D. Deshpande. Accurate torque control of finger joints In 2016 IEEE/RSJ International Conference on with ut hand exoskeleton through bowden cable sea. Intelligent Robots and Systems (IROS), pages 390397. IEEE, 2016. [48] P. Agarwal, J. Fox, Y. Yun, M. K. OMalley, and A. D. Deshpande. An index finger exoskeleton with series elastic actuation for rehabilitation: Design, control and performance characterization. The International Journal of Robotics Research, 34(14):17471772, 2015. [49] P. W. Ferguson, Y. Shen, and J. Rosen. Hand exoskeleton systemsoverview. Wearable Robotics, pages 149175, 2020. [50] W. P. Cooney, M. J. Lucca, E. Y. S. Chao, and R. L. Linscheid. The kinesiology of the thumb trapeziometacarpal joint. The Journal of Bone and Joint Surgery. American Volume, 63(9):13711381, 1981. doi:10.2106/00004623-198163090-00004. [51] C. Chen, Z. Yu, H. Choi, M. Cutkosky, and J. Bohg. Dexforce: Extracting force-informed actions from kinesthetic demonstrations for dexterous manipulation. IEEE Robotics and Automation Letters, 2025. [52] H. Fang, C. Wang, Y. Wang, J. Chen, S. Xia, J. Lv, Z. He, X. Yi, Y. Guo, X. Zhan, L. Yang, W. Wang, C. Lu, and H.-S. Fang. Airexo-2: Scaling up generalizable robotic imitation learning with low-cost exoskeletons. arXiv preprint arXiv:, 2025. [53] K. An, L. Askew, and E. Chao. Biomechanics and functional assessment of upper extremities. In Trends in ergonomics/human factors III, volume 1986, pages 573580. Elsevier Science Publishers North-Holland, 1986. [54] J. R. Napier. The prehensile movements of the human hand. The Journal of Bone and Joint Surgery. British Volume, 38-B(4):902913, 1956. doi:10.1302/0301-620X.38B4.902. [55] M. W. Marzke, K. L. Wullstein, and S. F. Viegas. Evolution of the power (squeeze) grip and its morphological correlates in hominids. American Journal of Physical Anthropology, 89(3):283298, 1992. doi:10.1002/ajpa.1330890303. [56] M. Heo, Y. Lee, D. Lee, and J. J. Lim. Furniturebench: Reproducible real-world benchmark for longhorizon complex manipulation. In Robotics: Science and Systems, 2023."
        },
        {
            "title": "Supplementary",
            "content": "S1. Electronics of DEXOP: Each revolute joint in the passive robotic hand is equipped with an iC-MH16 12-bit angular encoder, offering resolution of 1.5e-3 radians. An RS-485 interface IC is used to transmit the encoder signals. We designed custom PCB to aggregate all RS-485 signals and relay them to host computer via USB. The same PCB also supplies power to the tactile sensors. For the tactile camera modules, we use IMX219 color sensors with fisheye lenses. Data from multiple cameras is collected using Arducam 8MP4 quadrascopic camera bundle kits. S2. Design of DEXOP-9 and DEXOP-7: We illustrate the kinematics of the DEXOP-9 and DEXOP-7 in Figure S1. Compared to DEXOP-12, DEXOP-9 remove the ring finger, and DEXOP-7 further remove the abduction at the MCP joints. Figure S1: (a) Blue joints represent the kinematic chain of the DEXOP-9. (b) Blue joints represent the kinematic chain of the DEXOP-7. Orange joints are the missing joints compared to human hand kinematics. The implementation of DEXOP-9 closely follows the implementation of DEXOP-9 but just removes the ring finger. For the implementation of DEXOP-7, besides removing the abduction at the MCP joints, we also explore another method to connect human hand to the wearable exoskeleton, shown in Figure. S2. Instead of using fingertip cot, we attach both the distal and the proximal phalanges of the finger to the exoskeleton using velcro fastener (shown in black). By attaching both phalanges to the exoskeleton, we can provide force feedback to human users on each of those finger segments. However, since the kinematics of the exoskeleton and the human finger are not perfectly aligned, relative sliding between them would occur when the operator bends their fingers. Thus, each finger segment is attached to linear slider (shown as sliding joint in the figure), which is fixed on the exoskeleton and can compensate for relative sliding. S3. Co-design of DEXOP-7 and EyeSight Hand: The development of DEXOP-7 was closely coupled with the design of the EyeSight Hand. We began by constructing the passive robotic hand component of DEXOP-7. As discussed in Section 3.1, one of our guiding principles was to match the kinematic chain of the human hand, which required both the exoskeleton and the robotic hand to follow anatomically similar joint configurations. Fortunately, the EyeSight Hand was originally designed with this goal in mindit already aligned well with human hand kinematics. Therefore, the initial version of DEXOP-7 was built to mirror the existing EyeSight Hand. Figure S2: Illustration of attaching distal and proximal phalanges of human finger to wearable exoskeleton on DEXOP-7, which provides force feedback at each finger segment. 20 However, early testing revealed critical mismatch: the thumbs interphalangeal (IP) joint on the EyeSight Hand was designed to bend inward toward the palm to support precise grasping. In contrast, the human IP joint bends outward. This discrepancy made it difficult for users to wear the exoskeleton naturally. Figure S3: (a) The thumb on passive robotic hand without and with co-designing with the actual robotic hand. (b) The middle finger on the passive robotic hand without and with co-designing with the actual robotic hand. To address this, we redesigned the passive robotic hands thumb to orient the IP joint outward, consistent with human anatomy (see Figure S3 (a)). But simply modifying the thumb reduced its contact area with the middle fingertip during precise grasping. To compensate, we also adjusted the middle finger by tilting its fingertip (see Figure S3 (b), thereby restoring wide contact area for thumb-middle finger interactions. These two key modifications (adjusting the thumb and middle finger) were incorporated back into the EyeSight Hand. This ensures consistency between the exoskeleton and the robotic hand, enabling smoother policy transfer between human demonstrations and robot execution. S4. Details of hardware enhancements: The design principles discussed in Section 3.1 and Section 3.2 enable perioperation of passive robotic hand, but simply having multi-finger hand does not guarantee dexterous capabilities. We observed that the curved fingerpad and bulky backs of the fingers make it difficult to perform antipodal precision grasps on small objects placed on table, as the backs of the fingers tend to collide with the surface. Reducing finger thickness and adding fingernails enables DEXOP to manipulate small objects, such as picking up coin from table or retrieving an M2 screw from pile. In-hand reorientation often requires substantial side contact, so incorporating abduction at the MCP joints and increasing the lateral contact surface of the fingers improves the ability to grip objects between them. The palm is designed to be elastic, allowing it to press objects firmly against the distal phalanges. Finally, the exoskeletons finger cots are adjustable, enabling adaptation to different finger lengths and providing more intuitive control. 21 Figure S4: Fingernails (top left) assist in picking up small objects. Abduction joints (bottom left) enable inhand reorientation and gripping between fingers. Adjustable finger cots (top right) adapt the exoskeleton to different finger lengths. The palm pad (bottom right) secures objects during whole-hand manipulation."
        }
    ],
    "affiliations": [
        "Improbable AI Lab",
        "Massachusetts Institute of Technology",
        "UC Berkeley"
    ]
}