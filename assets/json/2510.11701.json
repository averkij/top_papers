{
    "paper_title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "authors": [
        "Zhaochen Yu",
        "Ling Yang",
        "Jiaru Zou",
        "Shuicheng Yan",
        "Mengdi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 0 7 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "DEMYSTIFYING REINFORCEMENT LEARNING IN\nAGENTIC REASONING",
            "content": "Zhaochen Yu1 Ling Yang3 1National University of Singapore 3Princeton University Jiaru Zou2 2University of Illinois at Urbana-Champaign Shuicheng Yan1 Mengdi Wang3 Code: Open-AgentRL, Model: DemyAgent-4B"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute high-quality, real end-to-end agentic SFT dataset along with highquality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Beyond pre-training and supervised fine-tuning (SFT) stages, recent advancements in reinforcement learning (RL) (Schulman et al., 2017; Rafailov et al., 2023; Yang et al., 2025c;b; Shao et al., 2024; Wang et al., 2025d;c) have introduced new scaling axis that aligns large language models (LLMs) behavior to incentivize reasoning fidelity by encouraging the generation of effective chain-of-thought (CoT) trajectories. Building on this, the paradigm of agentic reasoning (Li et al., 2025a; Wu et al., 2025; Li et al., 2025c; Sun et al., 2025; Jin et al., 2025; Feng et al., 2025; Dong et al., 2025b) further empowers LLMs to move beyond self-contained generation, equipping them with the ability to integrate external tools throughout the reasoning process. This shift has unlocked remarkable progress across domains such as mathematics, scientific discovery, and code generation. Despite the rapid growth of these advances, scaling RL for agentic reasoning remains challenging. Directly applying policy optimization methods such as GRPO (Shao et al., 2024; Guo et al., 2025) often leads LLM agents to suffer from suboptimal training and inference behaviors, including inefficient on-policy rollout sampling, reward & entropy collapse (Cui et al., 2025), and unstable training dynamics. This highlights the unsolved limitations from three perceptiveness: Equal Contribution. Contact: ly1988@princeton.edu, Corresponding authors."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Figure 1: An overview of our research on agentic RL."
        },
        {
            "title": "1 Data wise. Current data curation pipelines often rely on stitch-style data synthesis (Feng et al.,\n2025; Schick et al., 2023), where segments of internal reasoning are manually replaced with tool\noutputs. Such patchwork overlooks the natural connectivity of reasoning and tool use, preventing\nthe data from faithfully mimicking real multi-turn trajectories that indicate when and why tools\nshould be invoked.",
            "content": "2 Algorithm wise. Despite rapid progress in GRPO-based variants, the optimal RL recipe for agentic reasoning remains unclear. Existing methods differ in their optimization granularity (token-, sequence-, or trajectory-level) and impose distinct inductive biases: some encourage exploration by relaxing clipping (Yu et al., 2025) or managing entropy (Wang et al., 2025b), while others suppress it through strong KL regularization (Cheng et al., 2025b) or conservative clipping. principled understanding of when and how to deploy these algorithms is still missing. 3 Reasoning Mode wise. Open puzzles are unsolved regarding the allocation of turn budgets, the trade-off between response length and tool-call efficiency, and the impact of long-CoT predispositions on multi-turn reasoning. These uncertainties obscure the principles of agent reasoning modes, often leading to either overthinking (long, inefficient loops) or underthinking (premature tool reliance) during the agents reasoning process. The above challenges motivate us to perform systematic investigation of recent studies along these three perspectives, aiming to identify key factors that either hinder or enhance agentic reasoning in LLM agents as shown in fig. 1. Specifically, we organize our paper as follows: In Section 3 (to address 1 ), we analyze how data curation design and diversity affect SFT and RL training stages, respectively. We observe that training directly on synthetic trajectories fails to provide reliable signals for when and how to invoke tools, preventing agents from learning optimal integration points, and also lacks sufficient data diversity to encourage effective exploration. To address this, we curate real end-to-end SFT dataset and high-diversity, model-aware RL dataset that improve the training efficiency and agentic reasoning performance. In Section 4 (to address 2 ), we compare GRPO-based RL algorithms and find that conservative clipping and KL divergence penalty overly constrain exploration during training. In addition, we analyze the roles of pass@k and average@k as guiding metrics, revealing how they capture the explorationexploitation trade-off and highlight performance bottlenecks. We further show that sustaining higher entropy, especially for weaker models, is the key to improving RL efficiency. In Section 5 (to address 3 ), we investigate reasoning-mode components such as the number of tool calls and overall response length, and their relationship to performance. We find that fewer, more deliberate tool interactions often yield better results, showing that over-reliance on external calls does not necessarily improve performance and that the key lies in effective and accurate tool invocations integrated into the models agentic reasoning process. Beyond the insights gained from our comprehensive study, we also provide strong baseline model called DemyAgent-4B, which could achieve SOTA-level performance in challenging benchmarks and outperform larger-sized models in agentic reasoning as shown in table 2. We present our main contributions and the detailed baseline recipes in section 6."
        },
        {
            "title": "2 PROBLEM FORMULATION",
            "content": "In this section, we first formalize the notation used throughout the paper, followed by an overview of the reinforcement learning (RL) algorithms we adopt. Finally, we outline our training and evaluation setup. Research Purpose Our goal is to demystify reinforcement learning in agentic reasoning. By systematically analyzing three dimensions: data, algorithm, and reasoning mode. We extract insightful and practically applicable strategies that improve the stability, efficiency, and overall performance of agentic reasoning."
        },
        {
            "title": "2.1 AGENTIC REINFORCEMENT LEARNING",
            "content": "In this section, we formulate the agentic RL training objective as: ExD, yπθ(x; ) (cid:2) rϕ(x, y) (cid:3) β DKL (cid:0)πθ(y x; ) (cid:13) (cid:13) πref (y x; )(cid:1) . (1) max πθ where denotes the set of available tools, πθ represents the policy LLM, πref is the reference LLM, rϕ and DKL denote the reward function and KL divergence respectively. The input is sampled from dataset D, and is the corresponding output, possibly interleaved with tool-call feedback. Unlike conventional RL that relies purely on LLM rollouts, agentic RL (Dong et al., 2025b; Feng et al., 2025; Li et al., 2025d; Dong et al., 2025a; Singh et al., 2025) integrates tool-call feedback during reasoning. The rollout distribution factorizes as Pθ(R, x; ) = tR(cid:89) t=1 (cid:124) Pθ(Rt R<t, x; ) (cid:123)(cid:122) Agentic Reasoning (cid:125) ty (cid:89) t=1 (cid:124) Pθ(yt y<t, R, x; ) . (2) (cid:123)(cid:122) Answer Generation (cid:125) where is the reasoning trajectory of length tR, interleaved with tool-call feedback, and is the final answer with length ty. In this paper, we mainly focus on rule-based RL algorithms like GRPO (Shao et al., 2024), which is widely adopted to optimize LLM-based Agents. 2.2 GRPO-BASED ALGORITHM AND TECHNIQUES Here we utilize GRPO (Shao et al., 2024) as our baseline algorithm. To better compare the difference between RL techniques that improve GRPO algorithm, we formulate the following objective in more general format: JGRPO(θ) = (cid:20) D, {R}G i=1 πref (cid:21) (cid:40) Agg(G, R) min (cid:104) ri,t(θ) ˆAi,t, clip(cid:0)ri,t(θ), 1 ϵlow, 1 + ϵhigh (cid:1) ˆAi,t (cid:105) β DKL(πθ πref) (cid:41) . (2) Here Agg(G, R) is the loss aggregation granularity, ri,t(θ) is the importance ratio related to the type of loss aggregation granularity, and ϵ represents the clip ratio. ˆAi,t is the normalized advantage across all tokens: ˆAi,t = rϕ(x, yt) mean({rϕ(R1), . . . , rϕ(RG)}) std({rϕ(R1), . . . , rϕ(RG)}) . (3) In our study, we focus on three key improvement techniques (Yu et al., 2025; Zheng et al., 2025) for GRPO: 1) Loss Aggregation Granularity, 2) Reward Shaping, 3) Clipping Strategy. For loss"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "aggregation granularity, we compare two kinds of loss, which can be formulated as: AggTok(G, R) = 1 i=1 Ri (cid:80)G (cid:88) Ri (cid:88) , i=1 t=1 AggSeq(G) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) , i=1 (4) (5) where AggTok(G, R) is the token-level loss, and AggSeq(G) is the sequence-level loss, and the corresponding importance ratio is formulated as: rTok i,t (θ) = πθ(Ri,(t)Ri,<t, τ ) πref (Ri,(t)Ri,<t, τ ) rSeq i,t (θ) = ( πθ(Rix, τ ) πref (Rix, τ ) 1 Ri , ) (6) (7) i,t (θ) is the importance ratio of token-level loss and rSeq here rTok i,t (θ) is the importance ratio of sequence-level loss. For the reward function. We optimize composite reward that sums an outcome term (solution accuracy) and tool-use term (number of invocations), with the tool bonus clipped to avoid degenerate tool abuse reward hacking. It could be formulated as: rout+tool(x, y, n) = (cid:26)1 + 0.1n min(1 + 0.1n) if match(yt, y) otherwise (8) Here, is the number of tool invocations. For another reward function, which is known as overlong reward shaping. Specifically, overlong reward shaping gives zero reward when the output length is within safe budget, then applies linear penalty as length approaches the maximum (from Lmax Lcache to Lmax), and assigns -1 if it exceeds Lmax. This preserves smooth learning signal near the boundary while strongly discouraging overlong completions. It can be formulated as follows: rlength(y) = 0, (Lmax Lcache) Lcache 1, Lmax Lcache, , Lmax Lcache < Lmax, (9) Lmax < y. 2.3 RECIPE DESIGN In our study, we investigate three key techniques for agentic RL: Loss Aggregation Granularity, Reward Shaping, and Clipping Strategy. We construct three different recipes: GRPO-TCR: We incorporate Token-level loss, Clip higher and overlong Reward shaping techniques with GRPO. GRPO-SCR: We incorporate Sequence-level loss, Clip higher, overlong Reward shaping techniques with GRPO. GRPO-T: we adhere to the implementation in (Shao et al., 2024), and change the samplelevel loss to token-level, and we take this recipe as our baseline."
        },
        {
            "title": "3 DATA IN AGENTIC REASONING",
            "content": "This section empirically examines how data affects agent training, comparing real end-to-end agentic vs synthetic stitch-style trajectories for cold-start SFT and evaluating high-diversity and modelaware datasets designed to maintain exploration to achieve effective RL. 3.1 REAL END-TO-END TRAJECTORIES VERSUS SYNTHETIC STITCH-STYLE TRAJECTORIES Motivation Current agentic training pipelines often rely on LLM-edited or template-based synthetic trajectories, which replace selected reasoning steps with tool invocations like ReTool (Feng et al.,"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Table 1: Comparison between the impact of our curated real end-to-end SFT dataset and the synthetic SFT dataset on AIME 2024 and AIME 2025. Dataset & Metric Qwen2.5-7B-Instruct Qwen3-4B-Instruct-2507 w/ synthetic trajectory w/ real trajectory w/ synthetic trajectory w/ real trajectory AIME 2024 average@32 pass@32 maj@ AIME 2025 average@32 pass@32 maj@32 6.77% 42.11% 10.50% 5.21% 25.56% 12.08% 17.91% (+11.14 %) 57.57% (+15.46 %) 27.13% (+16.63 %) 18.24% (+13.03 %) 48.42% (+22.86 %) 29.18% (+17.10 %) 4.38% 35.15% 0.01% 3.65% 22.22% 0.10% 33.23% (+28.85 %) 75.66% (+40.51 %) 51.64% (+51.63 %) 29.79% (+26.14 %) 72.88% (+50.66 %) 45.82% (+45.72 %) 2025). While scalable, such stitch-style data inevitably misses critical decision cues: not only how to call tool, but also when, why, and what to do next. This raises the question: do real end-to-end trajectories provide qualitatively richer learning signals and stronger initialization for RL? Setup. For the synthetic baseline, we directly adopt the multi-turn SFT dataset from ReTool (Feng et al., 2025), where challenging long-CoT steps are substituted with tool invocations and responses. For real end-to-end trajectories, we use our curated dataset mentioned in appendix A.3. We finetune Qwen2.5-7B-Instruct and Qwen3-4B-Instruct-2507 on both datasets (real vs. synthetic) under identical settings, and evaluate the agentic reasoning performance on AIME2024/AIME2025. Result. We evaluate using average@32 (overall agent performance), pass@32 (ability boundary (Deng et al., 2025)), and maj@32 (performance stability). As shown in table 1, real trajectories deliver clear improvement: Qwen3-4B-Instruct-2507 trained on real data achieves 29.97% on average@32, 72.88% on pass@32, and 45.22% on maj@32 on AIME2025. In contrast, the synthetic baseline yields below 10% on average@32 with unstable performance and significantly lower ability upper bound. Thus, real trajectories establish much stronger and more stable starting point for RL. Unless otherwise stated, we use SFT checkpoints trained on our curated real agentic dataset, denoted as Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT. Analysis. The superiority of real trajectories lies in their ability to capture complete agentic reasoning behaviors through real end-to-end reasoning processes that synthetic stitching cannot replicate. Specifically, our dataset preserves: (i) pre-call analysis, localizing which subproblems are efficiently solved via tools; (ii) guarded execution, with intermediate checks; (iii) error recovery and strategy revision, after failed attempts; (iv) self-reflection and calibration, before invoking tools. Takeaway 3.1: Curating Agentic SFT Data Real agentic trajectories with coherent and end-to-end tool-use behaviors can not only teach the agent to use tools but it also scale the ability boundary and produce more stable reasoning, while synthetic trajectories fail. 3.2 DIVERSE DATA MAINTAINS HIGH ENTROPY IN TRAINING Motivation. Most existing works (Feng et al., 2025; Shang et al., 2025; Li et al., 2025d; Dong et al., 2025a) focus on purely mathematical datasets for RL training, aiming to enhance problemsolving ability on reasoning-heavy benchmarks. While intuitive, this narrow scope overlooks critical factor: dataset diversity. Prior discussions of diversity in multi-task RL (Havrilla et al., 2024; Shen et al., 2025) only emphasize outcome-level benefits. However, how diversity influences the training dynamics, especially policy entropy and exploration efficiency, remains underexplored. Setup. We construct our RL dataset with higher diversity in appendix A.4, for comparison, we choose DAPO-Math-17k as the baseline. We utilize GRPO-TCR (mentioned in section 2.3) to finetune Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT under identical hyperparameters and training budgets. Result. As shown on the right in fig. 2, training with the diverse dataset leads to significantly higher entropy gain during the early stage and sustains this entropy at higher level throughout"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Figure 2: Comparison between our dataset with higher diversity and the ReTool dataset, which only contains math problems. Left is the average@32 accuracy on AIME2025 during training based on two different dataset. Right is the policy entropy during the training process. convergence. This indicates that diverse data directly drives richer exploration behaviors. Moreover, in the left figures of fig. 2, we observe faster and more efficient learning: with our diverse dataset, the agent achieves over 50% average@32 accuracy on AIME2025 within only 150 steps, while the DAPO-Math baseline requires 220 steps to reach the same level. Analysis. We interpret entropy as proxy for exploration breadth: higher entropy means the policy continues to consider diverse reasoning paths rather than prematurely collapsing to narrow deterministic strategy (We will discuss the entropy mechanism in detail in section 4.2 and section 4.3). Thus, dataset diversity not only improves outcome metrics but also reshapes the training dynamics by maintaining exploration capacity, making RL both faster and more stable. Takeaway 3.2: Dataset Construction for RL training Diverse RL datasets sustain higher policy entropy, directly incentivizing broader exploration and yielding faster, more stable agentic RL training. 3.3 MODEL-AWARE DATASETS FOR MORE EFFECTIVE RL Motivation. During training, we observed clear divergence between two models of different capacity: Qwen3-4B-Instruct-2507 exhibited consistent and sustained policy improvement, while Qwen2.5-7B-Instruct failed to improve despite identical algorithms and datasets, rapidly encountering bottleneck. Specifically, the average reward of Qwen2.5 stagnated around zero, while Qwen3 consistently achieved positive rewards. This illustrates competencedifficulty mismatch: when the base policy is too weak relative to the dataset, it cannot extract meaningful gradients for policy updating. To address this issue, we construct model-aware RL dataset that adapts the task distribution to the capacity of the model. Setup. We use our SFT model to perform 8 rollouts per problem on the 30k RL dataset, taking the proportion of correct solutions as proxy for problem difficulty with respect to the given model. After trajectory verification, we discard the problems with 0% or 100% accuracy (which provide no learning signal) and label the remainder with three difficulty levels: easy (accuracy 0.75), medium (0.75 > accuracy > 0.25), and hard (accuracy 0.25). Since the Qwen3 model already shows effective training on the full dataset, we use its empirical difficulty histogram as the target distribution. Based on this distribution, we curate model-aware dataset tailored for Qwen2.5 and retrain it using the same GRPO-TCR algorithm and hyperparameters as in the original setting. Result. As shown in fig. 3, training with the curated model-aware dataset yields significantly more effective improvement than with the unfiltered dataset. Moreover, fig. 3 shows that the average reward rises substantially, producing stronger and more consistent gradient signals. Consequently, it provides more valid rewards for the computation of advantage, amplifying the gradient signals and leading to more effective and stable RL training. After breaking the performance bottleneck, we can collect the model-aware dataset based on its current ability for more effective training."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Figure 3: The comparison and analysis between the impact of the 30k full dataset and our tailored dataset for Qwen2.5-RA-SFT on subsequent RL training. Left is the average@32 performance on AIME2025. Right is the analysis for the average reward during training. Takeaway 3.3: Data Selection for RL training Model-aware data provides stronger gradient signals, amplifying learning feedback to overcome weak-model performance bottlenecks and improve RL training efficiency."
        },
        {
            "title": "4 ALGORITHMIC DESIGN AND TRAINING DYNAMICS IN AGENTIC RL",
            "content": "Recent advancements in RLVR for reasoning LLMs, especially GRPO-based methods (Yu et al., 2025; Dong et al., 2025b; Zheng et al., 2025; Zhao et al., 2025; Liu et al., 2025a;b) have demonstrated that there are many possible techniques that could further enhance policy optimization. Other works (Cui et al., 2025; Deng et al., 2025; Agarwal et al., 2025; Chen et al., 2025) focus on exploring the training dynamics (e.g., entropy and pass@k) to explain why and how these improvements could emerge. For agentic RL, however, it remains unclear (i) what techniques work best for policy optimization, (ii) what is the relationship between the exploration(pass@k)-exploitation(average@k), and (iii) how does entropy affect training effectiveness, stability, and final performance. 4.1 THE IMPACT OF THE RLVR TECHNIQUES ON AGENTIC RL Setup. In this section, we conduct our experiments based on three different recipes: GRPO-TCR, GRPO-SCR and GRPO-T as mentioned in section 2.3. Based on the experiment results, we aim to pinpoint simple yet effective techniques that deliver consistent improvements in final performance and training efficiency. Specifically, we utilize the hyperparameters and reward functions mentioned in section 2.2 to train both Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT based on our complete RL dataset to compare the training dynamics of different recipes. For GRPO-TCR, the ϵhigh is set to 0.28, and ϵlow is set to 0.20, for GRPO-SCR, the ϵhigh is set to 0.0004 and ϵlow is set to 0.0003. For GRPO-T, the ϵ is set to 0.20 and the reward function is rout+tool. For both recipes that incorporated with overlong reward shaping, the reward is denoted as rϕ = rout+tool + rlength. Specifically, we also use three metrics to comprehensively evaluate the impact of the applied RLVR techniques on AIME2024 and AIME2025 benchmarks. Result. First, we compare GRPO-TCR and GRPO-T to investigate the impact of clip higher and overlong reward shaping on Agentic RL. Specifically, for Qwen3-4B-RA-SFT, GRPO-TCR has achieved remarkable improvements compared to GRPO-T on AIME2024/AIME2025. It achieves 70.93%/68.13% with an initial accuracy of 29.79% and 33.23% on average@32 metric within only 450 steps. In contrast, GRPO-T only achieves the best average@32 performance of 54.7%/ 40.93% on AIME2024/AIME2025, which GRPO-TCR could achieve within only 100 training steps, utilizing only 25% of the training computation of GRPO-T. The results indicate that simply applying clip higher and overlong reward shaping techniques could effectively improve the agentic reasoning performance and the efficiency of agentic RL. Then, we compare GRPO-TCR and GRPO-SCR to investigate the impact of loss aggregation granularity on agentic RL. We observe that for Qwen2.5-7B-RA-SFT, which has weak initial performance and exploration capabilities, token-level loss and sequence-level loss achieve compara-"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Figure 4: The overall performance of our constructed three recipes: GRPO-T, GRPO-TCR, and GRPO-SCR on AIME2024/AIME2025 benchmark. ble average@32 performance on AIME2024/2025. However, for Qwen3-4B-RA-SFT, which has stronger initial performance and exploration capabilities, token-level loss consistently outperforms sequence-level loss in terms of convergence speed and peak accuracy. Specifically, token-level loss exceeds sequence-level loss by 3.95% on AIME24 and 3.86% on AIME25 under the same training budget. It is because the token-level loss ensures each token contributes equally to the optimization signal, thereby leveraging the models exploratory capacity more effectively. This suggests that token-level loss could improve training efficiency and agentic reasoning ability compared to sequence-level loss for models with better initial performance and exploration ability. Takeaway 4.1: The Effective Techniques for Agentic RL 1. Clip higher and overlong reward shaping are simple yet effective techniques to improve the performance of Agentic RL. 2. Token-level loss outperforms sequence-level loss when the models have better exploration ability in convergence speed, peak accuracy, and training robustness."
        },
        {
            "title": "4.2 EXPLORATION–EXPLOITATION DYNAMICS IN AGENTIC RL",
            "content": "Motivation. Prior studies (Chen et al., 2025; Deng et al., 2025) show that in conventional RL, most gains in the pass@k metric come from the SFT stage, where diverse external solutions expand the models ability bound. Subsequent RL training primarily strengthens existing internal solutions, yielding more deterministic policy that improves exploitation (Pass@1) but often suppresses further exploration (Pass@k). However, this characterization is largely based on self-contained generation process, where the model relies solely on its internal capacity. In contrast, agentic RL fundamentally changes this dynamic: the model actively interacts with external tools during reasoning, learning not just to refine internal solutions but to optimize its ability to explore, select, and exploit external resources. This opens the question of whether the classical explorationexploitation tradeoff still holds, or whether agentic RL enables qualitatively different trajectory where exploration is maintained or even amplified through tool use. Observation. Our experiments in fig. 4 show that in agentic RL, both GRPO-TCR and GRPOSCR achieve substantial and simultaneous improvements in pass@k and average@k (over 10% gains on AIME2024/AIME2025). However, this improvement does not hold unconditionally: with the baseline GRPO-T, we still observe the conventional trade-off where exploration is suppressed during training. We attribute this to the overly conservative design of GRPO-T: the combination of restrictive clip upper bound and strong KL-regularization creates severe constraints on distribution shift, forcing the model to maintain self-contained generation patterns and preventing it from fully leveraging tool interactions. Analysis. As we mentioned above, the multi-turn interactions with tools in agentic reasoning also introduce external information during training. The information from the external tools enables models to think smarter than purely think longer by developing more advanced cognitive abilities that autonomously utilize the tools to reason more efficiently, and learn from the feedback signals. Consequently, incentivizing these abilities through agentic reinforcement learning recipe like GRPO-TCR and GRPO-SCR helps to further improve the pass@k performance and lead to higher ability bound for more effective training and better average@k performance. Whats more, we also observe that the gap between average@k and pass@k emerges as critical bottleneck for training efficiency. RL training can be interpreted as process of progressively converting the models pass@k performance into actual average@k gains, with the achievable improvement bounded by an intrinsic ceiling determined by this gap. This perspective highlights that not only the absolute level of pass@k but also the magnitude of the averagepass discrepancy governs how much exploration can be effectively transformed into exploitation during training. Takeaway 4.2: Pass@k and Average@k in Agentic RL training. 1. With external tool interactions, agentic RL can jointly improve pass@k and average@k while conventional RL failed. 2. The critical bottleneck for training efficiency is the gap between pass@k and average@k 4.3 WHEN HIGH ENTROPY DRIVES BETTER EFFICIENCY Motivation. Recently, entropy has become central signal in RL research, yet prescriptions diverge: some advocate minimizing it for more deterministic policies (Agarwal et al., 2025; Cheng et al., 2025b), while others exploit high-entropy tokens to foster exploration and avoid early collapse (Cui et al., 2025; Wang et al., 2025b;a). These views largely arise from conventional RL. In agentic RL, ARPO (Dong et al., 2025b) observes entropy spikes after tool calls and leverages them via adaptive rollouts, implying that tool-call steps induce useful uncertainty. This raises central question: is higher/lower entropy generally beneficial, or is there an optimal range beyond which training destabilizes? Observation. We investigate by visualizing entropy trajectories and relating them to training efficiency and reasoning performance. As shown in fig. 4 and fig. 5, GRPO-T exhibits an early entropy collapse, whereas the entropy for models (trained with GRPO-TCR and GRPO-SCR) with better performance rises faster and stabilizes at higher level. This suggests that greater policy entropy is associated with more effective agentic RL training and stronger agentic reasoning, which is"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Figure 5: The analysis for the policy entropy in agentic RL training. not aligned with the entropy minimization theory in conventional RL. Motivated by our observation, and noting that ϵhigh controls the exploration budget, we utilize different ϵhigh to test for an optimal entropy regime under identical training conditions. Setup. To investigate the optimal entropy regime, we conduct experiments with different clip upper bounds, including 0.28,0.315,0.35 for GRPO-TCR and keep all other settings the same and train Qwen2.5-7B-RA-SFT and Qwen3-4B-RA-SFT. We report the three metrics including average@32, pass@32, and maj@32 to comprehensively evaluate when high entropy could improve training efficiency and when high entropy would lead to the collapse of the agentic reasoning performance. Results. As shown in fig. 6, we observe non-monotonic relation between the clip upper bound ϵhigh and training efficiency. Specifically, for Qwen2.5-7B-RA-SFT, modestly increasing ϵhigh (e.g., 0.28 0.315) accelerates performance improvements. It also improves learning efficiency across both models. For example, with ϵhigh = 0.315, we achieve equivalent performance 40% faster, reaching the same results at step 60 that would otherwise require 100 steps when ϵhigh = 0.28. However, pushing it further yields diminishing returns. For example, when we train Qwen3-4B-RA-SFT with higher ϵhigh = 0.35 it leads to worse training effectiveness despite faster initial lift compared to lower ϵhigh, which is set to 0.28. In summary, higher ϵhigh expands the exploration budget and improves short-horizon progress, yet overly aggressive clipping eventually slows convergence, introduces excessive entropy, which will lead to suboptimal agentic reasoning performance. It also indicates that when the entropy becomes too high, it will also lead to instability in training. Figure 6: The analysis of clipping strategy on AIME2025 benchmark. Left is the analysis for Qwen2.5-7B models. Right is the analysis for Qwen3-4B models."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Takeaway 4.3: Entropy as Driver of Training Efficiency. 1. Agentic RL requires balanced policy entropy, which avoids both excessive entropy (instability) and insufficient entropy (premature convergence) for optimal training effectiveness. 2. Weaker models require larger clip upper bounds to escape the performance bottleneck, while stronger models demand tighter bounds to prevent over-exploration."
        },
        {
            "title": "5 REASONING MODES IN AGENTIC RL",
            "content": "A central question in agentic RL is how an agent should allocate its reasoning budget between internal inference tokens and external tool calls. Should an effective agent rely on frequent tool interactions with minimal internal thinking, or invest more inference tokens in deliberate reasoning before acting? To address this, we characterize two regimes: (i) tool-call scaling, where the agent engages in many short-think rounds with frequent tool usage, and (ii) internal reasoning scaling, where the agent performs deeper reasoning before issuing fewer but more targeted tool calls. This section empirically investigates these reasoning modes and identifies which strategy leads to more efficient and effective agentic reasoning. Figure 7: Analysis of the average number of tool calls and average response length per round in Agentic RL. 5.1 WHEN FEWER TOOL CALLS LEAD TO BETTER TOOL USE Setup. Based on the main experiment in section 4, we first visualize the average number of tool calls and average response length per interaction round in the training process. To further investigate the rationality and efficiency of tool usage, we filter out the correctly executed tool calling queries and calculate the average success rate of the tool calls. Result. As shown in fig. 7, we identify two distinct modes in agentic reasoning: Reactive Mode (short-think + frequent tool calls) and Deliberative Mode (deliberate-think + fewer tool calls). Relating these modes to overall performance (average@32 in fig. 4), we find that the strongest models consistently adopt the Deliberative Mode, while weaker models predominantly fall into the Reactive Mode. Tool-call efficiency further explains this performance gap. As shown in fig. 8,"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Deliberative Mode agents achieve over 70% success in tool usage, indicating that careful reasoning before acting enables highly accurate and effective calls. In contrast, Reactive Mode agents exhibit substantially lower success rates, as their rapid, frequent calls often yield ineffective or erroneous results. Together, these findings highlight clear quality-over-quantity principle: agents that invest more inference tokens in deliberate reasoning ultimately make fewer but more successful tool calls, leading to higher efficiency of tool use and superior task performance. Thoughtful, selective tool usage thus consistently outperforms frequent but poorly targeted interactions. Takeaway 5.1: Effective mode for scaling Agentic Reasoning. Effective agentic reasoning follows quality-over-quantity principle: investing more in deliberate internal reasoning before tool calls yields fewer but far more successful interactions, leading to higher overall efficiency and stronger performance. Figure 8: Tool-use efficiency comparison across different models. 5.2 LIMITATIONS OF CURRENT LONG-COT MODELS IN AGENTIC RL Motivation. Motivated by our findings in section 5.1 that scaling internal reasoning before tool calls improves agentic reasoning, we explore whether incorporating Long-CoT models could further enhance agentic reasoning performance. Previous works combining Long-CoT with search engines (Search-R1 (Jin et al., 2025), R1-Searcher (Song et al., 2025) and Search-o1 (Li et al., 2025b)) have demonstrated success on knowledge-intensive tasks. Building on this foundation, we investigate whether such Long-CoT reasoning capabilities can be effectively used to benefit agentic reinforcement learning with code interpreters on reasoning-intensive problems. Setup. Here we directly utilize Long-CoT LLMs like Qwen3-4B-Thinking-2507 as the starting point for RL, and utilize GRPO-TCR algorithm with the same training settings in section 4. We report the average@k and the average number of tool calls throughout training. Result. As shown in fig. 9, we observed that the model achieved strong average@32 performance in the beginning, but it hardly call the tools. As training progressed, the average number of tool calls gradually converged to zero, indicating that Long-CoT models tend to avoid invoking tools and rely solely on internal reasoning when encountering reasoning-intensive tasks. This behavior"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "varies significantly by task type. For reasoning-intensive tasks, the Long-CoT models tend to utilize their internal reasoning capability to solve these tasks, thus focusing exclusively on the problem rather than analyzing the user instruction or considering calling available tools. Conversely, when confronting knowledge-intensive tasks that exceed their internal reasoning capabilities, these models could actively utilize available tools such as search engines to complete the tasks. Takeaway 5.2: Limitations of Current Long-CoT models in Agentic RL Current open-source Long-CoT LLMs optimized for reasoning tasks cannot be directly applied in Agentic RL, since they over-rely on internal reasoning and avoid invoking tools when encountering reasoning tasks. Figure 9: The training dynamics of current Long-CoT with Agentic RL. INTEGRATING LONG-COT WITH AGENTIC REASONING 5.3 Motivation Since we observe that current Long-CoT LLMs overly rely on internal reasoning and avoid calling the tools for reasoning tasks in section 5.2, we further explore how to effectively integrate Long-CoT with agentic reasoning. Setup. To address the limitation that Long-CoT models often avoid tool calls in reasoning-intensive tasks, we explicitly align them with agentic reasoning through SFT. Specifically, we leverage our SFT dataset (as described in section 3) to initialize Long-CoT models, thereby guiding them to balance deliberate internal reasoning with appropriate tool usage. This initialization enables the models to enter reinforcement learning (RL) training with prior for effective tool invocations. Result. As shown in fig. 10, the SFT-initialized Long-CoT model actively utilizes tools while retaining strong internal reasoning, demonstrating significantly improved agentic RL performance compared to the non-initialized version. However, despite this initial advantage, Long-CoT models ultimately achieve only comparable performance to instruction-based models rather than surpassing them. Analysis of response length evolution in fig. 10 reveals contrasting optimization dynamics. Analysis. Instruction-based models concentrate on developing agentic reasoning capabilities from scratch without specialized internal reasoning biases, enabling continuous growth through focused tool-use learning. However, Long-CoT models face conflicting objectives: their ingrained internal reasoning patterns contradict agentic reasoning paradigms, forcing scaling and pruning process where gains in agentic reasoning are offset by the need to suppress over-thinking behaviors. This dual optimization burden fragments learning efficiency, allowing instruction-based models to"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "achieve superior scaling through concentrated capability improvement rather than divided attention between acquiring new skills and unlearning incompatible reasoning paradigms. It reveals that direct agentic RL training, where models develop reasoning and tool-use capabilities jointly from scratch, outperforms training based on Long-CoT models with conflicting internal reasoning paradigms. Takeaway 5.3: Aligning Long-CoT with Agentic RL. 1. SFT initialization with multi-turn tool-use trajectories is essential for Long-CoT models to acquire effective tool-invocation priors before RL. 2. Instruction-based models are more suitable for agentic RL that scales the agentic reasoning ability from scratch compared to Long-CoT models with internal reasoning priors. Figure 10: Comparison between the instruction-based models and Long-CoT reasoning models. Left is the average@32 performance on AIME2025. Right is the average response length during training."
        },
        {
            "title": "6 CONTRIBUTIONS AND COMPARISON ON CHALLENGING BENCHMARKS",
            "content": "Beyond the insights gained from our comprehensive study, we make the following contributions: (i) 3k high-quality end-to-end agentic SFT dataset, (ii) 30k diverse and effective RL dataset, (iii) Two strong cold-start models (Qwen2.5-7B-RA-SFT and Qwen3-4B-RA-SFT) that enable broad downstream RL research, (iv) strong baseline model, DemyAgent-4B, that validates our training insights and achieves SOTA performance against significantly larger models. Training Recipe. DemyAgent-4B is trained using our complete 30k RL dataset with the GRPOTCR algorithm, applied to Qwen3-4B-RA-SFT as the base model. Following our study insights on clip range optimization, we use higher clip upper bound (ϵhigh = 0.315) to balance exploration and constraint satisfaction. Key Results. We evaluate on challenging reasoning benchmarks under two paradigms  (table 2)  : (1) Self-Contained Reasoning, where models rely solely on internal reasoning capabilities, and (2) Agentic Reasoning, where models leverage external tools such as code interpreters and search engines. As demonstrated in table 2, despite having only 4B parameters, DemyAgent-4B matches or even outperforms much larger models (14B/32B) across challenging benchmarks. Notably, DemyAgent-4B achieves state-of-the-art agentic reasoning performance, surpassing ReTool32B (Feng et al., 2025) and rStar2-Agent-14B (Shang et al., 2025), and even outperforming LongCoT models like DeepSeek-R1-Zero on AIME2025. These results further demonstrate that our simple yet effective training recipe unlocks strong agentic capabilities in compact models."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Table 2: Overall results on challenging reasoning benchmarks grouped by domain. Higher is better (%). The top two results are highlighted in bold and underlined. The results with * are our selfevaluated results for self-contained reasoning. The prompts for agentic reasoning and self-contained reasoning could be found in appendix B. MATH Science Code Method AIME2024 AIME2025 GPQA-Diamond LiveCodeBench-v Self-Contained Reasoning Qwen2.5-7B-Instruct Qwen3-4B-Instruct-2507 Qwen2.5-72B-Instruct DeepSeek-V3 DeepSeek-R1-Distill-32B DeepSeek-R1-Zero (671B) Agentic Reasoning Qwen2.5-7B-Instruct Qwen3-4B-Instruct-2507 ToRL-7B ReTool-32B Tool-Star-3B ARPO-7B rStar2-Agent-14B DemyAgent-4B (Ours) 16.7 63.3 18.9 39.2 70.0 71.0 4.8 17.9 43.3 72.5 20 30.0 80.6 72.6 10.0 47.4 15.0 28.8 46.7 53.5 5.6 16.3 30.0 54.3 16.7 30.0 69.8 70. 31.3 52.0 49.0 59.1 59.6 59.6 25.5 44.3 - - - 53.0 60.9 58.5 15.2 35.1 - 16.1 - - 12.2 23.0 - - - 18.3 - 26."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Tool-integrated Reasoning. Tool-integrated reasoning (TIR) enables large language models (LLMs) to leverage external tools such as code interpreters and search engines in order to overcome the limitations of pure internal reasoning. This approach extends the computational and knowledge capacity of LLMs and allows them to tackle tasks that are infeasible through text-only reasoning. Previous TIR methods are based on prompting engineering, such as PoT (Chen et al.), templateaugmented reasoning paradigm like BoT (Yang et al., 2024b), and supervised finetuning (SFT) methods. SFT-based methods like ToRA (Gou et al.), Tool-former (Schick et al., 2023), and QwenMath-TIR (Yang et al., 2024a) train models on datasets containing tool invocation demonstrations, teaching LLMs to follow predefined patterns of tool use. Similarly, works such as ReAct (Yao et al.), MathCoder (Wang et al., 2024), and Mario (Liao et al., 2024) incorporate tool usage examples into training data so that LLMs can interleave reasoning steps with external tool calls. However, these SFT-based approaches have inherent limitations. Since models are compelled to use tools according to the distribution of training data, they cannot develop adaptive strategies for tool use, such as deciding when to invoke tool, how often to call it, or how to balance tool use with internal reasoning. As result, previous SFT-driven TIR methods improve tool-following ability but lack the flexibility and autonomy required for robust agentic behavior. Agent Reinforcement Learning. Compared with SFT-based tool-integrated reasoning that merely imitates demonstrations of tool usage, it is more promising to leverage reinforcement learning algorithms (Shao et al., 2024; Guo et al., 2025; Yu et al., 2025; Liu et al., 2025a; Zhao et al., 2025) to train more capable agents. Agent Reinforcement Learning (Agent RL) explicitly models tool invocation as part of the action space and optimizes adaptive strategies through outcome-driven rewards, enabling agents to move beyond static supervision toward more flexible and effective reasoning behaviors. Representative works include search-oriented approaches such as Search-R1 (Jin et al., 2025) and R1-Searcher (Song et al., 2025), which train LLMs to interleave reasoning with search engine queries. ToRL (Li et al., 2025d) demonstrates that RL can directly optimize tool use at scale, enabling models to discover effective invocation strategies. ReTool (Feng et al., 2025) further highlights the benefit of RL by teaching models not only to use tools but also to decide when and how to call them. More recently, ZeroTIR (Mai et al., 2025) takes complementary perspective by analyzing scaling laws in RL-based tool use, showing how strategic code invocation gradually emerges as training progresses. More general frameworks such as ARTIST (Singh et al., 2025), Tool-Star (Dong et al., 2025a), and Auto-TIR (Wei et al., 2025), further extend Agent RL to the setting of multi-tool"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "integration. ARTIST focuses on multi-turn reasoning where agents autonomously decide not only whether to call tool but also which tool to invoke in complex reasoning chains. Together, these frameworks highlight the frontier of Agent RL: building agents that generalize beyond single-tool domains to flexibly coordinate multiple tools under reinforcement learning objectives. However, current Agent RL methods are often tied to specific workflows and lack systematic understanding of how reinforcement learning can more generally improve tool-use ability. Key issues such as algorithm design, tool-call efficiency, and reliance on synthetic trajectories remain underexplored. Entropy Mechanism for Reinforcement Learning. Recent advances in reinforcement learning have markedly improved LLM reasoning, yet entropy collapsethe failure to maintain exploration ability under outcome-driven optimization, which remains central obstacle for effective scaling of RL. At the mechanism level, Cui et al. (2025) formalizes how entropy governs exploration and identifies collapse as key bottleneck; deepening this view, Wang et al. (2025b) show that minority of high-entropy tokens, rather than the low-entropy majority, disproportionately drives effective learning. At the system level, He et al. (2025) provide empirical evidence that preserving entropy is essential for stable, long-horizon reasoning. Building on these insights, Cheng et al. (2025a) and Dong et al. (2025b) incorporate entropy-aware objectives into RL to better balance exploration and exploitation in multi-turn reasoning. More recently, Deng et al. (2025) propose exploration mechanisms in Reinforcement learning with verifiable rewards (RLVR), consolidating entropy as controllable signal, not merely regularizer for promoting exploration, stability, and sustained improvement in agentic RL."
        },
        {
            "title": "8 DISCUSSION AND FUTURE WORK",
            "content": "In this section, we outline the challenges and potential future directions of reinforcement learning in Agentic RL. 8.1 DATA-FUEL SCARCITY Based on our analysis in section 3, we believe that the training data plays crucial role in Agentic RL, which determines both the training effectiveness and the scaling upper bound for agentic reasoning. However, for the SFT dataset that requires full end-to-end generated trajectories, it is still computationally costly to collect. Works like s1 (Muennighoff et al.) and limo (Ye et al., 2025) have demonstrated that the effectiveness of curating small-sized but high-quality distilled dataset could significantly enhance the internal reasoning ability of LLMs. These findings suggest that we could also develop recipe for how to curate small-sized high-quality SFT datasets, which could not only alleviate the scarcity of data in Agentic RL, but it could also improve our understanding of agentic behaviors through the insights of curating these datasets. 8.2 EFFECTIVE SCALING OF AGENTIC REASONING As demonstrated in section 5, deliberate reasoning before tool invocation emerges as superior mode for agentic problem-solving, yet effectively scaling such reasoning behaviors remains challenging. Our analysis in section 5.2 reveals fundamental limitations in current open-source LLMs for agentic reasoning tasks. Based on that, exploring agent-specific reasoning frameworks that prioritize highlevel strategic planning and efficient tool orchestration, rather than relying heavily on the models internal reasoning capabilities, could be promising direction for future research. Such agent-oriented reasoning chains should emphasize problem decomposition into tool-executable subtasks, strategic tool selection, and synthesis of tool outputs. This shift from reasoning-centric to high-level toolplanning-centric approaches necessitates new training methodologies and evaluation frameworks specifically tailored for agentic workflows, potentially leading to more capable autonomous agents that can navigate complex multi-step scenarios with greater reliability. We look forward to future research on exploring the compatible inference scaling methods for agentic reasoning. 8.3 ADDITIONAL APPLICATION SCENARIOS IN AGENTIC REASONING In this work, we mainly focus on the code interpreter as tool for agentic reasoning and find valuable insights and recipes. But we can also generalize our insights from static and single tool environ-"
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "ment to multi-tool and optimizable environment. For example, in this multi-tool environment, the insights of encouraging exploration in agentic RL may still hold. In more complex environment, the correct solution to problem consists of different possible combinations of tools, which requires more exploration for the optimal strategy and the ability to select the most effective tools for corresponding tasks."
        },
        {
            "title": "9 LIMITATIONS",
            "content": "In this work, we investigate reinforcement learning for agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. However, our experiments are conducted on small-sized models (e.g, 4B/7B). While this has already provided valuable insights into challenges and design choices for Agentic RL, recent works (Vattikonda et al., 2025) has underscores RLs extreme hyperparameter sensitivity, especially for larger-sized models. In particular, larger models may demonstrate different sensitivities to reward signals, require different exploration strategies, or exhibit more robust reasoning patterns that interact differently with RL training dynamics. We leave more comprehensive study of RL with larger-sized models in broader agentic settings as an important future work direction."
        },
        {
            "title": "10 CONCLUSION",
            "content": "In this work, we conducted comprehensive empirical study of reinforcement learning for agentic reasoning across the axes of data, algorithm, and reasoning mode. For the perspective of data curation, our findings highlight that real end-to-end multi-turn trajectories are indispensable for building strong agentic SFT foundations, while diverse and model-aware RL datasets sustain exploration and yield stable training. Algorithmically, we show that simple but effective design choices, such as clip higher, reward shaping, and token-level loss, which substantially improve training effectiveness, and that maintaining appropriate entropy is the key driver of effective agentic RL. On the reasoning side, we find quality-over-quantity principle: fewer but more deliberate tool calls lead to superior efficiency, while Long-CoT priors often hinder tool adoption and slow down scaling. With our recipes, we effectively improve the agentic reasoning of LLMs, and we conduct comprehensive evaluation across challenging benchmarks, including AIME2024/2025, GPQA-Diamond, and LiveCodeBench-v6, which further validates our insights."
        },
        {
            "title": "REFERENCES",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025a. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025b. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: systematic analysis of llm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534, 2025. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025a. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025b. Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Alex Havrilla, Andrew Dai, Laura OMahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fabrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, et al. Surveying the effects of quality, diversity, and complexity in synthetic data from large language models. arXiv preprint arXiv:2412.02980, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free In The Thirteenth International Conference on evaluation of large language models for code. Learning Representations. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025b. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025c. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025d. Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code interpreter output-a reproducible pipeline. CoRR, 2024."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025a. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025b. Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Workshop on Reasoning and Planning for Large Language Models. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230, 2025. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Munoz-Marmol, Sahar Omidi Shayegan, Stefania Raimondo, et al. How to train your llm web agent: statistical diagnosis. arXiv preprint arXiv:2507.04103, 2025."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hierarchical reasoning in llms through reinforcement learning. arXiv preprint arXiv:2509.03646, 2025a. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. In 12th International Conference on Learning Representations (ICLR 2024). International Conference on Learning Representations, ICLR, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. RevolutionizarXiv preprint ing reinforcement learning framework for diffusion large language models. arXiv:2509.06949, 2025c. Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester via reinforcement learning. The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025d. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024a. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 37:113519113544, 2024b. Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical llm reasoning via scaling thought templates. arXiv preprint arXiv:2502.06772, 2025b. Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph Gonzalez, Bin Cui, and Shuicheng Yan. Supercorrect: Supervising and correcting language models with error-driven insights. 13th International Conference on Learning Representations (ICLR 2025), 2025c. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonfluxprm: Trajectory-aware prms for long chain-of-thought reasoning in llms. The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025."
        },
        {
            "title": "A EXPERIMENT SETUP",
            "content": "A.1 TRAINING SETUP We choose Qwen2.5-7B-Instruct (Team, 2024) and Qwen3-4B-Instruct-2507 (Yang et al., 2025a) as our base models. For datasets, we specifically curate 3k actual agentic trajectories for SFT and 30K high-quality RL data, including math, science, and code (For more detail, please refer to section 3). For training, we employ VeRL framework, and we conduct all our experiments on 8Tesla-A10080G GPUs. Regarding hyperparameters for SFT, we train the base models for 5 epochs with batch size of 32, we utilize AdamW Optimizer with an initial learning rate of 5e-5, and the max response length is set to 32768. For RL training, we train 3 epochs with the batch size of 64 and the learning rate of 1e-6; the max prompt length is set to 2560. For GRPO baseline, we set the KL loss coefficient β to 0.001, and the clip ratio ϵ = 0.2 along with token-level loss aggregation, the max response length is set to 16384. A.2 EVALUATION SETUP We focus on four challenging benchmarks, including AIME2024, AIME2025, GPQA-Diamond (Rein et al., 2024), and LiveCodeBench (Jain et al.). By default, we set the temperature to 1.0 and top to 0.6 and the maximum response length to 16384. For each problem, we sample 32 times to comprehensively evaluate average@32, pass@32, and maj@32 for AIME2024/2025 and GPQA-Diamond, for LivecodeBench, and we evaluate pass@1 and pass@5 according to its official evaluation guideline. A.3 SFT DATASET For our self-curated real end-to-end trajectories, we utilize Qwen3-Coder-30B-A3B as the teacher model and roll out multi-turn interactions via the open-source Qwen-Agent framework with SandBoxFusion as the code interpreter. The SFT problems are drawn from three sources: s1-1k (Muennighoff et al.), our self-curated 3k LeetCode dataset, and 2k ReTool multi-turn SFT set, yielding 6k problems in total. We generate trajectories for all 6k tasks and score the 3k LeetCode and 2k ReTool subsets using ReasonFlux-PRM (Zou et al., 2025) to filter for high-quality data. We then retain the top 1k LeetCode and 1k ReTool trajectories and keep s1-1k, resulting in 3k real-trajectory dataset. A.4 RL DATASET To investigate how diversity influences the training dynamics, we construct diverse 30k-sample RL dataset by combining 17k DAPO-Math samples (Yu et al., 2025), 4902 math and 3586 code samples from Skywork-or1 (He et al., 2025), and 3k science problems from MegaScience (Fan et al., 2025)."
        },
        {
            "title": "B PROMPT TEMPLATE",
            "content": "Since our constructed dataset encompasses diverse question types, including mathematics, programming, and scientific problems, the output answer formats vary across different problems. To standardize the accuracy of model output formats while encouraging the model to think and utilize tools simultaneously, we have designed distinct prompts for different task types and reasoning paradigms (agentic reasoning and self-contained reasoning). In this chapter, we will present these prompts. B.1 PROMPT FOR AGENTIC REASONING In this section, we present the prompts we utilized during agentic RL training and evaluation for agentic reasoning on different benchmarks."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Prompt Template for verifiable Math/Science Problems Analyze and solve the following [math/science domain] problem step by step. Problem: [Insert problem text here] Hint: The tool could be used for more precise and efficient calculations and could help you to verify your result before you reach the final answer. Note: You should first analyze the problem and form high-level solution strategy, then utilize the tools to help you solve the problem. Answer Format: Do not put units of the final answer inside boxed{}. The content of boxed{} should be the numerical value of the final answer only, without any units. Remember once you make sure the current answer is your final answer, do not call the tools again and directly output the final answer in the following text format, the answer format must be: boxed{The final answer goes here.}. Prompt Template for Scientific QA Problems Analyze and solve the following [science domain] problem step by step. Problem: [Insert problem text here] Hint: The tool could be used for more precise and efficient calculations and could help you to verify your result before you reach the final answer. Note: You should first analyze the problem and form high-level solution strategy, then utilize the tools to help you solve the problem. Answer Format: Remember once you make sure the current answer is your final answer, do not call the tools again and directly output the final answer in the following text format, the answer format must be: boxed{The final answer goes here.}. You need to put the final uppercase letter option of this problem into boxed{}. Prompt Template for Code Problems You will be given question (problem specification) and will generate correct Python program that matches the specification and passes all tests. Problem: [Insert problem text here] Public Examples: Here are some input and output examples of the expected code: Input: [sample inputs] Output: [sample outputs] Note: You should first analyze the problem and form high-level solution strategy, then utilize the tools to help you solve the problem. Instruction: Read the inputs from stdin, solve the problem, and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within the delimiters shown below. Ensure that when the Python program runs, it correctly reads inputs, executes the algorithm, and writes output to stdout. Submit: Before submitting your code, you can utilize tools to check its correctness. Once you make sure the current code is correct, do not call the tools again and submit your code within the following Python code block: python # YOUR CODE HERE B.2 PROMPT FOR SELF-CONTAINED REASONING We present the prompts for evaluating the self-contained reasoning abilities of open-source models like Qwen3-4B-Instruct-2507 in this section."
        },
        {
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "content": "Prompt Template for AIME2024 You are an expert mathematician specializing in competition mathematics. You excel at solving challenging problems from contests like AIME, AMC, and IMO. Problem: {problem} Instructions: 1. Read the problem carefully and identify what is being asked. 2. Plan your approach and identify relevant mathematical concepts (algebra, geometry, number theory, combinatorics, etc.). 3. Work through the problem step-by-step, showing all your reasoning. 4. Perform all calculations carefully and check your work. 5. Simplify your final answer to match the required format. Formating Requirements: - AIME answers are always integers between 0 and 999. - If the problem asks for m+n, a+b+c, or similar, compute the final sum. - You MUST put your final numerical answer in boxed{verifiable answer here...} notation. - Example: If your answer is 123, write boxed{123}. - Example: If you need to find m+n where m=25 and n=8, write boxed{33}. Do NOT: - Put formulas or expressions in the box (like boxed{m+n}). - Include units or text in the box. - Leave the answer in fraction form if an integer is requested. Begin your solution: Prompt Template for GPQA-Diamond You are an expert in science with deep knowledge in physics, chemistry, and biology. Question: {problem} Instructions: 1. Carefully analyze the question and all provided options 2. Apply relevant scientific principles and reasoning 3. Think step-by-step through the problem 4. Consider edge cases and eliminate incorrect options 5. Provide your final answer in the format: boxed{The final answer of the option letter.} Format Requirements: - Your answer must be one of the given options (A, B, C, or D) - You MUST put your final answer which is the option letter in boxed{The final answer of the option letter.} notation. - Example: boxed{B} Begin your analysis:"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Princeton University",
        "University of Illinois at Urbana-Champaign"
    ]
}