{
    "paper_title": "Boosting Healthcare LLMs Through Retrieved Context",
    "authors": [
        "Jordi Bayarri-Planas",
        "Ashwin Kumar Gururajan",
        "Dario Garcia-Gasulla"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 7 2 1 5 1 . 9 0 4 2 : r Boosting Healthcare LLMs Through Retrieved Context Jordi Bayarri-Planas Ashwin Kumar Gururajan Dario Garcia-Gasulla Barcelona Supercomputing Center Barcelona Supercomputing Center Barcelona Supercomputing Center (BSC) (BSC) (BSC) Barcelona, Spain Barcelona, Spain Barcelona, Spain jordi.bayarri@bsc.es ashwin.gururajan@bsc.es dario.garcia@bsc.es Abstract Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application."
        },
        {
            "title": "1 Introduction\nLarge Language Models (LLMs) are the default solution for most\ntext-related tasks. However, a critical concern remains: their factual\naccuracy1, a limitation inherent to their generative nature. LLMs\nare not designed to retrieve precise information, but rather to gener-\nate plausible text based on learned patterns. A popular approach to\nenhance the factuality of LLMs is to contextualize them by biasing\ntheir output through relevant input tokens. This ranges from simple\nprompting techniques, such as \"Let’s think step by step,\" to more\nsophisticated Retrieval Augmented Generation (RAG) systems. In-\ndeed, integrating context retrieval systems can significantly boost\nthe performance and reliability of LLMs.",
            "content": "In the domain of medical multi-choice question-answering (MCQA), the current state-of-the-art is dominated by private models like GPT4 and MedPalm-2. According to popular evaluation methods, open models significantly lag behind, limiting their practical use. However, comprehensive assessment must include context retrieval systems and consider more realistic evaluation methods. This study addresses two key research questions: First, how competitive can open LLMs be in healthcare when enhanced with optimized context retrieval systems? Second, how can these systems be extended beyond the limited domain of multi-choice QA? 1We consider reasoning to be beyond text task To address the first question, this work explores the limits of model boosting through context retrieval by optimizing its components. Open models of various sizes are then enhanced with this optimized system and compared with private solutions. For the second question, we propose an extension of the context retrieval system to generate open-ended (OE) answers, increasing the quality of responses not biased by possible answers. C1: Guidelines for an optimized context retrieval setup C2: Updated benchmark on open/private LLMs for healthcare MCQA C4: Novel context retrieval design for OE answer generation C4: Automated library for model boosting"
        },
        {
            "title": "2 Related Work\nAddressing the challenge of LLM factuality has spurred extensive\nresearch. Initial attempts to improve it centered on harnessing the\ninherent In-Context Learning (ICL) abilities of LLMs [5], allowing\nthem to adapt to new tasks with minimal examples and without\nspecific training. This paved the way for the development of sophis-\nticated prompting techniques designed to elicit more accurate and\nreasoned responses. Chain of Thought (CoT) prompting [32] guides\nLLMs through intermediate reasoning steps, enhancing their per-\nformance on complex tasks. In contrast, Self-Consistency (SC) [31]\nexploits the stochastic nature of LLMs, producing and comparing\nseveral outcomes for the same input, before producing a unified\nanswer. Both are combined in Self-Consistency Chain of Thought\n(SC-CoT). In a similar fashion, tree of Thought (ToT) [33] builds a\nsearch space across generated steps towards the answer. At each\nstep several follow-up options are generated, evaluated, and se-\nlected, building a pruned tree in the process, from which the final\nanswer is extracted. Reflection methods [23, 25] employ the same\ngeneration model as a critique model to iteratively critique the\nintermediate output and then generate an improved output based\non this feedback.",
            "content": "Recognizing the limitations of relying solely on internal knowledge, researchers turned to external knowledge integration through prompting techniques, ultimately leading to the emergence of Retrieval Augmented Generation (RAG) [13]. RAG systems retrieve and integrate relevant information from external knowledge bases, significantly enhancing LLM performance by biasing responses with factual data. In the healthcare domain, few-shot, CoT and SC are recurrently used to improve factuality [15, 21, 24, 29], and combination of those was proposed in Medprompt [19], context retrieval system designed for medical MCQA that achieves stateof-the-art results with GPT-4. While Medprompt has been adapted for open-source models like [16], thorough investigation into the optimal configuration of its components (e.g., DBs, embeddings) remains an open area of research. , , Jordi Bayarri-Planas, Ashwin Kumar Gururajan, and Dario Garcia-Gasulla Figure 1: Components of question-answering system based on context retrieval for LLMs. Despite recent advances, significant performance gap persists between large private models and their open-weight counterparts [8, 15]. However, the rapid evolution of accessible LLMs [3, 16], coupled with the potential of optimized RAG systems, suggests that this gap may be narrowing. The ability to leverage mediumsized open models with optimized RAG, potentially achieving performance comparable to larger closed-source alternatives, holds significant promise for reducing adoption costs and increasing accessibility. At this point, it is worth noting the limitations of MCQA benchmarks. While providing valuable assessment tool (given its reliable ground truth), it represents simplified and often unrealistic setup [8] only found in professional medicine exams. Indeed, when considering real-world clinical scenarios, we should not expect the question to include list of possible answers. That is, models ought to generate open-ended answers. While CoT mechanisms have also been applied to the task of open-ended answers in QAs, contributions in that regard are scarce and non-exhaustive [15, 24]."
        },
        {
            "title": "3 Methods\nThis section details the methodology employed to optimize a con-\ntext retrieval system and evaluate its performance. We first outline\nthe core components of the context retrieval system under investi-\ngation (§3.1), followed by a description of the benchmark datasets\nused for evaluation (§3.2), and conclude with the specific models\nand computational resources employed in the study (§3.3).",
            "content": "Let us start with the retrieval system architecture, which is based on the Medprompt design. Figure 1 illustrates this questionanswering system, highlighting the key components that will be explored and optimized in this work."
        },
        {
            "title": "3.1 Retrieval Components\nIn this work, we consider the role and impact of the following\ncomponents:",
            "content": "Choice shuffling. This technique involves randomly shuffling the order of the answer choices presented in multiplechoice questions to mitigate position bias [15, 19]). Number of ensembles. Refers to the number of independently generated responses produced by the LLM for given question. These responses are then aggregated, to arrive at the final answer (through techniques such as majority voting). The effect of this component is experimented with the SC-CoT technique. Database. This component represents the external knowledge source used to contextualize and bias the model. We experiment with two distinct database approaches: utilizing the validation set of the datasets to generate examples in execution time as seen in the original Medprompt methodology, and constructing custom databases derived from the training sets. Embedding model. The embedding model is crucial for retrieving relevant information from the database. It transforms both the input question and the database entries into numerical vector representations, allowing for similarity comparisons. We evaluate various embedding models, considering factors like dimensionality (e.g., 768 vs. 4096) and domain specificity (e.g., general purpose vs. healthcare-specific). Reranking model. This optional component aims to refine the initial retrieval results by re-ranking the candidate QAs retrieved from the database based on their relevance to the input question. We employ the MedCPT-Cross-Encoder [12], specialized medical reranking model trained on large corpus of biomedical literature. Beyond these core components, we also explore the impact of hyperparameters such as temperature and the number of few-shot examples, though their impact is less pronounced."
        },
        {
            "title": "3.2 Datasets\nTo evaluate the performance of our optimized system, we employ\nfour widely recognized medical Multiple-Choice Question Answer-\ning (MCQA) datasets:",
            "content": "MedQA [11]: Consists of 1,273 multiple-choice questions in the format of the US Medical License Exam (USMLE). MedMCQA [22]: Large-scale multiple-choice question answering dataset with questions from Indian medical school entrance exams. We use the validation set which is composed of 4,183 questions, as the answers of the test set are private. CareQA [6]: Multiple-choice question answering dataset based on the access exam for Spanish Specialised Healthcare Training. It consists of 5,621 questions. MMLU [9]: MMLU is multitask benchmark suite of 57 different datasets spanning domains across STEM. From all these tasks we use the medical related: anatomy, clinical knowledge, college biology, college medicine, medical genetics and professional medicine, which account for total of 1,089 questions. These datasets collectively provide comprehensive and diverse evaluation platform for assessing the performance of our system across different multi-choice medical question styles and sources."
        },
        {
            "title": "3.3 Models and Compute\nThe main model used in our experiments is Llama3-Aloe-8B-Alpha [7],\na state-of-the-art open-source LLM specifically fine-tuned for the\nhealthcare domain. With 8 billion parameters, this model builds\nupon the Meta Llama-3 architecture, leveraging a curated combi-\nnation of high-quality medical data, synthetic data, and targeted\nalignment via DPO and Red-teaming datasets. It offers a compelling\nbalance between performance and computational cost.",
            "content": "All experiments are conducted on single compute node equipped with 4x NVIDIA H100 (64GB) GPUs. We utilize single GPU for smaller models (<10B parameters) and employ tensor parallelism Boosting Healthcare LLMs Through Retrieved Context , , across multiple GPUs for larger models (>20B parameters). To assess the environmental impact of our experiments, we monitor the nodes power consumption and extrapolate these measurements to estimate the carbon footprint of each experiment, utilizing the latest 𝐶𝑂2 emissions ratio provided by the European Union. Furthermore, we developed custom software repository 2 to facilitate this research, which we release open-source to benefit the broader research community. This repository provides streamlined framework for evaluating various prompt strategies and optimizing context retrieval systems for diverse LLMs. Detailed documentation and an online tutorial are provided to facilitate its adoption."
        },
        {
            "title": "4.1 SC-CoT Experiments\nTable 1 presents the baseline performance of Llama3-Aloe-8B-Alpha,\nour primary evaluation model, on four benchmark datasets using\nzero-shot next token prediction, CoT, and SC-CoT. This serves\nas a reference point for evaluating the subsequent improvements\nachieved through the integration of various components.",
            "content": "CareQA MedMCQA MedQA MMLU Zero-shot CoT SC-CoT 67.57 65.11 67.64 58.91 55.10 56.78 62.45 64.26 64.81 72.76 72.93 73. Table 1: Baseline results of Llama3-Aloe-8B-Alpha using zeroshot next token prediction, CoT, and SC-CoT with 5 ensembles. We investigate the impact of choice shuffling and variable number of ensembles. The impact of choice shuffling, is among the most well-documented phenomenon [15], as LLMs answering MCQs seem to be biased towards the first response (e.g., \"A\"). Table 2 demonstrates the consistent benefits of incorporating choice shuffling within the SC-CoT framework, yielding improvements across all datasets and ensemble configurations. Based on these results, all subsequent experiments utilize choice shuffling to ensure unbiased evaluation. Next, we consider the second SC-CoT factor, the number of ensembles used in the self-consistency process. This choice has 2https://github.com/HPAI-BSC/prompt_engine CareQA MedMCQA MedQA MMLU 5 5 + CS 20 20 + CS 67.64 +0.85 68.89 +1.08 56.78 +2.13 56.78 +2.53 64.81 +0.24 64.10 +3.53 73.68 +1.88 73.79 +3.75 Table 2: Accuracy change when adding choice shuffling (CS) to the baseline with SC-CoT. 𝑁 represents the number of ensembles of the SC component. Figure 2: Trends on the accuracy gain obtained by using an increasing number of ensembles (horizontal axis) in SCCoT setup. significant effect, not only on task performance but also on computational cost and footprint. Such trade-off can be seen in Table 3, where performance gains as footprint consistently grow up to 5-6% and 1.76Kg of 𝐶𝑂2 respectively. The first five ensembles provide around 3.5% accuracy gains, while another five adds half of that. Gains beyond that are marginal, requiring up to twenty-five ensembles (fifteen more) to gain another 1%. Figure 2 visually depicts the trend of diminishing returns. The recommended number of ensembles depends on the criticality of the task, and the available resources. In our experimentation, and unless otherwise specified, all SC experiments will be conducted using 5 ensembles. CareQA MedMCQA MedQA MMLU 1 3 5 10 15 20 25 55.10 +1.58 +3.56 +4.78 +4.57 +5.31 +5.33 72.93 +0.32 +2.78 +4.51 +5.27 +5.34 +5. 65.11 +1.78 +3.36 +5.84 +6.26 +6.42 +6.56 64.26 +3.30 +4.40 +4.24 +4.08 +4.90 +5.18 𝐶𝑂2 0.08 Kg 0.20 Kg 0.33 Kg 0.69 Kg 1.02 Kg 1.38 Kg 1.76 Kg Table 3: Accuracy change of SC-CoT with variable number of ensembles, when compared to the baseline without SC (N=1). 𝐶𝑂2 indicates the associated footprint. , , Jordi Bayarri-Planas, Ashwin Kumar Gururajan, and Dario Garcia-Gasulla"
        },
        {
            "title": "4.2 Medprompt Experiments\nAt this point, we extend the SC-CoT scheme with retrieval com-\nponents, exploiting external data sources. This will include the\nmain elements included in Medprompt: An embedding model, a\ndatabase, and reranker model. The embedding model encodes both\ninput and database items before computing their similarity scores.\nFor this component, we consider four different models, ranging\nin embedding size and in number of parameters. We also consider\nwhether they have been specialized in the healthcare domain or\nnot. These properties are listed in Table 4, and their performance in\nTable 5. Results show all models achieve comparable performance\nin most datasets, with no embedding model clearly outperform-\ning the rest. As a result, we select the healthcare-specific, cheaper\nmodel PubMedBERT to be the embedding model for our future\nexperiments.",
            "content": "Model Domain PubMedBERT [18] Medical Medical MedCPT [12] General UAE-Large-V1 [14] General SFR-Mistral [17] Emb. size Params. 768 768 1024 4096 109M 109M 335M 7B Table 4: Properties of the models used to embed the questions. Embedding PubMedBERT MedCPT UAE-Large-V1 SFR-Mistral CareQA MedMCQA MedQA MMLU 68.65 68.81 68.08 68.61 59.55 59.29 59.53 60.60 69.60 67.16 69.05 70.15 75.55 75.44 76.70 73. Table 5: Accuracy for each embedding model on each dataset. We set 5 as the number of ensembles and few-shot examples, choice shuffling is activated. CoT database in use. The second retrieval component is the database. That is the documental source from which pieces of text are extracted and introduced into the model prompt for contextualization. Both the quality and the diversity of those databases have large impact on performance. To test this hypothesis we test two setups. First, smaller database, composed by the validation set of each dataset. For CareQA (which lacks validation split) we use the MedMCQA validation split as database. The second setup includes larger and augmented database. Instead of the validation split, we use the training splits of MedMCQA (180K QA pairs, also used for CareQA and MMLU) and MedQA (10K QA pairs). In Medprompt, these samples are enhanced using CoT. In addition, we consider ToT as well. In both cases, samples are enhanced by prompting an LLM chosen for its instruction-following capability. For CoT, we prompted the Mixtral-8x7B [2] model with the question, the possible answers, and the correct choice, and then asked to analyse each option individually, to explain the answer through detailed reasoning, and to end with re-identification of the selected option (which is tested for validity). For ToT, we followed the same approach but used Llama-3.1-70B-Instruct to generate the answers. We adapted the original ToT prompt to simulate three logical experts collaborating to answer the question. The size of both databases is exactly the same. While the MedMCQA and MedQA experiments study the impact of size and quality in databases, the experiments on CareQA and MMLU add factor of generalization (by using DB from different source). Results are shown in Table 6. In three out of four cases, the synthetically enhanced data improves the performance of the retrieval system. Even when the database comes from different source, the extended database contributes to increase accuracy. Between CoT and ToT, the first outperforms the second in three out of four datasets. All further experiments will make use of the CoT extended databases. Database MedMCQA MedQA CareQA MMLU Validation Train+CoT Train+ToT 68.65 +7.15 -1.83 59.55 -1.26 +4.40 69.60 +0.78 +0.63 75.55 +3.23 +1.88 Table 6: Accuracy change when extending the database through CoT and ToT (MedMCQA and MedQA train splits). Also when using database extended with CoT/ToT coming from different source (CareQA and MMLU use MedMCQA train split as database). The final component we study is the use of reranker model. The role of the reranker is to sort the most similar items retrieved from the database, which yields performance boosts in certain domains [30]. To that end, we use the MedCPT-Cross-Encoder model. That is contrastively pre-trained transformer, tuned on PubMed information retrieval. Table 7 shows the inconsistent performance gains achieved with the reranker, leading us to exclude it from further experimentation due to its added computational cost. CareQA MedMCQA MedQA MMLU -0.18 +0.10 +1.02 -1. Table 7: Accuracy change when adding the reranker, sorting the samples retrieved from the database."
        },
        {
            "title": "4.3 Proposed Scheme\nBased on our empirical findings, we propose an optimized con-\ntext retrieval configuration that follows the Medprompt (Figure 1)\nscheme. The main component removed is the reranker model. The\nproposed setup utilizes the following:",
            "content": "Choice Shuffling: Enabled to mitigate position bias. Number of Ensembles: 5 as the default, as this provides the best trade-off between performance gains and computational cost associated. 20 for benchmarking. Database: CoT-augmented training sets, providing the most comprehensive and effective knowledge source. Embedding Model: PubMedBERT, small and efficient healthcare-specific embedding model, is chosen. Reranking Model: Excluded due to its inconsistent performance gains and added computational overhead. Boosting Healthcare LLMs Through Retrieved Context , , This setup is the one provided by default in the software library Model CareQA MedMCQA MedQA MMLU released in association with this work."
        },
        {
            "title": "4.4 State-of-the-art Comparison\nIn this section, we benchmark the performance of our optimized\nCR system against a diverse set of open-source state-of-the-art\ngeneral-purpose and healthcare-specific LLMs, including models of\nvarying sizes and architectures.",
            "content": "Aloe-8B: fine-tuned version of Llama3 8B for the healthcare domain, tuned with multi-choice QA data. The retrieval system was optimized for this model, its results may be biased. Llama-3.1-8B/70B [3]: Latest models released by Meta in 2024. Instruct versions are tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). Qwen-2-7B/72B [4]: Instructed versions of the new series of Qwen large language models. Mistral-7B-Instruct-v0.3 [10]: Third version of the 7B model developed by Mistral AI. Gemma-2-27B-it [27]: Instruct tuned version of the larger model from the Gemma family, which is family of state-ofthe-art open models from Google. Yi-1.5B-34B-Chat-16K [1]: The Yi series of models are family of LLMs trained from scratch by 01.AI. Yi-1.5 is an upgraded version of Yi, continuously pre-trained on top of it. The chat version is the instruct-tuned version. For each model in this list, we conduct the same evaluation, with and without the context retrieval setup. Results, shown in Table 8, indicate unanimous boost in performance in all datasets and models. The gains are generalized but non-homogeneous. These are higher on less-performing models, clearly influenced by both the smaller model size and the larger room for improvement. Performance improvement seems to be highly dependent on the model family. Llama-based models (Llama3-Aloe 8B, Llama 3.1 8B/70B) seem to benefit particularly from context retrieval systems, while other models enjoy lower gains (e.g., Qwen2 72B). LLM performance on retrieval system is in fact being consistently affected by training policies. There are also differences in gains across benchmarks, with some obtaining consistently higher benefits from context retrieval than others (e.g., MedMCQA). That is remarkable difference taking into account the huge task similarities shared by all four datasets (i.e., multiple-choice medical question answering). This points towards the importance of data sources, and the challenges of generalization. Overall, the performance gain provided by the context retrieval scheme is highly valuable, as it reduces the costs of having highly reliable healthcare systems. It shows well-tuned system based on small LLMs reaching the accuracy levels of much bigger models."
        },
        {
            "title": "4.5 Private Comparison\nTo contextualize our results further, we include performance data\nreported for prominent private models, not been reproduced by\nthis work. The open models under study include:",
            "content": "GPT-4 [19, 20]: Developed by OpenAI, with an undisclosed number of parameters but estimated to be at least in the Mistral-7B with CR Qwen2-7B with CR Aloe-8B with CR Llama-3.1-8B with CR Gemma-2-27B with CR Yi-1.5-34B with CR Qwen2-72B with CR Llama-3.1-70B with CR Avg. with CR 60.72 +0.85 68.07 +1. 67.57 +4.11 70.25 +3.36 78.12 +0.05 73.23 +2.33 83.30 +2.12 83.06 +4. +2.27 48.22 +12.24 55.06 +8.27 58.91 +8.86 59.31 +8.85 61.61 +8. 57.54 +8.68 69.33 +4.57 72.17 +4.40 +8.08 52.32 +9.98 57.03 +4. 62.45 +9.98 63.71 +8.88 66.93 +2.75 61.43 +10.37 76.83 +2.44 79.81 +4. +6.74 66.05 +8.55 73.41 +3.57 72.76 +6.81 75.73 +5.45 81.70 +2. 78.69 +4.12 86.51 +2.41 87.21 +3.46 +4.87 Table 8: Accuracy of LLMs with and without the context retrieval components proposed, when evaluated on multichoice medical QA. hundreds of billions. We report GPT-4 results on these benchmarks with Medprompt. MedPalm-2 [26]: Developed by Google. We report MedPalm2, likely based on the Palm-2 model, with Ensemble refinement as prompting technique. Table 9 presents consolidated leaderboard, sorted by average performance across available datasets, by unifying the results shown in Table 8 with those reported for private models. This comparison reveals that our optimized CR configuration not only boosts the accuracy of open-source models but also enables them to achieve performance levels comparable to much larger private models. In particular, when augmented with our CR system, the Llama-3.1-70B and Qwen-2-72B models demonstrate competitive performance with Googles MedPalm-2 or OpenAIs GPT4."
        },
        {
            "title": "5 Open-Ended Answer Generation\nWhile multiple-choice question answering (MCQA) benchmarks\nhave been valuable in evaluating Large Language Models (LLMs)\nfor medical applications, they fail to fully capture the complexi-\nties of real-world clinical scenarios. In practice, healthcare profes-\nsionals often need to formulate comprehensive answers without\npre-defined options. This necessitates a shift towards open-ended\nquestion-answering capabilities in medical AI systems.",
            "content": "Our preliminary analysis revealed significant performance gap when transitioning from multiple-choice to open-ended formats. Table 10 illustrates this disparity, showing substantial decrease in accuracy of 10% for the Llama-3.1-8B-Instruct model on the MedQA dataset when transitioning from multiple choice questions (MCQs) to open-ended (OE) questions. Surprisingly, the incorporation of , , Jordi Bayarri-Planas, Ashwin Kumar Gururajan, and Dario Garcia-Gasulla Model RAG MedMCQA MedQA MMLU Avg. 87.83 GPT-4 MP 83.94 Llama-3.1-70B CR ER 82.36 80.69 CR 5S 80.40 79.60 5S 74.65 CR MedPalm-2 Qwen2-72B GPT-4 MedPalm-2 Gemma-2-27B 94.2 90.67 89.4 88.92 87.40 87.8 83. 90.2 84.60 85.4 79.26 81.40 79.7 69.68 79.1 76.57 72.3 73.89 72.40 71.3 70.38 Table 9: Accuracy of top performing models in medical MCQA, sorted by average performance. MP: Medprompt. CR: Context Retrieval. ER: Ensemble Refinement (Googles custom prompt technique). 5S: Five-shot Underlined values are reported by others [19, 26]. Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning did not improve performance for open-ended questions, contrary to their effectiveness in other domains. Type 0 Multiple-choice 0 5 CoT Open-Ended ToT Open-Ended Open-Ended Accuracy 63.71 53.34 52.40 51.93 Table 10: Baseline results of Llama-3.1-8B-Instruct for the MedQA dataset To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for openended medical question answering. We introduce the methodology in 5.1, our modifications for datasets and databases for OE generation in 5.2, the evaluation procedure in 5.3 and the results and discussion in 5.4."
        },
        {
            "title": "5.1 OpenMedprompt\nOpenMedprompt adapts the Medprompt architecture for open-\nended question answering. We replace the MCQA database with\nan OE-QA database and remove components specific to multiple-\nchoice formats. We propose two strategies for consensus-building\nand answer refinement:",
            "content": "OpenMedprompt with Ensemble Refining (OM-ER): This strategy leverages the diversity of multiple generated answers to produce refined and more accurate final response. It involves generating 𝑁 initial answers with randomized temperature and top_p parameters, incorporating 𝐾 relevant examples from the database into the prompt. Then, the LLM synthesizes these 𝑁 answers into single, refined response. OpenMedprompt with Self-reflection(OM-SR) [23, 25]: This strategy employs feedback loop to improve the generated answer. It begins by generating an initial answer using the 𝐾 most similar examples from the database. Then, it performs 𝑁 iterations of self-reflection, where the model generates feedback on its previous response and produces an improved answer based on this feedback. We integrate attribute scores from ArmoRM-Llama3-8B [28], reward model along with the critique models reflection as an external feedback to guide answer generation. The ArmoRM-Llama3-8B reward model provides reward scores across 19 attributes. Out of the nineteen total, we give the generation model only the following scores ultrafeedback-truthfulness, ultrafeedback-honesty,ultrafeedback-helpfullness and prometheusscore which have good correlation with correct responses. Figure 3 and Figure 4 illustrate the architecture of OM-ER and OM-SR, respectively."
        },
        {
            "title": "5.2 Dataset and Database Construction\nWe utilized the MedQA dataset, which is particularly suitable for\nthis task as it contains medical questions that can be answered\nwithout providing multiple-choice options, with minimal or no\nrephrasing of the original question required, making it ideal for\ntesting our proposed framework.",
            "content": "To create robust context retrieval system, we construct two distinct databases using the MedQA training set. We use LLaMA3.1-70B-Instruct to generate CoT and ToT answers for each question in the training set. Both databases are carefully curated to ensure the accuracy of the generated responses and their alignment with the ground truth answers. Figure 3: Components of the OpenMedprompt with Ensemble Refining (OM-ER) system. Figure 4: Components of the OpenMedprompt with Selfreflection(OM-SR) system."
        },
        {
            "title": "5.3 Experiments\nWe evaluate the performance of OM-ER and OM-SR using an au-\ntomated LLM-based judge (LLaMA-3.1-70B-Instruct). This judge\nassesses the correctness of the generated open-ended answers by\ncomparing them to the ground truth answers.",
            "content": "Table 11 presents the results of our experiments of OM-ER using the CoT and ToT databases. We see that both CoT and ToT databases show improvement over the baseline open-ended performance. ToT Boosting Healthcare LLMs Through Retrieved Context , , CoT-Accuracy ToT-Accuracy 3 5 7 10 15 20 56.17 56.40 56.87 56.72 59.31 58.76 57.82 56.40 57.58 59.54 59.00 60. Table 11: OM-ER accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database. 𝑁 represents the number of ensembles. Figure 5: OM-ER and OM-SR accuracy when changing the number of few-shots (K) with fixed number of ensembles (5). generally outperforms CoT, with the best result (60.02%) achieved using 20 ensembles. Table 12 shows the results for OM-SR with the CoT and ToT databases. We see that the OM-SR system shows more consistent improvement over the baseline compared to the OM-ER system. The CoT database performance peaks at N=15 (60.88%), outperforming ToT in this configuration. The ToT database on the other hand shows less variation across different values, suggesting more stable performance. CoT-Accuracy ToT-Accuracy 3 5 7 10 15 20 55.93 57.34 59.07 60.25 60.88 58. 57.50 57.89 58.21 60.02 58.52 58.68 Table 12: OM-SR accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database. 𝑁 represents the number of refinement iterations. While we expect performance to increase when the number of few shots increases we notice that in Figure 5 this isnt the case for OM-ER ToT. This could be attributed to the effective context length Figure 6: OM-ER and OM-SR accuracy when changing the number of ensembles (N) with fixed number of few-shots (5). of the LLM. ToT responses are very verbose and when increasing the number of few-shots the context size significantly grows making it difficult for the LLM to reason over this large context. We also see that the increase in the number of ensembles doesnt consistently improve the accuracy in Figure 6. OM-ER benefits from ensemble sizes up to 20 while for the OM-SR method ensemble size of 15 provides the best performance for CoT while ensemble size 10 provides the best performance for ToT."
        },
        {
            "title": "5.4 Results and Discussion\nOur results demonstrate the effectiveness of OpenMedprompt in\nimproving open-ended answer generation accuracy in the medical\ndomain. Both OM-ER and OM-SR contribute to performance gains\ncompared to the baseline. The choice of database and the number\nof retrieved examples also play a significant role in performance.",
            "content": "Specifically, we observe that OM-SR generally outperforms OMER across most configurations. This suggests that the iterative feedback loop and incorporation of reward model scores provide more effective mechanism for refining the generated answers. The choice of database (CoT vs. ToT) also has an impact on the performance differently for each approach. OM-ER benefits more from the ToT database, while OM-SR shows stronger results with the CoT database at higher iteration counts. OM-SR often achieves good performance with fewer iterations compared to the number of ensembles required for OM-ER to reach similar accuracy levels. However, OM-ER might be preferred when speed and simplicity are prioritized, or when exploring diverse perspectives is beneficial. Choosing the Right Configuration: Accuracy Priority: Choose OM-SR, particularly with higher iteration counts (𝑁 10). Complex Reasoning: OM-SRs self-reflection mechanism may be more effective for questions requiring intricate logical steps. Speed and Simplicity: OM-ER with moderate ensemble sizes (𝑁 = 510) offers good balance of improved accuracy and computational efficiency. , , Jordi Bayarri-Planas, Ashwin Kumar Gururajan, and Dario Garcia-Gasulla Diverse Perspectives: OM-ER inherently generates multiple initial answers, which can be valuable when diversity of thought is important. Task: The OM-ER approach is not suitable when large output texts are expected, as models start running out of effective context windows when merging the responses to create unified answer."
        },
        {
            "title": "6 Conclusions\nThis work underscores the significant potential of augmenting\nLarge Language Models (LLMs) with context retrieval systems to\nenhance their accuracy and reliability in the healthcare domain. Our\nexploration of Self-Consistency with Chain-of-Thought (SC-CoT)\ncomponents revealed substantial gains through choice shuffling\nand an optimal number of ensembles, striking a balance between\nperformance and computational cost. Further investigation into\nthe Medprompt architecture highlighted the effectiveness of small,\nhealthcare-specific embedding models and the value of enriching\ndatabases with Chain-of-Thought augmented examples. Conversely,\nthe inclusion of a reranking model was found to be computationally\nexpensive with inconsistent benefits, leading us to recommend\nagainst its use.",
            "content": "Our optimized context retrieval configuration, when applied to diverse set of open-source LLMs, consistently boosted performance across multiple medical question-answering benchmarks. Notably, smaller models experienced the most significant improvements, showcasing the ability of well-tuned retrieval systems to bridge the performance gap between smaller open models and larger private alternatives. This finding has profound implications for democratizing access to high-performing healthcare AI systems, reducing reliance on resource-intensive large models. Moreover, our results demonstrate that open LLMs augmented with our optimized CR system can achieve accuracy comparable to, and in some cases surpassing, state-of-the-art private solutions like MedPalm-2 and GPT-4. Recognizing the limitations of multiple-choice question answering (MCQA) in mirroring real-world clinical scenarios, we extended our approach to develop OpenMedprompt, novel framework for open-ended medical question answering. Two distinct strategies, OpenMedprompt with Ensemble Refining (OM-ER) and OpenMedprompt with Self-Reflection (OM-SR), were introduced and evaluated, revealing their effectiveness in improving open-ended answer generation accuracy. OM-SR, with its iterative feedback loop and integration of reward model scores, generally outperformed OM-ER, suggesting the potential of self-reflection mechanisms for complex medical reasoning. By releasing our custom software repository as an open-source resource, we aim to empower the research community to further explore and refine these techniques, contributing to the development of more accurate, accessible, and impactful LLM applications in healthcare."
        },
        {
            "title": "6.1 Future Work\nSeveral avenues for future research remain, encompassing both\nthe optimization of retrieval-augmented generation for multiple-\nchoice questions and the further development of OpenMedprompt\nfor open-ended answer generation.",
            "content": "Expanding Retrieval System Capabilities: Dynamic Retrieval: Exploring dynamic retrieval techniques that adapt the number of retrieved examples based on the complexity of the question, potentially improving efficiency and accuracy. Multi-Database Integration: Investigating the integration of multiple knowledge sources, such as medical ontologies or clinical guidelines, to enrich the retrieval database and enhance the LLMs understanding of complex medical concepts. Cross-Lingual Retrieval: Adapting the retrieval system to support multiple languages, facilitating broader access to medical information and enabling cross-lingual medical question answering. Enhancing OpenMedprompt: Hybrid Approaches: Exploring combinations of OM-ER and OM-SR to leverage the strengths of both methods, potentially leading to more robust and adaptable system for open-ended medical question answering. Advanced Reward Models: Investigating more sophisticated reward models tailored specifically to medical knowledge evaluation, capturing nuanced aspects of medical reasoning, factual accuracy, and clinical relevance. Prompt Engineering: Finetuning prompt structures for both initial answer generation and refinement stages, incorporating specific instructions, constraints, or contextual information to guide the LLM towards more accurate and comprehensive responses. Larger Models: Evaluating the performance of OpenMedprompt with more powerful language models to assess scalability and explore the potential for further accuracy gains. Domain Adaptation: Extending OpenMedprompt to other specialized domains beyond medicine, such as law or engineering, focusing on tailoring the retrieval database and reward models to the specific knowledge requirements of each domain."
        },
        {
            "title": "6.2 Carbon Footprint",
            "content": "Medprompt OpenMedprompt Time Power consumption Footprint 652.90 195.40 169.84 27.47 828.47 134.01 Total 848. 962.48 197.31 Table 13: Computational cost of the experiments conducted in this study. Time is expressed in hours, power consumption in KWh and carbon footprint in kg/CO2."
        },
        {
            "title": "6.3 Ethical Considerations\nThis work seeks to increase the factuality and reliability of LLM-\nbased systems. However, any work that deals with generative mod-\nels like LLMs should be done in awareness of the several ethical\nlimitations that the technology entails. This includes the compu-\ntational footprint (which is considered in this work), as well as\nthe factuality (which is the main target of this work). Limitations\nrelated to the potential for impersonation, self-diagnose and other\nnon-approved uses, are further discussed in publications related\nwith healthcare model releases (e.g., [7]).",
            "content": "Boosting Healthcare LLMs Through Retrieved Context , , Acknowledgments This work has been partly funded by the AI4Europe projects from the European Unions Horizon 2020 programme (Grant Agreements Nº951911 and Nº101070000), and by SGR-GRE grant from the Generalitat de Catalunya (code 2021 SGR 01187). References [1] 01. AI, :, and Alex Young et al. 2024. Yi: Open Foundation Models by 01.AI. arXiv:2403.04652 [cs.CL] https://arxiv.org/abs/2403.04652 [2] Mistral AI. 2024. Mixtral of Experts. https://mistral.ai/news/mixtral-of-experts/ The Llama 3 Herd of Models. [3] Abhimanyu Dubey et al. 2024. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [4] An Yang et al. 2024. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671 (2024). [5] Tom B. Brown et al. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165 [6] Lucia Urcelay Ganzabal and Pablo Bernabeu Pérez. 2024. //huggingface.co/datasets/HPAI-BSC/CareQA careqa. https: [7] Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio AlvarezNapagao, Eduard Ayguadé-Parra, and Ulises Cortés Dario Garcia-Gasulla. 2024. Aloe: Family of Fine-tuned Open Healthcare LLMs. arXiv:2405.01886 [cs.CL] https://arxiv.org/abs/2405.01886 [8] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. 2024. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nature medicine (2024), 110. [9] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. arXiv:2009.03300 [cs.CY] https://arxiv.org/abs/2009. [10] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] https: //arxiv.org/abs/2310.06825 [11] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences 11, 14 (2021), 6421. [12] Qiao Jin, Won Kim, Qingyu Chen, Donald Comeau, Lana Yeganova, John Wilbur, and Zhiyong Lu. 2023. MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval. Bioinformatics 39, 11 (2023), btad651. [13] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL] https://arxiv.org/abs/ 2005.11401 [21] Ankit Pal and Malaikannan Sankarasubbu. 2024. Gemini goes to med school: exploring the capabilities of multimodal large language models on medical challenge problems & hallucinations. arXiv preprint arXiv:2402.07023 (2024). [22] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA : Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. arXiv:2203.14371 [cs.CL] https://arxiv.org/abs/2203. 14371 [23] Matthew Renze and Erhan Guven. 2024. Self-Reflection in LLM Agents: Effects on Problem-Solving Performance. arXiv:2405.06682 [cs.CL] https://arxiv.org/ abs/2405. [24] Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H. Chen. 2024. Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine. npj Digital Medicine 7, 1 (2024), 20. https: //doi.org/10.1038/s41746-024-01010-1 [25] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs.AI] https://arxiv.org/abs/2303. 11366 [26] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with Large Language Models. arXiv:2305.09617 [cs.CL] https://arxiv.org/abs/2305.09617 [27] Gemma Team and Morgane Riviere et al. 2024. Gemma 2: Improving Open Language Models at Practical Size. arXiv:2408.00118 [cs.CL] https://arxiv.org/ abs/2408.00118 [28] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. [n. d.]. Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-ofExperts. arXiv preprint arXiv:2406.12845 ([n. d.]). [29] Li Wang, Xi Chen, XiangWen Deng, Hao Wen, MingKe You, WeiZhi Liu, Qi Li, and Jian Li. 2024. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. npj Digital Medicine 7, 1 (2024), 41. https://doi.org/10.1038/s41746-024-01029- [30] Xiaodan Wang, Lei Li, Zhixu Li, Xuwu Wang, Xiangru Zhu, Chengyu Wang, Jun Huang, and Yanghua Xiao. 2023. Agree: Aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 456464. [31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs.CL] https://arxiv.org/abs/2203.11171 [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] https: //arxiv.org/abs/2201.11903 [33] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv:2305.10601 [cs.CL] https://arxiv.org/abs/ 2305.10601 [14] Xianming Li and Jing Li. 2023. AnglE-optimized Text Embeddings. preprint arXiv:2309.12871 (2023). [15] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. 2024. Can large language models reason about medical questions? Patterns 5, 3 (2024). [16] Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, and Ritankar Das. 2024. OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models. Scientific Reports 14, 1 (2024), 14156. [17] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning. Salesforce AI Research Blog. https://blog.salesforceairesearch.com/sfrembedded-mistral/ [18] David Mezzetti. 2023. Embeddings for Medical Literature. (2023). https://medium. com/neuml/embeddings-for-medical-literature [19] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. 2023. Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine. arXiv:2311.16452 [cs.CL] https://arxiv.org/abs/2311. [20] OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv. org/abs/2303."
        }
    ],
    "affiliations": [
        "Barcelona Supercomputing Center"
    ]
}