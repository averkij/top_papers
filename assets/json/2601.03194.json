{
    "paper_title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "authors": [
        "Mohammad Zia Ur Rehman",
        "Sai Kartheek Reddy Kasu",
        "Shashivardhan Reddy Koppula",
        "Sai Rithwik Reddy Chirra",
        "Shwetank Shekhar Singh",
        "Nagendra Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST"
        },
        {
            "title": "Start",
            "content": "X-MuTeST: Multilingual Benchmark for Explainable Hate Speech Detection and Novel LLM-consulted Explanation Framework Mohammad Zia Ur Rehmana (phd2101201005@iiti.ac.in) Sai Kartheek Reddy Kasub (saikartheekreddykasu@gmail.com) Shashivardhan Reddy Koppulaa (cse210001032@iiti.ac.in) Sai Rithwik Reddy Chirrac (schirra7@asu.edu) Shwetank Shekhar Singhd (b21322@students.iitmandi.ac.in,) Nagendra Kumara (nagendra@iiti.ac.in) Indian Institute of Technology Indore, Madhya Pradesh India Indian Institute of Information Technology Dharwad, India Arizona State University, United States Indian Institute of Technology Mandi, India This is the preprint version of the accepted paper. Accepted in Proceedings of AAAI, 2026 6 2 0 2 J 6 ] . [ 1 4 9 1 3 0 . 1 0 6 2 : r X-MuTeST: Multilingual Benchmark for Explainable Hate Speech Detection and Novel LLM-consulted Explanation Framework Mohammad Zia Ur Rehman1, Sai Kartheek Reddy Kasu2, Shashivardhan Reddy Koppula1, Sai Rithwik Reddy Chirra3, Shwetank Shekhar Singh4, Nagendra Kumar1* 1Indian Institute of Technology Indore, India 2Indian Institute of Information Technology Dharwad, India 3Arizona State University, United States 4Indian Institute of Technology Mandi, India phd2101201005@iiti.ac.in, saikartheekreddykasu@gmail.com, cse210001032@iiti.ac.in, schirra7@asu.edu, b21322@students.iitmandi.ac.in, nagendra@iiti.ac.in"
        },
        {
            "title": "Abstract",
            "content": "training Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose novel explainabilityframework, X-MuTeST (eXplainable guided Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attentionenhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and the models explainability. Moreover, combining human rationales with our explainability method to refine the models attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on: https://github.com/ziarehman30/X-MuTeST. can be prompted to provide the explanations in the form of an identified hateful list of words (Roy et al. 2023) for given sentence, whereas methods such as BERT can provide explanations in the form of attention given to each word of the sentence (Mathew et al. 2021). However, in the case of under-resourced languages, where overall sequence classification performance has improved significantly over time (Rehman et al. 2023), machine-provided rationales, many time, do not align well with human rationales. For instance, in Figure 1, the translation of the first and third rationales provided by the human annotator would be to feel offended and to bark, respectively, which are very offensive terms in the Hindi language. Though the LLMs correctly identify the explicit term dog, they do not identify first and third rationale terms as hateful or offensive, which can be attributed to the fact that societal and cultural aspects of languages are often overlooked by these methods for under-resourced languages. This gap between machineprovided rationales and human rationales can be bridged by providing rationale resources for these languages. Introduction The rise in online media usage has heightened exposure to hate speech, making detection systems increasingly essential. While initial research focused only on the detection of hateful content (Waseem and Hovy 2016), more recent efforts have shifted towards explainability and addressing the rationales behind labelling content as hateful (Clarke et al. 2023). Advanced methods such as Large Language Models (LLMs) and their predecessors, such as BERT (Devlin et al. 2019), can provide explanations for such decisions. LLMs *Corresponding author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Example of human and LLM rationales Motivation: Scarcity of Hateful Rationales Though numerous rationale and reasoning-based resources for the English language (Rajani et al. 2019; Camburu et al. 2018) are compiled in ERASER (DeYoung et al. 2020) in various domains spanning from movie reviews (Zaidan and Eisner 2008), questions with multiple correct answers (Khashabi et al. 2018), and to other miscellaneous domains (Lehman et al. 2019; Clark et al. 2019), there is scarcity of rationale-based resources in the domain of hate speech detection. In 2021, Mathew et al. (2021) introduced the HateXplain dataset, which features word and phrase-level span annotations for hate speech in English, claiming to be the first of its kind. However, similar datasets for under-resourced languages such as Hindi and Telugu are nonexistent to the best of our knowledge. Even for English, HateXplain is one of the few datasets offering human rationales for hate speech detection. To address this gap, we provide word-level human rationales for hate speech detection datasets in Telugu, Hindi, and English. Our primary focus was on Indic languages. However, we believe that evaluating models on the English dataset alongside HateXplain enhances the generalizability of future approaches. Glance at the Proposed Solution We propose novel explainability framework that combines LLM-based and n-gram-based explanations to generate final explanations. We propose an eXplainable Multilingual haTe Speech deTection ( X-MuTeST) method. X-MuTeST is two-stage explainable hate speech detection framework designed to enhance both classification performance and explainability. It incorporates novel explainability method based on n-grams to identify relevant tokens. In the first stage, the models attention is guided by human-annotated rationales to align the models focus with key tokens identified by humans. In the second stage, training is guided by the n-gram-based explainability method, which generates attention masks based on model-driven token importance. This approach balances human rationales and the models insights, improving both explainability and classification performance. Finally, union of explanations is taken from LLama 3.1 and X-MuTeST. Through experiments on three datasets, Hindi, Telugu, and English, we demonstrate the efficacy of the proposed framework. The key contributions of this work are as follows: We contribute benchmark human-annotated rationales for under-resourced languages, such as Hindi and Telugu, alongside English. This will provide valuable resource for future research in explainable hate speech detection across multiple languages. We propose novel LLM-consulted and n-gram-based hybrid explainability method that combines LLMs with traditional transformer-based attention improvement. The proposed method enhances explainability performance. We propose novel explainable hate speech detection framework with two-stage training method that balances classification and explainability performance."
        },
        {
            "title": "Related Work",
            "content": "Explainability in Hate Speech Detection Explainability in hate speech detection has garnered significant attention, with recent advancements highlighting both its potential and underlying challenges. The works, such as Zaidan, Eisner, and Piatko (2007) and Yessenalina, Choi, and Cardie (2010) utilized rationales for improving sentiment classification, which set the stage for explainable approaches in more domains (Wang et al. 2024), including hate speech (Mathew et al. 2021; Lin et al. 2024). The work of Mathew et al. (2021) provides one of the first datasets to include human-annotated rationales for hate speech. For each sample, annotators not only label whether the text contains hate speech but also highlight the specific tokens that are relevant to that judgment. The authors highlight that even though model may be accurate in classification, its reasoning may not align with human logic or provide meaningful explanations. Recent works in LLMs and attention-based models also highlight critical challenges in explainability, as seen in works by Roy et al. (2023), Clarke et al. (2023), and Arshad and Shahzad (2024). These studies underscore difficulties in ensuring faithful rationales, even when powerful models achieve high classification accuracy. Multilingual Explainable Methods Kapil et al. (2023) and Sawant et al. (2024) emphasize the importance of precise annotations and transparency in lowresource languages such as Hindi, where cultural aspects affect how hate speech is perceived and classified. Similarly, Yadav, Kaushik, and McDaid (2023) address the complexities of code-mixed languages, highlighting how explainability helps clarify model predictions in multilingual settings. In this context, Geleta et al. (2023) introduce multilingual framework for assessing hate speech intensity levels using explainable AI, further underlining the need for transparent hate speech detection models. By leveraging explainable methodologies, these studies collectively enhance the detection of hate speech across linguistic boundaries, ensuring that cultural and linguistic differences are accounted for in model interpretation."
        },
        {
            "title": "Dataset Description and Annotation",
            "content": "Dataset Source The Hindi and English datasets utilized in this research are derived from the Hate Speech and Offensive Content Identification (HASOC) contest, which is dedicated to the detection of hate speech and offensive language across various languages and dialects. We collect datasets from HASOC 2020 (Mandl et al. 2020a,b) and HASOC 2021 (Modha et al. 2021) for Hindi and English, which include samples sourced from Twitter featuring annotated instances of hate speech and offensive content. We mix datasets of two years to increase the number of samples. We select binary labels that categorize the sentences into HATE and NOT HATE categories. Subsequently, the testing set is split, where 15% of the samples are retained for testing for both languages. Dataset details are shown in Table 1. The Telugu dataset is collected from the Hate and Offensive Language Detection in Telugu Codemixed Text (HOLD-Telugu) task (Priyadharshini et al. 2023; Premjith et al. 2024) organized as part of DravidianLangTech 2024 (Chakravarthi et al. 2024). The comments in the dataset are collected from YouTube and annotated for binary class labels, HATE and NON-HATE. The original dataset contains text in Latin script; we transliterate the text into Telugu script using the IndicXlit method (Madhani et al. 2023). testing set is provided in the dataset. Dataset details are shown in Table 1. Metric Total Records Hate Records Non-Hate Records Avg Word Count Avg Character Count Avg Rationale Length Telugu 4,492 2,131 2,361 6.46 68.98 3.10 Hindi 6,004 2,026 3,978 18.84 110.93 3. English 6,334 3,767 2,567 19.20 100.47 3.78 Table 1: Summary of hate and non-hate records Profile of Annotators To ensure the reliability and consistency of the data annotation process, we employed rigorous selection and training procedure for our annotators. Five annotators were hired based on their linguistic proficiency, subject matter expertise, and prior experience in annotation tasks. Annotators were selected such that we had three annotators for each language. All annotators underwent structured training phase to familiarize themselves with the annotation guidelines. Periodic quality checks were conducted, and discrepancies were resolved through consensus discussions (Rehman et al. 2025). Given the nature of our dataset, we carefully select annotators above the age of 18 from diverse regions across the country to mitigate potential demographic biases. Rationale Annotation Procedure To ensure the quality and accuracy of annotations, we engage native language experts fluent in Hindi, English, and Telugu. This is crucial for preserving the societal and cultural aspects within the dataset. Each post is annotated by three annotators. Annotators are tasked with identifying token-level rationales that explain why particular text is classified as hateful or non-hateful. For instance, in sentence marked as hateful, the rationale could be an offensive word or phrase that justifies the hate speech classification. Human annotators provide these rationales by marking hateindicating tokens with label of 1 and non-hate tokens with 0, enhancing both model interpretability and explainability. In non-hate samples, all tokens are marked as 0. During the initial annotation rounds, we observe moderate to substantial agreement (61-80%). Since our dataset is intended to support explainability in downstream models, maintaining high annotation consistency is crucial. Although annotators are initially provided with detailed guidelines, the agreement scores indicate the need for further refinement. As supported in prior works (Schmer-Galunder et al. 2024; Hahn et al. 2012), iterative annotation can significantly improve label quality and annotator alignment. To address this, we employ an iterative annotation process, as shown in Figure 2, conducting additional guidance sessions and having annotators re-annotate the dataset, which increased the agreement among annotators. As token of appreciation and to ensure fair compensation, annotators are provided with free access to our GPU for 150 hours, valid for one year. Each dataset is annotated by three annotators for rationales. Table 2 shows the inter-annotator agreement scores using Cohens Kappa method between any two annotators, whereas overall agreement is computed using Fleiss method. From Table 2, all three languages show substantial agreement. English exhibits the highest consistency (85.10), while Hindi shows slightly lower pairwise scores. These results confirm that iterative refinement notably improved annotation reliability across languages. Figure 2: Iterative annotation procedure"
        },
        {
            "title": "Language\nTelugu\nHindi\nEnglish",
            "content": "A1 & A2 82.50 86.45 87.60 A2 & A3 87.75 88.00 82.00 A3 & A1 88.25 79.20 89.30 Overall 81.00 83.15 85.10 Table 2: Kappa agreement scores Methodology We propose X-MuTeST, an explainable hate speech detection framework that leverages both human rationales and novel explainability formulation to train model. Training proceeds in two stages as shown in Figure 3. In Stage-1, the model is trained for three epochs with human rationales guiding its attention. In Stage-2, the target attention mask is derived from our explainability method, allowing the model to balance human annotations with tokens deemed salient by the model itself. This dual approach enhances both classification accuracy and interoperability. For the final, explainability, we take the union of explanations provided by LLama-3.1 and our attention-based method. We integrate LLMs, as they have shown promise in semantic and contextual reasoning (Kasu et al. 2025). Specifically, LLaMA-3.1 is selected based on empirical evidence, as it demonstrated higher token-level alignment with human rationales than other LLMs, for Telugu and Hindi. For each language, we select the encoder that provides the best classification performance after finetuning. Hence, for X-MuTeST framework, we employ Muril (Khanuja et al. 2021) for Telugu and English and XLMR (Conneau et al. 2020) for Hindi. Training Procedure In Stage-1, the models attention is guided by human rationales. Let ai denote the normalized attention score assigned Figure 3: Architecture of LLM-consulted explanation framework to the ith token (word) by the model and Ri the corresponding binary human rationale. The attention weights are computed using the dot product between token representations and the [CLS] token, followed by softmax normalization: Explainability via N-gram Contributions Our explainability method quantifies the importance of each token based on its contributions across overlapping n-grams. Let the models output logits for the original sequence be: ai = h[CLS]) exp(h j=1 exp(h h[CLS]) (cid:80)L (1) where hi and h[CLS] are the hidden states of the ith token and the [CLS] token respectively. The attention alignment loss is computed using crossentropy: Latt = (cid:88) i="
        },
        {
            "title": "Ri log ai",
            "content": "(2)"
        },
        {
            "title": "The overall loss function combines this attention loss with",
            "content": "the standard classification loss Lcl as follows: L1 = α Latt + (1 α) Lcl (3) where α is balancing coefficient (set to 0.3 in Stage-1). This helps the model to jointly learn task-specific classification while aligning its attention with human-provided rationale supervision. In Stage-2, the human rationales are replaced by modelgenerated attention targets derived from our explainability method. For sequence of length L, we identify the top tokens based on their explainability scores, where = (cid:26)5, if 10 L/2, otherwise (4) The loss function in this stage remains the same as in Equation (3), but with updated α values: 0.6 for Telugu and English, and 0.7 for Hindi. These values are selected through empirical tuning for best validation performance. Porig = (S) For each n-gram ng (n = 1, 2, 3), we compute: Png = (ng) and define the logit drop as: (5) (6) Png = Porig Png The unnormalized importance score for token is then computed as: (7) E[t] = 3 (cid:88) n=1 wn"
        },
        {
            "title": "1\nN (n)\nt",
            "content": "(cid:88) ngt Png (8) where w1 = 0.5, w2 = 0.3, and w3 = 0.2 are the weights assigned to unigrams, bigrams, and trigrams respectively, and (n) is the number of n-grams of length containing token t. Finally, the scores are normalized: E[t] = E[t] E[j] (cid:80) (9) This formulation captures both token-level and contextual relevance, providing more comprehensive guidance to the model in Stage-2. Final Explanations through LLM Consultation As shown in Figure 3, the final explanation is derived by combining rationales from two sources: our in-model ngram based method and LLM-generated rationales. Let EX denote the set of top tokens selected by the X-MuTeST explainability scores, and ELLM the rationales returned by LLaMa-3.1. The final explanation set is given by: Efinal = EX ELLM (10) To assess alignment between the two explanation sources, we also compute an agreement score: Agreement = EX ELLM EX ELLM (11) This union-based approach ensures both syntactic and semantic coverage, combining task-specific saliency with LLM-derived contextual reasoning. LLaMa-3.1 is used for LLM consultation due to its superior interpretability, as discussed in the results section."
        },
        {
            "title": "Experimental Setup and Results",
            "content": "Metrics for Evaluation Performance-Based Metrics We standard performance-based metrics, including accuracy, F1 score, and macro-F1, to evaluate the classification performance. use Explainability-Based Metrics We assess explainability using two metrics: plausibility (how believable the explanation is) and faithfulness (how well it reflects the models decision process) (Jacovi and Goldberg 2020; Mathew et al. 2021). Plausibility Plausibility is measured via token-level F1score and Intersection-over-Union (IOU) F1-score. IOU is computed as the ratio of overlapping tokens to the union of predicted and ground truth rationales, with match defined when the overlap exceeds 0.5 (DeYoung et al. 2020; Mathew et al. 2021). Faithfulness Faithfulness is evaluated using comprehensiveness and sufficiency (DeYoung et al. 2020; Mathew et al. 2021). Comprehensiveness quantifies the reliance on rationales by measuring the drop in prediction confidence when they are removed: Comprehensiveness = (xi)c (xi ri)c (12) where (xi)c is the probability for class using the full input xi, and (xi ri)c is the probability when the predicted rationale ri is masked. Sufficiency evaluates whether the rationale alone supports the prediction: Sufficiency = (xi)c (ri)c (13) with (ri)c computed from the rationale ri only. Lower sufficiency indicates that the rationale is informative. For both metrics, tokens are replaced with the < ASK > token, and the top-5 tokens are used as the predicted rationales. Comparison Methods transformer encoders We employ different multilingual for classification, whereas LIME and X-MuTeST methods are used for explainability. Additionally, we employ three LLMs, GPT-4o (OpenAI 2024), LLaMa-3.1 (Dubey et al. 2024), and Mistral-24.07 (MistralAI 2024), using the prompt-based zero-shot method. We use token-F1 and IOUF1 to evaluate the explainability of LLMs. As prompt-based methods do not provide probability scores for their decision, comprehensiveness and sufficiency scores could not be computed for LLMs. We also compared our method with BERTHateXplain-LIME (Mathew et al. 2021) and with BERT-ILP method (Nguyen and Rudra 2022). We evaluate transformer encoders as follows: We employ two multilingual encoders Muril (Khanuja et al. 2021) and XLMR (Conneau et al. 2020) for comparison. First, the base encoder model is finetuned for the sequence classification objective, and the LIME and the XMuTeST approaches are used to evaluate explainability performance after training. Next, the base encoder is trained by integrating human rationales, and subsequently, LIME and the X-MuTeST method are employed for explainability. Results Analysis for Telugu Table 3 presents the classification and explanation performance of various models on the Telugu dataset. The LLMconsulted X-MuTeST outperforms all other models across metrics. It achieves the highest accuracy (0.8881), F1 score (0.8762), and Macro-F1 (0.8849), surpassing the next-best baseline, Muril-RX, by 1.41% in accuracy and 1.29% in Macro-F1. These results highlight the effectiveness of integrating LLM-consulted explanations combined with XMuTeST. On explainability metrics, X-MuTeST yields the best plausibility performance, with token-F1 of 0.6231 and IOUF1 of 0.3189, improvements of 4.77% and 2.76%, respectively, over Muril-RX. This indicates stronger alignment of the generated rationales with human annotations. In terms of faithfulness, X-MuTeST obtains lower (better) sufficiency score of 0.0448 compared to 0.0747 for Muril-RX, suggesting that its identified rationales are more essential to the models prediction. While Muril-RX performs competitively, X-MuTeSTs two-stage learning process, combining n-gram-based attention improvement with LLM-generated explanation fusion, leads to superior interpretability and confidence. In contrast, LLMs such as GPT-4o, Llama-3.1, and Mistral-24.07 show lower performance in both classification and explainability, likely due to inaccurate transliteration from Latin to Telugu script. Results Analysis for Hindi Table 4 shows that X-MuTeST with LLM achieves the highest classification and explanation scores for Hindi, with an accuracy of 0.8745 and token-F1 of 0.4344. Compared to the next-best model (Muril-RX), it gains 3.88% in accuracy and 2.27% in token-F1. It also improves sufficiency (0.4768 vs. 0.5042), showing more faithful explanations. While LLMs like Llama-3.1 show high token-F1 (0.4263), their classification drops sharply by 35.73%, indicating inaccurate token lists. XLMR and BERT-based baselines also trail across all metrics. These results confirm that the LLM-guided twostage training in X-MuTeST produces more accurate and human-aligned explanations. Results Analysis for English Consistent with the findings for Telugu and Hindi, XMuTeST achieves the highest classification performance Model XLMR-LIME XLMR-XMuTeST XLMR-Rationale-LIME XLMR-Rationale-XMuTeST Muril-LIME Muril-XMuTeST Muril-Rationale-LIME Muril-Rationale-XMuTeST GPT-4o Llama-3.1 Mistral-24.07 BERT-HateXplain-LIME BERT-ILP Acc 0.7927 0.7927 0.8455 0.8455 0.8598 0. 0.8740 0.8740 0.6484 0.6362 0.6402 0. 0.8213 Performance Plausibility Faithfulness F1 Macro-F1 Token-F1 IOU-F1 Comp Suff 0.8030 0. 0.8355 0.8355 0.8595 0.8595 0.8640 0. 0.5929 0.6716 0.4451 0.8075 0.8183 0. 0.7921 0.8450 0.8450 0.8591 0.8591 0. 0.8733 0.6417 0.6319 0.5895 0.8084 0. 0.5143 0.5256 0.5492 0.5634 0.5515 0. 0.5608 0.5754 0.3893 0.5154 0.3453 0. 0.5236 0.2232 0.2261 0.2385 0.2417 0. 0.2919 0.2941 0.3013 0.1815 0.2806 0. 0.2233 0.2440 0.6470 0.6539 0.7395 0. 0.6676 0.6852 0.6968 0.7243 - - - 0.3094 0.2568 0.1380 0.1035 0. 0.0947 0.0802 0.0747 - - - 0.6851 0.5833 0.0516 0.0849 X-MuTeST with LLM 0. 0.8762 0.8849 0.6231 0.3189 0.7456 0. Table 3: Classification and explainability performance of various methods on Telugu"
        },
        {
            "title": "Faithfulness",
            "content": "F1 Macro-F1 Token-F1 IOU-F1 Comp Suff"
        },
        {
            "title": "Model",
            "content": "XLMR-LIME XLMR-XMuTeST XLMR-Rationale-LIME XLMR-Rationale-XMuTeST Muril-LIME Muril-XMuTeST Muril-Rationale-LIME Muril-Rationale-XMuTeST GPT-4o Llama-3.1 Mistral-24.07 BERT-HateXplain-LIME BERT-ILP Acc 0.8535 0.8535 0.8590 0. 0.8269 0.8269 0.8357 0.8357 0.5461 0. 0.7159 0.8557 0.8395 0.7988 0.7988 0. 0.8000 0.7679 0.7679 0.7716 0.7716 0. 0.5829 0.6684 0.7884 0.7918 0.8418 0. 0.8456 0.8456 0.8149 0.8149 0.8217 0. 0.5542 0.5049 0.7099 0.8376 0.8302 X-MuTeST with LLM 0.8745 0.8168 0.8623 0.3068 0.3948 0. 0.4007 0.3124 0.4182 0.3160 0.4117 0. 0.4263 0.3975 0.2822 0.3941 0.4344 0. 0.1799 0.1564 0.1832 0.1588 0.1734 0. 0.1906 0.2266 0.3063 0.2832 0.1470 0. 0.2667 0.6651 0.7122 0.7011 0.7261 0. 0.5486 0.6890 0.6945 - - - 0.7296 0.6528 0.5314 0.5612 0.5517 0. 0.4936 0.5042 - - - 0. 0.6299 0.2111 0.2704 0.7483 0.4768 Table 4: Classification and explainability performance of various methods on Hindi in English as shown in Table 5. In terms of explainability, X-MuTeST leads with token-F1 score of 0.5125 and demonstrates strong faithfulness with high comprehensiveness of 0.8433 and the lowest sufficiency score of 0.0507. LLMs show moderate classification performance, with GPT4o scoring relatively high in token-F1 of 0.4613 but lower in overall classification accuracy (0.7802). Mistral24.07 achieves the highest IOU-F1 of 0.3176, though its overall performance lags behind X-MuTeST. Overall, XMuTeST consistently outperforms other models in classification while maintaining competitive scores in explainability for the English dataset, too. Parameter Sensitivity Analysis & Ablation The graphs in Figure 4 show the influence of α on the classification performance in different languages. The α parameter denotes the weights given to attention loss, whereas (1α) weights are given to classification loss. The attention loss is computed between the attention mask generated through the X-MuTeST explainability method and the predicted average attention score for each token relative to the [CLS] token. For Hindi and English, the best results are achieved when α is set to 0.6, denoting that the X-MuTeSTs attention mask aligns well with that of the model. Lower performance is noted on the boundary conditions when either the entire weight is given to attention loss or the classification loss, denoting that integration of the X-MuTeST explainability method strikes balance, resulting in enhanced performance. The best performance for Telugu is noted when α is set to 0.7. Additionally, Figure 5 shows the ablation study results on two stages of training. Performance degradation is observed on the removal of any of the two stages of training as depicted in the graph. This further demonstrates that the proposed training method of X-MuTeST enhances the clasPerformance Plausibility Faithfulness F1 Macro-F1 Token-F1 IOU-F1 Comp Suff Model XLMR-LIME XLMR-XMuTeST XLMR-Rationale-LIME XLMR-Rationale-XMuTeST Muril-LIME Muril-XMuTeST Muril-Rationale-LIME Muril-Rationale-XMuTeST GPT-4o Llama-3. Mistral-24.07 BERT-HateXplain-LIME BERT-ILP Acc 0.8223 0. 0.8349 0.8349 0.8307 0.8307 0.8433 0. 0.7802 0.7560 0.7918 0.8339 0.8145 0. 0.8609 0.8673 0.8673 0.8694 0.8694 0. 0.8744 0.8266 0.8086 0.8448 0.8647 0. 0.8075 0.8075 0.8245 0.8245 0.8144 0. 0.8385 0.8385 0.7633 0.7362 0.7643 0. 0.8028 X-MuTeST with LLM 0.8604 0.8827 0.8513 0. 0.4087 0.4665 0.4752 0.3908 0.3928 0. 0.4743 0.4613 0.3995 0.3934 0.4454 0. 0.5125 0.2279 0.2195 0.2389 0.2357 0. 0.2184 0.2121 0.2195 0.3097 0.2819 0. 0.2382 0.2291 0.3086 0.6500 0.6842 0. 0.7682 0.7365 0.7527 0.7992 0.8372 - - - 0.1244 0.0945 0.0559 0. 0.0898 0.0871 0.0856 0.0795 - - - 0.8555 0.7824 0.2019 0.2008 0. 0.0507 Table 5: Classification and explainability performance of various methods on English Figure 4: Influence of α parameter on classification performance in stage-2 training results in this work by 9.2% and 8.1%, respectively. It also yielded 5.6% improvement in comprehensiveness. Similarly, on the HateBRXplain dataset, X-MuTeST with LLM recorded 6.9% gain in Token-F1 and 2.3% improvement in sufficiency over the strongest baseline. These results demonstrate that the proposed framework generalizes effectively across diverse datasets and settings."
        },
        {
            "title": "Conclusion",
            "content": "In this study, we present rationale resources and the LLMconsulted X-MuTeST framework, which demonstrates robust classification and explainability performance across three languages, Hindi, Telugu, and English, for the task of hate speech detection. By incorporating combination of traditional classification metrics and explainability measures, the framework not only improved classification accuracy but also provided deeper insights into model behaviour. The results suggest that two-stage training and integration of explainability into multilingual text classification enhances both model trustworthiness and user understanding. Further investigations could involve expanding the framework to handle more languages. Figure 5: Ablation on two stages of training sification performance while improving the explainability. Generalizability To assess the explanation generalizability of the proposed framework, we conducted additional experiments on two benchmark datasets: HateXplain (Mathew et al. 2021) and HateBRXplain (Salles, Vargas, and Benevenuto 2025). On the HateXplain dataset, X-MuTeST achieved an IOU-F1 of 0.314 and Token-F1 of 0.587, surpassing the best reported Acknowledgments We used two datasets, HASOC (2020 and 2021) and HOLD. We would like to acknowledge the contributions of the HASOC and HOLD teams. These datasets were instrumental in conducting this research. References Arshad, M. U.; and Shahzad, W. 2024. Understanding hate speech: the HateInsights dataset and model interpretability. PeerJ Computer Science, 10: e2372. Camburu, O.-M.; Rocktaschel, T.; Lukasiewicz, T.; and Blunsom, P. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31. Chakravarthi, B. R.; Priyadharshini, R.; Thavareesan, S.; Sherly, E.; Nadarajan, R.; Ravikiran, M.; et al. 2024. Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. In Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins, M.; and Toutanova, K. 2019. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Proceedings of NAACL-HLT, 29242936. Clarke, C.; Hall, M.; Mittal, G.; Yu, Y.; Sajeev, S.; Mars, J.; and Chen, M. 2023. Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 364376. Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzman, F.; Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2020. Unsupervised Cross-lingual RepIn Jurafsky, D.; Chai, J.; resentation Learning at Scale. Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 84408451. Online: Association for Computational Linguistics. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 41714186. Minneapolis, Minnesota: Association for Computational Linguistics. DeYoung, J.; Jain, S.; Rajani, N. F.; Lehman, E.; Xiong, C.; Socher, R.; and Wallace, B. C. 2020. ERASER: Benchmark to Evaluate Rationalized NLP Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 44434458. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Geleta, R. R.; Eckelt, K.; Parada-Cabaleiro, E.; and Schedl, M. 2023. Exploring intensities of hate speech on social media: case study on explaining multilingual models with XAI. In Proceedings of the 4th Conference on Language, Data and Knowledge, 532537. Hahn, U.; Beisswanger, E.; Buyko, E.; Faessler, E.; Traumuller, J.; Schroder, S.; and Hornbostel, K. 2012. Iterative Refinement and Quality Checking of Annotation GuidelinesHow to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC12), 38813885. Jacovi, A.; and Goldberg, Y. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 41984205. Kapil, P.; Kumari, G.; Ekbal, A.; Pal, S.; Chatterjee, A.; and Vinutha, B. 2023. HHLD: Hateful posts Identification in Hindi Language leveraging multi task learning. IEEE Access. Kasu, S. K. R.; Rehman, M. Z. U.; Dar, S. S.; Bharat Junghare, R.; Namboodiri, D. S.; and Kumar, N. 2025. D-humor: Dark humor understanding via multimodal open-ended reasoning. arXiv e-prints, arXiv2509. Khanuja, S.; Bansal, D.; Mehtani, S.; Khosla, S.; Dey, A.; Gopalan, B.; Margam, D. K.; Aggarwal, P.; Nagipogu, R. T.; Dave, S.; et al. 2021. Muril: Multilingual representations for indian languages. arXiv preprint arXiv:2103.10730. Khashabi, D.; Chaturvedi, S.; Roth, M.; Upadhyay, S.; and Roth, D. 2018. Looking beyond the surface: challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 252262. Lehman, E.; DeYoung, J.; Barzilay, R.; and Wallace, B. C. 2019. Inferring Which Medical Treatments Work from Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 37053717. Lin, H.; Luo, Z.; Gao, W.; Ma, J.; Wang, B.; and Yang, R. 2024. Towards explainable harmful meme detection through multimodal debate between large language models. In Proceedings of the ACM on Web Conference 2024, 23592370. Madhani, Y.; Parthan, S.; Bedekar, P.; Nc, G.; Khapra, R.; Kunchukuttan, A.; Kumar, P.; and Khapra, M. M. 2023. Aksharantar: Open Indic-language transliteration datasets and models for the next billion users. In Findings of the Association for Computational Linguistics: EMNLP 2023, 4057. Mandl, T.; Modha, S.; Kumar M, A.; and Chakravarthi, B. R. 2020a. Overview of the hasoc track at fire 2020: Hate speech and offensive language identification in tamil, malayalam, hindi, english and german. In Proceedings of the 12th annual meeting of the forum for information retrieval evaluation, 2932. Mandl, T.; Modha, S.; Shahi, G. K.; Jaiswal, A. K.; Nandini, D.; Patel, D.; Majumder, P.; and Schafer, J. 2020b. Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Identification in Indo-European Languages. Sawant, M.; Younus, A.; Caton, S.; and Qureshi, M. A. 2024. Using Explainable AI (XAI) for Identification of Subjectivity in Hate Speech Annotations for Low-Resource Languages. In Proceedings of the 4th International Workshop on Open Challenges in Online Social Networks, 1017. Schmer-Galunder, S.; Wheelock, R.; Jalan, Z.; Chvasta, A.; Friedman, S.; and Saltz, E. 2024. Annotator in the Loop: Case Study of In-Depth Rater Engagement to Create In Proceedings of the Prosocial Benchmark Dataset. AAAI/ACM Conference on AI, Ethics, and Society, volume 7, 13191328. Wang, B.; Ma, J.; Lin, H.; Yang, Z.; Yang, R.; Tian, Y.; and Chang, Y. 2024. Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom. In Proceedings of the ACM on Web Conference 2024, 24522463. Waseem, Z.; and Hovy, D. 2016. Hateful symbols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, 8893. Yadav, S.; Kaushik, A.; and McDaid, K. 2023. Hate speech is not free speech: Explainable machine learning for hate speech detection in code-mixed languages. In 2023 IEEE International Symposium on Technology and Society (ISTAS), 18. IEEE. Yessenalina, A.; Choi, Y.; and Cardie, C. 2010. Automatically generating annotator rationales to improve sentiment classification. In Proceedings of the ACL 2010 Conference Short Papers, 336341. Zaidan, O.; and Eisner, J. 2008. Modeling annotators: generative approach to learning from annotator rationales. In Proceedings of the 2008 conference on Empirical methods in natural language processing, 3140. Zaidan, O.; Eisner, J.; and Piatko, C. 2007. Using annotator rationales to improve machine learning for text categorization. In Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference, 260267. Mathew, B.; Saha, P.; Yimam, S. M.; Biemann, C.; Goyal, P.; and Mukherjee, A. 2021. Hatexplain: benchmark dataset In Proceedings of for explainable hate speech detection. the AAAI conference on artificial intelligence, volume 35, 1486714875. MistralAI. 2024. Large Enough mistral.ai. https://mistral. ai/news/mistral-large-2407/. Modha, S.; Mandl, T.; Shahi, G. K.; Madhu, H.; Satapara, S.; Ranasinghe, T.; and Zampieri, M. 2021. Overview of the hasoc subtrack at fire 2021: Hate speech and offensive content identification in english and indo-aryan languages and conversational hate speech. In Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation, 13. Nguyen, T. H.; and Rudra, K. 2022. Towards an interpretable approach to classify and summarize crisis events from microblogs. In Proceedings of the ACM Web Conference 2022, 36413650. OpenAI. 2024. ChatGPT 4o. url : https://platform.openai. com/docs/models/gpt-4o. Premjith, B.; Chakravarthi, B. R.; Kumaresan, P. K.; Rajiakodi, S.; Karnati, S.; Mangamuru, S.; and Janakiram, C. 2024. Findings of the Shared Task on Hate and Offensive Language Detection in Telugu Codemixed Text (HOLDTelugu)@ DravidianLangTech 2024. In Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, 4955. Priyadharshini, R.; Chakravarthi, B. R.; Malliga, S.; Subalalitha, C.; Kogilavani, S.; Premjith, B.; Murugappan, A.; and Kumaresan, P. K. 2023. Overview of Shared-task on Abusive Comment Detection in Tamil and Telugu. In Proceedings of the Third Workshop on Speech and Language Technologies for Dravidian Languages, 8087. Rajani, N. F.; McCann, B.; Xiong, C.; and Socher, R. 2019. Explain Yourself! Leveraging Language Models for ComIn Proceedings of the 57th Annual monsense Reasoning. Meeting of the Association for Computational Linguistics, 49324942. Rehman, M. Z. U.; Bhatnagar, A.; Kabde, O.; Bansal, S.; and Kumar, N. 2025. ImpliHateVid: Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1720917221. Rehman, M. Z. U.; Mehta, S.; Singh, K.; Kaushik, K.; and Kumar, N. 2023. User-aware multilingual abusive content detection in social media. Information Processing & Management, 60(5): 103450. Roy, S.; Harshvardhan, A.; Mukherjee, A.; and Saha, P. 2023. Probing LLMs for hate speech detection: strengths and vulnerabilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, 61166128. Salles, I.; Vargas, F.; and Benevenuto, F. 2025. HateBRXplain: Benchmark Dataset with Human-Annotated Rationales for Explainable Hate Speech Detection in Brazilian Portuguese. In Proceedings of the 31st International Conference on Computational Linguistics, 66596669."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Indian Institute of Information Technology Dharwad",
        "Indian Institute of Technology Indore",
        "Indian Institute of Technology Mandi"
    ]
}