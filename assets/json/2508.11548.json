{
    "paper_title": "Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends",
    "authors": [
        "Zhenhua Xu",
        "Xubin Yue",
        "Zhebo Wang",
        "Qichen Liu",
        "Xixiang Zhao",
        "Jingxuan Zhang",
        "Wenjun Zeng",
        "Wengpeng Xing",
        "Dezhang Kong",
        "Changting Lin",
        "Meng Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 4 5 1 1 . 8 0 5 2 : r Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends Zhenhua Xu1, Xubin Yue1, Zhebo Wang1, Qichen Liu2, Xixiang Zhao2, Jingxuan Zhang2, Wenjun Zeng2, Wenpeng Xing1, 2, Dezhang Kong1, 2, Changting Lin1, 2, Meng Han1, 2, 1Zhejiang University 2GenTel.io Abstract: Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated contentnamely, text watermarkingwhile systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents comprehensive survey of the current state of LLM copyright protection technologies, with focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property. We will continue to maintain and update curated list of related papers and resources at https://github.com/Xuzhenhua55/awesome-llm-copyright-protection. CCS Concepts: Security and privacy Digital rights management. Additional Key Words and Phrases: large language models,copyright protection, text watermarking,model fingerprinting"
        },
        {
            "title": "1 Introduction\nOver the past decade, deep learning has undergone remarkable evolution, progressing from early\nconvolutional and recurrent neural network architectures [45, 76, 94, 112] to today’s transformative\nlarge language models (LLMs) [75]. These models, underpinned by the advances in transformer\narchitectures and unprecedented access to large-scale training data, have demonstrated an extraor-\ndinary capacity for a wide range of natural language processing (NLP) tasks. Modern LLMs—such\nas GPT-4 [104], Claude [6], Gemini [130], DeepSeek [34, 35], and others—represent not only a\nculmination of research and engineering efforts, but also a shift in the landscape of artificial intelli-\ngence where general-purpose models can perform tasks that traditionally required task-specific\nfine-tuning.",
            "content": "The capabilities of these models extend far beyond conventional text generation. They exhibit remarkable proficiency in areas such as logical reasoning [146, 147], program synthesis [126], multilingual translation [11, 62], scientific question answering [24, 37], document summarization [5, 99], and even interpreting tabular data [14, 31, 58] or understanding structured information such as spreadsheets and charts [70]. Thanks to in-context learning and the integration of external tools Equal contribution. Corresponding author. Emails: xuzhenhua0326@zju.edu.cn,yuexubin@zju.edu.cn,breynald@zju.edu.cn,kdz@zju.edu.cn,wpxing@zju.edu.cn, mhan@zju.edu.cn,xixiangzhao77@gmail.com,linchangting@gmail.com,qichen.liu@alumni.wfu.edu,e1349386@u.nus. edu,jz97@iu.edu. , Vol. 1, No. 1, Article . Publication date: August 2025. 2 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. (e.g., code interpreters, retrieval plugins), LLMs are also increasingly adept at tool manipulation and zero-shot generalization, further solidifying their status as foundational AI platforms. As their capabilities and societal impact continue to expand, so too does the imperative to protect them. Unlike traditional software development pipelineswhich rely primarily on deterministic programming, transparent source management, and relatively lower computational overheadthe creation of modern LLMs entails substantial resource investment, opaque training dynamics, and limited post-hoc traceability. These models are thus not mere scientific outputs but constitute highvalue intellectual property (IP). In this context, ensuring responsible usage and legal ownership becomes critical, especially given the growing concerns around model misuse and unsanctioned redistribution."
        },
        {
            "title": "1.1 Why Do Large Language Models Need Copyright Protection?\nThe need for robust copyright protection stems from the increasing vulnerability of language\nmodels to unauthorized use and the difficulty of attribution once a model leaves the control of the\noriginal creator. Two representative scenarios illustrate the central challenges:",
            "content": "Unauthorized model distribution. In the case of privately held LLMssuch as proprietary models deployed on the cloudthere exists tangible risk of unintentional leakage. These leaks may occur through internal mishandling (e.g., by employees with access to model weights), or via external vectors such as cyberattacks. Once leaked, adversaries may redistribute or monetize the models without the original developers consent, leading to severe intellectual property and security concerns.1 Violation of open-source license agreements. For models released under open-source licenses, such as Creative Commons [29] or Apache 2.0 [7], usage often comes with specific terms and restrictions. For instance, model may be licensed strictly for non-commercial use or require attribution to the original authors. Nonetheless, it is not uncommon for third-party actors to make minimal algorithmic changes to the released models and then redistribute them, potentially for commercial use, thereby violating licensing terms and undermining the original creators intentions.2 Without effective mechanisms to identify, attribute, and trace model ownership, developers lack meaningful recourse in the face of infringement. As the generative AI ecosystem matures, copyright protection for LLMs is not merely legal or ethical concern, but foundational requirement for preserving incentives, ensuring accountability, and supporting long-term innovation sustainability."
        },
        {
            "title": "1.2 From LLM Watermarking to Model Fingerprinting\nWatermarking, in its classical form, refers to the practice of embedding identifiable patterns into\nphysical objects or media to assert ownership, verify authenticity, or deter forgery. Examples\ninclude the intricate designs in banknotes visible under light, embossed seals on official certificates,\nor an artist’s unique signature on a painting. These visible or hidden marks ensure traceability and\nsafeguard provenance.",
            "content": "1A notable example occurred in January 2024, when an anonymous user uploaded large-scale model to HuggingFace (https://huggingface.co/miqudev/miqu-1-70b). The leaked model, later confirmed by Mistral CEO Arthur Mensch to be an internal model provided under early-access, had been inadvertently made public by an enterprise partners employee. See: https://twitter.com/arthurmensch/status/1752737462663684344. 2A representative case occurred in 2024 when the Llama3-V team released model derived from MiniCPM-Llama3-V 2.5 without proper attribution. After public scrutiny, the authors acknowledged the violation and withdrew the model, demonstrating how even academic projects may inadvertently (or intentionally) bypass licensing requirements. See discussions at https://github.com/OpenBMB/MiniCPM-o/issues/196. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 3 In the digital realm, watermarking has become foundational technique for protecting intellectual property. With the emergence of LLMs, watermarking approaches have adapted accordingly. As described in [89], LLM watermarking broadly refers to any technique that embeds verifiable information into LLMs or their outputs to support copyright attribution and traceability. These techniques are generally grouped into two categories: text watermarking and model watermarking. Text watermarking embeds statistical or semantic signals into an LLMs generated content. The goal is to allow content verification without altering semantics or fluency, often using perturbation to token probabilities [69], sampling constraints [25] or neural rewriting [1]. Such signals are typically imperceptible to end users but detectable through specialized algorithms. This approach enables model owners to trace content distribution, enforce proper use, and support regulatory compliance. Model watermarking, in contrast, focuses on protecting the model artifact itself by embedding identifiable patterns that can be later extracted or verified. This can be achieved through various mechanisms, such as inserting functional triggers (i.e., backdoor watermarking [83]) or encoding information into weight distributions [137]. In principle, model watermarking supports the attribution of proprietary models, and helps detect unauthorized replication or redistribution, especially in scenarios involving fine-tuning from protected source. However, the distinction between text watermarking and model watermarking can be misleading. Not all methods that embed watermarks into model should be classified as model watermarking. Several approaches [47, 151] inject signals into model parameters at training time, yet their primary goal is to trace generated content. Despite operating on the model, these methods align more closely with text watermarking in terms of intent and evaluation3. Further blurring the boundaries in this taxonomy, recent backdoor-based model watermarking approaches [19, 120, 148, 157, 170, 171]which embed functional triggers for ownership verificationare increasingly characterized in the literature as instances of model fingerprinting. Historically, however, the term model fingerprinting was used to denote exclusively non-invasive techniques, such as output-based identification [116], feature-space analysis [167], or leveraging adversarial examples near the decision boundary [20]. To reconcile these evolving trends, we adopt the term model fingerprinting as unifying label. It encompasses both conventional, non-invasive fingerprinting methodsreferred to in this work as intrinsic fingerprintingand model watermarking techniques that aim to attribute ownership of the model itself, which we refer to as invasive fingerprinting. For clarity and compatibility with prior literature, we adopt hybrid terms such as backdoor watermark as fingerprint to reflect both the methodological origin and prevailing terminology in current research4."
        },
        {
            "title": "1.3 Why a Survey for Model Fingerprinting in the Era of LLMs?\nWatermarking and fingerprinting have been long-standing topics in digital content protection.\nEarly surveys, such as Alkawaz and Salim [4] and Kamaruddin et al. [65], focused on traditional\ntext watermarking via syntactic, lexical, or formatting transformations, but lacked consideration of\ndeep learning or neural models.",
            "content": "In the deep learning era, Boenisch et al. [17] systematically reviewed watermarking techniques for neural networks, though primarily in the image domain. Similarly, Lederer et al. [77] proposed 3In this survey, text watermarking refers to all methods whose ultimate goal is to trace or verify generated content, including those that embed watermark signals during model training [47, 151]. 4In this survey, model fingerprinting denotes methods for verifying models identity or provenance. This includes both non-invasive fingerprinting schemes and invasive model watermarking techniques, in accordance with evolving usage across the literature. , Vol. 1, No. 1, Article . Publication date: August 2025. 4 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Table 1. Overview of representative surveys that cover text watermarking or model fingerprinting in the context of language models. For clarity, only works that involve the textual domain are included."
        },
        {
            "title": "Year LLM Text Watermarking",
            "content": "Alkawaz and Salim [4] Kamaruddin et al. [65] Liu et al. [89] Liang et al. [88] Zhang et al. [176] Lalai et al. [75] Wang et al. [140] Yang et al. [162] This Survey 2017 2018 2024 2024 2024 2025 2025 2025"
        },
        {
            "title": "Intrinsic Transfer Removal Metrics",
            "content": "Legend: = comprehensively covered; = partially discussed; = not covered. \"LLM\" indicates whether the survey focuses on large language models. \"Invasive / Intrinsic / Transfer / Removal / Metrics\" refers to coverage of key aspects of model fingerprinting. unified taxonomy of watermarking and fingerprinting methods, again from computer vision perspective. More recently, Hwang and Song [59] examined global regulatory trends and industry adoption of generative AI watermarking, but offered limited technical analysis. With the emergence of LLMs, Liu et al. [89] surveyed text watermarking techniques for generated content, yet their scope is restricted to output-level tracing without addressing model-level ownership. Similar limitations appear in Yang et al. [162], Lalai et al. [75], and Wang et al. [140], which focus on text watermarking and discuss model protection only in the narrow form of backdoor-based methods. Zhang et al. [176] represent an initial attempt to distinguish between text watermarking and model fingerprinting. However, their discussion is limited to weight watermarking and excludes other important strategies, such as backdoor-based and intrinsic fingerprinting methods. Liang et al. [88] expand this line of inquiry by exploring broader range of invasive fingerprinting techniques. Despite the growing research interest in text watermarking for tracing LLM-generated content, the complementary challenge of attributing the model itself remains underexplored. Existing surveys on model fingerprinting are scarce, often limited to the vision domain. Notably, intrinsic fingerprinting techniques have received little attention, and coverage of invasive approaches remains fragmented. Moreover, to the best of our knowledge, no prior work has systematically investigated critical aspects such as fingerprint transferability, removability, or defined standardized experimental metrics for evaluating fingerprinting methods. Given the increasing importance of LLM intellectual property protection, systematic investigation into model fingerprinting is both timely and necessary. This survey fills this gap by: (1) clarifying the conceptual distinction between text watermarking and model fingerprinting; (2) organizing diverse fingerprinting approaches into coherent and extensible taxonomy; (3) analyzing under-explored challenges such as fingerprint transferability and removal; and (4) proposing standardized set of evaluation metrics. Organization: This survey is structured as follows. Section 2 introduces the fundamental concepts of model fingerprinting, including formal definitions and key algorithmic characteristics. Section 3 presents an overview of text watermarking techniques and analyzes why such methods are insufficient for model-level copyright attribution. Section 4 reviews intrinsic model fingerprinting , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 5 approaches, which exploit the inherent capabilities and behaviors of model to derive identity signatures. Section 5 discusses invasive fingerprinting techniques that require explicit modification of model weights during the embedding process. Section 6 explores the transferability of fingerprints. Section 7 examines current techniques for fingerprint detection and removal. Section 8 introduces set of evaluation metrics for systematically assessing model fingerprinting methods. Section 9 outlines open challenges and identifies promising directions for future research. Finally, Section 10 concludes the survey."
        },
        {
            "title": "2.1 Large Language Models\nWe begin by formalizing a LLM as a neural probabilistic model M𝜃 , parameterized by 𝜃 , which\nassigns likelihoods to sequences of discrete tokens 𝒙 = (𝑥 1, . . . , 𝑥𝑛). These models typically rely on\nan autoregressive factorization, where the joint probability is decomposed as 𝑝𝜃 (𝒙) = (cid:206)𝑛\n𝑖=1 𝑝𝜃 (𝑥𝑖 |\n𝒙 <𝑖 ), with 𝒙 <𝑖 = (𝑥 1, . . . , 𝑥𝑖 −1) denoting the prefix context at position 𝑖.",
            "content": "At each step, the model consumes the context 𝒙 <𝑖 , maps each token 𝑥 𝑗 within it to continuous embedding 𝒆 𝑗 R𝑑 , and processes the resulting sequence through stack of neural layersmost commonly Transformer blocks [138]. This yields hidden representation 𝒉𝑖 = F𝜃 (𝒙 <𝑖 ), where F𝜃 denotes the composition of Transformer layers. The model then transforms the hidden state 𝒉𝑖 into distribution over the vocabulary via linear projection followed by softmax operation. Formally, the conditional probability of the next token is given by: <𝑖 ) = Softmax(𝑾𝒉 where 𝑾 𝑑 and 𝒃 are learnable output projection parameters, and denotes the vocabulary set. This operation produces categorical distribution over all tokens in V, from which the next token 𝑥𝑖 is typically sampled or selected via greedy decoding. 𝑝𝜃 (𝑥𝑖 𝒙 𝑖 + 𝒃),"
        },
        {
            "title": "2.2 Model Fingerprinting Algorithms\nWe define a model fingerprint, denoted by 𝒇 , as a distinctive and verifiable signature that can\nbe associated with a model M𝜃 . Depending on whether the fingerprint is embedded via direct\nmodification of 𝜃 , fingerprinting algorithms can be broadly categorized into intrinsic (non-invasive)\nand invasive approaches.",
            "content": "Intrinsic fingerprinting operates under the assumption that trained model inherently encodes identity-related information, even without any explicit modification. In this setting, the fingerprint is extracted as 𝒇 = Fintrinsic(M𝜃 ), where Fintrinsic() denotes fingerprinting function that leverages the internal properties of the model. The main difference across intrinsic fingerprinting methods lies in how this fingerprint is derivedeither by encoding the models parameters [167] or hidden representations [169], by aggregating its output behavior on predefined probe set [116], or by designing adversarial inputs [49] that elicit uniquely identifiable responses. In contrast, invasive fingerprinting involves explicitly modifying the model to embed an externally defined fingerprint. This process typically consists of two stages: an embedding phase, where fingerprint payload 𝒇 is injected into the model via an embedding function (𝒇 ) 𝜃 = Fembed (M𝜃, 𝒇 ); and an extraction phase, where the fingerprint is later retrieved from the modified model using decoding function, i.e., ˆ𝒇 = Fextract(M (𝒇 ) ). Variations across invasive methods arise from both the encoding schemesuch as injecting fingerprint bits into the parameter space [175], or embedding functional backdoors within the model [19, 83, 120, 144, 148, 157]and the decoding strategy, which 𝜃 , Vol. 1, No. 1, Article . Publication date: August 2025. 6 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Text Watermarking Watermarking for Existing Text (section 3.1) LLMs for Text Watermarking (section 3.2) Format-Based (section 3.1.1) Synonym-Based (section 3.1.2) Syntactic-Based (section 3.1.3) Neural Rewriting-Based (section 3.1.4) Logit-Based (section 3.2.1) Sampling-Based (section 3.2.2) Learning-Based (section 3.2.3) Brassil et al. [18], UniSpaCh (Por et al. [111]), Rizzo et al. [117], EasyMark (Sato et al. [122]) Topkara et al. [134], DeepTextMark (Munyer and Zhong [100]), Yang et al. [161], Yoo et al. [163] Atallah et al. [9], Topkara et al. [133], Meral et al. [95] AWT (Abdelnabi and Fritz [1]), REMARK-LLM (Zhang et al. [174]) Kirchenbauer et al. [69], Takezawa et al. [128], Wang et al. [141], Nemecek et al. [102], Fu et al. [40], Ren et al. [115], Liu and Bu [92] Token-Level: Christ et al. [25], Kuditipudi et al. [73], Xu and Sheng [153], Yang et al. [160] Sentence-Level: Hou et al. [54], Hou et al. [55], Zhang et al. [172] Gu et al. [47], Xu et al. [151], Gloaguen et al. [43] Fig. 1. Taxonomy of text watermarking methods, including watermarking for existing text and LLMs for text watermarking. may rely on reading specific weights, observing triggered responses to secret inputs, or estimating gradient-based artifacts."
        },
        {
            "title": "2.3 Key Characteristics of Model Fingerprinting Algorithms\nTo systematically understand and evaluate model fingerprinting algorithms, we highlight five core\ncharacteristics that determine their effectiveness and practical utility.",
            "content": "Effectiveness. The fingerprint 𝒇 should be reliably extractable and verifiable, enabling consistent attribution of the model through its outputs, internal states, or parameters. Harmlessness. Fingerprinting should not significantly impair the models original performance. The model should retain its general-purpose capabilities after fingerprinting. Robustness. robust fingerprint is resilient to both model-level changes (e.g., fine-tuning, pruning, model merging) and interaction-level manipulations (e.g., input perturbations, decoding changes), remaining intact under such transformations. Stealthiness. The fingerprint should be difficult to detect or isolate, preventing unauthorized parties from identifying, removing or suppress it without access to proprietary knowledge. Reliability. Fingerprints should uniquely correspond to their source models. Unrelated models should not produce similar signatures, and for interaction-triggered schemes, the fingerprint should remain latent under benign usage and only activate upon specific triggers. These properties serve as guiding principles for fingerprint design and form the basis for comparisons across different algorithms, as discussed in subsequent sections."
        },
        {
            "title": "2.4 Taxonomy of Model Fingerprinting Algorithms\nTo facilitate the systematic review presented in Sections 4 and 5, this section introduces a taxonomy\nthat categorizes existing model fingerprinting algorithms into two major types, as summarized in\nFigure 5 and 6. The first category, intrinsic fingerprinting, leverages the inherent characteristics\nof a model M𝜃 to derive fingerprint information. As discussed in Section 4, such fingerprints can\nbe extracted from various properties of the model, including its weight parameters and activation\nrepresentations (§ 4.1), output semantics (§ 4.2), or model-specific reactions to adversarially designed\ninputs (§ 4.3). The second category, invasive fingerprinting, involves explicitly modifying the model to\nembed externally defined ownership information. These modifications—detailed in Section 5—may",
            "content": ", Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 7 Fig. 2. An overview of text watermarking techniques include embedding fingerprint payloads directly into the models weights ( 5.1) or utilizing backdoor-style watermarking schemes ( 5.2) as fingerprinting mechanism. Figure 4 provides more fine-grained taxonomy, covering representative techniques within each category and illustrating the diverse design choices found in the literature."
        },
        {
            "title": "3.1 Watermarking for Existing Text\nWatermarking for existing text involves modifying a human- or model-generated text post hoc in\norder to embed a watermark, without requiring access to the original generation process.",
            "content": "Format-Based Watermarking. Early format-based watermarking methods focus on manipu3.1.1 lating the spatial layout of text. For example, Brassil et al. [18] proposed technique that encodes information by slightly shifting the vertical or horizontal positions of lines and wordsknown as line-shift and word-shift coding. Detection involves measuring these spatial offsets. However, this approach is only applicable to image-formatted text (e.g., scanned documents or PDFs) and does not actually alter or embed information within the text content itself. Beyond layout manipulation, more prevalent class of methods exploits visually indistinguishable Unicode variants to embed information at the character level. These methods insert invisible tokens or replace standard symbols with perceptually similar alternatives that share glyph representations. For instance, UniSpaCh[111] embeds watermarks by replacing standard space characters with visually identical Unicode whitespaces. Similarly, Rizzo et al.[117] proposed homoglyph , Vol. 1, No. 1, Article . Publication date: August 2025. 8 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. substitution approach that replaces Latin characters with confusable Unicode lookalikesfor example, replacing the standard uppercase letter (U+0043) with the Unicode character \"U+216D\", or replacing (U+004C) with \"U+216C\". Sato et al. [122] developed EasyMark, which includes three distinct strategies: WhiteMark, which replaces normal space characters with Unicode spacing variants (e.g., U+2004 for U+0020); VariantMark, which leverages Unicode ideograph variants particularly for CJK (Chinese, Japanese, Korean) texts; and PrintMark, which encodes information through ligature manipulations that slightly alter rendering appearances without changing text semantics. Despite their effectiveness and minimal perceptual impactoften leaving the text visually identical to the originalformat-based watermarking approaches are inherently fragile. They are highly sensitive to standard normalization or reformatting operations (e.g., re-paragraphing or font rendering), and the embedded signals are relatively easy to spoof or remove due to their predictability and lack of semantic linkage. Synonym-Based Watermarking. Synonym-based watermarking methods typically embed in3.1.2 formation by replacing original words with semantically similar alternatives. This process generally involves generating synonym candidates based on the context, filtering them using task-specific linguistic or semantic criteria, and selecting the final substitution according to secret key or embedding objective. Early work by Topkara et al. [134] presented robust synonym-based watermarking scheme that prioritizes ambiguous words for substitution to increase resilience against attacks. secret key is used to determine both substitution positions and synonym choices. At the decoding stage, semantic graph is reconstructed to recover the embedded bitsencoded as \"colors\" indicating 0 or 1without requiring access to the original text. More recent techniques aim to improve semantic preservation. DeepTextMark [100], for example, first encodes candidate synonyms using Word2Vec [98] and evaluates substitute sentences using universal sentence encoder [22]. The sentence with the highest embedding similarity to the original is retained as the watermarked version. The watermark can later be detected using transformer-based classifier over sentence embeddings. However, many existing approaches overlook the broader linguistic context of word, potentially resulting in semantic drift or degraded fluency. To address this, context-aware methods have emerged. For instance, Yang et al. [161] proposed watermarking scheme based on statistical bias in encoded word distributions. Each synonym candidate is assigned binary code, and substitutions are chosen such that words encoding bit-1 dominate in the final text. Statistical tests can then confirm the presence of watermark signal by detecting skewed bit distributions. To further improve robustness, Yoo et al. [163] fine-tuned BERT-based model [36] to identify semantically and syntactically \"stable\" word positions. These positions are masked and then reconstructed using an infill language model to insert watermark symbols. During extraction, the same stable positions are recovered, and the embedded bits are inferred based on the replacements. This approach supports robust multi-bit watermarking. Despite improved embedding quality and robustness, synonym-based watermarking remains vulnerable to certain types of attacks, such as random synonym substitution or adversarial rewriting, which can distort or erase the watermark without significantly affecting text meaning. Syntactic-Based Watermarking. While synonym-level watermarking is susceptible to simple 3.1.3 substitutions, syntactic-based watermarking improves robustness by manipulating the grammatical structure of text. These methods embed information through controlled syntactic transformations that preserve semantics while altering sentence form. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends Atallah et al. [9] introduced three such transformations: adjunct movement (e.g., \"She quickly finished her homework\" vs. \"Quickly, she finished her homework\"), clefting (e.g., \"The chef cooked great meal\" vs. \"It was the chef who cooked great meal\"), and passivization (e.g., \"The teacher graded the exam\" vs. \"The exam was graded by the teacher\"). Each transformation encodes bit of information and remains reversible via syntactic parse tree comparisons. Extending this idea, Topkara et al. [133] proposed additional operations such as activation and topicalization to enlarge the code space. Meral et al. [95] further adapted syntactic watermarking to morphologically rich languages like Turkish, identifying over 20 grammar-informed transformation templates usable for watermark embedding. Although syntactic-based methods offer better resilience than lexical techniques, they are often language-specific and rely on hand-crafted rules. Moreover, repeated or forced transformations may degrade fluency and reduce the watermarks imperceptibility."
        },
        {
            "title": "3.1.4 Neural Rewriting-Based Watermarking. Format-based, synonym-based, and syntactic-based\nwatermarking techniques (Sections 3.1.1–3.1.3) are relatively straightforward to implement. How-\never, they often rely on fixed and easily discoverable transformation rules, making them more\nvulnerable to detection or reversal. Additionally, such rule-based modifications frequently result in\nreduced textual fluency or content distortion.",
            "content": "In contrast, neural rewriting-based watermarking aims to address these limitations by leveraging neural model to automatically rewrite the original text while embedding watermark information in less conspicuous and more semantically coherent manner. These approaches typically involve two jointly trained modules: message encoder that rewrites the input text to embed watermark message, and message decoder that reconstructs the embedded watermark from the rewritten text. The goal is to preserve the semantics and surface quality of the original text while ensuring accurate watermark extraction. For example, Abdelnabi and Fritz [1] proposed AWT, which uses adversarial training and smooth auxiliary losses to embed fixed-length watermark codes in English text imperceptibly. The model learns to substitute inconspicuous elements such as prepositions, conjunctions, and punctuation tokens to encode messages. These tokens are chosen because they minimally affect meaning, and the embedded watermark remains recoverable even if some words in the text are modified, as long as the key functional tokens are preserved. However, the watermarking capacity in this method is relatively limited. To address this capacity bottleneck, Zhang et al. [174] proposed REMARK-LLM, neural rewriting framework applicable to arbitrary existing text. REMARK-LLM introduces three-stage architecture: message encoding, reparameterization, and message decoding. First, sequence-to-sequence (Seq2Seq) model [127] encodes the original message into the target text through rewriting. Then, the framework employs Gumbel-Softmax [61] reparameterization to convert the continuous watermarked distribution into discrete token sequence with minimal loss in coherence. Finally, mapping network followed by transformer-based decoder is used to recover the embedded message from the reparameterized token embeddings. Furthermore, [150] proposes novel framework for embedding multi-bit watermarks through text paraphrasing. Its core lies in jointly fine-tuning LLM-based paraphrase encoder with trained language model decoder. The encoder is fine-tuned using PPO reinforcement learning, with the decoder serving as the reward model to optimize watermark detectability. The decoder employs standard classification loss to distinguish bit information. These two components are updated alternately, forming closed-loop optimization. This method breaks through the limitations of traditional synonym substitution. By leveraging the larger action space afforded by paraphrasing , Vol. 1, No. 1, Article . Publication date: August 2025. Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. operations, it achieves highly robust watermark embedding while strictly preserving text semantics and quality. Neural rewriting-based watermarking offers higher capacity and improved quality preservation compared to traditional rule-based watermarking, and serves as promising direction for watermarking in contexts where fluency and robustness are critical."
        },
        {
            "title": "3.2 LLMs for Text Watermarking",
            "content": "Logit-Based Watermarking. Logits represent the internal scores assigned by LLMs to potential 3.2.1 next tokens, based on both internal representations and input sequences. These scores determine the probability distribution from which the next token is sampled during generation. Watermarking methods based on logits operate by intentionally biasing or perturbing these values, thereby steering the models generation behavior to exhibit specific preferences and enabling the embedding of desired fingerprinting signals. Kirchenbauer et al. [69] introduced the well-known KGW method, which employs hash functiontaking the previously generated token and random seed as inputsto deterministically partition the vocabulary into green list (G) and red list (R). The method then adjusts the logits by boosting the values corresponding to tokens in the green list, thereby increasing their sampling probability. Formally, given the original logits vector lo and the watermark-adjusted logits vector lw, the modification is expressed as: lw = lo + 𝛿 I[𝑡 𝑗 G] = (cid:40)𝑙0 + 𝛿, 𝑙0, 𝑡 𝑗 𝑡 𝑗 (1) As the equation 1 shows, the model becomes more likely to select tokens from the green list, thus embedding higher frequency of watermark-indicative tokens. This approach is efficient and deployment-friendly since it does not require changes to the model parameters. However, manipulating the token distribution in this manner may cause semantic drift, resulting in outputs that deviate from the original intended meaning. To address this issue, subsequent works have focused on reducing the semantic distortion between the watermarked text 𝑇 𝑁 and the original reference text 𝑆 𝑁 . Takezawa et al. [128]. proposed applying minimal constraints on the logit modifications, adjusting them in accordance with the length of the original text 𝑆 𝑁 , to yield more natural outputs. Wang et al. [141] formalized the tradeoff between watermark effectiveness and text quality as multi-objective optimization problem, introducing an adaptive watermark strength control based on the cumulative probability of the green list to enhance overall performance. Nemecek et al. [102] further refined the method by adding slight biases to topic-relevant tokens within the green list, making the watermark signal more seamlessly integrated. Fu et al. [40] and Ren et al. [115] incorporated semantic similarity constraints when partitioning the vocabulary, thereby preserving semantic fidelity even under paraphrasing attacks and improving robustness. Liu and Bu [92] abandons the use of fixed green/red lists generated by random keys, which may be vulnerable to decryption and forgery. Instead, it employs an auxiliary model to adaptively watermark the distribution of tokens with high entropy measures while leaving the distribution of low-entropy tokens unaltered. Sampling-Based Watermarking. Token-Sampling-Based Watermarking Methods primarily 3.2.2 embed watermarks by utilizing watermark information to guide the sampling strategy for each token. Although token selection involves randomness, this randomness is controllable. During watermark embedding, the watermark guides the sampling process; during extraction, the watermark is detected by assessing the match between the selected token sequence and preset sampling sequence. Based on the granularity of guidance, these techniques can be categorized into , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends two main approaches: Token-Level Sampling Watermarking (embedding the watermark during the sampling process of each token) and Sentence-Level Sampling Watermarking (using watermark information to guide the sampling of entire sentences). Token-Level Sampling Watermarking: The watermark technique proposed by Christ et al. [25] utilizes Pseudo-Random Function (PRF) to control the watermark embedding timing. When generating each subsequent token, the model maintains standard output if the PRF output is below predefined threshold; otherwise, it deliberately selects non-preferred token to carry the watermark. The determinism of the PRF ensures that watermarked outputs are perfectly reproducible for identical prompts, but this severely limits the diversity of generated text. Kuditipudi et al. [73] aims to enhance the diversity of watermarked text by introducing random watermark key to generate very long pseudo-random sequence and mapping it to influence the sampling stage. The match between the generated sequence and the target sequence is quantified using the Levenshtein distance [166]. limitation of this method is that its intervention strategy at the individual token level can negatively impact text quality. Xu and Sheng [153] employs predefined periodic sinusoidal signal patterns (e.g., sin(x), sin(2x), etc.). During text generation, it selects tokens from sorted candidate pool based on the current signal value to embed the watermark. This process ensures the watermark remains imperceptible to humans while preserving text quality. Yang et al. [160] dynamically adjusts the watermark embedding strategy by perceiving the contextual generation state during LLM text generation, thereby maintaining detection rates while mitigating negative impacts on generation quality. Sentence-Level Sampling Watermarking: The SemStamp method [54] pioneered the idea of partitioning the semantic embedding space of sentences into watermark and non-watermark regions, embedding watermarks by employing sentence-level rejection sampling to ensure generated sentences fall within designated regions. However, its reliance on Locality-Sensitive Hashing (LSH) for random partitioning of the semantic space can place semantically similar sentences into different regions, compromising robustness. To address this limitation, Hou et al. [55] proposed k-SemStamp, which clusters the semantic space using k-means, assigning semantically similar sentences to the same cluster and designating specific clusters as watermark regions, thereby enhancing robustness against semantic-preserving edits. Building upon this, the CoheMark method proposed by Zhang et al. [172] further improves semantic coherence and text quality. CoheMark employs Fuzzy C-Means clustering for soft partitioning of the semantic space, offering greater flexibility than hard clustering in handling sentences belonging to multiple topics. Unlike the fixed region selection in k-SemStamp, CoheMark guides the sampling of the next sentence based on the relevance of the preceding sentence to each semantic cluster. This approach maintains contextual coherence and topic consistency while embedding the watermark, alleviating the semantic discontinuity often caused by random sampling in traditional methods. By incorporating inter-sentence semantic coherence evaluation, CoheMark achieves enhanced text naturalness and higher watermark robustness."
        },
        {
            "title": "3.2.3 Learning-Based Watermarking. Logit-based and sampling-based watermarking methods\nembed identifiable signals into text during inference by adjusting token probabilities or sampling\ndecisions. These approaches are generally effective when the model owner fully controls the\ninference process—such as in proprietary API servers or closed environments. However, in scenarios\nwhere the model is openly released or distributed to downstream users (e.g., via checkpoints or\nopen-source platforms5), there is no guarantee that watermarking logic will be preserved or applied\nduring inference.",
            "content": "To address this limitation, learning-based watermarking methods aim to endow the model with the ability to implicitly generate watermarked text without requiring explicit runtime interventions. 5https://huggingface.co/ is currently one of the most widely used platforms for hosting and distributing open-source LLMs. , Vol. 1, No. 1, Article . Publication date: August 2025. 12 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Fig. 3. Pipeline of learning-based watermarking That is, the watermarking behavior is transferred into the models weights during training, enabling watermark generation inherently through the models learned generation process. representative strategy is to distill the watermarking behavior from watermarked teacher model into student model.6 Xu et al. [151] propose two such distillation methods: logit-based distillation and sampling-based distillation. In logit-based distillation, both the teacher and student are given the same input sequence, and the student is trained to match the teachers output distribution by minimizing the KL divergence between their next-token probabilities. This approach aims to align the student models token-level predictive behavior with that of the teacher under watermarking constraints. In contrast, sampling-based distillation operates in the data space: the watermarked teacher generates text samples with watermark using specific decoding strategy, and these watermarked outputs are then used as training targets. The student is fine-tuned using conventional cross-entropy objective to reproduce the text content, thereby implicitly learning the style and statistical patterns associated with watermarked generation. In addition, Xu et al. [151] propose joint training framework that directly embeds watermarking capability into the models parameters through reinforcement learning. The method first trains detector modelfunctioning as reward modelthat assesses whether given text carries watermark. This detector then provides feedback to the LLM finetuning via Proximal Policy Optimization (PPO) [106], encouraging it to generate text recognizable as watermarked. Crucially, the detector is co-evolved with the language model: it is periodically updated using newly generated samples to remain effective as the models output distribution shifts. Through this iterative cotraining process, watermark generation becomes an inherent behavior of the model, eliminating the need for explicit interventions during inference and thus enabling zero-cost, model-level watermarking."
        },
        {
            "title": "3.3 Limitations of Text Watermarking as a Fingerprinting Method\nBefore formally introducing model fingerprinting algorithms, it is important to ask: Can existing\ntext watermarking methods serve as a reliable basis for tracing a model itself? The answer largely\ndepends on the type of watermarking mechanism employed.",
            "content": "Traditional watermarking strategies such as format-based, synonym-based, syntactic-based, and neural rewriting-based watermarking operate independently of the model. They modify or rewrite already generated text post hoc and are thus fundamentally decoupled from the model parameters. 6Here, the watermarked teacher model refers to model capable of producing watermarked outputs, typically via logit-based or sampling-based inference-time watermarking strategies discussed previously. The student model is the target model to which the watermarking behavior is transferred, with the goal of embedding such capability inherently in its learned parameters. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 13 Parameter and Representation (section 4.1) Semantic Feature Extraction (section 4.2) Adversarial Example-Based (section 4.3) Weight-based Watermarking (section 5.1) Backdoor-based Watermarking (section 5.2) FP-VEC (Xu et al. [155]) Parameter-based: HuRef (Zeng et al. [167]), REEF (Zhang et al. [169]), Yoon et al. [164] Representation-based: DEEPJUDGE (Chen et al. [23]), zkLLM (Sun et al. [125]), TensorGuard (Wu et al. [145]), Riemannian fingerprinting (Song and Itti [124]), Alhazbi et al. [3] Liu et al. [90], Llmmap (Pasquini et al. [108]), DuFFin (Yan et al. [158]), Bhardwaj and Mishra [12], Bitton et al. [15], CoTSRF (Ren et al. [116]), Dasgupta et al. [30] draw TRAP (Gubri et al. [49]), ProFLingo (Jin et al. [63]), RAP-SM (Xu et al. [154]), RoFL (Tsai et al. [135]), FIT-Print (Shao et al. [123]) EmMark (Zhang and Koushanfar [175]), Invariant-based Watermarking (Guo et al. [51]), Structural Weight Watermarking with ECC (Block et al. [16]), Functional Invariants (Fernandez et al. [39]) IF (Xu et al. [148]), UTF (Cai et al. [19], MergePrint(Yamabe et al. [157]), ImF (Wu et al. [144]), Nasery et al. [101], Chain&Hash (Russinovich and Salem [120]), Double-I (Li et al. [83]), PLMmark (Li et al. [81]) Training-time Removal MEraser (Zhang et al. [173]) Inference-time Removal Carlini et al. [21], Token Forcing (TF) (Hoscilowicz et al. [53]), Generation Revision Intervention (GRI) (Zhang et al. [170]), Intrinsic Fingerprinting (section 4) Invasive Fingerprinting (section 5) Fingerprint Transfer (section 6) Fingerprint Removal (section 7) LLM Model Fingerprinting Fig. 4. Taxonomy of model fingerprinting methods. In addition to intrinsic and invasive fingerprinting, this taxonomy includes fingerprint transferability and removal, covering dynamic scenarios across the model lifecycle. As result, they cannot provide any attribution or traceability information if the model is used without the associated watermarking pipeline. LLM-based watermarking methods, such as logit-based and sampling-based watermarking, apply modifications during the inference stage to bias the output for detectability. However, these methods rely on runtime watermark injection, meaning their effectiveness depends on whether the adversary retains the watermarking logic. As explained in Section 3.2.3, if the adversary gains access to the model weights but bypasses the decoding interface, the watermark will be absent. Only learning-based watermarking methodswhere watermarking behavior is embedded directly into the model weights via distillation or joint optimizationcan be meaningfully considered form of model fingerprinting. In this setting, any model derived from the watermarked model will inherently continue to generate watermarked text, regardless of the decoding strategy. The presence of consistent watermark signal can thus serve as evidence for ownership or source attribution. However, it is important to note that current learning-based watermarking techniques, while promising, are originally designed as content-level watermarking schemes prioritizing imperceptibility and capacity (e.g., low impact on text quality and high bit rate). They are not optimized with the full set of fingerprinting requirements described in Section 2.3. For instance, both Xu et al. [151] and Xu et al. [152] observed that distilled watermarking signals degrade quickly under continued training (e.g., model fine-tuning), posing significant challenges to robustness. Additionally, these methods often introduce noticeable trade-offs in model performance, violating the harmlessness criterion. To partially address this, Gloaguen et al. [43] extended the learning-based approach by constraining watermark generation to specific domain, thereby mitigating the impact on general-purpose capabilities. Nevertheless, robustness remains central weakness, and therefore, these methodswhile conceptually aligned with fingerprintingfall short of satisfying all practical requirements. , Vol. 1, No. 1, Article . Publication date: August 2025. 14 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Fig. 5. Pipeline of different intrinsic fingerprinting methods These limitations underscore the need for dedicated model fingerprinting methods that are explicitly designed for attribution, resilient to downstream modification, and do not compromise model utility. In the following sections, we shift our focus to fingerprinting algorithms that are purpose-built for model-level traceability."
        },
        {
            "title": "4.1 Parameter and Representation as Fingerprint\nParameter and representation-level approaches primarily identify and verify the ownership of\nLLMs by analyzing the internal parameter distribution or intermediate hidden representations\nof the models. The central idea of these methods is to capture structural features learned during\ntraining—such as parameters, hidden layer representations, or feature mappings—and treat them as\nunique “fingerprints” to distinguish the origins of different models. This field has evolved through\nthree progressive stages: behavioral analysis, geometric modeling, and anti-removal enhancement.\nThe pioneering DEEPJUDGE framework (Chen et al. [23]) established a functional-level similarity\nanalysis paradigm. By comparing neuron activations, layer-wise differences, and output distri-\nbutions through metrics like Jensen-Shannon divergence, combined with adversarial robustness\ntesting, it effectively detects model copying. Its non-intrusive design supports both white-box and\nblack-box scenarios while demonstrating robustness against model transformations like fine-tuning\nand pruning.",
            "content": "Building on this foundation, HuRef (Zeng et al. [167]) introduced interpretable fingerprint generation. Leveraging the convergence stability of parameter directions in pretrained models, the approach maps directional vectors to Gaussian distributions, transforms them into images via StyleGAN2 [67], and validates fingerprints through zero-knowledge proofs. This human-readable conversion mechanism breaks through limitations of traditional parameter comparison methods. REEF (Zhang et al. [169]) advanced the field through representational space alignment analysis. Its Centered Kernel Alignment (CKA)[72]-based framework evaluates cross-model similarity by comparing representation spaces under identical inputs, demonstrating strong adaptability to , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 15 transformations like pruning and layer reordering. Particularly suitable for open-source LLM IP protection, REEF marks paradigm shift from parameter analysis to functional equivalence determination. Recent breakthroughs emerge across three dimensions: TensorGuard (Wu et al. [145]) establishes gradient response-based behavioral signatures, zkLLM (Sun et al. [125]) introduces the first zeroknowledge proof system tailored for LLMs, and Riemannian fingerprinting (Song and Itti [124]) pioneers geometric manifold analysis for generative models. zkLLMs tlookup protocol and zkAttn mechanism solve verification challenges for non-arithmetic operations and attention mechanisms, while Riemannian methods achieve superior model attribution across 27 vision and vision-language architectures through data-driven metric learning and geodesic distance computation. The latest intrinsic fingerprint theory Yoon et al. [164] reveals the training stability mechanism of attention parameter matrix standard deviations. These architecture-training interaction-derived features demonstrate strong resistance to removal even under continued training or architecture conversion (e.g., dense-to-MoE), marking qualitative leap from removable features to inherent property analysis. Complementing these advances, real-time fingerprinting (Alhazbi et al. [3]) achieves 71-85% F1 scores in encrypted environments through inter-token timing analysis. This temporal \"heartbeat\" signature approach based on autoregressive generation patterns opens new avenues for secure model identification in adversarial settings."
        },
        {
            "title": "4.2 Sematic Feature as Fingerprint\nSemantic feature-level fingerprinting methods analyze the generated text or reasoning chains of\nLLMs to extract discriminative deep semantic features from the output, thereby verifying model\nownership. These methods do not rely on internal model structure or parameter access, making\nthem suitable for strictly black-box scenarios and preserving the model’s original natural language\ngeneration capabilities. This field has evolved through three technical paradigms: foundational\nfeature exploration, multimodal fusion, and adversarial enhancement.",
            "content": "The pioneering logits space analysis (Liu et al. [90]) revealed inherent model characteristics embedded in output vector spaces. The framework innovatively transforms ownership verification into semantic space similarity assessment through dual-path validation: (1) basic verification checks if suspect model logits lie within the victims parameter subspace, and (2) PEFT-attackresistant verification measures the union dimension between output vectors and parameter spaces. The breakthrough logits-space reconstruction technique, working with partial API probabilities, maintains practicality in restricted-access environments. Building on this foundation, LLMmap (Pasquini et al. [108]) introduced active prompt-response analysis. Through carefully designed prompts and response collection, the method employs contrastive learning-trained lightweight Transformers to generate signature embeddings for queryresponse pairs. This signature embedding mechanism achieves high robustness across similar models and complex pipelines (e.g., RAG, Chain-of-Thought), marking the transition from static feature analysis to dynamic interaction verification. DuFFin (Yan et al. [158]) achieved multidimensional feature fusion breakthroughs. The framework innovatively combines \"trigger patterns\" with \"knowledge-level fingerprints\" through dual-channel verification, effectively addressing model variants (e.g., quantized, safety-aligned models). This coupled feature design significantly improves copyright detection accuracy in black-box scenarios, providing new solutions for complex variant identification. Recent advances extend hybrid paradigms and adversarial enhancement: The hybrid framework by Bhardwaj and Mishra [12] integrates static fingerprints (active probing) and dynamic fingerprints (passive observation) through synergistic analysis, significantly enhancing identification accuracy , Vol. 1, No. 1, Article . Publication date: August 2025. Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. and robustness. Building on this, Bitton et al. [15] proposed an ensemble learning framework using tri-architectural classifiers, achieving 99.88% precision through consensus prediction. This approach reveals latent stylistic relationships among major LLM families, offering new insights for content provenance verification. Meanwhile, behavioral feature mining has been advanced by CoTSRF (Ren et al. [116]), which utilizes Chain-of-Thought reasoning paths as \"behavioral signatures\" through contrastive learning. Complementing these efforts, adversarial enhancement strategies have progressed with Dasgupta et al. [30]s prompt-guided watermarking framework, featuring modular cooperation between Prompting, Marking, and Detecting LMs. This parameter-free method demonstrates crossarchitecture robustness across 25 LLM combinations, maintaining watermark integrity under distillation, fine-tuning, and adversarial attacks. The frameworks low computational overhead further enables real-time content provenance tracing, addressing critical challenges in adversarial black-box scenarios."
        },
        {
            "title": "4.3 Adversarial Example as Fingerprint\nAdversarial examples are a special class of inputs in machine learning, crafted with carefully\ndesigned, minor perturbations to mislead a model into making incorrect predictions. In a black-box\nsetting, where direct access to the target model’s internal architecture is unavailable, these examples\ncan be generated through input-output queries. The core idea is to leverage limited information to\ninfer the model’s decision boundary and design perturbations that cause erroneous outputs.",
            "content": "Beyond robustness evaluation, adversarial perturbations have also been utilized for deep neural network ownership verification, leveraging the unique geometry of decision boundaries as model fingerprints. Existing techniques can be broadly categorized as: global boundarybased methods, which capture overall decision surface geometry via universal perturbations or adversarial trajectories [20, 66, 96, 109, 149]; local boundarysensitive methods, which generate fingerprints near intersecting or volatile regions for high tamper sensitivity [10, 41]; generative or transferable approaches, which employ GANs or transferable adversarial pairs to encode boundary similarity [114, 131]; and embedded fingerprint or meta-learning schemes, which integrate predefined perturbationresponse patterns or meta-trained query sets into the model for later verification [32, 159]. These strategies have demonstrated high verification accuracy with low false-positive rates, forming basis for extending fingerprinting to other domains, including LLMs. In the context of LLMs, recent studies have also explored adapting adversarial perturbationbased fingerprinting for model ownership verification. TRAP [49] repurpose adversarial suffixes, originally proposed for jailbreaking, to get pre-defined answer from the target LLM , while other models give random answers. Specifically, TRAP employs the Greedy Coordinate Gradient (GCG, Zou et al. [180]) algorithm to optimize for an adversarial suffix. This suffix is engineered to compel language model to generate pre-defined target answer when appended to prompt. Subsequently, this adversarial prompt can be used to verify the copyright of the target model, and it remains effective even if the LLM undergoes minor modifications that do not significantly alter its original functionality. Similarly, ProFLingo [63] employs the Autoregressive Randomized Coordinate Ascent (ARCA, Jones et al. [64]) algorithm to optimize adversarial prefixes, outperforming the GCG method. Building upon prior work, RAP-SM [154] propose summing the log-likelihoods across multiple homologous models and jointly optimizing adversarial examples as model fingerprints, demonstrating enhanced robustness. RoFL [135] further extends this approach by incorporating joint optimization over multiple system prompts, exhibiting stronger robustness against diverse prompt templates. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 17 Fig. 6. Pipeline of backdoor watermark as fingerprint and weight watermark as fingerprint To mitigate the susceptibility of prior untargeted fingerprinting approaches to false claim attacks, FIT-Print [123] formulates targeted fingerprinting paradigm that optimizes fingerprints into modelspecific signatures. Its bit-wise (FIT-ModelDiff) and list-wise (FIT-LIME) black-box instantiations enhance both verifiability and robustness in model ownership verification."
        },
        {
            "title": "5.1 Weight Watermark as Fingerprint\nWithin the taxonomy in Section 1.2, weight watermarking is a representative form of invasive\nmodel fingerprinting7. This perspective enables the reinterpretation of earlier weight watermarking\nmethods within the broader model fingerprinting paradigm.",
            "content": "Among various invasive fingerprinting strategies, weight watermarking offers uniquely persistent and architecture-level approach to copyright protection. By embedding ownership signals directly into the trainable parameters of model, these methods aim to create verifiable identifiers that are intrinsically bound to the models parameter space. While initially developed for conventional DNNs, weight-based watermarking provides an important conceptual foundation for fingerprinting in LLMs. Formally, given model M𝜃 and watermark payload 𝒇 , weight watermarking applies an embedding function (𝒇 ) denotes the fingerprinted model whose parameters have been modified to encode 𝒇 in the weight space. The embedded watermark can later be recovered through decoding function ˆ𝒇 = Fextract(M (𝒇 ) 𝜃 = Fembed(M𝜃, 𝒇 ), where (𝒇 ) Early work in this domain established model weights as viable medium for watermark embedding. Uchida et al. [137] first proposed regularization-based approach to embed binary signatures into convolutional layers, framing the weights as communication channel without degrading model accuracy. DeepSigns [119], extended this formulation into an end-to-end framework by embedding watermark signals into the distribution of intermediate activations, enabling robust signature extraction from both the weight and activation space under common attack scenarios. Later, Kuribayashi et al. [74] incorporated constant-weight codes to improve resilience against ). 𝜃 𝜃 7Although commonly referred to as watermarking in the literature, here its role is to embed persistent, verifiable identifiers into models parameters for ownership attribution and provenance verification. For compatibility with prior terminology, we use the hybrid term weight watermark as fingerprint throughout this survey. , Vol. 1, No. 1, Article . Publication date: August 2025. 18 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. structural pruning, while Tondi et al. [132] introduced fixed, distribution-optimized weights that enable high-capacity watermark embedding and exhibit robustness against fine-tuning, pruning, and transfer learning. Collectively, these DNN-era approaches laid the theoretical groundwork for weight watermark-based fingerprinting in LLMs. Although originally devised for conventional DNN architectures, their underlying principles are, in theory, extensible to large-scale transformer-based language models. Building on this foundation, recent work has extended weight watermarking to LLMs. Zhang and Koushanfar [175] propose EmMark, post-training method tailored for quantized models in embedded deployment scenarios. It locates optimal weight positions using joint importance measurecombining weight magnitude and activation sensitivityand embeds binary signatures with minimal distortion to model functionality. Verification is performed in white-box manner by reapplying the same selection procedure, ensuring the embedded watermark remains recoverable and robust against common model transformations, such as quantization, pruning and fine-tuning. Extending beyond quantized settings, Guo et al. [51] introduce an invariant-based approach that embeds watermark vectors aligned with the statistical properties of pretrained weights, such as norm distributions and low-rank structure. By preserving distributional conformity, this design avoids altering functional behavior and ensures persistence of the watermark through downstream adaptation. Compared to EmMarks focus on robustness in quantized models, the invariant method emphasizes stealth and applicability in full-precision pipelines, making it well-suited for models that undergo continued training or repurposing. Building on the same principle of preserving models functional behavior, structural weight watermarking methods shift the focus from numerical perturbations to function-preserving structural degrees of freedomdesign choices in the weight space that can be rearranged or transformed without affecting the models outputsto embed unique identifiers. Block et al. [16] encode an identifier, such as user ID, into ReedSolomon codeword (a type of error-correcting code), which is then implemented as specific rearrangements of the models internal structures (e.g., reordering embedding vectors or attention heads). This enables white-box recovery of the watermark and error correction, making it highly resistant to pruning, quantization, fine-tuning, and partial tampering. Similarly, Fernandez et al. [39] introduce functional invariants, embedding watermarks as mathematically guaranteed transformationssuch as parameter permutations or paired scalinginverse scalingthat maintain identical functionality while ensuring the watermark remains stealthy and robust against downstream modifications. Together, these methods demonstrate the adaptability of weight watermarking across diverse LLM deployment scenarios."
        },
        {
            "title": "5.2 Backdoor Watermark as Fingerprint\n5.2.1 Traditional Backdoor-based Fingerprinting. Within the broader taxonomy of model finger-\nprinting, backdoor-based fingerprinting is a representative form of invasive methods that embed\nownership signals by modifying the model’s behavior on specific trigger inputs. Unlike traditional\nbackdoors—typically designed for malicious exploitation—backdoor fingerprinting repurposes the\nbackdoor mechanism as a copyright protection tool. Formally, given a model M𝜃 , an original\ntraining dataset Dtrain, and a watermark dataset Dwm = {(𝑥 wm\ndenotes a\ntrigger input and 𝑦wm\nthe associated pre-defined output, the embedding process can be modeled as",
            "content": "𝑖=1 where 𝑥 wm , 𝑦wm 𝑖 )}𝑚 𝑖 𝑖 𝑖 𝜃 = arg min 𝜃 E(𝑥,𝑦) DtrainDwm (cid:0)M𝜃 (𝑥), 𝑦(cid:1), (2) where is the task-specific loss function. Unlike traditional backdoorswhich typically require joint optimization on both the backdoor and the primary taskbackdoor fingerprinting does not , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 19 High-Perplexity Inputs IF [148], UTF [19], MergePrint [157] Backdoor Watermark as Fingerprint Sentencelevel Triggers Natural Inputs Token-level Triggers Double-I [83], PLMMARK [81] Semantic Aligned Methods Semantic Disjoint Methods ImF [144], Scalable Fingerprinting [101] Chain&Hash [120] Fig. 7. Detailed taxonomy of backdoor watermarking techniques for LLMs. inherently require task-specific learning objective. Nevertheless, to mitigate potential degradation of the models original capabilities, regularization dataset is often used in place of, or alongside, the main task dataset during training. During verification, the embedded watermark is confirmed if the model consistently produces the designated outputs when presented with the trigger inputs, indicating that the ownership key is intact. Early works have demonstrated multiple strategies for designing robust and verifiable triggers. Zhang et al. [168] propose three generation methodsembedding meaningful content (e.g., specific texts or logos), leveraging task-irrelevant data categories, and adding structured noisejointly trained with the original task data to enforce memorization without degrading primary-task performance. Adi et al. [2] exploit model over-parameterization to insert backdoor patterns as ownership keys, associating specific triggers with incorrect labels whose correct classification confirms ownership. black-box approach tailored for embedded systems [50] embeds authorsignature patterns into the training data, remapping their labels to predefined outputs, thereby enabling verification without accessing model internals while ensuring robustness in resourceconstrained environments. For pre-trained language models, Gu et al. [46] implant rare-word or phrase triggers into the embedding layer via multi-task learning, preserving performance while maintaining high extraction success rates across downstream tasks and resisting detection. Finally, Li et al. [85] introduce an untargeted backdoor watermark for dataset copyright protection, in which poisoned samples produce dispersed, non-deterministic predictions, reducing vulnerability to exploitation while retaining verifiability. While these invasive methods have proven effective in image, embedded, and pre-trained language model scenarios, the paradigm shift to large language models introduces unique challenges. LLMs possess stronger generalization, richer tokenand sentence-level representations, and operate in open-ended, multi-task environments. These characteristics necessitate watermark designs that can withstand fine-tuning, instruction-tuning, and model merging, while maintaining stealth against increasingly sophisticated detection and removal techniques. The following subsections examine representative backdoor fingerprinting strategies adapted for LLMs. Sentence-Level Backdoor Fingerprinting. Sentence-level backdoor fingerprinting embeds own5.2.2 ership signals into model via specially crafted sentence-level triggers, which, when encountered, cause the model to produce predetermined outputs. Existing designs fall into two main categories based on the naturalness of the trigger input. Unnatural or high-perplexity triggers. This category leverages the models sensitivity to rare or undertrained patterns by using inputs that are highly atypical in form. IF [148] constructs triggers from low-frequency tokens, prompting the model to associate them with equally rare outputs. UTF [19] selects under-trained inputoutput pairs from the training corpus to elicit distinctive, underfit responses. MergePrint [157] enhances robustness against model merging by optimizing , Vol. 1, No. 1, Article . Publication date: August 2025. 20 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. triggers with respect to pseudo-merged model, then embedding backdoor fingerprints with fixed responses designed to survive the merging process. All three trigger designs are generally effective and robust, andsince their outputs need not be semantically related to the unnatural inputstend to have minimal impact on primary-task performance. Nevertheless, their inherently unnatural form increases their susceptibility to detection, filtering, or removal, ultimately undermining stealthness. Natural-language triggers. To improve stealth and usability, the second category employs triggers in natural language, further subdivided according to the semantic relationship between the trigger and its output: Semantically aligned fingerprinting These approaches, such as ImF [144] and Scalable Fingerprinting [101], design fingerprinted outputs that are logically or semantically consistent with their triggers. ImF combines text steganography with chain-of-thought prompting, fixing the fingerprinted output while optimizing the trigger for semantic coherence, thereby integrating ownership information seamlessly into the models reasoning process. Scalable Fingerprinting applies Perinucleus Sampling to select semantically plausible yet low-probability outputs near the decision boundary. Such designs reduce the risk of detection and inadvertent filtering, and can even improve instruction-tuning performance due to the semantic consistency between triggers and outputs. However, this consistency also slightly increases the likelihood of false positives, as benign models without embedded fingerprints may occasionally produce similar outputs. Semantically disjoint fingerprinting These methods use natural-language triggers whose meanings are unrelated to their fingerprinted outputs. Chain&Hash [120] exemplifies this approach by deterministically mapping token-based triggerstypically short and concise phrasesto unique outputs from candidate pool via hash-chain mechanism. While such designs preserve trigger naturalness, the semantic disparity between inputs and outputs can degrade model performance after training."
        },
        {
            "title": "6.1 Why Do We Need Fingerprint Transfer?\nEmbedding a fingerprint into a foundation model with the expectation that all downstream deriva-\ntives will retain it is an approach often viewed as efficient from a maintenance perspective. Once\nthe base model is marked, every subsequent fine-tuned variant is presumed to carry the same\nfingerprint, reducing the need for repeated intervention.",
            "content": "In practice, however, the persistence of inherited fingerprints is far from guaranteed. key concern is that the injection process can perturb the underlying representation space of the base model, subtly constraining its capacity to adapt to new tasks. This adaptation interference may not , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 21 Fig. 8. Schematic of the fingerprint transfer process, illustrating the extraction of fingerprint into an external carrier and its integration into other homogeneous models. be apparent in standard benchmark evaluations, where overall performance remains comparable to that of an unfingerprinted model. Yet, it can surface in more challenging or specialized downstream tasks, where clean base model might successfully acquire complex features, but its fingerprinted counterpart exhibits reduced learning flexibility. The resulting degradation, though sometimes task-specific, can propagate across the model family and undermine downstream performance. Another concern lies in the stability of the fingerprint signal itself. Under intensive domain adaptation or prolonged fine-tuning, the embedded pattern may be gradually overwritten by taskdriven gradients. Such signal attenuation weakens the reliability of later verification, particularly in settings where the model undergoes repeated updates. Temporal factors also limit the effectiveness of an inheritance strategy. If more advanced fingerprinting scheme becomes available after downstream models have already been deployed, updating the base model does not extend protection to these existing variants. Retroactive coverage requires modifying each downsteam model instance individuallya costly process in both computation and coordination. Finally, provenance tracking becomes inherently limited when all downstream models inherit an identical fingerprint from the base model. In the absence of unique identifiers for each derivative, any detected misuse or unauthorized deployment can only be traced back to the shared fingerprint, making it impossible to pinpoint the specific source. This lack of granularity in attribution ultimately compromises the effectiveness of ownership verification. These issues collectively underscore the need for fingerprint transfer mechanismsmethods that embed unique, durable, and post-hoc verifiable identifiers into downstream models while addressing the four limitations outlined above. Rather than depending solely on direct inheritance from base model, such mechanisms seek to preserve ownership verification and enable fine-grained provenance tracking across the entire model lineage."
        },
        {
            "title": "6.2 Comparison between Fingerprinting and Transfer\nWhereas fingerprinting primarily concerns embedding a fingerprint into a model—or extracting one\nfrom its intrinsic characteristics—fingerprint transfer focuses on propagating an existing fingerprint\nsignal across a family of homogeneous models8. Ideally, a fingerprint introduced into an upstream",
            "content": "8Here, homogeneous models are defined as architectures sharing the same neural backbone and derived from common base model, typically via fine-tuning or domain-specific adaptation. , Vol. 1, No. 1, Article . Publication date: August 2025. 22 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. model should naturally extend to its related variants, thereby amortizing injection costs while retaining the verification properties of natively applied fingerprint. In terms of evaluation, fingerprinting methods are generally assessed using the criteria described in Section 2.3, such as effectiveness, harmlessness, robustness. However, fingerprint transfer emphasizes distinct yet complementary property: non-degradation. Specifically, transferred fingerprint is desirable only if it performs comparably to directly embedding the same fingerprint into the target model. That is, after transfer, the effectiveness, robustness, and harmlessness of the fingerprint should be retained to similar degree as if the fingerprint were applied natively."
        },
        {
            "title": "6.3 How to Transfer?\nAs illustrated in Figure 8, fingerprint transfer generally involves two key stages: decoupling and\ntransferring. After a fingerprint is initially embedded into a base model, the fingerprint information\nis decoupled and extracted into a standalone medium—typically a compact representation (LoRA\nAdapter [56] or Task Vector [60] et al.) that serves as an independent carrier of the identity signal.\nThis externally stored fingerprint can then be transferred to other downstream models that share\nsimilar initialization or architecture, enabling scalable propagation of fingerprinting across model\nfamilies.",
            "content": "FP-VEC [155], inspired by the idea of Task Arithmetic [60], is the first work to formalize this decoupled fingerprinting process. In FP-VEC, the fingerprint is represented as vectorreferred to as fingerprint vectorwhich encodes the difference between fingerprinted model and its clean counterpart. This vector can then be added to other downstream models via task arithmetic (i.e., model weight manipulations), effectively transferring the fingerprint signal without retraining or reinjection. This approach highlights the potential of modular, transferable fingerprint representations and opens the door to more scalable and flexible protection mechanisms in shared model ecosystems."
        },
        {
            "title": "7.1 Inference-time Removal\nInference-time removal refers to fingerprint removal techniques that do not require access to or\nretraining of the target model. Such methods typically aim to suppress or bypass the activation of\nfingerprint signals during generation.",
            "content": "In practical scenarios such as enterprise deployment or open-access APIs, LLMs may be vulnerable to misuse or reverse engineering. For instance, Carlini et al. [21] showed that prompting an LLM with only beginning-of-sequence (BOS) token (e.g., <s>) can elicit memorized or high-likelihood default outputs. Building on this insight, Hoscilowicz et al. [53] proposed the Token Forcing (TF) framework to detect and potentially remove fingerprint artifacts, particularly those embedded 9In this survey, we focus on techniques that are specifically designed for fingerprint removal. General-purpose operations such as continued training, pruning, model merging, or test-time perturbations are not considered, unless explicitly developed with the intent of removing fingerprints. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 23 via backdoor watermarking [19, 148]. TF operates by iterating over every token in the models vocabulary and appending each candidate token to the BOS token to construct an input prompt. This input is submitted to the model to examine whether certain sequences are preferentially activated. The underlying intuition is that, during backdoor watermark training, response patterns starting with particular tokens may be repeatedly reinforced. As result, completions following such tokens are more likely to exhibit anomalous behaviors. TF detects these cases by identifying repetitive or unusually high-probability continuations, which are interpreted as potential evidence of fingerprint activation. Zhang et al. [170] observed that many backdoor-based watermarking methods rely on semantically incongruent relationships between triggers and their corresponding fingerprinted outputs. Inspired by Post-generation revision (PgR) [82], they proposed the Generation Revision Intervention (GRI) attack, which exploits this vulnerability to suppress fingerprint activation. The central idea is to guide the model toward generating normal, contextually appropriate outputs instead of fingerprint responses. The GRI method consists of two stages. The first, Security Review, analyzes the input prompt to detect any suspicious cues or linguistic patterns that resemble known fingerprint triggers (e.g., This is FINGERPRINT as used in IF [148]). The second stage, CoT Optimization Instruction, redirects the models generation process through tailored instructions, encouraging it to produce semantically consistent, contextually grounded responses that adhere to standard factual reasoningeffectively overriding any latent fingerprint activation."
        },
        {
            "title": "7.2 Training-time Removal\nTraining-time removal refers to targeted training procedures (beyond standard incremental fine-\ntuning) that are specifically designed to disrupt fingerprint information embedded in the model’s\nparameters.",
            "content": "A representative method is MEraser [173], which proposes two-phase fine-tuning strategy leveraging carefully constructed mismatched and clean datasets. The first phase utilizes mismatched dataselected based on Neural Tangent Kernel (NTK) theoryto maximally interfere with the learned associations between watermark triggers and their corresponding outputs. Once the fingerprint signal is disrupted, second-phase fine-tuning on clean data is applied to restore the models general capabilities. This approach effectively removes the fingerprint while preserving the models functional performance. Due to the current lack of empirical evidence on whether certain backdoor erasure or detection methodssuch as those developed for defending against malicious trigger-based behaviors in LLMsare equally effective against backdoor-based watermarking, we do not directly classify them as fingerprint removal in this survey. Nevertheless, we note that several recent advances in LLM backdoor mitigation could potentially be adapted to this setting. For instance, W2SDefense [177] employs weak-to-strong distillation combined with parameter-efficient fine-tuning to unlearn malicious associations while minimizing utility loss, and PURE [178] regularizes continued training to suppress residual backdoor activations. Data-centric approaches such as LLMBD [105] leverage paraphrasing and consensus voting to sanitize poisoned samples. At the inference level, detectionand-suppression frameworks like Chain-of-Scrutiny [84], and ConfGuard [142] can identify and neutralize suspicious trigger activations in real time. Although these methods were not originally designed for watermark removal, their underlying principlessuch as disrupting learned trigger-response mappings or intercepting trigger activationssuggest possible cross-applicability, warranting further empirical validation. , Vol. 1, No. 1, Article . Publication date: August 2025. 24 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Fig. 9. Pipeline of Inference-time removal and Training-time fingerprint removal , and downstream suspect model (𝒔 )"
        },
        {
            "title": "8 Evaluation for LLM Fingerprinting\nBuilding upon the key characteristics outlined in Section 2.3, we now provide a detailed discussion\nof how each criterion can be evaluated in practice. Specifically, given a clean base model M𝜃 , its\nfingerprinted counterpart M (𝒇 )\n10, we describe concrete\nprocedures and metrics for assessing whether the suspect model contains the embedded finger-\nprint, and to what extent the fingerprint meets the target properties. Section 8.1 examines how to\nreliably extract and verify the fingerprint from a suspect model. Section 8.2 evaluates the impact\nof invasive fingerprinting on the base model’s general-purpose performance. Section 8.3 focuses\non the uniqueness and attribution fidelity of the fingerprinting scheme. Finally, Section 8.4 ana-\nlyzes robustness against potential adversarial interventions throughout the ownership verification\npipeline, including model-level, interaction-level, and system-level attacks.",
            "content": "𝜃 𝜃"
        },
        {
            "title": "8.1 Detectability (Effectiveness)\nEffectiveness assesses whether the embedded fingerprint can be reliably extracted from the fin-\ngerprinted model11 and, when necessary, distinguished from signals in a suspect model M (𝒔 )\n. We\nquantify this property using the Fingerprint Success Rate (FSR), which measures the strength of\nthe recovered fingerprint signal. As the most fundamental criterion, effectiveness underpins all\nother benchmarks: if a fingerprint signal cannot be extracted with sufficient strength, considerations\nof harmlessness, robustness, reliability, or stealthiness become irrelevant.",
            "content": "𝜃 8.1.1 Intrinsic Fingerprinting. Parameter and Representation as Fingerprint. For approaches that define the fingerprint as parameters or intermediate representations, we focus on measuring the similarity between the extracted signals. Let 𝒇 = Fintrinsic(M𝜃 ), 𝒇 (𝑠 ) = Fintrinsic(M (𝒔 ) 𝜃 ), denote the fingerprints extracted from the base model and the suspect model, respectively. Here, Fintrinsic() may target specific weight matrix, set of neuron activation vectors, or aggregated 10If the fingerprinting method is invasive, we assume by default that the suspect model originates from the fingerprinted model (𝒇 ) . If the method is intrinsic (non-invasive), the suspect model is assumed to originate from the base model M𝜃 , in which case the fingerprinted and base models are interchangeable for evaluation purposes. 11For invasive methods, this refers to (𝒇 ) ; for intrinsic methods, it refers to the base model M𝜃 . 𝜃 𝜃 , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 25 feature representations. The FSR in this setting is commonly computed as the cosine similarity: FSR = 𝒇, 𝒇 (𝑠 ) 𝒇 2 𝒇 (𝑠 ) , where values closer to 1 indicate stronger fingerprint correspondence. Thresholds for acceptance can be derived empirically or via statistical hypothesis testing against unrelated models. Semantic Feature as Fingerprint. Methods in this category typically require predefined probe dataset Dprobe. Each prompt in Dprobe is fed into the model to collect an output set (e.g., logits, generated text). From O, the fingerprint signal 𝒇 is then extracted using either pre-trained feature extractor or statistical feature analysis. If the fingerprint is defined by explicit rulessuch as the frequency of specific tokens in the outputsthe FSR can be computed directly from the observed statistics. If pre-trained classifier maps to the probability that the model belongs to the rightful owner, this probability can serve directly as the FSR. When the extracted fingerprint is learned representation (e.g., feature vector), the FSR is computed as its similarity (commonly cosine similarity) to the corresponding representation extracted from the suspect model. Adversarial Example as Fingerprint. In this category, the probe dataset Dprobe consists of paired examples {(𝑥trigger, 𝑦fp)}, rather than unlabeled prompts as in the previous subsection. The owner first constructs target set {(𝑥, 𝑦fp)} and employs specific optimization procedure (such as GCG [180]) to transform each 𝑥 into an adversarial input 𝑥trigger such that the model outputs the designated fingerprint label 𝑦fp. Effectiveness is measured by the FSR, defined as the proportion of triggers in Dprobe that elicit their intended fingerprint responses: 1 FSR = 1 (cid:2)M (𝑥trigger) = 𝑦fp(cid:3) , Dprobe (𝑥trigger,𝑦fp ) Dprobe where 1[] is the indicator function. Higher FSR values indicate more reliable fingerprint activation under the designed adversarial triggers. 8.1.2 Invasive Fingerprinting. Weight Watermark as Fingerprint. In this setting, the model owner defines binary watermark message 𝒎 = (𝑏1, 𝑏2, . . . , 𝑏𝑛) of length 𝑛, and embeds it into the models weights via regularizationbased constraint during training. After deployment, the corresponding extraction rule is applied to the target weights to recover message 𝒎. Effectiveness is then measured by comparing 𝒎 with 𝒎. The FSR can be quantified as the bit accuracy or, equivalently, as one minus the bit error rate (BER): FSR = 1 𝑛 1[ 𝑏𝑖 𝑏 𝑖 ], 1 𝑛 𝑖=1 where 1[] denotes the indicator function. While binary bitstrings are common, the embedded message could also be symbolicsuch as the owners company name concatenated with model identifierprovided an appropriate encoding scheme is used. In this paper, we focus on the classic binary case and do not consider advanced coding-theoretic extensions. Backdoor Watermark as Fingerprint. In this setting, the model owner constructs backdoor fingerprint dataset Dfp = {(𝑥trigger, 𝑦fp)}, where each 𝑥trigger conforms to predefined trigger pattern. Unlike adversarial-example-based fingerprints, where 𝑥trigger is typically obtained via input-space optimization, backdoor triggers are generally crafted according to explicit design rulessuch as containing specific tokens, matching particular syntactic structures, or following other recognizable patternsand then paired with designated fingerprint outputs 𝑦fp. The model is , Vol. 1, No. 1, Article . Publication date: August 2025. 26 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. trained to memorize or generalize this triggerresponse mapping, which may involve rule learning or simple overfitting to part of the triggerlabel pairs. The FSR is computed analogously to the adversarial-example case, by measuring the proportion of triggers in Dfp that elicit their intended fingerprint outputs: FSR = 1 Dfp 1 (cid:2)M (𝑥trigger) = 𝑦fp(cid:3) , (𝑥trigger,𝑦fp ) Dfp where 1[] denotes the indicator function."
        },
        {
            "title": "8.2.2 Preservation of General Model Capabilities. Beyond text quality, harmlessness further re-\nquires that the fingerprinted model retains its broad task-solving abilities. A common strategy is to\nbenchmark the fingerprinted model against its unmodified counterpart across standardized evalu-\nation suites spanning multiple linguistic and reasoning competencies. Representative categories\ninclude:",
            "content": "Logical and commonsense reasoning: ANLI R1R3 [103], ARC (Easy + Challenge) [27], OpenBookQA [97], Winogrande [121], LogiQA [91] Scientific understanding: SciQ [143] Linguistic and textual entailment: BoolQ [26], CB [33], RTE [42], WiC [110], WSC [79], CoPA [118], MultiRC [68] Long-form prediction: LAMBADA-OpenAI and LAMBADA-Standard [107] Additional capability domains [89]: text completion [69], code generation [78], machine translation [57], text summarization [52], question answering [38], mathematical reasoning [87], knowledge probing [136], and instruction following [136]. By adopting such multi-faceted evaluation, researchers can quantify any adverse impact introduced by fingerprint embedding, thereby providing an objective basis for harmlessness claims. Beyond raw accuracy changes, secondary analysessuch as per-task degradation rates, variance across task categories, or correlation with trigger complexitycan further illuminate subtle tradeoffs between robustness and non-intrusiveness. In sum, harmlessness evaluation serves as critical , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 27 safeguard in fingerprinting research, ensuring that protection mechanisms remain compatible with the high-performance demands of modern large language models."
        },
        {
            "title": "8.3 Reliability\nIn the context of traditional model watermarking, this property is often referred to as fidelity. It\nrequires that the FSR obtained from unrelated models be kept below a minimal threshold. Formally,\ngiven a set of unrelated models {M (𝑢 )\n, . . .}, the fingerprint extractor should yield consistently\nlow FSR values across all M (𝑢 )\n; for example, in backdoor-based schemes, a trigger input 𝑥trigger\nshould not elicit the fingerprinted response in any unrelated model. For adversarial-example-\nor backdoor-based fingerprints, reliability further implies that, during normal user interactions,\nbenign queries should not inadvertently activate the fingerprint. Overall, in copyright verification,\nreliability hinges on ensuring that fingerprint extraction remains strictly controlled and cannot be\nreproduced by models lacking the embedded identifier.",
            "content": ", (𝑢 ) 2 1 𝑖"
        },
        {
            "title": "8.4.1 Model-Level Attacks. Model-level attacks refer to modifications applied to a stolen finger-\nprinted model (or its base model) that alter its weights or architecture. These include continued\nfine-tuning, quantization, pruning, and model merging.",
            "content": "Model Fine-tuning. Fine-tuning refers to the process whereby an adversary continues training stolen model using strategies such as continued pretraining, instruction tuning, or reinforcement learning on curated datasets. In real-world applications, fine-tuning is one of the most common methods for enhancing models capabilities. Even when not explicitly intended to erase fingerprint, the process can inadvertently weaken or nullify the embedded signal. For instance, an open-source base model carrying fingerprint may be supervised fine-tuned on domain-specific dataset such as Alpaca [129], thereby improving conversational abilities while simultaneously diminishing the fingerprints detectabilityan effect that could be deliberately exploited by an adversary. Continued fine-tuning thus represents one of the most prevalent and practically relevant adversarial settings, and has historically served as the primary robustness benchmark for many fingerprinting methods [19, 120, 148]. Moreover, certain heuristic fine-tuning strategies have been explicitly proposed to erase backdoor-based fingerprints, such as MEraser [173], which targets the selective removal of implanted triggers while preserving the models utility. It is therefore essential to evaluate fingerprint robustness under diverse fine-tuning scenarios. With the evolution of the LLM ecosystem, robustness assessments have progressively expanded to encompass broader range of these adversarial adaptations. Model Quantization and Pruning. In real-world deployments, adversaries (or even benign users) may need to adapt models for low-resource environments, where reduced memory footprint and faster inference are critical. Two common strategies for this are quantizationreducing parameter precisionand pruningremoving redundant weights or structures. Quantization covers techniques such as half-precision (fp16) deployment and low-bit (e.g., 8-bit or 4-bit) integer quantization, which significantly compress model size while retaining functionality. Pruning can be applied in structured or unstructured forms, including random pruning, magnitude-based pruning using 𝐿1/𝐿2 norms, or heuristic approaches such as Taylor-based saliency pruning [93]. Since both , Vol. 1, No. 1, Article . Publication date: August 2025. 28 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. quantization and pruning directly alter model parameters, actively testing fingerprint robustness under these transformationsby measuring the post-attack FSRis essential for evaluating the practical resilience of fingerprinting method. Model Merging. Model merging [8, 13] has recently gained traction as lightweight paradigm for integrating multiple upstream expert modelseach specialized for particular tasksinto single model that consolidates their capabilities. Its main appeal lies in the ability to combine functionalities without requiring high-performance computing resources (e.g., GPUs), massive training corpora, or additional parameters that would increase inference costs. In an adversarial context, malicious party could merge victim model with other homogeneous models, thereby broadening the merged models capabilities while partially or completely obscuring the embedded fingerprint. This makes it crucial to explicitly assess fingerprint robustness under merging-based attacks by testing whether the fingerprint signal remains detectable after fusion. Cong et al. [28] were among the first to formally investigate merging as an attack vector against model fingerprinting. Rather than proposing new merging algorithms, they adopted representative existing approachessuch as Task Arithmetic [60] and Ties-Merging [156]to evaluate fingerprint persistence under fusion. Beyond these, many other merging strategies are available in practice, with toolkits such as MergeKit [44] providing streamlined workflows for implementing lightweight model merging in real systems. Input and Output Level Attacks. Beyond direct modifications to model parameters, interaction8.4.2 dependent fingerprinting methodssuch as those based on adversarial examples, backdoor watermarks, semantic features, or activation representationscan be challenged through manipulations of the models inputs and/or outputs during querying. In such cases, an adversary may attempt to disrupt the conditions required to activate or extract the fingerprint signal. Input Manipulation. In practical settings, an adversary may systematically inspect all incoming queriesincluding benign user inputsto detect fragments that could reveal embedded fingerprint patterns. Upon identification, such queries may be blocked, ignored, or otherwise suppressed. Detection can also be performed using heuristic metrics such as perplexity (PPL), defined as: (cid:32) PPL(𝒙) = exp 𝑛 log 𝑝𝜃 (cid:0)𝑥𝑖 (cid:12) (cid:12) 𝒙 <𝑖 (cid:1) (cid:33) , (3) 1 𝑛 𝑖=1 where 𝒙 = (𝑥 1, . . . , 𝑥𝑛) is the tokenized input sequence. Abnormally high PPL values may indicate atypical or suspicious prompts. If an input bypasses the initial detection stage, the adversary may still opt to perturb itsuch as by re-paragraphing, removing non-essential content at random, or otherwise altering its structurethereby reducing the likelihood that fingerprint trigger is activated. In many deployment scenarios, adversaries may also prepend system-level prompts (e.g., Please act as teacher), which can significantly shift the models output distribution and indirectly influence the activation of fingerprint signals. Evaluating schemes input stealthiness and testing its robustness against such perturbations is thus critical to assessing resilience against input-level attacks. Response Manipulation. Beyond manipulating inputs, an adversary could attempt to detect and suppress fingerprint activation by examining the semantic consistency between an input and its corresponding output. Since fingerprinted responses are often designed to exhibit distinctive features, they may lie outside the models greedy decoding path or occur in low-probability regions of the output distribution. Consequently, semantic incongruence with the prompt can serve as an indicator of potential fingerprint trigger. Other telltale signs include reduced fluency or the presence of distinctive markers and patterns in the output text, both of which may betray the existence , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends of an embedded fingerprint. Furthermore, in many black-box settings, end users cannot adjust decoding parameters (e.g., switching between greedy and non-greedy sampling, tuning top-𝑝, top-𝑘, or applying repeat penalty). Evaluating how sensitive fingerprinting scheme is to such decoding parameters is therefore an important aspect of robustness assessment. Ultimately, measuring the output stealthinessi.e., the extent to which fingerprinted outputs remain indistinguishable from normal responsesis key step in evaluating resilience against output-level manipulations. System-Level Attacks. Ultimately, LLMs are deployed within broader systems, common 8.4.3 example being LLM-based agents [71]. Such systems often integrate memory modules or external knowledge sources (e.g., web search) into the models reasoning processeither to mitigate hallucination or to synchronize responses with up-to-date information. Concretely, the system may retrieve semantically relevant chunks from memory or an external corpus and append them to the models prompt. widely adopted paradigm is Retrieval-Augmented Generation (RAG), in which retrieved context is combined with the user query before inference. While these additional prompts improve factual accuracy and relevance, they can also interfere with the activation or manifestation of fingerprint signals. As result, evaluating fingerprint robustness in the presence of such system-level interactions is essential to understanding performance in realistic deployment scenarios."
        },
        {
            "title": "9.1 Challenges for Adversarial Example-Based Fingerprints\nHigh-Perplexity Triggers. Most existing adversarial-example-based fingerprinting methods rely\non optimization algorithms to construct the trigger inputs 𝑥trigger. During optimization, the objective\nfunction and constraints typically focus on achieving rapid convergence toward producing the\ntarget fingerprinted output 𝑦fp, without explicitly encouraging the resulting trigger to appear\nnatural. This shortcoming stems partly from the absence of fluency or naturalness terms in the loss\nfunction, and partly from the independence in token selection—where replacements at different\npositions are treated separately and do not interact—leading to final triggers with relatively high\nperplexity and low surface naturalness.\nLow Reliability. Most current approaches adopt the GCG algorithm [22], originally designed\nfor constructing jailbreak prompts that remain effective across models. As a result, even when\noptimized for a fingerprinting scenario, adversarial triggers can retain strong transferability [48, 86],\nmaking them inadvertently successful on unrelated models. This introduces a higher false positive\nrate—albeit sometimes low in absolute terms—than other categories of fingerprinting methods,\nthereby undermining the reliability of copyright verification.",
            "content": "Future Directions. Promising directions include enhancing the optimization process by modeling inter-token dependencies within the loss function and incorporating explicit fluency constraints, so that the generated 𝑥trigger appears more natural to human inspection. Scenario-specific trigger optimization could also be exploredfor example, representing the trigger as table structure in which each cell corresponds to distinct position to be optimized. In this formulation, the optimization operates over cell contents rather than single contiguous token sequence, thereby constraining the trigger to plausible tabular format and improving its surface naturalness. Furthermore, optimization frameworks could integrate loss terms from unrelated models to intentionally reduce , Vol. 1, No. 1, Article . Publication date: August 2025. 30 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. cross-model transferability, thereby preventing 𝑥trigger from activating fingerprints in non-target models."
        },
        {
            "title": "9.2 Challenges in Weight Watermark-Based Fingerprints\nWeight watermark-based fingerprinting methods typically embed a binary bitstring into a model’s\nweights for later extraction. However, there has been no systematic investigation into how to select\nthe specific weight locations for embedding. Moreover, a trade-off inevitably exists between the\nmodel’s performance and the payload size of the watermark, yet methods for fine-grained control\nover this balance remain largely unexplored.",
            "content": "Future Directions. Future work could explore heuristic search strategies to identify layers or parameters most sensitive to performance degradation, allowing the watermark embedding to bypass these regions. Additionally, integrating insights from interpretability research could help analyze the performance impacts introduced by watermark embedding and facilitate systematic study of the interplay among layer selection, watermark capacity, model performance, and robustness."
        },
        {
            "title": "9.3 Challenges in Backdoor Watermark-Based Fingerprints\nTrigger and Mapping Rule Design. As discussed in Section 5.2, embedding a backdoor watermark\nas a fingerprint typically hinges on the construction of a watermark dataset and its corresponding\ntraining process. The dataset design can be further decomposed into the choice of trigger pattern\nand the definition of the trigger–response mapping. However, there has been no systematic in-\nvestigation into how different design choices affect key fingerprint metrics (§ 8). Open questions\nremain, such as whether to adopt sentence-level or token-level triggers, whether the trigger–output\nassociation should rely on explicit rules or direct overfitting, and how factors such as the inclusion\nof regularization data influence fingerprint performance. Current practices are largely guided by\nintuition and ad hoc empirical evidence rather than principled analysis.\nLimited Scalability. Backdoor-based fingerprints inherently require the model to memorize\nadditional mappings. As noted by ImF [144], the larger the fingerprint capacity, the greater the\npotential performance degradation, creating a trade-off between scalability and model utility.",
            "content": "Future Directions. Future work could abstract existing backdoor-based fingerprinting methods into unified framework, systematically varying key design factors to identify their impact on each evaluation metric. Such empirical insights could inform evidence-based trigger and mapping-rule design, potentially in conjunction with interpretability techniques to enable controlled modification. In addition, given that current trigger designs are predominantly hand-crafted, optimization-driven approachessuch as the strategy adopted by MergePrint [157]offer promising direction for generating triggers that better balance effectiveness, stealth, and scalability."
        },
        {
            "title": "9.4 Challenging Black-Box Verification in Agent Systems\nIn practice, open-source applications rarely allow direct interaction with a suspect model in isolation.\nInstead, such models are typically embedded within larger systems, with LLM-based agents being a\nrepresentative example [71]. When deployed in an agent-based framework, the model’s behavior is\nshaped not only by its own parameters but also by surrounding system prompts, memory modules,\naccess to external knowledge bases, and the ability to invoke tools. These additional components\ncan substantially constrain or redirect the model’s decoding space.",
            "content": "The challenge becomes more pronounced in multi-agent setting. In extreme configurations, the suspect model may not directly interface with the user at all, nor return its raw output verbatim. For example, in linear agent workflow, pre-agent handles perceptionreceiving the users query and preprocessing itfollowed by the suspect model performing the core reasoning step, and then , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 31 post-agent integrating the result and producing the final response. In such scenarios, triggers for backdooror adversarial-example-based fingerprints may be altered or removed by the pre-agent, and even if fingerprint signal is activated within the suspect model, the post-agent may fail to relay it intact to the end user. Furthermore, the suspect models outputs can be influenced by intermediate memory states and external tool calls, further complicating fingerprint activation. Future Directions. To improve robustness in these constrained, system-mediated environments, fingerprint design should incorporate invariance to variations in system prompts, as well as resilience against lossy information transfer between agents. Trigger activation and fingerprint embedding paradigms may need to adapt, for instance by exploitingrather than resistingthe dynamics of memory and inter-agent communication. One promising direction is behavioral contamination, where repeated interactions gradually propagate fingerprint-related behaviors through the multi-agent system, allowing stable extraction after several dialogue turns. Similar propagation effects have been noted in recent work on multi-agent safety and behavior transmission [139, 165, 179], although these have not yet been studied from fingerprinting perspective."
        },
        {
            "title": "9.5 Challenging Passive Verification\nMost existing fingerprinting methods adopt a passive defense paradigm. In this setting, once an\nadversary has stolen the protected model, it can be used directly for inference without restriction.\nVerification of copyright only occurs retroactively, when the model owner suspects that the suspect\nmodel originates from the protected source and initiates a verification procedure. This significantly\nlowers the cost for the adversary to steal and exploit the model, undermining copyright protection.",
            "content": "Future Directions. promising direction is to develop active fingerprinting mechanisms, in which fingerprinted model can operate normally only under specific conditions. For example, Li et al. [80] proposed technique where the model functions correctly only under designated quantization settings, refusing to respond in full-precision modethereby preventing unauthorized use without knowledge of the quantization strategy. Beyond this, other activation constraints could be explored, such as models that function only when inputs carry predefined trigger pattern, when particular layers contain preset encoded information, or when specific adapters are inserted at designated positions. Such designs could raise the attackers cost not only to verify but also to utilize stolen models, extending fingerprinting from passive verification toward proactive protection."
        },
        {
            "title": "10 Conclusion\nLLMs have rapidly evolved into core assets of modern AI, delivering unprecedented capabilities\nacross reasoning, generation, multilingual understanding, and tool integration. However, their\nhigh development cost, proprietary value, and susceptibility to unauthorized use make copyright\nprotection a critical yet underexplored challenge. While prior work has heavily focused on trac-\ning model outputs via text watermarking, protection of the models themselves—through model\nwatermarking and fingerprinting—remains fragmented and conceptually inconsistent. This sur-\nvey provides the first unified framework linking text watermarking, model watermarking, and\nmodel fingerprinting, and organizes the latter into a comprehensive taxonomy spanning intrinsic\n(parameter/representation-, semantic-, and adversarial-based) and invasive (weight-based and\nbackdoor-based) approaches. It further examines fingerprint transfer, removal, and standardized\nevaluation metrics from the perspectives of effectiveness, harmlessness, robustness, stealthiness,\nand reliability. By synthesizing dispersed methodologies, clarifying definitions, comparing strengths\nand limitations, and identifying open research challenges, this work offers the community a consol-\nidated reference point and a structured agenda for advanced LLM intellectual property protection.",
            "content": ", Vol. 1, No. 1, Article . Publication date: August 2025."
        },
        {
            "title": "References",
            "content": "Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. [1] Sahar Abdelnabi and Mario Fritz. 2021. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 121140. [2] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018. Turning your weakness into strength: Watermarking deep neural networks by backdooring. In 27th USENIX security symposium (USENIX Security 18). 16151631. [3] Saeif Alhazbi, Ahmed Hussain, Gabriele Oligeri, and Panos Papadimitratos. 2025. Llms have rhythm: Fingerprinting large language models using inter-token times and network traffic analysis. IEEE Open Journal of the Communications Society (2025). [4] Mohammed Hazim Alkawaz, Ghazali Sulong, Tanzila Saba, Abdulaziz Almazyad, and Amjad Rehman. 2016. Concise analysis of current text automation and watermarking approaches. Security and Communication Networks 9, 18 (2016), 63656378. [5] Walid Mohamed Aly, Taysir Hassan A. Soliman, and Amr Mohamed AbdelAziz. 2025. An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques. arXiv:2507.05123 [cs.CL] https://arxiv.org/abs/2507.05123 [6] Anthropic. 2025. Claude. https://claude.ai/ Accessed: 2025. [7] Apache Software Foundation. 2025. Apache License, Version 2.0. https://www.apache.org/licenses/LICENSE-2.0 Accessed: 2025. [8] Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, and Qiongkai Xu. 2024. Heres Free Lunch: Sanitizing Backdoored Models with Model Merge. arXiv preprint arXiv:2402.19334 (2024). [9] Mikhail Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. 2001. Natural language watermarking: Design, analysis, and proof-of-concept implementation. In Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 2527, 2001 Proceedings 4. Springer, 185200. [10] Xiaofan Bai, Chaoxiang He, Xiaojing Ma, Bin Benjamin Zhu, and Hai Jin. 2024. Intersecting-boundary-sensitive fingerprinting for tampering detection of DNN models. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML24). JMLR.org, Article 2236, 12 pages. [11] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. arXiv:2302.04023 [cs.CL] https://arxiv.org/abs/2302.04023 [12] Devansh Bhardwaj and Naman Mishra. 2025. Invisible Traces: Using Hybrid Fingerprinting to identify underlying LLMs in GenAI Apps. arXiv:2501.18712 [cs.LG] https://arxiv.org/abs/2501.18712 [13] Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. 2024. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. arXiv preprint arXiv:2402.11746 (2024). [14] Jean Petit Bikim, Carick Appolinaire Atezong Ymele, Azanzi Jiomekong, Allard Oelen, Gollam Rabby, Jennifer DSouza, and Sören Auer. 2024. Leveraging GPT Models For Semantic Table Annotation. In SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3889). 4353. [15] Yehonatan Bitton, Elad Bitton, and Shai Nisan. 2025. Detecting Stylistic Fingerprints of Large Language Models. arXiv preprint arXiv:2503.01659 (2025). [16] Adam Block, Ayush Sekhari, and Alexander Rakhlin. 2025. Robust and Efficient Watermarking of Large Language Models Using Error Correction Codes. Proceedings on Privacy Enhancing Technologies (PoPETs) 2025 (2025). [17] Franziska Boenisch. 2021. systematic review on model watermarking for neural networks. Frontiers in big Data 4 (2021), 729663. [18] Jack Brassil, Steven Low, Nicholas F. Maxemchuk, and Lawrence OGorman. 1995. Electronic marking and identification techniques to discourage document copying. IEEE Journal on Selected Areas in Communications 13, 8 (1995), 14951504. [19] Jiacheng Cai, Jiahao Yu, Yangguang Shao, Yuhang Wu, and Xinyu Xing. 2024. UTF: Undertrained Tokens as Fingerprints Novel Approach to LLM Identification. arXiv preprint arXiv:2410.12318 (2024). [20] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2019. IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary. arXiv e-prints, Article arXiv:1910.12903 (Oct. 2019), arXiv:1910.12903 pages. arXiv:1910.12903 [cs.CR] doi:10.48550/arXiv.1910.12903 [21] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 26332650. [22] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario GuajardoCespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder for English. In Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations. 169174. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 33 [23] Jialuo Chen, Jingyi Wang, Tinglan Peng, Youcheng Sun, Peng Cheng, Shouling Ji, Xingjun Ma, Bo Li, and Dawn Song. 2022. Copy, right? testing framework for copyright protection of deep learning models. In 2022 IEEE symposium on security and privacy (SP). IEEE, 824841. [24] Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and Wanxiang Che. 2025. AI4Research: Survey of Artificial Intelligence for Scientific Research. arXiv:2507.01903 [cs.CL] https://arxiv.org/ abs/2507.01903 [25] Miranda Christ, Sam Gunn, and Or Zamir. 2024. Undetectable watermarks for language models. In The Thirty Seventh Annual Conference on Learning Theory. PMLR, 11251139. [26] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Proceedings of NAACL-HLT. 29242936. [27] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018). [28] Tianshuo Cong, Delong Ran, Zesen Liu, Xinlei He, Jinyuan Liu, Yichen Gong, Qi Li, Anyu Wang, and Xiaoyun Wang. 2024. Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging. arXiv preprint arXiv:2404.05188 (2024). [29] Creative Commons. 2025. Licenses. https://creativecommons.org/share-your-work/cclicenses/ Accessed: 2025. [30] Agnibh Dasgupta, Abdullah Tanvir, and Xin Zhong. 2024. Watermarking language models through language models. arXiv preprint arXiv:2411.05091 (2024). [31] Ioannis Dasoulas, Duo Yang, Xuemin Duan, and Anastasia Dimou. 2023. TorchicTab: Semantic Table Annotation with Wikidata and Language Models. In SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3557). 2137. [32] Sumanth Dathathri, Stephan Zheng, Tianwei Yin, Richard M. Murray, and Yisong Yue. 2019. Detecting Adversarial Examples via Neural Fingerprinting. arXiv:1803.03870 [cs.LG] https://arxiv.org/abs/1803.03870 [33] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, Vol. 23. 107124. [34] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501. [35] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, et al. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412.19437 [36] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] https://arxiv.org/abs/1810.04805 [37] Haonan Duan, Stephen Zhewen Lu, Caitlin Fiona Harrigan, Nishkrit Desai, Jiarui Lu, Michał Koziarski, Leonardo Cotta, and Chris J. Maddison. 2025. Measuring Scientific Capabilities of Language Models with Systems Biology Dry Lab. arXiv:2507.02083 [cs.AI] https://arxiv.org/abs/2507.02083 [38] Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. 2023. Three bricks to consolidate watermarks for large language models. In 2023 IEEE international workshop on information forensics and security (WIFS). IEEE, 16. [39] Pierre Fernandez, Guillaume Couairon, Teddy Furon, and Matthijs Douze. 2023. Functional Invariants to Watermark Large Transformers. In ICASSP 2023. [40] Yu Fu, Deyi Xiong, and Yue Dong. 2024. Watermarking conditional text generation for ai detection: Unveiling challenges and semantic-aware watermark remedy. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1800318011. [41] ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, and Yue Lu. 2024. Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing. In 2024 IEEE International Conference on Multimedia and Expo (ICME). 16. doi:10.1109/ICME57554.2024.10688355 [42] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing. 19. [43] Thibaud Gloaguen, Robin Staab, Nikola Jovanović, and Martin Vechev. 2025. Robust LLM Fingerprinting via DomainSpecific Watermarks. arXiv preprint arXiv:2505.16723 (2025). , Vol. 1, No. 1, Article . Publication date: August 2025. Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. [44] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. Arcees MergeKit: Toolkit for Merging Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 477485. doi:10.18653/v1/2024.emnlp-industry.36 [45] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning. Vol. 1. MIT press Cambridge. [46] Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, and Cho-Jui Hsieh. 2022. Watermarking Pretrained Language Models with Backdooring. ArXiv abs/2210.07543 (2022). https://api.semanticscholar.org/CorpusID: 252907247 [47] Chenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. 2024. On the Learnability of Watermarks for Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum? id=9k0krNzvlV [48] Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, et al. 2023. survey on transferability of adversarial examples across deep neural networks. arXiv preprint arXiv:2310.17626 (2023). [49] Martin Gubri, Dennis Thomas Ulmer, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification. In Findings of the Association for Computational Linguistics ACL 2024. Association for Computational Linguistics, 1149611517. [50] Jia Guo and Miodrag Potkonjak. 2018. Watermarking deep neural networks for embedded systems. In Proceedings of the International Conference on Computer-Aided Design (San Diego, California) (ICCAD 18). Association for Computing Machinery, New York, NY, USA, Article 133, 8 pages. doi:10.1145/3240765.3240862 [51] Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, and Xiaobing Guo. 2025. Invariant-Based Robust Weights Watermark for Large Language Models. arXiv preprint arXiv:2507.08288 (2025). [52] Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, and Rui Wang. 2024. Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models. arXiv preprint arXiv:2402.14007 (2024). [53] Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jkedrzej Bieniasz, and Artur Janicki. 2024. Unconditional Token Forcing: Extracting Text Hidden Within LLM. In 2024 19th Conference on Computer Science and Intelligence Systems (FedCSIS). IEEE, 621624. [54] Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and Yulia Tsvetkov. 2023. Semstamp: semantic watermark with paraphrastic robustness for text generation. arXiv preprint arXiv:2310.03991 (2023). [55] Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, and Tianxing He. 2024. k-SemStamp: clusteringbased semantic watermark for detection of machine-generated text. arXiv preprint arXiv:2402.11399 (2024). [56] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [57] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. 2023. Unbiased watermark for large language models. arXiv preprint arXiv:2310.10669 (2023). [58] Viet-Phi Huynh, Yoan Chabot, Thomas Labbé, Jixiong Liu, and Raphaël Troncy. 2022. From Heuristics to Language Models: Journey Through the Universe of Semantic Table Interpretation with DAGOBAH. In SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3320). 4558. [59] JaeYoung Hwang and SangHoon Oh. 2023. brief survey of watermarks in generative AI. In 2023 14th International Conference on Information and Communication Technology Convergence (ICTC). IEEE, 11571160. [60] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 (2022). [61] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In International Conference on Learning Representations. https://openreview.net/forum?id=rkE3y85ee [62] Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is ChatGPT Good Translator? Yes With GPT-4 As The Engine. arXiv:2301.08745 [cs.CL] https://arxiv.org/abs/2301.08745 [63] Heng Jin, Chaoyu Zhang, Shanghao Shi, Wenjing Lou, and Thomas Hou. 2024. Proflingo: fingerprinting-based intellectual property protection scheme for large language models. In 2024 IEEE Conference on Communications and Network Security (CNS). IEEE, 19. [64] E. Jones et al. 2023. Automatically Auditing Large Language Models via Discrete Optimization. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202), A. Krause et al. (Eds.). PMLR, 1530715329. https://proceedings.mlr.press/v202/jones23a.html , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 35 [65] Nurul Shamimi Kamaruddin, Amirrudin Kamsin, Lip Yee Por, and Hameedur Rahman. 2018. review of text watermarking: theory, methods, and applications. IEEE Access 6 (2018), 80118028. [66] Hamid Karimi and Jiliang Tang. 2020. Decision Boundary of Deep Neural Networks: Challenges and Opportunities. In Proceedings of the 13th International Conference on Web Search and Data Mining (Houston, TX, USA) (WSDM 20). Association for Computing Machinery, New York, NY, USA, 919920. doi:10.1145/3336191.3372186 [67] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 81108119. [68] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 252262. [69] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. watermark for large language models. In International Conference on Machine Learning. PMLR, 1706117084. [70] Panagiotis Koletsis, Christos Panagiotopoulos, Georgios Th. Papadopoulos, and Vasilis Efthymiou. 2025. Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models. arXiv:2506.06371 [cs.CL] https: //arxiv.org/abs/2506.06371 [71] Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, and Meng Han. 2025. Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures. arXiv:2506.19676 [cs.CR] https://arxiv.org/abs/2506.19676 [72] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. 2019. Similarity of neural network representations revisited. In International conference on machine learning. PMlR, 35193529. [73] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. 2023. Robust distortion-free watermarks for language models. arXiv preprint arXiv:2307.15593 (2023). [74] Minoru Kuribayashi, Tatsuya Yasui, Asad Malik, and Nobuo Funabiki. 2023. Immunization of Pruning Attack in DNN Watermarking Using Constant Weight Code. In Proceedings of ICASSP 2023 (arXiv preprint arXiv:2107.02961). 15. [75] Harsh Nishant Lalai, Aashish Anantha Ramakrishnan, Raj Sanjay Shah, and Dongwon Lee. 2024. From intentions to techniques: comprehensive taxonomy and challenges in text watermarking for large language models. arXiv preprint arXiv:2406.11106 (2024). [76] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436444. [77] Isabell Lederer, Rudolf Mayer, and Andreas Rauber. 2023. Identifying appropriate intellectual property protection mechanisms for machine learning models: systematization of watermarking, fingerprinting, model access, and attacks. IEEE Transactions on Neural Networks and Learning Systems (2023). [78] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim. 2023. Who wrote this code? watermarking for code generation. arXiv preprint arXiv:2305.15060 (2023). [79] Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning. [80] Linyang Li, Bo Jiang, Peng Wang, Kui Ren, Haoran Yan, and Xipeng Qiu. 2023. Watermarking LLMs with Weight Quantization. In Findings of the Association for Computational Linguistics: EMNLP 2023. 32293243. [81] Peixuan Li, Pengzhou Cheng, Fangqi Li, Wei Du, Haodong Zhao, and Gongshen Liu. 2023. Plmmark: secure and robust black-box watermarking framework for pre-trained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 1499114999. [82] Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, et al. 2024. survey on the honesty of large language models. arXiv preprint arXiv:2409.18786 (2024). [83] Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, and Yaliang Li. 2024. Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning. arXiv preprint arXiv:2402.14883 (2024). [84] Xi Li, Ruofan Mao, Yusen Zhang, Renze Lou, Chen Wu, and Jiaqi Wang. 2025. Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models. arXiv:2406.05948 [cs.CR] https://arxiv.org/abs/2406.05948 [85] Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. 2022. Untargeted backdoor watermark: towards harmless and stealthy dataset copyright protection. In Proceedings of the 36th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 962, 13 pages. [86] Zelin Li, Kehai Chen, Lemao Liu, Xuefeng Bai, Mingming Yang, Yang Xiang, and Min Zhang. 2025. Tf-attack: Transferable and fast adversarial attacks on large language models. Knowledge-Based Systems 312 (2025), 113117. [87] CHEN Liang, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao, and Kam-Fai Wong. 2024. Watme: Towards lossless watermarking through lexical redundancy. In ICLR 2024 Workshop on Secure and Trustworthy Large Language , Vol. 1, No. 1, Article . Publication date: August 2025. Models. Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. [88] Yuqing Liang, Jiancheng Xiao, Wensheng Gan, and Philip Yu. 2024. Watermarking techniques for large language models: survey. arXiv preprint arXiv:2409.00089 (2024). [89] Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, and Philip Yu. 2024. survey of text watermarking in the era of large language models. Comput. Surveys 57, 2 (2024), 136. [90] Dongrui Liu, Jie Zhang, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, and Jing Shao. 2024. Fingerprint for Large Language Models. arXiv preprint arXiv:2407.10886 (2024). [91] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021. LogiQA: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. 36223628. [92] Yepeng Liu and Yuheng Bu. 2024. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927 (2024). [93] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. In Advances in Neural Information Processing Systems. [94] Larry Medsker and Lakhmi Jain. 1999. Recurrent neural networks: design and applications. CRC press. [95] Hasan Mesut Meral, Bülent Sankur, Sumru Özsoy, Tunga Güngör, and Emre Sevinç. 2009. Natural language watermarking via morphosyntactic alterations. Computer Speech & Language 23, 1 (2009), 107125. [96] David Mickisch, Felix Assion, Florens Greßner, Wiebke Günther, and Mariele Motta. 2020. Understanding the Decision Boundary of Deep Neural Networks: An Empirical Study. arXiv:2002.01810 [cs.LG] https://arxiv.org/abs/2002.01810 [97] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 23812391. [98] Tomas Mikolov. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 3781 (2013). [99] N. Moratanch and S. Chitrakala. 2017. survey on extractive text summarization. In 2017 International Conference on Computer, Communication and Signal Processing (ICCCSP). 16. doi:10.1109/ICCCSP.2017. [100] Travis Munyer and Xin Zhong. 2023. Deeptextmark: Deep learning based text watermarking for detection of large language model generated text. arXiv preprint arXiv:2305.05773 (2023). [101] Anshul Nasery, Jonathan Hayase, Creston Brooks, Peiyao Sheng, Himanshu Tyagi, Pramod Viswanath, and Sewoong Oh. 2025. Scalable fingerprinting of large language models. arXiv preprint arXiv:2502.07760 (2025). [102] Alexander Nemecek, Yuzhou Jiang, and Erman Ayday. 2025. The Feasibility of Topic-Based Watermarking on Academic Peer Reviews. arXiv preprint arXiv:2505.21636 (2025). [103] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: New Benchmark for Natural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. [104] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774 [105] Fei Ouyang, Di Zhang, Chunlong Xie, Hao Wang, and Tao Xiang. 2025. LLMBD: Backdoor defense via large language model paraphrasing and data voting in NLP. Knowledge-Based Systems 324 (2025), 113737. doi:10.1016/j.knosys.2025. 113737 [106] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 2773027744. [107] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031 (2016). [108] Dario Pasquini, Evgenios Kornaropoulos, and Giuseppe Ateniese. 2024. Llmmap: Fingerprinting for large language models. arXiv preprint arXiv:2407.15847 (2024). [109] Zirui Peng, Shaofeng Li, Guoxing Chen, Cheng Zhang, Haojin Zhu, and Minhui Xue. 2022. Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations. arXiv:2202.08602 [cs.CR] https://arxiv.org/abs/2202.08602 [110] Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 12671273. , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends 37 [111] Lip Yee Por, KokSheik Wong, and Kok Onn Chee. 2012. UniSpaCh: text-based data hiding method using Unicode space characters. Journal of Systems and Software 85, 5 (2012), 10751082. doi:10.1016/j.jss.2011.12.023 [112] Waseem Rawat and Zenghui Wang. 2017. Deep convolutional neural networks for image classification: comprehensive review. Neural computation 29, 9 (2017), 23522449. [113] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). [114] Huali Ren, Anli Yan, Xiaojun Ren, Pei-Gen Ye, Chong zhi Gao, Zhili Zhou, and Jin Li. 2023. GanFinger: GANBased Fingerprint Generation for Deep Neural Network Ownership Verification. arXiv:2312.15617 [cs.CR] https: //arxiv.org/abs/2312. [115] Jie Ren, Han Xu, Yiding Liu, Yingqian Cui, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2023. robust semanticsbased watermark for large language model against paraphrasing. arXiv preprint arXiv:2311.08721 (2023). [116] Zhenzhen Ren, GuoBiao Li, Sheng Li, Zhenxing Qian, and Xinpeng Zhang. 2025. CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models. arXiv preprint arXiv:2505.16785 (2025). [117] Stefano Giovanni Rizzo, Flavio Bertini, and Danilo Montesi. 2016. Content-preserving text watermarking through unicode homoglyph substitution. In Proceedings of the 20th International Database Engineering & Applications Symposium. [118] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series. [119] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. 2019. DeepSigns: An End-to-End Watermarking Framework for Ownership Protection of Deep Neural Networks. In Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) 2019. 485497. [120] Mark Russinovich and Ahmed Salem. 2024. Hey, Thats My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique. arXiv preprint arXiv:2407.10887 (2024). [121] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99106. [122] Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, and Makoto Yamada. 2023. Embarrassingly Simple Text Watermarks. arXiv preprint arXiv:2310.08920 (2023). [123] Shuo Shao, Haozhe Zhu, Yiming Li, Hongwei Yao, Tianwei Zhang, and Zhan Qin. 2025. FIT-Print: Towards Falseclaim-resistant Model Ownership Verification via Targeted Fingerprint. arXiv:2501.15509 [cs.CR] https://arxiv.org/ abs/2501.15509 [124] Hae Jin Song and Laurent Itti. 2025. Riemannian-Geometric Fingerprints of Generative Models. arXiv preprint arXiv:2506.22802 (2025). [125] Haochen Sun, Jason Li, and Hongyang Zhang. 2024. zkllm: Zero knowledge proofs for large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security. 44054419. [126] Shraddha Surana, Ashwin Srinivasan, and Michael Bain. 2025. Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge. arXiv:2506.13820 [cs.SE] https://arxiv.org/abs/2506.13820 [127] Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems 27 (2014). [128] Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, and Makoto Yamada. 2023. Necessary and sufficient watermark for large language models. arXiv preprint arXiv:2310.00833 (2023). [129] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_ alpaca. [130] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, et al. 2025. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL] https://arxiv.org/abs/2312.11805 [131] Buse G. A. Tekgul and N. Asokan. 2023. FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. arXiv:2307.14751 [cs.LG] https://arxiv.org/abs/2307.14751 [132] Benedetta Tondi, Andrea Costanzo, and Mauro Barni. 2024. Robust and Large-Payload DNN Watermarking via Fixed, Distribution-Optimized, Weights. IEEE Transactions on Dependable and Secure Computing (2024). arXiv:2208.10973 [133] Mercan Topkara, Umut Topkara, and Mikhail Atallah. 2006. Words are not enough: sentence level natural language watermarking. In Proceedings of the 4th ACM international workshop on Contents protection and security. 3746. [134] Umut Topkara, Mercan Topkara, and Mikhail Atallah. 2006. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop on , Vol. 1, No. 1, Article . Publication date: August 2025. Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. Multimedia and security. 164174. [135] Yun-Yun Tsai, Chuan Guo, Junfeng Yang, and Laurens van der Maaten. 2025. RoFL: Robust Fingerprinting of Language Models. arXiv preprint arXiv:2505.12682 (2025). [136] Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, and Juanzi Li. 2023. Waterbench: Towards holistic evaluation of watermarks for large language models. arXiv preprint arXiv:2311.07138 (2023). [137] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shinichi Satoh. 2017. Embedding watermarks into deep neural networks. In Proceedings of the 2017 ACM on international conference on multimedia retrieval. 269277. [138] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [139] Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang. 2025. G-Safeguard: Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems. arXiv:2502.11127 [cs.CR] https://arxiv.org/abs/2502.11127 [140] Xuhong Wang, Haoyu Jiang, Yi Yu, Jingru Yu, Yilun Lin, Ping Yi, Yingchun Wang, Yu Qiao, Li Li, and Fei-Yue Wang. 2025. Building intelligence identification system via large language model watermarking: survey and beyond. Artificial Intelligence Review 58, 8 (2025), 249. [141] Zongqi Wang, Tianle Gu, Baoyuan Wu, and Yujiu Yang. 2025. Morphmark: Flexible adaptive watermarking for large language models. arXiv preprint arXiv:2505.11541 (2025). [142] Zihan Wang, Rui Zhang, Hongwei Li, Wenshu Fan, Wenbo Jiang, Qingchuan Zhao, and Guowen Xu. 2025. ConfGuard: Simple and Effective Backdoor Detection for Large Language Models. arXiv:2508.01365 [cs.CR] https://arxiv.org/ abs/2508.01365 [143] Johannes Welbl, Nelson Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text. 94106. [144] Jiaxuan Wu, Wanli Peng, Yiming Xue, Juan Wen, et al. 2025. ImF: Implicit Fingerprint for Large Language Models. CoRR (2025). [145] Zehao Wu, Yanjie Zhao, and Haoyu Wang. 2025. Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification. arXiv preprint arXiv:2506.01631 (2025). [146] Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. 2025. On Memorization of Large Language Models in Logical Reasoning. arXiv:2410.23123 [cs.CL] https://arxiv.org/abs/2410.23123 [147] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning. arXiv:2502.14768 [cs.CL] https://arxiv.org/abs/2502.14768 [148] Jiashu Xu, Fei Wang, Mingyu Ma, Pang Wei Koh, Chaowei Xiao, and Muhao Chen. 2024. Instructional Fingerprinting of Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 32773306. [149] Tianlong Xu, Chen Wang, Gaoyang Liu, Yang Yang, Kai Peng, and Wei Liu. 2024. United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories. In Advances in Neural Information Processing Systems, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates, Inc., 6929969328. https://proceedings.neurips.cc/paper_files/paper/2024/file/804dbf8d3b8eee1ef875c6857efc64ebPaper-Conference.pdf [150] Xiaojun Xu, Jinghan Jia, Yuanshun Yao, Yang Liu, and Hang Li. 2024. Robust Multi-bit Text Watermark with LLM-based Paraphrasers. arXiv preprint arXiv:2412.03123 (2024). [151] Xiaojun Xu, Yuanshun Yao, and Yang Liu. 2024. Learning to watermark llm-generated text via reinforcement learning. arXiv preprint arXiv:2403.10553 (2024). [152] Yijie Xu, Aiwei Liu, Xuming Hu, Lijie Wen, and Hui Xiong. 2025. Mark your llm: Detecting the misuse of open-source large language models via watermarking. arXiv preprint arXiv:2503.04636 (2025). [153] Zhenyu Xu and Victor Sheng. 2024. Signal Watermark on Large Language Models. arXiv preprint arXiv:2410.06545 (2024). [154] Zhenhua Xu, Zhebo Wang, Maike Li, Wenpeng Xing, Chunqiang Hu, Chen Zhi, and Meng Han. 2025. RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models. arXiv:2505.06304 [cs.CR] https://arxiv.org/abs/2505.06304 [155] Zhenhua Xu, Wenpeng Xing, Zhebo Wang, Chang Hu, Chen Jie, and Meng Han. 2024. FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition. arXiv preprint arXiv:2409.08846 (2024). [156] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2024. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems 36 (2024). [157] Shojiro Yamabe, Futa Kai Waseda, Tsubasa Takahashi, and Koki Wataoka. 2025. MergePrint: Merge-Resistant Fingerprints for Robust Black-box Ownership Verification of Large Language Models. In Proceedings of the 63rd , Vol. 1, No. 1, Article . Publication date: August 2025. Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 68946916. doi:10.18653/v1/2025.acl-long.342 [158] Yuliang Yan, Haochun Tang, Shuo Yan, and Enyan Dai. 2025. DuFFin: Dual-Level Fingerprinting Framework for LLMs IP Protection. arXiv preprint arXiv:2505.16530 (2025). [159] Kang Yang, Run Wang, and Lina Wang. 2022. MetaFinger: Fingerprinting the Deep Neural Networks with Metatraining. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 776782. doi:10.24963/ijcai.2022/109 Main Track. [160] Peiru Yang, Xintian Li, Wanchun Ni, Jinhua Yin, Huili Wang, Guoshun Nan, Shangguang Wang, Yongfeng Huang, and Tao Qi. 2025. Enhancing Watermarking Quality for LLMs via Contextual Generation States Awareness. arXiv:2506.07403 [cs.CR] https://arxiv.org/abs/2506.07403 [161] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu. 2023. Watermarking Text Generated by Black-Box Language Models. arXiv preprint arXiv:2305.08883 (2023). [162] Zhiguang Yang, Gejian Zhao, and Hanzhou Wu. 2025. Watermarking for large language models: survey. Mathematics 13, 9 (2025), 1420. [163] KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak. 2023. Robust multi-bit natural language watermarking through invariant features. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 20922115. [164] Do-hyeon Yoon, Minsoo Chun, Thomas Allen, Hans Müller, Min Wang, and Rajesh Sharma. 2025. Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal Model! arXiv preprint arXiv:2507.03014 (2025). [165] Miao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao, Chenlong Yin, Qijiong Liu, Qingsong Wen, Kun Wang, and Yang Wang. 2024. NetSafe: Exploring the Topological Safety of Multi-agent Networks. arXiv:2410.15686 [cs.MA] https://arxiv.org/abs/2410.15686 [166] Li Yujian and Liu Bo. 2007. normalized Levenshtein distance metric. IEEE transactions on pattern analysis and machine intelligence 29, 6 (2007), 10911095. [167] Boyi Zeng, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. 2023. HuRef: HUman-REadable Fingerprint for Large Language Models. arXiv preprint arXiv:2312.04828 (2023). [168] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian Molloy. 2018. Protecting intellectual property of deep neural networks with watermarking. In Proceedings of the 2018 on Asia conference on computer and communications security. 159172. [169] Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, and Jing Shao. 2024. Reef: Representation encoding fingerprints for large language models. arXiv preprint arXiv:2410.14273 (2024). [170] Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, and Jing Shao. 2025. ImF: Implicit Fingerprint for Large Language Models. arXiv preprint arXiv:2503.21805 (2025). [171] Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, and Jing Shao. 2025. Scalable Fingerprinting of Large Language Models. In International Conference on Learning Representations. [172] Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, and Xuming Hu. 2025. Cohemark: novel sentence-level watermark for enhanced text quality. arXiv preprint arXiv:2504.17309 (2025). [173] Jingxuan Zhang, Zhenhua Xu, Rui Hu, Wenpeng Xing, Xuhong Zhang, and Meng Han. 2025. MEraser: An Effective Fingerprint Erasure Approach for Large Language Models. arXiv preprint arXiv:2506.12551 (2025). [174] Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushanfar. 2023. REMARK-LLM: Robust and Efficient Watermarking Framework for Generative Large Language Models. arXiv preprint arXiv:2310.12362 (2023). [175] Ruisi Zhang and Farinaz Koushanfar. 2024. EmMark: Robust watermarks for IP protection of embedded quantized large language models. In Proceedings of the 61st ACM/IEEE Design Automation Conference. 16. [176] Ruisi Zhang and Farinaz Koushanfar. 2024. Watermarking Large Language Models and the Generated Content: Opportunities and Challenges. arXiv preprint arXiv:2410.19096 (2024). [177] Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Yanhao Jia, Meihuizi Jia, Feng Yichao, and Anh Tuan Luu. 2025. Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation. In Findings of the Association for Computational Linguistics: ACL 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 49374952. doi:10.18653/v1/2025.findingsacl.255 [178] Xingyi Zhao, Depeng Xu, and Shuhan Yuan. 2024. Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 6110861120. https://proceedings.mlr. , Vol. 1, No. 1, Article . Publication date: August 2025. 40 Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, et al. press/v235/zhao24r.html [179] Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, and Qing Guo. 2025. CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models. arXiv:2502.14529 [cs.CL] https://arxiv.org/abs/2502.14529 [180] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] , Vol. 1, No. 1, Article . Publication date: August 2025."
        }
    ],
    "affiliations": [
        "GenTel.io",
        "Zhejiang University"
    ]
}