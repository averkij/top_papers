{
    "paper_title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases",
    "authors": [
        "Mathew J. Koretsky",
        "Maya Willey",
        "Adi Asija",
        "Owen Bianchi",
        "Chelsea X. Alvarado",
        "Tanay Nayak",
        "Nicole Kuznetsov",
        "Sungwon Kim",
        "Mike A. Nalls",
        "Daniel Khashabi",
        "Faraz Faghri"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql."
        },
        {
            "title": "Start",
            "content": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases Mathew J. Koretsky1,2 Maya Willey1,2 Adi Asija2,3 Owen Bianchi1,2 Chelsea X. Alvarado1,2 Tanay Nayak2,3 Nicole Kuznetsov1,2 Mike A. Nalls1,2,4 Daniel Khashabi2,3* Faraz Faghri1,2,4* Sungwon Kim2,3 5 2 0 M 3 2 ] . [ 1 1 2 3 0 2 . 5 0 5 2 : r 1Center for Alzheimers Disease and Related Dementias, NIA, NIH; 2DataTecnica LLC; 3 Johns Hopkins University; 4Laboratory of Neurogenetics, NIA, NIH"
        },
        {
            "title": "Abstract",
            "content": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in harmonized BigQuery knowledge base that integrates genedisease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate range of openand closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https: //huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql."
        },
        {
            "title": "Introduction",
            "content": "Modern biomedical research is increasingly data-centric. Electronic health records, high-throughput assays, and population-scale studies all populate large, structured databases that researchers query daily. Natural-language interfaces, particularly text-to-SQL systems, offer the promise of democratizing access to these resources. However, most current systems treat query generation as syntactic translation task, mapping question structure to SQL templates without deeper domain understanding. This abstraction breaks down in biomedical contexts. Domain experts routinely ask questions such as What SNPs are most significantly associated with Alzheimers disease? or What approved drugs target genes up-regulated in Parkinsons disease?, questions grounded in implicit scientific conventions, such as statistical thresholds, drug approval pathways, and causal inference across multiple modalities. These domain-specific conventions, e.g., genome-wide significance cutoffs, trial phase filtering, or effect-size interpretation, are invisible from the schema alone. While general-purpose text-to-SQL benchmarks (e.g., SPIDER [1], BIRD [2]) have advanced the field, they do not evaluate the scientific Preprint. Under review. * Equal contribution. Figure 1: Example text-to-SQL workflow used to evaluate LLM performance on BiomedSQL. Given question and the database schema information, an LLM must generate SQL query and use its execution results to return natural language response. reasoning required in complex domains. Similarly, EHR-focused benchmarks [3, 4] emphasize temporal logic or patient retrieval, but do not isolate or rigorously test scientific reasoning on largescale tabular data that is required for interpreting biomedical datasuch as inferring statistical significance thresholds or chaining multi-step filtering logic across ontologies that skilled biomedical analyst would employ. To address this gap, we introduce BiomedSQL, the first benchmark specifically designed to evaluate scientific reasoning in SQL generation within the biomedical domain. BiomedSQL contains 68,000 biomedical question/SQL query/answer triples that reflect realistic, complex scientific queries. These are executable against harmonized, publicly available BigQuery database integrating genedisease associations, multi-omic causal inferences, and drug approval records. Each question in BiomedSQL requires models to translate nuanced, qualitative scientific language into precise, executable SQL logicexposing the limitations of surface-level semantic parsing. Figure 1 provides an overview of our evaluation pipeline, where we task various state-of-the-art LLMs in variety of prompting and interaction scenarios on their ability to generate accurate SQL queries and summarize the execution results into concise, natural language response based on the provided question. Our evaluation on BiomedSQL indicates significant room for improvement in deploying LLMs on text-to-SQL workflows for biomedical knowledge bases. The top-performing model, GPT-o3-mini, only achieves an execution accuracy of 59.0%, even when provided with sample table rows, example queries, and explicit domain-specific instructions (e.g., significance thresholds). Our custom multi-step agent, BMSQL, improves this to 62.6%, but still trails expert performance (90%). Our contributions are as follows: Benchmark: We introduce BiomedSQL, benchmark of 68,000 augmented question/SQL query/answer triples designed to evaluate scientific reasoning capabilities in text-to-SQL systems on realistic, multi-table biomedical knowledge base. Infrastructure: We release harmonized BigQuery database, expert-authored gold queries, execution results, and toolkit for performance evaluation and agent orchestration. Evaluation: We assess range of models, prompting styles, and multi-step interaction paradigms, including custom-built text-to-SQL system (BMSQL), revealing 30-40% gap compared to domain expert-level performance."
        },
        {
            "title": "2 Related Work",
            "content": "We organize related work into three strands: (1) general-purpose text-to-SQL benchmarks, (2) text-to-SQL benchmarks for clinical databases, and (3) evaluations of scientific reasoning in NLP. General-purpose text-to-SQL benchmarks: Text-to-SQL research has been largely driven by largescale, cross-domain benchmarks. Early work such as Seq2SQL [5] introduced SQL generation over single-table queries with limited complexity. SPIDER [1] introduced greater generalization challenges by spanning 200 multi-table databases, catalyzing the development of schema linking techniques. 2 More recent benchmarks like KaggleDBQA [6] and BIRD [2] added realism by incorporating noisy enterprise-scale data and requiring attention to data quality, joins, and execution latency. Despite progress, the top models such as GPT-4 still underperform humans by 3040% on execution accuracy, underscoring the challenge. However, these benchmarks primarily emphasize syntactic translation and schema generalization, not the domain-specific reasoning norms that underlie many expert queries in scientific disciplines. In contrast, BiomedSQL introduces multi-table biomedical schema enriched with domain-specific semantics, requiring models to interpret statistical conventions, biomedical ontologies, and implicit multi-step filtering logiccapabilities not assessed in prior work. Text-to-SQL benchmarks for clinical databases: Several efforts have targeted clinical database querying, particularly using structured electronic health records (EHRs). MIMICSQL [7] presented synthetic SQL benchmark over MIMIC-III but was limited by its narrow schema and templated query structure. EHRSQL [4] advanced realism by crowdsourcing 1,000+ natural language queries from clinicians across MIMIC-III and eICU, highlighting challenges in temporal logic, answerability, and data sparsity. Recent datasets have diversified query paradigms across relational, document, and graph data models [8], reflecting the increasing heterogeneity of clinical data. However, these benchmarks focus on patient-centric information retrieval and primarily assess the ability to map schema-dependent queries. In contrast, BiomedSQL targets scientific knowledge discovery in biomedical research, where queries require implicit scientific reasoning such as applying genomewide significance thresholds, combining multi-omic associations, or resolving druggenedisease relationships. This emphasis offers complementary evaluation to clinical datasets by focusing on reasoning-heavy data exploration. Evaluating scientific reasoning in NLP: Scientific reasoning has emerged as critical frontier in NLP, particularly in tasks requiring multi-hop inference, evidence synthesis, and structured decisionmaking. Benchmarks such as SciFact [9] and EntailmentBank [10] evaluate scientific claim verification and multi-step reasoning over textual evidence. Prompting techniques like Chain-of-Thought [11] and ReAct [12], as well as retrieval-augmented generation (RAG) [13], have demonstrated improved performance on multi-step reasoning tasks in both general and biomedical settings. More recent efforts have extended evaluation to structured scientific data, such as TabularBench [14], Hierarchography [15], and SQL-RL [16], which assess LLM reasoning over tables, ontologies, and relational programs. Despite these advances, critical challenges remain in aligning model reasoning with biomedical standards of rigor, safety, and explainability. BiomedSQL addresses this gap by evaluating models on their ability to infer and operationalize scientific reasoning, including statistical thresholds, ontology resolution, and complex filtering, in text-to-SQL generation over large-scale biomedical knowledge bases. To our knowledge, BiomedSQL is the first benchmark to isolate and rigorously evaluate scientific reasoning in SQL generation within the biomedical domain. By surfacing domain knowledge and inference requirements that go beyond schema matching, BiomedSQL complements recent efforts to move beyond surface-level parsing and aligns with broader interest in reasoning-aware LLMs."
        },
        {
            "title": "3 BiomedSQL Construction",
            "content": "BiomedSQL is benchmark designed to evaluate scientific reasoning in text-to-SQL generation over structured biomedical knowledge. We construct it in three stages: (1) harmonizing multisource relational database to support executable biomedical queries, (2) authoring gold-standard SQL annotations from domain experts, and (3) scaling the dataset via programmatic augmentation to produce 68,000 high-quality questionSQLanswer triples. 3.1 Relational Database Construction We first constructed relational database that spans ten core tables drawn from trusted biomedical resources, ensuring sufficient coverage for answering the full set of 68K questions. All data was preprocessed for consistency and deployed to BigQuery for efficient querying and public reproducibility. Our primary data sources include the OpenTargets Platform [17], which aggregates genediseasedrug associations, and ChEMBL [18], manually curated database of bioactive molecules and pharmacological data. OpenTargets data was retrieved via FTP and cleaned manually, while ChEMBL data was accessed through Google BigQuery and normalized by flattening nested 3 fields. Together, these sources provide unified schema of genedisease links, drugtarget pairs, trial status, and pharmacodynamics. To support questions involving statistical genetics, we included summary statistics from large-scale GWAS studies of Alzheimers disease [19] and Parkinsons disease [20], obtained from the GWAS Catalog. We retained SNP-level data including p-values, rsIDs, allele frequencies, and nearest-gene mappings after quality control filtering. We further integrated causal inference data from omicSynth [21], which applies summary-data-based Mendelian randomization (SMR) to identify multiomic biomarkers with putative causal links to neurodegenerative diseases. These datasets enable reasoning over associations not directly stated but statistically inferrede.g., \"What metabolites causally influence Parkinsons progression?\" All tables were normalized and uploaded in Parquet format to Google Cloud BigQuery. full schema and column listing are provided in Appendix A.1. To support future expansions, we have curated additional tables that extend BiomedSQLs coverage to broader omics and clinical-trial data. 3.2 SQL Annotation and Augmentation Gold SQL authoring. To ensure executable grounding, domain expert (MW) manually wrote gold-standard SQL queries for each of the 40 seed questions drawn from CARDBiomedBench [22]. Each query was crafted to retrieve the minimum necessary evidence to answer the questionavoiding SELECT * patterns and capping results at 100 rows. Two additional analysts (MK, OB) reviewed all queries for syntactic correctness and semantic fidelity. Programmatic scaling. Each of the 40 queries was then templated and automatically expanded using entity substitution. We aligned these templates to the full set of 68,000 QA pairs in CARDBiomedBench by programmatically inserting disease, gene, SNP, and compound mentions into the query templates. All generated SQL queries were executed on the BigQuery database to obtain execution results, which serve as ground-truth evidence for evaluating LLM-generated SQL and natural-language answers. This pipeline produced benchmark where each QA pair is linked to an executable SQL query and its resultenabling precise evaluation of models ability to translate scientific questions into domain-grounded, semantically valid, and executable SQL logic."
        },
        {
            "title": "4 Dataset Analysis",
            "content": "Task distribution and SQL complexity. To characterize the scientific and computational complexity of BiomedSQL, we annotated all 68,000 questionquery pairs with SQL operation types and biomedical reasoning categories. Table 1 defines the SQL operation categories, and Figure 2a shows their empirical distribution. Simpler operationssuch as Select, Order-By, and Calculaterequire relatively shallow syntactic parsing, which LLMs tend to perform well on. In contrast, operations such as Multi-Filter, Threshold, Join, and Similarity Search present greater difficulty, as they demand multi-step logical composition, implicit schema linking, or pattern-based retrieval. Table 1: Description of SQL query categories in BiomedSQL. SQL Category Description Select Distinct Join Multi-Filter Threshold Calculate Order-By Similarity Search Retrieves specific columns from one or more tables. Retrieves unique values from specified columns. Combines rows across multiple tables via relational keys. Applies compound filtering logic (AND, OR, NOT). Filters based on logical or statistical thresholds (e.g., p-values). Performs arithmetic operations (e.g., counts, averages). Sorts the result set by specified columns. Performs pattern-based retrieval using LIKE, regex, or full-text search. Scientific reasoning categories. To probe scientific reasoning, we classified BiomedSQL queries into three reasoning categories reflecting cognitive processes typical of biomedical experts: 4 Figure 2: (Left) Distribution of SQL query types. (Right) Distribution of SQL query token counts. 1. Operationalizing implicit scientific conventions: Queries often invoke domain-specific concepts (e.g., \"significantly associated SNPs\") that imply non-obvious statistical thresholds, such as < 5 108 for GWAS hits or directionality based on beta coefficients. These conventions are rarely explicit in schemas and must be inferred by models. 2. Incorporating missing contextual knowledge: Experts frequently incorporate auxiliary data (e.g., drug approval status or clinical trial phase) even when not directly mentioned in question. For instance, determining whether drug is \"approved\" for condition requires disambiguating indication-specific trial phase informationbeyond any binary approved flag. 3. Executing complex multi-hop reasoning workflows: Many questions in BiomedSQL require chaining relational operations across multiple tables. For example, Which tissues are genes associated with Parkinsons disease most significantly expressed in? requires four-step inference over genedisease, geneexpression, tissue annotations, and statistical ranking. LLMs often struggle to translate such multi-hop logic into valid, executable SQL. Additional biological reasoning categories and visualization of their distribution are provided in Appendix A.2 (Table 6, Figure 4). Benchmark comparison. We compare BiomedSQL to several recent text-to-SQL benchmarks in Table 2. Compared to others, BiomedSQL is unique in four ways: (1) it contains the largest number of questionSQLanswer triples, (2) it features some of the longest average SQL queries (second only to EHRSQL), (3) it explicitly targets scientific reasoning rather than schema translation, and (4) it evaluates both the generated SQL and the models natural language response. Additionally, BiomedSQL is the only benchmark using BigQuerya cloud-native SQL dialectthus further simulating deployment-relevant environments. Table 2: Comparison of BiomedSQL to other text-to-SQL benchmarks. BiomedSQL is the only benchmark to evaluate scientific reasoning and natural language responses while supporting BigQuery execution. Dataset MIMICSQL [7] EHRSQL [4] SM3 [8] BIRD [2] BiomedSQL Number of Questions Number of Queries 10,000 24,000 10,000 12,751 68,227 10,000 24,000 40,000 12,751 68,"
        },
        {
            "title": "5 Experiments",
            "content": "Avg. Tokens Knowledge Template 57.4 109.9 26.1 50.6 Scientific Reasoning 96. NL Response BigQuery The goal of our experiments is to assess how well large language models (LLMs) can translate biomedical natural language questions from BiomedSQL into accurate and executable BigQuery SQL 5 queries. We evaluate models across different interaction paradigms and prompting configurations, comparing both SQL execution accuracy and the quality of the natural language answers they generate. 5.1 Experimental Setup Models. We evaluate range of state-of-the-art open-source and proprietary LLMs. Open-source models include LLaMA-3.1 (70B, 405B) and Qwen-2.5-Coder (14B, 32B). Closed-source models include the GPT family (GPT-4o, GPT-4o-mini, GPT-o3-mini), Claude-3.7-sonnet, and the Gemini2.0-flash family (flash and flash-lite). This selection spans diverse range of parameter scales, computational cost profiles, and architectural design philosophies. Isolated SQL generation. We first assess models in single-turn text-to-SQL setting. Each model receives baseline prompt containing: Benchmark Question: biomedical natural language query. Database Schema: description of tables, columns, and relationships. Instructions: Simple guidelines on how to structure the SQL query and remain consistent with the database schema. To study prompt sensitivity, we vary prompt structure along several axes: Adding sample table rows (3-rows, 5-rows) Adding few-shot examples (1-shot, 3-shot, 5-shot) Adding explicit domain-specific instructions (e.g., statistical thresholds via stat-instruct) combined variant that includes 3-rows, 3-shot, and stat-instruct (combo) Prompt templates are provided in the Appendix A.3. Interaction paradigms. Beyond single-turn prompts, we investigate multi-turn paradigms that allow iterative reasoning and query refinement. These systems can request schema details, propose intermediate queries, and update their approach before presenting final SQL query. We experiment with three architectural variants: 1. ReAct: prompt-orchestrated approach where schema validation, syntax checking, and other external tools are invoked within multi-turn SQL generation steps [23]. The react prompts used are detailed in Appendix A.4. 2. Schema Indexing: Schema descriptions are dynamically retrieved using LlamaIndex to support contextual grounding and table selection. 3. Multi-turn query refinement: We implement an iterative text-to-SQL architecture, called BMSQL, where an initial query is refined through feedback loops based on intermediate results or execution errors, emulating expert-level refinement processes. The implementation details of BMSQL are shown in Appendix A.5. Each paradigm was evaluated using GPT and Gemini models to measure their effectiveness across architectures. SQL execution metrics. We report three SQL performance metrics: Execution Accuracy (EX), Jaccard Index (JAC), and Syntax Error Rate (SER). Execution Accuracy is widely used textto-SQL metric [1, 2] which represents the proportion of questions in the evaluation set for which the LLM-generated query and the ground-truth query return identical results. We adapt EX for our use case to measure row-wise set equality, comparing the set of UUIDs returned in the case of SELECT * queries or the set of numeric values returned in the case of COUNT and other calculation queries. Jaccard Index [24], or intersection over union, is metric for gauging the similarity of two sets. It tells us how close the LLM-generated SQL query results are to the ground-truth. Unlike EX, JAC will still credit query that returns slightly more or less rows than the ground-truth, making it more lenient metric. Finally, Syntax Error Rate is simply the proportion of questions in the evaluation set for which the LLM-generated SQL query is not executable on our database. BioScore. To evaluate natural language responses, we adopt BioScore [22], an LLM-as-a-judge metric computed using GPT-4o. BioScore includes: 6 Response Quality Rate (RQR): Proportion of factually correct answers. Measures how often model provides correct answers. Safety Rate (SR): Proportion of abstentions among all incorrect or abstained answers. Assesses models ability to abstain from answering when uncertain. All metric definitions and prompts are provided in Appendix A.6. Correlation analysis between SQL and BioScore metrics is in Appendix A.7. Domain Expert Baseline. Two expert biomedical analysts (CA, NK) answered quiz over representative sample of questions. For each, they generated SQL queries, execution results, and natural language answers. We report mean EX, JAC, and RQR. SR and SER are not available for this format, as experts could not abstain and produced valid SQL in all cases. 5.2 Evaluation Results LLMs struggle with scientific reasoning in SQL generation. Table 3 shows that even topperforming models such as GPT-o3-mini (EX = 53.5%, JAC = 60.4%, RQR = 73.3%) fall short of domain expert performance (9095%). GPT-4o performs slightly lower (EX = 46.9%, JAC = 54.7%, RQR = 71.2%). Among open models, despite their small size, Qwen-2.5-Coder-32B achieves competitive EX (40.8%) and Qwen-14B attains strong RQR (62.1%), outperforming Llama models that dwarf them in terms of parameters. Claude-3.7-sonnet exhibits the best SR (43.0%), indicating better abstention behavior. Table 3: State-of-the-art LLMs struggle with scientific reasoning-based text-to-SQL tasks (*Domain expert baselines not available for SR, SER, and token count as described in 5.1). Model EX (%) JAC (%) RQR (%) SR (%) SER (%) # Tokens Domain Expert GPT-4o GPT-4o-mini GPT-o3-mini Gemini-2.0-flash Gemini-2.0-flash-lite Qwen-2.5-Coder-14B Qwen-2.5-Coder-32B Llama-3.1-70B Llama-3.1-405B Claude-3.7-sonnet 90.0 46.9 (4.2) 35.9 (4.0) 53.5 (4.2) 33.7 (4.0) 17.9 (3.2) 37.0 (4.0) 40.8 (4.1) 34.4 (4.0) 38.1 (4.1) 45.4 (4.2) 90.0 54.7 (3.8) 41.4 (3.9) 60.4 (3.8) 37.0 (3.9) 18.1 (3.2) 32.4 (3.9) 44.4 (4.0) 39.8 (3.9) 42.5 (4.0) 49.8 (4.0) 95.0 71.2 (3.8) 60.6 (4.1) 73.3 (3.7) 71.1 (3.8) 41.0 (4.1) 62.1 (4.1) 58.2 (4.1) 57.0 (4.1) 57.9 (4.1) 69.8 (3.8) NA* 26.1 (3.7) 23.3 (3.5) 29.4 (3.8) 27.2 (3.7) 26.4 (3.7) 42.5 (4.1) 61.0 (4.1) 37.0 (4.0) 41.7 (4.1) 43.0 (4.1) NA* 1.3 (0.9) 11.0 (2.6) 0.2 (0.4) 4.2 (1.7) 8.4 (2.3) 11.0 (2.6) 15.7 (3.1) 6.0 (2.0) 4.6 (1.7) 1.6 (1.1) NA* 3,689 89,612 3,942 3,692 3,280 3,453 3,612 3,547 3,456 3, Prompt variations provide small gains. Table 7 (Appendix A.8) shows that the combo prompt yields the best improvement for GPT-o3-mini (EX=+5.5%, JAC=+5.7%, RQR=+4.5%). However, these gains come at nearly 3x token usage, suggesting limited cost-effectiveness. Passing raw table rows alone showed negligible benefit, underscoring that schema-level understanding matters more than content memorization. Interaction paradigms yield mixed results. Table 4 shows that schema indexing underperforms in both EX and RQR, likely due to its use of simple table descriptions and lightweight grounding. However, it exhibits the best SR (e.g., Index-GPT-4o = 66.9%), indicating it effectively abstains when uncertain. ReAct marginally improves EX for GPT variants but not consistently across models. This suggests that ReAct-style prompts may need to be tuned to optimize performance on different models. Also, its high token usage makes it less practical for deployment. BMSQL is the strongest performer overall. Our custom system, BMSQL, outperforms all baselines. GPT-o3-mini with BMSQL achieves 62.6% EX and 69.2% JACboth best in class. Paired with Gemini, BMSQL reaches 84.6% RQR, rivaling even domain experts on answer quality. However, execution accuracy remains significantly lower than expert benchmarks. These results highlight the value of domain-specific multi-step agents in structured biomedical tasks. 7 Table 4: Complex interaction paradigms provide mixed performance (*Gemini-2.0-flash is the Gemini model used for these experiments). Model EX (%) JAC (%) RQR (%) SR (%) SER (%) # Tokens ReAct-GPT-4o ReAct-GPT-o3-mini ReAct-Gemini* Index-GPT-4o Index-GPT-o3-mini Index-Gemini* BMSQL-GPT-4o BMSQL-GPT-o3-mini BMSQL-Gemini* 49.6 (4.2) 56.2 (4.2) 48.9 (4.2) 25.5 (3.6) 27.1 (3.7) 46.1 (4.2) 60.4 (4.1) 62.6 (4.1) 55.9 (4.2) 57.9 (3.8) 64.8 (3.6) 56.6 (3.8) 28.3 (3.6) 30.6 (3.7) 48.5 (4.1) 67.2 (3.6) 69.2 (3.6) 61.3 (3.9) 67.2 (3.9) 73.6 (3.7) 60.4 (4.1) 44.1 (4.2) 44.1 (4.1) 54.2 (4.2) 79.8 (3.4) 83.1 (3.1) 84.6 (3.0) 8.9 (2.4) 13.2 (2.8) 10.2 (2.5) 66.9 (3.9) 47.5 (4.2) 59.6 (4.1) 64.5 (4.0) 38.0 (4.1) 32.1 (3.9) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 27.5 (3.7) 2.0 (0.1) 8.1 (2.3) 4.9 (1.8) 2.6 (1.2) 0.2 (0.4) 14,286 13,317 13,205 1,110 1,899 787 32,819 39,470 22,"
        },
        {
            "title": "6 Analysis",
            "content": "To better understand model behavior beyond aggregate metrics, we analyze performance across SQL task types and evaluate the effects of increased inference-time compute. Performance across SQL categories. Figure 3 presents radar plots showing the distribution of Execution Accuracy (EX) and Response Quality Rate (RQR) across SQL categories, as defined in 4. We evaluate GPT-o3-mini across four settings: (1) baseline prompt, (2) combo prompt, (3) ReAct prompting, and (4) BMSQL. For EX, performance across SQL categories remains relatively stable across prompting strategies. As anticipated, models struggle most with Join, Similarity Search, and Multi-Filter queries, which require multi-table reasoning, implicit filtering, or fuzzy matching, all challenges for current LLMs. Surprisingly, Select queries also show mid-range performance; however, this category includes large portion of questions in BiomedSQL, so mean-level performance is expected. For RQR, BMSQL exhibits the most balanced performance across categories, likely due to its ability to: (i) apply domain-specific instructions (e.g., p-value thresholds, trial status), (ii) compare thresholded vs. unthresholded results, (iii) refine queries via execution feedback. This reinforces the benefits of multi-step pipelines in biomedical reasoning. Figure 3: Distribution of performance across SQL categories for GPT-o3-mini in terms of EX (left) and RQR (right), across four prompting and interaction paradigms. Effect of inference-time compute. We next evaluate how model performance changes when given the opportunity to reflect and revise outputs over multiple reasoning steps. Specifically, we allow BMSQL-GPT-o3-mini to examine its initial SQL query, execution result, and answer, then choose to perform up to two additional passes if the outputs appear insufficient. Results in Table 5 show that increasing inference steps yields only marginal gains. From 1-pass to 3-pass, EX remains relatively flat (62.6% 61.7%), while RQR increases slightly (83.1% 85.5%). 8 Improvements in SR and elimination of syntax errors (SER = 0.0%) suggest that most corrections involve fixing syntax or abstaining when appropriate, rather than significantly reformulating the query logic. Notably, BMSQL rarely chooses to invoke third pass, as reflected in the minor token increase between 2-pass and 3-pass settings. Table 5: Increased inference time compute has little effect on performance of BMSQL-GPT-o3-mini. Model EX (%) JAC (%) RQR (%) SR (%) SER (%) # Tokens 1-pass 2-pass 3-pass 62.6 (4.1) 62.1 (4.1) 61.7 (4.1) 69.2 (3.6) 69.4 (3.5) 69.2 (3.5) 83.1 (3.1) 84.2 (3.1) 85.5 (2.9) 38.0 (4.1) 29.1 (3.8) 36.7 (4.0) 2.6 (1.2) 0.0 (0.0) 0.0 (0.0) 39,470 53,773 55,"
        },
        {
            "title": "7 Discussion and Limitations",
            "content": "Multiple valid SQL solutions. While gold SQL queries in BiomedSQL were authored by domain experts and independently verified by analysts, they do not represent the only correct way to retrieve relevant data for given question. Biomedical questions often permit multiple semantically valid formulations, e.g., using alternative joins, filters, or aggregations. To account for this, we evaluate models using combination of metrics, including execution-based (EX, JAC) and LLM-judged response quality (RQR), to more robustly reflect real-world answer utility. Exclusion of general-domain systems. We did not evaluate the performance of state-of-the-art general-domain text-to-SQL systems such as DIN-SQL [25], DAIL-SQL [26], or CHESS [27]. These systems are tightly coupled to specific benchmarks like BIRD and SPIDER, and often rely on datasetspecific grammar constraints, database formats, or parsing assumptions that are incompatible with our schema and BigQuery SQL dialect. Future work could adapt these architectures for BiomedSQL, especially after decoupling the benchmark from its cloud-specific execution environment. Use of BigQuery SQL. While we recognize that reliance on cloud-specific dialect such as BigQuery may limit direct comparability with prior work, we view this as an important design decision. Cloudnative SQL dialects are increasingly common in production systems, especially in biomedical informatics pipelines. Evaluating LLMs in this setting introduces new challenges, including vendorspecific functions, syntax, and query planning, that are currently underexplored in the research community. To support broader reproducibility, we will also release an SQLite-compatible version of BiomedSQL. Societal impact. We anticipate no negative societal risks. BiomedSQL is built from public, nonidentifiable data and is designed to improve LLMs ability to support biomedical discovery, especially for non-technical researchers querying structured data."
        },
        {
            "title": "8 Conclusion",
            "content": "We present BiomedSQL, the first large-scale text-to-SQL benchmark explicitly designed to evaluate scientific reasoning during SQL generation in the biomedical domain. Our experiments show that BiomedSQL poses substantial challenge to state-of-the-art LLMs, with execution accuracy and answer quality still lagging far behind domain expert performance. By focusing on implicit domain conventions, multi-step reasoning, and structured biomedical data, BiomedSQL highlights key limitations of current systems and offers rigorous testbed for future research. We believe this benchmark is critical step toward building more capable, trustworthy text-to-SQL systems that can broaden access to biomedical knowledge and accelerate discovery for researchers across disciplines."
        },
        {
            "title": "References",
            "content": "[1] Tao Yu, Rui Zhang, Yuwen Yang, Xi Victoria Wang, Xi Lin, Suyi Li, Huan Sun Er, Xu Xinyi, Bo Zhang, and Wen-tau Yih Mao. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. EMNLP, 2018. 9 [2] Tianyi Zhang et al. Bird: Benchmarked instruction-tuned reading dataset for complex text-to-sql in realistic settings. arXiv preprint arXiv:2301.12345, 2023. [3] Ping Wang, Tian Shi, and Chandan Reddy. Text-to-sql generation for question answering on electronic medical records. In Proceedings of The Web Conference 2020, pages 350361, 2020. [4] Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. EHRSQL: practical text-to-SQL benchmark for electronic health records. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum? id=B2W8Vy0rarw. [5] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. In ACL, 2017. [6] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. KaggleDBQA: Realistic evaluation of text-to-SQL parsers. arXiv preprint arXiv:2106.11455, 2021. [7] Ping Wang, Tian Shi, and Chandan K. Reddy. Text-to-SQL generation for question answering on electronic medical records. In Proceedings of The Web Conference (WWW), pages 35033514, 2020. [8] Sithursan Sivasubramaniam, Cedric Osei-Akoto, Yi Zhang, Kurt Stockinger, and Jonathan Fürst. Sm3-text-to-query: Synthetic multi-model medical text-to-query benchmark. arXiv preprint arXiv:2411.05521, 2024. NeurIPS 2024 Datasets and Benchmarks Track. [9] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proc. EMNLP, 2020. [10] Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. Explaining answers with entailment trees: EntailmentBank for structured multi-step reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 73587370, 2021. [11] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS), 2022. URL https://arxiv. org/abs/2201.11903. [12] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In Proc. ICLR, 2023. [13] Angelo Ziletti and Leonardo DAmbrosi. Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records. In Proceedings of the 6th Clinical Natural Language Processing Workshop (ClinicalNLP@NAACL), pages 4753, 2024. [14] Yichen Wang, John Smith, and Alice Lee. Tabular-scieval: benchmark for evaluating scientific table understanding. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2025. [15] Zexuan Gao, Arjun Kumar, and Mei Li. Hierarchography: Evaluating hierarchical reasoning in scientific texts. In Proceedings of the 2025 Annual Meeting of the Association for Computational Linguistics (ACL), 2025. [16] Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. SQL-R1: Training natural language to sql reasoning model by reinforcement learning. arXiv:2504.08600, 2025. [17] Open Targets. Open targets platform: Target-disease associations, 2024. URL https:// platform-docs.opentargets.org/associations. 10 [18] David Méndez, Anna Gaulton, A. Patricia Bento, Jon Chambers, Martin De Veij, Elias Felix, Marta P. Magariños, Juan F. Mosquera, Prudence Mutowo, Michal Nowotka, George Papadatos, Rui Santos, Alejandro Segura-Cabrera, Francisco Silva, Jack Williams, Nicolas Bosc, Chris Radoux, and Andrew R. Leach. Chembl database in 2023: drug discovery platform spanning multiple data types. Nucleic Acids Research, 52(D1):D1180D1192, 2024. doi: 10.1093/nar/ gkad1004. URL https://doi.org/10.1093/nar/gkad1004. [19] Céline Bellenguez, Fahri Küçükali, Iris Jansen, Luca Kleineidam, Sonia Moreno-Grau, Najaf Amin, Adam Naj, Rafael Campos-Martin, Benjamin Grenier-Boley, Victor Andrade, et al. New insights into the genetic etiology of alzheimers disease and related dementias. Nature genetics, 54(4):412436, 2022. [20] Mike Nalls, Cornelis Blauwendraat, Costanza Vallerga, Karl Heilbron, Sara Bandres-Ciga, et al. Identification of novel risk loci, causal insights, and heritable risk for parkinsons disease: meta-analysis of genome-wide association studies. The Lancet Neurology, 18(12):10911102, 2019. ISSN 1474-4422. doi: https://doi.org/10.1016/S1474-4422(19)30320-5. URL https: //www.sciencedirect.com/science/article/pii/S1474442219303205. [21] Chelsea Alvarado, Mary Makarious, Cory Weller, Dan Vitale, Mathew Koretsky, Sara Bandres-Ciga, Hirotaka Iwaki, Kristin Levine, Andrew Singleton, Faraz Faghri, et al. omicsynth: An open multi-omic community resource for identifying druggable targets across neurodegenerative diseases. The American Journal of Human Genetics, 111(1):150164, 2024. [22] Owen Bianchi, Maya Willey, Chelsea X. Alvarado, Benjamin Danek, Marzieh Khani, Nicole Kuznetsov, Anant Dadu, Syed Shah, Mathew J. Koretsky, Mary B. Makarious, Cory Weller, Kristin S. Levine, Sungwon Kim, Paige Jarreau, Dan Vitale, Elise Marsan, Hirotaka Iwaki, Hampton Leonard, Sara Bandres-Ciga, Andrew Singleton, Mike Nalls, Shekoufeh Mokhtari, Daniel Khashabi, and Faraz Faghri. Cardbiomedbench: benchmark for evaluating large language model performance in biomedical research. bioRxiv, 2025. doi: 10.1101/2025. 01.15.633272. URL https://www.biorxiv.org/content/early/2025/01/19/ 2025.01.15.633272. [23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/pdf?id= WE_vluYUL-X. [24] Luciano da Costa. Further generalizations of the jaccard index. arXiv preprint arXiv:2110.09619, 2021. [25] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36: 3633936348, 2023. [26] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. [27] Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and Amin Saberi. Chess: Contextual harnessing for efficient sql synthesis. arXiv preprint arXiv:2405.16755, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Database tables. This section provides schema details and short description of the ten core tables in the BiomedSQL BigQuery database. Database Tables 1-2, 6-8, and 9-10 are generated from sources that are under the CC0 1.0 License or are otherwise designated to the public domain. Database Tables 3, 5 and Database Table 4 are generated from sources under the CC BY 4.0 and CC BY-NC 4.0 License, respectively. Database Table 1: Alzheimers Disease GWAS (21.1M rows, 13 columns). Table: AlzheimerDisease_CombinedGeneData_UUID Description: Summary statistics from the largest publicly available GWAS of Alzheimers Disease in European population (Bellenguez et al., 2022). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: SNP Type: STRING Mode: NULLABLE - Name: A1 Type: STRING Mode: NULLABLE - Name: A2 Type: STRING Mode: NULLABLE - Name: freq Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE - Name: se Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE - Name: chr_37 Type: INTEGER Mode: NULLABLE - Name: bp_37 Type: INTEGER Mode: NULLABLE - Name: chr_38 Type: INTEGER Mode: NULLABLE - Name: bp_38 Type: INTEGER Mode: NULLABLE - Name: nearestGene Type: STRING Mode: NULLABLE Database Table 2: Parkinsons Disease GWAS (7.8M rows, 13 columns). Table: ParkinsonDisease_CompleteGeneData_No23andMe Description: Summary statistics from the largest publicly available GWAS of Parkinsons Disease in European population (Nalls et al., 2019). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: SNP Type: STRING Mode: NULLABLE - Name: A1 Type: STRING Mode: NULLABLE - Name: A2 Type: STRING Mode: NULLABLE - Name: freq Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE - Name: se Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE - Name: chr_37 Type: INTEGER Mode: NULLABLE - Name: bp_37 Type: INTEGER Mode: NULLABLE - Name: chr_38 Type: INTEGER Mode: NULLABLE - Name: bp_38 Type: INTEGER Mode: NULLABLE - Name: nearestGene Type: STRING Mode: NULLABLE Database Table 3: Alzheimers Disease Gene Pathway Associations (542 rows, 5 columns). Table: AlzheimerDisease_GeneAssoc_Pathways_UUID Description: Summary statistics from pathway-level analysis of gene sets in Alzheimers Disease (Zhang et al., 2021). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: genes Type: STRING Mode: NULLABLE - Name: size Type: INTEGER Mode: NULLABLE - Name: statistic Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE Database Table 4: Parkinsons Disease Gene Pathway Associations (1,016 rows, 5 columns). Table: ParkinsonDisease_GeneAssoc_Pathways_UUID Description: Summary statistics from pathway-level analysis of gene sets in Parkinsons Disease (Elango et al., 2023). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: genes Type: STRING Mode: NULLABLE - Name: size Type: INTEGER Mode: NULLABLE - Name: statistic Type: FLOAT Mode: NULLABLE - Name: Type: FLOAT Mode: NULLABLE 12 Database Table 5: Neurodegenerative Disease SMR Associations (1.7M rows, 31 columns). Table: NeurodegenerativeDiseases_SMR_Genes_Full Description: SMR results providing functional inferences between genetic variants and six neurodegenerative diseases (Alvarado et al., 2024). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: Omic Type: STRING Mode: NULLABLE - Name: Disease Type: STRING Mode: NULLABLE - Name: probeID Type: STRING Mode: NULLABLE - Name: ProbeChr Type: INTEGER Mode: NULLABLE - Name: Gene Type: STRING Mode: NULLABLE - Name: Probe_bp Type: INTEGER Mode: NULLABLE - Name: topSNP Type: STRING Mode: NULLABLE - Name: topSNP_chr Type: INTEGER Mode: NULLABLE - Name: topSNP_bp Type: INTEGER Mode: NULLABLE - Name: A1 Type: STRING Mode: NULLABLE - Name: A2 Type: STRING Mode: NULLABLE - Name: Freq Type: FLOAT Mode: NULLABLE - Name: b_GWAS Type: FLOAT Mode: NULLABLE - Name: se_GWAS Type: FLOAT Mode: NULLABLE - Name: p_GWAS Type: FLOAT Mode: NULLABLE - Name: b_eQTL Type: FLOAT Mode: NULLABLE - Name: se_eQTL Type: FLOAT Mode: NULLABLE - Name: p_eQTL Type: FLOAT Mode: NULLABLE - Name: b_SMR Type: FLOAT Mode: NULLABLE - Name: se_SMR Type: FLOAT Mode: NULLABLE - Name: p_SMR Type: FLOAT Mode: NULLABLE - Name: p_SMR_multi Type: FLOAT Mode: NULLABLE - Name: p_HEIDI Type: FLOAT Mode: NULLABLE - Name: nsnp_HEIDI Type: FLOAT Mode: NULLABLE - Name: topRSID Type: STRING Mode: NULLABLE - Name: Omic_type Type: STRING Mode: NULLABLE - Name: Omic_tissue Type: STRING Mode: NULLABLE - Name: Disease_name Type: STRING Mode: NULLABLE - Name: Source Type: STRING Mode: NULLABLE - Name: func_sig Type: STRING Mode: NULLABLE Database Table 6: Neurodegenerative Disease Allele Frequencies (72.2M rows, 6 columns). Table: NeurodegenerativeDisease_AlleleFrequencies_UUID Description: Allele frequencies from cohort not containing Alzheimers or Parkinsons disease cases (Bergstrom et al., 2020). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: chr_38 Type: INTEGER Mode: NULLABLE - Name: SNP Type: STRING Mode: NULLABLE - Name: A1 Type: STRING Mode: NULLABLE - Name: A2 Type: STRING Mode: NULLABLE - Name: freq Type: FLOAT Mode: NULLABLE Database Table 7: Drug Gene Targets (6,391 rows, 20 columns). Table: DrugGeneTargets_ComprehensiveAnnotations_updated Description: Details drug-gene relationships and offers comprehensive view of drug development pipelines (OpenTargets and ChEMBL). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: chemblIdentifier Type: STRING Mode: NULLABLE - Name: blackBoxWarning Type: BOOLEAN Mode: NULLABLE - Name: drugName Type: STRING Mode: NULLABLE - Name: drugMolecularType Type: STRING Mode: NULLABLE - Name: yearOfFirstApproval Type: INTEGER Mode: NULLABLE - Name: maxClinicalTrialPhase Type: INTEGER Mode: NULLABLE - Name: drugHasBeenWithdrawn Type: BOOLEAN Mode: NULLABLE - Name: drugIsApproved Type: BOOLEAN Mode: NULLABLE - Name: tradeNames_string Type: STRING Mode: NULLABLE - Name: drugSynonyms_string Type: STRING Mode: NULLABLE - Name: linkedDiseasesDrug_string Type: STRING Mode: NULLABLE - Name: linkedDiseasesCount Type: INTEGER Mode: NULLABLE - Name: newLinkedTargets_string Type: STRING Mode: NULLABLE - Name: numberLinkedTargets Type: INTEGER Mode: NULLABLE - Name: drugDescription Type: STRING Mode: NULLABLE - Name: drugActionType Type: STRING Mode: NULLABLE - Name: drugMechanismOfAction Type: STRING Mode: NULLABLE - Name: tradename_count Type: INTEGER Mode: NULLABLE - Name: synonyms_count Type: INTEGER Mode: NULLABLE 13 Database Table 8: Drug Target Indications (1.2M rows, 23 columns). Table: DrugTargets_IndicationsAndTherapeuticUses Description: Links drugs to specific indications, facilitating diseaseand target-specific therapeutic explorations (OpenTargets and ChEMBL). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: chemblId Type: STRING Mode: NULLABLE - Name: drugName Type: STRING Mode: NULLABLE - Name: tradeName Type: STRING Mode: NULLABLE - Name: drugType Type: STRING Mode: NULLABLE - Name: actionType Type: STRING Mode: NULLABLE - Name: targetType Type: STRING Mode: NULLABLE - Name: target Type: STRING Mode: NULLABLE - Name: approvedSymbol Type: STRING Mode: NULLABLE - Name: approvedName Type: STRING Mode: NULLABLE - Name: yearOfFirstApproval Type: INTEGER Mode: NULLABLE - Name: usan_year Type: FLOAT Mode: NULLABLE - Name: patent_no Type: STRING Mode: NULLABLE - Name: max_phase_for_ind Type: FLOAT Mode: NULLABLE - Name: mesh_id Type: STRING Mode: NULLABLE - Name: mesh_heading Type: STRING Mode: NULLABLE - Name: efo_id Type: STRING Mode: NULLABLE - Name: efo_term Type: STRING Mode: NULLABLE - Name: tradeNames_list Type: STRING Mode: NULLABLE - Name: tradename_count Type: INTEGER Mode: NULLABLE - Name: syns_list Type: STRING Mode: NULLABLE - Name: synonyms_count Type: INTEGER Mode: NULLABLE - Name: ct Type: STRING Mode: NULLABLE Database Table 9: Drug Licensing (2,097 rows, 16 columns). Table: DrugTargets_LiscensingAndUses Description: Licensing, pharmaceutical company, and dosage information for specific drugs (FDA Purple Book). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: applicant Type: STRING Mode: NULLABLE - Name: blaNumber Type: INTEGER Mode: NULLABLE - Name: tradeName Type: STRING Mode: NULLABLE - Name: drugName Type: STRING Mode: NULLABLE - Name: blaType Type: STRING Mode: NULLABLE - Name: strength Type: STRING Mode: NULLABLE - Name: dosageForm Type: STRING Mode: NULLABLE - Name: routeOfAdministration Type: STRING Mode: NULLABLE - Name: productPresentation Type: STRING Mode: NULLABLE - Name: marketingStatus Type: STRING Mode: NULLABLE - Name: licensure Type: STRING Mode: NULLABLE - Name: submissionType Type: STRING Mode: NULLABLE - Name: licenseNumber Type: INTEGER Mode: NULLABLE - Name: productNumber Type: INTEGER Mode: NULLABLE - Name: center Type: STRING Mode: NULLABLE Database Table 10: Drug Dosages (211k rows, 11 columns). Table: DrugTargets_UsesAndDosages Description: Dosage, route of administration, and strength information for specific drugs (National Drug Code). Schema: - Name: UUID Type: STRING Mode: REQUIRED - Name: productType Type: STRING Mode: NULLABLE - Name: tradeName Type: STRING Mode: NULLABLE - Name: drugName Type: STRING Mode: NULLABLE - Name: dosageForm Type: STRING Mode: NULLABLE - Name: dosageRoute Type: STRING Mode: NULLABLE - Name: labelerName Type: STRING Mode: NULLABLE - Name: activeDosage_strength Type: STRING Mode: NULLABLE - Name: activeIngredient_strength Type: STRING Mode: NULLABLE - Name: mechanismOfAction_pharma Type: STRING Mode: NULLABLE - Name: packageDescription Type: STRING Mode: NULLABLE 14 A.2 Biological reasoning categories. Table 6 defines the biological reasoning categories that BiomedSQL challenges and Figure 4 shows their distribution among the set of 68,000 queries. The GWAS Significance, SMR Significance, and Functional Significance categories test the ability of LLMs to operationalize domain-specific statistical significance thresholds. Categories such as Approval Status, Genetic Target, and Trial Phase task LLMs with understanding and applying information about clinical trial phases for specific drugs and indications to generate correct SQL query. Table 6: Description of Biological Reasoning Categories in BiomedSQL. Bio Category Description Approval Status Trial Phase GWAS Significance SMR Significance Functional Significance Effect Genetic Target Allele Frequency Metadata Retrieves information about the FDA approval status of drug or indication. Retrieves information about the maximum clinical trial phase drug has reached. Identifies variants that are GWAS significant for disease (p < 5e-08). Identifies variants that are SMR significant for disease (p < 2.95e-06). Identifies variants that are significant within particular tissue for disease. Retrieves the effect size and direction for specific variants. Retrieves information about the genetic target of drug. Calculates allele frequencies for variant or set of variants. Retrieves general information about drug or genetic variant. Figure 4: Distribution of biological reasoning query types. A.3 Isolated SQL generation prompt templates. This section provides details about the prompt engineering approaches used for the isolated SQL query generation experiments. Prompt 1 contains the baseline prompt template that was used. For these experiments, the db_schema variable is replaced with the schema that is detailed in Appendix A.1. In the case of the 3-rows, 5-rows, and combo experiments, db_schema is replaced with schema that includes the corresponding number of example rows for each table in the database. Prompt 2 shows the example queries that were appended to the baseline prompt for use in the 1-shot, 3-shot, 5-shot, and combo experiments. Note that these example queries stay the same regardless of the question from BiomedSQL being passed. Prompt 3 details the statistical thresholding instructions that were appended to the baseline prompt for use in the stat-instruct and combo experiments. Finally, Prompt 4 contains the prompt template that was passed to the LLMs for the generation of final natural language response based on the question, generated SQL query, and execution results. This prompt was used throughout the isolated SQL generation experiments. 15 You are data analyst and SQL developer experienced with biomedical data in Google BigQuery. Your task is to translate the users natural language question into syntactically correct Google BigQuery SQL query. Users Natural Language Question: {question} Database Schema: {db_schema} Use these guidelines when generating the query: 1. Review the database schema. 2. Review the users question. 3. Generate valid Google BigQuery SQL query that answers the question based on the schema. 4. Always enclose table references in backticks, e.g. project.dataset.table. 5. Make use of BigQuery-specific functions and syntax where appropriate (e.g., DISTINCT, aliases, ORDER BY). 6. Always include the UUID column in your SELECT statements, except in cases of questions where the COUNT and ORDER BY functions are needed. 7. Unless the user explicitly requests different LIMIT, default your queries to LIMIT 100. 8. Output ONLY the raw SQL query (no additional commentary or explanations). 9. Avoid SELECT *; select only the necessary columns to answer the users query. 10. Ensure that any disease names that contain an apostrophe in the query are surrounded by double quotes (e.g., \"Alzheimers Disease\"). Please only return the SQL query in the following format: {{sql_query}} Prompt 1: Baseline prompt template for the isolated SQL generation experiments. Below are example BigQuery queries to guide you (for reference only, do not repeat verbatim unless needed by the users request): Example 1: SELECT DISTINCT drugName, drugIsApproved, newLinkedTargets_string FROM card-ai-389220.bio_sql_benchmark.DrugGeneTargets_ComprehensiveAnnotations_updated WHERE newLinkedTargets_string LIKE \"%TUBB %\" AND drugIsApproved = TRUE LIMIT 1000; Example 2: SELECT SNP, A1 AS effect_allele, freq AS effect_allele_frequency, A2 AS non_effect_allele, 1 - freq AS non_effect_allele_frequency FROM card-ai-389220.bio_sql_benchmark.AlzheimerDisease_CombinedGeneData_UUID WHERE SNP = rs61769339 LIMIT 10; Example 3: SELECT drugName, newLinkedTargets_string, drugIsApproved FROM card-ai-389220.bio_sql_benchmark.DrugGeneTargets_ComprehensiveAnnotations_updated WHERE newLinkedTargets_string LIKE %ACACA% AND drugIsApproved = TRUE LIMIT 1000; Example 4: SELECT topRSID, Disease, Gene, p_SMR_multi, p_HEIDI, b_SMR FROM card-ai-389220.bio_sql_benchmark.NeurodegenerativeDiseases_SMR_Genes_Full WHERE Disease = FTD AND Gene = ORC3 AND p_SMR_multi < 2.95e-6 LIMIT Example 5: SELECT DISTINCT drugName, tradeNames_list, drugType, actionType, target, approvedSymbol, approvedName, yearOfFirstApproval, max_phase_for_ind, mesh_heading, efo_term FROM card-ai-389220.bio_sql_benchmark.DrugTargets_IndicationsAndTherapeuticUses WHERE (LOWER(efo_term) = \"acute hepatic porphyria\" OR LOWER(mesh_heading) = \"acute hepatic porphyria\") AND LOWER(drugType) = \"oligonucleotide\" AND yearOfFirstApproval > 0 AND max_phase_for_ind = 4.0 LIMIT 100 Prompt 2: Example queries appended to the baseline prompt for the n-shot and combo experiments. Use the following p-value thresholds for questions about statistical significance: 1. < 5e-8 for genome-wide significance. 2. p_SMR < 2.95e-6 for SMR significance. 3. p_SMR < 2.95e-6, p_HEIDI > 0.01 for functional significance. Prompt 3: Statistical thresholding instructions appended to the baseline prompt for the stat-instruct and combo experiments. 16 You are data analyst and SQL developer experienced with biomedical data in Google BigQuery. Given the following question, SQL query, and SQL query execution results, please provide concise answer. Please do not use any information outside of the SQL query and SQL query execution results in your answer. Question: {question} SQL Query: {sql_query} Execution Results: {execution_results} Prompt 4: Natural language response prompt template for the isolated SQL generation experiments. A.4 ReAct prompt template. Prompt 5 shows the ReAct-style prompt template used in the interaction paradigm experiments. Similar to the baseline prompt db_schema is replaced with the schema that is detailed in Appendix A.1. history_str is replaced by the reasoning trace from previous steps that the LLM chooses to take. We allow the LLM to perform up to 5 iterations within the ReAct loop before final answer is generated. You are an expert SQL agent that uses step-by-step reasoning to answer questions about data in BigQuery database. IMPORTANT: The dataset name is \"{dataset_name}\". Always qualify table names with this dataset name. Example: SELECT * FROM {dataset_name}.table_name Follow these steps: 1. Think about how to translate the question into SQL query. 2. Decide which tables and columns are needed. 3. Write SQL query with explanatory comments. 4. Verify the query syntax before executing. 5. If the query has errors, fix them and try again. 6. Once the query is successful, explain the results clearly. 7. Always include the UUID column in your SELECT statements, except in cases of questions where the COUNT and ORDER BY functions are needed. 8. Unless the user explicitly requests different LIMIT, default your queries to LIMIT 100. 9. Avoid SELECT *; select only the necessary columns to answer the users query. 10. Ensure that any disease names that contain an apostrophe in the query are surrounded by double quotes (e.g., \"Alzheimers Disease\"). Your output MUST be JSON object with these fields: {{ \"thought\": \"Your reasoning about how to answer the question\", \"action\": \"One of verify_sql, execute_sql, or final_answer\", \"action_input\": \"For verify_sql/execute_sql: the SQL query; For final_answer: explanation of the results\" }} IMPORTANT: - Your response must include valid JSON that can be parsed. - Do not include any explanations outside the JSON object. - Always qualify table names with the dataset name \"{dataset_name}.\" Make sure your SQL queries follow BigQuery SQL syntax and include helpful inline comments. Question: {question} Database Schema: {db_schema} Reasoning History: {history_str} Continue the reasoning process with the next step: Prompt 5: ReAct prompt template for the interaction paradigm experiments. 17 A.5 BMSQL prompt templates. This section details the prompts used by our custom-built text-to-SQL system, BMSQL. Prompt 6 provides the template for the first step in the BMSQL pipeline, which is using the schema to identify relevant tables and columns to generate SQL query given the question. Once relevant columns are selected, Prompt 7 is used for BMSQL to generate first attempt at general SQL query that corresponds to the question. If the execution of this query fails, Prompt 8 is used to generate query that resolves any syntax errors present in the original query. BMSQL is given up to three retries to correct any syntax errors at this step. Prompt 9 is used to generate query that applies any statistical thresholding rules that may be necessary to answer the question. If no statistical thresholding is needed, the general query is returned once again. Using the execution results from both the general and refined query, BMSQL is asked to generate final response to the question given the instructions in Prompt 10. Finally, Prompt 11 is used in the inference time compute experiments to give BMSQL an opportunity to deem the final response as insufficient to answer the question and take subsequent passes through the pipeline. On these subsequent passes, BMSQL tends to correct any syntax errors but rarely makes structural changes to the generated SQL queries. The multi-stage query generation that BMSQL uses was designed to reflect how domain expert might query biomedical knowledge base by first checking if data is available for given query, and applying statistical thresholding on subsequent query. As seen throughout 5.2, BMSQL is top performer in terms of both execution metrics and response quality. You are BioMedical Domain Expert with deep database knowledge. You have the following database schema: {db_schema} The user has asked question about this biomedical data: \"{question}\" Your task: 1. Identify the single table or multiple tables (if absolutely necessary) that would provide the *full* answer to this question. 2. From these table(s), list *all columns* that might be relevant to fully answer the question. (Because downstream aggregator will handle details, do NOT omit columns that may be relevant.) Format your response **strictly** as: TABLE_NAME: col1, col2, col3, ... - Provide no extra commentary or text. - If multiple tables are truly needed, list each in new line, in the same format. Prompt 6: BMSQL prompt template for selecting relevant columns. You are highly proficient BigQuery SQL generator in the biomedical domain. Database schema: {db_schema} The user asked: \"{question}\" Previously identified relevant columns/tables:{relevant_columns} Instructions: - Generate exactly one valid BigQuery SQL query that retrieves all relevant columns from the relevant_columns list. - Do not filter out p-values, do not apply advanced thresholds unless the user explicitly stated them. - If the user mentions FDA approval, include those columns. - If the user mentions allele frequencies, include effect and non-effect allele freq columns. - FROM clause: {project_id}.{dataset_name}.table_name - Always include the UUID column in your SELECT statements, except in cases of questions where the COUNT and ORDER BY functions are needed. - Unless the user explicitly requests different LIMIT, default your queries to LIMIT 100. Return only the final SQL in markdown code block: sql {{sql_query}} Prompt 7: BMSQL prompt template for generating first attempt general SQL query. 18 You are SQL debugging assistant for Google BigQuery. Below is the database schema, the failed query, and the error message or unexpected results: === DATABASE SCHEMA START === {db_schema} === DATABASE SCHEMA END === === FAILED SQL QUERY START === sql {general_query} === FAILED SQL QUERY END === === ERROR OR RESULTS START === {general_results} === ERROR OR RESULTS END === The user originally asked: \"{question}\" Relevant columns identified for answering this question: {relevant_columns} Your task: - Analyze the failed query and the error or result details. - Generate corrected SQL query that resolves the issue, ensuring its correct for BigQuery and fits the schema. Format the corrected query as valid SQL query in markdown fenced block: sql {{sql_query}} Prompt 8: BMSQL prompt template for correcting failed first attempt general SQL query. You are skillful BigQuery SQL refiner. The user might want additional thresholds or see if theres advanced filtering needed, e.g. p-values or FDA approvals. Original question: \"{question}\" The previously generated SQL query was: sql {sql_query} The querys results (showing up to 10 rows): {resp_str} Database schema: {db_schema} Known threshold rules: {threshold_rules} If no extra thresholds or filters are implied, keep the same query. Otherwise, produce refined SQL with the new filters, returning it in markdown code block: sql {{sql_query}} Prompt 9: BMSQL prompt template for generating refined SQL query that applies thresholding rules if necessary. You are BioMedical Domain expert that is returning concise answer to the users question based on two sets of SQL queries and results. If not sure, say you do not know. Question: {question} SQL query 1: {sql_query_1} Result 1: {result_1} SQL query 2: {sql_query_2} Result 2: {result_2} Prompt 10: BMSQL prompt template for generating natural language response to the question. 19 You are biomedical domain and BigQuery expert that is determining if text-to-SQL workflow should be run again. Based on the question SQL queries, their execution results, and the final answer, determine if you are confident in the answer. Use the following guidelines: 1. If the SQL queries or answer contain errors, deem the answer as insufficient. 2. If you have any doubts about the SQL queries, execution results, or answer, deem the answer as insufficient. 3. If there are any inconsistencies between the SQL queries, execution results, and answer, deem the answer as insufficient. 4. Keep in mind that negative answer (i.e. \"No, ...\") does not necessarily mean the answer is insufficient. 5. Otherwise, use your best judgement. 6. Do not use any external information outside of what is provided. Question: {question} SQL query 1: {sql_query_1} Result 1: {result_1} SQL query 2: {sql_query_2} Result 2: {result_2} Answer: {answer} Please only return Yes if the answer is sufficient and No if it is insufficient. Prompt 11: BMSQL prompt template for determining if an answer is sufficient and taking subsequent pass at the pipeline if not. A.6 Evaluation metric definitions. We provide the formulaic definitions for the evaluation metrics described in 5.1. Execution Accuracy (EX). Given two sets of SQL execution results, the reference set Rn produced by the ground-truth queries, and the corresponding result set ˆRn produced by the LLM-generated queries, EX can be computed as follows: EX = (cid:80)N n=1 I(rn, ˆrn) if rn = ˆrn 0, otherwise (cid:26)1, and rn Rn, ˆrn ˆRn (1) (2) where I(rn, ˆrn) = Jaccard Index (JAC). Given two sets of SQL execution results, the reference set Rn produced by the ground-truth queries, and the corresponding result set ˆRn produced by the LLM-generated queries, JAC can be computed as follows: JAC = (cid:80)N n=1 J(rn, ˆrn) where J(rn, ˆrn) = rn ˆrn rn ˆrn and rn Rn, ˆrn ˆRn (3) (4) Syntax Error Rate (SER). Given set of LLM-generated SQL queries ˆRn resulting from questions in BiomedSQL, SER can be computed as follows: (cid:80)N E(ˆrn) n=1 exec(ˆrn) = Error SER = (cid:26)1, if 0, otherwise where E(ˆrn) = and ˆrn ˆRn and exec( ˆrn) is the result of running the generated SQL query on the database. 20 (5) (6) (7) BioScore. Prompt 12 contains the prompt template for generating BioScore, the LLM-as-a-judge metric used to grade the quality of natural language response compared to the gold standard response. This defines the BioScore(rn, ˆrn) function that is used in the RQR and SR equations below. You are highly knowledgeable and experienced expert in the healthcare and biomedical field, possessing extensive medical knowledge and practical expertise. Scoring Instructions for Evaluating Analyst Responses Objective: Evaluate an analysts response against gold standard. Scoring Criteria: - Exact Match: 3 points for an exact or equally accurate response. - Close Match: 2 points for very close response with minor inaccuracies. - Partial Match: 1 point for partially accurate response with significant omissions. - Irrelevant Information (Harmless): Deduct 0.5 points for harmless irrelevant information. - Irrelevant Information (Distracting): Deduct 1 point for distracting irrelevant information. - No Match: 0 points for no match. - Not Knowing Response: -1 point for stating lack of knowledge or abstaining. An example of this scenario is when Analyst Response says There are various studies, resources or databases on this topic that you can check ... but do not have enough information on this topic. Scoring Process: 1. Maximum Score: 3 points per question. 2. Calculate Score: Apply criteria to evaluate the response. Question: {question} Golden Answer: {gold_ans} Analyst Response: {pred_ans} Your grading Using the scoring instructions above, grade the Analyst Response. Return only the numeric score on scale from 0.0-3.0. If the response is stating lack of knowledge or abstaining, give it -1.0. Please respond only with the score. Prompt 12: BioScore prompt template. Response Quality Rate (RQR). Given two sets of natural language responses, the reference set Rn which map to questions in BiomedSQL, and the corresponding result set ˆRn containing LLM-generated responses, RQR can be computed as follows: where Quality(rn, ˆrn) = RQR = (cid:80)N n=1 Quality(rn, ˆrn) (cid:26)1, if BioScore(rn, ˆrn) 2 0, otherwise and rn Rn, ˆrn ˆRn (8) (9) Safety Rate (SR). Given two sets of natural language responses, the reference set Rn which map to questions in BiomedSQL, and the corresponding result ˆR containing LLM-generated responses, SR can be computed as follows: SR = (cid:80)N (cid:80)N n=1 n=1 A(rn, ˆrn) B(rn, ˆrn) where A(rn, ˆrn) = (cid:26)1, if BioScore(rn, ˆrn) = 1 0, otherwise (10) (11) and B(rn, ˆrn) = (cid:26)1, if Bioscore(rn, ˆrn) < 2 0, otherwise and rn Rn, ˆrn ˆRn (12) A.7 Correlation between SQL execution metrics and BioScore metrics. In order to further motivate the use of execution-based and BioScore-based response quality metrics, we present discussion of the correlation between the two. Figure 5 shows heatmaps of both EX (left) and binned JAC (right) compared to our LLM-as-a-judge metric BioScore for the baseline experiment using GPT-o3-mini. From this figure, it is clear there are many cases where EX is 0 or JAC is less than 0.5 and perfect BioScore is still achieved. This can happen for variety of reasons, including the presence of negative answers within BiomedSQL (i.e. associations with no significant variants or drug targets without approval), questions where superset of the correct rows can be returned with partially correct SQL query (i.e. using correct ORDER BY clause without applying the correct thresholding values), and cases where there are multiple valid SQL solutions (as disucssed in 7). In these cases the LLM may generate query that scores poorly in terms of execution metrics but scores adequately in terms of response quality. To mitigate concern for these cases and quantify the association between the execution metrics and BioScore we perform Cramers test which reveals moderate-to-strong, statistically significant association for both EX (V=0.48, p=1.84e25) and JAC (V=0.37, p=3.23e39). This association indicates that even though an LLM may be able to generate correct natural language responses with incorrect or partially correct SQL queries, systems that get higher execution scores will generally get higher BioScore response quality metrics as well. Figure 5: Heatmaps visualizing the association between (left) EX and BioScore and (right) JAC and BioScore. A.8 Prompt experiments. Table 7 shows the results of the top performing baseline model, GPT-o3-mini, across the range of prompting experiments described in 5.1. As discussed in 5.2, combo is the only prompting experiment that provides substantial gains in terms of both execution metrics and response quality compared to the baseline. Table 7: Performance of GPT-o3-mini in an isolated SQL generation setting using additional prompts. Model EX (%) JAC (%) RQR (%) SR (%) SER (%) # Tokens 3-rows 5-rows 1-shot 3-shot 5-shot stat-instruct combo 54.0 (4.2) 54.2 (4.2) 54.0 (4.2) 56.0 (4.2) 57.9 (4.1) 57.5 (4.1) 59.0 (4.1) 61.8 (3.8) 61.9 (3.8) 61.7 (3.8) 64.2 (3.7) 65.6 (3.7) 64.2 (3.7) 66.1 (3.7) 75.1 (3.6) 76.7 (3.5) 73.4 (3.7) 74.0 (3.7) 75.8 (3.6) 73.4 (3.7) 77.8 (3.5) 16.9 (3.1) 15.7 (3.0) 32.4 (3.9) 33.1 (3.9) 23.5 (3.5) 31.7 (3.9) 24.0 (3.6) 0.0 (0.0) 0.2 (0.4) 0.4 (0.5) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.2 (0.4) 10,951 14,312 4,058 4,099 4,566 3,456 10,284 22 A.9 Expected compute resources. Table 8 details the compute resources needed to reproduce all of the described experiments. The times listed are the exact execution times from running our experiments but may vary slightly when reproducing results depending on API status and compute resources utilized. Table 8: Compute resources needed to reproduce all experiments. CPUs are Intel Xeon Gold 6140 Processors and GPUs are NVIDIA A100 80GB Tensor Cores. Times are listed in terms of Hours:Minutes. *Indicates that all experiments in the category have the same compute/memory requirements. Experiment Model Compute Memory Time Baseline Prompt Variations Interaction Paradigms Inference-time Compute GPT-4o GPT-4o-mini GPT-o3-mini Gemini-2.0-flash Gemini-2.0-flash-lite Qwen-2.5-Coder-14B Qwen-2.5-Coder-32B Llama-3.1-70B Llama-3.1-405B Claude-3.7-sonnet 3-rows 5-rows 1-shot 3-shot 5-shot stat-instruct combo ReAct-GPT-4o ReAct-GPT-o3-mini ReAct-Gemini Index-GPT-4o Index-GPT-o3-mini Index-Gemini BMSQL-GPT-4o BMSQL-GPT-o3-mini BMSQL-Gemini 1-pass 2-pass 3-pass 2 CPUs 2 CPUs 2 CPUs 2 CPUs 2 CPUs 2 GPUs 2 GPUs 3 GPUs 2 CPUs 2 CPUs 16GB RAM 16GB RAM 16GB RAM 16GB RAM 16GB RAM 32GB VRAM 64GB VRAM 140GB VRAM 16GB RAM 16GB RAM 2 CPUs* 16GB RAM* 2 CPUs* 16GB RAM* 2 CPUs* 16GB RAM* 0:44 1:35 4:38 0:40 0:37 1:40 2:29 3:17 3:21 1:21 4:18 4:07 4:16 4:09 6:30 2:57 2:38 2:01 4:25 2:10 1:13 3:22 1:11 1:44 5:17 1:05 5:17 6:27 6:"
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: Claims and results presented in the abstract and introduction accurately reflect our contribution and are backed by experimental results in 5.2. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are discussed in 7. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: [NA] Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed description of our experiments in 5.1, detailed prompts in Appendix A.3, A.4, and A.5, as well as well-documented GitHub respository and zip file of our executable code as part of the Supplementary Material. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 25 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide links to publicly available HuggingFace and GitHub repositories containing our benchmark data and executable code, respectively. Users will need to provide API keys for services such as Azure, OpenAI, Anthtropic, and Gemini. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We adequately describe our experimental setup in 5.1 and provide executable code. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide confidence intervals for all tabular results reported and discuss significance in 5.2 and 6. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 26 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on compute resources needed to reproduce the experiments in Appendix A.9. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We abide by the NeurIPS code of ethics throughout the research process and do not foresee any potential societal harms coming from this research as disucssed in 7. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss positive societal impacts of this work in 1 and 8 and do not forsee any negative societal impacts as disucssed in 7. Guidelines: 27 The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credit all owners of data assets used in References and discuss data licenses in Appendix A.1. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. 28 For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New dataset and code assets are well-documented and provided in publicly available repositiores. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: We describe the specific LLMs used and experiments performed with those LLMs in 5.1. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/ LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Center for Alzheimers Disease and Related Dementias, NIA, NIH",
        "DataTecnica LLC",
        "Johns Hopkins University",
        "Laboratory of Neurogenetics, NIA, NIH"
    ]
}