{
    "paper_title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
    "authors": [
        "Songqiao Su",
        "Xiaofei Sun",
        "Xiaoya Li",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2"
        },
        {
            "title": "Start",
            "content": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li and Chris Shum"
        },
        {
            "title": "DeepReinforce Team",
            "content": "github.com/deepreinforce-ai/CUDA-L"
        },
        {
            "title": "Abstract",
            "content": "Matrix multiplication (matmul) is one of the most fundamental operations in LLMs. However, manually optimizing Matmul kernels is challenging due to the fact that different matrix dimension (M, N, K) require different optimization strategies and that optimizations rarely transform across different GPU architectures, which make comprehensive manual tuning hard at scale. In this paper, we propose CUDA-L2, system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. These configurations represent all 103 combinations of M, N, values from {64, 128, 256, 512, 1024, 2048, 4096, 8192, 12288, 16384}, and already covers those used in attention and FFN layers of widely open-sourced models like Qwen, Llama and DeepSeek. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art Nvidias closed-source libraries, i.e., cuBLAS, cuBLASLt. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8% over cuBLASLt-heuristic, which queries cuBLASLt library and selects the algorithm based on the heuristics suggestion; and +11.4% over the most competitive cuBLASLt-AutoTuning model, which selects the fastest algorithm from up to 100 candidates from cuBLASLts suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. While the current version of CUDA-L2 only focuses on A100 GPUs, the framework is designed for broad applicability, with ongoing work to extend it to other GPU architectures, including Ada Lovelace, Hopper and Blackwell. (cid:66) 5 2 0 2 ] . [ 1 1 5 5 2 0 . 2 1 5 2 : r Figure 1: Benchmark results on 1,000 (M, N, K) configurations for Half-precision General Matrix Multiply (HGEMM). We report CUDA-L2s performance against torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning for NN, TN, and max(NN,TN) layouts: (a) Offline scenario: kernel executed consecutively without time intervals. (b) Server scenario: kernel executed at intervals, simulating real-time inference. (cid:66) Email: {songqiao_su, xiaofei_sun, xiaoya_li, albert_wang, jiwei_li, chris_shum}@deep-reinforce.com"
        },
        {
            "title": "Introduction",
            "content": "Matrix multiplication (matmul) is one of the most fundamental operations in deep learning and LLMs and accounts for very significant portion of computation time in both training and inference. Consequently, its CUDA kernels are among the most deeply optimized by human experts. However, manually optimizing matmul kernels across diverse matrix sizes remains challenging due to the vast configuration space and architecture-specific constraints: different matrix dimension triplets (M, N, K) may require distinct optimization strategies, and these strategies rarely transfer across GPU architectures due to different hardware characteristics. Moreover, even for identical matrix dimensions on the same GPU, optimal kernel can be different depending on setups such as accumulator precision, where using FP16 versus FP32 accumulators (both valid choices for FP16 inputs) leads to different register pressure and thus different optimization strategies. Therefore, despite being deeply optimized, matmul is far from solved problem, and significant performance gains are still possible. This is evidenced by the 13% speedup from targeted Grouped GEMM optimizations for the deepseek-R1 model by the Tensor-RT LLM team1. This speedup was achieved when human expertise was narrowly focused on single target. Systematically improving matmul kernels calls for automated optimization methods that can scale across the vast space of problems and GPU architectures. Rapid advancements in LLMs [1, 3, 5, 15, 17] open the door to automatic kernel generation, and the past year has witnessed growing interest in harnessing LLMs, especially RL-augmented LLMs, for autonomous CUDA kernel generation [10, 2, 7, 6, 16]. Existing work, such as Sakana AIs AI CUDA Engineer [6] and CUDA-L1 [7], primarily optimize kernels from benchmarks like KernelBench [10], which covers diverse range of CUDA operations, each evaluated on single, fixed configuration (e.g., one specific input dimension). However, it remains unclear how these benchmark-optimized kernels translate to realworld production environments. To the best of our knowledge, no prior work has achieved performance comparable to manually-optimized matmul kernels, particularly when compared against NVIDIAs highly-optimized cuBLAS library. In this work, we propose CUDA-L2, system that combines LLMs and RL to automatically optimize matmul CUDA kernels. CUDA-L2 covers 1,000 configurations that represent all 103 combinations of M, N, values from {64, 128, 256, 512, 1024, 2048, 4096, 8192, 12288, 16384}, the size of which already covers those used in attention and FFN layers of open-sourced models like Qwen [17], Llama [4], and DeepSeek [5]. To tackle the challenging task of matrix multiplication, CUDA-L2 extends CUDA-L1 through several technical enhancements, including (1) continued pretraining with more diverse CUDA code; (2) multi-stage RL training progressing from broad (general kernel types) to specialized domains (matmul-specific kernels); (3) including more comprehensive NCU (NVIDIA Nsight Compute) profiling metrics (e.g., memory throughput, SM occupancy, cache efficiency) in the context for better optimization decisions; and (4) incorporating retrieval-augmented context to accommodate new knowledge or architectural characteristics not covered in the foundation model. CUDA-L2 systematically outperforms major matmul baselines to date, ranging from the widely-used torch.matmul to stateof-the-art optimized libraries (cuBLAS, cuBLASLt). We evaluate under two scenarios: (1) offline, where kernels are executed consecutively without time intervals, and (2) server, where kernels are executed at random intervals simulating real-time inference. In the offline scenario, across 1,000 configurations, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using its optimal layout configuration (NN or TN); +16.8% over cuBLASLt-heuristic, which queries the cuBLASLt library and selects the algorithm based on the heuristics suggestions; and +11.4% over the most competitive cuBLASLt-AutoTuning setup, which selects the top-performing algorithm from up to 100 candidates from cuBLASLts heuristic API. In the server scenario, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even for the most performance-critical and highly optimized CUDA kernels like HGEMM, LLM-guided RL can discover superior implementations by scaling optimization across configuration spaces, the scale of which is difficult for manual tuning. While the current version of CUDA-L2 is limited to A100 Ampere architectures, the framework is general one and designed for broad applicability, with ongoing work to extend it to additional GPU architectures, including Ampere (e.g., RTX 3090), Ada Lovelace (e.g,RTX 4090), Hopper (e.g., H100), and Blackwell (e.g., B200)."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we first describe preliminaries for developing HGEMM kernels. 1https://nvidia.github.io/TensorRT-LLM/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_ Throughput_on_NVIDIA_Blackwell_GPUs.html"
        },
        {
            "title": "2.1 HGEMM",
            "content": "HGEMM Half-precision General Matrix Multiplication (HGEMM) is one of the most widely used matmul kernels in current LLMs. It computes the matrix product = αAB + βC using 16-bit arithmetic. An (M, N, K) triplet denotes dimension configuration for HGEMM, where matrix RM and matrix RKN . In practice, the common case is α = 1 and β = 0, yielding = AB, which we focus on in this work. We adopt this setup throughout the paper. An (M, N, K) triplet denotes dimension configuration where RM and RKN . Modern HGEMM implementations follow tiled approach to maximize data reuse. The computation is organized into three main phases: First, the matrices and are divided into smaller tiles. Each thread block computes one tile of the output matrix C, with dimensions BM BN. Within each thread block, tiles are further subdivided to match the GPUs tensor core dimensions. Second, during the mainloop, data flows through multiple memory levels. Tiles of size BM BK from and BK BN from are loaded from global memory into shared memory, then from shared memory into registers. Once in registers, tensor cores perform the actual matrix multiply-accumulate operations. This process repeats across all tiles along the dimension, with partial results accumulated in registers. Finally, in the epilogue phase, the accumulated results are written back from registers to shared memory, and then from shared memory to global memory."
        },
        {
            "title": "2.2 Baselines for Comparison",
            "content": "2.2.1 torch.matmul PyTorchs torch.matmul naturally constitutes baseline. PyTorchs implementation automatically selects optimized kernels based on input dimensions and data types. For half-precision operations, it dispatches to cuBLAS internally but includes additional overhead from PyTorchs tensor dispatch system and memory management. torch.matmul represents standard and robust practice for the majority of users who rely on default PyTorch operations. 2.2.2 cuBLAS NVIDIAs cuBLAS library provides strong optimized baseline and delivers high performance without requiring any manual tuning. We evaluate two most widely-used matrix layouts: cuBLAS-NN (all row-major) and cuBLAS-TN (mixed layout). For each (M, N, K) configuration, we also compare CUDA-L2 with cuBLAS using the optimal layout with respect to each dimension, i.e., max(cuBLAS-NN, cuBLAS-TN), denoted by cuBLAS-max. We use the cublasGemmEx function offered by NVIDIAs cuBLAS library with the CUBLAS_GEMM_DEFAULT_TENSOR_OP operation to enable Ampere FP16 Tensor Cores and allow cuBLASs internal heuristics to automatically select the optimal algorithm. Code snippets for the cuBLAS-NN and cuBLAS-TN baselines are provided in Listing 6 in Appendix. 2.2.3 cuBLASLt cuBLASLt provides lower-level, more controllable interface to NVIDIAs optimized GEMM kernels than the higher-level cuBLAS. Unlike cuBLASs black-box heuristics that hide algorithm selection, cuBLASLt exposes all available algorithm variants to developers and allows for explicitly enumerating and evaluating them. Though this enables tuning that can yield additional speedups over cuBLASs defaults, it also demands substantial expertise and tuning effort. We compare CUDA-L2 with two cuBLASLt setups based on API offered by Nvidia, cuBLASLt-heuristic and cuBLASLt-AutoTuning. cuBLASLt-heuristic The heuristic strategy uses NVIDIAs cublasLtMatmulAlgoGetHeuristic API, which is the standard recommended approach for cuBLASLt optimization. For each matrix configuration, it queries the library for algorithm recommendations and selects the top-ranked algorithm from the heuristics suggestions, which is simply the recommendation with index 0. Similar to cuBLAS, for each dimension , we benchmark CUDA-L2 against cuBLASLt-heuristic-NN, cuBLASLtheuristic-TN and cuBLASLt-heuristic-max. To avoid the overhead introduced by calling the API repeatedly during evaluation, the algorithm selection for NN and TN is performed beforehand. The code for cuBLASLt-heuristic-TN is shown in Listing 7. cuBLASLt-AutoTuning AutoTuning2 employs an exhaustive strategy that first retrieves up to 100 algorithm candidates for each (M, N, K) from cuBLASLts heuristic API cublasLtMatmulAlgoGetHeuristic. Next, each returned algorithm is evaluated with randomized execution order and random input matrixes. The fastest algorithm is selected based on median execution time, as suggested by cuBLASLts documents. Again, both NN and TN layouts are tested, using whichever achieves faster speed. Selected algorithms are cached to eliminate overhead during evaluation. 2https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLASLt/LtSgemmSimpleAutoTuning"
        },
        {
            "title": "2.3 Kernel Successfulness",
            "content": "A custom HGEMM kernel is successful if it is both executable and correct."
        },
        {
            "title": "2.3.1 Executability",
            "content": "A kernel is executable if it successfully compiles, launches, and executes to completion within reasonable time limit. compute-sanitizer tool memcheck is used to check for memory access violations."
        },
        {
            "title": "2.3.2 Correctness",
            "content": "To valid kernel correctness, we need to compare its output to reference correct kernel with the same inputs. For HGEMM, we use the FP32 CPU implementation as the reference kernel due to its well-established correctness and numerical stability: ref = MatMul(float(a.cpu()), float(b.cpu()) (1) Theoretically, kernel is correct if it produces outputs identical to those of reference implementation. However, exact equivalence is usually impossible on GPUs due to the non-associative nature of floating-point arithmetic, i.e., (a + b) + = + (b + c). To address this, we adopt the following two practical criterion: Exact Match with binary Inputs. We randomly generate matrices A, with elements being binary in {0, 1}. For matrix multiplication = B, each output element is: cij = (cid:88) k=1 aik bkj Since aik, bkj {0, 1}, each product is either 0 or 1, and the sum is guaranteed to be non-negative integer. We first compute the reference output ref using FP32 on CPU, which provides exact integer results. We then compute the output custom using the custom kernel. For each position (i, j) where cref ij exactly, and we ignore positions cref ij > 2048. This is because half-precision has 10 mantissa bits plus 1 implicit leading bit, yielding 11 bits of significand precision. Thus all integers in [0, 2048) are exactly representable in half-precision, while integers 2048 may not be. Crucially, since each term aik bkj {0, 1}, the partial sums are monotonically non-decreasing. If the final element value is below 2048, all intermediate values must also be below 2048, ensuring exactness for all intermediate steps when doing the addition for (cid:80) aik bkj.3 4 We can adjust the binary probability for {0, 1} based on matrix sizes to ensure significant proportion of cref ij is below 2048 but larger than 0. We repeat this process multiple times with different random inputs; if any single iteration fails, the kernel is considered incorrect. ij < 2048, we require ctest ij = cref Baseline-Bounded Deviation We select set of highly reliable baseline kernels developed by Nvidia: including cuBLAS-NN, cuBLAS-TN, cuBLAS-TN, cuBLASLt-heuristic-NN, cuBLASLt-heuristic-TN, cuBLASLt-AutoTuning-NN, cuBLASLt-AutoTuningTN. For each test input, we compute the maximum elementwise difference among the outputs of these baseline kernels, which reflects the upper bound of variability for floating-point computations. custom kernel is considered incorrect if its maximum elementwise deviation from the reference output exceeds this baselines value."
        },
        {
            "title": "2.4 Evaluation",
            "content": "To benchmark custom kernel against reference kernel, we first define the single-run speedup s(custom) as: s(custom) = tref tcustom 1 (2) where tref and tcustom denote the execution time of the reference and custom kernels, respectively. Each run execute both the custom and reference kernels. To account for GPU performance variability, each evaluation runs for minimum of 30 seconds after 10-second warmup period. In each iteration, we randomize execution order to eliminate ordering effect. The final evaluation score is the mean speed score over all runs. 3As side note, sampling elements from {1, 0, 1} does not guarantee this property, since the partial sums are no longer monotonically non-decreasing. There can be cases where an intermediate sum exceeds 2048 (exactness already lost) before later cancellations bring the final result below 2048. 4One might consider using fractional values such as 1/2, 1/4, or 1/8, but this does not fundamentally change the analysis as it merely shifts the threshold. For example, in FP16, the exactness threshold depends on the smallest representable increment at given magnitude. Take 1/2 as an example, if elements are sampled from {0, 1/2, 1}, partial sums may be half-integers and the unit in the last place (ULP) equals 1/2 only for magnitudes in [512, 1024); once the magnitude reaches [1024, 2048), the ULP increases to 1, meaning half-integers can no longer be represented exactly and are rounded to the nearest integer. Therefore, the exactness threshold for half-integer sums is 1024."
        },
        {
            "title": "2.4.1 Avoiding Timing Measurement Hacking",
            "content": "1 torch.cuda.synchronize() 2 start_event = torch.cuda.Event(enable_timing=True) 3 end_event = torch.cuda.Event(enable_timing=True) 4 start_event.record() 5 kernel(a, b, b_col_major, out) 6 end_event.record() 7 torch.cuda.synchronize() 8 elapsed_time_ms = start_event.elapsed_time(end_event) Listing 1: Execution time for HGEMM kernels. Code for measuring the execution time for HGEMM kernels is shown in Listing 1, following the standard kernel timing strategy adopted in [7, 10]. [7] identified two strategies to hack this evaluation during RL training: (1) creating additional CUDA streams that execute asynchronously; and (2) pythons lazy mode where the calling of the kernel function does not ensure the output is actually materialized/computed. CUDA-L2 avoids both issues by (1) disallowing additional CUDA stream creation; (2) generating HGEMM kernels only as CUDA code in .cu files, which naturally bypasses Pythons lazy evaluation."
        },
        {
            "title": "2.4.2 Offline v.s. Server Mode",
            "content": "Following MLPerf [9, 12], widely-used benchmark criteria for machine learning performance, kernels evaluations are performed under two scenarios: (1) offline, where kernels are executed back-to-back without pauses, and (2) server, where kernels are executed at intervals to simulate real-time inference. Its worth noting that the interval time is not included in the execution time used for the speedup comparison in server mode. The offline scenario measures peak throughput by keeping the GPU fully busy, while the server scenario reflects real-world deployment where requests arrive at unpredictable times. Performance differs between them because back-to-back execution in offline mode keeps GPU caches warm with GPU running at full speed, while in server mode, GPUs caches can cool down and then start from cold stat."
        },
        {
            "title": "3.1 A Brief Review of CUDA-L1",
            "content": "CUDA-L1 is pipelined system described in [7] for optimizing kernels in KernelBench [10], benchmark comprising 250 diverse CUDA kernel tasks spanning from basic operators to full model architectures. CUDA-L1 consists of three components: (1) an SFT module that fine-tunes pre-trained LLM on kernels generated by existing LLMs; (2) self-supervised learning module that trains the LLM on successful self-generated kernels; and (3) contrastive RL module that uses kernel execution speed as the reward signal. However, CUDA-L1 has limitations that hinder its effectiveness on the more challenging HGEMM task. First, the SFT stage only fine-tunes on KernelBench kernels, limiting the models ability to generalize to other kernel types. Second, the pre-trained LLM has limited access to the most recent knowledge critical to HGEMM optimization, such as newer CUTLASS versions, the CuTe library, and new GPU architectures."
        },
        {
            "title": "3.2 CUDA-L2",
            "content": "CUDA-L2 extends CUDA-L1 with the following key technical enhancements: (1) continued pretraining with more diverse CUDA code; (2) multi-stage RL training progressing from broad (general kernel types) to specialized domains (matmul-specific kernels); (3) including more comprehensive NCU profiling metrics (e.g., memory throughput, SM occupancy, cache efficiency) in the context for better optimization decisions; and (4) incorporating retrieval-augmented context to accommodate new knowledge or architectural characteristics not covered in the foundation model."
        },
        {
            "title": "3.2.1 Continued Pretraining",
            "content": "To enable more general-purpose CUDA optimization, we extend pretraining beyond KernelBench to diverse CUDA code covering wide range of kernel types. We collect CUDA code from two sources: (1) web sources, which we clean, extract and segment using combination of rule-based filtering and LLM-based cleaning; and (2) implementations from established libraries including PyTorch, ATen, CUTLASS, Nvidias tutorials and examples, etc. For web-sourced code, since instruction tuning [11, 14] requires descriptive prompts that do not naturally accompany raw CUDA code, we use Claude Sonnet 4 to 5 generate instruction descriptions for each CUDA code snippet. To further augment the model with retrieval capabilities, we use each instruction as search query to retrieve relevant documentation and code examples in the search engine, which are concatenated as additional context. The resulting instruction-context-code triplets are used for continued pretraining on DeepSeek 671B [8], enabling the model to acquire more general CUDA optimization capabilities."
        },
        {
            "title": "3.2.2 General Kernel RL",
            "content": "In the continued pretraining stage, we collected roughly 1K CUDA kernels from established libraries, covering large range of operations including linear algebra, convolution operations, reduction operations, element-wise operations, attentions, sampling, and others that cannot be readily categorized (e.g., embedding lookups, loss functions, gradient clipping, optimizer steps). Each CUDA kernel is paired with an official/successful implementation from Pytorch, ATen, CUTLASS, etc. With reference implementations of these kernels, we train general-kernel LLM-guided RL with the reward being the average speedup score across all test iterations. CUDA-L2 adopts contrastive RL training strategy described in [7], where the model is prompted to perform comparative analysis of previously generated CUDA variants and their execution performances. GRPO [5, 13] is adopted for LLM parameter updates. As suggested in [7], rewards will be smoothed and clipped to alleviate the effect of reward hacking during RL training."
        },
        {
            "title": "3.2.3 HGEMM RL",
            "content": "In the last stage, we continue to train RL by limiting kernels to HGEMM with varying configurations of M, N, K. The contrastive RL strategy is adopted. Reward The reward for generated HGEMM kernel is computed by averaging speedup scores across all test iterations, similar to the evaluation section. To foster correctness, we also penalize the reward by numerical deviation from the FP32 CPU ground truth. Code length will further be penalized, pushing the model to generate concise code. r(custom) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:20) ti ref ti custom (cid:21) α diffi βL(custom) (3) where diffi = maxj outi coefficient. L(custom) denotes the length of the generated code and β > 0 is the associative penalty coefficient. custom[j] is the maximum element-wise absolute difference and α > 0 is the penalty FP32[j] outi NCU (NVIDIA Nsight Compute) profiling metrics are incorporated in the context for HGEMM training, enabling the model to learn from detailed performance characteristics such as memory throughput, compute utilization, warp occupancy, and cache hit rates rather than relying solely on end-to-end execution time. Generated HGEMM kernels will be parsed into .cu files and compiled with nvcc. Therefore, CUDA C/C++, CuTe, inline PTX assembly, CUDA intrinsics, and CUTLASS templates can be used, but not Python-based DSLs like Triton."
        },
        {
            "title": "4 Results",
            "content": "In this section, we present the performance of CUDA-L2 against baseline kernels."
        },
        {
            "title": "4.1 Overall Results",
            "content": "Comparisons within Baselines Results are shown in Table 1. Before examining CUDA-L2s performance, we first compare the baselines against each other. Since the table shows CUDA-L2s speedup over each baseline, higher percentage indicates weaker baseline. Comparing TN (transposed-normal) and NN (normal-normal) layouts, the NN layout slightly outperforms TN across all libraries. Specifically, for cuBLAS, CUDA-L2 gains 20.0% over NN versus 21.4% over TN. The same pattern holds for cuBLASLt-heuristic (17.3% vs 19.1%) and cuBLASLt-AutoTuning (12.1% vs 13.3%). Among the baseline libraries, torch.matmul is the weakest (22.0% speedup). cuBLAS-max performs better (19.2%) by selecting the optimal layout per configuration. cuBLASLt-heuristic improves further (16.8%) through algorithmic selection based on problem characteristics. The strongest baseline is cuBLASLt-AutoTuning (11.4%), which exhaustively test up to 100 kernel candidates per configuration. 6 Table 1: Performance comparison of CUDA-L2 against baseline implementations (speedup ratio). Offline denotes that case where kernels are executed back-to-back without pauses, and server denotes kernels being executed at random intervals to simulate real-time inference. max denotes comparing against the best kernel across TN and NN for each configuration. CUDA-L2 Performance CUDA-L2 consistently outperforms all baselines, from the commonly used torch.matmul to NVIDIAs heavily-optimized cuBLASLt-AutoTuning. In offline mode, CUDA-L2 achieves average speedups of 22.0% over torch.matmul, 19.2% over cuBLAS-max, 16.8% over cuBLASLt-heuristic-max, and 11.4% over cuBLASLt-AutoTuning-max. In server mode, these gains increase to 28.7%, 26.0%, 22.4%, and 15.9% respectively. The win rates span 79.3% to 95.7% across all baselines, confirming that improvements are systematic rather than driven by outliers. It is also worth noting the difference between the offline and server modes. We attribute this to the difference in GPU thermal dynamics for GPU environment between the two modes. In the offline model, continuous execution keeps the GPU in steady thermal state with predictable clock behavior, while in the server mode, idle periods between requests allow the GPU to cool, causing subsequent kernel launches to experience clock boosting followed by thermal throttling. The explains the larger variance of the server mode v.s. offline mode."
        },
        {
            "title": "4.2 Max(CUDA-L2, baseline)",
            "content": "Table 2: Comparing max(CUDA-L2, baseline) against baseline, which represents real-world situation where users have access to all kernels and can choose whichever runs faster. It is interesting to consider real-world scenario where users have access to all kernels and can choose whichever works best. Table 2 presents the mean speedup comparing max(CUDA-L2, baseline) against baselines, where max(CUDA-L2, baseline) denotes selecting the faster kernel between CUDA-L2 and the baseline for each configuration. As expected, max(CUDA-L2, baseline) provides additional gains across all baselines. In offline mode, the speedup increases from 22.0% to 23.1% for torch.matmul, from 19.2% to 20.2% for cuBLAS-max, from 16.8% to 17.0% for cuBLASLt-heuristic-max, and from 11.4% to 13.2% for cuBLASLt-AutoTuning-max. The server mode follows the same pattern, with speedups rising from 28.7% to 29.8%, 26.0% to 27.2%, 22.4% to 22.7%, and 15.9% to 18.1% respectively. 7 Table 3: Speedup over cuBLASLt-AutoTuning-max by matrix size in offline mode."
        },
        {
            "title": "4.3 Speedup vs. Problem Size",
            "content": "Figure 3 shows how CUDA-L2s speedup against cuBLASLt-AutoTuning-max varies with matrix dimensions. We observe clear trend that as problem size increases, the speedup decreases. In the left panel, speedup drops from around 1.4 for smaller problems (log2(M K) 18-20) to approximately 1.0 for larger problems (log2(M K) > 38). The middle and right panels confirm this pattern. When plotting against average dimension or maximum dimension, smaller matrices consistently show higher speedups (1.3-1.4), while larger matrices converge toward the baseline performance. This behavior is expected. For small matrix multiplications, the GPU is underutilized and there is significant room for optimization through better memory access patterns, tiling strategies, and kernel configurations. CUDA-L2 exploits these opportunities effectively. On the contrary, large matrices saturate the GPUs floating-point throughput, leaving less room for improvement."
        },
        {
            "title": "5 Optimization Techniques Discovered and Applied by CUDA-L2",
            "content": "In this section, we describe CUDA optimization techniques discovered and applied by CUDA-L2."
        },
        {
            "title": "5.1 Abstraction Selection",
            "content": "We first observe that CUDA-L2 automatically selects appropriate implementation abstractions based on (M, N, K). For smaller matrices, it generates lightweight kernels directly using raw WMMA. These kernels employ simpler memory access patterns, fewer pipeline stages, and minimal synchronization, which lead to faster speed. For larger matrices, CUDA-L2 leans towards using CuTes abstractions to manage sophisticated tiled MMA operations with multi-stage pipelining. CuTes abstractions result in more concise code compared to raw CUDA intrinsics. Since the RL reward favors shorter code, CUDA-L2 naturally leans towards CuTe for complex optimizations. This allows CUDA-L2 to more easily explore advanced techniques such as multi-stage pipelining, tile shape tuning, and swizzle patterns, which yield significant speedups for large matrices."
        },
        {
            "title": "5.2 Padding the Input Matrix with Zeros",
            "content": "Matrix multiplication on GPUs is typically implemented using tiled approach, where the two input matrices are partitioned into smaller tiles that can be efficiently processed by thread blocks. Each thread block loads tile of the input matrices into shared memory, performs local computation, and writes the result back to global memory. In this tiled approach, each thread block computes BM BN output tile. common constraint is that the matrix dimension must be divisible by BM and must be divisible by BN; otherwise, thread blocks handling boundary tiles may issue out-of-bounds memory accesses. For example, when = 8192, typical choices include BM {64, 128, 256}. We observe an interesting strategy discovered by CUDA-L2: setting BM to values where is not divisible by BM, then padding the input matrix with zeros to make larger to satisfy the divisibility constraint. This provides the flexibility to explore bigger set of BM values beyond those that evenly divide the original dimension. While padding introduces computational overhead from processing additional rows, the RL model automatically finds the sweet spot of balancing this overhead and the performance gains by selecting more effective BM. 8 For example, in the generated kernel for (M=8192, N=512, K=2048) , CUDA-L2 selects BM = 160, which does not divide 8192, and pads to 8320 (about 1.6% overhead), ultimately outperforming the conventional choice of BM = 128. The speedup over cublaslt-AutoTuning-TN is +15.2% when BM is set 160, and decreases to 0.4% when BM is set 128 and -15.7% when BM is set 256."
        },
        {
            "title": "5.3 Proficiency in Using CUDA Optimization Techniques",
            "content": "We observe that CUDA-L2 is proficient in using the following CUDA optimization techniques: Shared memory with bank conflict avoidance, which uses swizzle pattern to reorganize data layout, preventing conflicts from multiple threads accessing the same memory bank. It is parameterized by three parameters: bits, base, and shift, controlling the bit-level permutation of shared memory addresses. Multi-stage pipelining, which partitions shared memory into multiple buffering stages so that during the time that the current tile is being computed, subsequent tiles are prefetched into other buffers. It is parameterized by the number of buffering stages, denoted by n_stage. Asynchronous memory copy, which enables non-blocking 128-bit transfers from global to shared memory. Register accumulation, which stores partial results in register files to minimize memory traffic. Block swizzle, which improves L2 cache hit rates by reordering thread block execution so that spatially adjacent blocks can run concurrently, while by default, thread blocks are launched in row-major order, causing adjacent blocks to access disjoint memory regions resulting in poor L2 cache utilization. Block swizzle is parameterized by swizzle_stride, which determines the stride pattern used to reorder block indices. Epilogue optimization, which efficiently transfers accumulated results from registers to global memory, potentially using direct register-to-shared-memory copies with wide data types (e.g., 128-bit) to minimize intermediate temporaries and reduce memory traffic. With these techniques, CUDA-L2 can also automatically determine which to apply and figure out their optimal parameters. For instance, it decides whether to use block swizzle or bypass it entirely, whether to enable multi-stage pipelining or rely on simple loops, and how to set tile sizes, pipeline stages, etc. Moreover, within each technique, CUDA-L2 can discover novel variations beyond standard implementations. Rather than applying one-size-fits-all strategy, these variations target specific MNK configurations where they outperform conventional approaches. The following are few examples:"
        },
        {
            "title": "5.3.1 Double-Buffered Register Fragments with Ping-Pong Execution",
            "content": "In matrix multiplication, kernels must iteratively load data from shared memory into registers before performing tensor core computations. Typically, matrices and are loaded sequentially, causing compute units to stall while waiting for data to arrive. CUDA-L2 finds the optimized double-buffer approach that allocates two sets of register fragments and alternates between them in ping-pong fashion: while the tensor cores compute using one buffer, the next iterations data is prefetched into the other. This overlaps data movement with computation, eliminating the stall cycles present in the single-buffer approach. Single-buffer allocation works better for small or when register usage is already high, since double buffering doubles register consumption and can cause performance degradation. Double-buffer allocation performs better with large and sufficient available registers, where hiding memory access latency significantly improves throughput. Code is shown in Listing 2."
        },
        {
            "title": "5.3.2 Aggressive Register-Level Prefetching",
            "content": "Matrices and must be loaded from shared memory into registers before performing tensor core computations. Prefetching is technique that initiates these loads ahead of time, allowing the kernel to load data for future tiles while computing on the current tile. With standard single-step prefetching, kernels prefetch data for only the next tile (one tile ahead) during the current computation. While this provides basic overlap of memory loads and computation, it may not be sufficient when loading data takes longer than computing single tile. This leaves the GPU with idle execution slots that could be used for additional prefetching. In some configurations, CUDA-L2 chooses to prefetch more aggressively by loading multiple iterations ahead. Preloading data multiple iterations ahead allows the kernel to fully overlap memory operations with tensor core computations, leading to speedups. Generally, single-step prefetching works better for configurations with few loop iterations or high register pressure, 9 1 2 4 5 6 7 8 10 11 1 2 3 5 6 7 8 9 11 12 13 14 Standard: Single-Buffer auto tCrA = thr_mma.partition_fragment_A( gA(_, _, 0)); auto tCrB = thr_mma.partition_fragment_B( gB(_, _, 0)); auto tCrD = thr_mma.partition_fragment_C(gD); cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA(_, _, ik), tCrB(_, _, ik), tCrD); Optimized: Double-Buffer auto tCrA_buf0 = thr_mma.partition_fragment_A( gA(_, _, 0)); auto tCrA_buf1 = thr_mma.partition_fragment_A( gA(_, _, 0)); auto tCrA_view0 = s2r_thr_copy_a.retile_D(tCrA_buf0); auto tCrA_view1 = s2r_thr_copy_a.retile_D(tCrA_buf1); bool use_buf0 = (ik % 2 == 0); if (use_buf0) { cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view1(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA_buf0(_, _, ik), tCrB_buf0(_, _, ik), tCrD); } else { cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view0(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA_buf1(_, _, ik), tCrB_buf1(_, _, ik), tCrD); } 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 Listing 2: Register fragment buffering: single-buffer (left) vs. double-buffer with ping-pong (right). as multi-step prefetching requires additional register storage for the extra buffered data. Multi-step prefetching works better when iteration counts are high and register headroom is sufficient. Standard: Single-Step Prefetch Optimized: Multi-Step Prefetch #pragma unroll for (int ik = 0; ik < nk; ++ik) { int ik_next = (ik + 1) % nk; // K+1 prefetch cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view(_, _, ik_next)); cute::copy(s2r_tiled_copy_b, tBsB(_, _, ik_next, ismem_read), tCrB_view(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA(_, _, ik), tCrB(_, _, ik), tCrD); } 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 // Prime pipeline with K+0, K+1, K+2 cute::copy(s2r_tiled_copy_a, tAsA(_, _, 0, ismem_read), tCrA_view(_, _, 0)); cute::copy(s2r_tiled_copy_b, tBsB(_, _, 0, ismem_read), tCrB_view(_, _, 0)); if (nk > 1) { cute::copy(s2r_tiled_copy_a, tAsA(_, _, 1, ismem_read), tCrA_view(_, _, 1)); cute::copy(s2r_tiled_copy_b, tBsB(_, _, 1, ismem_read), tCrB_view(_, _, 1)); } #pragma unroll for (int ik = 0; ik < nk; ++ik) { int ik_prefetch = ik + 4; // K+4 distance if (ik_prefetch < nk) { cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_prefetch, ismem_read), tCrA_view(_, _, ik_prefetch)); cute::copy(s2r_tiled_copy_b, tBsB(_, _, ik_prefetch, ismem_read), tCrB_view(_, _, ik_prefetch)); } cute::gemm(tiled_mma, tCrD, tCrA(_, _, ik), tCrB(_, _, ik), tCrD); } Listing 3: Register-level prefetching: single-step (left) vs. aggressive multi-step (right)."
        },
        {
            "title": "5.3.3 Epilogue Optimization with Direct Register-to-Shared-Memory Copy",
            "content": "The epilogue stage first copies results in registers to shared memory and then to global memory. In the shared memory copying stage, normally, the kernel first copies register fragments to temporary tensor in the shared memory, then performs second copy from the temporary to shared memory. This is only necessay when the register layout doesnt match the shared memory layout directly, so CUTLASS/CuTe creates this intermediate tensor to reorganize the data. For cases where register layout actually matches the the shared memory layout, CUDA-L2 eliminates the intermediate temporary tensor and performs direct register-to-shared-memory transfers. Additionally, it employs wider copy atoms (e.g., uint128_t) to transfer more data per instruction, reducing the total number of copy operations and improving memory bandwidth utilization. 10 Standard: Two-Step Copy Optimized: Direct Wide Copy 2 3 4 5 6 // Copy via intermediate tensor auto = make_tensor_like<half>( tCrC_r2sx(_, + j)); cute::copy(tCrC_r2sx(_, + j), t); cute::copy(r2s_tiled_copy_c, t, tCsC_r2s(_, 0, 0, j)); 1 2 3 5 // Direct R2S with wide atom using R2SCopyAtomC = Copy_Atom UniversalCopy<cute::uint128_t>, T>; cute::copy(tCrC_r2sx(_, + j), tCsC_r2s(_, 0, 0, j)); Listing 4: Epilogue register-to-shared-memory copy: two-step (left) vs. direct wide copy (right)."
        },
        {
            "title": "5.3.4 Staggered/Split A-B Prefetch Scheduling",
            "content": "In standard prefetching, kernels prefetches both and matrices back-to-back before executing the MMA operation. Specifically, data for the next iteration is prefetched into registers while the MMA computes using already-loaded data from the current iteration. In some cases, issuing both prefetches consecutively can create instruction bubbles and underutilize available execution units, as the memory and compute pipelines are not fully overlapped. CUDA-L2 finds modification to this approach that separates the and prefetches around the MMA operation: the matrix prefetch is issued first, then the MMA executes using previously loaded data, and finally the matrix prefetch is issued. Since the MMA operates on already-resident register data from the current iteration, it does not depend on the prefetches being completed. This interleaving increases instruction-level parallelism by allowing the prefetch and MMA to overlap in the pipeline, with the prefetch filling the gap after the MMA issues. Consecutive prefetching works better when memory access is the bottleneck, where batching both prefetches together makes better use of available bandwidth. Staggered prefetching works better when computation is the bottleneck. Code is shown in Listing 5. Standard: Consecutive Prefetch Optimized: Staggered Prefetch 2 3 4 5 6 8 cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view(_, _, ik_next)); cute::copy(s2r_tiled_copy_b, tBsB(_, _, ik_next, ismem_read), tCrB_view(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA(_, _, ik), tCrB(_, _, ik), tCrD); 1 2 3 4 6 7 8 cute::copy(s2r_tiled_copy_a, tAsA(_, _, ik_next, ismem_read), tCrA_view(_, _, ik_next)); cute::gemm(tiled_mma, tCrD, tCrA(_, _, ik), tCrB(_, _, ik), tCrD); cute::copy(s2r_tiled_copy_b, tBsB(_, _, ik_next, ismem_read), tCrB_view(_, _, ik_next)); Listing 5: A-B prefetch scheduling: consecutive (left) vs. staggered (right)."
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we conduct systematic analysis of the 1,000 optimal configurations discovered by RL to understand which optimization techniques and parameter settings should be applied under different problem dimensions and hardware constraints."
        },
        {
            "title": "6.1 How to Choose BM, BN, BK",
            "content": "Figure 4(a)(b)(c) illustrates the relationship between problem dimensions and their corresponding tile sizes parameterized by BM, BN and BK, which determine how the input matrices are divided into smaller blocks for efficient computation on the GPU. Figure 4(a) shows that BM scales proportionally with the dimension (ρ = 0.652). As increases from small values (256) to large values (>4K), BM grows from approximately 60 to 160. This strong positive correlation indicates that larger dimensions require larger M-tiles to maintain efficiency. similar trend is observed between and BN in Figure 4(b), with an even stronger correlation (ρ = 0.705). In contrast, Figure 4(c) reveals that BK has only weak correlation with (ρ = 0.256), indicating that BK selection depends on additional factors beyond alone, such as register pressure, memory bandwidth constraints, and the number of pipeline stages. Figure 4(d) reveals that the values of BM and BN are highly correlated (ρ = 0.695), indicating that optimal configurations tend to use similar or same values for both tile dimensions. This is because the tensor cores operate in square or near-square 11 Table 4: Hyperparameter Selection Patterns in Optimized CUDA Matrix Multiplication Kernels instruction format (e.g., 16816), and that balanced tiles reduce the risk of resource imbalance where one dimension becomes bottleneck."
        },
        {
            "title": "6.2 How to Choose Stage Number in Multi-stage Pipelining",
            "content": "Figure 4(e) shows how matrix dimensions affect the number of pipeline stages in optimized CUDA kernels. Each box shows the distribution of dimension values for different stage counts. It shows clear positive relationship: as the dimension increases, the number of pipeline stages should increase to achieve the optimal performance. Small (128) needs only 2-3 stages for adequate latency hiding, while large (>8K) requires 6+ stages to maintain high throughput by keeping multiple data loads in flight simultaneously."
        },
        {
            "title": "6.3 When and How to Use Block Swizzling",
            "content": "BlockSwizzle is memory optimization technique that improves cache locality and load balancing by reordering how thread blocks access global memory. Figure 4(f) shows the relationship between matrix problem (M K) and BlockSwizzle usage across different configurations. The first box represents configurations that do not use BlockSwizzle, which typically handle smaller problem sizes. The subsequent boxes show increasingly larger swizzle stride values, with each handling progressively larger problems. The decision of when to enable BlockSwizzle is primarily driven by problem size. For small problems (less than 227 or approximately 134 million operations), BlockSwizzle is optional and used in only 44% of configurations, as the overhead may outweigh the benefits when data fits comfortably in cache. However, as problem size grows, BlockSwizzle becomes increasingly prevalent: medium problems (227 to 233) use it 73-80% of the time, while very large problems (greater than 236 or 68 billion operations) employ it almost every time at 99% usage. The value of swizzle_stride parameter, which controls the granularity of the block reordering, should be chosen adaptively based on problem dimensions. Smaller problems that do use swizzling typically employ stride values of 8-128, while larger problems benefit from stride values of 512-16,384. The strong correlation (ρ = 0.453) demonstrates that both the decision to use BlockSwizzle and the selection of an appropriate stride value are fundamentally tied to problem scale, with the optimization 12 becoming essential as memory access patterns grow more complex at larger scales."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose CUDA-L2, system that combines large language models and reinforcement learning to automatically optimize HGEMM CUDA kernels across diverse configurations. Through continued pretraining on diverse CUDA code, multi-stage RL training progressing from general kernel optimization to matmul-specific optimization, and retrieval-augmented context, CUDA-L2 achieves an 11.4% speedup over NVIDIAs cuBLASLt-AutoTuning library on A100 GPUs across 1000 configurations in offline mode, and 15.9% speedup in server mode. Against more commonly used baselines, CUDA-L2 delivers even larger gains: 22.0% over torch.matmul (28.7% server) and 19.2% over cuBLAS (26.0% server). CUDA-L2 demonstrates that even for heavily-optimized, performance-critical kernels like HGEMM, LLM-guided RL can discover superior implementations by systematically exploring configuration spaces at scales impractical for manual optimization."
        },
        {
            "title": "References",
            "content": "[1] ACHIAM, J., ADLER, S., AGARWAL, S., AHMAD, L., AKKAYA, I., ALEMAN, F. L., ALMEIDA, D., ALTENSCHMIDT, J., ALTMAN, S., ANADKAT, S., ET AL. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] BARONIO, C., MARSELLA, P., PAN, B., GUO, S., AND ALBERTI, S. Kevin: Multi-turn rl for generating cuda kernels. arXiv preprint arXiv:2507.11948 (2025). [3] COMANICI, G., BIEBER, E., SCHAEKERMANN, M., PASUPAT, I., SACHDEVA, N., DHILLON, I., BLISTEIN, M., RAM, O., ZHANG, D., ROSEN, E., ET AL. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 (2025). [4] DUBEY, A., JAUHRI, A., PANDEY, A., KADIAN, A., AL-DAHLE, A., LETMAN, A., MATHUR, A., SCHELTEN, A., YANG, A., FAN, A., ET AL. The llama 3 herd of models. arXiv e-prints (2024), arXiv2407. [5] GUO, D., YANG, D., ZHANG, H., SONG, J., ZHANG, R., XU, R., ZHU, Q., MA, S., WANG, P., BI, X., ET AL. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [6] LANGE, R. T., PRASAD, A., SUN, Q., FALDOR, M., TANG, Y., AND HA, D. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. Tech. rep., Technical report, Sakana AI, 02 2025, 2025. [7] LI, X., SUN, X., WANG, A., LI, J., AND SHUM, C. Cuda-l1: Improving cuda optimization via contrastive reinforcement learning. arXiv preprint arXiv:2507.14111 (2025). [8] LIU, A., FENG, B., XUE, B., WANG, B., WU, B., LU, C., ZHAO, C., DENG, C., ZHANG, C., RUAN, C., ET AL. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [9] MATTSON, P., REDDI, V. J., CHENG, C., COLEMAN, C., DIAMOS, G., KANTER, D., MICIKEVICIUS, P., PATTERSON, D., SCHMUELLING, G., TANG, H., ET AL. Mlperf: An industry standard benchmark suite for machine learning performance. IEEE Micro 40, 2 (2020), 816. [10] OUYANG, A., GUO, S., ARORA, S., ZHANG, A. L., HU, W., RÉ, C., AND MIRHOSEINI, A. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517 (2025). [11] PENG, B., LI, C., HE, P., GALLEY, M., AND GAO, J. Instruction tuning with gpt-4. arXiv preprint arXiv:2304. (2023). [12] REDDI, V. J., CHENG, C., KANTER, D., MATTSON, P., SCHMUELLING, G., WU, C.-J., ANDERSON, B., BREUGHE, M., CHARLEBOIS, M., CHOU, W., ET AL. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) (2020), IEEE, pp. 446459. [13] SHAO, Z., WANG, P., ZHU, Q., XU, R., SONG, J., BI, X., ZHANG, H., ZHANG, M., LI, Y., WU, Y., ET AL. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [14] SHENGYU, Z., LINFENG, D., XIAOYA, L., SEN, Z., XIAOFEI, S., SHUHE, W., JIWEI, L., HU, R., TIANWEI, Z., WU, F., ET AL. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792 (2023). [15] TEAM, G., ANIL, R., BORGEAUD, S., ALAYRAC, J.-B., YU, J., SORICUT, R., SCHALKWYK, J., DAI, A. M., HAUTH, A., MILLICAN, K., ET AL. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [16] TSCHAND, A., AWAD, M., SWANN, R., RAMAKRISHNAN, K., MA, J., LOWERY, K., DASIKA, G., AND REDDI, V. J. Swizzleperf: Hardware-aware llms for gpu kernel performance optimization. arXiv preprint arXiv:2508.20258 (2025). [17] YANG, A., LI, A., YANG, B., ZHANG, B., HUI, B., ZHENG, B., YU, B., GAO, C., HUANG, C., LV, C., ET AL. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025)."
        },
        {
            "title": "8.1 Code cuBLAS-TN, cuBLASLt-heuristic-TN and cuBLASLt-AutoTuning-TN",
            "content": "1 // NN: row major MxK, row major KxN, row major MxN 2 void cublas_nn(half *A, half *B, half *C, size_t M, size_t N, 3 4 5 6 8 size_t K) { static half alpha = 1.0; static half beta = 0.0; cublasGemmEx(g_handle, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &alpha, B, CUDA_R_16F, N, A, CUDA_R_16F, K, &beta, C, CUDA_R_16F, N, CUBLAS_COMPUTE_16F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);} 9 10 // TN: row major MxK, col major NxK, row major MxN 11 void cublas_tn(half *A, half *B, half *C, size_t M, size_t N, 12 14 15 16 17 size_t K) { static half alpha = 1.0; static half beta = 0.0; cublasGemmEx(g_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K, &alpha, B, CUDA_R_16F, K, A, CUDA_R_16F, K, &beta, C, CUDA_R_16F, N, CUBLAS_COMPUTE_16F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);} Listing 6: cuBLAS baseline implementations for NN and TN layouts. 1 // Part 1: Select and cache best algorithm suggested by heuristic 2 void cublaslt_heuristic_tn_setup(size_t M, size_t N, size_t K) { cublasLtMatmulDescCreate(&opDesc, CUBLAS_COMPUTE_16F, CUDA_R_16F); cublasOperation_t transa = CUBLAS_OP_T, transb = CUBLAS_OP_N; cublasLtMatmulDescSetAttribute(opDesc, CUBLASLT_MATMUL_DESC_TRANSA, &transa, sizeof(transa)); cublasLtMatmulDescSetAttribute(opDesc, CUBLASLT_MATMUL_DESC_TRANSB, &transb, sizeof(transb)); cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_16F, K, N, K); cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_16F, K, M, K); cublasLtMatrixLayoutCreate(&Cdesc, CUDA_R_16F, N, M, N); cublasLtMatmulHeuristicResult_t results[100]; int returnedResults = 0; // Query top-100 algorithms from heuristic cublasLtMatmulAlgoGetHeuristic(g_handle, opDesc, Bdesc, Adesc, Cdesc, Cdesc, pref, 100, results, &returnedResults); algo = results[0].algo; // Cache best algorithm suggested by heuristic 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 21 } 22 23 // Part 2: Execute with cached algorithm 24 void cublaslt_heuristic_tn(half *A, half *B, half *C, half alpha = 1.0, beta = 0.0; cublasLtMatmul(g_handle, opDesc, &alpha, B, Bdesc, A, Adesc, size_t M, size_t N, size_t K) { &beta, C, Cdesc, C, Cdesc, &algo, workspace, ws_size, 0); 25 26 28 29 30 } Listing 7: cuBLASLt-heuristic-TN: (1) selecting and caching the algorithm suggested by heuristic, (2) execution with cached algorithm. cuBLASLt-heuristic-NN is omitted for brevity. 15 1 // Part 1: Benchmark and cache best algorithm (one-time setup) 2 void cublaslt_benchmark_tn_setup(size_t M, size_t N, size_t K) { // Create descriptors and layouts cublasLtMatmulDescCreate(&opDesc, CUBLAS_COMPUTE_16F, CUDA_R_16F); cublasOperation_t transa = CUBLAS_OP_T, transb = CUBLAS_OP_N; cublasLtMatmulDescSetAttribute(opDesc, CUBLASLT_MATMUL_DESC_TRANSA, &transa, sizeof(transa)); cublasLtMatmulDescSetAttribute(opDesc, CUBLASLT_MATMUL_DESC_TRANSB, &transb, sizeof(transb)); cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_16F, K, N, K); cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_16F, K, M, K); cublasLtMatrixLayoutCreate(&Cdesc, CUDA_R_16F, N, M, N); // Query top-100 algorithms from heuristic cublasLtMatmulHeuristicResult_t results[100]; int returnedResults = 0; cublasLtMatmulAlgoGetHeuristic(g_handle, opDesc, Bdesc, Adesc, Cdesc, Cdesc, pref, 100, results, &returnedResults); // Benchmark all algorithms: 50 warmup + 100 measurement rounds for (int round = 0; round < 150; round++) { fill_random_half(A, M*K, stream); fill_random_half(B, K*N, stream); // Shuffle algorithm order to reduce cache effects std::shuffle(algoIndices.begin(), algoIndices.end(), rng); // Warmup call with last algorithm (untimed) cublasLtMatmul(g_handle, opDesc, &alpha, B, Bdesc, A, Adesc, // Random input per round &beta, C, Cdesc, C, Cdesc, &results[algoIndices.back()].algo, workspace, ws_size, stream); // Benchmark all algorithms in shuffled order for (int = 0; < returnedResults; i++) { int idx = algoIndices[i]; cudaEventRecord(start, stream); cublasLtMatmul(g_handle, opDesc, &alpha, B, Bdesc, A, Adesc, &beta, C, Cdesc, C, Cdesc, &results[idx].algo, workspace, ws_size, stream); cudaEventRecord(stop, stream); if (round >= 50) algoTimes[idx].push_back(elapsed_time); } } // Select algorithm with best median time int best_idx = 0; float best_median = FLT_MAX; for (int = 0; < returnedResults; i++) { float med = median(algoTimes[i]); if (med < best_median) { best_median = med; best_idx = i; } } algo = results[best_idx].algo; // Cache empirically best algorithm 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 } 49 50 // Part 2: Execute with cached algorithm 51 void cublaslt_benchmark_tn(half *A, half *B, half *C, half alpha = 1.0, beta = 0.0; cublasLtMatmul(g_handle, opDesc, &alpha, B, Bdesc, A, Adesc, size_t M, size_t N, size_t K) { &beta, C, Cdesc, C, Cdesc, &algo, workspace, ws_size, 0); 52 53 54 55 56 57 } Listing 8: cuBLASLt-benchmark-TN: (1) selecting and caching the best algorithm from the top-100 algorithms returned by heuristic, (2) execution with cached empirically-best algorithm."
        }
    ],
    "affiliations": [
        "deep-reinforce.com"
    ]
}