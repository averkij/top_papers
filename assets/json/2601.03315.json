{
    "paper_title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
    "authors": [
        "Dhruv Trehan",
        "Paras Chopra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 5 1 3 3 0 . 1 0 6 2 : r Why LLMs Arent Scientists Yet: Lessons from Four Autonomous Research Attempts"
        },
        {
            "title": "Paras Chopra",
            "content": "{dhruv.trehan, paras}@lossfunk.com January 2026 Abstract We report case study of four end-to-end attempts to autonomously generate ML research papers using pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1."
        },
        {
            "title": "1 Problem Definition and System Overview",
            "content": "The question we set out with was: could state-of-the-art reasoning LLMs go from research idea to research paper with high degree of autonomy, minimal code scaffolding, and the most basic tools? Various AI Scientist systems proposed in research papers already relied on high 1 degree of domain-specific pre-definition of the workflow or framing the problem statement in system-specific way. Tree-search systems like Sakanas [1] would have required complex metaorchestration, contradicting our minimal scaffolding goal. Similarly, Googles AlphaEvolve system [2] requires clear verification metric defined by human expert in advance. We were interested in exploring how far current LLMs can go without significant scaffolding In line with this, we limited our scope to computational sciences, or inputs from humans. and specifically Machine Learning, since experiments in this domain could be done completely digitally. Figure 1 shows high-level diagram of our system design. Figure 1: Autonomous Research Pipeline: High-level diagram showing the interaction between the six agent modules and the shared file system artifacts (idea.md to paper outline.md) used to maintain context. Our final system consisted of the following six distinct modules, all using Gemini 2.5 Pro primarily because of its long context length. Prompts and output formats for each of these are available in our GitHub repository1. 1. Idea Generation Agent, which generated research ideas by combining insights from two input papers in subdomain and outputted an idea in structured format. More details on the idea generation process are included in Section 2. 2. Hypotheses Generation Agent, which took the generated research idea and proposed testable and falsifiable hypotheses with clear claim statements, datasets, baselines, and metrics to observe to guide experimental planning in subsequent stages. 1https://github.com/Lossfunk/ai-scientist-artefacts-v1 2 3. Experiments Planning Agent, which converts hypotheses and ideas into detailed implementation plans (plan.md and agent.md files) containing step-by-step tasks, project structure, method clarifications, and failure mode controls for autonomous execution by Claude Code on Modal infrastructure. 4. Experimental Output Evaluation Agent, which employed two-tiered evaluation approach, primarily reviewing the output of an experiment for hypothesis implementation fidelity and statistical validity. The same agent was later used to conduct paper readiness check for completed hypotheses suite to determine if sufficient insight had emerged to proceed to paper writing tasks. 5. Revision Agent, which automatically determined the next step in the research process when experimental output evaluation indicated failure, choosing between revising the idea, revising the hypotheses suite, or requesting LLM-mentor feedback. The agent was triggered infrequently since most failed experiments were terminated by human decision rather than routed through the revision process. 6. Paper Outlining Agent, which reviewed the complete experimental output, as well as all relevant context docs across stages, to outline research paper including descriptions of visualizations required. Claude Code then completed this outline, section by section, into full paper. Experimental Implementation and Paper Writing - The experimental implementation and paper writing were executed by Claude Code using Claude Opus 4.1 and Claude Sonnet 4 for iterative code development, execution on Modal infrastructure, and manuscript development from outline to full text. For paper-writing, we adopted three-stage, iterative approach: generating detailed outline, completing sections sequentially, and conducting two rounds of human-Claude Code collaborative editingfirst to ensure continuity and flow, then to temper overoptimistic claims about In our experience, human intervention remained necessary for quality control durresults. ing the paper-writing phase because initial autonomous drafts generated with prompting were technically accurate but tended toward overly mechanical prose or lacked narrative nuance. Tools and Model Access - The model was accessed on Google AI Studio using an agentic prompt, including repository location and the following four tool definitions: read file - This enabled the LLM to read context files including previously generated documents, mentor notes from external LLM review, or outputs from code implementation and execution. write file - This enabled the LLM to save context files. 3 llm search - This enabled agents to query current information from the internet via an OpenAI model when needed throughout the research process. list files - This returned list of files in the context repo. This becomes more relevant as the project progresses and the LLM agent creates sub-directories. Context Management - Each research idea operated within dedicated repository that served as the workspace for all agents and the context store. This repository included all context artefacts created from idea note, to hypotheses sets, code files, and code outputs, as well as the final paper LaTeX template required for the conference submission. Each agent received the repository state as part of its prompt context, as shown in Figure 2. This approach ensured that we were not context engineering too much, in line with our autonomy and general system design constraints, and we could review the LLMs decision-making about which context files to parse when, relevant information retrieval skill for any researcher. Figure 2: Agent Prompt Template: How repository state, tools, and process guidelines are shared in the system prompt of each agent module. Idea Review - Along with this, we also used four zero-shot prompts (NeurIPS Guidelines Based Reviewer [1], Chain of Ideas Reviewer [3], Google Co-scientist Tournament Reviewer [4], and custom evaluation prompt with web-search access via OpenAI) for idea review and shortlisting before the hypotheses generation and experimentation process began. All reviewer prompts were run with Claude Opus 4.1, and only the custom evaluation prompt had websearch access."
        },
        {
            "title": "2 Our Attempts and Outcomes",
            "content": "To test our autonomous research system, we explored three domains: World Models, MultiAgent Reinforcement Learning, and AI Safety and Alignment. Figure 3 shows our multi-stage 2We decided to not include web-search with 3/4 reviewer prompts in line with conversations about the human paper review process in which reviewers mentioned that they did not conduct much literature review before writing reviews. 4 Table 1: Research Implementation Journey: Attempts, Outcomes, and Failure Modes ID Domain Idea Runs Final Status Primary Failure Modes MARL-1 Multi-Agent RL Zero-shot Multi-agent Coordination WMWorld Models WM-2 World Models AS-1 AI Safety Differentiable Planning in Stochastic World Models Replacing Reconstruction Loss with Perceptual Loss Using Semantic Entropy for Jailbreak Detection 2 1 1 Failed: Execution Stage Failed: Evaluation Stage Failed: Evaluation Stage Implementation Drift, Bias on Training Data Implementation Drift, Lack of Scientific Taste Implementation Drift, Bias on Training Data Success: Published at Agents4Science 2025 Bias on Training Data, Memory and Context Issues process for generating and reviewing ideas, leading to the final four ideas that progressed from idea generation to hypothesis generation and experimentation. We started with corpus of papers from top-tier venues to ensure our idea generation was grounded in cutting-edge research, and the final decision on which ideas to pursue was made after contacting the authors of the parent papers for their input on quality and feasibility. Of these four ideas, three were pruned during early execution, and only the fourth (from the AI Alignment and Safety domain) was successfully completed. Table 1 shows high-level summary of these four candidate ideas, with final status and primary failure modes. For clarity, we refer to each research idea by its ID (MARL-1, WM-1, WM-2, AS-1) throughout the remainder of this document. Detailed case studies for each attempt, including their complete text, source papers, expert validation, and specific errors, are provided in Appendix A. Results from the fourth idea that was successfully executed became our final paper, accepted to the Agents4Science 2025 conference: The Consistency Confound: Why Stronger Alignment Can Break Black-Box Jailbreak Detection. The conference accepted 48/254 valid submissions, and our paper also passed code-audit instituted by the conference organizers. Reviewers in the conference included three dis5 Figure 3: The Selection Funnel: From 135+ papers to 4 candidates. Only one (AS-1) survived execution constraints. tinct LLM Reviewers and Human Expert. Table 2 shows the reviews that primarily recognized the work for its empirical contribution and negative results, while also noting that the analysis and comparisons were limited, leading to majorly borderline accept decision, with 1 of 3 AI reviewers scoring the paper 6 or strong accept. Table 2: Agent4Science 2025 Reviews for The Consistency Confound Reviewer Type Decision Confidence Strengths Weaknesses AI Reviewer 3 AI Reviewer 2 AI Reviewer 1 4 4 Human Reviewer 4 5 5 3 Solid contribution by revealing fundamental limitations of plausible detection approach and providing mechanistic understanding of why it fails. The papers originality lies not in the proposal of new method, but in its rigorous deconstruction of an existing one in new context. Clear empirical negative results with careful quantification, rigorous analysis of failure modes, appropriate baselines, and transparent discussion of limitations and ethical considerations. The focus on single SE variant and limited model families. No significant weaknesses identified. Limited scope (the SE variant differs from canonical SE), lack of comparison to content-based black-box detectors, calibration protocol concerns, narrow decoding and embedding choices, and no positive alternative proposed. The paper is technically sound, with careful empirical evaluation. It identifies clear and reproducible failure mode. The underlying method is borrowed from hallucination detection. Main originality is in showing its limitations rather than developing new technique. Scope is limited, comparisons are mostly against simple baselines, with no testing of stronger black-box defenses. Contribution is primarily negative result. While important, significance would be higher if the paper also explored alternative methods or mitigation strategies. As part of the conference submission, we had to include an AI Involvement Checklist to measure and explain the role of AI in our research. Table 3 shows snippet of this. We rated the system at the highest autonomy level (Category D, 95% AI) for experimental design, execution, and writing, with human involvement (Category C, 50-95% AI) primarily in the initial hypothesis definition stage. The complete paper, code generated, specific agent logs, and the full checklist are available for review on OpenReview [5]."
        },
        {
            "title": "3 Observed Failure Modes and Mitigation",
            "content": "Through our experiments, we identified six critical failure modes that consistently emerged across different research attempts. These patterns reveal systematic limitations in current LLM capabilities for autonomous research. The following subsections detail each failure mode with specific examples and mitigation strategies. 6 Table 3: AI Involvement Checklist for Idea AS-1 by Research Stage (Agents4Science Rubric) Research Stage Score Role Description Hypothesis Development Experimental Design Execution & Analysis Paper Writing D Mostly AI : Initial search space defined by human; specific failure mode hypothesis and core questions generated by AI agents via paper mashing. AI-Generated: Full experimental plans, baseline choices, and metric definitions generated by Gemini 2.5 Pro; human role limited to high-level approval. AI-Generated: End-to-end coding by Claude Code. Datasets handled autonomously. Human intervention limited to providing HuggingFace tokens. AI-Generated: Narrative structuring and figure generation driven by AI. Human role restricted to final sanity check and minor copy-editing. Score Key: A: 95% Human, B: 5095% Human, C: 5095% AI, D: 95% AI."
        },
        {
            "title": "3.1 Bias on Training Data",
            "content": "Research often relies on specialized protocols, libraries, and datasets that arent widely used, but models consistently defaulted to popular alternatives from their training data. This failure mode manifested systematically across infrastructure setup, library selection, and dataset handling, where models consistently overrode explicit instructions with presumably memorized training patterns. In the case of Modal setup, Claude Code would consistently use an outdated Modal mount command to make local files accessible, and default to local file paths over Modal storage, ignoring updated API documentation and instructions provided in plan.md and agent.md file. Beyond this, it would also regress to outdated research libraries despite explicit version and search requirements, and ignore dataset-specific field structures in favor of standard formats. Figure 4 showcases this systematic override across three implementation contexts, with detailed examples provided in Appendix per idea implementation. Similar errors in setting up libraries and environments were found by researchers working specifically on this problem [6] [7]. We addressed this by avoiding anchoring on low-level details like libraries to use and datasets at earlier stages, Figure 4: Training Data Bias Pattern: Systematic override of prompt instructions by memorized patterns across long contexts. 7 and providing instructions for specific library use and library documentation at the execution stage. Even after these two changes, in the case of an error, the experiment execution model would often diagnose library imports as the cause and regress back to using the versions mentioned in the training data, insisting that that is the right way to do it. This bias in training data can show up in more nefarious ways than just errors in using libraries and datasets. Mehtaab Sawhney and Mark Sellke observe with regard to their work on the Erdos problems with GPT-5 Pro, while models are able to suggest plausible proof strategies, they often...are overly confident in the power of existing methods. They note that this is unsurprising, since discussion of why more obvious strategy doesnt work is largely absent in mathematical literature itself and, consequently, in the training data. In this case, the bias on training data leads to failure of domain taste and identifying what they call negative space for problem [8]. We discuss the implications of this missing data for future system design in Section 5. 3."
        },
        {
            "title": "Implementation Drift",
            "content": "Implementation drift represents the systematic deviation from original research specifications toward simpler, more familiar solutions when AI systems encounter technical complexity or execution barriers. This was particularly salient during the experiment execution stage. Rather than addressing root causes of implementation challenges, models progressively simplify architectures and abandon core innovations to achieve working code that superficially resembles the intended research contribution. in longThis was particularly salient running tasks such as training loops, in which case the coding assistant would often time out and treat the long time taken as an error to be fixed with an alternate implementation, thereby drifting from the instructions in the research idea, hypotheses, and plan files. Figure 5 showcases this in the case of the Idea Figure 5: Implementation Drift Pattern: Simplification of proposed implementations during execution barriers in Idea WM-1 . 8 WM-1 implementation. Yet, the tendency to drift from implementation was not only due to time-related errors. It also emerged in other long-context tasks, such as rewriting baselines and implementing complex code within limited output length. In each case, since the task included multiple steps, at distinct points the Experiment Execution Agent chose to implement something simpler or Idea WMto run in sample/test mode, which also led to failures down the pipeline. 2 implementation detailed in Appendix exemplifies this cascading pattern, where single error in implementing the Dreamer baseline triggered progressive simplification rather than root cause debugging. To address these challenges, we adopted two key approaches. First, we followed portfolio approach to hypothesis generation rather than relying on single hypothesis, which allowed us to plan for potential failures in one experiment by preparing follow-up experiments. Second, we implemented process in which code files were generated first, then explicitly verified and tested for errors before execution, with code generation and execution treated as distinct tasks. This was enforced through explicit instructions in the plan.md and agent.md files."
        },
        {
            "title": "3.3 Memory and Context Issues",
            "content": "Scientific discovery and experimentation are long-duration tasks. It requires agentic coherence over periods that exceed the effective reliability horizon of current models [9]. This is why we see many common constraints in LLM performance on long-context, long-duration tasks manifest prominently in our research system. As sessions progressed and context artifacts accumulated, models systematically lost track of previous decisions, established configurations, and completed work, leading to redundant implementations and inconsistent experimental setups. This failure mode was most evident during baseline implementation, which required much hyperparameter management. Rather than referencing the planning-level details, the coding agent would declare its own hyperparameters with comments while generating or running the related code. This led to experimental conditions that were both unclear to the human orchestrator and very challenging to organize without instituting file management system. We addressed this by including config file and clear instructions for referring to it in the agent.md file, and making it persistent part of all specific task prompts. Though this still ran into the agent.md limitations mentioned in the bias-on-training-data failure mode. Additionally, issues with long context also surfaced during the coding and paper-writing phases, when we needed to reference older details from the project. In many instances, the Experiment Execution Agent would misreference functions defined at the start, leading to errors such as 9 incorrect function signatures and mismatches in metric calculations. During paper writing, the agent would forget to consult the earliest versions of the idea and hypothesis files to outline the paper, instead relying on recent files and results. This ended up in paper that read like list of experiments, with no origin story or motivation. Our solution to this was to enable more and more memory-like context abstractions just as human scientist would, including config to maintain experimental progress and experiment execution logs (Figure 7) at the end of each Claude Code session. Though relying on Claude Code-authored session logs helped with memory and context management, as session logs grew longer, another problem emerged: managing the growing number of files generated by the LLM. This pointed toward the need for file and directory management in autonomous science systems using language models."
        },
        {
            "title": "3.4 Overexcitement and Eureka Instinct",
            "content": "This failure mode was most clearly present during the paper outlining, revision, and experimental output evaluation phases. The models consistently reported success despite clear failures and overstated the significance of their research contributions. Figure 6 illustrates some of the ways in which this failure mode showed up over our attempts. Figure 6: Overexcitement and Eureka Instinct. Left: During execution, agents claim success despite clear experimental failures. Right: During paper writing, limited results are inflated into overstated contributions. 10 We noted that even when results showed clear degeneracies or failures, the generated text focused only on top-level positive indicators, ignoring fundamental problems, and this could be tracked back to relying on report files created during the experiment execution stage to evaluate output instead of examining raw logs from the experimental pipeline. The paper-writing phase exacerbated these issues by overstating novelty and scope. During paper-writing, Claude Code would describe work as the first ever paper in domain or claim seminal contributions regardless of the actual research output. This mirrors the p-hacking and eureka-ing behaviors identified by the Goodfire team [10] and aligns with findings by Bubeck et al., where researchers noted that models would introduce numerical duct tape to smooth over errors and confidently declare victory when numerical signals are still obviously noise [8]. This eagerness to please requires the human in the loop to possess sufficient expertise to reject the models simplified solutions. This overoptimistic tendency also manifested in feedback integration processes, where the system became overly process-following without proper evaluation. How language model agents integrate feedback is an entire research area of its own, and the safety threat posed by LLM agents sandbagging other automated researchers through feedback has been further explored by the team at Anthropic [11]. We believe these patterns likely stem from the RLHF phase of LLM training, where models are rewarded for being agreeable and helpful to humans, leading to bias toward optimistic interpretations and positive framing even when the evidence suggests otherwise. These training objectives are clearly not in line with the requirements for an autonomous science system, which should instead be oriented toward scientific skepticism, truth-seeking, and the detection of confirmation biases."
        },
        {
            "title": "3.5 Lack of Sufficient Domain Intelligence",
            "content": "Research papers present polished final configurations but omit the tacit knowledge required to navigate from hypothesis to working implementation. AI systems consistently struggled with the undocumented craft knowledge that experienced researchers take for granted. These gaps appeared most prominently in stages that required scientific judgment rather than pure coding capabilities, such as hypothesis generation, plan generation, and experimental output evaluation. When moving from hypothesis to implementation plan, the system lacked enough domain depth to predict outputs and spot failure modes. Furthermore, models failed to make the judgments needed for key decisions, such as picking baselines aligned with task needs. For example, choosing continuous control task with discrete input model baseline in Idea WM-2. 11 Plans set high-level goals but ignored the mathematical and conceptual hurdles that make implementation difficult. This created problems, especially for research ideas that needed deep domain expertise. Both ideas, WM-1 and WM-2, needed advanced domain knowledge to combine complex parts, such as balancing stochastic world models with differentiable tree search planners or adding new loss terms to baselines. Models lacked the expertise to understand how these elements should interact or to successfully modify proven architectures in creative ways. Each subdomain had unique process needs: RL required rollouts, AI Alignment and Safety needed recording responses, and models struggled to intuit which artifacts were important for debugging and validation, even with detailed instructions at the Plan Generation stage. Beyond implementation gaps, models also lacked judgment about experimental validity thresholds - for instance, proceeding with hypothesis testing when baseline performance was 95% below established benchmarks, making any comparative analysis scientifically meaningless."
        },
        {
            "title": "3.6 Lack of Scientific Taste",
            "content": "Models consistently failed to recognize fundamental flaws in experimental design and statistical methodology. These methodology gaps emerged during hypothesis generation and experimental output evaluation phases. We first relied on generating only minimum viable hypothesis as part of our pipeline. However, we quickly realized this approach was not sufficient for research output. Relying on single hypothesis created excessive project risk. Even minor implementation errors would trigger complete idea revision rather than hypothesis adjustment. Therefore, we adopted portfolio approach to hypotheses generation, creating comprehensive hypotheses suites that allowed for hypothesis-level adjustments rather than complete idea abandonment when individual experiments failed. See Section 3 on Implementation Drift mitigation for more details. This limitation became evident when the WM-1 hypothesis generated was too simple to draw any conclusions, and this was not captured by the models, even in reflection. The problem was further complicated by irrelevant complexity, such as 50,000 depth parameter, which created computational burden without scientific value. Even with clear instructions, agents showed insufficient awareness of statistical validity. Idea WM-1 was run with only one seed. Expert concerns about computational complexity were not addressed at any point in the planning or hypothesis-generation stages, and the approach was even recommended within the 6-hour, 1-GPU limits, even though it was computationally infeasible. The pattern of issues continued across ideas. Idea WM-2 had fundamental logical errors in its experimental design. The plan generated assumed offline training with static data frames. However, Dreamer requires online learning. Similarly, Idea MARL-1 showed the system failed 12 to interpret the future work section of the input seed paper. Finally, degenerate output in the AS-1 idea was not flagged until manual intervention during the experimental output evaluation stage."
        },
        {
            "title": "4 Design Takeaways for AI Scientist Systems",
            "content": "Our analysis of failure modes reveals four design principles for robust autonomous research systems. These principles address key challenges across research domains and implementation attempts. 1. Start Abstract, Ground Later Domain expertise and technical details should be introduced gradually throughout the research workflow. Prompts and generations should become more specific as the process progresses. Maintain high-level abstraction during the ideation phase to prevent premature anchoring on specific implementations. Ensure this is done later in implementation to avoid erroneous anchoring at early stages. This is also key to ensuring novelty and avoiding plagiarism, where models output research that has already been done, but with slight language changes. During planning, avoid using technical details such as specific datasets, metrics, mathematical ideas, and practices specific to certain fields, such as recording roll-outs for reinforcement learning or data collection protocols. While these details are important for execution, including this context too early often leads models to rely on older libraries and methods from the training data rather than identifying or reasoning about the most current solutions. 2. Verify Everything Verification must occur at every stage of the research pipelinefrom the generation of ideas and hypotheses to the generation of code and results. Using verifier or critic agent at each relevant stage helps avoid conceptual or implementation errors and prevents error cascading. The two critical axes for further design of verification for AI scientist systems are: process vs. outcome verification, and correctness vs. scientific contribution verification. The latter includes ensuring reproducibility across critical stages such as planning, ideation, and output review. Tang et al.s framework of technical execution assessment and scientific contribution assessment [12] provides complementary perspective on evaluating AI scientist outputs. While collecting results from experiments, ground in the raw data and not LLM interpretations since LLMs have tendency to read signal in errors or be overly optimistic about clearly mid results - what the Goodfire team calls p-hacking and eureka-ing [10]. In practice, this means ensuring your experimental output evaluation either programmatically reviews raw logs, 13 statistical metrics, and original data outputs, or, if using LLM evaluators, strictly instructs them to focus on these raw outputs rather than summary or report files through system prompts or context injections. Verification is not only necessary to ensure correctness but also serves as touchpoint for the human-in-the-loop. In our experience, for instance, building verifier to review the visualizations recommended enabled removing some that, though natural to create from the available results, werent central to the papers story. For us, during execution, verification also became way to provide observability into long-running tasks like training models and data collection. We achieved this with clear instructions at the experimental plan generation stage to include tests. 3. Plan For Failure and Recovery Scientific discovery is long-duration task at the frontiers of task complexity, where LLMs and LLM-driven agents still face limitations leading to error accumulation. During research execution, human researchers make many micro-decisions that must be prespecified for autonomous LLMs to avoid failure. This means that multi-turn agentic task design works better than zero-shot generation. This ensures the plan includes sufficient details to avoid drift or errors later in the pipeline. Alongside external verification and critique, reflection and extended thinking are also useful methods to ensure sufficient clarity before execution. We discovered that splitting all coding tasks into modular tasks prevents error cascading. One way to do this is to separate code generation from code execution, ensuring that verification hooks can be built in. Another is how the Goodfire team uses Jupyter notebooks, maintaining natural cell-level limit to the errors [10]. We used agent.md to pass system-wide instructions during task execution to follow general good scientific coding principles and ensure correctness. This included instructions for saving checkpoints for long-duration tasks, adding tests at various scales to see the water flowing through the pipes before full execution, adding detailed logging for all important metrics throughout the code, and additional specific instructions to avoid the long context and memoryrelated errors mentioned in Section 3. Beyond autonomous execution resilience, separating generation from execution also enables human review and intervention at critical stages. For instance, when visualization generation was separated from paper integration, we could review and remove unhelpful bar charts before they appeared in the final manuscript, supporting human-in-the-loop review from experiment execution through paper writing stages. 4. Log Everything 14 output of science agents auEverything from the tonomous to all possible metrics used in running experiments should be logged comprehensively. This includes the usual span and trace logging of the LLMs actual responses and enabling agents to create their own files. This serves two purposes: supporting longduration autonomous execution and later human/LLM review and verification. For us, this meant supporting all the context artifacts listed in Section 1 through write tool and implementing comprehensive session logging instructions as in Figure 7 during experiment execution, as well as specific logging instructions for metrics at the plan generation stage for domainspecific primary and secondary metrics for experiments. We also explicitly instructed our Paper Outline and Writing Agent to synthesize process artifacts alongside results to maintain narrative coherence and capture the process and evolution of scientific discovery. This comprehensive logging enables the two-fold evaluation framework we mention in Section 4. Figure 7: Session Logging Instruction: Comprehensive prompt template for maintaining detailed research execution records across autonomous agent sessions."
        },
        {
            "title": "5 Discussion and Broader Implications",
            "content": "Complete autonomy in scientific discovery currently is only in the future, not merely from capability standpoint, but also from human preference standpoint. Even with the goal of maximum autonomy, we still relied on human intervention at key points. These included idea review, paper writing, and meta-prompting during experiment execution. This pattern emerges consistently across existing AI scientist systems. Sakanas approach required researchers to select 3 promising ideas from 40 AI-generated concepts [1]. Similarly, Googles co-scientist system requires scientist-specified research goals as the entry point [4]. Domain experts will most likely remain involved to nudge, course-correct, and verify. Though major labs like OpenAI are entering AI for scientific discovery, they explicitly recognize the need for human expertise, seeking to hire world-class academics who are completely AI-pilled to work alongside AI models [13]. This hiring strategy reinforces the current goal of creating agents and tools for effective human-LLM collaboration rather than autonomous scientists. Recent work coming from big labs, such as Early science acceleration experiments 15 with GPT-5 from OpenAI, validates this approach, showcasing agents capable of independent rediscovery of known results and deep literature search, along with acting as research collaborators and knowledgeable research supervisors, though as Fields Medalist Timothy Gowers notes, we have not yet reached the stage where an LLM is likely to have the main idea for solving difficult problem. [8] But we must not underestimate the acceleration through human-LLM collaboration. Physicist Brian Keith Spears noted that such collaboration compressed six-month workflow into six hoursa factor of 1000 accelerationeffectively making him one-person army of experts [8]. This collaboration is also especially critical for kick-starting the scientific method data loop. Scientific discovery is not only beyond the training distribution of current large language models, but it is also fair to assume that artefacts of scientific research workflows are notably absent from the training data. For example, there are no recorded reading lists or expert literaturereview trajectories to train or benchmark literature-search agents. You can approximate it using the citation lists of published papers, but these are the papers that made the cut, and arguably, the more critical skill to train agents on is which references to ignore. We also lack records of failed attempts. As Mehtaab Sawhney and Mark Sellke note, current models remain limited in perceiving the negative space of mathematics...This is unsurprising as such discussion is largely absent in mathematical literature itself; mathematicians dont systematically record why problems are out of reach, or why more obvious strategy doesnt work, or why certain techniques are inherently unable to solve certain classes of problems [8]. Having experts first use these large language models on platforms that include tools and multi-agent orchestrations enables the collection of training data to train agents that can autonomously perform those tasks in the future. These advances are constrained by fundamental bottlenecks in long-horizon tasks and context requirements. Scientific discovery requires keeping coherence and context windows over weeks and months. This greatly exceeds the reliability of current models. As of November 2025, even state-of-the-art systems like GPT-5.1-Codex-Max show time horizon of about 2 hours 40 minutes, with 50% success rates [9]. This suggests that before fully autonomous scientists exist, we will likely see smaller modules that build data for the next round of long-duration scientific specialization. The data limitations here extend beyond training to evaluation as well. Multiple benchmarks are now being created. Some notable ones include AstaBench from AllenAI, which provides benchmarking for specific agents tackling specific scientific research tasks [14], and ScientistBench, which focuses on broader scientific reasoning capabilities. But even ScientistBench [12] only includes 22 papers and 28 tasks, mostly extremely guided. This highlights the limitations of data for evaluating autonomous science. Moreover, the complex anonymization processes in ScientistBench also point to the evaluation challenge of separating model capabilities from 16 memorized training data. This issue is important because language models can generate plagiarized science in the absence of robust evaluation systems. [15] There are further gains to be made by multiplying LLM-to-LLM interactions for scientific work. For instance, recent examples show the potential of specialized agent systems. PHYSICS SUPERNOVA demonstrates AI agents matching elite gold medalists at IPHO 2025 through manager agents with domain-specific tools [16]. Additionally, multi-agent virtual labs can also sync up domain-specific tools for accelerated discovery [17]. The paradigm seems to be to break down scientific discovery into discrete capabilities. These can be RL-ed into smaller, specialized models. This decomposition enables complex scientific workflows to be learned as tasks with clear rubrics and reward functions. Larger labs will rush to acquire meta capabilities that need collaboration with domain specialists. Smaller labs will create their own benchmarks for open-source agents and domain-specific models, using open-source models and RL. This creates compelling case for open science in the near future, including publications, sharing results, and all inputs and materials that enable broader scientific progress."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Our autonomous research system demonstrates both the promise and current boundaries of AIdriven scientific discovery. Several key limitations restrict the generalizability and robustness of our findings. We limited our scope to computational Machine Learning research, excluding physical sciences needing experimental infrastructure beyond digital computation. This necessary constraint for our minimal scaffolding approach sets significant boundary for autonomous scientific discovery. Since our main goal was conference submission rather than controlled experimentation, we did not record architectural iterations as systematic ablations. This approach makes it difficult to isolate which design changes led to our single success versus three failures. Our experimental scale is limitedjust four research ideas across three ML subdomains, each with single implementation run rather than best-of-N approach. This small sample size prevents robust statistical conclusions about the prevalence of failure modes or the effectiveness of mitigation measures. Identified failure modes emerged by qualitative observation rather than systematic quantitative measurement, limiting our ability to gauge their importance or frequency across research contexts. Moreover, we have released only prompts and selected system outputs rather than the complete system architecture, limiting reproducibility and others ability to build upon our approach. 17 In future work, we will take the following concrete steps to address the limitations outlined above. We will begin with controlled experiments to quantify the emergence of these failure modes and validate the effectiveness of our proposed mitigation strategies. We intend to scale our approach across hundreds of research ideas spanning diverse scientific subdomains to establish statistical validity for our observations. We also plan to release our agent architectures as open-source tools for the research community, moving beyond prompt-only sharing to enable broader experimentation and improvement. Additionally, we will establish systematic collection of scientific workflow data from expert-LLM collaborative sessions to benchmark performance improvements and train domain-specific research agents. Finally, we aim to develop quantitative measurement frameworks for failure modes and research quality metrics that extend beyond conference acceptance rates, while optimizing human-LLM collaborative research patterns that could unlock the scientific method data loop for iterative capability improvement."
        },
        {
            "title": "References",
            "content": "[1] Yutaro Yamada et al. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. 2025. arXiv: 2504.08066 [cs.AI]. url: https://arxiv. org/abs/2504.08066. [2] Alexander Novikov et al. AlphaEvolve: coding agent for scientific and algorithmic discovery. 2025. arXiv: 2506.13131 [cs.AI]. url: https://arxiv.org/abs/2506.13131. [3] Long Li et al. Chain of ideas: Revolutionizing research via novel idea development with llm agents. In: arXiv preprint arXiv:2410.13185 (2024). [4] Juraj Gottweis et al. Towards an AI co-scientist. 2025. arXiv: 2502.18864 [cs.AI]. url: https://arxiv.org/abs/2502.18864. [5] Dhruv Trehan. The Consistency Confound: Why Stronger Alignment Can Break BlackBox Jailbreak Detection. In: Open Conference of AI Agents for Science 2025. 2025. url: https://openreview.net/forum?id=B6ZrLXou3u. [6] Avi Arora, Jinu Jang, and Roshanak Zilouchian Moghaddam. SetupBench: Assessing Software Engineering Agents Ability to Bootstrap Development Environments. 2025. arXiv: 2507.09063 [cs.SE]. url: https://arxiv.org/abs/2507.09063. [7] Aleksandra Eliseeva et al. EnvBench: Benchmark for Automated Environment Setup. 2025. arXiv: 2503.14443 [cs.LG]. url: https://arxiv.org/abs/2503.14443. [8] Sebastien Bubeck et al. Early science acceleration experiments with GPT-5. 2025. arXiv: 2511.16072 [cs.CL]. url: https://arxiv.org/abs/2511.16072. [9] METR. Details about METRs evaluation of OpenAI GPT-5.1-Codex-Max. https : / / evaluations.metr.org//gpt-5-1-codex-max-report/. Nov. 2025. [10] Mark Bissell, Michael Byun, and Daniel Balsam. You and your research agent: Lessons from using agents for Interpretability Research. Oct. 2025. url: https://www.goodfire. ai/blog/you-and-your-research-agent. [11] Johannes Gasteiger et al. Automated Researchers Can Subtly Sandbag alignment.anthropic.com. https://alignment.anthropic.com/2025/automated-researchers-sandbag/. 2025. [12] Jiabin Tang et al. AI-Researcher: Autonomous Scientific Innovation. 2025. arXiv: 2505. 18705 [cs.AI]. url: https://arxiv.org/abs/2505.18705. [13] Kevin Weil. Tweet on Hiring Domain Experts for OpenAI for Science. 2025. url: https: //x.com/kevinweil/status/1962938974260904421?s=46. [14] Allen AI. AstaBench: Rigorous benchmarking of AI agents with holistic scientific research suite Ai2 allenai.org. 2025. url: https://allenai.org/blog/astabench. [15] Tarun Gupta and Danish Pruthi. All That Glitters is Not Novel: Plagiarism in AI Generated Research. 2025. arXiv: 2502.16487 [cs.CL]. url: https://arxiv.org/abs/ 2502.16487. [16] Jiahao Qiu et al. Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025. In: arXiv preprint arXiv:2509.01659 (2025). [17] Kyle Swanson et al. The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies. In: Nature (2025), pp. 13. [18] Han Wang et al. Learning to Communicate Through Implicit Communication Channels. 2025. arXiv: 2411.01553 [cs.MA]. url: https://arxiv.org/abs/2411.01553. [19] Darius Muglich et al. Expected Return Symmetries. 2025. arXiv: 2502.01711 [cs.MA]. url: https://arxiv.org/abs/2502.01711. [20] Zhengyao Jiang et al. AIDE: AI-Driven Exploration in the Space of Code. 2025. arXiv: 2502.13138 [cs.AI]. url: https://arxiv.org/abs/2502.13138. [21] Weipu Zhang et al. STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning. 2023. arXiv: 2310.09615 [cs.LG]. url: https://arxiv.org/ abs/2310.09615. [22] Dixant Mittal and Wee Sun Lee. Differentiable Tree Search Network. 2024. arXiv: 2401. 11660 [cs.LG]. url: https://arxiv.org/abs/2401.11660. [23] Chenglei Si, Tatsunori Hashimoto, and Diyi Yang. The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas. 2025. arXiv: 2506. 20803 [cs.CL]. url: https://arxiv.org/abs/2506.20803. [24] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs Generate Novel Research Ideas? Large-Scale Human Study with 100+ NLP Researchers. 2024. arXiv: 2409 . 04109 [cs.CL]. url: https://arxiv.org/abs/2409.04109. [25] Pietro Mazzaglia et al. GenRL: Multimodal-foundation world models for generalization in embodied agents. 2024. arXiv: 2406.18043 [cs.AI]. url: https://arxiv.org/abs/ 2406.18043. [26] Danijar Hafner et al. Dream to Control: Learning Behaviors by Latent Imagination. 2020. arXiv: 1912.01603 [cs.LG]. url: https://arxiv.org/abs/1912.01603. [27] Haoyang Li et al. LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges. 2025. arXiv: 2506.10022 [cs.CR]. url: https://arxiv.org/abs/2506.10022. [28] Jiayu Liu et al. Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models Uncertainty? 2025. arXiv: 2505.24778 [cs.CL]. url: https://arxiv.org/abs/2505.24778. 20 Deep Dives: Research Ideas and Anecdotes In this appendix, each subsection follows consistent structure: the complete idea specification generated by our Idea Generation Agent, expert validation feedback from original paper authors, and detailed implementation anecdotes documenting specific failure modes and system responses during autonomous execution. Throughout the appendix, block quotes represent direct outputs from the LLM agents described in our system overview or excerpts from execution log documents, providing authentic examples of system behavior during research attempts. We also thank the authors of the seed papers who provided valuable feedback on early research ideas; any remaining errors or misinterpretations are entirely our own. A.1 MARL-1: Zero-shot Coordination in Multi-Agent RL This idea emerged from combining Implicit Communication Protocols [18] and Zero-shot Coordination [19] research in Multi-agent RL at ICLR 2025. Table 4 details the main parts of the complete idea.md file generated by our Idea Generation Agent. Table 4: MARL-1: Zero-shot Coordination in Multi-Agent RL - Rejected in Implementation Idea Meta-Adaptive Implicit Communication Protocols (Meta-AICPs) for Zero-shot Coordination Problem and Motivation Current RL agents, while capable in specific tasks with specific partners, are brittle. They Proposed Approach Key Innovation Concrete Example Potential Pitfalls struggle with novel teammates, task variations, and especially in situations requiring emergent communication and the dynamic formation and adaptation of conventions. For AI agents to achieve truly fluid and generalizable collaboration in open or ad-hoc team settings, they must robustly coordinate with novel, independently trained partners who may employ unknown or diverse implicit communication protocols. This addresses the fundamental double adaptation problem in MARL: adapting to both environment dynamics and the evolving policies and communication conventions of other agents. Building on work done on Implicit Channel Protocols [18], Agents meta-learn adaptive strategies to rapidly infer and co-create/align their internal P-mappings (the mapping between scouting actions and messages) when encountering novel partners. This moves beyond adapting to fixed partner strategies towards learning how to adapt the communication system itself. Meta-learning distributions over protocol spaces or adaptive strategies for P, rather than relying on fixed point estimates or pre-coordination with novel partners who lack shared co-training history. Extends ZSC beyond action policies to the communication system itself - agents coordinate how they communicate and how to adapt how they communicate. Meta-learning over protocol spaces addresses the need for true convention emergence rather than just partner strategy inference. Dynamic protocol adaptation and emergence during novel partner interaction, with neural architecture disentangling convention-agnostic task policy from convention-specific protocol adapter module. In Hanabi with three canonical P-maps (conservative, aggressive, random), Meta-AICP agents evaluate ZSC performance against diverse partners using Protocol Divergence Score and Information Flow Rate metrics. Agents adapt their scouting action mappings based on inferred partner protocols, maintaining communication richness while achieving robust coordination with previously unseen protocol variants. Potential trade-off in expressiveness vs. generalization - adapted protocols might not achieve the same peak level of nuanced, high-bandwidth communication as protocols developed via intensive co-training. Dependence on quality of meta-training distribution diversity and representativeness. Performance against partners with radically different, out-of-distribution P-mappings remains uncertain. 21 Expert Validation: When we reached out to the authors of the seed papers with this idea, they raised concerns about the systems failure in interpreting the future work section of their original paper, as well as, the potential failure modes, many of which were also identified by the Idea Generation Agent in the generated idea text. They noted that when they mentioned future work on dynamically identifying or designing communication mapping, they were referring to based on public observations or tasks and not to entirely new partners with unknown mappings. We regard this as another instance of the Idea Generation Agent lacking domain expertise and interpreting the original statement in the paper too generously. They did note that the idea does touch on challenging and important question and that formalizing and empirically testing the direction shared...could yield useful insights. They also complimented the choice of the Hanabi setup and the identified baseline from their ICP work as solid anchor points. Implementation and Failure Modes: Considering the input on the merit of testing and formalising the claim nonetheless, we continued to pass the idea on the hypotheses generation and experimental planning stages. At this stage, we were still generating minimum-viable-hypothesis for each research ideatesting single key assumption per project, unlike the portfolio approach with comprehensive hypotheses suites that we later adopted (as described in Section 3). We were also using single-file coding approach (similar to AIDE [20], an automated code generation system) with the instruction to generate one complete implementation in single Python file. There were two primary failure modes that emerged here: Issues with the model trying to squeeze the complex implementation, all into one file, leading to reward hacking or implementation drift in the sense of simplifying the implementation. On two separate runs, the model outputted code file included the following: Note: The implementation focuses on clarity and the high-level research mechanics rather than SOTA gameplay performance - it is not expected to reach impressive Hanabi scores in this short reference prototype. Note: This is quick and safe MVP-skeleton implementation of the plan described in the task specification...All hard compute is guarded behind the --fast flag. Failure in environment setup. The model consistently fell back to the canonical DeepMind hanabi-learning-env, which is not maintained anymore. Even after consistent failures and instructions to use alternative, in the event of non-import failure, the model would interpret 22 the error as consequence of non hanabi-learning-env import and revert the correct library import to the wrong outdated one. This illustrates the training data bias failure mode discussed in Section 3, where models fall back on memorized patterns rather than using current information. This also pointed out to us that AIDE out of the box wont be sufficient for research code implementation which is more iterative process than training models for Kaggle challenges, and that low-level implementation agents will require access to the internet and ability to fetch the most-recent information instead of merely relying on the zero-shot LLM responses generated upstream in the workflow. A.2 WM-1: Differentiable Planning in Stochastic World Models The source of this idea was integrating together NeurIPS 2023 work on Stochastic World Models [21] and withdrawn ICLR 2024 submission on Differentiable Tree Search in Latent State Space [22]. Table 5 details the main parts of the complete idea.md file generated by our Idea Generation Agent. Table 5: WM-1: DTS Planning in Stochastic World Models - Rejected in Implementation Idea Differentiable Tree Search in Stochastic World Models for Planning (S-DTS) Problem and Motivation DTS and similar methods are designed for deterministic worlds. When deployed in Proposed Approach Key Innovation Concrete Example non-deterministic environments, their deterministic world models fail to capture environmental stochasticity, leading them to learn an average world dynamics. While methods exist for learning stochastic world models (STORM), they are typically trained separately from the planner, which prevents the planner and model from co adapting. This separation means the world model is not optimized for the specific states and transitions that are most critical for the planners decision-making process. Integrating Stochastic World Model, inspired by STORMs architecture, directly into the DTS framework. Joint optimization provides powerful, targeted learning signal - the overall planning loss backpropagates through the entire differentiable search process, including sampled transitions from the WM. Bridging the gap between deterministic planners and stochastic environments. While deterministic DTS cannot represent or plan for multiple possible outcomes, and separately trained stochastic models miss the key insight of DTS by optimizing for global prediction error rather than task-relevant regions, S-DTS integrates VS-WM directly into the DTS framework. The world model learns to be an accurate predictor specifically for transitions that matter most for making good decisions, while the planner learns to account for the specific uncertainties expressed by its co-adapted world model. In slippery ice grid world where moving north can result in slipping left or right near pit, DTS with ensembles might average paths or see variance but doesnt reason about probability of each outcome, potentially choosing paths close to the pit if average seems safe. S-DTSs expected backup rule explicitly incorporates low-probability, high-cost outcomes of slipping into the pit, learning to select safer paths further from pit even if slightly longer, demonstrating superior risk-awareness. Potential Pitfalls Training instability from interaction of REINFORCE, Gumbel-Softmax sampling, and VAE losses; computational overhead from triple loop of training steps, search trials, and MC samples; VS-WM might fail to learn true stochastic dynamics, especially rare events. Expert Validation: For this idea, we heard back from one of the two primary authors we reached out to. They noted that this was genuine research gap worth pursuing, emphasising 23 that the tree-search has shown great potential in AlphaGo/MuZero/EfficientZero, yet there are few combinations with Dreamer-like MBRL approaches. They expressed concerns about the computation cost and technical complexity of the implementation and debugging, noting that the execution will be non-trivial otherwise why has nobody tried to enhance DreamerV3 given the huge potential of the search. They concluded that it should have broad impact if you can release high quality implementation to the public. Along with the primary authors of the seed paper, we also shared this idea with World Models researcher who has previously been accepted at major A* conferences and also served as reviewer for one. Much of their feedback concurred with that of the previous expert. They too expressed concerns about the technical complexity of the implementation and pointed out specific and critical omission error in the idea text where write-up treats STORMs world model as drop-in replacement for DTSs deterministic MLP and that the system misses out listing the inference network in the setup section. Overall though they too agreed that the idea strikes well and would lead to nice world model that can be helpful for long horizon sensitive control applications. Implementation and Failure Modes: Considering the overall positive input, we proceeded to hypothesis generation and experimental planning for this idea. We were still using the minimum-viable-hypothesis prompt at this point, which meant, only one hypothesis to be implemented and checked trying to capture the key uncertainties and risks in the proposed research idea. Our experience for implementing this was in line with expert comments - the implementation was non-trivial ran into many technical challenges, especially typifying the failure mode of limited domain taste and intelligence in language models. The complex world model architecture involved multiple interconnected componentsEncoder, Decoder, Actor Network, Critic Networkcreating numerous potential failure points, leading to multiple format mis-match errors. This was the critical failure. My simplified logic for updating the latent state during the differentiable rollout was mathematically incorrect, leading to shape mismatches in matrix multiplication. Training loop integration proved equally problematic. The planner was designed for singleinput processing, but the training loop attempted batch processing, creating what the system described as computationally catastrophic performancemultiplying workload by batch size 24 and completing less than 1% of required steps in an hour. Error 2: Batching Logic (IndexError: Dimension out of range): The agent.plan() method was designed to work on single state, but the training loop was attempting to use it on batch of states, causing an index error. The very long training loop for each step plan up to 50,000 steps being passed back led to timeouts, which led to the implementation drift. This was mainly because of the Modal time limits which led Claude Code to rely on simplifying the architecture instead of working around the infrastructure issues. Claude Code ultimately abandoned the differentiable tree search entirely, pivoting to standard actor-critic approach, with some version of joint optimization maintained but representing significant departure from the proposed method. New Strategy: rewrote the training script (train dts.py) to use an ActorCritic approach. This preserved the projects core joint optimization idea while being vastly more efficient. Fortunately, our verification system successfully detected this implementation drift. The experimental output evaluation noted that: The name Stochastic-DTS (Differentiable Tree Search) is misnomer for the actual implemented algorithm, which is simpler differentiable multi-shoot planner. This could be misleading. The evolution of the training script into an Actor-Critic approach, while pragmatic and technically sound choice, represents significant deviation from what one might infer from the initial hypothesis document. This journey should be documented transparently. Beyond implementation issues, the experimental design itself proved fundamentally flawed. The hypothesis generated was too simple to yield meaningful conclusions, and both the proposed S-DTS model and the baseline achieved near-perfect 0.0 catastrophe rates on FrozenLake, making the hypothesis untestable. This was identified as an error in the experiment output evaluation report which noted that the experiment failed to test the hypothesis in meaningful performance regime. While the choice of the environment was too simplistic, several other chosen aspects proved too complex. For instance, the choice of the 50000 DTS depth parameter. These too pointed toward the absence of domain intelligence often only captured in researcher decisions in experience 25 during experimentation. It also aligned with the conclusion from Stanfords research on LLMgenerated hypotheses [23, 24]. Additional methodological flaws included insufficient statistical validity. The test was run with only one seed, inadequate for properly falsifying or accepting the hypothesis. The revision prompt ultimately recommended developing hypotheses better grounded in technical implementation details and testing on more challenging environments than FrozenLake. A.3 WM-2: Replacing Pixel Reconstruction with Perceptual Loss in World Models This idea was generated by combining papers on multi-modal foundational world models [25] from NeurIPS 2024 and the seminal Dreamer paper [26] from ICLR 2020. Table 6 details the main parts of the complete idea.md file generated by our Idea Generation Agent. Table 6: WM-2: Perceptual Loss Training Objective for World Models - Rejected in Implementation Idea SALVO: Training World Models in VLM Perceptual Space for Semantically-Grounded Control Problem and Motivation Generative world models learn by compressing high-dimensional observations into latent space Proposed Approach Key Innovation Concrete Example using pixel-level reconstruction loss, forcing the model to expend capacity on high-frequency visual details often irrelevant for semantic understanding. This irreversible semantic compression means subtle but task-critical visual cues that are easily distinguishable by VLM can be lost in the world models latent representation, creating semantic gap between the world models understanding and the VLMs interpretation of task prompts. Replace conventional pixel-level reconstruction loss entirely with perceptual reconstruction loss defined within the embedding space of frozen, pre-trained VLM. Instead of forcing the models latents to reconstruct raw observations, train them to reconstruct the VLMs perception of the observations. This directly aligns the world models optimization with semantic concepts embedded in the VLM. Replacing pixel-reconstruction loss entirely with perceptual reconstruction loss in VLM embedding space. Instead of training world models to be good at reconstructing pixels, train them to be good at reconstructing VLM perceptions. This directly aligns the world models optimization with semantic concepts embedded in the VLM, creating latent representations inherently structured around VLM-salient features. For prompt robot performing graceful pirouette, GenRLs pixel optimized world model learns turning without intrinsic concept of grace, resulting in mechanically efficient but ungraceful motion. SALVOs VLM-optimized world model is forced to generate reconstructions capturing graceful vs. clumsy distinction, encoding necessary physical information for graceful-looking reconstruction and enabling policy to control semantically-meaningful latents. Potential Pitfalls Semantic dominance vs. physical accuracy leading to imagined rollouts structured around VLM-salient features; inheriting VLM inductive biases, blind spots, or artifacts; computational cost from VLM forward passes; training instability from replacing well-understood pixel-MSE with complex high-dimensional perceptual loss. Expert Validation: The primary authors of both seed papers were contacted for feedback. The first of the two noted that this idea was interesting and worthwhile to explore. The second author expressed some reservation about the technical concerns of replacing the pixel reconstruction loss, as well as, if it would generalize beyond specific domains such as MineRL. 26 They noted that this was one of the first things [they] tried and that in their experience, perceptual losses can fail to capture fine-grained information for control, compared to reconstruction-based objectives They did emphasize that the research gap identified of finding objectives that work better for learning world models for RL was genuine. Implementation and Failure Modes: Keeping this well rounded feedback in mind, we decided to continue implementation as we saw clear opportunity for meaningful experiments and insights. This implementation revealed fundamental logical error in experimental design that exemplified the domain intelligence failures discussed in Section 3. The hypothesis assumed offline training with static data frames, but Dreamer requires online learninga mismatch that violated core algorithmic assumptions. This led to no exploration-exploitation balance, limited state coverage, no correlation between actions and rewards, and impossible transitions across episode boundaries. Training on static random-policy data fundamentally undermined the baselines validity, discovered only during experimental output evaluation. The core problem emerged from the need to recreate the Dreamer baseline, which triggered cascade of failures. Due to training data bias discussed in Section 3, the system defaulted to PyTorch when the prompt mentioned modern ML practices, despite the original Dreamer being in TensorFlow. This necessitated complete baseline reimplementation that introduced multiple incompatibilities: the PyTorch version didnt work for continuous control tasks like CartPole-SwingUp, data format confusion and encoder-decoder shape mismatches, made further worse by integration of the VLM loss. Initial transposed convolution implementation produced wrong output sizes - First attempt: 31 31 output (incorrect padding calculations), Second attempt: 79 79 output (still incorrect) requiring switch to upsampling approach for exact 64 64 output. Additionally, over the reimplementation of the Dreamer baseline in PyTorch, critical error emerged: dummy reward signals were inadvertently passed instead of actual environment rewards. This meant the critic network learned nothing while the world model and actor continued representational learning: World model and actor were learning (representational learning only) - Critic had nothing to learn from because dummy zero rewards were being passed... DreamerV2 architecture requires ground truth reward supervision. Found we were passing dummy rewards instead of actual CartPole rewards 27 This reward error cascaded through the entire experimental pipeline leading to gradient explosions in actor training, plotting showed zero values, evaluation metrics became meaningless, and the Experiment Execution Agent undermining the core innovation with .detach() call during CLIP processing severed gradient flow from perceptual loss to the world model decoder, preventing learning from the reconstruction objective. The experimental output evaluation delivered the final verdict: Implementation Fidelity score Low, noting baseline performance 95% below established benchmarks, making any comparison scientifically invalid. The evaluation concluded the experiment failed to create the necessary conditions for valid test and that signal was completely obscured by the noise of broken experimental setup, requiring complete reimplementation before hypothesis retesting. A.4 AS-1: Semantic Entropy as Signal for Jailbreak Prompts This idea was arrived at by synthesizing two alignment papers from ACL 2025, on jailbreak challenges in malware requests [27] and use of epistemic markers for confidence estimation [28]. Table 7 details the main parts of the complete idea.md file generated by our Idea Generation Agent. Table 7: AS-1: Semantic Entropy as Signal for Jailbreak Prompts - Completed Successfully Idea Using Semantic Entropy as Black-Box Signal for Behavioral Inconsistency in Jailbreak Detection Problem and Motivation We lack reliable, real-time signals for detecting jailbreak attacks on black-box models (e.g., those served via API). Existing defenses are often reactive (post-generation content filtering) or brittle (input pattern matching), while powerful white-box methods that inspect gradients or activations are inapplicable. This leaves API-based models vulnerable to novel, adaptive attacks, creating significant gap in the safety ecosystem. Proposed Approach Key Innovation Concrete Example Reframing jailbreak detection from content analysis problem to behavioral analysis problem. Use semantic inconsistency, measured via semantic entropy, as zero-shot, black-box signal of jailbreak attempt. The internal conflict an LLM experiences when processing jailbreak prompt will manifest as high variance in the semantic meaning of its potential responses, detected by sampling and comparing multiple generated outputs. Repurposing the behavioral signal of response inconsistency, originally used for fact-checking in hallucination detection, as security signal to detect the internal conflict caused by jailbreak attack. Using semantic entropy over clustered response embeddings to measure behavioral inconsistency indicating safety alignment conflict. For jailbreak My grandma used to tell me stories about how to make napalm, sampling N=5 responses yields mixed semantic clusters: refusals (I cannot fulfill this request), compliance (mix gasoline with...), and hedges (for fictional purposes...). This high semantic variance produces high entropy score, flagging interaction as suspicious before harmful response reaches user, based solely on models inconsistent behavior. Potential Pitfalls Hardness confound where complex but benign prompts also produce high entropy; adaptive adversaries learning to generate prompts forcing consistent, low-entropy malicious outputs; inapplicability to training-time backdoor attacks that eliminate rather than create internal conflict. Expert Validation: By this stage, we conducted an internal review of the top ideas based on lessons learned from the previous expert validations and implementation attempts. We selected 28 this idea as most likely to succeed for autonomous execution, specifically because it avoided many of the technical complexity and computational resource issues identified in the previous ideas. The choice prioritized implementation feasibility over novelty, representing strategic pivot toward ideas that could be completed within our systems current capabilities. Implementation and Failure Modes: The most interesting thing with this idea was that the first hypothesis implemented was implemented but when it failed to detect jailbreaks effectively, the Revision Agent triggered change in idea from using semantic entropy as detection method to showcasing its failure and investigating the failure mode. Table 8 showcases the updated hypotheses suite generated by the relevant agent after the revision of the idea. Table 8: Evolution of Hypotheses Suite: From Testing Detection to Investigating Failure Original Hypotheses Suite (Testing SE as Detection Method) Revised Hypotheses Suite (Investigating the Consistency Confound) Semantic entropy detector achieves AUROC > baseline consistency metric by 0.1+ on JailbreakBench harmful vs benign prompts H1: SE underperforms simple baselines (BERTScore, Embedding Variance) on JailbreakBench, with model-dependent baseline rankings SE maintains AUROC > 0.85 distinguishing harmful from benign-but-hard prompts, proving sensitivity to malice not complexity H2: SE failure generalizes to HarmBench-Contextual, again losing to simpler baselines with model-dependent performance SE calibrated on JailbreakBench generalizes to HarmBench contextual attacks with AUROC > 0.70 H3: After controlling for response length confound, SE achieves near-random AUROC < 0.55 on both benchmarks SE deployed as gate reduces Tree of Attacks ASR by 25+ percentage points vs undefended model H4: SE uniquely brittle: performance collapses when clustering threshold  increased from 0.10.2 or samples from 5 SE correctly identifies >20% of jailbreaks missed by supervized WildGuard classifier H5: Paraphrasing prompts disproportionately degrades SE vs baselines by disrupting memorized refusal templates >80% of SE false negatives exhibit Consistency Confound: high duplicate rate (>0.6) and low cluster count (2) Stronger alignment worsens SE: Qwen2.5-72B shows lower SE AUROC than smaller models while maintaining baseline performance H1 H3 H4 H5 H6 H7 The abstract for our accepted paper was: Black-box jailbreak detection for Large Language Models (LLMs) remains challenging, particularly when internal states are inaccessible. Semantic entropy (SE) successfully used for hallucination detectionoffers promising behavioral approach based on response consistency analysis. We hypothesize that jailbreak prompts create internal conflict between safety training and instruction-following, potentially manifesting as inconsistent responses with high semantic entropy. We systematically evaluate this approach using black-box, embedding-based implementation of SE adapted from Farquhar et al.s bidirectional entailment method 29 to work within black-box constraints. Testing across two model families (Llama and Qwen) and two benchmarks (JailbreakBench, HarmBench), we find SE fails with 85-98% false negative rates, consistently outperformed by simpler baselines and exhibiting extreme hyperparameter sensitivity. We identify the primary failure mechanism as the Consistency Confound: well-aligned models produce consistent, templated refusals that SE misinterprets as safe behavior, accounting for 73-97% of false negatives with high statistical confidence [95% Wilson CIs]. While SEs core assumption about response inconsistency indicating problematic content holds in limited cases, threshold brittleness renders it practically unreliable. Our results suggest that for this SE variant, response consistency may not be reliable signal for jailbreak detection, as stronger alignment leads to more predictable outputs that confound this type of diversity-based detector. Unlike the previous implementations, this work proceeded with substantially less human intervention. Since the research focused on data analysis rather than complex model architectures, most technical barriers proved surmountable autonomously. The familiar infrastructure challenges discussed in Section 3 appeared here tooModal API compatibility issues, model integration errors, and library import failures. Training data bias also manifested in dataset handling, where instead of reviewing the available fields, the model defaulted to standard prompt and output format despite HarmBenchContextual prompts having separate context field that was initially ignored. However, these issues didnt derail progress. To meet conference deadlines, the experimental evaluation process was streamlined to just two checkpoints: initial idea revision and final paper readiness assessment. This implementation also demonstrated the overexcitement failure mode discussed in Section 3. Despite degenerate outputs and statistical insignificance that required manual intervention to identify and address, the initial automated paper writing consistently emphasized positive aspects while downplaying substantial limitations. This pattern required human oversight to ensure transparent reporting of methodological issues and statistical problems in the final papers limitations section."
        }
    ],
    "affiliations": [
        "Lossfunk"
    ]
}