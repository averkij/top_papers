{
    "paper_title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective",
    "authors": [
        "Deyang Kong",
        "Qi Guo",
        "Xiangyu Xi",
        "Wei Wang",
        "Jingang Wang",
        "Xunliang Cai",
        "Shikun Zhang",
        "Wei Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \\textbf{C}ompetence-\\textbf{D}ifficulty \\textbf{A}lignment \\textbf{S}ampling (\\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \\textbf{2.33} times slower than CDAS."
        },
        {
            "title": "Start",
            "content": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: Competence-Difficulty Alignment Perspective Deyang Kong1,2, Qi Guo1,2, Xiangyu Xi1, Wei Wang1, Jingang Wang1, Xunliang Cai1,Shikun Zhang2,Wei Ye2 1Meituan Group, Beijing, China 2National Engineering Research Center for Software Engineering, Peking University, Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces Competence-Difficulty Alignment Sampling (CDAS), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the models current competence using fixed-point system. Experimental results across range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, competitive strategy in DAPO, which is 2.33 times slower than CDAS. 5 2 0 2 3 2 ] . [ 1 2 5 6 7 1 . 5 0 5 2 : r Figure 1: Average accuracy (left) and training GPU hours (right) of different sampling strategies. Equal contributions. kong.dyang@outlook.com , qguo@stu.pku.edu.cn; Correspondence to: xixy10@foxmail.com, wye@pku.edu.cn. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Advanced large language models (LLMs) exemplified by DeepSeek-R1 [1] and OpenAI O1 [2] demonstrate remarkable performance in challenging tasks like mathematics. As core technology in their reports, the Reinforcement Learning algorithm (RL), such as Proximal Policy Optimization [3] (PPO) and Group Relative Policy Optimization [4] (GRPO), is employed to amplify the reasoning capabilities of the models. It works by utilizing verifier as the reward model to guide the generation of high-quality reasoning chains without the need for data annotation. Despite the promise, the RL training is costly and hard to scale, particularly due to its low sample efficiency during the rollout phase. Recent studies [5, 6, 7] indicate that sampling overly difficult problems often results in no correct chains, while sampling overly simple problems contributes little to model capabilities, leading to computational waste. Consequently, host of efforts are devoted to exploring sampling strategies for more efficient and stable RL training. Existing strategies draw inspiration from Curriculum Learning (CL) [8, 9], scheduling data based on problem difficulty to enhance training stability and efficiency. Curriculum Sampling Strategy [10] relies on prior difficulty labels, which are excessively offline, neglecting the inherent capabilities of the model. Dynamic Sampling used by DAPO[5] demonstrates promising results by oversampling and filtering out problems with the pass rate equal to 1 and 0, which incurs substantial rollout overhead and compromises the training efficiency. Prioritized Sampling Strategy used by Kimi k1.5[10] records the latest pass rate of each problem during training and adaptively assigns higher sampling probability to those with lower pass rates. Overall, the pass rate has been widely adopted as proxy for modeling problem difficulty. However, these strategies tend to be suboptimal due to two main issues: (1) Unstable and Biased Estimations of Problem Difficulty Using Single-Step Pass Rate. As our experiment of training Qwen-2.5 7B[11] with MATH dataset[12] shows  (fig. 2)  , the pass rate of individual problems exhibits considerable fluctuations throughout the training process, leading to unstable estimations of problem difficulty, consistent with several prior works[13, 14]. Moreover, focusing solely on the final pass rate can introduce difficulty bias. For instance, the pass rate of problem 2 at step 5 happens to surpass that of problem 1, despite their distinct trajectories and difficulty levels. This demonstrates that the pass rate at single step fails to capture the true complexity and learning dynamics associated with each problem. (2) Failing to Appropriately Capture the Alignment between Model Competence and Problem Difficulty. common strategy, such as in curriculum sampling and prioritized sampling, is to assign higher sampling probabilities to more difficult problems with lower pass rates. practical consequence is that overemphasizing difficult samples often leads to the selection of many zero-gradient problems with pass rate of 0 in GRPO training, limiting the training efficiency. However, more appropriate approach, as advocated in the curriculum learning[8, 9], is to prioritize problems that are more aligned with the current level of competency of the model for more efficient and effective training. Figure 2: The variation in pass rate of problems in palin GRPO on Qwen-2.5 7B[11] using MATH dataset. To address the issues above, we propose Competence-Difficulty Alignment Sampling (CDAS) for RL training, dynamically sampling problems whose difficulty matches the model competence at the step level. The core intuition behind CDAS is twofold: (1) instead of relying solely on the pass rate at single step, an accumulative estimation that incorporates all historical information tends to yield more stable assessment of problem difficulty; and (2) explicitly modeling model competence to measure its alignment with problem difficulty, thereby enabling more effective sampling decisions. Specifically, we model problem difficulty as the trajectory of performance discrepancy over training steps, where each point reflects the gap between the expected and actual pass rate. Then we use the centroid of this trajectory to provide stable and accurate assessment of problem difficulty. Further, model competence is defined as the negative expected difficulty across all problems, and the absolute difference between model competence and problems difficulty is used to quantify their alignment. Considering the dynamics of RL training, the competence-difficulty alignment estimation above is further formulated as difficulty-based fixed-point system, which can iteratively converge and ensures the stability of training with theoretical guarantees. To valid the effectiveness of CDAS, we conduct GRPO training on Qwen2.5-7B[11] using series of sampling strategies [10, 5]. Results across 7 comprehensive mathematical reasoning benchmarks[12, 15, 16, 17], show that CDAS consistently outperforms powerful baselines (see fig. 1), achieving the highest average accuracy of 46.77%. Compared to Dynamic Sampling, highly competitive baseline, CDAS achieves slightly better performance +0.12% while considerably reducing the training step time overhead by 57.06%. Additionally, CDAS continually improves performance on more challenging benchmarks, achieving an accuracy of 11.77% on AIME25, which surpasses the plain GRPO by +4.79% and Dynamic Sampling by +2.19%. Subsequent analysis demonstrates that CDAS flexibly allocates computational resources by successfully sampling valuable problems that exhibit pass rate not equal to 0 or 1. Further experiments on code generation tasks and larger 14B models also highlight the generalization ability of our method. The contributions of this paper can be summarized as follows: We identify and analyze the limitations of existing sampling strategies from new perspective, highlighting the importance of stable difficulty estimation and dynamic competencedifficulty alignment in RL training for LLMs. We introduced Competence-Difficulty Alignment Sampling (CDAS), adaptively selecting problems that match the model competence, which is grounded in theoretically guaranteed fixed-point system. Extensive experiments validate its effectiveness and efficiency."
        },
        {
            "title": "2 Related Work",
            "content": "RL for LLMs reasoning. Reinforcement learning (RL) has been widely adopted to enhance the reasoning abilities of LLMs, especially in mathematics and programming tasks [16, 18, 19, 20]. Actor-critic-based methods, such as Proximal Policy Optimization (PPO) [3], utilize value model to estimate the value function, guiding the policy updates. On the other hand, REINFORCE-based methods rely on policy gradients without value model. Group Relative Policy Optimization (GRPO) [4] normalizes rewards within group of generated outputs, eliminating the need for separate value model. REINFORCE++ [21] enhances the classical REINFORCE algorithm [22] by incorporating optimization techniques from PPO. Dynamic Sampling Policy Optimization (DAPO) [5] introduces several optimizations to enhance training efficiency and stability in long-CoT reasoning tasks. Sampling Strategies for RL training. Effective sampling strategies are crucial for efficient RL training with LLMs. Coarse-grained curriculum learning [10, 23] gradually increases problem difficulty based on predefined labels. LIMR [24] introduces Learning Impact Measurement (LIM) to select problems that align with the models learning trajectory. Prioritized Sampling [10] tracks the pass rate for each problem and samples problems in proportion to their failure rates. This approach directs the models focus toward more challenging problems. Dynamic Sampling [5] continues to sample problems within batch until their pass rates are neither 0 nor 1. While this ensures that the model focuses on problems that are neither too easy nor too hard, it can lead to extremely heavy computational overhead as the training steps increase."
        },
        {
            "title": "3 Preliminary",
            "content": "Group Relative Policy Optimization GRPO utilizes group-based advantage without value model, thereby reducing computational overhead. Formally, given problem x, the correct answer y, and group of sampled responses { ˆyi}G i=1, GRPO calculates the advantage by normalizing the rewards within each group. The original GRPO objective employs sample-level loss calculation which potentially introduces length biases [5, 25, 26], so we utilize token-level policy gradient loss as our objective function: i=1 with their corresponding rewards {ri}G JGRPO(θ) = [xD,{ ˆyi}G i=1πold(x)] 1 i=1 ˆyi (cid:80)G (cid:88) ˆyi (cid:88) (cid:18) i=1 t= (cid:16) (cid:16) ri,t(θ) ˆAi,t, clip min ri,t(θ), 1 ϵ, 1 + ϵ (cid:17) (cid:17) ˆAi,t βDKL[πθ πref] (cid:19) (1) , where ri,t(θ) = πθ(ˆyi,t x, ˆyi,<t) πθold (ˆyi,t x, ˆyi,<t) , ˆAi,t = ri mean({ri}G std({ri}G i=1) i=1) . (2) 3 Rule-Based Reward The use of reward model usually leads to reward hacking problem [27, 28], so we use rule-based reward function. The reward is computed using the following rule: (cid:26)1 0 is_equivalent(y, ˆy) otherwise r(y, ˆy) = (3) Notably, we do not employ format reward in GRPO implementation. Prior research indicates that strict format constraints may limit the upper bound of model performance [6, 29, 30]. Therefore, we directly use the final accuracy as the reward."
        },
        {
            "title": "4 Competence-Difficulty Alignment Sampling",
            "content": "In this section, we will systematically elaborate on the framework of Competence-Difficulty Alignment Sampling (CDAS), including modeling the problem difficulty from the perspective of trajectory, measuring the alignment between model and problems, and how to incorporate our CDAS into iterative GRPO training as fixed-point system. 4.1 Modeling Problem Difficulty as Trajectory We start with simple example in fig. 3, where the pass rate of problem increases from 0 to 1 as the training step increases. Measuring difficulty based solely on the pass rate fails to provide stable estimation due to the significant fluctuations in its pass rate along with the inherent limitations of information at single step. Intuitively, by taking into account the historical performance across all previous samplings, we can obtain much more stable and informative estimation. We plot the average historical pass rate up to the current step in fig. 3, which results in much smoother curve that reflects the underlying historical trend. Figure 3: Pass Rate vs Step. Inspired by this, we turn to model the difficulty of problem using the trajectory of the models performance discrepancy on the problem. Specifically, for the model Mn at the n-th training step, its performance discrepancy on is defined as dn(x) = ˆPMn (yx) PMn (yx), (4) where PMn (yx) represents the actual probability of the model solving the problem x, estimated by the pass rate s(x). ˆPMn (yx) represents the expected probability that the model can solve x. Then, given the existing trajectory {d1(x), d2(x), ..., dn(x)}, we quantify the difficulty of the problem using the centroid of the trajectory Dn(x) = 1 (cid:88) k= (cid:17) (cid:16) ˆPMk (yx) PMk (yx) . (5) The centroid considers the cumulative performance discrepancies over multiple training steps, providing more accurate and robust measure of problem difficulty. The concept of performance discrepancy can be seen as derivation from the 1 s(x) approach in Prioritized Sampling[10], serving as refined estimation of difficulty at each individual training step. The 1 s(x) metric can be regarded as special case of the performance discrepancy, where the model is assumed to be capable of solving all problems. The introduction of ˆPMn (yx) amplifies the influence of steps where the actual pass rate deviates significantly from the models expectation. The estimation of ˆPMn (yx) is supposed to account for both the problem difficulty and the model competence. Here we employ the sigmoid function, which is widely used in probability modeling, to describe such probability ˆPMn (yx) = 1 1 + e(Cn1Dn1(x)) = σ(Cn1 Dn1(x)), (6) 4 Algorithm 1 Competence-Difficulty Alignment Sampling in GRPO for = 1 to do A(xj, Mn1) Cn1 Dtj (x). end for Sample = B+ based on for (xj, yj) in do Require: Training set {(x1, y1), (x2, y2), . . . , (xn, yn)}, Model M0, total steps K, batch size 1: Initialize C0 = 0, tj = 0, Dtj (xj) = 0 (j = 1, 2, . . . , ) 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for end for Cn Ex[Dtj (x)] Mn+1 Update the policy model Mi by maximizing the GRPO objective Compute pass rate sn(xj) Compute rewards for each sampled response tj tj + 1 Dtj (xj) tj 1 tj (σ(Cn1 Dtj 1(xj)) sn(xj)) Dtj 1(xj) + 1 tj // GRPO Rollout where Ci1 represents the models competence at the previous step, quantified as the negative expectation of the difficulties over all problems Cn1 = Ex[Dn1(x)]. (7) When the model competence exceeds the problem difficulty, the expected probability of the problem being solved is higher. Otherwise, it is lower when the competence is lower than the difficulty. 4.2 Step-Level Adaptive Sampling based on Quantified Alignment From eq. (6), we can see that Cn1 Dn1(x) quantifies the degree of deviation between the models competence and the problem difficulty. As result, we naturally introduce the definition of the alignment between single problem and the model Mn as A(x, Mn) = Cn1 Dn1(x). (8) The smaller the value of A, the better the problem difficulty matches the current competence of the model. At each training step, we adopt symmetric sampling method to construct the problem set = B+ from problems with the smallest for rollout, where = argminB/2 A(x, Mn), B+ = argminB/2 A(x, Mn), (10) Compared to directly selecting the problems with the smallest A, such symmetric method maintains balanced proportion of hard and easy problems in B, promoting training stability. s.t. Cn1 Dn1(x) > 0. s.t. Cn1 Dn1(x) 0, (9) 4.3 Reinforcement Learning with CDAS Practically, for each training step, we first measure the alignment between the previous difficulty and model competence, and then obtain the problem set for rollout based on the symmetric sampling above. After one step, the models competence and the difficulty of the problems are dynamically updated. The whole process repeats until the training converges, illustrated in algorithm 1. Note that the update of problem difficulty can be viewed as fixed-point system, formulated as (cid:26)D(x) = σ(C D(x)) S(x), = Ex[D(x)] (11) Due to the properties of the sigmoid function which is contraction mapping, the system will converge to unique solution. Refer to appendix for detailed proof of the convergence. Note that since is usually much smaller than the size of the training set, performing full update of problem difficulties at each step will lead to heavy computational overhead. Instead, for problem xj, we record the number of times it is sampled as tj and update its difficulty only when it is sampled. Specifically, at the i-th step, the problem difficulty can be updated iteratively by Dtj (xj) = tj 1 tj Dtj 1(xj) + 1 tj (σ(Cn1 Dtj 1(xj)) sn(xj)). (12) Table 1: Performance comparison across different sampling methods on various math benchmarks. Metrics are Avg@32 for AIME/AMC and standard accuracy for others. We present the best results in bold and the second with underline. Average Step Time (s) AIME24 (Avg@32) AIME25 (Avg@32) AMC23 (Avg@32) MATH Minerva Math Olympiad Bench GSM8K Avg. - 1236 1236 1509 2875 1420 1245 1375 1670 3745 0.06 10.00 10.00 11.77 12.19 11.56 12.29 12.71 15.10 15.10 14.90 0 7.29 7.29 7.08 7.92 7.71 6.98 7.19 9.27 9.58 11. 9.92 49.92 49.92 49.92 51.88 49.92 53.98 53.98 54.61 53.44 52.03 47 74.20 74.20 74.00 75.00 76.20 75.2 76.0 75.0 77.2 75. 17.65 36.76 36.76 39.34 38.97 38.97 37.13 38.60 37.50 39.34 40.44 14.52 39.26 39.26 40.15 38.52 40.44 40.00 39.56 39.56 39.56 40. 81.5 91.66 91.66 92.04 91.36 92.19 92.95 92.57 91.51 92.34 91.96 24.38 44.73 44.73 44.90 45.12 45.28 45.50 45.80 46.08 46.65 46. Steps Method 0 Qwen2.5-7B-base 55 110 Random Sampling Curriculum Sampling Prioritized Sampling Dynamic Sampling CDAS (Ours) Random Sampling Curriculum Sampling Prioritized Sampling Dynamic Sampling CDAS (Ours)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setups Dataset Following [6], we utilize the MATH dataset [12] for RL training, including 7,500 training samples and 4,500 test samples. We reserve its MATH500 subset as the validation set for RL training. Baselines We compare CDAS against range of powerful baselines: (1) Random Sampling, which directly samples the problem randomly from the training dataset and can be viewed as plain GRPO training. (2) Curriculum Sampling [10], which uniformly samples easy problems first, then turns to harder ones. Here, we use the difficulty level tags included in the MATH dataset, ranging from 1 to 5, and conduct training from the middle checkpoint in Random Sampling on problems with difficulty level 4. (3) Prioritized Sampling, which is adopted in Kimi k1.5[10] that tracks the pass rate of each problem and then samples problems based on 1 s.(4) Dynamic Sampling, adopted in DAPO[5], which over-samples and filters out problems with the pass rate equal to 1 or 0. Training Details We conduct GRPO training experiments on Qwen-2.5-7B [11] using the veRL framework [31]. Following the training recipe in [6], we set the batch size = 1024, generating 8 rollouts for each problem with temperature 1.0 and maximum response tokens 4096. The training runs for total 110 steps, which equals training 10 epochs on the whole training set. Considering the stability of iterations, we adopt warm-up strategy in CDAS that ensures problems are sampled randomly within the first 11 steps to obtain more accurate initial values for problem difficulties. Refer to appendix for more training details. Evaluation For comprehensive evaluation, we select 7 mathematical reasoning tasks, including AIME 24&25, AMC23, MATH500 [12], Minerva Math [15], Olympiad Bench [16] and GSM8K [17]. Since the number of problems in AIME24&25 and AMC23 is relatively small, we report the Avg@32 metric to reduce randomness. Standard accuracy is reported for the remaining tasks. More details are shown in appendix B. 5.2 Main Results The main results are summarized in table 1, with training curves illustrated in fig. 4. Besides the final checkpoint, we also report the intermediate checkpoint at the 55-th step. The accuracy curve on each benchmark can be found in appendix D. From the results, we have the following findings: CDAS outperforms plain GRPO and other baselines, achieving the highest average accuracy of 45.28% at the 55-th step and 46.77% the 110-th step. We also find that the improvements offered by Curriculum Sampling (+0.30% at the 110-th step) and Prioritized Sampling (+0.58% at the 110-th step) over the Random Sampling baseline are limited. It indicates that relying solely on prior difficulty labels or single-step pass rates tends to be suboptimal. CDAS demonstrates substantial efficiency advantage over Dynamic Sampling. While Dynamic Sampling achieves performance comparable to CDAS (46.65% vs. 46.77%) by oversampling and excluding problems with pass rates of 1 or 0, it incurs significantly greater computational overhead, requiring 2.33X more computation than CDAS. This result clearly highlights the superior efficiency of CDAS to Dynamic Sampling. 6 (a) Random Sampling and Curriculum Sampling (b) Prioritized Sampling (c) Dynamic Sampling (d) CDAS Figure 4: Training curves of different sampling strategies. CDAS consistently enhances the performance on more challenging benchmarks. On benchmarks like AIME24/25, Minerva Math and Olympiad Bench, CDAS demonstrates remarkable improvements. For instance, CDAS achieves 11.77% accuracy on AIME25 and 40.89% on Olympiad Bench, notably higher than baselines. The reward curve in CDAS initially grows and then converges to median value. From fig. 4, after 30 steps, the amplitude of reward narrows to within the range of 0.4 to 0.6 and then converges around 0.5. This indicates that CDAS effectively selects problems with difficulties appropriate for the model. On the other hand, since Prioritized Sampling tends to select harder problems, its reward demonstrates continuous downward trend. An increased response length is not necessary for better performance. Although we observed rise in response length in CDAS and Curriculum Sampling, the response length in Dynamic Sampling stabilized after about 15 steps, yet the average accuracy on benchmarks continued to grow. 5.3 Ablations Since CDAS is built on fixed-point system, the choice of initial values can affect its convergence. We investigate this impact by removing the warmup phase and starting sampling from the first step. As observed in fig. 5, the training curve exhibits significant fluctuations in the early stages of training (the first 20 steps) without the warmup stage. Specifically, in the first 6 steps, the removal of the warmup phase leads to notable decrease in response length along with rapid increase in reward, suggesting that the model has over-learned from simpler samples. However, as the convergence of the entire fixed-point system is guaranteed, the two training curves gradually overlap, further corroborating the stability of our framework. Figure 5: Ablation study of the warm-up phase 7 We further investigate the effectiveness of alignment-based symmetric sampling. For comparison, we directly select the problems with the smallest value of for the rollout phase. As seen in fig. 6, after removing symmetric sampling, CDAS consistently outperforms the Random Sampling baseline. However, there is still an obvious decline in accuracy, especially in the later stages of training (after 100 steps). We attribute this phenomenon to the accumulated estimation errors in model competence due to the imbalanced sampling. In the early stages of training, imbalanced sampling can cause the models competence to be either overestimated or underestimated. This discrepancy from the true competence accumulates as the number of training steps increases and hinders the convergence of CDAS, impairing the performance in the later stages. Figure 6: Ablation study of symmetric sampling."
        },
        {
            "title": "6 Analysis and Discussion",
            "content": "In this section, we conduct comprehensive analysis and discussion of CDAS. We explore the statistical properties of CDAS in terms of sample utility, investigate its advantages over pass ratebased methods, and validate its generalization to code generation tasks along with effectiveness on larger-scale models. 6.1 Utility of the Sampled Problems From the perspective of the optimization objective of GRPO, the superior performance of Dynamic Sampling can be attributed to its filtering out of samples that do not contribute to model gradients[5] (i.e., those with pass rate of 0 or 1). Although CDAS does not explicitly constrain the pass rate in problem selection, its alignment-based symmetric sampling inherently mitigates the issue of oversampling the zero-gradient problems to some extent. As illustrated in fig. 7, the proportion of such zero-gradient problems within the batches sampled by CDAS is consistently lower than that of the other baselines, proving that CDAS can effectively improve the utility of sampled problems. We also observe that the proportion of zerogradient problems in CDAS exhibits rapid decline during the early stages of training, followed by slight increase in the later stages. The sharp decrease in the initial phase can be attributed to the swift correction of problem difficulty from its initial values. The modest rise in the later phase is mainly due to the increasing proportion of zero-gradient problems in the whole MATH training set, leading to more problems with pass rate of 0 sampled in the batch B+. Figure 7: The proportion of zero-gradient problems in the sampled batch. 6.2 Problem Difficulty vs. Pass Rate Since the problem difficulty in CDAS derived from the pass rates, we explore the relationship between them. As illustrated in fig. 8, problem difficulty and pass rate exhibit an overall negative correlation where problems with lower difficulty tend to have higher pass rates. Interestingly, we find that even among problems with the same pass rate, there can still be considerable differences in their estimated difficulties. To further investigate this phenomenon, we randomly selected two problems with pass rate of 1 at the final sampling step. 8 (a) Performance on Livecodebench. (b) Average accuracy on Qwen2.5-14B[11]. Figure 9: Generalization performance of CDAS against the Random Sampling Baseline. As shown in fig. 8, problem required 25 samplings to reach pass rate of 1, whereas problem achieved pass rate of 1 after only 6 samplings. Despite both having the same final pass rate in rollout, Problem is noticeably more difficult than Problem B, as indicated by its average accuracy of 32 inferences is 0.6875, which is much lower than that of Problem (Avg@32 = 1.0). This validates that CDAS, by leveraging historical information, provides more accurate and robust measure of problem difficulty. 6.3 Generalization Figure 8: Problem difficulty vs. pass rate in CDAS. Extension to Code Generations To further investigate the effectiveness of our approach in diverse domains, we apply CDAS to GRPO training on code generation tasks. Specifically, We aggregate open-source data from Apps[32], Taco[33], and CodeContests[34], and perform GRPO training on Qwen2.5-Coder-7B[35] for 100 steps. Evaluation results on the LiveCodeBench v5 [19] are shown in fig. 9(a). Despite the challenges posed by this task for 7B size models, CDAS consistently outperforms the vanilla GRPO baseline in both pass@8 and pass@1 metrics after just 50 steps. Effectiveness on Larger LLMs In addition to the strong performance observed on Qwen2.5-7B, we further validate the effectiveness of CDAS on larger LLMs. Specifically, we conduct training on Qwen2.5-14B[11] with batch size of 256 for 200 steps, matching the computational budget of our main experiments. The average accuracy of CDAS and Random Sampling is reported in fig. 9(b). We find that CDAS achieves substantial improvements over the Random Sampling baseline by +1.47%, which is even greater than the improvement observed on the 7B model +1.27%, showing effectiveness on larger models."
        },
        {
            "title": "7 Conclusion",
            "content": "We present Competence-Difficulty Alignment Sampling (CDAS), novel sampling strategy for RL training in LLM reasoning. CDAS addresses the limitations of existing methods by modeling problem difficulty as trajectory of performance discrepancies to provide more stable estimations and explicitly aligning it with model competence at each training step throughout fixed-point system. Extensive experiments on mathematical reasoning benchmarks demonstrate the superiority of CDAS in both accuracy and efficiency to powerful baselines. Our results highlight the importance of dynamically matching problem difficulty to model competence for efficient RL training."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [2] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [3] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [4] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [5] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [6] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [7] Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. [8] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. [9] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: framework and survey. Journal of Machine Learning Research, 21(181):150, 2020. [10] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [11] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [13] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. [14] Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, and Dong Yu. Stabilizing rlhf through advantage model and selective rehearsal. arXiv preprint arXiv:2309.10202, 2023. [15] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [16] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [18] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [19] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. [20] Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces, 2025. [21] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [22] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. [23] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [24] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025. [25] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [26] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. [27] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. [28] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024. [29] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. [30] Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024. [31] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [32] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [33] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. 11 [34] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022. [35] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [36] Walter Rudin. Principles of mathematical analysis. 2021. [37] Stefan Banach. Sur les opérations dans les ensembles abstraits et leur application aux équations intégrales. Fundamenta mathematicae, 3(1):133181, 1922."
        },
        {
            "title": "A Training Details",
            "content": "The prompt template used for our zero RL training is shown below: Training prompt of our zero RL Question:n{question} Please reason step by step, and put your final answer within boxed{}. Answer:n Table 2 presents the key configuration used for our Qwen2.5-7B experiment. Training was conducted on single node with 8 A100 GPUs, and the model was trained for 110 steps using the veRL library [31]. For Qwen2.5-14B experiment, training was conducted on four nodes, each equipped with 8 A100 GPUs. The model was trained for 200 steps with batch size of 256. Other configurations were kept consistent with those used in the Qwen2.5-7B experiment. 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 python -m verl . trainer . main_ppo algorithm . adv_estimator = grpo data . train_batch_size =1024 data . val_batch_size =500 data . max_p rompt_length =1024 data . max_r espo nse_ len gt =4096 actor_rollout _ref . actor . optim . lr =5 -7 actor_rollout _ref . model . use_remove_padding = True actor_rollout _ref . actor . ppo_mini_batch_size =256 actor _rollout_ref . actor . _ r _ c _ e _ _ =4 actor _rollout_ref . actor . use_kl_loss = True actor _rollout_ref . actor . kl_loss_coef =0.001 actor _rollout_ref . actor . entropy_coeff =0.001 actor _rollout_ref . actor . clip_ratio =0.2 actor _rollout_ref . actor . kl_loss_type = low_var_kl actor _rollout_ref . model . b _ d t _ c i n = True actor _rollout_ref . actor . fsdp_config . param_offload = False actor _rollout_ref . actor . fsdp_config . grad_offload = False actor _rollout_ref . actor . fsdp_config . optimizer_offload = False actor _rollout_ref . rollout . temperature =1.0 actor _rollout_ref . rollout . _ b _ ro _ c _ ze =160 actor _rollout_ref . rollout . s _ e _ a l _ e =4 actor _rollout_ref . rollout . en ab le_ chu nk ed_ pr efi ll = False actor _rollout_ref . rollout . ma x_ num _ba tc hed _t oke ns =5120 actor _rollout_ref . rollout . name = vllm actor _rollout_ref . rollout . gp u_ mem ory _u til iz ati on =0.8 actor _rollout_ref . rollout . =8 actor _rollout_ref . ref . g_ b _ r _b h _ e =160 actor _rollout_ref . ref . fsdp_config . param_offload = True algorithm . kl_ctrl . kl_coef =0.001 critic . _ r _ c _ e _ _ =4 trainer . critic_warmup =0 trainer . n_gpus_per_node =8 trainer . nnodes =1 trainer . remove_clip = False Table 2: Key configuration for our experiment."
        },
        {
            "title": "B Evaluation Benchmarks",
            "content": "We evaluate mathematical problem-solving ability using variety of well-known benchmarks, ranging from middle school to Olympiad-level difficulty: AIME (American Invitational Mathematics Examination): challenging high school level competition featuring 30 problems with integer answers. It is wellestablished benchmark for evaluating advanced mathematical reasoning. Available at https://huggingface.co/datasets/math-ai/aime25 and https://huggingface. co/datasets/math-ai/aime24. AMC (American Mathematics Competitions): benchmark focuses on mathematical problem-solving skills for middle and high school students. Problems range in difficulty and emphasize logical reasoning. Dataset available at https://huggingface.co/datasets/ math-ai/amc23. MATH500 [12]: curated subset of the larger MATH dataset, consisting of 500 problems that span various mathematical fields such as algebra, geometry, and number theory. MinervaMath [15]: benchmark used to evaluate the performance of large language models on detailed, multi-step quantitative reasoning problems. OlympiadBench [16]: benchmark specifically designed for assessing models on Olympiad-level mathematics, including problems from national and international math competitions. GSM8K [17]: dataset of grade school math word problems, commonly used to test arithmetic and step-by-step reasoning in models."
        },
        {
            "title": "C Convergence Analysis",
            "content": "Recall the following coupled fixed-point system: (cid:26)D(x) = σ(C D(x)) S(x), = Ex[D(x)] (13) where σ(z) = 1+ez is the sigmoid function, and S(x) is constant with x. Let = (D(x1), D(x2), ..., D(xN )) denote the vector of problem difficulties, and denote the model competence. The system can be written as joint mapping in (N + 1)-dimensional space: (cid:40) Dn+1(x) = σ(Cn Dn(x)) S(x) Cn+1 = 1 i=1 Dn+1(xi) (cid:80)N or equivalently, (Dn+1, Cn+1) = F(Dn, Cn). The sigmoid function σ(z) is Lipschitz continuous[36] with constant Lσ = 1 and C, , we have: 4 . For any D(x), D(x) σ(C D(x)) σ(C D(x)) 1 4 (C + D(x) D(x)) For two states (D, C) and (D, ), define the distance as (D, C) (D, ) = max (cid:110) max D(x) D(x), (cid:111) Then, Dn+1(x) n+1(x) 1 4 (Cn + Dn(x) n(x)) Taking the maximum over all x, we obtain max Dn+1(x) n+1(x) (cid:16) 1 Cn + max Dn(x) n(x) (cid:17) 1 2 δn 14 where δn = max {maxx Dn(x) For the update of C, we have n(x), Cn n}. Cn+1 n+1 = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) Dn+1(x) +"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) n+1(x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) max Dn+1(x) n+1(x) Therefore, the joint mapping satisfies. (Dn+1, Cn+1) (D n+1, n+1) 1 2 (Dn, Cn) (D n, n) That is, the contraction constant is 2 < 1. Since the joint mapping is contraction in the (N + 1)-dimensional space[37], by the Banach fixed-point theorem, the sequence (Dn, Cn) converges to unique fixed point (D, ), regardless of the initial values."
        },
        {
            "title": "D Full Experimental Results",
            "content": "Figure 10 displays the full results for the Qwen2.5-7B model, while fig. 11 displays the corresponding outcomes for the Qwen2.5-14B model. 15 (a) AIME24 (b) AIME25 (c) AMC23 (d) MATH (e) MinervaMath (f) OlympiadBench (g) GSM8K Figure 10: Qwen2.5-7B full experimental results. 16 (a) AIME (b) AIME25 (c) AMC23 (d) MATH500 (e) MinervaMath (f) OlympiadBench (g) GSM8K Figure 11: Qwen2.5-14B full experimental results."
        },
        {
            "title": "Limitations",
            "content": "(a) Easy-biased dataset (b) Hard-biased dataset Figure 12: The impact of dataset difficulty distribution. In our main experiments, the difficulty of the problems in MATH is distributed around the models capabilities. However, the difficulty distribution of the dataset may be biased in practical scenes. To explore the impact of the overall difficulty of the dataset on the training of CDAS, we sample an extremely easy subset containing 3000 problems with the highest pass rate and an extremely difficult subset containing 3000 problems with the lowest pass rate based on the results of Qwen2.5-7B. We compare CDAS with Prioritized Sampling and Random Sampling, training them for 30 steps. We observe an interesting phenomenon that when the overall difficulty of the dataset is extremely biased, there is no obvious difference between different sampling strategies (see fig. 12). We conjecture that one of the prerequisite for sampling strategies to be effective in the RL training is that the difficulty distribution of the dataset does not deviate too much from the models capabilities."
        }
    ],
    "affiliations": [
        "Meituan Group, Beijing, China",
        "National Engineering Research Center for Software Engineering, Peking University, Beijing, China"
    ]
}