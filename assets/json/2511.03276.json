{
    "paper_title": "Diffusion Language Models are Super Data Learners",
    "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Longxu Dou",
        "Chao Du",
        "Zili Wang",
        "Hang Yan",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime."
        },
        {
            "title": "Start",
            "content": "Jinjie Ni1, Qian Liu, Longxu Dou2, Chao Du2, Zili Wang3, Hang Yan4, Tianyu Pang2, Michael Qizhe Shieh1 1National University of Singapore, 2Sea AI Lab, 3StepFun, 4Shanghai Qiji Zhifeng Co., Ltd. GitHub Training Backend Resources Blog More from MDGA: Quokka OpenMoE 5 2 0 2 5 ] . [ 1 6 7 2 3 0 . 1 1 5 2 : r Under strictly controlled pre-training settings, we observe Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input/parameter noise improves AR under data constraint but cannot close the gap. At scale, 1.7B DLM trained with 1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime. 1. Introduction Autoregressive (AR) language models have been the default recipe for modern LLMs: causal factorization, teacher forcing delivering high signal-to-FLOPs training and efficient model serving (Brown et al., 2020; Raffel et al., 2020). Yet the regime in which AR thrivedever-growing curated corpora paired with limited computeis shifting. High-quality data is becoming the primary bottleneck, while compute continues to scale (Common Crawl, 2025; Sevilla and Rold치n, 2024). This raises central question for the next phase of scaling: which modeling paradigm extracts more signal per unique token when data, not FLOPs, is the constraint? We revisit strong modeling paradigmdiffusion language models (DLMs) with masked/absorbing discrete diffusion (Lou et al., 2023; Nie et al., 2025; Ou et al., 2024; Shi et al., 2024). DLMs exhibit several modeling advantages over AR models. Such as any-order modeling, on-the-fly context modification, multi-token generation, high compute-to-parameter ratio, etc. In this work, we focus on their data potential, studying DLMs and AR models under common pre-training regime and ask how far each can go when unique data is scarce but repetitions are allowed. The main empirical finding is Crossover: when total training tokens are fixed but the number of unique tokens is limited, DLMs consistently surpass equally sized AR counterparts. This crossover is not an isolated artifactit systematically shifts with core factors. With more unique data, it shifts later; with higher data quality, it shifts later; with larger models, the crossover arrives earlier; and it persists across dense and sparse (MoE) architectures (Figures 2, 3, 4). Under compute-bound settings with abundant unique data, AR recovers its edge by fitting the data more rapidly; but in data-bound regimes, which is our focus and, increasingly, the practical reality, DLM is the final winner. Correspondence to: Jinjie Ni <jinjieni@nus.edu.sg> This is an initial draft that will be further improved. Diffusion Language Models are Super Data Learners Why does the crossover occur? Our analysis isolate three contributors. (i) Any-order modeling relaxes ARs causal inductive bias, increasing the degrees of freedom available to fit the same dataset. (ii) Super-density: DLMs use substantially more FLOPs at train and test time (temporal refinement with bidirectional attention), enabling them to mine limited data more thoroughly and scale the test-time compute. (iii) Richer Monte Carlo augmentation: the diffusion objective explicitly averages over corruption patterns, turning each sequence into many informative variants. Injecting noise into AR inputs or parameters (dropout) does help under data scarcity, but cannot close the gap (Figures 5 and 6). We also clarify common diagnostics pitfall: rising validation cross-entropy (overfitting) does not necessarily imply degraded downstream accuracy (Figures 8, 9). Note that DLMs do overfit with enough epochs and small enough unique data, but significantly later than AR models, and they extract markedly more value before saturation; e.g., 1B DLM trained for 480 epochs on 1B tokens reaches 56% HellaSwag and 33% MMLU without clear saturation (Figure 10). To assess the data-efficiency advantages of DLMs in realistic large-scale settings, we trained two 1.7B-parameter models with total budget of 1.5T tokens compute, with AR and diffusion objective. Models are trained on 10B unique Python tokens for 150 epochs, where 10B is an amount representative of practical high-quality data availability for certain programming languages. We observe clear performance crossovers on coding benchmarks, with the resulting diffusion coder achieving parity with state-of-the-art AR code models trained on trillions of unique tokens. Key takeaways. Intelligence Crossover. Under limited unique data, DLMs surpass AR models of the same size; the crossover is robust across data budgets, model scales, and sparsity choices. We also ran 2 scaled-up runs up to 1.5 trillion tokens to verify the universal appearance of the crossover phenomenon. Data potential. DLMs achieve roughly >3 data potential compared with AR models, making them strong candidates when data, not FLOPs, is scarce. Factors driving the gains. Any-order modeling + super-dense compute + built-in noisy augmentation are jointly responsible; AR with input/parameter noise helps but cannot match DLMs. Scaling trends. More unique data moves the crossover later; larger models move it earlier; higher data quality moves it later. Sparse AR models perform badly under data constraint, while DLMs benefit consistently from scale regardless of sparsity. Caveats. Rising validation loss need not imply worse downstream performance. Compute trade-off. DLMs require more training and (parallelizable) inference FLOPs to reach their potential; when unique data is abundant, compute-efficient AR can still win. Together, these results sharpen forecast for the data-bound era: if high-quality tokens remain the scarcest resource, DLMs are compelling modeling paradigm for pushing frontier capability per unit of unique data, even at the cost of greater compute. 2. Preliminaries 2.1. Autoregressive Language Models Autoregressive (AR) language modeling is the mainstream modeling scheme in state-of-the-art LLMs. It parameterizes the joint distribution over tokenized sequence 洧논1:洧녢 {0, . . . , 洧1}洧녢 via the chain 2 Diffusion Language Models are Super Data Learners rule, 洧녷洧랚(洧논1:洧녢 ) = 洧녢 (cid:214) 洧녰=1 洧녷洧랚(洧논洧녰 洧논<洧녰) . (1) Modern decoder-only LLMs realize this factorization with stacks of causal self-attention blocks (Radford et al., 2019): triangular (causal) mask limits each positions receptive field to the prefix 洧논<洧녰, allowing rich conditioning within the visible context while preserving left-to-right generation. Training uses teacher forcingshifting the sequence so that all next-token conditionals are learned in parallelwhereas inference proceeds sequentially with KV-caching for efficiency. This recipe yields exact, normalized likelihoods; supports streaming generation and controllable decoding; and under sufficient scale enables strong in-context learning behaviors (Brown et al., 2020). Learning objective AR training maximizes the log-likelihood (equivalently, minimizes cross-entropy) under the data distribution: (洧랚) = 洧댶洧논1:洧녢 洧녷data (cid:35) log 洧녷洧랚(洧논洧녰 洧논<洧녰) . (cid:34) 洧녢 洧녰= (2) This objective computes in single forward pass by predicting the next token at every position. While AR models are inherently one-directional and can suffer exposure bias from teacher forcing, their simplicity, exact normalization, and scalability make decoder-only Transformers the dominant foundation for high-quality text generation. 2.2. Masked Diffusion Language Models Why masked diffusion? DLMs adopt noisingdenoising framework over sequences. Among their variants, masked diffusionalso known as absorbing discrete diffusion, which relies on an absorbing transition kernelhas emerged as the most effective formulation (Amin et al., 2025). Their bidirectional attention and diffusion objective enable any-order modeling, allowing data to be modeled in arbitrary directions during both training and inference. This property is particularly beneficial for tasks requiring non-causal dependencies and back-and-forth reasoning, such as coding (Wu et al., 2025; Xie et al., 2025), mathematics (Google DeepMind, 2025), and report generation (Han et al., 2025). DLMs bidirectional attention natively support on-the-fly context modification as new content is generated, desirable feature in these tasks. Multi-token generation is also naively supported by DLMs, providing the foundation for their bleeding fast decoding. Moreover, DLMs spend more parallelable FLOPs at both the inference and training time, leading to its superior data learning capability and potentially stronger reasoning capabilities. Forward (corruption) process Let 洧 be the vocabulary size, 洧 the sequence length, and 洧녴 the mask token. Given clean sequence 洧논0 {0, . . . , 洧1}洧, define monotone diffusion schedule 洧띺洧노 [0, 1] with 洧띺0 = 1 and 洧띺1 = 0, where 洧띺洧노 is the probability that token is clean (unmasked) at noise level 洧노 [0, 1]. The forward process independently masks tokens: 洧륋롐 0(洧논洧노 洧논0) = 洧 (cid:214) 洧녰=1 (cid:16) 洧논 (洧녰) 洧노 洧륋롐 0 (cid:17) , 洧논 (洧녰) 0 (cid:16) 洧논 (洧녰) 洧노 洧륋롐 0 (cid:17) 洧논 (洧녰) 0 = (cid:40)洧띺洧노, 1 洧띺洧노, , 洧노 = 洧논 (洧녰) 洧논 (洧녰) 0 洧논 (洧녰) 洧노 = 洧녴 , so that the expected unmasked fraction at level 洧노 equals 洧띺洧노. 3 Diffusion Language Models are Super Data Learners Reverse (denoising) process Starting from the fully masked sequence 洧논1 and decreasing schedule 1 = 洧노0 > 洧노1 > > 洧노洧녜 = 0, the reverse dynamics from 洧노 to 洧 < 洧노 acts independently across positions: (cid:16) 洧논 (洧녰) 洧 (cid:17) 洧논洧노 = 洧륋롐멇롐 , 1, 1 洧띺洧 1 洧띺洧노 洧띺洧 洧띺洧노 1 洧띺洧노 0, 洧노 洧녴, 洧논 (洧녰) 洧논 (洧녰) 洧 = 洧논 (洧녰) 洧노 , 洧노 = 洧녴, 洧논 (洧녰) 洧논 (洧녰) 洧 = 洧녴, (cid:16) 洧논 (洧녰) 洧 (cid:17) , 洧논洧노 洧0洧노 洧노 = 洧녴, 洧논 (洧녰) 洧논 (洧녰) 洧 {洧녴}, otherwise. i.e., already-revealed tokens stay fixed; masked tokens either remain masked with probability 1洧띺洧 are revealed by sampling from data-prediction distribution 洧0洧노 ( 洧논洧노) with probability time-agnostic property (Ou et al., 2024) of masked diffusion is that 1洧띺洧노 or 洧띺洧 洧띺洧노 1洧띺洧노 . key 洧0洧노 (cid:16) 洧논 (洧녰) 0 (cid:17) 洧논洧노 = 洧녷data (cid:16) 洧논 (洧녰) 0 (cid:12) (cid:12) (cid:12) (cid:17) , 洧논UM 洧노 the conditional distribution of the clean token depends only on the unmasked context 洧논UM ; it does not depend on 洧노 beyond which tokens are visible. This allows the denoiser to be parameterized without an explicit time embedding. 洧노 洧논洧노 Learning objective Let 洧녷洧랚 variational bound on log 洧녷洧랚(洧논0), which can be written as minimizing approximate 洧녷data 洧논UM 洧노 (cid:16) 洧논 (洧녰) 0 (cid:17) (cid:16) 洧논 (洧녰) 0 (cid:17) . Masked diffusion maximizes 1 = where the importance weight 洧녻(洧노; 洧띺) depends only on the schedule and, up to constant factor, takes the natural form 洧녻(洧노; 洧띺) 洧댶洧륋롐0 (洧논洧노 洧논0 ) log 洧녷洧랚 洧논 (洧녰) 洧녰: 洧논 (洧녰) 洧노 =洧녴 洧논洧노 d洧노, (3) (cid:17) (cid:16) 0 洧녻(洧노; 洧띺) = 洧띺 洧노 洧띺洧노 . Intuitively, 洧녻(洧노; 洧띺) compensates for the varying expected number of masked positions across noise levels. For the widely used linear schedule 洧띺洧노 = 1 洧노, this reduces to the familiar integrand weight 洧녻(洧노) = 1/洧노. 3. The Intelligence Crossover We present extensive results showing that, when trained on standard web tokens, masked DLMs consistently outperform AR counterparts across model sizes in data-constrained settings, achieving higher potential without performance saturation. To analyze this pattern more rigorously, we decompose its key factors and conduct controlled group experiments. 3.1. Experimental Settings Unless specifically mentioned, most of the below sections use the same basic settings. It is worth noting that the hyperparameters adopted in our experiments are primarily optimized for AR models, reflecting extensive prior tuning efforts by the broader LLM research community. Although we aimed to maintain identical settings across AR and diffusion models, this is inherently unfair for 4 Diffusion Language Models are Super Data Learners diffusion models. Consequently, the observed performance advantages of diffusion models could be under-estimate. All training runs were conducted using significantly modified Megatron-LM codebase. Cross-over experiments were trained on subset of the Nemotron-CC corpus (Su et al., 2024); the runs in Figure 11 utilized subset of the c4-en corpus (Raffel et al., 2020); the coders are trained on subset of the RefinedCode (Huang et al., 2024). Note that all token budgets used are randomly sampled from these corpus, without any special process. We used the same masked diffusion objective as in Nie et al. (2025), detailed in Equation 3. Specifically, we employed batch size of 256, sequence length of 2048, and warmup-stable-decay (WSD) learning rate schedule peaking at 2e-4 with 1000 warmup steps, followed by 10% exponential decay to 2e-5. Model parameters were randomly initialized from normal distribution with s.t.d. 0.02. We adopted performant architectural configuration, incorporating the GPT-2 tokenizer, RoPE, SwiGLU, pre-layer RMSNorm, bias-free, and qk normalization. All mixture-of-expert models in this work use token-choice routing with 1e-2 auxiliary loss and 1e-3 loss (Zoph et al., 2022). Validation loss was evaluated on the c4-en validation set using distinct 100M-token subsets per evaluation. Benchmark evaluations strictly adhered to official protocols, detailed in Table 1. 3.2. Data Budget Decides the Crossover Timing The unique data budget is the first key factor we ablate under data-constrained training. Here, we vary the number of unique tokens from 0.5B to 96B while fixing the total training tokens at 96B, constrained by computational resources, and keep the model size fixed at 1B. Notably, 0.5B is realistic regime, as many domain-specific datasetssuch as robotics logs, clinical records, and low-resource language corporaoften fall within this scale. As shown in Figure 1, we consistently observe crossover behavior: diffusion language models surpass their autoregressive counterparts at low data budgets. Our results indicate that DLMs exhibit more than threefold higher effective data efficiency compared to AR models. Empirically, DLM trained on only 0.5B unique tokens (not fully converged) achieves comparable performance to an AR model trained on 1.5B unique tokens (converged). Under compute-bound settingswhere data is abundantAR models fit the training distribution more effectively and achieve stronger end-of-training performance. In contrast, under data-bound conditionsreflecting todays reality where compute growth outpaces high-quality data availabilitydiffusion models eventually outperform AR models. Interestingly, the crossover timing is similar across both evaluation benchmarks. As the number of unique tokens increases, the crossover shifts later (0.51.5B) or beyond our observable range (1.596B). This delay is more clearly highlighted in 6. We also aggregated all runs into single plot for global comparison. Strikingly, DLMs exhibit minimal degradation across both benchmarks and the validation set even when the unique data budget is reduced from 96B to 0.5B tokensdemonstrating substantially higher data efficiency than typically assumed. The narrow gap between the 10B and 96B AR runs reflects the fact that crossover points are pushed further out, meaning that what we observe within the 096B range captures only the early phase, where differences remain modest. This also suggests that, despite being more sensitive to data repetition, current AR training pipelines may underutilize available data: repeating the given 10B tokens for 10 epochs leads to only mild short-term degradation, more than the 4 epochs empirically observed in Muennighoff et al. (2023). 5 Diffusion Language Models are Super Data Learners Figure 1 Diffusion vs. AR with various unique data budgets. All models are trained on 96B total tokens (including repetition), varying the unique tokens from 0.5B to 96B. In the bottom \"Put Together\" row, darker color means smaller unique data size, i.e., trained for more epochs. 3.3. Varying Data Quality Unique data size is not the only factor influencing scaling behavior; data quality is equally critical. Prior studies show that quality can even outweigh quantity, i.e., \"less is more\" in LLM training (Magnusson et al., 2025; Zhou et al., 2023). To isolate this effect, we conduct controlled experiments on datasets of varying quality. Specifically, we train 1B-parameter AR and diffusion models on three quality tierslow, medium, and highsampled from the same distribution (Su et al., 2024), using 1B unique tokens over 96 epochs. One might expect the crossover to amplify with higher data quality, since diffusion models typically outperform AR models in data-limited regimes. Our ablation shows that both diffusion and AR models benefit substantially from improved data quality across benchmarks (Figure 2). However, as quality increases, the crossover shifts slightly later, suggesting that AR models are more sensitive to 6 Diffusion Language Models are Super Data Learners Figure 2 Diffusion vs. AR with various data qualities. All models are trained on 1B unique tokens for 96 epochs. In the bottom \"Put Together\" row, darker color means higher data quality. quality variation. Notably, validation loss trends diverge from benchmark results: the medium-quality run achieves the lowest validation loss. This highlights that validation loss is not always reliable comparator, as model overconfidence can heavily distort cross-entropy. We expand on this in 5. 3.4. How Much Does Model Size Impact the Data-Constrained Training? According to computeand data-constrained scaling laws (Hoffmann et al., 2022; Muennighoff et al., 2023), model size should grow with the data budget and training epochs. We investigate whether both model families benefit from increased scale and how the crossover evolves. To this end, we train diffusion and AR models from 1B to 8B parameters on 1B unique tokens for 96 epochs, keeping all other settings identical (3.1). As shown in Figure 3, the crossover shifts consistently earlier with larger models. This arises because, under data constraints, AR models quickly saturate the available data, and further scaling not only yields diminishing returns but also increases overfitting (see aggregated plots, bottom). In contrast, diffusion models do not fully exploit the 96B-token budget even at 1B parameters; scaling them accelerates learning and consistently improves performance. Remarkably, under these constraints, even the smallest diffusion model outperforms AR models at all sizes. Conceptually, diffusion models learn any-order mappings: an input sequence of length 洧 can be corrupted into 2洧 variations, whereas AR models only learn causal mappings with 洧 sequence prefixes. This combinatorial expansion makes the diffusion learning spaceand thus its upper performance Diffusion Language Models are Super Data Learners Figure 3 Diffusion vs. AR with various model sizes ranging from 1B to 8B. All models are trained on 1B unique tokens for 96 epochs. In the bottom \"Put Together\" row, darker color means larger model size. boundmuch larger than AR under limited data. While not every corruption yields distinct signal, the enlarged hypothesis space requires larger models to fully capture. We discuss this further in 7. 3.5. Sparse, Dense, Super-Density key distinction between AR and diffusion models lies in compute efficiency. Compared to dense AR models, diffusion models require substantially more FLOPs during training to realize the full data potential, and far higher parallel FLOPs during inference to scale along the time axis. We term these diffusion-based dense models \"super-dense\" architectures. Further analysis of trainingand test-time compute trade-offs is provided in 7. Building on this, natural question arises: how do dense and super-dense models compare against sparse architectures, and does sparsity alter performance under data-constrained regimes? To study the effect of sparsity in data-constrained learning, we train Mixture-of-Experts (MoEs) (Shazeer et al., 2017) for both AR and diffusion models, using 8B total parameters with 1B active per 8 Diffusion Language Models are Super Data Learners Figure 4 Diffusion vs. AR with various model sparsities. All models are trained on 1B unique tokens for 96 epochs. Both 8B-A1B and 8B1A denote 8B total parameters and 1B activated parameters, parameter-matching the 8B dense and FLOPs-matching the 1B dense. forward pass (8B1A). For direct comparison, we also train 1B dense models to match the activated parameters (FLOPs-matching) and 8B dense models to match the total parameters (parametermatching). The density ordering is: AR MoE < AR dense < DLM MoE < DLM dense. All models are trained on 1B unique tokens for 96 epochs. Figure 4 reveals several key observations. First, across all sparsity levels, DLMs consistently surpass AR models, with crossover timing ordered as 8B dense < 8B1A MoE < 1B dense. Second, when comparing dense and sparse variants, the evaluation can be framed in two complementary ways: (1) under FLOPs-matching, how much benefit arises solely from increasing total parameters? (2) under parameter-matching, what is the performance cost of reducing per-task FLOPs? Given the distinct behaviors of AR and diffusion models under data constraints, we analyze them separately. For AR models, which fit the data within few epochs and then saturate (or overfit on the validation set), scaling from 1B to 8B parameters consistently degrades performance across evaluations, regardless of dense or sparse expansion. Sparse expansion (1B dense 8B1A MoE) performs worst. Alternatively, framing the comparison as FLOP reductions from the 8B dense baseline reveals that reducing only FLOPs (8B dense 8B1A MoE) hurts performance, while reducing both FLOPs and parameters (8B dense 1B dense) improves it. Together, these perspectives lead to clear conclusion: in data-constrained settings, given fixed parameter counts, higher FLOPs consistently improve performance, as evidenced by 8B dense consistently outperforming 8B1A 9 Diffusion Language Models are Super Data Learners MoE. Diffusion models behave differently: within the 96B-token window, they have not yet reached diminishing returns and remain far from overfitting (though 6 shows eventual overfitting). In this regime, the 8B1A diffusion MoE performs as expected, generally between the 8B dense and 1B dense counterparts, with its advantage most pronounced on knowledge-heavy benchmarks (e.g., MMLU). We believe the interplay of compute and parameterization under data constraints deserves dedicated study. 3.6. Is Noise Deciding the Game? In 7, we further analyze the factors underlying the advantage of DLMs over AR models. In summary, DLMs differ from AR models in three ways: (1) any-order modeling, (2) higher trainingand inferencetime FLOPs (super-density), and (3) richer Monte Carlo sampling via noisy data augmentation. We argue that noisy augmentation primarily benefits models in data-constrained regimes, whereas the first two constitute fundamental modeling advantages with universal impact. Ablating these factors is non-trivial: introducing any-order modeling into AR while holding other variables fixed is hard, and similarly, scaling AR FLOPs without altering other dynamics is difficult. Moreover, note that masked DLMs can be viewed as any-order models with more practical tractability. Figure 5 Injecting noise (random masking) to the AR models inputs helps in data-constrained settings but does not beat diffusion models. All models shown in this figure are 1B parameters trained on 1B unique tokens for 96 epochs. We therefore focus on ablating the data augmentation feature to isolate the contributions of the other two factors. direct approach is to mimic masked DLMs by injecting noise into AR inputsmasking fraction of tokensto evaluate the potential gains. While uncommon in large-scale LLM training, noisy data augmentation has precedent in computer vision. As shown in Figure 5, we apply input masking at ratios from 10% to 90%. Results show that AR performance improves with modest noise but collapses once inputs become overly corrupted. The substantial gains under low-noise regimes confirm that noisy data augmentation is indeed key contributor to diffusions advantage in data-constrained settings. However, even the best configuration (10% masking) falls well short of diffusion, and saturates by the end of the 96B-token training window, whereas diffusion remains far from saturation and would likely widen the gap with longer training. Alternative to injecting noise to the input for noisy data augmentation, we can also inject noise to the parameters by zeroing out random set of neuron outputs, a.k.a., dropout, to achieve similar effects. Similarly, we use dropout ratio from 10% to 90%. As shown in Figure 6, adding noise to the parameter also raises the performance of AR models in data-constrained settings while not fully eliminating the advantage of DLMs. The AR diminishes likewise but diffusion doesnt. 10 Diffusion Language Models are Super Data Learners Figure 6 Injecting noise to AR models parameters (dropout) helps in data-constrained settings but does not beat diffusion models. All models shown in this figure are 1B parameters trained on 1B unique tokens for 96 epochs. 4. Scaling Crossovers to Trillion-Level Total Tokens Figure 7 1.7B AR vs. DLM trained on 10B unique code tokens for 150 epochs. On downstream evaluations, we observe early crossovers where DLMs surpass AR models. This provides another evidence that crossovers emerge at larger scales under limited unique data. Different crossover timings are observed on two additional coding benchmarks (Figure 13). We further validate the crossover phenomenon under larger unique token budgets and on generative tasks. Among such tasks, code generation provides particularly clean and popular benchmark, while also being heavily constrained by available training tokens. Prior work suggests that 10B-token budget is already practical scale for certain programming languages (Huang et al., 2024). To this end, we construct 10B-token dataset by randomly sampling from RefineCode (Huang et al., 2024), consisting of 9B unique Python tokens and 1B annealing tokens, ensuring strictly non-repetitive training corpus. On this dataset, we train pair of 1.7B-parameter AR and DLM models under identical settings for 150 epochs. The training setup employs warmupstabledecay learning rate schedule (2000 warmup steps, followed by decay over 100B tokens), resulting in total compute budget of 1.5T tokens. Additional hyperparameters include sequence length of 4096, global batch size of 1024, weight decay of 0.1, and performant architecture: GPT-NeoX (Black et al., 2022) tokenizer, rotary position embeddings (RoPE), SwiGLU activations, pre-layer RMSNorm, bias-free layers, and 洧륋롐 normalization. Interestingly, we observe very clear crossover in the early stages of training across downstream benchmarks, further reinforcing the apparent universality of the diffusionAR crossover phenomenon. At the same time, we note that the DLM had not yet converged by the end of the 1.5T-token training 11 Diffusion Language Models are Super Data Learners cycle, suggesting substantial untapped potential if training were to continue. Beyond standard benchmarks, we evaluate both models on HumanEval and HumanEval+, and find that the crossover occurs at different point compared to MBPP and MBPP+. Specifically, in HumanEval(-plus), the performance curves intersect only near the end of the annealing stage (see Figure 13), without seeing diminish returns. We hypothesize that this timing discrepancy is rooted in the evaluation protocols: MBPP and MBPP+ adopt 3-shot setting, while HumanEval and HumanEval+ use 0-shot setting. Such differences in evaluation configuration likely amplify or suppress model capabilities at different stages of training, thereby shifting crossover points. These results highlight two key insights. First, the crossover phenomenon extends robustly to generative tasks, under more unique token budgets. Second, the precise timing of crossovers in generative tasks may be sensitive to evaluation protocols, which raises an important methodological consideration: the need for systematic studies that disentangle training dynamics from evaluation artifacts. We believe more rigorous examination of evaluation settings will be critical for fully characterizing crossover behavior in future work. 5. High Validation Loss Degraded Intelligence Figure 8 When models get overfit on pre-training validation sets, their performance on down-stream evaluations doesnt necessarily drop, and may keep improving till the end of training. We observe that the AR models exhibiting signs of \"overfitting\"indicated by an increase in validation losscontinue to improve on downstream tasks, as illustrated in Figures 8 and the previous ones. This phenomenon arises because validation loss is measured as an absolute Cross-Entropy loss (Negative Log-Likelihood, NLL), whereas accuracy on multiple-choice benchmarks depends on comparing the relative Cross-Entropy losses across options. Consequently, changes in absolute NLL values do not necessarily translate into changes in their relative ordering. In Figure 9, we visualize the average negative log-likelihood (NLL) for the ground-truth and alternative options across multiple-choice evaluations, along with their respective differences (풊NLL), during the pre-training of 1B-parameter autoregressive model over 1.5B unique tokens for 64 epochs. Notably, even at the first validation checkpoint (after 3,600 training steps), the model already exhibits substantially lower NLL (higher likelihood) on the ground-truth options, indicating an early capacity to preferentially assign higher logits to correct choices. As training continues, the model begins to overfit, causing an increase in NLL values for both ground-truth and incorrect options. Interestingly, even after this \"overfitting,\" the gap between ground-truth and alternative NLLs continues to widen consistently, indicating that the models underlying discriminative ability continues to improve despite the rise in validation loss. This phenomenon persists for both in-domain and out-of-domain training data. One plausible explanation is that repeated exposure to limited set of training data causes the model to become excessively confident on certain text segments, amplifying NLL values for incorrect 12 Diffusion Language Models are Super Data Learners Figure 9 An illustration of why the models performance keeps growing after they overfit on pre-training validation sets (indicated with dashed line). NLL: Negative log-likelihood on the ground-truth and other options of multiple-choice evals (NLLs on other options are averaged). 풊NLL: The differences between the NLLs on ground-truth and other options, which keeps growing. This is 1B autoregressive model trained on 1.5B unique tokens, 64 epochs, on both out-of-domain and in-domain pre-training data. predictions. Nevertheless, the persistent growth in relative NLL differences between ground-truth and other options reflects continuous improvement in the models discriminative power. similar rationale applies to generative evaluations, where choices are made at the token rather than sentence level, and we hypothesize that being mistakenly overconfident on non-essential tokens has limited impact on the overall task. The same overfitting patterns on generative benchmarks is revealed in 4 on code generation benchmarks, where the AR models overfit early on validation loss while the benchmark degradation is typically delayed. 6. Diffusion Language Models also Overfit the Data To study the full potential of tokens in DLM training, we launched an additional run in which the same 1B-token dataset was repeated for 480 epochs, yielding total of 480B training tokens. Notably, it achieves 56% accuracy on HellaSwag and 33% on MMLU, significantly outperforming ARs 41% and 29%, respectively. Surprisingly, even under such extreme repetition, performance did not saturate, suggesting that DLMs can extract substantially more signal from fixed 1B-token corpus. This leads us to investigate: Do DLMs eventually overfit given sufficient training? 13 Diffusion Language Models are Super Data Learners Figure 10 The 1B-parameter DLMtrained solely on the original 1B pre-training tokens for 480 epochsachieves 56% accuracy on HellaSwag and 33% on MMLU. Figure 11 Validation loss curves for models trained with various sizes and unique data budgets, repeating up to 1000 epochs. Diffusion language models will also overfit. The more unique data we train on, the later it overfits; the larger model we train, the earlier it overfits. We trained models across various sizes and unique data budgets, extending training up to 1000 epochs. As illustrated in Figure 11, when the unique data is sufficiently small and the model size sufficiently large, overfitting eventually emerges after prolonged training. Specifically, we observe that the epoch at which model begins to overfit positively correlates with the unique data size and negatively correlates with model size. In other words, larger unique data size delay overfitting, while larger models accelerate its onset. It is important to note that validation loss overfitting does not immediately imply decline in model capabilityactual performance degradation typically occurs much later (e.g., as seen in Figure 1 at 0.5B tokens and 192 epochs). 7. Discussions 7.1. What is the Real Advantage of Diffusion Language Models? Reduced inductive bias via any-order modeling Autoregressive language models impose strict causal inductive bias on textual data modeling, where each token prediction is conditioned solely 14 Diffusion Language Models are Super Data Learners on preceding tokens. While natural language exhibits inherent left-to-right causality from human perspective, evidence indicates that modeling language in reverse or arbitrary order remains feasible (Xue et al., 2025). Moreover, numerous non-causal data types, such as source code, database entries, symbolic notations, and biological sequences, etc., frequently appear online. Thus, enforcing purely causal inductive bias significantly restricts capturing the rich patterns embedded in diverse textual distributions. DLMs remove this inductive bias with the diffusion objective and the bi-directional attention, enabling such any-order modeling, fully squeezing the value of every single data point. Super-Density: more training and test time FLOPs per task As illustrated above, diffusion models outperform AR counterparts by repeatedly processing some portion of unique data during training, effectively scaling FLOPs along the temporal dimension. The continuous-time objective utilized by masked language models is particularly advantageous, enabling high granularity in temporal FLOPs scaling. Similarly, at inference, diffusion models iteratively refine predictions, further amplifying computational density per task. Notably, bidirectional attention implies each token is computed up to times to generate sequence of length N, contrasting with AR models using KV cache, which compute each token only once. The effectiveness of super-density in data-constrained settings has been validated in the strictly controlled experiments in 3.5. Figure 12 (left) The diffusion language models are approximated to consume >100 time more FLOPs than AR counterparts to achieve their full potential in training (where the peak performance is usually much greater than AR). (middle) The theoretical inference FLOPs controlling the sampling steps to be equal to the sequence length. The total inference FLOPs have power-law relationship with the generation sequence length for both. (right) The theoretical inference FLOPs controlling the generation sequence length, where sampling 512 steps from an AR model with KV cache sampling 1 step from the masked diffusion model. Figure 12 compares the FLOPs of autoregressive (AR) and masked diffusion models during training and inference. At training time, our preliminary experiments indicate diffusion models require at least about two orders of magnitude (>100) more FLOPs than AR models to reach optimal performance, with the exact figure varying depending on model size, data budget, etc. During inference, given fixed number of sampling steps, masked diffusion models consume between 16 and 4700 more FLOPs per task, with this gap widening as the target sequence length increases from 16 to 4096 (Figure 12 middle). Moreover, for constant sequence length, FLOPs consumed by diffusion models scale linearly with the sampling steps. In theory, diffusion models can generate an N-token sequence within single step, whereas AR models inherently require sequential steps. However, due to the KV-cache mechanism, the computational cost for AR models generating tokens is roughly 15 Diffusion Language Models are Super Data Learners equivalent to that of diffusion models performing single sampling step. Its worth noting that significant portion of diffusion model computations can be parallelized. Thus, in practice, before GPU compute bound is reached, the inference speed gap between diffusion and AR models at the same number of sampling steps remains acceptable. Additionally, advances in GPU architectures specifically optimized for compute-intensive workloads may further mitigate this performance gap in the near future. Reflecting on the LLM history, many recent intelligence leaps, such as T5 (Raffel et al., 2020), GPT-3 (Brown et al., 2020), and o1 (OpenAI, 2024), are the direct results of FLOPs scaling. 洧댶洧노,洧(洧눛洧노 洧눛0 ) 洧띺 洧노 1 洧띺洧노 {洧녰 洧눛洧녰 洧노=洧녴} log 洧녷洧랚(洧눛 洧녰 0 洧눛洧노) (4) Learning From Richer Monte Carlo Sampling When conducting multi-epoch training for masked DLMs, we effectively transform each unique data point into multiple noisy variants. Specifically, the loss function for masked diffusion models includes an expectation term 洧댶洧노,洧(洧눛洧노 洧눛0 ) , placed outside the negative log-likelihood component. Here, 洧(洧눛洧노 洧눛0) represents the distribution of masked sequences 洧눛洧노 conditioned on the clean input 洧눛0 at diffusion timestep 洧노, as determined by the forward corruption process. Intuitively, this means averaging the loss across all possible masking configurations 洧눛洧노 at each time step 洧노 [0, 1]. In other words, the objective function explicitly requires each data point in the pre-training dataset to be corrupted at multiple masking ratios and combinations for more effective training, by estimating more precise expectation. Thus, data repetition emerges inherently from the diffusion models objective rather than from an arbitrary source. Open-source models, such as LLaDA, typically corrupt each data point only once, likely due to computational limitations, approximating the expectation term using single-sample Monte Carlo estimator. In general, masked DLMs corrupt an input data with length into 2洧 different sequences, while AR learns causal mapping with 洧 different sequences from short to long for given input of the same length. Such drastic expansion of the learning space makes DLMs learning upper limit much higher than AR given limited portion of data (though not every corruption produces significant difference), requiring larger model size to fully fit it. There have been lot of frontier works trying to augmenting the AR models data with model rewritingrewriting an input sequence into multiple variants, which also demonstrated fruitful gains for AR models (Team et al., 2025). However, we note that such data rewriting requires expert design of the whole complicated pipeline and even though its improving the models performance, theres not guarantee that the improvement is well-rounded. In contrast, for diffusion language models, injecting noise to the input is all you need to do the data augmentation. 7.2. Autoregressive Models are Trading Data Potential for Compute Efficiency The autoregressive (AR) modeling methodology (decoder-only transformer architecture with teacherforcing and causal masking) is legendary local optimum in the AI history. Its success can be broken down into two factors: Optimal utilization of modern GPU architectures AR achieves an exceptionally high signal-toFLOPs ratio during training and high Model FLOPs Utilization (MFU) during batched inference. During training, each token in batch consistently receives gradient signals, approximately twice the expected signals compared to masked diffusion models with linear schedules. Indeed, it is 16 Diffusion Language Models are Super Data Learners challenging to identify alternative methodologies surpassing AR in terms of signal-to-FLOPs efficiency. At inference, token-by-token generation naturally facilitates throughput optimization techniques such as continuous batching, maximizing MFU and KV cache, minimizing computation. Thus, AR stands as an exceedingly robust and efficient baseline method. Natural language can be causally modeled with low loss Empirically, pre-training on web-scale corpora demonstrates that left-to-right modeling consistently attains lower loss compared to alternative sequence orders (see Figure 2 of Xue et al. (2025)). If one must select single sequence order for language modeling, the left-to-right order is empirically optimal (Eq. 1), as it effectively captures natural language patterns. Its also easy to interpret this: most text data are generated by humans, and humans are RNNs. However, as previously discussed, purely left-to-right modeling inherently misses certain contextual dependencies, indicating room for improvement in data potential. Currently, there is an emerging trend in which computational resources are becoming increasingly affordable, shifting the primary constraint for scaling intelligence towards data availability. Consequently, for researchers targeting advanced intelligence, the previous emphasis on maximizing GPU utilization has diminished. The inherent modeling limitations imposed by causal masking are now becoming unacceptable. This motivates our exploration of DLMs, which intentionally sacrifice computational efficiency to achieve higher data efficiencyrepresenting an approach diametrically opposed to autoregressive methods. To strike favorable balance between these two extremes, natural strategy is interpolation, as exemplified by block diffusion methods (Arriola et al., 2025). However, achieving comparable training efficiency remains challenging: block diffusion inherently conditions each generated block on clean context, significantly constraining training efficiency compared to the highly efficient teacher-forcing paradigm employed in autoregressive training. 7.3. Limitations DLMs trade compute for data potential. The crossovers rely on higher trainand test-time FLOPs with bidirectional attention and iterative refinement, which raises energy cost, processing time, and memory pressure compared to AR with KV cache. Whereas under the multi-token prediction scheme it can achieve much lower inference latency than AR models. Second, practical inference performance is sensitive to choices that are still under-explored at scalemasking schedules, denoising weights, step counts, and decoding policieswhich complicates fair, apples-to-apples comparisons with heavily optimized AR pipelines. Third, evaluation confounds remain: diffusion objectives optimize variational bound rather than normalized left-to-right likelihood, so perplexity is not directly comparable, and benchmark gains may not uniformly translate to streaming, tool-use, or long-horizon generation. Fourth, heavy data reuse heightens contamination and memorization risk if deduplication and auditing are imperfect; safety and privacy audits should therefore be stricter under super-dense training. Finally, the systems stack for practical deployment is less mature for DLMs than for AR, and our experiments focus on English-centric pretraining; broader multilingual, multimodal, and long-context regimes merit dedicated study. 8. Related Work Diffusion language models Building on the theoretical foundations of DLMs (Lou et al., 2023; Ou et al., 2024; Sahoo et al., 2024; Shi et al., 2024), Nie et al. (2025) trained the first large-scale DLM from scratch, achieving performance competitive with leading open-source AR models (Dubey 17 Diffusion Language Models are Super Data Learners et al., 2024). In parallel, several commercial DLMs have emerged, demonstrating strong coding and math capabilities while offering significantly lower generation latency (Google DeepMind, 2025; Khanna et al., 2025; Song et al., 2025). Efforts have also explored hybrid approaches bridging AR and diffusion. Block diffusion (Arriola et al., 2025) performs block-wise diffusion, with block size 1 reducing to AR modeling without shift. Dream (Ye et al., 2025) initialized DLMs with AR priors and employed \"shift-by-one\" diffusion strategy to better retain AR knowledge, offering another effective training paradigm. Recent work has also advanced DLM coders (Gong et al., 2025; Xie et al., 2025), DLM RL scaling (Zhu et al., 2025), accelerated inference techniques (Wu et al., 2025), pushing DLMs toward greater practicality and competitiveness. Mitigating data constraints and the token crisis. complementary line of work asks how to keep scaling when unique tokens are scarce. Muennighoff et al. (2025) formalize data-constrained scaling laws, showing that repeating data for up to 4 epochs yields little loss penalty at fixed compute but exhibits sharply diminishing returns thereafter; they also find that mixing in code or relaxing aggressive filters can substitute for some unique text. Xue et al. (2023) analyze multi-epoch degradation, identifying dropout as robust regularizer; this echoes earlier evidence that small repeated fractions can disproportionately harm generalization (Hern치ndez et al., 2022). In parallel, large open corpora aim to expand high-quality supply through better curation and deduplication: RefinedWeb (5T tokens) (Penedo et al., 2023), FineWeb (15T; with FineWeb-Edu) (Penedo et al., 2024), and Dolma (3T) (Soldaini et al., 2024). second strategy manufactures or selects higher-utility tokens. Rephrasing/rewriting pipelines improve pretraining efficiency by paraphrasing web documents (WRAP) (Maini et al., 2024), extending to multilingual rephrasing (Pieler et al., 2024) and targeted math/code rewriting (SwallowCode/SwallowMath) (Fujii et al., 2025). Large applied systems report related practices at scale; for example, Kimi K2 describes large agentic data synthesis, and independent commentary attributes part of its gains to rephrasing public corpora (Breunig, 2025; Kimi Team et al., 2025). Orthogonal levers include retrieval-time scaling with trillion-token datastores (Shao et al., 2024), perplexity-based data pruning (Ankner et al., 2024), and end-of-training domain upsampling (Blakeney et al., 2024). Our results are complementary: diffusion objectives further amplify value per unique token when repetition is unavoidable. 9. Acknowledgment We thank Shen Nie, Jiacheng Ye, and Cunxiao Du for their fruitful discussions and pointers."
        },
        {
            "title": "References",
            "content": "A. N. Amin, N. Gruver, and A. G. Wilson. Why masking diffusion works: Condition on the jump schedule for improved discrete diffusion. arXiv preprint arXiv:2506.08316, 2025. Z. Ankner, C. Blakeney, K. Sreenivasan, M. Marion, M. L. Leavitt, and M. Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. URL https://arxiv.org/abs/2405.20541. M. Arriola, A. Gokaslan, J. T. Chiu, Z. Yang, Z. Qi, J. Han, S. S. Sahoo, and V. Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. 18 Diffusion Language Models are Super Data Learners S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. C. Blakeney, M. Paul, B. W. Larsen, S. Owen, and J. Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training. arXiv preprint arXiv:2406.03476, 2024. URL https://arxiv.org/abs/2406.03476. D. Breunig. How rewriting training data improved kimi k2s performance. https://www.dbreunig. 2025. com/2025/07/27/kimi-applies-rephrasing-to-pre-training-data.html, Blog post. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Common Crawl. Statistics of common crawl monthly archives: Crawl size. 2025. URL https: //commoncrawl.github.io/cc-crawl-statistics/plots/crawlsize. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. K. Fujii, Y. Tajima, S. Mizuki, H. Shimada, T. Shiotani, K. Saito, M. Ohi, M. Kawamura, T. Nakamura, T. Okamoto, S. Ishida, K. Hattori, Y. Ma, H. Takamura, R. Yokota, and N. Okazaki. Rewriting pre-training data boosts llm performance in math and code. arXiv preprint arXiv:2505.02881, 2025. URL https://arxiv.org/abs/2505.02881. S. Gong, R. Zhang, H. Zheng, J. Gu, N. Jaitly, L. Kong, and Y. Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Google DeepMind. Gemini diffusion: Our state-of-the-art, experimental text diffusion model. https: //deepmind.google/models/gemini-diffusion/, 2025. Accessed: 2025-09-23. R. Han, Y. Chen, Z. CuiZhu, L. Miculicich, G. Sun, Y. Bi, W. Wen, H. Wan, C. Wen, S. Ma칥tre, et al. Deep researcher with test-time diffusion. arXiv preprint arXiv:2507.16075, 2025. D. Hern치ndez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume, S. Johnston, B. Mann, C. Olah, C. Olsson, D. Amodei, N. Joseph, J. Kaplan, and S. McCandlish. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022. URL https://arxiv.org/abs/2205.10487. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. S. Huang, T. Cheng, J. K. Liu, J. Hao, L. Song, Y. Xu, J. Yang, J. Liu, C. Zhang, L. Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. S. Khanna, S. Kharbanda, S. Li, H. Varma, E. Wang, S. Birnbaum, Z. Luo, Y. Miraoui, A. Palrecha, S. Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Kimi Team, Y. Bai, Y. Bao, G. Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. URL https://arxiv.org/abs/2507.20534. 19 Diffusion Language Models are Super Data Learners A. Lou, C. Meng, and S. Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. I. Magnusson, N. Tai, B. Bogin, D. Heineman, J. D. Hwang, L. Soldaini, A. Bhagia, J. Liu, D. Groeneveld, O. Tafjord, et al. Datadecide: How to predict best pretraining data with small experiments. arXiv preprint arXiv:2504.11393, 2025. P. Maini, S. Seto, R. Bai, D. Grangier, Y. Zhang, and N. Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1404414072, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.757. URL https://aclanthology.org/2024.acl-long.757/. N. Muennighoff, A. Rush, B. Barak, T. Le Scao, N. Tazi, A. Piktus, S. Pyysalo, T. Wolf, and C. A. Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36: 5035850376, 2023. N. Muennighoff, L. Soldaini, D. Groeneveld, K. Lo, J. Morrison, S. Min, W. Shi, P. Walsh, O. Tafjord, N. Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. N. Muennighoff, A. M. Rush, B. Barak, T. Le Scao, A. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and C. Raffel. Scaling data-constrained language models. Journal of Machine Learning Research, 26(53):166, 2025. URL https://jmlr.org/papers/v26/24-1000.html. S. Nie, F. Zhu, Z. You, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and C. Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. OpenAI. reason with learning-to-reason-with-llms/, 2024. Learning to llms. https://openai.com/index/ J. Ou, S. Nie, K. Xue, F. Zhu, J. Sun, Z. Li, and C. Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/ abs/2306.01116. G. Penedo, H. Kydl칤캜ek, L. Ben Allal, A. Lozhkov, M. Mitchell, C. Raffel, L. Von Werra, and In AdT. Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. vances in Neural Information Processing Systems 37 (NeurIPS 2024), Datasets and Benchmarks Track, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/hash/ 370df50ccfdf8bde18f8f9c2d9151bda-Abstract-Datasets_and_Benchmarks_Track. html. M. Pieler, M. Bellagente, H. Teufel, D. Phung, N. Cooper, J. Tow, P. Rocha, R. Adithyan, Z. Alyafeai, N. Pinnaparaju, M. Zhuravinskyi, and C. Riquelme. Rephrasing natural text data with different languages and quality levels for large language model pre-training. arXiv preprint arXiv:2410.20796, 2024. URL https://arxiv.org/abs/2410.20796. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 20 Diffusion Language Models are Super Data Learners C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. Chiu, A. Rush, and V. Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. J. and"
        },
        {
            "title": "Sevilla",
            "content": "Training by 28 training-compute-of-frontier-ai-models-grows-by-4-5x-per-year. grows ai models https://epoch.ai/blog/ E. Rold치n. 45 per of URL compute frontier 2024. year."
        },
        {
            "title": "May",
            "content": "R. Shao, J. He, A. Asai, W. Shi, T. Dettmers, S. Min, L. Zettlemoyer, and P. W. Koh. Scaling retrievalbased language models with trillion-token datastore. arXiv preprint arXiv:2407.12854, 2024. URL https://arxiv.org/abs/2407.12854. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. J. Shi, K. Han, Z. Wang, A. Doucet, and M. Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. URL https://arxiv.org/abs/2402.00159. Y. Song, Z. Zhang, C. Luo, P. Gao, F. Xia, H. Luo, Z. Li, Y. Yang, H. Yu, X. Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. D. Su, K. Kong, Y. Lin, J. Jennings, B. Norick, M. Kliegl, M. Patwary, M. Shoeybi, and B. Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. C. Wu, H. Zhang, S. Xue, Z. Liu, S. Diao, L. Zhu, P. Luo, S. Han, and E. Xie. Fast-dllm: Trainingfree acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Z. Xie, J. Ye, L. Zheng, J. Gao, J. Dong, Z. Wu, X. Zhao, S. Gong, X. Jiang, Z. Li, et al. Dream-coder 7b: An open diffusion language model for code. arXiv preprint arXiv:2509.01142, 2025. F. Xue, Y. Fu, W. Zhou, Z. Zheng, and Y. You. To repeat or not to repeat: Insights from scalIn Advances in Neural Information Processing Systems 36 (NeurIPS ing llm under token-crisis. 2023), 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ b9e472cd579c83e2f6aa3459f46aac28-Paper-Conference.pdf. 21 Diffusion Language Models are Super Data Learners S. Xue, T. Xie, T. Hu, Z. Feng, J. Sun, K. Kawaguchi, Z. Li, and Z.-M. Ma. Any-order gpt as masked diffusion model: Decoupling formulation and architecture. arXiv preprint arXiv:2506.19935, 2025. J. Ye, Z. Xie, L. Zheng, J. Gao, Z. Wu, X. Jiang, Z. Li, and L. Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. F. Zhu, R. Wang, S. Nie, X. Zhang, C. Wu, J. Hu, J. Zhou, J. Chen, Y. Lin, J.-R. Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. Table 1 Summary of downstream evaluation used in this work. CF stands for Completion/Cloze formulation, Gen stands for generative, Char stands for per-character normalization. MMLU combines 0-5 shot results for stability (Muennighoff et al., 2024). Dataset Format Shot Norm Split Codebase CF HellaSwag CF MMLU Gen HumanEval MBPP Gen HumanEval+ Gen Gen MBPP+ 0 0-5 0 3 0 3 Char Char - - - - Val Val Test Test Test Test Megatron LM Megatron LM lm-evaluation-harness lm-evaluation-harness lm-evaluation-harness lm-evaluation-harness Figure 13 1.7B AR vs. DLM trained 10B unique code tokens for 150 epochs. On HumanEval and HumanEval +, the crossover time is delayed and the two curves meet in the end, where the DLM didnt see diminish return. We suspect this is direct result of the zero-shot setting."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab",
        "Shanghai Qiji Zhifeng Co., Ltd.",
        "StepFun"
    ]
}