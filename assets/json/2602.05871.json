{
    "paper_title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
    "authors": [
        "Xunzhi Xiang",
        "Zixuan Duan",
        "Guiyu Zhang",
        "Haiyu Zhang",
        "Zhe Gao",
        "Junta Wu",
        "Shaofeng Zhang",
        "Tengfei Wang",
        "Qi Fan",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks."
        },
        {
            "title": "Start",
            "content": "Pathwise Test-Time Correction for Autoregressive Long Video Generation Xunzhi Xiang * 1 2 Zixuan Duan * 1 Guiyu Zhang 3 Haiyu Zhang 2 Zhe Gao 1 Junta Wu 2 Shaofeng Zhang 4 Tengfei Wang 2 Qi Fan 1 Chunchao Guo 2 6 2 0 2 5 ] . [ 1 1 7 8 5 0 . 2 0 6 2 : r Figure 1. 30-second video generation examples. Our method reduces error accumulation in CausVid and Self-Forcing, enabling longer and more stable videos with improved visual consistency. All samples are generated with the same random seed for fair comparison."
        },
        {
            "title": "Abstract",
            "content": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward *Equal contribution 1Nanjing University 2Tencent Hunyuan 3Chinese University of Hong Kong, Shenzhen 4University of Science and Technology of China. Correspondence to: Tengfei Wang <tengfeiwang12@gmail.com>, Qi Fan <fanqi@nju.edu.cn>. Preprint. February 6, 2026. 1 landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), training-free alternative. Specifically, TTC utilizes the initial frame as stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks. Pathwise Test-Time Correction for Autoregressive Long Video Generation 1. Introduction Video generation (Lu et al., 2025; Yesiltepe et al., 2025; Hong et al., 2023; Jia et al., 2025b) has advanced rapidly with the development of diffusion-based generative models (Kong et al., 2024; Wan et al., 2025; Hong et al., 2023; Ma et al., 2025a; Peebles & Xie, 2023; Rombach et al., 2022), which now enable the high-quality synthesis of complex motion (Zhu et al., 2024; Hu, 2024) and visual appearance (Guo et al., 2024; Zhang et al., 2025a). However, scaling these diffusion priors to extended video sequences remains formidable challenge. Beyond the escalating computational costs associated with longer contexts, maintaining temporal coherence over extended horizons is difficult without incurring excessive latency, thereby limiting their deployment in real-time applications. To overcome these limitations, recent studies (Yin et al., 2025b; Huang et al., 2025d) have shifted from bidirectional modeling to step-distilled autoregressive generation, enabling true real-time video synthesis. However, these methods remain constrained by cascading error accumulation: since each frame is conditioned on prior outputs, initial inaccuracies compound over time, resulting in temporal drift and long-horizon degradation. While recent extensions (Yi et al., 2025; Cui et al., 2026) like Rolling Forcing (Liu et al., 2025b), LongLive (Yang et al., 2025), Self-Forcing++ (Cui et al., 2025) , and WorldPlay (Sun et al., 2025) have achieved minute-level consistency through sink mechanisms and windowed DMD retraining, they necessitate substantial computational overhead for model fine-tuning. Consequently, pivotal question arises: Can we improve the stability of autoregressive video generation purely at inference time, bypassing the need for retraining the base model? Test-Time Optimization (TTO) (Wang et al., 2025; Sun et al., 2020) has emerged as compelling alternative for enhancing video quality without the need for retraining. However, while effective for short-video synthesis (Yu et al., 2025b; Eyring et al., 2025), our toy experiments reveal that scaling TTO to long-horizon autoregressive generation faces dual bottleneck consisting of the inherent difficulty in defining reward functions for long-range consistency and the extreme optimization sensitivity of distilled models. We observe that in these distilled models, even infinitesimal test-time gradients often trigger reward collapse and fail to mitigate cumulative error. Therefore, we propose Test-Time Correction (TTC), which is training-free framework that shifts the paradigm from parameter-space optimization to sampling-space stochastic intervention. TTC is grounded in the insight that few-step distilled samplers are inherently stochastic as they perturb intermediate states with injected noise. This property implies that intermediate predictions are not fixed outcomes but rather malleable latent states that can be rectified by subsequent diffusion steps to align with Figure 2. Comparison of sampling strategies. The Original Path suffers from error accumulation, while the Sink-based Path collapses into Sink Point (dynamic collapse). In contrast, our TTC strategy avoids these failures by employing reference-conditioned denoising and explicit Re-noising, effectively steering the trajectory away from the sink to preserve target distribution. the global initial context while preserving the underlying sampling distribution. Specifically, as shown in Figure 2, TTC applies small number of correction steps along the stochastic sampling path, only after the global structure has stabilized. This delay prevents the generation from falling into sink-collapse (Cui et al., 2026), phenomenon where newly generated frames repeatedly regress toward the sink frames instead of evolving naturally. At these chosen steps in the sampling path, TTC performs reference-conditioned denoising by utilizing the initial frame context to anchor corrected clean prediction. Then, this corrected state is re-noised back to the variance level corresponding to the current timestep, which ensures that the intervention remains compatible with the expected noise distribution. By integrating correction into the stochastic sampling path of the autoregressive diffusion process rather than directly replacing the denoised prediction, this mechanism suppresses long-term error accumulation and temporal drift without retraining, while preserving highfidelity temporal coherence over extended durations. In this work, we show that long-horizon stability in autoregressive video generation can be achieved through test-time intervention alone. Our method suppresses error accumulation with negligible computational overhead, without requiring any retraining. As result, it extends the stable generation length of distilled autoregressive models from few seconds to over 30 seconds, while achieving visual quality comparable to state-of-the-art training-based methods. Consistent improvements across multiple model architec2 Pathwise Test-Time Correction for Autoregressive Long Video Generation tures demonstrate that TTC is robust and general solution for stabilizing distilled autoregressive diffusion models. approach distinguishes itself from both paradigms by being fully training-free, avoiding both the overhead of candidate search and the complexity of parameter optimization. 2. Related Work Bidirectional Models for Video Generation. Diffusion models have been widely adopted for video generation, with recent works typically formulating video synthesis as sequence-level joint denoising problem. Under this bidirectional diffusion paradigm, all frames are denoised simultaneously via spatiotemporal attention, enabling the model to leverage global temporal context and produce temporally coherent, high-fidelity videos (Blattmann et al., 2023; Yin et al., 2023; Jia et al., 2025a; Ma et al., 2025b; Zhang et al., 2025c; Huang et al., 2025b). Large-scale systems such as Hunyuan (Kong et al., 2024) and Wan (Wan et al., 2025) further demonstrate the effectiveness of this joint denoising formulation at scale. However, because the entire sequence must be processed as whole during inference, this paradigm inherently precludes streaming or incremental generation, limiting its applicability in real-time and interactive scenarios. Autoregressive Models for Video Generation. Autoregressive Models for Video Generation. Autoregressive video diffusion models generate videos sequentially in strict causal manner, conditioning each new frame or segment on the historical context of previously generated content (Chen et al., 2024; Yin et al., 2025b; Huang et al., 2025d; Liu et al., 2025b; Yang et al., 2025; Chen et al., 2025a; Ji et al., 2025; Teng et al., 2025; Deng et al., 2025; Guo et al., 2025). While this formulation naturally supports streaming generation with low initial latency, it is inherently susceptible to error accumulation, where minor deviations propagate and amplify across steps, leading to severe temporal drift and degraded coherence in long videos. To mitigate this issue, recent works introduce planning methods (Zhang et al., 2025b; Xiang et al., 2025) or explicit memory mechanisms (Sun et al., 2025; Chen et al., 2025b; Yu et al., 2025a; Cai et al., 2025; Huang et al., 2025a; HunyuanWorld, 2025), strategies that typically necessitate complex architectural modifications and extensive re-training. Test-time Image/Video Generation. Test-time generation methods (Wang et al., 2025; Sun et al., 2020; Liang et al., 2025) aim to enhance the performance of pre-trained models directly during the inference phase. Test-time scaling improves quality by iteratively searching over multiple candidates, as seen in Video-T1 (Liu et al., 2025a) and EvoSearch (He et al., 2025), yet this comes at the price of prohibitive computational cost. Similarly, Test-time optimization refines generation via auxiliary parameter updates that necessitate instance-specific training, such as HyperNoise (Eyring et al., 2025), AutoRefiner (Yu et al., 2025b), and SLOWFAST-VGEN (Hong et al., 2025). In contrast, our 3. Test-time Optimization for Distilled Models 3.1. Background: Few-step Distilled Sampling In this section, we formulate autoregressive video generation as next-chunk prediction under context-conditional generative model. Given video sequence {x1, . . . , xN }, the joint distribution factorizes as p(x1:N ) = (cid:89) t=1 pθ(xt St), St = {x1, . . . , xt1}, (1) where St denotes the context at step t, consisting of all previously generated frames or chunks. As illustrated in Figure 3, existing autoregressive video generation methods typically fall into three categories. Discrete autoregressive models generate each chunk through single deterministic prediction conditioned on past outputs, while multi-step autoregressive diffusion models approximate the same conditional distribution via deterministic ODE-based sampling trajectory. In contrast, few-step distilled diffusion models replace deterministic ODE solvers with stochastic sampling process that explicitly injects noise at intermediate steps. Under this formulation, each conditional distribution pθ(xt St) is no longer realized as single deterministic mapping, but through stochastic diffusion sampling trajectory defined over sparse set of diffusion steps {T0 = 0, T1, . . . , TK = Tmax}. Specially, distilled video generation begins from Gaussian noise xTmax (0, I) and evolves progressively along this trajectory via sequence of denoiserenoise transitions. Specifically, at each denoising step Tj, the generation process starts from noisy latent state and applies the denoising network to produce an estimate of the underlying clean latent representation, Tj t,0 = Gθ (cid:16) Ψ(x Tj+1 t, , ϵ Tj , Tj); St, Tj (cid:17) , (2) where Gθ() denotes the parameterized denoising network, and St represents the autoregressive context at step t. After each denoising step, distilled diffusion models proceed by re-injecting noise according to the predefined schedule, mapping the clean estimate back onto the diffusion trajectory. Concretely, the estimated clean latent is re-noised to obtain the latent state at the next diffusion step, Tj1 (cid:16) = Ψ t,0, ϵ Tj1 Tj , Tj1 (cid:17) , ϵ Tj1 (0, I), (3) thereby yielding stochastic transition that advances the generation process to the next noise level. The forward diffusion process Ψ() is defined as Ψ(x, ϵT , ) = αT + σT ϵT , (4) 3 Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 3. Variants of autoregressive video generation. Discrete AR uses single-step deterministic prediction, multi-step diffusion follows deterministic ODE trajectory, while few-step distilled diffusion performs stochastic sampling with intermediate noise injection. Figure 4. Comparison of two toy test-time optimization variants based on LoRA fine-tuning. where αT and σT are predefined diffusion coefficients corresponding to step . Repeating this denoisere-noise procedure across diffusion steps forms the complete stochastic sampling trajectory of distilled diffusion models. 3.2. Toy Experiment: Apply Test-time Optimization to Long Video Generation Existing test-time optimization (TTO) methods improve generation quality by aligning the model distribution with predefined reward function. Given pre-trained generative model with output distribution pbase, TTO typically defines reward-weighted target distribution direct LoRA fine-tuning (Hu et al., 2022) at test time, following HyperNoise (Eyring et al., 2025) and AutoRefiner (Yu et al., 2025b). Both variants use the same backbone and identical LoRA adapters, and differ only in their optimization objectives. The first variant fine-tunes LoRA with standard denoising reconstruction loss on early frames across noise levels. The second variant replaces pixel-level reconstruction with semantic consistency objective, enforcing similarity to early frames in pretrained feature spaces (Radford et al., 2021; Oquab et al., 2024). Figure 4 shows that these two objectives lead to distinct failure modes. The reconstruction-based variant quickly collapses to trivial solution, where later frames become near-duplicates of the initial frame, resulting in severe motion loss. In contrast, the semantic objective fails to effectively reduce long-horizon error accumulation, and the generated videos still exhibit temporal drift similar to the baseline. These results indicate that naive TTO, whether based on low-level reconstruction or high-level semantics, is insufficient for stable long-horizon generation. 4. From Test-Time Optimization to Test-Time p(x) pbase(x) exp(r(x)), (5)"
        },
        {
            "title": "Correction",
            "content": "where r(x) encodes preferences of samples x. The optimization objective can be formulated as minimizing the KL divergence between parameterized distribution pϕ and the target distribution p, min ϕ DKL(pϕp) = min ϕ DKL(pϕpbase) Expϕ[r(x)], (6) which trades off reward maximization against deviation from the original model distribution. However, for long video generation, it remains challenging to design an explicit reward that effectively suppresses error accumulation. Temporal drift arises from coupled inconsistencies in semantics, appearance, and motion, which are difficult to characterize with single hand-crafted objective. naive alternative is to constrain each subsequent chunks predictive distribution to remain close to that of the initial frames, effectively anchoring generation to early content. To assess this idea, we conduct two toy experiments using Based on the above analysis, we identify two key limitations of TTO for long video generation. Reward design for error accumulation. Temporal drift stems from coupled errors in semantics, appearance, and motion, which are hard to capture with single reward: lowlevel reconstruction suppresses motion, while high-level semantic objectives lack frame-wise correction signals. Optimization Challenges and Collapse. Performing testtime optimization on distilled models presents significant training difficulties. The models tend to overfit rapidly to the auxiliary reward, causing the optimization trajectory to collapse into specific, degenerate solutions that violate the pre-trained generative prior. Together, these limitations motivate shift from parameterupdating test-time optimization to test-time correction, which avoids model updates and instead performs trajectoryaware interventions during sampling. Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 5. Overall pipeline of our method. sparse set of correction steps is inserted into the stochastic sampling path until the global structure stabilizes. At selected steps, TTC performs reference-conditioned denoising using the initial frame to obtain corrected prediction, which is then re-noised to the current timestep to remain consistent with the expected noise distribution. This on-path, training-free correction suppresses long-term error accumulation and stabilizes long-horizon generation. Figure 6. Intermediate predictions along the stochastic sampling path. High-noise steps determine global structure, while low-noise steps refine appearance details under fixed layout. 4.1. Correctability along the Stochastic Sampling Path Distilled few-step diffusion models maintain stochastic sampling trajectory through iterative noise re-injection, which prevents premature convergence and preserves flexibility in intermediate states. As shown in Figure 6, this trajectory exhibits clear functional phase transition. At high noise levels, the denoising process primarily determines global structure, such as scene layout and spatial relationships. As the noise level decreases, the generation progressively shifts to an appearance refinement stage, where local textures and fine visual details are synthesized while the global structure remains largely fixed. This phase-wise behavior naturally suggests principled test-time correction strategy. Rather than intervening throughout the sampling process, we apply alternative conditioning only during the appearance refinement stage, after the global structure has stabilized. At this stage, the model is less sensitive to structural changes, allowing visual attributes to be adjusted without affecting layout or geometry. As result, targeted test-time intervention can modulate appearance while preserving structural consistency. Motivated by planning-based video generation models (Zhang et al., 2025b; Xiang et al., 2025), which relax strictly unidirectional prediction via cross-frame context, we consider applying test-time intervention at single sampling step after structural stabilization. Specifically, at designated step j, we restrict the visible context state St to include only the earliest frame, forcing the model to rely exclusively on the initial frame for subsequent appearance refinement and texture generation. This single-point correction process can be formalized as: (cid:17) (cid:16) Ψ(x Tj t,0, ϵ Tj1 , Tj1); S(jj) Tj1 t,0 = Gθ , Tj1 , (7) where S(jj) denotes the modified context state. Specifically, at the designated sampling step j, the original autoregressive context St is replaced by the earliest-frame context S0, while all other sampling steps remain unchanged. Here, corresponds to the stage at which the global layout and object structure have stabilized. 4.2. Path-wise Test-time Correction Despite its conceptual simplicity, single-point latent correction frequently leads to visible artifacts in distilled autoregressive diffusion models, such as flickering, abrupt appearance changes, and temporal inconsistency. To overcome this, we propose path-wise self-correction strategy as shown in Figure 5, which leverages the models stochastic nature to ensure smooth state transitions. Instead of performing hard correction at single denoising step, our method first applies the intervention to the current prediction and then re-noises it back to the current noise level. By restarting the sampling process from this re-noised state, the correction is naturally integrated into the stochastic 5 Pathwise Test-Time Correction for Autoregressive Long Video Generation path. This avoids the abrupt state transitions typical of direct prediction replacement, allowing the model to smoothly assimilate the update while maintaining generation stability. Formally, consider generating chunk under denoising schedule Tmax > > Tj > Tj1 > > T0. At step j, given the current noisy latent xTj and the evolving context St, the denoiser produces clean prediction as: xTj t,0 = Gθ (cid:16) xTj ; St, (cid:17) . (8) Instead of directly executing the next denoising update following the standard path in Figure 5, we first apply forward diffusion noise injection to the current prediction and explicitly map it to the next noise level Tj1. Then, we replace the evolving context St with stable reference context S0 for denoising. This produces reference-aligned corrected clean prediction as: xTj1,c t, = Gθ (cid:16) (cid:16) Ψ xTj t,0, ϵj1 , Tj1 (cid:17) (cid:17) ; S0, 1 , (9) (cid:17) (cid:16) (cid:16) Ψ , Tj1 , ϵj1 xTj1,c t,0 xTj1 t,0 = Gθ The corrected prediction is then mapped back to the same noise level Tj1 through noise injection. Denoising is subsequently resumed under the true evolving context St (cid:17) , (10) This sequence of operations integrates test-time correction directly into the stochastic sampling process. All modified intermediate states are produced through valid diffusion transitions, as summarized in Algorithm 1. This effectively suppresses chunk-boundary flickering, mitigates long-horizon error accumulation, and preserves temporal coherence in autoregressive video generation. ; St, 1 Algorithm 1 Path-wise Test-time Correction 1: Input: Noise schedule {TJ > > T0 = 0}; Generator Gθ; Evolving context St; Ref context S0; Correction indices ; Diffusion forward process Ψ 2: Output: Final prediction x0 t,0 3: Sample xTJ (0, I) 4: for = down to 2 do (cid:16) (cid:17) xTj ; St, xTj t,0 Gθ with St Sample ϵTj1 if 1 then (0, I) # Initial Noise # Initial prediction # Correct (cid:17) (cid:16) (cid:17) Ψ , Tj1 ; S0, 1 Phase A: Reference-guided Correction t,0, ϵTj1 xTj xTj1,c (cid:16) xTj1,c xTj1,c t,0 Gθ trajectory using S0 Phase B: Re-noising & Re-denoising Sample ϵTj1 (cid:16) xTj1 Ψ new noise xTj1 t,0 Gθ step with St (0, I) xTj1,c t, ; St, 1 , ϵTj1 xTj1 , Tj1 (cid:17) (cid:17) (cid:16) # Inject # Finalize else (cid:16) xTj1 Ψ xTj1 t,0 Gθ xTj t,0, ϵTj1 (cid:16) xTj1 (cid:17) , Tj1 (cid:17) ; St, 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end if 18: 19: end for 20: return xT1 t,0 5. Experiments Baseline. We evaluate our test-time correction method on two baseline models, CausVid (Yin et al., 2025b) and SelfForcing (Huang et al., 2025d). Both baselines are built on the Wan2.1-T2V-1.3B model (Wan et al., 2025) and generate 5-second video clips at 16 FPS resolution of 832 480. Standard Evaluation. We benchmark our method against representative autoregressive video diffusion baselines, including CausVid (Yin et al., 2025b), Self-Forcing (Huang et al., 2025d), Rolling Forcing (Liu et al., 2025b), and LongLive (Yang et al., 2025). Following standard protocols, evaluations are conducted using VBench (Huang et al., 2024) on 128 prompts randomly sampled from MovieGen (Polyak et al., 2024). Unless otherwise specified, we conduct all experiments in the 30-second video generation setting, using Self-Forcing as the default baseline. More experimental settings and implementation details for fair comparison are included in the supplementary material. Additional Evaluation. To rigorously evaluate temporal coherence, we complement standard VBench quality metrics with temporal color histograms and JEPA scores (Balestriero et al., 2025) to assess long-term temporal drift. Since temporal consistency scores can be artificially improved by suppressing motion, allowing models to effectively cheat the evaluation, we conduct comparisons under matched dynamic degrees to ensure fair and meaningful assessment. In addition, we use t-LPIPS (Zhang et al., 2018) to explicitly measure visual discontinuities, serving as direct proxy for flickering artifacts at autoregressive chunk boundaries in our ablation studies. Comprehensive experimental settings and implementation details for fair comparison are included in the supplementary material. Qualitative Results. As shown in Figure 7, integrating our method with Self-Forcing and CausVid substantially reduces error accumulation in long-horizon video generation. While the original baselines exhibit temporal drift and visual degradation over time, our integration maintains stable temporal coherence and visual fidelity over 30-second 6 Pathwise Test-Time Correction for Autoregressive Long Video Generation Table 1. Comprehensive comparison with SOTA methods on prompt-conditioned 30-second video generation. We report Throughput (fps), VBench metrics, Color-shift metrics (L1, Correlation), and JEPA consistency (Standard Deviation, Difference). Method Training Free Speed VBench Metrics Color-shift JEPA Consistency Total fps Subject Consistency Background Consistency Dynamic Degree Motion Smoothness Imaging Quality Aesthetic Quality L1 Correlation Standard Deviation Difference Rolling Forcing LongLive CausVid CV + Ours Self-Forcing SF + Ours 15.38 - 15.79 10.53 15.79 10.53 95.8 95.5 91.2 93.2 92.5 94.0 95.1 95.4 91.4 93.3 93.2 94.2 35.9 44.5 50.8 69.5 62.5 60.2 98.9 98.8 98.1 97.6 98.0 98. 72.5 71.7 70.2 70.1 72.5 72.7 63.6 65.0 63.5 63.5 63.4 63.8 0.436 0.701 1.047 0.607 1.028 0.644 0.858 0.724 0.451 0.778 0.479 0.710 0.0162 0.0151 0.0199 0.0157 0.0145 0.0108 0.201 0.101 0.313 0.164 0.191 0. Table 2. Comprehensive comparison with test-time scaling methods on prompt-conditioned 30-second video generation. We report Throughput (fps) and VBench metrics. Method Train. Free Speed VBench Metrics Total fps Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. Self-Forcing SF + BoN SF + SoP SF + Ours 15.79 3.16 3.16 10.53 92.5 92.4 92.7 94.0 93.2 93.2 93.4 94.2 62.5 98.0 62.5 98.4 60.2 98.6 60.2 98. 72.5 72.7 72.7 72.7 63.4 63.3 63.1 63.8 Table 3. Ablation study on noise-correction steps. We evaluate quality using VBench metrics alongside the Boundary metric. Total NFE Timesteps VBench Metrics Boundary 750 500 250 Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. t-LPIPS 4 5 5 5 6 7 92.5 93.2 62.5 98.0 72.5 63.4 0. 93.6 94.3 60.2 98.6 72.6 63.2 93.2 93.9 60.9 98.5 72.8 63.1 93.6 94.1 57.0 98.5 72.9 63.4 94.0 94.2 60.2 98.3 72.7 63.8 93.1 93.9 61.7 98.4 73.0 63.1 93.4 94.2 62.5 98.5 72.4 63.8 0.161 0.182 0.183 0.176 0. 0.169 sequences, especially in videos with complex motion and appearance changes. Under the 30-second setting, our trainingfree approach achieves visual quality comparable to, and in some cases better than, Rolling Forcing and LongLive, which rely on additional training or specialized mechanisms. These results demonstrate that our method provides an effective and general test-time solution for improving long-term temporal consistency in autoregressive video generation. Quantitative Results. All quantitative results are summarized in Table 1. Under the 30-second generation setting on VBench, integrating our method into standard autoregressive baselines, Self-Forcing and CausVid, consistently improves long-horizon video generation quality across diverse prompts and scenes. In particular, our method substantially reduces error accumulation and temporal drift, leading to improved subject and background consistency while notably enhancing dynamic degree without sacrificing motion smoothness or imaging quality. Moreover, the proposed path-wise correction effectively stabilizes appearance evolution over time, as evidenced by lower color-shift L1 distances and higher histogram correlations between the first and last frames. At the semantic level, our method also improves JEPA consistency by reducing both the standard deviation and firstlast score difference across the entire sequence, indicating more coherent long-term representations. Compared with training-based methods such as Rolling Forcing and LongLive, our approach achieves comparable long-horizon consistency and visual quality while preserving stronger motion dynamics and requiring no additional training or parameter updates at test time. Comparison with Test-Time Scaling. We benchmark our approach against test-time scaling strategies, including Best-of-N (BoN) and Search-over-Path (SoP), as shown in Table 2. While these methods attempt to mitigate errors through redundant candidate generation or iterative search, they incur prohibitive computational overhead and inference latency. In contrast, our method embeds correction directly into single stochastic sampling trajectory. This design drastically reduces inference costs compared to multi-sample scaling, and by actively rectifying structural deviations rather than passively selecting from drifting candidates, it achieves superior suppression of long-term error accumulation with minimal overhead. More experimental settings and implementation details for are included in the supplementary material. Ablation Study on Path-wise Correction. We compare single-point and path-wise correction to evaluate the role of the stochastic sampling trajectory in practice. Single-point correction directly replaces the latent at fixed denoising step, whereas path-wise correction re-noises the corrected prediction to the same noise level and resumes denoising along the original trajectory. As shown in Figure 9 and Table 4, single-point correction frequently introduces flickering and temporal instability, leading to degraded consistency metrics and higher t-LPIPS scores. In contrast, path-wise correction achieves consistently higher temporal consistency and substantially lower t-LPIPS, resulting in more stable videos with improved temporal coherence. These results 7 Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 7. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. demonstrate that effective test-time intervention requires integrating corrections along the sampling path rather than directly replacing latents. Ablation Study on Noise-correction Steps. After establishing the necessity of path-wise correction, we evaluate how different numbers and placements of correction steps along the sampling path affect long-horizon generation quality. Enabling correction at noise levels 750, 500, and 250, either individually or in combination, consistently outperforms the baseline without correction, demonstrating the robustness and effectiveness of our method across different configurations. Considering the trade-off between performance and inference cost, we adopt correction at noise levels 500 and 250 in our experiments. Figure 8. Comparison between the sink-based method and path-wise correction. The sink-based method overly constrains intermediate states, leading to degraded motion dynamics and reduced temporal variation. Comparison with the Sink-based Method. We compare our proposed path-wise correction with the Sink-based method. The Sink-based approach keeps the Sink frame as visible context throughout the entire denoising process, effectively imposing persistent conditioning. In contrast, pathwise correction is applied only at later stages after structural stabilization, where corrected predictions are re-noised and integrated along the stochastic sampling trajectory. Because 8 Pathwise Test-Time Correction for Autoregressive Long Video Generation Table 4. Comparison of correction strategies. We evaluate quality using VBench metrics alongside the Boundary metric. Method VBench Metrics Boundary Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. t-LPIPS Single-point Path-wise 93.4 94.0 94.0 94. 57.0 60.2 98.3 98.3 71.6 72.7 62.8 63.8 0.205 0.176 Table 5. Comparison on prompt-conditioned 5-second video generation. We evaluate quality using standard VBench metrics. Method VBench Metrics Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. CausVid CausVid + Ours Self-Forcing Self-Forcing + Ours 96.2 96.6 97.0 97. 94.9 95.2 96.2 96.3 54.7 68.0 62.5 62.5 98.2 97.8 98.7 98. 70.5 70.5 72.9 73.0 63.8 64.2 64.5 64.6 the Sink frame continuously participates in all denoising steps, the model becomes overly conditioned on it, causing generated content to remain visually and structurally close to the Sink frame. This static conditioning restricts motion and scene variation, suppressing temporal dynamics, as shown in Table 1 and Figure 8. By contrast, path-wise correction preserves structural flexibility in early stages and introduces correction only during appearance refinement, maintaining temporal coherence while retaining meaningful video dynamics. Comparison on Short Video Generation. As shown in Table 5, we evaluate our method on short video generation. Although error accumulation is less pronounced under short temporal horizons, our method still consistently outperforms the baseline across most metrics. This indicates that the proposed correction strategy is not specialized to long-horizon generation, but also remains effective in short video settings. Together with the significant improvements observed for long video generation, these results demonstrate the robustness and general applicability of our approach. 6. Conclusion In this paper, we propose Test-Time Correction, trainingfree test-time method for stabilizing distilled autoregressive diffusion models in long-horizon video generation. The proposed approach addresses error accumulation by introducing training-free, reference-based correction along the stochastic sampling process, allowing corrected predictions to be smoothly inherited by subsequent denoising steps. Without modifying model parameters or requiring additional training, our method effectively suppresses temporal drift while preserving the original generation behavior. Extensive experiments demonstrate that Test-Time Correction Figure 9. Comparison of single-point and path-wise correction. Single-point correction causes temporal discontinuities, while onpath re-noising improves temporal stability and reduces flickering. consistently improves long-horizon stability across multiple distilled video generation models, extending the achievable generation length to 30 seconds with negligible computational overhead and competitive visual quality."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents training-free test-time method for improving the stability of autoregressive video generation models. The primary goal of this work is to advance the field of machine learning by enabling more reliable longhorizon video synthesis without additional training or model modification. While the proposed method may contribute to downstream applications that rely on long video generation, such as content creation and simulation, it does not introduce new capabilities beyond existing video generative models. The potential societal impacts of this work are therefore aligned with those already associated with video generation technologies, and no specific additional ethical concerns are introduced by the method itself."
        },
        {
            "title": "References",
            "content": "Balestriero, R., Ballas, N., Rabbat, M., and LeCun, Y. Gaussian embeddings: How jepas secretly learn your data density. CoRR, 2025. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: Highresolution video synthesis with latent diffusion models. In CVPR, 2023. Cai, S., Yang, C., Zhang, L., Guo, Y., Xiao, J., Yang, Z., Xu, Y., Yang, Z., Yuille, A., Guibas, L., et al. Mixture of contexts for long video generation. CoRR, 2025. Chen, B., Martı Monso, D., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Next9 Pathwise Test-Time Correction for Autoregressive Long Video Generation token prediction meets full-sequence diffusion. NeurIPS, 2024. Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., et al. Skyreels-v2: Infinitelength film generative model. CoRR, 2025a. Chen, Y., Liang, Y., Wang, J., Chen, T., Cheng, J., Gu, Z., Huang, Y., Jiang, Z., Li, W., Li, T., et al. Teleworld: Towards dynamic multimodal synthesis with 4d world model. CoRR, 2025b. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., and Hsieh, C.-J. Self-forcing++: Towards minute-scale high-quality video generation. CoRR, 2025. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., and Hsieh, C.-J. Lol: Longer than longer, scaling video generation to hour. CoRR, 2026. Deng, H., Pan, T., Diao, H., Luo, Z., Cui, Y., Lu, H., Shan, S., Qi, Y., and Wang, X. Autoregressive video generation without vector quantization. In ICLR, 2025. Eyring, L., Karthik, S., Dosovitskiy, A., Ruiz, N., and Akata, Z. Noise hypernetworks: Amortizing test-time compute in diffusion models. CoRR, 2025. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. Guo, Y., Yang, C., He, H., Zhao, Y., Wei, M., Yang, Z., Huang, W., and Lin, D. End-to-end training for autoregressive video diffusion via self-resampling. CoRR, 2025. He, H., Liang, J., Wang, X., Wan, P., Zhang, D., Gai, K., and Pan, L. Scaling image and video generation via test-time evolutionary search. CoRR, 2025. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. Hong, Y., Liu, B., Wu, M., Zhai, Y., Chang, K.-W., Li, L., Lin, K., Lin, C.-C., Wang, J., Yang, Z., Wu, Y. N., and Wang, L. Slowfast-VGen: Slow-fast learning for action-driven long video generation. In ICLR, 2025. Huang, J., Hu, X., Han, B., Shi, S., Tian, Z., He, T., and Jiang, L. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. CoRR, 2025a. Huang, T., Zheng, W., Wang, T., Liu, Y., Wang, Z., Wu, J., Jiang, J., Li, H., Lau, R. W. H., Zuo, W., and Guo, C. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. SIGGRAPH, 2025b. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. CoRR, 2025c. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In NeurIPS, 2025d. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. HunyuanWorld, T. Hy-world 1.5: systematic framework for interactive world modeling with real-time latency and geometric consistency. arXiv preprint, 2025. Ji, S., Chen, X., Yang, S., Tao, X., Wan, P., and Zhao, H. Memflow: Flowing adaptive memory for consistent and efficient long video narratives. CoRR, 2025. Jia, W., Lu, Y., Huang, M., Wang, H., Huang, B., Chen, N., Liu, M., Jiang, J., and Mao, Z. Moga: Mixture-of-groups attention for end-to-end long video generation. CoRR, 2025a. Jia, W., Lu, Y., Huang, M., Wang, H., Huang, B., Chen, N., Liu, M., Jiang, J., and Mao, Z. Moga: Mixture-of-groups attention for end-to-end long video generation. CoRR, 2025b. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. CoRR, 2024. Liang, J., He, R., and Tan, T. comprehensive survey on test-time adaptation under distribution shifts. IJCV, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Liu, F., Wang, H., Cai, Y., Zhang, K., Zhan, X., and Duan, Y. Video-t1: Test-time scaling for video generation. CoRR, 2025a. Hu, L. Animate anyone: Consistent and controllable imageIn CVPR, to-video synthesis for character animation. 2024. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. CoRR, 2025b. 10 Pathwise Test-Time Correction for Autoregressive Long Video Generation Lu, Y., Zeng, Y., Li, H., Ouyang, H., Wang, Q., Cheng, K. L., Zhu, J., Cao, H., Zhang, Z., Zhu, X., et al. Reward forcing: Efficient streaming video generation with rewarded distribution matching distillation. CoRR, 2025. Ma, X., Wang, Y., Chen, X., Jia, G., Liu, Z., Li, Y., Chen, C., and Qiao, Y. Latte: Latent diffusion transformer for video generation. TMLR, 2025a. Ma, Y., Liu, C., Wang, J., Liu, J., Huang, H., Wu, Z., Zhang, C., and Li, X. Tempomaster: Efficient long video generation via next-frame-rate prediction. CoRR, 2025b. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P., Li, S., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision. TMLR, 2024, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. CoRR, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, 2021. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. CoRR, 2025. Wang, R., Sun, Y., Tandon, A., Gandelsman, Y., Chen, X., Efros, A. A., and Wang, X. Test-time training on video streams. JMLR, 2025. Xiang, X., Chen, Y., Zhang, G., Wang, Z., Gao, Z., Xiang, Q., Shang, G., Liu, J., Huang, H., Gao, Y., et al. Macrofrom-micro planning for high-quality and parallelized autoregressive long video generation. CoRR, 2025. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Real-time interactive long video generation. CoRR, 2025. Yesiltepe, H., Meral, T. H. S., Akan, A. K., Oktay, K., and Yanardag, P. Infinity-rope: Action-controllable infinite video generation emerges from autoregressive self-rollout. CoRR, 2025. Yi, J., Jang, W., Cho, P. H., Nam, J., Yoon, H., and Kim, S. Deep forcing: Training-free long video generation with deep sink and participative compression. CoRR, 2025. Yin, S., Wu, C., Yang, H., Wang, J., Wang, X., Ni, M., Yang, Z., Li, L., Liu, S., Yang, F., et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. In ACL, 2023. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast causal video generators. In CVPR, 2025a. Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world modeling. CoRR, 2025. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025b. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al. Magi-1: Autoregressive video generation at scale. CoRR, 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., and Liu, X. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In SIGGRAPH Asia, 2025a. Yu, Z., Hayakawa, A., Ishii, M., Yu, Q., Shibuya, T., Zhang, J., and Mitsufuji, Y. Autorefiner: Improving autoregressive video diffusion models via reflective refinement over the stochastic sampling path. CoRR, 2025b. Zhang, G., Shi, C., Jiang, Z., Xiang, X., Qian, J., Shi, S., and Jiang, L. Proteus-id: Id-consistent and motion-coherent video customization. CoRR, 2025a. 11 Pathwise Test-Time Correction for Autoregressive Long Video Generation Zhang, L., Cai, S., Li, M., Wetzstein, G., and Agrawala, M. Frame context packing and drift prevention in nextframe-prediction video diffusion models. In NeurIPS, 2025b. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. In ICML, 2025c. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhu, S., Chen, J. L., Dai, Z., Dong, Z., Xu, Y., Cao, X., Yao, Y., Zhu, H., and Zhu, S. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV, 2024. 12 Pathwise Test-Time Correction for Autoregressive Long Video Generation A. Details on Samplers. Few-step stochastic sampling. The autoregressive video diffusion backbones used in our frameworkSelf-Forcing and CausVidare obtained by step distillation from multi-step bidirectional video diffusion model trained with the Rectified Flow (RF) objective. In RF, the forward noising process is defined by linear interpolation between clean latent video x0 and an isotropic Gaussian terminal state xTmax (0, I): Differentiating (11) with respect to gives the corresponding velocity along the path, xt = x0 + (1 t) xTmax, [0, 1]. vt dxt dt = x0 xTmax, (11) (12) which is constant in under this parameterization. time-conditioned flow network vθ0(xt, t) is trained to regress this velocity via mean squared error, Lflow = Ex0, xTmax , (cid:104)(cid:13) (cid:13)vθ0 (xt, t) vt (cid:105) . (cid:13) 2 (cid:13) 2 (13) For long-horizon video generation, the bidirectional RF predictor vθ0 is distilled into causal autoregressive model vθ by replacing bidirectional attention with causal attention, so that the prediction for the i-th frame conditions only on previously generated frames x<i (with KV caching used to reuse past attention states during sequential generation). At inference, sampling is carried out on small set of discrete timesteps {TJ > TJ1 > > T0} with much smaller than standard multi-step samplers. Given noisy latent at step Tj, the model outputs denoised estimate using the RF update form ˆx 0tj = Gθ (cid:16) tj ; x<i, tj (cid:17) = tj + (1 tj) vθ (cid:16) tj ; x<i, tj (cid:17) , (14) and then constructs the next noisy state tj1 by re-applying the RF forward interpolation with newly sampled Gaussian noise, i.e., using (11) with = tj1. This yields stochastic few-step sampler in which independent Gaussian noise is injected at each transition between adjacent timesteps. ODE-based sampling. Rectified Flow also supports deterministic sampler by treating the learned velocity predictor as an ordinary differential equation (ODE). Given the causal autoregressive velocity field vθ(; x<i, t), one can define the sampling dynamics as dxt dt = vθ(xt; x<i, t), xt=1 (0, I), (15) which maps an initial Gaussian state at = 1 to terminal sample at = 0 through deterministic integration. In practice, (15) is approximated on discrete time grid {TJ > TJ1 > > T0}. Compared with the stochastic transition that re-samples Gaussian noise at each step, fθ,tj (cid:17) (cid:16) tj = Ψ (cid:16) ˆx 0tj , ϵ j1, tj1 (cid:17) , ϵ j1 (0, I), (16) an ODE sampler removes the noise variable ϵ the explicit Euler method, the update from tj to tj1 is j1 and replaces the transition with deterministic one-step integrator. Using fθ,tj (cid:17) (cid:16) tj = tj + (tj1 tj) vθ (cid:16) tj ; x<i, tj (cid:17) . (17) Equation (17) is the standard explicit Euler discretization of the ODE (15) on the chosen timestep schedule. Finally, note that the two samplers differ only in whether the transition between adjacent timesteps introduces an additional Gaussian perturbation (stochastic) or performs purely deterministic numerical integration step (ODE). Under the ODE formulation, once the initial state xt=1 and the discretization scheme are fixed, the generated trajectory is fully determined by repeated application of (17). 13 Pathwise Test-Time Correction for Autoregressive Long Video Generation B. Details on Evaluations Boundary Continuity (t-LPIPS). To measure perceptual discontinuities at segment junctions in autoregressive generation, we compute LPIPS (Zhang et al., 2018) only on boundary-adjacent frame pairs. Our autoregressive model generates video as consecutive chunks. Let tk denote the last frame index of the k-th chunk; then the k-th boundary is the adjacent pair (ftk , ftk+1) for {1, . . . , 1}. We define the boundary score as the mean LPIPS over these 1 pairs: LPIPSboundary ="
        },
        {
            "title": "1\nK − 1",
            "content": "K1 (cid:88) k=1 LPIPS(ftk , ftk+1) . (18) This metric isolates changes that occur specifically when switching from one generated chunk to the next, rather than averaging over all within-chunk frame pairs. Color Shift (HSV Histogram). To quantify color distribution changes across the generated sequence, we compare the color histograms of the first and last frames. Let fstart and fend be the initial and final frames of generated video. We convert both frames to HSV space and compute an L1-normalized histogram of the Hue channel using 180 bins, denoted by hstart, hend R180 with hstart1 = hend1 = 1. We report two statistics between hstart and hend: (i) the L1 distance, hstart hend1, and (ii) the Pearson correlation coefficient, ρ(hstart, hend). # X: proxy (B,C,H,W) with torch.inference_mode(): = jacobian(lambda x: phi(x).sum(0), X) # (1) JEPA-score: Jacobian-based local complexity / density # phi(.) : frozen JEPA-style encoder (e.g., V-JEPA / DINOv2) = J.flatten(2).permute(1, 0, 2) sv = torch.linalg.svdvals(J) import torch import torch.nn.functional as from torch.autograd.functional import jacobian JEPA Consistency To rigorously quantify both the intrinsic distribution fidelity and the longhorizon semantic stability of autoregressive video generation, we adopt dual-metric evaluation framework based on frozen V-JEPA encoder ϕ(), grounded in recent theoretical findings that JEPA representations implicitly encode data density through Gaussian embeddings and local volume changes of the encoder mapping. Specifically, for each generated frame (or short temporal clip) xt, we compute the encoder Jacobian Jϕ(xt) = ϕ(xt)/xt and define an Intrinsic Density Score as Sdens 2 log det(cid:0)Jϕ(xt)Jϕ(xt)(cid:1), which estimates 1 the local log-volume expansion induced by ϕ and thus serves as proxy for the samples likelihood under the learned data manifold; monotonic decay of Sdens along time indicates progressive manifold departure and hallucination as the generation drifts into low-density regions of the data distribution. In parallel, to measure global semantic consistency, we compute the normalized embedding trajectory zt = norm(ϕ(xt)) and define the Temporal Drift Distance relative to the initial semantic anchor as dt = 1 z1, which captures distributional deviation in the JEPA-induced representation space. Aggregating these frame-wise measurements at fixed temporal granularity (e.g., per second), we report two summary statistics: JEPA-Std = Std({dt}T t=1) to characterize the volatility of representation drift, and JEPA-Diff = dT d1 to quantify the accumulated long-range semantic deviation, thereby providing holistic assessment of models robustness to both distributional collapse and semantic drift in long-horizon video generation. # (2) JEPA consistency: first-frame anchored temporal drift = phi(I_1_T) = F.normalize(Z, dim=-1) z_ref = Z[0] d_t = 1.0 - (Z @ z_ref) Figure 10. JEPA-score and JEPA consistency for long-video evaluation. JEPA_Std JEPA_Diff = (d_t[-1] - d_t[0]).abs() JEPA_score = sv.clamp(min=eps).log().sum(1) # (B, d, HW) # singular = d_t.std() # (T, d) # (B,) # (T,) values Test-time Scaling Configuration. We compare against two inference-time scaling protocols under fixed sampling budget of = 5. Best-of-N (BoN) performs selection at the trajectory level. For each video segment, we run independent sampling trajectories by drawing independent initial noise latents. Each trajectory is rolled out to complete segment, and we compute scalar reward for the resulting segment. Among the completed candidates, we keep the one with the highest reward score as the output of that segment. Search-over-Path (SoP) performs selection at the step level on the same timestep schedule. At each denoising timestep, we generate candidate next-step latents by injecting independent Gaussian noise realizations for that transition (equivalently, candidate stochastic updates from the current latent). We then evaluate the reward for each candidate at that timestep and select the candidate with the highest reward as the current latent for the next timestep. This greedy selection is repeated until the segment is completed. Pathwise Test-Time Correction for Autoregressive Long Video Generation C. Details on Methods. Details on Test-time Optimization (TTO). Following the test-time adaptation protocols established in HyperNoise (Eyring et al., 2025) and AutoRefiner (Yu et al., 2025b), we perform gradient-based optimization at each sampling step. We employ an AdamW optimizer with learning rate of 1 104. Specifically, at each denoising step for each latent chunk, the latent prediction is first decoded into pixel space via pre-trained VAE decoder (Wan et al., 2025). We then compute the Mean Squared Error (MSE) loss and the CLIP score (Radford et al., 2021) on the decoded image with the initial image, which serve as proxies for pixel-level and semantic-level rewards, respectively, to guide the optimization process. D. Further Quantitative Results. Full VBench Scores. We conduct comprehensive evaluation on the full VBench benchmark, using all 946 prompts and covering all 16 metrics reported in Table 6. For detailed metric definitions, we refer readers to the VBench paper. All values are computed with the official standardized evaluation scripts. Our method achieves substantial improvements in overall quality, particularly in frame-wise fidelity, and also outperforms distilled baselines on semantic scores. Table 6. Full evaluation on VBench metrics. We evaluate the performance across all quality and semantic dimensions. Quality Metrics Method Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality CausVid (Yin et al., 2025a) CausVid + Ours Self-Forcing (Huang et al., 2025c) Self-Forcing + Ours 89.1 91. 89.8 91.1 90.7 92.2 90.7 91.7 99.3 99.2 98.1 98.2 98.0 97. 98.5 98.8 62.5 71.9 69.4 68.1 61.7 61.1 60.0 60.7 65.3 66. 68.7 68.6 Method Object Class Multiple Objects Human Action Color Spatial Relationship Scene Temporal Style Appearance Style Overall Consistency Semantic Metrics Quality Score 80.8 81.9 81.4 82.1 Semantic Score CausVid (Yin et al., 2025a) CausVid + Ours Self-Forcing (Huang et al., 2025c) Self-Forcing + Ours 77.4 76.0 81.9 81.6 58.8 62.0 61.9 66.5 77.0 80.0 81.0 82. 84.2 80.6 88.0 92.2 61.8 63.9 79.2 80.2 32.2 34.9 32.2 30. 22.4 22.1 23.6 23.1 19.9 19.7 19.6 19.4 23.0 22.9 23.8 23. 65.9 66.3 70.0 70.7 Table 7. Dynamic Degree Analysis. Dynamic Preservation. Adhering to the evaluation protocols established in AutoRefiner (Yu et al., 2025b), we conduct comparative analysis of the dynamic degree against baseline methods, including Self-Forcing, Rolling Forcing, and Longlive. To rigorously assess the dynamic degree, we quantify the perceptual variation between temporally strided frames utilizing metrics such as LPIPS, SSIM, and PSNR with fixed sampling interval (e.g., = 12). We model the magnitude of motion and structural evolution over time by computing the average distance = Et[M(ft, ft+k)], where represents the specific metric function (e.g., LPIPS) and ft denotes the frame at time step t. As evidenced in Tab. 7, unlike baseline approaches that often compromise motion magnitude to ensure stability, our method sustains superior dynamic degree while preserving temporal coherence, thereby effectively maintaining the vividness of the generated content. Rolling Forcing (Liu et al., 2025b) Longlive (Yang et al., 2025) Self-Forcing (Huang et al., 2025c) Ours 16.2365 16.8669 15.6360 15. 0.5738 0.5969 0.5377 0.5440 0.2956 0.3056 0.3548 0.3489 LPIPS SSIM Dynamic Degree Method PSNR E. Further Qualitative Results. We provide additional visual results to further demonstrate the effectiveness of our method. Figure 11, Figure 12, and Figure 13 present more generated examples under diverse scenarios. These results consistently exhibit high visual quality and temporal coherence, reinforcing the robustness of our approach across different prompts and settings. 15 Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 11. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 12. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. 17 Pathwise Test-Time Correction for Autoregressive Long Video Generation Figure 13. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "Nanjing University",
        "Tencent Hunyuan",
        "University of Science and Technology of China"
    ]
}