{
    "paper_title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "authors": [
        "Zhankai Ye",
        "Bofan Li",
        "Yukai Jin",
        "Shuoqiu Li",
        "Wei Wang",
        "Yanfu Zhang",
        "Shangqian Gao",
        "Xin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 2 3 6 7 0 . 1 0 6 2 : r GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models Zhankai Ye1 Bofan Li1 Yukai Jin1 Shuoqiu Li1 Wei Wang2 Yanfu Zhang3 Shangqian Gao1,* Xiu Liu1,* 1Florida State University 2Texas Tech University 3William & Mary University"
        },
        {
            "title": "Abstract",
            "content": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLMs capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves 20% performance improvement over current stateof-the-art methods, validating that unified geometric basis effectively empowers the LLM for nuanced motion reasoning."
        },
        {
            "title": "Introduction",
            "content": "Human motion understanding stands as fundamental pillar for constructing embodied agents capable of perceiving and interacting with the physical world (Zhao et al., 2023; Driess et al., 2023). Recently, the integration of LLMs has revolutionized this domain, establishing new paradigm 1 for unified motion-language reasoning and generation (Wu et al., 2024a; Zhou et al., 2024). Central to this advancement is discrete motion tokenization, which quantizes continuous motion sequences into discrete codebook IDs to bridge the modality gap (Zhang et al., 2023; Guo et al., 2022b), enabling LLMs to leverage their vast pre-trained knowledge for motion tasks. However, relying solely on token IDs to bridge these modalities creates significant bottleneck. Existing pipelines (Zhang et al., 2023; Guo et al., 2022b; Jiang et al., 2023; Wang et al., 2024; Lai et al., 2025) typically decouple motion quantization from semantic embedding learning through disjoint two-stage protocol: they first train quantizer (e.g., Vector Quantized Variational Autoencoder (VQ-VAE)) to compress motion into codebook, and subsequently map the resulting discrete IDs to learnable embeddings to extend the LLMs vocabulary space. Crucially, this linkage is established purely via token IDs, disregarding the underlying geometric relationships between motion codes. Consequently, this approach fails to effectively align the intrinsic geometry of the motion space with the LLMs embedding space. Without unified geometric basis, the structural consistency across modalities is disrupted, thereby hindering the LLMs capacity for nuanced motion reasoning. To address this challenge, we propose GeoMotionGPT, novel framework grounded in the core insight that alignment is most effective when both modalities share unified geometric basis. Rather than forcing the LLM to reconstruct the unknown and complex intrinsic geometry among motion tokens from scratch, which is notoriously inefficient, we opt to construct shared geometric prior. Specifically, we select orthogonality as this unified basis, as it offers lightweight, controllable, and mathematically rigorous structure for alignment. By explicitly enforcing orthogonality on both the motion codebook and the LLM embedding space, we ensure that their relational structures naturally mirror each other. Our main technical contribution is achieved through three key architectural designs. ❶ We develop decoder-only quantizer optimized via Gumbel-Softmax (Jang et al., 2017). By making the quantization process fully differentiable, this design allows us to impose explicit regularization constraints directly on the codebook. This differentiability is critical, as it enables us to strictly enforce orthogonality among motion codes while simultaneously maximizing codebook utilization, effectively mitigating the codebook collapse often observed in standard VQ-VAEs. ❷ To bridge the modalities, we employ structure-preserving sparse projection. Specifically, it maps the motion code dimensions one-to-one into the LLMs embedding space and pads the remaining dimensions with zeros. This mechanism ensures that the geometric relationship and information in the codebook are efficiently propagated to the LLM. ❸ We devise two-stage orthonormal regularization scheme to balance geometric consistency with semantic flexibility. It imposes soft constraints during tokenizer pre-training to establish the unified geometric basis, followed by similar constraints during LLM finetuning to preserve alignment without hindering the models capacity for semantic adaptation. Our contributions can be summarized as follows: We propose GeoMotionGPT, novel framework that aligns motion and language via shared orthogonal geometric basis. Moving beyond superficial token-ID alignment, we enforce unified geometric structure across modalities, thereby enhancing the LLMs capacity for nuanced motion reasoning. To realize efficient geometric alignment, we design decoder-only quantizer with GumbelSoftmax and sparse projection mechanism, complemented by two-stage regularization schedule that balances geometric rigidity with semantic flexibility. Extensive experiments on HumanML3D demonstrate that our approach establishes new state-of-the-art performance, with at least 20% improvement, validating the effectiveness of explicit geometric alignment."
        },
        {
            "title": "2 Related Work",
            "content": "Motion Understanding Using LLMs. Motionlanguage research can be roughly grouped by how it connects motion and text. widely used approach targets motion understanding by aligning motion and text in shared embedding space through contrastive or retrieval objectives (Petrovich et al., 2023), and CLIP-style alignment further supports semantic matching across modalities (Tevet et al., 2022). Another line treats motion as discrete or token-like sequence and applies language-modeling objectives to textmotion problems, including reciprocal tokenized modeling and GPT-style motionlanguage models (Guo et al., 2022b; Jiang et al., 2023; Wang et al., 2024; Zhu et al., 2025). These models are often strengthened by self-supervised motion objectives such as masked modeling (Guo et al., 2024) and by unified formulations spanning granularities and interaction scenarios (Park et al., 2025; Wu et al., 2025). In somewhat orthogonal but complementary direction, multimodal LLM systems demonstrate that new modalities can be connected to LLMs via learnable adapters or projections into the LLM embedding space (Alayrac et al., 2022; Li et al., 2023). VQ-VAE and Its Variances. VAEs (Kingma and Welling, 2014) optimize the evidence lower bound (ELBO) to trade off reconstruction accuracy and latent regularization through amortized inference and the reparameterization trick. Building on this, (Higgins et al., 2017) modifies the VAE objective by up-weighting the KL term, encouraging more factorized and disentangled latent factors at the cost of reconstruction fidelity. separate line of work replaces continuous latents with discrete representations. VQ-VAE (Van Den Oord et al., 2017) introduces discrete latent representations by quantizing learned codebook at the bottleneck, with auxiliary losses to stabilize codebook learning and encourage encoder commitment. This framework is extended with hierarchical discrete latents in (Razavi et al., 2019). Recent advances in discrete quantization improve efficiency and scalability by simplifying codebook design and training, increasing representational capacity under limited token budgets, maintaining high utilization for large vocabularies, enabling structured reuse, etc., as shown in (Mentzer et al., 2024; Lee et al., 2022; Zhu et al., 2024; Zhang et al., 2024; Chen et al., 2025). Orthogonality in Representation Learning. Orthogonality is common geometric bias for learning stable and well-conditioned representations. From spectral viewpoint, training becomes unstable when singular values deviate significantly from one, which can amplify or suppress signals 2 Figure 1: Overall framework of GeoMotionGPT. Left: DVQ-based motion tokenizer encodes an input motion into discrete codebook indices and reconstructs ˆx via decoder. Middle: we introduce an auto-alignment objective with orthogonality, encouraging the normalized codebook correlation (and its projected embedding counterpart) to approach the identity matrix. Right: the LLM vocabulary is extended with trainable motion-token embeddings while keeping the original text embeddings frozen, enabling multimodal motion-language training and inference. and gradients, as formalized in (Jia et al., 2017). Beyond stability, orthogonality promotes representation compatibility by favoring rotation-like transformations that preserve distributional geometry, as discussed in (Ricci et al., 2025). This rotation-like property is closely related to isometry-motivated representation learning, as explored in (Qi et al., 2020) and to plug-and-play geometric embedding losses such as (Lezama et al., 2018). parallel line of work focuses on practical training mechanisms for orthogonality. (Huang et al., 2018) provides Stiefel-manifold perspective and normalizationstyle techniques to keep matrices near orthogonal during learning, while (Huang et al., 2020) motivates partial orthogonalization to balance stability and expressivity. Orthogonality has also been examined in modern architectures and settings, including CNN studies such as (Bansal et al., 2018) and initialization-focused methods like (Xie et al., 2017). Since our endpoint model is transformerbased LLM, orthogonality constraints in attention models are also relevant, as discussed in (Zhang et al., 2021; Fei et al., 2022)."
        },
        {
            "title": "3 Our Approach",
            "content": "Existing approaches typically learn quantization map : and separate embedding map ϕ : independently. Because the only connection between these stages is the discrete token IDs, the relational geometry among codes in is not explicitly preserved in S, leading to geometric mismatch. To resolve this, we impose unified geometric basis across both modalities, adopting orthogonality as the core structural constraint. As illustrated in Figure 1, we define the motion codebook = {c1, . . . , cK} RD such that it approximates an orthonormal basis: ci, cj δij, (1) where δij is the Kronecker delta. This orthogonality serves as structured inductive bias. To enforce this condition during training, we apply orthogonal regularization to the codebook. Let ˆC be the row-normalized codebook where ˆck = ck/ck2. We compute the Gram matrix = ˆC ˆC and define the orthogonal loss as: Lortho = IK2 . (2)"
        },
        {
            "title": "3.1 Geometric Unification as Alignment",
            "content": "Instead of treating motion tokenization and language modeling as loosely coupled tasks linked only by token IDs, we formulate alignment as geometric unification problem. Let denote the continuous intrinsic manifold of human motion, is the LLM embedding space, and = {1, . . . , K} is the discrete index set of vocabulary. This loss softly encourages pairwise orthogonality among motion codes, guiding them towards linear independence and maximal distinctness."
        },
        {
            "title": "3.2 Structure-Preserving Sparse Projection",
            "content": "Having established an orthogonal basis in the codebook via Lortho, our next goal is to transfer this geometric structure intact into the LLMs embedding space S. Directly learning dense mapping ϕ 3 often distorts the meticulously optimized geometry. Instead, we employ sparse projection to explicitly preserve this orthogonal structure. We map the D-dimensional motion codes into the higher-dimensional LLM space RD (D D) by distributing them across randomly selected active dimensions. Specifically, we define fixed projection matrix {0, 1}DD initialized by randomly selecting unique row indices {1, . . . , D} to act as identity mappings, while setting all other entries to zero. As illustrated in Figure 1, the embedding is computed as: ek = Pck. (3) Intuitively, this operation scatters the motion code values into the high-dimensional vector ek at random positions, filling the remaining dimensions with zeros. This projection acts as strict isometric embedding, preserving the inner product structure regardless of the random indices chosen. Lemma. Let be sparse projection matrix where each column contains exactly one 1 at unique row index and 0 elsewhere. If the original codebook vectors {ck} are pairwise orthogonal, then the projected embeddings {ek} are also pairwise orthogonal. Proof. The inner product in the LLM space is: ej = (Pci)(Pcj) = e (PP)cj. Since maps source dimension to unique target dimension without overlap, the columns of are orthonormal. Thus, PP = ID. It follows that: ej = e If ci cj, then ei ej. IDcj = cj. By freezing this projection during LLM finetuning, we ensure that the semantic space operates directly on the orthogonal geometry, free from the distortion of learnable adaptors."
        },
        {
            "title": "3.3 Decoder-Only Vector Quantization (DVQ)",
            "content": "With the sparse projection guaranteeing the transfer of geometric structure to the LLM, the critical task becomes regulating the geometric properties at their source: the motion codebook. As shown in Figure 1, the geometric alignment originates within the VQ stage. However, standard VQ-VAE offers limited control over codebook geometry: the non-differentiable nearest-neighbor assignment blocks direct, geometry-aware gradients from downstream objectives, including the reconstruction loss, thereby hindering fine-grained modulation of the codebook structure. 4 To overcome this, we propose decoder-only vector quantization scheme that replaces hard assignment with fully differentiable GumbelSoftmax operator (Jang et al., 2017). Given the quantizer output projected to logits = Q(x), RK, where is the raw input, we directly obtain one-hot vector by discretizing the output of the Gumbel-Softmax operator: ysoft = GumbelSoftmax(z; τ ), yhard = one-hot(ysoft), (4) where τ is the temperature. The selected motion embedding is then computed as follows: = (5) hardC. We employ the straight-through estimator (Bengio et al., 2013) for yhard to enable gradient calculation. The final output from the decoder is ˆx = D(h). Furthermore, we explicitly regulate codebook utilization to prevent token collapse. We track the empirical usage frequency of each motion code using mini-batch statistics, denoted as qk for the k-th code. To enforce balanced distribution, we maximize its self-entropy: Lutil = H(q) = (cid:88) k=1 qk log(qk). (6) This objective drives the motion tokens towards uniform distribution, ensuring the codebooks representational capacity is fully utilized."
        },
        {
            "title": "3.4 Two-Stage Orthonormal Regularization",
            "content": "To balance geometric consistency with semantic flexibility, we implement two-stage orthonormal regularization scheme, as illustrated in Figure 1. In the first stage, we train the DVQ model to establish unified geometric basis. We optimize composite objective that supplements the standard reconstruction loss Lrec. = ˆx2 2 with our proposed geometric and utilization constraints. To balance motion fidelity with codebook structure, we weight the orthogonality penalties and utilization via coefficients λortho and λutil, given by: LDVQ = Lrec. + λorthoLortho + λutilLutil. (7) Lortho and Lutil are defined in Eq. 2 and Eq. 6. In the second stage, we project the learned motion codes into the LLM for instruction tuning by extending the original token embedding matrix Eorg RN to Enew = [Eorg, E] and Enew R(N +K)D , where denotes the projected motion-token embeddings produced by DVQ. To preserve the semantics of the original embedding space, we freeze the original text embeddings, optimizing the projected motion-token embeddings and LLMs weights. Crucially, we continue to apply soft orthogonal regularization to these learnable tokens. This constraint ensures that while the tokens adapt to the semantic context of the language model, they remain anchored to the orthogonal geometry established in the first stage. More specifically, the LLM instruction tuning loss is defined as follows: Ltuning = E(X,Y ) (cid:88) log pθ(yt y<t, X) t=1 (cid:124) (cid:123)(cid:122) task loss (cid:13) ˆE ˆE IK (cid:13) (cid:13) (cid:13) 2 + λ (cid:13) , (cid:13) (cid:125) (cid:124) (cid:123)(cid:122) orthonormal regularization orth (cid:125) (8) where ˆE RKD is the row-normalized motion embeddings with ˆek = ek/ek2, = [Xprompt, Xmotion] denotes the input sequence formed by prompt tokens and motion tokens from DVQ, denotes the target response sequence, and pθ denotes the LLM."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we will describe our experimental setup and present comprehensive evaluation and ablation results in details."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "All experiments are conducted on single NVIDIA B200 GPU. We evaluate three representative language models with different scales and architectures, including GPT-2 (Radford et al., 2019), Qwen 3-0.6B (Yang et al., 2025), and LLaMA 3.21B (Dubey et al., 2024). For GPT2, we adopt full-parameter fine-tuning due to its relatively small model size. For the larger models, Qwen 3-0.6B and LLaMA 3.2-1B, we employ parameterefficient fine-tuning using Low-Rank Adaptation (LoRA) (Hu et al., 2022) to reduce memory consumption and training cost while maintaining competitive performance. When LoRA is enabled, we use rank of 16 with scaling factor of 32. All models are trained and evaluated on the HumanML3D dataset (Guo et al., 2022a), large-scale benchmark for text-driven human motion understanding and generation. We follow the official preprocessing pipeline provided by the HumanML3D repository, representing each motion frame as (a) Code Usage Curve (b) Code Usage Count Figure 2: Codebook utilization comparison between GeoMotionGPT and conventional VQ-VAE. GeoMotionGPT achieves more effective code usage (less skewed heavy-tailed usage pattern). 263-dimensional feature vector. Each motion instance is paired with three text captions. During training, we use the AdamW optimizer (Loshchilov and Hutter, 2019) with an initial learning rate of 1 104 and apply cosine scheduler. At test time, we strictly follow the evaluation protocol and experimental settings defined in MotionGPT3 (Zhu et al., 2025) to ensure fair and consistent comparison with prior work."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "To assess codebook quality, we compute the usage count of each code over the evaluation set and compare the resulting distributions between GeoMotionGPT and VQ-VAE baseline. We report (i) codebook utilization, defined as the percentage of codes with non-zero usage, and (ii) the standard deviation of usage counts to quantify how concentrated the assignments are. To provide unified view of overall captioning performance, we report an aggregated average score that combines retrieval-style alignment and text generation quality. Formally, the average score is defined as: Avg = 100 + B1 + B4 + RL + + 6 , , R1 + R2 + R3 3 where = which is the average score of R-Precision (Aslam and Yilmaz, 2005) at top-1, top-2, and top-3 (R1, R2, R3) and scaling the result by 100. And B1 and B4 denote BLEU-1 and BLEU-4 (Papineni et al., 2002), RL denotes ROUGE-L (Lin, 2004), denotes CIDEr (Vedantam et al., 2015), and denotes BERTScore (Zhang et al., 2019). This aggregated metric is intended to summarize overall performance trends rather than replace individual metrics, and all component scores are reported separately for transparency. 5 Table 1: Comparison with prior motion understanding methods on HumanML3D under the GPT-2. GeoMotionGPT achieves new state of the art, improving the aggregated average score by 22.4% over the strongest baseline. Approach"
        },
        {
            "title": "Real",
            "content": "R@1 R@2 R@3 MMDist Bleu@1 Bleu@4 Rouge Cider BertScore Average 0.523 0.725 0.828 2.901 - 0.823 TM2T (Guo et al., 2022b) 0.516 0.827 MotionGPT (Jiang et al., 2023) 0.543 0.831 0.547 LaMPM2T (Li et al., 2024) 0.871 0.577 MoTe (Wu et al., 2024b) MotionGPT3 (Zhu et al., 2025) 0.573 0.773 0.864 - - - - 2.835 2.821 2.808 2.649 2.430 GeoMotionGPT (Ours) 0.533 0.729 0.817 2.680 48.90 48.20 47.80 46.70 59.08 65. - 7.00 12.50 13.04 11.15 19.41 25.88 - - 38.10 37.40 37.10 37.40 46. 16.80 29.20 28.90 31.50 28.72 51.32 59.71 - 32.20 32.40 32.70 30.30 35.23 49. - 34.99 38.03 38.07 38.24 43.71 53.48 Table 2: Effect of orthogonal-loss ratio and sparse projected initialization in GeoMotionGPT (GPT-2 setting). Moderate regularization (around 102) yields the best overall performance, while removing orthogonal constraints or applying overly strong regularization degrades the performance of motion understanding. Projection Orthogonal Ratio R@1 R@2 R@3 MMDist Bleu@1 Bleu@4 Rouge Cider BertScore Average 0 1e-4 1e-3 1e-2 1e-1 1 1e-2 0.525 0.721 0.812 0.530 0.727 0.819 0.530 0.727 0.819 0.533 0.729 0.817 0.540 0.742 0.831 0.530 0.727 0.819 0.509 0.699 0.804 2.82 2.64 2.65 2.68 2.67 2.65 2.80 57.30 63.02 63.02 65.65 60.11 63.02 63.66 19.90 24.41 22.45 25.88 21.94 24.41 24.29 47.40 49.14 49.13 51.32 49.04 49.17 47. 47.40 55.47 55.47 59.71 51.87 55.47 50.69 42.90 44.54 44.54 49.03 43.63 44.54 41.49 47.23 50.96 50.96 53.48 49.50 50.97 49."
        },
        {
            "title": "4.3 Codebook Distribution Analysis",
            "content": "We first analyze the distributional characteristics of the learned codebook to understand how the learned discrete representation is consumed by downstream modeling. As shown in Fig. 2a, both methods exhibit heavy-tailed usage pattern where small subset of codes is used frequently, while many codes are used less often. However, compared to the conventional VQ-VAE, GeoMotionGPT displays visibly less skewed trend, with reduced dominance of the most frequently used codes and more stable usage level over broader portion of the codebook. This is further supported by the usage-count histogram in Fig. 2b, where GeoMotionGPT shifts more codes away from extremely low usage and concentrates them in more moderate usage range, indicating fewer underutilized (near-dead) codes. Overall, these results suggest that the proposed DVQ training objective (with utilization and orthogonality regularization) encourages healthier and more balanced token distribution, providing richer discrete inputs for motion-language learning."
        },
        {
            "title": "4.4 Performance on Motion Understanding",
            "content": "Table 1 shows that GeoMotionGPT sets new state of the art on HumanML3D under the GPT-2 setting. Compared to the strongest prior baseline (MotionGPT3), GeoMotionGPT improves the agFigure 3: Comparison between LLM Training with/without Ortho. Loss and without Sparse Projection gregated Average score by 22.4%. These gains are driven primarily by markedly stronger caption quality: GeoMotionGPT boosts CIDEr by 107.9%, BLEU@4 by 33.3%, and BERTScore by 39.2% over MotionGPT3 (Zhu et al., 2025), reflecting more accurate and informative descriptions with better semantic alignment to ground-truth captions (higher consensus/overlap and similarity at both ngram and embedding levels). Meanwhile, retrievalbased recalls remain competitive but trail the best method by small margin, and motiontext distance is slightly higher, leaving room to further improve motiontext consistency. Overall, these 6 Table 3: Performance of GeoMotionGPT under different LLM backbones and training strategies on HumanML3D. Full parameter tuning with GPT-2 yields the best overall results, while LoRA-adapted smaller backbones lead to noticeable degradation, especially in retrieval accuracy and motion-text alignment. Methods Type R@1 R@2 R@3 MMDist Bleu@1 Bleu@4 Rouge Cider BertScore Average Full 0.533 0.729 0.817 GPT2 Qwen3-0.6B LoRA 0.376 0.550 0.656 LLaMA3.2-1B LoRA 0.444 0.644 0.740 2.68 3.980 3.220 65.65 62.06 63.80 25.88 22.45 24.18 51.32 47.65 49. 59.71 45.98 50.60 49.03 42.43 45.45 53.48 45.55 49.05 results confirm that the design choices in GeoMotionGPT yield more informative motion token representations, which translate into tangible gains when fine-tuning LLMs for motion understanding."
        },
        {
            "title": "4.5 Ablation Studies",
            "content": "Effect of Orthogonal Loss Ratio. Table 2 studies how the orthogonal-regularization strength influences GeoMotionGPT under the GPT-2 setting. Adding orthogonal regularization substantially improves performance over the no-orthogonal baseline: the best configuration (102) raises the aggregated Average by 13.1%. While stronger regularization (101) yields the highest retrieval recalls, it comes at the cost of captioning quality and thus lower overall score. Additionally, Figure 3 shows that introducing orthogonal regularization during downstream LLM fine-tuning accelerates optimization, as the total training loss decreases faster and reaches lower values compared to training without the constraint. Overall, the results suggest clear sweet spotmoderate orthogonal regularization provides the best trade-off between structured codebook representations and motion understanding performance. In summary, orthogonal regularization is necessary for effective LLM training, but overly strong constraints can be harmful, highlighting the importance of balanced setting. Impact of Different LLM Backbones. Table 3 compares GeoMotionGPT on different LLM backbones and adaptation strategies. Overall, GPT-2 with full fine-tuning performs best, improving the aggregated Average by 17.4% over Qwen3-0.6B (LoRA) and by 9.0% over LLaMA3.2-1B (LoRA). The advantage is consistent across both captioning and alignment: GPT-2 yields notably higher caption consensus/semantic quality while also achieving stronger retrieval accuracy and better motiontext alignment. These results suggest that, within our framework, fully tuned medium-scale backbone provides more effective capacityadaptation trade-off than parameter-efficient tuning of smaller LLMs. Similar observations have also been reported in prior work (Wu et al., 2025). We hypothesize that this behavior may stem from the limited number and diversity of motion clips in HumanML3D, which can constrain effective model training and generalization. Impact of Sparse Projection-Based Initialization. Table 2 further examines the role of sparse projection-based initialization for motion-related embeddings. Compared to the default setting with sparse projection, replacing it with stochastic initialization (marked with ) causes clear degradation in downstream LLM performance: under the same orthogonal-loss ratio (102), the aggregated Average score drops by 8.3% (from 53.48 to 49.06). This consistent decline suggests that sparse projection provides better-conditioned and more structured starting point for motion embeddings, enabling the LLM to learn more effective motionlanguage mappings, whereas removing it leads to less effective training and weaker captions. Figure 3 shows that, without sparse projection, the training loss remains consistently higher than with sparse projection throughout the entire optimization process."
        },
        {
            "title": "4.6 Case Studies",
            "content": "Case Study on Codebook Usage Patterns. To qualitatively assess codebook usage, Table 4 compares token ID sequences produced by conventional VQ-VAE decoder and our DVQ on the same ground-truth motions. The VQ-VAE baseline often generates long runs of identical token IDs, indicating that small subset of codes dominates and motion dynamics are overly collapsed. In contrast, our DVQ yields more diverse token transitions while remaining temporally coherent, suggesting more balanced and fine-grained codebook utilization. This richer discrete representation provides more informative inputs for downstream motionlanguage modeling, aligning with the gains observed in subsequent LLM fine-tuning. 7 Table 4: Case study of codebook usage patterns on identical ground-truth motions. GeoMotionGPT produces more diverse yet temporally coherent token transitions, indicating more balanced and fine-grained codebook utilization for downstream motion-language modeling."
        },
        {
            "title": "Text\nDescription",
            "content": "VQ-VAE Motion Token IDs"
        },
        {
            "title": "GeoMotionGPT\nMotion Token\nIDs",
            "content": "Person leans forward slightly and moves right hand in wiping motion. 330 330 330 330 330 330 330 330 330 330 330 330 330 330 287 287 287 287 287 330 330 330 330 288 379 19 379 177 177 343 189 225 385 330 343 343 177 343 177 19 352 330 414 385 12 385 385 414 The person drinks from the big jug. 493 243 243 243 243 243 243 243 243 243 248 28 10 10 10 173 173 280 119 128 153 153 153 153 The man takes step and bends raising foot to wipe table. person rests their hands on their knees while squatting. 41 499 276 411 17 17 17 17 17 17 17 17 17 17 59 229 229 65 65 65 65 65 296 296 296 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 70 185 70 70 70 384 70 457 70 70 70 504 223 82 200 495 457 296 51 206 37 254 500 484 337 232 156 345 370 19 370 370 19 370 370 370 370 38 38 358 421 284 219 500 72 55 219 165 235 11 324 59 507 298 24 24 24 11 428 284 405 463 38 284 324 211 139 469 328 9 69 Table 5: Case study on motion understanding. GeoMotionGPT generates more faithful and fine-grained captions that better capture directional cues and specific motion semantics (e.g., leftward jumps, stumbling direction, and hand actions), indicating improved motion-language alignment."
        },
        {
            "title": "Text\nDescription",
            "content": "VQ-VAE Predicted Text"
        },
        {
            "title": "GeoMotionGPT\nPredicted Text",
            "content": "A man takes step forward, takes his left arm and moves it right to left then takes step back."
        },
        {
            "title": "A person walking\nforward and then bending\ndown to pick up\nsomething",
            "content": "A person steps forward, picks something up with their left hand, and then steps back. person jumps to his left. This person stumbles left and right while moving forward. person claps their hands. person jumps and lands. person walks forward and then walks sideways to the left. person juggles two balls with their hands. person jumps sideways to the left. person walks forward and then stumbles to the left. Person is clapping their hands. Case Study on Text Quality Table 5 presents qualitative comparison between the VQ-VAE baseline and GeoMotionGPT on representative motion sequences. Overall, GeoMotionGPT demonstrates noticeably stronger ability to capture fine-grained motion semantics and directional cues. Compared to VQ-VAE, which often produces generic or partially incorrect descriptions (e.g., missing lateral directions or confusing hand actions), GeoMotionGPT more accurately reflects key motion attributes such as leftward jumps, stumbling directions, and specific hand interactions. Notably, these qualitative improvements align well with the substantial quantitative gains reported in Table 1. These examples suggest that GeoMotionGPT benefits from improved token utilization and motionlanguage alignment, leading to more faithful and discriminative motion descriptions."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we presented GeoMotionGPT, novel framework that bridges the gap between discrete motion tokenization and LLM semantic embedding through unified geometric basis. Unlike prior approaches that rely on superficial token-ID linkages, our method explicitly enforces orthogonality across both the motion and the LLM embedding space. By synergizing decoder-only quantizer with GumbelSoftmax, structure-preserving sparse projection, and two-stage orthonormal regularization schedule, we effectively aligned the intrinsic geometry of motion with the semantic reasoning capabilities of LLMs. Extensive experiments on the HumanML3D benchmark demonstrate the efficacy of this approach, yielding 20% performance improvement over state-of-the-art methods."
        },
        {
            "title": "Limitations",
            "content": "Although our approach achieves state-of-the-art performance on motion understanding, there remain several limitations. First, our ablation analysis primarily varies the weight of the orthogonal regularization, which offers an interpretable but relatively coarse characterization of how geometric structure impacts representation learning. more comprehensive study would consider alternative and more fine-grained geometric constraints, such as different forms of decorrelation or whitening, spectral or entropy-based regularizers, or structureaware objectives that explicitly control cluster compactness and inter-code separation. Exploring these directions could help further improve codebook health and provide deeper insights into which geometric properties most directly translate to downstream gains. Second, our evaluation focuses on motion understanding tasks (e.g., captioning and motiontext alignment), and we did not evaluate motion generation. It therefore remains unclear to what extent the proposed tokenization and regularization generalize to motion synthesis settings, such as unconditional generation, text-conditioned generation, or controllable generation with longhorizon temporal coherence. Future work should investigate these scenarios and assess whether the improved codebook utilization and embedding geometry also lead to higher-fidelity, more diverse, and more controllable motion generation."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736. Javed Aslam and Emine Yilmaz. 2005. geometric In Prointerpretation and analysis of r-precision. ceedings of the 14th ACM international conference on Information and knowledge management, pages 664671. Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. 2018. Can we gain more from orthogonality regularizations in training deep networks? Advances in Neural Information Processing Systems, 31. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. 2025. Softvqvae: Efficient 1-dimensional continuous tokenizer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2835828370. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, and 1 others. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, pages 84698488. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yanhong Fei, Yingjie Liu, Xian Wei, and Mingsong Chen. 2022. O-vit: Orthogonal vision transformer. arXiv preprint arXiv:2201.12133. Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. 2024. Momask: Generative In Promasked modeling of 3d human motions. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022a. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161. Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. 2022b. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. 2020. Controllable orthogonalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64296438. Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. 2018. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32. 9 Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations. Kui Jia, Dacheng Tao, Shenghua Gao, and Xiangmin Xu. 2017. Improving training of deep neural networks via singular value bounding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 43444352. Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2023. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079. Diederik Kingma and Max Welling. 2014. Autoencoding variational bayes. stat, 1050:1. Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, and Ling Pei. 2025. Radarllm: Empowering large language models to understand human motion from millimeter-wave point cloud sequence. Preprint, arXiv:2504.09862. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. 2022. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532. José Lezama, Qiang Qiu, Pablo Musé, and Guillermo Sapiro. 2018. Ole: Orthogonal low-rank embeddinga plug and play geometric loss for deep learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 81098118. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, and Laurence Yang. 2024. Lamp: Language-motion pretraining for motion generation, retrieval, and captioning. arXiv preprint arXiv:2410.07093. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. 2024. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Jeongeun Park, Sungjoon Choi, and Sangdoo Yun. 2025. unified framework for motion reasoning and generation in human interaction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1069810707. Mathis Petrovich, Michael Black, and Gül Varol. 2023. Tmr: Text-to-motion retrieval using contrastive 3d In Proceedings of the human motion synthesis. IEEE/CVF International Conference on Computer Vision, pages 94889497. Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. 2020. Deep isometric learning for visual recognition. In International conference on machine learning, pages 78247835. PMLR. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32. Simone Ricci, Niccolò Biondi, Federico Pernici, Ioannis Patras, and Alberto Del Bimbo. 2025. $boldsymbol{lambda}$-orthogonality regularization for compatible representation learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. 2022. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, pages 358374. Springer. Aaron Van Den Oord, Oriol Vinyals, and 1 others. 2017. Neural discrete representation learning. Advances in neural information processing systems, 30. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image deIn Proceedings of the IEEE scription evaluation. conference on computer vision and pattern recognition, pages 45664575. Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. 2024. Motiongpt-2: general-purpose motion-language model for motion generation and understanding. arXiv preprint arXiv:2410.21747. Bizhu Wu, Jinheng Xie, Keming Shen, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, and Linlin Shen. 10 2025. Mg-motionllm: unified framework for motion comprehension and generation across multiple granularities. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27849 27858. Zixiang Zhou, Yu Wan, and Baoyuan Wang. 2024. Avatargpt: All-in-one framework for motion understanding planning generation and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13571366. Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, and Xin Chen. 2025. Motiongpt3: Human motion as second modality. arXiv preprint arXiv:2506.24086. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. 2024. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. Advances in Neural Information Processing Systems, 37:1261212635. Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. 2024a. Motion-agent: conversational framework for human motion generation with llms. arXiv preprint arXiv:2405.17013. Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, and Dong Xu. 2024b. Mote: Learning motion-text diffusion model for multiple generation tasks. arXiv preprint arXiv:2411.19786. Di Xie, Jiang Xiong, and Shiliang Pu. 2017. All you need is beyond good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61766185. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao Yao, and Roy Ka-Wei Lee. 2021. On orthogonality constraints for transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, volume 2, pages 375382. Association for Computational Linguistics. Baoquan Zhang, Huaibin Wang, Chuyao Luo, Xutao Li, Guotao Liang, Yunming Ye, Xiaochen Qi, and Yao He. 2024. Codebook transfer with part-of-speech for vector-quantized image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77577766. Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. 2023. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. 2023. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Implementation Details This section provides implementation details of the proposed decoder-only vector quantization (DVQ) that are omitted from the main text for clarity, but are essential for reproducibility. Our DVQ codebook consists of = 512 codewords, each with dimensionality of 512. We train DVQ for 500 epochs with batch size of 512 using the AdamW optimizer. The initial learning rate is set to 2104, and cosine learning rate scheduler with linear warmup is applied throughout training, where the warmup phase spans the first 3% of the total training steps and the learning rate decays to zero at the end of training. We use weight decay of 1 104 for all non-bias and non-normalization parameters, while setting the weight decay to zero for bias terms, normalization layers, and all quantizerrelated parameters. Parameters associated with the quantizer (including the codebook) are optimized using reduced learning rate of 1104, i.e., 0.5 the base learning rate. For Gumbel-Softmax quantization, we employ an explicit temperature and hardness scheduling strategy to stabilize training. The temperature τ is initialized to 0.4 and kept constant for the first 300 epochs, after which it is exponentially annealed to minimum value of 0.01 over the next 100 epochs and remains fixed thereafter. In parallel, we anneal hardness mixing coefficient (hard_util_rate) that controls the transition from soft to hard code assignments: it is set to 0 for the first 150 epochs, linearly increased to 1 over the subsequent 50 epochs, and fixed to 1 for the rest epochs. This joint scheduling strategy allows DVQ to gradually evolve from smooth, exploration-driven regime to near-discrete quantization regime, while maintaining stable optimization and high codebook utilization. For LLM finetuning, we followed the training setting of MotionGPT3 (Zhu et al., 2025), and the training is conducted for 100 epochs with batch size of 320, with an initial learning rate of 1104 and weight decay of 1102. We adopt cosine annealing learning rate scheduler, where the maximum number of scheduler steps is set to Tmax = 200 and the learning rate is annealed to minimum value of 1 106 at the end of training. The LLM is finetuned on the HumanML3D dataset following the official preprocessing protocol, using motion sequences sampled at 20 FPS with minimum length of 20 frames and maximum length of 200 frames. Temporal Resolution and Token Granularity. DVQ operates on temporally downsampled motion features. The quantizer reduces the input motion sequence length via strided 1D convolutions, resulting in shorter latent sequence where each discrete token corresponds to fixed temporal window in the original motion. This design ensures that motion tokens capture temporally coherent motion patterns rather than frame-level noise, and significantly reduces the effective token sequence length for downstream language modeling. Separation of Training and Inference Quantization Paths. DVQ explicitly distinguishes between training-time and inference-time quantization. During training, stochastic Gumbel-Softmax sampling is used to maintain differentiability and exploration of the codebook. At inference time, tokenization is deterministic and obtained by taking the arg max over encoder logits, yielding stable and reproducible motion token sequence. This separation ensures consistency between motion token extraction and downstream LLM usage. Decoder-Only Design Rationale. DVQ adopts decoder-only quantization structure in which the decoder exclusively consumes code embeddings. There is no continuous latent bypass from the quantizer to the decoder. As result, all reconstruction signals must flow through the discrete bottleneck, forcing the codebook to capture all motion-relevant information. This design contrasts with encoderdecoder VQ-VAE architectures that may partially rely on continuous latent features, and empirically leads to more informative and diverse motion tokens. Gumbel-Softmax. For completeness, we provide the detailed definition of the Gumbel-Softmax operator (Jang et al., 2017) used in DVQ, which is omitted from the main text. The GumbelSoftmax(; ) used in Eq. 4 is defined as: GumbelSoftmax(z; τ ) = softmax (cid:18) + τ (cid:19) , (9) where is sampled from the Gumbel(0, 1) distribution, and τ is the temperature parameter controlling the smoothness of the categorical distribution. As τ 0, the output approaches one-hot vector. 12 With the straight-through gradient estimator (Bengio et al., 2013), the gradient w.r.t ysoft in Eq. 4 is calculated as ysoft = yhard . Temperature Scheduling for Gumbel-Softmax. We adopt temperature scheduling strategy for the Gumbel-Softmax quantizer to balance exploration and discretization during training. At early training stages, relatively high temperature encourages smooth assignment distributions and facilitates gradient propagation across multiple codebook entries. As training progresses, the temperature is gradually annealed to promote sharper, near-discrete assignments that better approximate hard tokenization. Concretely, the temperature τ is initialized to τ0 and decayed following monotonic schedule: τ (t) = max (cid:0)τmin, τ0 γt(cid:1) , (10) where denotes the training step, γ (0, 1) is decay factor, and τmin is lower bound that prevents numerical instability. This scheduling ensures that the quantizer transitions smoothly from soft, exploration-driven regime to more deterministic, token-like regime."
        }
    ],
    "affiliations": [
        "Florida State University",
        "Texas Tech University",
        "William & Mary University"
    ]
}