{
    "paper_title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
    "authors": [
        "Zachary Novack",
        "Ge Zhu",
        "Jonah Casebeer",
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick",
        "Nicholas J. Bryan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 7 6 1 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "Presto! DISTILLING STEPS AND LAYERS FOR ACCELERATING MUSIC GENERATION Zachary Novack UC San Diego Ge Zhu & Jonah Casebeer Adobe Research Julian McAuley & Taylor Berg-Kirkpatrick UC San Diego Nicholas J. Bryan Adobe Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop new score-based distribution matching distillation (DMD) method for the EDM-family of diffus ion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop simple, but powerful improvement to recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) the fastest high-quality TTM to our knowledge."
        },
        {
            "title": "INTRODUCTION",
            "content": "We have seen renaissance of audio-domain generative media (Chen et al., 2024; Agostinelli et al., 2023; Liu et al., 2023; Copet et al., 2023), with increasing capabilities for both Text-to-Audio (TTA) and Text-to-Music (TTM) generation. This work has been driven in-part by audio-domain diffusion models (Song et al., 2020; Ho et al., 2020; Song et al., 2021), enabling considerably better audio modeling than generative adversarial network (GAN) or variational autoencoder (VAE) methods (Dhariwal & Nichol, 2021). Diffusion models, however, suffer from long inference times due to their iterative denoising process, requiring substantial number of function evaluations (NFE) during inference (i.e. sampling) and resulting in 5-20 seconds at best for non-batched latency. Accelerating diffusion inference typically focuses on step distillation, i.e. the process of reducing the number of sampling steps by distilling the diffusion model into few-step generator. Methods include consistency-based (Salimans & Ho, 2022; Song et al., 2023; Kim et al., 2023) and adversarial (Sauer et al., 2023; Yin et al., 2023; 2024; Kang et al., 2024) approaches. Others have also investigated layer-distillation (Ma et al., 2024; Wimbauer et al., 2024; Moon et al., 2024), which draws from transformer early exiting (Hou et al., 2020; Schuster et al., 2021) by dropping interior layers to reduce the cost per sampling step for image generation. For TTA/TTM models, however, distillation techniques have only been applied to shorter or lower-quality audio (Bai et al., 2024; Novack et al., 2024a), necessitate 10 steps (vs. 1-4 step image methods) to match base quality (Saito et al., 2024), and have not successfully used layer or GAN-based distillation methods. We present Presto1, dual-faceted distillation approach to inference acceleration for score-based diffusion transformers via reducing the number of sampling steps and the cost per step. Presto includes three distillation methods: (1) Presto-S, new distribution matching distillation algorithm for score-based, EDM-style diffusion models (see Fig. 1) leveraging GAN-based step distillation Work done while an intern at Adobe. Correspondnce to znovack@ucsd.edu and njb@ieee.org. 1Presto is the common musical term denoting fast music from 168-200 beats per minute."
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Presto-S. Our goal is to distill the initial real score model (grey) µθ into few-step generator (light blue) Gϕ to minimize the KL divergence between the distribution of Gϕs outputs and the real distribution. This requires that we train an auxillary fake score model µψ (dark blue) that estimates the score of the generators distribution at each gradient step. Formally: (1) real audio is corrupted with Gaussian noise sampled from the generator noise distribution pgen(σinf) which is then (2) passed into the generator to get its output. Noise is then added to this generation according to three different noise distributions: (3) pDMD(σtrain), which is (4) passed into both the real and fake score models to calculate the distribution matching gradient ϕLDMD; (5) pDSM(σtrain/inf), which is used to (6) train the fake score model on the generators distribution with Lfake-DSM; and (7) an adversarial distribution pGAN(σtrain), which along with the real audio is (8) passed into leastsquares discriminator built on the fake score models intermediate activations to calculate LGAN. with the flexibility of continuous-time models, (2) Presto-L, conditional layer distillation method designed to better preserve hidden state variance during distillation, and (3) Presto-LS, combined layer-step distillation method that critically uses layer distillation and then step distillation while disentangling layer distillation from real and fake score-based gradient estimation. To evaluate our approach, we ablate the design space for both distillation processes. First, we show our step distillation method achieves best-in-class acceleration and quality via careful choice of loss noise distributions, GAN design, and continuous-valued inputs, the first such method to match base TTM diffusion sampling quality with 4-step inference. Second, we show our layer distillation method offers consistent improvement in both speed and performance over SOTA layer dropping methods and base diffusion sampling. Finally, we show that layer-step distillation accelerates our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) while notably improving diversity over step-only distillation. Overall, our core contributions include the development of holistic approach to accelerating scorebased diffusion transformers including: The development of distribution matching distillation for continuous-time score-based diffusion (i.e. EDM), the first GAN-based distillation method for TTA/TTM. The development of an improved layer distillation method that consistently improves upon both past layer distillation method and our base diffusion model. The development of the first combined layer and step distillation method. Evaluation showing our step, layer, and layer-step distillation methods are all best-inclass and, when combined, can accelerate our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than Stable Audio Open (Evans et al., 2024c)), the fastest TTM model to our knowledge. For sound examples, see https://presto-music.github.io/web/."
        },
        {
            "title": "2.1 MUSIC GENERATION",
            "content": "Audio-domain music generation methods commonly use autoregressive (AR) techniques (Zeghidour et al., 2021; Agostinelli et al., 2023; Copet et al., 2023) or diffusion (Forsgren & Martiros, 2022; Liu et al., 2023; 2024b; Schneider et al., 2023). Diffusion-based TTA/TTM (Forsgren & Martiros, 2022; Liu et al., 2023; 2024b; Schneider et al., 2023; Evans et al., 2024a) has shown the promise of full-text control (Huang et al., 2023), precise musical attribute control (Novack et al., 2024b;a; Tal et al., 2024), structured long-form generation (Evans et al., 2024b), and higher overall quality over AR methods (Evans et al., 2024a;b; Novack et al., 2024b; Evans et al., 2024c). The main downside of diffusion, however, is that it is slow and thus not amenable to interactive-rate control."
        },
        {
            "title": "2.2 SCORE-BASED DIFFUSION MODELS",
            "content": "Continuous-time diffusion models have shown great promise over discrete-time models both for their improved performance on images (Balaji et al., 2022; Karras et al., 2023; Liu et al., 2024a) and audio (Nistal et al., 2024; Zhu et al., 2023; Saito et al., 2024), as well as their relationship to the general class of flow-based models (Sauer et al., 2024; Tal et al., 2024). Such models involve forward noising process that gradually adds Gaussian noise to real audio signals xreal and reverse process that transforms pure Gaussian noise back into data (Song et al., 2021; Sohl-Dickstein et al., 2015). The reverse process is defined by stochastic differential equation (SDE) with an equivalent ordinary differential equation (ODE) form called the probability flow (PF) ODE (Song et al., 2021): dx = σx log p(x σ)dσ, (1) where log p(x σ) is the score function of the marginal density of (i.e. the noisy data) at noise level σ according to the forward diffusion process. Thus, the goal of score-based diffusion models is to learn denoiser network µθ such that µθ(x, σ) = E[xreal x, σ]. The score function is: log p(x σ) µθ(x, σ) σ . (2) Given trained score model, we can generate samples at inference time by setting decreasing noise schedule of levels σmax = σN > σN 1 > > σ0 = σmin and iteratively solving the ODE at these levels using our model and any off-the-shelf ODE solver (e.g. Euler, Heun). The EDM-family (Karras et al., 2022; 2023) of score-based diffusion models is of particular interest and unifies several continuous-time model variants within common framework and improves model parameterization and training process. The EDM score model is trained by minimizing reweighted denoising score matching (DSM) loss (Song et al., 2021): LDSM = ExrealD,σp(σtrain),ϵN (0,I) (cid:2)λ(σ)xreal µθ(xreal + ϵσ, σ) 2 (cid:3) , (3) where p(σtrain) denotes the noise distribution during training, and λ(σ) is noise-level weighting function. Notably, EDM defines different noise distribution to discretize for inference p(σinf) that is distinct from p(σtrain) (see Fig. 2), as opposed to noise schedule shared between training and inference. Additionally, EDMs represent the denoising network using extra noise-dependent preconditioning parameters, training network fθ with the parameterization: µθ(x, σ) = cskip(σ)x + cout(σ)fθ(cin(σ)x, cnoise(σ)). (4) For TTM models, µθ is equipped with various condition embeddings (e.g. text) µθ(x, σ, e). To increase text relevance and quality at the cost of diversity, we employ classifier free guidance (CFG) (Ho & Salimans, 2021), converting the denoised output to: µw θ (x, σ, e) = µθ(x, σ, ) + w(µθ(x, σ, e) µθ(x, σ, )), where is the guidance weight and is null conditioning. 2.3 DIFFUSION DISTILLATION Step distillation is the process of reducing diffusion sampling steps by distilling base model into few-step generator. Such methods can be organized into two broad categories. Online consistency approaches such as consistency models (Song et al., 2023), consistency trajectory models (Kim et al.,"
        },
        {
            "title": "Technical Report",
            "content": "2023), and variants (Ren et al., 2024; Wang et al., 2024a) distill directly by enforcing consistency across the diffusion trajectory and optionally include an adversarial loss (Kim et al., 2023). While such approaches have strong 1-step generation for images, attempts for audio have been less successful and only capable of generating short segment (i.e. < 10 seconds), applied to lower-quality base models limiting upper-bound performance, needing up to 16 sampling steps to match baseline quality (still slow), and/or did not successfully leverage adversarial losses which have been found to increase realism for other domains (Bai et al., 2024; Saito et al., 2024; Novack et al., 2024a). In contrast, offline adversarial distillation methods include Diffusion2GAN (Kang et al., 2024), LADD (Sauer et al., 2024), and DMD (Yin et al., 2023). Such methods work by generating large amounts of offline noisesample pairs from the base model, and finetuning the model into conditional GAN for few-step synthesis. These methods can surpass their adversarial-free counterparts, yet require expensive offline data generation and massive compute infrastructure to be efficient. Alternatively, improved DMD (DMD2) (Yin et al., 2024) introduces an online adversarial diffusion distillation method for images. DMD2 (1) removes the need for expensive offline data generation (2) adds GAN loss and (3) outperforms consistency-based methods and improves overall quality. DMD2 primarily works by distilling oneor few-step generator Gϕ from base diffusion model µreal, while simultaneously learning score model of the generators distribution online µfake in order to approximate target KL objective (with µreal) used to train the generator. To our knowledge, there are no adversarial diffusion distillation methods for TTM or TTA. Beyond step distillation, layer distillation, or the process of dropping interior layers to reduce the cost per sampling step, has been recently studied (Moon et al., 2024; Wimbauer et al., 2024). Layer distillation draws inspiration from transformer early exiting and layer caching (Hou et al., 2020; Schuster et al., 2021) and has found success for image diffusion, but has not been compared or combined with step distillation methods and has not been developed for TTA/TTM. In our work, we seek to understand how step and layer distillation interact for accelerating music generation."
        },
        {
            "title": "3 Presto!",
            "content": "We propose dual-faceted distillation approach for inference acceleration of continuous-time diffusion models. Continuous-time models have been shown to outperform discrete-time DDPM models (Song et al., 2020; Karras et al., 2022; 2024), but past DMD/DMD2 work focuses on the latter. Thus, we redefine DMD2 (a step distillation method) in Section 3.1 for continuous-time score models, then present an improved formulation and study its design space in Section 3.2. Second, we design simple, but powerful improvement to the SOTA layer distillation method to understand the impact of reducing inference cost per step in Section 3.3. Finally, we investigate how to combine step and layer distillation methods together in Section 3.4. 3.1 EDM-STYLE DISTRIBUTION MATCHING DISTILLATION We first redefine DMD2 in the language of continuous-time, score-based diffusion models (i.e. EDM-style). Our goal is to distill our score model µθ (which we equivalently denote as µreal, as it is trained to model the score of real data) into an accelerated generator Gϕ that can sample in 1-4 steps. Formally, we wish to minimize the KL Divergence between the real distribution preal and the generator Gϕs distribution pfake: LDMD = D(prealpfake). The KL term cannot be calculated explicitly, but we can calculate its gradient with respect to the generator if we can access the score of the generators distribution. Thus, we also train fake score model µψ (or equivalently, µfake) to approximate the generator distributions score function at each gradient step during training. First, given some real data xreal, we sample noise level from set of predefined levels σ {σi}gen, and then pass the corrupted real data through the generator to get the generated output ˆxgen = Gϕ(xreal + σϵ, σ), where ϵ (0, I) (we omit the conditioning for brevity). The gradient of the KL divergence between the real and the generators distribution can then be calculated as: ϕLDMD = Eσ{σi},ϵN (0,I) [((µfake( ˆxgen + σϵ, σ) µw real( ˆxgen + σϵ, σ)) ϕ ˆxgen] , (5) where {σi} are the predefined noise levels for all loss calculations, and µw real is the CFG-augmented real score model. To ensure that µfake accurately models the score of the generators distribution"
        },
        {
            "title": "Technical Report",
            "content": "at each gradient update, we train the fake score model with the weighted-DSM loss (i.e. standard diffusion training), but on the generator outputs: Lfake-DSM = Eσ{σi},ϵN (0,I) (cid:2)λ(σ) ˆxgen µfake( ˆxgen + σϵ, σ)2 arg min (6) (cid:3) ψ To avoid using offline data (Yin et al., 2023), the fake score model is updated 5 times as often as the generator to stabilize the estimation of the generators distribution. DMD2 additionally includes an explicit adversarial loss in order to improve quality. Specifically, discriminator head Dψ is attached to the intermediate feature activations of the fake score network µfake, and thus is trained with the nonsaturating GAN loss: arg min ϕ max ψ σ{σi}, ϵN (0,I) [log Dψ(xreal + σϵ, σ)] + σ{σi}, ϵN (0,I) [ log Dψ( ˆxgen + σϵ, σ)], (7) which follows past work on using diffusion model backbones as discriminators (Sauer et al., 2024). In all, the generator Gϕ is thus trained with combination of the distribution matching loss LDMD and the adversarial loss LGAN, while the fake score model (and its discriminator head) is trained with the fake DSM loss Lfake-DSM and the adversarial loss LGAN. To sample from the distilled generator, DMD2 uses consistency model-style ping-pong sampling (Song et al., 2023), where the model iteratively denoises (starting at pure noise σmax) and renoises to progressively smaller noise levels. Regarding past work, we note Yin et al. (2024) did present small-scale EDM-style experiment, but treated EDM models as if they were functions of discrete noise timesteps. This re-discretization runs counterintuitive to using score-based models for distribution matching, since the fake and real score models are meant to be run and trained in continuous-time and can adapt to variable points along the noise process. Furthermore, this disregards the ability of continuous-time models to capture the entire noise process from noise to data and enable exact likelihoods rather than ELBOs (Song et al., 2021). Additionally, since DMD2 implicitly aims to learn an integrator of the PF ODE Gϕ(x, σ) + (cid:82) σmin σ δ log p(x δ)dδ (like other data-prediction distillation methods (Song et al., 2023)), learning this integral for any small set {σi} restricts the generators modeling capacity. 3.2 PRESTO-S: SCORE-BASED DISTRIBUTION MATCHING DISTILLATION Given the new notation, we proceed to develop our score-based distribution matching step distillation, Presto-S. Our algorithm is shown in Fig. 1, in Appendix 1, and discussed below. 3.2.1 CONTINUOUS-TIME GENERATOR INPUTS In Section 3.1, the noise level and/or timestep is sampled from discrete, hand-chosen set {σi}gen. Discretizing inputs, however, forces the model to 1) be function of specific number of steps, requiring users to retrain separate models for each desired step budget (Yin et al., 2024; Kohler et al., 2024) and 2) be function of specific noise levels, which may not be optimally aligned with where different structural, semantic, and perceptual features arise in the diffusion process (Si et al., 2024; Kynkaanniemi et al., 2024; Balaji et al., 2022; Sabour et al., 2024). When extending to continuoustime models, we train the distilled generator Gϕ as function of the continuous noise level sampled from the distribution σ p(σ). This allows our generator to both adapt better to variable budgets and to variable noise levels, as the generator can be trained with all noise levels sampled from p(σ). 3.2.2 PERCEPTUAL LOSS WEIGHTING WITH VARIABLE NOISE DISTRIBUTIONS key difference between discrete-time and continuous-time diffusion models is the need for discretization of the noise process during inference. In discrete models, single noise schedule defines particular mapping between timestep and its noise level σ, and is fixed throughout training and inference. In continuous-time EDM models, however, we use noise distribution p(σtrain) to sample during training, and separate noise distribution for inference p(σinf) that is discretized to define the sampling schedule. In particular, when viewed in terms of the signal-to-noise ratio 1/σ2 or SNR as shown in Fig. 2, the training noise distribution puts the majority of its mass in the mid-to-high SNR range of the diffusion process. This design choice focuses on semantic and perceptual features, while the inference noise distribution is more evenly distributed but with bias towards the low-SNR region, giving bias to low-frequency features."
        },
        {
            "title": "Technical Report",
            "content": "However, recall that every loss term including (5), (6), and (7) requires an additional re-corruption process that must follow noise distribution, significantly expanding the design space for score-based models. Thus, we disentangle these forward diffusion processes and replace the shared discrete noise set with four separate noise distributions pgen, pDMD, pDSM, and pGAN, corresponding to the inputs to the generator and each loss term respectively, with no restrictions on how each weights each noise level (rather than forcing particular noise weighting for all computation). if we apply the original DMD2 method Then, naively to the EDM-style of score-models, we get pgen(σinf) = pDMD(σinf) = pDSM(σinf) = pGAN(σinf). This choice of pgen(σinf) reasonably aligns the generator inputs during distillation to the inference process itself, but each loss noise distribution is somewhat misaligned from its role in the distillation process. In particular: Figure 2: Training/Inference distributions for EDM models, in decibel SNR space. pDMD: The distribution matching gradient is the only point that the generator gets signal from the CFG-augmented outputs of the teacher. CFG is critical for text following, but primarily within the mid-to-high SNR region of the noise (Kynkaanniemi et al., 2024). pGAN: As in most adversarial distillation methods (Sauer et al., 2023; Yin et al., 2023), the adversarial losss main strength is to increase the perceptual realism/quality of the outputs, which arise in the mid-to-high SNR regions, rather than structural elements. pDSM: The score model training should in theory mimic standard diffusion training, and may benefit from the training distributions provably faster convergence (Wang et al., 2024b) (as the fake score model is updated online to track the generators distribution). Thus, we shift all of the above terms to use the training distribution pDMD(σtrain), pDSM(σtrain) and pGAN(σtrain) to force the distillation process to focus on perceptually relevant noise regions. 3.2.3 AUDIO-ALIGNED DISCRIMINATOR DESIGN The original DMD2 uses classic non-saturating GAN loss. The discriminator is series of convolutional blocks downsampling the intermediate features into single probability for real vs. fake. While this approach is standard in image-domain applications, many recent adversarial waveform synthesis works (Kumar et al., 2023; Zhu et al., 2024) use Least-Squares GAN loss: arg min ϕ max ψ σpGAN(σtrain), ϵN (0,I) [Dψ(xreal +σϵ, σ)2 2]+E σpGAN(σtrain), ϵN (0,I) [1Dψ( ˆxgen +σϵ, σ) 2], (8) where the outputs of the discriminator Dψ are only partially downsampled into lower-resolution version of the input data (in this case, latent 1-D tensor). This forces the discriminator to attend to more fine-grained, temporally-aligned features for determining realness, as the loss is averaged across the partially downsampled discriminator outputs. Hence, we use this style of discriminator for Presto-S to both improve and stabilize (Mao et al., 2017) the GAN gradient into our generator. 3.3 PRESTO-L: VARIANCE AND BUDGET-AWARE LAYER DROPPING Given our step distillation approach above, we now seek methods to reduce the cost of individual steps themselves here through layer distillation, and then combine both step and layer distillation in Section 3.4. We begin with the current SOTA method: ASE (Moon et al., 2024). ASE employs fixed dropping schedule that monotonically maps noise levels to compute budgets, allocating more layers to lower noise levels. We enhance this method in three key ways: (1) ensuring consistent variance in layer distilled outputs, (2) implementing explicit budget conditioning, and (3) aligning layer-dropped outputs through direct distillation. Variance Preservation: First, we inspect the within-layer activation variance of our base model in Fig. 4. We find that while the variance predictably increases over depth, it notably spikes on the"
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Baseline layer dropping (left) vs. Presto-L (right). Standard layer dropping uses the noise level σ to set the budget of layers to drop, starting from the back of the DiT blocks. Presto-L shifts this dropping by one to the second-to-last block and adds explicit budget conditioning. last layer up to an order of magnitude higher, indicating that the last layer behaves much differently as it is the direct input to the linear de-embedding layer. ASE, however, always drops layers starting from the last layer and working backwards, thus always removing this behavior. Hence, we remedy this fact and shift the layer dropping schedule by 1 to drop starting at the second to last layer, always rerouting back into the final layer to preserve the final layers behavior. Budget Conditioning: We include explicit budget conditioning into the model itself so that the model can directly adapt computation to the budget level. This conditioning comes in two places: (1) global budget embedding added to the noise level embedding, thus contributing to the internal Adaptive Layer Norm (AdaLN) conditioning inside the DiT blocks, and (2) an additional AdaLN layer on the outset of the DiT blocks conditional only on the Figure 4: Hidden activation variance vs. budget, in order to effectively rescale the outputs to layer depth. Each line is unique noise level. account for the change in variance. Following (Peebles & Xie, 2023; Zhang et al., 2023), we zero-initialize both budget conditioning modules to improve finetuning stability. Knowledge Distillation: To encourage distillation without holding the base model in memory, we employ self-teacher loss. Formally, if hL(x, θ) and hfull(x, θ) are the normalized outputs of the final DiT layer with and without layer dropping respectively, the self-teacher loss is Lst = hL(x, θ) sg(hfull(x, θ))2 2, where sg denotes stop-gradient. This gives additional supervision during the early phases of finetuning so the layer-dropped outputs can match full model performance. We show the differences between our Presto-L and the baseline approach in Fig. 3. By conditioning directly on the budget, and shifting the dropping schedule to account for the final DiT block behavior, we able to more adapt computation for reduced budgets while preserving performance. 3.4 PRESTO-LS: LAYER-STEP DISTILLATION As the act of layer distillation is, in principle, unrelated to the step distillation, there is no reason priori that these methods could not work together. However, we found combining such methods to be surprisingly non-trivial. In particular, we empirically find that attempting both performing Presto-L finetuning and Presto-S at the same time OR performing Presto-L finetuning from an initial Presto-S checkpoint results in large instability and model degradation, as the discriminator dominates the optimization process and achieves near-perfect accuracy on real data. We instead find three key factors in making combined step and layer distillation work: (1) LayerStep Distillation we first perform layer distillation then step distillation, which is more stable as the already-finetuned layer dropping prevents generator collapse; (2) Full Capacity Score Estimation we keep the real and fake score models initialized from the original score model rather than the layerdistilled model, as this stabilizes the distribution matching gradient and provides regularization to the discriminator since the fake score model and the generator are initialized with different weights; and (3) Reduced Dropping Budget we keep more layers during the layer distillation. We discuss more in Section 4.6 and how alternatives fail in Appendix A.4."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We show the efficacy of Presto via number of experiments. We first ablate the design choices afforded by Presto-S, and separately show how Presto-L flatly improves standard diffusion sampling. We then show how Presto-L and Presto-S stack up against SOTA baselines, and how we can combine such approaches for further acceleration, with both quantitative and subjective metrics. We finish by describing number of extensions enabled by our accelerated, continuous-time framework."
        },
        {
            "title": "4.1 SETUP",
            "content": "Model: We use latent diffusion (Rombach et al., 2022) with fully convolutional VAE (Kumar et al., 2023) to generate mono 44.1kHz audio and convert to stereo using MusicHiFi (Zhu et al., 2024). Our latent diffusion model builds upon DiT-XL (Peebles & Xie, 2023) and takes in three conditioning signals: the noise level, text prompts, and beat per minute (BPM) for each song. We use FlashAttention-2 (Dao, 2023) for the DiT and torch.compile for the VAE decoder and MusicHiFi. For more details, see Appendix A.1. Data: We use 3.6K hour dataset of mono 44.1 kHz licensed instrumental music, augmented with pitch-shifting and time-stretching. Data includes musical meta-data and synthetic captions. For evaluation, we use Song Describer (no vocals) (Manco et al., 2023) split into 32 second chunks. Baselines: We compare against number of acceleration algorithms and external models, including Consistency Models (CM) (Song et al., 2023), SoundCTM (Saito et al., 2024), DITTO-CTM (Novack et al., 2024a), DMD-GAN (Yin et al., 2024), ASE (Moon et al., 2024), MusicGen (Copet et al., 2023), and Stable Audio Open (Evans et al., 2024c). See Appendix A.2 for more details. Metrics: We use Frechet Audio Distance (FAD) (Kilgour et al., 2018), Maximum Mean Discrepancy (MMD) (Jayasumana et al., 2024), and Contrastive Language-Audio Pretraining (CLAP) score (Wu et al., 2023), all with the CLAP-LAION music backbone (Wu et al., 2023) given its high correlation with human perception (Gui et al., 2024). FAD and MMD measure audio quality/realness with respect to Song Describer (lower better), and CLAP score measures prompt adherence (higher better). When comparing to other models, we also include density (measuring quality), recall and coverage (measuring diversity) (Naeem et al., 2020), and real-time factor (RTF) for both mono (M) and stereo (S, using MusicHiFi), which measures the total seconds of audio generated divided by the generation time, where higher is better for all. 4.2 EXPLORING THE DESIGN SPACE OF PRESTO-S FAD pgen MMD CLAP pGAN pDSM pDMD Least-Squares GAN Inf. Tr. Tr. Tr. Inf. Tr. Tr. Inf. Inf. Inf. Tr. Tr. Tr. Tr. Inf. Inf. Tr. Inf. Tr. Tr. Tr. Inf. Inf. Inf. Inf. Inf. Inf. Tr. 0.37 0.37 0.37 0.27 0.23 0.22 0. 1.73 1.58 1.51 1.27 0.86 0.83 0.99 Loss Distribution Choice: In Table 1 (Top), we show the FAD, MMD, and CLAP score for many Presto-S distilled models with different noise distribution choices. We find that the original DMD2 (Yin et al., 2024) setup (first row) underperforms compared to The adapting the loss distributions. largest change is in switching pDMD to the training distribution, which improves all metrics. This confirms our hypothesis that by focusing on the region most important for text guidance (Kynkaanniemi et al., 2024), we improve both audio quality and text adherence. Switching pGAN to the training distribution also helps; in this case, the discriminator is made to focus on higher-frequency features (Si et al., 2024), benefiting quality. We also find only small improvement when using the training distribution for pDSM. This suggests that while the training distribution should lead to more stable learning of the online generators score (Wang et al., 2024b), this may not be crucial. For all remaining experiments, we use pDMD(σtrain) = pGAN(σtrain) = pDSM(σtrain) and pgen(σinf). Table 1: (Top) Comparing different choices of noise distribution for the Presto-S process. (Bottom) for best performing noise distributions, performance for standard GAN design vs. proposed least-squares GAN. 27.45 26.45 24.90 33.12 33.29 33.13 30.89 Non-Saturating GAN 31.48 31.78 29. 0.24 0.25 0.26 0.89 0.96 1.04 Inf. Inf. Tr. Inf. Tr. Tr. Tr. Tr. Tr. Tr. Tr. Tr."
        },
        {
            "title": "Technical Report",
            "content": "Model NFE RTF-M/S () FAD () MMD () CLAP Score () Density () Recall () Coverage() External Baselines* MusicGen-Small MusicGen-Medium MusicGen-Large Stable Audio Open 1.6K 1.6K 1.6K 100 0.77 0.39 0.37 4. Base Model, Diffusion Sampling 80 DPM-2S 80 DPM-2S+ASE DPM-2S+Presto-L (ours) 80 Consistency-Based Distillation CM SoundCTM DITTO-CTM Adversarial Distillation DMD-GAN Presto-S (ours) Presto-LS (ours) 4 4 4 4 4 4 7.72 / 7.34 9.80 / 9.22 9.80 / 9. 118.77 / 67.41 105.78 / 63.01 118.77 / 67.41 118.77 / 67.41 118.77 / 67.41 138.84 / 73.43 0.31 0.27 0.25 0.23 0.24 0.25 0.18 0.47 0.35 0.36 0.29 0.22 0. 1.60 1.30 1.21 1.07 0.82 1.12 0.61 2.50 1.72 1.62 1.16 0.83 0.73 30.61 31.85 32.83 35.05 31.56 30.03 32. 26.33 29.61 28.31 27.56 33.13 32.21 0.36 0.43 0.44 0.29 0.31 0.27 0.38 0.17 0.17 0.22 0.57 0.60 0. 0.16 0.19 0.15 0.37 0.20 0.16 0.29 0.01 0.17 0.04 0.07 0.10 0.14 0.43 0.54 0.54 0.49 0.41 0.41 0. 0.16 0.26 0.32 0.41 0.50 0.48 Table 2: Full Results on Song Describer (No vocals).External baseline RTFs are all natively stereo. Discriminator Design: We ablate the effect of switching from the chosen least-squares discriminator to the original softplus non-saturating discriminator, which notable treats the discriminator as binary classifier and predicts the probability of real/generated. In Table 1 (Bottom), we find that using the least-squares discriminator leads to consistent improvements in audio quality (FAD/MMD) and in particular text relevance (CLAP), owing to the increased stability from the least-squares GAN. Figure 5: Continuous generator inputs vs. discrete inputs. Continuous inputs shows more consistent scaling with compute, while generally performing better in both quality and text relevance. Continuous vs. Discrete Generator Inputs: We test how continuous-time conditioning compares against discrete and find the former is preferred as shown in Fig. 5. Continuous noise levels maintain correlation where more steps improve quality, while discrete time models are more inconsistent. Additionally, the continuous-time conditioning performs best in text relevance. While the 1 and 2-step discrete models show slightly better FAD metrics than continuous on 1 and 2-step sampling, these models have failure mode as shown in Fig. 11: 2-step discrete models drop high-frequency information and render transients (i.e. drum hits) poorly for genres like R&B or hip-hop. 4.3 PRESTO-L RESULTS We compare Presto-L with both our baseline diffusion model and ASE (Moon et al., 2024) using the 2nd order DPM++ sampler (Lu et al., 2022) with CFG++ (Chung et al., 2024). For ASE and Presto-L, we use the optimal D3 configuration from Moon et al. (2024), which corresponds to dropping schedule, in terms of decreasing noise level (in quintiles), of [14, 12, 8, 4, 0] (i.e. we drop 14 layers for noise levels in the top quintile, 12 for the next highest quintile, and so on). Layer distillation results at various sampling budgets are shown in Fig. 6. Presto-L yields an improvement over the base model on all metrics, speeding up by 27% and improving quality and text relevance. ASE provides similar acceleration but degrades performance at high sampling steps and scales inconsistently. The behavior of dropping layers improving performance can be viewed via the lens of multi-task learning, where (1) denoising each noise level is different task (2) later layers only activating for lower noise levels enables specialization for higher frequencies. See Appendix A.7 for further ablations."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Presto-L results. Presto-L improves both the latency and the overall performance across all metrics, against both the leading layer dropping baseline and the base model."
        },
        {
            "title": "4.4 FULL COMPARISON",
            "content": "In Table 2, we compare against multiple baselines and external models. For step distillation, PrestoS is best-in-class and the only distillation method to close to base model quality, while achieving an over 15x speedup in RTF from the base model. Additionally, Presto-LS improves performance for MMD, beating the base model with further speedups (230/435ms latency for 32 second mono/stereo 44.1kHz on an A100 40 GB). We also find Presto-LS improves diversity with higher recall. Overall, Presto-LS is 15x faster than SAO. We investigate latency more in Appendix A.6. 4.5 LISTENING TEST We also conducted subjective listening test to compare Presto-LS with our base model, the best non-adversarial distillation technique SoundCTM (Saito et al., 2024) distilled from our base model, and Stable Audio Open (Evans et al., 2024c). Users (n = 16) were given 20 sets of examples generated from each model (randomly cut to 10s for brevity) using random prompts from Song Describer and asked to rate the musical quality, taking into account both fidelity and semantic text match between 0-100. We run multiple paired t-tests with Bonferroni correction and find Presto-LS rates highest against all baselines (p < 0.05). We show additional plots in Fig. 12. 4.6 PRESTO-LS QUALITATIVE ANALYSIS While Presto-LS improves speed and quality/diversity over step-only distillation, the increases are modest, as the dropping schedule for Presto-L was reduced ([12, 8, 8, 0, 0]) for step distillation stability. To investigate more, we analyze the hidden state activation variance of our step-distilled model in Fig. 7. The behavior is quite different than the base model, as the spike in the final layer is more amortized across the last 10 layers and never reaches the base models magnitude. We hypothesize step-distilled models have more unique computation throughout each DiT block, making layer dropping difficult. 4.7 EXTENSIONS Figure 7: Presto-S hidden activation var. Adaptive Step Schedule: benefit of our continuous-time distillation is that besides setting how many steps (e.g., 1-4), we can set where those steps occur along the diffusion process by tuning the ρ parameter in the EDM inference schedule, which is normally set to ρ = 7. In particular, decreasing ρ (lower bounded by 1) puts more weight on low-SNR features and increasing ρ on higher-SNR features (Karras et al., 2022). Qualitatively, we find that this process enables increased diversity of outputs, even from the same latent code (see Appendix A.5). CPU Runtime: We benchmark Presto-LSs speed performance for CPU inference. On an Intel Xeon Platinum 8275CL CPU, we achieve mono RTF of 0.74, generating 32 seconds of audio in 43.34 seconds. We hope to explore further CPU acceleration in future work."
        },
        {
            "title": "Technical Report",
            "content": "Fast Inference-Time Rejection Sampling: Given Presto-LSs speed, we investigated using inference-time compute to improve performance. Formally, we test the idea of rejection sampling, inspired by Kim et al. (2023), where we generate batch of samples and reject fraction of them according to some ranking function. We use the CLAP score to discard samples that have poor text relevance. Over number of rejection ratios (see Fig. 13), we find that CLAP rejection sampling strongly improves text relevance while maintaining or improving quality at the cost of diversity."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We proposed Presto, dual-faceted approach to accelerating latent diffusion transformers by reducing sampling steps and cost per step via distillation. Our core contributions include the development of score-based distribution matching distillation (the first GAN-based distillation for TTM), new layer distillation method, the first combined layer-step distillation, and evaluation showing each method are independently best-in-class and, when combined, can accelerate our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model), resulting in the fastest TTM model to our knowledge. We hope our work will motivate continued work on (1) fusing step and layer distillation and (2) new distillation of methods for continuous-time score models across media modalities such as image and video."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We would like to thank Juan-Pablo Caceres, Hanieh Deilamsalehy, and Chinmay Talegaonkar."
        },
        {
            "title": "REFERENCES",
            "content": "Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating music from text. arXiv:2301.11325, 2023. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, C. K. Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Shunting Zhang, Michael Suo, Phil Tillet, Xu Zhao, Eikan Wang, Keren Zhou, Richard Zou, Xiaodong Wang, Ajit Mathews, William Wen, Gregory Chanan, Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2024. Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, and Somayeh Sojoudi. Accelerating diffusion-based text-to-audio generation with consistency distillation. In Interspeech, 2024. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. eDiff-I: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv:2211.01324, 2022. Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. MusicLDM: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. In IEEE International Conference on Audio, Speech and Signal Processing (ICASSP), 2024. Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. CFG++: Manifold-constrained classifier free guidance for diffusion models. arXiv:2406.08070, 2024. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. In Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "Technical Report",
            "content": "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv:2307.08691, 2023. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Neural Information Processing Systems (NeurIPS), 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv:2407.21783, 2024. Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv:2210.13438, 2022. Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. International Conference on Machine Learning (ICML), 2024a. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Long-form music generation with latent diffusion. arXiv:2404.10301, 2024b. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv:2407.14358, 2024c. Seth Forsgren and Hayk Martiros. Riffusion: Stable diffusion for real-time music generation, 2022. URL https://riffusion.com/about. Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou. Adapting Frechet In IEEE International Conference on Audio, Audio Distance for generative music evaluation. Speech and Signal Processing (ICASSP), 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Gen. Models and Downstream Applications, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Neural Information Processing Systems (NeurIPS), 2020. Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. Neural Information Processing Systems (NeurIPS), 2020. Qingqing Huang, Daniel Park, Tao Wang, Timo Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2Music: Text-conditioned music generation with diffusion models. arXiv:2302.03917, 2023. Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. arXiv:2405.05967, 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. arXiv:2312.02696, 2023. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv:1812.08466, 2018."
        },
        {
            "title": "Technical Report",
            "content": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In International Conference on Learning Representations (ICLR), 2023. Jonas Kohler, Albert Pumarola, Edgar Schonfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter VaImagine Flash: Accelerating Emu Diffusion Models with Backward jda, and Ali K. Thabet. Distillation. arXiv:2405.05224, 2024. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. Highfidelity audio compression with improved RVQGAN. In Neural Information Processing Systems (NeurIPS), 2023. Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv:2404.07724, 2024. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models, 2024a. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In International Conference on Machine Learning (ICML), 2023. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024b. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv:2211.01095, 2022. Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv:2406.01733, 2024. Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bodganov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, et al. The song describer dataset: corpus of audio captions for music-and-language evaluation. arXiv:2311.10057, 2023. Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. Taehong Moon, Moonseok Choi, Eunggu Yun, Jongmin Yoon, Gayoung Lee, Jaewoong Cho, and Juho Lee. simple early exiting framework for accelerated sampling in diffusion models. In International Conference on Machine Learning (ICML), 2024. Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable In International Conference on Machine fidelity and diversity metrics for generative models. Learning. PMLR, 2020. Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, and Stefan Lattner. Diff-a-riff: Musical accompaniment co-creation via latent diffusion models. arXiv:2406.08384, 2024. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO-2: Distilled diffusion inference-time t-optimization for music generation. In International Society for Music Information Retrieval (ISMIR), 2024a. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO: Diffusion inference-time T-optimization for music generation. In International Conference on Machine Learning (ICML), 2024b."
        },
        {
            "title": "Technical Report",
            "content": "William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Visio (ICCV), 2023. Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-SD: Trajectory segmented consistency model for efficient image synthesis. arXiv:2404.13686, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your steps: Optimizing sampling schedules in diffusion models, 2024. Koichi Saito, Dongjun Kim, Takashi Shibuya, Chieh-Hsin Lai, Zhi-Wei Zhong, Yuhta Takida, and Yuki Mitsufuji. Soundctm: Uniting score-based and consistency models for text-to-sound generation. arXiv:2405.18503, 2024. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv:2202.00512, 2022. Axel Sauer, Dominik Lorenz, A. Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv:2311.17042, 2023. Axel Sauer, Frederic Boesel, Tim Dockhorn, A. Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv:2403.12015, 2024. Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. Moˆ usai: Text-to-music generation with long-context latent diffusion. arXiv:2301.11757, 2023. Tal Schuster, Adam Fisch, Tommi Jaakkola, and Regina Barzilay. Consistent accelerated inference via confident adaptive transformers. arXiv:2104.08803, 2021. Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. FreeU: Free lunch in diffusion U-Net. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2021. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning (ICML), 2023. Or Tal, Alon Ziv, Itai Gat, Felix Kreuk, and Yossi Adi. Joint audio and symbolic conditioning for temporally controlled text-to-music generation. arXiv:2406.10970, 2024. Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li, and Xiaogang Wang. Phased consistency model. arXiv:2405.18407, 2024a. Yuqing Wang, Ye He, and Molei Tao. Evaluating the design space of diffusion-based generative models. arXiv:2406.12839, 2024b. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024."
        },
        {
            "title": "Technical Report",
            "content": "Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-tocaption augmentation. In IEEE International Conference on Audio, Speech and Signal Processing (ICASSP), 2023. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv:2311.18828, 2023. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, Improved distribution matching distillation for fast image synthesis. and William Freeman. arXiv:2405.14867, 2024. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundIEEE/ACM Transactions on Audio, Speech, and Stream: An end-to-end neural audio codec. Language Processing (TASLP), 2021. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Ge Zhu, Yutong Wen, Marc-Andre Carbonneau, and Zhiyao Duan. Edmsound: Spectrogram based diffusion models for efficient and high-quality audio synthesis. arXiv:2311.08667, 2023. Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, and Nicholas J. Bryan. MusicHiFi: Fast high-fidelity stereo vocoding. IEEE Signal Processing Letters (SPL), 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MODEL DESIGN DETAILS As we perform latent diffusion, we first train variational autoencoder. We build on the Improved RVQGAN (Kumar et al., 2023) architecture and training scheme by using KL-bottleneck with dimension of 32 and an effective hop of 960 samples, resulting in an approximately 45 Hz VAE. We train to convergence using the recommended mel-reconstruction loss and the least-squares GAN formulation with L1 feature matching on multi-period and multi-band discriminators. Our proposed base score model backbone builds upon DiT-XL (Peebles & Xie, 2023), with modifications aimed at optimizing computational efficiency. Specifically, we use streamlined transformer block design, consisting of single attention layer followed by single feed-forward layer, similar to Llama (Dubey et al., 2024). Our model utilizes three types of conditions including noise levels (timesteps) for score estimation, beat per minute (BPM) values of the song, and text descriptions. Following EDM, we apply logarithmic transformation to the noise levels, followed by sinusoidal embeddings. Similarly, BPM values are input as scalars then go through sinusoidal embeddings to generate BPM embeddings. These processed noise-level embeddings and BPM embeddings are then combined and integrated into the DiT block through an adaptive layer normalization block. For text conditioning, we compute text embedding tokens with T5-based encoders and concatenate with audio tokens at each attention layer. As result, the audio token query attends to concatenated sequence of audio and text keys, enabling the model to jointly extract relevant information from both modalities. To provide baseline architectural speedups, we use FlashAttention-2 (Dao, 2023) for the DiT and Pytorch 2.0s built in graph compilation (Ansel et al., 2024) for the VAE decoder and MusicHifi mono-to-stereo. For the diffusion model hyparameter design, we follow Karras et al. (2024). Specifically, we set σdata = 0.5, Pmean = 0.4, Pstd = 1.0, σmax = 80, σmin = 0.002. We train the base model with 10% condition dropout to enable CFG. The base model was trained for 5 days across 32 A100 GPUs with batch size of 14 and learning rate of 1e-4 with Adam. For all score model experiments, we use CFG++ (Chung et al., 2024) with = 0.8. For Presto-S, following Yin et al. (2024) we use fixed guidance scale of = 4.5 throughout distillation for the teacher model as CFG++ is not applicable for the distribution matching gradient. Additionally, we use learning rate of 5e-7 with Adam. For all step distillation methods, we distill each model with batch size of 80 across 16 Nvidia A100 GPUs for 32K iterations. We train all layer distillation methods for 60K iterations with batch size of 12 across 16 A100 GPUs. A.2 BASELINE DESCRIPTION We benchmark against multiple diffusion acceleration algorithms and open-source models: Consistency Models (CM) (Song et al., 2023; Bai et al., 2024): This distillation technique learns mapping from anywhere on the diffusion process to the data distribution (i.e. xt x0) by enforcing the self-consistency property that Gϕ(xt, t) = Gϕ(xt, t) t, t. We follow the parameterization used in past audio works (Bai et al., 2024; Novack et al., 2024a) that additionally distills the CFG parameter into the model directly. SoundCTM (Saito et al., 2024): This approach distills model into consistency trajectory model (Kim et al., 2023) that enforces the self-consistency property for the entire diffusion process, learning an anywhere-to-anwhere mapping. SoundCTM forgoes the original CTM adversarial loss and calculates the consistency loss using intermediate features of the base model. DITTO-CTM (Novack et al., 2024a), This audio approach is also based off of (Kim et al., 2023), yet brings the consistency loss back into the raw outputs and instead replaces CTMs multi-step teacher distillation with single-step teacher (like CMs) and removes the learned target timestep embedding, thus more efficient (though less complete) than SoundCTM. DMD-GAN (Yin et al., 2024): This approach removes the distribution matching loss from DMD2, making it fully GAN-based finetuning method, which is in line with past adversarial distillation methods (Sauer et al., 2023))."
        },
        {
            "title": "Technical Report",
            "content": "ASE (Moon et al., 2024), This funetuning approach for diffusion models, as discussed in Sec. 3.3, finetunes the base model with the standard DSM loss, but for each noise level drops fixed number of layers, starting at the back of the diffusion models DiT blocks. MusicGen (Copet et al., 2023): MusicGen is non-diffusion based music generation model that uses an autoregressive model to predict discrete audio tokens (Defossez et al., 2022) at each timestep in sequence, and comes in small, medium, and large variants (all stereo). Stable Audio Open (Evans et al., 2024c): Stable Audio Open is SOTA open-source audio diffusion model, which can generate variable lengths up to 45s in duration. Stable Audio Open follows similar design to our base model, yet uses cross-attention for conditioning rather than AdaLN which we use, which increases runtime. A.3 PRESTO-S ALGORITHM Our complete Presto-S algorithm is outlined below. For visual representation, please see Fig. 1. Algorithm 1 Presto-S input : generator Gϕ, real score model µreal, fake score model µψ, discriminator Dψ , CFG weight w, pgen(σinf), pDMD(σtrain), pDSM(σtrain), pGAN(σtrain), real sample xreal, GAN weights ν1, ν2, optimizers g1, g2, weighting function λ σ pDMD(σtrain) ϵdmd (0, I) σ pGAN(σtrain) ϵfake (0, I) LGAN = 1 Dψ( ˆxgen + σϵfake, σ)2 2 ϕ ϕ g1(ϕLDMD + ν1ϕLGAN) 1: σ pgen(σinf) 2: ϵgen (0, I) 3: ˆxgen = Gϕ(xreal + σϵgen, σ) 4: if generator turn then 5: 6: 7: ϕLDMD = ((µψ( ˆxgen + σϵdmd, σ) µw 8: 9: 10: 11: 12: else 13: 14: 15: 16: 17: 18: 19: ψ ψ g2(ψLfake-DSM + ν2ψLGAN) 20: end if output : ϕ, ψ σ pDSM(σtrain) ϵdsm (0, I) Lfake-DSM = λ(σ) ˆxgen µψ( ˆxgen + σϵdsm, σ)2 2 σreal, σfake pGAN(σtrain) ϵreal, ϵfake (0, I) LGAN = Dψ( ˆxgen + σfakeϵfake, σfake)2 real( ˆxgen + σϵdmd, σ)) ϕ ˆxgen 2 + 1 Dψ(xreal + σrealϵreal, σreal)2 2 A.4 ANALYZING FAILURE MODES OF COMBINED LAYER AND STEP DISTILLATION We empirically discovered number of failure modes when trying to combine step and layer distillation. As noted in Section 4.6, the heavier per-layer requirements of distilled few-step generation made all standard dropping schedules (Moon et al., 2024) intractable and prone to quick generator collapse, necessitating more conservative dropping schedule. In Fig. 8, we show the generator loss, discriminator loss, distribution matching gradient, and the discriminators accuracy for the real inputs over distillation, for number of different setups: Presto-S, pure step distillation mechanism (blue). Presto-LS, optimal combined setup where we pretrain the model with Presto-L and then perform Presto-S, but with keeping the real and fake score models initialized from the original score model (orange). LS with L-Fake/Real, which mimics Presto-LS but uses the Presto-L model for the fake and real score models as well (green)."
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Step distillation losses for early distillation for multiple combination methods. Presto-LS is the only setup that avoids generator degradation and high variance distribution matching gradients. Step then Layer, where we first perform Presto-S distillation and then continue distillation with Presto-L layer dropping on the generator (red). Step and Layer jointly, where we perform Presto-S and Presto-L at the same time initialized from the original score model (purple), We see that the runs which do not initialize with pretrained Presto-L (Step then Layer, Step and Layer) show clear signs of generator degradation, with increased generator loss, decreased discriminator loss, and notably near perfect accuracy on real samples, as attempting to learn to drop layers from scratch during step distillation gives strong signal to the discriminator. Additionally, LS with L-Fake/Real inherits similar collapse issues but has higher variance distribution matching gradient as the layer-distilled real and fake score models are poor estimators of the gradient. A.5 INFERENCE-TIME NOISE SCHEDULE SENSITIVITY ANALYSIS Given our final Presto-LS distilled 4-step generator, we show how changing the inference-time noise schedule can noticeably alter the outputs, motivating our idea of continuous-time conditioning. The EDM inference schedule follows the form of: σi<N = (cid:18) σ1/ρ max + 1 (cid:19)ρ (σ1/ρ min σ1/ρ max ) , (9) where increasing the ρ parameter puts more weight on the low-noise, high-SNR regions of the diffusion process. In Fig. 9, we show number of samples generated from Presto-LS with identical conditions and latent codes (i.e. starting noise and all other added gaussian noise during sampling), only changing ρ, from the standard of 7 to 1000 (high weight in low-noise region). We expect further inference-time tuning of the noise schedule to be beneficial. A.6 RTF ANALYSIS We define the RTF for model θ as: RTFb(θ) = latencyθ (b) , where Tθ is the generation duration or how much contiguous audio the model can generate at once and latencyθ(b) is the time it takes bTθ"
        },
        {
            "title": "Technical Report",
            "content": "Figure 9: Generations from Presto-LS from the same text prompt and latent code (i.e. starting noise and added noise during sampling), only varying the ρ parameter between (7 and 1000). Purely shifting the noise schedule for 4-step sampling allows for perceptually distinct outputs. for generating batch of samples, following (Evans et al., 2024b; Zhu et al., 2024). This is different from the fixed-duration batched RTF used in Nistal et al. (2024). We test = 1 as well as the maximum batch size we could attain for each model on single A100 40GB to get sense of maximum throughput. We show results in Table 3 and Table 4 for all components of our generative process, including latency metrics for generation (i.e. the diffusion model or distilled generator), decoding (i.e. VAE decoder from latents to audio) and the optional mono-to-stereo (M2S), as well as overall RTF/latency for mono and stereo inference. We omit the MusicGen models and the other step-distillation methods which share the same RTF as Presto-S. For the fastest model Presto-LS, the biggest latency bottleneck is the mono-to-stereo model (Zhu et al., 2024) and VAE decoder. In future work, we hope to optimize the VAE and mono-to-stereo modules for faster inference. A.7 PRESTO-L DESIGN ABLATION To investigate how each facet of our Presto-L method contributes to its strong performance vs. ASE, we ran an additional ablation combining ASE with each component (i.e. the shifted dropping"
        },
        {
            "title": "Technical Report",
            "content": "Model Stable Audio Open Base DM ASE Presto-L SoundCTM Presto-S Presto-LS Generation Decoding Mono RTF Latency Latency 6159.01 4079.81 3200.73 3201.19 238.06 204.98 166. 887.99 64.45 64.45 64.45 64.45 64.45 64.45 N/A 7.72 9.80 9.80 105.78 118.77 138.84 Mono Latency N/A 4144.27 3265.19 3265.64 302.51 269.43 230.49 M2S Latency Stereo RTF 0 205.31 205.31 205.31 205.31 205.31 205.31 4.54 7.36 9.22 9.22 63.01 67.41 73.43 Stereo Latency 7047 4349.58 3470.50 3470.95 507.83 474.74 435.8 Table 3: Latency (ms) and real-time factor for batch size of one on an A100 40GB GPU. Model Generation Decoding Mono RTF Latency Latency Stable Audio Open Base ASE Presto-L SoundCTM Presto-S Presto-LS 34602.86 18935.26 14584.85 14655.02 1135.65 715.41 695.19 4227.54 1198.21 1198.21 1198.21 1198.21 1198.21 1198. N/A 14.3 18.25 18.17 123.4 150.5 152.11 Mono Latency N/A 20133.46 15783.05 15853.23 2333.86 1913.62 1893.4 M2S Latency 0 1775.73 1775.73 1775.73 1775.73 1775.73 1775.73 Stereo RTF Stereo Latency 7.42 96.38 96.25 96.25 92.98 92.18 92.13 38830.4 21909.19 17558.78 17628.96 4109.58 3689.34 3669.13 Table 4: Latency (ms) and real-time factor for max batch size on an A100 40GB GPU. Figure 10: Presto-L ablation. Each individual change of our layer distillation vs ASE is beneficial. schedule, explicit budget conditioning, and the self-teacher loss). In Fig. 10, we see that the core of Presto-Ls improvements come from the shifted dropping schedule (which preserves final layer behavior), as the ASE+shift performs similarly to Presto-L on high-step FAD and MMD. Additionally, we find that the budget conditioning and self-teacher loss help text relevance more so than the shifted schedule does. All together, the combination of Presto-Ls design decisions leads to SOTA audio quality (FAD/MMD/Density) and text relevance compared to any one facet combined with ASE. A.8 DISCRETE-TIME FAILURE MODES In Fig. 11, we visualize the poor performance of distilled models that use 1-2 step discrete-time conditioning signals. Notice that for the same random seed, the high-frequency performance is visually worse for discrete-time vs. continuous-time conditioning, motivating our proposed methods."
        },
        {
            "title": "Technical Report",
            "content": "Figure 11: Failure mode of 1-2 step discrete models vs. continuous models (each row is same random seed and text prompt), with 2-step generation. Hip-Hop adjacent generations noticeably drop high frequency information, and render percussive transients (hi-hats, snare drums) poorly. A.9 LISTENING TEST RESULTS We visualize our listening test results from Section 4.5 using violin plot. Figure 12: Violin plot from our listening test. Presto-LS is preferred over other baselines (p < 0.05). A.10 REJECTION SAMPLING RESULTS We show rejection sampling results where we generate batch during inference and then use CLAP to reject the least similar generations to the input text prompt. CLAP rejection sampling noticeably improves CLAP Score and maintains (and sometimes improves) FAD and MMD, but reduces diversity as expected. Figure 13: Rejection sampling eval metrics vs. rejection sampling improves both CLAP score and overall quality, while reducing diversity. rejection ratio. Base Presto-LS in red. CLAP"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "UC San Diego"
    ]
}