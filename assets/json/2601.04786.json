{
    "paper_title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "authors": [
        "Lang Feng",
        "Fuchao Yang",
        "Feng Chen",
        "Xin Cheng",
        "Haiyang Xu",
        "Zhenglin Wan",
        "Ming Yan",
        "Bo An"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 6 8 7 4 0 . 1 0 6 2 : r AgentOCR: Reimagining Agent History via Optical Self-Compression Lang Feng1*, Fuchao Yang1*, Feng Chen1, Xin Cheng1, Haiyang Xu2, Zhenglin Wan1, Ming Yan2, Bo An1 1Nanyang Technological University, Singapore 2Tongyi Lab, Alibaba Group Abstract: Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95% of text-based agent performance while substantially reducing token consumption (>50%), yielding consistent token and memory efficiency. Our further analysis validates 20 rendering speedup from segment optical caching and the effective strategic balancing of self-compression. Date: January 8, 2026 Correspondence: Bo An (boan@ntu.edu.sg) 1. Introduction Recent advancements in large language models (LLMs) (Achiam et al., 2023, Guo et al., 2025, Yang et al., 2025, Comanici et al., 2025) have enabled agentic systems (Xu et al., 2025b, Chang et al., 2025, Zhang et al., 2025c, Xie et al., 2025) that tightly couple perception, deliberation, and actuation, thereby reducing the need for continuous human supervision (Yao et al., 2023, Zhang et al., 2025b). These successes increasingly motivate framing agent improvement as reinforcement learning (RL) over long-horizon trajectories-optimizing tool use, planning, and control policies end-to-end from interaction feedback (Feng et al., 2025b, Wang et al., 2025b). However, RL training for LLM agents remain difficult. major challenge arises from the burden of long-context processing (Lu et al., 2025, Kang et al., 2025). As agents interact with environments through multi-turn decision loops, they must buffer comprehensive trajectory of past observations and action sequences. As shown in Fig. 1(a), this historical data accumulates relentlessly, causing the input context to swell to massive volume of tokens within single trajectory. Such rapid expansion not only exhausts the finite token budget of current LLMs but also incurs prohibitive inference latency and compute cost due to expensive attention prefill and KV-cache management (Shah et al., 2024, Jiang et al., 2024). Recent breakthroughs in vision language models (VLMs) (Bai et al., 2025b,a, Chen et al., 2024b) and optical character recognition (OCR) (Wei et al., 2025, Cui et al., 2025a, Xing et al., 2025) suggest promising solution: visual information density. Notably, DeepSeek-OCR (Wei et al., 2025) shows that the visual modality can serve as far more compact carrier of information than text. By rendering textual content into images, the token footprint can be compressed by approximately 10 compared to raw text tokens, substantially reducing the number of *Equal Contribution AgentOCR: Reimagining Agent History via Optical Self-Compression Figure 1: Comparison of text agent and AgentOCR. (a) Text agent accumulates heavy token burden from raw text history. (b) Our AgentOCR requires significantly fewer visual tokens via optical self-compression. tokens processed during model inference. Building on this, we propose AgentOCR, visually-grounded method that reimagines agent history not as string of text, but as dynamic sequence of images (Fig. 1(b)). Specifically, AgentOCR represents the accumulated observation-action history as compact image and conditions the agents policy on this visual history for the multi-turn decision making. To ensure scalability and efficiency in long rollouts, AgentOCR introduces an segment optical caching. This mechanism decomposes the history context into segments and maintains hash-based optical cache, allowing the agent to reuse previously rendered content and avoid redundant processing as the history expands. Beyond static image, AgentOCR features agentic self-compression, which empowers the agent to actively modulate its own visual fidelity at each step. By learning via RL, AgentOCR adaptively selects the most appropriate compression factor to save token costs, thereby achieving favorable balance between task success and efficiency. We evaluate AgentOCR on two challenging agentic benchmarks: ALFWorld (Shridhar et al., 2021), which features long-horizon decision-making, and search-based QA (Jin et al., 2025), characterized by highly text-dense interactions. Our results demonstrate that AgentOCR preserves over 95% of the task performance of strong text-based agent pipelines, while reducing token consumption by over 50% (up to 80% in peak tokens), leading to substantially lower overhead. Furthermore, our analysis validates that segment optical caching accelerates rendering by over 20, and self-compression mechanism effectively optimizes the trade-off between information density and cost. 2. Related Work 2.1. Reinforcement Learning for LLM Agents RL (Sutton and Barto, 2018) has become commonly adopted paradigm for aligning agents with human preferences (Stiennon et al., 2020, Ouyang et al., 2022, Rafailov et al., 2023) and improving their behaviors (Sheng 2 AgentOCR: Reimagining Agent History via Optical Self-Compression et al., 2024, Wang et al., 2025a) in complex scenarios, such as PPO (Schulman et al., 2017), GRPO (Shao et al., 2024), Dr. GRPO (Liu et al., 2025b), Clip-Cov (Cui et al., 2025b), GSPO (Zheng et al., 2025), DAPO (Yu et al., 2025b), and GiGPO (Feng et al., 2025b). These RL-trained agents have been widely deployed in dynamic and open-ended environments, spanning interactive games (Narasimhan et al., 2015, Brockman et al., 2016), GUI control (Rawles et al., 2023, Ye et al., 2025, Liu et al., 2025a), embodied tasks (Shridhar et al., 2021), as well as web (Zhou and Zanette, 2024, Putta et al., 2024, Feng et al., 2025a) and tool-enhanced environments (Qian et al., 2025, Sun et al., 2025, Dong et al., 2025, Xue et al., 2025). 2.2. Optical Character Recognition OCR converts textual information in images into computer-readable text and has been widely applied in document digitization (Doermann, 1998, Smith, 2007, Arlazarov et al., 2022), image text extraction (Jaderberg et al., 2016, Baek et al., 2019, Long et al., 2018), and document parsing (Katti et al., 2018, Xu et al., 2020, Rausch et al., 2021). With the advent of deep learning, OCR systems have shifted toward end-to-end frameworks, including Nougat (Blecher et al., 2023), Donut (Kim et al., 2022), TrOCR (Li et al., 2023), Pix2Struct (Lee et al., 2023) and GOT-OCR2.0 (Wei et al., 2024), enabling unified image-to-text modeling for complex scenarios. Recently, OCR has begun to be explored as vision-text compression mechanism, such as DeepSeek-OCR (Wei et al., 2025), VIST (Xing et al., 2025), and Glyph (Cheng et al., 2025). These approaches offer novel solution for processing extremely long contexts, however, research in this direction remains in its early stages. 2.3. Agent Memory Long-horizon interaction with LLM-based agents requires persistent memory, but naively appending full histories quickly exceeds fixed context windows (Zhang et al., 2025d,b, Hu et al., 2025b). Accordingly, long-context language modeling focuses on efficiency and length generalization, including sparse or hierarchical attention (Beltagy et al., 2020, Fu et al., 2024, Xiao et al., 2024), recency-biased positional schemes (Ding et al., 2024, Su et al., 2024, Xiong et al., 2024), and prompt compression (Ge et al., 2023, Yoon et al., 2024, Zhang et al., 2024). In parallel, retrieval-based methods treat external stores as non-parametric memory and fetch relevant information on demand (Ge et al., 2023, Chen et al., 2024a). Beyond context optimization, recent agent frameworks introduce explicit memory modules to support long-term behavior, ranging from virtualized context management and scalable backends (Packer et al., 2023, Chhikara et al., 2025) to structured or hierarchical representations for extended and multi-agent tasks (Xu et al., 2025a, Anokhin et al., 2024, Hu et al., 2025a, Zhang et al., 2025a). More recent work explores learning-based memory control, where agents adaptively write, retain, and retrieve information, treating memory as an active component (Yan et al., 2025, Zhou et al., 2025, Yu et al., 2025a). 3. Preliminaries 3.1. Problem Setup We formulate the interaction between an LLM agent and an environment (e.g., physical simulator or an external tool API) as sequential decision-making process over finite horizon ğ‘‡ N. The agent is instantiated as an LLM parameterized by ğœƒ, and its behavior is modeled as stochastic policy ğœ‹ğœƒ. At each step ğ‘¡ {1, . . . , ğ‘‡ }, the agent receives an observation oğ‘¡ ğ’ª (e.g., API outputs) and has the interaction history up to step ğ‘¡ as The agent then samples textual action ağ‘¡ ğœ‹ğœƒ( â„, hğ‘¡), where â„ denotes the task instruction, ağ‘¡ ğ’± ğ‘› is token sequence drawn from the vocabulary ğ’± with maximum length ğ‘› N. The action ağ‘¡ is flexible and may hğ‘¡ = (o1, a1, o2, a2, . . . , oğ‘¡). (1) 3 AgentOCR: Reimagining Agent History via Optical Self-Compression explicitly include intermediate reasoning (e.g., chain-of-thought (Wei et al., 2022)) or tool invocations. After executing ağ‘¡, the environment returns scalar reward ğ‘Ÿğ‘¡ and the next observation oğ‘¡+1. Notably, hğ‘¡ often becomes extensively long due to extended horizons or verbose environmental observations, posing significant challenges for context processing. 3.2. Agentic Reinforcement Learning RL has become standard post-training paradigm for enhancing LLM agents. While our work is algorithm-agnostic and compatible with various agentic RL algorithms, we consider Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as the representative algorithm due to its simplicity and efficiency. Specifically, GRPO samples group of trajectories {ğœğ‘–}ğº izing rewards within the group. To ensure training stability, we optimize the clipped surrogate objective: for each input and estimates advantages Ë†ğ´ğ‘– by normalğ‘–=1 ğ½ (ğœƒ) = [ 1 ğºğ‘‡ ğº ğ‘‡ ğ‘–=1 ğ‘¡=1 min ( ğœŒğ‘¡,ğ‘– Ë†ğ´ğ‘–, clip(ğœŒğ‘¡,ğ‘–, 1 ğœ–) Ë†ğ´ğ‘– )] , (2) where ğœŒğ‘¡,ğ‘– = ğœ‹ğœƒ(ağ‘¡,ğ‘–â„,hğ‘¡,ğ‘–) is the importance sampling ratio, ğœ– is the clipping hyperparameter. Here, we omit the ğœ‹ğœƒold (ağ‘¡,ğ‘–â„,hğ‘¡,ğ‘–) KL-divergence regularization for notational brevity. Crucially, optimizing Eq. (2) necessitates computing gradients over the entire cumulative history hğ‘¡. In realistic agent scenarios, hğ‘¡ rapidly accumulates thousands of tokens (e.g., >10k tokens in multi-turn search tasks (Jin et al., 2025)). Since the computational complexity and memory footprint scale with token count, processing such lengthy textual trajectories becomes prohibitively expensive. 4. AgentOCR Agentic tasks require the policy to condition on an ever-growing multi-turn interaction history. This growth creates severe bottleneck in practice. Not only does it reach the models context window limits, but it also drives up computational costs as transformer computation scales superlinearly with sequence length. Hence, efficient token compression is imperative for deploying LLM agents. In this section, we introduce AgentOCR that addresses this bottleneck by reimagining interaction history as an optical memory. Instead of processing raw textual logs, we render the accumulated history into compact visual representation. By leveraging the superior information density of visual tokens compared to text, this approach substantially reduces the token footprint while maintaining full access to historical details. To ensure scalability and dynamic adaptability in long-horizon rollouts, AgentOCR incorporates two innovations: (1) segment optical caching (Fig. 2(a)), which eliminates redundant rendering overhead by systematically reusing cached visual segments, and (2) agentic self-compression (Fig. 2(b)), mechanism that empowers the agent to actively modulate the compression rate, thereby optimizing the trade-off between information density and token cost. In the remainder of this section, we detail optical encoding in Sec. 4.1 for visual representation, followed by segment optical caching in Sec. 4.2 and self-compression and RL training in Sec. 4.3. 4.1. Optical Memory Encoding Memory buffer. AgentOCR maintains an external memory buffer â„³ğ‘¡ that stores the interaction records up to step ğ‘¡. Each record contains an observation-action pair (oğ‘¡, ağ‘¡) (or task-specific equivalents such as tool queries and results). The memory module serializes the entire interaction history into textual form hğ‘¡ = Fetch(â„³ğ‘¡1). AgentOCR: Reimagining Agent History via Optical Self-Compression Figure 2: Overview of AgentOCR. (a) Segment optical caching decomposes the history context into segments, reuses cached renderings via content keys, and assembles the optical memory by stacking segment images. (b) The agent receives the optical observation and history, selects an environment action, and compression rate. (c) The agent is trained with RL, jointly optimizing task performance and token efficiency. Memory rendering. We define deterministic renderer â„› that maps the textual interaction history to an RGB image Iğ‘¡ = â„›(hğ‘¡; ğœ“), where ğœ“ denotes rendering hyperparameters (e.g., font family and size, colors, padding, and bounds on image width and height). At step ğ‘¡, AgentOCR constructs multimodal policy input by combining â„ and the rendered history image Iğ‘¡, and samples an action from vision-language policy: ağ‘¡ ğœ‹ğœƒ( â„, Iğ‘¡). The sampled action ağ‘¡ is then applied to the environment to obtain the next observation oğ‘¡+1, and the memory buffer is updated accordingly. 4.2. Segment Optical Caching Rendering the entire history hğ‘¡ from scratch at every step is wasteful and becomes major latency bottleneck in multi-turn rollouts. naive alternative is to render only the newly appended context and append it to the previously rendered history image. This yields near-constant per-step rendering overhead, but it cannot reuse recurring content, and its memory still grows with the accumulated rendered pixels. AgentOCR instead performs caching at the granularity of segments. The core idea is to decompose the full history context into independent segments and memory rendered segments in dictionary keyed by segment content. At each step, we assemble the history image by stacking cached segment images in order, rendering only segments that have not been seen before. This cache naturally accelerates both recurring boilerplate and repeated tool outputs, and can also reuse newly arrived observations/actions whenever they match previously observed segments. Segment representation. We split the history context into segments. Let Split(h) = (â„“1, . . . , â„“ğ¾ ) denote this operation, where each â„“ğ‘– is text segment. We use deterministic segment renderer â„›(â„“; ğœ“) that maps single segment to an RGB image under the same rendering hyperparameters. 5 AgentOCR: Reimagining Agent History via Optical Self-Compression Segment cache. For each environment instance ğ‘’, AgentOCR maintains per-episode cache ğ’(ğ‘’) = {(ğ‘˜(â„“), I(â„“))}, (3) where ğ‘˜(â„“) is fast content key (e.g., hash of the normalized segment text, optionally including style metadata) and I(â„“) is the rendered image of segment â„“. Unlike naive cache, which incrementally renders the newly appended context, this cache stores each unique segment at most once and reuses it whenever the same segment reappears. In our implementation, ğ’(ğ‘’) persists within an episode and is reset at episode boundaries. Cache lookup and assembly. At step ğ‘¡, we obtain the full history hğ‘¡ and split it into segments Split(hğ‘¡) = (â„“ğ‘¡,1, . . . , â„“ğ‘¡,ğ¾ğ‘¡). For each segment â„“ğ‘¡,ğ‘–, we first query the cache. On miss, we render and insert it: I(â„“ğ‘¡,ğ‘–) = {ğ’(ğ‘’)[ğ‘˜(â„“ğ‘¡,ğ‘–)], â„›(â„“ğ‘¡,ğ‘–; ğœ“), if ğ‘˜(â„“ğ‘¡,ğ‘–) ğ’(ğ‘’), otherwise. if miss: ğ’(ğ‘’)[ğ‘˜(â„“ğ‘¡,ğ‘–)] I(â„“ğ‘¡,ğ‘–). The full optical memory image is then constructed by vertically stacking segment images in order: Iğ‘¡ = Stack ( I(â„“ğ‘¡,ğ‘–) )ğ¾ğ‘¡ . ğ‘–=1 (4) (5) (6) Because newly appended observations or actions are processed as just additional segments, they can be reused without re-rendering whenever they match cached content (e.g., repeated queries and tool responses). Complexity. Let ğ‘ˆğ‘¡ be the number of cache-miss segments among {â„“ğ‘¡,ğ‘–}ğ¾ğ‘¡ . The per-step rendering cost becomes ğ‘–=1 ğ‘‚(ğ‘ˆğ‘¡) segment renders, while cache hits require only dictionary lookup and image stacking. In many agent workloads, interaction logs contain substantial repetition, so typically ğ‘ˆğ‘¡ ğ¾ğ‘¡ and rendering overhead is significantly reduced. For spatial complexity, the cache stores one image per unique segment per episode, yielding ğ‘‚(ğ’(ğ‘’)) images rather than ğ‘‚(ğ‘‡ ) full-history images, thereby avoiding heavy duplication across environment timesteps. 4.3. Agentic Self-Compression Instead of treating the optical renderer â„› (defined in Sec. 4.1) as static background Compression decision. process, AgentOCR exposes it as an executable tool. We conceptualize this interaction as parameterized invocation alongside environment actions, the policy generates structured call via <compression>ğ‘ğ‘¡</compression>, where the compression factor ğ‘ğ‘¡ 1 dynamically modulates the rendering fidelity. This design aligns with standard tool-use paradigms, allowing the agent to explicitly query its interaction history with variable precision. Upon receiving the call, the system executes the renderer with the specified compression factor. Formally, this produces scaled image Iğ‘¡+1: size(Iğ‘¡+1) = ( ğ»ğ‘¡+1 ğ‘ğ‘¡ ğ‘Šğ‘¡+1 ğ‘ğ‘¡ , ) . (7) This spatial downsampling operation effectively reduces the number of visual tokens. Consequently, the agent can strategically modulate the compression rate based on specific task characteristics, thereby optimizing the trade-off between token cost and information density. AgentOCR: Reimagining Agent History via Optical Self-Compression Compression-aware reward. To incentivize the agent to identify suitable compression without compromising task success, we introduce compression-aware reward term for RL training. This reward is strictly conditioned on episode success, ensuring that the agent treats compression as secondary cost-optimization objective rather than primary goal. Then, we employ logarithmic reward formulation that reflects the diminishing returns of information density. Formally, let Isucc(ğœ ) {0, 1} be the success indicator. The compression reward at step ğ‘¡ is defined as: comp ğ‘Ÿ ğ‘¡ = {ln(ğ‘ğ‘¡), 0, if Isucc(ğœ ) = 1, otherwise. (8) comp ğ‘Ÿğ‘¡ = ğ‘Ÿğ‘¡ + ğœ†ğ‘Ÿ ğ‘¡ The total reward used for RL optimization is given by , where ğœ† 0 is the weight parameter of compression reward, which controls the trade-off between task performance and compression efficiency. This scalar reward is used by the agentic RL optimizer (e.g., GRPO) to update the policy ğœ‹ğœƒ through the objective in comp Eq. (2). However, applying the compression reward ğ‘Ÿ at every training iteration can induce overly greedy ğ‘¡ behavior, where the agent aggressively increases compression to maximize immediate reward. To mitigate this effect, we adopt an intermittent reinforcement schedule, injecting the compression reward only at intervals of ğ¾ training iterations. This schedule introduces the efficiency signal periodically while maintaining the primary optimization pressure on task completion. Under this design, AgentOCR learns to allocate vision tokens adaptively, achieving further token reduction while maintaining strong task performance. 5. Experiment In this section, we present comprehensive empirical evaluations of AgentOCR across two representative multi-turn agent benchmarks. Specifically, our experiments aim to investigate the following key aspects: (1) the comparative performance and token efficiency of AgentOCR relative to text-based agents; (2) the quantitative analysis of the vision-text compression ratio of AgentOCR; (3) the computational efficiency of the segment optical caching mechanism; (4) the ablation study on the effectiveness of the self-compression mechanism. 5.1. Experimental Setup Benchmarks. We evaluate AgentOCR on ALFWorld (Shridhar et al., 2021) and search-based QA (Jin et al., 2025). Both benchmarks exhibit sustained context growth but with different interaction profiles. ALFWorld contains the embodied tasks, requiring the agent to manipulate objects within simulated household environment. Search-based QA, conversely, focuses on multi-turn tool use and information retrieval. The agent must actively interact with search engine to query external knowledge, requiring it to deal with denser, web-style textual traces. Baselines. We compare AgentOCR against both text-based and optical-history variants across prompting and RL regimes. Specifically, we evaluate: (1) Text (w/o RL), which feeds raw textual history to text-only model; (2) OCR (w/o RL), which conditions the model on rendered optical memory images without RL; and (3) Text + GRPO, strong baseline that applies RL directly to the raw textual context. In contrast, our AgentOCR applies GRPO to the optical-memory agent, enabling efficient optimization over compact visual histories. Training details. We use the Qwen2.5-VL (Bai et al., 2025b) family as the backbone models. Text-only variants use Qwen2.5-3B/7B-Instruct, while optical-history variants use Qwen2.5-VL-3B/7B-Instruct. We keep all training settings and hyperparameters identical across methods to ensure controlled comparisons. For extra hyperparameters of AgentOCR, we set ğœ† = 0.01 and ğ¾ = 5. More details are provided in Appendix B. 7 AgentOCR: Reimagining Agent History via Optical Self-Compression Method Pick & Place Look Clean ALFWorld Heat Cool Pick2 & Place Avg. Avg. Max. Tokens/Step Text (w/o RL) OCR (w/o RL) Text + GRPO AgentOCR Text (w/o RL) OCR (w/o RL) Text + GRPO AgentOCR 34.7 42.8 92.6 91. 67.6 61.0 92.6 95.6 18.4 21.8 85.7 81.8 35.4 33.2 93.8 96.2 Qwen2.5-(VL)-3B-Instruct 7.3 6.2 86.6 73.3 14.5 6.2 79.3 76. Qwen2.5-(VL)-7B-Instruct 31.3 11.6 80.0 73.2 30.1 12.5 82.7 72.4 12.7 10.1 70.6 76.0 19.3 17.2 85.2 78.1 10.4 9.9 65.0 70. 4.4 16.5 56.5 72.0 1.09k 0.49k 1.02k 16.3 16.2 79.9 78.2 0.38k(61.7%) 3.04k 1.63k 3.13k 1.14k(63.6%) 1.08k 0.47k 0.95k 31.3 25.3 81.8 81.2 0.43k(54.7%) 3.36k 1.36k 2.81k 1.22k(56.6%) Table 1: Performance on ALFWorld tasks. We report the success rate (%) and the average and peak memory context token cost per step. Method NQ TriviaQA PopQA HotpotQA 2Wiki MuSiQue Bamboogle Avg. Avg. Max. Single-Hop Multi-Hop Tokens/Step 9.4 Text (w/o RL) OCR (w/o RL) 10.2 Text + GRPO 39.3 38.6 AgentOCR Text (w/o RL) 10.4 OCR (w/o RL) 6.9 Text + GRPO 45.1 43. AgentOCR 31.3 27.7 60.6 56.5 32.4 30.4 63.7 61.0 19.8 10.9 41.1 41.7 22.3 12.0 44.0 45.4 Qwen2.5-(VL)-3B-Instruct 15.0 9.1 37.4 33.6 14.8 12.2 34.6 30.7 4.7 3.7 15.4 14.6 Qwen2.5-(VL)-7B-Instruct 15.8 10.5 43.6 40.8 15.4 9.1 43.2 38. 7.2 5.5 16.8 15.7 16.8 15.2 26.4 24.0 19.2 24.0 37.6 36.8 0.48k 0.15k 0.61k 15.9 12.7 36.4 34.2 0.26k(57.4%) 2.50k(73.8%) 7.34k 1.33k 9.55k 0.70k 0.26k 0.73k 17.5 14.0 41.9 40.1 0.36k(50.7%) 2.65k(80.9%) 10.96k 2.21k 13.84k Table 2: Performance on search-based QA tasks. We report the exact matching score (%) and the average and peak memory context token cost per step. and denote in-domain and out-of-domain respectively. 5.2. Main Results We first evaluate the overall performance of all methods, with results reported in Tab. 1 and Tab. 2. direct comparison of the inference-only baselines (Text vs. OCR) reveals the inherent efficiency advantage of the visual modality. Across both benchmarks, optical history drastically reduces token consumption, cutting average usage by approximately 55% on ALFWorld and 70% on search tasks. However, this compression initially comes at cost. The off-the-shelf VLM struggles to ground the condensed visual history effectively, resulting in substantial performance drop relative to their text-based counterparts. AgentOCR effectively bridges this gap through RL training, aligning the policy to the visual modality to attain task performance comparable to text-based baselines across model scales. On ALFWorld, AgenOCR with 3B and 7B models achieves 78.2% and 81.2% respectively, virtually matching the text agents (within 1% margin). This trend holds for search tasks, where AgentOCR retains over 95% of the performance of the Text+GRPO baselines (e.g., achieving 40.1% vs. 41.9% on the 7B model). Crucially, these results underscore that AgentOCR offers highly favorable trade-off between token cost and task success. Rather than merely matching the baseline, our method fundamentally alleviates the inference bottleneck by slashing token consumption by >50% (up to 80.9% in peak contexts), proving that high-density visual representations can support rigorous agentic reasoning with 8 AgentOCR: Reimagining Agent History via Optical Self-Compression significantly reduced overhead. 5.3. Vision-Text Compression Analysis In this part, we investigate the trade-off between token compression efficiency and task performance using the trained AgentOCR (7B) across varying fixed compression factors (ğ‘ğ‘¡ [1.0, 2.0]). As illustrated in Fig. 3, increasing the compression factor yields substantial gains in token efficiency but incurs performance penalty. Notably, we identify robust compression zone up to 55% token savings (at ğ‘ğ‘¡ = 1.2), where the model successfully maintains over 95% of the text-based performance (99.5% for ALFWorld and 95.0% for Search). However, surpassing this efficiency threshold triggers an accelerated performance decay. For instance, as savings increase further to 67% (ğ‘ğ‘¡ = 2.0), the average performance drops significantly, highlighting the non-linear tension between information density and reasoning accuracy. Figure 3: Vision-text compression efficiency. The bars (left axis) denote the success rate relative to the text-based agent baseline, while the lines (right axis) indicate the percentage of tokens saved. We further observe distinct divergence in robustness beyond this threshold. ALFWorld demonstrates high resilience, retaining 87.2% of performance even at ğ‘ğ‘¡ = 2.0, likely due to its reliance on coarse-grained scene understanding. In contrast, the text-dense search task is highly sensitive, with performance plummeting to 66.8% as aggressive downscaling blurs critical textual cues. These findings underscore the necessity of our agentic self-compression mechanism. While static image offers safe baseline, dynamic policy is required to exploit higher compression rates in robust steps while reverting to high fidelity for sensitive reasoning, thereby breaking the static trade-off ceiling. 5.4. Analysis of Cache To evaluate the scalability of segment optical caching, we compare it against two alternatives: no cache, which re-renders the full history hğ‘¡ at every step, and naive cache, which incrementally renders the newly appended context at step ğ‘¡ and appends it to growing optical-memory image. Render Time Cache Mem Method Avg (ms) Grow (ms/step) Speedup Peak (MB) Grow (MB/step) Mem Save No Cache Naive Cache Ours 3509.39 203.08 168.77 115.43 0.03 -1.23 1.00 17.28 20.79 151.41 110.80 2.79 1.91 0.00% 26.82% Table 3: Cache mechanism ablation. Growth/step is the slope from least-squares linear fit over steps 150. Speedup is relative to no cache. Mem Saving is relative to the peak cache memory of naive cache. Tab. 3 shows that no cache suffers from large latency and strong growth over time (3509.39 ms on average and 115.43 ms/step), reflecting the redundant cost of repeatedly rendering an increasingly long history. Naive cache removes this time growth and achieves 17.28 speedup, since each step renders only the appended suffix, AgentOCR: Reimagining Agent History via Optical Self-Compression yielding an effective per-step cost of ğ‘‚(1) in typical rollouts. However, because it treats each newly arrived line as distinct and permanently appends the rendered result, its cache memory still grows with rollout length (151.41 MB peak and 2.79 MB/step), i.e., space scales with the accumulated rendered pixels. Segment optical caching further reduces per-step rendering work. By memoizing newline-level segments in content-keyed dictionary, newly appended context can often be satisfied by cache hits and thus requires no rendering. Therefore, the number of cache-miss segments may shrink as the cache warms up, producing negative time growth (1.23 ms/step) and the best average latency (168.77 ms). Meanwhile, segment-level reuse also reduces redundant storage (110.80 MB), corresponding to 26.82% peak-memory saving relative to naive cache and aligning with space scaling dominated by the number of unique segments rather than the number of steps. 5.5. Analysis of Self-Compression At last, we conduct ablation studies to analyze the effectiveness of the agentic self-compression mechanism using the Qwen2.5-VL-3B-Instruct on the ALFWorld. The results are summarized in Tab. 4. Configuration SR (%) Avg. ğ‘ğ‘¡ Avg. Vis. Tok. w/o Self-Compression Self-Compression Without RL 12.1 11.8 1.00 1.05 441.2 436.9 RL Training 78.4 45.3 78. w/o Self-Compression Self-Compression (ğ¾=1) Self-Compression (ğ¾=5) The results demonstrate that RL is essential for effectively leveraging the self-compression mechanism. In the absence of RL, AgentOCR lacks the prior knowledge required to modulate the compression factor, resulting in negligible change in token usage and slight performance decline compared to the fixed baseline. When RL is applied with dense reward schedule (ğ¾ = 1), the agent prioritizes the immediate compression reward by aggressively increasing the compression factor to 4.91, which degrades visual fidelity and causes the success rate to plummet to 45.3%. However, by adopting the intermittent reinforcement schedule (ğ¾ = 5), AgentOCR successfully balances the trade-off between information density and token cost. This configuration learns favorable compression rate, reducing average visual token consumption from 458.1 to 381.7 while maintaining success rate of 78.2%, comparable to the 78.5% achieved by the uncompressed visual baseline. Table 4: Ablation study on self-compression. SR, Avg. ğ‘ğ‘¡, and Avg. Vis. Tok. denote the success rate, average compression factor, and average vision tokens, respectively. 458.1 193.2 381.7 1.00 4.91 1.28 6. Conclusion In this work, we present AgentOCR as an exploration into the potential of visual tokens as compact history medium for multi-turn LLM agents. By integrating segment optical caching to mitigate rendering overhead and agentic self-compression to adaptively balance cost and fidelity, our method demonstrates that the visual modality can effectively complement textual history. Empirical results on ALFWorld and search-based QA suggest that this optical approach allows agents to retain the majority of their decision-making capabilities while significantly reducing token consumption, offering resource-efficient alternative to text-only processing. We envision future research expanding on this foundation to explore hybrid storage architectures and unified multimodal interfaces, moving closer to the versatile and efficient information processing found in biological systems. 10 AgentOCR: Reimagining Agent History via Optical Self-Compression 7. Limitations While AgentOCR demonstrates promising results in agentic tasks, several limitations warrant discussion and suggest directions for future work: First, AgentOCR relies on off-the-shelf VLMs (Qwen2.5-VL series) that were not specifically designed for OCR intensive tasks. Although the proposed AgentOCR framework is model-agnostic in principle, we do not evaluate its behavior across broader range of VLM architectures, like DeepSeek-OCR (Wei et al., 2025), with different visual tokenization strategies or patch resolutions. Performance and compression efficiency may vary depending on the inductive biases of the underlying vision encoder. Second, AgentOCR relies on deterministic text-to-image renderer with fixed hyperparameters such as font size, line spacing, color schemes, and image resolution. While we observe stable performance under our default configuration, we do not systematically explore the sensitivity of the agent to different rendering choices. Suboptimal rendering settings may reduce text legibility or distort layout cues, potentially affecting downstream reasoning. Last, the current design assumes agent history consists primarily of text (observations, actions, tool outputs) that can be rendered as text-as-image. However, many realistic agent scenarios involve inherently visual elements: GUI screenshots with complex layouts, scientific plots, diagrams, and structured tables. Investigating compression strategies for such multimodal histories could significantly expand AgentOCRs application scope beyond textcentric domains."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, and Evgeny Burnaev. AriGraph: Learning knowledge graph world models with episodic memory for LLM agents. arXiv preprint arXiv:2407.04363, 2024. Vladimir Arlazarov, Elena Andreeva, Konstantin Bulatov, Dmitry Nikolaev, O.O. Petrova, B.I. Savelev, and Oleg Slavin. Document image analysis and recognition: survey. Computer Optics, 2022. Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-VL technical report. arXiv preprint arXiv:2511.21631, 2025a. 11 AgentOCR: Reimagining Agent History via Optical Self-Compression Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025b. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin-Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Mahashweta Das, and Na Zou. MAIN-RAG: Multi-agent filtering retrievalaugmented generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2025. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-Embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, et al. Glyph: Scaling context windows via visual-text compression. arXiv preprint arXiv:2510.17800, 2025. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. PaddleOCR 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025b. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. LongRoPE: Extending LLM context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. David Doermann. The indexing and retrieval of document images: survey. Computer Vision and Image Understanding, 1998. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. 12 AgentOCR: Reimagining Agent History via Optical Self-Compression Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, and Bo An. Towards efficient online tuning of VLM agents via counterfactual soft reinforcement learning. In Proceedings of the International Conference on Machine Learning, 2025a. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for LLM agent training. In Proceedings of the Advances in Neural Information Processing Systems, 2025b. Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, et al. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. arXiv preprint arXiv:2307.06945, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. HiAgent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3277932798, 2025a. Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, et al. Memory in the age of AI agents. arXiv preprint arXiv:2512.13564, 2025b. Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Reading text in the wild with convolutional neural networks. International Journal of Computer Vision, 2016. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In Proceedings of the Advances in Neural Information Processing Systems, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, and Saravan Rajmohan. ACON: Optimizing context compression for long-horizon LLM agents. arXiv preprint arXiv:2510.00615, 2025. Anoop Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes HÃ¶hne, and Jean Baptiste Faddoul. Chargrid: Towards understanding 2D documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. OCR-free document understanding transformer. In Proceedings of the European Conference on Computer Vision, 2022. 13 AgentOCR: Reimagining Agent History via Optical Self-Compression Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019. Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2Struct: Screenshot parsing as pretraining for visual language understanding. In Proceedings of the International Conference on Machine Learning, 2023. Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. TrOCR: Transformer-based optical character recognition with pre-trained models. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, et al. PC-Agent: hierarchical multi-agent collaboration framework for complex task automation on pc. arXiv preprint arXiv:2502.14282, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding R1-Zero-Like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu, and Cong Yao. TextSnake: flexible representation for detecting text of arbitrary shapes. In Proceedings of the European Conference on Computer Vision, 2018. Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, and Jiecao Chen. Scaling LLM multi-turn RL with end-to-end summarization-based context management. arXiv preprint arXiv:2510.06727, 2025. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022. Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using In Proceedings of the Conference on Empirical Methods in Natural Language deep reinforcement learning. Processing, 2015. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of the Advances in Neural Information Processing Systems, 2022. Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. MemGPT: Towards LLMs as operating systems. 2023. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP, 2023. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent Q: Advanced reasoning and learning for autonomous AI agents. arXiv preprint arXiv:2408.07199, 2024. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-TÃ¼r, Gokhan Tur, and Heng Ji. ToolRL: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Proceedings of the Advances in Neural Information Processing Systems, 2023. 14 AgentOCR: Reimagining Agent History via Optical Self-Compression Johannes Rausch, Octavio Martinez, Fabian Bissig, Ce Zhang, and Stefan Feuerriegel. DocParser: Hierarchical document structure parsing from renderings. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control. In Proceedings of the Advances in Neural Information Processing Systems, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. FlashAttention-3: Fast and accurate attention with asynchrony and low-precision. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, pages 6865868685, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. arXiv preprint arXiv: 2409.19256, 2024. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning text and embodied environments for interactive learning. In Proceedings of the International Conference on Learning Representations, 2021. R. Smith. An overview of the tesseract ocr engine. In Proceedings of the International Conference on Document Analysis and Recognition, 2007. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Proceedings of the Advances in Neural Information Processing Systems, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. ZeroSearch: Incentivize the search capability of LLMs without searching. arXiv preprint arXiv:2505.04588, 2025. Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT press, 2018. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2023. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025a. 15 AgentOCR: Reimagining Agent History via Optical Self-Compression Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. RAGEN: Understanding self-evolution in LLM agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General OCR theory: Towards OCR-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. Haoran Wei, Yaofeng Sun, and Yukun Li. DeepSeek-OCR: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Proceedings of the Advances in Neural Information Processing Systems, 2022. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. DuoAttention: Efficient long-context LLM inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. GUI-explorer: Autonomous exploration and mining of transition-aware knowledge for GUI agent. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2025. Ling Xing, Alex Jinpeng Wang, Rui Yan, Xiangbo Shu, and Jinhui Tang. Vision-centric token compression in large language model. arXiv preprint arXiv:2502.00791, 2025. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-MEM: Agentic memory for LLM agents. arXiv preprint arXiv:2502.12110, 2025a. Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. AndroidLab: Training and systematic benchmarking of android autonomous agents. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2025b. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. LayoutLM: Pre-training of text and layout for document image understanding. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. SimpleTIR: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Pan, Hinrich SchÃ¼tze, et al. Memory-R1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 16 AgentOCR: Reimagining Agent History via Optical Self-Compression Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In Proceedings of the International Conference on Learning Representations, 2023. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-Agent-v3: Fundamental agents for GUI automation. arXiv preprint arXiv:2508.15144, 2025. Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. COMPACT: Compressing retrieved documents actively for question answering. arXiv preprint arXiv:2407.09014, 2024. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. MemAgent: Reshaping long-context LLM with multi-conv RL-based memory agent. arXiv preprint arXiv:2507.02259, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-Memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025a. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for LLMs: survey. arXiv preprint arXiv:2509.02547, 2025b. Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. AdaComp: Extractive context compression with adaptive predictor for retrieval-augmented large language models. arXiv preprint arXiv:2409.01579, 2024. Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, and Xiaofeng He. BELLE: bi-level multi-agent reasoning framework for multi-hop question answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2025c. Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 2025d. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Yifei Zhou and Andrea Zanette. ArCHer: Training language model agents via hierarchical multi-turn RL. In Proceedings of the International Conference on Machine Learning, 2024. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025. 17 AgentOCR: Reimagining Agent History via Optical Self-Compression // 1. Physical environment step Execute ağ‘¡, receive oğ‘¡+1, ğ‘Ÿğ‘¡, and success_flag Algorithm 1 Environment Wrapper of AgentOCR 1: Input: Reward param ğœ†, Compression interval ğ¾. 2: Internal State: Memory â„³, Cache ğ’(ğ‘’), Renderer â„›. 3: Function Step(ağ‘¡, ğ‘ğ‘¡, ğ‘¡) 4: 5: 6: â„³ğ‘¡+1 â„³ğ‘¡ {(oğ‘¡+1, ağ‘¡)} 7: 8: 9: 10: 11: 12: // 2. Optical memory rendering (tool execution â„›) hğ‘¡+1 Fetch(â„³ğ‘¡+1) Segments (â„“ğ‘¡+1,1, . . . , â„“ğ‘¡+1,ğ¾ ) Split(hğ‘¡+1) Limgs [] for ğ‘– = 1 to ğ¾ do ğ‘˜ğ‘– Hash(â„“ğ‘¡+1,ğ‘–) if ğ‘˜ğ‘– / ğ’(ğ‘’) then end if Append ğ’(ğ‘’)[ğ‘˜ğ‘–] to Limgs 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: End Function ğ‘ğ‘¡) end for Iraw Stack(Limgs) // Apply compression ğ‘ğ‘¡ (Eq. (7)) Iğ‘¡+1 Resize(Iraw, scale = 1/ // 3. Compression-aware reward (Eq. (8)) if success_flag is True then comp ğ‘¡ ln(ğ‘ğ‘¡) if success ğ‘Ÿ else comp ğ‘¡ 0 ğ‘Ÿ end if // Apply sparse reward injection comp ğ‘Ÿğ‘¡ ğ‘Ÿğ‘¡ + ğœ† ğ‘Ÿ ğ‘¡ return (oğ‘¡+1, Iğ‘¡+1), I(ğ‘¡ mod ğ¾ = 0) ğ‘Ÿğ‘¡ ğ’(ğ‘’)[ğ‘˜ğ‘–] â„›(â„“ğ‘¡+1,ğ‘–; ğœ“) {Cache miss: render segment} A. Pseudo Code We provide the detailed pseudo code for AgentOCR to facilitate reproduction. Algorithm 1 handles the backend mechanics: it maintains the segment cache, executes the rendering tool with the requested compression ğ‘ğ‘¡, and computes the efficiency-aware reward. Algorithm 2 details the interaction loop, where the policy explicitly selects the tool parameter ğ‘ğ‘¡ to control the resolution of the subsequent visual observation. B. Experiments B.1. Details of Benchmarks ALFWorld. ALFWorld (Shridhar et al., 2021) is an embodied environment comprising 3,827 tasks, which is publicly available for non-commercial research purposes. The objective for LLM agents is to accomplish household tasks spanning six categories: Pick & Place, Examine in Light (Look), Clean & Place (Clean), Heat & Place (Heat), Cool & Place (Cool), and Pick Two & Place. At each interaction step, the LLM agent selects an action based on the current observation and interaction history, then receives feedback from the environment to verify task completion. 18 AgentOCR: Reimagining Agent History via Optical Self-Compression Algorithm 2 AgentOCR Policy Rollout 1: Input: Task Instruction â„, Total training steps ğ‘‡max. 2: Initialize: Policy ğœ‹ğœƒ, Env Wrapper â„°. 3: Initialize: Global training step ğ‘¡ 0. 4: while ğ‘¡ < ğ‘‡max do â„°.Reset() 5: Get initial observation (oğ‘¡, Iğ‘¡) from â„° 6: 7: while episode not done do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end while // 1. Decision making (policy generates action + tool call) Sample action ağ‘¡ ğœ‹ğœƒ( â„, Iğ‘¡) // Parse specific tag as described in Sec. 4.3 Parse output (ağ‘¡, ğ‘ğ‘¡) via <compression> tag // 2. Execute action and tool call (oğ‘¡+1, Iğ‘¡+1), ğ‘Ÿğ‘¡ â„°.Step(ağ‘¡, ğ‘ğ‘¡, ğ‘¡) Store transition for optimization using (oğ‘¡, Iğ‘¡) (oğ‘¡+1, Iğ‘¡+1) ğ‘¡ ğ‘¡ + 1 end while Update ğœ‹ğœƒ via RL optimizer ğ‘Ÿğ‘¡ Search-based QA. We utilize the QA dataset used in Search-R1 (Jin et al., 2025), which is publicly available for non-commercial research purposes. It contains two categories of benchmark datasets. The first category is singlehop question answering, which includes NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2022). The second category is multi-hop question answering, which includes HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2Wiki) (Ho et al., 2020), MuSiQque (Trivedi et al., 2023), and Bamboogle (Press et al., 2023). In this scenario, the agent autonomously generates search queries during step-by-step reasoning. It uses the E5 retriever (Wang et al., 2022) to retrieve relevant documents from knowledge base, returning the top-3 most relevant passages for each query. The agent then analyzes and reasons over the retrieved information, iteratively refining its queries and incorporating new evidence until it arrives at the final answer. B.2. Details of Training Hyperparameters for ALFWorld. For ALFWorld, we follow the default settings used in GiGPO (Feng et al., 2025b). Text-only variants are assigned maximum prompt length of 5120 tokens. Conversely, optical-history variants are constrained to maximum prompt length of 2048 tokens. Across both modalities, the maximum response length is standardized to 512 tokens. The agent is permitted to interact with the environment for maximum of 50 steps per episode, with full history. The learning rate is fixed at 1e-6. During the training phase, rollouts are executed on 16 samples per iteration, generating 8 trajectories per sample. We grant reward of 10 for successful actions and 0 otherwise. The temperature parameter is set to 1.0 during rollouts and reduced to 0.4 for validation. The mini-batch size is maintained at 256. Consistent with standard practices, no KL-divergence loss is applied during training. Hyperparameters for search-based QA. For the search-based QA tasks, text-only variants operate with an expanded maximum prompt length of 14000 tokens. For optical-history variants, the limit is set to 4096 tokens. Similar to ALFWorld, the maximum response length is capped at 512 tokens. The interaction horizon is shorter, allowing up to 4 environmental steps per episode and retaining full history. The learning rate remains at 1e-6. Training rollouts involve 128 samples per iteration, with each sample producing 8 trajectories. The reward 19 AgentOCR: Reimagining Agent History via Optical Self-Compression structure assigns 1 for correct answers and 0 for incorrect ones. Rollout temperature is set to 1.0, while validation uses greedy decoding strategy (temperature 0.0). The mini-batch size is 256, and the KL-divergence loss is excluded. B.3. Computing Details All experiments utilizing the Qwen2.5-VL-3B-Instruct model for both ALFWorld and search-based QA were executed on 2H100 GPUs. Conversely, the larger Qwen2.5-VL-7B-Instruct model required configuration of 4H100 GPUs. The agents were trained for total duration of 150 iterations. B.4. Optical Rendering Details The optical memory is generated via deterministic renderer Iğ‘¡ = â„›(hğ‘¡; ğœ“). We employ specific typographic settings and semantic color codes to facilitate efficient parsing by the VLM. The detailed rendering hyperparameters ğœ“ for both benchmarks are provided in Tab. 5. B.5. Prompts The specific prompts employed for agents on the ALFWorld and search-based QA tasks are illustrated in Fig. 4, 5, 6, and 7. Specifically, Fig. 4 and 6 detail the templates for text-only baselines, whereas Fig. 5 and 7 present the templates for optical-history variants. These templates are constructed using Pythonstyle string formatting, where placeholders in curly For instance, braces ({}) mark semantic slots. {task_description} denotes the task definition, and {current_observation} indicates the immediate environmental feedback. These slots are populated with dynamic content during training. Parameter ALFWorld Search Typography & Layout Font Family Font Size Line Spacing Max Width Semantic Color Mapping Task & Context [Observation] [Action] <search> <information> Monospace 10pt 1.2 392px Monospace 12pt 1.2 560px Black Blue (0,0,255) Red (255,0,0) Black Blue (0,0,255) Red (255,0,0) To structure the models reasoning and outputs, we utilize specific control tags. The <think> </think> tags enclose the mandatory step-by-step reasoning chain. Final decisions are wrapped within <action> </action> tags. In the context of search agents, queries are issued between <search> <search> tags, with retrieved evidence presented inside </information> </information> tags; final answers are enclosed in <answer> </answer>. Uniquely for our AgentOCR method, the model is instructed to specify the compression ratio using <compression> </compression> tags. Table 5: Rendering hyperparameters ğœ“ for optical memory generation. C. Case Study We present the complete trajectory of multi-turn interaction between the agent and the search tool, and demonstrate the reasoning behavior of AgentOCR trained by GRPO. As shown in Fig. 8 and Fig. 9, the agent progressively accumulates search results in its optical memory and adaptively adjusts compression factors at each step. The agent successfully retrieves relevant information across multiple search iterations and arrives at the correct answer while maintaining efficient token usage through visual history compression. 20 AgentOCR: Reimagining Agent History via Optical Self-Compression Prompt Template of Text Agent on ALFWorld You are an expert agent operating in the ALFRED embodied Environment. Your task is to: {task_description}. Prior to this step, you have already taken {step_count} step(s). Below are the most recent {history_length} observations and the corresponding actions you took: {action_history}. You are now at step {current_step} and your current observation is: {current_observation}. Your admissible actions of the current situation are: [{admissible_actions}]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags. Figure 4: The prompt template of text agent on ALFWorld. Prompt Template of AgentOCR on ALFWorld <image> You are an expert agent operating in the ALFRED embodied Environment. Your task is to: {task_description}. Prior to this step, you have already taken {step_count} step(s). The provided image shows the most recent {history_length} observations and the corresponding actions you took. You are now at step {current_step} and your current observation is: {current_observation}. Your admissible actions of the current situation are: [{admissible_actions}]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags. Additionally, select an image compression factor larger than 1.0 for the next image. Higher compression lowers cost, but too much compression harms image quality. You must provide the next compression factor within <compression> </compression> tags (e.g., <compression>1.1</compression>). Figure 5: The prompt template of AgentOCR on ALFWorld. Prompt Template of Text Agent on Search-based QA You are an expert agent tasked with answering the given question step-by-step. Your question: {task_description}. Prior to this step, you have already taken {step_count} step(s). Below is the interaction history, where <search>...</search> wrapped your past search queries and <information>...</information> wrapped the corresponding search results. History: {memory_context} Now its your turn to respond for the current step. You should first conduct reasoning process. After completing your reasoning, choose only one of the following actions (do not perform both): (1) If any required knowledge is missing or uncertain, you MUST call search engine to get more external information using format: <search> your query <search>. (2) Only if you have sufficient information to answer the question with high confidence, provide your final answer within <answer> </answer> tags. Figure 6: The prompt template of text agent on search-based QA. 21 AgentOCR: Reimagining Agent History via Optical Self-Compression Prompt Template of AgentOCR on Search-based QA <image> You are an expert agent tasked with answering the given question step-by-step. Your question: {task_description}. Prior to this step, you have already taken {step_count} step(s). The image contains the full history: - Past queries are inside <search>...<search> - Past results are inside </information>...</information> Now its your turn to respond for the current step. You should first conduct reasoning process. After completing your reasoning, choose only one of the following actions (do not perform both): (1) If any required knowledge is missing or uncertain, you MUST call search engine to get more external information using format: <search> your query <search>. (2) Only if you have sufficient information to answer the question with high confidence, provide your final answer within <answer> </answer> tags. Additionally, select an image compression factor larger than 1.0 for the next image. Higher compression lowers cost, but too much compression harms image quality. You must provide the next compression factor within <compression> </compression> tags (e.g., <compression>1.1</compression>). Output format: 1. Reasoning: state what you found in the image. 2. <search>...<search> or <answer>...<answer> 3. </compression>...</compression> Figure 7: The prompt template of AgentOCR on search-based QA. 22 AgentOCR: Reimagining Agent History via Optical Self-Compression Figure 8: Case study of AgentOCR on HotpotQA (part I) 23 AgentOCR: Reimagining Agent History via Optical Self-Compression Figure 9: Case study of AgentOCR on HotpotQA (part II)"
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore",
        "Tongyi Lab, Alibaba Group"
    ]
}