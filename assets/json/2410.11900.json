{
    "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
    "authors": [
        "Erik Arakelyan",
        "Pasquale Minervini",
        "Pat Verga",
        "Patrick Lewis",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided $\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$ out of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search."
        },
        {
            "title": "Start",
            "content": "FLARE: FAITHFUL LOGIC-AIDED REASONING AND EXPLORATION Pasquale Minervini23 Erik Arakelyan1 1University of Copenhagen {erik.a, augenstein}@di.ku.dk {pat, patrick}@cohere.com Pat Verga4 2University of Edinburgh p.minervini@ed.ac.uk Patrick Lewis4 3Miniml.AI Isabelle Augenstein1 4Cohere 4 2 0 2 9 1 ] . [ 2 0 0 9 1 1 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chainof-Thought (CoT), assuming the resulting generation will have more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast high degree of faithfulness, they usually require model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (FLARE), novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan solution, soft-formalise the query into facts and predicates using logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that FLARE allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search."
        },
        {
            "title": "INTRODUCTION",
            "content": "Complex Reasoning in natural Question Answering (QA) tasks assumes the capability to explore the problem space of the designated query with formalised set of facts, relations, commonsense knowledge and logical implications. In line with this, LLMs have been enhanced with CoT (Wei et al., 2022) prompting, which supplements the QA process by generating intermediate reasoning chains given set of in-context examples (Brown et al., 2020a), as shown in Figure 1. This allowed for advancement in commonsense (Madaan et al., 2022), symbolic (Wang et al., 2022; Sprague et al., 2024) and mathematical (Jie et al., 2023) reasoning. Although CoT allows for problem exploration in natural language steps, such an approach has been shown to cause performance degradation for nuanced reasoning tasks involving multi-hop planning (Valmeekam et al., 2022; Suzgun et al., 2023), problem exploration (Yao et al., 2022) and arithmetic (Hendrycks et al., 2021b; Madaan & Yazdanbakhsh, 2022b). These discrepancies arise as CoT suffers from limited ability to decompose, search, verify and backtrack using intermediate rationale chains (Yao et al., 2022), cascading hallucinations and errors (Ling et al., 2023) and that natural language might not be an optimal representation for describing the reasoning process (Li et al., 2024). Simultaneously, LLM output has been shown to be unfaithful and inconsistent w.r.t. the intermediate CoT rationale (Jacovi et al., 2024; Lanham et al., 2023b; Turpin et al., 2023). To circumvent the problem of CoT faithfulness Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: depiction of the plan, code and simulated search in FLARE. Each module has breakdown of the relevant components composed by the LLM explained in Section 2. and allow for more robust reasoning during QA, Xu et al. (2024, Faithful CoT) suggested generating code which is further executed using an external symbolic solver. Producing and executing code allows one to both create an output that can be conditioned on heuristic search in the problem space and perform backtracking. However, strict translations of natural language queries into code, such as autoformalisation (Szegedy, 2020; Wang et al., 2018), is non-trivial task involving direct inference of implicit commonsense and domain-specific knowledge and the ability to align abstract and informal concepts directly to constrained formal definitions for further execution (Wu et al., 2022). An example query Do all parts of the aloe vera plant taste good? seen in Figure 1 is not trivial to formalise, nor to provide strict algorithmic solution for it, making it much more suitable for soft reasoning. Using external solvers makes such fuzzy reasoning impossible and requires consistently generating syntactically correct executable code. While some LLMs have coding capabilities stemming from their pretraining (Jiang et al., 2024; Aryabumi et al., 2024), relative code consistency is more probable with models explicitly trained for coding (Chen et al., 2021). To overcome these problems with CoT and F-CoT, we propose Faithful Logic-Aided Reasoning and Exploration (FLARE), an interpretable method that allows for planning, fuzzy reasoning, and traversing the problem space with backtracking, exact task decomposition, and measuring faithfulness. In FLARE, given natural language query, we prompt an LLM to sequentially generate plan that includes an analysis and the logical steps necessary for formalising and answering the question, logical programming Prolog (Wielemaker et al., 2012) code that allows formalising the query into set of facts, relations and their composition forming the problem space and the search which is an LLM generated simulation of exhaustive multi-hop traversal of that space from the code. An illustration of FLARE can be seen in Figure 1. In our framework, the generated code must not be consistently executable by an external solver, allowing for the soft-formalisation of natural language. Although we see that even generalist LLMs are able to produce executable code in 50% of cases. FLARE allows us to measure the faithfulness of the outcome w.r.t. the simulated multi-hop logical traversal by directly comparing the search paths generated from executable Prolog code to that LLM generation. This comparison also allows for pinpointing model hallucinations and inconsistencies. We systematically study the effectiveness of our method using 4 general-purpose LLMs of varying scales across 9 diverse benchmarks, covering Math World Problems, Multi-hop QA and Relation inference and show that our method achieves state-of-the-art results in 7 out of 9 in comparison to CoT and F-CoT. We also show that the method is rather competitive for models tuned for coding, with an average overall increase of 16% over F-Cot and 9% over CoT. Our findings show that model accuracy strongly correlates with the faithfulness of the reasoning process towards the problem space. We also provide ablations showing that the model can interpretably pinpoint hallucinations, search degeneracies, and the limitations of the search over the problem space. Our key contributions are the following:"
        },
        {
            "title": "Preprint",
            "content": "We introduce FLARE novel paradigm for logic-aided and interpretable formalisation and search over the problem space in QA and reasoning tasks. We perform systematic evaluation across 9 benchmarks and 4 models of varying scales, showing the advantages of using FLARE for QA in few-shot setup over prior approaches. The modularity of FLARE allows defining simple ingrained method for measuring model faithfulness and shows that it is strongly correlated with performance. We further show that using FLARE allows us to interpretably and rigorously detect hallucinations along with sub-optimal and inconsistent reasoning patterns."
        },
        {
            "title": "2.1 LLM SIMULATED SEARCH",
            "content": "FLARE comprises three modules for generating plan, code and simulated search for answering natural language query = {T1 . . . TQ}, where each Ti is token. Generating Plan For each query Q, given an LLM M, we initially prompt it to generate plan, P, which should be comprised of task explanation, analysis and plan for further formalising the query. An example of this can be seen in the plan section in Figure 1. We use in-context few shot examples EP of such plan generations along with greedy decoding for obtaining the final plan. Pi arg max pM(Ti EP , Q) (1) Generating Code After generating the plan, we prompt the LLM to generate Prolog code C, an example of which can be seen in Figure 1. We append executable code generation samples Csample to the previous in-context examples EP and obtain few-shot code generation demonstrations EC = [EP ; Csample] Ci arg max pM(Ti EC, Q, P) Fcode, Rcode, Gcode = EXTRACT(Ci) (2) Benefits of Prolog Prolog is symbolic logic-programming engine (Bowen, 1979) used for heuristic search over Horn Clauses (Chandra & Harel, 1985). It is declarative programming paradigm (Lloyd, 1994), meaning that the code is expressed as the logic of computation. In particular, this logic is formalised as set of facts and relations forming our problem space, while the final goal is first-order logic combination of them. As default, Prolog uses depth-first search (DFS) strategy (Bowen, 1979) for sub-goal decomposition and feasible traversal of the problem space that satisfies the goal G. Such traversal is referred to as the trace. At each trace step, the program can either confirm or invalidate the sub-goal using the feasibility of fact and relation combinations, expand the search tree or retry satisfying failed sub-goal with new combinations. An example of such search can be observed in Figure 1. It is possible to complete an exhaustive search, exploring all possible paths that do or do not satisfy the goal. These characteristics are beneficial as we can explicitly access and segment the facts and relations that form the problem space and the search strategy used for query formalisation. As Prolog is declarative, it is sufficient to use regexp heuristic for the segmentation, which is referred to as EXTRACT in Equation (2) and Equation (3). Furthermore, including exhaustive traversal traces in-context allows the LLM to simulate sub-goal decomposition, backtracking, intermediate goal invalidation, etc. We discuss this in more depth in the next paragraph. Simulating Search After generating the logic-programming code, we want to simulate program execution by generating problem space traversal trace with our LLM M. We update our incontext samples by appending search traces Ssample constructed from Prolog execution of sample codes Csample, i.e. ES = [EC; Ssample]. Si arg max pM(Ti ES , Q, P, C) Asearch, Fsearch, Rsearch = EXTRACT(Si) During iterative problem space traversal, we can segment the facts Fsearch, relations Rsearch, completed and backtracked paths with their answers Asearch used during the search simulation. To get the (3)"
        },
        {
            "title": "Preprint",
            "content": "Method GSM8K SVAMP MultiArith ASDiv AQuA StrategyQA Date Sport CLUTRR Math Word Problems Multi-hop QA Relation Llama-3.1-8BFLARE Llama-3.1-8BF-CoT Llama-3.1-8BCoT CmDRFLARE CmDRF-CoT CmDRCoT CmDR+FLARE CmDR+F-CoT CmDR+CoT GPT-3.5FLARE GPT-3.5F-CoT GPT-3.5CoT 72.7 0 59.2 52.4 0 46.5 71.4 0 48.7 68.1 75.8 79.8 86.0 0 58. 74.0 0 57.3 83.5 0 81.1 82.7 83.0 82.4 96.3 0 60.1 84.5 0 83.1 90.4 0 86. 98.3 95.3 98.2 83.1 0 61.9 72.2 0 37.2 81.3 0 44.6 85.4 81.7 75.8 62.9 12.2 43.7 0 28.3 55.9 15.4 44.1 55.1 53.5 59.4 70.2 53.2 2.9 67.0 59.7 21.3 70.8 57.6 48. 65.5 51.5 51.7 59.3 0 20.9 52.3 0 47.4 61.8 0 79.1 82.4 73.5 69.9 76.6 0 95. 78.9 0 55.2 77.7 0 62.6 85.6 52.3 95.8 36.8 32 42.2 29.1 8.6 29.5 41.0 35.3 42. 49.8 12.1 4.3 Table 1: The following table shows the performance of each of the tested models given technique for reasoning. Each bold, underlined, and italicised element highlights the best, second best and worst technique per specific model. The overall best method per dataset is highlighted in green . final answer we update in-context samples with the correct final answers Asample from the executed search Ssample, EA = [ES ; Asample]. AFinal arg max pM(Ti EA, Q, P, C, ) (4) The prompts used for generating each part in FLARE can be seen in Appendix A. 2.2 DETECTING REASONING INCONSISTENCIES For each query given the code and the simulated search along with the extracted facts Fcode, Fsearch and relations Rcode, Rsearch from each designated module, we aim to detect the inconsistencies during the reasoning process of the LLM. We use exact string matching between all these facts and relations in code and simulated search. i, j, such that such that code = code = search search and v, Rv and q, Rv code = Rq code = Rq search search (5) (6) With this framework in mind, we define two reasoning failure modes. In the first failure mode, given that some fact or relation was used in the simulated search but did not exist in the generated code, i.e. such that search / Fcode, we claim that the LLM has hallucinated. We postulate that the model either produced incomplete knowledge during formalisation to code or created piece of non-existing information during the search. We do not consider facts that emerged during direct inference step within the simulated search during our calculation. For example, if we are dealing with mathematical query 4 (5 + 6) =?, the search would involve separately evaluating the expression 5 + 6 = 11. In this case, 11 will not be treated as hallucinated fact within the search but rather as an emergent fact obtained from direct inference. The second failure mode is the reciprocal case, where fact or relation present in the code is not used during the search. We refer to this phenomenon as sub-optimal reasoning as it shows that the LLM could not explore the problem space completely or injected unsuitable knowledge during formalisation into code. 2.3 MEASURING FAITHFULNESS We propose method to measure the faithfulness of the LLM reasoning process when using FLARE. As mentioned in Section 2.1, for each query in dataset = [Q1, . . . , QD], we generate set of codes Φ = [C1, . . . , CΦ] and simulated problem space searches Ψ = [S1, . . . , SΨ]. We use the Prolog engine to execute all of the codes Φ and obtain set of correctly written programs Φ and exact search paths Ψ. As we do not require explicit programmatic correctness during inference in FLARE for any code Ci, some Prolog executions resulting in an error are filtered out in Ψ. To assess model reasoning faithfulness towards code formalisations, we compare the search paths Φ obtained from Prolog execution with their designated counterparts Φ gen generated by the LLM from the same"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: The trend of mean model accuracy w.r.t mean faithfulness (ROUGE-Lsum) for all the models. Faithfulness is positively correlated with model performance. code. We use ROUGE (Lin, 2004) to compute the matching score for each executed and simulated search path. In particular, we use ROUGE-Lsum, which uses the longest common subsequence (LCS) over each line to obtain the final score. This method fits our cause as line in Prolog search execution represents single logic step within the traversal. This allows us to measure the similarity of the reasoning contents and structure in exact and simulated searches."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "3.1 DATASETS To evaluate FLARE, we use benchmark of 9 tasks that cover Math Word Problems (MWP), multihop QA and relation inference. For testing numeric and mathematical reasoning, we follow CoT (Wei et al., 2022) by including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy & Roth, 2015), ASDiv (Miao et al., 2020) and AQuA (Ling et al., 2017). Among these, GSM8K, SVAMP, MultiArith and ASDiv cover elementary and middle school arithmetic word problems with set of integers or decimals as the answer. AQuA is multiple-choice numerical, symbolic reasoning dataset where each answer is mathematical expression containing notations, values and expressions not defined in the query. We also test FLARE using three multi-hop QA datasets. We use StrategyQA (Geva et al., 2021), which is boolean QA task that requires sub-goal decomposition and multi-hop reasoning strategy to answer. The example Do all parts of the aloe vera plant taste good? used in Figure 1, is taken from StratedyQA. The multi-hop QA testing also includes Date and Sports Understanding, subsets of BIG-Bench (bench authors, 2023). The tasks involve inferring an exact date given some calculations in the relative time period and understanding if an artificially created sports statement is feasible. Finally, we assess FLARE on Relational Inference using CLUTRR (Sinha et al., 2019), which involves inferring the familial relation between two entities mentioned in natural language description of the partial family graph. We adopt the same in-context samples used in (Xu et al., 2024; Wei et al., 2022). The complete descriptions, statistics and examples for each dataset can be found in Table 7 in Appendix A. 3.2 BENCHMARKS We compare FLARE with CoT (Wei et al., 2022) as prompting method that reasons using natural language chains and with F-CoT (Xu et al., 2024) that formalises the query into code and offload the reasoning to an external symbolic solver. We use Llama3.1 (8B) (Dubey et al., 2024), CmDR"
        },
        {
            "title": "Preprint",
            "content": "Method CmDRplan-only CmDRFLARE CmDR+plan-only CmDR+FLARE GPT-3.5plan-only GPT-3.5FLARE GSM8K AQuA StrategyQA 24.7 35.0 65.5 52.4 43.7 67.0 40.7 55.1 75. 71.4 55.9 70.8 36.1 54.3 62.3 68.1 55.1 65.5 Table 2: The table shows the accuracy of an LLM with FLARE compared to prompting for final answer directly after generating (plan-only) plan P. Figure 3: The figure shows the percentage of executable code per model (right) and the accuracy of the executable code when answering the queries (left). (30B) (Cohere, 2024), CmDR+ (100B) (Cohere, 2024) and GPT3.5 (Brown et al., 2020b) ( 100B (Ye et al., 2023)). As the coding model OpenAI Codex (code-DaVinci-002) (Chen et al., 2021) used in F-CoT has been deprecated, we replace it with the new GPT3.5 as suggested by OpenAI and recalculate the results accordingly."
        },
        {
            "title": "4 RESULTS",
            "content": "4.1 MAIN RESULTS To evaluate FLARE, we use set of models of varying sizes on diverse benchmarks, as defined in Section 3. We compare the performance of each model while using FLARE, CoT and F-CoT prompting. The results for F-CoT and CoT on all the models are computed using the codebase of the original study (Xu et al., 2024). LLMs for general reasoning Our results, presented in Table 1, show that using FLARE allows the LLMs to achieve state-of-the-art results on 7 out of 9 datasets, with an average 28% increase over CoT. We can see clear trend that FLARE increases the performance compared to CoT and F-CoT for all the models of varying scales. We also see that LLMs that are not explicitly tuned for coding suffer massive degeneracies when using F-CoT. We postulate that they are unable to consistently produce executable programs that satisfy predefined scheme in F-CoT, thus resulting in an error during execution. This further highlights the value of simulating program execution using an LLM instead of using external solvers. The results show that using FLARE yields more benefit on datasets that require longer chains of multi-hop and symbolic reasoning, like AQuA and StrategyQA."
        },
        {
            "title": "Preprint",
            "content": "Model Avg. Number of Paths Avg. #Hops per path Avg. #Fails per path Avg. Total Hops Avg. Total Fails Llama-3.1-8BFLARE CmDRFLARE CmDR+FLARE GPT-3.5 Llama-3.1-8BFLARE CmDRFLARE CmDR+FLARE GPT-3.5FLARE 1.55 1.51 0.92 0.68 1.43 1.19 0.97 0. Incorrect Answers 11.12 6.55 7.52 5.22 Correct Answers 9.12 7.10 7.19 5.65 1.52 0.68 1.13 0.71 0.62 0.42 0.42 0.26 15.09 10.56 8.57 5. 12.36 11.29 8.22 5.69 2.26 1.39 1.32 0.74 0.96 0.66 0.61 0.27 Table 3: The table depicts the difference in the average explored paths, hops, and fails during the reasoning process, which leads to incorrect or correct answers. The purple colour illustrates that incorrect reasoning paths have fewer explorations that led to Failed search paths. LLMs for code generation To understand the effect of FLARE on models tuned for coding, we use GPT3.5 (Brown et al., 2020a) as it was the OpenAI suggested succession model for Codex (Chen et al., 2021) which is used in F-CoT and possesses strong coding capabilities (Ye et al., 2023). The results in Table 1 show that using FLARE is beneficial for models that are tuned for coding and boost the accuracy with 16% increase over F-CoT and 9% over CoT. The reason is that many natural language queries with non-trivial formalisations are more suited to be tackled with more commonsense soft reasoning than direct code execution. This is evident in Table 1 where FLARE and CoT are consistently better than F-CoT in StrategyQA, Sports and CLUTRR. The opposite case of numeric and algorithmic heavy reasoning tasks is also covered by FLARE as it maintains strong performance similar to F-CoT on MWP problems Table 1. Consequently, FLARE allows combining algorithmic formalisation with simulated soft-reasoning, circumventing the pitfalls of using deterministic external solver while still producing query formalisation and problem space traversal. 4. IS SIMULATING SEARCH USEFUL? To understand if simulating search over the problem space is useful, we compare the performance of FLARE where we only generate the plan without the subsequent code or search components. We refer to this framework setup as plan-only, which can be seen in Figure 1 if we were to use only the plan for answer generation. We completed this ablation using CmDR, CmDR+, and GPT-3.5, and we used GSM8K, AQuA, and StrategyQA as our baselines. The results in Table 2 confirm that all of the models suffer massive performance degradation from 61.1 49.9 when omitting the code and the search components of FLARE. We hypothesise that this is caused by insufficient problem space exploration when using the plan-only setting. Furthermore, we have already seen in Table 1 that in methods, like F-CoT, that do not use simulated problem space exploration for soft-reasoning and only rely on plan and code, the performance also deteriorates even resulting in complete breakdown of reasoning over the designated datasets. This can be viewed as constrained version of FLARE with code-only execution. Consequently, our results show that simulating problem space traversal is highly beneficial as it avoids the pitfalls posed by plan-only and code-only modes by exploring the problem space more rigorously and soft-reasoning during that traversal instead of using external solvers. 4.3 FAITHFUL REASONING IMPROVES PERFORMANCE As described in Section 2, using FLARE allows us to measure the faithfulness of the LLM reasoning process by comparing the simulated problem space traversals Φ gen with actual traces Φ produced from symbolic Prolog solver. To do this, we initially compute the percentage of syntactically correct executable code each LLM produces. We can see from the right part of Figure 3 that all of the models are capable of producing correct executable Prolog code in 67% of cases on average and 50% of cases at the very least. This shows that the simulated searches Φ gen can be considered representative sample that will be further used to accurately measure the faithfulness of the simulated search w.r.t. the generated code. After measuring the reasoning faithfulness for each model, we want to understand what impact it has on the performance of the LLM. In Figure 2, we segment the models"
        },
        {
            "title": "Preprint",
            "content": "Model Unique Explorations (%) in Search Relation overlap (%) Unused Code relations (%) Llama-3.1-8BFLARE CmDRFLARE CmDR+FLARE GPT-3.5FLARE Llama-3.1-8BFLARE CmDRFLARE CmDR+FLARE GPT-3.5FLARE Correct Answers Incorrect Answers 43.65 35.96 34.47 37.55 35.04 32.76 24.98 24.44 74.14 59.06 64.30 64.46 54.69 54.50 44.12 36.02 5.73 4.02 4.54 1.90 9.28 6.23 8.22 6. Table 4: The table shows how the percentage of unique emergent inferences in search, overlapping relations between code and search, and unused relations in code impact answer correctness. w.r.t. their ROUGE-Lsum scores. The results show that model performance is strongly positively correlated with reasoning faithfulness. However, we also observe in the left part of Figure 3 that executing semantically precise code results in an accurate answer only in 47% of cases on average. Indeed, having simulated search trace with ROUGE-Lsum faithfulness score of 1, would be equivalent to simply executing the program as proposed in F-CoT. Yet we have priorly shown that F-CoT struggles with reasoning tasks that are hard to formalise and require multi-hop commonsense and soft reasoning. These two discoveries show that optimal LLM reasoning, conditioned on search in the problem space, should be increasingly faithful toward the facts, relations and the search strategy defined within the code while simultaneously maintaining the capability for soft-reasoning along more abstractly defined concepts. Our results show that FLARE allows LLMs to maintain similar reasoning capacity. 4.4 WHAT IS IMPORTANT DURING THE SEARCH? We expand the analysis of the simulated search traces to detect the reasons which can lead to optimal reasoning within an LLM. For this purpose, we calculate several statistics, like the average number of explored paths, average and total hops and failures per path, for each model during the simulated traversal. The failure in path is an invalidation of solution for sub-goal explored during the search, which is used for backtracking, as explained in Section 2. Calculating these statistics is simple as the search component of FLARE, seen in Figure 1, is structured simulation of Prolog trace, where each line contains hop of reasoning inference. We split these statistics for the reasoning paths that lead to correct or incorrect outcomes. Our results in Table 3 show that LLM performance and reasoning optimally are not directly connected to the amount of explored paths or multi-hop inferences per path. We also see that traces that lead to incorrect answers have higher number of failures per path and in total. We explain this phenomenon with the hypothesis that LLMs with traces that were optimal for reasoning and led to correct answers could skip exploring degenerate solutions due to strong commonsense reasoning capabilities. Further analyses focus on identifying inconsistencies and failure modes (Section 2.2). By comparing relations in code with those in search traces, we measure emergent hallucinations and unused relations, highlighting areas of sub-optimal reasoning. Additionally, we assess the uniqueness of emergent facts per inference hop, which indicates the extent of problem-space exploration  (Table 4)  . The results in Table 4 show consistently over each model that, on average, traces that lead to correct answers had higher percentage of unique emergent facts and overlap in the relations used between the code and search, while the portion of underutilized relations was lower. This means that optimal reasoning with an LLM requires great degree of problem-space exploration with fewer relation hallucinations during the search and more relation utilization from the defined code. This aligns with our prior discoveries, which show strong correlation between simulated search faithfulness towards the formalised code and model performance. Our framework FLARE has these reasoning patterns ingrained within its inference pipeline."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The effect of the model parameter scale from 8B to 100B+ on model accuracy (left) and faithfulness (right)."
        },
        {
            "title": "Model",
            "content": "Avg. hops per Paths Hallucination (%) Unutilised knowledge (%) Llama-3.1-8B CmDR CmDR+ GPT-3.5 9.4 6.7 7.2 5.5 63.3 54.7 54.3 49.3 62.9 56.9 56.3 52.1 Table 5: The table shows the changes in simulated search statistics when using FLARE w.r.t model scale from 8B to 100B+. Hallucinations refer to facts and predicates only used in trace, while unutilised knowledge relates to the facts and relations only seen in the code. 4.5 THE EFFECT OF SCALE We want to assess the impact of the number of parameters in the model on the overall performance and faithfulness. The results in Figure 4 show no precise relation between model scale, performance and faithfulness. However, scaled models from the same family, i.e. CmDR (30B) and CmDR+ (100B), show improvements in reasoning faithfulness and model performance. We can also see in Table 5 that as the model size increases, the average number of hops and the portion of hallucinations and unutilised knowledge decreases. This further confirms our prior assumptions that models with strong commonsense soft-reasoning capabilities can skip steps during the search while maintaining the knowledge and structure of the traversal strategy outlined in the code."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reasoning in Natural Language Few-shot prompting (Brown et al., 2020b) has been shown to be an effective approach for increasing the reasoning capabilities of LLMs in natural language generation (Gehrmann et al., 2021; Reif et al., 2022; Sanh et al., 2022). LLM reasoning can be further enhanced with prompting techniques such as CoT (Wei et al., 2022), which attempts to segment reasoning into explicitly written intermediate steps. Concurrent work has also proposed that models think step by step (Kojima et al., 2022), or divide the problem into subtasks before the solution (Zhou et al., 2023, Least-to-Most). These approaches have been shown to suffer from arithmetic inaccuracies (Lewkowycz et al., 2022; Hendrycks et al., 2021a) and reasoning inconsistencies (Madaan & Yazdanbakhsh, 2022a). Further attempts have been made to add planning stage before reasoning by dividing the process into recursive plan formulation and execution steps (Yao et al., 2023b; Wang et al., 2023a). The plan generation step in FLARE is hybrid technique inspired by these methods but focused on generating natural language strategy for formalising the query into code."
        },
        {
            "title": "Preprint",
            "content": "Reasoning with Search Several lines of work propose using techniques to expand the reasoning paths over the problem space. Self-consistency decoding (Wang et al., 2023b) is an approach used to sample many natural language reasoning paths and take majority vote for an answer. Another popular approach is Tree-of-Thoughts (ToT, Yao et al. (2023a)), which proposes to explore the problem space with reasoning similar to tree traversal, where each state is created and evaluated using an LLM. Similar techniques try to adapt symbolic search approaches akin to DFS, BFS (Besta et al., 2024), (Lehnert et al., 2024) or other combinations (Gandhi et al., 2024) with direct tuning (Lehnert et al., 2024), imitation training (Yang et al., 2022) or few-shot prompting (Zhang et al., 2024). It must be noted that all of these techniques have only been tested in constrained mathematical puzzle-solving and algorithmic domains like the 24 Game (Yang et al., 2022), Countdown (Wikipedia, 2024), Sorting (Besta et al., 2024), maze solving (Yang et al., 2022), Sokoban (Lehnert et al., 2024) and others. Although the search component of FLARE has some similarities to these techniques, we argue that our method allows for generalistic reasoning with interpretable multi-hop search through iterative logic-based problem space exploration. Reasoning with Formalisation Another line of research has tried formalising natural language queries into code (Gao et al., 2023; Li et al., 2024) or pseudo-code (Chae et al., 2024; Gandhi et al., 2024). This allows the translation of the query into strict structure and offloads the reasoning and search components to deterministic solvers like Python Chen et al. (2023), PDDL Xu et al. (2024); Liu et al. (2023), DataLog Xu et al. (2024) and others. While models are reasonably capable of synthesising programs (Austin et al., 2021; Nijkamp et al., 2023) and benefit from the use of code in numerical and algorithmic reasoning settings (Chen et al., 2023; Gao et al., 2023), the usage of code for general QA has not been rigorously explored. The reasons are that formalisation from natural language into strict and executable code is challenging (Wu et al., 2022), following the exact syntactic constraints of the programming language not abundantly used during pre-training is onerous (Liu et al., 2024) and can require models explicitly tuned for coding (Chen et al., 2021). Using an external solver for reasoning also limits the capability for soft reasoning in commonsense knowledge and implications. Although we formalise the natural language query into logic programming Prolog program during the code generation part of FLARE, we do not explicitly require the code to be executable and do not use external solvers during inference. This allows for the further use of the LLM for soft-reasoning to simulate code execution in logic-based problem space traversal similar to Prolog while circumventing the need for code tuning generalist model. Reasoning Faithfulness An explanation is considered faithful if it explicitly and accurately describes the reasoning process of the model during inference (Gilpin et al., 2018; Jacovi & Goldberg, 2020). In the context of prompting techniques such as CoT, we are interested in the faithfulness of the intermediate reasoning chains towards the final output. Faithful intermediate reasoning chains should not just look plausible (Herman, 2017) but have exact reflections of the problem exploration and reasoning used to arrive at the final answer. Natural language reasoning chains prevalent in CoT and similar methods are shown to be unfaithful, either masking the reasoning biases (Turpin et al., 2023) of the model or outright ignoring the intermediate reasoning (Lanham et al., 2023a). In FLARE, we introduce method to seamlessly measure the faithfulness of the final outcome w.r.t. completed search."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work introduces FLARE, novel approach for logic-aided interpretable formalisation and reasoning with simulated search over the problem space. We show that models of varying scales obtain state-of-the-art results compared to prompting paradigms like CoT and F-CoT. We further pinpoint that using FLARE allows us to perform soft-reasoning with simulated search, making it flexible for diverse reasoning benchmarks. We introduce method to measure model reasoning faithfulness w.r.t. the problem formalization ingrained within FLARE. Our results show that model performance is positively correlated with the faithfulness of the reasoning process. The systematic studies of the method show the benefits of using simulated search compared to natural language reasoning and external symbolic solvers. We further show that using FLARE allows us to interpretably and rigorously detect hallucinations and sub-optimal and inconsistent reasoning patterns."
        },
        {
            "title": "REPRODUCIBILITY REPORT",
            "content": "To reproduce the results of our study, we provide the complete codebase, processing pipelines and prompts for each dataset. The only model hyper-parameter we explicitly fix is the temperature for greedy decoding. We also make the inference of all of the models using FLARE, F-CoT and CoT across all of the datasets publicly available for further experimentation and exploration."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "Erik is partially funded by DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, as well as by NEC PhD fellowship. Pasquale was partially funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and donation from Accenture LLP. Isabelles research is partially funded by the European Union (ERC, ExplainYourself, 101077481). This work was further supported by by the Pioneer Centre for AI, DNRF grant number P1, the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh."
        },
        {
            "title": "REFERENCES",
            "content": "Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Ustun, and Sara Hooker. To code, or not to code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL https: //doi.org/10.48550/arXiv.2408.10914. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https: //arxiv.org/abs/2108.07732. BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 1768217690. AAAI Press, doi: 10.1609/AAAI.V38I16.29720. URL https://doi.org/10.1609/aaai. 2024. v38i16.29720. Kenneth A. Bowen. Prolog. In Proceedings of the 1979 Annual Conference, Detroit, Michigan, USA, October 29-31, 1979, pp. 1423. ACM, 1979. doi: 10.1145/800177.810020. URL https: //doi.org/10.1145/800177.810020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec In Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
        },
        {
            "title": "Preprint",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020b. URL https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, and Jinyoung Yeo. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. CoRR, abs/2404.02575, 2024. doi: 10.48550/ARXIV.2404.02575. URL https://doi.org/10.48550/arXiv.2404.02575. Ashok K. Chandra and David Harel. Horn clauses queries and generalizations. J. Log. Program., 2 (1):115, 1985. doi: 10.1016/0743-1066(85)90002-0. URL https://doi.org/10.1016/ 0743-1066(85)90002-0. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=YfZ4ZPt8zd. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Cohere. Command r: Retrieval-augmented generation at production scale. https://txt. cohere.com/command-r, March 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy"
        },
        {
            "title": "Preprint",
            "content": "Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and CoRR, Noah D. Goodman. abs/2404.03683, 2024. doi: 10.48550/ARXIV.2404.03683. URL https://doi.org/10. 48550/arXiv.2404.03683. Stream of search (sos): Learning to search in language. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html. Sebastian Gehrmann, Tosin P. Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondrej Dusek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura PerezBeltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Joao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. CoRR, abs/2102.01672, 2021. URL https://arxiv.org/abs/2102.01672. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346361, 2021. doi: 10.1162/TACL 00370. URL https://doi. org/10.1162/tacl_a_00370. Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, and Lalana Kagal. Explaining explanations: An overview of interpretability of machine learning. In Francesco Bonchi, Foster J. Provost, Tina Eliassi-Rad, Wei Wang, Ciro Cattuto, and Rayid Ghani (eds.), 5th IEEE International Conference on Data Science and Advanced Analytics, DSAA 2018, Turin, Italy, October 1-3, 2018, pp. 8089. IEEE, 2018. doi: 10.1109/DSAA.2018.00018. URL https://doi.org/10.1109/DSAA.2018.00018. and Jacob Steinhardt. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Measuring mathematical problem solving with In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings Information Processing Systems Track on Datasets and Benchmarks URL Dawn Song, the MATH dataset. the Neural of 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021a. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Bernease Herman. The promise and peril of human evaluation for model interpretability. arXiv preprint arXiv:1711.07414, 2017. Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational"
        },
        {
            "title": "Preprint",
            "content": "Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 41984205. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.386. URL https://doi.org/10. 18653/v1/2020.acl-main.386. Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, and Mor Geva. chain-of-thought is as strong as its weakest link: benchmark for verifiers of reasoning chains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pp. 46154634. Association for Computational Linguistics, 2024. URL https: //aclanthology.org/2024.acl-long.254. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. CoRR, abs/2406.00515, 2024. doi: 10.48550/ARXIV.2406.00515. URL https://doi.org/10.48550/arXiv.2406.00515. Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. Design of chain-ofthought in math problem solving. arXiv preprint arXiv:2309.11054, 2023. Large language models are zero-shot Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html. reasoners. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-ofthought reasoning. CoRR, abs/2307.13702, 2023a. doi: 10.48550/ARXIV.2307.13702. URL https://doi.org/10.48550/arXiv.2307.13702. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023b. Lucas Lehnert, Sainbayar Sukhbaatar, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. CoRR, abs/2402.14083, 2024. doi: 10.48550/ARXIV.2402.14083. URL https://doi.org/10.48550/arXiv. 2402.14083. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language In Forty-first International Conference on Machine Learnmodel-augmented code emulator. ing, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=vKtomqlSxm. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013."
        },
        {
            "title": "Preprint",
            "content": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Regina Barzilay and MinYen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 158167. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1015. URL https://doi.org/10.18653/v1/P17-1015. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 72393bd47a35f5b3bee4c609e7bba733-Abstract-Conference.html. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. LLM+P: empowering large language models with optimal planning proficiency. CoRR, abs/2304.11477, 2023. doi: 10.48550/ARXIV.2304.11477. URL https://doi.org/10. 48550/arXiv.2304.11477. Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. Exploring and evaluating hallucinations in llm-powered code generation. CoRR, abs/2404.00971, 2024. doi: 10.48550/ARXIV.2404.00971. URL https://doi.org/10.48550/arXiv.2404. 00971. John W. Lloyd. Practical advtanages of declarative programming. In Marıa Alpuente, Roberto Barbuti, and Isidro Ramos (eds.), 1994 Joint Conference on Declarative Programming, GULPPRODE94 Peniscola, Spain, September 19-22, 1994, Volume 1, pp. 1830, 1994. Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. CoRR, abs/2209.07686, 2022a. doi: 10.48550/ARXIV.2209.07686. URL https: //doi.org/10.48550/arXiv.2209.07686. Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022b. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 975984. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.92. URL https://doi.org/10. 18653/v1/2020.acl-main.92. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/ forum?id=iaYcJKpY2B_. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 20802094. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.168. URL https://doi.org/10.18653/v1/2021. naacl-main.168."
        },
        {
            "title": "Preprint",
            "content": "Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. recipe for arbitrary text style transfer with large language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 837848. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-SHORT.94. URL https://doi.org/10.18653/v1/2022.acl-short.94. Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluıs M`arquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 17431752. The Association for Computational Linguistics, 2015. doi: 10.18653/V1/D15-1202. URL https://doi.org/10.18653/v1/d15-1202. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: diagnostic benchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 4505 4514. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1458. URL https://doi.org/10.18653/v1/D19-1458. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023. findings-acl.824. Christian Szegedy. promising path towards autoformalization and general artificial intelligence. In Christoph Benzmuller and Bruce R. Miller (eds.), Intelligent Computer Mathematics - 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings, volume 12236 of Lecture Notes in Computer Science, pp. 320. Springer, 2020. doi: 10.1007/ 978-3-030-53518-6 1. URL https://doi.org/10.1007/978-3-030-53518-6_1. Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models dont In Alalways say what they think: Unfaithful explanations in chain-of-thought prompting. ice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still cant plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022."
        },
        {
            "title": "Preprint",
            "content": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 26092634. Association for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.ACL-LONG.147. URL https://doi.org/10. 18653/v1/2023.acl-long.147. Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments with neural translation of informal to formal mathematics. In Florian Rabe, William M. Farmer, Grant O. Passmore, and Abdou Youssef (eds.), Intelligent Computer Mathematics - 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018, Proceedings, volume 11006 of Lecture Notes in Computer Science, pp. 255270. Springer, 2018. doi: 10.1007/978-3-319-96812-4 22. URL https://doi.org/10.1007/978-3-319-96812-4_22. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/ forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jan Wielemaker, Tom Schrijvers, Markus Triska, and Torbjorn Lager. Swi-prolog. Theory and Practice of Logic Programming, 12(1-2):6796, 2012. Wikipedia. Countdown (game show) Wikipedia, the free encyclopedia. http: //en.wikipedia.org/w/index.php?title=Countdown%20(game%20show) &oldid=1248084922, 2024. [Online; accessed 09-September-2024]. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logical reasoning via symbolic chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1332613365. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.720. URL https://doi.org/10.18653/v1/2024.acl-long.720. Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Chain of thought In Sanmi Koyejo, S. Mohamed, A. Agarwal, imitation with procedure cloning. Information Danielle Belgrave, K. Cho, Processing Systems 35: Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ ebdb990471f653dffb425eff03c7c980-Abstract-Conference.html. and A. Oh (eds.), Advances Annual Conference on Neural in Neural Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022."
        },
        {
            "title": "Preprint",
            "content": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/ hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/forum?id=WE_vluYUL-X. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. comprehensive capability analysis of GPT-3 and GPT-3.5 series models. CoRR, abs/2303.10420, 2023. doi: 10.48550/ARXIV.2303.10420. URL https://doi.org/10.48550/arXiv. 2303.10420. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. LLM as mastermind: survey of strategic reasoning with large language models. CoRR, abs/2404.01230, 2024. doi: 10.48550/ARXIV.2404.01230. URL https://doi.org/10.48550/arXiv.2404.01230. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WZH7099tgfM."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM PROMPTS We define straight-forward prompts for generating plan, code and search simulation in FLARE, which can be observed in Appendix A.2. A.2 DATASET STATISTICS The datasets used in this study encompass variety of domains, specifically targeting the performance of the models in interpreting Math Word Problems, multi-hop question answering, and relational inference. Table 7 provides detailed breakdown of each dataset, including the number of few-shot in-context samples (shots), the number of test samples, and representative examples from each dataset. The datasets provide comprehensive basis for evaluating the models abilities to handle complex tasks across different domains, facilitating an in-depth analysis of model performance under few-shot conditions. Task Plan Generation Code Generation Simulated Search Final Answer Prompt Description Generate an explanation and analysis, and plan to generate prompt for writing swi-prolog code for the last task. The 3 sections should be exactly outlined. Your plan should show enough intermediate reasoning steps towards the answer. Construct the plan as much as you can and describe the logic specifically. When constructing the plan for the code prompt, actively use swi prolog search capabilities. Write Prolog code to solve usIf there are uning the plan. known or stochastic atoms or predicates, fill in the values for them as logical assumption and add comment in the same line Assumed atom/predicate. Do not use write and read commands within the code. The code should be very detailed and utilize swi prolog capabilities to the fullest. To run the program, at the end create predicate named query that returns the correct numerical answer. The last line of the program should be the commented-out driver predicate query. Write only the code. Ignoring the read commands, explicitly write out the search paths that are explored by the code: #### Here are the paths [Starting Search Simulation]: #### [Path 1]: Given the plan, the code and the explored search paths answer the question above. Answer with the correct numerical answer. ##### Here is the answer: Detailed instructions for generating an outline and plan, with an emphasis on reasoning steps and using Prologs search capabilities. Instructions for generating Prolog code based on the plan with assumptions for unknown atoms. Emphasizes code details and final query predicate. task to simulate and display the search paths that the Prolog code would follow during execution. Final prompt asking for the correct numerical answer based on the previous steps. Table 6: Table of Prompts for Plan, Code, Simulated Search, and Final Answer generation for GSM8K (Cobbe et al., 2021)."
        },
        {
            "title": "Preprint",
            "content": "Domain Dataset Shots Test Samples Example Math Word Problems GSM8K SVAMP MultiArith ASDiv AQuA Multihop QA StrategyQA Date Understanding Sports Understanding 8 8 8 8 6 10 10 1,319 1,000 2,096 254 2,290 359 977 Relational Inference CLUTRR 8 1,042 Q: robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? A: 3 Q: Dan had $3 left with him after he bought candy bar. If he had $4 at the start, how much did the candy bar cost?A: 1 Q: pet store had 13 siamese cats and 5 house cats. During sale they sold 10 cats. How many cats do they have left? A: 8 Q: Adam has five more apples than Jackie. Jackie has nine apples. How many apples does Adam have? A: 14 Q: man walks at 5 kmph for 6 hrs and at 4 kmph for 12 hrs. His average speed is Answer option: A)4 1/3 km/h, B)7 2/3 km/h, C)9 ½ km/h, D)8 km/h, E)81 km/h A: Q: Did Aristotle use laptop? A: False Q: Yesterday was April 30, 2021. What is the date tomorrow in MM/DD/YYYY? A: 05/02/2021 Q: Is the following sentence plausible? Lionel Messi was called for icing? A: False Q: [Carlos] is [Clarence]s brother. [Carlos] and his sister, [Annie], went shopping. asked her mom [Valerie] if she wanted anything, but [Valerie] said no. How is [Valerie] related to [Clarence]? A: mother Table 7: The statistics and examples of the datasets used in benchmarking. Shots refers to the number of few-shot in-context samples used during benchmarking."
        }
    ],
    "affiliations": [
        "Cohere",
        "Miniml.AI",
        "University of Copenhagen",
        "University of Edinburgh"
    ]
}