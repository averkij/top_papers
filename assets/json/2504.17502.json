{
    "paper_title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation",
    "authors": [
        "Aviv Slobodkin",
        "Hagai Taitelbaum",
        "Yonatan Bitton",
        "Brian Gordon",
        "Michal Sokolik",
        "Nitzan Bitton Guetta",
        "Almog Gueta",
        "Royi Rassin",
        "Itay Laish",
        "Dani Lischinski",
        "Idan Szpektor"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consistent character representation in video rendering -- progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single prediction. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories (e.g., \\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency. It also excels with lesser-known concepts, aligning with human preferences at over 87\\% accuracy."
        },
        {
            "title": "Start",
            "content": "REFVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation Aviv Slobodkin1* Hagai Taitelbaum1 Yonatan Bitton1 Brian Gordon1 Michal Sokolik1 Nitzan Bitton Guetta2 Almog Gueta1 Itay Laish1 Dani Lischinski1 Idan Szpektor1 Royi Rassin1 5 2 0 2 4 ] . [ 1 2 0 5 7 1 . 4 0 5 2 : r 1Google Research 2Ben Gurion University lovodkin93@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Subject-driven text-to-image (T2I) generation aims to produce images that align with given textual description, while preserving the visual identity from referenced subject image. Despite its broad downstream applicabilityranging from enhanced personalization in image generation to consistent character representation in video renderingprogress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this, we introduce REFVNLI, costeffective metric that evaluates both textual alignment and subject preservation in single prediction. Trained on large-scale dataset derived from video-reasoning benchmarks and image perturbations, REFVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories (e.g., Animal, Object), achieving up to 6.4point gains in textual alignment and 8.5-point gains in subject consistency. It also excels with lesser-known concepts, aligning with human preferences at over 87% accuracy. 1. Introduction In well-known scene from The Little Prince, the narrator attempts to comfort grieving prince by saying Ill draw you fence around your flower. While fairly simple, this offer raises deeper question: what makes such drawing adequate? Naturally, it would require precisely following the textual description, i.e., depicting fence around flower. However, the use of your implies that the drawing must also portray specific flowerthe Princes ownone with which he shares history. Given the flowers uniqueness and distinctive visual features, the narrators task proves far more complex than it first appears. * Work done during an internship at Google Research. Figure 1. Illustration of REFVNLI: Given reference image of subject, prompt referring to the subject, and target image, REFVNLI assesses both subject consistency and textual alignment. For subject consistency, it distinguishes identity-preserving variations, like dew on flower (top image), from identity-altering changes, such as color change (middle image). For textual alignment, it assesses whether the target image reflects all details from the prompt, such as the fences position relative to the flower (bottom image). Subject-driven text-to-image (T2I) generation [3, 8, 32, 51] enables variety of downstream applications, such as personalized image generation [51], character consistency in video generation [36], and enhancing vanilla T2I evaluation frameworks for less-known entities via image retrieval. Unlike standard T2I models that are only conditioned on text inputs, subject-driven T2I methods take both textual prompt and reference image, enabling more precise subject representation. For example, when creating an image of fenced-in flower for the Little Prince, such models should use reference image of the Princes flower to ensure the output preserves its unique features (see Fig. 1). Despite its wide applicability, research in this area has been hindered by the absence of reliable automatic evaluaFigure 2. Qualitative Comparison: We compare REFVNLI with DreamBench++ and CLIP, which score both Subject Consistency (SC) and Textual Alignment (TA), using examples from the Animal, Object, and Human categories. DreamBench++ scores (0-4) are scaled to 0-100 for better readability. REFVNLI demonstrates superior robustness to identity-agnostic changes (SC), such as the zoomed-out parrot (top-middle) and the zoomed-out man with different attire (bottom-middle). It is also more sensitive to identity-defining attributes, assigning low scores to the altered-faced man (bottom-left) and the balloon with mismatched patterns (middle-left). Additionally, REFVNLI excels at detecting text-image mismatches (TA), as seen in its penalization of the top-left image for lacking waterfall. tion frameworks. Existing metrics typically correlate poorly with humans, and often focus only on textual alignment between the input prompt and the target image, as in CLIP-T [47] and SigLIP [73], or on subject consistency between the input and target images, as in CLIP-I [47] and DINO-I [7], while both aspects are needed for successful subjectdriven generation. More correlative alternatives, like DreamBench++ [46] and VIEScore [27], depend on expensive API calls to models like GPT-4 [42], making them less practical for large-scale research. To bridge this gap, we present REFVNLI, cost-effective fine-tuned auto-rater for subject-driven T2I generation. Given triplet <imageref, prompt, imagetgt>, REFVNLI predicts two scorestextual alignment and subject consistencyin single classification, as shown in Fig. 1. To train REFVNLI, we automatically curate large-scale dataset of <imageref, prompt, imagetgt> triplets, labeled with <textual alignment, subject preservation> {0, 1}2. For subject preservation, we identify subjects across video frames, creating positive examples using pairs of frames depicting the same subject, and negative ones by pairing frames of different subjects. This approach enables robustness to variations in subject appearance (e.g., rotation, setting, clothing), as well as to the presence of extraneous elements (e.g., dew on the Little Princes flower in Fig. 1, top). At the same time, REFVNLI must also be sensitive to identity-defining traits, such as human facial features or object shapes and colors (e.g., middle image in Fig. 1). To this end, we modify images by masking and inpainting identity-critical regions, while keeping everything else unchanged. The original subject crops are then paired with the unaltered images as positive pairs and with the modified images as negative pairs, thereby ensuring the model focuses on key identity attributes. For textual alignment, we first create positive imageprompt pairs. For that, we use an LLM to caption each image in the aforementioned pairs, ensuring focus on the subject by enclosing it within bounding box. Negative pairs are then formed by replacing these captions with those of different scenes. To further enhance sensitivity to subtle misalignments, such as fence drawn next to rather than around the Princes flower (Fig. 1, bottom), we also create negative pairs by modifying single detail in each original (positive) caption, following [12]. Finally, to get the <imageref, prompt, imagetgt> triplets from each pair of frames and associated captions, we use the cropped subject from one frame as imageref and the entire second frame, alongside its corresponding caption, as {prompt, imagetgt}, resulting in total of 1.2 million instances. We evaluate REFVNLI on multiple human-labeled test sets for subject-driven generation, including DreamBench++ Figure 3. Generating subject consistency classification training instances from video frames. Given two pairs of frames, each extracted from distinct video scenes featuring the same entity (e.g., dog), where both frames within each pair depict the same subject (e.g., the same dog), we construct training {imageref, imagetgt} pairs for subject consistency classification. Positive pairs are formed by pairing cropped subject from one frame (e.g., dog from left frame in Scene 1) with the full frame from the same scene (right frame in Scene 1). In contrast, negative pairs are created by pairing the cropped subject with the other scenes full frames (e.g., Scene 2). This process is applied to all four frames, with each taking turns as the cropped reference image (imageref), while the corresponding full-frame counterparts serve as imagetgt, yielding total of 4 positive and 8 negative training pairs. [46], ImagenHub [26], and KITTEN [20], across categories such as Humans, Animals, Objects, Landmarks, and multisubject setting. For textual alignment, REFVNLI consistently matches or outperforms all baselines, achieving up to 6.4-point gains in Landmarks and excelling at detecting subtle text-image misalignments (e.g., missing waterfall in Fig. 2, top-left). It also shows strong performance for subject preservation, with gains of 6.3 points in the Object category (DreamBench++) and 8.5 points in the multi-subject setting (ImagenHub), surpassing the much larger GPT-4obased DreamBench++ metric. As seen in Fig. 2, it balances robustness to non-critical changes (e.g., zoomed-out toucan, top-middle) with sensitivity to identity shifts (e.g., altered facial features, bottom-left). Further, as shown in Fig. 6, REFVNLI effectively handles rare subjects (e.g., scientific animal names), aligning with human preferences over 87% of the time. 2. Training Dataset Construction This section outlines the automated construction process of our training dataset for REFVNLI, comprising triplets of <imageref, prompt, imagetgt> along with two binary labels: one for the subject preservation of imageref in imagetgt, and another for the textual alignment between the prompt and imagetgt. To that end, we first create subject-driven {imageref, imagetgt} pairs (2.1) and then auto-generate subject-focused prompts for each imagetgt (2.2). 2.1. Subject-driven Image Pairs To ensure our {imageref, imagetgt} dataset remains robust to identity-agnostic variations (e.g., pose, clothing, lighting, and background changes), we leverage video-based datasets that inherently capture these differences. Specifically, we use Mementos [63], which provides scene-specific video frames with human-written textual descriptions, and TVQA+ [31], which contains human-annotated bounding boxes for characters and objects in TV episodes. We first identify subjects within frames: for Mementos, we extract entities from the provided textual descriptions using Gemini [58] and localize them in the corresponding frames with an object detection model [41], while for TVQA+, we directly use the provided bounding boxes. Positive pairs are formed from frames featuring the same subject, typically within the same scene,1 while negative pairs consist of frames with distinct subjects, often across scenes, as illustrated in Fig. 3. These frame-pairs are then converted into {imageref, imagetgt}-pairs by cropping the subject from one frame as imageref (e.g., left frame in Scene 1 of Fig. 3) and using the full second frame as imagetgt (each of the other 3 frames in Fig. 3). This is then repeated with reversed roles to create an additional {imageref, imagetgt} pair. In total, we collect 338,551 image pairs (228,661 from Mementos and 109,890 from TVQA+) from 44,418 unique frames. To further enhance sensitivity to identity-specific attributes, such as facial features in humans or shapes and patterns in objects, we leverage the Open Images dataset [30] to create additional training instances, as shown in Fig. 4. Using its gold segmentation masks, we selectively mask and inpaint identity-critical regions while preserving other details. Specifically, we randomly sample 5 sub-masks covering 30%-50% of the subject mask ([1] in Fig. 4), which we use to create 5 inpainted variants ([2]). The version with the highest Mean Squared Error (MSE) between the modified and original regions (e.g., Fig. 4, bottom image, MSE=3983) is then paired with the unmodified cropped subject to form negative pair of {imageref, imagetgt}, while the original image and the same crop form positive pair, with the crop serving as imageref in both cases. This process yields extra 1For TVQA+, we also include cross-scene positive pairs for named entities, such as TV characters. Figure 4. Creating identity-sensitive {imageref, imagetgt} pairs. Starting with an image and mask of subject (e.g., briefcase), we randomly keep 5 patches within the masked area ([1]) and use them to create 5 inpainted versions ([2]). The version with the highest MSE between the altered and original areas (e.g., bottom image, MSE = 3983) is paired with the unmodified crop to form negative pair, while the original image and the same crop create positive pair, with the crop acting as imageref in both cases. model to focus on it, as well as filtering out prompts where it is missing. For negative prompts (Fig. 5, middle), we swap prompts between frames containing the same type of entity (e.g., dog). To further enhance sensitivity to subtle mismatches, we also create hard-negative prompts (Fig. 5, bottom) by using Gemini to modify single non-subject detail in the positive prompts, following [12]. Altogether, combining this step with the image-pairing step (2.1) yields 1.2 million <imageref, prompt, imagetgt> triplets labeled with textual alignment and subject preservation. 3. Experimental Settings This section outlines the training process of REFVNLI (3.1), followed by description of our meta-evaluation protocol and subject-driven benchmarks (3.2). Lastly, we provide an overview of the baseline models used for comparison (3.3). 3.1. REFVNLI Training To train REFVNLI, we fine-tune PaliGemma [4], 3B VisionLanguage Model (VLM) known for effective transfer learning, focusing on variant adapted for multi-image inputs.2 The model takes as input two images (imageref and imagetgt), and prompt that includes <u> and <u> markups around the referenced subject. During training, the model performs two sequential binary classificationsfirst assessing textual alignment, then subject preservationoutputting 1 (positive) or 0 (negative) for each task. At inference, we compute the probabilities of predicting 1 and 0 for the first and second generated tokens, and use their ratio to cal2https://huggingface.co/google/paligemma-3b-ftnlvr2-448 Figure 5. Example of prompt-imagetgt pairs. Given an image with specific subject (e.g., dog), we generate positive prompt by placing bounding box around the subject and instructing Gemini to describe it (top prompts). Negative prompts are created by swapping positive prompts between images of the same entity (middle prompts). For additional hard negatives, we instruct Gemini to modify single non-subject detail in the positive prompt while keeping the rest unchanged (bottom prompts). 16,572 pairs, helping the model focus on fine-grained identity details. To further improve data quality, we also apply multiple filtering steps, including removing blurry images and those with unclear subjects (see Sec. 9.1 for details). 2.2. Image-prompt Pairs For each {imageref, imagetgt} pair, we generate positive and negative prompts for imagetgt, as shown in Fig. 5. Positive prompts (Fig. 5, top) are created by instructing Gemini [58] to describe imagetgt, ensuring the subject is explicitly mentioned by enclosing it in bounding box and guiding the culate the textual alignment and subject preservation scores, respectively. For more details, see Sec. 8. 3.2. Meta-evaluation and Benchmarks We include 3 subject-driven generation benchmarks with human annotations for textual alignment and subject preservation across various subject categories (e.g., Human, Animal, Object, Landmark). To enable unified evaluation framework, despite differing scoring methods (5-scale and binary), we convert all annotations into binary labels: one for whether imagetgt fully captures the prompt (textual alignment) and another for whether it correctly depicts the referenced subject (subject preservation). For meta-evaluation, we report the Area Under the ROC Curve (ROC AUC) for each criterion, following standard meta-evaluation practices [18, 70, 72], and also provide unified score by calculating the harmonic mean of the two ROC AUC scores. We next detail the 3 datasets included in our analyses. Dreambench++ [46] is subject-driven generation benchmark with human annotations for 8,190 images generated by 7 models. Annotators rated textual alignment and subject preservation on 0-4 scale, with each image evaluated by two raters. To convert these ratings into binary labels, we classify criterion as positive if both annotators assigned score of at least 3, with at least one giving 4. We report performance separately for the benchmarks three subject categories: Human, Animal, and Object.3 ImagenHub [26] is standardized evaluation library for conditional image generation models, featuring human annotations across various T2I tasks, including subject-driven generation (150 instances) and multi-concept image composition (102 instances), which involves two referenced subjects per instance. Each image was rated by 3 annotators. Instead of separate ratings for textual alignment and subject preservation, annotators provided single adherence score (0, 0.5, or 1) per image. To align with our binary labeling framework, images rated 1 by all 3 annotators were assigned positive labels for both criteria, while the rest were re-annotated by this papers authors. In the Multi-subject setting, positive subject preservation label was assigned only when both subjects were accurately depicted. For evaluation, we report performance separately for Animal and Object subjects in the single-subject task. In the Multi-subject task, each instance is split into two single-subject evaluations, with the final score being the lower of the two ratings (for each criterion), to ensure stricter assessment. KITTEN [20] evaluates the ability of T2I models to generate diverse real-world entities, including plants, airplanes, 3A fourth style category is excluded as it is beyond our works scope. cars, and landmarks. Given 5 reference images and prompt, annotators rated how well the generated image captured the entity on 15 scale and provided binary textual alignment score, with each image assessed by five annotators. Unlike our focus on specific subjects, KITTEN evaluates general entity alignment (e.g., generic rose rather than specific one). Hence, we only use the 256 Landmark images, as landmarks are unique entities where entity adherence coincides with subject adherence. To convert ratings into binary labels, we apply majority voting for textual alignment and consider subject preservation positive only if most annotators rated it at least 4 and the average score is 4 or higher. 3.3. Baselines We evaluate REFVNLI against both standard and state-ofthe-art methods for measuring textual alignment, subject preservation, or both. Baselines for textual alignment. We compare REFVNLI with two groups of automatic metrics for textual alignment. The first group leverages large vision-language models (VLMs), computing cosine similarity between text and image encodings. This includes BLIPScore [33], CLIPScore [15], and SigLIP [73]. The second group, which includes TIFA [19] and VQAScore [35], evaluates textual alignment via visual question answering (VQA). We also include baseline where PaliGemma is finetuned on our dataset exclusively for textual alignment, given only the prompt and target image, referred to as PaliGemmatext. Baselines for subject consistency. For subject consistency, we compare REFVNLI against baselines that rely on large VLMs by measuring cosine similarity between the reference and generated image encodings. These baselines include DINO-I [7], and Crop-IR [66]. For the Human category, we also use ArcFace [9], face-embedding model designed for identity recognition. Furthermore, similar to textual alignment, we evaluate against PaliGemma model finetuned on our dataset specifically for subject consistency evaluation, using only the reference and target images (formatted as in Sec. 3.1). We refer to this model as PaliGemmaref. Baselines for both criteria. We also include 3 metrics that assess both textual alignment and subject preservation. CLIP [47] computes scores separately for each criterion by calculating cosine similarity between the encodings of imagetgt and those of prompt and imageref. VIEScore [27] uses an elaborate GPT-4o [42] few-shot strategy, simultaneously generating two 010 ratings, one for each criterion. Finally, DreamBench++ [46] evaluates each criterion separately using distinct GPT-4o prompts with hand-crafted instructions and examples. This method follows two-step prompting Textual Alignment Animal Human Object Subject Consistency Animal Human Object Unified Evaluation Animal Human Object CLIP DINO Crop-IR ArcFace CLIPScore BLIPScore SigLIP TIFA VQAScore VIEScore DreamBench++ PaliGemmatext/ref REFVNLI 72.8 - - - 71.5 75.4 72.5 70.6 79.3 76.6 79.5 77.9 80.1 77.4 - - - 76.2 79.6 80.1 75.7 78.1 78.1 82.7 79.1 82.5 74.6 - - - 72.9 78.8 77.1 69.5 82.6 78.5 82.5 81.2 82.0 72.4 80.1 56.1 - - - - - - 65.4 74.5 70.2 79.4 87.7 78.0 56.7 61.1 - - - - - 80.3 84.2 71.2 86.0 76.4 77.4 53.4 - - - - - - 76.0 79.4 77.6 85. 72.6 - - - - - - - - 70.6 76.9 73.8 79.7 82.2 - - - - - - - - 79.2 83.4 74.9 84.2 75.5 - - - - - - - - 77.2 80.9 79.4 83.8 Table 1. ROC AUC scores on DreamBench++ for textual alignment, subject consistency, and their harmonic mean (as unified evaluation) across Animal, Human, and Object categories. The last two rows feature models finetuned on our dataset, with PaliGemmatext/ref comprising two separate models (PaliGemmatext and PaliGemmaref) trained exclusively for each criterion. Bold indicates the highest score per column. Textual Alignment Animal Object Multi-subj. Subject Consistency Animal Object Multi-subj. Unified Evaluation Animal Object Multi-subj. CLIP DINO Crop-IR CLIPScore BLIPScore SigLIP TIFA VQAScore VIEScore DreamBench++ PaliGemmatext/ref REFVNLI 81.7 - - 81.7 82.9 80.6 79.7 77.4 62.1 86.4 81.3 84.4 74.7 - - 74.9 79.7 80.5 76.1 83.8 54.0 85.5 88.0 89.5 81.1 - - 79.2 84.2 82.3 79.2 87.8 71.6 88.2 85.2 86. 63.9 81.6 62.0 - - - - - 56.5 71.0 82.1 80.4 73.3 77.4 54.5 - - - - - 49.4 84.0 74.2 83.8 52.6 50.1 51.3 - - - - - 50.1 54.3 62.0 62.8 71.7 - - - - - - - 59.2 77.9 81.7 82.4 74.0 - - - - - - - 51.6 84.7 80.5 86.6 63.8 - - - - - - - 59.0 67.2 71.8 72. Table 2. ROC AUC scores on ImagenHub for textual alignment, subject consistency, and their harmonic mean (as unified evaluation) across Animal and Object categories, as well as for the Multi-subject setting. The last two rows feature models finetuned on our dataset, with PaliGemmatext/ref comprising two separate models trained exclusively for each criterion. Bold indicates the highest score per column. Textual Alignment Subject Consistency Unified Evaluation 4o-based DreamBench++ by 6.3 points. CLIP DINO Crop-IR CLIPScore BLIPScore SigLIP TIFA VQAScore VIEScore DreamBench++ PaliGemmatext/ref REFVNLI 83.3 - - 83.4 82.5 75.5 90.6 88.9 82.6 87.0 94.5 97.0 80.2 85.2 85.5 - - - - - 87.4 89.8 87.3 82.3 81.7 - - - - - - - 84.9 88.4 90.8 89.0 Table 3. ROC AUC scores on KITTEN (landmarks) for textual alignment, subject consistency, and their harmonic mean (as unified evaluation). The last two rows feature models finetuned on our dataset, with PaliGemmatext/ref consisting of separate models for each criterion. Bold indicates the highest score per column. process, where GPT-4o first summarizes the evaluation task to increase task comprehension before assigning 04 score. 4. Results Our main results are summarized in Tab. 1, Tab. 2, and Tab. 3, with qualitative examples in Fig. 2. For DreamBench++ (Tab. 1), REFVNLI ranks among the top models for textual alignment, either outperforming all or slightly trailing larger models. For subject consistency, it is among the top-2 models across all categories, and performs best in the Object category, surpassing the next-best GPTFor ImagenHub (Tab. 2), REFVNLI ranks in the top-2 for textual alignment over Animals and achieves the highest score in Objects, surpassing the best non-finetuned model by 4 points. It also excels in the Multi-subject setting, ranking top-3. In subject consistency, REFVNLI continues to rank among the top-2 models for both Animal and Object categories, while performing best in the Multi-subject setting. Finally, for KITTEN (Tab. 3), REFVNLI achieves the highest textual alignment score but struggles with subject consistency. We hypothesize this may be due to its identitysensitive training, which penalizes even minor mismatches in identity-defining traits (2.1). Given that landmarks often have intricate details, this may have led to hypersensitivity to minor discrepancies, as can be observed in Fig. 11. Notably, Fine-tuning exclusively for textual alignment (PaliGemmatext) slightly reduces performance, particularly for Animals and Humans, while training solely for subject consistency (PaliGemmaref) results in more significant drop, with up to 14.8-point decrease for Humans (Tab. 1). This suggests that joint training provides complementary benefits, with subject consistency gaining the most. Qualitative examples in Fig. 2 further illustrate REFVNLIs strengths, highlighting its sensitivity to subtle textual alignment mismatches, such as missing waterfall (top-left) or grassy field (mid-right versus mid-center). For DreamBench++ KITTEN DreamBench++ KITTEN DreamBench++ KITTEN Textual Alignment ImagenHub (Single/Multi) Subject Consistency ImagenHub (Single/Multi) Unified Evaluation ImagenHub (Single/Multi) REFVNLI (ours) reverse classification order multiclass separate classification no markup concatenated images 81.5 80 79.5 79.7 78.4 79. 87.7 / 86.3 85.2 / 85.5 83.7 / 84.7 85.2 / 87.5 87 / 84.3 86.2 / 86.2 97 95.3 94.7 95.8 92.3 93.6 82.7 80.9 79.6 78.3 65.5 74.2 83 / 62.8 84.3 / 68.7 76 / 61.1 77.1 / 56.7 75.9 / 60.8 81.1 / 81.2 82.3 87 86.3 89.2 88.7 89.9 82.1 80.4 79.5 79.0 71.4 76. 85.3 / 72.7 84.7 / 76.2 79.7 / 71 80.9 / 68.8 81.1 / 70.6 83.6 / 83.6 89.0 91.0 90.3 92.4 90.5 91.7 Table 4. Ablation Study: ROC AUC scores for various ablated versions of REFVNLI across benchmarks (over all subjects). The ablations test alternative output formulations, including reversing classification order, using four-label multiclass framework, and incorporating designated token for separate aspect classification. We also examine variant that excludes the subject markup from the input caption, as well as one that concatenates the reference and target images into single image input. Textual Alignment Visual Quality Overall Preference CLIP DINO Crop-IR CLIPScore BLIPScore SigLIP VQAScore VNLI VIEScore DreamBench++ PaliGemmatext/ref REFVNLI 51.9 - - 47.3 39.4 74.6 52.3 60.6 56.4 61.3 87.1 91.3 91.3 87.1 - - - - - 65.1 82.9 82.9 95.5 69.3 - - - - - - - 69.3 78.8 82.9 91.3 Table 5. Results on the ImageRAG rare concepts benchmark, where users compare image pairs and select the better one based on textual alignment, visual quality, and overall preference. We report accuracy in predicting these human choices, with the overall preference score computed as the harmonic mean of the textual and visual scores. The last two rows include models finetuned on our dataset, with PaliGemmatext/ref consisting of separate models for each criterion. Bold indicates the highest score per column. subject preservation, it demonstrates robustness to identityagnostic changes, like zoomed-out parrot (top-center) or different clothing (bottom-center), while remaining sensitive to key identity traits, such as facial features (bottom-left) and color variations (left and middle balloons). Overall, while REFVNLI is not always the top performer, it consistently ranks among the top-3 models. More importantly, it achieves the best balance between textual alignment and subject consistency, consistently leading in the Unified Evaluation score across all benchmarks. 5. Analysis 5.1. Ablation Study To assess the impact of various design decisions in REFVNLI, we run an ablation study examining alternative input and output configurations. On the output side, we test (1) reversing the classification order (subject preservation before textual alignment), (2) 4-label multiclass framework for joint textimage alignment classification, and (3) model that prefixes designated token (TEXT or IMAGE) to the prompt to enable separate classification of each aspect within unified model. For inputs, we explore the effect of removing subject markup from the prompt and of concatenating the imageref and imagetgt instead of processing them separately. Results (Tab. 4) show that reversing the classification order degrades performance, particularly in subject preservation, as does evaluating each aspect separately. This suggests that prioritizing textual alignment helps in subject preservation assessment. The multiclass approach also underperforms compared to our dual binary classification setup, highlighting the benefits of treating each criterion independently. Further, removing subject markup weakens subject preservation, underscoring its role in linking the reference image to the prompt. Lastly, concatenating images instead of processing them separately harms performance, emphasizing the advantage of distinct image inputs. 5.2. Applicability to Rare Entities To assess REFVNLI on unfamiliar subjects, we use the rare entities benchmark from ImageRAG [53], which evaluates generated images based on prompts and reference images of uncommon subjects (e.g., scientific animal names, lesserknown dishes). Human annotators compared image pairs, selecting the better one based on emphTextual Alignment, Visual Quality (focusing on overall depiction rather than subject accuracy), as well as Overall Preference. As shown in Tab. 5, REFVNLI consistently outperforms all baselines in aligning with human preferences across these criteria, showcasing strong robustness to rare subjects.4 This is further supported by Fig. 6, where only REFVNLI repeatedly matches human selections. Notably, in the bottom example, it is the only metric to correctly identify the better option in terms of Visual Quality, despite the challenge of this out-of-distribution case where the superior option is inspired by, rather than preserves the identity of, the reference. 6. Related Work Evaluation of Visual Language Models (VLMs) spans various settings, including visual reasoning [5, 25] and visual question-answering [2, 39, 40]. For text-to-image (T2I) models, assessments normally focus on image quality [16, 52], diversity [48], and alignment with the text [15, 19, 35, 47, 70, 73]. Assessing subject preservation, which is crucial for subject-driven generation, is typically 4TIFA was excluded due to assigning identical scores to 61% of pairs, making accuracy calculations unreliable. Figure 6. ImageRAG Rare Entities Examples: We compare REFVNLI with CLIP and DreamBench++ in aligning with human preferences (top rows of each example) across Textual Alignment (TA), Image Quality (IQ), and Overall Preference (OP). DreamBench++ scores (04) are rescaled to 0100 for readability. The higher of the two criterion-wise scores is emphasized unless both are equal. REFVNLI consistently aligns with human judgments across all three criteria. Notably, in the bottom example, it is the only metric to correctly identify the higher-quality image based on IQ, albeit by small margin. This case is particularly challenging as it is out-of-distribution for REFVNLI, being that the preferred image is inspired by the reference rather than being of the same identity. done using embedding-based metrics like CLIP [47] and DINO [7]. More comprehensive metrics, like VIEScore [27] and DreamBench++ [46], employ GPT-4o [42] to measure both textual alignment and subject consistency. Subject-driven T2I models have gained significant traction in recent years, with some approaches fine-tuning general models into specialist versions that capture specific subjects and styles [10, 29, 43, 51, 57]. Other works enable broader applicability of subjector style-guided image generation, via one-shot examples. These methods typically fall into 2 categories: adapter-based approaches [11, 23, 64, 71], which encode reference images for integration into the diffusion model, and adapter-free methods [14, 37, 38], which directly incorporate extracted features like attention maps. Closely related, image editing complements subjectdriven T2I generation in that the generated images appearance is primarily governed by the input image, with the text only impacting specific aspects (unlike our setting, where the input image only affects one aspect, with text guiding the rest). The task has evolved from pixel-to-pixel translation for predefined transformations [21, 60, 77] to more flexible, text-guided modifications [6, 44, 59], with recent diffusionbased approaches improving precision via cross-attention manipulation [13, 69]. Beyond images, personalized generation extends to other modalities, including videos and texts. Video generation can be conditioned on text [17, 34, 55], reference images [65, 76], or other videos [28]. In text generation, efforts focus on style transfer [50, 74], debiasing [49, 75], and enabling broader semantic control [54, 56, 68]. Lastly, several studies leveraged intra-frame relationships in videos to learn more human-aligned visual representations. These works aim to improve robustness to identity-agnostic variations (e.g., rotation, lighting), by analyzing consecutive frames sourced from public video datasets [24, 45, 61, 62, 67] or captured by cameras on moving agents [1, 22]. 7. Conclusion In this work, we introduced REFVNLI, reliable, costeffective metric for subject-driven text-to-image generation, addressing both textual alignment and subject preservation. Trained on large-scale, auto-generated dataset, REFVNLI balances robustness to identity-agnostic variations (e.g., pose, lighting, background) with sensitivity to identityspecific traits (e.g., facial features, object shape, and unique details) when evaluating subject preservation. For textual alignment, we generate subject-specific prompts, including perturbations for hard negatives, enhancing the models ability to detect subtle mismatches. Evaluations on several benchmarks show that REFVNLI consistently matches or surpasses all baselines, including much larger GPT-4o-based ones, especially for less-common subjects. Future work should enhance REFVNLIs ability to evaluate subject preservation across artistic styles, including when the reference represents style rather than specific entity, as well as handle textual modifications that explicitly alter identity-defining attributes (e.g., changing an objects color). Further improvements should also focus on processing multiple reference images, both for the same subject and for distinct subjects. Overall, by providing cost-efficient and human-aligned evaluation framework, this work aims to drive progress in personalized image generation, fostering more precise and reliable subject-driven T2I methods."
        },
        {
            "title": "References",
            "content": "[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the IEEE international conference on computer vision, pages 3745, 2015. [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. [3] William Berman and Alexander Peysakhovich. Mumu: Bootstrapping multimodal image generation from text-to-image data. arXiv preprint arXiv:2406.18790, 2024. [4] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [5] Nitzan Bitton-Guetta, Aviv Slobodkin, Aviya Maimon, Eliya Habba, Royi Rassin, Yonatan Bitton, Idan Szpektor, Amir Globerson, and Yuval Elovici. Visual riddles: commonsense and world knowledge challenge for large vision and language models. arXiv preprint arXiv:2407.19474, 2024. [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 36, 2024. [9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. [12] Brian Gordon, Yonatan Bitton, Yonatan Shafir, Roopal Garg, Xi Chen, Dani Lischinski, Daniel Cohen-Or, and Idan Szpektor. Mismatch quest: Visual and textual feedback for imagetext misalignment. In European Conference on Computer Vision, pages 310328. Springer, 2024. [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [14] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared atIn Proceedings of the IEEE/CVF Conference on tention. Computer Vision and Pattern Recognition, pages 47754785, 2024. [15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [18] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Reevaluating factual consistency evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39053920, 2022. [19] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2040620417, 2023. [20] Hsin-Ping Huang, Xinyi Wang, Yonatan Bitton, Hagai Taitelbaum, Gaurav Singh Tomar, Ming-Wei Chang, Xuhui Jia, Kelvin C. K. Chan, Hexiang Hu, Yu-Chuan Su, and MingHsuan Yang. Kitten: knowledge-intensive evaluation of image generation on visual entities, 2024. [21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. [22] Dinesh Jayaraman and Kristen Grauman. Learning image representations tied to ego-motion. In Proceedings of the IEEE International Conference on Computer Vision, pages 14131421, 2015. [23] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. [24] SouYoung Jin, Aruni RoyChowdhury, Huaizu Jiang, Ashish Singh, Aditya Prasad, Deep Chakraborty, and Erik LearnedMiller. Unsupervised hard example mining from videos for improved object detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 307324, 2018. [25] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [26] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models, 2024. URL https://arxiv. org/abs/2310.01596. [27] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. VIEScore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, Bangkok, Thailand, 2024. Association for Computational Linguistics. [28] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. [29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. [31] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. TVQA+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 82118225, Online, 2020. Association for Computational Linguistics. [32] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. [34] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In Proceedings of the AAAI conference on artificial intelligence, 2018. [35] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2024. [36] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. [37] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: concept neurons in diffusion models for customized generation. In Proceedings of the 40th International Conference on Machine Learning, pages 2154821566, 2023. [38] Henglei Lv, Jiayu Xiao, and Liang Li. Pick-and-draw: Training-free semantic guidance for text-to-image personIn Proceedings of the 32nd ACM International alization. Conference on Multimedia, pages 1053510543, 2024. [39] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [40] Thomas Mensink, Uijlings, Lluıs Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, Araujo, and Vittorio Ferrari. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories. in 2023 ieee. In CVF International Conference on Computer Vision (ICCV), pages 30903101, 2023. [41] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European Conference on Computer Vision, pages 728755. Springer, 2022. [42] OpenAI. Gpt-4 technical report, 2024. [43] Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, and Min Song. Cat: Contrastive adapter training for personalized image generation. arXiv preprint arXiv:2404.07554, 2024. [44] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. [45] Nikhil Parthasarathy, SM Eslami, Joao Carreira, and Olivier Henaff. Self-supervised video pretraining yields robust and more human-aligned visual representations. Advances in Neural Information Processing Systems, 36:6574365765, 2023. [46] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation, 2024. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [48] Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, and Yoav Goldberg. Grade: Quantifying sample diversity in text-to-image models. arXiv preprint arXiv:2410.22592, 2024. [49] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 72377256, 2020. [50] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. recipe for arbitrary text style transfer with large language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837848, 2022. [51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [52] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [53] Rotem Shalev-Arkushin, Rinon Gal, Amit Bermano, and Ohad Fried. Imagerag: Dynamic image retrieval for referenceguided image generation. arXiv preprint arXiv:2502.09411, 2025. [54] Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Ido Dagan, and Yael Amsterdamer. Interactive query-assisted summarization via deep reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25512568, Seattle, United States, 2022. Association for Computational Linguistics. [55] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [56] Aviv Slobodkin, Niv Nachum, Shmuel Amar, Ori Shapira, and Ido Dagan. SummHelper: Collaborative human-computer summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 554565, Singapore, 2023. Association for Computational Linguistics. [57] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, et al. Styledrop: Text-to-image synthesis of any style. Advances in Neural Information Processing Systems, 36:6686066889, 2023. [58] Gemini Team. Gemini: family of highly capable multimodal models, 2024. [59] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-toIn Proceedings of the IEEE/CVF Conimage translation. ference on Computer Vision and Pattern Recognition, pages 19211930, 2023. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87988807, 2018. [61] Xiaolong Wang and Abhinav Gupta. Unsupervised learning In Proceedings of of visual representations using videos. the IEEE international conference on computer vision, pages 27942802, 2015. [62] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual representation learning. In Proceedings of the IEEE international conference on computer vision, pages 13291338, 2017. [63] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, and Furong Huang. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 416442, Bangkok, Thailand, 2024. Association for Computational Linguistics. [64] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. [65] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. [66] Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectmate: recurrence prior for object insertion and subject-driven generation. arXiv preprint arXiv:2412.08645, 2024. [67] Haiping Wu and Xiaolong Wang. Contrastive learning of image representations with cross-video cycle-consistency. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1014910159, 2021. [68] Yujia Xie, Xun Wang, Si-Qing Chen, Wayne Xiong, and Pengcheng He. Interactive editing for text summarization. arXiv preprint arXiv:2306.03067, 2023. [69] Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing crossattention leakage for text-based image editing. Advances in Neural Information Processing Systems, 36:2629126303, 2023. [70] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving textimage alignment evaluation. Advances in Neural Information Processing Systems, 36:16011619, 2023. [71] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. [60] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. [72] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. Alignscore: Evaluating factual consistency with unified alignment function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132811348, 2023. [73] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [74] Chiyu Zhang, Honglong Cai, Yuexin Wu, Le Hou, Muhammad Abdul-Mageed, et al. Distilling text style transfer with self-explanation from llms. arXiv preprint arXiv:2403.01106, 2024. [75] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018. [76] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Nanxuan Zhao, Jing Shi, and Tong Sun. Sugar: Subject-driven video customization in zero-shot manner. arXiv preprint arXiv:2412.10533, 2024. [77] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent In Proceedings of the IEEE interadversarial networks. national conference on computer vision, pages 22232232, 2017. REFVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "9.2. Generation of prompt-imagetgt Hard-negatives Fig. 7 showcases the prompt used to generated the promptimagetgt hard negatives. 10. Additional Qualitative Examples for Subject Preservation Evaluation In Fig. 8, Fig. 9, Fig. 10, and Fig. 11 we present additional qualitative examples of subject preservation evaluation for the Animal, Human, Object, and Landmark categories, respectively. Context Misalignment Injection Instructions (Short Captions) 1. Understand the Caption: Carefully read the short caption to fully grasp the scene it describes. 2. Identify and Swap: Select single visual detail within the caption to modify. Replace this detail with different, incorrect, but still plausible visual detail. For example, you might change color, an object, or location. Do not modify the underlined entity (if any). 3. Apply the Tags: Enclose the original visual detail within <swap> tags. Immediately after the closing </swap> tag, write the new, incorrect visual detail. There should be no space between the closing </swap> and the new word. Example: If the original sentence is The cat sat on the red mat, and you want to change red to blue, the result should be: The cat sat on the <swap>red</swap><blue> mat. 4. Final Check: Ensure the modified caption is grammatically correct and reads naturally, even though it now contains factual error. The sentence should be internally logical, despite contradicting the actual visual content. Again, ensure the underlined entity (if any) remains completely unchanged. Few-Shot Here are some examples: INPUT: woman is sitting in living room, and <u>she</u> is looking at something with concerned expression OUTPUT: woman is sitting in </swap>living room</swap><kitchen>, and <u>she</u> is looking at something with concerned expression. INPUT: Two men are sitting on leather couch in living room. One <u>man</u> is sitting on the left side of the couch, looking at laptop. The other man is sitting on the right side of the couch, talking on phone. The room is decorated with various items, including large model of spaceship. OUTPUT: Two men are sitting on leather couch in living room. One <u>man</u> is sitting on the left side of the couch, looking at laptop. The other man is sitting on the right side of the couch, talking on phone. The room is decorated with various items, including large model of <swap>spaceship</swap><sailboat>. Now its your turn! Follow the instructions. Answer only with the corrupted sentence, Dont forget to add the tags. INPUT: lizard is perched on rock, surrounded by other rocks and foliage. The <u>lizard</u> is facing the camera, with its head raised and its tail curled behind it. OUTPUT: Generated OUTPUT: lizard is perched on <swap>rock</swap><branch>, surrounded by other rocks and foliage. The <u>lizard</u> is facing the camera, with its head raised and its tail curled behind it. Figure 7. Hard Negative Caption Generation. This figure illustrates the prompting strategy used to generate hard negative captions, containing single, plausible but factually incorrect visual detail, for enhanced misalignment detection. 8. Reproducibility We fine-tuned PaliGemma [4], on balanced version of our dataset (by undersampling the more common labels). We finetuning took 24 hours, using 2 NVIDIA A100 cards, each with 80GiB memory. 9. Further Information on the Data Construction Pipeline 9.1. Collection of Subject-driven Image Pairs When collecting the subject-driven image pairs, we added several filtering operations to reduce noise. These included removing blurred images, as well as those not portraying the subject, fur which we used Gemini [58]. Additionally, for the dataset sourced from TVQA+, we also filtered images containing subtitles and credits, also using Gemini. Figure 8. Qualitative Examples of Subject Preservation Evaluation for the Animal Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability. Figure 9. Qualitative Examples of Subject Preservation Evaluation for the Human Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability. Figure 10. Qualitative Examples of Subject Preservation Evaluation for the Object Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability. Figure 11. Qualitative Examples of Subject Preservation Evaluation for the Landmark Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability."
        }
    ],
    "affiliations": [
        "Ben Gurion University",
        "Google Research"
    ]
}