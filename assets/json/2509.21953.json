{
    "paper_title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning",
    "authors": [
        "Tao Wu",
        "Yibo Jiang",
        "Yehao Lu",
        "Zhizhong Wang",
        "Zeyi Huang",
        "Zequn Qin",
        "Xi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 5 9 1 2 . 9 0 5 2 : r arXiv Preprint MULTICRAFTER: HIGH-FIDELITY MULTI-SUBJECT GENERATION VIA DISENTANGLED ATTENTION AND IDENTITY-AWARE PREFERENCE ALIGNMENT Tao Wu1, Yibo Jiang2, Yehao Lu1, Zhizhong Wang3, Zeyi Huang3, Zequn Qin2, Xi Li1 1College of Computer Science and Technology, Zhejiang University 2School of Software Technology, Zhejiang University 3Huawei Technologies Ltd Figure 1: MultiCrafter enables multi-subject personalization. Inputs are surrounded by squares."
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-subject image generation aims to synthesize user-provided subjects in single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of Equal contribution. Corresponding authors. 1 arXiv Preprint attribute leakage is significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ Mixture-of-Experts architecture to enhance the models capacity, allowing different experts to focus on different scenarios. Finally, we design novel online reinforcement learning framework to align the model with human preferences, featuring scoring mechanism to accurately assess multi-subject fidelity and more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better."
        },
        {
            "title": "INTRODUCTION",
            "content": "Subject-driven image generation, which aims to create images featuring user-provided subjects, has become cornerstone of personalized content creation. Propelled by higher-quality data and the widespread adoption of Diffusion Transformers (Labs, 2024; Esser et al., 2024; Gao et al., 2025; Li et al., 2024b), text-to-image models have seen rapid advancements. This progress has significantly enhanced single-subject image generation, where models now excel at preserving subject fidelity (Li et al., 2025d; Feng et al., 2025; He et al., 2025). Among the various techniques, the In-ContextLearning (ICL) paradigm (Huang et al., 2024a;b;c; Tan et al., 2024; Liang et al., 2025) has emerged as mainstream approach. Its high adaptability to the transformer architecture and strong capability for maintaining subject fidelity have made it particularly effective for single-subject tasks. However, extending this success from single-subject to multi-subject generation is not trivial. Building on the ICL framework, recent works like UNO (Wu et al., 2025c) and OmniGen (Xiao et al., 2025) have ventured into this complex multi-subject customization task. Despite their efforts, these methods often produce suboptimal results, especially when dealing with intricate subjects like human faces. They frequently suffer from suboptimal results, such as identity fusion and attribute leakage between subjects. This raises critical question that forms the very foundation of our work: why do existing ICL-based methods falter in the multi-subject setting? The primary challenge stems from the training objective. Existing ICL-based methods are typically optimized with simple reconstruction loss. This objective implicitly tasks the model with the dual responsibilities of distinguishing subject features and arranging them spatially. However, such supervision alone proves insufficient for the complexities of multi-subject scenarios. As shown in Fig. 2, this inadequacy leads to an undesired entanglement between subject-specific attention fields in these methods, like UNO. This phenomenon, which we term attention bleeding, causes attribute leakage and severely damages subject fidelity. Furthermore, simple reconstruction objective fails to capture nuanced human preferences, such as aesthetic quality and precise prompt alignment. To address these limitations, we introduce MultiCrafter, framework that achieves high-fidelity, preference-aligned multi-subject image generation through three key innovations. To address the attribute leakage caused by attention bleeding, we propose an Identity-Disentangled Attention Regularization. This mechanism applies explicit positional supervision only during the training phase to double blocks in FLUX (Labs, 2024), which are pivotal regions for feature injection and spatial control. This compels the model to distinguish between different subject features and learn distinct, disentanglement attention regions for each subject, drastically reducing attribute leakage. Considering single model struggles to cope with various attention layouts caused by diverse range of subjects and prompts, we enhance its capacity by incorporating Mixture-of-Experts (MoE) architecture. Inspired by MoE-LoRAs success in multitask tuning (Feng et al., 2024; Zhang et al., 2025), our Efficient Adaptive Expert Tuning allows different expert networks to specialize in varied scenarios, dynamically selected via routing mechanism. This ensure our method to maintain excellent subject fidelity in various scenarios without an increase in inference complexity. For aligning with human aesthetic and semantic preferences while ensuring subject fidelity, we design novel online reinforcement learning framework. We introduce Identity-Preserving Preference Optimization that aligns the model across three axes: aesthetic quality, text-image alignment, and subject fidelity. To accurately measure subject fidelity, we introduce Multi-ID Alignment Reward, which use the Hungarian matching algorithm to maximize the overall match quality be2 arXiv Preprint Figure 2: Visual comparison of attention maps. The ICL-based method UNO, which only relies on reconstruction loss (left), fails to preserve the subject fidelity, where the double blocks attention regions for each subject are entangled, leading to attribute leakage. Our method overcomes this problem and maintains subject fidelity. tween multiple generated subjects and their references for precise scoring. Besides, we introduce Group Sequence Policy Optimization (GSPO) Zheng et al. (2025) to adapt to our MoE-LoRA design, thereby avoiding the core issue of training instability caused by expert-activation volatility inherent in MoE architectures. Our experiments demonstrate that MultiCrafter achieves significant improvements over existing methods. Our main contributions are as follows: We propose explicit positional supervision that disentangles attention across subjects, thereby reducing attribute leakage and enhancing subject fidelity. We integrate an MoE-LoRA architecture that increases capacity for diverse subjects and spatial layouts while maintaining inference efficiency. We design the first online reinforcement learning framework tailored for multi-subject generation, introducing Multi-ID Alignment Reward and GSPO for stable and preferencealigned training."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Subject-driven Generation has attracted increasing attention with the development of diffusion models and multimodal (Chen et al., 2023; Han et al., 2023; Shi et al., 2024; Ruiz et al., 2024; Wu et al., 2024a;b; 2025d; Hua et al., 2023; Han et al., 2024; Liu et al., 2023a;b; Dou et al., 2024; Gu et al., 2024; Feng et al., 2025; Ma et al., 2025b; 2024; 2025a; Zhao et al., 2024; Li et al., 2025c). These works can be broadly categorized into two types. (1) Fine-tuning-based methods, includes methods such as Textual Inversion (Gal et al., 2022), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al., 2023). These approaches achieve customization by fine-tuning part of the models parameters. (2) Tuning-free methods, like IP-Adapter (Ye et al., 2023), InstantID (Wang et al., 2024), and PhotoMaker (Li et al., 2024a). These approaches leverage large-scale training to eliminate the need for retraining when the subject changes. However, customized generation of multiple subjects from single image introduces new challenges, especially in maintaining individual subject fidelity and mitigating attribute entanglement. Recently, the powerful capabilities of foundation models based on the DiT (Peebles & Xie, 2023) architecture have greatly enhanced the generation of multiple subjects, leading to the emergence of series of works such as HunyuanCustom (Hu et al., 2025), OmniControl (Tan et al., 2024), UniReal (Chen et al., 2025b), UNO (Wu et al., 2025c), DreamO (Mou et al., 2025) and XVerse (Chen et al., 2025a). Reinforcement Learning for Text-to-Image Generation has become an active area of research. Taking advantage of the development of reinforcement learning technology Wang et al. (2025a); Pang et al. (2023); Wang et al. (2025b), initial strategies included policy gradient methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017; Black et al., 2023; Fan et al., 2023; Gupta et al., 2025; Miao et al., 2024; Zhao et al., 2025). subsequent major development is the adoption of Direct Preference Optimization (DPO) and its variants (Wallace et al., 2024; Yang et al., 2024; Yuan et al., 2024; Liu et al., 2025b; Zhang et al., 2024; Furuta et al., 2024; Li et al., 2025a; Wang et al., 2025b). Some recent works have introduced online RL technology, Group Relative Policy Optimization (GRPO) Shao et al. (2024), into Text-to-Image Generation, achieving significant performance gains. Flow-GRPO (Liu et al., 2025a), DanceGRPO (Xue et al., 2025) introduce exploration by reformulating the deterministic Ordinary Differential Equation (ODE) of flow-matching models into Stochastic Differential Equation (SDE). The improved MixGRPO (Li et al., 2025b) 3 arXiv Preprint further boosts training efficiency with mixed ODE-SDE framework. However, these methods have been limited to the basic text-to-image task, leaving the application of online reinforcement learning to multi-subject driven generation largely unexplored."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Flow Matching. Flow Matching (Lipman et al., 2022) is gradually replacing DDPM as the mainstream for text-to-image models due to its more efficient sampling strategies. These models typically first train an autoencoder (consisting of an encoder and decoder D) to obtain the latent space representation z0 = E(x) of an image x. Let z0 be data sample, ϵ (0, 1) is the Gaussian noise, and ctext be the prompt of this image. Flow Matching formulates generation as continuous transformation along an Ordinary Differential Equation (ODE), dzt dt = v(zt, t), [0, 1], which deterministically maps noise to data. The interpolated data at time is zt = (1 t)z0 + tϵ. neural network vθ(xt, t) is trained to approximate the velocity field of this ODE, with the objective Ldif = Et,z0,ϵN (0,1)v vθ(zt, t, ctext)2, = z0 ϵ. (1) The DiT framework and Flow Matching are widely used in recent diffusion models, such as Stable Diffusion3 (Esser et al., 2024) and Flux (Labs, 2024). In this paper, we use Flux as our base model. Group Relative Policy Optimization (GRPO). GRPO struggles with ODE-based flow models because their deterministic systems lack the inherent stochasticity essential for reinforcement learning frameworks. So DanceGRPO Xue et al. (2025) and Flow-GRPO Liu et al. (2025a) convert ODEs to SDEs, enabling stochastic reinforcement learning for image generation. MixGRPO Li et al. (2025b) further improves efficiency by applying this only within sliding time window during training. Given the prompt ctext, the training process in MixGRPO is similar to Flow-GRPO and DanceGRPO, but only optimizes the time steps sampled within the interval S. The behavior of this window is governed by key hyperparameters: the window size sets the number of consecutive timesteps to optimize at once, the shift interval τ determines how many training iterations pass before the window moves, and the window stride specifies how many timesteps the window advances during shift. The final training objective is given by: J(θ) = cctext,{xT }N i=0πθold (c) (cid:34) 1 N (cid:88) i=1 1 (cid:88) tS (cid:16) min i(θ)Ai, clip(rt rt i(θ), 1 β, 1 + β)Ai (cid:35) (cid:17) , (2) i(θ) is the policy ratio and Ai is the advantage score. β is hyperparameter that serves to where rt clip the policy ratio, ensuring stable updates, and rt i(θ) = πθ(xt+1 xt, c) πθold(xt+1 xt, c) , Ai = R(xT , c) mean{R(xT , c)}N std{R(xT i=1 , c)}N i=1 , (3) where πθ(xt+1 xt, c) is the policy function and R(x0 , c) is provided by the reward model."
        },
        {
            "title": "4 METHODS",
            "content": "In this section, we first explore the underlying causes of feature entanglement in multi-subject customized generation and present the overall framework of our method in Sec. 4.1. Then, in Sec. 4.2, we introduce Identity-Disentangled Attention Regularization to address attribute leakage issues arising from attention bleeding. To enhance the models ability to maintain subject fidelity in different scenarios, we introduce Efficient Adaptive Expert Tuning in Sec. 4.3. Lastly, we incorporate Identity-Preserving Preference Optimization in Sec. 4.4, leveraging reinforcement learning to align the model with human preferences. 4.1 EXPLORE THE IN-COTEXT-LEARNING Given reference images of different subjects, we aim to enable the model to generate images of these subjects according to text prompt. Existing ICL-based methods encodes the subject images through an encoder to obtain latent space feature of these subjects, = {zref }N i=1, where 4 arXiv Preprint Figure 3: Overall pipeline of MultiCrafter. Our framework is built on three core innovations: (Top Left) Identity-Disentangled Attention Regularization uses positional supervision to prevent attribute leakage; (Top Right) the MoE-LORA architecture boosts model capacity for diverse scenarios; and (Bottom) the IdentityPreserving Preference Alignment framework employs novel online reinforcement learning strategy with Multi-ID Alignment Reward and the stable GSPO algorithm to align the model with human preferences. each zref Rcwh, (w, h) is spatial size, is the number of channels. However, we find that when multiple subjects are input, especially when the attributes of the subjects themselves are similar, it is easy to cause confusion between the attributes of different subjects, resulting in decrease in subject fidelity. To investigate the underlying reasons, we leveraged representative method (Wu et al., 2025c) to visualize the attention maps between each reference feature zref and the feature of the generated image ˆz. Since the base model Flux comprises both double block and single block structures, we visualized the attention maps for each block type separately. As shown on the left of Fig. 2, we identify two key phenomena. First, the double blocks of Flux are far more pivotal in determining the spatial layout of the reference subject than the single block. Second, methods trained solely on reconstruction loss lead to an undesired entanglement between subject-specific attention fields, causing attribute leakage and severely compromising subject fidelity. We can therefore infer critical condition for high fidelity: the peak response within the double blocks attention scores, corresponding to specific subject, must consistently align with that subjects spatial region in the generated output. We leverage this in our method. Furthermore, this simple supervision method fails to account for human preferences adequately. Therefore, we introduce reinforcement learning as post-training to align human preferences. 4.2 IDENTITY-DISENTANGLED ATTENTION REGULARIZATION Based on that, the subject fidelity can be enhanced by strictly aligning the hot spot areas of the attention scores of double blocks with the spatial positions of the corresponding subjects in the generated image. We design simple but effective regularization. Specifically, during the training process, for the latent space feature zref of the i-th reference subject. Following (Wu et al., 2025c), we partition the reference feature zref into patches and apply positional encoding, resulting in Rlc, is the number of tokens, is the number of channels. Then, we sequence of 1D tokens zr can obtain its attention map with the generated content zt at the k-th double block: mi = Softmax( Qk,iKT ), 5 (4) arXiv Preprint where Qk,i Rlc is the query generated from the i-th subject image within the k-th double block, Rltc is the key produced by the noisy image latent tokens in the current layer, and lt denotes the number of tokens in the noisy image latent. For model with double blocks, we can obtain the attention maps corresponding to the i-th subject from all blocks, which are aggregated into set {mi K}. We then average and normalize this set to obtain the mean attention map ˆMi. By pre-annotating the training data, we obtain the ground-truth mask Mi corresponding to the i-th subject within the generated image. Notably, for human subjects, we exclusively use the facial region as the reference image and similarly focus only on the facial area for the generated image. 2, ..., mi 1, mi Finally, we employ the dice loss (Milletari et al., 2016), standard loss function for segmentation tasks, to minimize the discrepancy between each ground-truth mask Mi and the corresponding mean attention map ˆMi. The formulation is as follows: Lattn = (cid:32) 1 (cid:88) i= 2 (cid:80) (cid:80) j( ˆMi,j Mi,j) + ϵ ˆMi,j + (cid:80) Mi,j + ϵ (cid:33) , (5) where the index iterates over all spatial locations of the maps, and ϵ is small constant added for numerical stability. By minimizing this attention regularization loss Lattn, we explicitly encourage the models attention mechanism to concentrate on the precise spatial regions occupied by each subject. This forces spatial disentanglement of subjects within the attention maps. This avoids attribute leakage and improves the subject fidelity of the generated image. The final loss function of our framework is defined as (λ is factor that balances the loss weight): = Ldif + λ Lattn. (6) 4.3 EFFICIENT ADAPTIVE EXPERT TUNING For efficient training, existing ICL-based methods usually use LoRA to fine-tune the model. But the significant variance in the spatial layout of subjects across various subjects and prompts poses challenge for standard LoRA, whose limited capacity is insufficient for diverse subject-driven generation. To address this, and inspired by the success of MoE-LoRA in multitask tuning (Feng et al., 2024; Liu et al., 2024; Gou et al., 2023), we adopt an MoE-LoRA architecture to expand model capacity without substantial increase in inference overhead. This enables different experts to focus on spatial layout for variety of scenarios, effectively tackling the challenge of scene diversity. We strategically integrate the MoE-LoRA into the output feed-forward network (FFN) layers of the Flux, while other layers are adapted using standard LoRA for parameter efficiency. Specifically, given an input vector to the FFN layer, we define the number of experts as Ne, the rank of each LoRA as r, and the scaling factor as α. lightweight gating network, gθ, which dynamically routes the input vector to the most suitable experts, computes vector of logits, RNe , for the experts: = Softmax(TopK(Wg h, k)) where Wg RNedin is the weight of the gating network. TopK(, k) enforces sparsity by retaining only the top logit values and masking the others to , thus activating only small subset of experts. Each of the Ne experts is an independent LoRA module, parameterized by matrices Rdoutr. The final output of the MoE-LoRA layer, hout, is computed by adding the weighted sum of the selected experts outputs to the output of the original FFN layer: Rrdin and hout = FFN(h) + Ne(cid:88) i=1 pi (cid:16) α i (cid:17) . 4. IDENTITY-PRESERVING PREFERENCE OPTIMIZATION To further enhance the generation quality and align with human preferences, we introduce posttraining stage using reinforcement learning. This final stage aims to refine aesthetic appeal and text-image alignment without compromising the subject fidelity. We adapt the efficient MixGRPO framework (Li et al., 2025b), which confines stochastic optimization to sliding window S. However, standard GRPO with its token-level policy ratios can exhibit instability, particularly when training MoE models due to expert routing fluctuations (Zheng et al., 2025). To mitigate this and 6 arXiv Preprint better suit our MoE-LoRA architecture from Sec. 4.3, we replace the GRPO objective with the more stable Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025). Specifically, we replace the token-level policy ratio in Eq. (3) with sequence-level policy ratio si(θ) defined over the denoising steps within the sliding window S: (cid:32) si(θ) = exp 1 (cid:88) tS log πθ(xt+1xt, c, Z) πθold(xt+1xt, c, Z) (cid:33) . (7) This sequence-level ratio reflects the overall policy shift for the entire sequence within the optimization window, leading to more stable gradients. The advantage score Ai is calculated as in Eq. (3), but using our composite reward from Eq. (9). The final optimization objective is thus: (cid:34) (cid:35) JMixGSPO(θ) = c,Z,{xT }N i=0πθold (c,Z) min si(θ)Ai, clip(si(θ), 1 β, 1 + β)Ai 1 N (cid:88) i=1 (cid:16) (cid:17) . (8) Besides, good reward model is very important for online reinforcement learning. We construct reward model based on three dimensions: aesthetics, text alignment, and subject fidelity. The total , c, Z) for generated image xT reward R(xT given text prompt ctext and set of reference subject latents is weighted sum of three scores: R(xT , c, Z) = wtextRtext + waesRaes + widRid, (9) where Rtext is the text alignment reward from pre-trained CLIP model, Raes is an aesthetic reward from predictor like HPSv2 (Wu et al., 2023), Rid is used to evaluate subject fidelity, and wid, wtext, waes are their corresponding weights, . To accurately measure the subject fidelity of multi-subject generation results, we built the Multi-ID Alignment Reward using the Hungarian matching algorithm. For human subjects, we first employ face detector (Deng et al., 2020) to extract facial embeddings from each reference image. We then apply the same detector to the generated image to identify all present faces and extract their embeddings. Then we construct pairwise similarity matrix where Cij is the cosine similarity between the embedding of the i-th reference face and the j-th detected face. The Hungarian algorithm is then used to solve the assignment problem by finding an assignment matrix {0, 1}Nref Ngen that maximizes the total similarity: max Nref (cid:88) Ngen (cid:88) i=1 j=1 CijXij s.t. Ngen (cid:88) j=1 Xij 1, Nref (cid:88) i=1 Xij 1. (10) where Nref and Ngen are the number of reference and generated faces, respectively. These ensure each face is matched at most once, preventing reward hacking, stopping the model from using attribute leakage to generate multiple average faces\" for an unearned high reward. For object subjects, each reference object is pre-annotated with text prompt. We leverage Florence-2 (Xiao et al., 2024) and SAM2 (Ravi et al., 2024) to locate the corresponding object in the generated image. Then we compute the cosine similarity between the DINOv2 (Oquab et al., 2023) embeddings of the segmented region and the reference object. We provide the complete pseudocode at Sec. A."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Implementation Details. To achieve precise multi-subject customized generation, our training process is divided into three stages: Single-Subject Pre-training, Multi-Subject Customization Training, and Identity-Preserving Preference Optimization. Following (Wu et al., 2025c), we set the resolution of generated images to 512 512 , the resolution of reference images to 320 320, and each LoRA modules rank to = 512. For the MoE-LoRA applied to the FFN layers, we configure it with 4 experts and activate 1 expert per forward pass. For reinforcement learning, we configure with sampling step 16, window size of = 2, shift interval of τ = 50, and window stride of = 1. More details settings can be found in Sec. B. Datasets and BenchMark. We constructed separate datasets for multi-human and multi-object generation. Due to the scarcity of public multi-human customization datasets with adequate annotations, we designed data collection pipeline to curate new dataset from videos for multi-human 7 arXiv Preprint Methods MS-Diffusion MIP-Adapter OmniGen UNO OmniGen2 DreamO XVerse Ours Multi Human Genertaion Multi Object Generation Overall CLIP-T Face-Sim DINO-I CLIP-I HPS CLIP-T DINO-I CLIP-I HPS AVG 0.2498 0.2631 0.2741 0.2645 0.2837 0.2747 0.2591 0.2753 0.0945 0.2117 0.3238 0.1474 0.2453 0.3345 0.4117 0.5284 0.4767 0.6959 0.7225 0.5972 0.6788 0.7441 0.7665 0. 0.5801 0.7140 0.7642 0.6489 0.7205 0.7988 0.8027 0.8524 0.2461 0.2791 0.2996 0.2954 0.3103 0.3056 0.2498 0.2915 0.2887 0.2984 0.3151 0.3259 0.3310 0.3207 0.2981 0.3380 0.4002 0.5470 0.6727 0.7374 0.7538 0.7394 0.7449 0.7824 0.6681 0.7776 0.8085 0.8392 0.8470 0.8393 0.8456 0.8608 0.2685 0.2374 0.2561 0.2676 0.2872 0.2637 0.2595 0. 0.3636 0.4471 0.4930 0.4582 0.4953 0.5134 0.5153 0.5592 Table 1: Quantitative comparison with state-of-the-art methods on the multi-human and multi-object generation. The best results are in bold, and the second-best are underlined. Our method outperforms others, especially in subject fidelity metrics, and achieves the highest overall average score. customization. We supplemented the details in the Sec. C. total of 200k pairs of cross-pair data were obtained for training. For the reinforcement learning stage, we curated face collection of 80 celebrities and 80 non-celebrities. We paired the faces in the collection and used Qwen2.5-VL (Bai et al., 2025) to generate prompt for each pair, resulting in 12,720 data points. We randomly selected 1,000 of these data points as our benchmark, and the remaining data was used as reinforcement learning training data. For multi-object customization, we use the public MUSAR-Gen (Guo et al., 2025) Dataset as our foundation. We use Florence-2 and SAM to obtain the detailed positions of reference objects. To ensure segmentation quality, we further employ Qwen2.5-VL to filter the results. We split 1,000 samples from this dataset that were not seen during training as benchmark. Evaluation metrics. Following the (Le et al., 2025; Mou et al., 2025), we evaluate generated image quality using standard metrics. We calculate the cosine similarity between the prompt and the image CLIP (Radford et al., 2021) embeddings (CLIP-T) to evaluate text fidelity. We also use HPSv2 for aesthetic and human preference scoring. For subject fidelity, we employ cosine similarity measures between generated images and reference images within CLIP and DINO (Zhang et al., 2022) spaces, referred to as CLIP-I and DINO-I scores, respectively. Note that in order to accurately calculate CLIP-I and DINO-I, we first use face detector (Deng et al., 2019) or Florence-2 to accurately locate the position of subjects in the generated image, and then calculate them. Additionally, for multi-human generation, we incorporate Face Similarity (Face-Sim) (Deng et al., 2019) and the Hungarian algorithm, enabling them to assess subject fidelity more accurately. 5.2 QUANTITATIVE COMPARISON We conducted comprehensive comparisons with existing methods in both multi-subject and multiobject customized generation. These methods include MS-Diffusion (Wang et al., 2025c), MIPAdapter (Zhong et al., 2025), OmniGen (Xiao et al., 2025), UNO (Wu et al., 2025c), OmniGen2 (Wu et al., 2025b), DreamO (Mou et al., 2025), and XVerse (Chen et al., 2025a). The results, as shown in Tab. 1, demonstrate that our method achieves significant improvements over existing approaches, particularly in terms of subject fidelity. In the multi-human generation task, our model shows commanding lead in identity preservation, achieving the top scores in subject fidelity. Notably, the significant 28.3% relative improvement in the Face-Sim over the next-best method highlights our models ability to effectively distinguish between different subjects and preserve their individual detailed features. This is crucial for accurately and reliably accomplishing the highly sensitive and challenging task of multi-human customization. Concurrently, the model remains highly competitive in text-image alignment, achieving strong balance between subject fidelity and prompt alignment. This outstanding performance extends to the multi-object generation benchmark, where our method again secures the top ranks in text alignment and subject fidelity, validating its robustness and versatility. While HPS score tends to vibrant yet unnatural (\"oily\") outputs, our model is specifically trained for multi-human customization with an emphasis on photorealism. To preserve this quality, we follow Xue et al. (2025) and employ the CLIP Score during reinforcement learning. Therefore, we only achieve competitive objective scoring results in aesthetics, but we can generate more realistic results. Ultimately, by achieving the best overall score, our method achieves significant improvements over existing methods, validating the effectiveness of our framework. 8 arXiv Preprint Figure 4: Qualitative comparison with existing methods on the multi-human generation (Zoom in for best visual comparison). Our method significantly improves subject fidelity. IDAR MoE-LoRA IPPO CLIP-T Face-Sim DINO-I CLIP-I HPS 0.2645 0.2637 0.2674 0.2753 0.1474 0.4983 0.5154 0.5284 0.5972 0.7953 0.8107 0.8294 0.6489 0.8032 0.8480 0. 0.2954 0.2653 0.2661 0.2915 Table 2: Ablation study of the core module of MultiCrafter. IDAR is Identity-Disentangled Attention Regularization (Sec. 4.2), MoE-LoRA is our Efficient Adaptive Expert Tuning (Sec. 4.3) , and IPPO is our IdentityPreserving Preference Optimization (Sec. 4.4). The results demonstrate the effectiveness of each module. 5.3 QUALITATIVE COMPARISON We provide qualitative comparison against existing methods on the more challenging task of multihuman customized generation in Fig. 4. As shown in the first two rows of Fig. 4, methods trained directly with In-Context Learning, such as UNO, OmniGen, and OmniGen2, struggle with attribute confusion when generating subjects of the same gender, which degrades subject fidelity. In contrast, our method accurately preserves the unique features of each individual. Our method maintains strong subject fidelity even in interactive scenarios, as shown in the third row of Fig. 4. This result validates the effectiveness of our framework. Aesthetically, while DreamO and OmniGen2 produce vibrantly colored images, our approach generates images with higher degree of realism. Additional qualitative comparisons for both multi-subject generation (includes both human and object) are provided in the Secs. to I. 5.4 ABLATION STUDIES We conduct detailed ablation studies to validate the effectiveness of each component of our proposed framework. Since the multi-human customization evaluation metrics is more accurate, we conducted an ablation experiment on multi-human customization. We use UNO as our baseline for compari9 arXiv Preprint son, and the detailed results are presented in Tab. 2. The baseline model relies solely on simple reconstruction loss. As shown in Tab. 2, the base model performs poorly in terms of subject fidelity, achieving Face-Sim score of only 0.1474. This confirms that simple objective is insufficient for handling complex multi-subject scenarios. Upon introducing our Identity-Disentangled Attention Regularization (IDAR), we observe dramatic improvement across all subject fidelity metrics. As shown on the left of Fig. 2, our method successfully separates the attention of different subjects. This result underscores the critical role of our IDAR in explicitly disentangling subject features and preventing attribute leakage. We attribute the slight decrease in the HPS score to the quality of our training dataset, point we discuss further in the Sec. E. Building on this, we integrate the MoE-LoRA architecture, which yields further gains across subject fidelity while also improving text alignment. This demonstrates that our strategy of using MoE to allow different experts to specialize in handling diverse spatial layouts is effective in enhancing generation quality. Finally, IdentityPreserving Preference Optimization (IPPO) further boosts subject fidelity. More importantly, it significantly enhances alignment with human preferences, leading to notable improvements in both text alignment and aesthetic scores. This shows that our IPPO successfully aligns the models output with human preferences without compromising the subject fidelity."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduced MultiCrafter, novel framework designed to address the critical challenges of feature bleed and identity degradation in multi-subject customized generation. Our framework uses Identity-Disentangled Attention Regularization to prevent \"attention bleeding\" and alleviate the degradation of subject fidelity caused by attribute leakage. Then, we introduce Mixture-ofExperts architecture to enhance model capacity. We further align the model with human preferences using novel online reinforcement learning framework featuring Multi-ID Alignment Reward and the stable GSPO algorithm. Experiments show that MultiCrafter significantly improves subject fidelity while better aligning with human preferences and generating realistic images."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025a. Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint arXiv:2305.03374, 2023. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proc. CVPR, pp. 1250112511, 2025b. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proc. CVPR, pp. 46904699, 2019. Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In Proc. CVPR, pp. 52035212, 2020. Huanzhang Dou, Ruixiang Li, Wei Su, and Xi Li. Gvdiff: Grounded text-to-video generation with diffusion models. arXiv preprint arXiv:2407.01921, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proc. ICML, 2024. 10 arXiv Preprint Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. In Proc. NeurIPS. Neural Information Processing Systems Foundation, 2023. Haoran Feng, Zehuan Huang, Lin Li, Hairong Lv, and Lu Sheng. Personalize anything for free with diffusion transformer. arXiv preprint arXiv:2503.12590, 2025. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, and Hao Wang. Mixture-of-loras: An efficient multitask tuning for large language models. arXiv preprint arXiv:2403.03432, 2024. Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Mixture of cluster-conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023. Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. In Proc. NeurIPS, 2024. Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, and Qian He. Musar: Exploring multi-subject customization from single-subject dataset via attention routing. arXiv preprint arXiv:2505.02823, 2025. Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, and Satya Narayan Shukla. simple and effective reinforcement learning method for text-to-image diffusion fine-tuning. arXiv preprint arXiv:2503.00897, 2025. Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proc. ICCV, pp. 73237334, 2023. Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with fine-grained id and attribute control. arXiv preprint arXiv:2405.12970, 2024. Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, and Liefeng Bo. Anystory: Towards unified single and multiple subject personalization in text-to-image generation. arXiv preprint arXiv:2501.09503, 2025. Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation, 2025. URL https://arxiv.org/abs/2505.04512. Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. arXiv preprint arXiv:2312.13691, 2023. Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. 2024a. Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong In-context lora for diffusion transformers. arXiv preprint Feng, Yu Liu, and Jingren Zhou. arXiv:2410.23775, 2024b. 11 arXiv Preprint Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong Shen, Han Zhang, Huanzhang Dou, Yu Liu, and Jingren Zhou. Chatdit: training-free baseline for task-agnostic free-form chatting with diffusion transformers. arXiv preprint arXiv:2412.12571, 2024c. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proc. CVPR, pp. 19311941, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. In Proc. CVPR, pp. 2671 2682, 2025. Hengjia Li, Lifan Jiang, Xi Xiao, Tianyang Wang, Hongwei Yi, Boxi Wu, and Deng Cai. Magicid: Hybrid preference optimization for id-consistent and dynamic-preserved video customization. arXiv preprint arXiv:2503.12689, 2025a. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025b. Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, Chuanyun Deng, Yepan Xiong, Min Chen, et al. Realcam-i2v: Real-world image-to-video generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025c. Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. PhoIn Proc. CVPR, pp. tomaker: Customizing realistic human photos via stacked id embedding. 86408650, 2024a. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024b. Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025d. Chen Liang, Lianghua Huang, Jingwu Fang, Huanzhang Dou, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Junge Zhang, Xin Zhao, and Yu Liu. Idea-bench: How far are generative models from professional designing? In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1854118551, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 11041114, 2024. Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. In Proc. CVPR, pp. 8009 8019, 2025b. 12 arXiv Preprint Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023a. Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. In Proc. NeurIPS, pp. 5750057519, 2023b. Zehong Ma, Shiliang Zhang, Longhui Wei, and Qi Tian. Ovmr: Open-vocabulary recognition with multi-modal references. In Proc. CVPR, pp. 1657116581, 2024. Zehong Ma, Hao Chen, Wei Zeng, Limin Su, and Shiliang Zhang. Multi-modal reference learning for fine-grained text-to-image retrieval. IEEE Transactions on Multimedia, 2025a. Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, and Qi Tian. Magcache: Fast video generation with magnitude-aware cache. arXiv preprint arXiv:2506.09045, 2025b. Debapriya Maji, Soyeb Nagori, Manu Mathew, and Deepak Poddar. Yolo-pose: Enhancing yolo for multi person pose estimation using object keypoint similarity loss. In Proc. CVPR, pp. 2637 2646, 2022. Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In Proc. CVPR, pp. 1084410853, 2024. Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pp. 565571. Ieee, 2016. Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. arXiv preprint arXiv:2305.14483, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proc. ICCV, pp. 41954205, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proc. ICML, pp. 87488763. PMLR, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proc. CVPR, pp. 2250022510, 2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proc. CVPR, pp. 65276536, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 13 arXiv Preprint Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proc. CVPR, pp. 85438552, 2024. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proc. CVPR, pp. 82288238, 2024. Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, XuHui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, et al. survey on large language models for mathematical reasoning. arXiv preprint arXiv:2506.08446, 2025a. Peng-Yuan Wang, Jing-Cheng Pang, Chen-Yang Wang, Xuhui Liu, Tian-Shuo Liu, Si-Hang Yang, Hong Qian, and Yang Yu. Inclet: Large language model in-context learning can improve embodied instruction-following. In Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems, pp. 21342142, 2025b. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. MS-diffusion: Multi-subject In Proc. ICLR, 2025c. URL https: zero-shot image personalization with layout guidance. //openreview.net/forum?id=PJqP0wyQek. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-tomore generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025c. Tao Wu, Xuewei Li, Zhongang Qi, Di Hu, Xintao Wang, Ying Shan, and Xi Li. Spherediffusion: In Proc. AAAI, volume 38, pp. Spherical geometry-aware distortion resilient diffusion model. 61266134, 2024a. Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, and Xi Li. Videomaker: Zero-shot customized video generation with the inherent force of video diffusion models. arXiv preprint arXiv:2412.19645, 2024b. Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. In Proc. AAAI, volume 39, pp. 84698477, 2025d. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proc. CVPR, pp. 48184829, 2024. 14 arXiv Preprint Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, In Proc. Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. CVPR, pp. 1329413304, 2025. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proc. CVPR, pp. 89418951, 2024. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. Proc. NeurIPS, 37:7336673398, 2024. Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. 2022. Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. Hanyang Zhao, Haoxian Chen, Ji Zhang, David Yao, and Wenpin Tang. Score as action: Finetuning diffusion generative models by continuous-time reinforcement learning. arXiv preprint arXiv:2502.01819, 2025. Ruisi Zhao, Mingming Li, Zheng Yang, Binbin Lin, Xiaohui Zhong, Xiaobo Ren, Deng Cai, and Boxi Wu. Towards fine-grained hboe with rendered orientation set and laplace smoothing. In Proc. AAAI, volume 38, pp. 75057513, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Weizhi Zhong, Huan Yang, Zheng Liu, Huiguo He, Zijian He, Xuesong Niu, Di Zhang, and Guanbin Li. Mod-adapter: Tuning-free and versatile multi-concept personalization via modulation adapter. arXiv preprint arXiv:2505.18612, 2025. 15 arXiv Preprint MORE DETAILS FOR IDENTITY-PRESERVING PREFERENCE OPTIMIZATION We provide detailed algorithm execution flow for Identity-Preserving Preference Optimization Sec. 4.4, as shown in Algorithm 1. Algorithm 1 Identity-Preserving Preference Optimization Training Process Init the same noise s0 (0, I) for generate i-th image from = 1 to do Sample batch prompts Cb and corresponding subjects Zb Zdata Update old policy model: πθold πθ for each prompt Cb and subject Zb do Require: initial policy model πθ; composite reward model R; prompt dataset C; reference subjects dataset Zdata; total sampling steps ; number of samples per prompt ; sliding window (l), window size w, shift interval τ , window stride 1: Init left boundary of (l): 0 2: for training iteration = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Calculate advantage: Ai for sampling timestep = 0 to 1 do Use ODE Sampling to get si πθold mixed sampling loop Use SDE Sampling to get si ,c,Z)mean({R(sj if (l) then end for end if else j=1) R(si t+1 t+1 std({R(sj ,c,Z)}N ,c,Z)}N j=1) for optimization timestep (l) do Update policy model via gradient ascent: θ θ + ηθJGSP 17: 18: 19: 20: 21: 22: 23: 24: end for end if end for end for if mod τ = 0 then min(l + s, w) optimize policy model πθ move sliding window MORE IMPLEMENTATION DETAILS. In this section, we provide more details on the hyperparameter settings and specific training details. As mentioned in the main text, we divide the training phase into three parts, and the details of each phase are as follows: 1. Single-Subject Pre-training. We first pre-train the model for 40,000 steps on an internal single-subject dataset to equip it with foundational subject customization capabilities. In this stage, only Ldif is used for supervision. We use the AdamW optimizer with 3 105 learning rate and weight decay of 1 102. We use 8 cards for training and set the batch size of each card to 6. 2. Multi-Subject Customization Training. Then we train two models for customized human generation and object generation, respectively. For multi-human customized generation, we decrease the learning rate to 1 105, set the loss weight λ = 0.3, and introduce the attention loss Lattn. This stage runs for 25,000 steps. We use 8 cards at this stage and set the batch size to 4. For multi-object customized generation, since the size of the dataset is smaller than multi-human datasets, we only train for 15,000 steps. 3. Identity-Preserving Preference Optimization. Finally, we fine-tune the model using our proposed reinforcement learning method. Following (Li et al., 2025b), we configure with sampling step of 16, window size of = 2, shift interval of τ = 50, and window stride of = 1. This stage consists of 300 steps. For multi-human customization, the reward weights are set to wid = 0.5, wtext = 1.4, and waes = 0.7. For multi-object customization, we adjust the subject fidelity weight to wid = 1.0, while keeping wtext = 1.4 and waes = 0.7. In this stage, we used 16 cards for training and set the batch size of each card to 1. 16 arXiv Preprint TRAINING DATASET CONSTRUCTION PIPELINE. Due to the scarcity of public multi-human customization datasets with adequate annotations, we designed comprehensive and automated data curation pipeline as shown in Fig. 5. This pipeline processes raw video clips to generate structured training samples, each containing target image with multiple subjects, corresponding identity reference images, segmentation masks, and detailed textual description. The entire workflow ensures subject fidelity, high image quality, and rich annotation. Specifically, we sample video clips featuring two individuals from large database. The process begins with frame selection and subject localization. For each input video, we sample an initial frame as the source for reference images and middle frame as the target scene. We first employ YOLO-Pose (Maji et al., 2022) to obtain initial bounding boxes and keypoint information for each person. Following localization, we leverage the Segment Anything Model (SAM) (Ravi et al., 2024) to generate high-fidelity segmentation masks for each individual, effectively isolating them from the background. To refine this output, only the largest connected component of the mask is retained. critical subsequent step is ensuring subject fidelity across frames. We apply face detection modell (Deng et al., 2019) to the segmented portraits to locate facial regions, and then use face recognition model (Deng et al., 2019) to extract normalized feature embedding for each face. By computing the cosine similarity between embeddings from the reference and target frames, we enforce stringent threshold to discard pairs where identity cannot be confidently verified. Once pair of frames passes this verification, the pipeline generates the final training sample. The segmented portraits and cropped faces from the initial frame are saved as the reference =images. The target frame is cropped into square as target image, with its corresponding body and face masks preserved. Finally, powerful vision-language model, Qwen2.5-VL (Bai et al., 2025), is prompted with the reference images and the target image to produce rich text prompt of the entire scene, ensuring descriptive consistency for each subject. Figure 5: Data processing pipeline for customized multi human image generation. MORE DETAILS FOR OUR BENCHMARK. To advance research on high-fidelity multi-subject generation, we constructed benchmark dataset by collecting images from publicly available sources and extracting the corresponding facial regions as reference images. The dataset comprises 80 celebrities and 80 non-celebrities, covering diverse attributes in terms of gender, age, and ethnicity (male/female; young/elderly; Caucasian, Black, and Asian). We used this face collection as reference pool and paired faces within it. For each pair, we employed Qwen2.5-VL to generate distinctive natural-language prompts to provide diverse textual descriptions. Representative samples are shown in Fig. 6. 17 arXiv Preprint Figure 6: Visualization for part of our multi-human evaluation benchmarks. LIMITATION. Although MultiCrafter has achieved excellent performance in multi-subject driven image generation tasks, our work still has certain limitations, which also point the way for future research. First, the scale and quality of the training data are the primary limiting factors. Currently, highquality, publicly available datasets for multi-subject driven generation remain scarce (Wu et al., 2025c; Chen et al., 2025a). Although we have designed complete automated data processing pipeline to extract training samples from videos, our dataset is still limited in scale and diversity due to the quantity and quality of open-source video data. Second, the effectiveness of our method has been primarily validated in two-subject scenarios.Since our multi-person dataset and the public MUSAR dataset (Guo et al., 2025) mainly contain two subjects, the experiments in this paper were centered around this setting. The models generation capability when handling three or more subjects has not been fully verified. It is worth noting that our framework was designed with scalability in mind; both the attention regularization mechanism and the Multi-ID Alignment Reward (based on the Hungarian algorithm) in the reinforcement learning framework can be directly extended to scenarios with more subjects. For future work, we plan to explore improvements from both data and model perspectives. On one hand, we will attempt to construct larger, higher-quality datasets containing more diverse number of subjects by combining synthetic data with image editing Wu et al. (2025a) techniques. On the other hand, we will train and evaluate the model in scenarios with more subjects to further enhance the generalization and robustness of MultiCrafter, enabling it to handle more complex personalized image generation. THE USE OF LARGE LANGUAGE MODELS. In this paper, we only use the large language model to help polish our text. The large language model has no role in the research conception. MORE RESULTS OF MULTI-HUMAN PERSONALIZATION. To further demonstrate our methods performance in multi-human personalization, we present qualitative comparisons in Fig. 7 and Fig. 8. The results show that our model effectively preserves the identity of each subject and avoids the \"attribute leakage\" common in other methods. This outcome validates the efficacy of our Identity-Disentangled Attention Regularization (IDAR). While some baselines produce more stylized outputs that may yield higher HPSv2 scores, this is often at the expense of subject fidelity. Our method prioritizes photorealism and faithful subject appearance consistency, which leads to more reliable results in multi-subject customization. 18 arXiv Preprint Figure 7: More Visualization of our method in Multi-Human Personalization. MORE RESULTS OF MULTI-OBJECT PERSONALIZATION. To evaluate the generalization of our framework, we showcase multi-object customization comparisons in Fig. 9. Our method demonstrates high object fidelity, accurately preserving core visual attributes such as toys texture or glasss geometry. In contrast, competing approaches often introduce artifacts like deformation and detail loss. This highlights our models strength in precise subject representation rather than hyper-stylization, crucial capability for practical applications that require accuracy. MORE RESULTS OF SINGLE-SUBJECT PERSONALIZATION. Effective multi-subject generation builds on strong single-subject performance. We validate this capability in Fig. 10 and Fig. 11, showing six diverse samples for each of four individuals and six frontal single-subject comparisons against SOTA models. Our method consistently preserves identity across varying styles, poses, and scenes, and even improves fidelity to the reference over baselines. These results confirm that the proposed framework not only enables reliable multi-subject generation but also enhances single-subject identity fidelity. 19 arXiv Preprint Figure 8: More Visualization of our method in Multi-Human Personalization. Figure 9: Visualization of our method in Multi-Object Personalization. 20 arXiv Preprint Figure 10: Visualization of our method in Single-Subject Personalization. 21 arXiv Preprint Figure 11: Qualitative comparison with existing methods on the single human generation."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Zhejiang University",
        "Huawei Technologies Ltd",
        "School of Software Technology, Zhejiang University"
    ]
}