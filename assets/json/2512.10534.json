{
    "paper_title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "authors": [
        "Haiteng Zhao",
        "Junhao Shen",
        "Yiming Zhang",
        "Songyang Gao",
        "Kuikun Liu",
        "Tianyou Ma",
        "Fan Zheng",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 3 5 0 1 . 2 1 5 2 : r ACHIEVING OLYMPIA-LEVEL GEOMETRY LARGE LANGUAGE MODEL AGENT VIA COMPLEXITY BOOSTING REINFORCEMENT LEARNING Haiteng Zhao,1, Junhao Shen,1,2, Yiming Zhang,1, Songyang Gao1, Kuikun Liu1 Tianyou Ma1,3, Fan Zheng4, Dahua Lin1,5, Wenwei Zhang1,, Kai Chen1, 1Shanghai AI Laboratory 2Shanghai Jiao Tong University 3Peking University 5MMLab, The Chinese University of Hong Kong {zhaohaiteng,zhangwenwei,chenkai}@pjlab.org.cn 4ICMAT, Spanish National Research Council"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with symbolic engine, and reflecting on the engines feedback to guide subsequent proposals. dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language model (LLM) agents have demonstrated general problem-solving ability across domains such as mathematics and programming. By interacting with tools such as code interpreters and LEAN (Moura & Ullrich, 2021) multiple times, LLM agents can reason and reflect on the tool execution feedback and progressively solve complex problems. Such universal paradigm can obtain medalist-level performance on International Mathematical Olympiad (IMO) level problems and is believed to have better generalization ability (Huang & Yang, 2025; Luong & Lockhart, 2025). However, when faced with geometry problems, the potential of LLM agents is still underexplored. The IMO-level geometry problems usually require extra-long proving steps, whose solution not only combines various geometry theorems but also contains creative auxiliary constructions that have weak heuristics and require multiple trials, as shown in Figure 1. Consequently, current state-of-the-art approaches (Trinh & Luong, 2024; Chervonyi et al., 2025; Chen et al., 2025a) mainly adopt expert models learned from large-scale synthesized data to guide the large-scale search with symbolic engine for finding the geometry proof. Given the success of LLM agents in other mathematical domains, this raises natural question: Can we adopt LLM agents in solving geometry problems for achieving higher efficiency and generalization? 1 Figure 1: An example of IMO-level geometry problems. (a) The configuration in IMO 2018 Problem 6 appears simple, but it is difficult to prove. Its solution relies on sophisticated constructions, as illustrated in (b). To answer this question, this paper makes the first attempt to investigate LLM agents for solving IMO-level geometry problems and proposes InternGeometry, an LLM agent that obtains medalistlevel performance for solving geometry problems. We first identify the limitations in the current open-sourced symbolic engines for geometry problems and build InternGeometry-DDAR, which contains rich theorem library whose search space theoretically covers the complete solution of most of the IMO geometry problems. Taking InternGeometry-DDAR as tool, InternGeometry solves the geometry problem through long-term LLM-tool interactions, where the LLMs continuously propose propositions or auxiliary constructions after thinking with natural language, verify those ideas in the symbolic engine with formal language, and reflect on the feedback from the symbolic engine at each interaction. In the long-term reasoning process, InternGeometry adopts dynamic memory to maintain the exploration history and the observed geometry properties in compact form, which not only reduces the context without losing key information but also guides diverse explorations in future interactions. By extra-long-horizon LLM-tool interactions with memory, InternGeometry can conquer the weak heuristics of geometry proof and progressively find the feasible solution based on the accumulated geometry properties observed during explorations. Such design also aligns with human experts who obtain insights into auxiliary construction by exploratory probing (Trinh & Luong, 2024; Chervonyi et al., 2025). To train InternGeometry, we first apply cold start training using 7K examples created by formalizing existing geometry problems and constructing trajectory data. After the cold start, we introduce complexity-boosting reinforcement learning (CBRL) framework, multi-stage curriculum RL pipeline (Wang et al., 2025b; Chen et al., 2025b; Zhang et al., 2025b; Parashar et al., 2025), to further improve training efficiency. Specifically, we build data synthesis pipeline that can generate geometry tasks with specified complexity (e.g., required proof steps). At each stage, we first synthesize problems at the current complexity level, then perform RL training on the current InternGeometry model, and update the target complexity based on the results to best fit the current model. Over iterations, the synthesized problems become highly challenging, providing the foundation for acquiring expert-level capabilities on high-difficulty tasks. We conduct extensive experiments to verify the effectiveness of InternGeometry. InternGeometry solves 44 out of 50 geometry problems from 2000 to 2024, surpassing the average score of IMO gold medalists (40.9 points) and the score of AlphaGeometry2 (42) and SeedGeometry (43), and it also solves the geometry problem in IMO 2025. Notably, the model attains this performance with only approximately 13K training examples, 0.004% of AlphaGeometry2 and 0.006% of SeedGeometry. Our ablation studies further demonstrate that long-horizon proof interaction is critical to the agents ability: removing proposition proving steps and only allowing the agent to add auxiliary constructions significantly degrades performance, substantiating the importance of long-horizon trial-and-error for the weak-to-strong heuristic transition. Complexity escalation plays pivotal role in RL convergence: directly training on high-difficulty data leads to low task completion rates and poor convergence, whereas using data below certain difficulty threshold substantially impairs generalization to IMOlevel tasks. In addition, our case studies show that the model can devise novel auxiliary constructions compared to human solutions, exhibiting creativity in geometric reasoning. 2 Figure 2: An overview of InternGeometry and Complexity-Boosting Reinforcement Learning (CBRL). (a) InternGeometry performs natural-language reasoning (Think), outputs structured action in domain-specific language (Action), and receives execution results (Feedback) in each turn. dynamic memory module compresses the multi-turn interaction history to preserve essential actions and outcomes. (b) CBRL optimizes the agent policy by generating synthetic training data with controllable difficulty, assigning binary rewards to effective steps and successful outcomes, and optimizing policy through iterative reinforcement learning."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 GEOMETRY PROOF LANGUAGE AND ENVIRONMENT In previous work such as AlphaGeometry, geometric structures are defined point by point through domain-specific language (DSL). Once the construction is complete, deductive database arithmetic reasoning (DDAR) system is employed to exhaustively search for theorems, deriving all conclusions reachable from the known facts. In this work, we build InternGeometry-DDAR, an interactive geometric proof engine based on the open-source DDAR system Newclid (Sicca et al., 2024). To support more complex geometric structures, we introduce several advanced definition strategies, such as globally optimizing point placements to satisfy constraints. During interaction, an agent not only employs the DSL for both problem specification and auxiliary point construction, but also proposes sub-proof goals that will be subsequently verified by the engine. As an interactive proof engine, InternGeometry-DDAR maintains state across steps, including the geometric configuration, constructed auxiliary points, and all proven preliminaries and propositions. Further details are provided in the Appendix B. 2.2 GEOMETRIC PROOF AGENT The agent is allowed to perform natural-language reasoning at each step and then mark its final action output using an action-separating token. Let the agent be denoted by and the interactive proof engine by E. Given geometric problem X, at step the agents output is [Pt, At] = (X, W(Ht1)) where Pt denotes the slow chain-of-thought reasoning, At the final formalized code, and Ht1 the interaction history, which includes each rounds thoughts, actions, and the feedback observations obtained from the environment prior to step t. The module is memory manager that returns compressed long history to improve the agents long-horizon capacity. (1) The code At is executed by the proof engine E, which is in state 1. The execution result Ot is appended together with the corresponding thoughts and actions to the interaction history as feedback 3 to the agent, guiding its next step of reasoning and action. Ot, Et = Et1(At) Ht = Ht1 + [Pt, At, Ot] (2) At each reasoning step, the agent may summarize progress, analyze the problem, or plan future proof strategies. In its action code, the agent can choose specific operationssuch as constructing geometric objects, adding auxiliary constructions, or verifying whether proposition holds. When all targets in the problem have been proven, the geometric reasoning tool determines that the problem is fully solved, aggregates the entire proof process, and produces complete proof of the problem, thereby concluding the reasoning session. We posit that long-horizon capability is key to addressing the weak-heuristic challenge of auxiliary construction in geometric proofs. To this end, we introduce dynamic memory management strategy for the agent, as well as prior-guided rejection sampling method for G. To shorten the long interaction history H, which can span hundreds of turns, summarizes earlier exchanges, including their thoughts and detailed environment feedback, while retaining only core action outputs and key environment feedback to improve the agents context efficiency, as illustrated in Figure 2. Through W, the agent obtains concise overview of its action history and their outcomese.g., whether an auxiliary construction was successfully added and whether proposed proposition holds. The last turn feedback remains unchanged, informing the agent of the currently known propositions. Another major challenge for long-horizon agents is action collapse, where the model falls into poor patterns, such as producing highly repeated outputs or outputs similar to previous rounds (Sinha et al., 2025). To address this challenge, we use an intuitive rejection sampling method during agent inference to avoid such patterns. Denote naive LLM inference as [ ˆPt, ˆAt] = (X, W(Ht1)). Then, for the sampled value [ ˆPt, ˆAt], If PassCheck([ ˆPt, ˆAt]) : (X, W(Ht1)) = [ ˆPt, ˆAt] Else: reject the value of [ ˆPt, ˆAt] and return to the sampling step Here, PassCheck is rule-based multi-condition checking policy that enforces no repeated actions relative to the history, no excessively long thinking without stop, no turn without valid action or with formatting issues, and no use of the same action type for too many consecutive rounds. (3) 2.3 COMPLEXITY BOOSTING REINFORCEMENT LEARNING Before reinforcement learning begins, there is cold-start phase in which we perform supervised fine-tuning on small amount of synthetic data to help the agent quickly adapt to the task paradigm. Denote the training dataset as = (cid:0)X i, hi, yi(cid:1)N i=1, where denotes the input, = W(H) denotes the compressed history (aligned with the agents input), and = [P, A] denotes the output, including the thinking content and the action content A. Let the agent model have parameters θ. The supervised fine-tuning objective is: Lst = 1 (cid:34) (cid:88) (cid:88) i=1 t=1 log Gθ (cid:0)yi xi, hi (cid:1) (cid:35) (4) Based on the supervised fine-tuning result θst, the agent exhibits the basic behavior patterns expected in geometric reasoning tasks, such as slow thinking and proactively invoking tools. The subsequent CBRL phase is an iterative interactiontraining loop. In each iteration, the agent first attempts proof on the training task and then performs online learning using reward signals from its trajectories. Following GRPO (Shao et al., 2024), given task X, the training gradient is: Jrl(X, θ) = Ey,hGθ (X) (cid:88) min (cid:18) Gθ (yt X, ht) Gθold (yt X, ht) t=1 βDKL (GθGref ) , clip( Gθ (yt X, ht) Gθold (yt X, ht) (cid:19) , 1 ϵ, 1 + ϵ) A(X, yt)Gθ (yt X, ht) (5) where Ai (X, yt) = mean (cid:0)(cid:8)r1 ri std (cid:0)(cid:8)r1 1, r1 1, r1 2, , r1 2, , r1 , r2 1, , rK (cid:9)(cid:1) , r2 1, , rK (cid:9)(cid:1) , (6) represents the advantage at step of the i-th trajectory within batch of samples. It measures the quality improvement at step of the i-th generated trajectory relative to the average policy. Here, ri denotes the reward at step of the i-th trajectory. ϵ is hyperparameter that constrains the policy ratio. Gθold is the policy model from the previous iteration. Gref is the initial model. DKL is the KL divergence, used as regularizer to constrain optimization of the agent model. Here, the reward is binary value, computed as the conjunction of the outcome reward and the step effectiveness reward: = ro rs (7) The outcome reward ro is 1 if the proof is complete; otherwise, it is 0. The step effectiveness reward is defined by whether the steps action succeeds. For proposition-proposing steps, rs is 1 if the proposed proposition is successfully proven by the engine. For auxiliary-construction steps, rs is 1 if the construction is successfully added and used in the final proof of the overall question; otherwise, it is 0. Note that the reward in our work is deliberately simple and can be computed by rules automatically. It rewards effective steps in trajectories that succeed while penalizing all steps in failed trajectories and ineffective steps in successful trajectories. Next, we focus on the curriculum algorithm for CBRL in geometric tasks. One major advantage of our data synthesis pipeline is the fully controllable difficulty of the problems. As illustrated by AlphaGeometry (Trinh et al., 2024), the difficulty of IMO geometry problems for humans is positively correlated with the number of DDAR proof steps. Therefore, we choose the DDAR proof step count as the measure of task complexity, denoted as κ. Denote the data synthesis pipeline as X. We implement CBRL as follows: θ = arg max θ κ = arg max κ EXX(κ)Jrl(X, θ) EXX(κ)EyGθ A(X, y) (8) (9) where the goal of κ optimization is to maximize the average absolute advantage during learning, and A(X, y) is the advantage of outcome reward. We then present properties of CBRL. As shown in Wang et al. (2025b) and Chen et al. (2025b), maximum absolute advantage has the following properties: Theorem 1. Given model parameter θ, the κ obtained from Equation 9 approximately optimally accelerates the learning progress. Theorem 2. For binary rewards, the maximum average absolute advantage is 0.5, which indicates that the task is of moderate difficulty for the modelneither too difficult nor too trivial. In practice, in each CBRL round, we sample data conditioned on complexity κ, perform RL training to the agent, and finally update κ according to learning rate α. See Appendix for details. 2.4 DATA SYNTHESIS PIPELINE The proposed data pipeline targets to synthesize geometry problems with adaptable levels of difficulty for InternGeometry. Specifically, it comprises two stages: cold start and expert-level problem synthesis for CBRL. First, due to the scarcity of data in DSL form, we fine-tuned InternThinker-32B (Lyu et al., 2025) as InternGeometry-Formalizer through expert iteration (Anthony et al., 2017) and then exploit largescale natural language problem data from diverse sources. This process produced total of 7K formal problem and solution trajectory pairs, which provide cold start for InternGeometry. However, these paired data are constrained by the imbalanced difficulty distribution with relatively few problems at the expert level. To endow LLM with expert-level problem-solving abilities, we further synthesize problems dynamically during reinforcement learning. We first add auxiliary constructions into randomly constructed problems with statistical prior of given complexity, then Table 1: Comparison of overall performance on IMO 50 between InternGeometry and SOTA geometry expert models. Model Model Type Training Data Sampling Setting IMO 50 Pass@K AlphaGeometry 2 SeedGeometry InternGeometry Expert Model Expert Model LLM Agent 300M 230M 13K Ensemble of search trees N/A Pass@256 42/50 43/50 44/50 Table 2: problem-by-problem comparison on IMO 50 between InternGeometry and SOTA geometry expert models. Year ID Split AG2 2000 2000 2001 2001 2002 2002 2002 2003 2003 2003 2004 2004 2004 2005 2005 2006 2006 P1 P6 P1 P5 P2 P2 P6 P3 P4 P4 P1 P5 P5 P1 P5 P1 P6 b SG IG Year ID Split AG2 a 2007 2007 2008 2008 2008 2009 2009 2009 2010 2010 2011 2012 2012 2013 2013 2014 2014 P2 P4 P1 P1 P6 P2 P4 P4 P2 P4 P6 P1 P5 P3 P4 P3 P4 SG IG Year ID Split AG2 2015 2015 2016 2017 2018 2018 2019 2019 2020 2020 2021 2021 2022 2023 2023 2024 2025 P3 P4 P1 P4 P1 P6 P2 P6 P1 P6 P3 P4 P4 P2 P6 P4 P2 N/A SG IG leveraging the InternGeometry-DDAR to filter valid constructions and goals to form new problems. Finally, total of 6K problems are constructed by difficulty based on proof steps during CBRL. See Appendix for details."
        },
        {
            "title": "3 EXPERIMENT",
            "content": "3.1 EXPERIMENT SETUP Implementation. We use InternThinker-32B (Lyu et al., 2025) as the backbone model for our method. For the agent model, we set the maximum number of steps to 200 by default, with inference hyperparameters of temperature 0.9 and top-p 0.9. During the test, the pass@K is set to 256. Dataset and Baselines. We use IMO 50 (Chervonyi et al., 2025) as the test set, which includes all geometry problems from IMO 2000 to IMO 2024. We additionally evaluate InternGeometry on the geometry problem from IMO 2025, reported separately in Table 2. We use AlphaGeometry 2 (Chervonyi et al., 2025) and SeedGeometry (Chervonyi et al., 2025) as our baselines, both of which are state-of-the-art geometry proving methods based on expert models. The performance of these baselines is taken directly from the results reported in their respective papers. 3.2 OVERALL RESULTS We compare the performance of InternGeometry with baselines in Table 1. InternGeometry solved 44 problems in IMO 50, surpassing AlphaGeometry 2 and SeedGeometry. The split in the table refers to subproblems of questions that contain multiple subquestions. Notably, InternGeometry used only 13K data pointsjust 0.004% of AlphaGeometry 2 and 0.006% of SeedGeometry. Furthermore, its test-time scaling budget was also far lower than AlphaGeometry 2, which use ensembles of beam search, and the reported optimal single beam tree configuration is beam size 128, branching number (samples) 32, and beam depth 4. See Appendix for more discussion. These comparisons clearly demonstrate the potential of LLM-based agent approaches on expert-level tasks. We list the individual results on IMO 50 in Table 2. We additionally include the geometry problem of IMO 2025 in the table. InternGeometry solved 45 out of 51 problems, covering all problems solved by AlphaGeometry 2, and additionally solving 2018 P6 and 2023 P6. Compared to SeedGeometry, it additionally solved 2001 P5 and 2009 P4b, but missed 2006 P1. Notably, the remaining unsolved 6 Figure 3: Left: The effect of long-horizon interaction on the proof. As the interaction steps increase, the proving success rate improves significantly, which holds for different sampling times. As sampling times increase, Pass@K also rises, indicating the test-time scalability of InternGeometry. Right: Extending the agents trajectory length is more effective than repeated sampling for scaling. The total inference budget is defined as the sampling number multiplied by the agents steps. When the maximum length is capped (the blue lines), performance improves with inference budget at slower rate for shorter trajectories. On the other hand, when the sampling size is fixed (the green lines), increasing the budget by lengthening the trajectory yields efficient scaling. Table 3: Ablation study on long-horizon agents in InternGeometry. Propositions Slow Thinking Context Compression Reject Sampling IMO 50 Pass@256 44/50 35/50 23/50 20/50 38/50 problems largely involve computations that go beyond the scope of pure geometric proof, and thus fall outside the current expressive range of geometric DDAR systems. See Appendix for cases. 3.3 ANALYSIS FOR LONG HORIZON AGENT To analyze the effect of long-horizon interaction to the proof, we compare the pass@K on IMO 50 under different max step setting, and the result is in Figure 3 (left). It is evident that as interaction steps increase, the proving success rate improves significantly, which holds for different sampling times. Shorter interactions significantly limit the success rate of agent proofs. As the interaction trajectory grows, the agent can continually explore to develop heuristics about the problem, enabling it to better leverage its reasoning ability for generalization. Additionally, the figure illustrates the trend of test-time scaling. As shown, as sampling times increase, Pass@K also rises, indicating the test-time scalability of InternGeometry. We emphasize that extending an agents trajectory length scales performance more effectively than repeated sampling. As shown in Figure 3 (right), with total inference budget defined as times the number of steps, performance grows slowly when the step cap is 64 but improves much faster when it is 200. For fixed K, increasing trajectory length consistently yields higher efficiency, supporting our hypothesis that long-horizon interactions enhance heuristics more effectively in geometric proofs. An ablation on IMO 50  (Table 3)  further confirms this. InternGeometry solves fewer problems when any long-horizon component is removed. Slow thinking and context compression increase solved problems from 20 and 23 to 44, underscoring their key roles in enabling IMO-level reasoning. 3.4 ANALYSIS ON COMPLEXITY BOOSTING REINFORCEMENT LEARNING In this section, we analyze the effectiveness of the CBRL method. We first present the distribution of the synthetic data obtained during the CBRL process in Figure 4 (left). As noted earlier, we use the length of problems proof steps as an indicator of its difficulty. Accordingly, we compile statistics on the distribution of proof lengths in the synthetic data generated during model training, as shown Figure 4: Left: The distribution of proof lengths in the synthetic data generated during model training, indicating task complexity. The figure shows that the difficulty distribution of the synthetic data exhibits fairly uniform improving trend, providing well-structured curriculum from simple to difficult tasks. Right: Agents generalization performance on IMO 50 during training. The agents overall performance on IMO 50 shows steady upward trend. Notably, there is significant performance jump in the sixth training round. Table 4: Ablation study on CBRL in InternGeometry. Training Setting IMO 50 Pass@256 With CBRL SFT Cold Start Easy Data Only Challenging Data Only Same Data without Schedule 44/50 22/50 29/50 24/50 38/ in the figure. The figure shows that the difficulty distribution of the synthetic data exhibits fairly uniform improving trend, indicating that our agent training provides well-structured curriculum from simple to difficult tasks, which helps the agent master complex combinations of proof skills. We further demonstrate the agents generalization performance on IMO 50 during training in Figure 4 (right). As the training data is continually updated, the agents overall performance on IMO 50 shows steady upward trend. Notably, there is significant performance jump in the sixth training round, indicating breakthrough progress as task complexity is progressively increased in the reinforcement learning process. We then conduct ablation studies on the key components of CBRL. The result is in Table 4. Notably, after SFT cold start, the models baseline performance on IMO 50 is 22/50. We first analyze the impact of data difficulty on reinforcement learning, revealing the importance of allowing the learning algorithm to autonomously control the difficulty of synthesized data. To strictly control variables, we run experiments with the same data scale and training steps as in InternGeometry. Specifically, we modify the data distribution in the RL phase to: using only low-difficulty data, and using only high-difficulty data. The corresponding results are shown in the rows Easy Data Only and Challenging Data Only in the table. The results indicate that using only low-difficulty data limits the agents generalization ability to IMO-difficulty problems; on the other hand, using only high-difficulty data also leads to suboptimal training outcomes. The latter occurs because learning becomes slow and fails to converge within the same training budget: the agent remains in an early trial-and-error stage with extremely sparse learning signals. This highlights the efficiency of CBRL. Finally, we ablate the dynamic difficulty curriculum by uniformly sampling from all synthesized data throughout the InternGeometry training phase. The result (Same Data without Schedule) shows that removing the curriculum again degrades performance. Without progressive difficulty, the agent struggles with sparse or absent rewards on sampled hard problems, preventing it from learning effective strategies under the same training budget. This confirms that CBRL significantly improves data efficiency and training effectiveness in reinforcement learning."
        },
        {
            "title": "3.5 CASE STUDY",
            "content": "During our manual checking, we find that InternGeometry shows remarkable creativity on certain problems. As shown in Figure 1, while most human solvers relied on inversion, trigonometry or complex numbers, the agent solves this problem via an elegant geometric construction using classical angle chasing and basic theorems. Specifically, InternGeometry first places point on segment AC such that BDA = DC, and defines point as the intersection of two circles. These two points form an isogonal conjugate pair in quadrilateral ABCD, revealing that the agent can discover this implicit structure through exploration. To further exploit the isogonal property, it then constructs the symmetric points of with respect to each side of the quadrilateral, showing both an understanding of rotational symmetry and an ability to generalize the use of auxiliary points for handling isogonal conjugates from triangles to quadrilaterals. Overall, this case study highlights how InternGeometry generate creative constructions that differ fundamentally from human solutions."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Reinforcement learning agents in the field of mathematics Currently, large language model agents have achieved remarkable performance in tasks such as Code, Search, GUI, and also Mathematics through reinforcement learning (RL). In mathematics, RL agents based on informal proofs solve problems using general-purpose tools like Python compilers. Examples include OR (Open Reasoning) approaches Singh et al. (2025); Li et al. (2025b); Mai et al. (2025); Zuo et al. (2025); Prabhudesai et al. (2025); Shen et al. (2025); Shang et al. (2025) and PR (Proof Reasoning) approaches Li et al. (2025a); Simonds & Yoshiyama (2025); Goldie et al. (2025); Hao et al. (2025). Alternatively, agents built on Interactive Theorem Provers (ITPs) specialized for mathematics can handle more complex problems. Works like Xin et al. (2024); Ren et al. (2025); Zhang et al. (2025a); Wang et al. (2025a) achieve strong results on benchmarks such as miniF2F (Zheng et al., 2021) and ProofNet (Azerbayev et al., 2023). However, few agents address geometry problems. Our work targets this gap, developing an interactive geometric prover and showing the potential of data synthesis and difficulty scaling. Rurriculum learning for agents While several research (Wang et al., 2025b; Zhang et al., 2025b; Parashar et al., 2025) study curriculum reinforcement learning, curriculum agents learning remains limited, and most approaches rely on highly structured task types. For example, Voyager (Wang et al., 2023) uses manually designed curricula in MineDojo (Fan et al., 2022) to teach agents complex skills. WebRL (Qi et al., 2024) iteratively generates increasingly complex task instructions and employs reward model for automatic success evaluation. In scaling RL for agents and task synthesis, Envgen (Zala et al., 2024) trains large language models to generate formal parameters for Crafter (Hafner, 2021) and Heist (Cobbe et al., 2020), enabling dynamic environment and task generation. Unlike these approaches, our method fully automates large-scale synthesis at specified difficulty levels, allowing unrestricted and unbiased curriculum adjustment, especially at higher complexity. Automatic geometry theorem proving Current AI-based approaches to automated geometric proofs remain largely expert-model driven. State-of-the-art systems like AlphaGeometry (Chervonyi et al., 2025; Trinh & Luong, 2024) and SeedGeometry (Chen et al., 2025a) typically decompose problem into two tasks: (1) auxiliary construction prediction, where the model proposes additional geometric elements (e.g., lines, points); and (2) formal reasoning, where search algorithm assembles complete proof using geometric theorems (e.g., angle bisector, triangle similarity) and logical inference rules. Following this paradigm, most methods train specialized models on large datasets to predict constructions and then combine them with formal search engine for proof generation. Recent work has also explored using large language models for geometric reasoning, but these efforts mainly target elementary-level problems and computational task formats."
        },
        {
            "title": "5 CONCLUSION",
            "content": "LLM-based agents can solve tough math problems, even IMO-level with formal provers, but geometry remains dominated by expert systems like AlphaGeometry2 that depend on massive synthetic data and search.We introduce InternGeometry, medalist-level LLM agent for geometry. It overcomes weak auxiliary-construction heuristics by iteratively proposing propositions and constructions, verifying them with symbolic engine, and refining based on feedback. dynamic memory enables over 9 200 interaction steps. To speed learning, we use CBRL, which gradually increases synthesized problem difficulty during training. Built on InternThinker-32B, InternGeometry solves 44/50 IMO geometry problems (20002024), surpassing the average gold medalist score (40.9), using only 13K training examplesabout 0.004% of AlphaGeometry2s data. It can also generate novel auxiliary constructions unseen in human solutions. We will release the model, data, and symbolic engine."
        },
        {
            "title": "REFERENCES",
            "content": "Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023. Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, et al. Seed-prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025a. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint arXiv:2505.14970, 2025b. Yuri Chervonyi, Trieu Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXiv preprint arXiv:2502.03544, 2025. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation In International conference on machine learning, pp. to benchmark reinforcement learning. 20482056. PMLR, 2020. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 1834318362, 2022. Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher Manning. Synthetic data generation & multi-step rl for reasoning & tool use. arXiv preprint arXiv:2504.04736, 2025. Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021. Qianyue Hao, Sibo Li, Jian Yuan, and Yong Li. Rl of thoughts: Navigating llm reasoning with inference-time reinforcement learning. arXiv preprint arXiv:2505.14140, 2025. Yichen Huang and Lin Yang. Winning gold at imo 2025 with model-agnostic verification-andrefinement pipeline. arXiv preprint arXiv:2507.15855, 2025. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools. arXiv preprint arXiv:2503.04625, 2025a. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025b. Thang Luong and Edward Lockhart. Advanced version of gemini with deep think officially achieves gold-medal standard at the international mathematical olympiad, july 2025. URL https://deepmind. google/discover/blog/advanced-version-ofgemini-with-deep-think-officiallyachieves-gold-medal-standardat-the-international-mathematical-olympiad, 2025. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, and Haian Huang. Exploring the limit of outcome reward for learning mathematical reasoning. 2025. 10 Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In International Conference on Automated Deduction, pp. 625635. Springer, 2021. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, and Y. Wu. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024. Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-ofaction-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. Vladmir Sicca, Tianxiang Xia, Mathïs Fédérico, Philip John Gorinski, Simon Frieder, and Shangling Jui. Newclid: user-friendly replacement for alphageometry. arXiv preprint arXiv:2411.11938, 2024. Toby Simonds and Akira Yoshiyama. Ladder: Self-improving llms through recursive problem decomposition. arXiv preprint arXiv:2503.00735, 2025. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, and Jonas Geiping. The illusion of diminishing returns: Measuring long horizon execution in llms. arXiv preprint arXiv:2509.09677, 2025. Trieu Trinh and Thang Luong. Alphageometry: An olympiad-level ai system for geometry. Google DeepMind, 17, 2024. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, et al. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025a. 11 Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, and Wentian Zhao. Dump: Automated distribution-level curriculum learning for rl-based llm post-training. arXiv preprint arXiv:2504.09710, 2025b. Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024. Jingyuan Zhang, Qi Wang, Xingguang Ji, Yahui Liu, Yang Yue, Fuzheng Zhang, Di Zhang, Guorui Zhou, and Kun Gai. Leanabell-prover: Posttraining scaling in formal reasoning. arXiv preprint arXiv:2504.06122, 2025a. Xuemiao Zhang, Liangyu Xu, Feiyu Duan, Yongwei Zhou, Sirui Wang, Rongxiang Weng, Jingang Wang, and Xunliang Cai. Preference curriculum: Llms should always be pretrained on their preferred data. arXiv preprint arXiv:2501.13126, 2025b. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. 12 THE USE OF LARGE LANGUAGE MODELS (LLMS) Beyond these technical roles, LLMs did not play significant part in research ideation, experimental design, or manuscript writing, the authors conceived the study, designed/evaluated experiments, analyzed results, and wrote the paper. Any automated assistance, if present, was limited to nonsubstantive copy-editing/LaTeX linting. The authors take full responsibility for all contents, including verifying any machine-generated intermediate artifacts, and acknowledge that LLMs are not eligible for authorship. This disclosure follows the ICLR policy on LLM usage and research integrity. IMPROVEMENTS IN INTERNGEOMETRY-DDAR InternGeometry-DDAR builds upon open-sourced symbolic engines (i.e., Newclid (Sicca et al., 2024) and AlphaGeometry (Trinh & Luong, 2024)) and mainly consist of two components: deductive database and algebraic reasoning. The former expands the current set of premises toward the proof goal based on geometry rules, while the latter performs angle, length, and ratio chasing using Gaussian elimination. We introduce three main improvements: dynamic diagram adjustment, the incorporation of syntax and rules for handling double points, and the addition of new predicates and rules. First, open-sourced symbolic engines can only create points one by one based on existing construction definitions, with each point constrained by at most two construction definitions. However, in IMO geometry problem, it is often necessary to make global adjustments to previously constructed points so they satisfy more specific requirements (e.g., line defined as two existing points may also need to be tangent to existing circle). Consider IMO 2003 P4a as an example: \"Let ABCD be cyclic quadrilateral. Let , and be the feet of the perpendiculars from to the lines BC, CA and AB, respectively. Show that = QR if the bisectors of angles ABC and ADC meet on segment AC\". The condition \"the bisectors of angles ABC and ADC meet on segment AC\" cannot be enforced automatically during point-by-point construction; it is specialized condition that holds only for certain point placements. Such hard configurations require globally adjusting previously constructed points so that multiple geometric constraints are satisfied simultaneously. To address this issue, we use gradient descent to adjust certain specific points so that they simultaneously satisfy multiple requirements. Second, we address the issue of double points (i.e., distinctly named points with identical coordinates) by proving that they represent the same geometric point, which is an important technique. Difficult geometry proofs often rely on reformulating intersections or handling degeneracies that naturally create such overlapsa common trick used by human solvers. To this end, we introduce new syntax: prefixing construction statement with ! allows the system to create point even if it shares coordinates with an existing one. In addition, we update the inference module of the symbolic engine to support this extended behavior. We also introduce new predicates and rules for handling double points. Specifically, we define the predicate idc to indicate that points and are geometrically considered the same, and we provide rules for determining this relationship. Additionally, we add several common geometry theorems, such as the Power of Point and Menelaus theorem into InternGeometry-DDAR. InternGeometry-DDAR serves both as an automated geometry problem solver and as powerful symbolic tool for InternGeometry."
        },
        {
            "title": "C INTERACTION BETWEEN INTERNGEOMETRY AND",
            "content": "INTERNGEOMETRY-DDAR InternGeometry has three interaction with InternGeometry-DDAR, which includs obtain initial state in symbolic engine, adding auxiliary construction and proposing proof steps. Specifically, when giving formal geometry problem, InternGeometry first output <build> tag to construct this problem and retrieve the initial geometric relationships from the symbolic engine. Then, in the following turn, InternGeometry performs thinking and then automatically decides whether to add an auxiliary construction using <add> tag or propose proof step using <propose>. InternGeometry-DDAR executes the instructions from InternGeometry and returns feedback, such as successfully proving the proposed proposition or reporting failure when adding new point. After each step, InternGeometry 13 summarizes and compresses the current proof state to support long-horizon interaction. When the final goal is proven, InternGeometry reviews the entire proof and briefly summarizes the reasoning process, extracting key steps and auxiliary constructions."
        },
        {
            "title": "D DETAILS OF COMPLEXITY BOOSTING REINFORCEMENT LEARNING",
            "content": "(CBRL) We provide detailed introduction to CBRL in this section, as the main paper is space-constrained. As outlined in Subsection 2.3, CBRL adapts task complexity to maximize the expected absolute advantage during reinforcement learning. The core intuition is to present tasks that are neither too difficult nor too easy for the policy modeli.e., tasks that best match the models current capability. Following AlphaGeometry (Trinh & Luong, 2024), which observed that human-perceived difficulty of IMO geometry problems (measured by average IMO scores) correlates positively with the number of proof steps taken by the DDAR solver, we quantify task complexity using DDAR proof length. We first introduce the geometry question generation algorithm, then describe the data synthesis pipeline. Finally, we present the CBRL algorithm and explain its motivation. The question generation procedure, Generate Data (Algorithm 1), aims to sample nontrivial geometry questions that cannot be solved by exhaustive search using the InternGeometry-DDAR engine alone. The algorithm iteratively samples raw geometric structure, Xraw, by randomly instantiating DDAR predicates and points. It then augments this structure with auxiliary constructions to obtain Xadd. Both stages are conditioned on user-specified complexity parameter κ, for which we design distinct priors and construction patterns to improve the hit rate of valid problems. We perform exhaustive search on both Xraw and Xadd. Conclusions that (i) involve only points in Xraw, (ii) are provable in Xadd, and (iii) are not provable in Xraw, are deemed nontrivial problems rooted in the raw structure. Among these candidates, we select the most complex oneprimarily by proof length, while also considering priors such as predicate distributions. The algorithm repeats to continuously generate questions targeting the specified complexity prior κ. Because the actual complexity of generated items is only loosely controlled by κ via stochastic priors, we apply additional post-sampling to better align the dataset with the desired complexity. Algorithm 1 Generate Data Require: Complexity κ 1: for _ in range(MaxSample) do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: end for end if Xraw RANDDDARCONSTRUCTION(κ) Xadd ADDAUXCONSTRUCTIONS(Xraw, κ) Praw EXHAUSTSEARCH(Xraw) Padd EXHAUSTSEARCH(Xadd) , Padd Praw & conclusion of uses only points in Xraw, if = then data, pl SELECTMOSTCOMPLEX(Xraw, C) yield data, pl primarily by proof length Next, Algorithm 2 presents the complete Data Synthesis Pipeline. We maintain global cache of all generated questions. Given target complexity κ and required sample count, we first check whether the cache already contains enough items within tolerance range around κ. If not, we invoke Generate Data to enrich the cache until we can retrieve sufficient number of questions at the desired complexity. Finally, we introduce Complexity Boosting Reinforcement Learning (CBRL) in Algorithm 3. The idea is straightforward: in each iteration, we sample batch of data from the Data Synthesis Pipeline, run RL on that batch, and record rewards. After processing the batch, we compute the average reward and update κ by comparing this average to 0.5: increase κ if the average reward exceeds 0.5, otherwise decrease it. Below, we justify why using 0.5 as the target lead to optimized expected absolute advantage, as indicated by Theorem 2. cache: list of (data, proof length) Algorithm 2 Data Synthesis Pipeline for (data, pl) in GENERATEDATA(κ) do Require: Complexity κ, Data Number 1: global 2: ok, dataset SELECTAROUNDRANGE(C, κ, K) 3: while not ok do 4: 5: 6: 7: 8: end while 9: return dataset end for ok, dataset SELECTAROUNDRANGE(C, κ, K) C.append((data, pl)) exactly items around target complexity For binary reward with (r = 1) = and (r = 0) = 1 p, we have mean(r) = and std(r) = (cid:112)p(1 p). The expected absolute advantage is (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:112)p(1 p) ri (cid:112)p(1 p) (cid:112)p(1 p) = 2(cid:112)p(1 p). E[Ai] = + (1 p) (cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (10) (cid:35) This quantity is concave in on [0, 1] and is maximized at = 0.5. Therefore, we can maximize the expected absolute advantage by steering the average reward toward 0.5 via κ-adjustment. Algorithm 3 Complexity Boosting Reinforcement Learning (CBRL) rewards, θ EVALUATEANDUPDATE(θ, minibatch) batchrewards.extend(rewards) batchdata X(κ, DataNumEachIter) batchrewards [, ] for minibatch in SPLITMINIBATCH(batchdata) do Require: Initial complexity κ, Initial policy parameter θ 1: for in range(MaxIter) do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: return θ end for if AVERAGE(batchrewards) > 0.5 then κ κ + α κ κ α end if else We further explain why maximizing the expected absolute advantage benefits policy learning (Wang et al., 2025b; Chen et al., 2025b), as stated in Theorem 1. The key idea is that the expected absolute advantage serves as the primary learning signal that scales the gradient. The simplified gradient of the GRPO objective is θJ (θ) = ExX(κ) (cid:2)EyiGθ(x) [Ai θ log Gθ (yi x)](cid:3) Thus, the gradient norm is θJ (θ) = ExX(κ) (cid:2)EyiGθ(x) [Ai θ log Gθ (yi x)](cid:3)(cid:13) (cid:13) (cid:13) (cid:13) (11) (12) Assuming that the gradient term θ log Gθ (yi x) is bounded and approximately random in direction, the gradient norm is approximately maximized by maximizing EXX(κ)EyGθ A(X, y). In other words, increasing the expected absolute advantage yields larger gradients during optimization and thus can accelerate learning."
        },
        {
            "title": "E DETAILS OF TRAINING TOKEN",
            "content": "We report the total number of training tokens and token-based comparison to prior work. Our model totally trains approximately 1.91 109 tokens. For reference, AlphaGeometry 2 reports training on up to 1 1012 tokens. Framed in terms of training tokens, InternGeometry is therefore substantially more data-efficient."
        },
        {
            "title": "F FAILED CASES",
            "content": "We illustrate failure cases of InternGeometry on IMO 50 in this section. Notably, the unsolved problems are largely beyond pure geometry, and they primarily rely on numerical or non-geometric analysis. 2001 P1 Let ABC be an acute-angled triangle with as its circumcenter. Let on line BC be the foot of the altitude from A. Assume that BCA ABC +30. Prove that CAB +COP < 90. 2002 P6 Let 3 be positive integer. Let C1, C2, . . . , Cn be unit circles in the plane, with centers O1, O2, . . . , On respectively. If no line meets more than two of the circles, prove that (cid:88) 1i<jn 1 OiOj (n 1)π 4 . 2003 Each pair of opposite sides of convex hexagon has the property that the distance between their times the sum of their lengths. Prove that the hexagon is equiangular. 3 2 midpoints is 2006 Let ABC be triangle with incenter I. point in the interior of the triangle satisfies BA + CA = BC + CB. Show that AP AI and that equality holds if and only if = I. 2006 P6 Assign to each side of convex polygon the maximum area of triangle that has as side and is contained in . Show that the sum of the areas assigned to the sides of is at least twice the area of . 2020 Consider an integer > 1, and set of points in the plane such that the distance between any two different points in is at least 1. Prove there is line ℓ separating such that the distance from any point of to ℓ is at least Ω(n1/3). (A line ℓ separates set of points if some segment joining two points in crosses ℓ.)"
        },
        {
            "title": "G DISCUSSION OF INFERENCE COST",
            "content": "Because neither AlphaGeometry 2 nor SeedGeometry has released open-source code or models, we cannot perform direct, controlled comparison. Therefore, we rely on the configurations reported in their respective papers to provide rough estimate and comparison. Since the SeedGeometry paper does not detail its inference budget, we primarily base our equivalent estimates on AlphaGeometry 2. According to the AlphaGeometry 2 paper, the method uses Shared Knowledge Ensemble of Search Trees (SKEST) that integrates classical beam search with several tree-search variants (e.g., predicting multiple auxiliary points at each node) and runs multiple trees with different search configurations and models in parallel. Consequently, the total inference budget scales with the number of configurations the number of instantiated trees the number of models. The reported optimal single-tree configuration uses beam size of 128, branching number (samples) of 32, and beam depth of 4. However, within SKEST there are configurations that incur higher budgets, such as beam size 64 with depth 10 or beam size 512 with depth 4. The AlphaGeometry 2 model size is 3.3B parameters. We compare inference efficiency from four perspectives: the equivalent number of solutions explored during inference, the overall inference steps, the environment execution cost, and the overall computation cost. 1. Equivalent number of solutions explored. For single beam search, the equivalent number of explored solutions can be approximated as beam size branching number. Under the optimal single-tree configuration, this is 128 32 = 4,096, and it can be larger for other configurations (e.g., beam size 512). The total number further scales with the number of configurations the number of instantiated trees the number of models. By contrast, InternGeometrys best-of-K inference explores only solutions (256 in our experiments), addressing its solution-efficiency. 2. Inference steps. The optimal beam-tree configuration results in 128 32 4 = 16,384 steps, and the same number of symbolic-engine executions per tree. Other configurations, such as beam size 64 and depth 10 (64 32 10 = 20,480) or beam size 512 and depth 4 (512 32 4 = 65,536), can require more steps. InternGeometry uses simple pass@256 parallel-inference setting with up to 200 turns of agentic interaction per pass, totaling 51,200 steps. Overall, the per-tree inference budget of AlphaGeometry 2 is on the same order as InternGeometry. However, the total inference cost for AlphaGeometry 2 scales with the number of configurations the number of instantiated trees the number of models, leading to larger overall inference steps. 3. Environment execution cost. Each step in both AlphaGeometry 2 and InternGeometry is executed in an engine. Because InternGeometrys total number of steps is smaller than the full SKEST budget of AlphaGeometry 2, it yields fewer total executions. Furthermore, each AlphaGeometry 2 step requires the DDAR system to attempt solving the entire problem, whereas each InternGeometry step either adds an auxiliary construction or attempts to prove subgoaloperations that are less expensive than attempting to solve the whole problem. 4. Overall computation cost. InternGeometrys reasoning style and larger model size indeed increase computation. Each InternGeometry step involves natural-language reasoning, resulting in more tokens and higher compute: for IMO-50, the average number of output tokens per trajectory is 89.6K. The InternGeometry model (32B) is also larger than AlphaGeometry 2 (3.3B). Due to the unknown total cost of AlphaGeometry 2, direct comparison is difficult. However, we emphasize that the increased computation from deeper reasoning and larger models should not be viewed as drawback. Instead, it represents feasible new scaling dimensionalongside training data size and the number of searched solutionsone that aligns more naturally with LLM-based approaches than with expert-model diagram."
        }
    ],
    "affiliations": [
        "ICMAT, Spanish National Research Council",
        "MMLab, The Chinese University of Hong Kong",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University"
    ]
}