{
    "paper_title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference",
    "authors": [
        "Yuxuan Song",
        "Zheng Zhang",
        "Cheng Luo",
        "Pengyang Gao",
        "Fan Xia",
        "Hao Luo",
        "Zheng Li",
        "Yuehang Yang",
        "Hongli Yu",
        "Xingwei Qu",
        "Yuwei Fu",
        "Jing Su",
        "Ge Zhang",
        "Wenhao Huang",
        "Mingxuan Wang",
        "Lin Yan",
        "Xiaoying Jia",
        "Jingjing Liu",
        "Wei-Ying Ma",
        "Ya-Qin Zhang",
        "Yonghui Wu",
        "Hao Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models."
        },
        {
            "title": "Start",
            "content": "Seed Diffusion: Large-Scale Diffusion Language Model with High-Speed Inference 1ByteDance Seed 2Institute for AI Industry Research (AIR), Tsinghua University 3SIA-Lab of Tsinghua AIR and ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "We present Seed Diffusion Preview, large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder [1], Gemini Diffusion [2]). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini, establishing new state of the art on the speed-quality Pareto frontier for code models. Demo is available at https://studio.seed.ai/exp/seed_diffusion/. Date: August 5, 2025 Correspondence: zhouhao@air.tsinghua.edu.cn, zhangzheng.jacob@bytedance.com Project Page: https://seed.bytedance.com/seed_diffusion 5 2 0 2 4 ] . [ 1 3 9 1 2 0 . 8 0 5 2 : r Figure 1 Seed Diffusions inference speed is measured over H20 GPUs across eight open code benchmarks. Direct comparison with baselines is challenging due to differing test conditions: Mercury Coder was evaluated on proprietary dataset with H100s, while Gemini Diffusions speed was averaged over mixed-task benchmark using unknown hardware. Furthermore, reported speeds on these benchmarks can benefit from format-constraining system prompts. LiveCodeBench results are specifically on the 1055 problems from v1-v6 for the unknown baselines protocol."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [35] learn to reverse process that incrementally corrupts data with noise, effectively decomposing complex distribution into hierarchy of simplified representations. This coarse-to-fine generative approach has proven remarkably successful across wide range of applications, including image and video synthesis [6] as well as solving complex challenges in natural sciences [7]. However, translating this success to the discrete domain of natural language presents critical challenges. The primary difficulty stems from the fact that standard diffusion process is naturally defined over continuous state spaces, thus not directly applicable to discrete domains such as natural language. To bridge this gap, many efforts have focused on novel adaptations, ranging from projecting discrete tokens into continuous latent space (e.g., embeddings or simplex) where diffusion can be applied [810], to constructing the diffusion process directly over discrete-state space by defining explicit state transition matrices [1114]. Recent discrete-state approaches have demonstrated scalability and effectiveness with advanced architectures and training recipes [15]. Despite impressive progress, real-world deployment of discrete diffusion models for language is still hampered by two key challenges: Inductive bias on tokenorder modeling. The usage of discrete diffusion for modeling and generating tokens in arbitrary orders is theoretically powerful and appealing [13, 14]; however, natural language is overwhelmingly processed in sequential order. purely random-order learning signal can in consequence be inefficient, or even detrimental for language modeling, dampening model performance. Inference inefficiency. Although diffusion models are non-autoregressive, their iterative step-sensitive denoising procedure introduces severe latency, which undermines their major advantage over traditional autoregressive models, acting as cumbersome bottleneck in practice. In this work, we introduce Seed Diffusion Preview, code-focused language model designed to achieve an elegant balance between speed and quality. Tackling these challenges directly, our model achieves remarkable speed of 2146 tokens/second on H20 GPUs while maintaining competitive performance against similarly-sized standard language models across diverse set of code evaluation benchmarks, establishing new state of the art on the speed-quality Pareto frontier."
        },
        {
            "title": "2 Related Work",
            "content": "Non-autoregressive (NAR) models have long been considered an alternative to sequential decoding, valued for their potential of parallel inference. In the pre-LLM era, many early NAR methods demonstrated strong performance on specific tasks such as machine translation [1618]. However, these approaches often lacked rigorous theoretical foundation for density estimation, which limited their viability as general-purpose probabilistic language models. Discrete diffusion models [1114, 19] have emerged to close this gap. By optimizing the Evidence Lower Bound (ELBO), they provide principled probabilistic framework for language modeling. The recent success of large-scale systems such as Mercury Coder [1] and Gemini Diffusion [2] is particularly notable. These models show that it is possible to narrow the quality gap with autoregressive systems while offering substantial speedup, thereby challenging the conventional wisdom on \"quality-speed trade-off\", raising new interest in NAR in the modern LLM era."
        },
        {
            "title": "3 Seed Diffusion",
            "content": "As the first experimental model in our Seed Diffusion series, Seed Diffusion Preview is specifically focused on code generation, thus adopting the data pipelines (code/code-related data only) and processing methodology of the open-sourced Seed Coder project [20]. The architecture is standard dense Transformer, and we intentionally omit complex components such as LongCoT reasoning in this initial version to first establish strong and efficient performance baseline. This section introduces its key components and training strategies."
        },
        {
            "title": "3.1 TSC: A Two-Stage Curriculum for Robust Diffusion Training\nThe first stage is scaled diffusion training. Seed Diffusion Preview is a discrete-state diffusion language model\ntrained with two types of forward corruption process. Given an initial data sequence x0 ∼ pdata and continuous\ntimestep settings where t ∼ [0, 1], the forward process implied by the marginal q(xt|x0) is defined as follows:",
            "content": "Mask-based Forward Process For the first 80% diffusion training steps, we use standard mask-based corruption process [12, 15]. This process gradually replaces tokens in the original sequence x0 with special [MASK] token(m). The corrupted sequence xt is sampled from the conditional distribution q(xtx0) where each token is treated independently: qmask(xtx0) = x0 (cid:89) i=1 qmask(xt[i]x0[i]) (1) The probability of token remaining unchanged or being masked is determined by noise schedule γt. For any position i, the marginal probability is: q(xt[i] = cx0[i]) = (cid:40) 1 γt γt if = x0[i] if = (2) γt refers to the noise schedule function designed to be monotonically increased [16, 21]. Edit-based Forward Process For the last 20% diffusion training steps, we add an extra edit-based corruption process as augmentation to improve calibration and eliminate unexpected behavior such as repetitions in the sampling process. Similar to mask-based approaches, we control designed signal-to-noise ratio based on Levenshtein distance, dLev(xa, xb), which measures the minimum number of token-level edits required to change one sequence into another (refer to [16] for more insights). The forward process then samples corrupted sequence based on predefined edit operation set (e.g., deletions, insertions, and substitutions) and defines the total edit-operation number as kt to approximately control langevin distance. The qedit(xtx0) is implicitly defined by: z0 = x0; zj = oj(zj1) for = 1, . . . , kt, O; xt = zkt (3) is predefined operations set. Although applying kt edits does not guarantee that the final Levenshtein distance L(x0, xt) is exactly kt (e.g., an insertion followed by deletion can cancel out), it provides tractable and scalable method to control the corruption level. The target number of edits kt is scheduled as: kt = x0 (αt) (4) Here αt denotes the scheduler for the approximate signal-to-noise ratio. As an auxiliary objective, we make αt lie in the range [0, 0.1] to maintain the density estimation ability of mask-based forward process. Overall Learning Objective The reverse process is parameterized as pθ(xsxt). We set the predicted probability over the mask token always as 0 [12] by adding inf to the corresponding logits. With this formulation, the mask-based forward process implies an analytical posterior, hence tractable and simple formulation ELBO: LELBO = log pθ (x0 x0) (cid:125) (cid:123)(cid:122) Recontruct Loss (cid:124) Eqmask,t γ γt x0 (cid:88) i=1 1 [xt[i] = m] log pθ (x0[i] xt[i]) (5) Our learning objective derives from ELBO in Eq. 6 by substituting the reconstruct loss with denoised loss based on the edit-based forward process qedit as: Ldiff(θ) = Eqedit,t log pθ(x0xt) Eqmask,t γ γt x0 (cid:88) i= 1 [xt[i] = m] log pθ (x0[i] xt[i]) (6) 3 Remark 3.1 Unlike some prior work [12, 13], we do not employ the strategy of \"Carry Over Unmasking\", i.e. directly copying unmasked input tokens to the output, despite potential benefits to perplexity. While purely mask-based diffusion process offers low-variance training objective (each position is either the ground truth or [MASK] token), it introduces detrimental inductive bias. Such model learns spurious correlation that unmasked tokens are always correct, leading to overconfidence and unable to perform self-correction during inference. To mitigate this, our edit-based augmentation forces the model to re-evaluate all tokens, including those unmasked."
        },
        {
            "title": "3.2 Tailoring the Trajectory Space of Diffusion",
            "content": "An important perspective to understand the essentials of mask-based diffusion models is its equivalence to any-order autoregressive modeling, which has been revealed and illustrated by [22]. This perspective builds the correlation between ELBO and an expected log-likelihood of any-order autoregressive models as: (θ) = EπU (Sd) d=x (cid:88) log pθ(xπ(r) xπ(<r)) (7) r=1 Here Sd stands for the symmetric group of all possible permutations π over {0, 1, . . . , 1}. pθ(xπ(r) xπ(<r)) models the conditional probability of xπ(r) given all preceding tokens in the permutations π denoted as xπ(<r). Now recall that with the transition probability qmask(xtxs), > denotes as: qmask(xt[i] xs[i]) = 1γt 1γs γtγs 1γs 1 if xt[i] = xs[i] = if xt[i] = and xs[i] = if xt[i] = xs[i] = (8) Intuitively, we interpret any trajectory τ with elements τ = {x0 xi xK} (xK are all mask tokens) obtained based on the above transition conditional probability as an order of autoregressive decomposition. Mask-based diffusion training presents more complex learning problem than standard left-to-right autoregressive (AR) training. By design, diffusion models must learn from all possible generation orders, including many that are redundant, detrimental, or misaligned with the natural structure of language [23]. Consequently, diffusion-trained language models lag significantly behind their AR counterparts, even on code data that lacks strong left-to-right prior. This persistent gap presents fundamental challenge for the diffusion approach. We propose constrained-order diffusion training process after the two-stage diffusion learning. This procedure involves creating distilled dataset of optimal generation trajectories. Specifically, for any given sample, candidate pool of trajectories is generated at scale using the pre-trained diffusion model. selection criterion based on maximizing the Evidence Lower Bound (ELBO) is applied to filter this pool, and the resulting high-quality trajectories are used to fine-tune the model. With the high-quality synthesized trajectories as T, the constrained-order training takes the form of the following: Lc(θ) = Eτ (T),(xi,x0)τ λ(xi) log pθ(x0f (xi)) λ(xi) is the weight for balancing loss toward different noise levels, and is an augmentation function similar to qedit in Section. 3.1. (9)"
        },
        {
            "title": "3.3 On-policy Diffusion Learning",
            "content": "Although in theory discrete diffusion models should offer the advantage of parallel decoding, in practice realizing this potential is challenging. single parallel inference step is computationally expensive, meaning large number of tokens must be generated simultaneously to amortize this overhead in order to achieve actual inference efficiency gains. Reducing the total number of generation steps for better efficiency, however, can result in severe degradation in performance, especially in mask-based approaches [15]. To fully unlock the parallel power, we propose simple yet effective on-policy learning paradigm: for the reverse process parameterized with θ, we optimize the objective as: 4 promptpdata τ pθ(prompt) [τ (τ [0])] (10) Here τ = {τ [K], , τ [i], , τ [0]} is the sampled trajectory of the reverse process with strategic sampling strategy, and the model θ is conditioned on the given prompts. The trajectory starts from the sequence with all mask tokens, and the final generated samples are τ [0]. τ denotes the sample steps and () represents model-based verifier that ensures the sampling process always converges to reasonable/correct sample. The verifier-related term (V (τ [0])) in Equation. 10 can be optimized with the log-derivative trick [24]. We observed that directly minimizing trajectory length led to unstable training dynamics. Therefore, we optimize progressive surrogate loss based on the fact that τ Ei,j{0, ,K} 1 dLev(τ [i], τ [j]) (11) The speed-up dynamics during on-policy training is illustrated in Figure 2a. Interestingly, this procedure has an effect analogous to mode filtering, technique previously explored in the non-autoregressive (NAT) text generation literature [16, 25]. (a) The changes of speedup ratio estimated by sampling with certain block size during the on-policy training. (b) The relative forward time( (B=b) different block size(b) (B=1) ) changes over the Figure 2 On-policy Training Dynamics & Block-wise Inference Time"
        },
        {
            "title": "3.4 Inference and Infrastructure",
            "content": "To balance computation and latency, we employ block-level parallel diffusion sampling scheme that maintains causal ordering between blocks. For generating tokens in the n-th block (Bn), the reverse process is expressed as pθ(xtxs, xB0, ,Bn ). Here xB0, ,Bn denotes the previously generated block of tokens with xB0 = . This semi-autoregressive (semi-AR) process is well-established technique for balancing generation quality and efficiency [15, 26, 27]. We avoid block-specific training[26, 27] to retain flexibility for arbitrary block partitioning during inference. We use KV-caching for previously generated blocks to condition subsequent ones. Although this risks potentially introducing potential bias, we empirically observe no significant degradation in generation quality. This robustness is probably due to the distillation of constrained-order trajectories as introduced in Section 3.2. Beyond algorithmic design, our work employs holistic system optimization to support block-level inference efficiently. Specifically, we leverage our internal infrastructure framework, featuring specialized optimizations 5 Table 1 Performance on Aider (\"whole\" format) and CanItEdit. Model Size Aider tries=2 CanItEdit pass@1 CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Coder-14B-Instruct StarCoder2-15B-Instruct Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Seed-Diffusion-Preview(0705) 15B Models 7B 6.7B 7B 9B 14B 15B 8B 8B 7B 8B 8B - Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct 15B+ Models 22B 70B 33B 2.4B/16B 1.5 44.4 38.3 54.1 69.2 38.2 33.1 30.8 57.9 55.6 57.1 44.4 51.1 15.0 54.5 52.6 25.7 36.9 34.8 50.5 52.9 31.4 39.5 39.0 49.5 45.7 50. 54.3 52.4 40.5 46.2 45.2 for diffusion sampling, to accelerate generation. The impact on performance across different block sizes is detailed in Figure 2b. This analysis informs our selection of the optimal block size, determined by the trade-off between the latency of single forward pass and the corresponding token generation rate."
        },
        {
            "title": "4 Experiments",
            "content": "We benchmark the performance and decoding speed of Seed Diffusion across range of code-related tasks. Our evaluation protocols and primary baselines are adapted from [20]. To provide comprehensive comparison, we also include state-of-the-art Diffusion Language Models for experiments: Mercury [1] and Gemini-Diffusion [2]."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "To provide rigorous assessment of Seed Diffusion Preview, we evaluate its performance across diverse suite of code generation benchmarks: HumanEval and MBPP We present HumanEval and MBPP results for the evaluation of basic coding ability. BigCodeBench BigCodeBench [28] is recent benchmark that assesses LLMs on real-world programming tasks involving multi-tool use. It features 1,140 Python tasks from 7 domains, requiring models to utilize 139 different libraries. The benchmark emphasizes compositional reasoning and is evaluated with notable rigor, using an average of 5.6 test cases and 99% branch coverage per task. LiveCodeBench For Competitive Coding tasks, we utilize LiveCodeBench [29], which continuously curates new problems from prominent competitive programming platforms including LeetCode, AtCoder and CodeForces. Crucially, it also time-stamps each problem with its release date. This temporal tagging enables the creation of contamination-free evaluation slices, ensuring models are assessed only on problems published after their training data cutoff. We provide the evaluation of all stage \"v1-v6\" and the most recent stage \"v6\" (250201-250501). MBXP The MBXP benchmark [30] was designed for multilingual code evaluation. It adapts the problems and 6 Table 2 Performance on MBXP. Model Size Python Java C++ C# TS JS PHP Go Kotlin Perl Ruby Scala Swift Average CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Coder-14B-Instruct StarCoder2-15B-Instruct Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Seed-Diffusion-Preview(0705) 7B 6.7B 7B 9B 14B 15B 8B 8B 7B 8B 8B - Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct DeepSeek-Coder-V2-Instruct Qwen2.5-Coder-32B-Instruct 22B 70B 33B 2.4B/16B 21B/236B 32B 15B Models 42.3 64.8 66.7 74.1 50.0 55.9 64.4 70.3 80.1 21.7 56.6 71.0 71.5 68.9 74.2 70.3 77.6 20.7 59.1 67.6 72.2 73.0 72.8 73.0 15B+ Models 71.7 47.8 72.4 72.4 74.8 70.1 69.2 69.9 72.4 72. 73.5 78.3 32.9 30.9 66.8 79.1 84.8 25.9 59.1 71.3 74.1 72.8 77.0 72.6 77.3 68.6 76.8 75.3 77.6 86. 38.8 52.2 66.6 73.4 77.5 25.1 59.8 68.1 70.5 69.0 72.7 67.7 73.6 66.6 71.8 73.3 78.2 80.4 54.0 74.9 77.7 82.0 86.2 78.0 70.1 79.1 83.5 77.0 85.2 79. 78.2 77.8 80.4 82.8 89.4 90.2 45.5 64.7 67.5 73.3 77.7 59.8 59.1 61.4 74.1 73.8 78.8 76.6 68.5 62.5 69.8 73.1 80.5 79. 36.6 25.8 67.3 76.4 79.7 53.5 62.5 68.1 74.2 72.3 74.7 74.7 74.9 70.5 75.1 75.1 75.8 87.6 48.8 93.8 55.1 90.9 97.1 90.4 85.7 94.4 96.0 92.9 95.5 92. 97.1 77.7 96.4 95.1 89.1 96.4 47.2 59.6 60.9 64.4 75.3 46.7 52.2 66.4 65.5 62.0 73.4 71.2 71.0 57.2 70.1 69.9 74.5 75.6 50.1 3.3 61.1 60. 76.2 31.9 42.6 56.1 64.4 64.6 72.5 71.2 66.6 51.1 66.6 61.6 70.7 74.7 36.9 65.9 65.9 67.3 79.3 56.1 55.9 70.5 75.5 69.0 78.0 72.5 74.2 67.0 75.1 74.5 80. 83.4 40.2 54.8 60.0 63.5 73.1 43.2 44.5 63.1 64.2 63.1 70.3 67.0 64.4 51.3 64.6 63.5 67.9 63.3 33.2 47.4 54.7 57. 67.2 42.0 31.8 56.7 62.0 42.2 54.2 54.2 50.1 48.7 54.3 55.0 59.0 66.7 42.8 53.4 64.2 71.8 79.4 45.8 56.8 68.8 72.9 69.3 75.3 72. 72.1 62.8 72.6 72.6 76.2 79.7 unit tests from the original, Python-centric MBPP benchmark for usage across more than ten programming languages. Table 3 Performance on NaturalCodeBench. Model Size NCB (zh) NCB (en) Total Python Java Total Python Java Total CodeLlama-7B-Instruct DeepSeek-Coder-6.7B-Instruct Yi-Coder-9B-Chat Qwen2.5-Coder-14B-Instruct StarCoder2-15B-Instruct Llama-3.1-8B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-8B Seed-Coder-8B-Instruct Seed-Diffusion-Preview(0705) 7B 6.7B 9B 14B 15B 8B 8B 7B 8B 8B - Codestral-22B CodeLlama-70B-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-V2-Lite-Instruct 22B 70B 33B 2.4B/16B 15B Models 18.6 38.6 41.4 48.6 44.3 27.1 40.0 34.3 37.1 55.7 52.9 15B Models 40.0 35.1 44.3 41.4 8.6 31.4 45. 48.6 30.0 24.3 30.0 37.1 32.9 45.7 38.6 44.3 32.1 38.9 47.1 13.6 35.0 43.6 48.6 37.2 25.7 35.0 35.7 35.0 50.7 45.8 42.2 33.6 41. 44.3 17.1 32.9 38.6 42.9 38.6 22.9 35.7 34.3 34.3 50.0 45.7 41.4 32.8 44.3 41.4 14.3 32.9 44.3 45.7 42.9 22.9 24.3 35.7 38. 47.1 38.6 45.7 30.5 44.3 37.1 15.7 32.9 41.5 44.3 40.8 22.9 30.0 35.0 36.5 48.6 38.6 43.6 31.7 44.3 39. 14.6 33.9 42.5 46.4 39.0 24.3 32.5 35.4 35.7 49.6 42.2 42.9 32.6 43.0 41.8 NaturalCodeBench NaturalCodeBench (NCB) [31] was developed to provide more realistic evaluation environment through curated set of 402 problems in Python and Java, derived from genuine user queries. Its problems span across six key domains and employ complex test inputs, including varied file types and data structures. Aider and CanItEdit To assess code-editing capabilities, we use the Aider and CanItEdit benchmark. Aider 1 features 133 coding exercises from Exercism, where model must edit existing code. The primary challenge is 1https://aider.chat/docs/leaderboards/edit.html 7 that the models modifications must be formatted for automated application without any human intervention. Meanwhile, the CanItEdit benchmark [32] provides rigorous evaluation of models instructional codeediting capabilities. It comprises 105 hand-crafted problems with mix of \"descriptive\" (explicit) and \"lazy\" (ambiguous) instructions."
        },
        {
            "title": "4.2 Performance",
            "content": "Seed-Diffusion-Preview has demonstrated huge potential of diffusion for code generation. As shown in Tables 1, 2, 3 and Fig. 1, our model not only achieves performance comparable to advanced autoregressive models while operating at significantly higher speeds, but also delivers notable boost on editing tasks. These results mark discrete diffusion as promising direction for future exploration."
        },
        {
            "title": "5 Discussion",
            "content": "This work presents the key technical components of an experimental model from our Seed Diffusion project, demonstrating its potential for significant inference acceleration in large-scale language models. We posit that faster inference is merely the most immediate benefit of discrete diffusion. Exploring alternatives to the conventional left-to-right modeling order represents valuable research direction, as it involves moving away from pervasive, human-centric assumption in machine learning. Unlocking the full capabilities of discrete diffusion will require significant efforts from the community, particularly in exploring its scaling properties and its applications to complex reasoning tasks."
        },
        {
            "title": "Contributions",
            "content": "Project Lead Yuxuan Song1,2,3, Zheng Zhang1,3 (Alphabetical Order) Core Contributor Yuxuan Song1,2,3, Zheng Zhang1,3, Cheng Luo1 Contributor Pengyang Gao1, Fan Xia1, Hao Luo1, Zheng Li1, Yuehang Yang1, Hongli Yu1,2,3, Xingwei Qu1, Yuwei Fu1, Jing Su1, Ge Zhang1, Wenhao Huang1 Supervision Mingxuan Wang1,3, Lin Yan1, Xiaoying Jia1, Jingjing Liu2,3, Wei-Ying Ma2,3, Ya-Qin Zhang2,3, Yonghui Wu1, Hao Zhou2,3 Affiliation 1ByteDance Seed 2Institute for AI Industry Research (AIR), Tsinghua University 3SIA-Lab of Tsinghua AIR and ByteDance Seed"
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the Seed-Coder team for their help with the data pipelines and our many colleagues at ByteDance for their support of the Seed Diffusion project."
        },
        {
            "title": "References",
            "content": "[1] Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv e-prints, pages arXiv2506, 2025. [2] Google DeepMind. https://blog.google/technology/google-deepmind/gemini-diffusion/. https://blog.google/ technology/google-deepmind/gemini-diffusion/, 2025. Accessed: 2024-07-24. [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020. [4] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 1191811930, 2019. [5] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015. [6] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [7] Mihaly Varadi, Damian Bertoni, Paulyna Magana, Urmila Paramval, Ivanna Pidruchna, Malarvizhi Radhakrishnan, Maxim Tsenkov, Sreenath Nair, Milot Mirdita, Jingi Yeo, et al. Alphafold protein structure database in 2024: providing structure coverage for over 214 million protein sequences. Nucleic acids research, 52(D1):D368D375, 2024. [8] Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow networks. arXiv preprint arXiv:2308.07037, 2023. [9] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022. [10] Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36, 2024. [11] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. [12] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. [13] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. [14] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [15] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [16] Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. Glancing transformer for non-autoregressive neural machine translation. In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), July 2021. [17] Fei Huang, Hao Zhou, Yang Liu, Hang Li, and Minlie Huang. Directed acyclic transformer for non-autoregressive machine translation. In International Conference on Machine Learning, pages 94109428. PMLR, 2022. [18] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019. [19] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. 2023. 10 [20] Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, et al. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524, 2025. [21] Lihua Qian, Mingxuan Wang, Yang Liu, and Hao Zhou. Diffusion glancing transformer for parallel sequence-tosequence learning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 48464862, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [22] Emiel Hoogeboom, Alexey Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021. [23] Ning Miao, Yuxuan Song, Hao Zhou, and Lei Li. Do you have the right scissors? tailoring pre-trained language models via Monte-Carlo methods. In the 58th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers, July 2020. [24] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Journal of Machine Learning Research, 21(132):162, 2020. [25] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. In ICLR, 2018. [26] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [27] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. [28] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. [29] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [30] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868, 2022. [31] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts. arXiv preprint arXiv:2405.04520, 2024. [32] Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, et al. Can it edit? evaluating the ability of large language models to follow code editing instructions. arXiv preprint arXiv:2312.12450, 2023."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Institute for AI Industry Research (AIR), Tsinghua University",
        "SIA-Lab of Tsinghua AIR and ByteDance Seed"
    ]
}