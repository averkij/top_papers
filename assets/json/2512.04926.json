{
    "paper_title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "authors": [
        "Yueming Pan",
        "Ruoyu Feng",
        "Qi Dai",
        "Yuqi Wang",
        "Wenfeng Lin",
        "Mingyu Guo",
        "Chong Luo",
        "Nanning Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/."
        },
        {
            "title": "Start",
            "content": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion Yueming Pan1,2 * , Ruoyu Feng3 , Qi Dai2, Yuqi Wang3, Wenfeng Lin3, Mingyu Guo3, Chong Luo2 , Nanning Zheng1 1IAIR, Xian Jiaotong University 2Microsoft Research Asia 3ByteDance 5 2 0 D 5 ] . [ 2 6 2 9 4 0 . 2 1 5 2 : r (a) Overview of Semantic-First Diffusion (SFD). Semantics (dashed curve) and textures (solid curve) follow asynchronous Figure 1. denoising trajectories. SFD operates in three phases: Stage Semantic initialization, where semantic latents denoise first; Stage II Asynchronous generation, where semantics and textures denoise jointly but asynchronously, with semantics ahead of textures; Stage III Texture completion, where only textures continue refining. After denoising, the generated semantic latent s1 is discarded, and the final image is decoded solely from the texture latent z1. (b) Training convergence on ImageNet 256256 without guidance. SFD achieves substantially faster convergence than DiT-XL/2 and LightningDiT-XL/1 by approximately 100 and 33.3, respectively."
        },
        {
            "title": "Abstract",
            "content": "Latent Diffusion Models (LDMs) inherently follow coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit the texture generation by providing semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining the compact semantic latent, which is extracted from pretrained visual encoder via dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate *This work was performed during Yueming Pans internship at MSRA. Equal contribution. Corresponding author. noise schedules: semantics precede textures by temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiTXXL), while achieving up to 100 faster convergence than original DiT without guidance. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/ SFD.github.io/. 1. Introduction Latent Diffusion Models (LDMs) [37] have emerged as the leading approach for modeling visual signals, demonstrating remarkable performance in high-quality image synthesis [28, 32, 37]. LDMs comprise two key components: Variational Autoencoder (VAE) [21] that compresses highdimensional visual signals into compact latent space, and diffusion model that learns the distribution of this latent space. However, this design presents an inherent challenge. The VAE, optimized for pixel-level reconstruction, predominantly captures low-level texture features in its latent representation. Consequently, the diffusion model faces conflicting objective: it must simultaneously capture high-level semantic understanding while preserving low-level textural details, which leads to slow convergence and suboptimal generation quality. To overcome these challenges, recent studies enhance LDMs with discriminative semantic priors from pretrained visual encoders, enabling faster convergence and improved generation quality. These approaches typically achieve this by explicitly aligning semantic representations with the VAE latent space [49] or diffusion intermediate features [24, 50], or by jointly modeling concatenated semantic and texture representations within the diffusion process [22, 47]. All these methods share similar paradigm, i.e., all latent information, including high-level semantics and low-level textures, is denoised synchronously at the same noise level throughout the diffusion process. However, such design deviates from the inherent nature of diffusion model, which follows coarse-to-fine mechanism that progressively generates low-frequency structure before high-frequency textures [30, 36, 41, 46]. Inspired by this natural property, we argue that discriminative semantics, which capture structural and high-level information, should not only be embedded in the latent space as part of the denoising target, but should also actively lead the generation process by evolving earlier than textures. This philosophy is akin to the principle that one should first draw blueprint before engaging in fine decoration, rather than attempting to simultaneously define structure and detail from chaos. In this paper, we propose to explicitly intervene in the order of information formation during generation, where discriminative semantics are synthesized first and serve as priors to guide texture generation. However, hard sequential generation scheme would exhibit training-inference mismatch similar to exposure bias in teacher forcing [34]: the model is trained with ground-truth semantic conditions but must generate based on its own imperfect predictions at inference, leading to performance degradation. To address this, we introduce asynchronous diffusion to harmonize the joint modeling of semantics and textures: semantics evolve ahead to guide texture synthesis, while both denoise simultaneously at different noise levels. Motivated by these insights, we propose Semantics-First Diffusion (SFD), new paradigm for LDMs that consists of two key components. First, an explicitly constructed composite latent space that combines discriminative semantics and low-level textures. Second, asynchronous diffusion guided by cleaner semantic information. Specifically, for the first component, building upon texture VAE (e.g., SDVAE), we introduce dedicated Semantic VAE (SemVAE) that compresses high-dimensional semantic representations from pretrained visual encoders into compact latent space, which is then concatenated with the texture latents. For the second component, as illustrated in Figure 1 (a), threestage asynchronous denoising process is proposed. In the first stage, only the semantic latents are denoised, allowing the model to establish coarse global layout initially. In the second stage, semantics and textures are denoised jointly but at different noise levels. Since semantic features evolve ahead, they provide stronger global guidance for texture refinement. In the third stage, with semantics fully denoised, only textures continue refining details. Finally, the output image is decoded solely from the texture latent. Our contributions are summarized as follows: We design composite latent space composed of semantic latents from dedicated Semantic VAE and texture latents from SD-VAE, where the Semantic VAE compresses high-level features from pretrained visual encoders into compact representations while largely preserving semantic integrity and spatial layout. We propose the semantic-first asynchronous diffusion mechanism, which employs three-stage denoising schedule where semantics evolve earlier and subsequently guide texture generation. SFD achieves state-of-the-art FID score of 1.04 on ImageNet 256256, while demonstrating 100 and 33.3 faster training convergence compared to DiT and LightningDiT, respectively. We validate the effectiveness and generalizability of SFD by integrating it into existing synchronous diffusion models like ReDi and VA-VAE, thus improving their performance. 2. Related Work 2.1. Diffusion Models for Image Generation Probabilistic diffusion models synthesize images by iteratively denoising from Gaussian noise. Early models [14, 42] operate in pixel space, suffering from high computational cost and slow convergence. Latent Diffusion Models (LDMs) [37] mitigate this by performing diffusion in VAE-compressed latent space, greatly improving efficiency and visual fidelity. Building upon this foundation, DiT [32] and SiT [28] replaced the U-Net backbone [38] with Vision Transformers, demonstrating superior scalability and generative capacity. More recent efforts have sought to accelerate convergence by optimizing the diffusability of latent representations. For instance, [41] regularizes the frequency spectrum of the latent space to make it more compatible with diffusion dynamics. Despite these advances, standard LDMs still treat all latent components uniformly during denoising, leaving the coarse-to-fine nature [30] of the diffusion synthesis process implicit. In contrast, SFD explicitly models this hierarchical evolution via asynchronous denoising: high-level semantic components are denoised earlier and progressively guide the refinement of low-level textures at cleaner noise levels, thereby accelerating convergence and improving generation quality. 2.2. Semantic Representation Enhanced Diffusion Motivated by the discriminative gap between generative models and pretrained visual encoders, parallel line of research seeks to enhance diffusion models with external semantic representations. REPA [50] performs featurespace alignment between diffusion features and pretrained visual encoders. REPA-E [24] extends this alignment by enabling end-to-end joint optimization of the VAE and diffusion model. REG [47] and ReDi [22] jointly learn the distribution of low-level VAE features and high-level semantic features from DINOv2 [31], where REG employs the class token as the semantic descriptor while ReDi adopts PCA-compressed patch embeddings. Another line of work focuses on improving or changing the VAE latent space. VA-VAE [49] aligns latent space with pretrained vision foundation models to to enrich its semantic representations. RAE [51] and SVG [40] replace the conventional VAE with pretrained visual encoder representations, where RAE adapts the diffusion transformer with wide DDT head, while SVG employs residual branch to capture finegrained details. Collectively, these approaches reveal that integrating semantic signals into the diffusion process benefits generation. We extend this paradigm further with SFD, which introduces asynchronous denoising schedules that allow semantics to be established earlier and guide the refinement of low-level textures throughout generation. 2.3. Asynchronous Denoising Methods Asynchronous denoising allows different components (e.g., tokens, spatial regions, pixels) to evolve under distinct noise schedules rather than enforcing strict synchronicity. Diffusion Forcing [2] assigns each token an independent noise level and enables arbitrary per-token denoising schedule, combining the strengths of next-token prediction models and full-sequence diffusion models. AsynDM [16] dynamically modulates per-pixel timestep schedules, allowing prompt-related regions to denoise more gradually and thereby improving text-to-image alignment. In this paper, SFD applies asynchronous denoising to semantic and texture subspaces within the latent representation, enabling early semantic guidance during denoising while preserving simple and unified latent diffusion architecture. 3. Method We propose Semantic-First Diffusion (SFD), which employs asynchronous denoising to harmonize semantic and texture modeling, achieving faster convergence and superior performance without sacrificing reconstruction fidelity. Figure 2. Architecture of the Semantic VAE (SemVAE). Transformer-based VAE compresses high-dimensional vision foundation model (VFM) features into compact semantic latents. Section 3.1 introduces the preliminaries. Section 3.2 describes our Semantic VAE and composite latent construction, and Section 3.3 presents the complete SFD framework. 3.1. Preliminaries Flow matching-based [1, 8, 27, 28] diffusion models learn to reverse continuous noising process that transforms clean data into Gaussian noise. Following the flow matching formulation, the forward process is modeled as linear interpolation: xt = tx1 + (1 t)x0, (1) where x1 p(x) denotes clean data sampled from the data distribution, x0 (0, I) denotes sampled Gaussian noise, and [0, 1] denotes time. Generation starts at = 0 with random noise and follows the learned velocity field toward clean data at = 1. The evolution of xt is governed by the velocity field: v(xt, t) = E[ xt xt] = E[x1 x0 xt]. (2) This velocity field uniquely defines the associated ordinary differential equation (ODE) or stochastic differential equation (SDE) for sampling. It is also closely related to the score function s(xt, t) through simple proportional relationship, so estimating either one is sufficient. During training, neural network vθ(xt, t) is optimized to approximate the velocity field by minimizing: Lvel(θ) = (cid:90) 1 0 Ex1,x0 (cid:2)vθ(xt, t) (x1 x0)2 (cid:3) dt. (3) During inference, numerical solver integrates this learned velocity field from noise to data. 3.2. Composite Latent Construction 3.2.1. Semantic VAE To comprehensively leverage high-level semantics from pretrained vision foundation models, we introduce dedicated Semantic VAE (SemVAE) specifically designed to compress rich semantic features into compact latent representations while maintaining their spatial layout and minimizing information loss. Figure 2 illustrates the architecture of SemVAE. Given an input image x1, we extract its patch-level semantic features fs = (x1) RLCin via frozen vision foundation model (VFM) denoted as (), where denotes the number of flattened patches and Cin is the VFM feature dimension. The SemVAE encoder Es(), consisting of linear projection layer, four Transformer blocks, LayerNorm layer, and an output linear layer, maps these features to lower-dimensional latent space: hs = Es(fs), (4) where hs RL2Cs and Cs denotes the latent dimension. The factor of 2 accounts for the mean and variance parameters. The encoder outputs the parameters of Gaussian distribution. Specifically, hs is split into mean and variance: Figure 3. Composite Latent Construction. An input image is encoded into semantic and texture latents via distinct VAE encoders, which are then concatenated to form composite latent for asynchronous diffusion modeling. 3.2.2. Latent Construction As illustrated in Figure 3, the composite latent is constructed by combining compressed high-level semantics s1 and low-level textures z1, which are encoded via SemVAE encoder Es and texture VAE encoder Ez, respectively. Here we implement SD-VAE [37] as the texture VAE. The two latents are concatenated along the channel dimension: µ, σ2 = hs[:, : Cs], hs[:, Cs :], (5) = [s1, z1] RL(Cs+Cz), (11) and the latent variable s1 RLCs is sampled via the reparameterization trick [20]: where [, ] denotes channel-wise concatenation, and Cs, Cz are the semantic and texture channel dimensions. s1 = µ + σ ϵ, ϵ (0, I). (6) 3.3. Semantic-First Diffusion The SemVAE decoder Ds() mirrors the encoder architecture and reconstructs the original VFM features from the latent variables: ˆfs = Ds(s1), ˆfs RLCin . (7) The core motivation of Semantic-First Diffusion (SFD) is to enable semantic latents to be denoised ahead of texture features, thereby providing clearer structural guidance throughout the asynchronous generation process. We describe its key components below. Training Objective. The SemVAE is trained with combination of reconstruction and regularization losses. The reconstruction quality is ensured by MSE loss and cosine similarity loss: LMSE = ˆfs fs2, Lcos = 1 ˆfs fs ˆfsfs , (8) where LMSE enforces reconstruction fidelity, while Lcos ensures directional alignment of the feature vectors. The KL divergence regularizes the latent space: LKL = DKL(q(s1fs)N (0, I)) (cid:0)µ2 log σ2 + σ2 (cid:88) = 1(cid:1) . 1 2 The total training loss is: Distinct timesteps for semantics and textures. To model semantics and textures asynchronously with fixed temporal offset while ensuring both timesteps remain within [0, 1], distinct timesteps ts and tz are assigned to the semantic and texture latents during training. Specifically, for each image, we first sample the semantic timestep ts from an extended interval, then derive the texture timestep tz by subtracting the offset t, and finally clamp both to [0, 1]: ts U(0, 1 + t), tz = max(0, ts t), ts = min(ts, 1), (12) (13) (14) (9) which ensures ts, tz [0, 1] and ts tz. This guarantees the semantic latent experiences less noise corruption than the texture latent at each denoising step, thereby providing clearer structural guidance for texture denoising. LSemVAE = LMSE + Lcos + λklLKL. (10) λkl is set as 107 by default. Once trained, SemVAE is frozen during diffusion model training. Diffusion transformer with dual timesteps. As shown in Figure 4, the diffusion model adopts Transformer backbone vθ() that takes as input the noisy composite latent Three-phase denoising schedule. During inference, SFD employs three-phase asynchronous denoising schedule, as illustrated in Figure 1(a): 1. Semantic initialization, where ts [0, t), tz = 0: Only semantic latents are denoised to establish global structural guidance. 2. Asynchronous generation, where ts [t, 1], tz [0, 1 t): Both semantic and texture latents are denoised jointly yet asynchronously, with semantics advancing slightly ahead to provide clearer structural guidance for texture generation. 3. Texture completion, where ts = 1, tz [1 t, 1]: With semantic latents fully denoised, noisy texture latents continue to refine fine-grained details. Formally, two binary masks Ms {0, 1}BCsHW and Mz {0, 1}BCzHW are introduced to control the denoising updates of semantic and texture latents, respectively. According to the three-phase asynchronous denoising schedule, the masks (Ms, Mz) are defined as: [Ms, Mz] = [1, 0], ts [0, t), tz = 0, [1, 1], ts [t, 1], tz [0, 1 t), [0, 1], ts = 1, tz [1 t, 1], (19) where 1 and 0 denote all-one and all-zero tensors with shapes matching Ms and Mz, respectively. The masked velocity for updating is then computed as: (cid:3), ˆv = (cid:2)Ms ˆvs, Mz ˆvz (20) where denotes element-wise multiplication. This mechanism explicitly controls which latents denoise at each phase, ensuring semantic latents denoise earlier to guide texture refinement continuously. By enabling asynchronous yet coordinated updates between semantic and texture latents, SFD achieves more stable optimization and naturally aligns with the coarse-to-fine generation paradigm of diffusion models. Notably, while SFD extends the denoising timestep range by t, we proportionally increase the interval between successive steps, keeping the total number of diffusion steps fixed. Therefore, no additional denoising steps are required for inference. Upon completion, only the fully denoised texture latent z1 is decoded to the final image. 4. Experiments 4.1. Experimental Setup Implementation details. We employ SD-VAE [37] f16d32 from LightningDiT [49] to encode texture into 32channel latents with 16 spatial downsampling, and the SemVAE encoder (29M parameters) to encode semantic features extracted by DINOv2-B with registers [4, 31] into 16-channel latents. The concatenated 48-channel representation forms unified 256-token latent for each 256 256 Figure 4. Input and output of Diffusion Transformer. DiT backbone takes as input composite latent that combines noisy semantic and texture features [sts , ztz ], along with their respective timestep [ts, tz] and class label y. It jointly predicts the velocities of both semantics and textures. [sts , ztz ] at different noise levels, two separate timesteps [ts, tz], and the class label y: [ˆvs, ˆvz] = vθ (cid:0)[sts, ztz ], [ts, tz], y(cid:1), (15) where ˆvs and ˆvz denote the predicted velocities of the semantic and texture components, respectively. Training objective. The training objective combines velocity prediction losses for both semantic and texture latents: Lpred = Es0,s1,z0,z1,ts,tz + β (cid:13) (cid:13)ˆvs (s1 s0)(cid:13) (cid:13) 2(cid:105) , (cid:104)(cid:13) (cid:13)ˆvz (z1 z0)(cid:13) 2 (cid:13) (16) where s0 (0, I), z0 (0, I) are sampled from the prior, and β is weighting hyperparameter. Additionally, the representation alignment loss from REPA [50] is employed, which aligns the diffusion hidden states with pretrained vision encoder representations. Formally, it is defined as: LREPA(ψ, ϕ) := Ests ,ztz ,ts,tz (cid:2)Lsim (cid:0)y, hϕ(ht)(cid:1)(cid:3) , (17) where = (x1) denotes the pretrained visual encoder output, ht = fψ([sts, ztz ], [ts, tz]) is the diffusion transformer encoder output, hϕ(ht) projects ht through trainable projection head, and Lsim(, ) is the alignment function. Notably, corresponds to the representation input for SemVAE (Section 3.2.1). Under this formulation, LREPA can be regarded as striving to reconstruct the noisy semantic latents sts back to their clean representations y. Compared to the original REPA, which distills the VFMs analytical capabilities, this explicit reconstruction from semantic latents offers more tractable learning objective, thereby better preserving the integrity of semantic information and enabling more effective utilization of semantic knowledge. The final objective becomes the following: Ltotal = Lvel + λ LREPA. (18) maintaining cooperative optimization. However, increasing beyond 0.3 progressively degrades performance. When = 1.0, the model reduces to teacher-forcing sequential generation scheme, where semantics are fully synthesized before texture generation begins, leading to traininginference mismatch and suboptimal results. Overall, these results demonstrate that moderate offset (t = 0.3) achieves the optimal trade-off between early semantic stabilization and texture collaboration, better harmonizing the joint modeling of semantics and textures and thereby yielding the best generation quality. Accelerating training convergence. Table 1 presents comprehensive comparison between DiT [32], LightningDiT [49], REPA [50], and our SFD on ImageNet 256256 without guidance. Results for LightningDiT and its REPA variant are our reproductions. SFD consistently achieves superior FID performance while significantly accelerating convergence across all evaluated model scales. For smaller models trained for 400K iterations, SFD reduces FID from 21.45 to 10.40 for LightningDiTB/1 + REPA, and from 7.48 to 3.89 for LightningDiT-L/1 + REPA. In the large-scale setting, LightningDiT-XL/1 with SFD achieves FID 3.53 at only 400K iterations, outperforming LightningDiT-XL/1 with REPA at 4M iterations by 2.31 points (from 5.84 to 3.53) and vanilla DiT-XL/2 at 7M iterations by 6.09 points (from 9.62 to 3.53), with only 10% and 5.7% of the training cost, respectively. Notably, SFD achieves comparable performance to DiT-XL trained for 7M iterations and LightningDiT-XL/1 trained for 4M iterations in just 70K and 120K iterations, achieving 100 and 33.3 faster convergence (see Figure 1(b)). Comparison with SOTA methods. Table 2 presents system-level comparison with recent state-of-the-art methods with guidance. Our proposed SFD achieves both significantly faster convergence and superior generation performance. Remarkably, SFD surpasses DiT-XL trained for 1400 epochs within only 80 epochs, demonstrating exceptional training efficiency. At this early stage (80 epochs), SFD already achieves impressive FID scores of 1.30 with LightningDiT-XL and 1.19 with the 1.0Bparameter LightningDiT-XXL, both surpassing many existing methods. With extended training to 800 epochs, SFD achieves new state-of-the-art results of FID 1.06 with LightningDiT-XL and FID 1.04 with LightningDiT-XXL on ImageNet 256256. Complete comparisons with and without guidance are provided in Appendix B. 4.3. Ablation Studies ablation experiments conducted using the All LightningDiT-XL model trained for 400K iterations on ImageNet 256256. Models are optimized with the are Figure 5. Effect of the temporal offset in asynchronous denoising. moderate offset (t = 0.3) yields the lowest FID, indicating the best semantictexture cooperation. image. We adopt LightningDiT [49] as the diffusion backbone and train on ImageNet-1K [5] with batch size of 256, learning rate of 1 104, and the AdamW optimizer. We set β = 2.0 and = 0.3. For REPA, we set λ = 1.0, the alignment depth to 2, and use cosine similarity as the similarity function. For sampling, the dopri5 solver [7] with adaptive sampling steps is employed, following the implementation in LightningDiT1 [49]. The absolute and relative tolerances are set to 106 and 103, respectively. AutoGuidance [18] is used as the guidance method with DiT-B degradation model. Implementation details of SemVAE and complete configurations of diffusion models are provided in Appendix A. Evaluation protocol. We adopt comprehensive quantitative metrics to assess generation quality: Frechet Inception Distance (FID) [12] for visual realism, structural FID (sFID) [29] for spatial coherence, Inception Score (IS) [39] for class-conditional diversity, Precision (Prec.) for sample fidelity, and Recall (Rec.) for distribution coverage [23]. All metrics are computed on 50K generated samples following the standardized ADM [6] evaluation pipeline. 4.2. Main Results Effect of Temporal Offset in Asynchronous Denoising. We analyze how the temporal offset between semantics and textures influences SFD performance. In this experiment, we set the learning rate to 2 104 and train for 400K iterations to accelerate convergence. As shown in Figure 5, when = 0, SFD degenerates to conventional joint denoising of semantic and texture representations, similar to ReDi [22] and REG [47]. As increases, FID gradually decreases and reaches its optimal value of 3.03 at = 0.3, corresponding to our proposed semantics-leadIn this setting, semantics evolve slightly texture scheme. ahead of textures, providing clearer global guidance while 1https://github.com/hustvl/LightningDiT FID comparison on ImageNet Table 1. 256256 without guidance across various model sizes for DiT with REPA and SFD (ours). Table 2. System-level comparison of class-conditional generation on ImageNet 256256 with guidance. Performance metrics are annotated with (higher is better) and (lower is better). Model #Params Iter. FID Model Epochs #Params FID sFID IS Pre. Rec. DiT-B/2 LightningDiT-B/1 + REPA + SFD (Ours) DiT-L/2 LightningDiT-L/1 + REPA + SFD (Ours) DiT-XL/2 DiT-XL/2 LightningDiT-XL/1 LightningDiT-XL/1 LightningDiT-XL/1 LightningDiT-XL/1 + REPA + REPA + REPA + REPA + SFD (Ours) + SFD (Ours) + SFD (Ours) + SFD (Ours) + SFD (Ours) + SFD (Ours) 130M 130M 130M 130M 458M 458M 458M 458M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 400K 43.47 400K 22.86 400K 21.45 400K 10. 400K 23.33 400K 10.08 7.48 400K 3.89 400K 400K 19.47 9.62 7M 9.29 400K 7.48 1M 6.88 2M 6.50 4M 6.94 400K 6.17 1M 5.87 2M 5.84 4M 8.79 70K 6.22 120K 3.53 400K 2.82 1M 2.74 2M 2.54 4M VAR [43] MAR [26] xAR [35] DiT-XL [32] MaskDiT [52] SiT-XL [28] FasterDiT [48] MDT [9] MDTv2 [10] DDT [46] Autoregressive Models 350 800 2.0B 943M 1.1B 1.80 1.55 1.24 Latent Diffusion Models 1400 1600 1400 400 1300 1080 400 675M 675M 675M 675M 675M 675M 675M 2.27 2.28 2.06 2.03 1.79 1.58 1. Leveraging Visual Representations VA-VAE [49] REPA [50] REPA-E [24] ReDi [22] REG [47] RAE [51] (DiT-XL) RAE [51] (DiTDH-XL) SFD (XL) SFD (XL) SFD (XXL) SFD (XXL) 800 800 800 800 800 800 800 80 800 80 800 675M 675M 675M 675M 677M 676M 839M 675M 675M 1.0B 1.0B 1.35 1.42 1.12 1.61 1.36 1.41 1.13 1.30 1.06 1.19 1.04 - - - 4.60 5.67 4.50 4.63 4.57 4.52 - 4.15 4.70 4.09 4.66 4.25 - - 3.87 3.89 4.00 3.75 365.4 303.7 301.6 278.2 276.6 270.3 264.0 283.0 314.7 310.6 295.3 305.7 302.9 295.1 299.4 309.4 262.6 233.4 267.0 240.4 264.2 0.83 0.81 0. 0.83 0.80 0.82 0.81 0.81 0.79 0.79 0.79 0.80 0.79 0.78 0.77 0.80 0.78 0.78 0.78 0.78 0.78 0.57 0.62 0.64 0.57 0.61 0.59 0.60 0.61 0.65 0.65 0.65 0.65 0.66 0.64 0.66 0.63 0.67 0.64 0.67 0.65 0.66 Figure 6. Qualitative samples from our model trained at 256256 resolution. AdamW optimizer using learning rate of 2 104 and β2 = 0.95. FID-50K is reported as the evaluation metric. Effect of different components. Table 3 summarizes the contribution of each component in SFD. Starting from the baseline with FID 8.17, adding REPA yields moderate improvement to FID 7.08. Introducing semantic latents from SemVAE substantially improves performance to FID 5.24, confirming the effectiveness of explicit semantic representations. Finally, integrating the semantic-first mechanism further reduces FID to 3.03, demonstrating the effectiveness of our proposed asynchronous denoising strategy. Table 3. Ablation study on different components of SFD. Table 6. Comparison of reconstruction performance. REPA SemVAE Semantic-First FID 8.17 7.08 5.24 3."
        },
        {
            "title": "Method",
            "content": "rFID PSNR LPIPS SSIM VA-VAE RAE SD-VAE 0.28 0.57 0.26 27.96 18.86 28. 0.096 0.256 0.089 0.79 0.42 0.80 Table 4. Ablation on semantic latent compression methods. Table 5. Semantic-First helps with ReDi [22]."
        },
        {
            "title": "Method",
            "content": "FID"
        },
        {
            "title": "PCA\nSemVAE",
            "content": "4.06 3.03 Semantic-First FID 5.33 4.41 Ablation on semantic latent compression methods. Table 4 compares PCA dimensionality reduction employed in ReDi with our SemVAE as different compression methods. SemVAE achieves significantly superior FID of 3.03 compared to PCAs 4.06, demonstrating the necessity of preserving semantic information completeness. Other Ablations. Due to space limitations, additional ablation studies on vision foundation model selection and scaling, SemVAE bottleneck dimension, semantic loss weight β, and REPA parameters are deferred to Appendix C. 4.4. Generalization of Semantic-First Mechanism To validate the generalization of our semantic-first mechanism to other methods, we conduct experiments on ReDi [22] and VA-VAE [49]. ReDi employs PCA-reduced DINOv2-B features as semantic latents and concatenates them with texture latents encoded by SD-VAE for simultaneous denoising. As shown in Table 5, incorporating our semantic-first mechanism into ReDi improves FID from 5.33 to 4.41, demonstrating the effectiveness of the semantic-first approach for methods that leverage semantictexture composition. Due to space constraints, results on VA-VAE are provided in Appendix B. 4.5. Reconstruction Performance Analysis While recent work has highlighted the dilemma between reconstruction and generation in VAE [49], our SFD framework achieves state-of-the-art generation performance, without sacrificing the reconstruction fidelity. Here we compare the reconstruction quality of latent space variants (VA-VAE [49] and RAE [51]) to SD-VAE, which is adopted as the texture VAE in our method. As shown in Table 6, SD-VAE achieves the best performance with the lowest rFID of 0.26 and LPIPS of 0.089, along with the highest PSNR of 28.59 and SSIM of 0.80. VA-VAE, which aligns Figure 7. Qualitative reconstruction comparison among different latent space variants. From left to right: Original image, VA-VAE, RAE and SD-VAE reconstructions. The top-left insets show zoomed-in regions focusing on text details (40 g). SDVAE achieves the best reconstruction fidelity. its latent space with visual foundation model features, enhances semantic representation but slightly compromises pixel-level reconstruction fidelity. In contrast, RAE builds its latent space purely on pretrained visual encoders, leading to texture-deficient representations and significantly degraded reconstruction quality: rFID 0.57, PSNR 18.86, LPIPS 0.256, and SSIM 0.42. As illustrated in Figure 7, RAE severely loses fine-grained details such as the 40g text region and exhibits noticeable color distortion, while VA-VAE shows slightly reduced fidelity and SD-VAE faithfully maintains superior reconstruction quality. By adopting SD-VAE for texture modeling while introducing separate semantic pathway, our composite latent design enables SFD to achieve significant convergence acceleration and superior generation performance without sacrificing reconstruction fidelity. This preservation of reconstruction quality makes SFD inherently more suitable for complex image synthesis tasks, such as text-to-image generation and consistencydemanding image editing. 5. Conclusion We propose Semantic-First Diffusion (SFD), novel paradigm that performs asynchronous denoising on semantic and texture latents in latent diffusion models. By prioritizing semantic denoising to guide texture refinement, SFD achieves faster convergence and superior generation quality. Extensive experiments on ImageNet class-conditional generation demonstrate that SFD consistently outperforms competing methods. Our findings suggest that controlling the relative denoising pace between semantics and textures is crucial for efficient generative modeling, establishing representation-level asynchronous denoising as promising direction for future diffusion research."
        },
        {
            "title": "Acknowledgments",
            "content": "Yueming Pan and Nanning Zheng were supported in part by the NSFC under Grant No. 62088102."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions, 2023. URL https://arxiv. org/abs/2303.08797, 3, 2023. 3 [2] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 3 [3] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 3 [4] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 5, 1 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6, 1 [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 6, 1, [7] John Dormand and Peter Prince. family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):1926, 1980. 6, 1 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [9] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2316423173, 2023. 7, 3 [10] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 7, [11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 5 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [15] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. 3 [16] Zijing Hu, Yunze Tong, Fengda Zhang, Junkun Yuan, Jun Xiao, and Kun Kuang. Asynchronous denoising diffusion models for aligning text-to-image generation. arXiv preprint arXiv:2510.04504, 2025. 3 [17] Allan Jabri, David Fleet, and Ting Chen. Scalable adaparXiv preprint tive computation for iterative generation. arXiv:2212.11972, 2022. 3 [18] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 6, 1, 4 [19] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 4 [20] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [21] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 1 [22] Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. arXiv preprint arXiv:2504.16064, 2025. 2, 3, 6, 7, 8, 4 [23] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6 [24] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 2, 3, 7, 4 [25] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441125468, 2024. [26] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 7, 2, 3 [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [28] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 1, 2, 3, 7, 4 [29] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 6 [30] Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneˇs, Wenshuo Chen, Albert Ali Salah, and Intriguing properties of imItir Onal Ertugrul. Dctdiff: age generative modeling in the dct space. arXiv preprint arXiv:2412.15032, 2024. 2 [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3, 5, [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1, 2, 6, 7, 3, 4 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 5 [34] MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. ICLR, 2016. 2 [35] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 7, 2, 3 [36] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In International Conference on Learning Representations, 2023. 2 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 4, [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 2 [39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [40] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. 3, 4 [41] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and AliImproving the diffusability of autoenaksandr Siarohin. In Forty-second International Conference on Macoders. chine Learning, 2025. 2 [42] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [43] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 7, 2, 3 [44] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 5 [45] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 3 [46] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 2, 7, 3, [47] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. 2, 3, 6, 7, 4, 5 [48] Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. 7, 3 [49] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 2, 3, 5, 6, 7, 8, 1, 4 [50] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 5, 6, 7, 4 [51] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 3, 7, 8, 2, 4, 5 [52] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 7, Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Implementation Details Table 7. SemVAE training configuration. A.1. SemVAE Configuration"
        },
        {
            "title": "Setting",
            "content": "semantic representation extraction, we For employ DINOv2-B with registers [4, 31] on 256 256 images. The SemVAE architecture consists of 4 transformer blocks for both the encoder and decoder, with 29M parameters each (58M total). We train on ImageNet-1K [5] for 1M iterations with random cropping as data augmentation. Detailed hyperparameters are shown in Table 7. A.2. Diffusion Model Configuration We adopt LightningDiT [49] as our diffusion backbone, which is available in multiple scales (B/L/XL/XXL). For latent construction, SD-VAE [37, 49] (implemented in LightningDiT2 [49]) encodes textures into 32 channels with 16 spatial compression, while SemVAE extracts 16-channel semantic representations. Their concatenation forms unified 48-channel, 256-token latent for each 256256 image. Following the ADM [6] preprocessing pipeline, all images are center cropped and resized to 256256 resolution. Training is conducted on ImageNet-1K [5] for 800 epochs with batch size of 256, using AdamW optimizer with learning rate of 1 104 and β values of (0.9, 0.999). We employ logit-normal timestep sampling following LightningDiT [49]. For sampling, the dopri5 solver [7] with adaptive step size is employed, with absolute and relative tolerances set to 106 and 103 respectively. Detailed hyperparameters across different model scales are shown in Table 8. A.3. Dual Timestep Embedding To support asynchronous denoising, SFD employs two independent timestep embedders corresponding to the semantic and texture timesteps ts and tz. Unlike LightningDiT [49], which uses single MLP-based embedder of hidden dimension H, SFD constructs two smaller embedders whose hidden dimensions are reduced to H/2. Each embedder independently processes its respective timestep, the two embeddings are then concatenated along the channel dimension and injected into the backbone. This design allows the model to supply distinct timestep signals to the semantic and texture latents: = [ τs(ts), τz(tz) ], (21) 2https://github.com/hustvl/LightningDiT"
        },
        {
            "title": "Feature Extraction\nFeature Extractor\nInput Patch Size",
            "content": "Architecture Total Parameters Encoder Parameters Decoder Parameters Encoder Blocks Decoder Blocks Hidden Dimension Attention Heads Bottleneck Channels KL Weight λkl"
        },
        {
            "title": "Training\nDataset\nTotal Iterations\nBatch Size\nData Augmentation",
            "content": "Optimization Optimizer Learning Rate (β1, β2)"
        },
        {
            "title": "LR Schedule\nWarmup Steps\nConstant Steps\nAnnealing",
            "content": "DINOv2-B-reg 256256 58M 29M 29M 4 4 768 6 16 107 ImageNet-1K 1,000,000 64 Random cropping AdamW 5 105 (0.9, 0.999) 500 800,000 Cosine to 5 106 where [ , ] denotes channel-wise concatenation, and τs() and τz() are the semantic and texture timestep embedders. A.4. Evaluation Details AutoGuidance. We employ AutoGuidance [18] as our primary guidance method. Unlike Classifier-Free Guidance (CFG) [13], which relies on an unconditional model, AutoGuidance guides the main diffusion model using weaker version of itselftypically model with smaller capacity or an earlier training snapshot. This self-guidance mechanism effectively suppresses out-of-manifold samples by aligning the denoising trajectory toward regions of higher data density, thereby improving image quality without sacrificing sample diversity. In practice, we use the degraded LightningDiT-B model as the guiding network. After searching, configurations of degraded models are illustrated in Tab. 9. Table 8. Hyperparameter settings across different model scales."
        },
        {
            "title": "Backbone",
            "content": "Architecture #Params Input Layers Hidden dim. Num. heads SFD settings β REPA visual encoder REPA weight λ REPA alignment depth REPA similarity function Optimization Batch size Optimizer lr (β1, β2)"
        },
        {
            "title": "Sampling\nSampler\nAbsolute tolerance\nRelative tolerance",
            "content": "LightningDiT-B LightningDiT-L LightningDiT-XL LightningDiT-XXL 130M 16 16 48 12 768 12 458M 16 16 48 24 1024 16 675M 16 16 48 28 1152 2.0 0.3 DINOv2-B-reg 1.0 2 cosine 2.0 0.3 DINOv2-B-reg 1.0 2 cosine 2.0 0.3 DINOv2-B-reg 1.0 2 cosine 256 AdamW 1 104 (0.9, 0.999) 256 AdamW 1 104 (0.9, 0.999) 256 AdamW 1 104 (0.9, 0.999) dopri5 106 103 dopri5 106 103 dopri5 106 103 1.0B 16 16 48 32 1280 16 2.0 0.3 DINOv2-B-reg 1.0 2 cosine 256 AdamW 1 104 (0.9, 0.999) dopri5 106 103 Table 9. Configurations of degraded models used for guidance."
        },
        {
            "title": "Iterations Guidance Scale",
            "content": "LightningDiT-XL LightningDiT-XL LightningDiT-XXL LightningDiT-XXL 80 800 80 800 675M LightningDiT-B 675M LightningDiT-B LightningDiT-B 1.0B LightningDiT-B 1.0B 70K 70K 60K 120K 1.6 1.5 1.5 1.5 Class-balanced Sampling. RAE [51] shows that classbalanced sampling yields more reliable and lower FID estimates. To ensure fair comparison with prior work [26, 35, 43, 46, 51], we follow this protocol and adopt classbalanced sampling for FID-50K evaluation. Specifically, we generate 50 images per class (50,000 in total). B. Additional Experimental Results B.1. Complete Comparisons Table 10 presents system-level comparison of classconditional generation on ImageNet 256 256. In the guidance setting, our SFD achieves state-of-the-art performance, surpassing existing methods in both FID and sFID. Notably, our SFD-XL (675M) outperforms the previous best model, RAE DiTDH (839M), with lower FID (1.06 vs. 1.13), demonstrating superior generation quality with fewer parameters. Scaling up to SFD-XXL (1.0B) further pushes the performance boundary to FID of 1.04. Notably, SFD achieves superior sFID of 3.75, outperforming previous methods by substantial margin. Since sFID serves as metric for structural coherence and spatial alignment, this improvement validates the advantage of our explicit compression of semantic representations with spatial layouts, which ensures robust global structure before texture refinement. Regarding the unguided setting, SFD remains competitive but exhibits limitations in texture convergence. This is primarily attributed to the high complexity of the texture latents. Unlike methods such as ReDi [22] or REG [47] that utilize standard f8d4 VAE, we employ the f16d32 variant (following LightningDiT), which results in latent space with double the dimensionality. Consequently, modeling these high-dimensional texture latents is inherently more challenging and harder to converge. B.2. Inference Strategies Inference Steps. Table 11 illustrates FID scores without guidance of various sampling steps, showing that SFD Table 10. System-level comparison of class-conditional generation on ImageNet 256256."
        },
        {
            "title": "Epochs",
            "content": "#Params Generation@256 w/o guidance Generation@256 w/ guidance FID sFID IS Prec. Rec. FID sFID IS Prec. Rec."
        },
        {
            "title": "Autoregressive",
            "content": "VAR [43] MAR [26] xAR [35]"
        },
        {
            "title": "Pixel Diffusion",
            "content": "ADM [6] RIN [17] PixelFlow [3] PixNerd [45] SiD2 [15]"
        },
        {
            "title": "Latent Diffusion",
            "content": "DiT [32] MaskDiT [52] SiT [28] FasterDiT [48] MDT [9] MDTv2 [10] DDT [46] 350 800 800 400 480 320 160 1280 1400 1600 1400 400 1300"
        },
        {
            "title": "Leveraging Visual Representations",
            "content": "VA-VAE [49] REPA [50] REPA-E [24] ReDi [22] REG [47] RAE [51] (DiT-XL) RAE [51] (DiTDH-XL) SFD (XL) SFD (XL) SFD (XXL) SFD (XXL) 800 800 800 800 800 800 800 80 800 80 800 2.0B 943M 1.1B 554M 410M 677M 700M - 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 677M 676M 839M 675M 675M 1.0B 1.0B - 2.35 - 10.94 3.42 - - - 9.62 5.69 8.61 7.91 6.23 - 6.27 2.17 5.90 1.69 3.30 1.80 1.87 1.51 3.43 2.54 2.84 2.38 - - - 6.02 - - - - 6.85 10.34 6.32 5.45 5.23 - 4.36 - 4.17 4.80 4.59 - - 4.34 4.38 4.25 4.37 - 227.8 - 101.0 182.0 - - - 121.5 177.9 131.7 131.3 143.0 - 154. 205.6 - 219.3 188.9 230.8 209.7 242.9 162.0 191.7 172.6 197.9 - 0.79 - 0.69 - - - - 0.67 0.74 0.68 0.67 0.71 - 0.68 0.77 - 0.77 0.74 0.77 0.80 0.79 0.75 0.75 0.75 0.75 - 0.62 - 0.63 - - - - 0.67 0.60 0.67 0.69 0.65 - 0.69 0.65 - 0.67 0.68 0.66 0.63 0.63 0.65 0.67 0.65 0.67 1.80 1.55 1.24 3.94 - 1.98 2.15 1.38 2.27 2.28 2.06 2.03 1.79 1.58 1. 1.35 1.42 1.12 1.61 1.36 1.41 1.13 1.30 1.06 1.19 1.04 - - - 6.14 - 5.83 4.55 - 4.60 5.67 4.50 4.63 4.57 4.52 - 4.15 4.70 4.09 4.66 4.25 - - 3.87 3.89 4.00 3.75 365.4 303.7 301. 215.8 - 282.1 297.0 - 278.2 276.6 270.3 264.0 283.0 314.7 310.6 295.3 305.7 302.9 295.1 299.4 309.4 262.6 233.4 267.0 240.4 264.2 0.83 0.81 0.83 0.83 - 0.81 0.79 - 0.83 0.80 0.82 0.81 0.81 0.79 0. 0.79 0.80 0.79 0.78 0.77 0.80 0.78 0.78 0.78 0.78 0.78 0.57 0.62 0.64 0.53 - 0.60 0.59 - 0.57 0.61 0.59 0.60 0.61 0.65 0.65 0.65 0.65 0.66 0.64 0.66 0.63 0.67 0.64 0.67 0.65 0.66 Table 11. FID () comparison across inference steps. All models are trained for 400K iterations and evaluated using the Euler sampler without guidance. Reported values are FID-10K scores computed at different inference step counts."
        },
        {
            "title": "Inference steps",
            "content": "250 200 150 100 80 LightningDiT LightningDiT+REPA LightningDiT+VA-VAE LightningDiT+ReDi LightningDiT+SFD (Ours) 12.50 10.00 7.66 8.58 6.32 12.58 10.10 7.66 8.63 6.26 12.67 10.23 7.68 8.72 6.41 12.91 10.50 7.70 8.86 6.35 13.03 10.67 7.76 9.02 6. 13.40 10.94 7.83 9.32 6.77 maintains strong performance even with significantly fewer inference steps. While other baselines require 200250 steps to approach their optimal FID, SFD already achieves competitive score of 6.35 at only 100 steps, and further increasing the steps to 250 yields only marginal improvement (6.32). This observation suggests that the semantic-first design may facilitate more efficient sampling: by stabilizing global semantics early, the model requires fewer refinement steps to reach high-quality solutions. Table 12 presents the FID scores with guidance across Table 12. FID () comparison across inference steps for SFD (XL) and SFD (XXL) models at 4M training iterations with guidance. Reported values are FID-50K scores."
        },
        {
            "title": "Method",
            "content": "dopri5 250 200 150 100 60 50 40 30 25 SVG [40] SFD (XL) SFD (XXL) - 1.064 1.040 - 1.051 1.035 - 1.050 1.040 - 1.048 1.041 - 1.045 1.058 - 1.086 1. - 1.102 1.106 - 1.206 1.190 - 1.510 1.429 - 1.447 1.456 1.920 1.865 1.844 Table 13. Comparison of class-random sampling and class-balanced sampling."
        },
        {
            "title": "Balanced sampling",
            "content": "FID sFID IS Prec. Rec. FID sFID IS Prec. Rec. SiT [28] REPA [50] REPA-E [24] DDT [46] VA-VAE [49] ReDi [22] REG [47] RAE [51] (DiTDH-XL) SFD (XL) 2.06 1.42 1.26 1.40 1.35 1.61 1.36 1.28 1.18 4.50 4.70 4.11 - 4.15 4.66 4.25 - 3.89 270.3 305.7 314.9 303.6 295.3 295.1 299.4 262.9 266. 0.82 0.80 0.79 - 0.79 0.78 0.77 - 0.78 0.59 0.65 0.66 - 0.65 0.64 0.66 - 0.67 1.95 1.29 1.12 1.26 1.23 1.60 1.19 1.13 1.06 - - 4.09 - 4.20 5.99 4.44 - 3.89 259.5 306.3 302.9 310.6 296.0 294.7 305.4 262.6 267.0 - 0.79 0.79 0.79 0.79 0.78 0.78 0.78 0. - 0.64 0.66 0.65 0.65 0.64 0.66 0.67 0.67 varying sampling steps. Notably, SFD (XL) achieves superior FID of 1.045 at only 100 steps using the Euler sampler, surpassing the result yielded by the dopri5 sampler (1.064). Furthermore, in the few-step regime (25 steps), SFD (XL) maintains its advantage over SVG [40], recording an FID of 1.865 compared to 1.920 by SVG. Class-balanced Sampling. To ensure rigorous comparison, we re-evaluate prior state-of-the-art methods employing the same class-balanced protocol as discussed in RAE [51]. Specifically, results for SiT [28], REPA [50], and DDT [46] are adopted from RAE [51], while REPA-E [24] figures are sourced from its original publication. Additionally, we conduct independent evaluations for VA-VAE [49], ReDi [22], and REG [47]. The quantitative comparison results are presented in Table 13. As observed, our proposed SFD (XL) demonstrates consistent superiority across both protocols. Remarkably, whether using class-balanced or class-random sampling, SFD achieves the best performance in terms of FID and sFID metrics, surpassing all competing state-of-the-art methods. B.3. Unconditional Generation We further evaluate the proposed SFD on unconditional image generation on ImageNet 256256. During both training and sampling, we set the class label to 1000 (the null label). As shown in Table 14, SFD demonstrates remarkable performance with high training efficiency. Even without AutoGuidance (AG) [18], SFD significantly surpasses ReDi (FID 25.10 10.24) after only 80 epochs and further improves to an FID of 8.46 after 200 epochs. With AG Table 14. Comparison of unconditional generation on ImageNet 256256. RG and AG are short of Representation Guidance [22] and AutoGuidance [19]."
        },
        {
            "title": "Method",
            "content": "Epochs Params FID IS DiT-XL [32] ReDi [22] ReDi [22] (w/ RG) RAE [51] (w/ AG) RCG [25] (DiT-XL/2) RCG [25] (MAGE-L) RCG-G [25] (MAGE-L) SFD (w/o AG) SFD (w/ AG) SFD (w/o AG) SFD (w/ AG) 400 80 80 200 400 800 800 80 80 200 200 675M 30.68 675M 25.10 675M 22.60 839M 4.96 675M 4.89 502M 3.44 502M 2.15 675M 10.24 675M 3.77 675M 8.46 675M 2.90 32.7 123.1 143.2 186.9 253.4 78.5 127.9 89.9 148. enabled, SFD achieves substantial gains, reaching FIDs of 3.77 and 2.90 at 80 and 200 epochs, respectively. We attribute these improvements to the asynchronous denoising mechanism of SFD, which becomes especially crucial in the unconditional setting. These results suggest that, without class labels as conditional guidance, smoother semantic representations are more easily modeled, thus providing accurate global structural cues for superior generation performance. B.4. SFD for VA-VAE Tab. 15 analyzes the impact of applying Semantic-First Diffusion (SFD) to VA-VAE [49]. For both VA-VAE and ReDi settings, the SFD implementations used for comparison are Table 15. Effect of SFD for VA-VAE. C. Additional Ablation Studies TexEnc SFD FID VA-VAE VA-VAE SD-VAE (ours) 4.52 4.14 3.03 Table 16. Computational cost and performance comparison between LightningDiT and LightningDiT+SFD at 400K iterations on ImageNet 256256. SFD adds negligible computational overhead while delivering substantially improved generation quality. Method #Params (M) GFLOPs FID LightningDiT-XL LightningDiT-XL + SFD 683.39 682.77 116.479 116. 9.29 3.53 our reproduced versions. When equipped with SFD, VAVAE improves performance from an FID of 4.52 to 4.14, indicating that SFD is also compatible with joint semantictexture latent space. However, its overall performance still lags behind our SD-VAE-based SFD (FID 3.03). This is likely because VA-VAEs latent space inherently entangles semantics and textures, leaving limited flexibility for the asynchronous denoising mechanism to operate effectively. In contrast, disentangling semantic and texture representations (as done in SD-VAE) allows the semantic latents to stabilize early and provide clearer global guidance for texture refinement, ultimately yielding higher generative quality. B.5. Computational Cost We evaluate the computational overhead introduced by integrating SFD into LightningDiT-XL. SFD modifies the backbone in two ways. First, it augments the latent representation with 16-channel semantic latent, which introduces marginal increase in backbone FLOPs. Second, SFD replaces the single timestep embedder in LightningDiT with two independent embedders that operate on the semantic and texture timesteps ts and tz. As shown in equation 21, although two embedders are used, the total parameter count is actually smaller, since MLP parameters grow quadratically with hidden dimension. Consequently, two (H/2)- width MLPs contain only 0.5 the parameters and FLOPs of single H-width MLP. Table 16 reports the computational cost comparison. SFD incurs only negligible increase in FLOPs (less than 0.01%) while delivering dramatic improvement in FID at 400K iterations. This indicates that SFD achieves an extremely favorable costperformance tradeoff with virtually no additional computational burden. C.1. Semantic VAE Design Our Semantic VAE (SemVAE) compresses pretrained vision foundation model features into compact semantic representations. To investigate its design choices, we conduct series of ablation studies on three key aspects: the choice of pretrained vision encoder, model scaling within the encoder family, and the number of output channels representing semantic capacity. Different target representation and model scaling. Tab. 17 (a) compares several pretrained vision encoders used as target representations. Among all candidates, DINOv2-B achieves the lowest FID of 3.03, outperforming MAE [11], CLIP [33], and SigLip [44], indicating that DINOv2 provides the most effective supervision for compact semantic latent learning. Tab. 17 (b) studies different model scales within the DINOv2 family. Larger encoders yield better semantic guidance, with DINOv2-L achieving the best FID of 2.97. Notably, this finding stands in contrast to recent works like REG [47] and RAE [51], which identified DINOv2-B as the optimal choice and observed performance degradation when scaling to larger VFMs due to their increased dimensionality. Our results demonstrate the superiority of our explicit semantic compression strategy, which effectively handles high-dimensional features and unlocks the potential for further scaling with more powerful VFMs. Considering the trade-off between performance and efficiency, we adopt DINOv2-B as the default pretrained visual encoder. Channel capacity. DINOv2-B outputs 768-dimensional features, which are compressed by the Semantic VAE into lower-dimensional semantic latent. Tab. 17 (c) investigates the impact of varying the latent channel capacity. We observe consistent performance improvement as the number of channels increases from 2 to 16. This trend indicates that higher channel capacity is essential for preserving the rich semantic information embedded in the original high-dimensional features. The 16-channel configuration achieves the best FID of 3.03, confirming that retaining more semantic details directly contributes to superior generation quality. C.2. Effect of semantic loss weight Tab. 18 analyzes the impact of the semantic loss weight β in the velocity prediction objective. As the weight increases from 0.25 to 2.0, the FID score consistently decreases, indicating that stronger semantic supervision enhances training stability and generation performance. However, when β becomes excessively large (e.g., 4.0 or 8.0), the performance Table 17. Ablation on Semantic VAE design. (a) compares different target representation models; (b) studies model scaling within DINOv2 family; (c) analyzes semantic channel capacity. Target Repr. FID DINOv2-B MAE-B CLIP-B SigLip-B 3.03 6.29 4.89 4.15 Target Repr. FID DINOv2-S DINOv2-B DINOv2-L 4.14 3.03 2.97 #Channels FID 2 4 8 16 3.90 3.67 3.16 3. (a) Model comparison. (b) Scaling comparison. (c) Channel capacity. Table 18. Effect of semantic loss weight. Weight β 0. 0.5 1.0 2.0 4.0 8.0 FID 3.46 3.26 3.08 3.03 3.28 3. Table 19. Ablation on REPA configurations. Depth of conducting REPA loss, loss weight λ, and loss type are included. Depth Weight λ 2 4 6 8 10 12 2 2 2 2 2 2 2 2 0.5 0.5 0.5 0.5 0.5 0.5 0.25 0.5 1.0 2.0 4.0 0.5 0.5 0.5 Type cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine+MSE cosine MSE cosine+MSE FID 4.15 3.03 3.07 3.24 3.16 3.19 3.28 3.30 3.03 3.18 3.25 3.20 3.16 3.13 3. degrades, suggesting that overemphasizing semantics suppresses texture learning and leads to loss of fine details. Overall, β = 2.0 achieves the best balance between semantic guidance and texture refinement, yielding the lowest FID of 3.03. C.3. Effect of REPA configurations Alignment depth. Tab. 19 presents systematic study of REPA configurations. In our experiments, applying the REPA loss at shallow layers (specifically at depth 2) yields the best performance with an FID of 3.03, whereas the original REPA [50] reports optimal performance at depth 8. We attribute this discrepancy to the distinct role of the alignment loss in our framework. While the original REPA operates as distillation process that forces the diffusion model to gradually analyze and understand the input latents, our approach utilizes the REPA loss to drive the model to decode and reconstruct high-level semantic representations from the noisy compressed latents. Since decoding from semantic latent is inherently more straightforward task than analyzing semantics from scratch, our model can achieve effective alignment at much shallower layers. Consequently, early-layer alignment suffices to recover the semantic guidance, avoiding the need for deeper intervention. REPA loss weight λ. For the REPA loss weight λ, the model achieves the lowest FID at λ = 0.5. This indicates that moderate alignment strength provides good balance between semantic consistency and generative fidelity. REPA similarity function. We also compare results of different REPA similarity functions. While conventional REPA employs cosine similarity for feature alignment, we additionally explore combining cosine and MSE losses inspired by our SemVAE training. The combined objective (cosine+MSE) achieves the best performance of 3.03 FID score, outperforming single-loss variants. This suggests that employing similarity function consistent with the SemVAE training metric yields optimal results. Furthermore, it demonstrates the complementary nature of the two terms: MSE ensures distribution-level precision, whereas cosine similarity enhances directional alignment, leading to better semantic matching and visual realism. It is worth noting that the optimal settings identified in this ablation study differ slightly from the final hyperparameters presented in Table 8. This discrepancy arises because the ablation experiments were evaluated at 400K iterations; however, over the full training duration (4M iterations), the configuration detailed in Table 8 yielded superior performance. Consequently, our final model adopts the settings from Table 8 rather than strictly following the ablation outcomes. D. Limitation and Future Work Currently, SFD employs fixed temporal offset to manage the asynchronous denoising process. However, static Figure 8. Visualization of training results across different iterations (160K, 320K, and 480K). Under fixed random seed and identical initial noise, SFD produces clearer structures and more realistic details at early stages, demonstrating faster convergence compared with other variants. offset may not be optimal across all noisy levels. Future work could explore dynamic or adaptive schedules for to further enhance the synergy between semantic and texture generation. Furthermore, our framework presently relies on the REPA loss as an auxiliary objective to enforce feature alignment. promising direction for future research is to investigate methods that eliminate the need for such auxiliary supervision, aiming for cleaner and more streamlined optimization structure. Beyond algorithmic refinements, extending and scaling SFD to more complex application scenarios represents highly valuable research direction. Specifically, adapting SFD to text-to-image and text-to-video generation tasks could further validate its potential in handling intricate multimodal guidance and temporal consistency. E. More Visualization Results We qualitatively compare the training progression in Figure 8, where all models are evaluated using the same initial noise. The baseline LightningDiT, REPA, and VA-VAE variants exhibit weaker structural consistency and struggle to form coherent details in the early training stages. In contrast, SFD produces clearer structures and more realistic details at much earlier iterations, demonstrating noticeably faster convergence. We also present more visualization results of SFD in Figures 9 - 17. Figure 9. Visualization results of LightningDiT-XL + SFD for the ImageNet class Bald eagle (22). Figure 10. Visualization results of LightningDiT-XL + SFD for the ImageNet class Sulphur-crested cockatoo (89). Figure 11. Visualization results of LightningDiT-XL + SFD for the ImageNet class Giant panda (388). Figure 12. Visualization results of LightningDiT-XL + SFD for the ImageNet class Teapot (848). Figure 13. Visualization results of LightningDiT-XL + SFD for the ImageNet class Hamburger (933). Figure 14. Visualization results of LightningDiT-XL + SFD for the ImageNet class Strawberry (949). Figure 15. Visualization results of LightningDiT-XL + SFD for the ImageNet class Castle (483). Figure 16. Visualization results of LightningDiT-XL + SFD for the ImageNet class Lakeside (975). Figure 17. Visualization results of LightningDiT-XL + SFD for the ImageNet class Hot-air balloon (417)."
        }
    ],
    "affiliations": [
        "ByteDance",
        "IAIR, Xian Jiaotong University",
        "Microsoft Research Asia"
    ]
}