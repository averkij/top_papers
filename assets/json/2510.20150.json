{
    "paper_title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning",
    "authors": [
        "Yaochen Zhu",
        "Harald Steck",
        "Dawen Liang",
        "Yinhan He",
        "Vito Ostuni",
        "Jundong Li",
        "Nathan Kallus"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 3 0 5 1 0 2 . 0 1 5 2 : r Published as conference paper at ICLR 2026 RANK-GRPO: TRAINING LLM-BASED CONVERSATIONAL RECOMMENDER SYSTEMS WITH REINFORCEMENT LEARNING Yaochen Zhu1,, Harald Steck2, Dawen Liang2, Yinhan He1, Vito Ostuni2, Jundong Li1, Nathan Kallus2,3 1University of Virginia, 2Netflix Research, 3Cornell University. {uqp4qh, nee7ne, jundong}@virginia.edu {hsteck, dliang, vostuni, nkallus}@netflix.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct behavioral-cloning dataset with RemapReflectAdjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, principled extension of group relative policy optimization (GRPO) (Shao et al., 2024) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing rank-level importance ratio based on the geometric mean of rankwise token probabilities to stabilize policy updates. Experiments on the public REDDIT-V2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recommender systems (RS) are undergoing fundamental shift in the era of generative AI. Instead of passively predicting user behaviors, the emerging conversational RSs (CRS) paradigm position themselves as proactive agents that allow users to articulate nuanced preferences and receive tailored recommendations through interactive dialogue (Zhang et al., 2024). Large language models (LLMs) are at the center of this transformation: Their broad knowledge and reasoning capabilities enable both preference understanding and recommendation generation directly from user queries. Unlike earlier LLM-based RS that embed item ID tokens into the LLM (Hua et al., 2023), recent studies emphasize the benefits of representing items in natural language (He et al., 2023; Zhu et al., 2025). This representation not only preserves the LLMs core language capability but also provides flexibility to verbally guide CRS toward different recommendation objectives, such as novelty, diversity, or emotion-oriented relevance, while seamlessly integrating recommendation functionality. Despite this promise, aligning LLMs with real-world recommendation tasks remains challenging, primarily due to the following three key limitations. First, trained on general web datasets, LLMs inherently lack awareness of item catalog specific to given platform (e.g., the full set of TV shows available on streaming service platforms). Without explicit catalog grounding, they frequently generate out-of-catalog or even non-existent items in zero-shot manner, severely limiting their practical utility. Second, LLMs often fail to conform to predefined output formats for the items (e.g., Work done when Yaochen was machine learning research intern at Netflix Research. 1 Published as conference paper at ICLR 2026 including both the title and release year for movies), which complicates item matching and integration with downstream systems. Third, the quality of generated recommendation lists deteriorates sharply toward the end, due to the scarcity of high-quality ranking-style data in the pretraining stage. These challenges are particularly severe for smaller LLMs, which are less powerful but commonly adopted in practice to meet industrial-level efficiency and latency constraints. Collectively, the above limitations present significant barriers to the deployment of LLM-based CRS in real-world settings. Recently, reinforcement learning from verifiable reward (RLVR) (Lambert et al., 2024) offers promising direction for alignment, as it introduces structured rewards that can explicitly guide LLMs toward generating catalog-grounded, high-quality recommendation lists. However, applying RL to LLM-based CRS presents two fundamental challenges. First, since RLVR relies on self-exploration for policy update, warm-start behavior cloning stage with high-quality human demonstrations is generally required beforehand to ensure efficiency and stability (Shao et al., 2024; Zhu et al., 2024a; Wei et al., 2025). Yet it is impractical to expect human annotators to produce large-scale cataloggrounded item lists with good rankings, both due to the difficulty of recalling large and dynamic catalog and the influence of subjective biases. Furthermore, prevalent RLVR algorithms such as group relative policy optimization (GRPO) (Shao et al., 2024; Yu et al., 2025; Liu et al., 2025) are fundamentally misaligned with tasks with rank-style outputs. These methods typically conduct token-wise policy updates based on sequence-level rewards. However, sequence-level rewards (e.g., the overall NDCG of recommendation list) are too coarse to capture the contribution of individual items in each rank, while token-level updates are overly fine-grained as each item can be represented by multiple tokens. This mismatch leads to non-causal credit assignment, misaligned importance weighting, unstable policy optimization, and ultimately poor quality in generated recommendations. To address these challenges, we propose ConvRec-R1, novel two-stage framework for end-to-end training of LLM-based CRS. Specifically, in Stage 1, we design lightweight distillation pipeline, RemapReflectAdjust, which leverages zero-shot recommendations from powerful teacher LLM on training conversations to construct catalog-grounded demonstrations for supervised fine-tuning (SFT). Specifically, teacher-generated recommendation lists are first projected into the target catalog space (Remap), then refined with contextual judgments to improve ranking quality (Reflect), and finally corrected for residual popularity biases relative to training distributions (Adjust). This process establishes exemplar demonstrations of high-quality ranked items for training conversations, which warm-starts the LLM with catalog awareness, proper item formatting, and initial ranking ability before subsequent RL post-training. Furthermore, in Stage 2, we develop Rank-GRPO, principled extension of GRPO designed for tasks with rank-style outputs, where user feedback can be treated as verifiable reward. Rank-GRPO considers each rank as the unit for both advantage estimation and importance reweighting, introducing reward masking to prevent non-causal credit assignment and defining rank-level importance ratio via the geometric mean of item token probabilities to stabilize policy updates. Experiments on the public REDDIT-V2 dataset demonstrate that ConvRecR1 substantially improves the recommendation quality over zero-shot LLMs, converges faster and achieves superior Recall and NDCG in recommendations than GRPO-style baselines."
        },
        {
            "title": "2.1 PROBLEM FORMULATION",
            "content": "In this paper, we formulate conversational recommendation as sequential decision problem. Given dialogue between user and the system = (x1, . . . , xTx ) composed of Tx tokens, the goal is to learn policy πθ(yx), parameterized by target LLM with learnable parameters θ, that generates ranked list of items = (y(1), y(2), . . . , y(N )) as recommendations. Here, we note that each item y(k) = (y(k) ) is itself sequence of tokens represented in natural language (e.g., movie title with release year), rather than numerical identifier in traditional RS. Ideally, each selected item should belong to predefined item catalog, i.e., y(k) C. The generation process of can be decomposed into sequence of rank-wise decisions. At each rank k, the policy generates the next item y(k) conditioned on the context and the previously generated items y(<k) = (y(1), . . . , y(k1)). During generation, we instruct the LLM-based policy πθ to separate items in the recommendation list with special delimiter (e.g., newline token n) as follows: 1 , . . . , y(k) Lk ytext = [ y(1) y(2) y(N ) <eos> ], (1) 2 Published as conference paper at ICLR where denotes concatenation and <eos> marks the end of the generation process. To simplify the notation, we absorb with the previous item (and <eos> for the last item) and use and ytext interchangeably. The LLM-based πθ generates this sequence auto-regressively at the token level."
        },
        {
            "title": "2.2 SUPERVISED FINE-TUNING",
            "content": "The Stage 1 of the ConvRec-R1 framework is supervised fine-tuning (SFT), which aims to warmstart the LLM-based CRS policy πθ(yx) with the foundational knowledge of the item catalog, required output formats, and the ability to generate and rank lists of items for recommendations. To address the critical challenge of lacking high-quality in-catalog item ranking data, we construct )}M behavior cloning dataset, DSFT = {(xSFT i=1 by distilling exemplar catalog-grounded recommendation demonstrations ySFT from powerful teacher LLM (e.g., GPT-4o), which is often infeasible for direct deployment due to high costs and latency. for training conversations xSFT , ySFT i"
        },
        {
            "title": "2.2.1 TEACHER-GENERATED PRELIMINARY DEMONSTRATIONS",
            "content": "i,raw πΘ(yxSFT The distillation process begins by generating zero-shot list of candidate items for each dialogue xSFT in the SFT training set. This is achieved by prompting powerful teacher LLM Θ to produce preliminary item list ySFT ), following the same structure as Eq. (1). The items and ranks in ySFT i,raw benefit from the teachers advanced knowledge and reasoning ability, which represents the state of the art in open-item recommendations (He et al., 2023; Zhu et al., 2025). However, these recommendations lie within the teachers own recommendation space CΘ, which is not necessarily aligned with the target catalog C. Consequently, ySFT i,raw may still include out-of-catalog (OOC) items, minor formatting inconsistencies, or rankings with the teachers internal biases. i"
        },
        {
            "title": "2.2.2 THE REMAP–REFLECT–ADJUST PIPELINE",
            "content": "To correct these misalignments, we refine the teachers raw recommendations ySFT i,raw through threestep RemapReflectAdjust pipeline. This process calculates score sfinal RC over all items in the catalog C, which is then ranked to produce the demonstration ySFT (see Appendix for details). (i) Remap. This step grounds the teachers recommendations by mapping them from the teachers recommendation space CΘ into the target catalog space C. Specifically, we compute an initial score sremap RC using the aggregation: sremap = (Sitem-item + Iic) + λ sconv-item. Here, RCΘ is sparse vector where non-zero entries represent the positional scores of items in ySFT i,raw, heuristically calculated as 1/ for the item at rank k. The item-item similarity matrix Sitem-item RCΘC maps each item from CΘ to the target catalog based on semantic similarity, where Iic {0, 1}CΘC is sparse indicator mapping matrix with Iic[u, v] = 1 iff the u-th item in CΘ equals the v-th catalog item, which directly transfers scores for in-catalog items. Finally, sconv-item RC encodes the content similarity between the dialogue xSFT (ii) Reflect. While the remapping step grounds ySFT i,raw to scores in the catalog space, the ranking quality based on semantic similarity can be further improved. In the reflect step, we use LLM-as-ajudge to enhance the contextual relevance by asking the same teacher LLM to rate candidates with top-Nr > remapped scores sremap on numerical scale from (worst) to +L (best) based on their suitability as recommendations for the context xSFT . These raw ratings are then normalized to form sparse vector rreflect RC, which is scaled by weight γ and added to the remapped scores to produce context-enhanced refined score vector as: sreflect = sremap + γ rreflect. and in-catalog items. (iii) Adjust. The reflected scores already provide strong basis for ranking, but the final adjust step can further correct for residual popularity biases inherited from the teacher model. Specifically, we align the scores with the empirical distribution of the groundtruth items in the training data by learning an item-specific multiplicative bias RC and an additive bias RC, where the final score is computed as sfinal = sreflect + b, where denotes the Hadamard (element-wise) product. The final demonstration list ySFT can then be constructed by selecting the top-N items from the catalog according to their scores in sfinal and formatting them as the structured textual string shown in Eq. (1). With the established SFT dataset DSFT, the objective of behavior cloning can be 3 Published as conference paper at ICLR 2026 formalized as minimizing the negative log-likelihood of the demonstrations as follows: LSFT(θ) = (xSFT ,ySFT )DSFT [log πθ(ySFT xSFT )]. (2)"
        },
        {
            "title": "2.3 REINFORCEMENT LEARNING-BASED ALIGNMENT",
            "content": "After the initial SFT stage, we further align the LLM-based CRS policy πθSFT(yx) (hereafter, subscript SFT will be omitted for brevity) using reinforcement learning (RL), optimizing it directly )}MRL against the structured reward derived from user feedback. We use DRL = {(xRL i=1 to dei is training conversation and ygt note the training data in the RL phase, where xRL is the set of in-catalog items that the user provided positive feedback after the conversation. , ygt i"
        },
        {
            "title": "2.3.1 VANILLA GRPO",
            "content": "In this paper, we mainly focused on the prevalent RL alignment method, group relative policy optimization (GRPO) (Shao et al., 2024), which leverages the relative rewards among group of responses to estimate advantage and eliminate the need for learned value model (Schulman et al., 2017). For each dialogue xRL i=1 from sampling policy πθold(yx), where θold is typically the model µ update-steps before θ. Under the setting of CRS, each yi is ranked list of items, i.e., yi = (y(1) ) , which is evaluated against the groundtruth ygt to calculate reward score reflecting its overall quality. Based on the relative rewards across the generations in the group, GRPO estimates low-variance advantage for each response ˆAi, which can be used to update the policy by maximizing the following objective: DRL, GRPO first generates group of responses {yi}G , . . . , y(N ) JGRPO(θ) = xDRL,{yi}G i=1πθold (x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:16) min wi,t(θ) ˆAi,t, clip(wi,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:35) (cid:17) , (3) . where the importance ratio wi,t(θ) and advantage ˆAi,t for each token yi,t are: wi,t(θ) = πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) , ˆAi,t = ˆAi = r(x, yi) mean({r(x, yi)}G std({r(x, yi)}G i=1) i=1) We omit the KL-divergence with base policy (i.e., the SFT model θSFT) in Eq. (3) for readability. Here, the importance sampling term wi,t(θ) allows unbiased advantage estimate with samples generated from the old policy πθold . To facilitate stable updates, the objective function clips the importance ratio, limiting how far the new policy can diverge from πθold. The sequence-level reward r(x, yi) is typically ranking-based metric such as DCG@N , defined as (cid:80)N log2(k+1) . The relevance score relk is 1 if the item y(k) and has not appeared at an earlier rank; the relevance is 0 for all other cases, including repeated, non-relevant, and out-of-catalog items. at rank is in the groundtruth set ygt k= relk Gradient Analysis and Limitations. Although GRPO has shown effectiveness in many other domains with verifiable rewards, such as math problem solving (Guo et al., 2025) and code generation (Zhu et al., 2024a), the gradient analysis of its unclipped objective (for simplicity) reveals two fundamental misalignments when applied to ranking tasks (see Appendix for derivation): θJGRPO(θ) (cid:34) (cid:88) i= 1 yi yi (cid:88) t=1 wi,t(θ) (cid:124) (cid:123)(cid:122) (cid:125) token (t)-level importance ratio ˆAi (cid:124)(cid:123)(cid:122)(cid:125) sequence (i)-level advantage θ log πθ(yi,tx, yi,<t) (cid:125) (cid:123)(cid:122) token (t)-level gradient of log-likelihood (cid:124) (cid:35) . (4) Intuitively, Eq. (4) shows that maximizing Eq. (3) increases or decreases the log-likelihood of each token in the generated response yi according to the sign of the advantage (scaled by the importance weight). However, (i) for the ranking task, the sequence-level reward r(x, yi) (e.g., DCG@N of the whole list) is assigned uniformly to every token, leading to non-causal credit assignment: tokens in the later items inherit reward from earlier ranks, but the auto-regressive generation goes the other way around. Take the DCG reward as an example, we note that it can be naturally decomposed as sum of temporally discounted rank-level rewards (i.e., the relevance relk here) as follows: DCG@N = (cid:88) j=1 relj log2(j + 1) (cid:88) j=1 rewardj discountj = 4 k1 (cid:88) j=1 (cid:124) rewardj log2(j + 1) (cid:125) (cid:123)(cid:122) non-causal for item at rank + (cid:88) rewardj log2(j + 1) j=k (cid:124) (cid:125) (cid:123)(cid:122) causal for item at rank . (5) Published as conference paper at ICLR 2026 Here, tokens of item at rank not only take credit for itself and items recommended afterwards, but also from previous recommendations. This becomes especially problematic toward the end of the list (i.e., when is close to ), as the recommendation quality deteriorates significantly, but they accumulate credit from strong recommendations at front ranks. Moreover, (ii), in the off-policy step where µ > 0, the token-level importance ratio is too fine-grained and mismatched with sequencelevel advantages. Recent methods such as group sequence policy optimization (GSPO) (Zheng et al., 2025) mitigate the mismatch with sequence-level importance weights, yet they remain misaligned for the ranking task, as each rank should be the natural action unit instead of the whole sequence."
        },
        {
            "title": "2.3.2 RANK-GRPO",
            "content": "To address the misalignment of vanilla GRPO and GSPO, we propose Rank-GRPO, principled RL alignment algorithm for tasks with rank-structured outputs (e.g., recommendations). The core insight is to treat each rank as distinct action unit instead of token (too fine-grained) or sequence (too coarse), which enables more precise and rank-aware credit assignment and importance weighting for policy updating. Here, another major challenge is that computing the true advantage for an item at rank would, in principle, require enumerating trajectories that share the same prefix items y(<k), which grows combinatorially. To address this, Rank-GRPO adopts rank-wise advantage estimation based on the same trajectories used in GRPO, reducing complexity from exponential to linear while retaining effectiveness. The objective of Rank-GRPO is defined as: JRank-GRPO(θ) = xDRL,{yi}G i=1πθold (x) (cid:34)"
        },
        {
            "title": "1\nGN",
            "content": "G (cid:88) (cid:88) i=1 k=1 (cid:35) min(wi,k(θ) ˆAi,k, clip(wi,k(θ), 1ϵ, 1+ϵ) ˆAi,k) , (6) where the rank-level importance ratio wi,k(θ) and advantage ˆAi,k are defined as: wi,k(θ) = πθ(y(k) πθold(y(k) x) x) , ˆAi,k = r(x, y(k) ) meani [r(x, y(k) )] stdi [r(x, y(k) )] , where πθ(y(k) x) = (cid:32) y(k) (cid:89) t= (cid:33)1/y(k) πθ(yi,k,tx, yi,k,<t) = exp (cid:32) 1 y(k) y(k) (cid:88) t=1 log πθ(yi,k,tx, yi,k,<t) (cid:33) . We again omit the KL-divergence with the SFT base policy in Eq. (6) for readability (like in Eq. (3)). Here, πθ(y(k) x) denotes the effective probability for rank-k, which is defined as the geometric mean of the token probabilities for the item at the k-th rank. This normalization ensures stable importance weights across items with varying token lengths. The rank-wise advantage ˆAi,k is derived from rank-level return r(x, y(k) ) that evaluates both the immediate reward (quality) of the item at rank and its downstream influence on subsequent recommendations from rank + 1 through . Gradient Analysis and Insights. Before delving into the specific form of the rank-level return r(x, y(k) ), we first analyze the gradient of Rank-GRPO and demonstrate that it effectively addresses the two fundamental misalignments of vanilla GRPO on rank-structured outputs. The policy gradient for Rank-GRPO (Here we also consider the unclipped objective like Eq. (4) for simplicity) highlights its rank-centric policy update mechanism (see Appendix for the derivation): θJRank-GRPO(θ) (cid:34) (cid:88) (cid:88) i=1 k=1 wi,k(θ) ˆAi,kθ log πθ(y(k) x) (cid:35) (cid:34) (cid:88) = (cid:88) i= k=1 wi,k(θ) ˆAi,k (cid:124) (cid:123)(cid:122) (cid:125) rank (k)-level importance ratio (cid:124)(cid:123)(cid:122)(cid:125) rank (k)-level advantage y(k) (cid:88) t=1 1 y(k) (cid:124) θ log πθ(yi,k,tx, yi,k,<t) . (cid:35) (cid:123)(cid:122) rank (k)-level gradient of log-likelihood (cid:125) (7) The formulation in Eq. (7) shows that Rank-GRPO fundamentally resolves the issues of vanilla GRPO: First, instead of applying uniform sequence-level reward to every token, we assign rank (k)-specific advantage ˆAi,k for all the tokens of items at the k-th rank, ensuring that all tokens of Published as conference paper at ICLR 2026 the k-th item are updated consistently based on its contribution. Second, the importance weight and the gradient also operate at the rank level, where the effective probability πθ(y(k) x) aggregates token-level information for each rank, thereby improving the stability for policy updates. i"
        },
        {
            "title": "2.3.3 REWARD SHAPING",
            "content": "Following the analysis of the sequence-level DCG@N reward as Eq.(5), in Rank-GRPO, we define the rank (k)-level return r(x, y(k) ) by masking out the non-causal part in DCG@N and keeping only the causal part, which we denote as DCG@k:N as follows: r(x, yi) DCG@N = (cid:88) j=1 relj log2(j + 1) r(x, y(k) ) DCG@k:N = (cid:88) j=k relj log2(j + 1) . (8) DCG@k:N ensures each item y(k) receives credit only for its own contribution, respecting the sequential generation from higher rank to lower rank where earlier decisions influence the later ones. Although DCG@k:N minimally adapts the metric of interest (NDCG@N ), it is not necessarily the best reward for generative retrieval tasks, as the most influence of each item generation should come from the user query as context compared to the already-generated items in the higher ranks y(<k) . Therefore, we introduce an exponential decay variant of reward for Rank-GRPO as follows: rexpΓ (x, y(k) ) (cid:88) j=k relj Γ (jk) = relk + 1 Γ rexpΓ (x, y(k+1) ), (9) where Γ > 1 controls the discount rate. In the limit Γ = , only the current items relevance is credited. In practice, we find that Γ = provides simple, stable and effective learning signal. We distinguish the two Rank-GRPO variants with the rank-wise return defined in Eqs. (8) and (9) as Rank-GRPO (log) and Rank-GRPO (exp), respectively. To further stabilize RL training, we add two penalties for instruction-following failures: (i) Incomplete Lists. If the model generates fewer than items before emitting an <eos> token (No < ), the missing ranks are treated as irrelevant (zero return), and the premature stop token in this rollout receives penalty ϵo < 0 to discourage under-generation. (ii) Overflow Items. If the model generates more than items, each overflow item beyond rank receives penalty ϵo < 0 to discourage over-generation despite explicit instructions. Although more sophisticated reward shaping methods are possible (e.g., using sliding-window constraints on the horizon of the return), as the first rank-aware RL alignment framework, we find that Eqs. (8) and (9) are effective in practice and leave these extensions to future work."
        },
        {
            "title": "3.1 DATASETS",
            "content": "In the main paper, we focus on the REDDIT-V2 dataset, the largest publicly available benchmark for CRS ( 400k) that could support LLM post-training (He et al., 2023; Zhu et al., 2025). REDDITV2 consists of multi-turn dialogue sessions in which users mention or request movies, paired with groundtruth items that received positive feedback during the conversation. Following the protocol of He et al. (2023) and Zhu et al. (2025), the dataset is split into training, validation, and test sets with 383,013, 9,421, and 10,972 conversations. One notable difference with prior work is that we also require the LLM to output the release year together with the title for each recommendation, which is more difficult but important for disambiguation and catalog matching. To construct our supervised fine-tuning (SFT) dataset, we apply the RemapReflectAdjust pipeline to 25% of the training set and the entire validation set (for monitoring). This produces catalog-grounded behavior cloning dataset DSFT that serves to warm-start the RL phase and is released along with trained SFT checkpoints to facilitate future research. The full experiments on the REDIAL dataset are reported in Appendix D.7. 6 Published as conference paper at ICLR 2026 Figure 1: Training dynamics on loss, validation metrics, validation in-catalg recommendation ratio with different backbone models during the SFT stage on the REDDIT-V2 dataset. Figure 2: Left + Middle: Training dynamics of the reward acquired by ConvRec-R1 with RankGRPO on different rank during the RL stage. Right: Comparison of validation NDCG between GRPO and Rank-GRPO (both on-policy) on the REDDIT-V2 dataset. 3."
        },
        {
            "title": "IMPLEMENTATION",
            "content": "We evaluate ConvRec-R1 with three open-source instruction-tuned LLMs as the backbone models: Qwen2.5-0.5B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct, as models larger than 3B may introduce higher latency and computational overhead, which pose significant challenges for their practical deployment. More details on the implementation and hyperparameters are provided in Appendix C. We also provide the detailed establishment process of the SFT dataset, the exact prompt templates used for the reflect step and recommendation, additional experimental studies, and qualitative analyses on generations in Appendix A, and F, respectively."
        },
        {
            "title": "3.3 EVALUATION OF THE SFT STAGE",
            "content": "In Sections 3.3 and 3.4, we analyze training dynamics on the train/validation sets, while test set evaluation is deferred to Section 3.5. During the SFT stage, we observe three key phenomena in Fig. 1. First, while training loss decreases steadily, validation loss plateaus around 800 steps1, reflecting the difficulty of learning long, structured recommendation lists purely through behavior cloning. Second, the high-quality SFT data provides strong initialization by grounding the model in the catalog and enforcing output format, where the proportion of in-catalog recommendations rapidly surpasses 99% across backbones. Third, the ranked list generation ability improves drastically over the initial zero-shot model, with NDCG@20 increasing over 30, 3, and 1.5 for the 0.5B, 1B, and 3B model, respectively. Overall, SFT is crucial for warming up the models, i.e., constraining them to the catalog, ensuring proper formatting, and improving long list generation ability before RL, which improves both sampling efficiency (for self-exploration) and training stability."
        },
        {
            "title": "3.4 EVALUATION OF THE RL STAGE",
            "content": "We then analyze the training dynamics of ConvRec-R1 in the RL stage. Figures 2 and 3 show the rewards obtained at different ranks in the generated recommendation list on the training set. From the 1Nevertheless, we adopt the 1,500-step SFT checkpoint for RL for stronger catalog memory. Starting with the InstructGPT (Ouyang et al., 2022), it is common to initialize RL from slightly overfitted SFT model. 7 Published as conference paper at ICLR 2026 Figure 3: Training dynamic of reward and validation NDCG between GRPO, GSPO and RankGRPO (off-policy) on REDDIT-V2. See Appendix Fig. 5 for results with Llama3.2-1B-Instruct. figures we observe that during training with Rank-GRPO, rewards increase monotonically across ranks in both on-policy and off-policy settings. Figures 2 and 3-(right) further report the dynamics of NDCG on the validation set. Compared to GRPO and GSPO (in the off-policy setting, as both are the same in the on-policy setting), Rank-GRPO converges faster and achieves higher NDCG, with particularly larger improvements with large k. This suggests that the generated recommendation sequences with Rank-GRPO-trained LLM not only improve in overall quality but also maintain good quality toward the tail of the list, where standard methods typically degrade. Theoretically, differences in reward between GRPO and Rank-GRPO at the front ranks are relatively small (e.g., for both sequence-level DCG@N and rank-level DCG@1:N , rewards on the first item are identical, see Eq. (8)), while improvements accumulate more substantially at later positions. In contrast, Rank-GRPO (exp) significantly improves the performance at higher ranks, demonstrating the major influence of context on recommendations compared to the items generated in the higher ranks. Another interesting phenomenon we observed from the training dynamic of Rank-GRPO (exp) (see the middle plots in Figures 2 and 3) is that the relevance (reward) of recommendations generated at lower ranks (e.g., 15, 20, represented by the darker lines) increases and then decreases as the training proceeds, while the relevance of recommendations generated at higher ranks (e.g., 1, 2, 5 represented by the lighter lines) increases monotonically in trend. This is interesting as it shows the RL alignment stage of Rank-GRPO (exp) resembles retrieval and rerank strategy where relevant items are first included in the generated list and then moved to higher ranks motivated by the reward. In addition, the improvement is also evident in the off-policy setting where we set the per-sampling policy update step µ = 2 (Figure 3), which demonstrates the advantage of Rank-GRPO over GRPO and GSPO by aligning rank-wise importance weights with rank-wise rewards. We also note that off-policy performance at the same step lags behind on-policy performance due to the higher variance introduced by importance weighting, though the ability to reuse trajectories sampled by previous policies offers favorable trade-off in efficiency. Due to space limits, we present results for Qwen2.5-0.5B-Instruct (on/off policy) and Llama3.2-3B-Instruct (on policy) in the main text. The full experiments with the remaining LLM backbones are reported in Appendix D.4."
        },
        {
            "title": "3.5 COMPARISON WITH BASELINES",
            "content": "Finally, we present the one-time evaluation on the test set, which is collected one month after the validation set (He et al., 2023; Zhu et al., 2025). We compare different backbone LLMs trained with ConvRec-R1 with state-of-the-art CRSs, including training-based methods and prompting-based black-box LLMs. The detailed description of baselines is provided in Appendix D.2. From Table 1, we observe that both stages of ConvRec-R1 contribute substantially to its superior performance over the baselines. The SFT stage provides strong boost over the zero-shot baseline by grounding the recommendations in the catalog with the correct format, which improves the efficiency and stability of the RL training. The RL stage further enhances performance, especially at higher ranks, by aligning generation with rank-wise rewards. With these improvements, ConvRec-R1 with Qwen2.5-0.5B-Instruct backbone substantially outperforms GPT-4o-mini, ConvRec-R1 with Llama3.2-1B-Instruct backbone performs on par with GPT-4o, and ConvRec-R1 with Llama3.28 Published as conference paper at ICLR 2026 Table 1: Comparison between ConvRec-R1 and various baselines on the REDDIT-V2 dataset. Here, R@k and N@k stand for Recall@k and NDCG@k, respectively. Method R@ R@10 R@15 R@20 N@5 N@10 N@ N@20 Redial (Li et al., 2018) KBRD (Chen et al., 2019) KGSF (Zhou et al., 2020) UniCRS (Wang et al., 2022) NBCRS (Xie et al., 2024) GPT-4o-mini (zero-shot) GPT-4o (zero-shot) (He et al., 2023) CRAG (Zhu et al., 2025) Traditional and Transformer-based CRS 0.0103 0.0444 0.0579 0.0722 0.0801 0.0228 0.0813 0.0921 0.1053 0.1290 0.0274 0.1058 0.1246 0.1344 0. 0.0322 0.1223 0.1433 0.1494 0.2019 LLM Prompting-Based Methods 0.1687 0.2147 0.2212 0.1348 0.1625 0.1715 0.1600 0.1992 0.2030 0.0949 0.1106 0.1146 0.0073 0.0305 0.0405 0.0548 0. 0.0113 0.0418 0.0503 0.0640 0.0833 0.0128 0.0490 0.0599 0.0726 0.0954 0.0143 0.0545 0.0662 0.0778 0.1048 0.0747 0.0861 0.0885 0.0877 0.1028 0.1065 0.0950 0.1136 0. 0.0973 0.1197 0.1227 LLM RL Post-Training-Based Methods (off-policy results see Appendix Table 4) Most results are significant as standard error are between 0.0020-0.0035 for R@5-20 and 0.0010-0.0025 for N@5-20 Qwen2.5-0.5B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-1B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-3B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) 0.0021 0.0642 0.0834 0.0892 0.0946 0.0187 0.0754 0.0979 0.1014 0.1039 0.0574 0.0788 0.1034 0.1103 0.1178 0.0021 0.1027 0.1298 0.1353 0. 0.0240 0.1148 0.1474 0.1527 0.1569 0.0805 0.1191 0.1568 0.1638 0.1756 0.0021 0.1212 0.1617 0.1720 0.1813 0.0257 0.1354 0.1806 0.1907 0.1937 0.0912 0.1410 0.1946 0.2033 0.2150 0.0021 0.1308 0.1803 0.1946 0. 0.0263 0.1498 0.2037 0.2159 0.2165 0.0961 0.1541 0.2186 0.2302 0.2368 0.0023 0.0502 0.0651 0.0701 0.0744 0.0157 0.0595 0.0762 0.0792 0.0813 0.0463 0.0615 0.0824 0.0862 0.0919 0.0022 0.0625 0.0801 0.0849 0. 0.0170 0.0723 0.0920 0.0956 0.0985 0.0535 0.0744 0.0996 0.1035 0.1107 0.0021 0.0678 0.0895 0.0957 0.1016 0.0175 0.0782 0.1026 0.1067 0.1092 0.0566 0.0807 0.1106 0.1169 0.1223 0.0021 0.0704 0.0945 0.1017 0. 0.0176 0.0819 0.1097 0.1134 0.1153 0.0580 0.0842 0.1170 0.1239 0.1283 3B-Instruct backbone even surpasses GPT-4o and CRAG (GPT-4o) on recall/NDCG@20. Notably, CRAG requires 57 GPT-4o API calls per recommendation (for mentioned item extraction, reflection, and re-ranking), incurring far higher cost and latency, whereas ConvRec-R1 enables smaller open-source LLMs to directly produce high-quality recommendations. These results highlight that properly aligned small LLMs can match or even exceed much larger proprietary models in CRS."
        },
        {
            "title": "3.6 ABLATION STUDY",
            "content": "In this section, we present an ablation study where we remove the remap and reflect steps in the behavior cloning dataset construction, as well as the entire SFT stage. The results are summarized in Table 2. We observe that SFT plays crucial role in warming up RL with strong catalog awareness and preliminary ability to generate coherent ranked item lists. In particular, removing the remap step significantly harms performance by weakening catalog grounding, while removing the reflect step degrades the quality of the learned ranking, leading to lower Recall/NDCG at larger k. The full RemapReflectAdjust pipeline followed by SFT and Rank-GRPO thus yields the best overall performance, confirming that all components contribute meaningfully to the final CRS quality."
        },
        {
            "title": "4.1 LLM-BASED CONVERSATIONAL RECOMMENDER SYSTEMS",
            "content": "LLM-based conversational recommender systems (CRS) leverage the language understanding and generation capabilities of LLMs to capture user preferences and deliver recommendations through dialogue (Zhao et al., 2023; Zhang et al., 2024; Lambert et al., 2024). Early approaches often encoded item IDs as special tokens in the model vocabulary, allowing the LLM to generate IDs directly as recommendations (Hua et al., 2023; Zhu et al., 2024b; Zhang et al., 2025b). More recent work, however, has highlighted the benefits of representing items in natural language (e.g., titles with at9 Published as conference paper at ICLR 2026 Table 2: Comparison between ConvRec-R1 and various ablation models on the REDDIT-V2 dataset. Method R@ R@10 R@15 R@20 N@5 N@10 N@ N@20 Qwen2.5-0.5B (SFT) remove remap-reflect step remove reflect step Qwen2.5-0.5B (SFT + Rank-GRPO (exp)) remove SFT stage (R1-zero) Llama-3.2-1B (SFT) remove remap-reflect step remove reflect step Llama-3.2-1B (SFT + Rank-GRPO (exp)) remove SFT stage (R1-zero) Llama-3.2-3B (SFT) remove remap-reflect step remove reflect step Llama-3.2-3B (SFT + Rank-GRPO (exp)) remove SFT stage (R1-zero) 0.0642 0.0579 0.0623 0.0946 0.0440 0.0754 0.0713 0.0714 0.1039 0.0769 0.0788 0.0724 0.0748 0.1178 0.0802 0.1027 0.0885 0.0947 0.1468 0.0545 0.1148 0.1085 0.1102 0.1569 0. 0.1191 0.1187 0.1236 0.1756 0.1295 0.1212 0.1111 0.1190 0.1813 0.0584 0.1354 0.1312 0.1339 0.1937 0.1426 0.1410 0.1369 0.1371 0.2150 0.1639 0.1308 0.1192 0.1307 0.2047 0.0618 0.1498 0.1421 0.1460 0.2165 0. 0.1541 0.1382 0.1441 0.2368 0.1771 0.0502 0.0450 0.0496 0.0744 0.0373 0.0595 0.0574 0.0576 0.0813 0.0651 0.0615 0.0611 0.0619 0.0919 0.0737 0.0625 0.0548 0.0600 0.0915 0.0405 0.0723 0.0690 0.0698 0.0985 0. 0.0744 0.0740 0.0757 0.1107 0.0898 0.0678 0.0614 0.0670 0.1016 0.0419 0.0782 0.0757 0.0767 0.1092 0.0854 0.0807 0.0801 0.0801 0.1223 0.1001 0.0704 0.0637 0.0698 0.1079 0.0431 0.0819 0.0786 0.0800 0.1153 0. 0.0842 0.0809 0.0818 0.1283 0.1039 tributes or descriptions) and prompting the model to output recommendation lists (He et al., 2023; Zhang et al., 2025a; Zhu et al., 2025) or rankings (Hou et al., 2024). This natural-language formulation of the recommendation task maximally preserves the LLMs core language generalization and reasoning abilities and enables flexible verbal guidance of the CRS by conditioning on conversational context, attribute constraints, or other objectives such as novelty and diversity (He et al., 2023; Zhang et al., 2024). However, zero-shot generation suffers from catalog unawareness, formatting errors, and sharp decline in quality toward the end of the recommendation list. While fine-tuning approaches (Luo et al., 2025) or retrieval-augmented generation (RAG) (Zhu et al., 2025) can mitigate these issues, they require large amounts of high-quality data. In contrast, large-scale RL for LLM-based CRS with inexpensive, verifiable rewards remains largely underexplored."
        },
        {
            "title": "4.2 REINFORCEMENT LEARNING WITH VERIFIABLE REWARD",
            "content": "Reinforcement learning from verifiable rewards (RLVR) has emerged as central paradigm for aligning LLMs. core algorithm in this domain is group relative policy optimization (GRPO) (Shao et al., 2024), which estimates advantages by comparing relative rewards across groups of responses. Several extensions, such as DAPO (Yu et al., 2025), Dr. GRPO (Zheng et al., 2025), and GSPO (Liu et al., 2025), have enhanced GRPO through improved length normalization, sample efficiency, and sequence-level stability. Yet, GRPO-style methods remain underexplored for tasks with rankstructured outputs such as recommendation. To our knowledge, only two works, Rec-R1 (Lin et al., 2025) and RecLLM-R1 (Xie et al., 2025), have applied GRPO to recommender systems. Rec-R1 generates item descriptions that are later matched to catalog to compute rewards, differing from our goal of enabling LLMs to directly generate ranked lists of in-catalog items in natural language. RecLLM-R1 generates lists directly but still applies sequence-level reward for token-level updates, which misaligns with the structure of ranking tasks and limits performance as reflected in the vanilla GRPO baseline. In contrast, we propose Rank-GRPO, which introduces rank-level advantages for precise credit assignment and rank-level importance ratio based on the geometric mean of itemtoken probabilities, providing principled framework for aligning LLMs on rank-structured outputs."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced ConvRec-R1, two-stage framework for aligning LLM-based conversational recommender systems with reinforcement learning. ConvRec-R1 combines lightweight distillation pipeline, RemapReflectAdjust, with Rank-GRPO, principled extension of GRPO tailored to rank-style outputs, addressing its fundamental misalignment of sequence-level rewards and tokenlevel updates. Empirically, ConvRec-R1 significantly improves the recommendation quality of the base model on the REDDIT-V2 dataset, surpassing GRPO-style baselines and even large zero-shot black-box models in Recall and NDCG. Beyond recommendation, our method shows promise for 10 Published as conference paper at ICLR 2026 broad range of tasks with rank-structured outputs, suggesting new opportunities to adapt RL-based alignment techniques for ranking, retrieval, and other sequential decision-making problems."
        },
        {
            "title": "REFERENCES",
            "content": "Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yanchen Luo, Chong Chen, Fuli Feng, and Qi Tian. bi-step grounding paradigm for large language models in recommendation systems. ACM Transactions on Recommender Systems, 3(4):127, 2025. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, and Jie Tang. Towards knowledge-based recommender dialog system. In EMNLP, 2019. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and Tat-Seng Chua. On softmax direct preference optimization for recommendation. NeurIPS, 37: 2746327489, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Chongming Gao, Ruijun Chen, Shuai Yuan, Kexin Huang, Yuanqing Yu, and Xiangnan He. SPRec: Self-play to debias LLM-based recommendation. In WWW, pp. 50755084, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi, and Zhou Yu. Inspired: Toward sociable recommendation dialog systems. In EMNLP, pp. 81428152, 2020. Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. Large language models as zero-shot conversational recommenders. In CIKM, pp. 720730, 2023. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin In ECIR, pp. Zhao. Large language models are zero-shot rankers for recommender systems. 364381. Springer, 2024. Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. How to index item IDs for recommendation foundation models. In SIGIR, pp. 195204, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In SOSP, pp. 611626, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. NeurIPS, 33:94599474, 2020. Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, and Chris Pal. Towards deep conversational recommendations. In NeurIPS, 2018. Dawen Liang, Rahul Krishnan, Matthew Hoffman, and Tony Jebara. Variational autoencoders for collaborative filtering. In WWW, pp. 689698, 2018. 11 Published as conference paper at ICLR 2026 Jiacheng Lin, Tian Wang, and Kun Qian. Rec-R1: Bridging generative large language modarXiv preprint els and user-centric recommendation systems via reinforcement learning. arXiv:2503.24289, 2025. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding R1-Zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Sichun Luo, Bowei He, Haohan Zhao, Wei Shao, Yanlin Qi, Yinya Huang, Aojun Zhou, Yuxuan Yao, Zongpeng Li, Yuanzhang Xiao, et al. RecRanker: Instruction tuning large language model as ranker for top-k recommendation. ACM TOIS, 43(5):131, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:2773027744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In Supercomputing Conference, pp. 116, 2020. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERTnetworks. In EMNLP, pp. 39823992, 2019. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. NeurIPS, 12, 1999. Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. Towards unified conversational recommender systems via knowledge-enhanced prompt learning. In KDD, pp. 19291937, 2022. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025. Junda Wu, Rohan Surana, Zhouhang Xie, Yiran Shen, Yu Xia, Tong Yu, Ryan Rossi, Prithviraj Ammanabrolu, and Julian McAuley. In-context ranking preference optimization. COLM, 2025. Yu Xie, Xingkai Ren, Ying Qi, Yao Hu, and Lianlei Shan. RecLLM-R1: two-stage training paradigm with reinforcement learning and chain-of-thought v1. arXiv preprint arXiv:2506.19235, 2025. Zhouhang Xie, Junda Wu, Hyunsik Jeon, Zhankui He, Harald Steck, Rahul Jha, Dawen Liang, Nathan Kallus, and Julian McAuley. Neighborhood-based collaborative filtering for conversational recommendation. In RecSys, pp. 10451050, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation. In SIGIR, pp. 18071817, 2024. 12 Published as conference paper at ICLR 2026 Junjie Zhang, Ruobing Xie, Yupeng Hou, Xin Zhao, Leyu Lin, and Ji-Rong Wen. Recommendation as instruction following: large language model empowered recommendation approach. ACM TOIS, 43(5):137, 2025a. Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. CoLLM: Integrating collaborative embeddings into large language models for recommendation. IEEE Transactions on Knowledge and Data Engineering, 2025b. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, and Jingsong Yu. Improving conversational recommender systems via knowledge graph based semantic fusion. In KDD, pp. 10061014, 2020. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024a. Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for recommender systems. In WWW 2024, pp. 31623172, 2024b. Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, and Jundong Li. Collaborative retrieval for large language model-based conversational recommender systems. In WWW, pp. 33233334, 2025. Published as conference paper at ICLR"
        },
        {
            "title": "APPENDIX",
            "content": "A DETAILS FOR THE REMAP-REFLECT-ADJUST PIPELINE This section provides the detail for the RemapReflectAdjust pipeline used to generate the catalog-grounded behavior cloning dataset for the supervised fine-tuning (SFT) stage of ConvRecR1 (see Section 2.2). We note that the following pipeline is specifically designed for the movie recommendation task, but the generalization to other items generally follows similar procedure. A.1 STEP 1. REMAP The remap step aims to ground the raw recommendations ySFT i,raw in the teacher LLMs recommendation space CΘ, into score vector sremap RC in the target catalog space C. The process involves three components: item metadata generation, similarity calculation, and score aggregation. A.1.1 METADATA GENERATION To facilitate semantic similarity-based remapping, we first generate consistent and comprehensive metadata for every item in both CΘ and C. Based on whether the teacher LLM Θ has the knowledge of the item and whether the item is in the catalog, we have the following three cases: (i) For in-catalog items known to teacher LLM Θ, we use zero-shot prompt to generate summary including keywords and plot description. This ensures consistent style and broad coverage of aspects that facilitate high-quality item-item similarity calculation. (ii) For out-of-catalog (OOC) or hallucinated items. Since these items are recommended by the LLM, it can still generate plausible metadata crucial for mapping them back to the catalog. (iii) For in-catalog items unknown to the LLM (e.g., niche or recent items), we employ retrieval augmented generation (RAG) (Lewis et al., 2020) and in-context learning (ICL). We retrieve external information about the item via Google Search API on Wikipedia, IMDb, and Rotten Tomatoes and provide it to the LLM along with few-shot examples to guide the generation. The prompts we are using to generate the metadata for known/unknown items are as follows. Prompt to Generate Metadata for Items Known to the Teacher LLM Θ Summarize the metadata for the movie movie_name (release year) based on the information in the format of: Movie_Name (Release_Year) Keywords: <comma-separated keywords> Plots: <one-paragraph plot summary> You should separate the keywords with commas without numbering. The plot should be one paragraph. Prompt to Generate Metadata for Items Unknown to the Teacher LLM Θ with RAG and ICL Summarize the metadata for the movie movie name (release year) based on the information in the format of: Movie_Name (Release_Year) Keywords: <comma-separated keywords> Plots: <one-paragraph plot summary> You should separate the keywords with commas without numbering. The plot should be one paragraph. Here are the information related to the target movie: {movie info} Here are some examples for the generated metadata for different movies: {examples} Published as conference paper at ICLR 2026 Here we provide some examples of the generated metadata as follows:"
        },
        {
            "title": "Examples for the Generated Metadata in the Remap Step",
            "content": "Case (i). In-catalog items known to teacher LLM Stranger Things (2016) Keywords: supernatural, mystery, 1980s, parallel universe, friendship, government conspiracy, small town, telekinesis, missing child, monster Plots: young boy named Will Byers sets off chain of events that unravels series of supernatural mysteries. As his friends and family search for him, they encounter mysterious girl with telekinetic powers who may hold the key to finding Will. Meanwhile, secretive government agency conducts experiments that open portal to parallel universe, unleashing terrifying creatures into the town... In the small town of Hawkins, Indiana, the sudden disappearance of a"
        },
        {
            "title": "In a future society divided into five distinct factions based on human",
            "content": "Case (ii). Out-of-catalog (OOC) or hallucinated items The Divergent Series (2014) (a series but not movie) Keywords: dystopian, factions, rebellion, identity, society, future, action, adventure, science fiction, young adult, survival, government, control, freedom Plots: virtues, everyone must choose where they belong. is Divergent, meaning she doesnt fit into any one group. conspiracy to eliminate all Divergents, Tris must rely on her unique abilities to survive and protect those she loves. With the help of the mysterious Four, she embarks on journey of self-discovery and rebellion against the oppressive system, challenging the status quo and fighting for freedom."
        },
        {
            "title": "As she uncovers a",
            "content": "\"La strada verso casa\" tells the poignant story of Michelangelo, Antonio, Case (iii). In-catalog items UNKNOWN to teacher LLM La strada verso casa (2011) Keywords: drama, pain, death, life, Michelangelo, Antonio, Giulia, Italy, subtle pressures, intertwine, healing Plots: and Giulia, three individuals whose lives are abruptly disrupted by pain and death. each other, subtly influencing one anothers paths. journeys, they navigate the complexities of grief and healing, ultimately discovering renewed sense of life and purpose. Italy, the film explores themes of connection, resilience, and the transformative power of human relationships. As they face their personal struggles, they find solace and strength in"
        },
        {
            "title": "Through their intertwined",
            "content": "A.1.2 SIMILARITY AND SCORE CALCULATION With the generated metadata for items in CΘ and in place, we compute two types of similarities: (i) Item-Item Similarity (Sitem-item): We calculate the content-based semantic similarity between every item in the teachers space CΘ and every item in the target catalog C. The generated metadata are embedded using pre-trained sentence-transformer model (Reimers & Gurevych, 2019) (we use NovaSearch/stella en 400M v5 in this paper). The cosine similarity between these embeddings forms the item-item similarity matrix Sitem-item RCΘC. (ii) Conversation-Item Similarity (scontext): To further enhance the contextual relevance, we and each item in the catalog C. We with the same Stella model and its cosine similarity with each catalog items compute the similarity between the conversation xSFT embed xSFT metadata embedding is calculated, resulting in context vector scontext RC. The final remapped scores are calculated by aggregating the signals from the teachers raw recommendations ySFT i,raw, and Sitem-item, scontext as follows: sremap = (Sitem-item + Iic) + λ scontext, (10) where RCΘ is sparse vector representing the positional scores of items in the teachers k, and 0 for raw recommendation list. The score for an item at rank is heuristically set to 1/ items not present, Iic {0, 1}CΘC is sparse indicator mapping matrix with Iic[u, v] = 1 iff the u-th item in CΘ equals the v-th catalog item, which directly transfer the positional scores of any recommended items that are already in the catalog, λ is hyperparameter that weights the importance of the conversation-item similarity for the remapping. In the paper, we set λ = 1. 15 Published as conference paper at ICLR 2026 A.2 STEP 2. REFLECT The reflect step adjusts the ranking of the top candidates from the remap stage by leveraging the teacher LLMs judgment on the contextual relevancy w.r.t the conversation xSFT . While the remapping step grounds the raw recommendations from the teacher ySFT i,raw to the catalog space as sremap, the ranking may not be perfectly aligned with the nuances of the users conversational context as they are conducted based on embedding models, which are known to be relatively insufficient to capture multiple aspects and subtle preferences behind the lines that are crucial for recommendations. Therefore, we leverage LLM-as-a-judge to rate the top-Nr > items (e.g., top 100) from sremap based on their suitability for the conversation. We use the following prompt template: Prompt for LLM-as-a-Judge in the reflect step Pretend you are movie recommender system. will give you conversation between user and you (a recommender system) as well as some movie candidates from our movie database. You need to rate each retrieved movie as recommendations into five levels based on the conversation: 2 (great), 1 (good), 0 (normal), -1 (not good), -2 (bad). Here is the conversation: {context} Here are the movie candidates: {rec_titles}. You need to reply with the rating of each movie in line, in the form of movie_name (release_year)####rating The numerical ratings from the LLM form sparse reflection vector rreflect RC. This vector is then scaled by hyperparameter γ and added to the remapped scores to produce the reflected scores: sreflect = sremap + γ rreflect. (11) This step helps to elevate contextually more relevant items to higher ranks, and therefore further improves the quality of the exemplar ranking demonstration data. In this paper, we choose five-scale reflection scores from [2, 2] representing bad to great and therefore γ is set to 0.5 to normalize the reflection scores to [1, 1], which has similar scale as sremap. A.3 ADJUST Finally, the adjust step is valuable final stage that corrects for residual popularity biases that may be inherited from the teacher model or the remapping process due to its unawareness of the current trend. Specifically, we align the score distribution with the empirical distribution observed in the groundtruth training data (i.e., items with positive user feedback). This is framed as residual learning problem. We learn an item-specific multiplicative bias vector RC and an additive bias vector RC to fine-tune the scores. The final, adjusted scores are computed as: sfinal = sreflect + b, (12) where denotes the element-wise product. The vectors and are optimized by maximizing the multinomial log-likelihood of observing the groundtruth positive items from the training data Dtrain (Liang et al., 2018). If we denote the groundtruth items for conversation as multi-hot vector ygt , the optimization objective can be formulated as: Ladjust = (cid:88) (cid:88) iDtrain j:ygt i,j =1 log exp ((w sreflect,i + b)j) kC exp ((w sreflect,i + b)k) (cid:80) + λww 12 2 + λbb2 2, (13) where λw and λb are hyperparameters for weight decay and are set to 0.01. After this final adjustment step, for each training conversation xSFT , the top-N items from the catalog are selected based on their scores in sfinal to form the final, high-quality demonstration list ySFT used in the SFT stage. i A.4 EXAMPLE OF DEMONSTRATION GENERATED BY REMAPREFLECTADJUST PIPELINE In this section, we present an example from the constructed behavior cloning dataset DSFT for ConvRec-R1, along with qualitative ablation study of the RemapReflectAdjust pipeline. The 16 Published as conference paper at ICLR 2026 example illustrates given conversation xSFT , how the the teachers zero-shot recommendations and the intermediate results evolve across each stage of the pipeline. To facilitate future research on LLM-based CRS, we also release the behavior cloning dataset, together with the accompanying code and trained models, at https://anonymous.4open.science/r/ConvRec-R1-5615/. Exemplar Demonstrations from the Remap-Reflect-Adjust Pipeline (i) Conversation xSFT : Just looking for something like master and commander. (cid:160) User: any good ocean sailing movies like master and commander?. of the old days and not something with submarines/ww2 but did like hunt for red october lot. to be military; can be pirates too. movies, that new netflix animated ocean movie with jared harris/karl urban, waterworld (lmao), ect. think made this topic before and someone suggested horatio hornblower. show of the few episodes watched but had to watch it on youtube and didnt like the quality and dont think it had subtitles, which is almost must for me. suggestions. It doesnt necessarily have know of the obvious stuff like pirates of carribean"
        },
        {
            "title": "I liked the",
            "content": "(ii) Groundtruth ygt : 1492: Conquest of Paradise (1992) (iii) Outputs from the Remap-Reflect-Adjust Pipeline:"
        },
        {
            "title": "Remap",
            "content": "Step 1. i,raw (OOC titles in red) Rk. Zero-Shot ySFT -------------------------------------------------------------------- 1 Mutiny on the Bounty (1935) 2 The Bounty (1984) 3 Captain Blood (1935) 4 The Sea Hawk (1940) 5 Treasure Island (1950) 6 Moby Dick (1956) 7 The Buccaneer (1958) 8 Billy Budd (1962) 9 Swashbuckler (1976) 10 The Black Swan (1942) 11 The Crimson Pirate (1952) 12 Damn the Defiant! (1962) 13 The Long Ships (1964) 14 The Black Pirate (1926) 15 Cutthroat Island (1995) 16 The Adventures of Robin Hood (1938) 17 The Vikings (1958) 18 The Sea Wolf (1941) 19 The Spanish Main (1945) 20 Against All Flags (1952) The Bounty (1984) Captain Blood (1935) The Sea Hawk (1940) Treasure Island (1950) The Crimson Pirate (1952) The Spanish Main (1945) Cutthroat Island (1995) The Adventures of Robin Hood (1938) The Vikings (1958) Master and Commander (2003) Run Silent Run Deep (1958) Das Boot (1981) Black Sails (2014) Captain Phillips (2013) The Caine Mutiny (1954) Greyhound (2020) Captains Courageous (1937) 20,000 Leagues Under the Sea (1954) In the Heart of the Sea (2015) The Wreck of the Mary Deare (1959)"
        },
        {
            "title": "Adjust ySFT",
            "content": "i Remap) Step 3. Reflect (vs. (with popularity) The Bounty (1984) Captain Blood (1935) The Sea Hawk (1940) Treasure Island (1950) Master and Commander (2003) 5 The Crimson Pirate (1952) 1 Das Boot (1981) Rk. Step 2. -------------------------------------------------------------------------------- 1 2 3 4 5 6 7 8 The Spanish Main (1945) 2 9 Cutthroat Island (1995) 2 10 The Adventures of Robin Hood (1938) 2 11 The Vikings (1958) 2 12 Run Silent Run Deep (1958) 1 13 Black Sails (2014) 14 Captain Phillips (2013) 15 The Caine Mutiny (1954) 16 Greyhound (2020) 17 Captains Courageous (1937) 18 20,000 Leagues Under the Sea (1954) 19 In the Heart of the Sea (2015) 20 The Wreck of the Mary Deare (1959) The Bounty (1984) 56 Captain Blood (1935) 31 Master and Commander (2003) 262 2 Das Boot (1981) 523 3 The Sea Hawk (1940) 17 2 Treasure Island (1950) 27 2 Cutthroat Island (1995) 66 The Adventures of Robin Hood (1938) 120 The Crimson Pirate (1952) 7 1 The Vikings (1958) 24 Run Silent Run Deep (1958) 27 The Spanish Main (1945) 3 Black Sails (2014) 103 Captain Phillips (2013) 183 The Caine Mutiny (1954) 59 Greyhound (2020) 78 Captains Courageous (1937) 25 20,000 Leagues Under the Sea (1954) 98 In the Heart of the Sea (2015) 71 The Wreck of the Mary Deare (1959) 2 17 Published as conference paper at ICLR 2026 GRADIENT ANALYSIS FOR GRPO, GSPO AND RANK-GRPO In this section, we provide the full policy gradient derivations for GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025), and our proposed Rank-GRPO. The clipping term from the PPO-style objective is omitted for brevity. All derivations leverage the log-derivative trick (Sutton et al., 1999), which states that for function (θ), its gradient can be expressed as θf (θ) = (θ)θ log (θ). This allows us to rewrite the gradient of the objective in form suitable for sampling-based estimation. (i) GRPO. The gradient of the GRPO objective is derived with the following steps. We start by bringing the gradient operator inside the expectation based on the interchangeability of integration and differentiation. Then we apply the gradient to the terms dependent on θ (which is only the importance ratio wi,t(θ)), and then use the log-derivative trick as follows: θJGRPO(θ) = θEπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:35) wi,t(θ) ˆAi,t = Eπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:35) ˆAi,tθwi,t(θ) = Eπθold = Eπθold (cid:34) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:88) i=1 1 yi 1 yi yi (cid:88) t=1 yi (cid:88) t=1 (cid:35) ˆAi,twi,t(θ)θ log wi,t(θ) (Log-derivative trick) (cid:35) wi,t(θ) ˆAi,tθ log πθ(yi,tx, yi,<t) Eπθold (cid:34) (cid:88) i=1 1 yi yi (cid:88) t=1 wi,t(θ) (cid:124) (cid:123)(cid:122) (cid:125) token (t)-level importance ratio ˆAi (cid:124)(cid:123)(cid:122)(cid:125) sequence (i)-level advantage θ log πθ(yi,tx, yi,<t) (cid:125) (cid:123)(cid:122) token (t)-level gradient of log-likelihood (cid:124) (cid:35) . (14) From Eq. (14), we see that GRPO updates the token-level policy using sequence-level rewards. This introduces two sources of misalignment: (i) the importance ratio is defined at the token level, while the reward is defined at the sequence level, conflicting with the principle of importance sampling (Zheng et al., 2025); and (ii) the reward signal is too coarse for ranking tasks, whereas token-level updates are overly fine-grained. Together, these mismatches make the vanilla GRPO algorithm fundamentally ill-suited for reinforcement learning on rank-structured outputs. (ii) GSPO. Different from GRPO that combines sequence-level rewards with token-level updates, group sequence policy optimization (GSPO) defines both the reward and the importance ratio at the sequence level. Its derivation largely follows Eq. (14) as follows: θJGSPO(θ) = θEπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:35) si(θ) ˆAi = Eπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:35) ˆAiθsi(θ) (cid:35) si(θ) ˆAiθ log si(θ) (Log-derivative trick) = Eπθold = Eπθold (cid:34) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:88) i=1 Eπθold (cid:34) (cid:88) i=1 si(θ) ˆAi 1 yi yi (cid:88) t=1 si(θ) ˆAi (cid:124) (cid:123)(cid:122) (cid:125) sequence (i)-level importance ratio (cid:124)(cid:123)(cid:122)(cid:125) sequence (i)-level advantage θ log πθ(yi,tx, yi,<t) (cid:35) (15) 1 yi (cid:124) yi (cid:88) θ log πθ(yi,tx, yi,<t) , (cid:35) t= (cid:123)(cid:122) sequence (i)-level gradient of log-likelihood (cid:125) (cid:16) πθ(yix) πθold (yix) (cid:17)1/yi where si(θ) = is the length-normalized sequence importance ratio. From Eq. (15), we observe that GSPO aligns the importance ratio with sequence-level rewards, which has been shown to stabilize training in mixture-of-experts models such as Qwen3 series (Yang et al., 2025) where different experts may be activated between the roll-out and current policies. However, both the reward and importance ratio remain too coarse for rank-structured tasks, where each rank should naturally serve as the action unit rather than the entire sequence. Published as conference paper at ICLR 2026 (iii) Rank-GRPO. Finally, for the Rank-GRPO proposed in this paper, the objective, advantage, and importance ratio are all defined at the rank level, where the gradient can be derived as follows: θJRank-GRPO(θ) = θEπθold (cid:34)"
        },
        {
            "title": "1\nGN",
            "content": "G (cid:88) (cid:88) i=1 k=1 (cid:35) wi,k(θ) ˆAi,k = Eπθold (cid:34)"
        },
        {
            "title": "1\nGN",
            "content": "G (cid:88) (cid:88) i=1 k=1 (cid:35) ˆAi,kθwi,k(θ) = Eπθold = Eπθold Eπθold (cid:34) (cid:34)"
        },
        {
            "title": "1\nGN",
            "content": "G (cid:88) (cid:88) i=1 k=1 (cid:88) (cid:88) i=1 k=1 (cid:35) wi,k(θ) ˆAi,kθ log wi,k(θ) (Log-derivative trick) wi,k(θ) ˆAi,kθ log πθ(y(k) x) (cid:35) (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "G (cid:88) (cid:88) i=1 k=1 wi,k(θ) ˆAi,kθ log πθ(y(k) x) (cid:35) = Eπθold (cid:34) (cid:88) (cid:88) i= k=1 wi,k(θ) ˆAi,k (cid:124) (cid:123)(cid:122) (cid:125) rank (k)-level importance ratio (cid:124)(cid:123)(cid:122)(cid:125) rank (k)-level advantage"
        },
        {
            "title": "1\nN |y(k)\ni\n(cid:124)",
            "content": "θ log πθ(yi,k,tx, yi,k,<t) (cid:35) . y(k) (cid:88) t=1 (cid:123)(cid:122) rank (k)-level gradient of log-likelihood (cid:125) x) (16) Here, wi,k(θ) = πθ(y(k) denotes the rank-level importance ratio, and πθ is the length-normalized πθold(y(k) item probability, computed as the geometric mean of its token probabilities. From Eq. (16), we see that Rank-GRPO fundamentally resolves the shortcomings of GRPO and GSPO for rank-structured outputs. By treating each rank as the action unit, it provides more natural granularity for both credit assignment and importance weighting, leading to more stable and task-aligned optimization. x) i"
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "(i) Training Setup. All models are trained on single node via full-parameter fine-tuning. The smaller models (Qwen2.5-0.5B-Instruct, Llama-3.2-1B-Instruct) were trained on single node with four NVIDIA H100 GPUs, each with 80GB of memory. The larger Llama-3.2-3B-Instruct model was trained on four NVIDIA H200 GPUs, each with 141GB of memory. (ii) Optimization and Efficiency. To optimize GPU utilization and training speed, we adopt several standard techniques. We use DeepSpeed (Rajbhandari et al., 2020) with the ZeRO-3 offload strategy for distributed training. We also enable gradient checkpointing (Chen et al., 2016), FlashAttention2 (Dao, 2023), and bf16 mixed-precision training to improve computational efficiency. The effective batch size for most experiments is set to 384, which is calculated as follows: batch size = #GPUs Per GPU batch size Gradient Accumulation Steps (17) (iii) Hyperparameters for the SFT stage. For the SFT stage, we use learning rate of 5 105 with cosine learning rate scheduler that includes 5% warm-up period. (iv) Hyperparameters for the RL stage. For the RL stage, we first tune the learning rate and KL weights for the GRPO baseline and apply the same hyperparameters to Rank-GRPO to ensure fair comparison that slightly favors the baseline. For the on-policy experiments (Section 3.4), we use learning rate of 1 106 for the 0.5B model, and 1 106 with decay to 1 107 for the 1B and 3B models to prevent collapse due to larger size. For the off-policy experiments (Section 3.4, with µ = 2), we keep the same learning rate for the 0.5B model but halve it to 5 107 with decay to 5 108 for the 3B models to account for the instability of importance weighting. In addition, we found that training is comparatively less stable for the 1B model (which we attribute to the base model itself as training for both 0.5B and 3B backbones are stable), so we adopt finer-grained schedule 5 107 2.5 107 1 107 5 108 to prevent divergence. The KL coefficient is set to 0.001. The clip range ϵlow and ϵhigh is set to 0.2 and 0.26 for GRPO and 0.06 and 0.08 for Rank-GRPO to ensure similar expected clipping range after applying the rank-wise geometric mean and effective rank probabilities. The clip range for GSPO is set to 3e 4 and 4e 4 as recommended by the paper (Zheng et al., 2025). For Rank-GRPO, both ϵo and ϵu are set to 0.1. 19 Published as conference paper at ICLR 2026 (v) Inference and Generation. For efficient LLM rollouts during the RL stage, we use vLLM (Kwon et al., 2023) with tensor parallel size of 2 for the 0.5B and 1B models and 4 for the 3B model. We set the GPU memory utilization ratio to 0.5. All rollouts are generated with both temperature and top-p (for nucleus sampling) set to 1.0, respectively."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL DETAILS AND RESULTS",
            "content": "D.1 PROMPT USED TO GENERATE RECOMMENDATIONS Following He et al. (2023) and Zhu et al. (2025), we fix the number of required recommendations to = 20 and use consistent prompt across all experiments to elicit item recommendation lists from the LLMs. The prompt explicitly instructs the model to output standardized English movie titles with release years in line-by-line format, ensuring uniformity and facilitating evaluation."
        },
        {
            "title": "Prompt to Generate Recommendations",
            "content": "Pretend you are movie recommender system. will give you conversation between user and you (a recommender system). Based on the conversation, you need to reply with 20 recommendations. List the standardized English title of each movie in each line in the form of \"movie name\" (release_year) with NO extra words or sentences. Here is the conversation: {context} D.2 BASELINES USED IN THE MAIN PAPER Redial (Li et al., 2018) employs denoising autoencoder to model usermentioned items and generate recommendations, while simultaneously using an recurrent neural network (RNN)-based architecture to model and produce conversational utterances. KBRD (Chen et al., 2019) augments CRS with external knowledge graphs. It applies relational graph neural network (RGNN) over the DBpedia knowledge graph to encode entities and maximizes the similarity between co-occurring words and entities, thereby fusing semantic and knowledge-level signals for recommendation. KGSF (Zhou et al., 2020) further extends knowledge-enhanced CRS by integrating word-level KG (from ConceptNet) into conversations. It leverages mutual information maximization between conversational semantics and entity KG embeddings to fuse entity information at the word level. UniCRS (Wang et al., 2022) introduces unified pre-trained transformer backbone to capture dialogue context, with cross-attention mechanism over entity embeddings learned from RGNN. This enables the model to fuse knowledge graph information with conversational semantics. Zero-shot LLM (He et al., 2023) applies large language models directly to CRS. Dialogues are input with task-specific prompts and formatting instructions to elicit recommendation lists, but without grounding to catalog or augmenting with external retrieval. NBCRS (Xie et al., 2024) uses Sentence-BERT to retrieve training conversations similar to given test conversation and takes the majority vote of the groundtruth items as recommendations. Neighbor size is selected based on the validation set. CRAG (Zhu et al., 2025) retrieves collaborative knowledge based on mentioned items and historical user-item interactions. Specifically, it introduces two-step reflection mechanism with LLM-as-a-judge to refine the retrieved items and the final ranking. D.3 ADDITIONAL COMPARISON WITH RL-FREE POST-TRAINING-BASED METHODS In this part, we compare Rank-GRPO with several RL-free post-training methods for LLM-based recommender systems based on direct preference optimization (DPO) (Rafailov et al., 2023). DPO assumes access to pairwise preferences over responses to the same prompt and shows that, under BradleyTerry model (Bradley & Terry, 1952) for the latent reward, simple maximum-likelihood objective can be derived to directly optimize the policy πθ from preference data. Due to the pairwise 20 Published as conference paper at ICLR 2026 Table 3: Comparison between ConvRec-R1 and various DPO-based RL-free preference alignment baselines with different backbone models on the REDDIT-V2 dataset. Method R@5 R@10 R@15 R@20 N@5 N@ N@15 N@20 Qwen2.5-0.5B (zero-shot) + SFT (DPO) + DPO + SFT (DPO) + S-DPO + SFT (DPO) + SPRec + SFT + Rank-GRPO (exp) Llama-3.2-1B (zero-shot) + SFT (DPO) + DPO + SFT (DPO) + S-DPO + SFT (DPO) + SPRec + SFT + Rank-GRPO (exp) Llama-3.2-3B (zero-shot) + SFT (DPO) + DPO + SFT (DPO) + S-DPO + SFT (DPO) + SPRec + SFT + Rank-GRPO (exp) 0.0021 0.0716 0.0875 0.0841 0. 0.0187 0.0823 0.0904 0.0928 0.1039 0.0574 0.0894 0.1072 0.1053 0.1178 0.0021 0.1058 0.1240 0.1256 0.1468 0.0240 0.1158 0.1307 0.1375 0.1569 0.0805 0.1279 0.1419 0.1491 0.1756 0.0021 0.1300 0.1488 0.1524 0. 0.0257 0.1437 0.1588 0.1591 0.1937 0.0912 0.1491 0.1734 0.1809 0.2150 0.0021 0.1473 0.1699 0.1686 0.2047 0.0263 0.1636 0.1860 0.1889 0.2165 0.0961 0.1697 0.1943 0.1965 0.2368 0.0023 0.0570 0.0657 0.0657 0. 0.0157 0.0646 0.0718 0.0758 0.0813 0.0463 0.0708 0.0807 0.0828 0.0919 0.0022 0.0681 0.0772 0.0775 0.0915 0.0170 0.0752 0.0829 0.0887 0.0985 0.0535 0.0808 0.0933 0.0956 0.1107 0.0021 0.0752 0.0845 0.0855 0. 0.0175 0.0834 0.0926 0.0953 0.1092 0.0566 0.0915 0.1019 0.1043 0.1223 0.0021 0.0797 0.0888 0.0898 0.1079 0.0176 0.0887 0.0987 0.1050 0.1153 0.0580 0.0896 0.1079 0.1109 0.1283 comparison nature of DPO (where it is difficult to define preference between two ranked lists), existing DPO variants for recommendation are primarily designed for candidate-selection scenarios, i.e., choosing positive items from small candidate set given in the prompt (Wu et al., 2025). In contrast, Rank-GRPO targets more generative setting, where ranked list of items is generated directly by the LLM-based policy based on the conversational context, removing the need for an explicit retriever. In addition to vanilla DPO, the DPO-based baselines considered in this paper are: S-DPO (Chen et al., 2024) extends DPO to multiple negatives in the candidate set and optimizes softmax-based DPO loss over one preferred item and several dispreferred items. SPRec (Gao et al., 2025) builds on S-DPO with self-play procedure that alternates SFT on positive items and DPO steps where negatives are drawn from the models own previous predictions. During training, we first perform an SFT (DPO) phase, where we modify the prompt used in RankGRPO (see Section D.1) to request only single recommendation, and use the ground-truth items as supervision. We find this setup substantially improves performance for DPO-based methods compared to directly using ranked-list data for SFT (as with ConvRec-R1). To adapt vanilla DPO and S-DPO to the generative ranking setting studied in this paper, we follow SPRec (Gao et al., 2025) and adopt BIGRec (Bao et al., 2025) to score items and induce full ranking over the catalog. Even though we cache shared prefix for the conversational context, this pipeline is still slower than ConvRec-R1, as it needs to score large number of catalog items. Empirically, as shown in Table 3, both vanilla DPO and its extensions (S-DPO and SPRec) significantly improve upon the zero-shot model. However, there remains gap between these adapted DPO variants and RankGRPO. This highlights the advantage of Rank-GRPO for generative ranking: the model directly produces ranked list by jointly considering the context and previously generated items, rather than relying on post-hoc ranking from per-item scores. At the same time, we emphasize that DPO and SDPO are not originally designed for generative ranking tasks; they remain strong RL-free baselines when high-quality retriever is available to propose compact candidate set in the prompt. D.4 ADDITIONAL RESULTS FOR THE LLAMA3.2-1B MODEL In this section, we provide the training dynamics of ConvRec-R1 with Llama3.2-1B-Instruct model as the backbone under the on/off-policy setting, as illustrated in Figs. 4, 5. The results mirror the observations from smaller and larger backbones in Section 3.4: (i) training rewards increase consistently across ranks, confirming that Rank-GRPO enables more stable credit assignment even in mid-sized models; (ii) validation NDCG improves more rapidly than with vanilla GRPO, indicating that the rank-level importance weighting contributes to faster convergence; and (iii) improvements accumulate particularly toward the tail positions of the list, where methods with sequence-level re21 Published as conference paper at ICLR 2026 Figure 4: Training dynamics of ConvRec-R1 with Llama3.2-1B-Instruct backbone (on-policy). Figure 5: Comparison of training dynamics of reward and validation NDCG between GRPO, GSPO and Rank-GRPO (off-policy) with Llama3.2-1B-Instruct backbone on the REDDIT-V2 dataset. ward typically degrade. These dynamics highlight that the advantages of Rank-GRPO generalize across backbone sizes, striking balance between training efficiency and model capacity. D.5 TEST SET EVALUATION RESULTS FOR THE OFF-POLICY SETTING Table 4 presents the test set performance of ConvRec-R1 and baselines under the off-policy setting (µ = 2). As expected, all methods perform worse than their on-policy counterparts in Table ??, reflecting the additional variance introduced by importance weighting. Nevertheless, Rank-GRPO consistently outperforms both vanilla GRPO and GSPO across nearly all metrics, delivering the most substantial improvements over the SFT baseline. These results highlight that, although off-policy training is inherently more challenging for RL-based post-training, the use of rank-level importance weighting enables more stable and effective updates than sequenceor token-level approaches. Consequently, ConvRec-R1 maintains strong recommendation quality (compared with GPT-4o and GPT-4o-mini) while benefiting from the efficiency of reusing previously sampled trajectories. D.6 ADDITIONAL ANALYSIS OF CATALOG MEMORIZATION DURING THE RL SETTING To further evaluate the impact of RL on catalog grounding, we track the catalog hit ratio on the validation set throughout training (Figure 6). We find that the recommendations from both GRPO and Rank-GRPO gradually drift away from the catalog as training progresses, due to the absence of direct catalog grounding as in the SFT stage. However, the drift is noticeably slower for Rank-GRPO, since its rank-level return explicitly assigns zero reward in-position to outof-catalog items, whereas GRPO struggles with proper credit assignment with the sequence-level reward. An additional artifact is observed when comparing backbone sizes: the 0.5B model exhibits stronger fluctuations in the in-catalog ratio, while the 3B model yields smoother curve. This difference stems from our stabilization strategy: on larger models (1B and 3B), we reduce the learning rate in the second epoch to prevent vanilla GRPO from collapsing (we use the same learning rate on Rank-GRPO for fairness of comparison), whereas the 0.5B model remains stable with 1 106. Figure 6: Dynamics of percentage of in-catalog recommendations on the REDDIT-V2 validation set. D.7 ADDITIONAL RESULTS ON THE REDIAL DATASET In this section, we present experiments on the REDIAL dataset (Li et al., 2018). Compared to the REDDIT-V2 dataset reported in the main paper, the REDIAL dataset is smaller in scale, with fewer conversations and smaller movie catalog. It consists of multi-turn dialogues between seeker and recommender, where the seeker describes their preferences and the recommender suggests movies in natural language. We follow the standard split protocol: 80% for training (20,896 conversations), 22 Published as conference paper at ICLR 2026 Table 4: Comparison between ConvRec-R1 and various baselines on the REDDIT-V2 dataset in the off-policy setting. Here, R@k and N@k stand for Recall@k and NDCG@k, respectively. Method R@5 R@ R@15 R@20 N@5 N@10 N@15 N@ Redial (Li et al., 2018) KBRD (Chen et al., 2019) KGSF (Zhou et al., 2020) UniCRS (Wang et al., 2022) NBCRS (Xie et al., 2024) GPT-4o-mini (zero-shot) GPT-4o (zero-shot) (He et al., 2023) CRAG (Zhu et al. (2025)) Traditional and Transformer-based CRS 0.0103 0.0444 0.0579 0.0722 0.0801 0.0228 0.0813 0.0921 0.1053 0.1290 0.0274 0.1058 0.1246 0.1344 0.1655 0.0322 0.1223 0.1433 0.1494 0. LLM Prompting-Based Methods 0.1687 0.2147 0.2212 0.1348 0.1625 0.1715 0.1600 0.1992 0.2030 0.0949 0.1106 0.1146 0.0073 0.0305 0.0405 0.0548 0.0661 0.0113 0.0418 0.0503 0.0640 0. 0.0128 0.0490 0.0599 0.0726 0.0954 0.0143 0.0545 0.0662 0.0778 0.1048 0.0747 0.0861 0.0885 0.0877 0.1028 0.1065 0.0950 0.1136 0.1164 0.0973 0.1197 0. LLM Post-Training-Based Methods (off-policy, µ = 2) Most results are significant as standard error are between 0.0020-0.0035 for R@5-20 and 0.0015-0.0025 for N@5-20 Qwen2.5-0.5B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+ GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-1B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-3B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) 0.0021 0.0642 0.0781 0.0816 0.0818 0.0935 0.0187 0.0754 0.0937 0.0901 0.0937 0.1037 0.0574 0.0788 0.0952 0.0942 0.1037 0.1092 0.0021 0.1027 0.1176 0.1243 0.1256 0. 0.0240 0.1148 0.1415 0.1389 0.1464 0.1555 0.0805 0.1191 0.1425 0.1436 0.1572 0.1653 0.0021 0.1212 0.1429 0.1511 0.1604 0.1738 0.0257 0.1354 0.1726 0.1693 0.1832 0.1927 0.0912 0.1410 0.1726 0.1770 0.1958 0.2010 0.0021 0.1308 0.1538 0.1641 0.1764 0. 0.0263 0.1498 0.1974 0.1899 0.2066 0.2166 0.0961 0.1541 0.1952 0.1943 0.2220 0.2272 0.0023 0.0502 0.0623 0.0647 0.0644 0.0735 0.0157 0.0595 0.0728 0.0700 0.0731 0.0806 0.0463 0.0615 0.0766 0.0758 0.0816 0.0845 0.0022 0.0625 0.0748 0.0784 0.0784 0. 0.0170 0.0723 0.0882 0.0857 0.0902 0.0973 0.0535 0.0744 0.0916 0.0914 0.0989 0.1028 0.0021 0.0678 0.0823 0.0862 0.0886 0.0982 0.0175 0.0782 0.0975 0.0947 0.1011 0.1081 0.0566 0.0807 0.1004 0.1011 0.1101 0.1132 0.0021 0.0704 0.0852 0.0896 0.0930 0. 0.0176 0.0819 0.1042 0.1002 0.1074 0.1146 0.0580 0.0842 0.1065 0.1058 0.1169 0.1203 Figure 7: Training dynamics on loss, validation metrics, validation in-catalog recommendation ratio with different backbone models during the SFT stage on the REDIAL dataset. 10% for validation (2,612), and 10% for testing (2,613) (He et al., 2023). In addition, we choose different teacher model, i.e., Claude-3.7-Sonnet, to demonstrate the generalization over different large teacher LLM to generate the catalog-grounded behavior cloning dataset. D.7.1 EVALUATION ON THE SFT STAGE As shown in Fig. 7, as with the results on the REDDIT-V2 dataset (see Section 3.3), training loss decreases steadily, while validation loss quickly plateaus, reflecting the difficulty of direct imitation learning when the outputs from teachers are long, structured recommendation lists. At the same time, the in-catalog recommendation ratio rapidly approaches nearly 100% across all three backbones, indicating that the RemapReflectAdjust pipeline plus SFT is effective at grounding the models in the Redial catalog. Finally, NDCG@20 on the validation set improves substantially over the zero23 Published as conference paper at ICLR 2026 Figure 8: Left + Middle: Training dynamics of the reward acquired by ConvRec-R1 with RankGRPO on different rank during the RL stage. Right: Comparison of validation NDCG between GRPO and Rank-GRPO (both on-policy) on the REDIAL dataset. Table 5: Comparison between ConvRec-R1 and various baselines on the REDIAL dataset in the on-policy setting. Here, R@k and N@k stand for Recall@k and NDCG@k, respectively."
        },
        {
            "title": "Method",
            "content": "R@5 R@10 R@15 R@20 N@5 N@ N@15 N@20 Redial (Li et al., 2018) KBRD (Chen et al., 2019) KGSF (Zhou et al., 2020) UniCRS (Wang et al., 2022) NBCRS (Xie et al., 2024) Traditional and Transformer-based CRS 0.0178 0.0763 0.0995 0.1241 0.1848 0.0440 0.1697 0.1999 0.2156 0.3687 0.0380 0.1356 0.1535 0.1756 0. 0.0502 0.1907 0.2235 0.2330 0.4108 0.0114 0.0474 0.0630 0.0851 0.1228 0.0187 0.0686 0.0826 0.1050 0.1568 0.0203 0.0778 0.0951 0.1153 0.1788 0.0221 0.0840 0.1020 0.1199 0.1891 Claude-3.7 (zero-shot) (He et al., 2023) CRAG (Zhu et al., 2025) 0.2088 0.2097 0.3135 0.3211 0.3717 0.3810 0.4097 0.4201 0.1346 0.1300 0.1691 0. 0.1849 0.1889 0.1941 0.1976 LLM Prompting-Based Methods Qwen2.5-0.5B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-1B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-3B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) LLM RL Post-Training-Based Methods 0.0050 0.1634 0.2114 0.2251 0.2385 0.1036 0.1714 0.2236 0.2365 0. 0.1473 0.1756 0.2282 0.2378 0.2691 0.0050 0.2570 0.3095 0.3253 0.3542 0.1522 0.2731 0.3296 0.3392 0.3651 0.2030 0.2750 0.3265 0.3391 0.3740 0.0050 0.3145 0.3633 0.3885 0.4153 0.1672 0.3345 0.3924 0.4059 0. 0.2284 0.3392 0.3936 0.4084 0.4361 0.0050 0.3512 0.3917 0.4272 0.4431 0.1704 0.3715 0.4196 0.4472 0.4526 0.2309 0.3793 0.4300 0.4471 0.4648 0.0042 0.1036 0.1431 0.1531 0.1640 0.0657 0.1078 0.1526 0.1605 0. 0.0972 0.1114 0.1574 0.1630 0.1820 0.0042 0.1345 0.1756 0.1857 0.2018 0.0818 0.1414 0.1876 0.1942 0.2092 0.1158 0.1441 0.1898 0.1964 0.2167 0.0042 0.1502 0.1902 0.2029 0.2184 0.0859 0.1580 0.2047 0.2125 0. 0.1228 0.1615 0.2080 0.2153 0.2336 0.0042 0.1590 0.1971 0.2123 0.2252 0.0867 0.1671 0.2114 0.2226 0.2323 0.1234 0.1712 0.2168 0.2247 0.2405 shot models, confirming that SFT again provides strong warm start for the subsequent RL stage by bestowing the backbone LLM with preliminary generative ranking abilities. Published as conference paper at ICLR 2026 Figure 9: Left + Middle: Training dynamics of the reward acquired by ConvRec-R1 with RankGRPO on different rank during the RL stage. Right: Comparison of validation NDCG between GRPO and Rank-GRPO (off-policy) on the REDIAL dataset. D.7.2 EVALUATION ON THE RL STAGE On the RL stage, we also observe similar trends to the results on REDDIT-V2 (Section 3.4). In the on-policy setting (left and middle panels of Fig. 8), the rank-wise rewards under Rank-GRPO increase across ranks, indicating the ability of ConvRec-R1 to improve the entire recommendation list. The right panel shows that Rank-GRPO consistently converges to higher validation NDCG than vanilla GRPO for all three backbones, with the gains being more pronounced at larger k, again suggesting that Rank-GRPO is particularly effective at maintaining quality toward the tail of the list. In the off-policy setting with per-sampling update step µ = 2 (see Fig. 9), Rank-GRPO also compare favorably over GRPO and GSPO in terms of validation NDCG. The off-policy curves lag slightly behind their on-policy counterparts due to the additional variance introduced by importance weighting, but they still achieve clear improvements over the SFT and vanilla GRPO baselines. Furthermore, Tables 5 and 6 show that on REDIAL, ConvRec-R1 again substantially improves over both zero-shot and SFT-only backbones, and consistently outperforms vanilla GRPO (and GSPO in the off-policy case) across backbones and metrics. The Llama-3.2-3B backbone with Rank-GRPO (exp) achieves the best overall performance, surpassing both zero-shot Claude-3.7 and RAGbased LLM system composed of Claude-3.7 with substantially less inference cost. D.7.3 SENSITIVITY W.R.T. REWARD SHAPING HYPERPARAMETER In this part, we examine the influence of the rewardshaping hyperparameter introduced in Section 2.3.3. Recall that we add per-token penalty ϵo < 0 to enforce instruction following: premature <eos> tokens that stop generation before items are produced receive penalty, and any overflow items beyond rank are also penalized. We sweep ϵo {0, 0.01, 0.1, 0.5, 1} and train Rank-GRPO Figure 10: Sensitivity of Recall@k for Rank- (exp) with the Qwen2.5-0.5B-Instruct backbone; GRPO (exp) to ϵo with Qwen2.5-0.5B-Instruct. the resulting Recall@K curves are shown in Fig. 10. When no penalty is applied (ϵo = 0), performance is noticeably worse, reflecting the fact that failIn contrast, once ures to respect the required list length are not corrected by the reward signal. 25 Published as conference paper at ICLR 2026 Table 6: Comparison between ConvRec-R1 and various baselines in the off-policy setting on the REDIAL dataset. Here, R@k and N@k stand for Recall@k and NDCG@k, respectively. Method R@ R@10 R@15 R@20 N@5 N@10 N@ N@20 LLM Post-Training-Based Methods (off-policy, µ = 2) Qwen2.5-0.5B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-1B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) Llama-3.2-3B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+GSPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) 0.0050 0.1634 0.2095 0.2056 0.2140 0.2363 0.1036 0.1714 0.2218 0.2156 0.2249 0.2479 0.1473 0.1756 0.2237 0.2242 0.2327 0.2648 0.0050 0.2570 0.3005 0.3005 0.3143 0. 0.1522 0.2731 0.3255 0.3102 0.3277 0.3562 0.2030 0.2750 0.3228 0.3198 0.3404 0.3713 0.0050 0.3145 0.3506 0.3536 0.3694 0.3993 0.1672 0.3345 0.3790 0.3705 0.3910 0.4225 0.2284 0.3392 0.3895 0.3781 0.3997 0.4332 0.0050 0.3512 0.3829 0.3841 0.4038 0. 0.1704 0.3715 0.4143 0.4157 0.4345 0.4429 0.2309 0.3793 0.4268 0.4188 0.4424 0.4610 0.0042 0.1036 0.1422 0.1379 0.1470 0.1614 0.0657 0.1078 0.1495 0.1438 0.1535 0.1659 0.0972 0.1114 0.1546 0.1535 0.1620 0.1792 0.0042 0.1345 0.1720 0.1692 0.1801 0. 0.0818 0.1414 0.1835 0.1748 0.1874 0.2020 0.1158 0.1441 0.1872 0.1853 0.1976 0.2148 0.0042 0.1502 0.1857 0.1837 0.1950 0.2118 0.0859 0.1580 0.1981 0.1911 0.2046 0.2200 0.1228 0.1615 0.2053 0.2012 0.2137 0.2322 0.0042 0.1590 0.1936 0.1911 0.2034 0. 0.0867 0.1671 0.2066 0.2022 0.2153 0.2251 0.1234 0.1712 0.2144 0.2111 0.2240 0.2385 Table 7: Comparison between ConvRec-R1 and various baselines on the REDIAL dataset with Qwen2.5-7B-Instruct backbone. R@k and N@k stand for Recall@k and NDCG@k, respectively."
        },
        {
            "title": "Method",
            "content": "R@5 R@10 R@15 R@20 N@5 N@ N@15 N@20 Qwen-2.5-7B (zero-shot) + SFT + SFT+Vanilla GRPO + SFT+Rank-GRPO (log) + SFT+Rank-GRPO (exp) 0.1272 0.1807 0.2286 0.2415 0.2723 0.1913 0.2723 0.3288 0.3491 0. 0.2175 0.3342 0.3753 0.4012 0.4427 0.2217 0.3703 0.4207 0.4495 0.4745 0.0795 0.1164 0.1516 0.1646 0.1811 0.1006 0.1465 0.1879 0.1986 0.2158 0.1078 0.1636 0.2019 0.2176 0.2338 0.1088 0.1723 0.2095 0.2245 0. any negative penalty is introduced, the performance first increases and then decreases when ϵo decreases from 0.01 to 1, but the fluctuation is not very significant. This indicates that performance of Rank-GRPO is quite robust to the exact magnitude of the penalty, with only mild fluctuations. D.7.4 GENERALIZATION TO LARGER MODELS Although in practice we expect models up to 3B parameters to be the most suitable for real-world deployment of CRS due to latency and memory constraints, we also conduct an exploratory study with larger backbone to assess the scalability of ConvRec-R1 and Rank-GRPO. Specifically, Table 7 reports additional results on the REDIAL dataset with the Qwen2.5-7B-Instruct backbone. Training this 7B model on the full REDDIT-V2 dataset (400k conversations) is beyond our current computational budget, so we only experiment on the smaller REDIAL benchmark. Notebly, we observe trends that are consistent with those of smaller models: SFT provides strong improvement over the zero-shot baseline, and Rank-GRPO further boosts the ranking ability of the LLM. In particular, the Qwen2.5-7B-Instruct + Rank-GRPO (exp) model slightly surpasses the Llama-3.23B-Instruct counterpart on most metrics (see Table 5). These findings suggest that ConvRec-R1 and Rank-GRPO generalize well to larger backbones when computational resources permit. DISCUSSION ON GENERALIZATION OF CONVREC-R1 There are three main datasets widely used in CRS research, i.e., REDDIT-V2 (He et al., 2023; Zhu et al., 2025), REDIAL (Li et al., 2018), and INSPIRE (Hayati et al., 2020), all focused on movie recommendations. We exclude INSPIRE from experiments because it contains only 1,000 conversations, which is insufficient to reliably fine-tune LLMs with billions of parameters. An advantage Published as conference paper at ICLR 2026 of the movie recommendation setting is that each item (movie) naturally comes with short textual representation (e.g., title with release year), which can be understood by the LLM and directly used as the item tokens. To generalize ConvRec-R1 to other domains such as e-commerce or short-video recommendation, we can either use pretrained textual tokens (Hou et al., 2024; Zhang et al., 2025a) or introduce new special tokens (Zhu et al., 2024b; Hua et al., 2023) as codebook to represent items. However, since not all arbitrary token combinations in the codebook correspond to valid items, the proposed SFT stage with reflect-rank-adjust can still be applied to teach the LLM catalog awareness and to endow it with preliminary generative ranking ability over predefined item catalog. The subsequent RL stage with Rank-GRPO can then be applied to utilize the verifiable, rank-wise rewards defined over the generated items to align the generations with user preferences. 27 Published as conference paper at ICLR"
        },
        {
            "title": "F QUALITATIVE EXAMPLES",
            "content": "Comparison between Zero-Shot and SFT/RL stage of ConvRec-R1 with Qwen2.5-0.5B (i) Conversation xSFT : (cid:160) User: Best 3d movies to watch?. What are the best 3d movies to watch on my vr headset (except Avatar and mcu movies)? want the 3d effects to actually be good and not some cheap garbage 3d processing some studios apply to their movies. immersive and cool. Movies have already seen in 3d are: Avatar, avengers, godzilla, star wars the force awakens, pixels, terminator genesys and jurassic world. also want the 3d to actually be : Terminator 2: Judgment Day (1991), Hugo (2011), The Walk (2015), Titanic (ii) Groundtruth ygt (1997), Ready Player One (2018), Predator (1987), Jurassic Park (1993), Mad Max: Fury Road (2015), Kong: Skull Island (2017), Gravity (2013), Life of Pi (2012), Blade Runner 2049 (2017), Pacific Rim (2013) (iii) Outputs from Qwen2.5-0.5B: Processed Rec. Rk. (a) Zero-Shot Raw Output (NDCG@20=0.000) ------------------------------------------------------------------------------------- 1 Avatar 2 Avengers 3 Godzilla 4 Star Wars: The Force Awakens 5 Pixels 6 Terminator: Genesys 7 Jurassic World Avatar Avengers Godzilla Star Wars: Pixels Terminator: Genesys Jurassic World The Force Awakens (NDCG@20=0.249, 0.249 ) Fury Road (2015) Processed Rec. Rk. (b) After SFT (Checkpoint-1500) Raw Output ------------------------------------------------------------------------------------- 1 Guardians of the Galaxy (2014) 2 Mad Max: Fury Road (2015) 3 The Jungle Book (2016) 4 Interstellar (2014) 5 Gravity (2013) 6 Inception (2010) 7 The Matrix (1999) 8 Ready Player One (2018) 9 Oblivion (2013) 10 The Dark Knight Rises (2012) 11 The Adventures of Tintin (2011) 12 Tron: Legacy (2010) 13 Valerian and City of Thousand Planets (2017) Valerian and City of Thousand Planets (2017) 14 The Fifth Element (1997) 15 Jumanji: Welcome to the Jungle (2017) 16 The Last of Us (2013) 17 The Hobbit: An Unexpected Journey (2012) ... Guardians of the Galaxy (2014) Mad Max: The Jungle Book (2016) Interstellar (2014) Gravity (2013) Inception (2010) The Matrix (1999) Ready Player One (2018) Oblivion (2013) The Dark Knight Rises (2012) The Adventures of Tintin (2011) Tron: The Fifth Element (1997) Jumanji: The Last of Us (2013) The Hobbit: An Unexpected Journey (2012) Welcome to the Jungle (2017) Legacy (2010) (NDCG@20=0.444, 0.195 ) Fury Road (2015) Processed Rec. Rk. (c) After RL (Checkpoint-15800) Raw Output ------------------------------------------------------------------------------------- 1 Mad Max: Fury Road (2015) 2 Interstellar (2014) 3 The Matrix (1999) 4 Blade Runner 2049 (2017) 5 Gravity (2013) 6 Inception (2010) 7 Ready Player One (2018) 8 Guardians of the Galaxy (2014) 9 Alita: Battle Angel (2019) 10 Tron: Legacy (2010) 11 The Fifth Element (1997) 12 Ready or Not (2019) 13 Valerian and City of Thousand Planets (2017) Valerian and City of Thousand Planets (2017) 14 Edge of Tomorrow (2014) 15 Oblivion (2013) 16 The Dark Knight Rises (2012) 17 Dunkirk (2017) ... Mad Max: Interstellar (2014) The Matrix (1999) Blade Runner 2049 (2017) Gravity (2013) Inception (2010) Ready Player One (2018) Guardians of the Galaxy (2014) Alita: Tron: The Fifth Element (1997) Ready or Not (2019) Edge of Tomorrow (2014) Oblivion (2013) The Dark Knight Rises (2012) Dunkirk (2017) Battle Angel (2019) Legacy (2010) Published as conference paper at ICLR 2026 Comparison between Zero-Shot and SFT/RL stage of ConvRec-R1 with Llama-3.2-1B (i) Conversation xSFT : (cid:160) User: Best 3d movies to watch?. What are the best 3d movies to watch on my vr headset (except Avatar and mcu movies)? want the 3d effects to actually be good and not some cheap garbage 3d processing some studios apply to their movies. immersive and cool. Movies have already seen in 3d are: Avatar, avengers, godzilla, star wars the force awakens, pixels, terminator genesys and jurassic world. also want the 3d to actually be : Terminator 2: Judgment Day (1991), Hugo (2011), The Walk (2015), Titanic (ii) Groundtruth ygt (1997), Ready Player One (2018), Predator (1987), Jurassic Park (1993), Mad Max: Fury Road (2015), Kong: Skull Island (2017), Gravity (2013), Life of Pi (2012), Blade Runner 2049 (2017), Pacific Rim (2013) (iii) Outputs from Llama-3.2-1B: Processed Rec. Rk. (a) Zero-Shot Raw Output (NDCG@20=0.000) ------------------------------------------------------------------------------------- 1 2 3 4 5 6 7 8-20 (repeats The Prestige) Inception (2010) Interstellar (2014) The Matrix (1999) The Prestige (2006) The Lord of the Rings (2001) The Dark Knight (2008) The Bourne Identity (2002) (repeats The Prestige) Inception (2010) Interstellar (2014) The Matrix (1999) The Prestige (2006) The Lord of the Rings (2001) The Dark Knight (2008) The Bourne Identity (2002) Processed Rec. (NDCG@20=0.342, 0.342 ) Blade Runner 2049 (2017) Inception (2010) Gravity (2013) The Matrix (1999) Interstellar (2014) Dunkirk (2017) Ready Player One (2018) Alita: Blade Runner 2049 (2017) Inception (2010) Gravity (2013) The Matrix (1999) Interstellar (2014) Dunkirk (2017) Ready Player One (2018) Alita: Battle Angel (2019) Valerian and City of Thousand Planets (2017) Valerian and City of Thousand Planets (2017) Rk. (b) After SFT (Checkpoint-1500) Raw Output ------------------------------------------------------------------------------------- 1 2 3 4 5 6 7 8 9 10 Tron: Legacy (2010) 11 Jupiter Ascending (2015) 12 Guardians of the Galaxy (2014) 13 Rogue One: Star Wars Story (2016) 14 The Final Cut (2004) 15 The Green Inferno (2013) 16 The Thirteenth Floor (1999) 17 War for the Planet of the Apes (2017) 18 The Lord of the Rings (1978) ... Tron: Jupiter Ascending (2015) Guardians of the Galaxy (2014) Rogue One: The Final Cut (2004) The Green Inferno (2013) The Thirteenth Floor (1999) War for the Planet of the Apes (2017) The Lord of the Rings (1978) Star Wars Story (2016) Battle Angel (2019) Legacy (2010) (NDCG@20=0.129, 0.471 ) Processed Rec. Battle Angel (2019) Gravity (2013) Blade Runner 2049 (2017) The Matrix (1999) Alita: Ready Player One (2018) Tron: Gravity (2013) Blade Runner 2049 (2017) The Matrix (1999) Alita: Battle Angel (2019) Ready Player One (2018) Tron: Legacy (2010) Valerian and City of Thousand Planets (2017) Valerian and City of Thousand Planets (2017) Rogue One: Star Wars Story (2016) Inception (2010) Rk. (c) After RL (Checkpoint-15800) Raw Output ------------------------------------------------------------------------------------- 1 2 3 4 5 6 7 8 9 10 Guardians of the Galaxy (2014) 11 Interstellar (2014) 12 Jaws (1975) 13 Pacific Rim (2013) 14 War for the Planet of the Apes (2017) 15 Nightmare on Elm Street 3 (1987) 16 Blade Runner (1982) 17 Life of Pi (2012) 18 Edge of Tomorrow (2014) ... Rogue One: Inception (2010) Guardians of the Galaxy (2014) Interstellar (2014) Jaws (1975) Pacific Rim (2013) War for the Planet of the Apes (2017) Nightmare on Elm Street 3 (1987) Blade Runner (1982) Life of Pi (2012) Edge of Tomorrow (2014) Star Wars Story (2016) Legacy (2010) 29 Published as conference paper at ICLR 2026 Comparison between Zero-Shot and SFT/RL stage of ConvRec-R1 with Llama-3.2-3B : (i) Conversation xSFT (cid:160) User: Best 3d movies to watch?. What are the best 3d movies to watch on my vr headset (except Avatar and mcu movies)? want the 3d effects to actually be good and not some cheap garbage 3d processing some studios apply to their movies. immersive and cool. Movies have already seen in 3d are: Avatar, avengers, godzilla, star wars the force awakens, pixels, terminator genesys and jurassic world. also want the 3d to actually be : Terminator 2: Judgment Day (1991), Hugo (2011), The Walk (2015), Titanic (ii) Groundtruth ygt (1997), Ready Player One (2018), Predator (1987), Jurassic Park (1993), Mad Max: Fury Road (2015), Kong: Skull Island (2017), Gravity (2013), Life of Pi (2012), Blade Runner 2049 (2017), Pacific Rim (2013) (iii) Outputs from Llama-3.2-3B: Fury Road (2015) Processed Rec. King of the Monsters (2019) Into the Spider-Verse (2018) (nDCG@20=0.385, 0.238 ) Processed Rec. (NDCG@20=0.147) Interstellar (2014) The Martian (2015) Mad Max: Fury Road (2015) The Jungle Book (2016) The BFG (2016) Dunkirk (2017) Wonder Woman (2017) Justice League (2017) Ready Player One (2018) Mission: Impossible - Fallout (2018) Spider-Man: Alita: Battle Angel (2019) Captain Marvel (2019) Godzilla: Rk. (a) Zero-Shot Raw Output ------------------------------------------------------------------------------------- 1 Interstellar (2014) 2 The Martian (2015) 3 Mad Max: Fury Road (2015) 4 The Jungle Book (2016) 5 Fantastic Beasts and Where to Find Them (2016) Fantastic Beasts and Where to Find Them (2016) 6 The BFG (2016) 7 Dunkirk (2017) 8 Wonder Woman (2017) 9 Justice League (2017) 10 Ready Player One (2018) 11 Mission: Impossible - Fallout (2018) 12 Spider-Man: Into the Spider-Verse (2018) 13 Alita: Battle Angel (2019) 14 Captain Marvel (2019) 15 Godzilla: King of the Monsters (2019) ... Rk. (b) After SFT (Checkpoint-1500) Raw Output ------------------------------------------------------------------------------------- 1 Blade Runner 2049 (2017) 2 Mad Max: Fury Road (2015) 3 Inception (2010) 4 The Revenant (2015) 5 Interstellar (2014) 6 Alita: Battle Angel (2019) 7 Dunkirk (2017) 8 Gravity (2013) 9 The Fall (2006) 10 The Adventures of Milo and Otis (1986) 11 Tron: Legacy (2010) 12 Ready Player One (2018) 13 The Jungle Book (2016) 14 Shang-Chi andLegend of the Ten Rings (2021) 15 War for the Planet of the Apes (2017) ... Rk. (c) After RL (Checkpoint-15800) Raw Output ------------------------------------------------------------------------------------- 1 Life of Pi (2012) 2 Alita: Battle Angel (2019) 3 Ready Player One (2018) 4 The Matrix (1999) 5 Blade Runner 2049 (2017) 6 Gravity (2013) 7 Interstellar (2014) 8 Tron: Legacy (2010) 9 Mad Max: Fury Road (2015) 10 Dunkirk (2017) 11 Valerian and City of Thousand Planets (2017) Valerian and City of Thousand Planets (2017) 12 Inception (2010) 13 The Fall (2006) 14 Pans Labyrinth (2006) 15 Blade Runner (1982) ... Blade Runner 2049 (2017) Mad Max: Inception (2010) The Revenant (2015) Interstellar (2014) Alita: Dunkirk (2017) Gravity (2013) The Fall (2006) The Adventures of Milo and Otis (1986) Tron: Ready Player One (2018) The Jungle Book (2016) Shang-Chi and Legend of the Ten Rings (2021) War for the Planet of the Apes (2017) Life of Pi (2012) Alita: Ready Player One (2018) The Matrix (1999) Blade Runner 2049 (2017) Gravity (2013) Interstellar (2014) Tron: Mad Max: Dunkirk (2017) Inception (2010) The Fall (2006) Pans Labyrinth (2006) Blade Runner (1982) (NDCG@20=0.475, 0.237 ) Battle Angel (2019) Battle Angel (2019) Processed Rec. Fury Road (2015) Legacy (2010) Legacy (2010)"
        }
    ],
    "affiliations": [
        "Cornell University",
        "Netflix Research",
        "University of Virginia"
    ]
}