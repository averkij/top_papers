{
    "paper_title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "authors": [
        "Hanyang Wang",
        "Lu Wang",
        "Chaoyun Zhang",
        "Tianjun Mao",
        "Si Qin",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 3 3 2 2 . 5 0 5 2 : r TEXT2GRAD: REINFORCEMENT LEARNING FROM NATURAL LANGUAGE FEEDBACK Hanyang Wang1,, Lu Wang2, Chaoyun Zhang2, Tianjun Mao3, Si Qin2, Qingwei Lin2, Saravan Rajmohan2, Dongmei Zhang 1University of Chicago 2Microsoft 3Fudan University Work done during an internship at Microsoft Correspondence to: Lu Wang, wlu@microsoft.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce TEXT2GRAD, reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, TEXT2GRAD aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the models policy. This yields precise, feedback-conditioned adjustments instead of global nudges. TEXT2GRAD is realized through three components: (1) high-quality feedbackannotation pipeline that pairs critiques with token spans; (2) fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, TEXT2GRAD consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad."
        },
        {
            "title": "Introduction",
            "content": "Free form natural language feedback is abundant in real world applications [1]. Users leave suggestions in reviews, developers comment on code pull requests, and customers critique responses from virtual assistants. Unlike scalar ratings or preference scores, this form of feedback is inherently rich and expressive. It not only identifies which parts of an output are correct or problematic but also explains why, providing detailed signals that can guide model improvement. Despite its ubiquity and usefulness, most existing learning paradigms fail to fully utilize such feedback. Reinforcement learning from human feedback (RLHF) has become dominant method for aligning large language models (LLMs) with human preferences [2, 3, 4, 5, 6]. RLHF typically Figure 1: Comparison of PPO and TEXT2GRAD converts feedback into scalar values derived from preference comparisons, and updates the model using policy optimization algorithms such as Proximal Policy Optimization (PPO) [7] and Direct Preference Optimization (DPO) [5]. While these methods have led to impressive improvements in model helpfulness and safety, they also introduce limitations. By reducing rich, contextual critiques to single scalar reward, RLHF discards token-level information about what was right or wrong, and where. This makes credit assignment imprecise, slows convergence, and limits interpretability [8, 9, 10]. An alternative line of research maintains feedback in its natural language form. Methods such as ReAct [11] and Reflexion [12] prompt the model to reflect on its outputs [13], generate critiques, and use them to self-correct in subsequent steps [14]. These approaches are inspired by how humans operate in open-ended tasks through reasoning, explanation, and dialogue, rather than numeric reward [15, 16, 17]. Natural language feedback in this context improves transparency and sometimes leads to better task performance. However, these methods do not update the model parameters, meaning the model does not internalize the feedback it receives. As result, the same mistakes must be corrected repeatedly, and feedback remains ephemeral [18, 19, 20]. In this paper, we propose TEXT2GRAD, novel framework that transforms free form textual feedback into actionable gradients for policy optimization. As shown in Figure 1, unlike prior work that either compresses feedback into scalar rewards or applies textual critiques only at inference time, TEXT2GRAD brings feedback into the training loop. Given human or programmatic critique, our method aligns feedback clauses with relevant output token spans, converts these alignments into span-level reward signals, and computes natural language gradient. This gradient is then used to perform policy updates that precisely adjust the parts of the model responsible for the error. The result is more targeted, efficient, and interpretable learning. TEXT2GRAD is built on full pipeline for learning from text. First, we construct high quality annotation pipeline that uses GPT-4o to label model outputs with both scalar scores and span-level critiques, following recent work in automated feedback generation [21, 22]. Second, we train dual-headed reward model inspired by generative reward modeling [23] that jointly predicts span-level reward distributions and generates corresponding textual rationales. Third, we apply span-level policy optimization using variant of Proximal Policy Optimization that integrates these fine-grained reward signals, drawing on advances in token-aware credit assignment [24] and text-based gradients [25]. We evaluate TEXT2GRAD on summarization [26], code generation [27], and open-domain question answering [28]. In all tasks, our method outperforms scalar-reward RLHF and prompt-based reflection baselines, showing stronger alignment with human preferences, higher sample efficiency, and more interpretable learning dynamics. These results suggest that natural language feedback can be more than an interpretability toolit can be converted into principled gradients to train more capable and aligned models. Overall, this paper makes the following contributions: We introduce the problem of learning from natural language feedback via gradient-based optimization and present TEXT2GRAD as the first complete framework to address it. We develop scalable annotation pipeline and dual-headed reward model that together produce span-level rewards and explanatory critiques, enabling interpretable fine-grained supervision. We demonstrate that TEXT2GRAD outperforms strong scalar-reward and prompt-based baselines across summarization, code generation, and question answering benchmarks. TEXT2GRAD demonstrates that natural language feedback, when properly aligned and grounded, can serve as direct training signal rather than just auxiliary guidance, opening new path for building language models that learn from human-like supervision."
        },
        {
            "title": "2 Related Work",
            "content": "RLHF with scalar rewards Reinforcement learning from human feedback replaces supervised labels with reward model trained on pairwise human preferences [29, 3]. The reward is single scalar, and policy optimization methods such as Proximal Policy Optimization and Direct Preference Optimization update the language model toward higher scores [7, 5]. This recipe has advanced instruction following, safety, and summarization; 1.3B InstructGPT model aligned in this way outperformed 175B GPT 3 on adherence and toxicity [3, 4, 2]. Subsequent work studies reward hacking and data noise [30, 31, 32]. Despite those successes, scalar rewards collapse multidimensional critiques into one number, obscure where an error occurs, and demand careful regularization such as Kullback Leibler penalties to remain stable [10, 9]. Natural language feedback at inference time complementary line of research keeps feedback in natural language but applies it only while the model is running. ReAct interleaves chain of thought reasoning with tool use to refine 2 answers in question answering and text games [11]. Reflexion stores self generated critiques between attempts and improves coding and decision tasks [12]. Language Feedback Training incorporates human written refinements during supervised fine tuning [3]. Surveys categorize the many emerging feedback formats [18, 22]. These methods lift interpretability and sometimes quality, yet the model weights stay frozen, so lessons are not retained and error corrections must be rediscovered each time [19, 20]. TEXT2GRAD draws inspiration from both threads yet differs in crucial way, by training reward model that generates interpretable textual critiques, uniquely leveraging natural language gradients in token-level PPO to drive fast, interpretable policy improvements."
        },
        {
            "title": "3 Method",
            "content": "This section details TEXT2GRAD, novel framework for Reinforcement Learning from Natural Language Feedback. We begin by motivating and defining the concept of the natural language gradient, then outline the system architecture. Next, we address two core challenges, constructing reward model that delivers actionable feedback, and translating this feedback into precise policy updates, to demonstrate how natural language feedback can be directly integrated into gradient. 3.1 Natural Language Gradient: Definition and Motivation (cid:2)R(y)(cid:3), where R(y) is Traditional policy gradient methods optimize an expected scalar return J(θ) = Eyπθ(x) sequencelevel reward. Such scalar signals obscure which tokens contribute to success or failure, limiting diagnostic insight. To address this, we introduce the Natural Language Gradient (NL-Gradient), which transforms textual critiques into token-level gradient signals. Definition 1 (Natural Language Gradient) Given generated sequence = (y1, . . . , yT ) and its textual critique c, let {δt}T t=1 denote pseudo-rewards derived from aligning critique content to tokens. The NL-Gradient is defined by NL(c y) = (cid:88) t=1 δt θ log πθ(yt x, y<t). Here, δt encodes the critiques local intensity on token yt, enabling: (1) Fine-Grained Guidance: Pseudo-rewards δt highlight specific tokens needing improvement. (2) Interpretability: Each update step is grounded in human-readable feedback. (3) Transferability: The model learns mapping from text to gradient signals, facilitating generalization across tasks. 3.2 Overview of TEXT2GRAD The central objective of TEXT2GRAD is to construct Natural Language Gradient that directly drives policy updates. This requires solving two key challenges: (1) converting free-form textual critiqueswhich humans can easily highlightto fine-grained numerical feedback, and (2) leveraging these numerical signals to compute token-level advantages and update the policy. To address these challenges, as shown in Figure 2, TEXT2GRAD comprises three steps: Dual-Feedback Reward Annotation, which uses GPT-4o to produce high-quality paired critiques and scores; Reward Model Training, which learns to map query-response into text critiques and calibrated numerical rewards; and NL-Gradient Policy Optimization, which leverages per-token advantages and applies NL-Gradient PPO updates. Together, these phases realize end-to-end Natural Language Gradient descent for LLM. 3.3 Reward Labeling Natural language gradient optimization requires dense, interpretable feedback that can be mapped to token-level learning signals. To support this, we develop general reward annotation framework that produces both free-form critiques and structured span-level numerical feedback. This dual-feedback design is applicable across diverse tasks, and supports the construction of token-level pseudo-rewards required for fine-grained policy optimization. Dual-Feedback Annotation Given prompt and generated response = (y1, . . . , yT ), we aim to annotate each sample with natural language critique c, describing strengths or weaknesses of the response in free text, and structured span-level reward map A(y), where each span is assigned label from {positive, neutral, negative}. 3 Figure 2: An overview of TEXT2GRAD. In practice, we prompt strong LLM (e.g., GPT-4o) to output both feedback modalities. For example, in summarization task, the model may generate textual critique such as: The summary omits key information about the characters concern that the manuscript may be rejected. followed by structured JSON object assigning sentiment values to spans in the summary: { } \"Good spans\": [\"a first time author\"], \"Poor spans\": [\"was hopeful about the submission\"] Span-level feedback is particularly useful for aligning language-level reasoning with output structure. We focus annotation efforts on non-neutral spans, as these provide the most useful learning signal while reducing labeling overhead. Reasoning-Augmented Annotation For datasets lacking human-written textual feedback, we employ multi-step prompting strategy to elicit structured feedback from the LLM. Given response y, the model is guided to: (1) Reason about the response quality step-by-step; (2) Output critique based on this reasoning; (3) Produce span-level JSON map A(y) associating specific subphrases with sentiment labels. Formally, the reward labeler outputs: RLLM(x, y) = (c, A(y)) , where A(y) : sk (cid:55) ℓk maps span sk to label ℓk {positive, neutral, negative}. This dual annotation process allows the model to derive semantically grounded, structured supervision even in the absence of explicit references. The full prompt used for reasoningaugmented annotation is provided in Appendix C. Token-Level Reward Mapping Although feedback is annotated at the span level, policy optimization requires tokenlevel rewards. We convert each labeled span sk into token-aligned supervision by assigning uniform pseudo-reward δt {1, 0, +1} to each token: δt = +1, 1, 0, if sk and A(y)[sk] = positive, if sk and A(y)[sk] = negative, otherwise. To reduce labeling cost while retaining informativeness, we adopt class-prioritized strategy: only positive and negative spans are explicitly labeled, while neutral spans are left unannotated and default to δt = 0. This yields token-level reward vector δ = (δ1, . . . , δT ), which supports token-wise advantage estimation and construction of 4 the NL-Gradient (see Section 3.5). This component enables scalable, interpretable, and task-general supervision from natural language feedback. 3.4 Reward Model Learning To support natural language gradient optimization, we train reward model Rϕ that jointly generates free-form critiques and structured token-level feedback in unified autoregressive format. Instead of predicting scalar scores, we frame reward modeling as text generation taskproducing both natural language evaluations and span-level labels as output sequences. Model Objective. Given prompt and model response = (y1, . . . , yT ), the reward model outputs sequence = [c; A(y)], where is critique and A(y) is JSON-formatted map labeling spans in as positive, negative, or neutral. We model this as conditional language generation: pϕ(z x, y) = (cid:81)z likelihood with cross-entropy loss: LR(ϕ) = E(x,y,z)DR [log pϕ(z x, y)] . This formulation provides three advantages: (1) flexibility across tasks via textual supervision; (2) fine-grained gradient flow through tokenized outputs; and (3) interpretable feedback combining explanation and token-level reward in one model. Each training instance is serialized as [x; y; z], and the model is fine-tuned using teacher forcing under standard causal LM objective. This unified, text-based approach simplifies the pipeline while enabling both structured and natural language feedback to drive token-level learning in TEXT2GRAD. t=1 pϕ(zt z<t, x, y), and optimize via maximum 3.5 NL-Gradient Policy Optimization Traditional RL methods rely on sequence-level scalar rewards, which obscure token-level credit assignment and limit precision. This is especially problematic in tasks like summarization and code generation, where only specific parts of the output may be incorrect. To address this, TEXT2GRAD uses dense token-level pseudo-rewards {δt} derived from structured textual feedback to enable fine-grained advantage estimation: At = (cid:80)T Vψ(x, y<t), where rtotal,A Given response y, we query the trained reward model Rϕ to generate natural language critique and span-level reward map, which is parsed into token-wise rewards {δt}T t=1. These are used to construct the Natural Language Gradient (NL-Gradient): gNL = (cid:80)T t=1 δt θ log πθ(yt x, y<t), providing localized learning signals aligned with feedback. We then compute token-level advantages using GAE and integrate them into the PPO objective: k=t γktrtotal,A = δk + rKL . LPPO(θ) = Et [min (ρtAt, clip(ρt, 1 ϵ, 1 + ϵ)At)] β (πθ( x, y<t)) , where ρt is the importance ratio, is the entropy bonus, and ϵ is the clipping threshold that stabilizes updates by constraining large policy shifts. By transforming natural language feedback into token-level gradients, TEXT2GRAD enables interpretable, precise, and efficient policy optimization. 3.6 Theoretical Analysis: Discriminative Power of Token-Level Rewards = (cid:80)T Our theoretical analysis shows that token-level rewards derived from textual feedback lead to sharper and more discriminative advantage estimates than end-of-sequence rewards. Under our formulation, the advantage at timestep is k=t(γλ)kt δk, where δk are pseudo-rewards aligned to tokens via natural language critiques. In computed as AA contrast, end-of-sequence rewards yield AB k=t δk, discounting all feedback uniformly. The difference between these settings is given by AA k=t (γλ)ktδk, which amplifies early feedback differences. For typical settings where γλ 0.95, token-level reward at step = 20 is weighted nearly 0.9520 2.8 times more than it would be under end-of-sequence supervisionshowing that natural language-guided token-level feedback is nearly 3 more effective for early credit assignment. This yields more informative gradients and improves the policys ability to localize and correct errors in long-form outputs. The full derivation and comparison are provided in Appendix A. = (γλ)T (cid:80)T = (cid:80)T 1 AB 5 Table 1: Reward model performance comparison. Dataset Positive Token Negative Token Prec. Rec. Prec. Rec. SLF5K SLF5K (w/o CoT) UltraFeedback UltraFeedback (w/o CoT) KodCode KodCode (w/o CoT) 0.58 0.63 0.66 0.61 0.64 0.62 0.63 0. 0.43 0.59 0.68 0.61 0.58 0.53 0.46 0.40 0.84 0.75 0.43 0. 0.22 0.35 0.71 0.78 Win-Rate (W:T:L) Human-anno Acc. 62:9:29 53:9:38 72:7:21 86% 82% 94%"
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate TEXT2GRAD on summarization, code generation, and question answering to test its ability to transform natural language feedback into fine-grained policy updates. Our experiments demonstrate that TEXT2GRAD outperforms scalar-reward baselines such as PPO, with improved sample efficiency, faster convergence, and better accuracy. 4.1 Datasets Overview SLF5K [26]: summarization dataset with 5,000 Reddit posts, human-written summaries, and feedback. We use all 5,000 samples for SFT, reward modeling, and policy training, with 500 held out for evaluation. KodCode [28]: code generation benchmark with 447K questionsolutiontest triplets across 12 domains. We sample 9K GPT-4o completions (with ChatGPT-3.5 negatives) to train the reward model, and use KodCode-Light-RL-10k for policy optimization. UltraFeedback [27]: QA dataset with 64K prompts and 256K completions from 17 models. Following Huang et al. [33], we split the data into 30% SFT, 50% reward modeling, and 20% RL. 4.2 Reward Model Evaluation central component of TEXT2GRAD is the generative reward model, trained to emulate the evaluative reasoning of advanced LLMs (i.e., GPT-4o) by producing structured, token-level feedback. Experimental Setup We fine-tune Llama3.1-8B-Instruct [34] to serve as the reward model across all tasks. The model is trained to output both natural language critique and span-level reward map, using supervision generated by GPT-4o. To ensure high-quality labels, we use chain-of-thought (CoT) prompting strategy [15, 35] in which GPT-4o first reasons through the correctness of model response, then articulates strengths and weaknesses, and finally highlights token spans as positive or negative. This structured annotation improves feedback precision and interpretability, enabling richer training signals than scalar-only supervision. Main Results Table 1 presents the precision and recall for token-level feedback identification, as well as the win rate of span-level reward signals in pairwise comparisons, with and without CoT reasoning, alongside human-annotated alignment accuracy. Across all datasets, the CoT-based reward model consistently outperforms the ablated variant without textual reasoning. On SLF5K, our model achieved 62% win rate over the non-CoT variant and aligned with human annotations at 86% accuracy. Although precision for positive spans slightly decreased (58% vs. 63%), recall improved significantly (63% vs. 46%), indicating better coverage and less overfitting to obvious phrases. Similar trends were observed on UltraFeedback and KodCode, with particularly strong span-level grounding in the code domain (KodCode win rate: 72%). These results confirm that natural language feedbackwhen structured through reasoningenables more accurate and discriminative reward modeling, forming the basis for token-level learning in TEXT2GRAD. The pairwise-comparison prompt is listed in Appendix E. In addition, we evaluate the reliability of our span-level reward predictions by comparing them with human-annotated results across three datasets, using 50 sampled cases from each. Our method achieves an alignment rate of over 82% with human judgments, indicating that the span-level rewards are both reliable and effective for identifying good or bad spans in model outputs, and are well-suited for guiding policy updates. Additional reward model evaluation results are listed in Appendix C. 6 4.3 SLF5K [26]: Summarization We evaluate TEXT2GRAD on the SLF5K dataset [26], which involves generating high-quality summaries of Reddit posts that closely align with human-written references. This task provides natural language feedback and spanlevel annotations, making it well-suited for evaluating the effectiveness of token-level reward modeling. Additional hyperparameters are provided in Appendix D. Table 2: Performance on SLF5K dataset. All models use Llama-3.1-8B Instruct. SFT=Supervised Fine-Tuning, SFT+Reflection adds reward-guided reflection. TEXT2GRAD is our approach. Model ChatGPT-3.5 ChatGPT-4o 8B-SFT 8B-SFT+Reflection 8B-PPO TEXT2GRAD-8B ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore 0.155 0.296 0.285 0.329 0.365 0.400 0.059 0.066 0.078 0.087 0.132 0.155 0.108 0.203 0.195 0.225 0.262 0. 0.020 0.030 0.032 0.041 0.075 0.094 0.844 0.886 0.875 0.888 0.893 0.902 Experimental Setup We use Llama3.1-8B-Instruct [34] as the base policy model. It is first fine-tuned using supervised learning on SLF5K to control output length and content coverage, and subsequently optimized using our NL-Gradient method. We compare TEXT2GRAD against several baselines: (1) PPO [7] trained with scalar rewards, (2) supervised fine-tuning (SFT), and (3) SFT augmented with reward-guided reflection strategies [12, 36]. We also include GPT-3.5 and GPT-4o outputs as reference points. Evaluation metrics include ROUGE [37], BLEU [38], BERTScore [39], and LLM-as-a-Judge [40]. (a) Reward curve for SLF5K dataset. (b) GPT4 Judge Comparison on different Methods for SLF5K dataset. Figure 3: Combined figure for SLF5K dataset analysis. Main Results TEXT2GRAD consistently outperforms all baselines, as shown in Table 2. It achieves the highest scores across all automatic metrics, including substantial +25.3% BLEU improvement over PPO and +3.3 ROUGE-L gain over the strongest reflection baseline. These results demonstrate the benefits of fine-grained token-level reward modeling enabled by structured natural language feedback. Human preference evaluation using GPT-4-as-a-Judge (Figure 3b) shows that TEXT2GRAD achieves 12% win-rate improvement over PPO. This confirms that its summaries are not only better aligned with references but also more coherent and informative from qualitative perspective. Additionally, the reward curve in Figure 3a shows that TEXT2GRAD converges significantly faster, reaching optimal performance within 75% of training steps compared to PPOs 97%. These highlight the efficiency of token-level feedback in accelerating training and guiding interpretable policy updates. 4.4 KodCode [28]: Code Generation We evaluate TEXT2GRAD on the KodCode dataset [28], which focuses on generating correct and efficient Python solutions across 12 diverse problem domains. This task highlights the importance of span-level feedback in structured text generation where subtle errors can invalidate the entire output. Experimental Setup We use Llama3.1-8B-Instruct [34] as the policy model. To train the reward model, we sample 10,000 promptcompletion pairs from the supervised dataset. GPT-4o solutions serve as high-quality references, while GPT-3.5 completions provide challenging negatives to construct pairwise training examples. Annotations include textual critiques and span-level labels derived. We compare TEXT2GRAD against standard PPO [7] and several strong baselines, including pre-trained models (Llama3.2-Instruct, CodeLlama-34B, and Gemini Pro). Evaluation is conducted using pass@1 accuracy on HumanEval [41], MBPP [42], and their enhanced counterparts (HumanEval+ and MBPP+ [43]), which include additional test cases for robustness evaluation. Table 3: Performance comparison on code benchmarks. Results show pass@1 scores (%). HumanEval+ and MBPP+ include additional test cases beyond the base benchmarks. HumanEval MBPP Base Plus Base Plus Model Pre-Trained Models Llama-3.2-Instruct Llama-3.1-Instruct CodeLlama Gemini Pro Size 3B 8B 34B 48.2 64.0 51.8 63.4 Fine-tuned Models (Llama3.1-8B-Instruct) PPO TEXT2GRAD (w/o CoT) TEXT2GRAD 64.6 63.8 67.7 8B 8B 8B 43.9 58.5 43.9 55. 61.0 57.3 61.6 61.9 66.7 69.3 72.9 68.5 62.2 73.3 51.3 55.0 56.3 57.9 55.8 53.4 61.6 Main Results Table shows that TEXT2GRAD outperforms all fine-tuned and pre-trained baselines across both standard and enhanced Compared to PPO, benchmarks. TEXT2GRAD achieves +5.8 point improvement on MBPP+ and +3.6 point gain on HumanEval+, demonstrating its ability to generalize to challenging and previously unseen test cases. The ablated version of TEXT2GRAD, without natural language feedback, underperforms by 6.9 points on average, highlighting the critical role of structured critiques in token-level optimization. These results confirm that TEXT2GRAD effectively leverages textual feedback to correct fine-grained coding errors, producing semantically Avg. 51.3 61.1 55.3 62.4 62.5 59.2 66.1 robust programs that generalize across both clean and adversarial evaluation settings. 4.5 UltraFeedback [27]: Open-Domain Question Answering To evaluate TEXT2GRAD on general-purpose alignment and reasoning, we test it on the UltraFeedback dataset [27], which comprises diverse prompts spanning multiple domains and difficulty levels. This task assesses how well models trained with natural language feedback generalize to open-ended questions, factual correctness, and multi-turn coherence. Experimental Setup We use Llama3.1-8B-Instruct as the policy backbone. For evaluation, we benchmark against standard PPO and report performance on three widely adopted QA alignment metrics: (1) AlpacaEval 2.0 [44], which measures instruction-following via GPT-4-based pairwise comparisons, (2) ARC-Challenge [45], which tests science and commonsense reasoning, and (3) MT-Bench [46], which evaluates multi-turn conversation quality. base, +1.7 vs. indicating stronger the base model and 2.3-point Table 4: Performance comparison across benchmarks for Llama3-8BInstruct trained with Ultrafeedback. Main Results As shown in Table 4, TEXT2GRAD consistently improves over both the base SFT On AlpacaEval 2.0, TEXT2GRAD achieves 12.1-point gain model and PPO across all metrics. over instruction alignimprovement over PPO, ment and preference satisfaction. On ARC-Challenge, TEXT2GRAD shows improved reasoning (+3.9 PPO), while MT-Bench results highlight better multi-turn dialogue performance. vs. Our ablation study clarifies the significance of structured feedback by examining the impact of excluding CoT reasoning during the annotation phase, where feedback is provided directly as span-level scores without prior natural language explanations. Results show that training without CoT reasoning leads to consistent decline in performance across all metrics, with notable drop on AlpacaEval (-6.1 points). This underscores the critical role of natural language explanations in generating effective token-level supervision. These results reinLlama3-8B Llama3-8B PPO TEXT2GRAD (w/o CoT) TEXT2GRAD AlpacaEval 2.0 ARC-C MTBench 22.6 32.4 28.6 34.7 6.87 7.43 7.49 7.58 80.5 82.7 83.1 84. GPT-4 GPT-3.5 96.4 85.2 30.2 22.7 7.93 6.91 Model by explicit feedback, enhances both alignment and reasoning. 4.6 Case Study force that NL-Gradient optimization, guided Figure 4: case study from the code generation scenario comparing PPO vs. TEXT2GRAD. Figure 4 shows how TEXT2GRAD corrects faulty implementation of match_parens while standard PPO fails. The policy LM first produces buggy patch. scalar reward model gives PPO single negative score (2), leaving the optimiser without guidance on where the error resides. After several updates, it still ignores the two crossconcatenation checks required by the hidden tests. TEXT2GRAD proceeds differently. The natural language reward model highlights the exact faulty span for char in lst[0] ... and explains that the code fails to check lst[0] + lst[1] and lst[1] + lst[0]. This critique is aligned with the offending tokens and converted into negative rewards for that span and positive rewards for the rest. single NLGradient update rewrites only the highlighted lines. The resulting function passes all unit tests. This example underscores the advantages of TEXT2GRAD. Additional qualitative results appear in Appendix F."
        },
        {
            "title": "5 Conclusion",
            "content": "vspace-3mm We presented TEXT2GRAD, new framework for learning from natural language feedback by converting free form textual critiques into span-level reward signals and actionable gradients. Unlike traditional RLHF approaches that rely on scalar rewards or inference-time prompting strategies, TEXT2GRAD directly incorporates feedback into the training process through token-aware policy updates. This enables precise credit assignment and more interpretable learning dynamics. Experimental results across summarization, code generation, and question answering demonstrate that TEXT2GRAD consistently outperforms scalar-reward PPO and prompt-based baselines in both alignment quality and sample efficiency. Overall, TEXT2GRAD opens new direction for fine-grained, feedback-driven optimization of language models, moving beyond scalar supervision toward more human-like, interpretable, and effective learning."
        },
        {
            "title": "References",
            "content": "[1] Chaoyun Zhang, Zicheng Ma, Yuhao Wu, Shilin He, Si Qin, Minghua Ma, Xiaoting Qin, Yu Kang, Yuyi Liang, Xiaoyu Gou, et al. Allhands: Ask me anything on large-scale verbatim feedback via large language models. arXiv preprint arXiv:2403.15157, 2024. [2] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. [3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 9 [5] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [7] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [8] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. [9] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36:5900859033, 2023. [10] Sebastian Raschka. Build Large Language Model (From Scratch). Simon and Schuster, 2024. [11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [12] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [13] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. [14] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025. [15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [16] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [17] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [18] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, et al. Bridging the gap: survey on integrating (human) feedback for natural language generation. Transactions of the Association for Computational Linguistics, 11:16431668, 2023. [19] Alexander Pan, Erik Jones, Meena Jagadeesan, and Jacob Steinhardt. Feedback loops with language models drive in-context reward hacking. arXiv preprint arXiv:2402.06627, 2024. [20] Archit Sharma, Sedrick Scott Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. critical evaluation of ai feedback for aligning large language models. Advances in Neural Information Processing Systems, 37:2916629190, 2024. [21] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [22] Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196, 2024. [23] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [24] Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. Improving large language models via fine-grained reinforcement learning with minimum editing constraint. arXiv preprint arXiv:2401.06081, 2024. [25] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496, 2024. [26] Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. [27] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. 2023. [28] Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. [29] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [30] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. [31] Nathan Lambert. Reinforcement learning from human feedback. arXiv preprint arXiv:2504.12501, 2025. [32] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [33] Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Self-evolved reward learning for llms. arXiv preprint arXiv:2411.00418, 2024. [34] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [35] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. In Findings of the Association for Computational Linguistics ACL 2024, pages 16381662, 2024. [36] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [37] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [39] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [40] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [41] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [42] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [43] Zhaojian Yu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. Humaneval pro and mbpp pro: Evaluating large language models on self-invoking code generation. arXiv preprint arXiv:2412.21199, 2024. [44] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [45] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [46] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 11 Discriminative Power of Token-Level Rewards key design choice in our method is to provide dense, token-level feedback rather than sparse, end-of-sequence rewards. Intuitively, localized reward signals allow the policy to attribute credit or blame more precisely to specific parts of the output. In this section, we formalize this intuition and show how token-level rewards lead to sharper and more discriminative advantage estimates, thereby improving policy learning. Background. In reinforcement learning, policy updates are guided by the advantage function, which measures how much better (or worse) an action is compared to the policys expected value. Using Generalized Advantage Estimation (GAE), the advantage at timestep is computed from the temporal-difference (TD) errors: At = (cid:88) l=0 (γλ)l δt+l, where δt = rt + γV (st+1) (st), and is the value function, γ is the discount factor, and λ is the GAE parameter. Comparing Token-Level vs. End-of-Sequence Reward Settings. We define two settings for reward assignment: Setting A: Token-Level Rewards. Each token may receive its own feedback: rtoken,A Total reward: rtotal,A = 0 for many [1, ] = rtoken,A t + rKL Setting B: End-of-Sequence Reward. Only the final token is rewarded: rtoken,B rtoken,B = 0 for all < = 0; total reward: rtotal,B = rtoken,B + rKL Let τ1 and τ2 be two trajectories, where τ1 is qualitatively better than τ2. Define rt = rtoken,A (τ1) rtoken,A assume all KL terms and value functions are held constant for simplicity (the general case follows similarly). (τ2), and Advantage Difference Across Trajectories. The advantage difference under each setting is: AA = (cid:88) k=t (γλ)kt rk, AB = (γλ)T (cid:88) k=t rk. Even if (cid:80)T rk > 0 for < , because: k=t rk is the same in both cases (i.e., the same total reward difference), AA > AB whenever any (γλ)kt > (γλ)T t, for all < T. This means the earlier the reward difference occurs in the sequence, the more strongly it is emphasized in Setting relative to Setting B. Amplification of Early Signal. To quantify this difference, define the amplification factor: α(k, ) = (γλ)kt (γλ)T = (γλ)(T k). For typical value of γλ = 0.95 and gap of = 20 steps (i.e., the difference occurs 20 tokens before the final token), we have: α(k, ) 0.9520 2.8, meaning that in Setting A, the advantage function weights early reward differences nearly 3 more than in Setting B. This analysis confirms that token-level feedback improves the discriminative power of the advantage signal: even if the total reward difference is the same, Setting assigns more importance to earlier deviations in quality. This sharper signal allows the policy to learn localized correctionse.g., improving grammar or factual consistency in specific parts of summaryrather than attributing success or failure to the entire sequence. As result, our method enables faster convergence and better fine-tuning, especially on open-ended tasks where quality varies across tokens. GPT-4o CoT thinking Annotation Prompt B.1 SLF5K Listing 1: SLF5K GPT-4o Annotation Prompt 1 Please critique the following summary of post and provide feedback in the specified JSON format : 2 3 --- 4 5 ** Original Post :** 6 { post } 7 8 ** Generated Summary :** 9 { generated_summary } 10 11 --- 12 13 ** Definitions :** 14 - ** good_spans **: 0 -2 phrases from the summary that greatly improve its quality by accurately and concisely capturing the original post core meaning or key details , as explained in textual_feedback . Empty if none apply . 15 - ** poor_spans **: 0 -2 phrases from the summary that noticeably harm its quality due to inaccuracy , redundancy , poor wording , or being less important and replaceable with more critical content , as explained in textual_feedback . Empty if none apply . 16 17 --- 18 19 ** Instructions :** 20 1. Identify the summary most essential strengths that reflect the original post accurately and its most critical weaknesses that misrepresent or confuse it . 21 2. Select 0 -2 of the most significant phrases for good_spans and poor_spans , keeping them concise and impactful , with brief justifications . Include none if no phrases stand out . 22 3. Ensure good_spans and poor_spans are directly supported by the analysis in textual_feedback . 23 24 --- 25 26 ** Chain of Thought :** 27 First , carefully analyze both the original post and the generated summary : 28 1. What are the key points of the original post ? 29 2. Which of these key points are accurately captured in the summary ? 30 3. What important information is missing from the summary ? 31 4. Are there any inaccuracies or mis repre se nt ations in the summary ? 32 5. Which specific phrases in the summary represent its strongest elements ? 33 6. Which specific phrases in the summary represent its weakest elements ? 34 35 Based on this analysis , formulate your textual feedback and identify the good and poor spans . 36 37 --- 38 39 ** Output Format :** 40 Provide concise , one - paragraph critique and the GOOD / POOR spans in this JSON structure : 41 json 42 { 43 \" textual_feedback \": \" Your critique here summarizing key strengths 44 and weaknesses in one paragraph .\" , \" good_spans \": [\" phrase1 \" , \" phrase2 \"] , // 0 -2 concise phrases from the generated summary , tied to textual_feedback , or [] if none \" poor_spans \": [\" phrase1 \" , \" phrase2 \"] // 0 -2 concise phrases from the generated summary , tied to textual_feedback , or [] if none 46 } 47 48 49 Focus on precision : include only the most impactful phrases of the generated summary , avoiding excessive or minor details . B.2 UltraFeedback 1 < CritiquePrompt > 2 Listing 2: UltraFeedback GPT-4o Annotation Prompt 4 5 6 7 8 10 11 12 13 14 16 17 18 19 < Instructions > Critique response to user input and provide feedback in JSON format : </ Instructions > < EvaluationCriteria > < Criterion name =\" Accuracy \" > Does it correctly address the input ? </ Criterion > < Criterion name =\" Relevance \" > Does it stay on topic ? </ Criterion > < Criterion name =\" Clarity \" > Is it easy to understand ? </ Criterion > < Criterion name =\" Completeness \" > Does it cover the input core needs ? </ Criterion > </ EvaluationCriteria > < SpanGuidelines > < GoodSpans > < Description > Phrases from the response that best capture its strengths ( . . , accurate , relevant , clear ) . Select only the most essential and impactful phrases , directly tied to textual_feedback . </ Description > </ GoodSpans > < PoorSpans > < Description > Phrases from the response that best highlight its weaknesses ( . . , inaccurate , irrelevant , vague ) . Select only the most essential and impactful phrases , directly tied to textual_feedback . </ Description > 14 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 </ PoorSpans > < Requirement > Spans must be exact quotes from the response . </ Requirement > </ SpanGuidelines > < ReflectionProcess > < Step > First , carefully analyze the user input to understand the core question or request . </ Step > < Step > Next , examine the generated response against each evaluation criterion . </ Step > < Step > For each criterion , identify specific strengths and weaknesses with supporting evidence from the response . </ Step > < Step > Consider how well the response addresses the user explicit and implicit needs . </ Step > < Step > Finally , synthesize your analysis into coherent critique that highlights the most important points . </ Step > </ ReflectionProcess > < Separator > - - - </ Separator > < UserInput >{ entry [ prompt ]} </ UserInput > < GeneratedResponse >{ entry [ response ]} </ GeneratedResponse > < Separator > - - - </ Separator > < OutputFormat > < Description > Provide the critique in the following JSON structure : </ Description > < JSONExample > {{ \" textual_feedback \": \" One - paragraph critique summarizing strengths and weaknesses , tied to spans .\" , \" good_spans \": [\" phrase1 \" , \" phrase2 \" , ...] , // Impactful phrases from < GeneratedResponse > , or [] if none \" poor_spans \": [\" phrase1 \" , \" phrase2 \" , ...] // Impactful phrases from < GeneratedResponse > , or [] if none }} </ JSONExample > </ OutputFormat > 54 55 </ CritiquePrompt > 15 B.3 KodCode Listing 3: KodCode GPT-4o Annotation Prompt 1 Analyze the following code solution for the given problem : 2 3 Problem Description : 4 5 { problem } 6 7 8 Submitted Code : 9 10 { solution } 11 12 13 Test Results : 14 Passed : { passed } 15 16 {% - if not passed -%} 17 Test Question : 18 { test_question } 19 20 Error Output : 21 { stdout } 22 {% - endif -%} 23 24 Please analyze the code and identify the following in JSON format : 25 26 1. Identify any error - causing code segments directly from the submitted solution . 27 2. Provide detailed feedback on the code functionality , issues , 28 29 30 and improvement suggestions . - First , understand what the code is trying to accomplish - Analyze the algorithm and approach used - Identify any logical errors or inefficiencies - Consider edge cases and potential improvements 31 32 3. Point out any code segments from the solution that work but could be improved . 33 34 Return your analysis in this JSON structure : 35 json 36 { 37 \" Code Feedback \": \" Provide detailed explanation of the code functionality , any potential issues , and suggestions for improvement . Use markdown formatting for better readability .\" , 38 39 \" wrong_code \": [\" Extract ONLY the problematic code segments FROM THE SUBMITTED SOLUTION that cause failures . Must be exact quotes . Leave empty [] if none found .\"] , \" improvement_code \": [\" Extract ONLY the working but improvable code segments FROM THE SUBMITTED SOLUTION . Must be exact quotes . Leave empty [] if none needed .\"] 40 } 41 42 Note : For wrong_code and improvement_code , only include direct quotes from the submitted code above , not suggested fixes .\"\"\""
        },
        {
            "title": "C Additional Reward Model Performance Results",
            "content": "Table 5: Span prediction quality on UltraFeedback dataset. GT/Pred shows the count of ground truth and predicted spans, with E-P indicating exact matches and partial matches. OUI (Overlap Unit Index) measures boundary precision. Dataset GOOD Spans POOR Spans UltraFeedback 1150/736 (292-237) 812/400 (101-104) OUI 0.40 0.27 Table 6: Code suggestion quality on KodCode dataset. Exact Match shows the proportion of suggestions that perfectly match ground truth. RougeL measures lexical overlap between predicted and reference code segments. Code Type Exact Match RougeL Improvement Wrong 0.47 0.55 0.64 0.59 Table 7: Additional metrics for all models on the SLF5K dataset, including perplexity and BERTScore components. Human perplexity: 37.375. Model Perplexity BERTScore ChatGPT-3.5 ChatGPT-4o 8B 8B-SFT 8B-SFT+Reflection 8B-PPO TEXT2GRAD-8B 27.288 53.242 19.248 44.103 34.823 28.472 25. Precision Recall 0.806 0.879 0.848 0.865 0.880 0.892 0.903 0.884 0.894 0.894 0.885 0.897 0.895 0. 17 Hyperparamesters of NL-Gradient PPO Optimization We report the full training hyperparameters used for NL-Gradient PPO optimization across three datasets: SLF5K, UltraFeedback, and KodCode. All experiments use the Llama 3 or Llama 3.1 8B Instruct variant as both the base policy and reward models. For the SLF5K dataset  (Table 8)  , we adopt linear learning rate scheduler with base learning rate of 1 106. The model is trained using batch size of 12 and gradient accumulation over 12 steps. PPO is run for 4 epochs per batch with mini-batch size of 1. The KL penalty is enforced with full KL term, an initial coefficient of 0.2, and target KL divergence of 6. Adaptive KL control is enabled, and training is conducted over 4 full epochs. For the UltraFeedback dataset  (Table 9)  , we use cosine learning rate scheduler with the same learning rate of 1 106. The training is conducted with batch size of 8, gradient accumulation over 8 steps, and 4 PPO epochs per batch. The KL penalty remains full, but with lower initial KL coefficient of 0.05 and tighter target KL of 3. The training is distributed across 8 GPUs using DeepSpeed ZeRO-3 parallelism for efficiency and scalability. For the KodCode dataset  (Table 10)  , we apply more conservative learning rate of 5 107 with cosine scheduler. The batch and gradient accumulation sizes match UltraFeedback, but the target KL is stricter at 1, with an initial KL coefficient of 0.07. The model is again trained with 4 PPO epochs per batch and full KL penalty. Training is conducted on 8 GPUs using DeepSpeed ZeRO-1 parallelism. These configurations were selected to balance stability and efficiency, while ensuring sufficient capacity for fine-grained span-level optimization guided by natural language feedback. Table 8: Hyperparameters for NL-Gradient PPO Training on SLF5K Hyperparameter Value Base Policy Model Base Reward Model Learning Rate LR Scheduler Batch Size Mini-batch Size PPO Epochs Gradient Accumulation Steps Initial KL Coefficient Target KL KL Penalty Adaptive KL Control Training Epochs Llama 3.1 8B Instruct Llama 3.1 8B Instruct 1 106 Linear 12 1 4 12 0.2 6 Full True 4 18 Table 9: Hyperparameters for NL-Gradient PPO Training on UltraFeedback Hyperparameter Value Base Policy Model Base Reward Model Learning Rate LR Scheduler Batch Size Mini-batch Size PPO Epochs Gradient Accumulation Steps Initial KL Coefficient Target KL KL Penalty Adaptive KL Control Training Epochs Dataset Number of GPUs Parallelism Llama 3 8B Instruct Llama 3.1 8B Instruct 1 106 Cosine 8 1 4 8 0.05 3 Full True 4 Ultrafeedback 8 DeepSpeed ZeRO-3 Table 10: Hyperparameters for NL-Gradient PPO Training on KodCode Hyperparameter Value Base Policy Model Base Reward Model Learning Rate LR Scheduler Batch Size Mini-batch Size PPO Epochs Gradient Accumulation Steps Initial KL Coefficient Target KL KL Penalty Adaptive KL Control Training Epochs Dataset Number of GPUs Parallelism Llama 3.1 8B Instruct Llama 3.1 8B Instruct 5 107 Cosine 8 1 4 8 0.07 1 Full True 4 KodCode 8 DeepSpeed ZeRO-1 19 GPT-4o Judge CoT Influence Annotation Prompt E.1 SLF5K Evaluation Prompt The following prompt template was used to evaluate model responses on the SLF5K dataset. To prevent position bias in the evaluation, the order of model responses (analysis_1 and analysis_2) was randomly shuffled for each comparison: 1 Compare and evaluate two different summaries of the same query . You must respond in valid JSON format . Listing 4: SLF5K Evaluation Prompt 2 3 Original Query : 4 { query } 5 6 { analysis_1_label }: 7 { response_1 } 8 9 { analysis_2_label }: 10 { response_2 } 11 12 Evaluation Criteria : 13 1. Accuracy (0 -10) : 14 - Does it capture the main points correctly ? - Is it faithful to the original content ? - Are there any factual errors ? 15 16 20 21 25 30 31 17 18 2. Completeness (0 -10) : 19 - Are all key points included ? - Is any important information missing ? - Does it cover the core message ? 22 23 3. Conciseness (0 -10) : 24 - Is it clear and to the point ? - Does it avoid unnecessary details ? - Is the language efficient ? 27 28 4. Coherence (0 -10) : 29 - Is the summary well - organized ? - Does it flow logically ? - Is it easy to understand ? 32 33 Compare both summaries and evaluate them . Respond ONLY with JSON object in this exact format : 34 { 35 \"{ score_key_1 }\": { 36 37 38 39 40 42 43 44 \" strengths \": [\" specific strength 1\" , \" specific strength 2\" , ...] , \" weaknesses \": [\" specific weakness 1\" , \" specific weakness 2\" , ...] \" score \": < overall score between 0 -10 > , \" accuracy \": < score between 0 -10 > , \" completeness \": < score between 0 -10 > , \" conciseness \": < score between 0 -10 > , \" coherence \": < score between 0 -10 > , } , \"{ score_key_2 }\": { 20 \" strengths \": [\" specific strength 1\" , \" specific strength 2\" , ...] , \" weaknesses \": [\" specific weakness 1\" , \" specific weakness 2\" , ...] \" score \": < overall score between 0 -10 > , \" accuracy \": < score between 0 -10 > , \" completeness \": < score between 0 -10 > , \" conciseness \": < score between 0 -10 > , \" coherence \": < score between 0 -10 > , 45 46 48 49 50 51 } 52 53 } E.2 KodCode Evaluation Prompt The following prompt template was used to evaluate the quality of code span selections for the KodCode dataset, which resulted in the win-rate metrics (72.17 : 7.01 : 20.82) comparing Chain-of-Thought (CoT) feedback quality: 1 Evaluate the precision and specificity of code span selections in two different analyses . Listing 5: KodCode Evaluation Prompt 2 3 Problem : 4 { problem } 5 6 Solution Code : 7 { solution } 8 9 { analysis_1_label }: 10 Selected spans : { spans_1 } 11 Suggestions : { improve_1 } 12 13 { analysis_2_label }: 14 Selected spans : { spans_2 } 15 Suggestions : { improve_2 } 16 17 Please evaluate the quality of span selections in JSON format , focusing on precision and minimality : 18 { 19 20 21 22 24 25 26 27 \"{ score_key_1 }\": { \" score \": (0 -10 score for span selection precision ) , \" Reason \": \" Explain the reason for the score \" } , \"{ score_key_2 }\": { \" score \": (0 -10 score for span selection precision ) , \" Reason \": \" Explain the reason for the score \" } , \" comparison \": \" Explain which analysis has more precise and minimal span selections \" 28 } 29 30 Guidelines for span evaluation : 31 1. Each span should capture ONLY the specific problematic code , nothing more 21 32 2. General or overly broad selections ( like entire functions ) are penalized 33 3. Spans should not include irrelevant surrounding code 34 4. Multiple small precise spans are better than one large span 35 5. Spans must directly relate to the identified issue E.3 UltraFeedback Evaluation Prompt The following prompt template was used to evaluate the precision and specificity of text span selections for the UltraFeedback dataset: 1 \"\"\" Evaluate the precision and specificity of text span selections in two different analyses . Listing 6: UltraFeedback Evaluation Prompt 2 3 Problem : 4 { problem } 5 6 Response : 7 { solution } 8 9 { analysis_1_label }: 10 Problematic spans : { spans_1 } 11 Good spans : { improve_1 } 12 13 { analysis_2_label }: 14 Problematic spans : { spans_2 } 15 Good spans : { improve_2 } 16 17 Please evaluate the quality of span selections in JSON format , focusing on precision and minimality : 18 { 20 21 22 23 24 26 27 \"{ score_key_1 }\": { \" score \": (0 -10 score for span selection precision ) , \" Reason \": \" Explain the reason for the score \" } , \"{ score_key_2 }\": { \" score \": (0 -10 score for span selection precision ) , \" Reason \": \" Explain the reason for the score \" } , \" comparison \": \" Explain which analysis has more precise span selections while avoiding unnecessary or overly broad spans \" 28 } 29 30 Guidelines for span evaluation : 31 1. Each problematic span should capture ONLY the specific problematic text , nothing more 32 2. General or overly broad selections are penalized 33 3. Spans should not include irrelevant surrounding text 34 4. Multiple small precise spans are better than one large span 35 5. Spans must directly relate to the identified issue 36 6. Good spans should highlight the best parts of the response 3 4 5 6 7 9 10 11 12"
        },
        {
            "title": "F Case Studies on HumanEval",
            "content": "We present three case studies from the HumanEval benchmark to demonstrate the effectiveness of our approach. F.1 Special Factorial"
        },
        {
            "title": "Problem Statement",
            "content": "1 def special_factorial ( ) : 2 \"\"\" The Brazilian factorial is defined as : razilian_factorial ( ) = ! * (n -1) ! * (n -2) ! * ... * 1! where > 0 For example : >>> special_factorial (4) 288 The function will receive an integer as input and should return the special factorial of this integer . \"\"\" Instruct Model Solution 1 def special_factorial ( ) : 2 if < 0: 3 4 6 7 8 9 raise ValueError ( \" Input must be non - negative integer \" ) result = 1 for in range (1 , + 1) : result *= math . factorial ( ) return result PPO Model Solution 1 def special_factorial ( ) : 2 result = 1 for in range (1 , + 1) : result *= return result Our Approach 1 import math 2 def special_factorial ( ) : 3 if not isinstance (n , int ) or <= 0: 4 6 7 8 raise ValueError ( \" Input must be positive integer . \" ) result = 1 for in range (1 , + 1) : result *= math . factorial ( ) 23 1 def prime_fib ( : int ) : 2 \"\"\" prime_fib returns - th number that is Fibonacci number and 9 10 return result F.2 Prime Fibonacci Numbers"
        },
        {
            "title": "Problem Statement",
            "content": "it also prime . >>> prime_fib (1) 2 >>> prime_fib (2) 3 >>> prime_fib (3) 5 >>> prime_fib (4) 13 >>> prime_fib (5) 89 \"\"\" 3 4 5 7 8 9 10 11 13 14 10 11 12 14 15 16 Instruct Model Solution 1 def is_prime ( num : int ) -> bool : 2 if num < 2: 3 4 5 6 return False for in range (2 , int ( num ** 0.5) + 1) : if num % == 0: return False return True 7 8 def prime_fib ( : int ) -> int : 9 , = 0 , 1 count = 0 while True : , = , + if is_prime ( ) : count += 1 if count == : return PPO Model Solution 1 def prime_fib ( : int ) : , = 0 , 1 2 count = 0 while True : 4 3 6 7 8 , = , + if is_prime ( ) : count += 1 if count == : 9 return b"
        },
        {
            "title": "Our Approach",
            "content": "1 def is_prime ( num : int ) -> bool : 2 if num < 2: 3 4 5 return False for in range (2 , int ( num ** 0.5) + 1) : if num % == 0: return False return True 7 8 def prime_fib ( : int ) -> int : 9 , = 0 , 1 count = 0 while True : , = , + if is_prime ( ) : count += 1 if count == : return 10 11 13 14 15 16 F.3 File Name Validation Problem Statement 1 def file_name_check ( file_name ) : 2 \"\"\" Create function which takes string representing file 3 4 5 7 8 9 10 11 13 14 name , and returns Yes if the the file name is valid , and returns No otherwise . file name is considered to be valid if and only if all the following conditions are met : - There should not be more than three digits ( 0 - 9 ) in the file name . - The file name contains exactly one dot . - The substring before the dot should not be empty , and it starts with letter from the latin alphapet ( -z and -Z ) . - The substring after the dot should be one of these : [ txt , exe , dll ] Examples : file_name_check (\" example . txt \") # = > Yes file_name_check (\"1 example . dll \") # = > No ( the name should start with latin alphapet letter ) \"\"\" Instruct Model Solution 1 def file_name_check ( file_name ) : # First , split the file name by the dot name , extension = file_name . rsplit ( . , 1) 3 25 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 # Check if the extension is valid valid_extensions = [ txt , exe , dll ] if extension not in valid_extensions : return No # Check if the substring before the dot is not empty if not name : return No # Check if the substring before the dot starts with letter from the latin alphabet if not name [0]. isalpha () : return No # Check if there are more than three digits in the file name if sum ( . isdigit () for in file_name ) > 3: return No # If all conditions are met , return Yes return Yes PPO Model Solution 1 def file_name_check ( file_name ) : # Split the file name into two parts : the name and the extension name , extension = file_name . split ( . ) # Check if the name starts with latin alphapet letter if not name [0]. isalpha () : return No # Check if there are more than three digits in the name if sum ( . isdigit () for in name ) > 3: return No # Check if the extension is valid if extension not in [ txt , exe , dll ]: return No # Check if the file name contains exactly one dot if file_name . count ( . ) != 1: return No # If all checks pass , return Yes return Yes Our Approach 1 def file_name_check ( file_name ) : \"\"\" Checks if file name is valid according to the given conditions . 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 3 4 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 file name is considered to be valid if and only if all the following conditions are met : - There should not be more than three digits ( 0 - 9 ) in the file name . - The file name contains exactly one dot . . - The substring before the dot should not be empty , and it starts with letter from the latin alphabet ( -z and -Z ) . - The substring after the dot should be one of these : [ txt , exe , dll ]. Args : file_name ( str ) : The name of the file to check . Returns : str : Yes if the file name is valid , No otherwise . \"\"\" # Check if the file name contains exactly one dot if file_name . count ( . ) != 1: return No # Split the file name into two parts : before and after the dot before_dot , after_dot = file_name . split ( . ) # Check if the substring before the dot is not empty and starts with letter if not before_dot or not before_dot [0]. isalpha () : return No # Check if the substring before the dot contains more than three digits if sum ( . isdigit () for in before_dot ) > 3: return No # Check if the substring after the dot is one of the allowed extensions if after_dot not in [ txt , exe , dll ]: return No # If all conditions are met , return Yes return Yes"
        },
        {
            "title": "G Limitations",
            "content": "Despite its effectiveness, TEXT2GRAD has two primary limitations. First, its performance depends on the quality of the reward model. While our model achieves moderate alignment with reference critiques, further improvements in critique generation and span-level accuracy could enhance downstream policy optimizationparticularly in tasks requiring nuanced feedback. Second, generating and applying token-level rewards introduces computational overhead compared to scalar reward methods, which may impact scalability in large-scale deployments. In future work, we aim to improve reward model precision and efficiency, and to extend our framework to broader generation settings, including open-ended tasks where fine-grained feedback is harder to define. 27 (a) Training dynamics on KodCode dataset (b) Training dynamics on UltraFeedback dataset Figure 5: Comparative analysis of training dynamics between TEXT2GRAD and standard PPO. The results demonstrate that TEXT2GRAD (red line) achieves more stable and consistent learning progress, while standard PPO (blue line) shows significant volatility and unstable oscillations throughout the training process."
        },
        {
            "title": "I Hyperparameters for Natural Language Reward Model Training",
            "content": "Parameter Base Model Hardware Parallelization Batch Size Learning Rate Epochs Prompt Max Length Max Sequence Length LoRA Rank (r) LoRA Alpha LoRA Target Modules LoRA Dropout Optimizer Adam β1, β2 Adam ϵ Weight Decay Gradient Clipping FP16 Value Llama-3.1-8B-Instruct 8 NVIDIA V100 (32GB) DeepSpeed ZeRO-3 1 per GPU 1e-5 3 950 tokens 1250 tokens 16 32 q_proj, v_proj 0.1 AdamW 0.9, 0.999 1e-8 3e-7 1.0 Enabled Table 11: Hyperparameters for Natural Language Reward Model Training 28 Ablation Study for SLF5K To demonstrate the effectiveness of our proposed components, we conduct ablation studies on the SLF5K dataset. Figure 6 shows the performance comparison when removing the Chain-of-Thought (CoT) reasoning process and the span-level reward signals from our framework. Figure 6: Ablation study results on SLF5K dataset. The graph shows the win rate comparison between our full model and the variant without CoT reasoning and span-level reward signals during reward model training. The results demonstrate that these components are crucial for achieving optimal performance in natural language gradient policy optimization. Pseudocode for the Text2Grad Framework Algorithm 1 TEXT2GRAD: Reinforcement Learning from Natural Language Feedback (Overall Framework) Input: Set of prompts for policy training. Output: Optimized policy πθ. Phase 1: Dual-Feedback Reward Annotation (Described in Section 3.3) 1: Initialize dataset for reward model training DR . 2: Generate initial responses yi for set of prompts xi (e.g., using base policy). 3: for all prompt xi and its corresponding response yi do 4: 5: 6: 7: end for (ci, A(yi), δi) GenerateDualFeedback(xi, yi) Let zi [ci; A(yi)] Add (xi, yi, zi) to DR. See Algorithm 2 ci is critique, A(yi) is span-JSON Phase 2: Reward Model Training (Described in Section 3.4) 8: Rϕ TrainRewardModel(DR) Phase 3: NL-Gradient Policy Optimization (Described in Section 3.5) 9: Initialize policy πθ (e.g., with pre-trained LLM) and value function Vψ. 10: πθ OptimizePolicyWithNLGradient(πθ, Rϕ, Vψ) 11: return Optimized policy πθ. See Algorithm See Algorithm 4 29 Algorithm 2 Dual-Feedback Reward Annotation (Section 3.3) 1: procedure GENERATEDUALFEEDBACK(x, y) 2: 3: Input: Prompt x, generated response = (y1, . . . , yT ). Output: Natural language critique c, structured span-level reward map A(y), token-level pseudo-rewards δ. 4: 5: 6: 7: 8: 9: 10: 11: 12: // Dual-Feedback Annotation using strong LLM (e.g., GPT-4o) if human-written feedback is lacking (Reasoning-Augmented Annotation) then Guide LLM to: (1) Reason about the quality of response step-by-step. (2) Output critique based on this reasoning. (3) Produce span-level JSON map A(y) associating spans sk with labels ℓk {positive, neutral, negative}. else Prompt LLM to output critique and span-level JSON map A(y). end if // Token-Level Reward Mapping Formally, RLLM(x, y) = (c, A(y)), where A(y) : sk (cid:55) ℓk Initialize token-level pseudo-rewards δ = (δ1, . . . , δT ) with zeros. for all labeled span sk in A(y) do Let ℓk = A(y)[sk]. if ℓk = positive then for all token index such that yt sk do else if ℓk = negative then for all token index such that yt sk do 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end procedure end if δt +1. end for δt 1. end for end for return c, A(y), δ. neutral spans are typically unannotated and default to δt = 0. Algorithm 3 Reward Model Training (Section 3.4) 1: procedure TRAINREWARDMODEL(DR) Input: Dataset DR = {(xi, yi, zi)}N 2: Output: Trained reward model Rϕ. 3: Initialize reward model parameters ϕ. 4: The reward model Rϕ is trained to predict given x, y: pϕ(z x, y) = (cid:81)z 5: Define the loss function: LR(ϕ) = E(x,y,z)DR [log pϕ(z x, y)]. 6: Train Rϕ by minimizing LR(ϕ) on DR using teacher forcing and standard causal LM objective. 7: return Trained reward model Rϕ. 8: 9: end procedure i=1, where zi = [ci; A(yi)]. j=1 pϕ(zj z<j, x, y). 30 Input: Initial policy πθinit, trained reward model Rϕ, initial value function Vψinit. Hyperparameters: Learning rates, PPO clipping ϵ, entropy bonus β, GAE γ, λ. Output: Optimized policy πθ. Initialize policy πθ πθinit, value function Vψ Vψinit. for each iteration iter = 1, . . . , MaxIterations do Algorithm 4 NL-Gradient Policy Optimization (Section 3.5) 1: procedure OPTIMIZEPOLICYWITHNLGRADIENT(πθinit, Rϕ, Vψinit) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Sample prompt x. Generate response = (y1, . . . , yT ) πθold( x). Generate feedback = [c; A(y)] Rϕ(z x, y). Parse A(y) to get token-level pseudo-rewards δ = (δ For = 1, . . . , : rtotal,A Compute advantages A1, . . . , AT . For = . . . 1: Let πθold πθ. Initialize batch of rollouts . for each sample = 1, . . . , NumSamplesPerIteration do 1, . . . , δ At = (cid:80)T Vψ(x, y<t). k=t γktrtotal,A + rKL δ γVψ(x, y<t+l+1) Vψ(x, y<t+l)) ) Add (x, y, δ, A, rtotal,A) to B. end for for each epoch = 1, . . . , NumEpochsPPO do for all (x, y, δ, A, rtotal,A) in do For = 1, . . . , : 17: 18: 19: 20: 21: 22: ρt(θ) = πθ(ytx,y<t) πθold (ytx,y<t) . LCLIP LVF LENT (ψ) = (Vψ(x, y<t) ((cid:80)T (θ) = H(πθ( x, y<t)). 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: end procedure end for end for return Optimized policy πθ. end for LPPO(θ) = EB,t LVF(ψ) = EB,t Update policy parameters: θ optimizer_step(θ, θLPPO(θ)). Update value function parameters: ψ optimizer_step(ψ, ψLVF(ψ)). (cid:2)LCLIP (cid:2)LVF (θ) βLENT (ψ)(cid:3). (θ)(cid:3). T ) (using lines 11-20 of Alg. 2). rKL is an optional KL-penalty term. (Or use GAE: At = (cid:80)T t1 l=0 (γλ)l(rtotal,A t+l + (θ) = min (ρt(θ)At, clip(ρt(θ), 1 ϵ, 1 + ϵ)At). k=t γktrtotal,A ))2. Value target is discounted sum of rewards."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft",
        "University of Chicago"
    ]
}