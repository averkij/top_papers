{
    "paper_title": "A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning",
    "authors": [
        "Qianben Chen",
        "Jingyi Cao",
        "Jiayu Zhang",
        "Tianrui Qin",
        "Xiaowan Li",
        "King Zhu",
        "Dingfeng Shi",
        "He Zhu",
        "Minghao Liu",
        "Xiaobo Liang",
        "Xin Gui",
        "Ge Zhang",
        "Jian Yang",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy."
        },
        {
            "title": "Start",
            "content": "A2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning"
        },
        {
            "title": "Abstract",
            "content": "Large language models split into two families: reasoning-centric LLMs, which strengthen internal chainof-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A2FM), unified framework that follows route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under shared backbone. To address the inefficiency gap, we introduce third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies cost-regularized reward. On the 32B scale, A2FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy. Date: October 17, 2025 Open Source: Code Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com Models 5 2 0 2 6 1 ] . [ 2 8 3 8 2 1 . 0 1 5 2 : r Figure 1 Average performance on agentic, reasoning, and general (ARG) benchmarks. Overall, A2FM ranks 1st, 2nd, and 1st on the three categories, respectively. Note that we uses the best-suited mode for each dataset respectively."
        },
        {
            "title": "Contents",
            "content": "3.3 3"
        },
        {
            "title": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.1 Agent Systems and Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Adaptive Reasoning Approaches\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.1\nLength-Aware Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.2 Capability-Aware Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nStage 1: Route-then-Align Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2\nStage2: Adaptive Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1 Dataset Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n7 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n18",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 System Prompt 5 5 5 5 5 7 9 9 6 A.2 Summary Prompt"
        },
        {
            "title": "20\nB Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nC Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1 Introduction",
            "content": "Recent reasoning-centric Large Language Models (LLMs)e.g., OpenAI o3 (OpenAI, 2025c) and DeepSeek-R1 (Guo et al., 2025)show strong chain-of-thought (CoT) abilities for mathematics, scientific diagram interpretation, and logic. Their post-training typically optimizes internal deliberation and solution steps. By design, they are trained in closed, text-only settings and do not learn to invoke external tools. This difference in training objective and environment leaves capability gap in tool use. Figure 2 Overview of A2FM. Left: the framework integrates three execution modesinstant, reasoning, and agenticunder unified backbone with task-aware routing. Right: mode allocation across six benchmarks (MMLU-Pro, GPQA-d, AIME25, MATH500, Xbench-DeepSearch (Xbench), and GAIA-text (GAIA), illustrating how A2FM adapts routing to task characteristics. In parallel, agentic LLMse.g., OpenAI GPT-5(OpenAI, 2025a), GLM-4.5 (Zeng et al., 2025), Kimi K2 (Team et al., 2025a), DeepSeek-V3.1 (DeepSeek-AI, 2025), and agent frameworks such as OAgents (Zhu et al., 2025) and BrowseMaster (Pang et al., 2025), as well as Agent Foundation Models (Li et al., 2025b)emphasize environmentdependent interaction. They plan, search, crawl, and execute code. Their post-training focuses on tool correctness, call patterns, and interaction quality. Yet when deep multi-step reasoning is required (e.g., competitive math), these systems often trail state-of-the-art reasoning models (Jimenez et al., 2023). Thus, the two families exhibit complementary strengths: internal CoT versus external tool use. Recent advances in agent systems and frameworks (Li et al., 2025a; Roucher et al., 2025; Tang et al., 2025; Tao et al., 2025; Wu et al., 2025b; Zhou et al., 2023b, 2024; Zhu et al., 2025) have greatly expanded the scope of LLM-based autonomy, enabling complex behaviors such as planning, multi-step tool use, and collaborative decision-making. These systems often rely on external orchestrationmanually designed workflows, multi-agent coordination, or prompt-based tool pipelinesto decompose tasks and integrate environment feedback. Despite their impressive flexibility, such designs face several inherent limitations. First, they incur significant computational and latency overhead due to redundant inter-agent communication and long reasoning traces. Second, their generalization ability is limited: adapting to new domains typically requires human-engineered prompt flows or redefined agent roles (Zeng et al., 2023; Zhang et al., 2024). Third, these frameworks depend on general-purpose LLMs that are not trained end-to-end to handle multi-turn, multi-tool reasoning, preventing them from learning data-driven strategies for mode switching. Complementary to the above, recent works attempt to blend thinking and non-thinking behaviors to reduce redundant computation (Chen et al., 2024b; Fatemi et al., 2025; Kumar et al., 2025; Lou et al., 2025; Sui et al., 2025). However, these approaches generally operate along single binary axisdeciding whether to reason or notwithout the capacity to adaptively invoke agentic behaviors such as tool use and task planning. As result, while they can shorten overextended reasoning traces, they remain unable to manage the broader spectrum of reasoningacting trade-offs inherent to real-world tasks. Moreover, introducing agentic behaviors further compounds this limitation by increasing runtime and inflating the overall cost of pass (Erol et al., 2025). To address the challenges mentioned above, we propose A2FM (Adaptive Agent Foundation Model), which unifies three complementary modes within single backbone - agentic (tool-aware action), reasoning (explicit CoT), and instant (direct answer) - thereby bridging the gap between reasoning-centric LLMs and orchestrated agent systems. To bridge the capability gap, A2FM internalizes self-adaptive router that learns what to do per query, naturally combining the strengths of reasoning and agentic paradigms in one model. To address efficiency, we add the instant mode so that simple queries are answered directly, avoiding unnecessary reasoning or tool calls. Supervised fine-tuning follows 3 route-then-align principle: the model first performs task-aware routing, then aligns mode-conditioned trajectories under shared backbone using diverse datasets spanning tool use, explicit reasoning, and direct answering. Nevertheless, simply mixing modes falls short: the model must not only maintain high accuracy but also minimize computational cost, yet boundary queries remain difficult to route and data often underutilized. Prior work either omits explicit modeling of mode mixing (Jiang et al., 2025; Zhang et al., 2025b), or focuses solely on efficiency by shortening reasoning traces in post-training (Arora and Zanette, 2025; Team et al., 2025b), or fails to jointly optimize routing and trajectory generation in an end-to-end framework (Wu et al., 2025a). We therefore introduce Adaptive Policy Optimization (APO), reinforcement learning procedure tailored for mode selection. APO builds on group-relative policy optimization (Shao et al., 2024), enforces adaptive sampling across modes to prevent collapse, and applies dynamic, cost-regularized reward (with LLM-as-Judge signals (Zheng et al., 2023)) that balances accuracy with efficiency. It explicitly favors minimal-effort instant solutions on easy queries while escalating to reasoning or agentic modes when external evidence or extended deliberation is needed. Empirically, A2FM achieves state-of-the-art results across benchmarks: on agentic tasks it obtains 13.4% on BrowseComp, on reasoning it reaches 70.4% on AIME25, and on general tasks it delivers 16.7% on HLE. Beyond raw accuracy, APO substantially improves efficiency: on SuperGPQA, the adaptive execution yields cost of pass of only $0.00487 per correct answercutting cost by 45.2% relative to reasoning and 33.5% relative to agenticwhile maintaining comparable accuracy. In summary, our key contributions include: We present A2FM, the first adaptive agent foundation model that integrates agentic, reasoning, and instant modes under single backbone via self-adaptive router. We propose two-stage process: (i) supervised route-then-align fine-tuning for mode-conditioned trajectories; (ii) APO for mode selection with adaptive sampling and cost-regularized rewards. We empirically demonstrate that A2FM achieves state-of-the-art results at the 32B scale across agentic, reasoning, and general benchmarks, with substantial reductions in token usage and computation versus mode-forcing baselines."
        },
        {
            "title": "2.1 Agent Systems and Frameworks",
            "content": "Recent agent systems extend large language model (LLM) autonomy beyond text-only reasoning by introducing planning, multi-step tool use, and collaborative decision-making pipelines (Hu et al., 2025; Jin et al., 2025b; Mai et al., 2025; Roucher et al., 2025; Tang et al., 2025; Zhou et al., 2023b, 2024). common design pattern involves external orchestration: the system decomposes tasks into subgoals, assigns them to role-specialized agents such as planners, researchers, coders, and verifiers, and coordinates calls to search, browsing, and code-execution tools. These frameworks demonstrate strong performance on complex, open-ended problems through explicit workflow design and tool integration (Chen et al., 2023; Fourney et al., 2024; Hong et al., 2024; Li et al., 2023; Qian et al., 2023). Within this paradigm, multi-agent frameworks leverage division of labor and tool diversity to improve coordination and task efficiency (Roucher et al., 2025; Tang et al., 2025; Xie et al., 2025; Zhang et al., 2024; Zhou et al., 2023b, 2024). However, they often exhibit high computational and latency overhead due to inter-agent communication and extended reasoning traces, and generalization to new domains typically requires additional prompt or workflow engineering (Hu et al., 2025; Zeng et al., 2023; Zhang et al., 2024). The backbone LLMs in these systems are not trained end-to-end for multi-turn, multi-tool protocols, and therefore rely on predefined orchestration rules rather than learned decision policies (Li et al., 2025b; Liu et al., 2023; Zhou et al., 2023a). Singleand few-agent frameworks focus on tool-centric and deep-search tasks with tighter reasoningaction loops. Representative examples include WebDancer (Wu et al., 2025b), WebShaper (Tao et al., 2025), WebSailor (Li et al., 2025a), Asearcher(Gao et al., 2025), and AFM (Li et al., 2025b), which emphasize structured browsing, information extraction, and verification, as well as OAgents (Zhu et al., 2025) and DeepResearcher (Zheng et al., 2025), which integrate planning and tool APIs based on strong proprietary backbones. These systems enhance reliability in search and code-execution tasks and introduce structured planning procedures such as step-wise verification or retry mechanisms (Gao et al., 2025; Pang et al., 2025; Zhang et al., 2025a). Nevertheless, they are largely governed by controller 4 heuristics and fixed procedural flows, where routing among direct answering, internal reasoning, or external tool use is manually designed rather than adaptively learned from data (Li et al., 2025d; Qian et al., 2025; Xue et al., 2025)."
        },
        {
            "title": "2.2 Adaptive Reasoning Approaches",
            "content": "A growing line of work improves reasoning efficiency by letting models adaptively regulate how long and when to think, rather than relying on fixed, full-length chain-of-thought (CoT) reasoning. These methods broadly fall into two complementary categories: length-aware control, which optimizes the depth of reasoning, and capability-aware routing, which decides when extended reasoning should be invoked."
        },
        {
            "title": "2.2.1 Length-Aware Control",
            "content": "Length-aware methods reduce redundant token consumption by training models to produce concise CoT traces when fullstep reasoning is unnecessary. Two main paradigms are common: (1) reinforcement learning with length regularization, which introduces token-length penalties or efficiency terms to balance accuracy and brevity (Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Yeo et al., 2025); and (2) supervised CoT compression, which yields variable-length CoT via post-hoc summarization or by learning compressed reasoning patterns from data (Kang et al., 2024; Liu et al., 2024; Munkhbat et al., 2025; Xia et al., 2025). These approaches demonstrate consistent token savings while maintaining task performance."
        },
        {
            "title": "2.2.2 Capability-Aware Routing",
            "content": "Capability-aware routing focuses on when to reason by using internal signals (e.g., uncertainty, logit margins, hidden representations) to estimate difficulty and trigger appropriate reasoning depth (Chen et al., 2024a; He et al., 2025). Several works further adopt reinforcement learning to learn when-to-think policies (Lou et al., 2025; Yu et al., 2025); Bimodal Policy Optimization contrasts utilities of reasoning vs. non-reasoning trajectories for the same input (Yang et al., 2025b); and Large Hybrid Reasoning Models route by query semantics to invoke extended reasoning when beneficial (Jiang et al., 2025). Overall, this literature primarily studies binary switching between short and long CoT and generally does not incorporate end-to-end training for adaptive tool use."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We consider system that processes queries drawn from task mixture and produces outputs Y. For each query, router selects one of three execution modes: = {instant, reasoning, agentic}, πroute(m x) where πroute is routing policy over the mode set M. Given the selected mode m, the system then generates an output through mode policy πm, i.e., πm(y x) Each mode induces distinct form of trajectory τm: direct answers for instant, chain-of-thought sequences for reasoning, and tool-interaction traces for agentic. We denote the decoding function of trajectory under mode as fm(x, τm). Let Acc(x, y) [0, 1] be task-specific accuracy metric. The expected performance of mode on input is Qm(x) = Eτmπm(x)[Acc(x, fm(x, τm))] Our objective is to jointly optimize the router and mode policies such that the system maximizes expected accuracy across the task mixture: max πroute, {πm,fm} ExD πroute(m x) Qm(x) mM 5 This unified formulation makes explicit the two levels of decision-making: (1) the router πroute decides which mode to activate for given query, and (2) the mode-specific policy πm determines the quality of the generated trajectory. Together, they provide the foundation for both supervised alignment and reinforcement learning."
        },
        {
            "title": "3.2 Stage 1: Route-then-Align Fine-tuning",
            "content": "Training Data Generation. The first stage centers on supervised route-then-align training, where the model learns to classify query into one of three modesinstant, reasoning, or agenticand then generate mode-consistent trajectories. Mode routing is triggered by paired <classification> tags, after which the model proceeds with different behaviors: - In the instant mode, the model directly outputs the final prediction within <answer> tags, minimizing deliberation. - In the reasoning mode, the model provides chain-of-thought wrapped in <reasoning> tags, followed by the final result in <answer> tags. - In the agentic mode, the model interleaves high-level reasoning with external tool usage. While inspired by Agent Foundation Models (Li et al., 2025b), our design departs significantly in how plan and summary are used. plan: appears only once at the beginning, decomposing the query into multiple sub-goals that can be executed in parallel. summary: operates dynamically across the process, allowing the system to concurrently aggregate solved sub-tasks, terminate completed threads, and open new ones when needed. This explicitly parallel architecture enables simultaneous multi-tool execution, substantially improving both efficiency and effectiveness of tool usage. Environment Interaction (Agentic Mode). To support agentic execution, the policy interacts with external resources through three MCP-style tool interfaces that return structured observations: web_search: We interface with the Google Search engine for information retrieval, conducting web searches via the SerpAPI1 tool. The core SerpAPI configuration parameters are the search query string and the number of results to return. In practice, the tool issues model-generated queries and, by default, retrieves the top 5 results for each query. Each result includes title, snippet, and the corresponding Uniform Resource Locator (URL). This setup provides essential support for subsequent analysis and decision-making processes. crawl_page: We employed web-crawling tool authenticated via Jina API2 and summarized the retrieved content using gpt-5-mini. The tools core configuration parameters comprise the Uniform Resource Locator (URL), the summary query. Candidate URLs are generated by the model, and the tool uses the Jina API to crawl each URL. Subsequently, we invoke the gpt-5-mini model to produce summary for each crawled page; the summary prompt is provided in Appendix A.2. code_execute: To ensure usability and security, we realize the code sandbox with nsjail3, lightweight utility that provisions isolated execution environments for Python code. By leveraging Linux namespace isolation, nsjail hardens file-system boundaries and prevents unauthorized access to host resources. notable strength of this approach is its compatibility with containerized ecosystems (e.g., Docker), which facilitates seamless migration across diverse training and testing settings. The tool also offers fine-grained resource controls. during the training process, we cap CPU time at 5 seconds and memory at 5 GB to keep code execution strictly bounded. Unless otherwise noted, tokens originating from <tool_response> are treated as observations and excluded from the optimization loss; the model is trained only on its own generated tokens, so learning focuses on routing, reasoning, and action selection rather than memorizing tool outputs. The agentic trajectory begins with <plan> section, executes tools in parallel (each wrapped in <tool_call> tags), and collects their results within <tool_response> tags. These tool results are masked during trainingfollowing the 1https://google.serper.dev/search 2https://jina.ai/ 3https://github.com/google/nsjail 6 strategy of Search-R1 (Jin et al., 2025a)so that during the fine-tuning stage, the model focuses on reasoning and routing, not memorization of tool outputs. final <summary> may be added before producing the <answer>. Details of the training data trajectories template are deferred to Appendix C. Data Curation. To construct diverse and challenging training data, we employ two complementary heuristics. First, difficulty-based sampling strategy adjusts the natural distribution of task success. In the raw dataset, sampling typically yields U-shaped distributiontasks are either consistently solved or consistently unsolved, with few of intermediate difficulty. To enhance the representation of moderately challenging cases, we deliberately downsample always-solved instances, resulting in J-shaped distribution that better balances task difficulty (see Fig. 3). This modification ensures broader coverage across the difficulty spectrum and exposes the model to queries near its decision boundaries. Second, we explicitly address classification-ambiguous queries where routing decisions are uncertain. Instead of relying on majority votes, we assign the mode label corresponding to the trajectory that achieves the highest accuracy on that query. This encourages stronger supervision near ambiguous boundaries, improving the models ability to discriminate between reasoning, agentic, and instant behaviors when multiple modes appear plausible. For distillation, we adopt mode-specialized teachers: DeepSeek R1 (Guo et al., 2025) (strong reasoning capability) for the reasoning mode, and DeepSeek V3.1 (DeepSeek-AI, 2025) (broad general competence) for the agentic and instant modes. This complementary setup ensures that each mode benefits from the most suitable teacher, leading to more effective supervision and consistent alignment under unified backbone. a p S Easiness (x/4) Easiness (x/4) Figure 3 Difficulty-based sampling produces J-shaped distribution for both the agentic (left) and reasoning (right) datasets, reducing trivially solved cases while emphasizing moderately challenging queries."
        },
        {
            "title": "3.3 Stage2: Adaptive Policy Optimization",
            "content": "Aiming to tackle the challenges in balancing accuracy and efficiency in query processing, we propose Adaptive Policy Optimization (APO), which equips the model with dynamic mode-selectionchoosing among agentic, reasoning, or instantbased on task demands. APO builds on GRPO (Shao et al., 2024) but extends it in two key ways: (i) tailored rollout strategy that ensures sufficient exploration of all modes, avoiding under-sampling; and (ii) an adaptive reward that explicitly encodes the accuracyefficiency trade-off, rather than relying on coarse binary rewards. This design enables the model to not only maintain correctness but also minimize unnecessary computation, achieving accurate yet cost-effective routing. Rollout Strategy. For each query, APO performs both mode-forced rollouts and adaptive rollouts. In the forced setting, the model is compelled to operate in each of the three modesagentic, reasoning, and instantfor ρ rollouts per mode, where ρ is tunable hyperparameter. Mode enforcement is implemented via prefix injection, in which pre-specified classification tag is inserted at the beginning of the models response. This guarantees that every query is explored 7 Figure 4 Overview of Adaptive Policy Optimization (APO). Left: Rollout and reward process. For each query, mode-specific rollouts are generated either by prefix injection (forced agentic/reasoning/instant) or by adaptive classification. Both prefix-injection tokens and tool-response tokens are excluded from loss since they are not model-generated. Right: Accuracyefficiency trajectory under APO, showing how A2FM progressively approaches the Pareto frontier by improving accuracy while reducing non-instant triggering (excluding AIME24/25). under all modes, enabling an unbiased estimate of their relative success rates. Such statistics provide the foundation for adaptive rewards (See Adaptive Reward below), which encourages the use of more accurate modes while preferentially selecting the instant mode whenever possible to minimize token consumption. The enforced prompts for each mode are outlined below: 1. Reasoning Mode: This task requires complex logical reasoning (such as mathematical proofs, multi-step problem solving) and causal analysis, so will select reasoning_agent. <classification> reasoning_agent </classification> 2. Agentic Mode: This task requires acquiring real-world information (such as news and data) or executing code (such as programming problems, data processing, or statistics), so will select agentic_agent. <classification> agentic_agent </classification> 3. Instant Mode: This task needs no real-world info, code, or complex reasoningjust basic knowledge or brief responses, so will select instant_agent. <classification> instant_agent </classification> In addition, APO samples γ adaptive rollouts, where γ is another hyperparameter controlling the number of trials in which the model autonomously selects its operating mode. These adaptive samples provide the opportunity to reward correct self-routing, thereby reinforcing accurate and cost-efficient mode selection. Accuracy Reward. We adopt the LLM-as-Judge framework (Zheng et al., 2023) to assess correctness, where judge model Mj provides binary feedback: racc = I[Mj(x, ˆy) = 1]. This avoids the limitations of rule-based metrics (e.g., F1, EM) that cannot fully capture the validity of open-ended outputs. Adaptive Reward. To further guide proper mode selection, we introduce an adaptive reward that explicitly favors minimal-effort solutions on easy tasks. Intuitively, if query can already be solved by the instant mode, then choosing more costly mode (reasoning or agentic) should incur penalty. Formally, we label query as easy if the instant mode achieves accuracy above threshold τ . For such easy queries, the reward is defined as 1 pα, 1, if non-instant mode is chosen, otherwise, radaptive = 8 where is the empirical success rate of all forced rollouts on that query, and α > 0 is scaling factor. This design ensures that correct use of the instant mode always receives full reward, while reasoning or tool use on easy tasks is penalized in proportion to how confidently the query could be solved instantly. For hard queries, no penalty is applied, prioritizing the focus on correctness. Format Reward. We enforce strict schema compliance: if the output violates the mode-specific format (e.g., tool tags in instant), the reward is zero; otherwise it is one: rformat = 1, 0, if matches the format of mode m, otherwise. Total Reward. The final reward combines accuracy, adaptivity, and format constraints: rtotal = raccuracy radaptive rformat. This multiplicative design ensures that failure in any single component (e.g., wrong answer, misuse of modes, or format violation) immediately results in reward deduction, thereby enforcing strict correctness while still encouraging efficiency and proper schema adherence. We adopt GRPOs objective calculation with certain specializations. Specifically, we strictly employ on-policy training, as varying model capabilities may affect query classification, thereby leading to an unstable training process. Additionally, to expedite training and explore more efficient mode selections, we omit the computation of KL divergence. Consequently, given an actor parameterized by θ, for each question xi, APO samples group of outputs {yi1, yi2, , yiG} from the old policy πθold and then optimizes the policy model by maximizing the following objective: JAPO(θ) = [xiD,{yij }G j=1πθold (Y xi)] 1 t=0 1 yij 1 yij 1 t= min πθ (o(ij) πθold (o(ij) s(ij) s(ij) ) ) where o(ij) is the token at index in response yij; s(ij) is the sum of all mode trajectory in the rollout stage. = (xi, a(ij) ˆAij, clip ) πθ (o(ij) πθold (o(ij) t1 ) is the prefix context when generating a(ij) s(ij) s(ij) , 1 ε, 1 + ε ˆAij ) , ..., a(ij) ;G = 3ρ+γ This improved framework with rollout strategy and adaptive reward ensures more stable policy optimization and targeted mode selection. Through this refined reinforcement learning paradigm, A2FM achieves enhanced performance in adaptive mode routing, while simultaneously boosting overall accuracy and efficiency across diverse query scenarios."
        },
        {
            "title": "4.1 Dataset Composition",
            "content": "Our dataset sources encompass variety of task types and domains. We specifically chose not to focus on easy datasets but rather to select data from more challenging and complex sources. These include datasets such as Webdancer(Wu et al., 2025b), Webshaper(Tao et al., 2025), Taskcraft(Shi et al., 2025), 2Wiki(Ho et al., 2020), MusiqueHo et al. (2020), and HotpotQA(Yang et al., 2018) for agentic-related tasks, and mathematic reasoning datasets like Skywork-Math(Zeng et al., 2024), DAPO-Math-17K(Yu et al., 2025), and GSM8K(Cobbe et al., 2021). Additionally, code-related data sources like Skywork-Code(Zeng et al., 2024) and Codeforces(Penedo et al., 2025) were incorporated for programming tasks. Importantly, while we did not curate separate datasets solely for the instant mode, we deliberately retained relatively simple examples within the search and reasoning datasets. This ensures balance across the three modes and allows the model to naturally learn to apply the instant mode on straightforward tasks. Finally, we created 5289, 2829, and 2890 trajectories for agentic, reasoning and instant modes respectively during the route-then-align SFT phase and 6000 question-answer pairs during the APO phase."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We train A2FM on mixture of agentic, reasoning, and code-related datasets, while also retaining relatively simple cases to enable learning of the instant mode. Full dataset composition and sampling strategy are provided in Appendix 4.1. Baselines. For comparison, we conducted comprehensive evaluation of our trained model against three categories of systems: (1) general-purpose large language models (LLMs), including closed-source models such as GPT-4.1(OpenAI, 2025b), o1 (Jaech et al., 9 42.1 52.8 5.2 AFM Code 94.6 66.7 59. 2024), Claude-4-Sonnet(Anthropic, 2025), and open-source models DeepSeek-R1(Guo et al., 2025), Qwen2.5-Instruct-32B (Yang et al., 2024), Qwen3-32B(Yang et al., 2025a), QwQ-32B(Team, 2025); (2) state-of-the-art agent frameworks, such as OAgents(Zhu et al., 2025); and (3) Agent Foundation Models, including DeepDive(Lu et al., 2025), WebDancer(Wu et al., 2025b), WebSailor(Li et al., 2025a), Asearcher-Web(Gao et al., 2025), AFM-Search(Li et al., 2025b), SimpleTIR(Xue et al., 2025), EffectiveTIR(Bai et al., 2025), AutoTIR(Wei et al., 2025b), and ReTool(Feng et al., 2025). Evaluation Benchmarks. We evaluate our model on three key types of benchmarks. First, for agentic tasks, we use BrowseComp(Wei et al., 2025a), GAIA(Mialon et al., 2023) (we test just 103 text-only subset extracted in (Wu et al., 2025b)), and XBench-DeepSearch(Chen et al., 2025), which assess the models ability to interact with external tools like search engines and databases for information seeking. Second, for reasoning-dependent tasks, we utilize AIME24(of America , MAA), AIME25(of America , MAA), and Math500(Hendrycks et al., 2021), which focus on mathematical reasoning and problem-solving without relying on external information. Finally, for general knowledge and comprehension, we use GPQA-d(Rein et al., 2024), SuperGPQA(Du et al., 2025), MMLU-Pro(Wang et al., 2024), and HLE(Phan et al., 2025) (We use 500 text-only subset defined in (Li et al., 2025c)), which test the models ability to synthesize information and apply broad general knowledge across wide range of domains. (a) Agentic benchmarks (GAIA, BrowseComp, XBench-DeepSearch). Benchmark A2FM A2FM Agentic XBench-DS 56.0 54.0 -2.0 57.3 60.7 +3.4 GAIA 14.4 +1.0 13.4 BC GPT-4. Claude 4 Sonnet Qwen2.5 Instruct-32B OAgents (GPT-4.1) DeepDive WebSailor Asearcher 17.0 22.3 1.5 21.5 22.3 2. 8.7 13.6 0.6 47.0 58.3 13.7 50.5 - 14.8 53.3 53.2 10.5 AFM Search 54.0 55.3 11. (b) Reasoning benchmarks (MATH500, AIME24, AIME25). Benchmark A2FM MATH500 AIME24 AIME25 95.0 74.5 70.4 A2FM Reasoning 95.2 +0.2 74.5 +0.0 70.4 +0.0 GPT-4. Claude 4 Sonnet o1 Simple TIR Effective TIR Auto TIR ReTool 92.4 46.5 37.0 94.0 43.4 33.1 96.4 74.3 79.2 92.9 59.9 49.2 86.4 42.3 25.2 62.6 33.3 16. 93.2 67.0 49.3 (c) General-knowledge benchmarks (GPQA-d, SuperGPQA, MMLU-Pro, HLE). Benchmark A2FM A2FM Agentic 67.7 +4.6 GPQA-d SuperGPQA 54.7 56.0 +1.3 75.8 +2.0 73.8 MMLU-Pro 16.7 20.6 +3.9 HLE 63.1 A2FM Reasoning 64.7 +1.6 51.2 -3.5 77.0 +3.2 13.4 -3.3 GPT-4.1 Claude 4 Sonnet Qwen2.5 Instruct-32B Qwen3 -32B QwQ 66.3 50.8 81.8 3. 68.3 55.7 83.5 5.8 49.5 38.8 69.0 3.8 54.6 43.2 72.7 8.3 65.6 43.6 76.4 8.2 Table 1 Unified results across (a) agentic, (b) reasoning, and (c) general-knowledge benchmarks. Bold = best; underline = second-best. Teal/Red superscripts indicate gain/loss of the forced mode relative to adaptive A2FM. indicates results reproduced by us. All numbers are reported as avg@1, except AIME24/25 which use avg@32. Implementation Details. We conduct all experiments on the Qwen2.5-32B-Instruct backbone. In the route-then-align SFT stage, we train for 3 epochs with batch size of 256, AdamW optimizer, cosine decay learning rate schedule, and max sequence length of 32,768. In the APO stage, we train for 2 epochs with learning rate of 1e-6, 5 warmup steps, batch size 128, and 12 rollouts per prompt (with ρ = 3 and γ = 3) capped at 65,536 tokens. We set α = 2 for adaptive reward calculation. For inference, we set temperature 1.0, top-p 0.9, top-k 20, and max output length of 131,072."
        },
        {
            "title": "4.3 Main Results",
            "content": "Evaluation of Agentic Abilities. The results in Table 1a show that our adaptive mode (A2FM) achieves consistently competitive performance across all agentic benchmarks. On XBench-DS, A2FM reaches 56.0%, the highest among all baselines and surpassing the second-best model, AFM-Search (54.0%), by +2.0 points. On GAIA, A2FM ranks second overall, trailing only OAgentswhich leverages GPT-4.1 as its backbonewhile still outperforming all general-purpose LLMs (e.g., Claude-4-Sonnet at 2.6%) and all 32B-scale agent foundation models. On BrowseComp, A2FM achieves 12.4%, ranking third behind DeepDive (14.8%) and OAgents (13.7%), but still exceeding AFM-Search (11.1%) and all general-purpose LLMs. When forcing the model into the agentic mode (A2FM-Agentic), performance further improves: on BrowseComp it rises to 14.4%, 10 (a) Relation between task difficulty, allocation ratio, and accuracy for instant and non-instant modes. (b) Cost of Pass from different modes. Figure 5 Efficiency analysis on SuperGPQA: mode allocation vs. difficulty and token usage across modes. surpassing OAgents and achieving the second-highest score overall, just behind DeepDive (14.8%); on GAIA it reaches 60.7%, establishing new SOTA. This demonstrates that A2FM not only delivers robust adaptive performance but also, when focused on tool-intensive reasoning, can outperform specialized frameworks in complex deep-search scenarios. Evaluation of Reasoning. As shown in Table 1b, A2FM achieves reasoning performance comparable to o1 on MATH500 (95.0%), while significantly outperforming all 32B-scale agent foundation models (e.g., ReTool 67.0% on AIME24, AFM Code 59.8% on AIME25). On AIME24, it sets new SOTA with 74.5%, and on AIME25 it attains 70.4%, second only to o1 (79.2%). Compared to Claude 4 Sonnet, A2FM is stronger by +33.3 and +40.2 points on AIME24/25, underscoring reasoning capabilities far beyond even top general-purpose LLMs. Notably, adaptive and reasoning-specific variants yield identical results on AIME24/25, showing that A2FM reliably routes nearly all AIME queries into the reasoning mode. Evaluation of General Abilities. The results in Table 1c show that A2FM delivers highly competitive general-domain performance at the 32B scale. The adaptive mode consistently outperforms instruction-tuned baselines (e.g., Qwen2.5-32B) and even surpasses the reasoning-focused QwQ. Despite starting from 32B instruct backbone, A2FM achieves large gains: on GPQA-d it improves by +13.6 points over Qwen2.5-32B (63.1 vs. 49.5), and on SuperGPQA it is +15.9 points higher (54.7 vs. 38.8), notably surpassing GPT-4.1 (50.8) and Claude 4 Sonnet (55.7). HLE further highlights the models strength on integrated tasks requiring both reasoning and tool use. Here, A2FM-Agentic achieves remarkable 20.6%, exceeding the next-best baseline (QwQ, 8.2%) by +12.4 points, while the adaptive mode (16.7%) still leads all comparable 32B models by wide margin. The gap between adaptive and agentic variants reflects that some queries are routed into the instant mode, illustrating A2FMs ability to balance effectiveness with efficiency even under challenging integrated settings. Evaluation of Efficiency. We focus on A2FMs efficiency by analyzing its mode allocation across task difficulty (Fig. 5a). On SuperGPQA (with human-annotated difficulty), the model uses instant mode for 61.1% of easy questions, but this drops to just 5.3% for difficult ones. This shows adaptive routing: it leans on reasoning/tool use for complex tasks and direct answers for simple ones to save computation. Notably, instant response accuracy stays stable at 55% across all difficulty levels, highlighting this modes robustness even for harder queries. We further analyze efficiency through the metric cost-of-pass (dollar cost per correct answer) on the four general benchmarks (GPQA-d, SuperGPQA, MMLU-Pro and HLE). Formally, for model on problem p, Cost-of-Pass(m, p) = Cm(p) Rm(p) , where Rm(p) is the accuracy and Cm(p) is the expected inference cost, computed from input/output token counts and unit token prices. We follow the official Qwen2.5-32B-Instruct pricing, with $0.00028 per 1k input tokens and $0.00084 per 1k output tokens. As illustrated in Fig. 5b, The reasoning mode incurs cost of $0.00889 per correct answer, while the agentic mode reduces this to $0.00732. Strikingly, the adaptive mode further lowers the cost to only $0.00487, corresponding to reduction of 45.2% relative to reasoning and 33.5% relative to agentic. In other words, adaptive execution delivers each correct answer at roughly half the cost of reasoning-based execution. Figure 6 Comparison of adaptive mode (red) against forced single modes across four benchmarks."
        },
        {
            "title": "5 Analysis",
            "content": "Approaching the Pareto Frontier under APO. Figure 4 illustrates how A2FM evolves under APO training, evaluated across all benchmarks except AIME24/25, since these tasks are routed entirely into the reasoning mode and offer no meaningful trade-off with instant responses. The three yellow stars mark checkpoints at step 10, 30, and 50, showing clear trajectory where the adaptive model steadily reduces non-instant usage while improving accuracy. As training progresses, A2FM moves closer to the Pareto frontier of accuracyefficiency trade-offs, indicating that APO effectively teaches the router to allocate more queries to the instant mode without sacrificing correctness. At convergence, A2FM (RL) achieves an average accuracy of 53.8% with non-instant ratio of 77.1%, compared to the Best Mode oracle (accuracy 55.4%, non-instant ratio 100%). Although the accuracy gap is marginal (1.6 points), our adaptive model lowers non-instant triggering by 22.9 points, demonstrating that APO attains near-oracle accuracy while substantially improving efficiency. Accuracy of Mode Routing. We evaluate routing accuracy on three representative datasets: GAIA (mixed agentic and reasoning), BrowseComp (agentic-dominated), and AIME24 (reasoning-dominated). For GAIA, ground-truth labels are obtained via majority voting from five strong baselinesDeepSeek-R1, DeepSeek-V3.1, GPT-4.1, GPT-5, and Claude-4-Sonnetfollowed by manual verification, while BrowseComp and AIME24 are directly labeled as agentic and reasoning, respectively. As shown in Table 2, our model achieves 92.2% accuracy on GAIA and 94.0% on BrowseComp. Errors on GAIA often stem from queries where both reasoning and agentic are plausible (e.g., solving math via code or explicit reasoning, see Case Study in Appendix B), while BrowseComp errors arise when the model overestimates its internal knowledge and avoids tool calls. On AIME24, the model reaches 100%, confirming robust mode discrimination on reasoning tasks. Table 2 Human-labeled mode distribution and model classification accuracy. GAIA (N=103) 13.6 BC (N=1266) 0.0 AIME (N=30) 100.0 84.5 1.9 92.2 100.0 0.0 94.0 0.0 100.0 0. Acc. Dataset Robustness of Adaptive Routing on Complex Tasks. We compare adaptive routing with forced single-mode execution (agentic, reasoning, and near-instant) across four benchmarks: GAIA, XBench-DeepSearch, AIME24, and SuperGPQA. GAIA and XBench-DeepSearch are primarily agentic-oriented with some reasoning and instant cases; SuperGPQA contain mix of tasks where tool use and fast responses are critical; AIME24 is reasoning-dominated. Results  (Fig. 6)  show that adaptive mode achieves competitive or superior performance in most settings. On GAIA, adaptive slightly underperforms pure agentic (57.3 vs. 60.7), primarily due to classification noise. On XBench-DeepSearch and AIME24, adaptive surpasses single modes, demonstrating its strength in routing composite, multi-facet queries to the appropriate execution mode rather than over-committing to single behavior. On AIME24, adaptive mode strictly matches reasoning mode, since all queries are routed into reasoning where accuracy dominates. Overall, adaptive mode provides robust performance while retaining flexibility across diverse task types."
        },
        {
            "title": "6 Conclusion",
            "content": "We present A2FM, an Adaptive Agent Foundation Model that unifies agentic, reasoning, and instant modes under single backbone with self-adaptive router. By combining route-then-align training with Adaptive Policy Optimization, A2FM achieves both high accuracy and efficiency, matching or surpassing strong baselines across agentic, reasoning, and general benchmarks. Our results highlight the promise of adaptive multi-mode modeling as scalable path toward efficient and versatile LLM agents."
        },
        {
            "title": "7 Contributions",
            "content": "Core Contributors Qianben Chen Jiayu Zhang Contributors Tianrui Qin King Zhu He Zhu Xiaobo Liang Ge Zhang Yuchen Eleanor Jiang Corresponding Authors Wangchunshu Zhou Jingyi Cao Xiaowan Li Dingfeng Shi Minghao Liu Xin Gui Jian Yang"
        },
        {
            "title": "References",
            "content": "Aggarwal, P. and Welleck, S. (2025). L1: Controlling how long reasoning model thinks with reinforcement learning. Anthropic (2025). Claude 4 (opus 4 and sonnet 4) [Hybrid reasoning large language models]. Released May 22, 2025; available via Anthropic API, Claude.ai Pro/Max/Team/Enterprise, Amazon Bedrock, Google Cloud Vertex AI. Opus 4: state-of-the-art coding and long-horizon agentic reasoning with 200K-token context window; Sonnet 4: efficient hybrid reasoning suited for assistants and customer support. Arora, D. and Zanette, A. (2025). Training language models to reason efficiently. arXiv preprint arXiv:2502.04463. Bai, F., Min, Y., Zhang, B., Chen, Z., Zhao, W. X., Fang, L., Liu, Z., Wang, Z., and Wen, J.-R. (2025). Towards effective code-integrated reasoning. arXiv preprint arXiv:2505.24480. Chen, K., Ren, Y., Liu, Y., Hu, X., Tian, H., Xie, T., Liu, F., Zhang, H., Liu, H., Gong, Y., et al. (2025). xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651. Chen, L., Liang, Z., Wang, X., Liang, J., Xiao, Y., Wei, F., Chen, J., Hao, Z., Han, B., and Wang, W. (2024a). Teaching large language models to express knowledge boundary from their own signals. Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. (2023). Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. (2024b). Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. DeepSeek-AI (2025). Deepseek-v3.1: Hybrid thinking and non-thinking modes. Du, X., Yao, Y., Ma, K., Wang, B., Zheng, T., Zhu, K., Liu, M., Liang, Y., Jin, X., Wei, Z., Zheng, C., Deng, K., Jia, S., Jiang, S., Liao, Y., Li, R., Li, Q., Li, S., Li, Y., Li, Y., Ma, D., Ni, Y., Que, H., Wang, Q., Wen, Z., Wu, S., Xing, T., Xu, M., Yang, Z., Wang, Z. M., Zhou, J., Bai, Y., Bu, X., Cai, C., Chen, L., Chen, Y., Cheng, C., Cheng, T., Ding, K., Huang, S., Huang, Y., Li, Y., Li, Y., Li, Z., Liang, T., Lin, C., Lin, H., Ma, Y., Pang, T., Peng, Z., Peng, Z., Qi, Q., Qiu, S., Qu, X., Quan, S., Tan, Y., Wang, Z., Wang, C., Wang, H., Wang, Y., Wang, Y., Xu, J., Yang, K., Yuan, R., Yue, Y., Zhan, T., Zhang, C., Zhang, J., Zhang, X., Zhang, X., Zhang, Y., Zhao, Y., Zheng, X., Zhong, C., Gao, Y., Li, Z., Liu, D., Liu, Q., Liu, T., Ni, S., Peng, J., Qin, Y., Su, W., Wang, G., Wang, S., Yang, J., Yang, M., Cao, M., Yue, X., Zhang, Z., Zhou, W., Liu, J., Lin, Q., Huang, W., and Zhang, G. (2025). Supergpqa: Scaling llm evaluation across 285 graduate disciplines. Erol, M. H., El, B., Suzgun, M., Yuksekgonul, M., and Zou, J. (2025). Cost-of-pass: An economic framework for evaluating language models. arXiv preprint arXiv:2504.13359. Fatemi, M., Rafiee, B., Tang, M., and Talamadupula, K. (2025). Concise reasoning via reinforcement learning. arXiv preprint arXiv:2504.05185. Feng, J., Huang, S., Qu, X., Zhang, G., Qin, Y., Zhong, B., Jiang, C., Chi, J., and Zhong, W. (2025). Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. (2024). Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468. Gao, J., Fu, W., Xie, M., Xu, S., He, C., Mei, Z., Zhu, B., and Wu, Y. (2025). Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. He, Y., Ding, X., Cai, B., Zhang, Y., Xiong, K., Sun, Z., Qin, B., and Liu, T. (2025). Self-route: Automatic mode switching via capability estimation for efficient reasoning. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. (2020). Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., et al. (2024). Metagpt: Meta programming for multi-agent collaborative framework. In International Conference on Learning Representations, ICLR. Hu, M., Zhou, Y., Fan, W., Nie, Y., Xia, B., Sun, T., Ye, Z., Jin, Z., Li, Y., Chen, Q., Zhang, Z., Wang, Y., Ye, Q., Ghanem, B., Luo, P., and Li, G. (2025). Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720. Jiang, L., Wu, X., Huang, S., Dong, Q., Chi, Z., Dong, L., Zhang, X., Lv, T., Cui, L., and Wei, F. (2025). Think only when you need with large hybrid-reasoning models. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. (2023). Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. (2025a). Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Jin, Y., Xu, K., Li, H., Han, X., Zhou, Y., Li, C., and Bai, J. (2025b). Reveal: Self-evolving code agents via iterative generationverification. arXiv preprint arXiv:2506.11442. Kang, Y., Sun, X., Chen, L., and Zou, W. (2024). C3ot: Generating shorter chain-of-thought without compromising effectiveness. Kumar, A., Roh, J., Naseh, A., Karpinska, M., Iyyer, M., Houmansadr, A., and Bagdasarian, E. (2025). Overthink: Slowdown attacks on reasoning llms. arXiv preprint arXiv:2502.02542. Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. (2023). Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., et al. (2025a). Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Li, W., Lin, J., Jiang, Z., Cao, J., Liu, X., Zhang, J., Huang, Z., Chen, Q., Sun, W., Wang, Q., et al. (2025b). Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl. arXiv preprint arXiv:2508.13167. Li, X., Jin, J., Dong, G., Qian, H., Zhu, Y., Wu, Y., Wen, J.-R., and Dou, Z. (2025c). Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Li, X., Zou, H., and Liu, P. (2025d). Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383. Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. (2023). Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477. Liu, T., Guo, Q., Hu, X., Jiayang, C., Zhang, Y., Qiu, X., and Zhang, Z. (2024). Can language models learn to skip steps? Lou, C., Sun, Z., Liang, X., Qu, M., Shen, W., Wang, W., Li, Y., Yang, Q., and Wu, S. (2025). Adacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning. arXiv preprint arXiv:2505.11896. Lu, R., Hou, Z., Wang, Z., Zhang, H., Liu, X., Li, Y., Feng, S., Tang, J., and Dong, Y. (2025). Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl. arXiv preprint arXiv:2509.10446. Mai, X., Xu, H., Wang, W., Zhang, Y., Zhang, W., et al. (2025). Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. (2023). Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Munkhbat, T., Ho, N., Kim, S. H., Yang, Y., Kim, Y., and Yun, S.-Y. (2025). Self-training elicits concise reasoning in large language models. of America (MAA), M. A. (2024). American invitational mathematics examination (aime) 2024. Competitive mathematics examination. of America (MAA), M. A. (2025). American invitational mathematics examination (aime) 2025. Competitive mathematics examination. OpenAI (2025a). Gpt-5 [large language model]. Released August 7, 2025; available via OpenAI API and ChatGPT. multimodal large language model featuring system-of-models architecture with router-based model selection. 15 OpenAI (2025b). Introducing GPT-4.1 in the API. https://openai.com/index/gpt-4-1/. Model release announcement. OpenAI (2025c). Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/. Model release announcement (o3). Pang, X., Tang, S., Ye, R., Du, Y., Du, Y., and Chen, S. (2025). Browsemaster: Towards scalable web browsing via tool-augmented programmatic agent pair. arXiv preprint arXiv:2508.09129. Penedo, G., Lozhkov, A., Kydlíˇcek, H., Allal, L. B., Beeching, E., Lajarín, A. P., Gallouédec, Q., Habib, N., Tunstall, L., and von Werra, L. (2025). Codeforces. https://huggingface.co/datasets/open-r1/codeforces. Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. (2025). Humanitys last exam. arXiv preprint arXiv:2501.14249. Qian, C., Acikgoz, E. C., He, Q., Wang, H., Chen, X., Hakkani-Tür, D., Tur, G., and Ji, H. (2025). Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958. Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., et al. (2023). Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. (2024). Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismäki, E. (2025). smolagents: smol library to build great agentic systems. https://github.com/huggingface/smolagents. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shi, D., Cao, J., Chen, Q., Sun, W., Li, W., Lu, H., Dong, F., Qin, T., Zhu, K., Yang, M., Yang, J., Zhang, G., Liu, J., Zhang, C., Wang, J., Jiang, Y. E., and Zhou, W. (2025). Taskcraft: Automated generation of agentic tasks. Sui, Y., Chuang, Y.-N., Wang, G., Zhang, J., Zhang, T., Yuan, J., Liu, H., Wen, A., Zhong, S., Chen, H., et al. (2025). Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Tang, X., Qin, T., Peng, T., Zhou, Z., Shao, D., Du, T., Wei, X., Xia, P., Wu, F., Zhu, H., et al. (2025). Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint arXiv:2507.06229. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025). Webshaper: Agentically data synthesizing via information-seeking formalization. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. (2025a). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. (2025b). Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Team, Q. (2025). Qwq-32b [Reasoning Large Language Model]. Released March 5, 2025; available via Hugging Face, ModelScope, and Qwen Chat. 32B-parameter reasoning model using reinforcement learning to rival state-of-the-art models like DeepSeek-R1. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. (2024). Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. (2025a). Browsecomp: simple yet challenging benchmark for browsing agents. Wei, Y., Yu, X., Weng, Y., Pan, T., Li, A., and Du, L. (2025b). Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836. Wu, H., Yao, Y., Liu, S., Liu, Z., Fu, X., Han, X., Li, X., Zhen, H.-L., Zhong, T., and Yuan, M. (2025a). Unlocking efficient long-to-short llm reasoning with model merging. arXiv preprint arXiv:2503.20641. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Fu, G., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025b). Webdancer: Towards autonomous information seeking agency. Xia, H., Leong, C. T., Wang, W., Li, Y., and Li, W. (2025). Tokenskip: Controllable chain-of-thought compression in llms. 16 Xie, Z., Wu, Q., Yu, C., Zhuang, C., and Gu, J. (2025). Profile-aware maneuvering: dynamic multi-agent system for robust gaia problem solving by aworld. Xue, Z., Zheng, L., Liu, Q., Li, Y., Zheng, X., Ma, Z., and An, B. (2025). Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. (2025a). Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. (2024). Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yang, Q., Ni, B., Xiang, S., Hu, H., Peng, H., and Jiang, J. (2025b). R-4b: Incentivizing general-purpose auto-thinking capability in mllms via bi-mode annealing and reinforce learning. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018). Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380. Association for Computational Linguistics. Yeo, E., Tong, Y., Niu, M., Neubig, G., and Yue, X. (2025). Demystifying long chain-of-thought reasoning in llms. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. (2025). Dapo: An open-source llm reinforcement learning system at scale. Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. (2025). Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Zeng, L., Zhong, L., Zhao, L., Wei, T., Yang, L., He, J., Cheng, C., Hu, R., Liu, Y., Yan, S., Fang, H., and Zhou, Y. (2024). Skywork-math: Data scaling laws for mathematical reasoning in large language models. Zeng, Z., Watson, W., Cho, N., Rahimi, S., Reynolds, S., Balch, T., and Veloso, M. (2023). Flowmind: automatic workflow generation with llms. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages 7381. Zhang, D., Zhao, Y., Wu, J., Li, B., Yin, W., Zhang, L., Jiang, Y., Li, Y., Tu, K., Xie, P., et al. (2025a). Evolvesearch: An iterative self-evolving search agent. arXiv preprint arXiv:2505.22501. Zhang, J., Lin, N., Hou, L., Feng, L., and Li, J. (2025b). Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417. Zhang, J., Xiang, J., Yu, Z., Teng, F., Chen, X., Chen, J., Zhuge, M., Cheng, X., Hong, S., Wang, J., et al. (2024). Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Zheng, Y., Fu, D., Hu, X., Cai, X., Ye, L., Lu, P., and Liu, P. (2025). Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Zhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y., Cotterell, R., and Sachan, M. (2023a). Recurrentgpt: Interactive generation of (arbitrarily) long text. Zhou, W., Jiang, Y. E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., Zhu, S., Chen, J., Zhang, W., Tang, X., Zhang, N., Chen, H., Cui, P., and Sachan, M. (2023b). Agents: An open-source framework for autonomous language agents. Zhou, W., Ou, Y., Ding, S., Li, L., Wu, J., Wang, T., Chen, J., Wang, S., Xu, X., Zhang, N., Chen, H., and Jiang, Y. E. (2024). Symbolic learning enables self-evolving agents. Zhu, H., Qin, T., Zhu, K., Huang, H., Guan, Y., Xia, J., Yao, Y., Li, H., Wang, N., Liu, P., et al. (2025). Oagents: An empirical study of building effective agents. arXiv preprint arXiv:2506.15741."
        },
        {
            "title": "A Prompts",
            "content": "A.1 System Prompt SYSTEM PROMPT OF A2FM You are required to solve the task by using one of the three agent options: agentic_agent, reasoning_agent, and instant_agent. 1. Agent Options: (a) agentic_agent: choose this agent if the task needs to search and crawl real-world / factual information (such as news and data) or executing code (such as programming tasks, data processing or statistics). (b) reasoning_agent: choose this agent if the task requires complex logical reasoning (such as mathematical proofs, multi-step problem solving) and causal analysis. (c) instant_agent: use this agent for simple tasks needing no real-world info, code, or complex reasoning. Instead, just basic knowledge or brief responses. 2. Trajectory Formulation: (a) You should first predict one of the three agents above within the function <classification> ... </classification>. (b) Then you should formulate your thinking and processing trajectory according to the rule of the agent you choose: i. agentic_agent rule: A. Objective: Your core goal is to systematically solve user-assigned tasks by: Decomposing the task into clear goals & paths. Executing tools purposefully and efficiently. Advancing all goals in parallel, while keeping each goals paths sequential. Tracking progress with summaries. Delivering final confirmed answer only when all goals are resolved. B. Execution Requirements: Follow logical order of functions/tools. Parallelize independent goals; within each goal, execute paths sequentially as fallbacks. Each step must include: thinking (before you execute tools, why this tool/path is chosen). <tool_call> execution (with correct parameters). Use results from observations to refine next actions. Ensure no redundant tool calls (dont repeat identical queries). Never assume goal is completed without explicit verification. Continue advancing all goals until they are resolved. C. Functions: <plan> Function: Role: Decompose the original task into goals and execution paths. Rules: - 15 parallelizable goals. - Each goal has 15 paths, executed sequentially as fallback options. - Define success criteria for each path. Timing: Only the first step. Format Example: <plan> ## Goal 1: [Name] - Path 1.1: [Approach] - Success: [Criteria] - Path 1.2: [Approach] - Success: [Criteria] ## Goal 2: [Name] - Path 2.1: [Approach] - Success: [Criteria] </plan> 18 <summary> Function: Role: Recap execution status and decide next actions. Content: - Plan summary (original goals/paths). - Execution status for each goal: Completed / In Progress / Blocked. - Path analysis (which worked, which failed). - Next steps: specify which sub-paths to run in parallel. Timing: Every several steps, occurs when there are enough actions to summarize. Example: <summary> ## Plan Summary [Brief recap of goals] ## Execution Status ### Goal 1: [Status] - Path Analysis: [...] ### Goal 2: [Status] - Path Analysis: [...] ## Next Parallel Sub-Paths - Goal 1: Path 1.2 - Goal 2: Path 2.1 </summary> <tool_call> Tool: Role: Execute tools to advance goals. - web_search: one {'id': xxx, 'name': 'web_search', 'arguments': {'query': 'xxx'}} parameter: (search query only has it statement). Example: - crawl_page: {'id': xxx, 'name': 'crawl_page', 'arguments': {'url': 'xxx', 'query': 'xxx'}} url (valid link) and query (info to extract). it has two parameters: Example: code_execute: - (Markdown one {'id': xxx, 'name': 'code_execute', 'arguments': {'code': 'xxx'}} parameter: code only has it snippet). Example: Rules: - Use 110 tools per step (each targeting distinct task part). - Each tool call must have complete, valid parameters. - Always prefer verifying accuracy with crawl_page after web_search. Timing: All steps except <plan>, <summary>, and <answer>. <answer> Function: Role: Deliver the final confirmed answer. Rules: - Only after all goals are resolved. - Must consolidate results across all goals. - Answer language must match task language. Format Example: <answer> [Final Answer Content] </answer> D. Execution Rules (Critical): Parallel Goals, Sequential Paths No Early Termination Result Verification Parallel Functions with Limited workers Final Answer Condition ii. reasoning_agent rule: A. Trajectory: 19 Reasoning Phase: Output <reasoning>...</reasoning> with detailed steps (>1000 words). Answer Phase: Present the final conclusion within <answer>...</answer>. B. Detailed Function Specifications: <reasoning> Function <answer> Function C. Notes: Do not return any other functions or tools. Output sequence is always reasoning then answer. Reasoning must exceed 1000 words. iii. instant_agent Specification: A. Objective: Rapidly solve tasks without tool usage or complex reasoning. Provide clear and relevant answers. B. Detailed Function Spec: <answer> Function only. Executed immediately, no planning or tool-calling. C. Notes: Must not return other functions or tools. Entire trajectory under 300 words. 3. Important Tips: (a) You should obey the rule of the agent option you choose. (b) Do not give an answer easily unless you are absolutely sure. The answer should be as concise as possible and avoid detailed descriptions. For example, <answer>Beijing</answer>. A.2 Summary Prompt CASE TRAJECTORY OF POST-RL MODEL ON SAME QUESTION ABOVE. Target: Extract all content from web page that matches specific web search query, ensuring completeness and relevance. (No response/analysis required.) web search query: ... Clues and ideas: ... Searched Web Page: ... Important Notes: - Summarize all content (text, tables, lists, code blocks) into concise points that directly address the query and clues, and ideas. - Preserve and list all relevant links ([text](url)) from the web page. - Summarize in three points: web search query-related information, clues and ideas-related information, and relevant links with descriptions. - If no relevant information exists, just output \"No relevant information.\""
        },
        {
            "title": "B Case Study",
            "content": "CASE TRAJECTORY OF POST-SFT MODEL ON EASY QUESTION. Question: How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need? This question asks for specific architectural comparison (number of blocks/layers in BERT Base vs. the Transformer in Attention is All You Need), which requires precise factual lookup about model definitions. Because it needs accurate, Classification 20 up-to-date model parameter information rather than simple common sense, the toolcalling_agent which can query external sources is the appropriate choice. <classification> toolcalling_agent </classification> First tool call round Now its step 1, need to make plan. <plan> Goal 1: Determine the number of encoder blocks (layers) in BERT Base Path 1.1: Search authoritative BERT paper and official codebase - Core approach: Use web_search to locate the BERT paper (Devlin et al., 2018) and the Hugging Face or official BERT model repository, then crawl_page to extract the explicit encoder-layer count from the papers experimental setup or model configuration file. - Success: direct citation or model config line (e.g., num_hidden_layers: X) explicitly stating the encoder block count for BERT Base is extracted and saved. Path 1.2: Query popular NLP model resources and documentation - Core approach: Use web_search to find Hugging Face model card, PyTorch/Transformer documentation, or TensorFlow model card for BERT Base, then crawl_page to extract the stated number of encoder blocks. - Success: At least one high-quality model resource (model card or documentation) explicitly lists the encoder block count for BERT Base. Path 1.3: Inspect canonical model configuration code (fallback) - Core approach: Use web_search to locate the canonical configuration file or constructor in framework (e.g., bert-base-uncased) and crawl_page or code_execute to read the model config and extract the layer count programmatically. - Success: The layer count is programmatically read from the model config and matches the value from Path 1.1 or 1.2. Goal 2: Determine the number of encoder blocks (layers) in the Transformer architecture from \"Attention is All You Need\" Path 2.1: Locate and extract from the original paper - Core approach: Use web_search to find the original paper (Vaswani et al., 2017) and crawl_page to extract the encoder layer count from the experimental setup or model configuration section (e.g., Table/sections describing the base model). - Success: direct citation or explicit statement from the paper that gives the encoder block count for the base Transformer model is extracted. Path 2.2: Consult authoritative implementations and model cards - Core approach: Use web_search to find official or widely-used implementations (e.g., TensorFlow official models, Hugging Face transformer-base if available, or the original repo) and crawl_page to extract the encoder block count. - Success: At least one authoritative implementation explicitly lists the encoder block count for the base Transformer model from the paper. Path 2.3: Verify via secondary literature or reviews - Core approach: Use web_search to find reviews, tutorials, or survey papers that summarize the architecture of Attention is All You Need and extract the encoder layer count. - Success: Secondary sources corroborate the encoder block count from the primary paper and provide consistent numbers. Goal 3: Compute the difference (BERT Base encoder blocks minus Transformer base encoder blocks) and verify accuracy Path 3.1: Direct numeric subtraction and cross-check 21 - Core approach: After Goals 1 and 2 return values, compute difference = (BERT Base encoder blocks) - (Transformer base encoder blocks). Verify consistency across at least two independent sources per model. - Success: The computed difference is produced and matches across at least two independent sources for each model. Path 3.2: Programmatic verification using model configs - Core approach: Use code_execute to load model configs (or synthetic config files) for BERT Base and the base Transformer and programmatically extract the encoder layer counts, then compute the difference. - Success: Programmatic extraction yields the same layer counts and the computed difference equals the result from Path 3.1. Path 3.3: Cite final difference with provenance - Core approach: Prepare concise statement of the difference and list the provenance (paper URLs, model card links) that support the numbers used. - Success: short, unambiguous answer stating how many more blocks are in BERT Base than in the base Transformer, with at least two cited sources (one per model). </plan> Now its step 2, need to make tool_calls. will advance both goals in parallel. For Goal 1 (BERT Base encoder blocks) the primary path is to locate the BERT paper and its model config; for Goal 2 (Transformer base encoder blocks) the primary path is to locate the Attention is All You Need paper and its model config. Ill perform web searches for each paper/model in parallel to find authoritative sources that state the encoder layer counts. After receiving those search results Ill crawl the specific pages to extract the explicit number of encoder blocks/layers for each model. <tool_call> {\"name\": \"web_search\", \"arguments\": {\"query\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding number of encoder layers\"}} {\"name\": \"web_search\", \"arguments\": {\"query\": \"Attention Is All You Need number of encoder layers base model\"}} </tool_call> <tool_response> Results for Tool Call 1 Query: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding number of encoder layers Total Results: 5 1. Source: arXiv:1810.04805 (J. Devlin et al., 2018, Cited by 143,290) Key Content: Mentions BERT as pre-trained deep bidirectional model, with fine-tuning via an additional output layer (encoder layer count not explicitly stated here). 2. Source: ACL Anthology: N19-1423 (J. Devlin et al., 2019, Cited by 143,290) Key Content: Similar to the arXiv version; focuses on fine-tuning performance, no explicit encoder layer count. 3. Source: Medium Article Key Content: Explicitly states: BERT-Base has 12 transformer layers, 12 attention heads, and 110 million parameters; BERT-Large has 24 transformer layers. 4. Source: Semantic Scholar Key Content: Summarizes BERTs pre-training of bidirectional representations, no explicit encoder layer count. 5. Source: Paperspace Blog Key Content: Introduces BERTs improvement on fine-tuning techniques, no explicit encoder layer count. 22 Results for Tool Call 2 Query: Attention Is All You Need number of encoder layers base model Total Results: 5 1. Source: NeurIPS 2017 Paper (A. Vaswani et al., Cited by 192,399) Key Content: Explicitly states: The encoder is composed of stack of = 6 identical layers (each with multi-head self-attention and feed-forward sublayers). 2. Source: arXiv:1706.03762 (Jun 12, 2017) Key Content: Introduces the Transformer as replacement for recurrent/convolutional models, no explicit encoder layer count. 3. Source: Wikipedia Key Content: Incorrectly mentions encoder and decoder were both 8 layers of bidirectional LSTM (misrepresents Transformers structure; original paper uses 6 encoder layers). 4. Source: Medium Article (3 months ago) Key Content: Confirms the original papers architecture: The encoder stack contains 6 identical layers, and the decoder stack also contains 6 layers. 5. Source: Plain English AI Blog (Jul 12, 2025) Key Content: States: The encoder has 6 layers, each consisting of 2 sublayers (multi-head self-attention + feed-forward network). </tool_response> Second tool call round will advance both goals in parallel using the primary path for each: verify the encoder layer counts directly from the original papers (BERT and Attention Is All You Need) by crawling the PDFs found in the web_search results. Ill extract the explicit statements for BERT Bases encoder blocks and for the base Transformers encoder layers, then compute the difference. If the PDFs dont contain the explicit counts, Ill use fallback paths (model cards or official repos) in subsequent steps, but first Ill attempt direct extraction from the paper PDFs. <tool_call> {\"name\":\"crawl_page\", \"arguments\": {\"url\": \"https://arxiv.org/pdf/1810.04805.pdf\", \"query\": \"number of encoder layers BERT Base, number of hidden layers, BERT-base encoder blocks\"}} {\"name\":\"crawl_page\", \"arguments\": {\"url\": \"https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\", \"query\": \"Encoder: The encoder is composed of stack of = 6 identical layers\"}} </tool_call> <tool_response> Results for Tool Call Page 1 [https://arxiv.org/pdf/1810.04805.pdf] Summary: - Model architecture: BERT is multi-layer bidirectional Transformer encoder. The paper denotes the number of layers (i.e., Transformer blocks) as and the hidden size as H. - BERTBASE (commonly BERT-Base) specifications: - Number of encoder layers / Transformer blocks (L): 12 - Hidden size (H): 768 - Number of self-attention heads (A): 12 - Feed-forward/filter size: 4H = - Total parameters: 110M - (For context) BERTLARGE: = 24, = 1024, = 16, total parameters 340M. - Note: layers in the paper explicitly refers to Transformer encoder blocks (i.e., encoder layers). 23 Results for Tool Call 2 Page 1 [https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf] Summary: - Exact statement: \"Encoder: The encoder is composed of stack of = 6 identical layers.\" (Section 3.1) - Encoder layer structure: - Each of the = 6 layers has two sub-layers: 1. Multi-head self-attention mechanism. 1. Position-wise fully connected feed-forward network. - Residual connections are applied around each sub-layer, followed by layer normalization: - Output form: LayerNorm(x + Sublayer(x)). - All sub-layers and embedding layers produce outputs of dimension dmodel = 512 to facilitate residual connections. - Feed-forward sub-layer details (applies to each encoder layer): - FFN formula: FFN(x) = max(0, xW1 + b1)W2 + b2 - Input/output dimensionality: dmodel = 512 - Inner layer dimensionality: dff = 2048 - Can be seen as two 1 1 convolutions (position-wise, applied identically at every position). - Attention used in encoder: - Multi-head self-attention (encoder self-attention): queries, keys, values all come from the previous encoder layers outputs; each position may attend to all positions in the previous layer. - Multi-head attention configuration used in the paper: - = 8 heads - dk = dv = dmodel/h = 64 - Related decoder note (parallel statement): - \"Decoder: The decoder is also composed of stack of = 6 identical layers.\" - Decoder layers include the two sub-layers above plus third encoder-decoder multi-head attention sub-layer; decoder self-attention is masked to prevent attending to subsequent positions. - Embeddings and dimension notes relevant to encoder context: - Input tokens are embedded to vectors of dimension dmodel = 512. - Positional encodings (sinusoidal) of dimension dmodel are added to embeddings at the bottoms of encoder/decoder stacks. - Model configurations  (Table 3)  confirming = 6 for both base and big variants: - base: = 6, dmodel = 512, dff = 2048, = 8 - big: = 6, dmodel = 1024, dff = 4096, = 16 - (These points summarize all content in the paper directly related to: \"Encoder: The encoder is composed of stack of = 6 identical layers\" and the immediate technical details of those encoder layers.) </tool_response> Answer advanced both goals in parallel: (1) determine the number of encoder blocks (layers) in BERT Base, and (2) determine the number of encoder blocks in the Transformer base model from Attention is All You Need. used crawl_page to extract authoritative statements from the original papers: the BERT arXiv PDF (Devlin et al., 2018) explicitly states BERT-Base has = 12 encoder layers, and the Attention is All You Need NeurIPS PDF (Vaswani et al., 2017) states the encoder is composed of stack of = 6 identical layers. With both verified, computed the difference: 12 - 6 = 6. will now provide the final concise answer with provenance. 24 Answer <answer> BERT Base has 6 more encoder blocks/layers than the base Transformer encoder from Attention is All You Need (BERT-Base: 12 encoder layers; Attention is All You Need base encoder: = 6 encoder layers). Sources: BERT paper (Devlin et al., 2018; arXiv:1810.04805) and Attention is All You Need (Vaswani et al., 2017; NeurIPS 2017 PDF). </answer> CASE TRAJECTORY OF POST-RL MODEL ON SAME QUESTION ABOVE. Question: How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need? Classification This is straightforward factual/comparison question about known transformer architectures (BERT base vs. original Attention is All You Need encoder). It doesnt require web search or code execution; the information is common knowledge. Therefore the default_agent, which handles basic factual queries, is appropriate. <classification> default_agent </classification> recall that BERT base has 12 transformer encoder layers, while the original Attention is All You Need paper used 6 encoder layers for their base model. So the difference is 12 - 6 = 6 layers. Answer Answer <answer> 6 more layers </answer>"
        },
        {
            "title": "C Template",
            "content": "Table 3 Templates used in training Instant mode <classification> instant </classification> Thinking mode <classification> reasoning </classification> Agentic mode <classification> agentic </classification> <answer> {response} </answer> <reasoning> {reasoning_content} </reasoning> <plan> {plan_content} </plan> <answer> {response} </answer> <tool_call> {tool_dicts} </tool_call> <tool_response> {tool_obs} <tool_response> <summary> {summary_content} </summary> <answer> {response} </answer>"
        }
    ],
    "affiliations": [
        "DeepSeek-AI",
        "OpenAI"
    ]
}