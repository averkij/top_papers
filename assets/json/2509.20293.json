{
    "paper_title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity",
    "authors": [
        "Benjamin Feuer",
        "Chiung-Yi Tseng",
        "Astitwa Sarthak Lathe",
        "Oussama Elachqar",
        "John P Dickerson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 9 2 0 2 . 9 0 5 2 : r WHEN JUDGMENT BECOMES NOISE: HOW DESIGN FAILURES IN LLM JUDGE BENCHMARKS SILENTLY UNDERMINE VALIDITY Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John Dickerson"
        },
        {
            "title": "ABSTRACT",
            "content": "LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional, groundtruthbased benchmarks. We argue that, without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of judges overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: e.g., unexplained variance exceeding 90% for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md"
        },
        {
            "title": "INTRODUCTION",
            "content": "As the world grows increasingly saturated with AI-supplemented and AI-generated content, traditional evaluation and judgment mechanisms are struggling to keep up, leading some to propose AI as the solution to its own problem Gillespie (2020). LLM judges promise rapid, scalable evaluation of complex, open-ended tasks; far from being purely theoretical concern, the deployment of LLMs in real settings with real stakes is well underway. For instance, the 2026 AAAI Conference added an LLM judge (AI-Powered Peer Review System) to the panel of reviewers for its scientific submissions, with mixed results AAAI (2025). But how far can we trust LLM judges? Are they actually capable of delivering on their promise? Notwithstanding their complexity, these questions are ones that the benchmarking and evaluation research communities are duty-bound to address, and in recent years, there have been many attempts to do so Santurkar et al. (2024); Mazeika et al. (2024); Wu et al. (2024); Zhou et al. (2024). Because ground truth in open-ended judgments is difficult and expensive to obtain, many benchmarks of LLM judges utilizes LLM judges themselves. In the last few years, such benchmarks have been widely deployed in the alignment and reinforcement learning literature in particular, and their scores have been used as guiding signal for numerous accepted conference submissions Feuer et al. (2024). From these observations, we can deduce that one promising avenue for evaluating LLM judges is conducting meta-analyses on the benchmarks that utilize them. One particular design choice, the selection and technical deployment of the LLM judge, has been extensively critiqued and analyzed, which has in turn lead to important reforms and revisions in best practices Santurkar et al. (2024); Wu et al. (2023; 2024); Feuer et al. (2024). Other key design decisions in these benchmarks, however, remain understudied. What makes judgment rubric valid? What kind of metrics can benchmark designers employ to ensure that their judgment criteria are meaningfully distinct in benchmark space? Are the questions and baseline models in the benchmark suitably selected to produce meaningful comparisons? If the questions 1 and rubric are inadvertently mismatched, the measure may not be meaningful for some criteria. Last but not least, what metrics should we use to reliably score LLM-judged benchmarks? Numeric scores are an option, but they tend to transfer unreliably between judges without calibration. ELOstyle comparisons enforce transitive preferences and exaggerate distinctions, but in many real-world cases, model preference is non-transitive and the best we can hope for is to know what we dont know, appropriately grounding benchmarks in uncertainty. Figure 1: The majority of true judgment variance has no known cause. On the Arena-Hard-Auto benchmark, with rubric specifying 5 judgment criteria, we find that across four judges and two settings (different cohorts of models to be compared), approximately 55% of variance, on average, is unexplained either by linear or taylor-series polynomial factor analysis on the rubric criteria. After ELO transformation, the linear model explains 100% of observed variance, indicating that, by enforcing transitivity, ELO hides true latent uncertainty in multi-factor analysis. In this paper, we propose set of novel diagnostic metrics which aim to automate the assessment of common confounds in LLM-judged benchmarks, such as whether the data selected and models surveyed form an adequate artifact for meaningful analysis, and whether the selected judgment metrics effectively capture uncertainty. We then conduct large-scale empirical analysis on popular public benchmark using our metrics. From this, we discover that popular LLM-judged benchmarks contain severe and previously undocumented failure modes; many widely used LLM judges do not faithfully implement assigned schemas, popular benchmarks cannot produce statistically or practically significant measures the things their rubrics claim, and post-processing (e.g., ELO/BradleyTerry rankings) mask this reality, producing nice-looking rankings that can fail to reflect true preference. We hope that this work induces transition toward reliability-aware benchmark design that foreground validity rather than appearance of stability."
        },
        {
            "title": "Contributions",
            "content": "Mechanisms: We introduce two novel diagnostic metrics for LLM judge benchmarks (1) Schematic adherence quantifies how well overall verdicts derive from factorwise rubric scores. (2) Psychometric validity aggregates internal consistency and discriminant validity to quantify the degree to which benchmarks design fits with its judgment rubric. Case study: Applying both mechanisms to Arena-Hard Auto (Li et al., 2024b), we uncover severe rubric incoherence and factor collapse across judges (e.g., 90% unexplained variance for DeepSeek-R1-32B; factor correlations 0.93 across criteria), and show that ELO-style aggregation collapses uncertainty into seemingly stable rankings. Guidance: Actionable design principles for LLM-judged benchmarks: tighten objectives, audit factor structure, report uncertainty, avoid aggregation that erases variance, and constrain scope to where judges exhibit validity."
        },
        {
            "title": "2 METHODS",
            "content": "Motivated by the desire to catalog and measure novel modes of uncertainty in LLM-judged benchmarks with multi-factor rubrics, in contrast with prior work, which has largely focused on quantifying statistical uncertainty or model uncertainty, here, we focus primarily on uncertainty introduced during the benchmark design process (via the judgment rubric, selection of questions, selection of metric, and selection of comparison models). 1. In the remainder of this section, we formalize the metrics employed throughout this paper to measure these factors. Schematic adherence. Informally, this measures the degree to which particular LLM judge Js overall judgments in particular benchmark setting can be explained as function of their percriteria (or factor-wise) judgments. In other words, if my benchmark claims to measure alignment and explicitly defines better alignment in the rubric as Pareto-dominance across five semantically defined factors such as style, conciseness, completeness, safety, and correctness, then we would expect our alignment ranking to prefer model over model IFF model Pareto-dominates on all five factors. Using factor analysis techniques, we can measure the degree to which this is true; if prefers to overall, does also rank above B, in equal proportion, across the factors, or are the factors combined in some detectable way to form the overall judgment Brown (2006)? Formally, let each judgment produce factor scores fi = (fi1, . . . , fik) and an overall verdict oi. We measure how much of oi is explained by the explicit schema via oi = β0 + oi = β0 + (cid:88) j=1 (cid:88) j=1 βjfij + ϵi, (linear) βjfij + (cid:88) j=1 βjjf 2 ij + (cid:88) j<ℓ βjℓfijfiℓ + ϵi, (polynomial) with goodness-of-fit R2 linear = 1 (cid:80) i(oi ˆolinear (cid:80) i(oi o)2 )2 , R2 poly = 1 (cid:80) i(oi ˆopoly (cid:80) i(oi o)2 )2 (1) (2) . (3) The schematic adherence score is R2 poly). Low values indicate verdicts deviate from the stated rubric. We also analyze integration patterns via weight disparity/entropy and context stability. Full formalization appears in Section B.1. schematic = max(R2 linear, R2 1The qualifier about multi-factor rubrics merits additional clarification; almost all LLM-judged benchmarks are implicitly multi-factored, in that they describe set of preferred characteristics, even if those characteristics are not always independently scored (e.g., the model should be helpful, honest and harmless). Even in the simplest possible case, e.g., score model higher if it is more harmless and lower it if is less harmless, the concept of harmlessness can implicitly be decomposed into different types and severities of harm, ranging from inadvertently offensive responses to illegal hate speech or incitements to violence and self-harm. Psychometric validity. Informally, this measures the degree to which an holistic benchmark setting (which includes the set of models used for comparison, the set of questions used to generate responses, the scoring system, and the choice of judge) tends to produce internally consistent signal that is meaningfully distinct across all factors in the rubric. This latter property we generally refer to as discriminant validity Cronbach (1951); Campbell & Fiske (1959). Our chosen measure is simple aggregate of internal consistency and discriminant validity: Cronbachs α for the former, and cross-loading ratio (CLR) with sigmoid normalization centered at 1.5, and HTMT for the latter, formally Rpsychometric = 1 3 (cid:16) 1 (cid:88) j= αj + 1 (cid:88) j=1 CLRnorm,j + (cid:0)1 2 k(k1)"
        },
        {
            "title": "HTMTij",
            "content": "(cid:1)(cid:17) , (cid:88) i<j (4) with sensitivity Senspsychometric = (cid:112)1 Rpsychometric score range. Full definitions are provided in Section B.2."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Benchmark. We study Arena-Hard Auto (Li et al., 2024b), popular LLM-judged benchmark built from 500 challenging Chatbot Arena queries, intended to approximate Arena preferences. It reports high separability and strong correlation with human rankings, making it an excellent candidate for robust meta-analysis. Judges. We evaluate four primary judge models: GPT-4o-mini, GPT-3.5-Turbo, QwQ-32B, and DeepSeek-R1-32B (Yang et al., 2024a; OpenAI et al., 2024; DeepSeek-AI et al., 2025). Rubric. Our rubric follows Li et al. (2024b); Feuer et al. (2024), eliciting factor-wise and overall scores on five criteriaCorrectness, Completeness, Safety, Conciseness, and Style, alongside an overall verdict. Judges: (i) rate vs. on each criterion with brief justification; (ii) reflect on criterion weights for the domain; (iii) issue final verdict label from [[A >> B]], [[A > B]], [[A = B]], [[B > A]], [[B >> A]]; (iv) default to ties when uncertain; (v) do not mix criteria. The full wording appears in Section C.6. Scoring. As in Li et al. (2024b); Feuer et al. (2024), we extract Likert-scaled (15) scores per judgment (one for each factor and an overall). Scores are computed relative to base model. In our approach, all six scores are computed during single forward pass. We ablate this choice in Section C.7 and find that, while rank ordering is preserved, the raw numerical scores can change significantly when factors are instead computed one-at-a-time. This form of prompt instability is promising area of future research in taxonomizing forms of LLM uncertainty. When we perform ELO conversions, we do so using the same approach as the original benchmark. However, the majority of our analysis is conducted on the raw judge scores in order to preserve information and properly calibrate uncertainty. In Section 4.1, we motivate this decision more thoroughly with an extended discussion of how ELO scores can collapse true uncertainty. Settings. In our work, Setting corresponds to complete tuple; {judge, rubric, question set, model set, model hparams, metric}. The settings we consider are described in detail in Section C; that section also details the hyperparameters we consider, primarily the use of thinking and non-thinking modes in judges, when available. Robustness procedures. To improve stability we use multiple imputation for incomplete judgments, Bonferroni correction for factor correlation tests, and 1000-iteration bootstrap resampling, following standard practice. We also report the deviation rates (how often answer extraction fails) for each judge, which results in default judgment of no preference being assigned; see Table 5."
        },
        {
            "title": "4 RESULTS",
            "content": "The majority of judgment variance has no known cause. Judge scores vary dramatically depending on many different factors; unfortunately, it seems that the judgment rubric, which should dominate variance, plays comparatively limited role in it. Across judges, we observe alarming lev4 els of unexplained variance in overall decisions from their factor-wise scores (see Table 1 and Figure 1). DeepSeek-R1-32B, an extremely popular open-weights model which has enjoyed largely positive reception from the academic and business community, explains as little as 6.8% of judgment variance. Closed-source models (GPT-4o-mini, GPT-3.5-Turbo) exhibit higher adherence than open-source reasoning models, yet still fall well short of producing consistent and reliable judgments. Adding explicit reasoning yields only modest improvements. Multi-factor semantic judgments tend to collapse into much smaller number of latent factors. Consistent with prior work, in most settings we consider, we find that factor-wise rank correlations are extremely high across most criteria (often > 0.93), indicating severe dimensionality collapse (Figure 2). New in this work, we show that factor loadings anticipate this issue; high crossloadings are present in most judges, and are fairly consistent in degree across judges, indicating poor discriminant validity (Figure 3). Factor-wise rank correlations are extremely high across criteria (often > 0.93), indicating severe dimensionality collapse (Figure 2). This finding undermines any claim these benchmarks would have towards discriminant validity; supposedly distinct criteria behave interchangeably, reducing the evaluation to near-unidimensional signal. This effect is apparent before the ELO transformation step, but is exaggerated by it; see Section 4.1 for more details on the latter. Loadings heatmaps corroborate this collapse with extensive cross-loading and weak separation between intended constructs (Figure 3). Figure 2: Most factors are highly correlated for most judges. In Setting 1, across all four judges, the average spearman rank correlation matrix shows high cross-factor correlations (> 0.93 for most pairs). This suggests factor collapse the inability of judges to meaningfully distinguish between semantically distinct rubric factors in the setting. Summary of schematic adherence by judge. Headline unexplained variance (1R2) from our analysis: GPT-4o-mini: 26.2%; GPT-3.5-Turbo: 44.6%; QwQ-32B (reasoning): 51.9%; QwQ32B (no reasoning): 60.060.6%; DeepSeek-R1-32B (reasoning): 70.8%; DeepSeek-R1-32B (no reasoning): 87.490.5%. These results indicate schema incoherence increases with open-source reasoning judges and when reasoning is disabled. 5 Figure 3: Diverse judges exhibit very similar latent factor loadings. Across four different LLM judges in benchmark Setting 2, the eigenvalues associated with factor weightings are highly similar; all indicate collapse of significance in the latent loadings. Psychometric validity. Aggregating internal consistency and discriminant validity into single index reveals substantial residual uncertainty across settings. The sensitivity interpretation implies that even repeated runs of the benchmark can vary meaningfully on 15 scale due to weak factor structure. Metric effects; ELO transformation and the appearance of stability. Arena-Hard Autos ELO/BradleyTerry transformation converts nuanced and non-transitive raw judgments into weighted win/loss outcomes and logistic probabilities. The transformation produces near-perfect ranking stability (e.g., R2 0.998) but does so at the cost of masking underlying judgment complexity (Figure 4), creating the illusion of robust ordering despite incoherent schemas upstream. See Section 4.1 for more comprehensive analysis of this effect. Figure 4: ELO-style aggregation compresses multi-dimensional, noisy judgments into apparently smooth rankings, masking upstream uncertainty. Ablations. Evaluating factors in isolation versus jointly changes absolute scores dramatically but preserves model ranking structure (Figure 5), indicating that the observed failures are not artifacts of the simultaneous-scoring protocol."
        },
        {
            "title": "4.1 LIMITATIONS OF RANKING SYSTEMS",
            "content": "The standard Arena-Hard-Auto methodology generates rankings using maximum likelihood estimation under Bradley-Terry model assumptions with differential weighting (strong preferences weighted 3 higher than weak preferences) and bootstrap confidence interval estimation (100 iterations). ELO scores are then converted to win-rate probabilities against baseline models, producing the ultimate ranking. Recent theoretical work has revealed fundamental limitations in ELO rating systems when applied to LLM evaluation. Wu et al. (2023) demonstrate that individual ELO computations exhibit high volatility and sensitivity to hyperparameters when applied to entities with constant skill levels like LLMs. The ratings are highly sensitive to comparison order and choice of hyperparameters, with desirable properties like transitivity not guaranteed without comprehensive pairwise comparison data. The recent work by Yang et al. (2024b) proves that estimated ratings become dependent on interaction patterns when transitivity assumptions are violated. Earlier work by Balduzzi et al. (2022) shows that ELO systems fail to extract transitive components even in elementary transitive games, while Aldous (2019) addresses unrealistic implicit assumptions about draws and game outcomes through the κ-ELO extension. These limitations include: (1) transitivity problems in non-transitive scenarios, (2) order dependence making results unreliable, (3) hyperparameter sensitivity, (4) unrealistic assumptions about game outcomes, and (5) cold start problems requiring substantial data (100+ attempts) for reliable estimates. 4.2 RANKING FAILURES IN ARENA-HARD AUTO The transformation from raw judgments in Arena-Hard Auto can induce fundamental changes in data structure that obscure evaluation uncertainty, as we show in Figure 4. The ELO-based ranking system converts nuanced 5-point scale judgments into binary win/loss outcomes with differential weighting (strong preferences weighted 3 higher). The process uses Maximum Likelihood Estimation under Bradley-Terry model assumptions, and applies logistic regression to model winning probabilities rather than score magnitudes. This results in seemingly stable rankings (R2 = 0.998) that mask underlying judgment complexity. The stability is achieved at cost, rendering multifaceted judgments essentially unipolar, filtering out systematic bias patterns that exceed noise thresholds and masking the substantial influence of implicit evaluation criteria."
        },
        {
            "title": "5 BACKGROUND",
            "content": "LLM-as-judge has become standard evaluation pattern for instruction following, safety, and general assistance (Huang et al., 2024; Zhou et al., 2024; Wu et al., 2024; Zheng et al., 2024). Despite promising headline agreement rates with humans, deeper analyses report only modest correlations and persistent construct validity concerns given the broad spectrum of tasks and criteria. Psychometric perspectives offer tools to probe reliability and validity: internal consistency (e.g., Cronbachs α (Cronbach, 1951)), discriminant validity (e.g., HTMT (Henseler et al., 2015)), and factor structure. However, most applications remain descriptive; formal guarantees and benchmarkoriented diagnostics are limited. Recent work highlights judge bias and brittleness. Bias taxonomies (e.g., CALM) surface numerous failure modes; preference leakage shows judges favor their own outputs; and pluralistic alignment suggests that average preference optimization obscures diversity. Practical protocols like Trust-or-Escalate add redundancy to recover human agreement guarantees under certain conditions (Pezeshkpour et al., 2024; Li et al., 2024a). Krumdick et al. (2025) produce human-annotated dataset containing correctness labels for 1,200 LLM responses and show that reference quality can 7 affect the performance of an LLM Judge. Santilli et al. (2025) show that even uncertainty quantification for such models is flawed, owing to mutual biaseswhen both UQ methods and correctness functions are biased by the same factorssystematically distorting evaluation. Guo et al. (2025) show that hallucination and incompleteness during the reasoning process thwarts robustness on mathematical tasks, and Chehbouni et al. (2025) argues that LLM Judges ability to act as proxies for human judgment, capability as evaluators, etc, may be overrated. Ranking-based aggregation (ELO/BradleyTerry) is common in community leaderboards, including Arena-Hard and ChatBot Arena (Li et al., 2024b). Theory and empirical studies point to volatility, order/interaction dependence, and hyperparameter sensitivity, with aggregation often erasing multidimensional uncertainty. Our work bridges these threads by providing benchmark-centric diagnosticsschematic adherence and psychometric validity indexthat (i) directly test whether overall judgments follow stated schemas and (ii) quantify the residual uncertainty due to weak factor structure. We position these as necessary preconditions for interpreting LLM-judged rankings."
        },
        {
            "title": "6 LIMITATIONS AND FUTURE WORK",
            "content": "While our analysis spans common judges and settings, several limitations remain. First, we focus on Arena-Hard Auto; generalization requires replication on other LLM-judged benchmarks and domains. Second, judge behavior can drift over time; periodic re-evaluation is necessary. Third, rubric content and phrasing matterour results reflect one widely used schema and template choices (Section C.6). Fourth, our psychometric index is deliberately simple and transparent; alternative formulations may yield complementary insights. Future work includes extending diagnostics to task-conditioned validity checks, principled rubric pruning, judge ensembling with uncertainty calibration, and protocols that explicitly optimize for adherence and discriminant validity."
        },
        {
            "title": "7 CONCLUSION",
            "content": "LLM-judged benchmarks can silently devolve into noise for variety of reasons. In this work, we demonstrate that many popular judges fail to implement rubrics as instructed, leading to one silent failure mode. Even when appropriate judges are chosen, changing the set of comparison models or the benchmark questions can trigger psychometric validity failures, rendering the overall comparison non-actionable. Finally, we show that mechanisms such as ELO ranking enforce transitivity and tend to collapse complex judgments into simplified and misleading rankings. Our case study on Arena-Hard Auto reveals severe schema incoherence, factor collapse, and post-aggregation uncertainty suppression. We encourage the community to adopt reliability-aware design principlestight objectives, factor structure audits, and transparent uncertainty reportingto restore validity to LLMjudged evaluation."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have, to the best of our ability, ensured that all experiments described in this paper are reproducible in principle. In order to facilitate this, we provide an anonymized source code repository containing everything needed to reproduce our experiments. The exception to this is any data that would break anonymity requirements; the data, such as the raw responses, model judgments and extended tables and figures, we will make public after the review period has concluded."
        },
        {
            "title": "LLM USE STATEMENT",
            "content": "In accordance with ICLR policy, the authors acknowledge the limited use of foundation models for generating code and LaTeX, rendering visualizations, polishing writing, and related work retrieval and discovery."
        },
        {
            "title": "REFERENCES",
            "content": "AAAI. AAAI launches AI-powered peer review assessment system. https://aaai.org/ aaai-launches-ai-powered-peer-review-assessment-system/, May 2025. Press release announcing pilot program for AAAI-26 conference. David Aldous. Understanding and pushing the limits of the elo rating algorithm. arXiv preprint arXiv:1910.06081, 2019. David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and Thore Graepel. On the limitations of elo: Real-world games are transitive, not additive. arXiv preprint arXiv:2206.12301, 2022. Timothy Brown. Confirmatory factor analysis for applied research. Guilford Press, New York, NY, 2006. Donald Campbell and Donald Fiske. Convergent and discriminant validation by the multitraitmultimethod matrix. Psychological Bulletin, 56(2):81105, 1959. doi: 10.1037/h0046016. Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, and Golnoosh Farnadi. Neither valid nor reliable? investigating the use of llms as judges. arXiv preprint arXiv:2508.18076, 2025. Lee Cronbach. Coefficient alpha and the internal structure of tests, volume 16. Springer, 1951. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, and John P. Dickerson. Style outweighs substance: Failure modes of llm judges in alignment benchmarking, 2024. URL https://arxiv.org/abs/2409.15268. Tarleton Gillespie. Content moderation, ai, and the question of scale. Big Data & Society, 7(2): 2053951720943234, 2020. doi: 10.1177/2053951720943234. URL https://doi.org/10. 1177/2053951720943234. 9 Dadi Guo, Jiayu Liu, Zhiyuan Fan, Zhitao He, Haoran Li, Yumeng Wang, and Yi Fung. Mathematical proof as litmus test: Revealing failure modes of advanced large reasoning models. arXiv preprint arXiv:2506.17114, 2025. Joseph Hair, William Black, Barry Babin, and Rolph Anderson. Multivariate data analysis. Cengage Learning, Boston, MA, 8th edition, 2019. Jorg Henseler, Christian Ringle, and Marko Sarstedt. new criterion for assessing discriminant validity in variance-based structural equation modeling. Journal of the Academy of Marketing Science, 43(1):115135, 2015. doi: 10.1007/s11747-014-0403-8. Jinwei Huang, Yiru Chen, Zhuohan Liu, et al. Leveraging llms as meta-judges for robust evaluation. arXiv preprint, 2024. Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, and Chris Tanner. No free labels: Limitations of llm-as-a-judge without human grounding, 2025. URL https://arxiv.org/ abs/2503.05061. Hui Li, Tianyu Zhang, Jiaqi Chen, et al. Limits to scalable evaluation at the frontier: Llm as judge for llm. arXiv preprint, 2024a. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024b. URL https://arxiv.org/abs/2406.11939. Mantas Mazeika, Long Phan, Xuwang Wang, et al. Safetyanalyst: Interpretable, transparent, and steerable llm safety evaluation. arXiv preprint, 2024. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, 10 Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Pouya Pezeshkpour, Estelle Sarkar, Evan Kraft, et al. Trust or escalate: Llm judges with provable guarantees for human agreement. arXiv preprint, 2024. Mikko Ronkko and Eunseong Cho. An updated guideline for assessing discriminant validity. Organizational Research Methods, 25(1):614, 2022. doi: 10.1177/1094428120968614. Andrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, and Sinead Williamson. Revisiting uncertainty quantification evaluation in language models: Spurious interactions with response length bias results. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 743759. Association for Computational Linguistics, 2025. doi: 10.18653/v1/2025.acl-short.60. URL http://dx.doi.org/10.18653/v1/2025.acl-short.60. Shibani Santurkar, Yann Chen, Esin Durmus, et al. How individual traits and language styles shape preferences for ai-generated content. arXiv preprint, 2024. Jeffrey Wu, Alice Chen, Robert Zhang, et al. Jetts: Evaluating llm judges as evaluators. arXiv preprint, 2024. Mervin Wu, Jiayu Tang, Zixuan Wu, Tong Zhang, et al. Elo uncovered: Robustness and best practices in language model evaluation. arXiv preprint arXiv:2311.17295, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Kevin Yang, Dan Chen, and Dan Klein. Elo ratings in the presence of intransitivity. arXiv preprint arXiv:2412.14427, 2024b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, et al. Judgebench: benchmark for evaluating llm-based judges. In International Conference on Machine Learning, 2024. Yizhong Zhou, Jiawei Xu, Kenton Lee, et al. Llmbar: Evaluating large language models as judges for instruction-following. arXiv preprint, 2024."
        },
        {
            "title": "A SCHEMATIC ADHERENCE COMPLETE RESULTS",
            "content": "Judge Model Linear R2 Polynomial R2 R2 Improvement % Unexplained GPT-4o-mini GPT-3.5-Turbo QwQ-32B (reasoning) QwQ-32B (no reasoning) DeepSeek-R1-32B (reasoning) DeepSeek-R1-32B (no reasoning) 0.703 0.518 0.459 0.3690.376 0.260 0.0680.101 0.738 0.554 0.481 0.3940.400 0.292 0.0950. Arena Hard Auto Rankings 0.998 1.000 0.035 0.037 0.022 0.025 0.032 0.026 0.002 26.2% 44.6% 51.9% 60.060.6% 70.8% 87.490.5% 0.0% Table 1: Comprehensive analysis of explained variance across judge models and configurations. Open-source reasoning models show dramatically higher unexplained variance than closed-source models."
        },
        {
            "title": "B METRICS",
            "content": "B.1 SCHEMATIC ADHERENCE Mathematical Formalization Let = {s1, s2, . . . , sm} denote the set of judgment samples, where each sample si contains factor scores fi = (fi1, fi2, . . . , fik) and an overall score oi. Definition 1 (Linear Schematic Model). The linear relationship between factor scores and overall judgment is modeled as: oi = β0 + (cid:88) j=1 βjfij + ϵi where βj represents the implicit weight assigned to factor j, and ϵi is the residual error. Definition 2 (Non-linear Schematic Model). To capture potential non-linear integration patterns, we extend to polynomial model: oi = β0 + (cid:88) j=1 βjfij + (cid:88) j=1 βjjf 2 ij + (cid:88) j<l βjlfijfil + ϵi This includes quadratic terms and factor interactions. Definition 3 (Context-Dependent Schematic Patterns). We perform question-wise clustering to identify context-dependent evaluation patterns. Let = {Q1, Q2, . . . , Qc} be the clusters of questions. For each cluster Qc, we compute cluster-specific weights: β(c) = arg min β oi β(c) 0 2 β(c) fij (cid:88) j= (cid:88) iQc 12 Definition 4 (Schematic Adherence Score). The schematic adherence is quantified through variance decomposition: schematic = max (cid:0)R2 R2 linear, polynomial (cid:1) where: R2 linear = 1 polynomial = 1 (cid:80)m )2 i=1(oi ˆolinear (cid:80)m i=1(oi o)2 i=1(oi ˆopoly )2 (cid:80)m i=1(oi o)2 (cid:80)m Definition 5 (Integration Bias Metrics). We further quantify the integration bias through: Weight Disparity: WD = σ(β) µ(β) - measures variability in factor importance (5) (6) Weight Entropy: WE = (cid:80)k j=1 pj log pj where pj = βj (cid:80)k l=1 βl Context Stability: CS = 1 1 c(c1) (cid:80) i<j β(i) β(j)2 The schematic adherence sensitivity, representing the unexplained variance in overall judgments, is: Schematic Adherence Sensitivity = (cid:113) 1 schematic This provides direct measure of judgment variance attributable to coherence failures between the rubric factors and overall scoring. B.2 PSYCHOMETRIC VALIDITY This measure consists of three key components: Cronbachs Alpha (α): Measures internal consistency within each factor Cross-loading Ratio (CLR): Quantifies factor discriminant validity Heterotrait-Monotrait Ratio (HTMT): Assesses construct validity between factors Mathematical Formalization Let = {f1, f2, . . . , fk} denote the set of explicit rubric factors, and let Xij represent the score for factor on question j. Definition 6 (Cronbachs Alpha). For each factor fi, Cronbachs alpha is defined as: αi = (cid:18) n1 1 (cid:80)n Var((cid:80)n j=1 Var(Xij ) j=1 Xij) (cid:19) where is the number of questions. λii Definition 7 (Cross-loading Ratio). For each factor fi, the cross-loading ratio is: CLRi = maxj=i λij where λij represents the loading of factor on latent factor obtained through factor analysis. Definition 8 (HTMT Ratio). The HTMT ratio between factors fi and fj is: HTMTij = rij riirjj where rij is the mean correlation between items of different factors, and rii is the mean correlation between items within factor i. Definition 9 (Sigmoid CLR Normalization). To address asymmetric penalties in CLR assessment while maintaining literature-supported thresholds Campbell & Fiske (1959), we apply sigmoid normalization centered at the psychometric threshold CLR = 1.5 from the 0.40-0.30-0.20 rule Hair et al. (2019), which states that items should have strong factor loadings on their intended factors and relatively low loadings on all secondary factors: CLRnorm,i = 1+exp(2(CLRi1.5)) This transformation provides smooth, bounded scoring where CLR = 1.5 yields 0.5, poor discriminant validity (CLR 0) yields 0.05, and strong validity (CLR 2.0) yields 0.73. Definition 10 (Unified Psychometric Validity Score). We combine these metrics into unified psychometric validity score using equal weighting: 1 Rpsychometric = 1 1 3 (cid:88) i=1 αi + 1 (cid:88) i=1 CLRnorm,i + 1 HTMTij 2 k(k 1) (cid:88) i<j 13 where the HTMT component is inverted since lower HTMT values indicate better discriminant validity. The psychometric reliability sensitivity, representing the unexplained variance due to poor factor structure and judge consistency, is then: Sensitivitypsychometric = (cid:112)1 Rpsychometric score range where score range scales the normalized sensitivity to the judgment scale (e.g., 4.0 for Arena-Hards 1-5 Likert scale). Methodological Justification The sigmoid CLR normalization addresses several methodological concerns identified in our analysis: Literature grounding: The 1.5 threshold derives from established discriminant validity criteria where primary loadings 0.40 and cross-loadings 0.30 yield CLR 1.331.5 Hair et al. (2019) Balanced weighting: Prevents poor CLR values from creating excessive negative penalties that dominate well-established metrics (Cronbachs α, HTMT) Psychometric validity: Maintains standard practice in discriminant validity assessment while avoiding the asymmetric penalty problem of linear normalization Ronkko & Cho (2022)"
        },
        {
            "title": "C EXPERIMENTAL EVALUATION SETTINGS",
            "content": "Our empirical analysis employs eleven distinct evaluation settings to comprehensively assess LLM judge behavior across different configurations. Each setting is defined by judge template configuration, baseline model for comparison, and set of respondent models to be evaluated. C.1 GENERAL EVALUATION SETTINGS (SETTINGS 15) These settings evaluate models across all criteria simultaneously: Setting Model Set Baseline Reasoning Description 1 2 3 4 5 V1 V2 V2 V3 V1 V1 V1 V1 V2 No No Yes No Yes Standard evaluation without reasoning Extended model set without reasoning Extended model set with judge reasoning Latest model set without reasoning Latest model set with judge reasoning Table 2: General evaluation settings for comprehensive model assessment C.2 ISOLATED CRITERIA EVALUATION (SETTINGS 611) These settings evaluate models on individual criteria in isolation: Setting Model Set Baseline Criterion Reasoning Purpose 6 7 8 9 10 11 V1 V1 V1 V1 V1 V1 V1 V1 V3 V3 V3 Correctness Conciseness Style Correctness Conciseness Style No No No No No No Isolated correctness assessment Isolated conciseness assessment Isolated style assessment Correctness with updated baseline Conciseness with updated baseline Style with updated baseline Table 3: Isolated criteria evaluation settings for factor-specific analysis C.3 CONFIGURATION DETAILS Each evaluation setting is configured using YAML files with the following structure: Judge Template: Specifies the evaluation criteria and judgment format Baseline Model: Reference model for relative comparisons (not evaluated itself) Respondent Models: Set of models to be evaluated against the baseline Judgment Reasoning: Whether judges provide explicit reasoning for their decisions C.4 MODEL SETS Three distinct model sets are used across the evaluation settings, described in detail in Table 4: 1. Model Set V1: Initial collection of models for baseline experiments 2. Model Set V2: Extended collection including newer model releases 3. Model Set V3: Latest collection with state-of-the-art models Table 4: Models across Evaluation Settings Model Name Creator Creation Date Model Size HF Link Setting List Meta Meta Meta-Llama-3-8B Meta-Llama-3-8B-Instruct Llama-3-8B-Magpie-Align-SFT-v0.2 Magpie-Align Magpie-Align Llama-3-8B-Magpie-Align-v0.2 Allenai Llama-3-8B-Tulu-330K NYU DICE Lab Llama-3-8B-WildChat Allenai llama-3-tulu-2-dpo-8b MBZUAI bagel-8b-v1.0 OpenAI gpt-3.5-turbo-0125 OpenAI gpt-4-0314 OpenAI gpt-4-0613 Meta opt-125m claude-3.7-sonnet cohere-command-R7B dolphin-2.9-llama3-8b gemini-2.5-flash-preview gemma-3-27b-it gpt-4o-mini-2024-07-18 grok-3-beta Mistral-8B Phi4 Qwen-Plus Anthropic Cohere Cognitivecomputations Google Google OpenAI xAI Mistral AI Microsoft Alibaba Cloud DeepResearcher-7b OpenThinker2-7B Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct-abliterated-v2 Qwen2.5-Coder-7B Qwen2.5-Math-7B s1.1-7B Alpha-VLLM Open-Orca Alibaba Cloud Alibaba Cloud Alibaba Cloud Alibaba Cloud Alibaba Cloud HippoAI Apr 2024 Apr 2024 May 2024 May 2024 May 2024 Jan 2025 May 2024 Apr 2024 Jan 2024 Mar 2024 Jun 2023 May Feb 2025 Mar 2024 Jun 2024 Apr 2025 Jul 2024 Jul 2024 Jul 2024 Feb 2024 Jul 2024 Sep 2024 Jun 2024 Jun 2024 Sep 2024 Sep 2024 Sep 2024 Sep 2024 Sep 2024 Jun 2024 8B 8B 8B 8B 8B 8B 8B 8B Unknown Unknown Unknown 125M 175B 7B 8B Unknown 27B Unknown Unknown 8B 7B 14B 7B 7B 7B 7B 7B 7B 7B 7B https://huggingface.co/meta-llama/Meta-Llama-3-8B V1 https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct V1 https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.2 V1 https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-v0.2 V1 https://huggingface.co/allenai/Llama-3-8B-Tulu-330K V1 https://huggingface.co/Magpie-Align/Llama-3-8B-WildChat V1 https://huggingface.co/allenai/llama-3-tulu-2-dpo-8b V1 https://huggingface.co/MBZUAI/BAGEL-8B-v1.0 V1 V1, V3 N/A V1, V3 N/A V1, V3 N/A https://huggingface.co/facebook/opt-125m V1, N/A N/A https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b N/A https://huggingface.co/google/gemma-3-27b-it N/A N/A https://huggingface.co/mistralai/Mistral-8B-v0.1 https://huggingface.co/microsoft/Phi-4 https://huggingface.co/Qwen/Qwen-Plus https://huggingface.co/Alpha-VLLM/DeepResearcher-7B https://huggingface.co/Open-Orca/OpenThinker2-7B https://huggingface.co/Qwen/Qwen2.5-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-7B N/A https://huggingface.co/Qwen/Qwen2.5-Coder-7B https://huggingface.co/Qwen/Qwen2.5-Math-7B https://huggingface.co/HippoAI/s1.1-7b V2 V2 V2 V2 V2 V2 V2 V2 V2 V2 V3 V3 V3 V3 V3 V3 V3 V3 C.5 BASELINE MODELS Three baseline configurations are employed: 1. Baseline V1: GPT-4-0314 2. Baseline V2: gpt-4o-mini-2024-07-18 3. Baseline V3: gpt-4-turbo-2024-04-09 These diverse evaluation settings enable comprehensive analysis of judge behavior across different configurations, revealing how judgment patterns vary with model sets, baseline choices, reasoning requirements, and criteria isolation. C.6 JUDGE TEMPLATE You are an impartial judge of the responses provided by two AI assistants, assistant and assistant B, to user prompt. You will judge based on the following criteria. 1. Correctness: whatever information the assistant provides should be factually correct, free of typos or misleading generalizations. The assistant should follow all instructions in the prompt, including style, formatting, and role-playing instructions. Short answers typically score higher on correctness. 15 2. Completeness: if the user prompt specifies particular audience, the response should contain all information necessary for that audience to understand it. Otherwise, the response should contain all information necessary for an average adult human to understand it. 3. Safety: if, in the course of providing correct and complete response, the assistant would break any law or potentially cause someone harm, the assistant should respond only to the safe parts of the prompt. 4. Conciseness: The assistant should not ramble or include unnecessary details. If instructed to omit content, that content should not be present in the reply. Short answers typically score higher on conciseness. 5. Style: the agent should employ diverse vocabulary and sentence structure and demonstrate creativity, avoiding formulaic constructions such as unnecessary or long lists, generic introductions, and pat summaries. Unless otherwise specified, the tone should be conversational and friendly. Additional guidelines: do not provide your own answers, simply judge the answers provided. Do not judge based on any criteria other than the aforementioned criteria; in particular, do not favor longer responses, or responses stylistically similar to your output. Do not mix criteria while judging; for example, when judging correctness, it is irrelevant how complete the models answer is. When in doubt, choose A=B. Begin your reply by ranking the two assistants according to each of the criteria. For each criteria, provide brief justification followed by verdict: e.g., for completeness, you may choose from Completeness: ((A>>B)) , Completeness: ((A>B)) Completeness: ((A=B)) Completeness: ((B>A)) Completeness: ((B>>A)) After you render your factor-wise judgments and before your render your overall judgments, please think about how you should weight each of your factor-wise judgments for this particular task and knowledge domain. Use what you know about the domain to guide your weighting; is factuality more important here than style, or vice versa? What about the other factors? Consider whether you have weighted all factors reasonably. Consider how important each infraction you have observed is, and whether it should be penalized more strongly. Finally, issue verdict with label: 1. Assistant is much better: [[A>>B]] 2. Assistant is better: [[A>B]] 3. Tie, close to the same: [[A=B]] 4. Assistant is better: [[B>A]] 5. Assistant is much better: [[B>>A]] Example output: My final verdict is tie: [[A=B]]. C.7 ABLATION ON JUDGE TEMPLATE STRUCTURE Ablation studies confirm that evaluating factors in isolation versus collectively changes absolute scores dramatically but preserves model rankings. Although there are advantages, from an efficiency standpoint, to scoring all factors simultaneously, we nevertheless acknowledge that this is an important continuing direction for future work. Figure 5: Factor ablation study results showing that while absolute ELO scores change dramatically when factors are evaluated in isolation versus collectively, model rankings remain largely stable. This demonstrates that our findings about judge bias are robust to different evaluation methodologies. C.8 ABLATION ON DEVIATION RATE In Table 5, we observe that deviation rate is fairly stable across criteria but highly sensitive to setting, particularly in open-weights judges such as QwQ-32B and DeepSeek-R1-32B. Table 5: Score Deviation Rates by Judge and Setting (%) Judge Setting Correctness Completeness Safety Conciseness Style Average GPT-4o-mini-0718 QwQ-32B QwQ-32B QwQ-32B QwQ-32B QwQ-32B DeepSeek-R1-32B GPT-3.5-Turbo-0125 DeepSeek-R1-32B DeepSeek-R1-32B DeepSeek-R1-32B DeepSeek-R1-32B setting1 setting3 setting2 setting5 setting4 setting1 setting3 setting1 setting1 setting2 setting4 setting5 0.06 0.60 0.81 2.73 4.22 5.13 7.87 0.12 13.27 16.71 21.20 39. 0.06 0.62 0.80 2.73 4.22 5.16 7.88 0.12 13.32 16.74 21.39 40.57 0.06 0.61 0.80 2.69 3.76 5.16 7.92 33.10 13.69 16.62 21.35 39.94 0.06 0.62 0.80 2.73 4.23 5.20 7.94 0.46 13.55 16.71 21.39 40.57 0.06 0.62 0.80 2.73 4.22 5.18 7.93 6.02 13.50 16.71 21.39 40.57 0.06 0.61 0.80 2.72 4.13 5.16 7.91 7.96 13.47 16.70 21.34 40."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "University of California, Berkeley",
        "University of Maryland, College Park"
    ]
}