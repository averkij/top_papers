{
    "paper_title": "P3-SAM: Native 3D Part Segmentation",
    "authors": [
        "Changfeng Ma",
        "Yang Li",
        "Xinhao Yan",
        "Jiachen Xu",
        "Yunhan Yang",
        "Chunshi Wang",
        "Zibo Zhao",
        "Yanwen Guo",
        "Zhuo Chen",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon."
        },
        {
            "title": "Start",
            "content": "P3-SAM: Native 3D Part Segmentation , Xinhao Yan1,3, Jiachen Xu1, Yunhan Yang1,4, Changfeng Ma1,2, Yang Li1, Chunshi Wang1,5, Zibo Zhao1, Yanwen Guo2, Zhuo Chen1, Chunchao Guo1, 1 Tencent Hunyuan, 2NJU, 3ShanghaiTech, 4HKU, 5ZJU"
        },
        {
            "title": "Abstract",
            "content": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose native 3D pointpromptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon. 5 2 0 2 0 1 ] . [ 3 4 8 7 6 0 . 9 0 5 2 : r Figure 1: P3-SAM produces precise part segmentation results for any object. Project Leader Corresponding Author"
        },
        {
            "title": "Tencent Hunyuan",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "As fundamental 3D version task, the segmentation of 3D assets plays crucial role in shape analysis, editing, and reuse, as well as other downstream tasks such as mesh simplification and animation design. Many works have been proposed to address 3D segmentation and have achieved notable success, but there are still challenges in this task. Traditional learning-based segmentation methods attempt to segment 3D point clouds using predefined part categories, relying on direct supervision from part labels. They can only segment certain parts within specific categories and struggle to handle objects with arbitrary parts. To this end, recent works leverage the capabilities of the 2D SAM Kirillov et al. (2023) by lifting 2D segmentation results to serve as 3D segmentation ground truth for training 3D segmentation models. Among them, SAMPart3D Yang et al. (2024) and PartField Liu et al. (2025) train feature learning networks through distillation or contrastive learning and achieve 3D segmentation by feature clustering. Point-SAM Zhou et al. (2024), on the other hand, directly adapts the 2D SAM framework to 3D point clouds, training promptable 3D segmentation model. However, these methods still rely on 2D SAM results, which cannot provide 3D-consistent segmentation signals and often produce imprecise part boundaries. As result, these method suffer from imprecise results and lack strong robustness when dealing with arbitrarily complex objects. Additionally, they are still one step away from full automatic segmentation, as they require users to provide the number of parts or prompt points. In this paper, we propose native 3D Point-Promptable Part segmentation model termed P3-SAM, designed to fully automate the segmentation of any complex 3D objects into components with precise mask and strong robustness. The improvements and differences of our method compared to previous methods are summarized in Table1. As pioneering promptable image segmentation work, SAM provides feasible implementation approach. However, our method focuses on achieving precise part segmentation automatically, and we simplify the architecture of SAM. Without adopting the complex segmentation decoder and multiple types of prompts from SAM, our model is designed to handle only one positive point prompt. Specifically, P3-SAM contains feature extractor, three segmentation heads, and an IoU prediction head. We employ PointTransformerV3 Wu et al. (2024) as our feature extractor and integrate its features from different levels as extracted point-wise features. The input point prompt and feature are fused and passed to the segmentation heads to predict three multi-scale masks and an IoU (Intersection of Union) predictor is utilized to evaluate the quality of the masks. To automatically segment an object, we apply our segmentation model using point prompts sampled by FPS (Farthest Point Sampling) and utilize NMS (Non-Maximum Suppression) to merge redundant masks. The point-level masks are then projected onto mesh faces to obtain the part segmentation results. Another key aspect of this paper is to eliminate the influence of 2D SAM, and rely exclusively on raw 3D part supervision for training native 3D segmentation model. While existing 3D part segmentation datasets are either too small (e.g. PartNet Mo et al. (2019)) or lack part annotation (e.g. Objaverse Deitke et al. (2022)), this work addresses the data scarcity by developing an automated part annotation pipeline for artist-created meshes and used it to generate dataset comprising 3.7 million meshes with high-quality part-level masks. Our model demonstrates excellent scalability with this dataset and achieves robust, precise, and globally coherent part segmentation. Our extensive experiments demonstrate that our method achieves state-of-the-art performance in part segmentation for any parts of any objects, especially on complex objects with highly detailed geometry. The main contributions of our P3-SAM are summarized as follows: We propose native 3D point-promptable part segmentation model to segment any parts of any objects. We propose fully automatic part segmentation approach using our model and mask merging algorithm."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Table 1: The comparison of our method with related works across several key aspects, including the number and type of training data, the number of parameters, the time cost for full and interactive segmentation, and the ability to automatically segment objects. Methods Data Num. Data Type Param. Time(Seg.) Time(Inter.) Auto. SAMesh Find3D SAMPart3D ParField Point-SAM - 30K 200K 360K 100K 2D Lifting 2D Data Engine 2D Data Engine 2D Data Engine 2D Data Engine Ours 3.7M 3D Native - 46M 114M 106M 311M 112M 7min 10s 15min 10s - 8s - - - - 5ms 3ms Ø Ø Ø Ø With high accuracy, generalization, and robustness across various tasks and data types, our method can be applied to interactive, multi-head and hierarchical part segmentation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Traditional 3D Part Segmentation Traditional 3D part segmentation methods usually train their networks on specific part labels from object or scene datasets, such as PartNet Mo et al. (2019), Princeton Mesh Segmentation Chen et al. (2009), ScanNet Dai et al. (2017), and S3DIS Armeni et al. (2016). These methods employ point cloud encoders like PointNet Qi et al. (2017) and PointTransformerV3 (PTv3) Wu et al. (2024) or mesh encoders like MeshCNN Hanocka et al. (2019) and Mesh Transformer (Met) Zhou et al. (2023a) to extract 3D features for the segmentation head to predict part labels. In addition to segmenting parts with semantic meaning, recent works aim to segment out geometrically significant parts. However, traditional methods suffer from limited categories and part labels and struggle to generalize to arbitrary categories and parts. 2.2 2D Lifting 3D Part Segmentation With the development of 2D foundation models, significant progress has been made by models such as CLIP Radford et al. (2021), GLIP Li et al. (2022), SAM Kirillov et al. (2023), Dinov2 Oquab et al. (2023) and VLM in image-text alignment and zero-shot detection and segmentation. Rendering 3D models into multi-view images and leveraging these 2D foundation models for lifting 2D capabilities to 3D is an obvious but effective approach. Recent methods, such as SAMesh Tang et al. (2024), SAM3D Yang et al. (2023) and SAMPro3D Xu et al. (2023), directly apply SAM to rendered 2D images and aggregate multi-view masks to achieve class-agnostic segmentation for any 3D objects or scenes. Additionally, several methods utilize text descriptions of categories as prompts on 2D rendered images to enhance the querying of 3D parts. PartSLIP Liu et al. (2023) and SATR Abdelreheem et al. (2023) employ text prompts and GLIP Li et al. (2022) to detect parts, followed by post-processing to segment parts on point clouds and meshes. Besides GLIP, PointSLIP++ Zhou et al. (2023b) and ZeroPS Xue et al. (2023) also leverage SAM, achieving more precise segmentation results. The MeshSegmenter Zhong et al. (2024) employs Stable Diffusion Rombach et al. (2022) to generate textures for mesh, enabling SAM Kirillov et al. (2023) and Grounding DINO Liu et al. (2024) to clearly segment and detect parts. PartDistill Umam et al. (2024) utilizes 2D Vision-Language Models for forward and backward knowledge distillation to achieve 3D part segmentation. COPS Garosi et al. (2025) leverages Dinov2 Oquab et al. (2023) to project visual features onto 3D point clouds and then uses VLM for part segmentation. Directly lifting 2D knowledge to 3D may encounter limitations such as data gaps, 3D consistency issues, and unstable post-processing, leading to poor robustness and inaccurate segmentation results. Text-query-based methods also require prompt engineering. Rendering multi-view images and using SAM or VLM to process these images can also consume significant resources and time."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "2.3 2D Data Engine for 3D Part Segmentation To alleviate the 3D consistency issues and data gaps brought by directly lifting 2D knowledge, recent works attempt to use 2D foundation models to build data engine for training feed-forward networks on 3D point clouds and meshes. OpenScene Peng et al. (2023) employs feature extractor to learn the CLIP features projected onto scene point clouds and uses text prompts to query these features for segmentation. Segment3D Huang et al. (2024) and SAL Oˇsep et al. (2024) use feed-forward networks to learn the projected masks of scene meshes and LiDAR data predicted by SAM, and then use CLIP to assign categories to each part. Find3D Ma et al. (2024) trains network to segment objects given text prompts by building data engine that allows the VLM to query parts after SAM processes multi-view images. SAMPart3D Yang et al. (2024) employs network to distill the projected Dinov2 features of point clouds. To achieve more accurate segmentation of each object, it then trains lightweight MLP for each object to predict segmentation masks by conducting contrastive learning on SAM projections. Finally, MLLM is utilized to annotate each part. PartField Liu et al. (2025) directly supervises network composed of voxel CNN and tri-plane transformer with contrastive learning loss on both 2D and 3D masks, where the 2D masks are obtained using SAM. Point-SAM Zhou et al. (2024) adapts SAM to 3D point clouds and utilizes SAM to design data engine based on multi-view images. This data engine continuously trains and refines PointViT model to achieve part segmentation based on prompt points. Although 2D data engine can reduce 3D inconsistencies and improve the networks generalization ability, segmentation based on 2D data can still suffer from boundary ambiguities and data gaps, leading to inaccurate segmentation results, especially on complex data. Additionally, these methods either require specifying the number of categories or need user-provided prompt points, which means they cannot fully automate object segmentation."
        },
        {
            "title": "3 Method",
            "content": "Given the mesh = (V, NNf 3) of an object, our goal is to predict mask mpart {1, 2, 3, ..., Npart}Nf that segments each face into Npart parts, where Nf indicates the number of faces and Np represents the number of parts for the object. Here, each part is instance-specific but class-agnostic. We first introduce our data curation (Section 3.1), then describe the architecture of our point-promptable part segmentation module termed P3-SAM (Section 3.2), and finally present the automatic segmentation algorithm (Section 3.3). 3.1 Data Curation To construct our dataset, we aggregated 3D models from multiple sources, including Objaverse Deitke et al. (2022), Objaverse-XL Deitke et al. (2023), ShapeNet Chang et al. (2015), PartNet Mo et al. (2019), and other internet repositories. The aforementioned 3D assets are mainly created by artists. Artists tend to craft 3D shapes part by part and assemble them together. The assembly process may not forge parts meshes together; this gives us the chance to reverse the part information from the asset. Specifically, the complete mesh is first decomposed into sub-meshes based on connected components. Then we calculate the surface area of each sub-mesh, and build adjacency graph between parts (we voxelize the space with resolution of 128, two sub-meshes are considered adjacent if they share any voxel). We iteratively merge small parts (with surface area less than 1% of the total) with their adjacent, larger parts. This bottom-up process continues until all parts exceed the 1% area threshold. After merging, we filter out objects that have too few (less than 2) or too many (more than 50) parts. To prevent the impact of mask area imbalance, we filtered out objects with disproportionately large parts (where single part occupies more than 0.85%) and objects with significant number of very small parts (where parts smaller than 1% in area collectively account for more than 10% of the total area). After the aforementioned filtering steps, we obtain nearly 3.7 million objects. However, these object models are non-watertight at the object-level, often containing internal structures and clear boundaries. Training solely on such data can lead to poor gener-"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 2: The Network Architecture of P3-SAM. Input point clouds are fed to feature extractor to obtain point features. The features, point prompts, and original point clouds are then fed to two stage multi-mask segmentor to obtain three masks in various scales for the prompts. Finally, the IoU predictor is utilized to evaluate the quality of the masks and select the best one as the final prediction. alization on watertight 3D models, such as scanned mesh or AI-generated ones. We then made the filtered models watertight, resulting in nearly 2.3 million successfully watertight models. These watertight models do not contain internal structures and only include the outer surfaces of the models. To obtain the labels of each face on watertight models, we query their nearest faces on the non-watertight model and assign the label of the nearest face as the label for the corresponding face on the watertight model. During training, if model has watertight version, we set an 80% probability of selecting the watertight data for training. This allows our network to handle both watertight and non-watertight data. 3.2 Point-Promptable Part Segmentation Model 3.2.1 Network Architecture To achieve class-agnostic part segmentation for arbitrary objects, providing prompts related to parts is more efficient than direct label supervision or contrastive learning. Previous methods utilize text as prompts to query parts, while methods like SAM Kirillov et al. (2023) and Point-SAM Zhou et al. (2024) use positive and negative prompt points or bounding boxes as prompts. To fully facilitate the automatic part segmentation of objects using point-based prompts, we design our P3-SAM to segment part using only single point prompt. This allows the network to avoid adapting to diverse prompts, simplifying the network and improving its convergence, generalization, and accuracy. The input to our P3-SAM consists of the point cloud RNp3 and its normals RNp3 sampled from the input mesh and point R3 as prompt to indicate the part that needs to be segmented. As shown in Figure 2, the architecture of our method consists of feature extractor, multi-head segmentor, and an IoU predictor. Since our method only requires single point prompt, there is no need to design prompt encoder. We directly input the prompt into the multi-head segmentor. Feature Extractor. Recent point cloud encoders have achieved excellent results on various point cloud tasks, especially Sonata Wu et al. (2025), self-supervised pre-trained Point Transformer V3 Wu et al. (2024). We then employ Sonata with its pre-trained weights as our feature extractor to extract multi-scale features from point clouds. To better handle complex objects, we reduced the voxel size of the input to Sonata. We then aggregate these multi-scale features together and use an weight-shared MLP Fe to obtain point-wise features , as shown in Figure 2, fp = Fe(E(P, N)1, E(P, N)2, ..., E(P, N)n),"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "where the subscripts indicate features at different scales. The point features need to be predicted only once and can be used for predicting part masks with different point prompts. Multi-Head Segmentor. Our part dataset, introduced in Section 3.1, integrates multiple data sources which may involve varying granularity and conflicting criteria for part separation. Additionally, the point prompts might be ambiguous in indicating the specific scale of part, as mentioned in SAM. Therefore, we use multi-head segmentor to predict multiple alternative masks at various scales in order to mitigate this conflict and ambiguity. Our multi-head segmentor contains two-stage prediction process. In the first stage, three MLPs (1) take mixed input fin, including the point-wise features fp, the input points and Np copies of the point prompt p, to predict three different masks, m(1) = (1) (fin) = (1) (fp, P, p), = 1, 2, 3. However, the first stage is naive implementation that lacks support for global information. Therefore, in the second stage, we introduce global feature and re-predict the three masks based on the results from the first stage. As shown in the Figure, we use an MLP Fg to predict point-wise features and apply max pooling along the point dimension. We utilize an MLP Fg to predict point-wise features, and apply max pooling along the point dimension to derive global feature, Finally, three new MLPs (2) global features and the outcomes from the first phase, fg = axP ool(Fg(fin, m(1) 2 , m(1) are employed to predict more accurate results based on the 1 , m(1) 3 )). m(2) = (2) (fin, fg, m(1) 1 , m(1) 2 , m(1) 3 ), = 1, 2, 3. While the second stage optimizing the results from the first stage, the initial masks m(1) also help the second stage to focus the extraction of global features on the parts that need segmentation, making feature extraction more efficient and improves the accuracy of segmentation results. IoU Predictor. To achieve automatic identification of the best mask, we introduce an IOU 2 , m(2) predictor to assess the quality of m(2) 3 and select the best mask as the networks final prediction. The assessment is achieved by directly predicting the IoU values of the predicted masks and the ground truth masks. The IOU predictor first uses an MLP iou and max pooling to obtain global feature from the global feature and three masks of second stage, and then employs another MLP Fiou to predict three IoU values, 2 , m(2) v1, v2, v3 = Fiou(M axP ool(F iou(fin, fg, m(2) 1 , m(2) 1 , m(2) 3 ))). The multi-head segmentor and IoU predictor are lightweight models capable of real-time computation. Consequently, once the global feature of given 3D model is extracted, our P3-SAM can be utilized for real-time interactive segmentation. 3.2.2 Training Data augmentation. To enhance the robustness of our network, we introduce random noise to the input points P, normals N, and point prompts during training. Furthermore, we randomly remove normals with probability of 0.3. We also mix watertight and nonwatertight data, as mentioned in Section 3.1. Optimization Losses. During training, for given model, we randomly select part masks and then randomly choose one point from the part points corresponding to each mask, resulting in prompts pj R3 and ground truth part masks m(gt) {0, 1}Np , where = 1, 2, ..., K. For the three masks generated by the network in the first and second stages, we apply both Dice loss Ldice and Focal loss Lf ocal for supervision. Backpropagation is applied only to the output with the lowest loss, which encourages each segmentation head to predict masks at different scales. So, the mask loss Lmask can be calculated as: L(t) mask = 1 (cid:88) j=1 (cid:16) 3 min i=1 αdiceLdice(m(t) ij , m(gt) ) + Lf ocal(m(t) ij , m(gt) (cid:17) ) ,"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 3: Automatic Segmentation Pipeline. Point prompts are sampled by FPS and go through the P3-SAM to obtain multiple masks. NMS is then adopted to merge redundant masks. The point-level masks are then projected onto mesh faces to obtain the part segmentation results. Algorithm 1 Automatic Segmentation Input: Mesh with Nf faces Output: Mask mpart {1, 2, ..., Npart}Nf with Npart parts 1: Sample Np points with normals from 2: Sample Npp prompt points pj from using FPS 3: Extract point-wise feature fp from and using P3-SAM 4: Predict Npp masks mj and IoU value vj based on pj using P3-SAM 5: Filter masks mj using NMS and retain Npart masks 6: Assign the Npart point masks mj to with part labels 7: Fill the faces without labels using flood fill algorithm and produce the mask mpart where αdice is weighting parameter, and = 1, 2 indicates whether the loss is computed for the first or second stage of the network. To supervise the IoU, we first calculate the IoU between m(2) . We then use MSE to compute the loss based on these IoU values, ij and m(gt) LIoU = 1 3K (cid:88) 3 (cid:88) j=1 i=1 (cid:16) LM SE vij, IoU (I(m(2) ij ), m(gt) (cid:17) ) , where is an indicator function that indicates whether the mask value is greater than 0.5. The overall loss is the sum of the mask losses from both the first and second stages and the IOU loss, that is = L(1) mask + L(2) mask + LIoU . 3.3 Automatic Segmentation Methods such as interactive segmentation, text prompt extraction, and clustering often require human intervention during the segmentation process. To achieve fully automatic segmentation, we propose an automated approach based on our P3-SAM, as shown in Algorithm 1. Our method begins by sampling points and normals from given mesh M. Subsequently, we use Farthest Point Sampling (FPS) to select Npp point prompts pj from P. After extracting features fp from P, we predict mask mj and an IoU value vj based on each point prompt pj utilizing our P3-SAM. To ensure that each part can be segmented out, point prompts are always over-sampled, with their number being significantly greater than the actual number of parts in the object. To obtain the true number of parts, we use Non-Maximum Suppression (NMS) to filter out the numerous duplicate masks. We first sort the masks in descending order according to their IoU values to form candidate queue. We take out the first mask and use it to filter out other masks in the queue that have an IoU greater than TNM with this mask. We repeat this process of selecting and filtering until no masks remain in the queue. The set of all selected masks constitutes the final result of NMS. The part number Npart is the number of the selected masks, and each mask has its own part label. According to which face and mask each point belongs to, we assign"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Table 2: The comparison of our method with previous methods on PartObjectarverseTiny. The first two blocks represent class-agnostic part segmentation without and with connectivity, respectively, and the last block represents interactive segmentation. Task Method Human Animals Daily Build. Trans. Plants Food Elec. AVG. Find3D Seg. w/o SAMPart3D PartField Connect. Ours Seg. w/ Connect. Interact. SAMesh PartField Ours Point-SAM 26.13 49.01 Ours 23.99 55.03 54.52 60.77 66.03 80.85 80. 23.99 57.98 58.07 59.43 60.89 83.43 86.46 29.25 53.45 16.03 22.67 40.36 49.17 56.46 42.47 62.98 50.82 56.53 77.83 80.97 41.03 69.66 67. 28.85 23.58 52.36 38.50 14.11 47.38 49.09 57.72 46.89 73.85 68.44 22.91 51.52 21.77 62.14 59.16 70.53 65.12 80.21 90. 31.44 62.57 25.71 64.59 55.4 54.04 60.56 85.27 92.90 21.28 19.83 53.47 51.15 56.29 53.93 61.96 59.88 57.81 82.30 81.52 56.86 79.18 81. 33.04 27.91 28.05 50.80 51.86 51.23 Table 3: The comparison of our method with previous methods on the watertight version of PartObjectarverse-Tiny. Task Fully Segmentation w/o Connectivity Interactive Seg. Method Find3D SAMPart3D SAMesh PartField Ours Point-SAM Ours PartObj-Tiny-WT PartNetE 20.76 21.69 48.79 56.17 - 26.66 51.54 59. 58.10 65.39 24.16 45.85 49.11 63.48 the corresponding part labels to each face and determine final part label for each patch through voting. We use the flood fill algorithm to assign labels to faces that do not have label. Specifically, for each unlabeled face, we assign it the most frequent label among its neighboring faces (or its nearest several faces if there is no connectivity in the mesh). We repeat this process until all faces have been assigned label and obtain the final mask mpart of each faces in mesh M."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details The channel of the point-wise feature fp is 512. The point number Np is set to 100, 000 during training, evaluation, and inference. We randomly select = 8 parts in the training process and set αdice to 0.5. For automatic segmentation, we sample Npp = 400 prompts from points, and the threshold TNM is set to 0.9. Our network is trained on our dataset using 64 H20 for 9 epochs. We set the batch size to 2 per GPU, and the training took approximately 4 days. We employ the Adam optimizer with learning rate of 105. 4.2 Comparison Evaluation Datasets. We evaluate each method on three datasets: PartObj-Tiny Yang et al. (2024), PartObj-Tiny-WT, and PartNetE Liu et al. (2023). PartObj-Tiny is subset of Objarvse Deitke et al. (2022), containing 200 data samples across 8 categories, with manually annotated part segmentation information. PartObj-Tiny-WT is the watertight version of PartObj-Tiny. To evaluate the performance of various networks on watertight data, we converted the meshes from PartObj-Tiny to watertight versions and successfully obtained 189 watertight meshes. We then acquired the ground truth segmentation labels following the method described in Section 3.1. And there is no color on the watertight mesh, which could pose challenge for methods based on rendering multi-view images. PartNetE, derived from PartNet-Mobility, contains 1,906 shapes covering 45 object categories in the form of point clouds. We also evaluate various networks on it to verify their generalization performance on point cloud. Tasks. There are three tasks: full segmentation without connectivity, full segmentation with connectivity, and interactive segmentation. The meshes in PartObj-Tiny are non-watertight,"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 4: The comparison of our automatic segmentation approach with previous methods on PartObj-Tiny. with each face exhibiting strong connectivity, forming distinct connected components that are strongly correlated with the segmentation results. Introducing such connectivity may improve segmentation performance. However, in real-world applications, the connectivity relationships of meshes are often chaotic or may not even exist. We then divide the full segmentation task into the first two tasks. For watertight data and point clouds, there is no connectivity, so the second task does not apply. Baseline Methods. We compare our P3-SAM with recent related works including SAMesh Tang et al. (2024), Find3D Ma et al. (2024), SAMPart3D Yang et al. (2024), ParField Liu et al. (2025) and Point-SAM Zhou et al. (2024). Among these, SAMesh is method based on 2D lifting that enables fully automatic segmentation. The other methods based on 2D data engines require human intervention in the overall segmentation process: Find3D requires text prompts, SAMPart3D and ParField need part categories for clustering, and Point-SAM necessitates manual selection of prompt points. Since Point-SAM cannot segment the entire model, we only compare its performance on interactive segmentation. More comparisons on different aspects, including time cost, number of parameters, amount of training data, etc., are shown in Table 1. Metric. The evaluation metric for fully segmentation is the same as in previous work Liu et al. (2025), using Intersection over Union (IOU) to measure the accuracy of mask predictions. To evaluate the interactive segmentation, we sample 10 prompt points for each part, then measure the average IOU between the predicted masks for all prompts of all parts and their corresponding ground truth masks. Results. Table 2 shows the evaluation results of various methods on PartObj-Tiny across three tasks. In the second task, the results for PartField Liu et al. (2025) are based on its original version, which incorporates connected components as the basis for hierarchical clustering and selects the optimal results from multiple levels. To ensure fair comparison, we also introduced connected components and used random prompts for each part. The detailed methodology can be found in Section 4.3. In the third task, since Point-SAM can only segment point clouds, we sample the point clouds from the meshes for comparison. Note that our method is also capable of segmenting point clouds because we do not require the connectivity of the mesh. Although PartField Liu et al. (2025) performs well in the"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 5: The comparison of our method with previous methods on PartObj-Tiny and PartObj-Tiny-WT across different tasks including full segmentation and interactive segmentation. Here, the green points indicate the input prompt pints. second task, once connectivity information is removed, its performance drops significantly, indicating that the PartField method is not robust to meshes without connected components. This is further evidenced by the comparison on watertight data, where the absence of connectivity also affects its performance. In contrast, our method consistently achieves the best performance regardless of whether connectivity information is present or not. This indicates that our approach effectively learns the geometric features of objects, enabling accurate part segmentation. In interactive segmentation, our method also performs the best, thanks to our unique prompt point segmentation head and IOU prediction module. These components enable precise multi-scale segmentation predictions and automatically select the optimal results. The comparisons on PartObj-Tiny-WT and PartNetE are shown in Table 3. Since watertight data and point clouds lack connectivity information, PartFields performance is not as good as on non-watertight data. This again validates that our method effectively learns the geometric features of objects. Here, SAMesh will get stuck when processing watertight meshes due to the high number of faces. Another observation is that the metrics for interactive segmentation are lower than those for full segmentation. This is because, in the evaluation of interactive segmentation, the method can only rely on single prompt point, focusing more on the accuracy of individual mask segmentation. This further validates the precision of our approach. Comparisons across various datasets and tasks, involving different data forms such as non-watertight meshes, watertight meshes, and point clouds, demonstrate the remarkable effectiveness, robustness and generalization ability of our methods, confirming its superior performance under diverse conditions. We also conduct qualitative comparison of our method and previous methods on PartObjTiny for full segmentation with connectivity, as shown in Figure 4. SAMesh tends to oversegment objects, while other methods struggle with handling complex objects, resulting in inaccurate masks, failure to separate multiple part masks, and other segmentation errors. However, our method can accurately segment complex objects, even performing part segmentation on the lizard and beetle in the scene of the last row. Figure 5 shows comparison between our method and PartField on the PartObj-Tiny-WT dataset for full segmentation without connectivity. PartField struggles to segment watertight meshes, while our method can maintain the segmentation quality. Figure 5 also illustrates the interactive segmentation results of our method and Point-SAM given the green point prompts, where our segmentation results exhibit accurate boundaries and scales. The results show that our method can predict masks with reasonable number of categories, clear and accurate boundaries, and can handle complex models as well as different types of data and tasks. They also demonstrate the great generalization and robustness of our approach."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 6: The applications of our method including multi-prompts and hierarchical part segmentation and part generation. 4.3 Applications Multi-Prompts Auto-Segmentation. Our method can also segment the object given several point prompts that indicate specific parts. In this setting, we have strong condition where each prompt corresponds to specific part of the object. Therefore, instead of using the predicted IoU to evaluate mask quality, we only need to select one mask for each part such that all masks collectively cover the entire object as much as possible while minimizing overlap between the masks. As shown in Figure 6, the multi-prompts segmentation can follow the users instructions. Compared to automatic segmentation, it can both extend unsegmented regions, such as the horses body, and merge over-segmented regions, such as flower petals. Hierarchical Part Segmentation. After segmentation, we can obtain the average feature of each part by finding the corresponding point-wise feature fp. We then directly employ hierarchical clustering on these average features to obtain hierarchical segmentation results. As shown in Figure 6, our hierarchical segmentation results effectively aggregate different parts at various levels, validating the effectiveness of our feature extraction method in accurately representing the information of each part. Compared to the results from PartField, our methods aggregation better adheres to the relationships between parts. Part Generation. Our results can also be applied to part generation as an instruction for splitting objects. Figure 6 demonstrates the exploded part generation results of HoloPartYang et al. (2025) when given the segmentation masks from SAMPart3D and ours. Our more accurate segmentation masks can significantly improve HoloParts performance, helping it generate cleaner and more precise parts."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 7: The ablation study of our automatic segmentation and the visualization of our point-wise feature. Table 4: The comparison of four ablated methods and our full method on the test set of our dataset. w/o Augmentation w/ Augmentation Ablation Single-Head Stage 1 Only Stage 2 Only Stage 1 + Stage 2 mIoU 0.2801 0. 0.6647 0.7464 Full 0.7906 4.4 Ablation Study We first conduct ablation studies on our network architecture. The feature extractor can be any point cloud encoder, and we select the current state-of-the-art one that is Sonata. The IoU predictor is critical for selecting the best mask; without it, our method cannot function properly. We then focus on conducting ablation studies of our two-stage multihead segmentor. As shown in Table 4, we evaluate four ablated models and our full method on the test set of our dataset. The first model contains only one segmentation head of the first stage. The second and third models respectively include only the first and second stages. The fourth model includes both stages but is trained without data augmentation. The last model is our full version. This progressive ablation study clearly demonstrates the importance of each component in our method. Notably, the difference between the second and third models is that the latter extracts global feature during segmentation. The better performance metrics of the third model highlight the importance of this global feature. As shown in Figure 7, we also present the full segmentation results without using the NMS or flood fill algorithm in our automatic segmentation approach. The results highlight the necessity of these two steps, as without them, the segmentation masks are not complete, have unclear boundaries, and do not yield reasonable number of parts. Figure 7 also visualizes the point-wise features of objects. The results show that for the same type of data, our method produces similar features for corresponding parts, while our features can capture more detailed geometry compared to PartField, such as the eyes and ears of the person on the right. This fully demonstrates the advantages of our method in extracting accurate features."
        },
        {
            "title": "5 Limitations and Conclusions",
            "content": "In this paper, we propose P3-SAM, native 3D part segmentation method. Our approach employs Sonata to extract point-wise features and uses two-stage multi-head segmentor to predict multi-scale masks given point prompt indicating part. An IoU predictor is employed to evaluate and select the best mask. We also propose an automatic segmentation approach using our P3-SAM. We train our model on 3.7 million models, resulting in part segmentation method with high accuracy, generalization, and robustness across various tasks and data types. Our method is also flexible and can be applied to multiple applications such as real-time interactive, hierarchical, or multi-prompt part segmentation. We observe that our method may rely too heavily on the geometric information of the objects surface and lacks an understanding of the spatial volume of the objects parts. This is because our"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "training data consists solely of surface point clouds. Therefore, future work may focus on developing models with spatial segmentation capabilities to broaden their applicability to wider range of tasks."
        },
        {
            "title": "References",
            "content": "Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zeroshot semantic segmentation of 3d shapes. In ICCV, 2023. Iro Armeni, Ozan Sener, Amir Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR, 2016. Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. Xiaobai Chen, Aleksey Golovinskiy, and Thomas Funkhouser. benchmark for 3d mesh segmentation. Acm transactions on graphics (tog), 2009. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. Marco Garosi, Riccardo Tedoldi, Davide Boscaini, Massimiliano Mancini, Nicu Sebe, and Fabio Poiesi. 3d part segmentation via geometric aggregation of 2d visual features. In WACV, 2025. Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel CohenOr. Meshcnn: network with an edge. ACM Transactions on Graphics (ToG), 38(4): 112, 2019. Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, and Francis Engelmann. Segment3d: Learning fine-grained class-agnostic 3d segmentation without manual labels. In ECCV, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4015 4026, 2023. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded languageimage pre-training. In CVPR, 2022. Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In CVPR, 2023. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond, 2025."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. Ziqi Ma, Yisong Yue, and Georgia Gkioxari. Find any part in 3d. arXiv preprint arXiv:2411.13550, 2024. Kaichun Mo, Shilin Zhu, Angel Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, and Hao Su. Partnet: large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 909918, 2019. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Aljoˇsa Oˇsep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, and Laura Leal-Taixe. Better call sal: Towards learning to segment anything in lidar. In ECCV, 2024. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In CVPR, 2023. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652660, 2017. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv:2408.13679, 2024. Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. Partdistill: 3d shape part segmentation by vision-language model distillation. In CVPR, 2024. Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In CVPR, 2024. Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, and Julian Straub. Sonata: Self-supervised learning of reliable point representations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2219322204, 2025. Mutian Xu, Xingyilang Yin, Lingteng Qiu, Yang Liu, Xin Tong, and Xiaoguang Han. Sampro3d: Locating sam prompts in 3d for zero-shot scene segmentation. arXiv preprint arXiv:2311.17707, 2023. Yuheng Xue, Nenglun Chen, Jun Liu, and Wenyun Sun. Zerops: High-quality cross-modal knowledge transfer for zero-shot 3d part segmentation. arXiv:2311.14262, 2023. Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. Sam3d: Segment anything in 3d scenes. arXiv preprint arXiv:2306.03908, 2023."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. Meshsegmenter: Zero-shot mesh semantic segmentation via texture synthesis. In ECCV, 2024. Pengwei Zhou, Xiao Dong, Juan Cao, and Zhonggui Chen. Met: mesh transformer with an edge. The Visual Computer, 39(8):32353246, 2023a. Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. Partslip++: Enhancing low-shot 3d part segmentation via multi-view instance segmentation and maximum likelihood estimation. arXiv:2312.03015, 2023b. Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, and Hao Su. Point-sam: Promptable 3d segmentation model for point clouds, 2024. URL https://arxiv.org/ abs/2406.17741."
        }
    ],
    "affiliations": [
        "HKU",
        "NJU",
        "ShanghaiTech",
        "Tencent Hunyuan",
        "ZJU"
    ]
}