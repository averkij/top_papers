{
    "paper_title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
    "authors": [
        "Elif Nebioglu",
        "Emirhan Bilgiç",
        "Adrian Popescu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X."
        },
        {
            "title": "Start",
            "content": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Elif Nebioglu * 1 Emirhan Bilgic * 2 3 Adrian Popescu"
        },
        {
            "title": "Abstract",
            "content": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit dramatic drop in accuracy (e.g., from 91% to 55%), frequently approaching chance level. We provide theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https: //github.com/emirhanbilgic/INP-X. 6 2 0 2 0 3 ] . [ 1 2 9 1 0 0 . 2 0 6 2 : r 1. Introduction Inpainting, the task of filling masked regions with plausible content, has become mainstream image editing capability through diffusion-based tools (Rombach et al., 2022; Podell et al., 2024) and commercial services (DeepImage, 2026). *Equal contribution 1Independent Researcher 2Universite Sorbonne, Pierre et Marie Curie, ISIR 3Institut Polytechnique de Paris, U2IS 4Universite Paris-Saclay, CEA, LIST. Correspondence to: Elif Nebioglu <elifnebiogllu@gmail.com>, Emirhan Bilgic <emirhan.bilgic@ip-paris.fr>. Preprint. February 3, 2026. This accessibility poses significant risks for misinformation and content authenticity (Verdoliva, 2020), making reliable detection of inpainted content critical. Both academic methods (Ojha et al., 2023; Corvi et al., 2023) and commercial APIs (SightEngine, 2026; Hive Moderation, 2026) report high detection accuracy (>90%) on benchmark datasets. However, fundamental question remains: what are these detectors actually learning? For inpainting, which synthesizes only local region, we would expect detectors to focus on the generated content within the edited region. Instead, we show that many state-of-the-art detectors rely primarily on global artifacts introduced by the image generation pipeline, exploiting form of shortcut learning (Geirhos et al., 2020) that undermines their intended purpose. This behavior occurs because diffusion-based inpainting processes the entire image through VAE encoder-decoder, leading to subtle yet widespread spectral shift across the image. This global fingerprint provides trivial detection signal that bypasses the need to identify locally generated content and calls into question the robustness of AI-generated image detectors. We address this problem with the following contributions: We introduce INP-X, illustrated in Figure 1, that surgically restores original pixels outside the edited region while preserving the generated content within the mask. If detectors truly identify synthetic content, they should spot it in exchanged images since the fake content remains intact. We provide theoretical analysis and experimental validation linking the observed spectral shift to high-frequency attenuation caused by VAE information bottleneck constraints (Theorem 3.2), and show that our exchange operation minimizes distributional divergence by eliminating background artifacts (Theorem 3.4). We construct 90K-image benchmark extending SemiTruths across 4 datasets and 3 inpainting models, with matched real/standard/exchanged triplets. We evaluate 11 pretrained detectors and 2 commercial APIs, demonstrating consistent and severe performance degradation under our attack. Notably, commercial systems (HiveModeration, Sightengine) drop from >91% to 55% accuracy, approaching random chance. AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange 2025b), which leverages spectral distributions as invariant patterns, and Artifact Purification Networks (Meng et al., 2024), which separate artifact features from semantic content via frequency-band proposals. Universal Detectors and Generalization. The quest for detectors that generalize across generators has yielded several influential approaches. (Ojha et al., 2023) proposes learning generator-agnostic representations in CLIPs feature space, achieving strong cross-generator performance. Similarly, (Cozzolino et al., 2024a;b) leverage CLIP embeddings with lightweight classifiers, demonstrating impressive generalization and zero-shot capabilities. However, recent studies question whether CLIP-based detectors rely on artifact cues or semantic shortcuts. (Chu et al., 2025) shows that patch shuffling forces models toward artifact-oriented representations, while (Yan et al., 2025a) introduces NULLspace projection to decouple semantic information from forgery-relevant features. Shortcut Learning and Dataset Biases. Despite reported successes, mounting evidence suggests that detector performance may originate in dataset biases rather than genuine detection capabilities. (Rajan & Lee, 2025) argue that detectors largely ignore real-image features, instead relying on easy-to-learn shortcuts in fake images. (Ha et al., 2024) shows that detectors struggle when generators improve sufficiently. (Yan et al., 2025b) provides rigorous sanity check, revealing that many reported gains stem from dataset biases. (Grommelt et al., 2024) exposes systematic biases related to JPEG compression and resolution. (Guillaro et al., 2025) introduce bias-free training paradigm, while (Li et al., 2024) highlight persistent vulnerabilities. Our work provides controlled intervention that demonstrates the phenomenon in the inpainting setting. Manipulation, Localization, and Forensic Analysis. Beyond binary detection, some work tries to address localizing manipulated regions in images. Classical splice detection methods (Farid, 2009) analyze inconsistencies in JPEG compression artifacts, lighting, and noise patterns. Photo Response Non-Uniformity (PRNU) (Lukaˇs et al., 2006) detects authenticity by verifying sensor-specific noise patterns; our Theorem 3.2 proves VAE reconstruction attenuates such high-frequency sensor noise, explaining why these methods succeed on standard inpainting (PRNU disrupted globally) but fail on INP-X (PRNU restored in background). Noiseprint (Cozzolino & Verdoliva, 2020) extends this by learning camera model fingerprints robust to in-camera processing. The objective of our work is different: we do not propose new localization method; rather, we demonstrate that existing general-purpose detectors fail to localize inpainted content and instead rely on global VAE artifacts. Figure 1. a) Overview of the INP-X pipeline. The unmasked regions of the generated output are replaced with the corresponding regions from the original image, preserving original content while retaining the synthesized masked area. b) The difference between Original-Inpainted and Original-Ours. c) INP: Inpainting Pipeline. We demonstrate that training detectors on INP-X images forces models to learn local content features rather than global shortcuts, significantly improving both crossdistribution robustness and manipulation localization (mIoU, mAP). However, detecting INP-X images remains challenging, underscoring the relevance of the proposed image-editing operation. these contributions suggest Taken together, that AIgenerated image detection remains far from solved. Robust evaluation frameworks should therefore go beyond clean generated images and include realistic post-edits, such as INP-X, that reveal failure modes and mirror the kinds of modifications likely to appear in real deployments. 2. Related Work AI-Generated Image Detection. The detection of AIgenerated images starts with the observation that generative models leave characteristic fingerprints in their outputs. (Marra et al., 2019) first identified model-specific fingerprints exploitable for detection. (Frank et al., 2020) revealed that these fingerprints manifest as severe artifacts in the frequency domain, attributable to upsampling operations. (Wang et al., 2020) showed that classifier trained on single GAN can generalize to unseen architectures, suggesting CNN-based generators share common statistical flaws. With the rise of diffusion models, (Corvi et al., 2023) extended this analysis, finding distinct spectral characteristics from the iterative denoising process. (Wang et al., 2023) introduced the Diffusion Reconstruction Error, taking advantage of the observation that diffusion-generated images can be reconstructed more accurately than real images. More recent work includes SPAI (Karageorgiou et al., AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Original Image Standard Inpainting Figure 2. Impact of standard inpainting on unmasked regions. Even though the mask primarily targets specific areas (nose in this case), the VAE encoding-decoding cycle inherently alters the pixel values of the entire image (unmasked regions, such as the change in the eye shown in the figure), distinguishing them from the original. Adversarial and Anti-Forensic Methods. Prior work on evading AI-generated image detectors has predominantly focused on post-processing perturbations. (Hou et al., 2023) demonstrate that adversarial perturbations can evade deepfake detectors, while anti-forensic methods (Stamm et al., 2013) explore various evasion strategies. These approaches share common paradigm: they add perturbations (noise, blur, compression) or degrade images to disrupt detection. INP-X differs from these methods in that it removes the global artifact by restoring the original pixels outside the inpainted region rather than adding perturbations. Our goal is diagnostic rather than adversarial. We expose what detectors actually learn, revealing that they may not be detecting the generated content at all. Benchmarks for Manipulation Detection. Evaluating detector robustness requires carefully constructed benchmarks. GenImage (Zhu et al., 2023) provides million-scale dataset for cross-generator evaluation with controlled splits that address JPEG and resolution biases. For partial image manipulation, Semi-Truths (Pal et al., 2024) pairs real images with AI-inpainted counterparts and corresponding masks. By applying INP-X to Semi-Truths, we disentangle these factors, providing benchmark that isolates the detectors ability to identify what was generated rather than how it was processed. 3. Theoretical Analysis We first define latent diffusion inpainting. Then, we formally analyze the source of global artifacts in Latent Diffusion Models (LDMs) and prove why INP-X minimizes the detectability of generated images. 3.1. Latent Diffusion Inpainting Let RHW 3 be real image and {0, 1}HW be binary mask where 1 denotes the region to inpaint. Latent Diffusion Models (LDMs) operate in compressed latent space Z. The process involves an encoder E, decoder D, and denoiser ϵθ. The inpainted image is generated via: = D(ReverseDiffusion(E(x), )). (1) Crucially, even if the latent diffusion process perfectly preserves the unmasked latents, the final image is reconstructed by the decoder D(), which affects the entire spatial domain. As illustrated in Figure 2 and observed in (Hou et al., 2025), this process introduces subtle yet pervasive changes even in the unmasked regions, thereby differentiating them from the original source. 3.2. Spectral Bias in VAE Reconstruction Standard LDMs utilize an autoencoder = to map between pixel space and latent space Z. We model real image as superposition of semantic signal and stochastic sensor noise (e.g., photon shot noise, PRNU), such that = + n, where (0, Σnoise). Definition 3.1 (Spectral Variance Gap). Let F[x](ω) be the Fourier transform of image at frequency ω. We define the expected spectral power spectrum as Sx(ω) = E[F[x](ω)2]. Theorem 3.2 (Variance Contraction in VAEs). Let be an autoencoder trained to minimize reconstruction objective dominated by the L2 norm (MSE), i.e., = (x)2. Assuming the latent code = E(x) effectively captures the semantic signal but is approximately independent of the stochastic high-frequency noise due to the information bottleneck, the reconstruction = (x) satisfies: Sx(ω) Sx(ω) ω Ωhigh, (2) where Ωhigh denotes the frequency band dominated by sensor noise n. Proof. The L2-optimal reconstruction approximates the conditional expectation of the posterior: E[x z]. Using the Law of Total Variance, we decompose the variance of the real data: Var(x) = Var(E[xz]) + E[Var(xz)]. (3) The first term, Var(E[xz]), represents the variance captured by the reconstruction x. The second term, E[Var(xz)], represents the irreducible error (information lost in the bottleneck). Since sensor noise is stochastic and non-semantic, it is not encoded in z. Therefore, the variance associated with falls into the residual term E[Var(xz)]. In the frequency domain, since is high-frequency dominant, the power spectrum of the reconstruction Sx(ω) must be strictly lower than Sx(ω) for frequencies where noise dominates signal. Remark 3.3 (Extension to Modern VAEs). Modern VAEs employ hybrid losses (perceptual (Johnson et al., 2016) + 3 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange adversarial + L2), which violate the strict MSE-dominance assumption of Theorem 3.2. However, high-frequency attenuation persists in practice for two reasons: (1) perceptual losses operate on low-pass VGG features that inherently ignore sensor noise, and (2) the spatial compression (typically 8 downsampling factor (Rombach et al., 2022), yielding 64 reduction in spatial dimensions) imposes fundamental information-theoretic limit on encoding non-semantic high-frequency content. Our empirical validation (Figure 3) confirms this using Stable Diffusion 1.4s VAE, and Section A.10 extends the analysis to SDXL and FLUX.1, showing the phenomenon persists across architectures. Empirical Validation: VAE as the Source of Artifacts. To rigorously validate that the observed global artifacts stem specifically from the VAE encoding-decoding process (as predicted by Theorem 3.2), we conduct controlled experiment using the pre-trained Variational Autoencoder of Stable Diffusion v1.4. We isolate the VAE component and apply strict encoding and decoding to real images, without any diffusion or inpainting steps. We analyze the correlation between three key signals: (1) the inpainting difference (standard inpainting - original), (2) the image highfrequency content, and (3) the pure VAE reconstruction loss. Figure 3 visualizes these components, showing striking spatial structural similarity between the VAE reconstruction error and the artifacts observed in standard inpainting. The quantitative analysis confirms this visual intuition (Figure 4). We computed Pearson (r) and Spearman (ρ) correlation coefficients at the image level (mean error) and pixel level. At the image level, we observe very strong correlation between VAE Loss and High Frequency content (r = 0.941), supporting our claim that the VAE struggles to reconstruct high-frequency details. There is also significant correlation between VAE Loss and Inpainting Loss (r = 0.600), indicating that VAE error is major component of the total inpainting error. level, we compute per-image correlathe pixel At tions and report their distribution across the dataset (mean std): VAEInpaint (Pearson r=0.450.23, Spearman ρ=0.550.21), VAEHighFreq (r=0.520.08, ρ=0.540.10), and InpaintHighFreq (r=0.330.15, ρ=0.440.15). We note that these pixel-level correlations are computed over the entire image, including the masked region, where diffusion-generated content introduces additional variance uncorrelated with VAE reconstruction error, partially attenuating the measured correlation. When restricted to unmasked pixels, we expect even stronger correspondence. These results empirically validate that the shaving off of the spectral power spectrum is indeed direct consequence of the VAEs compression mechanism, which preferentially 4 filters out high-frequency sensor noise that holds no semantic value but is crucial for forensic authenticity. 3.3. INP-X and Divergence Minimization We analyze the detectability of the image using the KullbackLeibler (KL) divergence between the distribution of real images Preal and the manipulated distribution Pmanip. lower divergence implies the distributions are harder to distinguish. Let the image domain Ω be partitioned into the background Ωbg (where mask = 0) and the foreground Ωf (where = 1). Let xbg and xf denote the pixel content in these regions. Theorem 3.4 (Divergence Reduction via Exchange). Let Pstd be the distribution of standard inpainted images and Pex be the distribution of images generated via Inpainting Exchange. Under the assumption that standard inpainting introduces non-zero spectral shift in the background (from Theorem 3.2), we have: DKL(PrealPex) < DKL(PrealPstd). (4) Proof. We utilize the Chain Rule for KL Divergence to decompose the total divergence over the joint distribution of background and foreground pixels: DKL(P Q) = DKL(P (xbg)Q(xbg)) + ExbgP [DKL(P (xf gxbg)Q(xf gxbg))]. (5) Case 1: Standard Inpainting (Q = Pstd). The background is reconstructed via the VAE. As per Theorem 3.2, the marginal distribution Q(xbg) differs from (xbg) due to spectral attenuation. Thus, the first term DKL(P (xbg)Pstd(xbg)) > 0. Since Ωbg typically covers the majority of the image, this term significantly impacts total divergence. Case 2: Inpainting Exchange (Q = Pex). By definition, the background pixels are copied exactly from the real image: xex bg = xbg. Therefore, the marginal distributions are identical: Pex(xbg) Preal(xbg). This forces the first term to zero: DKL(Preal(xbg)Pex(xbg)) = 0. (6) Consequently, the total divergence for our method is reduced to the conditional divergence of the foreground, whereas standard inpainting suffers from divergence across the entire spatial domain. AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 3. Verification of VAE-induced artifacts. Columns from left to right: (1) Original Image, (2) Mask, (3) Inpainted Result, (4) High-Frequency Filters of the Original Image, (5) VAE Reconstruction Artifacts (decoding encoded original), (6) Difference Map (Inpainted - Original). Note the strong structural correlation between the VAE artifacts and the final inpainting difference, confirming the VAE as the primary source of global noise. 4. Experiments and Analysis 4.1. Dataset Construction We extend the Semi-Truths dataset to construct 90Kimage benchmark across 4 datasets: CelebA-HQ (Karras et al., 2018), CityScapes (Cordts et al., 2016), OpenImages (Kuznetsova et al., 2020), and SUN-RGBD (Song et al., 2015). This combination ensures topical diversity and supports robust evaluation. The dataset includes 30k samples for each subset: (1) real images (x), standard inpainted images (x), exchanged inpainted images (xex). All images are paired with masks, enabling controlled evaluation of detector behavior. Inpainting is performed using three models: Kandinsky 2.2 (Razzhigaev et al., 2023), OpenJourney (PromptHero, 2023), and Stable Diffusion v1.4 (Rombach et al., 2022). 4.2. Experimental Setup We evaluate the capacity of AI-generated image detectors to discriminate between real and images edited with standard and inpainting exchange, respectively. First, we test 11 open-source pretrained detectors and two commercial APIs 1 in binary classification setting. Second, we finetune four detectors using subsets of and xex data for training, and test both inpainting localization and classification. These tests enable comprehensive evaluation of the detectors behavior when presented with INP-X-edited images. We complement them with thorough analysis of factors influencing performance, including robustness to other corruptions, spectral analysis of inpainting artifacts, and the impact of mask size and dataset. 4.3. Evaluation of Pretrained Detectors The performance of the detectors tested in Table 1 varies widely for standard inpainting, with several performing at near-random levels, and Corvi2023 being notable exception. The INP-X editing significantly degrades performance across the board, with accuracy scores ranging from random1We tested commercial APIs with 1,000 each due to rate limits. 5 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 4. Scatter plots showing strong correlations between VAE reconstruction loss and inpainting artifacts. Table 1. Evaluation of pretrained detectors classification performance for real vs. standard inpainting (INP) and Inpainting Exchange (INP-X), respectively. DETECTOR DATA ACC AUC PREC REC F1 SPAI (KARAGEORGIOU ET AL., 2025A) CO-SPY (CHENG ET AL., 2025) DNF (ZHANG & XU, 2024) SYNTHBUSTER (BAMMEY, 2024) CORVI2023 (CORVI ET AL., 2023) CLIP 10 (COZZOLINO ET AL., 2024A) CLIP 10+ (COZZOLINO ET AL., 2024A) UNIVFD (OJHA ET AL., 2023) DEFAKE (SHA ET AL., 2023) CROSS-EFFICIENTVIT (COCCOMINI ET AL., 2022) CNNDETECTION (WANG ET AL., 2020) INP 0.661 INP-X 0.542 INP 0.549 INP-X 0.532 0.710 INP INP-X 0.604 INP 0.615 INP-X 0. INP 0.942 INP-X 0.554 0.556 INP INP-X 0.509 INP 0.658 INP-X 0.563 INP 0.552 INP-X 0.550 INP 0.581 INP-X 0.536 INP 0.502 INP-X 0. INP 0.503 INP-X 0.501 0.743 0.567 0.649 0.594 0.779 0.643 0.681 0.619 0.989 0. 0.845 0.606 0.844 0.673 0.632 0.592 0.673 0.589 0.511 0.508 0.506 0. 0.638 0.546 0.638 0.613 0.724 0.607 0.586 0.559 0.995 0.959 0.944 0. 0.895 0.800 0.746 0.739 0.649 0.579 0.524 0.517 0.838 0.745 0.744 0. 0.229 0.224 0.697 0.602 0.783 0.741 0.890 0.114 0.119 0.025 0.358 0. 0.159 0.154 0.351 0.262 0.052 0.050 0.007 0.004 0.687 0.525 0.337 0. 0.710 0.604 0.670 0.637 0.939 0.203 0.211 0.048 0.512 0.279 0.262 0. 0.456 0.360 0.094 0.092 0.013 0.008 Differences between INP and INP-X are significant (p 0.05, paired t-test). level to only 0.604 for DNF. Notably, purely frequencybased methods, such as Corvi2023, show significant drops, validating our hypothesis that they rely on the global spectral artifacts we eliminate. The individual metric analysis shows that performance degradation is attributable to low recall in majority of cases. The tested detectors use different deep learning architectures and training strategies, yet all fail to detect INP-X. In Table 2, we evaluate two leading commercial detection APIs. (Li et al., 2024) identified Sightengine as topperforming commercial detector, while (Ha et al., 2024) highlights HiveModeration as achieving state-of-the-art accuracy on organic AI-generated images. Both detectors perform well on standard inpainting, with scores close to the best ones reported in Table 2. However, they too suffer catastrophic drop in performance when tested with INP-X edited. This result confirms the hypothesis derived in Section 3: commercial models likely use high-frequency noise analysis, which is globally disrupted by the VAE in standard inpainting but is restored by our exchange method. Globally, the results in Tables 1 and 2 show that INP-X is very challenging for pretrained detectors. Table 2. Commercial API Results (N=1000). Comparison of Standard Inpainting vs. our Inpainting Exchange. DETECTOR DATA ACC AUC PREC RECALL F1 HIVE MODERATION 0.914 INP INP-X 0.548 0.921 0. 0.993 0.944 0.834 0.102 0.906 0.184 INP 0.926 INP-X 0.550 0.921 SIGHTENGINE 0.212 Differences are significant (p 0.05, bootstrap test, 1000 samples). 0.862 0. 0.935 0.588 0.989 0.923 4.4. Fine-tuned Detectors We trained four detectors using pretrained models with CLIP ViT B/32, ViT B/16, EfficientNet, and ResNet-50 backbones to detect standard inpainted and INP-X images. We provide training details in Section A.2. Table 3 results reveal several findings. First, the fine-tuned detectors have better performance than their pretrained counterparts tested in Table 1. However, detecting INP-Xedited images remains difficult even for specifically trained detectors, with the best accuracy score at 0.753. Second, standard inpainting was learned much more easily, achieving high classification performance on the standard inpainting test set, particularly with CLIP and ResNet-50 backbones. This result suggests that the global artifact is trivial signal for deep networks to fit. Third, we find asymmetric transferability. Models trained on standard inpainting exhibit catastrophic performance drops when evaluated on INP-X, with the best accuracy of only 0.603 for the ViT In contrast, models trained on InpaintB/16 backbone. ing Exchange perform better on standard inpainting (e.g., ResNet-50 achieves 0.745 accuracy). This finding indicates that training on local artifacts (INP-X) generalizes better AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Table 3. Evaluation of detectors trained with standard inpainting and INP-X. We report both classification metrics (Accuracy, AUC, F1) and localization metrics (mIoU, mAP). Table 4. Robustness of top-performing detectors and commercial APIs to various corruptions compared to Inpainting Exchange. Standard corruptions (Blur, Light, JPEG) are generally handled well, whereas our attack evades detection. ARCH. TRAIN TEST ACC AUC F1 MIOU MAP DETECTOR ATTACK ACC AUC PREC REC CLASSIFICATION LOCALIZATION CLIP VIT B/32 VIT B/16 EFFICIENTNET 0.984 INP INP INP INP-X 0.598 INP-X INP-X 0.753 0.688 INP-X INP INP 0.819 INP INP INP-X 0.603 INP-X INP-X 0.630 0.644 INP-X INP INP 0.952 INP INP INP-X 0.567 INP-X INP-X 0.672 0.676 INP-X INP 0.998 0.825 0.890 0.831 0.908 0.689 0.709 0.742 0.991 0.749 0.771 0.821 0.984 0.350 0.795 0. 0.826 0.519 0.701 0.716 0.952 0.296 0.698 0.702 0.373 0.376 0.380 0.382 0.393 0.393 0.405 0.404 0.466 0.476 0.486 0.479 0.201 0.205 0.212 0.224 0.226 0.227 0.230 0.229 0.369 0.352 0.408 0.392 RESNET-50 0.308 0.973 INP INP 0.350 INP INP-X 0.557 0.382 INP-X INP-X 0.712 0.353 0.745 INP-X INP Results averaged over 3 runs. Localization improvement over INP-trained baseline is modest but significant (p 0.05, t-test). 0.451 0.463 0.472 0.463 0.973 0.237 0.750 0. 0.996 0.682 0.838 0.858 than training on global artifacts. Fourth, models trained on INP-X achieve better localization performance than those trained on standard inpainting. Illustrated in Figure 11. Removing the global shortcut forces the models to learn the actual content discrepancies within the mask, leading to more precise localization of the manipulated region. The advantage of INP-X training is larger for CNN backbones than for transformer backbones. We also observe that CNNbased models consistently achieve superior localization performance compared to ViTs  (Table 3)  , aligning with prior findings on the limitations of transformer interpretability for precise spatial attribution (Chefer et al., 2021). We detail this analysis in Section A.12. 4.5. Robustness to Other Corruptions. To further assess the robustness of the top-performing detectors (SPAI, Corvi2023, CLIP10, CLIP10+) and commercial APIs (Sightengine, Hive), we subjected them to additional common image corruptions. We applied (1) Gaussian Blur (σ = 3, µ = 0) as standard degradation (Hendrycks & Dietterich, 2019), (2) Gaussian Light Spot attack to simulate localized illumination anomalies, inspired by (Li et al., 2023), and (3) JPEG compression at quality 80. The light spot attack adds spatially localized intensity gain defined by (x, y) = I(x, y) (1 + (A 1)e d2 2r2 ), where is the Euclidean distance from random center, = 120 is the spot radius, and = 1.5 is the peak intensity multiplier. Results are presented in Table 4. We observe that while Gaussian Blur and Light Spot attacks degrade performance for some detectors (e.g., SPAI drops on light spot), they are generally much less effective than our Inpainting Exchange. Notably, commercial APIs remain highly robust to these standard corruptions (Accuracy > 72% for Sightengine, SPAI (KARAGEORGIOU ET AL., 2025A) CORVI2023 (CORVI ET AL., 2023) CLIP 10 (COZZOLINO ET AL., 2024A) CLIP 10+ (COZZOLINO ET AL., 2024A) SIGHTENGINE BLUR LIGHT JPEG INP-X BLUR LIGHT JPEG INP-X BLUR LIGHT JPEG INP-X BLUR LIGHT JPEG INP-X BLUR LIGHT JPEG INP-X 0.712 0.553 0.699 0.542 0.768 0.924 0.803 0. 0.539 0.590 0.503 0.509 0.607 0.676 0.607 0.563 0.724 0.780 0.831 0.548 0.813 0.576 0.797 0.567 0.944 0.985 0.951 0.519 0.762 0.897 0.619 0. 0.728 0.879 0.770 0.673 0.831 0.918 0.864 0.588 0.668 0.551 0.661 0.546 0.991 0.994 0.992 0.959 0.924 0.964 0.662 0.778 0.858 0.903 0.859 0. 0.975 0.983 0.984 0.944 0.846 0.516 0.820 0.506 0.541 0.852 0.611 0.114 0.085 0.186 0.014 0.025 0.255 0.393 0.256 0.169 0.406 0.570 0.680 0. 0.746 0.533 0.732 0.525 0.700 0.918 0.756 0.203 0.155 0.312 0.027 0.048 0.394 0.548 0.395 0.279 0.574 0.722 0.804 0.184 HIVE MODERATION BLUR LIGHT JPEG INP-X 0.905 0.896 0.895 0.884 0.885 0.871 0.555 0.212 JPEG compression at quality 80. Interestingly, SPAI performs better under JPEG (0.699 Acc) than on INP (0.661 Acc). 0.988 0.988 0.987 0.923 0.913 0.901 0.890 0.588 0.820 0.800 0.780 0.120 > 89% for Hive Moderation) but fail catastrophically on our attack (Accuracy 55%). This finding confirms that Inpainting Exchange targets fundamental vulnerability: restoring high-frequency background statistics, rather than simply degrading image quality. To verify that the performance drop is not due to edge artifacts, we conducted additional ablation studies using Soft Alpha Blending (Edge Blurring). It resulted in similarly low detection scores, confirming that edge discontinuities are not the primary signal used by detectors. We provide detailed results and methodology provided in Section A.11. 4.6. Spectral Analysis of Inpainting Artifacts To validate the theoretical insights of Theorem 3.2 regarding spectral bias, we analyzed the frequency domain characteristics of the inpainted images using specialized detection pipeline adapted from (Durall et al., 2020). The analysis proceeds in three steps: First, all images are resized to 512512 resolution. We then apply pixel-wise cross-difference highpass filter (CDi,j = Ii,j Ii+1,j Ii,j+1 + Ii+1,j+1) to suppress semantic content and accentuate high-frequency generative artifacts. Finally, we compute the 2D FFT, normalize the spectra, and average them across the dataset to reveal global spectral fingerprints (Bammey, 2024). Vulnerability to Spectral Detection: As observed in the spectral heatmaps (Figure 13), standard latent diffusion inpainting introduces distinct periodic grid-like artifacts, AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange manifesting as bright uniform peaks. These artifacts are not merely visual imperfections; they act as traceable fingerprints for forensic detectors. Works such as Synthbuster (Bammey, 2024) explicitly exploit similar FFT-based spectral disparities to train robust classifiers that distinguish diffusion-generated content from real images. As shown in Table 5, our method suppresses these artifacts (improving spectral MSE by 11 on SUN-RGBD), effectively eliminating the primary signal leveraged by such detectors. 4.7. Impact of Mask Size and Dataset Bias We further investigated the relationship between mask size and detector performance. As shown in Figure 5, increasing the mask size generally correlates with higher detection accuracy, particularly for the Inpainting Exchange method. This finding is intuitive: as the mask size increases, the proportion of exchanged (real) pixels decreases, leaving more generated content for the detector to latch onto. Conversely, smaller masks in our exchange method are harder to detect because they retain more of the original, highfrequency clean pixels. We also inspected the effect of different datasets on performance. We provide them, along with additional mask-size analysis metrics, in the Appendix (Section A). We further investigated whether these spectral discrepancies correlated with the extent of the inpainted region. Surprisingly, our analysis reveals an inverse relationship in some cases. While CelebA-HQ exhibits narrower spectral gap  (Table 5)  , its mean mask ratio (µm 0.10) is larger than that of SUN-RGBD (µm 0.06). Despite having smaller inpainted regions on average, SUN-RGBD displays far more severe spectral artifacts in standard inpainting. This result confirms that the observed spectral bias is not trivial function of mask size, but rather stems from the models struggle with complex frequency distributions. The muted gap in CelebA-HQ likely arises from the intrinsic spectral noise introduced by its alignment and super-resolution preprocessing, which creates noisy baseline that partially obscures generative artifacts. Table 5. Spectral Difference Scores (MSE 1000). We measure the divergence between the frequency spectra of real vs. manipulated images. DATASET SEMI-TRUTHS INP-X CELEBA-HQ CITYSCAPES OPENIMAGES SUN-RGBD 15.3093 11.4707 17.1325 23.4154 12.8710 5.8937 1.9152 1.9436 MSE reduction is less pronounced in CelebA-HQ, suggesting that the VAE can be better optimized for faces. However, in diverse scenes (OpenImages, SUN-RGBD), the VAE struggles to preserve high-frequency texture, which (a) Accuracy vs Mask Size (INP) (b) Accuracy vs Mask Size (INP-X) Figure 5. Impact of mask coverage on detection accuracy. Larger masks generally enable better detection, but our exchange method consistently degrades performance compared to standard inpainting across all sizes. our method explicitly corrects. 5. Limitations limitation of our current study is that the dataset focuses on VAE-based architectures. While currently less efficient and significantly less used, deeper analysis of non-VAE architectures, such as component-based or pixel-space models, is warranted. While addressable by alpha blending (see Section A.11), subtle boundary effects may still occur depending on mask and blending, and we do not claim perceptual quality. Finally, while our intent is diagnostic, the exchange operation could be used to evade existing detectors. We view this risk as intrinsic to vulnerability analysis and as further motivation for content-aware and localization-based detection approaches. 6. Conclusion We expose fundamental vulnerability in AI detectors: reliance on global VAE encoding-decoding artifacts rather than local content. By removing these artifacts via INP-X, we show that current detectors are significantly less robust. And training models with INP-X improves robustness and localization performance. Future work can focus on model improvement, such as frequency-preserving VAEs or deAI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange coding strategies that explicitly enforce spectral consistency."
        },
        {
            "title": "Impact Statement",
            "content": "This paper studies AI-generated image detection in the context of inpainting with implications for misinformation. Our findings reveal existing detectors vulnerability. We expect these insights to promote the development of more contentaware detection methods. potential negative impact lies in the facilitation of evasion attacks against existing detectors. However, we believe that this potential risk also motivates future research on robust localization-based detectors. Our released benchmark provides concrete tools for this effort."
        },
        {
            "title": "References",
            "content": "Abnar, S. and Zuidema, W. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 41904197, 2020. Bammey, Q. Synthbuster: Towards detection of diffusion model generated images. IEEE Open Journal of Signal Processing, 5, 2024. Chefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782791, 2021. Cheng, S., Lyu, L., Wang, Z., Zhang, X., and Sehwag, V. Co-spy: Combining semantic and pixel features to detect synthetic images by ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1345513465, June 2025. Chu, B., You, W., Li, M., Zheng, T., Zhao, K., Xu, X., Lu, Z., Song, J., Xu, M., and Zhou, L. When semantics regulate: Rethinking patch shuffle and internal bias for generated image detection with clip. arXiv preprint arXiv:2511.19126, 2025. Coccomini, D. A., Messina, N., Gennaro, C., and Falchi, F. Combining efficientnet and vision transformers for video deepfake detection. In International Conference on Pattern Recognition, pp. 219229. Springer International Publishing, 2022. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. Corvi, R., Cozzolino, D., Zingarini, G., Poggi, G., Nagano, K., and Verdoliva, L. On the detection of synthetic images generated by diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Cozzolino, D. and Verdoliva, L. Noiseprint: cnn-based camera model fingerprint. IEEE Transactions on Information Forensics and Security, 15:144159, 2020. doi: 10.1109/TIFS.2019.2916364. Cozzolino, D., Poggi, G., Corvi, R., Nießner, M., and Verdoliva, L. Raising the bar of ai-generated image detection In IEEE/CVF Conference on Computer Viwith clip. sion and Pattern Recognition Workshops, pp. 43564366, 2024a. Cozzolino, D., Poggi, G., Nießner, M., and Verdoliva, L. Zero-shot detection of ai-generated images. In European Conference on Computer Vision, 2024b. DeepImage. Inpainting image //deep-image.ai/app/tools/inpainting, 2026. Accessed: January 2026. ai. https: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. Durall, R., Keuper, M., and Keuper, J. Watch your upconvolution: Cnn based generative models yield artificial high frequency patterns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. Farid, H. Image forgery detection. IEEE Signal processing magazine, 26(2):1625, 2009. Frank, J., Eisenhofer, T., Schonherr, L., Fischer, A., Kolossa, D., and Holz, T. Leveraging frequency analysis for deep fake image recognition. In International Conference on Machine Learning, pp. 32473258. PMLR, 2020. Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. Grommelt, P., Weiss, L., Pfreundt, F.-J., and Keuper, J. Fake or jpeg? revealing common biases in generated image detection datasets. In European Conference on Computer Vision Workshops, 2024. Guillaro, F., Zingarini, G., Usman, B., Sud, A., Cozzolino, D., and Verdoliva, L. bias-free training paradigm for more general ai-generated image detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 9 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Ha, A. Y. J., Passananti, J., Bhaskar, R., Shan, S., Southen, R., Zheng, H., and Zhao, B. Y. Organic or diffused: Can we distinguish human art from ai-generated images? arXiv preprint arXiv:2402.03214, 2024. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. Hive Moderation. Ai-generated content detechttps://hivemoderation.com/ 2026. tion. ai-generated-content-detection, Accessed: January 2026. Hou, X., Wu, J., Liu, B., Zhang, Y., Song, G., Liu, Y., Liu, Y., and You, H. Towards seamless borders: method for mitigating inconsistencies in image inpainting and outpainting. arXiv preprint arXiv:2506.12530, 2025. Hou, Y., Guo, Q., Huang, Y., Xie, X., Ma, L., and Zhao, J. Evading deepfake detectors via adversarial statistical consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pp. 694711. Springer, 2016. Karageorgiou, D., Papadopoulos, S., Kompatsiaris, I., and Gavves, E. Any-resolution ai-generated image detection by spectral learning. arXiv preprint arXiv:2411.19417, 2025a. Karageorgiou, D., Papadopoulos, S., Kompatsiaris, I., and Gavves, E. Any-resolution ai-generated image detection by spectral learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025b. Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=Hk99zCeAb. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al. The open images dataset v4: Unified image classification, object detection, and visual reInternational journal of lationship detection at scale. computer vision, 128(7):19561981, 2020. 10 Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., Kulal, S., Lacey, K., Levi, Y., Li, C., Lorenz, D., Muller, J., Podell, D., Rombach, R., Saini, H., Sauer, A., and Smith, L. Flux.1 kontext: Flow matching for incontext image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Li, Y., Yang, F., Liu, Q., Li, J., and Cao, C. Light can be dangerous: Stealthy and effective physical-world adversarial attack by spot light. Computers & Security, 132: 103345, 2023. Li, Y., Liu, Z., Zhao, J., Ren, L., Li, F., Luo, J., and Luo, B. The adversarial ai-art: Understanding, generation, detection, and benchmarking. arXiv preprint arXiv:2404.14581, 2024. Lugmayr, A., Danelljan, M., Romero, A., Fisher, Y., Timofte, R., and Van Gool, L. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1146111471, 2022. Lukaˇs, J., Fridrich, J., and Goljan, M. Digital camera identification from sensor pattern noise. IEEE Transactions on Information Forensics and Security, 1(2):205214, 2006. Mallat, S. Wavelet Tour of Signal Processing. Academic Press, 2nd edition, 1999. Mallat, S. G. theory for multiresolution signal decomposition: The wavelet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(7):674 693, 1989. Marra, F., Gragnaniello, D., Verdoliva, L., and Poggi, G. Do gans leave artificial fingerprints? In IEEE Conference on Multimedia Information Processing and Retrieval, pp. 506511, 2019. Meng, Z., Peng, B., Dong, J., and Tan, T. Artifact feature purification for cross-domain detection of ai-generated images. Computer Vision and Image Understanding, 2024. Ojha, U., Li, Y., and Lee, Y. J. Towards universal fake image detectors that generalize across generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Pal, A., Kruk, J., Phute, M., Bhattaram, M., Yang, D., Chau, D. H., and Hoffman, J. Semi-truths: large-scale dataset of ai-augmented images for evaluating robustness of aigenerated image detectors. In Advances in Neural Information Processing Systems, 2024. AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Perez, P., Gangnet, M., and Blake, A. Poisson image editing. ACM Transactions on Graphics (Proceedings of SIGGRAPH), 22(3):313318, 2003. Stamm, M. C., Wu, M., and Liu, K. J. R. Information forensics: An overview of the first decade. IEEE Access, 1:167200, 2013. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2024. Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pp. 61056114. PMLR, 2019. Verdoliva, L. Media forensics and deepfakes: An overview. IEEE Journal of Selected Topics in Signal Processing, 14 (5):910932, 2020. Wang, S.-Y., Wang, O., Zhang, R., Owens, A., and Efros, A. A. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 86958704, 2020. Wang, Z., Bao, J., Zhou, W., Wang, W., Hu, H., Chen, H., and Li, H. Dire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2244522455, 2023. Yan, J., Wang, F., Jiang, W., Li, Z., and Fu, Z. Ns-net: Decoupling clip semantic information through null-space for generalizable ai-generated image detection. arXiv preprint arXiv:2508.01248, 2025a. Yan, S., Li, O., Cai, J., Hao, Y., Jiang, X., Hu, Y., and Xie, W. sanity check for ai-generated image detection. In International Conference on Learning Representations, 2025b. Zhang, Y. and Xu, X. Diffusion noise feature: Accurate and fast generated image detection. arXiv preprint arXiv:2312.02625v2, 2024. Zhu, M., Chen, H., Yan, Q., Huang, X., Lin, G., Li, W., Tu, Z., Hu, H., Hu, J., and Wang, Y. Genimage: million-scale benchmark for detecting ai-generated image. In Advances in Neural Information Processing Systems, 2023. PromptHero. Openjourney: Stable diffusion fine-tuned on midjourney images. https://huggingface.co/ prompthero/openjourney, 2023. Accessed: January 2026. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR, 2021. Rajan, A. S. and Lee, Y. J. Stay-positive: case for ignoring real image features in fake image detection. In International Conference on Machine Learning, 2025. Razzhigaev, A., Shakhmatov, A., Maltseva, A., Arkhipkin, V., Pavlov, I., Ryabov, I., Kuts, A., Panchenko, A., Kuznetsov, A., and Dimitrov, D. Kandinsky: An improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618626, 2017. Sha, Z., Li, Z., Yu, N., and Zhang, Y. De-fake: Detection and attribution of fake images generated by text-to-image In Proceedings of the IEEE/CVF generation models. Conference on Computer Vision and Pattern Recognition, 2023. SightEngine. Detect deepfakes automatically. https:// sightengine.com/detect-deepfakes, 2026. Accessed: January 2026. Song, S., Lichtenberg, S. P., and Xiao, J. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 567576, 2015. 11 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange A. Appendix A.1. Why Not Restore After Generation? Our findings raise fundamental question about diffusion-based inpainting pipelines: if the denoising process necessarily operates over the entire latent representation during generation, why is the original image not restored outside the mask as post-processing step? During the generation phase, full-image denoising is required. The diffusion model operates in latent space where spatial locality is entangled, and the VAE decoder must reconstruct the complete image to ensure smooth, coherent boundaries between edited and unedited regions. However, once the generation is complete, there is no fundamental reason why the original pixels outside the mask cannot be restored. Our Inpainting Exchange method demonstrates precisely this: by simply replacing pixels where = 0 with the original image, we eliminate the global artifact entirely. One might argue that naive pixel replacement could introduce visible seams at mask boundaries. However, such edge artifacts can be effectively mitigated using classical image blending techniques such as Poisson editing (Perez et al., 2003), which solves for gradient-domain blend that preserves the generated content while matching the boundary conditions of the original image. We also show in Table 6 that the accuracy drop is not due to edge artifacts. A.2. Implementation Details Model Architectures. We evaluate four detector architectures, all initialized with ImageNet-pretrained weights: ResNet-50 (He et al., 2016): We replace the final fully-connected layer (fc) with linear layer mapping to 2 classes. EfficientNet-B0 (Tan & Le, 2019): We replace classifier[1] with 2-class linear head. ViT-B/16 (Dosovitskiy et al., 2021): We replace heads.head with 2-class linear layer. CLIP ViT-B/32 (Radford et al., 2021): We freeze the vision encoder for 3 epochs of linear probing (lr=103), then fine-tune all parameters for 1 epoch with differential learning rates (encoder: 106, classifier: 104). Training Configuration. All models (except CLIP) are trained for 3 epochs using the Adam optimizer with learning rate of 104 and batch size of 32. We use cross-entropy loss for binary classification. The training set is split 90%/10% for train/validation using stratified sampling. For CLIP, we use AdamW with weight decay 0.01 during fine-tuning. Saliency Methods. For CNN architectures (ResNet-50, EfficientNet-B0), we use Grad-CAM (Selvaraju et al., 2017) with the final convolutional layer as the target (layer4[-1] for ResNet, features[-1] for EfficientNet). For Transformer architectures (ViT, CLIP), we use Attention Rollout (Abnar & Zuidema, 2020), which recursively multiplies attention matrices across all encoder layers and extracts the CLS tokens attention to patch tokens. This provides more interpretable localization for self-attention mechanisms. We discuss the choice further at Section A.12. Localization Metrics. We compute mIoU and mAP by binarizing saliency maps at threshold of 0.5 and compare them against ground-truth masks resized to 224 224 using nearest-neighbor interpolation. A.3. Correlation Analysis We provide additional visualizations of the correlations between VAE loss, inpainting loss, and high-frequency content.Figure 6 shows correlation matrices across datasets, while Figures 7 and 8 present the distribution of pixel-level correlations. Note on Reported Detector Accuracies. The open-source detectors evaluated in this work often report substantially higher accuracies in their original publications. Their reported results are typically obtained on datasets of fully synthetic images (e.g., entire AI-generated faces or scenes) rather than partially edited images, such as inpainting outputs. Detecting inpainting is inherently more challenging, as only fraction of the image contains generated content, and even standard inpainting detection yields near-chance performance for several detectors in our evaluation  (Table 1)  . Our INP-X method further exacerbates this difficulty by eliminating the global VAE artifacts that many detectors implicitly rely on. 12 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 6. Sequence of correlation matrices showing the relationships between different error metrics across datasets. Figure 7. Histograms of pixel-level correlations. Figure 8. Boxplots of pixel-level correlations. A.4. Sub-Dataset Analysis We analyzed the performance of detectors on specific sub-folders within each major dataset to ensure our findings are consistent across diverse data distributions. Figure 9 shows detailed accuracy breakdowns by sub-folder, and Figure 10 reports accuracy on real images across datasets. A.5. Localization Examples Figure 11 presents localization examples showing detector attention via GradCAM. A.6. Spectral Analysis As explained in Section 4.6, we present visual comparison of the frequency spectra of real, standard-inpainted, and Inpainting Exchange images in Figure 13. Standard inpainting exhibits distinct grid-like high-frequency artifacts (bright spots/lines in the spectrum) due to the VAE decoding process. Our method eliminates these artifacts, resulting in spectrum that closely resembles that of the original real image. A.7. JPEG Normalization Motivated by recent findings on format-level biases (Grommelt et al., 2024), all images in our dataset are converted to JPEG format. This normalization is critical for two reasons: (1) real photographs are typically stored as JPEG, while AI-generated images are often saved in lossless PNG format, creating trivial shortcut for detectors (Zhu et al., 2023); and (2) JPEG compression introduces uniform quantization artifacts across both real and generated images, ensuring that detectors must rely on semantic or structural cues rather than format-level statistical differences. By standardizing the compression pipeline, AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 9. Detailed accuracy breakdown by sub-folder for each dataset. we eliminate confounding variables and provide fair evaluation setting that reflects real-world deployment scenarios where images undergo lossy compression during storage and transmission. A.8. VAE-less Inpainting Methods Modern inpainting methods predominantly rely on VAE-based architectures (e.g., Latent Diffusion) for efficiency. However, there exist pixel-space diffusion methods that do not use VAE, such as RePaint (Lugmayr et al., 2022). While these methods theoretically avoid VAE-induced reconstruction artifacts in the unmasked regions (since they operate directly in pixel space), they suffer from significant practical limitations that hinder their widespread adoption: Inference Latency: Pixel-space diffusion is extremely computationally expensive. For single 256 256 image, RePaint takes approximately 9 minutes on P100 GPU. In contrast, Latent Diffusion models (like Stable Diffusion) can generate higher-resolution outputs in less than 30 seconds with the same number of sampling steps. Generalization and Training: RePaint typically requires training separate models for specific datasets (e.g., dedicated model for CelebA-HQ). This lacks the zero-shot generality of large-scale latent models which can handle arbitrary domains. Generation Quality: As shown in Figure 14, while RePaint preserves the background, the inpainting quality itself can be inconsistent compared to state-of-the-art latent models. A.9. Multiresolution Analysis of VAE Artifacts We present wavelet-theoretic analysis of VAE-induced artifacts using the multiresolution framework of (Mallat, 1989; 1999). It provides spatially localized counterpart to the Fourier-domain attenuation established in Theorem 3.2 and connects the observed artifacts to classical rate distortion limits. Definition A.1 (Multiresolution Approximation (Mallat, 1989)). multiresolution approximation (MRA) of L2(R2) is sequence of closed subspaces {Vj}jZ such that: 1. Vj Vj+1 for all (nested structure; increasing corresponds to finer scales), 14 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 10. Accuracy of detectors on Real Images across different datasets. 2. (cid:84) jZ Vj = {0} and (cid:83) jZ Vj = L2(R2), 3. (x) Vj (2x) Vj+1 (scaling property), 4. There exists scaling function ϕ V0 such that {ϕ(x k)}kZ2 is an orthonormal basis of V0. For each j, define the wavelet subspace Wj by Vj+1 = Vj Wj. The subspace Wj captures detail information at spatial scale 2j. Let PWj denote the orthogonal projector onto Wj. For any L2(R2), the wavelet expansion takes the form = + (cid:88) j= PWj x, (7) with convergence in L2. For finite images, the sum is truncated to finite range of scales; we assume standard periodic or symmetric boundary handling. Theorem A.2 (Wavelet attenuation under spatial compression). Let = be an autoencoder whose latent spatial resolution is reduced by factor = 2jc relative to the input. Let Nj denote the number of scalar wavelet coefficients at scale j; for 2D image, Assume: Nj HW 22j , Njc HW r2 . 1. Mean-squared error is the distortion metric; 2. the latent code satisfies an entropy constraint H(Z) (bits); 3. for coefficients at each scale, Gaussian high-rate rate distortion approximation is valid. Then for any fine scale > jc, there exists constant and decoder noise term σ2 0, such that = 2 ln 2 Njc , E(cid:2)PWj x2 2 (cid:3) 4(jjc) E(cid:2)PWj x2 2 (cid:3) + σ2 . (8) 15 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Sketch. By the data-processing inequality, I(PWj X; PWj X) I(X; Z) C. The number of coefficients at scale satisfies Nj Njc 4 jjc. Under the Gaussian rate distortion approximation, the per-coefficient distortion at rate Rj = C/Nj is Dj = σ2 X,j 22Rj , where σ2 X,j = E[PWj X2 2]/Nj is the per-coefficient variance. For small Rj, Thus, the retained energy per coefficient scales as σ2 X,j 2 ln 2 C/Nj. Multiplying by Nj yields 1 22Rj 2 ln 2 Rj. E[PWj x2 2] (2 ln 2)"
        },
        {
            "title": "C\nNjc",
            "content": "4(jjc) E[PWj x2 2]. The additive term σ2 captures decoder hallucination and non-ideal reconstruction effects. Corollary A.3 (Detectability via wavelet modulus maxima). Let be real image and xstd the output of standard VAE-based inpainting, with difference δstd = xstd x. For fine scales > jc, E(cid:2)PWj δstd2 (cid:3) 4(jjc) E(cid:2)PWj x2 2 (cid:3) , so long as the residual energy exceeds the decoder noise floor σ2 maxima appear in the background region Ωbg and propagate across scales. Assume instead that INP-X enforces exact background preservation, i.e. xexΩbg = xΩbg . Then PWj δexΩbg = 0 for all j, and wavelet modulus maxima are confined to the mask boundary and foreground region Ωf g. Remark A.4 (Parseval equivalence and Fourier attenuation). For orthonormal wavelets, . Consequently, statistically significant wavelet modulus 2 = (cid:88) PWj x2 2, which is equivalent (up to normalization constants determined by the Fourier transform convention) to Parsevals identity in the Fourier domain: (cid:90) x2 2 = ˆx(ω)2 dω. R2 Thus, the Fourier-domain spectral gap established in Theorem 3.2 corresponds exactly to attenuation of PWj x2 at fine scales > jc. The wavelet formulation makes this deficit spatially localized, enabling precise detection of where high-frequency information is lost. Remark A.5 (Finite images and boundary effects). For finite-resolution images, wavelet decompositions require boundary handling (e.g. periodic extension or symmetric padding). These affect only O(H + ) coefficients near image borders and do not alter the asymptotic 4(jjc) decay of fine-scale energy, nor the detectability conclusions of Theorem A.3. A.10. VAE Artifacts in Improved Models We extend our analysis to state-of-the-art models, specifically SDXL (Podell et al., 2024) and FLUX.1 (Labs et al., 2025), to investigate if newer architectures with higher channel counts resolve the VAE artifact issue. Section A.10 illustrate the results. Our findings confirm that despite architectural improvements, the correlation between global VAE loss and inpainting error persists, suggesting that this is fundamental property of autoencoding-based generation. For SDXL, we observe the following correlations: Image-Level Correlations (Pearson / Spearman ρ): VAE Loss vs High Freq: = 0.9374, ρ = 0.9009 VAE Loss vs Inpaint Error: = 0.1619, ρ = 0.3672 Inpaint Err vs High Freq: = 0.1609, ρ = 0.3415 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Pixel-Level Correlations (Mean Std): VAE vs Inpaint: Pearson 0.2662 0.1624, Spearman 0.4533 0.1801 VAE vs High Freq: Pearson 0.5602 0.0683, Spearman 0.5468 0.0949 Inpaint vs High Freq: Pearson 0.2456 0.1568, Spearman 0.4202 0.1699 For FLUX.1, which utilizes 16-channel VAE: Image-Level Correlations (Pearson / Spearman ρ): VAE Loss vs High Freq: = 0.9362, ρ = 0.8397 VAE Loss vs Inpaint Error: = 0.1633, ρ = 0.3092 Inpaint Err vs High Freq: = 0.1530, ρ = 0.3372 Pixel-Level Correlations (Mean Std): VAE vs Inpaint: Pearson 0.2114 0.1362, Spearman 0.3059 0. VAE vs High Freq: Pearson 0.4502 0.0724, Spearman 0.3614 0.1501 Inpaint vs High Freq: Pearson 0.2496 0.1533, Spearman 0.4236 0.1689 A.11. Analysis of Edge Artifacts To verify that the observed drop in dramatic accuracy with Inpainting Exchange is not caused by edge artifacts, we conducted additional experiments using advanced blending techniques. We implemented soft blending strategy comprising: Soft Alpha Map Generation: The binary edge band is subjected to Gaussian Blur (σ implicitly calculated from kernel size 5 5) to generate continuous, soft alpha matte α [0, 1]. Soft Alpha Blending (Edge Blurring): low-pass filtered version of the input image (Iblur) is generated via Gaussian smoothing. The final output Iout is synthesized by linearly interpolating between the original sharp image (Iorig) and the blurred version, modulated by the alpha map: Iout = Iorig (1 α) + Iblur α (9) We compared this INP-X against standard edge blurring on the best-performing detectors and commercial APIs. The results in Table 6 show that the performance remains drastically lower than Standard Inpainting. This finding confirms that edge artifacts do not drive the accuracy drop; rather, it is the absence of global VAE traces. A.12. CNN vs. ViT Localization Reliability Our experiments reveal consistent localization gap between CNN-based and ViT-based architectures  (Table 3)  . Across all training conditions, EfficientNet and ResNet-50 achieve substantially higher mIoU (0.450.49) compared to ViT-based models (mIoU 0.370.41). This observation aligns with prior research on the interpretability of different architectures (Chefer et al., 2021; Selvaraju et al., 2017). Several factors contribute to this gap: Spatial Locality in CNNs: Convolutional networks process images through hierarchical local operations, where each layers receptive field gradually expands. This structural inductive bias means that gradient-based attribution methods like Grad-CAM can precisely trace which spatial regions contributed to the final prediction, as gradients flow through spatially coherent feature maps. 17 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Table 6. Are sharp borders the main reason behind low-performing detectors? INP-X vs. Soft Alpha Blending. Both methods yield low detection accuracy, indicating that edge artifacts are not the primary signal for detection. MODEL METHOD ACC AUC PREC REC F1 CORVI2023 CLIP 10 CLIP 10+ SPAI SIGHTENGINE HIVE MODERATION INP-X ALPHA INP-X ALPHA INP-X ALPHA INP-X ALPHA INP-X ALPHA INP-X ALPHA 0.554 0.519 0.959 0.114 0.203 0.593 0.646 0.975 0.190 0.318 0.509 0.606 0.779 0.025 0.048 0.513 0.566 0.823 0.033 0.063 0.563 0.673 0.800 0.169 0.279 0.588 0.728 0.838 0.218 0. 0.543 0.567 0.546 0.506 0.525 0.576 0.618 0.576 0.573 0.575 0.548 0.588 0.944 0.102 0.184 0.575 0.628 0.941 0.160 0.274 0.555 0.578 0.923 0.120 0.212 0.591 0.614 0.970 0.220 0.359 Global Attention in ViTs: Vision Transformers divide images into patches and process them through self-attention layers that allow every patch to attend to every other patch from the first layer. While this enables capturing long-range dependencies, it also means that attribution is distributed across the entire image, making precise localization inherently more diffuse (Abnar & Zuidema, 2020). Method Suitability: Grad-CAM was specifically designed for CNN architectures and leverages the spatial structure of convolutional feature maps. For ViTs, methods like Attention Rollout provide interpretability but are known to be less precise for localization tasks. Specialized methods such as the relevance propagation approach of (Chefer et al., 2021) improve ViT interpretability but may still not match CNN localization precision for pixel-level tasks. For inpainting forensics, where accurate localization of the manipulated region is paramount, these findings suggest that CNN-based detector architectures may be preferable when localization is primary objective, despite ViTs advantages in other classification tasks. 18 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 11. Localization examples showing detector attention via GradCAM. (a) Mask indicates the inpainted region. (b) Standard inpainting triggers global attention across the image. (c) Our Inpainting Exchange method better localizes attention to the actual edited region. 19 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 12. Additional Comparison of standard inpainting versus Inpainting Exchange. From left to right: Original image, Mask, Standard Inpainting, Difference (Original - Inpainting), Inpainting Exchange (INP-X), and Difference (Original - INP-X). The difference maps reveal that standard inpainting introduces global artifacts across the entire image, while our method produces differences only within the masked region. Figure 13. Frequency domain analysis comparing real images, standard inpainting, and our Inpainting Exchange method. The FFT magnitude spectra reveal the global spectral artifacts introduced by standard inpainting, which are eliminated in our approach. 20 AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange Figure 14. Analysis of RePaint (VAE-less inpainting). Comparison of (1) Original Image, (2) Mask, (3) RePaint Output, and (4) Difference Map. While the unmasked region is perfectly preserved (zero difference), the method is prohibitively slow and requires dataset-specific training. Analysis of SDXL VAE artifacts. Analysis of FLUX.1 VAE artifacts. Figure 15. Even improved models like SDXL and FLUX.1 exhibit characteristic VAE reconstruction artifacts that correlate with the inpainting error. Columns from left to right: (1) Original Image, (2) Mask, (3) Inpainted Result, (4) High-Frequency Filters of the Original Image, (5) VAE Reconstruction Artifacts (decoding encoded original), (6) Difference Map (Inpainted - Original)."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Institut Polytechnique de Paris, U2IS",
        "Universite Paris-Saclay, CEA, LIST",
        "Universite Sorbonne, Pierre et Marie Curie, ISIR"
    ]
}