{
    "paper_title": "Φeat: Physically-Grounded Feature Representation",
    "authors": [
        "Giuseppe Vecchio",
        "Adrien Kaiser",
        "Rouffet Romain",
        "Rosalie Martin",
        "Elena Garces",
        "Tamy Boubekeur"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce $Φ$eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that $Φ$eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 0 7 2 1 1 . 1 1 5 2 : r Φeat: Physically-Grounded Feature Representation"
        },
        {
            "title": "Elena Garces",
            "content": "Tamy Boubekeur {gvecchio, akaiser, rouffet, rmartin, elenag, boubek}@adobe.com Adobe Research Figure 1. We present Φeat, novel physically-grounded foundation model sensitive to the physical properties that govern real-world appearance such as geometry, reflectance, transparency, and lighting. We visualize the cosine similarity maps obtained with Φeat output features between the patches marked with red cross and all other patches."
        },
        {
            "title": "Abstract",
            "content": "Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce Φeat, novel physically-grounded visual backbone that encourages representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that pure self-supervised training strategy, without explicit labels, already provides strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that Φeat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as foundation for physics-aware perception in vision and graphics. 1 1. Introduction Self-supervised visual backbones such as DINO [12, 15, 39, 48] and MAE [29] have emerged as powerful feature learners across wide range of computer vision tasks. Trained on large-scale unannotated datasets, these models are typically optimized for high-level semantic understanding, learning to represent objects, categories, or parts. While effective for classification and segmentation, the learned features remain largely agnostic to the physical properties that govern real-world appearance: how light interacts with surface, what material is made of, or how geometry and illumination shape visible structure. This semantic bias poses fundamental limitation in domains that require understanding material identity e.g. intrinsic decomposition [23, 24], material capture [16, 36, 52] and segmentation [27, 46], or robotics [54], where high degree of realism and physical fidelity are essential. Indeed, understanding how light interacts with matter through reflection, refraction, anisotropy, subsurface scattering, and other effects to produce the observed color is far from trivial. Existing foundation models, by design, deprioritize such low-level cues in favor of object-level consistency, making them poorly suited for tasks requiring physicallygrounded understanding. In contrast, specialized models are generally trained in supervised manner, requiring large annotated datasets [16, 18, 51] and limiting their application to other tasks. We address this gap with Φeat, new self-supervised visual backbone trained to encode physically-grounded features from raw RGB data. Rather than relying on photometric augmentations and semantic grouping, Φeat learns to associate images that share the same underlying material, decoupling appearance and semantic. Specifically, we replace the photometric augmentations, of the DINOv3 teacherstudent framework, with physical ones renderings of the same material on different geometries and under varying illumination. This setup encourages the model to develop representations that are invariant to the context e.g, macrogeometry (the overall shape) and lighting, while remaining sensitive to reflectance, transparency, and fine structure. Φeat is initialized from DINOv3-pretrained ViT backbone and fine-tuned using our material-aware pretraining strategy which preserves previously learned semantics of the world while specializing it for materials. We complement the DINO losses with cross material contrastive term, which further grounds invariance in real physical variation rather than semantic similarity. Φeat shifts the focus of the training from object semantics to intrinsic appearance, producing features that capture reflectance, geometry, and lighting interactions in unified, physically meaningful space. In nutshell, our main contributions are: Φeat, self-supervised visual backbone fine-tuned to encode physically-grounded material features such as reflectance and geometric mesostructure; material-aware pretraining pipeline that leverages synthetic renderings under varied lighting and geometry to encourage invariance to extrinsic appearance factors; large-scale, semantically coherent data generation pipeline, leveraging artists-designed mesh templates to render vast collection of materials. 2. Related Work Intrinsic Scene Understanding. The problem of recovering intrinsic scene characteristics from images [24] was formulated in the 70s [5], describing the world as combination of three intrinsic components: surface reflectance, shape (as depth or orientation), and incident illumination. key insight behind this formulation is that the human visual system can perceive these components separately, even under novel lighting conditions, viewpoints, or unfamiliar objects. Despite decades of progress [10, 56, 59], the problem remains only partially solved. Learning-based methods have achieved remarkable advances, but the task is still commonly framed as regression problem where single image is mapped to multiple layers encoding the physical attributes of the scene. These approaches typically rely on large-scale labeled datasets that target specific scenarios (indoor/outdoor [31], faces [44, 47], flat materials [36], etc.), which are costly to produce and difficult to scale. Early progress in intrinsic decomposition was driven by nonlearning-based approaches, which produced promising results but struggled to generalize. Two main strategies emerged. The first attempted to classify image gradients as either reflectance or shading, relying on heuristics such as entropy minimization [21] and Retinex-based assumptions [7, 30, 32], both of which proved brittle in practice. second line of work extended this idea by clustering regions of similar reflectance [6, 23], typically assuming smooth or continuous lighting within each cluster. fundamental limitation of these methods is that they operate directly on raw RGB values, which provide only narrow basis for reasoning about complex, spatially varying materials. Our method addresses this limitation by learning richer representation that encodes low-level physical attributes explicitly, enabling more robust reasoning beyond what is possible from RGB pixels alone. Material Capture and Segmentation. Material capture is branch of intrinsic decomposition focused on recovering materials without contextual information. To reduce ambiguity in the estimation, previous work typically assumes some known acquisition conditions, such as flash illumination [16, 17, 22, 34], outdoor lighting [36, 52, 53, 59], or data captured with dedicated scanning devices [25, 42]. While these methods infer physically meaningful parameters, they remain task-specific, rely on synthetic data and require dense ground truth reflectance maps. related problem is material segmentation. Materialistic [46] leveraged DINO features and cross-attention to segment image regions of the same material from single query point, later refined by Guerrero-Viu et al. [27] incorporating multi-resolution features to improved detail. While effective for the target task, these approaches rely on annotated synthetic datasets for supervision and pre-trained semantic backbones that remain frozen, limiting their adaptability to other problems. In contrast, our method learns generic physic-aware features that, as we show, can be used as is for downstream tasks such as material segmentation. Learning. SelfRepresentation Self-Supervised supervised learning (SSL) has evolved into the dominant strategy for visual representation learning, removing the dependency on manual annotation while scaling effectively with data and model capacity. Early methods used handcrafted pretext tasks e.g., context prediction [19, 37], inpainting [40], colorization [57], which promoted spatial reasoning but was limited in scalability. The field shifted with contrastive learning [13, 28], where models learn augmentation-invariant features by pulling together view of the same image and pushing apart the rest. Later methods like SwAV [11], BYOL [26], and SimSiam [14] removed the need for explicit negatives, using architectural asymmetry or stop-gradient mechanisms to avoid collapse. Masked Image Modeling (MIM) [4, 20, 29] with transformers emerged as complementary reconstruction-based approach, shifting the focus from matching views to predicting missing content. While effective for dense prediction, MIM often favors local texture over global structure. More recent works combine both contrastive and reconstruction cues within unified objective [2, 3, 33], improving scalability and generalization. DINO [12] and its successors [15, 39, 48] extend these ideas with teacherstudent self-distillation setup to learn both global and dense features. DINOv3, in particular, scales this approach to billions of parameters and adds refined losses (e.g., Gram anchoring, multi-student consistency), achieving state-of-the-art performance across semantic and dense tasks. These models, while highly successful in semantic tasks, remain limited in their ability to reason about low-level physical properties such as material appearance, geometry, or illumination. Φeat physically grounds these features by using data from the same material under varying lighting and geometry. This shift redefines the notion of similarity during pretraining, preserving the learned global context while aligning it to physically meaningful cues. Figure 2. Dataset samples. Each row shows different template, first with no materials applied (first column) and then different materials (following columns). Materials are rendered with transparency on objects matching their semantic categories, and randomized object and lighting rotation. 3. Data Collection and Curation The effectiveness of self-supervised pretraining depends strongly on how the input variability reflects the desired invariances. In Φeat, the goal is to disentangle intrinsic physical properties from extrinsic factors such as geometry and illumination. We design large synthetic dataset which focuses on variability in lighting and shape while keeping the underlying material identity constant. This controlled variability is what enables the model to learn physicallygrounded features rather than semantic or contextual similarity, albeit without the need to anchor it in specific physical model e.g., reflectance distribution function. Similar datasets have been used for inverse rendering [8, 36, 51] or material similarity tasks by randomly pairing objects, materials, and lighting. This arbitrary pairing introduces two issues: first, it might bias the training toward 3 Figure 3. Φeat training pipeline. Two renderings of the same material are sampled and augmented with multi-crop strategy that yields global and local views. The student processes all crops (globals and locals), with random masking on patch tokens for latent reconstruction; the teacher and the Gram teacher process global crops only. Both networks output classand patch-level embeddings. At the image level, Sinkhorn-balanced teacher assignments supervise student prototype predictions for all student views of the same material, defining Limage. At the patch level, masked student tokens are regressed to the teacher tokens at matching spatial indices, giving Lpatch. On the student global feature before the prototype head, KoLeo encourages dispersion (LKoLeo) and an in-batch InfoNCE pulls together the two views of the same material while pushing away other materials (Lcontrast). Gram anchoring aligns second-order structure on global crops. The teacher is an EMA of the student, and the Gram teacher is frozen snapshot used only for Gram anchoring. unrealistic combinations, affecting material apperance [45]; second, it makes large-scale generation impractical. These limitations motivated us to follow more principled design. We carefully build set of base geometric templates which we pair with semantically meaningful material categories. For example, cork material is rendered on rigid, lowcurvature surface rather than on highly wrinkled cloth. This semantic alignment between material and macrogeometry allows the network to focus on contexts where those materials are likely to appear in real world. Figure 2 shows different pairing examples of templates and materials. We rely on the Adobe Substance 3D Assets library [1], which provides over 9,500 procedural materials covering including faba wide range of 21 appearance classes, rics, metals, wood, plaster, marble, and plastic. We vary the procedural parameters of these materials following artist-designed presets, to synthesize approximately 36,000 unique sets of PBR (Physically Based Rendering) maps [9]. Each material instance is rendered on collection of semantically aligned geometries, and illuminated under four high dynamic range environment maps randomly selected from list of twenty, providing diverse lighting conditions. Additional material samples and the environment maps are provided in the supplementary materials. For each render, we randomly vary both the object and environment rotations to ensure rich sampling of view and light directions. Rendering is performed with Monte Carlo path tracer using physically accurate light transport and microfacet materials [50, 55], ensuring high realism and consistent energy conservation across all scenes. We employ GPU-native path tracing with 128 samples per pixel and apply denoising to all renders. We tessellate each object using the materials displacement map to synthesize realistic local shadows and inter-reflections. We render the environment map behind transparent regions (e.g., holes) to increase variability, helping the model separate thin structures from their surroundings. This process yields roughly one million high-quality renders. 4 4. Method The goal of Φeat is to learn visual representations that capture the intrinsic physical properties of appearance directly from unlabelled data. Traditional self-supervised frameworks achieve semantic invariance by contrasting different spatial crops or photometric augmentations of the same image. In contrast, Φeat replaces these transformations with physically meaningful variations: multiple renderings of the same material observed on diverse geometries and under diverse lighting conditions  (Fig. 3)  . 4.1. Model Architecture Φeat builds upon the vision transformer (ViT) architecture used in DINOv3 [48], chosen for its strong representational capacity and scalability. The network consists of stack of transformer encoder blocks operating on non-overlapping image patches. Each input image is divided into fixed-size patches of 16 16 pixels, which are flattened and linearly projected into patch tokens. special global token (often referred to as the [CLS] token) is prepended for imagelevel representation. The model incorporates small set of learnable register tokens that act as dedicated memory slots for global context and mitigate patch-token artefacts. Positional information in the token embeddings is handled via the Rotary Positional Embedding (RoPE) [49] mechanism, which supports token-sequence extrapolation and variable input resolutions. The tokens (global, register, patch) are fed into the transformer encoder stack, which outputs one vector for each token. From the final layer we extract both the patch-level features (one per patch) and the global representation vector from the [CLS] token. 4.2. Training Objectives The self-supervised training of Φeat follows the formulation of DINOv3 [48], adapted to physically varying input pairs. Each batch contains 2 global crops and 8 local crops extracted from renderings (x1, ..., xN ) of the same material under different lighting and geometry. For each crop, the network produces both global and patch-level embeddings. Following DINOv3 [48], we use combination of image-level objective DIN and patch-level latent reKoLeo regconstruction objective [58] ularizer and Gram anchoring. We complement these objectives introducing an in-batch contrastive term which pulls together representations of the same material while pushing away the embedding corresponding to different materials. This combination of objectives encourages alignment between physically equivalent renderings while maintaining diversity across unrelated materials. iBOT , as well as L Image-level objective. The global alignment loss follows the formulation of the DINOv3 objective [48], which 5 replaces the centering and sharpening mechanisms of DINOv2 with the SinkhornKnopp normalization from SwAV [11]. Such normalization of the teacher probabilities enforces balanced assignments over learnable prototypes, improving stability and preventing feature collapse. Let fs(v) and ft(v) be the student and teacher global emRKD beddings for view v, and let = [w1, . . . , wK] be the learnable prototype matrix. Student probabilities are fs(vs)(cid:1) exp(cid:0) 1 Ts ℓ fs(vs)(cid:1) , ℓ=1 exp(cid:0) 1 pk(vs) = (cid:80)K Ts while teacher assignments qk(vt) are obtained by applying SinkhornKnopp normalization [11] to the teacher logits 1 ft(vt) over the batch, enforcing balanced use of the Tt prototypes. Given set Vt of teacher views and Vs of student views, the cross-entropy objective is image = 1 (cid:88) (cid:88) (cid:88) Vs Vt vtVt vsVs k=1 qk(vt) log pk(vs). (1) Patch-level latent objective. Following iBOT [58] and DINOv3 [48], masked latent reconstruction objective is applied at the patch level. random subset of the students patch tokens is masked, and the network is trained to predict the corresponding patch embeddings produced by the teacher on the same spatial positions: iBOT = 1 (cid:88) m=1 (cid:13) (cid:13)hθ(ps m) pt (cid:13) 2 2 , (cid:13) (2) and pt where ps denote student and teacher patch embeddings, hθ is projection head, and is the number of masked patches. KoLeo loss. We use the KozachenkoLeonenko entropy estimator on the batch of L2-normalized student features to encourage dispersion and avoid collapse. Let i=1 be } the normalized global student embeddings in batch and let ρi = minj=i 2 be the nearest-neighbor distance for sample i. Ignoring constants independent of the parameters, maximizing entropy corresponds to minimizing zi { zj zi KoLeo = 1 (cid:88) i= log (cid:0)ρi + ε(cid:1), (3) with small ε > 0 for numerical stability. This promotes more uniform features distribution on the unit hypersphere. Gram anchoring. secondary frozen teacher (Gram teacher) provides structural target at patch level. We deRN the student and note by Ps RN and PG Gram-teacher patch matrices after mean-centering across patches. We align second-order patch relations via Gram = L"
        },
        {
            "title": "1\nN 2",
            "content": "(cid:13) (cid:13) PsP PGP (cid:13) 2 . (cid:13) (4) Contrastive loss. We complement the teacherstudent alignment with an in-batch InfoNCE [38] loss on L2normalized global student embeddings. For each anchor zi, positives (i) are the other views of the same material, and all remaining samples in the batch are negatives: i=1 (cid:80) log"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) contrast = jP (i) exp(cid:0) sim(zi, zj)/τ (cid:1) k=i exp(cid:0) sim(zi, zk)/τ (cid:1) , (cid:80) (5) where sim is cosine similarity and τ is temperature. This objective explicitly pulls together representations of the same physical material, while pushing apart embeddings corresponding to different materials. It complements the DINO objective by reinforcing instance-level consistency independently of the teacher distribution, anchoring the models global features to material identity. Total objective. The total loss consists of weighted sum of these components: total = image+λp iBOT +λk KoLeo+λg Gram+λc contrast, (6) with the coefficients reported in Section 5.1. The combination of the physical augmentation strategy and the contrastive loss in Φeat, shifts the bias learned during pretraining, from semantic consistency across crops to physical consistency across renderings. 5. Implementation & Results We evaluate Φeat examining whether the features exhibit invariance to lighting and geometry while remaining sensitive to intrinsic material properties. We compare our method to CLIP [41], DINOv2 with registers [15] and DINOv3 [48], which represent the strongest publicly available self-supervised baselines. CLIP is included only in classification-based evaluations where global image embeddings are meaningful; patch-level selection is evaluated solely against DINOv2 and DINOv3. All models use their ViT-B backbones, and we adopt input resolutions corresponding to 1024 patch tokens (448 448 for patch size 14, 512 512 for patch size 16) for fair comparison. 5.1. Training Procedure We train Φeat for 10,000 iterations using batches of 512 render pairs, each consisting of two physically distinct views of the same material. Each pair is augmented using multicrop strategy, producing 2 global and 8 local crops per view. 6 ℓ"
        },
        {
            "title": "IoU",
            "content": "F1 DINOv2 0.276 0.566 0.698 DINOv3 0.271 0.599 0.724 0.265 0.776 0.860 Φeat Table 1. Material selection results. Quantitative comparison of patch-level material selection performance on the DuMaS dataset [27]. Φeat outperforms both DINOv2 and DINOv3 across all metrics, demonstrating improved discrimination of materials. The global and local views are extracted by randomly cropping 40-100% and 10-40% of the renders respectively, ensuring diverse distribution of spatial context and texture. We use ViT-B/16 backbone initialized from DINOv3 weights and optimized using AdamW [35] with base learning rate of 0.001 and weight decay of 0.05. The teacher momentum follows cosine schedule between 0.996 and 1.0, and the student temperature τs is fixed to 0.1. We train on input resolutions of 2242 for global crops and 1122 for local crops, following the multi-crop strategy described in Section 4. Training is performed on 16 NVIDIA A100 GPUs using mixed-precision distributed data parallelism, for total approximate time of 100 hours. We set the loss weights to λp = 1.0, λk = 0.1, λg = 0.7, and λc = 0.25. For the KoLeo term we use ε = 106 in Eq. (3); for the patch objective we randomly mask between 10% and 50% of the student patches with probability of 50%; the InfoNCE temperature is fixed to τ = 0.1. We enable the Gram anchoring for the last 2,000 iteration steps. 5.2. Quantitative Results We evaluate the representations learned by Φeat across two downstream tasks: material selection and feature separability via k-NN. These tasks assess how effectively the learned features capture intrinsic appearance properties, regardless of geometry or lighting. Additional examples, including cross-frames similarity propagation for selection in videos, are provided in the supplementary materials. Material selection. We adopt the evaluation framework and DuMaS dataset of Guerrero-Viu et al. [27] to measure how well features separate materials based on appearance similarity. Given query patch, we compute cosine similarity between its feature embedding and those of all image patches, producing dense similarity map. The resulting map is binarized with threshold of 0.5 into mask, which is compared against the ground-truth material segmentation. As shown in Table 1, Φeat significantly outperforms both DINOv2 and DINOv3 across all metrics. It achieves lower ℓ1 error and higher IoU and F1, indicating more precise and coherent material selection. Unlike semantic models that primarily group regions by object or context, Φeat groups"
        },
        {
            "title": "Input",
            "content": "Patch-wise similarity DINOv3 DINOv2 Φeat Unsupervised segmentation DINOv3 DINOv2 Φeat Figure 4. Patch-wise similarity and unsupervised segmentation. Left group shows cosine similarity maps between the embedding of reference patch (red cross) and all others, visualizing the spatial coherence of learned representations. We show examples gradually growing from mostly flat surface to medium scale scene. Right group displays K-means segmentations obtained from the patch embeddings. Compared to DINOv2 and DINOv3, Φeat produces similarity responses and clusters that are more spatially consistent and physically meaningful, grouping regions by reflectance and texture rather than by semantic or geometric cues. patches by shared physical appearance, demonstrating that its representations capture material identity with higher fidelity and robustness to illumination and geometry changes. Feature separability via k-NN evaluation. To assess whether the learned representations naturally organize materials into semantically meaningful clusters, we adopt non-parametric k-nearest neighbors (k-NN) evaluation. We use controlled synthetic test set containing 972 materials within 16 categories. Each material is rendered on 6 geometry templates and under 4 lighting conditions, producing 24 distinct variants per material, resulting in total of 23, 328 renders. Each image is embedded using the frozen encoder, and cosine similarity is computed between all feature vectors. For each query sample, we retrieve its = 16 nearest neighbors and assign label through weighted majority voting, where weights are proportional to the cosine similarity. We compare Φeat against CLIP, DINOv2, and DINOv3. The overall classification accuracy is then computed with respect to the ground-truth material labels. The results  (Table 2)  show that Φeat forms tighter, geometryand lighting-invariant clusters aligned with material identity, better grouping similar materials together. Robustness. To assess invariance to extrinsic factors, we measure robustness across variations in illumination and geometry. For each material class we compute predictions using the same k-NN classifier described above. We then evaluate the average pairwise Hamming distance between predictions obtained under different lighting conditions (fixed geometry) and under different geometry templates (fixed lighting). Lower values indicate higher invariance. Table 3 shows that Φeat outperforms the other methods exibiting less variability under both types of perturbation, confirming that the representation remains stable across extrinsic appearance changes. 5.3. Qualitative Results We qualitatively evaluate Φeat by comparing patch-wise similarity and unsupervised segmentation against DINOv2 [39] and DINOv3 [48] (Figure 4). Each heatmap shows cosine similarity between reference patch (marked with cross) and all other patches in the image. DINOv2 and DINOv3 highlight semantically or spatially related regions (visible falloff around the selected patch), typically other parts of the same object, even when materials differ. Φeat instead produces spatially coherent responses aligned with material boundaries, grouping patches with similar re7 CLIP DINOv2 DINOv3 Φeat Top-1 0.552 0.563 0.600 0."
        },
        {
            "title": "Precision",
            "content": "0.444 0.554 0.483 0."
        },
        {
            "title": "Recall",
            "content": "F1 0.415 0.419 0.425 0.431 0.457 0.451 0.472 0.469 Table 2. k-NN classification results. Non-parametric evaluation of representation separability across 16 material classes using k=16 nearest neighbors and cosine similarity. Φeat achieves the highest Top-1 accuracy and balanced precisionrecall tradeoff, indicating that its representations form coherent, lightingand geometry-invariant clusters that align with material identity. CLIP DINOv2 DINOv3 Φeat"
        },
        {
            "title": "Illumination",
            "content": "0.403 0.284 0.240 0."
        },
        {
            "title": "Geometry",
            "content": "0.534 0.435 0.365 0.305 Table 3. Robustness to illumination and geometry changes, measured as the average pairwise Hamming distance between predictions obtained from different variants of the same underlying asset. Lower values indicate higher invariance. flectance and texture rather than object identity. For unsupervised segmentation, we apply K-means clustering on patch embeddings to visualize how features organize the image space. The number of clusters is selected 2, .., 12 and choosing the automatically by evaluating value that maximizes the silhouette score [43]. Segmentations from DINOv2 and DINOv3 often reflect semantic cues, with clusters following object parts while mixing different materials. Φeat instead produces clusters that are both spatially consistent and physically meaningful, effectively separating regions according to material properties. These qualitative results demonstrate that Φeat learns representations driven by reflectance and texture rather than semantics, capturing physically-grounded appearance features that remain invariant to geometry and lighting. Additional results showing invariance to lighting are provided in the supplemental material. 5.4. Ablation Study We conduct an ablation study to evaluate the effect of each novel training component. Table 4 shows that fine-tuning with single-render material supervision already improves the material-selection metrics (ℓ1, IoU, F1), but it also substantially reduces kNN accuracy, showing that the model becomes more physically grounded while losing global discriminability. Adding the multi-render scheme improves both material-selection and classification performance, suggesting that exposure to 8 Material selection F1 ℓ1 IoU 0.724 0.599 0.271 0.831 0.748 0.277 0.843 0.753 0.272 0.860 0.776 0.265 k-NN Top-1 0.563 0.345 0.513 0.643 DINOv3 + single render + multi render + contrastive Table 4. Ablation. Fine-tuning with single-render material supervision improves material selection metrics but reduces k-NN accuracy. Multi-render supervision improves both segmentation and classification, recovering much of the lost discriminability. Adding the contrastive term produces features that are both physically grounded and well separated, achieving the best performance across all metrics. varied viewpoints and illumination stabilizes the representation. However, only the contrastive term fully resolves the loss of separability introduced by material supervision, enforcing compact intra-material clusters and clearer intermaterial boundaries. This combination yields the strongest performance across all metrics and consistently surpasses DINOv3, leading to representation that is both physically grounded and discriminative. 6. Limitations While Φeat successfully learns invariance to geometry and illumination, it currently does not explicitly disentangle the latent space into interpretable physical factors. more expressive formulation would disentangle material, lighting, and geometry within the learned space, allowing users to query or manipulate features along specific physical dimensions. Moreover, our pretraining relies entirely on synthetic data; closing the domain gap to unpaired real photographs remains an open challenge. 7. Conclusion We proposed Φeat, self-supervised visual backbone that learns physically-grounded representations of appearance directly from unlabelled data. By replacing photometric augmentations with physically meaningful variations i.e., different renderings of the same material under diverse geometries and lighting conditions, our approach bridges the gap between semantic invariance and physical understanding. Through combination of DINO-based teacherstudent alignment and contrastive regularization, Φeat captures material identity while remaining invariant to extrinsic factors such as shape and illumination. We demonstrated that the learned features transfer effectively to downstream material-aware tasks, including fine-grained material selection and clustering under changing illumination and geometry. These results confirm that physically structured pretraining can yield representations that are both robust and interpretable, without relying on semantic labels."
        },
        {
            "title": "References",
            "content": "[1] Adobe. Substance 3D Assets. https://substance3d. adobe.com/assets, 2025. 4 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 3 [3] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. In International Conference on Machine Learning, pages 1416 1429. PMLR, 2023. 3 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. [5] Harry Barrow, Tenenbaum, Hanson, and Riseman. Recovering intrinsic scene characteristics. Comput. vis. syst, 2 (3-26):2, 1978. 2 [6] Sean Bell, Kavita Bala, and Noah Snavely. Intrinsic images in the wild. ACM Transactions on Graphics (TOG), 33(4): 112, 2014. 2 [7] Sai Bi, Xiaoguang Han, and Yizhou Yu. An 1 image transform for edge-preserving smoothing and scene-level intrinsic decomposition. ACM Transactions On Graphics (TOG), 34 (4):112, 2015. 2 [8] Michael Birsak, John Femiani, Biao Zhang, and Peter Wonka. Matclip: Light-and shape-insensitive assignment of In Proceedings of the Special Interpbr material models. est Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 3 [9] Brent Burley. Extending the disney brdf to bsdf with integrated subsurface scattering. SIGGRAPH 2015 Course: Physically Based Shading in Theory and Practice, 2015. 4 [10] Chris Careaga and Yagız Aksoy. Intrinsic image decomposition via ordinal shading. ACM Transactions on Graphics, 43 (1):124, 2023. [11] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912 9924, 2020. 3, 5 [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 2, 3 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. 3 ference on computer vision and pattern recognition, pages 1575015758, 2021. 3 [15] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 2, 3, 6 [16] Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, and Adrien Bousseau. Single-image svbrdf capture with rendering-aware deep network. ACM Transactions on Graphics (ToG), 37(4):115, 2018. [17] Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, and Adrien Bousseau. Flexible SVBRDF capture In Computer Graphics with multi-image deep network. Forum, pages 113. Wiley Online Library, 2019. 2 [18] Valentin Deschaintre, Julia Guerrero-Viu, Diego Gutierrez, Tamy Boubekeur, and Belen Masia. The visual language of fabrics. ACM Transactions on Graphics (TOG), 42(4):115, 2023. 2 [19] Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pages 14221430, 2015. 3 [20] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herve Jegou, and Edouard Grave. Are large-scale arXiv datasets necessary for self-supervised pre-training? preprint arXiv:2112.10740, 2021. 3 [21] Graham Finlayson, Mark Drew, and Cheng Lu. Intrinsic images by entropy minimization. In European conference on computer vision, pages 582595. Springer, 2004. 2 [22] Duan Gao, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, and Xin Tong. Deep inverse rendering for high-resolution svbrdf estimation from an arbitrary number of images. ACM Trans. Graph., 38(4):1341, 2019. [23] Elena Garces, Adolfo Munoz, Jorge Lopez-Moreno, and In ComDiego Gutierrez. puter graphics forum, pages 14151424. Wiley Online Library, 2012. 2 Intrinsic images by clustering. [24] Elena Garces, Carlos Rodriguez-Pardo, Dan Casas, and Jorge Lopez-Moreno. Survey on Intrinsic Images: Delving Deep into Lambert and Beyond. International Journal of Computer Vision, 130(3):836868, 2022. 2 [25] Elena Garces, Victor Arellano, Carlos Rodriguez-Pardo, David Pascual-Hernandez, Sergio Suja, and Jorge LopezMoreno. Towards material digitization with dual-scale optical system. ACM Transactions on Graphics (TOG), 42(4): 113, 2023. 3 [26] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 3 [27] Julia Guerrero-Viu, Michael Fischer, Iliyan Georgiev, Elena Garces, Diego Gutierrez, Belen Masia, and Valentin Deschaintre. Fine-grained spatially varying material selection in images. arXiv preprint arXiv:2506.09023, 2025. 2, 3, [14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF con- [28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep9 resentation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 3 [29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2, 3 [30] Berthold KP Horn. Determining lightness from an image. Computer graphics and image processing, 3(4):277299, 1974. [31] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor single-view material esIn Proceedings of the IEEE/CVF Conference timation. on Computer Vision and Pattern Recognition, pages 5198 5208, 2024. 2 [32] Edwin Land and John McCann. Lightness and retinex theory. Journal of the Optical society of America, 61(1):1 11, 1971. 2 [33] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 3 [34] Zhengqin Li, Kalyan Sunkavalli, and Manmohan Chandraker. Materials for masses: Svbrdf acquisition with In Proceedings of the Eurosingle mobile phone image. pean Conference on Computer Vision (ECCV), pages 7287, 2018. 2 [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [36] Rosalie Martin, Arthur Roullier, Romain Rouffet, Adrien Kaiser, and Tamy Boubekeur. Materia: Single image highresolution material capture in the wild. In Computer Graphics Forum, pages 163177. Wiley Online Library, 2022. 2, 3 [37] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Eurovisual representations by solving jigsaw puzzles. pean conference on computer vision, pages 6984. Springer, 2016. 3 [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 6 [39] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 7 [40] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature In Proceedings of the IEEE conlearning by inpainting. ference on computer vision and pattern recognition, pages 25362544, 2016. 3 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [42] Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, and Jorge Lopez-Moreno. Textile: differentiable metric for In Proceedings of the IEEE/CVF Contexture tileability. ference on Computer Vision and Pattern Recognition, pages 44394449, 2024. [43] Peter Rousseeuw. Silhouettes: graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:5365, 1987. 8 [44] Soumyadip Sengupta, Angjoo Kanazawa, Carlos Castillo, and David Jacobs. Sfsnet: Learning shape, reflectance and illuminance of facesin the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 62966305, 2018. 2 [45] Ana Serrano, Bin Chen, Chao Wang, Michal Piovarci, HansPeter Seidel, Piotr Didyk, and Karol Myszkowski. The effect of shape and illumination on material perception: model and applications. ACM Trans. on Graph., 40(4), 2021. 4 [46] Prafull Sharma, Julien Philip, Michael Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschaintre. Materialistic: Selecting similar materials in images. ACM Transactions on Graphics, 42(4), 2023. 2, 3 [47] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, and Dimitris Samaras. Neural face editing In Proceedings of the with intrinsic image disentangling. IEEE conference on computer vision and pattern recognition, pages 55415550, 2017. 2 [48] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 2, 3, 5, 6, 7 [49] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [50] S. Trowbridge and K. P. Reitz. Average irregularity representation of rough ray reflection. Journal of the Optical Society of America, 65(5):531536, 1975. [51] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: In Proceedings of the modern pbr materials dataset. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2210922118, 2024. 2, 3 [52] Giuseppe Vecchio, Simone Palazzo, and Concetto Spampinato. Surfacenet: Adversarial svbrdf estimation from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 1284012848, 2021. 2 [53] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. Controlmat: controlled generative approach to material capture. ACM Transactions on Graphics, 43(5): 117, 2024. 2 [54] Giuseppe Vecchio, Simone Palazzo, Dario Guastella, Daniela Giordano, Giovanni Muscato, and Concetto SpampTerrain traversability prediction through selfinato. supervised learning and unsupervised domain adaptation on synthetic data. Autonomous Robots, 48(2):4, 2024. 2 10 [55] Bruce Walter, Stephen Marschner, Hongsong Li, and Kenneth Torrance. Microfacet models for refraction through In Proceedings of the 18th Eurographics rough surfaces. conference on Rendering Techniques, pages 195206, 2007. 4 [56] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGBX: Image decomposition and synthesis using materialand lighting-aware diffusion models. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, page 111. ACM, 2024. [57] Richard Zhang, Phillip Isola, and Alexei Efros. Colorful In European conference on computer image colorization. vision, pages 649666. Springer, 2016. 3 [58] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. 5 [59] Xilong Zhou and Nima Khademi Kalantari. Adversarial single-image svbrdf estimation with hybrid training. In Computer Graphics Forum, pages 315325. Wiley Online Library, 2021."
        }
    ],
    "affiliations": [
        "Adobe Research"
    ]
}