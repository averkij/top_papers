{
    "paper_title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
    "authors": [
        "Yifei Li",
        "Junbo Niu",
        "Ziyang Miao",
        "Chunjiang Ge",
        "Yuanhang Zhou",
        "Qihao He",
        "Xiaoyi Dong",
        "Haodong Duan",
        "Shuangrui Ding",
        "Rui Qian",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Conghui He",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench."
        },
        {
            "title": "Start",
            "content": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? Yifei Li1,2, Junbo Niu1,3, Ziyang Miao3, Chunjiang Ge2, Yuanhang Zhou2, Qihao He4, Xiaoyi Dong1,5, Haodong Duan1, Shuangrui Ding1,5, Rui Qian1,5, Pan Zhang1, Yuhang Zang1, Yuhang Cao1, Conghui He6, Jiaqi Wang1(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2Tsinghua University, 3 Beihang University, 4 Communication University of China, 5 The Chinese University of Hong Kong, 6 SenseTime Group 5 2 0 2 9 ] . [ 1 0 1 5 5 0 . 1 0 5 2 : r Figure 1. demonstrative comparison between offline and online video understanding [5]. Offline video understanding focuses on answering questions based on the entirety of video. In contrast, online video understanding involves posing queries about the context of video at intermediate points, demanding the ability to trace back past information, perceive ongoing events, and adapt to continuous input."
        },
        {
            "title": "Abstract",
            "content": "Temporal Awarenessthe ability to reason dynamically based on the timestamp when question is raisedis the key distinction between offline and online video LLMs. Un- * indicates equal contribution. indicates interns at IXCLab, Shanghai AI Laboratory like offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), novel 1 video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately humancurated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench. 1. Introduction Large Vision Language Models (LVLMs) [27, 35, 47, 60] and Video-LLMs [23, 34, 57] have shown remarkable progress, achieving impressive scores on existing benchmarks [11, 12, 24]. Recent works, such as VideoLLM-online [5] and Flash-VStream [58], have pioneered J.A.R.V.I.S1-like real-world video assistants by integrating pre-trained vision encoders [40] with LLMs [9, 46]. However, critical question remains: How far are current state-of-the-art models from achieving human-level online video understanding? Despite the existence of dozens of evaluation benchmarks in video understanding, there remains significant domain gap between these evaluations and real-world video understanding tasks. Early evaluations [19, 53, 55] are largely based on video understanding and retrieval datasets [2, 54], assessing models through coarse-grained QA tasks, such as Q: Who is dancing? A: Man. These QAs predominantly focus on short videos with fixed question types and lack temporal indispensability [11]. Subsequent works [12, 24, 61] attempt to address these limitations by extending video temporal length and incorporating more diverse tasks and video sources. E.T.Bench [30] advances this further by exploring inherent temporal information in 1J.A.R.V.I.S. is fictional AI assistant from Marvels Iron Man and Avengers series. videos and evaluating fine-grained temporal event detection capabilities. However, all the aforementioned works are limited to offline settings, where models have access to all video frames when answering queries. While these models exhibit impressive performance on offline video understanding benchmarks, substantial gap remains between their demonstrated capabilities and the requirements of real-world assistant or autonomous agent. pioneering benchmark, VStream-QA [58], represents one of the earliest efforts to evaluate streaming understanding, leveraging video sources from Ego4d [15] and MovieNet [17]. Meanwhile, StreamingBench [26], most recent work, expands the scope by evaluating Video-LLMs on larger scale in streaming scenarios. However, three primary evaluation categories of StreamingBench primarily target the leverage of existing visual inputs to respond to incoming queries immediately, resulting in an incomplete portrayal of streaming perception. In this work, we propose that effective online video understanding requires simultaneous capabilities to trace back past information, perceive the going-on, and forward active responding simultaneously. Given query during streaming video, Video-LLM must determine whether to respond immediately using past and ongoing information or wait until sufficient evidence has been accumulated. We refer to this as the Video Chain-of-Time thinking process (Figure 3), inspired by the Chain-ofThought reasoning in LLMs [49]. We introduce OVO-Bench (Online-VideO-Benchmark) to evaluate Video-LLMs online video understanding capabilities. The benchmark comprises 644 videos from diverse sources, including curated datasets and web videos, spanning 7 major domains (Sports, Video Games, Ego Centric, etc.) with durations ranging from minutes to half an hour. Using hybrid approach combining semiautomated MLLM generation and human curation, we created 2814 high-quality samples (Meta-Annotations) with precise event timestamps. These Meta-Annotations are organized into 12 tasks across three categories: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding, reflecting the human video understanding process illustrated in Fig. 3. Notably, the proposed Forward Active Responding marks the first evaluation that requires models to continuously adapt their responses to ongoing visual input for online video understanding. Building on the human-reviewed meta-annotations, we develop an evaluation pipeline that queries Video-LLMs densely along temporal axes to simulate continuous information processing. For Backward Tracing and RealTime Visual Perception, we adopt multiple-choice evaluation, converting videos into segments from start to query time to accommodate offline models. With this approach, we explore the potential of explicitly leveraging state-of2 the-art offline Video-LLMs for online video understanding. We evaluated 9 Video-LLMs, including proprietary models GPT-4o [35] and Gemini-1.5-Pro [45], alongside six recent open-source MLLMs like Qwen2-VL [47] and LLaVA-OneVision [22]. Despite their strong offline performance, these models struggle with online-style queries (e.g., What is happening now?), showing significant gap from human performance. Further experiments on recent streaming models, such as Flash-VStream [58], reveal an even wider performance gap compared to offline counterparts, highlighting substantial research space for further exploration and improvement. 2. Related Works Video Large Language Models. Video Large Language Models (VLLMs) can process video by treating it as sequence of video frames. Projects like VideoChat [23], Video-LLaMA [57], and Video-ChatGPT [34] project the CLIP-ViT [41] embeddings of selected video frames through Multi-Layer Perceptron (MLP) projector into the LLM embedding space, then concatenate these embeddings with text embeddings for enhanced video understanding. However, the context length of MLLMs limits their effectiveness in understanding long videos [23, 34], as longer videos require more frames and longer context length. To address this limitation, two major approaches have been developed: compressing video features and selecting critical frames. In the realm of feature compression, Chat-UniVi [21] merges similar visual tokens through clustering techniques. MovieChat [43] and MA-LLM [16] employ memory bank to store fixed number of video tokens by iteratively merging the most similar tokens. ST-LLM [29] and MovieChat [43] reduce video tokens to 32 using pretrained Q-Former from BLIP2 [10]. LLaMA-VID [25] takes more radical approach, compressing each frame into content token and context token. On the other hand, frame selection methods aim to identify the most representative frames. VideoStreaming [39] utilizes small LLM to select critical video clips, while FlashVstream [58] employs clustering method to choose representative frames for high-resolution processing. LongVU [42] leverages question embeddings to select question-related frames, thereby enhancing video understanding. Benchmarks for Video Understanding. Traditional video benchmarks, e.g., MSVD-QA [53], MSRVTT-QA [53], and ActivityNet-QA [55], predominantly consist of short videos, typically ranging from 1 to 2 minutes in duration. These datasets are meticulously annotated with corresponding questions and ground truth answers. GPT-4 [35] is employed to assess the accuracy of the answers by comparing them against the provided questions and ground truth responses. However, these benchmarks primarily focus on evaluating short, static video scenes. Hence, new benchmarks designed to test causal and temporal understanding, e.g., NExT-QA [52], TemporalBench [3], and AutoEvalVideo [7] are proposed. To gauge the capabilities of models on long-duration videos, benchmarks like EgoSchema [33] covering over 5,000 egocentric videos with an average length of 3 minutes have been introduced. In contrast, Video-MME [12], LVBench [48], and LongVideoBench [51] feature videos spanning from 20 minutes to over an hour, evaluating broad spectrum of video understanding capabilities. HourVideo [4] stands out with egocentric videos extending up to 2 hours, accompanied by more than 12,976 multiplechoice questions. Unlike these offline video benchmarks, our proposed OVO-Bench is designed to evaluate online, interactive video understanding. Online Video Understanding. Traditional offline video understanding methodologies primarily focus on accessing entire video sequences to facilitate prediction tasks. Conversely, online video understanding demands models to process video streams sequentially, making decisions based on current and past information. This approach is particularly well-suited for scenarios where future data is unavailable, such as in embodied intelligence, autonomous driving, and augmented reality applications. Among online video understanding methods, FlashVStream [58] employs clustering method to select representative frames, enabling MLLMs for real-time interactions. LIVE [5] introduces comprehensive framework for learning in video streams, which includes training objective, data generation schema, and an inference pipeline tailored for online video understanding. 3. OVO-Bench In this section, we present the construction process of our OVO-Bench. We start with detailed introduction to the three different modes of online video understanding, followed by comprehensive description of the data collection and annotation procedures. statistical report of our proposed benchmark is displayed at the end of this section. 3.1. Online Video Understanding Mode Taxonomy Online video understanding aims to equip real-world, always-on agents with the ability to receive and process video inputs continuously, which closely mimics the human visual perception process. We categorize online video understanding into three distinct problem-solving modes: (1) Backward Tracing, (2) Real-Time Visual Perception, and (3) Forward Active Responding. Given userprovided text query Qt0 at the current time t0 and streaming video input X(,+), these modes are formally defined as follows: 3 1. Backward Tracing: Rt0 = (Qt0, X(,T ]) 2. Real-Time Visual Perception: Rt0 = (Qt0, X(T,t0]) 3. Forward Active Responding: R(t0,+] = (Qt0 , X(t0,+)) in which represents threshold that defines the boundary for recent times, and denotes the models response. The first two modes, Backward Tracing and Real-Time Visual Perception, involve collecting visual information from past and current timeframes respectively, and are expected to give immediate responses. In contrast, Forward Active Responding requires the model to withhold response until sufficient future information becomes available to ensure confident answer. Based on these distinctions, we have meticulously designed tasks tailored to each mode to effectively evaluate the performance of Video-LLMs across these diverse capabilities. 3.1.1. Backward Tracing Memory, particularly long-term memory, is crucial aspect In video understanding systems, of human intelligence. this capability involves recalling and reasoning about past events. We focus on the following three tasks to evaluate this capability: 1. [EPM] Episodic Memory: Backtrack and retrieve key moments from past video inputs. 2. [ASI] Action Sequence Identification: Identify the correct sequence of human actions in the video streams. 3. [HLD] Hallucination Detection: Ask questions irrelevant to existing video inputs. 3.1.2. Real-Time Visual Perception Accurate real-time perception of visual content is crucial, as actions undertaken in the present shape future outcomes. In various real-world scenarios, an immediate and precise understanding of ongoing visual inputs is essential. We propose six critical categories that constitute the foundational capabilities for effective real-time visual perception: 1. [STU] Spatial Understanding. Reason over the spatial relationships between objects occurring in nearby frames. 2. [OJR] Object Recognition. Recognize the objects appearing in the current frames. 3. [ATR] Attribute Recognition. Identify the characteristics or properties of objects, such as color, texture, and size that appear in nearby frames. 4. [ACR] Action Recognition. Recognize and interpret the actions being performed by individuals in the current frame. Figure 2. Examples of each task in OVO-Bench. The 14 tasks are categorized into three different kinds of perceiving modes in online video understanding: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding. 5. [OCR] Optical Character Recognition. Recognize and interpret characters that appear within the frame. 6. [FPD] Future Prediction. Forecast the most probable subsequent phase of the current scene, including changes in object states, actions, and other dynamic elements. 3.1.3. Forward Active Responding Transitioning from passive reception to active perception is essential for advanced video understanding systems. Existing benchmarks primarily focus on the aforementioned two understanding modes, where Video-LLMs are required to respond immediately based on available information. In contrast, we introduce the Forward Active Responding mode, which allows the model to adjust its responses based on forthcoming visual inputs. We devise four task dimensions to evaluate the models active responding abilities: 1. [REC] Repetition Event Count. Respond when 4 including both highrepetitive event occurs again, frequency repetitive actions over short durations and semantically long-term repetitive occurrences of certain events. 2. [SSR] Sequential Steps Recognition. Respond when certain procedure or sequence of actions has transitioned to another stage. 3. [CRR] Clues Reveal Responding. Delay responding until sufficient information or clues are provided. 3.2. Benchmark Construction Under the taxonomy guidelines above, we make our first step by collecting video data and annotations from existing datasets and crawling data from the web to increase diversity. As our proposed evaluation pipeline highly relies on the accurate timestamp annotations of the referred events in the constructed prompt, the scarcity of event-level timestamps in existing datasets [50][32][38] promotes the design of our highly efficient meta-data generation pipeline 3. Raw annotations with coarse timestamps are then refined by humans to ensure accuracy. Our final questions and options for evaluation are constructed using our rule-based pipeline based on these human-refined meta-annotations. All QA samples undergo manual inspection before being included in the final test set. 3.2.1. Video and Annotation Collection Video Source Selection. We follow existing benchmarks [30][24] by exploiting high-quality customized video datasets, and enrich our diversity by utilizing self-crawling (1) Human-annotated videos from different domains. Video Dataset. Our main consideration for utilizing organized datasets is to alleviate the labor-intensive source video collection process. Specifically, we include QA-Ego4D[1] and OpenEQA[31] for the [EPM] task, STAR[50], YouCook2[62], CrossTask[64], HiREST[56], and COIN[44] for the [ASI] task, Perception-Test[38] and Thumos[20][13] for the [REC] task, COIN[44] for the [SSR] task, MovieNet[17] for the [CRR] task, and Ego4D[15] for tasks under Real-Time Visual Perception. All samples are selected from val or test sets to avoid (2) Web-crawling Videos. To potential data leakage. further extend the diversity of our benchmark, we follow the existing practice [11][26] of crawling source videos from YouTube. Meta-Annotations Collection. We employ three approaches to collect our meta-annotations which contain (1) Existing Annotation Reevent-level purposing. For human-annotated datasets with accurate event-level timestamps [1][44][15], we explicitly take advantage of these labels and reconstruct them to our final (3) Semi-Automatic Generation. For datasets prompt. that provide video-level QA pairs without complete including [32][50][38][20][13], temporal localization, timestamps: we prompt temporal-sensitive Video-LLMs like Gemini1.5[45] to provide coarse-grain timestamps which fit the event referred in question and answer. For tasks under the Real-Time Visual Perception scenario, timestamps are given during our automatic QA construction process, which will be illustrated in 3.2.2. We then perform meticulously inspect all collected source videos and the corresponding meta-annotations to ensure precision. 3.2.2. Prompt Generation Question and Answer Generation. Besides carefully selecting QA pairs from existing datasets to fit into our proposed tasks, we also adopt highly efficient automatic question and answer generation pipeline, particularly for the Real-Time Visual Perception scenario. We randomly sample short clips from original long-form videos and then leverage GPT-4o[18] to select potential candidates and construct questions and corresponding answers using human-refined prompts. Human-proposed questions are also adopted as part of these tasks to alleviate possible LLM preferences. For the novel [CRR] task, even the strongest VideoLLMs/MLLMs like Gemini-1.5-Pro struggle to construct desired problems. Volunteers are then recruited to provide QA pairs under our guidance. Options Generation and Selection. We adopt multiplechoice questions as testing forms for Backward Tracing and Real-Time Visual Perception scenarios. However, as revealed in [6], the naively designed options of multi-choice form query can cause information leakage about answers. We propose to generate options using carefully designed rule-based and visually grounded transformation of correct answers, bringing misleading information from original videos to increase difficulty. Specifically, we prompt VideoLLMs with original QA pairs and corresponding video clips to generate visual-related options. careful human review is then conducted to further ensure the options effectiveness. All options are shuffled after human review to avoid potential preference bias. Prompting Offline-Models for Simulated Online Understanding. With the significant performance gap between main-streaming powerful offline Video-LLMs [45][36][47] and existing online models [5][58], one natural question is made: Is it effective to prompt offline models directly for online video understanding? For the Real-Time Visual Perception setting, we make human curation to the original question to include implies about the real-time query scenarios, for example, by using sentence patterns like What is/What am or containing words like Now/Currently. We made another intuitive attempt to prompt offline models to solve tasks under our novel Forward Active Responding scenario, which asks for continuous adapting capability. Specifically, we devise multiple-triggering densely query and evaluation pipeline, allowing the model to decide whether 5 Figure 3. Generation pipeline of OVO-Bench. Within public annotations, data is carefully filtered and relevant multiple-choice QAs are auto-generated. The effective system prompt and efficient answer prompt are employed to guide MLLMs toward precise outputs. The Video-LLMs we use to annotate videos are GPT-4o and Gemini-1.5 Pro. Figure 4. Left Queries Temporal Distribution in OVO-Bench. Center Linguistic Characteristics of Text Queries. Right Video category distribution of OVO-Bench. existing information has provided enough clues to answer the users query. 3.3. Datasets Statistics OVO-Bench consists of 644 unique videos spanning 7 major domains, including Sports, Video Games, and Tutorial, among others. The video durations range from few minutes to half an hour, with the average query timepoint being 428.89 seconds. Figure 4 Left illustrates the duration distribution of the queries within OVO-Bench. The benchmark includes 2,814 question-answer (QA) pairs, featuring large number of multiple-choice questions and smaller set of open-ended questions. The number of options for the multiple-choice questions varies between 2 and 5, rather than being fixed at four. The distribution of video category is visualized in Figure 4 Right. Gemini-1.5-Pro [45], Qwen2-VL [47], LLaVA-NeXTVideo [28], LLaVA-OneVision [27], InternVL-V2 [8] and LongVU [42], (2) Online Multimodal Models, including Flash-VStream-7B [58] and Videollm-Online[5] (3) Blind LLMs, including GPT-4-turbo [35]. (4) Human Agents. To ensure fair comparison of model performance, we adhere to the principle of consistency by maintaining the same number of frames or frames per second (fps) across all models. Considering the limitations on input video length for existing offline Video-LLMs, we adopt specialized video input methods tailored to such models. Specifically, we segment the video into clips based on the timestamps of the questions.For instance, for question Qi posed at timestamp ti, we extract the video clip Video[0 : ti] as the visual input. This approach simulates streaming questionanswering scenario in online video understanding. 4. Experiments 4.2. Main Results This section presents comprehensive experiments and indepth analyses of OVO-Bench. 4.1. Models and Evaluation Strategies We evaluate four existing types of models: fline Multimodal Models, (1) Ofincluding GPT-4o [37], Table 1 reports the performance of eleven models under different settings on OVO-Bench, including the Real-Time Visual Perception, Backward Tracing, and Forward Active Responding. Our evaluation brings several important findings, as follows: Offline Video-LLMs video understanding capabili6 Model # Frames Human Agents GPT-4-turbo Gemini 1.5 Pro GPT-4o Qwen2-VL-72B LLaVA-NeXT-Video-7B LLaVA-OneVision-7B Qwen2-VL-7B InternVL-V2-8B LongVU-7B Flash-VStream-7B VideoLLM-online-8B - - 1fps 64 64 64 64 64 1fps 1fps 2fps OCR ACR Real-Time Visual Perception STU ATR FPD OJR Backward Tracing Avg. EPM ASI HLD Avg. Forward Active Responding Overall Avg. Overall Avg. REC CRR Avg. SSR Human 93. 92.57 94.83 92.70 91.09 28.86 24. 25.67 33.76 27.72 94.02 93.20 Blind LLMs 27.90 26. 92.59 93.02 91.37 92.33 95.48 89. 93.56 92.90 92.81 42.76 48.65 70. 53.82 - - - - - Proprietary Multimodal Models-Offline 87.25 69.13 66.97 65.14 80.17 65.52 54.49 50.00 68.32 68. 67.39 63.68 70.77 63.63 68.59 49.83 75.68 70.95 52.69 55.38 62.32 58. 35.53 27.58 74.24 73.21 61.67 59.4 57.15 53.40 Open-source Multimodal Models-Offline 72.48 69.80 67.11 69.13 68.46 55. 56.88 59.63 58.72 53.21 58.72 49.54 77.59 66.38 69.83 63.79 68.97 59.48 52.25 50.56 49.44 50.56 44.94 48.31 74.26 72.28 71.29 66.34 67.33 68.32 61.41 61.41 60.33 60.87 55.98 63.04 65.81 63.34 62.79 60.65 60.73 57. 51.52 51.18 52.53 44.44 43.10 43.10 73.65 64.19 58.78 66.89 61.49 66.22 63.44 9.68 23.66 34.41 27.41 9.14 62.87 41.68 44.99 48.58 44.00 39.49 37.68 34.10 24.79 30.09 25.79 16.62 60.10 67.57 66.93 65.66 57.55 69. 45 60.83 60.83 50.83 52.92 60.00 47.59 54.17 50.85 48.86 45.42 48.54 Open-source Multimodal Models-Online 25.50 8.05 32.11 23.85 29.31 12. 33.71 14.04 29.70 45.54 28.80 21.20 29.86 20.79 36.36 22.22 33.78 18. 5.91 12.18 25.35 17.73 5.44 - 67.25 - 60.00 - 44.23 - 65.25 58.58 58.76 53.06 52.88 52.70 50.05 48.48 33.15 - Table 1. Detailed evaluation results on OVO-Bench. refers to using low resolution. To enhance the challenge of the questions by increasing the time interval between the question and the clues, the question time for [EPM] and [ASI] in the table is uniformly placed at the end of the video. [HLD] are evaluated in the standard manner following the streaming mode. For Forward Active Responding, accuracybased evaluation metrics are utilized in this table. More high-quality [ASI] tasks are supplemented, further enhancing differentiation. ties can be effectively transferred to real-time video understanding. The results demonstrate that offline VideoLLMs, despite being designed for offline processing, perform competitively in Real-Time Visual Perception tasks. This suggests that the advanced video comprehension abilities developed in offline settings are transferable and can enhance performance in certain online scenarios, thereby partially bridging the gap between offline and online video understanding. Current Video-LLMs lack temporal prioritization when handling VQA tasks. Existing Video-LLMs do not prioritize real-time temporal information when answering questions, leading to an inability to accurately locate the correct scene when multiple misleading scenes matching the question appear in the video stream, as shown in Fig2. Even the best current proprietary models achieve only 54.49% and 66.97% on [STU] and [ACR] tasks, respectively, which represents significant gap compared to Human Agents. powerful LLM backbone is the key to achieving high-performance video understanding. As shown in Table 1, the Qwen2-VL-72B model significantly outperforms its smaller counterpart, Qwen2-VL-7B, across all evaluated metrics. The larger models superior architecture allows it to excel in Real-Time Visual Perception with an average score of 65.81%, compared to 60.65% for the 7B variant. Additionally, Qwen2-VL-72B demonstrates enhanced capability in Backward Tracing tasks, achieving score of 62.87%, notably higher than the 48.58% scored by Qwen2-VL-7B. These results highlight the importance of robust and powerful LLM backbone in effectively processing and understanding complex video data, underscoring the need for more powerful architectures to achieve high performance in video understanding tasks. Hallucinations are prevalent in Video-LLMs. The [HLD] in Table 1 measures hallucinations in VideoLLMs [59], indicating that hallucinations are significant issue, particularly in open-source and online models. Proprietary models like Gemini 1.5 Pro perform better in managing hallucinations, yet there remains notable gap compared to human performance(52.69% vs. 91.37%). This problem arises due to the models inability to fully comprehend complex visual and temporal contexts, leading to errors in interpretation and response. Addressing hallucinations is crucial for improving the reliability and accuracy of Video-LLMs in real-world applications. 4.3. Comparison between online Video-LLMs and offline Video-LLMs Models like Gemini 1.5 Pro and Qwen2-VL-72B, representative of offline Video-LLMs, demonstrate strong performance across various tasks, as shown in Fig5. Specifically, Gemini 1.5 Pro achieves the highest average score among these models. This superior performance suggests that offline models, despite not being designed for online or realtime processing, can effectively comprehend and process complex visual information when provided with sufficient computational resources and pre-processing time. Their architectures typically allow for processing the entire video sequence holistically, leveraging global context and detailed temporal information, which enhances their temporal understanding and reasoning capabilities. In contrast, Flash-VStream-7B, representing online Video-LLMs, shows comparatively lower performance in 7 ing thinking schema and further explore their potential in always-on visual perception. Evaluation Pipeline and Metrics. As illustrated in Fig.6, We propose to query the Video-LLMs densely along the temporal axes, particularly around the interested events. Our main concerns are twofold: 1) Encourage models timely finding of the right clues, and 2) Avoid any possible hallucination before the right clue appears. For the [REC] task, larger counting numbers are awarded. Based on this, we proposed our designed scoring metrics for the three tasks in the Forward Active Responding. Offline Models for Online Video Understanding. Despite their promising performance on the Backward-Tracing and Real-Time Visual Perception, in which the models are given full information for making confident responses, our preliminary results show that even state-of-the-art offline models like Gemini-1.5-Pro, fails to capture the linguistic information of ongoing querying, showing limited understanding of online video content. 5. Conclusion and Future Work In this work, we introduced OVO-Bench, comprehensive benchmark designed to assess online video understanding capabilities of Video-LLMs across three critical modes: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding.We anticipate that OVO-Bench will serve as valuable resource for the research community, guiding the development of Video-LLMs toward practical, real-world applications. By highlighting current limitations and providing platform for rigorous evaluation, we hope to inspire future research dedicated to advancing online video understanding and achieving human-level comprehension in artificial intelligence systems. Figure 5. Performance comparison between online VideoLLMs and offline Video-LLMs. The figure illustrates the average scores of different models on the OVO-Bench in real-time visual perception tasks. Figure 6. Multiple triggering evaluation pipeline of prompt offline models for online video understanding. Offline VideoLLMs are densely queried along the temporal axes to make independent decisions of whether existing visual content provide enough clues for answering. real-time perception tasks compared to offline models. This model is designed to process video in streaming manner, handling inputs frame by frame with strict latency constraints to achieve real-time responsiveness. The performance gap highlights potential trade-off between realtime processing capabilities and the depth of visual understanding. 4.4. Forward Active Responding We include our evaluation pipeline design for our proposed Forward Active Responding. While our high-quality human-annotated queries and clues lay an ideal testbed for future real-world online understanding models, existing naively designed online video models usually collapse in our evaluation process. We made our initial attempts to leverage our multiple-triggering query pipeline to prompt offline VideoLLMs to perform online video understand-"
        },
        {
            "title": "References",
            "content": "[1] Leonard Barmann and Alex Waibel. Where did leave my keys?-episodic-memory-based question answering on egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1560 1568, 2022. 5, 4 [2] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 2 [3] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models, 2024. 3 [4] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: arXiv preprint 1-hour video-language understanding. arXiv:2411.04998, 2024. 3 [5] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In CVPR, 2024. 1, 2, 3, 5, 6 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [7] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering, 2024. 3 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 6 [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 2 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 3 [11] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 2, 5 [12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3 [13] Alex Gorban, Haroon Idrees, Yu-Gang Jiang, Roshan Zamir, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. Thumos challenge: Action recognition with large number of classes, 2015. 5, [14] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 4 [15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 2, 5 [16] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding supplementary material. 3 [17] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 709727. Springer, 2020. 2, 5, 4 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5 [19] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766, 2017. 2 [20] Yu-Gang Jiang, Jingen Liu, Roshan Zamir, George Toderici, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. Thumos challenge: Action recognition with large number of classes, 2014. 5, 4 [21] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. 3 [22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [23] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 3 [24] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 9 Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 5 [25] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. 2024. [26] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. 2, 5 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 2, 6 [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6 [29] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2025. 3 [30] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Chang Wen Chen, and Ying Shan. E.t. bench: Towards open-ended event-level video-language understanding. In Neural Information Processing Systems (NeurIPS), 2024. 2, 5, 4 [31] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16488 16498, 2024. 5, [32] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16488 16498, 2024. 5 [33] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding, 2023. 3 [34] Salman Khan Muhammad Maaz, Hanoona Rasheed and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. ArXiv 2306.05424, 2023. 2, 3 [35] OpenAI. Gpt-4 technical report, 2023. Technical report. 2, 3, 6 [36] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt4o, 2024. 5 [37] OpenAI. Hello gpt-4o, 2024. https://openai.com/ index/hello-gpt-4o. 6 [38] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 5, 4 [39] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. arXiv preprint arXiv:2405.16009, 2024. 3 [40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3 [42] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 3, 6 [43] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding, 2023. 3 [44] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video In Proceedings of the IEEE/CVF Conference analysis. on Computer Vision and Pattern Recognition, pages 1207 1216, 2019. 5, 4 [45] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3, 5, 6 [46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3, 5, 6 [48] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 3 [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [50] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reason10 ing in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 5, 4 videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 4 [64] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Crosstask weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35373545, 2019. 5, 4 [51] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. [52] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 3 [53] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 2, 3 [54] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 2 [55] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 2, 3 [56] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. 5, 4 [57] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2, [58] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memorybased real-time understanding for long video streams, 2024. 2, 3, 5, 6 [59] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024. 7 [60] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition, 2023. 2 [61] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2 [62] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 5 [63] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? 6. More Details of Evaluation Accuracy-Based."
        },
        {
            "title": "Supplementary Material",
            "content": "6.1. Evaluation for Online Models on Forward Active Responding As no existing online models can satisfy the demand imposed by our original designs, we choose not to cover this part in our main paper. We introduce an effective evaluation metric tailored for each task consisting of two different dimensions. Guidance for evaluation metrics design. Accuracy-Based. The models responses should, first of all, be correct without misleading information. We judge the effectiveness of the answer given by the model, and simply average all of them to give the accuracy. the appropriate response Am at time tm, Score-Based. Based on the accuracy-based evaluation, we encourage the response to be both accurate and timely and therefore devise scoring metric. Details of evaluation metrics. Given the users queries Qti at time ti, the referred events Ej (such as specific step of tutorial procedure) with the time interval from tj to the j, models responses Rm at time m, the evaluation function (Rm, Am), which directly compare the models responses against the right ones, the evaluation metrics of different models are formally given as follows. 1. [REC] In this task, the query is only made at certain time before one complete repetition event happens. In our benchmark, the query is made at the start of the video, i.e. only Q0 is made. Accuracy-Based. Acc = (cid:80)N i=1 (Rm, Am) Score-Based. Score = (cid:88) i=1 eip1 (Rm, Am) 2(mm)p2 where (Rm, Am) = [Am == Rm], which gives 1 if the models response is the same as the answer, and gives 0 otherwise. p1 and p2 are parameters to balance the weight. In our evaluation, they are set to 0.2 and 0.05 respectively. 2. [SSR] In this task, query like Illustrate me on how to make sandwich according to the video is made before the start of the procedure. Akin to [REC], the query is only made at the start of the video, i.e. only Q0 is made. Acc = (cid:80)N i=1 (Rm, Am) Score-Based. Score = (cid:88) i= (Rm, Am) 2(mm)p where we leverage GPT-4o to give (Rm, Am), measuring the effectiveness of Rm given the reference answer Am and relevant visual content. is set to 0.5 to balance weight in our evaluation. 3. [CRR] In this setting, queries are made before every Am, i.e. range(i) == range(m). Accuracy-Based. Acc = (cid:80)N i=1 (Rm, Am) Score-Based. Score = (cid:88) i=1 (Rm, Am) 2(mm)p where we leverage GPT-4o to give (Rm, Am), measuring the effectiveness of Rm given the reference answer Am and relevant visual content. is set to 0.5 to balance weight in our evaluation. Prompt Design. To adapt to the online scenarios, we constructed streaming mode prompts with accurate timestamps and also deleted the complicated instructional statement compared to 6.2. Prompts and examples of models responses are shown in 7. 6.2. Prompt Design for Offline Models on Forward"
        },
        {
            "title": "Active Responding",
            "content": "The Forward Active Responding task is intrinsically inappropriate for offline models, as these models only support queries about existing video contents and can not receive additional visual frames after the query is made. However, considering the superiority of offline models against existing online models, we design multiple-triggering evaluation pipeline and prompt offline models to decide whether the current time is appropriate for answering the users query. Formally, given the users query Qt0 at t0, we leverage offline models to decide at ti, 1; ti > t0 whether 1 Figure 7. Prompts used for Online(up) and Offline(down) Models on Forward Active Responding and Response Examples. Despite our vision for online models, existing online models, like videollm-online, are still far from satisfactory, showing limited adaptation ability, and would easily encounter collapse when processing complicated or out-of-training-domain video and queries. Offline models are inclined to perform random guessing when the queries contain words like is/currently/ongoing. video contents from t0 to ti offer sufficient clues. Specifically, for each of the tasks under the Forward Active Responding setting, instructional prompts and examples of models [5][58] responses are shown in Fig. 7. 6.3. Prompt Design for Models on Backward Tracing and Real-Time Visual Perception We use the clip from the beginning to the query time to query models. Prompts and examples of models responses are shown in Fig. 8. 2 7. More Details of Benchmark Construction 7.1. Human-annotated QA Generation We leverage meticulous human labor for part of the QA generation. Real-Time Visual Perception. For tasks, including [STU], [OJR], and [ATR], we invite volunteers to propose candidate questions in supplement to our Video-LLMsbased automatic generation pipeline. This procedure is designed to alleviate possible bias and increase diversity. Specifically, we provide our volunteers with the following guidelines: Watch the video and decide whether this candidate is appropriate for constructing questions that can be classified into the above three types. Figure 8. Prompts used for Online(up) and Offline(down) Models on Real-Time Visual Perception and Response Examples. Three tasks including [ACR], [OCR], and [ASI] are included as demonstrations. Our benchmarks involve large ratio of questions, whose answers shift over time, which means that models can hardly figure out the answer by randomly selecting frames from original videos. Selected appropriate moments for problem construction. Consider whether the moment contains: 1. Obvious spatial relationships between several objects; 2. Interested objects, such as something that appears in the moment, and so on; 3. Objects with unusual attributes, e.g. green fire, smooth woods. Construct options for the questions. Ensure that 1. Options should be relevant to the visual content; 2. Incorrect options should bring misleading information from the visual content; 3. Options should be as close in length as possible. Clue Reveal Responding. For our novel [CRR] task, we find it difficult to construct satisfactory question proposals by straightly prompting Video-LLMs with original video content as reference or LLMs with the provided scripts and subtitles as reference. So we recruit volunteers to propose queries and corresponding answers. Our guidelines for volunteers are as follows: Find scenes with apparent discontinuity. For example, character performs certain action at query time Qi. However, the actions complete process or outcome is not immediately shown during query time. Continue watching the video, find clues for your query, and annotate the clues revealing time as Ai. Try to provide concise timestamps, let Ai be the time when enough visual information has just been revealed. 8. Additional Dataset Analysis 8.1. Task and Sample Distribution Fig. 9 illustrates the distribution of questions and videos in OVO-Bench across the twelve tasks listed in Fig. 2. 3 10. Licenses The annotations of our OVO-Bench are provided to the community under CC BY-NC-SA 4.0 license. By downloading our dataset from our website or other sources, the user agrees to adhere to the terms of CC BY-NC-SA 4.0 and licenses of the source datasets. Download links are provided for our self-crawled YouTube videos. Licenses of the source datasets are listed in"
        },
        {
            "title": "License",
            "content": "N/A MIT License Apache License 2.0 MIT License MIT License BSD 3-Clause License Research Purpose Only MIT License Research Purpose Only Research Purpose Only QAEgo4D [1] OpenEQA [31] STAR [50] HiREST [56] YouCook2 [63] CrossTask [64] COIN [44] Ego4D [14] THUMOS14 [20] THUMOS15 [13] Perception Test [38] CC BY 4.0 MovieNet [17] E.T.Bench [30] N/A CC BY 4.0 Table 2. License of source datasets in OVO-Bench. 11. Data Examples We provide more examples extracted from our benchmark. We try to cover different video categories in every task to offer holistic overview of OVO-Bench. Figure 9. Distribution of questions and video in OVO-Bench. Figure 10. Distribution of averaged query timestamps and video duration (in seconds) in OVO-Bench. Specifically, the averaged video duration in CRR is 6,857 seconds. 8.2. Query Timestamps and Video Duration Fig. 10 illustrates the distribution of averaged query timestamps and video duration in OVO-Bench across the twelve tasks listed in Fig. 2. 9. Limitations While we have tried hard to cover wide range of reasonable video domains and QA generation methods, the scarcity of existing datasets with annotations that fit requirements, the unsatisfactory results of automatic QA generation, and the high human annotation cost, hinder diversity and can cause potential bias. Offline Models for Online Video Understanding. As implied in our analysis 7, offline models usually perform random guesses in the forward active responding scenarios, making our evaluation unfair. For example, model that always outputs Yes can still achieve score above zero in our evaluation. Moreover, the absence of online models with satisfactory performance, makes our benchmarks more suitable for future advancements. We hope our intensive work and intuitive ideas can guide the development of video understanding models toward real-world online video understanding."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Communication University of China",
        "SenseTime Group",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}