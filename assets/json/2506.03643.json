{
    "paper_title": "Images are Worth Variable Length of Representations",
    "authors": [
        "Lingjun Mao",
        "Rodolfo Corona",
        "Xin Liang",
        "Wenhao Yan",
        "Zineng Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a simple image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a dynamic vision encoder that produces a variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder.github.io/dove-encoder."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 3 4 6 3 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Images are Worth Variable Length of Representations",
            "content": "Lingjun Mao1 Rodolfo Corona2 1University of California, San Diego Xin Liang2 Wenhao Yan3 Zineng Tang2 2University of California, Berkeley 3University of Washington lingjun@ucsd.edu, {rcorona, terran, xinl}@berkeley.edu wenhao77@uw.edu Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Most existing vision encoders map images into fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, visually complex image (e.g., cluttered room) inherently carries more information and thus deserves more tokens than simple image (e.g., blank wall). To address this inefficiency, we propose DOVE, dynamic vision encoder that produces variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder. github.io/dove-encoder. Figure 1: Dynamic Visual Representations. As the number of tokens used by DOVE increases, the reconstructed images shows finer and high frequency details."
        },
        {
            "title": "Introduction",
            "content": "Image representation learning [56] is fundamental component of computer vision; it plays pivotal role in various visual tasks, including image classification [38, 12], object detection [62, 61], and semantic segmentation [26, 27]. Vision representation models are also widely used in multi-modal learning, where they serve as powerful vision encoders within vision-language models (VLMs), converting image information into discrete token sequences. Existing image representation learning methods generally fall into two categories: semantic feature learning (e.g., CLIP [46], DINO [10]) and autoencoder-based image tokenization (e.g., VQGAN [21], VAE [31]). All of which aim to generate fixed length sequences. However, studies have shown that vision tokens suffer from information redundancy [11]. We conjecture that different images have different complexity such that they can be represented with different lengths of tokens for reconstruction. To this end, we propose DOVE (Dynamic Output Vision Encoder), visual tokenizer that adaptively generates variable-length sequences of continuous visual tokens for image reconstruction. Our method extends the standard visual autoencoder framework by incorporating transformer-based dynamic token generator (Figure 2), which is capable of generating an end-of-sequence (EOS) token at any position to terminate the output sequence. We jointly optimize image reconstruction quality and EOS token prediction based on an MSE threshold, and truncate token sequences at the predicted EOS. Our method effectively shortens the token sequence length while maintaining high reconstruction quality (Figure 1). As token sequences progress, their reconstructions show more high-frequency details and additions of objects, and then saturate at (EOS) token. By learning dynamic token lengths, we find that the tokenizer learns richer semantics and observe the emergence of zero-shot semantic segmentation by PCA on the hidden features. We perform extensive experiments on reconstruction, classification, and question answering by replacing vision backbones in vision language models. Our approach consistently and significantly outperforms other autoencoder-based tokenization methods while enjoying improved efficiency from dynamic length. Considering that human vision is an active and task-driven process, and that humans tend to focus on task-relevant regions while ignoring irrelevant ones when answering questions [4, 35, 17], we additionally introduce query-conditioned variant of DOVE. This model is able to read the users query and reconstruct the input by focusing on semantically relevant regions, thereby further reducing the length of the generated token sequence. In practice, given text query and corresponding salient image region during training, we feed the text query to the token generator and apply higher weights to the reconstruction loss specifically corresponding to the salient region. We find that this approach further improves token efficiency, semantics, and vision language model performance. We summarize our contributions as follows: We propose DOVE, visual tokenizer that dynamically generates tokens based on image complexity. Unlike previous visual tokenization, our model supports arbitrary control over the token sequence length in single parallel forward. We propose variant of DOVE that grounds token generation on text query and its corresponding salient visual regions. This query-conditioned model achieves higher token compression rate (averaging 68%) and demonstrates stronger semantic representation. We observe phenomenon of emergent semantics by probing the latent representation. Compared to other autoencoder-based tokenization methods with fixed-length token representations, our model achieves significantly better performance on classification, vision-language QA, and shows emerging semantic segmentation properties."
        },
        {
            "title": "2 Dynamic Vision Tokenizer",
            "content": "We introduce DOVE, dynamic vision encoder that adaptively generates variable number of continuous visual tokens to reconstruct each image."
        },
        {
            "title": "2.1 Model Architecture",
            "content": "An overview of our model is shown in Figure 2. Our model consists of four main components: VQGAN Encoder, VQGAN Decoder, transformer-based dynamic token generator, and transformerbased token decoder. We use 70M transformer [7] as the backbone for both the autoregressive token generator and non-autoregressive version for token decoder. For each image Xv, the VQGAN Encoder converts the visual information into fixed-length token sequence Hv. Timestamp encodings t1, t2, . . . , tn, generated using periodic embeddings such as sinusoidal encodings [55], are then appended to Hv. This combined sequence is input into the dynamic token generator fϕ. To enable sequential token generation, we restrict each position to attend 2 Figure 2: Dynamic Tokenizer. only to its current or preceding timestamps. The dynamic token generation process from timestamp t0 to ti is defined as: = fϕ(Hv, t1, t2, . . . , ti) = (d1, d2, . . . , di) (1) where denotes the generated token sequence, and di is the token produced by the model at ti. We introduce dynamic length variation by detecting the EOS token from the models discrete output and replacing all visual token (continuous latent outputs) from that position onward with zero vectors. Since the EOS token can appear at any position, the length of the generated token sequence can vary based on the complexity of the image. We use an additional non-autoregressive token decoder gϕ to decode the padded dynamic vision token sequence and feed it to the final VQGAN decoder."
        },
        {
            "title": "2.2 Dynamic Image Reconstruction",
            "content": "A more complex image, which contains richer and finer-grained details, will require more tokens to capture all its visual information compared to simpler one. By learning when to generate EOS, the model can adaptively produce token sequence that is just long enough to capture the images essential visual content. We jointly train all components of the model. Following the training strategy of VQGAN [21], we adopt combination of mean squared error (MSE) loss and perceptual loss to supervise the image reconstruction process. lightly weighted adversarial (GAN) loss is also applied to enhance the realism of reconstructed images. The final reconstruction loss Lrec between the input image Xv and the reconstructed image ˆXv is defined as: Define: weights λrec, λeos, time encodings Image Xv, max tokens K, window , Hv VQGAN_Encoder(X) Initialize EMArec 0 for each training iteration do [ ], 1 while do di fϕ(Hv, T1:i) (generating token) append di to D, + 1 Find the first index such that D[j] = EOS if such exists then for = + 1 to do D[k] 0 ˆX VQGAN_Decoder(cid:0)gϕ(D)(cid:1) Compute Lrec via Eq. (2) Update EMArec over the last losses if Lrec > EMArec then Leos peos(i) else Leos 1 i1 Lrec = λmse Lmse +λperc Lperc +λgan Lgan (2) During training, we set the weighting factors to λmse = 1, λperc = 0.1, and λgan = 5 1010 to prevent hallucination. In parallel with improving reconstruction quality, we guide the model to adaptively adjust the length of the generated token sequence through EOS prediction. Specifically, we use the average reconstruction loss Lrec over the previous 100 training steps as dynamic threshold. For given sample, if its current reconstruction loss is lower than the threshold, it indicates that fewer tokens are sufficient for satisfactory reconstruction, and we encourage earlier Ltotal λrec Lrec + λeos Leos Update parameters ϕ using ϕLtotal Table 1: Training Pseudocode j=1 peos(j) (cid:80)i 3 EOS prediction by maximizing the EOS probabilities at all preceding positions. Conversely, if the reconstruction loss exceeds the threshold, it suggests that more tokens are needed, and we minimize the EOS probability at the current position. We denote the predicted EOS probability at position as peos(i), where indicates the current EOS position. The token length control loss is defined as: Leos = peos(m), 1 1 if Lrec > Threshold peos(i), if Lrec Threshold m1 (cid:80) i= (3) Finally, we jointly optimize Lrec and Leos to guide the model in dynamically reconstructing the image. The overall training loss is defined as: Ltotal = λrecLrec + λeosLeos (4) where λrec and λeos are the corresponding weighting coefficients. To facilitate faster convergence, we initially set λeos to small value and gradually increase it during training, allowing the model to first focus on accurate reconstruction before learning to adaptively control the token sequence length."
        },
        {
            "title": "2.3 Q-DOVE: Query-conditioned Tokenization",
            "content": "We extend DOVE to Q-DOVE for use in text-conditioned vision and language domains (Figure 3), allowing it to dynamically adapt image representations in query-dependent manner. Q-DOVE is trained to focus image representation resources on image regions relevant to given query. Given supervised dataset of images paired with text queries and bounding boxes encapsulating their answers, we modify the reconstruction loss to focus over image regions within each examples set of bounding boxes Sbb. Specifically, we upsample each image region contained by bounding box bi Sbb to an image bb and compute the reconstruction loss over it as in Eq. 2: rel = Lrec(I Li bb) (5) In order to encourage the model to maintain some fidelity over the region outside of the bounding boxes, we also compute the MSE loss over Io, the complement of Sbb: The final loss averages over relevant regions and weighs loss over the irrelevant region down by λo: Lirr = Lmse(Io) (6) Lqry = (cid:80) biSbb Sbb"
        },
        {
            "title": "Li\nrel",
            "content": "+ λo Lirr (7) In our experiments, we set λo to 1e-10. To compute Leos, we employ the same procedure as in Eq. 3, comparing Lrel to threshold determined by its average loss over previous training steps. If Lirr falls below the threshold, we introduce an additional penalty Lpen to explicitly encourage the model to generate the EOS token earlier: Lpen = 1 1 m1 (cid:80) i= peos(i). Our supervised masking strategy yields dual benefit, allowing the model to learn both where to look and how much information to encode from image regions relevant to inputted queries. Bounding boxes are only used during training."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, We evaluate our approach at multiple levels, including the quality of the generated vision tokens (e.g., image reconstruction and token length distribution), as well as their effectiveness in downstream vision-language tasks. The results demonstrate that our model achieves high reconstruction quality with significantly fewer tokens, while capturing richer semantic information compared to static autoencoder-based tokenization methods. We further investigate the phenomenon of emergent semantics in Section 3.4. 4 Figure 3: Query Conditioning. DOVE is trained with bounding-box based loss, learning to focus its dynamic token resources on representing query-relevant image regions."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Training Details. We use pretrained VQGAN [21] with codebook size of 8192 and lightweight Pythia-70M [7] language model as the backbone of our framework. The model is fine-tuned on ImageNet-1K [16] for 20 epochs using two NVIDIA RTX 4090 GPUs. For the query-conditioned variant, we conduct an additional 5 epochs of training on the Visual Genome [32] and Open Images [34] datasets. We directly use the provided questions and region-level captions in Visual Genome as textual queries to guide the model in reconstructing content within specified bounding boxes, while ignoring irrelevant regions. Since Open Images does not offer region-level descriptions or questions, we instead construct text queries from relation graph annotationsfor example, cup on tableand define the target region by concatenating the bounding boxes of the associated objects. To improve the models generalization ability, we randomly replace 50% of the training text queries with the string null, and train the model to reconstruct the entire image when this placeholder is provided as input. Baselines. We compare our model against several state-of-the-art encoder-decoder frameworks, including TiTok[60] and VQGAN. We choose VQGAN with an output length of 256 tokens. For TiTok, we consider three variants with token lengths of 32, 64, and 128. We also include ALIT [20], dynamic vision encoder trained via recurrent distillation from VQGAN. Unlike our method, however, ALIT only supports token lengths that are multiples of fixed stride (e.g., 32). All models are trained on ImageNet-1K under the same configuration to ensure fair comparison."
        },
        {
            "title": "3.2 Token-Level Evaluation",
            "content": "Image Reconstruction Quality. We report FID scores of the reconstructed images across varying token lengths. Our results show that as the token length increases, the reconstruction quality of our model consistently improves. At all evaluated token lengths, our method outperforms ALIT. This advantage becomes especially clear at lower token counts. ALIT often generates hallucinated content, including severe object distortions. For example, when the token length is limited to 32, the reconstructed chameleon and beetle exhibit noticeable deformations (Figure 4). In contrast, our model produces slightly blurry but structurally and semantically faithful reconstructions. When using the full token length of 256, our method surpasses VQGAN on the COCO and WIT datasets. Detailed results are provided in Table 2. Approach ImageNet100 COCO Wikipedia (WIT) 64 96 128 160 192 256 TiTok-L-32 TiTok-B-64 TiTok-S-128 VQGAN ALIT DOVE 11.60 - - - 22.31 18.91 - 8.22 - - 15.92 11.46 - - - - 13.08 10.84 - - 8.22 - 11.45 9. - - - - 10.01 8.61 - - - - 9.12 8.25 - - - - 8.37 7.96 - - - 7.04 8.06 7.73 32# / 64 14.18# 9.15 - - 22.01 15.50 - - 9.15 - 13.98 9.83 256 - - - 7.77 9.51 7.54 32# / 64 53.57# 42.86 - - 61.32 14.83 128 - - 38.16 - 47.52 8.56 - - - 31.27 38.10 7.84 Table 2: FID scores () across the ImageNet100, COCO, and WIT datasets. Our method consistently outperforms ALIT across all token lengths, and achieves comparable or even better results than VQGAN and TiTok at several lengths. Classification. We evaluate the representation quality of DOVE as an off-the-shelf, frozen backbone across three standard recognition benchmarks, including CIFAR-100 [33], ImageNet-100 [18], and 5 Figure 4: Reconstructed images on ImageNet-1K using different methods. As the token length increases, our method produces progressively clearer reconstructions with more visual details. STL-10 [45]. Specifically, we train lightweight MLP classifier on top of the frozen features, using both mean and max pooling over the final layer representations. As the number of tokens increases, the classification accuracy of both DOVE and ALIT steadily improves. Our approach consistently outperforms all other vision tokenizers by substantial margin. Even when using as few as 32 tokens, it achieves higher classification accuracy than all competing methods. We attribute this advantage to our dynamic reconstruction training objective, which enables the model to capture additional semantic information during representation learning. This is further evidenced by the linear probing and PCA-based zero-shot segmentation results presented in Section 3.4. (a) CIFAR100 (b) ImageNet100 (c) STL-10 Figure 5: Classification accuracy with different visual tokenizers under varying token lengths. DOVE consistently outperforms all baselines across all lengths. Token Length Distribution. Unlike ALIT, our model explicitly supports mechanism for generating arbitrary-length token sequences at inference time. We analyze the distribution of token sequence lengths (i.e., EOS positions) generated by DOVE. As shown in Figure 6a, most sequences are shorter than 100 tokens, with smaller peaks around 150 and 250. We randomly sample 5,000 images from the MS COCO 2017 validation set [36] and compute the reconstruction loss across different token lengths. Figure 6b shows that reconstruction loss decreases as token length increases. This decline is steepest between 0 and 100 tokens, and becomes more gradual beyond that. To further investigate the relationship between token length and image content, we calculate the complexity of input images using Laplacian variance [5] and analyze the correlation between image complexity and the length of the generated token sequences. As shown in Figure 6c, by encouraging samples with lower reconstruction quality to delay the EOS position and those with higher quality to emit EOS earlier during training, DOVE naturally learns to allocate longer token sequences to more complex images, while assigning shorter sequences to simpler ones. The Pearson correlation coefficient between image complexity and token sequence length is 0.742."
        },
        {
            "title": "3.3 Downstream Vision-Language Task Evaluation",
            "content": "Query-conditioned Tokenization. We visualize the behavior of our query-conditioned DOVE (Q-DOVE) on the Visual Genome dataset. Figure 7 presents several examples. The results show 6 (a) Distribution of token sequence lengths (i.e.,EOS positions) generated by DOVE. (b) The relation between token length and reconstruction loss across different input samples. (c) The relation between token sequence lengths (i.e.,EOS positions) and image complexity. Figure 6: Token length analysis that when the input query is null, the model clearly reconstructs the entire image. In contrast, when relevant question or description is provided, the reconstruction focuses on the semantically related regions and produces lower frequency outputs for background. This task-driven compression even further reduces the average token sequence length. We then evaluate Q-DOVE and the original DOVE model as vision encoders in downstream vision-language tasks. Figure 7: Reconstructed images from the Q-DOVE. When the text query is set to null, the model reconstructs the entire image. When query is provided, the model focuses on query-relevant regions. Visual Question Answering Evaluation. To evaluate the quality of our models token representations, we replace the vision encoder in vision-language model with different visual representation methods and evaluate them on downstream vision-language tasks. We adopt Vicuna-7B-v1.5 [37] as the language model, interfacing it with two-layer MLP that maps the vision encoder outputs to the language model input space. Following the training strategy of AIM V2 [22], we set the learning rate of the language model to 2e-5 and that of the adapter layers to 2e-4. This setup enables joint finetuning in single-stage training process. We fine-tune the model with different vision encoders for one epoch on the 665K mixed VQA dataset used in LLaVA [37]. The model is evaluated on broad set of benchmarks, including VQAv2 [23], GQA [2], OK-VQA [41], TextVQA [51], DocVQA [44], InfoVQA [43], ChartQA [42], and ScienceQA [39]. Results show that the VLM equipped with DOVE significantly outperforms other models across all datasets. Moreover, integrating Q-DOVE further improves the accuracy. By leveraging DOVEs EOS token as truncation point, we achieve substantial reduction in token count with performance comparable to the full set of 256 tokens. For Q-DOVE, we include two input strategies for the vision encoder: providing the actual question or directly inputting null. While the null setting yields slightly better performance than using the questionwhich filters out task-irrelevant regionsthe question-guided strategy achieves comparable accuracy while further reducing the token length. 7 We also measure the inference time and floating-point operations (FLOPs) of each model, as shown in Table 3. Both our method and ALIT can effectively reduce FLOPs by shortening the length of the visual token sequence. However, due to ALITs use of recurrent distillation, where dynamic tokens are generated through multiple passes over VQGAN tokens, its inference speed is adversely affected despite the reduced sequence length. In contrast, our method relies on single forward pass, resulting in much faster inference. Model Titok VQGAN ALIT DOVE Q-DOVE # Token Count VQAv2 GQA OKVQA TextVQA DocVQA InfoVQA ChartQA ScienceQA 128 (S) 256 32 64 128 256 32 64 128 256 121.6 (Avg) 256# 256 82.4 (Avg) 43.3 40.2 38.4 39.7 41.0 43.8 50.3 51.8 52.0 52.4 52.2 55.0 53.9 52.8 38. 38.1 37.6 38.0 38.0 38.3 47.2 50.2 50.7 51.8 51.4 53.2 52.6 52.1 38.6 37. 35.6 36.4 37.2 37.8 42.2 43.5 44.8 46.2 46.0 46.7 46.2 46.0 14.3 14.3 14.2 14.3 14.3 14. 14.6 14.9 15.0 15.0 15.0 15.3 15.2 15.2 8.1 8.2 7.8 8.1 8.2 8.2 7.9 8.2 8.2 8.4 8. 8.6 8.2 8.2 17.0 16.3 16.0 16.2 16.3 16.5 18.4 18.8 19.1 19.4 19.2 19.7 19.4 19. 11.8 11.1 11.4 11.6 11.7 12.0 11.2 12.1 12.4 12.6 12.6 12.8 12.5 12.4 67. 66.3 66.0 66.2 66.5 66.8 69.6 71.7 72.5 72.8 72.6 74.8 74.0 73.1 Table 3: Performance comparison of VLMs equipped with different vision encoders. DOVE/Q-DOVE consistently achieves the best performance on most tasks. For Q-DOVE, # indicates that the input query is set to null; otherwise, the original question is used. Model VQGAN-256 ALIT-256 ALIT-128 ALIT-64 ALIT-32 DOVE-256 DOVE-128 DOVE-64 DOVE-32 Speed () FLOPs (T, ) 1.00 2.62 0.63 2.73 0.82 1.74 0.88 1. 0.92 0.98 0.96 2.66 1.14 1.70 1.19 1.29 1.26 0.96 Table 4: Inference speed and FLOPs (in teraflops) of different models. Inference speed is reported as the ratio relative to VQGAN, based on actual inference time measured on the VQAv2 test set."
        },
        {
            "title": "3.4 Probing Emerging Semantics",
            "content": "From previous experiments, we observe that the visual representations generated by DOVE significantly outperform those produced by fixed-length, autoencoder-based tokenization methods in both classification and downstream multimodal tasks. In this section, we further investigate this emergent semantic property through series of analyses. Specifically, we evaluate the quality of the learned representations via linear probing on models hidden layers instead of generated visual tokens and PCA-based image segmentation. We compare DOVE, Q-DOVE, and other fixed-length autoencoder-based tokenizers by conducting linear probing on seven benchmark datasets: CIFAR10 [33], CIFAR-100 [33], DTD [14], FGVC [40], Food101 [9], STL-10 [15], and SUN397 [57]. For Q-DOVE, we set all text queries to null to simulate the unconditional setting. Table 5 shows that DOVE consistently outperforms other methods by large margin across all datasets, and Q-DOVE further improves upon DOVEs performance. To gain deeper insight into the structure of the learned representations, we apply PCA for dimensionality reduction and visualize the results in image space. As shown in Figure 8, DOVE yields more semantically coherent segmentations compared to VQGAN, while Q-DOVE exhibits even stronger semantic alignment and clarity."
        },
        {
            "title": "Method",
            "content": "CIFAR-10 CIFAR-100 DTD FGVC Food101 STL-10 SUN397 TiTok-32 TiTok-64 TiTok-128 ALIT VQGAN DOVE Q-DOVE 24.87 25.95 18.33 41.08 41. 54.31 56.44 6.11 7.34 3.10 16.87 19.37 31.13 33.70 9.46 10.74 6.80 26.96 24.47 26.70 30.48 1.95 2.61 2.34 4.47 4. 5.85 6.03 3.81 4.53 3.05 14.47 13.28 21.18 25.32 23.23 28.06 20.25 42.15 40.46 48.38 54.86 4.44 5.23 3.02 20.94 15. 30.62 38.18 Table 5: Linear probing performance (%) of various models across benchmark datasets. 8 Figure 8: Semantics Visualization with PCA on latent features."
        },
        {
            "title": "4 Related Works",
            "content": "Image Tokenization. Image tokenization methods represent images as discrete sets of patch embeddings. In ViT formulations [19], patch representations allow for efficient feature extraction with transformer [55] in addition to direct compatibility with tokenized representations in other modALITies, such as text, through the use of projection layers [46, 37]. Through vector quantization [54, 49], patch embeddings from both CNN and transformer encoders can be represented with finite token codebook, allowing for autoregressive image generation both unimodally [21] and multimodally by conditioning on queries such as text descriptions of images [50, 59, 47]. Whether continuous or quantized, these formulations all encode images into standardized numbers of tokens, independent of image complexity or downstream task demands. In contrast, DOVE represents images using variable numbers of tokens, dynamically adapting to the complexity of images in unimodal settings and to the information demands of downstream tasks in text-conditioned ones. Token Pruning and Compression. Token pruning methods reduce computation costs by iteratively reducing the set of tokens to be processed across transformer layers, either by dynamically omitting them [58, 48] or by aggregating them in between layers of the transformer [8]. Because these methods iteratively modify the number of tokens across transformer layers, they require modification of the internal structure of models they are applied to. In contrast, DOVE produces variable numbers of tokens, allowing for it to be directly integrated into model pre-training and fine-tuning pipelines. Another branch of work reduces computational costs by compressing token sets at the input level. The Perceiver architecture uses transformer to compress set of input tokens into smaller, fixed set of latent tokens [30, 29], allowing for greater computational tractability in multimodal settings [3]. Similarly, TiTok [60] compresses image patches into small set of latent tokens, which are then quantized for image reconstruction or other downstream tasks. Closest to our work is ALIT [20], which uses recurrent process to distill 2D tokens into set of 1D latent tokens. Although this iterative process allows for images to be represented by variable numbers of tokens, this is only evidenced through post-hoc analyses, and ALIT does not propose an automated method for dynamically determining the number of tokens to represent an image with at inference time. One of the key innovations of DOVE is the use of dynamic EOS prediction mechanism, which is employed at inference time to produce per-image variable length token sequences based on image and downstream task complexity. DOVE uses parallel transformer forward pass to generate variable number of tokens, which is more efficient ALITs recurrent formulation. Dynamic Sequence Termination. In the context of transformers, dynamic sequence termination is most commonly associated with the <EOS> token in LLMs [24, 53, 1], although the concept has been applied in language modeling since N-gram models [13]. This concept has also been generalized for generating variable length subsequences of specialized text, such as chain-of-thought chains generated between thinking tokens in LLMs [25]. In sequential decision making, dynamic termination has been operationalized through the use of terminal states in Hidden Markov Models [6], termination conditions in the options reinforcement learning framework [52], as well as by using specialized stop actions within the low-level components of hierarchical policies [28]."
        },
        {
            "title": "5 Conclusion\nWe have introduced DOVE, a dynamic vision encoder that adaptively generates variable-length\ntoken sequences based on image complexity. DOVE predicts an end-of-sequence (EOS) token to\ndynamically determine the number of tokens needed for image reconstruction, resulting in signif-\nicantly improved efficiency and semantic representation. We further extended our model with a",
            "content": "9 query-conditioned variant, enabling task-specific focus on relevant image regions. Q-DOVE further improves the representations and token compression achieving stronger efficiency and performance."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [4] Ruzena Bajcsy, Yiannis Aloimonos, and John Tsotsos. Revisiting active perception. Autonomous Robots, 42:177196, 2018. [5] Raghav Bansal, Gaurav Raj, and Tanupriya Choudhury. Blur image detection using laplacian operator and open-cv. In 2016 International Conference System Modeling & Advancement in Research Trends (SMART), pages 6367. IEEE, 2016. [6] Leonard Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):15541563, 1966. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [8] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In The Eleventh International Conference on Learning Representations, 2023. [9] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 mining discriminative components with random forests. In European Conference on Computer Vision, 2014. [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [11] Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Efficient large multi-modal models via visual context compression. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [12] Leiyu Chen, Shaobo Li, Qiang Bai, Jing Yang, Sanlong Jiang, and Yanming Miao. Review of image classification algorithms based on convolutional neural networks. Remote Sensing, 13(22):4712, 2021. [13] Stanley Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359394, 1999. [14] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. [15] Adam Coates, Honglak Lee, and AY Ng. An analysis of single layer networks in unsupervised feature learning aistats. 2011. [16] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory. In International Conference on Machine Learning, pages 65656590. PMLR, 2023. [17] Marianne DeAngelus and Jeff Pelz. Top-down control of eye movements: Yarbus revisited. Visual Cognition, 17(6-7):790811, 2009. [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 10 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [20] Shivam Duggal, Phillip Isola, Antonio Torralba, and William Freeman. Adaptive length image tokenization via recurrent allocation. In First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models, 2024. [21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [22] Enrico Fini*, Mustafa Shukor*, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Louis Béthune, Zhe Gan, Victor Turrisi, Alexander Toshev, Marcin Eichner, Yinfei Yang, Moin Nabi, Josh Susskind, and Alaaeldin El-Nouby*. Multimodal autoregressive pre-training of large vision encoders, 2024. [23] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. [24] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael Lew. review of semantic segmentation using deep neural networks. International journal of multimedia information retrieval, 7:8793, 2018. [27] Shijie Hao, Yuan Zhou, and Yanrong Guo. brief survey on semantic segmentation with deep learning. Neurocomputing, 406:302321, 2020. [28] Muhammad Zubair Irshad, Chih-Yao Ma, and Zsolt Kira. Hierarchical cross-modal agent for robotics vision-and-language navigation. In 2021 IEEE international conference on robotics and automation (ICRA), pages 1323813246. IEEE, 2021. [29] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. [30] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651 4664. PMLR, 2021. [31] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [34] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. [35] Michael Land, Neil Mennie, and Jennifer Rusted. The roles of vision and eye movements in the control of activities of daily living. Perception, 28(11):13111328, 1999. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 11 [38] Dengsheng Lu and Qihao Weng. survey of image classification methods and techniques for improving classification performance. International journal of Remote sensing, 28(5):823870, 2007. [39] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. [40] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft, 2013. [41] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge, 2019. [42] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. [43] Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. Infographicvqa, 2021. [44] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images, 2021. [45] N/A. Stl-10, nov 2024. [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [48] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:1393713949, 2021. [49] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [51] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019. [52] Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181211, 1999. [53] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [54] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [56] Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu, and Shuicheng Yan. Supervised hashing for image retrieval via image representation learning. In Proceedings of the AAAI conference on artificial intelligence, volume 28, 2014. [57] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 34853492, 2010. [58] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1080910818, 2022. [59] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [60] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940128966, 2024. [61] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: review. IEEE transactions on neural networks and learning systems, 30(11):32123232, 2019. [62] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: survey. Proceedings of the IEEE, 111(3):257276, 2023."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Model Architecture Our framework builds on pretrained VQGAN and two instances of the lightweight Pythia-70M language model. The VQGAN handles initial visual processing and image reconstruction, while two Pythia models are responsible for generating variable-length visual tokens and decoding them into fixed-length sequence. Although our design uses transformer-based Pythia models to support dynamic sequence generation, the overall architecture remains lightweight, with total parameter count roughly twice that of VQGAN alone. Details of the VQGAN and both Pythia-70M models we used are provided in Table 6."
        },
        {
            "title": "VQGAN",
            "content": "Pythia-70M Visual Tokenizer (Autoencoder) 163M 8192 256 Encoder: 4, Decoder: 4 1616 ImageNet Image Tokenization and Reconstruction Language Model (Decoder-only Transformer) 70M 512 6 Transformer layers 50,304 The Pile Dynamic Token Generation and Decoding Table 6: Model architecture details for VQGAN and Pythia-70M. A.2 Training Data We train our model on ImageNet-1K, curated variant of the standard ImageNet dataset that contains 1.2 million images across 1,000 object categories. All images are resized to 256256, and data augmentation is applied using mild random cropping and grayscale adjustment to improve generalization. For Query-conditioned DOVE (Q-DOVE), we further fine-tune the original DOVE model on the Visual Genome and Open Images datasets for an additional five epochs. The Visual Genome dataset consists of 108,077 images, from which 5.4 million region descriptions and 1.7 million visual questionanswer pairs are used as textual conditions. Additionally, we utilize 3.3 million relationship annotations from Open Images, where the bounding boxes of each object pair are spatially concatenated to define the conditioning region. Detailed statistics and usage of each dataset are summarized in Table 7. Dataset #Images #Textual Inputs #Relationship Annotations Use Case Epochs ImageNet-1K Visual Genome Open Images 1.2M 108K 9M 5.4M region desc. + 1.7M QA 3.3M relationships Pretraining (DOVE) Fine-tuning (Q-DOVE) Fine-tuning (Q-DOVE) 20 5 Table 7: Training datasets used for DOVE and Q-DOVE. Textual inputs include region descriptions and questionanswer pairs. A.3 Reconstruction Loss Function Design To optimize image reconstruction, we combine mean squared error (MSE) loss, perceptual loss, and adversarial (GAN) loss. We find that incorporating small weight for the GAN loss (e.g., 5 1010) enhances the realism and fine details of the reconstructed images. Figure 9 presents some qualitative comparisons of reconstructions across range of GAN loss weights, from 0 to 5 109. As shown, increasing the GAN loss weight enhances texture detail; for example, the fur of dog appears noticeably sharper with weight of 5 109 compared to reconstructions without GAN loss. However, assigning larger GAN weight also introduces hallucinated content, leading to shape distortions and reduced fidelity to the original image. In addition, we evaluate the average L1 reconstruction loss on the ImageNet-1K validation set for each setting. The results indicate that small GAN loss weight initially improves reconstruction accuracy. But when the weight increases further, the L1 loss also increases and eventually becomes higher than that of the model trained 14 without GAN loss. Based on this trade-off, we choose 5 1010 as the GAN loss weight for our final model. Figure 9: Effect of varying GAN loss weight on image reconstruction quality. small weight (e.g., 5 1010) improves perceptual detail without sacrificing fidelity, while larger weights introduce artifacts and increase L1 loss."
        },
        {
            "title": "B Multimodal Understanding",
            "content": "B.1 Instruction Tuning Setup We follow the evaluation setup of AIM V2 and fine-tune Vicuna-7B-v1.5 models with different vision encoders on the 665K mixed VQA dataset from LLaVA. This mixed dataset includes training data from COCO, GQA, OCR-VQA, TextVQA, and Visual Genome. Detailed training configurations are provided in Table 8. Config Optimizer Optimizer Momentum Decoder Peak Learning Rate Connector Peak Learning Rate Minimum Learning Rate Weight Decay Batch Size Gradient Clipping Warmup Iterations Training Iterations Learning Rate Schedule Transformations LLaVA SFT Mixture PyTorchs AdamW β1 = 0.9, β2 = 0.999 2e-5 2e-4 0 0.01 8 1.0 250 5000 cosine decay [PadToSquare, Resize] Table 8: Training configurations for fine-tuning VLM on the LLaVA SFT mixture. B.2 Evaluation Benchmarks We evaluate Vicuna models equipped with different vision encoders across eight diverse datasets. Table 9 summarizes the benchmarks used in our evaluation, including dataset split, prompt style, and evaluation metric. Benchmark Split Prompt Evaluation Metric VQAv2 [23] GQA [2] OK-VQA [41] TextVQA [51] DocVQA [44] InfoVQA [43] ChartQA [42] ScienceQA [39] Test (image split) Answer with the options letter from the given choices directly. Accuracy Accuracy Accuracy Accuracy Accuracy ANLS ANLS Relaxed Accuracy Answer the question using single word or phrase. Val Val Val Val Test Test Test Table 9: Evaluation benchmarks used in Visual Question Answering Evaluation. 15 B.3 Case Study We conduct case study to analyze the VLMs responses under different token counts. Figure 10 shows reconstructed images and the corresponding answers generated by the model. We find that as the number of tokens increases, both reconstructed image quality and answer accuracy improve. With fewer tokens, the images become blurry and the VLM is more likely to hallucinate; for example, when using only 16 tokens, the VLM misreads the word STOP on sign as SHOP. Figure 10: Model predictions under varying token counts. As the number of tokens increases, both image reconstruction quality and answer accuracy improve."
        },
        {
            "title": "C Linear Probing Datasets",
            "content": "We evaluate the quality of visual representations in vision encoders using standard linear probing setup. In this setup, the vision encoder is frozen, and single-layer linear classifier is trained on top of selected hidden layer to perform image classification. The classifier is trained and evaluated across suite of standard benchmarks, including CIFAR-10 [33], CIFAR-100 [33], DTD [14], FGVCAircraft [40], Food101 [9], STL-10 [15], and SUN397 [57]. Descriptions of each dataset are provided below. CIFAR-10 / CIFAR-100 CIFAR-10 and CIFAR-100 [33] are widely used benchmarks for object classification. Both datasets consist of 60,000 low-resolution (3232) color images, with 50,000 for training and 10,000 for testing. CIFAR-10 includes 10 coarse classes such as airplane, dog, and truck, while CIFAR-100 contains 100 fine-grained categories organized into 20 superclasses. DTD The Describable Textures Dataset (DTD) [14] contains 5,640 images organized into 47 texture categories annotated with human-interpretable attributes (e.g., striped, dotted, bumpy). The images are collected from the wild, with significant variation in lighting, scale, and viewpoint, serving as benchmark for texture recognition and attribute prediction. 16 FGVC-Aircraft FGVC-Aircraft [40] is fine-grained classification dataset containing 10,000 images of aircraft across 100 classes, such as Boeing 747 and Airbus A320. The dataset emphasizes subtle inter-class visual differences, with consistent poses but variations in background and lighting. Food101 Food101 [9] comprises 101,000 high-resolution images across 101 food categories, with 1,000 images per class. The dataset reflects real-world variability, including occlusions, diverse lighting conditions, and broad range of cuisines and presentation styles. STL-10 STL-10 [15] is designed for unsupervised and semi-supervised learning. It includes 13,000 labeled images spanning 10 object categories (e.g., bird, cat, ship) at resolution of 9696, along with an additional 100,000 unlabeled images for representation learning. SUN397 SUN397 [57] is large-scale scene classification dataset comprising over 100,000 images across 397 categories. It includes diverse range of indoor and outdoor environments such as kitchens, libraries, highways, and mountains. The dataset is intended to assess models ability to recognize complex and varied semantic scenes."
        },
        {
            "title": "D Experiments with Gaussian Latent Space",
            "content": "To ensure that the generated representations converge to known distribution, we adopt the reparameterization technique from variational autoencoders (VAEs). Specifically, we map the tokens generated by DOVE into Gaussian latent space. Our results show that after Gaussianization, the model maintains reconstruction quality comparable to the original version. FID scores are reported in Table 10, and qualitative examples are shown in Figure 11. We also observe that the token representations generated by DOVE are unevenly distributed. For example, most of the information is concentrated in the first 64 tokens, while the remaining tokens contribute only subtle variations. This uneven distribution poses challenges for effective quantization into discrete representation space such as codebook. We will further investigate improved quantization strategies for DOVE in future work. Approach ImageNet100 COCO Wikipedia (WIT) 32 64 96 128 160 224 256 TiTok-L-32 TiTok-B-64 TiTok-S-128 VQGAN ALIT DOVE DOVE (Gaussian) 11.60 - - - 22.31 18.91 19.87 - 8.22 - - 15.92 11.46 12.03 - - - - 13.08 10.84 10. - - 8.22 - 11.45 9.28 9.46 - - - - 10.01 8.61 8.95 - - - - 9.12 8.25 8.28 - - - - 8.37 7.96 8.01 - - - 7.04 8.06 7.73 7.80 32# / 64 14.18# 9.15 - - 22.01 15.50 16. 128 - - 9.15 - 13.98 9.83 10.03 256 - - - 7.77 9.51 7.54 7.58 32# / 64 53.57# 42.86 - - 61.32 14.83 15.34 256 - - 38.16 - 47.52 8.56 8.84 - - - 31.27 38.10 7.84 7.87 Table 10: FID comparison between DOVE and DOVE (Gaussian) on various datasets. Figure 11: Reconstruction results of DOVE and DOVE (Gaussian) under varying token budgets. Overall, DOVE (Gaussian) achieves similar visual quality to DOVE."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "University of California, San Diego",
        "University of Washington"
    ]
}