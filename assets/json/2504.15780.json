{
    "paper_title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
    "authors": [
        "Daocheng Fu",
        "Zijun Chen",
        "Renqiu Xia",
        "Qi Liu",
        "Yuan Feng",
        "Hongbin Zhou",
        "Renrui Zhang",
        "Shiyang Feng",
        "Peng Gao",
        "Junchi Yan",
        "Botian Shi",
        "Bo Zhang",
        "Yu Qiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 8 7 5 1 . 4 0 5 2 : r TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving Daocheng Fu1,2* Zijun Chen2,3* Renqiu Xia2,3* Qi Liu2,3 Yuan Feng2,3 Hongbin Zhou2 Renrui Zhang2 Shiyang Feng2 Peng Gao2 Junchi Yan3 Botian Shi2 Bo Zhang2 Yu Qiao2 1Fudan University, 2Shanghai Artificial Intelligence Laboratory, 3Shanghai Jiao Tong University {fudaocheng,zhangbo}@pjlab.org.cn, {chenzijun,xiarenqiu}@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose scalable data engine called TrustGeoGen for problem generation, with formal verification to provide principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrusttest testset. Experiments reveal the state-of-the-art models achieve only 49.17% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen. Geometric Problem Solving (GPS), as pivotal branch of mathematical reasoning or more broadly speaking, general reasoning, demands tripartite capabilities [40]: 1) extracting spatial relationships from visual diagrams, 2) performing mathematical reasoning in language modality, and 3) maintaining stepwise logical coherence where intermediate conclusions rigorously justify subsequent deductions. The development of trustworthy AGI systems fundamentally relies on reliably annotated multimodal data that not only bridges visual-textual modalities but also preserves closed-loop reasoning chains. This requirement stems from critical axiom: geometrically valid solution demands both correct answers and air-tight logical pathways, as human experts reject proofs with leap-of-faith* arguments even when final results are accurate. The recent advancement of Multimodal Large Language Models (MLLMs) [4, 8, 11, 12, 14, 20, 22, 24, 27, 28, 30, 37, 40, 45] has revealed promising capabilities in geometric reasoning, demonstrating competence in solving elementary geometric problems. Parallel developments in external formalized expert systems [7, 18, 21, 44] have extended problem solving capabilities to more complex geometric configurations, with highly specialized models [9, 35, 43] even approaching challenges of the International Mathematical Olympiad (IMO) level. However, these achievements are critically dependent on high-quality geometric data, currently in short supply. The existing datasets are mainly derived from manual annotations of secondary school textbook problems [6, 7, 15, 21, 44], and this process is limited in scale and diversity. Although recent integration of MLLMs (e.g. GPT series [2, 23, 24]) for synthetic questionanswer generation [14, 46] has partially ameliorated data scarcity concerns, significant challenges persist in ensuring *Equal contribution Corresponding author. *Here it means an unjustified or unsubstantiated jump in reasoning, where conclusion is reached with insufficient logical steps or evidence. Figure 1. TrustGeoGen overview: (1) Constructor builds geometric premises from base scene by incrementally adding clauses and enforces constraints via compiler, (2) Reasoner generates rigorously validated reasoning graph from premises by geometric rules, (3) Sampler selects conclusion nodes from the graph and traces reasoning paths to formalize problems and solutions, (4) Translator converts formal language to natural language for metadata. bootstrap mechanism further enhances data complexity and problem difficulty. stepwise solution validity, which may undermine the logical coherence of reasoning chains during model training. However, current geometric datasets exhibit four deficiencies: 1) Modality Fragmentation: existing datasets suffer from incomplete modality alignment, lacking comprehensive annotations that synchronize images, textual captions, questions, and chain-of-thought reasoning processes within unified instances. 2) Scalability Issues: Most approaches rely on repurposing human-authored problems from textbooks or examinations rather than gen3) Cost erating novel, curriculum-aligned challenges. Constraints: Manual annotation requires costly geometry expertise, while generating solutions through MLLMs 4) Trust consumes excessive computational resources. Deficits: Model-generated solutions lack reliable automated verification mechanisms, introducing risks of undetected logical errors or misaligned reasoning steps. To this end, we develop TrustGeoGen, formal language-verified geometric data generation engine that autonomously produces geometric problems, diagrams, and stepwise solutions through four integrated components: 1) Constructor to generate constraint-satisfying problem premises and diagrams, 2) Reasoner expanding geometrically valid reasoning graphs with rigorous node verification, 3) Sampler leveraging the algorithm GeoExplore to exSpecifically, automatically generated procedural labels frequently contain latent inconsistencies, such as misapplied geometric axioms or diagram-coordinate mismatches. tract high-quality reasoning paths, which are later converted into problems by pairing initial premises with derived conclusions, and 4) Translator rendering formal specifications into natural language. Furthermore, we improve the problem complexity through bootstrapping mechanism in TrustGeoGen, where formally verified geometric premises in prior stages are recursively fed back to the Constructor to iteratively append new premises. This closed-loop process enables systematic escalation of difficulties while maintaining logical consistency. Additionally, we refine the algorithm GeoExplore to simultaneously explore alternative solution paths and self-correcting reasoning trajectories, generating problems with multiple valid solutions and self-reflective traceback data. Our proposed TrustGeoGen ensures: 1) Modality Completeness with synchronized dual-form (formal & informal) captions, questions, solutions, and diagrams, 2) Scalability in generating geometry problems by bootstrapping mechanisms without reliance on existing data source and 3) Verifiability through formal validation of all solution steps in its integrated reasoner. Based on TrustGeoGen, we develop dataset GeoTrust containing 200K samples and compact testset GeoTrust-test with 240 samples. Experimental results demonstrate state-of-the-art reasoning models DeepSeek-R1 [10] and OpenAI-o1 [25] achieve only 45.00% and 49.17% accuracy respectively on GeoTrusttest. Furthermore, training with our strictly verified reasoning step annotation data proves to be more effective compared to using pseudo-labels with unverifiable reasoning processes. Additionally, our training data from GeoTrust enhances model performance on the out-of-distribution (OOD) testset GeoQA [6], highlighting its broader generalization capability. In nutshell, our contributions are as follows. We develop TrustGeoGen, formalized geometric data generation engine, producing scalable and multimodalintegrated data (geometric diagrams, formal & natural language captions, questions and solutions) with trustworthy reasoning guarantees via formal verification. By introducing bootstrap mechanism, we iteratively generate data with more complex premises and longer reasoning chains, enabling automated complexity scaling. Our GeoExplore series algorithms generates multisolution problems and self-reflective traceback data, enriching diversity while ensuring trustworthiness. We construct the dataset GeoTrust with 200K samples and GenTrust-test testset, dynamically generated without dependency on prior data sources, ensuring unbiased and extensible evaluating capabilities. Experiments demonstrate that existing MLLMs significantly underperform on high-difficulty geometric reasoning tasks, and our verified data exhibit effectiveness and generalization as training source. 2. Methodology The data engine TrustGeoGen serves as the core for constructing high-quality geometric reasoning datasets, enabling the generation of complex geometric scenes and reasoning paths to support robust and scalable inference. 2.1. Data Engine In the process of generating multi-modal geometric reasoning data, it employs rule-based data construction method augmented by geometric verifier to synthesize highquality and scalable datasets. This approach ensures that the data is accurate, controllable, and scalable. As shown in Fig. 1, our framework consists of four components: Constructor, Reasoner, Sampler, and Translator. Constructor starts the data construction process by building geometric scenes. It incrementally adds new conditions to basic geometric scene, thus constructing increasingly complex scenes. In TrustGeoGen, basic geometric scene consists of simple geometric shapes (e.g., triangles, circles, parallelograms) and their associated numerical information (e.g., side lengths, radii, angles). When base scene is created, it contains an initial set of geometric premises Pbase = {pg , pd represents geometric relationship conditions (e.g., AB//CD) and pd reprej }, where pg Here, pseudo-label means that the solution to given question is annotated using SOTA MLLMs (e.g. OpenAI-o1 [25]). sents numerical conditions (e.g., BAC = 30). Subsequently, the Constructor incrementally adds new premises to the base scene, increasing its complexity while expanding the premise set . Notably, the Constructor requires that each newly added premise must be connected to the preceding scene to ensure the overall geometric scene remains cohesive and connected. By controlling the number of new premises added, the Constructor generates geometric scenes of varying complexity levels. Each constructed scene is then validated by geometric compiler [31] to ensure the validity of its geometric relationships. Finally, the process yields geometric scene with specified level of complexity and an expanded premise set = {pg j+n}. Reasoner leverages predefined set of geometric theorems to infer new states from premises generated by the constructor, formulating reasoning graph: i+m, pd = (S, S0, R, (cid:44)) (1) is the set of states, where each state represents derived conclusion or fact in the reasoning process. S0 = is the set of initial states (the premises generated by the constructor). is the set of rules, where each rule defines logical inference relationship between states. (cid:44) = {(Sr, r, s) Sr S, R, S} is the (cid:44) indicates state transition relation. transition Sr state is derived from by applying rule r. Reasoning begins with the initial states S0 = (given premises from the constructor), forming the root nodes of the graph G. The graph expands by applying inference rules to existing states. For every state set Sr and rule R, if can be applied to Sr to derive new state (formally (cid:44) s), the reasoner performs two atomic updates: 1) Sr Add the new state Sr to and 2) Add the edges (Sr, s) to the transition relation (cid:44) with rule r. Finally, the complete graph is the smallest closure satisfying: (cid:110) = S0 Sr S, R, Sr (cid:44) s(cid:111) (2) Sampler operates on the reasoning graph by sampling target state sn and subsequently searching for its path: = {(Si1, rs, s) Si, Si rs(cid:44) s, = n, . . . , 1} (3) where each triplet (Si, rs, s) denotes specific transition during the reasoning process, derived by applying rule rs to state set Si1 to produce state s. Any state sn within the state set can be treated as problem to solve in order to derive reasoning path P. The sampler is initialized with state Sn to retrieve the corresponding transition rule rs and the upstream dependent states Si. This process is repeated iteratively until the state set Si consists entirely of initial premises S0. At that point, the full reasoning path has been successfully constructed. Notably, during the graph Algorithm 1 GeoExplore Algorithm 2 GeoExplore-M Input: Reasoning graph = (S, S0, R, (cid:44)), target state sn, threshold τl, threshold τr Output: Filtered reasoning path Input: Reasoning graph = (S, S0, R, (cid:44)), target state sn, threshold τl, threshold τr, number of searches Output: Set of filtered reasoning paths Pset 1: Initialize 2: sn 3: while / S0 do 4: Identify rule rs and parent state set Si1 rs(cid:44) such that Si1 Append transition (Si1, rs, s) to Si1 5: 6: 7: end while 8: Assess path length: 9: Compute premises ratio: RP SP /S0, where SP Return 10: if τl and RP τr then 11: 12: else 13: 14: end if Discard construction process in the Reasoner, once state is derived through rule rs, no additional state transition relations are appended to s. Consequently, the Sampler is always able to identify unique inference rule rs corresponding to any state s, thereby guaranteeing the absence of cycles in the reasoning path. To ensure the quality of the reasoning paths, the Sampler employs two metrics to filter out unsuitable paths. The first metric evaluates the length of the reasoning path against predefined threshold τl. If the length of exceeds τl, the path is considered difficult enough. Formally, this condition is expressed as τl, where denotes the number of transitions in the reasoning path P. The second metric assesses the ratio of the states in the reasoning path that are derived from the initial premises set S0. If this ratio exceeds predefined threshold τr, the path is deemed sufficient enough. This condition is formally defined as SP S0 τr, where SP represents the number of initial premises used in the reasoning path P, and S0 is the number of all initial premises. By applying these two metrics, the Sampler ensures that only high-quality reasoning paths are retained, thereby maintaining the integrity and usefulness of the generated data. The detailed procedure of the aforementioned reasoning path exploration and filtering algorithm, GeoExplore, is presented in Algorithm 1. Translator is responsible for converting the reasoning path into human-readable natural language explanation. To accomplish this, the Translator leverages large language model (LLM) to smooth the reasoning path. For each state and rule in the reasoning path P, the Translator provides the LLM with detailed natural language inter1: Initialize Pset 2: Initialize used options 3: while True do 4: 5: 6: 7: Initialize sn while / S0 do Identify set of rules Rs and corresponding parent state sets i1 such that rs rs(cid:44) Rs, Si1 i1 : Si1 Select rs Rs and Si1 options in different searches) used options used options {(rs, Si1)} if (Si1, rs, s) / then (Si1, rs, s) i1 (choose different end if Si end while Assess path length: Compute premises ratio: RP SP /S0, where SP if τl and RP τr then Pset Pset {P} 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if if used options contains all possible options then Break 19: 20: 21: 22: 23: end while 24: return Pset end if pretation of the geometric states and inference rules, ensuring that the LLM fully understands the semantic and logical meaning of the components in P. Additionally, the Translator utilizes few-shot learning strategy to enhance translation accuracy and consistency. Specifically, it includes pre-curated set of high-quality human-generated translations as few-shot examples, which serve to guide the LLM in generating precise and context-appropriate explanations. By iteratively processing each triplet (Si1, rs, s) P, the Translator generates coherent chain-of-thought explanation that mirrors the logical flow of the reasoning graph G. This ensures that the resulting translation not only reflects the exact logical derivations but also facilitates human understanding by articulating complex geometric relationships in an accessible and comprehensive manner. 2.2. Bootstrap Augmentation As previously described, TrustGeoGen begins by constructing complex geometric figure from completely random initial scenario and then derives reasoning paths for this instance. While this approach is scalable, its efficiency in generating high-quality data is relatively low, as it may randomly produce geometric scenarios that fail to yield meaningful or valuable results. To address this, TrustGeoGen employs an iterative bootstrap strategy to guide the generation process, thereby improving the efficiency of producing high-quality data. In each iteration, TrustGeoGen utilizes the metrics from the Sampler to evaluate the quality of sampled data on given graph G. If the average quality of the sampled data on particular graph is sufficiently high, TrustGeoGen selects this data as the basis for the subsequent iterations initial scenario. Specifically, the process can be represented as j+n}, where serves as the starting point. The Constructor then incrementally adds new premises to this foundation, increasing the complexity of the scenario. This leads to the generation of = {pg j+n+y}. Following this, the Reasoner, Sampler, and Translator collaboratively work to complete graph construction, problem-solving, and translation, ultimately producing new batch of high-quality data. base = = {pg i+m+x, pd i+m, pd 2.3. Multi-solution Data In the field of GPS, exploring multiple solutions to given problem is crucial method for understanding geometric relationships in depth. By investigating different solution logics, model can examine geometric scenarios from various perspectives, achieving comprehensive understanding of the relationships among geometric elements. TrustGeoGen facilitates the construction of multi-solution datasets for specific geometric scenarios through the graphbuilding procedure of the modify Reasoner and the solving procedure of the Sampler. During the graph construction phase, in order to obtain multiple solutions, for each state s, when the Reasoner rer s, each peatedly identifies transitions in the form Sr identified transition is retained. This retention of multiple transitions provides potential paths for exploring diverse solutions. In the solving process, the Sampler employs an adaptive search algorithm to generate reasoning paths. For any given problem sn, the Sampler performs multiple path searches. If state si has multiple outgoing transitions, the Sampler selects different transition at each iteration for the next reasoning step. Consequently, the reasoning paths obtained during these iterations will differ. It is worth noting that to ensure the generated reasoning paths are acyclic, the Sampler enforces constraint during path construction. Specifically, when attempting to add triplet (Si, rs, s) to the reasoning path P, the Sampler checks whether this triplet already exists in P. If it does, the state is not revisited, thereby preventing cyclical paths in the reasoning process. For details of the multi-solution path searching algorithm, GeoExplore-M, refer to Algorithm 2. Algorithm 3 GeoExplore-T Input: Directed graph = (S, S0, R, (cid:44)), target state st S, threshold τp Output: Trace-back reasoning data 1: Initialize 2: Compute all reasoning paths set = {P 1, 2, . . . } to st using Algorithm 2 3: Randomly sample state se such that se / UpstreamDependencies(P t) for any t set 4: Search for reasoning path leading to se 5: if t 6: 7: end if 8: return set, Overlap(P e, t) τp then Add t to 2.4. Self-reflective Traceback Data Recent studies on the reasoning capabilities of large models suggest that these models acquire substantial knowledge during the pre-training phase. By providing training data with various cognitive templates during the post-training phase, the models can better utilize their inherent knowledge, thereby enhancing reasoning ability and improving response quality[41, 42]. Among these cognitive templates, self-reflection trace-back reasoning serve as highly effective approaches. However, self-reflection and retrospective reasoning often occur during the authors cognitive process and are rarely preserved in the final outputs, making the collection of relevant data particularly challenging. These types of data are even more scarce in the domain of multi-modal geometric problem-solving. TrustGeoGen defines trace-back reasoning data as subgraph of = (S, S0, R, (cid:44)), characterized as the union of sub-graphs with identical upstream dependencies but differing final states. Specifically, for given piece of traceback reasoning data, the process involves first deriving an incorrect final state, then retrospectively navigating correct reasoning path (shared by both the incorrect and correct paths), and ultimately arriving at the correct final state. To achieve this, TrustGeoGen selects target state st and employs Algorithm 2 to enumerate all possible reasoning paths t set. Next, state se is randomly sampled from the state set S, ensuring that se does not belong to the upstream dependencies of any t. Subsequently, TrustGeoGen searches for reasoning path leading to se. If overlaps with any t set and the overlapping portions exceed the threshold τp, TrustGeoGen outputs t as trace-back reasoning data. Notably, Algorithm 2 is utilized to identify all possible reasoning paths for st to ensure that se does not appear in any potential upstream dependencies, thereby maintaining the integrity of the reasoning process leading to st.The details of the trace-back data acquisition algorithm, GeoExplore-T, are presented in Algorithm 3. process, the data distribution in the region where reasoning length is smaller than 40 decreased, while it increased in the region where reasoning length is larger than 40. This method can be repeatedly applied to efficiently construct reasoning problems with significant depth and complexity. 3.3. Construction and Analysis of GeoTrust-Test The synthetic data generated by TrustGeoGen can also be utilized to evaluate models capabilities in the domain of deep geometric reasoning. From an initial dataset of 200K samples, we manually curated set of 240 high-quality problems as the test set, graded by different levels of difficulty. As shown in Fig. 4, these problems are divided into four tiers, with each tier containing 60 problems. The reasoning lengths for ier1 range from 5 to 10 steps, ier2 spans from 10 to 20 steps, ier3 ranges from 20 to 50 steps, and ier4 exceeds 50 steps. It is worth noting that, to introduce distractors into the questions, the premises provided may not necessarily all be utilized in the reasoning process, resulting in potentially lower premise ratio. 4. Experiment To validate the quality and effectiveness of the geometric Problem Solving data generated by TrustGeoGen, we conducted experiments to explore: Whether existing MLLMs demonstrate competent performance on complex geometric problems? Whether geometric reasoning data with rigorous formal verification improve MLLMs capabilities and provide specific advantages? Whether GeoTrust, constructed without prior data sources, can generalize to OOD geometric testset? 4.1. Dataset, Metric, and Implementation Detail Dataset. To evaluate the capability of MLLMs in solving complex geometry problems, we construct GeoTrust-test by partitioning the original GeoTrust dataset. As introduced in Sec. 3.3, the testset comprehensively covers four difficulty levels ranging from ier1 to ier4. To validate the effectiveness of our data construction, we additionally sample 3K geometry problems as GeoTrust-train, with each ) containing 1K instances. difficulty tier (T ier1 to ier3 GeoQA [6] is employed to evaluate the OOD generalization performance, comprising 4,998 geometric problems and solutions in natural language. Metric. To eliminate potential evaluation bias introduced by the multiple-choice format, all input questions in the experiments are presented without answer options. Both the models final output answers and GT labels are extracted Considering the maximum output token limit of the trained opensource models, we excluded data from ier4. Figure 2. Simple examples of Multi-solution and Self-reflective traceback data. 3. Data Analysis The modality-completed trustworthy geometric reasoning data holds great potential for improving the geometric problem-solving capabilities of MLLMs. Moreover, it could generalize these abilities to other tasks requiring deep reasoning. To validate it, we utilized TrustGeoGen to produce the dataset GeoTrust of 200K samples, from which 8K were sampled as the training set. Additionally, we employed different thresholds, τl and τr, in conjunction with manual screening to curate testset, GeoTrust-test, with 240 samples of varying levels of difficulty. 3.1. Data Distribution TrustGeoGen was executed on 60-core Intel Xeon (32) CPU @ 2.900GHz for two days, generating an initial dataset of 200K geometric reasoning instances in formal language. During the data generation process, we set the two filtering thresholds in the Sampler to τl = 5 and τr = 0.5, respectively. The distributions of the reasoning length and premises ratio for the resulting 200K dataset are illustrated in Fig. 3a and Fig. 3b, respectively. The majority of the data is concentrated in the region where the reasoning length is less than 60, and it decreases sharply beyond 80. However, small portion of the data exhibits reasoning lengths exceeding 150, indicating considerably high level of reasoning complexity. For more detailed view of the distribution, please refer to the zoomed-in section of Fig. 3a. 3.2. Deep Reasoning Augmentation As mentioned earlier, the randomly constructed scenarios by TrustGeoGen exhibit rapid decline in reasoning length within the ultra-long range, posing challenges for generating deep-reasoning problems. To address this, we selected 226 samples with the longest reasoning lengths from an initial dataset of 200K and applied bootstrap augmentation to increase the proportion of deep-reasoning problems. Finally we obtained 376 geometric scenes after applying the bootstrap method. As shown in Fig. 3c, after the bootstrap (a) (b) (c) Figure 3. Data distribution and augmentation. (a) Distribution of reasoning lengths, with most samples below 60 steps and sharp decline beyond 80; the zoomed-in view highlights the ultra-long reasoning range. (b) Distribution of premise ratios, reflecting variability in logical dependencies. (c) Comparison of reasoning lengths before and after bootstrap augmentation, showing increased deep-reasoning samples (reasoning length 40) and reduced shallow ones. Figure 4. Visualization examples of different difficulty levels in GeoTrust-test, where reasoning length indicates step length of reasoning path and premise ratio refer to the ratio of premises unutilized during formal reasoning and premises provided in the question. and converted into floating-point values for numerical comparison, with relative error tolerance of 1% permitted. Implementation Detail. In the experimental training protocol, all models are trained through supervised finetuning (SFT) with full-parameter updates for one epoch to prevent overfitting. All training and evaluation processes are conducted on 8 NVIDIA A100 (80G) GPUs. 4.2. Performance of Existing MLLMs As shown in Tab. 1, the experimental results from the GeoTrust-test testset reveal critical insights into the capabilities of MLLMs in tackling complex geometric problems. Existing open-source models, such as Qwen2.5-VL7B-Instruct [5], demonstrate significant limitations, achieving only 12.92% overall accuracy (31/240), which also underscores the inherent challenges of the dataset and confirms its independence from prior biases in existing opensource testsets. Besides, closed-source commercial models like OpenAI-o1 [25] exhibit stronger reasoning capabilities, solving 109 problems (45.42% accuracy), though their performance sharply declines on higher-difficulty ier4 questions (18.33% accuracy), highlighting persistent gaps in handling advanced geometric reasoning. Notably, the open-source reasoning model DeepSeek-R1 [10] matches OpenAI-o1s overall accuracy, bridging the performance gap between open and closed-source systems and emphasizing the potential of scalable architectures. consistent trend across all models is the progressive degradation of accuracy as problem difficulty escalates, validating the rigorous tiered difficulty framework of GeoTrust-test. These findings collectively emphasize the need for enhanced reasoning architectures and training paradigms to address the steep complexity gradient in geometric problem solving. For DeepSeek-R1, we only provide natural language questions. Table 1. Performance of state-of-the-art multi-modal language models on GeoTrust-test, which is divided into four difficulty levels. Model #Params Total (out of 240) num acc ier1 (out of 60) num acc ier2 (out of 60) num acc ier3 (out of 60) num acc ier4 (out of 60) num acc LLaVA-1.5 [19] Qwen2-VL-Instruct [36] Qwen2.5-VL-Instruct [5] GPT-4o [24] Claude-3-7-Sonnet [3] DeepSeek-R1 [10] OpenAI-o1 [25] 7B 7B 7B - - 671B - 7 9 24 55 83 108 118 2.91% 4.17% 10.00% 22.92% 34.58% 45.00% 49.17% 3 5 12 25 35 40 42 2 5.00% 2 8.33% 6 20.00% 15 41.67% 17 58.33% 66.67% 24 70.00% 28 0 3.33% 0 3.33% 3 10.00% 9 25.00% 24 28.33% 40.00% 25 46.67% 2 0.00% 2 0.00% 3 5.00% 6 15.00% 7 40.00% 41.67% 19 43.33% 22 3.33% 3.33% 5.00% 10.00% 11.67% 31.67% 36.67% Table 2. Comparison of performance on GeoTrust-test using trustworthy GeoTrust-train data and pseudo-labels annotated by OpenAI-o1 [25]. All trained on Qwen2-VL-7B-Instruct [5]. Table 3. OOD Generalization on GeoQA [6] when GeoTrust-train is leveraged for training, where indicates the improvement compared to the pre-trained baseline. Training Data Total ier1 ier2 ier3 - pseudo-labels 100% GeoTrust-train 50% GeoTrust-train 30% GeoTrust-train 7 21 (+14) 38 (+31) 27 (+20) 12 (+5) 5 13 (+8) 27 (+22) 22 (+17) 10 (+5) 2 3 (+1) 6 (+4) 4 (+2) 2 (-) 0 5 (+5) 5 (+5) 1 (+1) 0 (-) 4.3. Advantages Provided by Trustworthy Data To validate the effectiveness of the trustworthy data generated by TrustGeoGen, we compared our data with the ones generated by OpenAI-o1 [25]. We collect pesudo-labels generated by the OpenAI-o1 [25] via prompting the model with the problems in GeoTrust-train. We train Qwen2VL-7B-Instruct [36] on both GeoTrust-train (with 100%, 50% and 30% data) and pesudo-labels, and report the number of problems solved from ier1 to ier3. As shown in Tab. 2, the model trained on 100% of GeoTrust-train outperforms the model trained on pesudo-labels, which demonstrates that trustworthy data hold substantial advantages over SOTA-generated pseudo-labels under equivalent data amount. Besides, as the amount of GeoTrust-train data increases, the models overall performance continues to improve, which further validates the effectiveness of our dataset. However, while the improvements on ier1 are notable, the gains on ier2 and ier3 remain significantly less pronounced, suggesting that supervised finetuning (SFT) does not produce substantial effects on more challenging geometric reasoning tasks. 4.4. OOD Generalization The experiments are conducted on three pre-trained vision-language models: LLaVA-1.5-7B [19], LLaVA-1.513B [19] and Qwen2-VL-7B-Instruct [36]. For each model, we evaluate on GeoQA [6] with two training settings: (1) fine-tuning solely on the GeoQA dataset, and (2) finetuning with combination of GeoQA and GeoTrust-train datasets to explore generalization. The results in Tab. Model Training Data acc LLaVA-1.5-7B [19] LLaVA-1.5-13B [19] Qwen2-VL-7B-Instruct [36] - GeoQA GeoQA+GeoTrust-train - 16.31% 25.73% +9.42% 28.12% +11.81% - GeoQA GeoQA+GeoTrust-train - 18.83% 29.84% +11.01% 33.16% +14.33% - GeoQA GeoQA+GeoTrust-train - 37.67% 41.11% +3.44% 43.24% +5.57% incorporating GeoTrust-train data not demonstrate that only achieves substantial improvements over the original pre-trained models (e.g., +11.81% for LLaVA-1.5-7B and +5.57% for Qwen2-VL-7B ), but also surpasses models fine-tuned solely on the in-distribution GeoQA training set. For example, LLaVA-1.5-7B improves from 25.73% accuracy (GeoQA-only fine-tuning) to 28.12% when augmented with GeoTrust-train, marking +2.39% absolute gain over the in-distribution baseline, highlighting the generalization capability of our data. 5. Conclusion and Further Discussion Conclusion. We present TrustGeoGen, multimodalintegrated and scalable geometric data engine that automatically generates fully verified geometric reasoning data. The resulting GeoTrust dataset demonstrates validated effectiveness and generalization capabilities, with its trustworthy reasoning data showing significant advantages over conventional pseudo-labels. Notably, the GeoTrust-test exposes critical limitations of existing MLLMs in handling complex geometric reasoning tasks. GenGen represents crucial first step towards trustworthy geometric problem solving, establishing new paradigm for generating rigorous data. Trustworthy Geometric Problem Solving. Models should produce verifiable reasoning steps, not merely correct answers. Achieving this demands both reliable data and robust evaluation mechanisms are required. Leveraging the generated data from TrustGeoGen, further study can focus on autoformalization approachs that translate natural language reasoning into formalized steps, enabling automated verification of each deductions logical validity. Moreover, TrustGeoGen can dynamically generate evaluation data to prevent prior data leakage during evaluation, ensuring higher reliability in unseen geometric scenarios. Formal Enhanced Reasoning. TrustGeoGens formalized reasoning environment enables the generation of trustworthy geometric data by constructing rigorous reasoning graphs that ensure logical correctness at each step. These graphs provide structured foundation for exploring diverse mathematical reasoning strategies through tailored sampling methods: the current work implements multisolution and self-reflection traceback data, while future extensions could incorporate the idea of reverse thinking and categorical discussion, etc. These ideas could be further enhanced by integrating alternative training approaches (e.g. RL) to generalize to other reasoning scenarios."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 12 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com,, 2024. 8, 12 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 12 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2024. 7, [6] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. 1, 3, 6, 8, 12 [7] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3313 3323, 2022. 1, 12 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 1 [9] Yuri Chervonyi, Trieu H. Trinh, Miroslav Olˇsak, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc V. Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2. ArXiv, 2502.03544, 2025. 1, 12 [10] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, JunMei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jiong Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Zehui Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. 2, 7, 8, 12 [11] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form textimage composition and comprehension in vision-language large model. ArXiv, abs/2401.16420, 2024. [12] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. ArXiv, abs/2404.06512, 2024. 1 [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 12 [14] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. In The Thirteenth International Conference on Learning Representations, 2025. 1, 12 [15] Yihan Hao, Mingliang Zhang, Fei Yin, and Lin-Lin Huang. Pgdp5k: diagram parsing dataset for plane geometry probIn 2022 26th International Conference on Pattern lems. Recognition (ICPR), pages 17631769. IEEE, 2022. 1 [16] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 12 [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 12 [18] Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: foundational and multimodal mathematical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 71267133, 2023. 1 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 8, [20] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 1 [21] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 1 [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 1, 12 [23] OpenAI. Gpt-4v. https://openai.com/index/ gpt-4v-system-card/, 2023. 1 [24] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 1, 8, 12 [25] OpenAI. Gpt-o1. https://openai.com/index/ openai-o1-system-card/, 2024. 2, 3, 7, 8, 12 [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 12 [27] Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, Botian Shi, Tao Chen, Bo Zhang, and Xiangyu Yue. Chimera: Improving generalist model with domainspecific experts. ArXiv, abs/2412.05983, 2024. 1 [28] Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui Improving generalHe, Aojun Zhou, et al. Chimera: arXiv preprint ist model with domain-specific experts. arXiv:2412.05983, 2024. 1 [29] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [30] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. 1 [31] Vladmir Sicca, Tianxiang Xia, Mathıs Federico, Philip John Gorinski, Simon Frieder, and Shangling Jui. Newclid: user-friendly replacement for alphageometry. arXiv preprint arXiv:2411.11938, 2024. 3, 12 [32] InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. 12 [33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [35] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. 1, 12 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv, abs/2409.12191, 2024. 8 [37] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. 1 [38] Renqiu Xia, Bo Zhang, Haoyang Peng, Ning Liao, Peng Ye, Botian Shi, Junchi Yan, and Yu Qiao. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023. 12 [39] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie Wu, Hancheng Ye, et al. Docgenome: An open largescale scientific document benchmark for training and testarXiv preprint ing multi-modal large language models. arXiv:2406.11633, 2024. 12 [40] Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, and Bo Zhang. Geox: Geometric problem solving through unified formalized vision-language pretraining. Learning Representations, 2025. 1, 12 In The Thirteenth International Conference on [41] Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical llm reasoning via scaling thought templates. arXiv preprint arXiv:2502.06772, 2025. 5 [42] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. 5 [43] Chi Zhang, Jiajun Song, Siyu Li, Yitao Liang, Yuxi Ma, Wei Wang, Yixin Zhu, and Song-Chun Zhu. Proposing and solving olympiad geometry with guided tree search. ArXiv, abs/2412.10673, 2024. 1 [44] Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. multimodal neural geometric solver with textual clauses parsed from diagram. arXiv preprint arXiv:2302.11097, 2023. 1, [45] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. ArXiv, abs/2407.03320, 2024. 1 [46] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 1,"
        },
        {
            "title": "Appendix",
            "content": "A. Related Work Advances in Multimodal Large Language Model. Recent advances in Large Language Models (LLMs) have demonstrated remarkable progress in linguistic intelligence, achieving human-level capabilities in various applications [26, 3234]. Building upon this foundation, the research community has focused on extending these textbased architectures to process visual information, leading to the emergence of sophisticated Multimodal Large Language Models (MLLMs) [1, 4, 29]. These integrated systems typically employ modality alignment mechanisms, such as Qformer [17] and linear layers [19], to establish connections between visual representations and text embeddings. Although current MLLMs show strong performance in standard vision-language benchmarks [13, 16, 22, 38, 39], their effectiveness decreases significantly when processing mathematical visualizations that require reasoning. Recent specialized approaches address this limitation through targeted training strategies. For example, geometric reasoning capabilities have been enhanced through domain-specific datasets containing annotated diagrams [14, 46]. Geometric Problem Solving. As challenging task, geometric problem solving requires understanding diagrams, interpreting symbols, and performing complex reasoning. While MLLMs have shown remarkable proficiency in human-level reasoning and generation capabilities, they struggle with automatic geometric problem solving due to their pre-training on natural images and texts, as well as the lack of automated verification in the problem-solving process. To address this limitation, researchers have proposed various approaches to improve the understanding of geometric images and corpora, including unimodal pretraining, vision-language alignment, and visual instruction tuning [6, 7, 1416, 40, 44, 46]. Another major approach involves the use of formalized solvers to tackle geometric problems. While these solvers [9, 31, 35] are capable of addressing challenges at the level of the International Mathematical Olympiad (IMO), they require precise modeling of geometric problems to fit the solvers language. This necessity introduces significant obstacles in terms of practical Model Name Model / API Version GPT-4o [24] Claude-3-7-Sonnet [3] DeepSeek-R1 [10] OpenAI-o1 [25] gpt-4o-2024-11-20 claude-3-7-sonnet-20250219 deepseek-r1 o1-2024-12-17 Table 4. Model / API versions used for evaluation across different MLLMs. application and generalization, limiting the usability in realworld scenarios. B. Versions of Closed-source Model During the evaluation on GeoTrust-test, we utilize some closed-source commercial models, with the specific version detailed in Table 4. C. More Visualization Example In Fig. 5, we provide one more visualization example in GeoTrust with 94 reasoning steps and 0.95 premises ratio. Figure 5. One visualization example in GeoTrust."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}