{
    "paper_title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
    "authors": [
        "Xingyu Shen",
        "Yingfa Chen",
        "Zhen Leng Thai",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 3 6 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "STATEX: ENHANCING RNN RECALL VIA POSTTRAINING STATE EXPANSION Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, & Maosong Sun Department of Science and Technology, Tsinghua University, Beijing, China xingyu-c21@mails.tsinghua.edu.cn, chenyingfa1999@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, recurrent neural networks (RNNs) such as gated linear attention (GLA) (Yang et al., 2024) and Mamba2 (Dao & Gu, 2024) have shown promising capabilities in language modeling. These architectures have constant per-token complexity, while the more popular Transformer architecture (Vaswani et al., 2023) has per-token complexity that grows linearly with the context length. Thus, RNNs are much more efficient than Transformers in processing long contexts. However, RNNs still underperform Transformers in certain aspects, with one of the most critical being the long-context recall capability (Jelassi et al., 2024b). Different from Transformers, which store the representations of every token in the context, RNNs compress all contextual information into constant-size state2. As result, the recall ability of RNNs heavily depends on the size and capacity of this state (Jelassi et al., 2024a; Arora et al., 2024a; Yang et al., 2025; Chen et al., 2025). Despite the positive gains of increasing the state size, considering the increased training costs and the limited benefits in short-context scenarios and various downstream tasks, most RNNs are still trained with relatively small state size compared to the rest of the model. For instance, in Mamba2-2.8B and GLA-1.3B, their recurrent states are smaller than 2% of their model sizes. In this paper, we propose StateX, which expands the state size while keeping the training costs low and introducing little to no additional parameters. Specifically, we expand the state size of pretrained RNNs through post-training on much less data than pre-training. Moreover, since larger recurrent states are more important for long-context models, we perform state expansion prior to long-context post-training (LPT). The training pipeline is illustrated in Figure 1. The state expansion process is an architectural change and depends on the pre-trained model architecture. Therefore, we design two state expansion methods, targeting two popular RNN classes: 1The code is released at https://www.github.com/THUNLP/StateX 2Also called recurrent state in various contexts. We use these two terms interchangeably in this paper. *Equal contributions."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Difference between the traditional pipeline and StateX for training long-context models. We introduce state expansion step (architectural modification) before the long-context posttraining (LPT) stage to enhance RNN recall abilities without requiring expensive re-training. linear attention (Katharopoulos et al., 2020; Yang et al., 2024) and state space models (Dao & Gu, 2024). Additionally, we explore various parameter initialization techniques and select key layers for expansion rather than all layers, to balance model performance and adaptation efficiency. Compared to other state expansion methods that require training from scratch (e.g., MoM (Du et al., 2025), LaCT (Zhang et al., 2025)), our method is simpler and can be seamlessly applied to existing effective RNN implementations and training pipelines. We evaluate our method on public 1.3B parameter checkpoints of GLA3 and Mamba24, by conducting post-training on 10B tokens. Our empirical results demonstrate that, compared to the traditional two-stage method, StateX significantly improves performance on recall-intensive tasks, in-context learning tasks, and needle-in-a-haystack (NIAH) (Hsieh et al., 2024) tasks while maintaining performance on common-sense reasoning tasks. While using the same amount of training data as ordinary LPT, StateX yields consistently better results: the relative accuracy gain in recall-intensive tasks is 3.36% for GLA and 1.1% for Mamba2, and the relative performance gain in in-context learning is 7.2% for GLA and 1.0% for Mamba2. Also, the average NIAH accuracy up to 64K context length improves from 26.0% to 42.2% for GLA, and from 33.2% to 39.2% for Mamba2. Overall, our contributions include: To the best of our knowledge, StateX represents the first work that focuses on expanding the state size of RNNs through post-training. For two popular RNN variants, GLA and Mamba2, we design simple and effective state expansion techniques and training recipes for efficient post-training. We evaluate our method on public 1.3B checkpoints. Our results show consistent improvements in recall-intensive tasks, in-context learning, and long-context retrieval, without sacrificing performance on common-sense reasoning benchmarks."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "In this section, we provide brief description of RNNs and related work on expanding their state sizes. For more details about RNNs, please refer to the surveys (Wang et al., 2025; Lv et al., 2025). Modern RNNs Recently, some RNN variants have shown promising results in sequence modeling. Some representative examples include state space models (SSMs) (Dao & Gu, 2024; Gu & Dao, 2024), the RWKV series (Peng et al., 2025; 2024; 2023), linear attention models (Katharopoulos et al., 2020; Sun et al., 2023; Yang et al., 2024), and DeltaNet (Yang et al., 2025). Some results have shown that these RNNs can outperform Transformers up to several billion parameters on certain language tasks, such as common-sense reasoning (Waleffe et al., 2024; Team, 2024), and some hybrid models have scaled up to over 100B parameters and trillions of training tokens (MiniMax 3https://huggingface.co/fla-hub/gla-1.3B-100B 4https://huggingface.co/AntonV/mamba2-1.3b-hf"
        },
        {
            "title": "Preprint",
            "content": "Method Vanilla RNNs (small states) Training large states from scratch Novel architectures with large states StateX (ours) Performance Efficient Training Easy Adoption ? ? Table 1: Comparison between our work and existing approaches for increasing RNN state sizes. Vanilla RNNs underperform due to their smaller state sizes. ? means that these works are rather new and therefore yet to be extensively tested at scale. et al., 2025). RNNs are attractive alternatives to Transformers because their per-token complexity is constant, while Transformers per-token complexity scales linearly with the context length. However, since Transformers cache all previous token representations, they outperform RNNs in recalling contextual information. This is one of the reasons why RNNs have seen limited adoption. Increasing RNN State Size Many previous works have investigated the influence of state size on the capabilities of RNNs. One important improvement of modern RNNs over previous works such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014) is the adoption of larger matrix-valued recurrent states over smaller vector-valued states (Sun et al., 2023; Qin et al., 2024; Katharopoulos et al., 2020; Hua et al., 2022). Some later efforts focus on improving the forget mechanisms to remove unneeded information in the recurrent states, saving capacity to store more contextual information (Gu & Dao, 2024; Schlag et al., 2021). Arora et al. (2024a) provides comprehensive comparison of the recall-throughput tradeoff of various recent RNN architectures. Although these methods show promising results, their state size is still rather small, and they lag behind Transformers in recall-intensive tasks. Recent State Expansion Works More recently, Du et al. (2025) proposes MoM, new architecture that maintains large state size but with lower computational overhead, by updating only parts of the recurrent state at each time step. LaCT (Zhang et al., 2025) is concurrent work to ours that proposes novel recurrent architecture based on the test-time training (TTT) framework (Sun et al., 2025). LaCT utilizes much larger state than other RNNs (e.g., GLA and Mamba2) and has demonstrated strong recall and long-context capabilities. Another relevant concurrent work is by Liu et al. (2025). They utilize low-rank projections to increase the state size of RNNs with small parameter overhead, resulting in considerably better recall performance. However, these architectures have not been thoroughly evaluated across different tasks and may be hard to adopt into existing codebases. In brief, the state size is critical bottleneck of RNNs. Increasing the state size provides consistent performance gains for many RNN variants. However, previous works on expanding RNN states are trained from scratch, which is highly expensive and requires significant changes to the model architecture and implementation. This paper, to the best of our knowledge, is the first effort to expand states through post-training. Compared to existing architectures with larger states, our method is simpler and can be seamlessly integrated into popular RNN variants such as linear attention methods and SSMs. Table 1 shows the comparison between our work and existing works with larger states."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "In this section, we first provide formulation of RNNs as well as two variantsGLA and SSM (Sections 3.1, 3.2, and 3.3). Then, we discuss how the recurrent state size influences the models recall capabilities and cost-efficiency (Section 3.4). 3.1 RECURRENT NEURAL NETWORKS In RNNs, all contextual information is stored in constant-size recurrent state St, where denotes the time step. At each time step, new information is inserted into the previous state St1 with an update rule fupdate, and then retrieves information from St with query rule fquery, which is given as St = fupdate(St1, xt), yt = fquery(St, xt), 3 (1)"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Illustration of StateX (our method) for expanding the state size of linear attention and state space models with little to no parameter increase. The red parts indicate the additional state parameters unlocked by StateX. where xt, yt Rd are the input and output representations at the time step t. In this paper, we define state size as the parameter count of St. 3.2 GATED LINEAR ATTENTION The GLA model consists of stack of interleaved layers of GLA blocks and feed-forward network (FFN) blocks. Since we only modify the GLA block, we omit the formulation for FFNs. Each GLA block consists of heads computed in parallel, and the layer output is the sum of the head outputs. Each GLA head can be formulated as: t,h = xtW,h, {q, k, v}, Ft,h = diag(αt,h) Rdkdk , St,h = Ft,hSt1,h + yt,h = qt,hSt,h Rdv , t,hvt,h Rdkdv , (2) where {1, , H} is the head index, dk, dv are the key and value dimensions. xt, yt Rd denote the input and output representations at the time step t, respectively, and qt,h, kt,h, αt,h Rdk , vt,h Rdv are projection functions of xt. The state size in each GLA layer is Hdkdv. 3.3 STATE SPACE MODELS We focus on Mamba2, which is state-of-the-art SSM. Mamba2 layer can be formulated as:5 vt,h = fv(xt, θv,h) Rdv , kt = fk(xt, θk) Rdk , qt = fq(xt, θq) Rdk , t,h = f(xt, θ,h) R, αt,h = exp(tAh) R, St,h = St1,hαt,h + t,hk yt,h = qtSt,h + Dhvt,h Rdv , vt,h Rdkdv , (3) where fv, fk, fq, are differentiable projection functions parameterized with θv, θk, θq, θ,h, respectively, Ah, Dh are learnable parameters. dk and dv are hyperparameters and are called the state dimension and head dimension in SSM literature. The state size of Mamba2 is also Hdkdv, although these hyperparameter values may differ from GLA. 5We use attention notations (qt, kt, vt) instead of SSM notations (xt, Bt, Ct) from the Mamba2 paper for simplicity and to highlight the analogy between the two RNN variants."
        },
        {
            "title": "Preprint",
            "content": "Relationship with GLA It has been identified that Mamba2 can be viewed as variant of GLA (Yang et al., 2024) where heads share the same query/key (QK). In this paper, we view these two variants as different because this QK sharing mechanism influences our state expansion. 3."
        },
        {
            "title": "INFLUENCE OF STATE SIZE",
            "content": "Recall Ability Since all contextual information is stored in St, the ability of RNNs to recall contextual information depends on the capacity of St, which in turn depends on the size of St. Extensive empirical evidence indicates strong positive correlation between the size of the recurrent states and their performance on recall-intensive tasks (Arora et al., 2024a; Hua et al., 2022; Zhang et al., 2025; Jelassi et al., 2024b). These findings highlight the critical role of state size in determining RNN recall abilities, underscoring the importance of state expansion for improving recall capabilities. Efficiency The computational complexity of the token mixing component (i.e., update rule and query rule) scales linearly with the state size. Therefore, blindly increasing the state size can lead to high training and inference costs. StateX alleviates these problems during both training and inference by expanding the states via post-training (so the model is trained with smaller states most of the time) and expanding only subset of layers."
        },
        {
            "title": "4 METHOD",
            "content": "Our method, StateX, involves architectural modifications that expand the RNN state sizes prior to long-context post-training to boost their recall abilities. Meanwhile, we aim to minimize the additional parameters introduced by this modification and keep the final architecture similar to the original architecture to make it easier for the modified model to adapt. An overview of the architectural modifications is illustrated in Figure 2. In this section, we describe the state expansion recipe for two popular classes of RNNsGLA (Yang et al., 2024) and SSM (Dao & Gu, 2024) (Sections 4.1 and 4.2). Then, we describe parameter initialization methods after the expansion (Section 4.3) and which layers to expand (Section 4.4). 4.1 STATEX FOR GLA Since GLA employs multi-head mechanism with different query, key, and value (QKV) vectors for each head, we can increase the state size by simply merging multiple heads into one larger head. This is because the state size of heads is dk dv, and merging them into one head results in state size of 1 Hdk Hdv, which is times larger. Meanwhile, no additional parameters are introduced since the total number of channels in the QKV vectors remains the same. The effect of this change is illustrated in the left side of Figure 2. Merging GLA heads activates non-diagonal regions of the state matrix, thereby achieving larger states than the multi-head counterparts. In implementation, the only difference between GLA with expanded states and the vanilla formulation (described in Section 3.2) is the number of heads and head dimension. Thus, this modification can be seamlessly applied to existing GLA implementations. We always merge all heads into one large head. This is motivated by the finding that single-head GLA generally outperforms multi-head GLA (reported in Section 5.7). 4.2 STATEX FOR SSM The head merging method is not applicable to SSMs because there is only one key vector in each layer. For this RNN variant, we increase the key dimension by expanding the key and query projection layers. Specifically, we increase the hyperparameter dk (the original Mamba2 paper refers to this as the state dimension) and the parameters θk, θq that depend on it. Since these two sets of parameters are much smaller than the other components, the increase in total parameters is less than 1% when we increase dk by 4. This modification is illustrated by Figure 2 (right)."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Accuracy on recall-intentive tasks with sequences truncated to maximum of 2K tokens, as well as the model size and state size of each model. The best scores are bolded. Model Params Total State SWDE SQuAD TQA NQ Drop Avg. Linear Attention GLA Original Model 1.365B LPT StateX (ours) 1.365B 1.365B 12.48M 12.48M 18.72M 44.64 47.16 50.32 54.96 56.84 59.15 54. 19.10 33.64 56.04 55.04 21.95 21.82 36.56 39.58 State Space Model Mamba Original Model 1.343B LPT StateX (ours) 1.343B 1.350B 24.96M 24.96M 37.44M 57.43 54.19 56.17 59.58 57.81 57.91 63.27 63.51 63. 5.16 36.87 36.43 36.22 35.46 36.37 41.42 43.71 45. 44.33 49.56 50.11 Table 3: In-context learning performance of GLA and Mamba2 variants, evaluated on 12 downstream classification tasks. Higher is better. 8-shot 24-shot Mamba2 16-shot 16-shot 24-shot 8-shot GLA Original LPT StateX (ours) 48. 47.33 48.15 47.91 49.70 52.42 48.50 48.45 51.95 Original LPT StateX (ours) 51.40 47.72 47.68 54.34 49.79 52.34 51. 52.49 53.03 4.3 PARAMETER INITIALIZATION After the modification, we can inherit the parameters from the pre-trained model and initialize only the added parameters (for SSMs). However, perhaps surprisingly, we find that inheriting pre-trained parameters can be detrimental to downstream performance. Thus, we present better parameter initialization strategy. We assume that world knowledge is usually stored in FFN blocks and the embedding table, and these parameters take longer to learn than the token-mixing parameters (GLA and SSM blocks). Thus, we reinitialize parameters that are responsible for token-mixing while other components inherit from the pre-trained checkpoint. An ablation study on initialization strategies is provided in Section 5.4. GLA Initialization GLA models consist of interleaving layers of GLA blocks and FFN blocks. After state expansion, we reinitialize all parameters associated with the GLA blocks, while FFN blocks and the embedding table inherit the pre-trained parameters. SSM Initialization Mamba2 merges FFN blocks and the SSM mechanism into one unified layer. Motivated by the SSM literature, we only reinitialize the parameters of the SSM mechanism, which are Ah, θk, θq, θ,h, while other modules inherit the pre-trained parameters. Further implementation details can be found in Appendix A.4. 4.4 HOW MANY LAYERS TO EXPAND? Modifying all layers may result in too disruptive change, making it harder for the modified model to recover from this change through post-training. Existing works have shown that not all layers are responsible for recalling information (Bick et al., 2025). Thus, we hypothesize that only subset of layers can benefit from larger state. Concretely, we adopt uniform expansion strategy by expanding one layer every L/m layers (where is the total number of layers), starting from the first layer, so that exactly layers are expanded. For both GLA and Mamba2, we use = 4 by default. In Section 5.5, we empirically ablate the influence of the number of expanded layers."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Performance on language modeling and zero-shot common-sense reasoning. PIQA Hella. Wino. ARC-e ARC-c acc acc acc SIQA Avg. acc Model acc acc Original Model LPT StateX (ours) Original Model LPT StateX (ours) 69.70 69.64 69.75 73. 73.07 73.67 Linear Attention GLA 38.97 38.21 37.16 53.35 54.78 54. 55.13 54.59 53.91 State Space Model Mamba2 45.89 45.48 45.09 60. 59.67 59.98 64.31 64.31 64.02 23.38 22.70 22.53 30. 29.10 29.61 39.92 39.61 39.97 43.14 41.10 41.61 46. 46.58 46.37 52.93 52.12 52."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We first describe the details of the experiments (Section 5.1). Then, we present the main results of our method (Section 5.2) as well as improvement on long-context retrieval tasks (Section 5.3). Finally, we provide ablation studies involving the choices of parameter initialization (Section 5.4), the number of expanded layers (Section 5.5), multi-head mechanism in GLA (Section 5.7). We also report the training loss in Section 5.6. 5.1 EXPERIMENTAL DETAILS Models We apply StateX to the official 1.3B checkpoints of GLA and Mamba2. In StateX for Mamba2, we increase the dk hyperparameter from 128 to 512. For GLA, the pre-trained 1.3B checkpoint has four heads, so StateX versions of the expanded layers have 4 larger states. Data All models are trained on SlimPajama (Soboleva et al., 2023), widely-used, high-quality, and deduplicated corpus with 627B tokens extracted from the Internet. We concatenate documents with special token as the delimiter. Then, these concatenations are split into chunks of the specified training context length. Training Configuration The training follows common practices in context length extension by post-training as closely as possible. Concretely, we use the cosine learning rate scheduler, with maximum learning rate of 3e-4, and warmup phase of 5% of the total training steps. To better evaluate the ability to recall information from long contexts, we use 64K context length. The training spans total of 10B tokens, with batch size of 0.5M tokens. Evaluation We evaluate the models context utilization abilities with recall-intensive tasks and incontext learning (ICL). The recall-intensive tasks involves 5 popular document question-answering tasks. To assess ICL, we adopt suite of 7 classification and 5 multiple-choice tasks selected from Min et al. (2022), study that systematically evaluates ICL capabilities. Models are evaluated with accuracy across varying number of demonstrations, and ICL performance is summarized by the mean accuracy averaged over all tasks. Furthermore, we measure the general language processing abilities with 6 popular multiple-choice common-sense reasoning tasks. More details are given in Appendix B.1. Baselines We mainly compare StateX against vanilla RNNs and the ordinary LPT versions. The LPT models undergo the same post-training process, but without any architectural modifications, so their state sizes remain unchanged. 5.2 MAIN RESULTS Recall Abilities Table 2 presents scores on recall-intensive tasks for the original model, the model using the standard long-context post-training (LPT), and the model enhanced with StateX. The"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Performance on retrieving specific information (i.e., needle) from synthetically generated long documents up to 64K tokens. Model 4K 8K 16K 32K 64K GLA Passkey Retrieval Original LPT StateX (ours) 0.25 0.74 0.93 0.01 0.41 0.77 0.00 0.13 0.34 Mamba2 NIAH-Single-2 Original LPT StateX (ours) 0.05 0.83 0.94 0.00 0.43 0.61 0.00 0.30 0.32 0.00 0.01 0.06 0.00 0.09 0.09 0.00 0.01 0. 0.00 0.01 0.00 Figure 4: Model performance of reinitialization and parameter inheritance. columns Params and Total State report the number of model parameters and state parameters for each model, respectively. StateX increases the total state sizes by roughly 50%. The main takeaway is that StateX models achieve the highest average performance, underscoring the advantage of larger states. In-Context Learning Table 3 shows the in-context learning performance of various RNN variants, and StateX variants exhibits significantly greater in-context learning abilities. Common-Sense Reasoning Table 4 shows that StateX models performance on common-sense reasoning is comparable to the vanilla model, implying that pre-training knowledge remains largely unaffected by the architectural change. 5. IMPROVEMENT ON LONG-CONTEXT RETRIEVAL The recall-intentive tasks we used in Section 5.2 contain mostly sequences with fewer than 4K tokens. To evaluate the models abilities to retrieve information from longer contexts, we use the popular NIAH task (Hsieh et al., 2024). Due to differences in the recall abilities between the GLA and Mamba2, we evaluate them using NIAH tasks of varying difficulty to avoid score saturation and preserve discriminative resolution. For the GLA model, we employed the simpler passkey retrieval task from Bench (Zhang et al., 2024), which involves retrieving single 5-digit passkey from long documents consisting of repeated text. For Mamba2, we use the more challenging NIAH-Single2 task from RULER (Hsieh et al., 2024), where 7-digit passkey is embedded in semantically meaningful, non-repetitive distractor content. More details can be found in Appendix B.3. Results Table 3 reports the models performances in NIAH. It shows that, by unlocking larger state size, StateX significantly improves the models recall performance in long contexts. 5.4 COMPARISON BETWEEN REINITIALIZATION AND PARAMETER INHERITANCE Although it may seem natural to inherit pre-trained parameters, our experiments show that reinitializing the modified parameters yields better performance. For Mamba2, whose state expansion process introduces new parameters, we initialize the new parameters with zeros. As illustrated in Figure 4, the model with reinitialized parameters (Reinit) consistently outperforms the one that inherits parameters (Inherit) on both common-sense reasoning and recall tasks. We hypothesize that the performance gap arises because the inherited parameters have already converged, making it difficult to effectively utilize the newly introduced channels (indicated in red in Figure 2) via post-training."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Model performance under varying numbers of expanded layers. Mamba2 has twice as many layers as GLA because it does not have FFN layers. Figure 6: Post-training loss (on SlimPajama) of vanilla models and expanded models. GLA has lower loss as it is pre-trained on SlimPajama while Mamba2 is pre-trained on Pile. 5.5 BEST PROPORTION OF EXPANDED LAYERS As mentioned in Section 4.4, it is important to balance the number of expanded layers. To investigate this trade-off, we conducted an ablation study by varying the number of expanded layers. The results, shown in Figure 5, indicate that both the GLA and Mamba2 models achieve optimal average performance when four layers are expanded (out of 24 layers and 48 layers, respectively). When too many layers are modified, the reinitialized parameters fail to converge effectively under limited post-training, leading to sharp drop in overall performance. 5.6 TRAINING LOSS We also tracked the training loss curves of models trained with standard LPT and with StateX. Figure 6 shows the loss curves for both GLA and Mamba2. The former has generally lower loss because it was pre-trained on SlimPajama, while Mamba2 was not. Notably, the StateX models have higher initial training loss due to the architectural change, but quickly close the gap. Interestingly, although their final training loss is slightly higher than the LPT counterparts, they achieve better performance on downstream tasks. 5.7 THE OPTIMALITY OF SINGLE-HEAD GLA As mentioned in Section 4.1, the multi-head mechanism in GLA significantly reduces the size of the recurrent state, which in turn leads to degradation in model performance. This section presents an ablation study on the number of heads for GLA models trained from scratch. Table 5: Common-sense reasoning (CSR), recall, and training loss of GLA-340M models with different numbers of heads. Single-head GLA outperforms other configurations due to larger states. Head number CSR Recall Tr. Loss We conducted experiments on GLA models with 340M parameters, trained on 20B tokens from the SlimPajama dataset (Soboleva et al., 2023). More experimental details are described in Section B.4. Table 5 reports the performance of these models on range of common tasks. As shown, the single-head model achieves higher average scores on the benchmark tasks and converges to lower final training loss. Given the same number of parameters and other configurations, using fewer heads allows for larger state size, which in turn leads to improved performance in common-sense reasoning, recall, and training loss. 42.715 42.029 42.401 41.527 25.992 24.012 21.780 15.395 2.722 2.762 2.798 2.883 1 4 8"
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "We have proposed StateX, novel method for enhancing the recall abilities of two popular RNN variants by expanding the state sizes of pre-trained RNNs through post-training. Compared to training RNNs with larger state sizes from scratch, our method is much faster to train and can be seamlessly applied to existing pre-trained models of said RNN variants. StateX is valuable for closing the gap in the recall abilities of RNNs and Transformers, especially in long-context scenarios. This work represents an important step toward RNNs as an efficient alternative to attention-based architectures."
        },
        {
            "title": "REFERENCES",
            "content": "Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re. Simple linear attention language models balance the recall-throughput tradeoff. In Proceedings of the 41st International Conference on Machine Learning, pp. 1763 1840, 2024a. Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, and Christopher Re. Just read twice: closing the recall gap for recurrent language models, 2024b. URL https://arxiv.org/abs/2407.05483. Aviv Bick, Eric Xing, and Albert Gu. Understanding the skill gap in recurrent language models: The role of the gather-and-aggregate mechanism, 2025. URL https://arxiv.org/abs/ 2504.18574. Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Stuffed mamba: Oversized states lead to the inability to forget, 2025. URL https://arxiv.org/ abs/2410.07145. Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches, 2014. URL https://arxiv.or g/abs/1409.1259. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through In International Conference on Machine Learning, pp. 10041 structured state space duality. 10071. PMLR, 2024. Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with mixture-of-memories, 2025. URL https://arxiv.org/abs/2502.13685. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 2024. URL https://zenodo.org/records/12608602. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 17351780, 1997. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024. URL https://arxiv.org/abs/2404.06654. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time, 2022. URL https://arxiv.org/abs/2202.10447. Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. In International Conference on Machine Learning, pp. 2150221521. PMLR, 2024a."
        },
        {
            "title": "Preprint",
            "content": "Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024b. URL https://arxiv.org/ abs/2402.01032. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. URL https://arxiv.or g/abs/2006.16236. Kai Liu, Jianfei Gao, and Kai Chen. Scaling up the state size of RNN LLMs for long-context scenarios. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1151611529, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org/202 5.acl-long.564/. Xingtai Lv, Youbang Sun, Kaiyan Zhang, Shang Qu, Xuekai Zhu, Yuchen Fan, Yi Wu, Ermo Hua, Xinwei Long, Ning Ding, and Bowen Zhou. Technologies on effectiveness and efficiency: survey of state spaces models, 2025. URL https://arxiv.org/abs/2503.11224. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1104811064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/20 22.emnlp-main.759. URL https://aclanthology.org/2022.emnlp-main.759/. MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, and Zijia Wu. Minimax-01: Scaling foundation models with lightning attention, 2025. URL https://arxiv.org/abs/2501.08313. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023. URL https://arxiv.org/abs/2305.13048. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi Kiran GV, Jan Kocon, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Jiaju Lin, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Cahya Wirawan, Stanisław Wozniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence, 2024. URL https://arxiv.org/abs/2404.05892. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. Rwkv-7 goose with expressive dynamic state evolution, 2025. URL https://arxiv.org/abs/2503.1 4456."
        },
        {
            "title": "Preprint",
            "content": "Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion, 2024. URL https://arxiv.org/abs/24 04.07904. Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021. URL https://arxiv.org/abs/2102.11174. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://cere bras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-v ersion-of-redpajama, 2023. URL https://huggingface.co/datasets/cere bras/SlimPajama-627B. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states, 2025. URL https://arxiv.org/ abs/2407.04620. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621. Falcon-LLM Team. The falcon 3 family of open models, December 2024. URL https://hugg ingface.co/blog/falcon3. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://ar xiv.org/abs/1706.03762. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An empirical study of mamba-based language models, 2024. URL https://arxiv.org/abs/2406.07887. Ke Alexander Wang, Jiaxin Shi, and Emily B. Fox. Test-time regression: unifying framework for designing sequence models with associative memory, 2025. URL https://arxiv.org/ab s/2501.12352. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In International Conference on Machine Learning, pp. 5650156523, 2024. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule, 2025. URL https://arxiv.org/abs/2412.06464. Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, and Hao Tan. Test-time training done right, 2025. URL https://arxi v.org/abs/2505.23884. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. bench: Extending long context evaluation beyond 100k tokens, 2024. URL https://arxiv.org/abs/2402.13718."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Overview of GLA and Mamba2, two popular RNNs with matrix-valued recurrent states. H, P, N, dk, dv are hyperparameters of the architectures. is the expansion ratio of StateX for SSMs, which is set to 4, as mentioned in Section 4.2 Model GLA Mamba2 Update rule Query rule State size St1,hdiag(αt,h) + kT St1,hαt,h + t,hkT t,hvt,h vt,h qt,hSt,h qtSt,h + Dhvt,h Hdkdv Hdkdv StateX state size 2dkdv HdvdkE FORMULATION OF GATED LINEAR ATTENTION AND MAMBA For completeness, we provide the complete formulation of GLA and Mamba2 in this section. These models are trained on the next-token prediction task, which means that their input is sequence of token IDs and their output is sequence of probability distributions over the vocabulary {1, , }, where is the vocabulary size. At the beginning, each token ID is converted to d-dimensional token embedding by looking up an embedding table (also called the input embeddings) before passing to the backbone network. Let denote the sequence length. This creates sequence of embeddings X(0) RT d. On the output side, the output embeddings at each position {1, , } are converted to probability distribution over the vocabulary via linear layer called the language modeling head. In the following discussion, we denote the input and output sequences of representations for the l-th layer as: x(l) 1 ... x(l) where is the sequence length, and x(l) , y(l) R1d are the input and output representations at time step t. Since the input of each layer is the output of the previous layer, we have X(l) = Y(l1). , Y(l) = y(l) 1 ... y(l) X(l) = (4) A.1 GATED LINEAR ATTENTION The entire model of GLA consists of interleaving GLA blocks and FFN blocks. Y(l) = GLA(l) (cid:16) Y(l) = FFN(l) (cid:16) X(l1)(cid:17) Y(l1)(cid:17) + Y(l1) + X(l1) (5) Each GLA block consists of multiple heads that are computed in parallel, and the blocks output is the sum of the head outputs. This can be formulated as (omitting the layer index for simplicity): yt = (cid:88) h= GLAh(xt) Each head in GLA can be formulated as: t,h = xtW, {q, k, v, α}, St,h = diag(αt,h)St1,h + t,hvt,h, ot,h = LN(qt,hSt,h), rt = SILU(xtWr + br), GLAh(xt) = (rt ot,h)Wo. A.2 MAMBA Mamba2 does not have FFNs and consists only of stack of Mamba2 blocks: Y(l) = Mamba2(l) (cid:16) X(l)(cid:17) + X(l) 13 (6) (7) (8)"
        },
        {
            "title": "Preprint",
            "content": "Mamba2 also employs multi-head mechanism where the layer output is the sum of the head outputs (omitting the layer index for simplicity): Mamba2(xt) = (cid:88) h=1 Mamba2h(xt) (9) where is the number of heads, and is the head index. Each Mamba2 head can be formulated as: vt,h = fv(xt, θv,h) Rdv kt = fk(xt, θk) Rdk qt = fq(xt, θq) Rdk t,h = SILU(xtW,h + b,h) αt,h = exp(tAh) St,h = St1,hαt,h + t,hk ot,h = qtSt,h + Dhvt,h Rdv zt,h = SILU(xtWz,h) Rdv yt,h = Norm(ot,h zt,h)Wo,h Rd vt,h Rdkdv (10) A.3 UPDATE RULE AND QUERY RULE Central to recurrent architectures are the update rule and query rule (described in Section 3.1), which dictate how the architecture models inter-token dependencies. Table 6 shows the update rule and query rule of GLA and Mamba2. A.4 DETAILS OF PARAMETER REINITIALIZATION In the case of GLA, we reinitialize all parameters within the GLA block, including its normalization layer. For Mamba, we reinitialize all parameters of Ah, θk, θq. And θ,h is reinitialized specifically by resetting its internal dt bias component."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "B.1 EVALUATION We configure the evaluation tasks using the lm-evaluation-harness framework Gao et al. (2024). set of widely adopted benchmark tasks is selected to assess the models capabilities in commonsense reasoning and information recall. For the common-sense and recall tasks, we adopt accuracy (not normalized accuracy) and contains as the respective evaluation metrics. Accuracy directly reflects the correctness of the common-sense task results, while contains measures the proportion of recall task outputs that include the passkey. Notably, for tasks related to recall ability, we adopt the Just Read Twice prompt from Arora et al. (2024b), which is also used in Yang et al. (2024) and Yang et al. (2025), given that all models under evaluation are based on recurrent architectures. B.2 IN-CONTEXT LEARNING EVALUATION For the in-context learning (ICL) evaluation, we follow the setup introduced by Min et al. (2022), which systematically benchmarks ICL capabilities across classification and multiple-choice tasks. Our evaluation adopts the same protocol, but we evaluate also evaluate with different number of in-context demonstrations for comprehensiveness. The tasks that were used for evaluation are: commonsense qa ai2 arc superglue-copa"
        },
        {
            "title": "Preprint",
            "content": "superglue-cb glue-mrpc glue-sst2 glue-qqp glue-cola superglue-rte superglue-wic codah dream B.3 NEEDLE-IN-A-HAYSTACK TASKS As mentioned in the previous section, we design two passkey retrieval tasks with varying levels of difficulty. The specific noise configurations and prompt templates used in each task are detailed in Table 7. We use 5-digit passkeys in Passkey Retrieval and 7-digit passkeys in NIAH-Single-2. For each unique test length, the task will be tested on 256 randomly generated examples to ensure the consistency of the results. Table 7: The prompt templates of the NIAH tasks used to evaluate the models in retrieving information from long contexts. Passkey Retrieval (Bench) Task Template: The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. ...... The pass key is {number}. Remember it. {number} is the pass key. ...... The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. Task Answer Prefix: What is the pass key? The pass key is NIAH-Single-2 (RULER) Task Template: Some special magic numbers are hidden within the following text. Make sure to memorize it. will quiz you about the numbers afterwards. Paul Graham Essays. ...... One of the special magic numbers for {word} is: {number}. ...... What is the special magic number for {word} mentioned in the provided text? Task Answer Prefix: The special magic number for {word} mentioned in the provided text is B.4 MORE DETAILS: ABLATION STUDY ON THE NUMBER OF GLA HEADS The training procedure for these models follows common language model pre-training practices as closely as possible. The model is trained on 20B tokens from SlimPajama, with 0.5M tokens per batch, and sequence length of 4k. We employ cosine learning rate scheduler with an initial learning rate of 3e-4 and no specified minimum learning rate. All models consist of 340 million parameters and comprise 24 layers, each with an identical hidden state dimension. The only architectural difference lies in the number of attention heads: the single-head model uses one head with"
        },
        {
            "title": "Preprint",
            "content": "a dimensionality of 512, while the four-head model uses four heads, each with dimensionality of 128, and so on, following the same principle."
        },
        {
            "title": "C THE USE OF LARGE LANGUAGE MODELS",
            "content": "Large language models (LLMs) were used to quality-check the final draft, but we never explicitly instruct LLMs to write any parts of this paper."
        }
    ],
    "affiliations": [
        "Department of Science and Technology, Tsinghua University, Beijing, China"
    ]
}