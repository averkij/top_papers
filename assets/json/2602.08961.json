{
    "paper_title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "authors": [
        "Ruijie Zhu",
        "Jiahao Lu",
        "Wenbo Hu",
        "Xiaoguang Han",
        "Jianfei Cai",
        "Ying Shan",
        "Chuanxia Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page"
        },
        {
            "title": "Start",
            "content": "MotionCrafter: Dense Geometry and Motion Reconstruction with 4D VAE Ruijie Zhu1,2 Jiahao Lu3 Wenbo Hu2, Xiaoguang Han4 Jianfei Cai5 Ying Shan2 Chuanxia Zheng1 1NTU 2ARC Lab, Tencent PCG 3HKUST 4CUHK(SZ) 5Monash University 6 2 0 2 9 ] . [ 1 1 6 9 8 0 . 2 0 6 2 : r Figure 1. MotionCrafter is video diffusion-based framework for jointly dense geometry and motion reconstruction. Given monocular video as input, MotionCrafter simultaneously predicts dense point map and scene flow for each frame within shared world coordinate system, which outperforms optimization-based alternatives, yet without requiring any post-optimization."
        },
        {
            "title": "Abstract",
            "content": "MotionCrafter_Page/ We introduce MotionCrafter, video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from monocular video. The core of our method is novel joint representation of dense 3D point maps and 3D scene flows in shared coordinate system, and novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latentsdespite their fundamentally different distributionswe show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/ Corresponding author. 1. Introduction We consider the problem of simultaneously reconstructing 4D scene geometry and estimating dense point motion from monocular RGB video of dynamic scene in feedforward manner. This formulation mirrors how the physical world operates: an object is structured by its geometry in 3D space, as well as its motion across time. Achieving this goal is highly challenging, as monocular 4D reconstruction is inherently ill-posed, and dense temporal correspondences remain difficult, especially under occlusions and significant motion. However, successful solution to this problem would have wide spectrum of applications, from video understanding to robotics [70, 95] and world models [22, 23]. Traditional methods tackle this problem by finding the pixel correspondences over time, and then iteratively optimizing 3D mesh to fit the RGB(D) observations [32, 63, 131]. However, they often produce noisy results limited by the sensor, and need per-scene optimization, which is less generalizable. In the deep learning era, this problem is typically divided into two sub-tasks: dynamic geometry reconstruction [34, 93, 122] and correspondence estimation [37, 84], although they are inherently related, both relying on pixel correspondence in multi-view geometry [24]. Recent feed-forward methods such as St4RTrack [14], Dynamic Point Maps [80] and Stereo4D [35] have emerged as promising alternatives to address this problem, by extending the static 3D reconstruction networks [44, 90, 96] to dynamic scenes via target-timepoint map prediction. Even so, these methods process only pairwise frames at once and rely on post-optimization to align the results, reducing their ability to capture long-range motion coherence. In this paper, we introduce MotionCrafter, video diffusion-based framework that simultaneously reconstructs 4D geometry and estimates dense motion for long monocular video sequence, in feed-forward manner, without any post-optimization. We achieve this by proposing worldcentric 4D representation that denotes the dynamic scene using sequence of point maps [34, 90, 96] and the corresponding scene flow [26], both are defined in the world coordinate system. This representation is intuitive and effective: by eliminating the camera-induced motion components, static background points ideally exhibit zero flow in the system, making it easier to learn the motion patterns of dynamic objects. By comparison, prior works [14, 35, 80] only predict the target time point maps, paired with the reference frame, and do not explicitly model dense motion throughout the whole video. We hence argue that to understand the dynamic 3D scene fully, it is crucial to jointly model both the dense geometry and motion in shared coordinate system throughout the entire video sequence. Another challenge of this task is the lack of largescale in-the-wild datasets with dense geometry and motion. Following recent trends in leveraging pre-trained generative models for 3D [34, 39, 59], we do not train our model from scratch but start from pre-trained video generator [1]. This strategy significantly alleviates the data scarcity issue, as the generator is trained on large-scale visual data. Moreover, the video generator inherently models spatiotemporal consistency across multiple frames, making it well-suited for capturing long-term motion correspondence. While Geo4D [34] has explored leveraging video generators [1, 110] for 4D reconstruction, it only output independent point maps for each frame, without modeling of dense motion. In this work, we take further step towards jointly modeling dense geometry and motion. We do so by encoding the unified 4D representation that combines both point maps and scene flows into compact latent space, Without the need to build cost volumes [85] or establish dense correspondence [80] in pixel space, this integrated representation efficiently transfers the strong priors of the video generator to the task of dense 4D geometry and motion reconstruction. Moreover, we show that it is unnecessary to strictly align the 4D data value range with the original range in the VAE of the Diffusion model. While it is widely believed that maintaining such alignment is crucial for leveraging the pre-trained priors, even if for 3D geometry [34, 39, 59, 124] whose distribution differs significantly from natural imagesour findings suggest otherwise. In our case, we adopt the canonical normalization for point maps, i.e., centering the 3D coordinates and scaling them based on the scenes mean scale. Despite the misalignment with the original RGB distribution in the VAE, we demonstrate that our MotionCrafter can still achieve strong generalization and accurate 4D reconstruction and motion estimation. This finding challenges conventional beliefs and opens new possibilities for geometric diffusion models. To summarize, our key contributions are: (1) We introduce MotionCrafter, Diffusion-based framework that can simultaneously reconstruct 4D scene geometry and dense motion from monocular videos. (2) We propose novel 4D latent representation that unifies the modeling of geometry and motion, making our model simple but effective and easy to extend. (3) We also show that strong generalization can be achieved without strictly aligning our 4D representation to the latent space of video diffusion, challenging the conventional wisdom in diffusion-based 3D learning. 2. Related Work 4D Scene Reconstruction. Early 4D reconstruction works mainly focus on optimization-based approaches [8, 13, 15, 48, 49, 63, 65, 69, 86, 92, 99, 101, 114, 115], which iteratively fits 4D representation to monocular or multiview videos. With the development of neural radiance fields (NeRFs) [62], many time-dependent NeRFs [8, 13, 15, 48, 49, 65, 69] fit deformable 3D representations to dynamic scenes. However, these approaches suffer from the expensive volumetric rendering, making them less practical for real-world applications. 3D Gaussian Splatting (3DGS) [40] avoids expensive sampling using rasterizationbased rendering pipeline. Several works [57, 92, 99, 101, 114, 115, 130] extend it to dynamic scene reconstruction, which significantly reduces the rendering time. Besides, some examples [51, 126] achieve accurate and robust 4D reconstruction by leveraging depth priors [27, 39, 67, 112, 113]. However, they still require per-scene optimization. Several recent works [9, 14, 34, 35, 52, 80, 83, 93, 111, 122] have explored feed-forward 4D scene reconstruction from monocular videos. Among them, MonST3R [122] adapts the notable static 3D reconstructor DUSt3R [96] to dynamic scenes. The follow-ups [14, 35, 58, 80, 83, 93] took similar path, explicitly predicting the point correspondences. However, due to DUSt3Rs limitations, they process pairs of frames at time. To handle long monoc2 Figure 2. Overview of MotionCrafter. We first train novel 4D VAE (bottom-right), consisting of Geometry VAE and Motion VAE. These two components jointly encode the point map and scene flow into unified 4D latent representation. Within the Diffusion Unet, we leverage the pretrained VAE from SVD (Stable Video Diffusion) to encode video latents as conditional inputs, which are then channel-wise concatenated with our 4D latent to guide the denoising process. We only add noise to the 4D latents during model training for the Diffusion version. Note that we do not enforce the 4D latent distribution to strictly align with the original SVD VAE latent distribution. And we find that this relaxed training strategy consistently improves the generalization performance of both the VAE and the Diffusion Unet. ular videos, π3 [100] builds permutation-equivariant architecture on top of VGGT [90] for static and dynamic 3D reconstruction. Geo4D [34] leverages video generators [106] to directly infer 4D point maps from monocular videos. 4DGT [111] and BTimer [52] utilize transformerbased architectures to predict dynamic 3D Gaussian representations [40]. However, they do not explicitly model dense point correspondences over time. Scene Flow Estimation. Early works define point correspondences as optical flow estimation in pixel space [4, 5, 26, 31, 84]. One popular pipeline is to directly estimate dense pixel-wise correspondences in coarse-to-fine manner [12, 31, 81, 84]. However, such coarse-to-fine strategy may fail in the presence of large motions or occlusions [72]. More recently, GMFlow [108, 109] proposes to reformulate optical flow estimation as global matching problem rather than local regression. The followups [30, 76] also use transformer-based neural networks to model the global correlations. However, these methods still deal with 2D point correspondences in image space. In contrast, we address the 3D scene flow estimation in world space. Some researchers also explore to estimate 3D scene flow directly from image pairs, examples including RAFT-3D [85], SpatialTracker [104], SceneTracker [88] and TAPVid-3D [42]. More closely related to our work, several recent works [14, 35, 80] explore to reconstruct dynamic 3D geometry, along with 3D scene flow estimation in world space. However, they process only two images at time and require post-processing to refine the results. Geometric Diffusion Model. Like our approach, many recent works have leveraged pre-trained off-the-shelf diffusion models [1, 2, 17, 18, 21, 25, 29, 41, 53, 73, 78, 89, 98, 106, 120] to tackle 3D tasks [10, 16, 39, 45, 55, 75, 77, 79, 82, 87, 103, 107, 116, 118, 127], thanks to rich priors learned from large-scale image or video datasets. When it comes to 4D reconstructions, straightforward solution is to generate multi-view videos, and then fit 4D representation via per-scene optimization [102, 105, 119]. Inspired by score distillation sampling (SDS) [68], another line of works [11, 33, 50, 71, 121] directly distill 4D priors from pre-trained video generators. However, these approaches still rely on iterative per-scene optimization, which is expensive when dealing with in-the-wild videos. The most related work is Geo4D [34] that fine-tunes pre-trained video diffusion model to directly infer dynamic 3D point maps, depths, and camera poses from monocular videos. Our method differs from Geo4D in two aspects: 1) we simultaneously reconstruct dynamic 3D geometry and estimate dense point correspondences in unified 4D VAE framework; and 2) we show it is not necessary to align the data and latent spaces during fine-tuning diffusion model. 3. Method Given as input monocular video sequence with dynamic objects, our goal is to learn neural network fθ that can output 4D representation of its geometry along with dense point-wise correspondences, simultaneously: fθ : {Ii}N i=1 {Xi, Vii+1}N i=1. (1) = {Ii}N i=1 is the input monocular video sequence with frames, where each frame Ii RHW 3 is an RGB image. The network fθ predicts viewpoint-invariant point map Xi RHW 3 for each frame i, and the 3D scene 3 flow1 Vii+1 RHW 3 between each pair of consecutive frames and + 1. Both the point map and scene flow are represented in shared world coordinate system. Note that, since we only predict forward scene flow, the last frame does not have corresponding flow prediction, i.e., we do not supervise VNN +1. To smoothly model the long-term motion and enable generalization to diverse scenes, we build fθ upon pretrained video diffusion model, where θ denotes the learnable parameters. Our framework is illustrated in Fig. 2. We first introduce our unified 4D representation in Section 3.1 Then, in Section 3.2, we present dedicated 4D2 VAE architecture that jointly encodes geometry and motion into unified latent space. Finally, in Section 3.3, we describe the overall training and inference strategy for our model. 3.1. Unified Geometry & Motion Representation Here, like in DUSt3R [96], we define the point maps and scene flows in the coordinate system of the first frame, which serves as the world coordinate system. In particular, the point map Xi RHW 3 stores the 3D coordinates (x, y, z) of each pixel from the frame in the world coordinate system, while the scene flow Vi RHW 3 represents the 3D motion vector (x, y, z) of each pixel from the frame to + 1. Ideally, the deformed point map = Xi + Vi (2) should be spatially aligned with the point map of the next frame Xi+1. However, due to viewpoint changes, and Xi+1 are not in one-to-one correspondence in pixel space, as they represent different frame contents, as illustrated in Fig. 3. Note that, our scene flow is also defined directly in the (world) coordinate system, meaning that each Vi represents the motion vector (x, y, z) in the world space, naturally eliminating camera-induced motion components. Such unified geometry-motion representation offers several advantages: 1) Camera-free modeling. Similar to DUSt3R [96], defining the geometry and motion in chosen world coordinate system removes the need for additional camera pose estimation. 2) Temporal consistency. In continuous video sequence, geometry and motion are temporally coherent. Modeling them jointly in the same coordinate system makes them easier to learn. 3) Richer motion modeling. Unlike existing methods [14, 80], we define scene flow between every pair of consecutive frames in the video, rather than only between the first frame and others. Consequently, this representation is less sensitive to occlusions induced by viewpoint variations and remains capable of capturing motion information of newly emerging dynamic objects in subsequent frames. 1Unless otherwise noted, we simplify Vii+1 as Vi for clarity. 2Here we use 4D VAE to refer to the fused geometry and motion VAEs. 4 Figure 3. Geometry and Motion representation. For pixel pi in frame Ii, Xi is its corresponding 3D point. As this 3D point moves, we use to represent the moved point and Vi = (x, y, z) to represent the motion. Ideally, should align with matching point Xi+1 in next frame Ii+1. However, their pixel indexes are totally different (pi vs. pi+1) and pi+1 might even be out of view due to camera/object motion, making it impossible to build one-to-one correspondence between and Xi+1. 3.2. Unified 4D Geometry-Motion VAE Here, we describe how to encode the above 4D representation into latent space effectively, which can then be used as the denoised target for video diffusion model. Recent works in diffusion models [34, 39, 124] only encode the geometry of 3D attributes, neglecting explicit modeling of motion for scene dynamics. In contrast, we design novel 4D VAE architecture that jointly encodes geometry and motion into unified 4D latent, as illustrated in Fig. 2. To leverage the priors of pretrained diffusion models, it is widely believed that the input to the VAE should be strictly aligned with the original data distribution of the pre-trained diffusion model [34, 39, 124]. That is, naive approach is to directly rescale 3D attributes (such as disparity [39] and point maps [124]) into the range [1, 1], by performing max normalization, and then encode them using the frozen VAE weights. However, the world-coordinate 3D attributes are typically unbounded, with coordinates spanning (, +), in contrast to images with bounded pixel ranges [0, 255]. Moreover, the distribution of 3D attributes is inherently distinct from that of natural RGB images. Hence, in this work, we investigate fundamental question: Is strict alignment with the diffusion models input space essential for finetuning diffusion models? Geometry VAE with Revised Normalization. To answer the above question, one key insight of our model is slightly adjusted point map normalization strategy for the Geometry VAE. Note that, unlike the max normalization to [1, 1] commonly used in existing geometric diffusion models [34, 39, 124], we instead apply canonical normalization to each sequence of world-coordinate point maps: ˆXi = Xi µ , (3) (cid:80) where µ = 1 denoted by D, in the point map sequence (cid:13)Xd µ(cid:13) (cid:13) = 1 dD Xd is the mean of all valid points, i=1{Xi} and (cid:13)2 + ε is the mean distance for dD (cid:80) inherently correlated, learning motion independently may be suboptimal. We therefore explore several fusion strategies between geometry and motion: (1) no fusion, where geometry and motion are encoded separately without any interaction; (2) offset fusion, inspired by LayerDiffuse [123], where the motion latent is added as an offset to the geometry latent; and (3) unified fusion, where the geometry and motion latents are concatenated into unified 4D latent and passed to the motion VAE decoder to reconstruct the scene flow. As reported in Tab. 4, although the unified concatenation strategy does not yield the best reconstruction quality at the VAE stage, it leads to superior performance in the subsequent diffusion U-Net. During Motion VAE training, we freeze the Geometry VAEs parameters to preserve its learned geometric priors. The training objective is formulated as: LM = (cid:88) ˆVd Vd2 2 1 (cid:125) (cid:123)(cid:122) (cid:124) Scene flow reconstruction loss dD +λreg (cid:88) ˆVn2 2 1 nN (cid:125) (cid:123)(cid:122) (cid:124) Zero-flow regularization , (5) where ˆVd is the predicted scene flow, Vd is the ground truth, denotes valid pixels, and denotes all pixels. The first term denotes the MSE loss on the valid scene flow, and the second term is regularization term to encourage the scene flow to zero, following the as-static-as-possible assumption. By combining the Geometry VAE and Motion VAE into unified 4D VAE, we successfully achieve an integrated representation of geometry and motion within single latent space, enabling efficient 4D scene encoding and decoding. 3.3. Model Training Training Data. Dynamic datasets with annotated 3D geometry and dense scene flow are complex to collect in realworld scenarios. Therefore, we rely on synthetic datasets for training the scene flow estimation task. In particular, we divide our training data into two categories: (1) Geometry Datasets: Dynamic Replica [36], GTA-SFM [91], MatrixCity [46], MVS-Synth [28], Point Odyssey [128], TartanAir [97], ScanNet++ [117], BlinkVision [47], OmniWorld [129] and Synthia [74]; (2) Geometry-and-Motion Datasets: Kubric [19], Spring [61], and Virtual KITTI 2 [7]. The first category provides only geometric data, including per-frame depth maps, camera intrinsics and extrinsics. Following DUSt3R [96], we express the ground-truth point clouds into shared first frame coordinate system. The second category additionally provides dense scene flow annotations. During geometry reconstruction training, we use both dataset groups (1)+(2), whereas during motion reconstruction training, only datasets in group (2) are employed. Figure 4. Results of different normalization and VAE training strategies. For outdoor scenes with significant variations in depth (the second row), the original VAE fails to recover the scene structure. Even with decoder fine-tuning, the reconstruction quality remains poor. Our proposed mean normalization and VAE training strategy significantly improve reconstruction quality. scale normalization with small constant ε for numerical stability. This normalization maintains the scale invariance of point maps, while significantly improving the reconstruction quality of the Geometry VAE and better preserving finer structural details compared to max normalization [90, 96], especially when handling large-scale outdoor scenes, as shown in Fig. 4. Here, we finetune the entire encoder-decoder using our new normalization strategy, which provides more flexible input distribution. We define the training objective as: LG = Lpoint + λdLdepth + λnLnormal, (4) where Lpoint is the MSE loss for point map reconstruction, Ldepth is multi-scale loss computed on the projected depth maps, and Lnormal enforces consistency of surface normals, following [94, 110]. The difference is that we are encoding world-based point clouds. Therefore, we normalize ground truth camera poses together with the point clouds, so that we can use scale-aligned camera parameters to project the point cloud into depth. Experiments have shown that this supervision is similar to multimodal fusion in Geo4D [34], both of which improve the reconstruction quality of point clouds. Here we also tried using KullbackLeibler (KL) divergence loss [43] to constrain the distribution of the latent to standard Gaussian distribution, but found that it led to significant drop in VAE performance. The proposed normalization and VAE training strategy consistently improve the performance of the VAE and the downstream diffusion U-Net in Tab. 3, suggesting that strict alignment with the diffusion models input and latent space is not always necessary, especially for the 3D attributes. Motion VAE. simple way to model scene flow is to train separate motion VAE with the same architecture as the Geometry VAE. However, because motion and geometry are Training Strategy. We adopt two-stage training pipeline for the VAE components. We begin by training the Geometry VAE independently to capture scene geometry. Next, 5 we train the Motion VAE while keeping the Geometry VAE frozen, thereby preserving its learned geometric priors. After convergence, we combine them into unified 4D VAE, whose parameters remain frozen during the training of the diffusion U-Net. For Unet training, we combine the datasets from groups (1) and (2) to provide geometry supervision, and only the datasets from group (2) for motion supervision. Following prior works [27, 110] in employing EDM [38] pre-conditioning, our framework supports both the deterministic and denoising diffusion paradigms. For the deterministic paradigm, the training objective is defined as: Ldeterministic = Llatent + λGLG + λM LM, (6) where LG the geometry reconstruction loss same as (4), LM the motion reconstruction loss same as (5), and Llatent denotes the latent-space diffusion loss as: Llatent = (cid:88) ˆzG zG 1 (cid:125) (cid:123)(cid:122) (cid:124) geometry latent supervision 2 2 + 1 1 (cid:88) 1 ˆzM zM 2 2 , (cid:124) (cid:123)(cid:122) motion latent supervision (cid:125) (7) , zM where denotes the number of frames, and ˆzG are the denoised latent and the original latent of Geometry and Motion, respectively. We only perform forward scene flow estimation, thus discarding the motion latent in the last frame. For the denoising paradigm, the objective simplifies to: , ˆzM , zG Ldenoise = Llatent. (8) This progressive and modular training pipeline allows the model to first acquire strong geometry and motion priors before integrating temporal motion reasoning, ultimately enabling robust and coherent dense 4D reconstruction. Implementation Details. To inherit the strong priors from the video generator, both the VAE and Unet of our MotionCrafter are initialized with the pretrained weights of SVD [1], and trained using the AdamW optimizer [56] with learning rate of 1e-4. We first train the Geometry VAE for 40,000 iterations, followed by training the Motion VAE for 20,000 iterations. Subsequently, we merge the Geometry VAE and Motion VAE into unified 4D VAE, and train the U-Net for another 40,000 iterations with the encoded 4D latent representations. The batch size is set to 8 for VAE training and 25 for Unet training. All experiments are conducted on 8 GPUs with 40 GB of memory each and take about 3 days. More implementation details are provided in the supplementary material. 4. Experiments 4.1. Experimental Setting Datasets. shot For geometry evaluation, we perform zerotesting on three unseen dynamic scene datasets: 6 DDAD [20], Monkaa [60], and Sintel [6]. These datasets cover both real-world and synthetic scenes, including indoor and outdoor environments. For motion evaluation, due to the limited availability of datasets with dense scene flow annotations, we use combination of three in-domain datasets (Kubric [19], Spring [61], and VKITTI2 [7]) and two out-of-domain datasets (Dynamic Replica [36] and Point Odyssey [128]). Since Dynamic Replica and Point Odyssey only provide sparse scene flow annotations, we compute metrics only on the annotated points. Metrics. Unlike previous methods, we evaluate Geometry and Motion in the world coordinate system. The predicted world-space point cloud is aligned with the ground truth by optimizing per-sequence scale and shift parameters. We report the relative point error (Relp) and the percentage of inlier (δp, threshold 0.25) as evaluation metrics. The predicted scene flow is aligned according to the point map scale. We compute the End Point Error (EPE) and the Average Percent of Points within Delta (APD), where the subscript of APD denotes the inlier threshold in the metric scale. We provide details in the supplementary material. 4.2. Comparison with the State-of-the-art Methods Assessing joint Geometry and Motion Reconstruction. We compare our MotionCrafter with recent representative methods in joint geometry and motion estimation in Tab. 1. Here, we assess these metrics in the world coordinate space. Our model directly outputs sequence of predictions in the word coordinates. Still, existing methods are mainly pairwise-style, following DUSt3R [96], which need postoptimization or camera poses to align with ground truth. To ensure fairness, we uniformly use the camera pose predicted by VGGT [90] to transform their predictions into the world coordinate system. Through both quantitative and qualitative comparison in Fig. 5 and Tab. 1, we observe that these pair-wise approaches usually exhibit degraded performance when extended to video sequences, while our method outperforms state-of-the-art methods 38.64% in geometry and 25.0% in motion on average. Note that, unlike ZeroMSF [54], our model is trained without the motion annotations on Dynamic Replica [36] and Point Odyssey [128] but performs even better expect one comparable metric. Furthermore, Fig. 6 shows that our method estimates temporally consistent scene flow in the world coordinate system, remaining robust to camera motion and capable of describing 4D scene dynamics more accurately and efficiently. Assessing Geometry Reconstruction. We further compare our method with several representative approaches in geometry reconstruction in Tab. 2. We group them by their geometric representation: (1) Camera-centric methods predict depth or point maps in the camera coordinate system. We transform their outputs to world coordinates using VGGT Table 1. Evaluation on joint world-centric geometry and motion reconstruction. All metrics are reported without percentage symbols for readability. * denotes not zero-shot scene flow evaluation. -S and -P denote the Sequence mode and Pair mode of ST4RTrack. Since ST4RTrack always compares with the first frame for motion, for fair comparison, we run it on every pair of consecutive frames and then transform the results into the world coordinate system using VGGT poses. Plus, we add Zero-MSF + GT pose as reference. Method Geometry Kubric [19] δp Relp Spring [61] δp Relp VKITTI2 [7] δp Relp Dynamic Replica [36] Point Odyssey [128] Relp Relp δp δp POMATO [125] + VGGT ST4RTrack-S [14] + VGGT ST4RTrack-P [14] + VGGT DELTA [64] + VGGT Zero-MSF [54] + VGGT Zero-MSF [54] + GT MotionCrafter (ours) 25.56 6.61 17.81 14.09 8.79 8.78 3.40 77.85 95.59 80.76 85.73 94.73 94.73 98.73 98.13 123.27 157.05 106.88 142.44 142.45 29.20 61.71 42.26 38.00 50.81 40.66 40.69 77.27 32.16 67.68 84.77 57.03 15.76 11.22 14.60 54.89 21.87 14.46 42.76 80.04 89.92 84. 7.26 5.65 4.87 23.21 7.11 7.11 4.04 94.64 96.29 97.13 74.35 97.03 97.01 99.00 19.88 31.00 29.13 51.47 22.55 22.52 9.94 80.08 68.17 71.66 50.92 78.27 78.27 94.90 Motion EPE APD0.05 EPE APD0.1 EPE APD0.3 EPE APD0.05 EPE APD0.05 79.58 POMATO [125] + VGGT ST4RTrack-S [14] + VGGT 59.34* ST4RTrack-P [14] + VGGT 217.20* DELTA [64] + VGGT Zero-MSF [54] + VGGT Zero-MSF [54] + GT MotionCrafter (ours) 8.29* 8.59* 5.74* 4.60* 5.23 1.68* 5.65* 52.95* 50.13* 72.37* 68.01* 180.13 105.02 441.84 8.59 7.78* 5.50* 5.61* 20.16 33.47 9.07 81.55 85.59* 87.59* 90.17* 368.78 156.85 874.94 156.64 112.99* 73.81* 71.75* 15.66 22.11 13.16 17.7 21.69* 25.15* 25.90* 14.56* 1.12* 0.99* 0.75 0.59* 0.40* 0.51 36.34* 97.13* 97.55* 99.57 99.80* 99.80* 99. 31.94* 10.37* 58.62* 6.09 3.58* 2.30* 3.49 30.32* 53.39* 51.19* 62.11 80.12* 91.04* 80.66 Avg. Rank 5.0 4.0 3.4 6.0 4.6 - 1.0 5.0 4.0 4.8 3.2 2.4 - 1. Figure 5. Qualitative comparison with Zero-MSF [54]. Zoom in for the details. Compared to Zero-MSF, we have more reasonable scene structure and better geometric details. More importantly, our predicted 3D scene flow has more accurate direction of motion. Table 2. Evaluation on world-centric geometric reconstruction. denotes using post-optimization. Note that, our results are reported without any post-optimization. Method Camera-centric DepthPro [3] MoGe [94] GeoCrafter [110] World-centric MonST3R [122] VGGT [90] Geo4D [34] St4RTrack [14] Ours Monkaa [60] Relp δp Sintel [6] Relp δp DDAD [20] Relp δp Avg. Rank 36.96 35.21 33.44 41.41 34.54 28.04 47.04 25.88 63.65 65.95 65.79 31.46 56.65 69.52 45.46 74.01 43.30 35.28 30.61 37.65 26.83 34.61 40.59 32. 42.30 60.97 68.07 51.41 67.91 59.54 51.54 63.14 35.38 18.63 19.17 31.56 15.98 14.58 39.59 21.27 45.49 77.07 76.39 55.30 84.06 83.68 32.47 72. 7.00 4.33 3.33 6.33 2.33 2.33 7.67 2.67 poses for fair comparison. (2) World-centric methods predict geometry directly in the reference frame coordinate system, and we align their results to ground truth using an affine transformation. Our method achieves state-of-the-art performance on Monkaa, demonstrating the advantages of our proposed architecture. For Sintel and DDAD, our performance is inferior to VGGT [90], which we attribute to our single-modal design (w/o camera rays and depth maps) and the limited scale of our outdoor training data. Note that we do not perform post-optimization, as in Geo4D [34]. The visual comparisons (see in supp.) further validate that our approach produces more coherent and consistent reconstructions in dynamic environments. 4.3. Ablation Study We conduct thorough ablations to analyze MotionCrafter. Results are reported in Tabs. 3 and 4, and aim to investigate the following three crucial questions: Is it necessary to strictly align the input distribution with that of Video Diffusion? The answer is no. Most of the existing geometric diffusion models [34, 39, 124] strictly normalize the 3D attributes (such as depth and point maps) 7 Table 4. Ablation study on motion VAE. Comparison of different designs across three dynamic scene flow datasets. Again, we report results on both VAE and U-Net. Model Fusion Type Spring [61] Point Odyssey [128] EPE APD0.03 EPE APD0.03 VAE - 5 VAE - 6 VAE - 7 VAE - 8 Unet -III Unet - IV Original Offset Separate Unify Separate Unify 6.43 1.83 0.66 0.88 6.37 5.16 50.49 83.51 96.75 94.78 65.94 72.81 1.55 2.00 0.77 0.77 3.65 3. 91.68 86.66 94.74 94.19 63.19 66.36 tance of tightly coupling geometry and motion representations for coherent 4D modeling. What prior knowledge does Video Diffusion provide? When using the original pretrained VAE to encode geometry and motion, we observe that the model already exhibits reasonable reconstruction ability in indoor scenes, as seen in the first rows of Tabs. 3 and 4. However, due to the significant scale discrepancy between point maps, scene flow, and image-space distributions, the original VAE struggles to handle large-scale variationsespecially in outdoor scenes, shown in Fig. 4. We argue that appropriate normalization and fine-tuning strategies are essential to leverage the prior embedded in the diffusion model fully. As shown in VAE2 of Table 3, training from scratch leads to suboptimal results, confirming that the pretrained Video Diffusion model indeed contains rich priors beneficial for dense 4D reconstruction. By explicitly modeling these priors, we effectively unlock the 4D representation capability of the video diffusion model. 5. Conclusion We introduce MotionCrafter, Diffusion-based framework capable of jointly reconstructing dense geometry and motion from monocular video. By defining both in unified world coordinate system and designing novel 4D VAE that encodes them into shared latent space, we achieve stateof-the-art performance even without any post-processing. Notably, we show that it is not necessary to strictly align the 4D latent distribution with the original SVD latent distribution. In fact, our relaxed alignment and VAE retraining strategy not only preserves but also improves the diffusion models generalization, offering broader insight for future research to adapt the diffusion prior for new modalities. Limitations. Currently, we focus solely on dense geometry and motion reconstruction, but prior work has shown that incorporating multiple geometric modalities can substantially improve the prediction of 3D attributes, including camera parameters, point maps, depth maps, point tracks, and novel views. Thus, exploring multi-modal integration is promising direction for future work. Figure 6. Qualitative comparison with ST4RTrack [14]. In the first case, the pixel trajectory shows that we yield cleaner scene flow, while ST4RTrack suffers from noisy drift. In the second case, the deformed point map (with darker color) shows that our method predicts more temporally consistent geometry and motion. Table 3. Ablation study on geometry VAE. Here, we report geometry reconstruction results on both VAE and U-Net, as better VAE may not always lead to better final results. The models are trained on subset only for geometry reconstruction. Model Training Type Rescale VAE - 1 VAE - 2 VAE - 3 VAE - Unet - Unet - II Original From scratch Finetune decoder Finetune all VAE - 3 + Unet VAE - 4 + Unet Max Max Max Mean Max Mean Sintel[6] Relp δp Monkaa [60] Relp δp 39.96 18.80 20.76 5.68 40.47 35.17 62.63 79.68 77.77 98. 49.42 58.08 23.78 11.48 14.44 5.03 33.66 27.36 67.33 90.55 85.91 99.13 56.42 66.21 to [1, 1], in order to inherit the diffusion prior. However, we find that such max-rescale normalization results in suboptimal reconstruction accuracy for the pretrained VAEs (VAE-1&2 of Tab. 3). Although Geo4D [34] alleviates this issue by freezing the VAE encoder and fine-tuning only the decoder, our ablation shows that this strategy still yields inferior results (VAE-3 in Tab. 3). In contrast, our proposed mean rescale strategycombined with fine-tuning all VAE componentsachieves the best performance without strict adherence to the original VAE distribution (VAE-4 of Tab. 3). We further validate this finding in the Diffusion Unet stage (Unet-I vs. Unet-II), resulting in an average of 16.6% gain in geometry. This finding suggests that MotionCrafter maintains better generalization ability, even do not align the distribution with that of video diffusion. Which strategy for fusing geometric and motion latent information is most effective? We explore different strategies, including Offset, Separate, and Unify as discussed in Sec. 3.2, for jointly encoding geometry and motion information in Tab. 4. Experimental results indicate that although separate VAEs achieve optimal performance in VAE reconstruction, unified VAE ultimately performs better in Unet prediction. This phenomenon highlights the impor-"
        },
        {
            "title": "Acknowledgments",
            "content": "Ruijie Zhu completed this work during his internship at Tencent ARC Lab. We thank Tian-Xing Xu for providing the codebase of GeometryCrafter. Chuanxia Zheng is supported by NTU SUG-NAP and the National Research Foundation, Singapore, under its NRF Fellowship Award NRF-NRFF17-2025-0009."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 6, 17 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synIn Proceedings of thesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 2256322575, 2023. 3 [3] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 7 [4] Thomas Brox, Andres Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on theory for warping. In European conference on computer vision (ECCV), pages 2536. Springer, 2004. 3 [5] Andres Bruhn, Joachim Weickert, and Christoph Schnorr. Lucas/kanade meets horn/schunck: Combining local and global optic flow methods. International journal of computer vision (IJCV), 61(3):211231, 2005. 3 [6] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In European Conf. on Computer Vision (ECCV), pages 611 625. Springer-Verlag, 2012. 6, 7, [7] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 5, 6, 7, 17 [8] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 130141, 2023. 2 [9] Weirong Chen, Ganlin Zhang, Felix Wimbauer, Rui Wang, Nikita Araslanov, Andrea Vedaldi, and Daniel Cremers. Back on track: Bundle adjustment for dynamic scene reconIn Proceedings of the IEEE/CVF International struction. Conference on Computer Vision (ICCV), 2025. 2 [10] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Feed-forward 360 scene synthesis from Mvsplat360: In Neural Information Processing Systems sparse views. (NeurIPS), 2024. 3 9 [11] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. Advances in Neural Information Processing Systems (NeurIPS), 2024. [12] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision (ICCV), pages 27582766, 2015. 3 [13] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua TenenNeural radiance flow for 4d baum, and Jiajun Wu. In Proceedings of view synthesis and video processing. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1430414314. IEEE Computer Society, 2021. 2 [14] Haiwen Feng*, Junyi Zhang*, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J. Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2, 3, 4, 7, 8, 21 [15] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1247912488, 2023. 2 [16] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [17] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan De Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 753762. IEEE, 2025. 3 [18] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2293022941, 2023. 3 [19] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: In Proceedings of the IEEE scalable dataset generator. Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 5, 6, 7, [20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised the monocular depth estimation. IEEE conference on computer vision and pattern recognition(CVPR), 2020. 6, 7 In Proceedings of [21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. In International Conference on Learning Representations (ICLR), 2024. 3 [22] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. 1 [23] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. Nature, pages 17, 2025. [24] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 2 [25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Neural Information Processing Systems (NeurIPS), 35:86338646, 2022. 3 [26] Berthold KP Horn and Brian Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. 2, 3 [27] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences In Proceedings of the IEEE/CVF for open-world videos. Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 6, 17 [28] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 2821 2830, 2018. 5, 17 [29] Xingchang Huang, Corentin Salaun, Cristina Vasconcelos, Christian Theobalt, Cengiz Oztireli, and Gurprit Singh. Blue noise for diffusion models. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. [30] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: transformer architecture for opIn European conference on computer vision tical flow. (ECCV), pages 668685. Springer, 2022. 3 [31] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 24622470, 2017. 3 [32] Matthias Innmann, Michael Zollhofer, Matthias Nießner, Christian Theobalt, and Marc Stamminger. Volumedeform: In EuroReal-time volumetric non-rigid reconstruction. pean conference on computer vision (ECCV), pages 362 379. Springer, 2016. 1 [33] Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360 dynamic object generation from monocular video. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 3 [34] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2, 3, 4, 5, 7, 8, 15, 21 [35] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2, [36] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13229 13239, 2023. 5, 6, 7, 17 [37] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European conference on computer vision (ECCV), pages 1835. Springer, 2024. 2 [38] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. NeurIPS, 35:2656526577, 2022. 6, 17 [39] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94929502, 2024. 2, 3, 4, 7 [40] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [41] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [42] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, Joao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. TAPVid-3D: benchmark for tracking any point in 3D. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [43] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22 (1):7986, 1951. 5 [44] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. In EuroGrounding image matching in 3d with mast3r. pean Conference on Computer Vision (ECCV), pages 71 91. Springer, 2024. 2 [45] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dso: Aligning 3d generators with simulation 10 In Proceedings of the feedback for physical soundness. IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [46] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: largescale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 32053215, 2023. 5, 17 [47] Yijin Li, Yichen Shen, Zhaoyang Huang, Shuo Chen, Weikang Bian, Xiaoyu Shi, Fu-Yun Wang, Keqiang Sun, Hujun Bao, Zhaopeng Cui, et al. Blinkvision: benchmark for optical flow, scene flow and point tracking estimation using rgb frames and events. In European conference on computer vision (ECCV), pages 1936. Springer, 2024. 5, 17 [48] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 64986508, 2021. 2 [49] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 42734284, 2023. 2 [50] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussianmesh hybrid representation. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [51] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [52] Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, et al. Feed-forward bullet-time reconstruction of dynamic scenes from monocular videos. arXiv preprint arXiv:2412.03526, 2024. 2, 3 [53] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Chih-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, et al. Diffusion renderer: Neural inverse and forward rendering with video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2606926080, 2025. 3 [54] Yiqing Liang, Abhishek Badki, Hang Su, James Tompkin, and Orazio Gallo. Zero-shot monocular scene flow estiIn Proceedings of the Computer Vimation in the wild. sion and Pattern Recognition Conference (CVPR), pages 2103121044, 2025. 6, 7, 20 [55] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision (ICCV), pages 92989309, 2023. 3 [56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [57] Jiahao Lu, Jiacheng Deng, Ruijie Zhu, Yanzhe Liang, Wenfei Yang, Tianzhu Zhang, and Xu Zhou. Dn-4dgs: Denoised deformable network with temporal-spatial aggregation for dynamic scene rendering. Advances in Neural Information Processing Systems, 37:8411484138, 2024. [58] Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2282022830, 2025. 2 [59] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, and Shiwei Li. Matrix3d: Large photogrammetry model allIn Proceedings of the IEEE/CVF Conference on in-one. Computer Vision and Pattern Recognition (CVPR), 2025. 2 [60] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 40404048, 2016. 6, 7, 8 [61] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4981 4991, 2023. 5, 6, 7, 8, 17 [62] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision (ECCV), 2020. 2 [63] Richard Newcombe, Dieter Fox, and Steven Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 343352, 2015. 1, 2 [64] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video. arXiv preprint arXiv:2410.24211, 2024. 7, [65] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58655874, 2021. 2 [66] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 18, 19 [67] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vi11 sion and Pattern Recognition (CVPR), pages 1010610116, 2024. 2 [68] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [69] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields In Proceedings of the IEEE/CVF for dynamic scenes. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1031810327, 2021. 2 [70] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: robust and versatile monocular visual-inertial state estimator. IEEE transactions on robotics, 34(4):10041020, 2018. 1 [71] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [72] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. Epicflow: Edge-preserving interpolation of correspondences for optical flow. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 11641172, 2015. 3 [73] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1068410695, 2022. 3 [74] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio Lopez. The synthia dataset: large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 32343243, 2016. 5, 17 [75] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94209429, 2024. 3 [76] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 15991610, 2023. 3 [77] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [78] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [79] Ziyang Song, Zerong Wang, Bo Li, Hao Zhang, Ruijie Zhu, Li Liu, Peng-Tao Jiang, and Tianzhu Zhang. Depthmaster: Taming diffusion models for monocular depth estimation. arXiv preprint arXiv:2501.02576, 2025. 3, 17 [80] Edgar Sucar, Zihang Lai, Eldar Insafutdinov, and Andrea Vedaldi. Dynamic point maps: versatile representation for dynamic 3d reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2, 3, 4 [81] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 89348943, 2018. 3 [82] Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3d: Generating 3d scenes in seconds. In International Conference on Computer Vision (ICCV), 2025. 3 [83] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometricaware unified world modeling. In International Conference on Computer Vision (ICCV), 2025. [84] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In European conference on transforms for optical flow. computer vision (ECCV), pages 402419. Springer, 2020. 2, 3 [85] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigid-motion embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 83758384, 2021. 2, 3 [86] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from In Proceedings of the IEEE/CVF Inmonocular video. ternational Conference on Computer Vision (ICCV), pages 1295912970, 2021. 2 [87] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision (ECCV), pages 439457. Springer, 2024. 3 [88] Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, and Dewen Hu. Scenetracker: Long-term scene flow estimation network. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025. 3 [89] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [90] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 52945306, 2025. 2, 3, 5, 6, 7, 12 [91] Kaixuan Wang and Shaojie Shen. Flow-motion and depth network for monocular stereo and beyond. IEEE Robotics and Automation Letters, 5(2):33073314, 2020. 5, 17 [92] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In International Conference on Computer Vision (ICCV), 2025. 2 [93] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [94] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. arXiv preprint arXiv:2410.19115, 2024. 5, 7 [95] Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo: Towards end-to-end visual odometry with deep reIn 2017 IEEE incurrent convolutional neural networks. ternational conference on robotics and automation (ICRA), pages 20432050. IEEE, 2017. 1 [96] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, 2024. 2, 4, 5, 6, [97] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. 5, 17 [98] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems (NeurIPS), 36:75947611, 2023. 3 [99] Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, and Xiaowei Zhou. Freetimegs: Free gaussian primitives at anytime anywhere for dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2175021760, 2025. 2 [100] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Scalable permutationChunhua Shen, and Tong He. arXiv preprint equivariant visual geometry learning. arXiv:2507.13347, 2025. 3 [101] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2031020320, 2024. 2 [102] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 26057 26068, 2025. [103] Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, and Tat-Jen Cham. Amodal3r: Amodal 3d reconstruction from occluded 2d images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 3 [104] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2040620417, 2024. 3 [105] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. SV4D: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 3 [106] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), pages 399 417. Springer, 2024. 3 [107] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion modarXiv preprint els for general dense perception tasks? arXiv:2403.06090, 2024. 3, 17 [108] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81218130, 2022. 3 [109] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying IEEE Transactions on flow, stereo and depth estimation. Pattern Analysis and Machine Intelligence (TPAMI), 45 (11):1394113958, 2023. [110] Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, SongHai Zhang, and Ying Shan. Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2, 5, 6, 7, 17 [111] Zhen Xu, Zhengqin Li, Zhao Dong, Xiaowei Zhou, Richard Newcombe, and Zhaoyang Lv. 4dgt: Learning 4d gaussian transformer using real-world monocular videos. information processing systems In Advances in neural (NeurIPS), 2025. 2, 3 [112] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1037110381, 2024. 2 13 Computer Vision and Pattern Recognition (CVPR), 2025. 2, 4, 7 [125] Songyan Zhang, Yongtao Ge, Jinyuan Tian, Guangkai Xu, Hao Chen, Chen Lv, and Chunhua Shen. Pomato: Marrying pointmap matching with temporal motion for dynamic 3d reconstruction. arXiv preprint arXiv:2504.05692, 2025. 7 [126] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William T. Freeman. Structure In European Conference and motion from casual videos. on Computer Vision (ECCV), 2022. [127] Chuanxia Zheng and Andrea Vedaldi. Free3d: Consistent novel view synthesis without 3d representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97209731, 2024. 3 [128] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1985519865, 2023. 5, 6, 7, 8, 17 [129] Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, et al. Omniworld: multi-domain and multi-modal dataset for 4d world modeling. arXiv preprint arXiv:2509.12201, 2025. 5, 17 [130] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting. Advances in Neural Information Processing Systems (NeurIPS), 2024. 2 [131] Michael Zollhofer, Matthias Nießner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, et al. Real-time non-rigid reconstruction using an rgb-d camera. ACM Transactions on Graphics (ToG), 33 (4):112, 2014. 1 [113] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems (NeurIPS), 2024. 2 [114] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2033120341, 2024. [115] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In International Conference on Learning Representations (ICLR), 2024. 2 [116] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 43(6):118, 2024. 3 [117] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d In Proceedings of the IEEE/CVF Internaindoor scenes. tional Conference on Computer Vision (ICCV), pages 12 22, 2023. 5, 17 [118] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [119] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gausIn European Conference on Computer Vision sians. (ECCV), pages 163179. Springer, 2024. 3 [120] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent difInternational fusion models for text-to-video generation. Journal of Computer Vision (IJCV), pages 115, 2024. 3 [121] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, 4diffusion: Multi-view Yunhong Wang, and Yu Qiao. video diffusion model for 4d generation. Advances in Neural Information Processing Systems (NeurIPS), 37:15272 15295, 2024. [122] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for esIn Intertimating geometry in the presence of motion. national Conference on Learning Representations (ICLR), 2025. 2, 7 [123] Lvmin Zhang and Maneesh Agrawala. Transparent image layer diffusion using latent transparency. arXiv preprint arXiv:2402.17113, 2024. 5, 15 [124] Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3d modeling. In Proceedings of the IEEE/CVF Conference on 14 MotionCrafter: Dense Geometry and Motion Reconstruction with 4D VAE"
        },
        {
            "title": "Supplementary Material",
            "content": "In the supplementary video, we provide additional visual results. In this supplementary document, we present further details and analyses to complement the main paper. A. Data Processing For each video sequence, we preprocess the corresponding point maps and scene flow into unified [world]-coordinate system, referenced by the first camera pose. The processing pipeline consists of three steps: (1) camera-pose normalization, (2) transformation of point maps and scene flow into the world coordinate frame, and (3) global normalization of world-space geometry and motion. Below, we detail each component. A.1. Camera Pose Normalization Monocular reconstruction systems often produce camera poses that contain arbitrary global rotation and translation. To eliminate this ambiguity, following DUSt3R [96], we align all poses to canonical coordinate frame defined by the first camera. In particular, given sequence of camera i=1, where each Pi R44, we decompose the poses {Pi}N first pose as R0 = P0[: 3, : 3], t0 = P0[: 3, 3]. (9) Each pose is then normalized by scene flow, we first compute the deformed points:"
        },
        {
            "title": "X C",
            "content": "ii+1 = + , (12) and transform them using the camera pose of the next frame: Xii+1 = Ri+1X ii+1 + ti+1. The world-space scene flow is computed as Vi = Xii+1 Xi. (13) (14) If deformability mask is available, we apply it to zero out scene flow in non-dynamic regions. A.4. Global World-Coordinate Normalization To ensure consistent training across scenes with different scales, the global normalization is applied in the [world]- space geometry. Centering. We first compute the centroid of all valid points: µ = 1 (cid:88) dD Xd. (15) Isotropic Rescaling. Instead of scaling by the maximum radius, we compute the mean scale of valid points to the centroid: Ri = 0 Ri, ti = 0 (ti t0), (10) = Xd µ2. (16) 1 (cid:88) dD which preserves the relative motion within the sequence while removing global rotation and translation. Then we uniformly normalize the point map, camera pose, and scene flow with the affine transformation: A.2. Point Map Transformation R3 expressed in the camera coorGiven point map dinate system of frame i, we transform it into the first-frame coordinate system using the normalized camera poses: Xi = RiX + ti. (11) This transformation is applied to all valid pixels, while invalid pixels (as indicated by the validity mask) are set to zero. For these invalid points, we use pyramid padding [123] to fill them in. Note that, we do not supervise these invalid points; filling them in is solely to prevent the VAE from being affected by missing values during feature extraction. A.3. Scene Flow Transformation The original scene flow is defined in the camera coori dinates of frame i. To obtain the first-frame (world) space Xi Xi µ , ti ti µ , Vi Vi . (17) This isotropic scaling preserves the geometric structure while normalizing the absolute scale across datasets. The normalization parameters (µ, S) are stored for optional recovery of the original metric scale. B. Additional Ablations B.1. Ablation on the Multimodal Supervision Motivation. While methods such as Geo4D [34] rely on multi-modality outputs (e.g., depth, point maps, and normals) together with post-optimization fusion stage to obtain the final reconstruction, our main goal is to achieve fully feed-forward 4D geometry and motion reconstruction. Therefore, we intentionally avoid introducing any auxiliary outputs or post-refinement during inference. Interestingly, 15 Table 5. Ablation study on Geometry VAE components. Metrics are reported for ScanNet, Sintel, and Monkaa datasets: point accuracy (Relp , δp ) and depth accuracy (Reld , δd ). Model Training Rescale Depth Loss ScanNet Sintel Monkaa Relp δp Reld δd Relp δp Reld δd Relp δp Reld δd 1 2 3 4 5 6 7 Original From scratch From scratch Finetune decoder Finetune decoder Finetune all Finetune all Max Mean Max Max Max Mean Mean 14.96 7.01 11.88 10.45 4.46 4.46 3.03 86.95 99.29 91.08 92.30 98.20 99.69 99.88 1.98 1.88 1.82 2.73 0.54 1.11 0.76 99.90 99.92 99.59 98.83 99.97 99.97 99.99 39.96 10.40 18.80 20.76 12.04 5.68 4. 62.63 93.89 79.68 77.77 88.28 98.77 99.14 12.62 42.44 10.21 12.67 4.30 9.73 8.04 92.65 92.38 92.26 89.08 98.64 94.60 95.31 23.78 8.02 11.48 14.44 8.50 5.03 3.74 67.33 96.91 90.55 85.91 93.36 99.13 99.47 4.30 4.10 5.93 7.10 1.37 3.54 1. 98.91 98.47 93.80 91.66 99.64 99.27 99.77 Table 6. Ablation study on Unet components for Geometry Reconstruction. We compare models trained with different strategies, rescaling methods, and decoder losses. Model Training Strategy Normalization Decoder Loss GMU Kitchen Relp δp Monkaa Sintel DDAD Relp δp Relp δp Relp δp II III Finetuning VAE Decoder Finetuning the whole VAE Finetuning the whole VAE Max Mean Mean 17.49 17.72 14.47 80.65 82.90 90.54 33.66 27.36 25.33 56.42 66.21 71.77 40.47 35.17 33. 49.42 58.08 56.94 41.66 33.78 22.39 39.45 50.34 70.42 although multimodal outputs are not used at test time, we find that multimodal supervision during VAE training can still benefit the reconstruction quality of the 4D latent. In particular, we leverage depth as an additional supervision signal derived from the world-coordinate point maps. Depth supervision. Given the reconstructed worldcoordinate point map ˆX and the ground-truth point map X, we project both into the depth domain using the normalized camera pose P: ˆD = Π( ˆX, ), = Π(X, ), (18) where Π() denotes standard projection into the depth map. We apply two complementary losses: Per-pixel L1 depth loss. This loss encourages accurate depth prediction and is masked by the depth validity map: LL1-D = (cid:13) (cid:13) ( ˆD D) (cid:13) (cid:13) (cid:13) (cid:13) , (19) where is the binary valid-mask. Multi-scale patch depth loss. To improve geometric consistency across different spatial scales, we compute an L1 loss over patches defined by the scale factors {4, 16, 64}. For each scale, the depth maps are divided into non-overlapping patches; within each patch, the mean depth (computed with masked averaging) is subtracted to remove global bias: LPatch-D = (cid:88) s{4,16,64} (cid:16) ˆD(s) D(s)(cid:17) (cid:13) (cid:13) (cid:13) W(s)(cid:13) (cid:13) (cid:13)1 . The total multimodal depth supervision is: Ldepth = λL1-DLL1-D + λPatch-DLPatch-D. (21) Results. As shown in Tab. 5, introducing depth-based multimodal supervision significantly improves the reconstruction quality of the world-coordinate point maps (with 13.55% improvement in point map and 16.41% in depth map). Notably, this improvement is achieved without modifying the inference pipeline or introducing any extra modalities at test time. This ablation demonstrates that multimodal supervision is an effective strategy for enhancing the 4D latent representation learned by our VAE. B.2. Ablation on the Decoder Loss Motivation. In the deterministic setting, the denoising process in conventional diffusion models can be viewed as collapsing from multi-step procedure into single step. Therefore, in addition to supervising the denoised latent representation, we introduce decoder loss that directly supervises the VAE decoders output. Compared to latent regression, this supervision is more direct and provides stronger training signals to the UNet. Implementation. During training, we do not update the VAE decoders weights. However, we still compute its gradients so that the loss can be back-propagated through the decoder to update the UNet parameters. To reduce memory consumption, we apply gradient checkpointing to the VAE decoder during this process. (20) This term encourages consistent local geometric structure and suppresses depth-shift artifacts. Results. As shown in Tab. 6, incorporating the decoder loss consistently improves the UNet training. Across four unseen datasets, it yields an average improvement of 15.01%, 16 with particularly notable gains on the outdoor dataset DDAD, where the performance improves by 36.80%. Table 8. An overview of the training datasets. To balance the training, we sample the subset of some datasets. B.3. Ablation on the training paradigm Motivation. Following prior works [27, 110] in employing EDM [38] pre-conditioning, our framework supports both the deterministic and denoising diffusion paradigms, on top of the pretrained SVD model. Since 4D Reconstruction is deterministic task, we use deterministic paradigm by default, which has been widely explored and shown to be effective in previous dense prediction frameworks [79, 107, 110]. However, we also want to know exactly how different these two training paradigms are in our framework, especially for 4D latents that incorporate both geometry and motion information. Therefore, we conduct ablation experiments to verify this. Implementation. As described in Section 3.3, we have already introduced the loss functions for two training paradigms. For fair comparison, we do not use decoder loss in the deterministic paradigm. Specifically, we directly feed video latents into the U-Net to predict 4D latents. For the denoising paradigm, we first add noise to the 4D latents and channel-wise concatenate the video latents, then use the U-Nets multi-step denoising to predict the 4D latents. All U-Net weights are initialized from the original SVD, with channel dimensions adjusted only at the first layer to accommodate different training paradigms. Results. As shown in Tab. 7, the deterministic paradigm reduces Relp by about 12.4% and improves δp by approximately 12.7% compared to the diffusion paradigms averaged across datasets. This result strongly demonstrates the effectiveness of the deterministic paradigm in dense prediction tasks. Also, this shows that prior knowledge of SVD can be inherited by the model without relying on denoising mechanism. Table 7. Ablation on different training paradigm. Training Type Monkaa Sintel DDAD Relp δp Relp δp Relp δp Diffusion Deterministic 30.11 25.88 65.49 74.01 35.95 32. 53.82 63.14 24.58 21.27 67.54 72.82 C. Implementation Details C.1. Hyperparameter We list the loss weights used for training the 4D VAE. Point Map Reconstruction Loss: λpoint = 1.0 Per-pixel L1 Depth loss: λL1-D = 1.0 Multi-Scale Depth Supervision: λPatch-D = 1.0 Normal Consistency Loss: λnormal = 0. Dataset Domain #Frames #Videos DynamicReplica [36] Indoor/Outdoor Outdoor GTA-SfM [91] Indoor Kubric [19] MatrixCity [46] Outdoor-Driving MVS-Synth [28] Outdoor-Driving Spring [61] Point Odyssey [128] Synthia [74] TartanAir [97] VirtualKitti2 [7] BlinkVision [47] OmniWorld [129] Scannet++ [117] Total Outdoor Indoor Outdoor-Driving Outdoor Driving Indoor/Outdoor Indoor/Outdoor Indoor-Real - 1126 145K 234 19K 5736 137K 3029 452K 120 12K 49 5K 120 18K 1276 178K 2245 306K 320 43K 72 11K 350 35K 310K 2078 1.67M 16.8K Scene Flow Reconstruction Loss: λsceneflow = 1.0 Scene Flow Regulation Loss: λreg = 0.01 The pretrained video diffusion UNet is optimized with the following latent regression loss and decoder loss (optional): Latent Regression Loss: λlatent = 1.0 Point Map Decoder Loss: λG = 1.0 Scene Flow Decoder Loss: λM = 1.0 C.2. Used Training Set We list the used training datasets in Tab. 8, and provide some visual samples in Fig. 7. C.3. Model Information Our system adopts the VAE and video UNet backbone from the stable video diffusion (SVD) [1] model. The pipeline consists of three major components: (1) video VAE encoder for encoding per-frame latent representations, (2) 4D VAE decoder for reconstructing geometry and motion fields from latent space, and (3) 3D spatiotemporal UNet for latent denoising. We report the parameter counts of each component below: Video UNet: 1524.62M parameters. This large spatiotemporal UNet is responsible for denoising the latent representations over both space and time, enabling the modeling of dynamic geometry and motion. Video VAE Encoder: 34.16M parameters. This module processes each input frame independently and encodes it into latent space with spatial downsampling factor of 8. 4D VAE Decoder: 99.00M parameters. This decoder reconstructs 4D point maps and scene flow from the latent representation. Total Parameters: 1657.79M parameters. 17 D. More Visualization Results We select some in-the-wild videos from the Davis [66] dataset as samples for zero-shot testing, and the results are shown in Fig. 8. more intuitive visualization is provided in the attached video demo. We also provide more qualitative comparisons with other methods, as shown in Figs. 9 to 11. The comparisons are for two different tasks: 1) joint geometry and motion estimation, and 2) geometry reconstruction only. Inference Timing. All timings are measured on single GPU with 40 GB of memory. For video clip of 25 frames at resolution 320 640, the average processing time per frame is as follows: 52.0 ms for VAE encoding, 13.4 ms for latent denoising, and 73.5 ms for VAE decoding, resulting in total of 138.9 ms per frame. These measurements reflect the end-to-end processing required for full forward pass of our geometrymotion reconstruction pipeline. C.4. Evaluation Metrics We provide detailed definitions of the evaluation metrics used for geometry and motion reconstruction. Geometry Alignment. Since monocular reconstruction is defined up to scale ambiguity, the predicted world-space point map ˆXi is aligned to the ground truth Xi using persequence scale and shift t: Xi = ˆXi + t, where and are optimized by minimizing: min s,t (cid:88) (cid:13) (cid:13)s ˆXi + Xi (cid:13) (cid:13) 2 (cid:13) (cid:13) . (22) (23) Relative Point Error (Relp). We measure the relative geometry error as: Relp = (cid:13) Xi Xi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 Xi2 . 1 (cid:88) (24) Inlier Ratio (δp). We compute the percentage of points whose relative error is below threshold τ (0.25 in our experiments): δp = 1 (cid:88) 1 (cid:13) Xi Xi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 Xi < τ . (25) Scene Flow Alignment. The predicted scene flow ˆVi is scaled using the same geometry scale s: Vi = ˆVi. (26) End-Point Error (EPE). We compute the average endpoint error between predicted and ground-truth scene flow: EPE = 1 (cid:88) (cid:13) Vi Vi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 . (27) Average Percent of Points within Delta (APD). APD measures the percentage of scene flow vectors whose error is below threshold γ: APDγ = 1 (cid:88) 1 (cid:16)(cid:13) Vi Vi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:17) . < γ (28) 18 Figure 7. The examples of our training set. We randomly sample video frames from these datasets. In geometric training, we set random stride to sample the video at different intervals. In motion training, we always keep the stride at 1 to continuously sample frames. Figure 8. Zero-shot results on Davis [66] dataset. Despite the very limited number of samples used for training scene flow estimation, our method generalizes well across different scene types. Thanks to our end-to-end model design and unified definitions of geometry and motion in the world coordinate system, all results are directly output by the model without any post-optimization. See the video visualization for more intuitive understanding of the dynamics. 19 Figure 9. Qualitative comparison with the state-of-the-art methods Zero-MSF [54] and DELTA [64]. In the first case, our method demonstrates scene flow estimation accuracy comparable to Zero-MSF, even without training on the dynamic replica dataset like it. In the other cases, our method significantly outperforms existing methods in both geometric structure and motion pattern estimation. 20 Figure 10. Qualitative geometric comparison with VGGT [90], Geo4D [34], and ST4RTrack [14]. For moving objects, such as the finger in the first case, our method estimates more accurate scale and motion changes. For outdoor scenes, our method estimates more accurate scene structure. Notably, our method, like VGGT, can directly output point clouds in world coordinates without requiring postoptimization steps such as Geo4D. Furthermore, our method has much smaller training scale than VGGT, yet exhibits good robustness in dynamic scenes. We attribute this to pre-training knowledge of video diffusion and our proposed training strategy. Figure 11. Qualitative geometric comparison with ST4RTrack [14] on zero-shot generalization. Compared with ST4RTrack, our results show better multi-view consistency, smoother Geometry, and fewer stray spots."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "CUHK(SZ)",
        "HKUST",
        "Monash University",
        "NTU"
    ]
}