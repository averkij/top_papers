{
    "paper_title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
    "authors": [
        "Chuhan Li",
        "Ziyao Shangguan",
        "Yilun Zhao",
        "Deyuan Li",
        "Yixin Liu",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 5 7 0 4 0 . 1 1 4 2 : r M3SCIQA: Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models"
        },
        {
            "title": "Chuhan Li",
            "content": "* Ziyao Shangguan Yilun Zhao"
        },
        {
            "title": "Allen Institute for AI",
            "content": "{chuhan.li.cl2575, ziyao.shangguan}@yale.edu (cid:135) https://github.com/yale-nlp/M3SciQA"
        },
        {
            "title": "Abstract",
            "content": "Existing benchmarks for evaluating foundation models mainly focus on single-document, textonly tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SCIQA, multi-modal, multi-document scientific question answering benchmark designed for more comprehensive evaluation of foundation models. M3SCIQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents primary paper along with all its cited documents, mirroring the workflow of comprehending single paper by requiring multi-modal and multi-document data. With M3SCIQA, we conduct comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis."
        },
        {
            "title": "Introduction",
            "content": "In scientific research, the findings presented in paper often serve as foundation for further investigation. When studying research papers, researchers typically explore related and cited scholarly works to acquire additional context and insights. Simultaneously, research papers are inherently multi-modal, presenting additional and often important insights in the form of figures and tables. Such properties can pose challenges for AI systems *Equal contribution. in accurately interpreting and integrating diverse data formats across multiple research papers. Recent studies have showcased foundation models remarkable performance across variety of tasks in scientific literature understanding, including summarization (Goyal et al., 2023; Liu et al., 2023c), document-based question answering (Newman et al., 2023; Zhao et al., 2024; Xu et al., 2024), and scientific figure question answering (Masry et al., 2022; Yue et al., 2023; Lu et al., 2024b). However, current investigations are mostly confined to single-document or text-only setting, ignoring the multi-modal and multi-document nature of scientific research, where insights are often derived from interpreting interconnected texts, figures, and tables across multiple scholarly works. To address this gap, we introduce M3SCIQA, Multi-Modal, Multi-document Scientific Question Answering benchmark. This benchmark contains 1,452 expert-annotated questions spanning 70 natural language processing (NLP) paper clusters, encompassing 3,066 papers. Each paper cluster comprises of an anchor paper and all its cited papers. Inspired by the common workflow of comparative analysis in scientific research (as illustrated in Figure 1), our benchmark simulates process in which finding, derived from scientific image in the anchor paper, prompts further investigation into specific referenced paper. This simulation enriches the benchmark by requiring the models to engage in cross-referencing among related documents, setting new testbed for evaluating foundation models in scientific documents understanding and reasoning (Section 2.1). We evaluate wide spectrum of open-source and proprietary large language models (LLMs) and large multi-modal models (LMMs). Our experimental results reveal significant limitations in both open-source and proprietary LMMs, particularly in their ability to translate and interpret scientific images and perform effective re-ranking Figure 1: (Top) The common workflow of comparative analysis in scientific research, particularly when result, such as figure/table in the Information Value paper (anchor paper) (Giulianelli et al., 2023), prompts further examination of related research, such as details from DialoGPT (reference paper) (Zhang et al., 2020b). (Bottom) demonstration of the workflow for constructing visual context question, reference-based question, and combined question. based on these images, with the best-performing model, GPT-4o, achieving Mean Reciprocal Rank (MRR) of 0.488 compared to human expert score of 0.796, corresponding to performance gap of 0.308. Similarly, we observe that both open-source and proprietary LLMs struggle with long-range retrieval tasks, specifically with extracting and analyzing information from one or more academic documents. Here, the best-performing model, Command R+, achieves an accuracy score of 33.25 compared to an human expert accuracy score of 76.561. These findings underscore the challenges that current models face in handling complex, multi-modal, multi-document, and domain-specific information. Our main contributions are as follows: We introduce M3SCIQA, comprehensive benchmark designed to evaluate the multi-modal reasoning abilities in interpreting multiple scientific documents. We conduct an extensive evaluation covering 1Human expert performance is assessed in the setting where the correct reference paper is known. wide range of LMMs and LLMs. Our experimental results reveal noticeable performance gap between foundation models and human experts. To better understand the limitations of current foundation models, we conduct detailed analysis of scientific figure information retrieval, longcontext re-ranking, and long-range retrieval, providing valuable insights for future advancements of foundation models."
        },
        {
            "title": "2.1 Overview of M3SCIQA",
            "content": "Our objective is to develop challenging yet realistic QA benchmark that necessitates both multimodal and multi-document reasoning over scientific papers. To achieve this objective, we define two types of intermediate questions in our question construction pipline: Visual Context Question: question derived from figure or table of an anchor paper, with its answer pointing to reference paper. Each Figure 2: An overview of M3SCIQA question construction pipeline. figure or table can correspond to multiple visual context questions. Reference-based Question: question regarding specific detail in the reference paper. Each visual context question can correspond to multiple reference-based questions. The final combined questions are created by combining each visual context question with each of its related reference-based questions. The overview of this pipeline is shown in Figure 2. In constructing M3SCIQA, expert annotators are tasked with composing visual context questions from the 70 curated anchor papers, adhering to four pre-defined reasoning categories: comparisons, data extraction, locations, and visual understanding (Table 6 in Appendix A.2). By answering visual context question, expert annotators can pinpoint reference paper that provides further elaboration on the topic from among all the publications cited by the anchor paper. Subsequently, GPT-42 is employed to generate reference-based questions from the identified reference paper. GPT-4 is utilized again to rephrase and combine each visual context question with each of the related reference-based questions to form comprehensive question that embodies both multi-modal and multi-document reasoning. Finally, expert annotators are tasked with verifying the quality of these GPT-4-assisted questions. Statistics of the benchmark are listed in Table 1; distributions of reasoning types across visual context and reference-based questions are illustrated in Figure 3."
        },
        {
            "title": "2.2 Benchmark Construction Principles",
            "content": "To bridge the gap in current benchmarks that separately assess either multi-modal or multi-document reasoning, our benchmark, M3SCIQA, aims to encompass both elements in single QA pair. Therefore, our benchmark construction pipeline adheres (1) it includes dito the following guidelines: verse modalities, such as texts, figures (including line plots, bar plots, scatter plots, etc.), and tables 2gpt-4-0125-preview (stored as images to preserve format integrity rather than as plain texts); (2) it necessitates connecting information across multiple documents; (3) it spans variety of reasoning types, including four types of visual context reasoning and five types of reference-based reasoning; (4) it poses significant challenges in both multi-modal comprehension and multi-document information retrieval; and (5) it generates realistic QA pairs that reflect the workflows common in scientific literature analysis."
        },
        {
            "title": "2.3 Benchmark Construction",
            "content": "Expert Annotators. We recruit three computer science graduate students with expertise in the field of NLP, each of whom have authored at least one peer-reviewed publication in top-tier NLP conferences. Their responsibilities include: (1) curating anchor papers from pool of candidates and composing visual context questions; (2) reviewing and verifying the reasoning types of reference-based questions; (3) resolving discrepancies between answers generated from the two rounds of referencebased answer generation; and (4) checking consistency, clarity, and redundancy in the combined questions. Further details on annotations are provided in Appendix B. Anchor Papers. To mitigate the risk of data contamination, where models might rely on pre-trained knowledge to answer the visual context questions rather than analyzing the provided scientific images, we curate anchor papers from recent NLP conference, EMNLP 2023. Among the 1,047 papers accepted by EMNLP 2023, we select 441 papers that were released on arXiv after October 1st, 2023 as candidate anchor papers. Visual Context QAs from Anchor Papers. Two of the expert annotators curate 70 papers by manually examining 441 candidate anchor papers collected. Subsequently, they select 21 figures and 62 tables from the 70 papers to compose 300 visual context questions and answers that conform to four visual reasoning types. The ground truth answer to each visual context question is the single refFigure 3: Distribution of reasoning types of visual context and reference-based questions in M3SCIQA. erence paper to which the visual context question directly refers. This facilitates transition from an anchor paper to reference paper that elaborates on the subject. The third annotator is responsible for validating the accuracy and relevance of these questions and answers. 371 papers are excluded in this process because they either lack figures or tables that can be analyzed by one of the reasoning types, or transition to cited paper that is not available on arXiv. Furthermore, due to the occurrence of identical answers among some visual context questions, these 300 questions correspond to only 107 reference papers. Reference-Based QAs from Reference Papers. By requiring that the 107 reference papers be available on arXiv, we ensure access to their complete content. This enables us to utilize GPT-4 to generate open-ended, reference-based questions from the papers. For each reference paper, we create five questions each corresponding to reasoning type illustrated in Table 7 in A.3. These questions are designed to be answerable in text-only setting, without the need for visual reasoning or OCR. Considering the possibility that GPT-4 may incorrectly categorize the questions, expert annotators manually examine the reasoning types associated with the questions and reassign when necessary. This process yields total of 519 reference-based questions after filtering out duplicates, overly complex questions, questions that do not require specific insights from the paper (e.g., What is the mathematical expression for calculating the F-1 score?), and questions that do not belong to any of the five predefined reasoning types. To establish gold answer for each question, we generate answers in two rounds. In the first round, answers are generated concurrently with the questions. In the second round, the model is prompted to answer the questions using the reference paper as context. We employ GPT-4 to determine whether the answers from both rounds are consistent. If any discrepancy is identified, expert annotators are enlisted to verify Statistics Avg. Value Visual Context Question Length (tokens) Reference-based Question Length (tokens) Combined Question Length (tokens) Answer Length (tokens) # Cluster # Anchor Paper per Cluster # Reference Paper per Cluster Paper Length (tokens) Validation Set Size Test Set Size 12.9 25.95 41.3 24. 70 1 42.8 11.8K 452 1000 Table 1: Key statistics of the M3SCIQA benchmark. and finalize the answers. Further details can be found in Appendix B.5. Combined Questions. We utilize GPT-4 to compile the final questions for the benchmark by combining each visual context question with its corresponding reference-based questions. After the combination, expert annotators are tasked with verifying the question validity and rephrasing the GPT-4-assisted combined question when necessary. Overall, we form 1,452 combined questions, each associated with specific figure or table. Expert annotators then review these combined questions to ensure that each visual context question and its corresponding reference-based questions are logically connected and relevant. They also check for clarity, consistency, and redundancy to maintain the quality and difficulty of the benchmark."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate 18 foundation models, including both open-source and proprietary LMMs and LLMs. For each model, we select the most recent, largest, and best-performing checkpoint as of April 15th, 2024. The evaluation of the M3SCIQA benchmark is structured into two distinct stages: visual context evaluation and reference-based evaluation. Modality Reasoning Type Model Expert Performance Random Table Figure COM DE 0.678 0.134 0.765 0.106 0.751 0.134 0.872 0. LOC 0.711 0.110 VU All 0.732 0.111 0.796 0. Simple Baselines text-embedding-3-large text-embedding-3-small text-embedding-ada-002 Contriever BM25 0.321 0.223 0.185 0.165 0.138 0.239 0.205 0.168 0.229 0.098 0.267 0.221 0.200 0.196 0.118 0.323 0.223 0.171 0.144 0. 0.384 0.267 0.224 0.274 0.160 Open-Source Large Multi-modal Models (LMMs) InternVL-Chat-V1.1 Yi-VL-34B Qwen-VL-Plus LLaVA-1.6 DeepSeek-VL GPT-4o GPT-4V(ision) Claude-3-Sonnet Claude-3-Opus Gemini-Pro-Vision-1.0 Claude-3-Haiku 0.168 0.105 0.065 0.079 0.075 0.084 0.057 0.131 0.000 0. 0.136 0.101 0.077 0.088 0.064 0.153 0.088 0.053 0.044 0.081 0.170 0.080 0.148 0.052 0.109 Proprietary Large Multi-modal Models (LMMs) 0.520 0.440 0.385 0.256 0.217 0.189 0.454 0.309 0.369 0.343 0.188 0. 0.443 0.383 0.357 0.320 0.196 0.194 0.565 0.407 0.363 0.362 0.160 0.201 0.570 0.523 0.395 0.301 0.284 0.130 0.218 0.138 0.096 0.142 0.110 0.109 0.086 0.136 0.000 0.070 0.418 0.288 0.422 0.204 0.195 0. 0.297 0.217 0.180 0.184 0.127 0.144 0.091 0.089 0.056 0.079 0.500 0.400 0.374 0.316 0.197 0.188 Table 2: Mean reciprocal rank (MRR) on the test set of M3SCIQA. The best-performing model in each category is bold, and the second best is underlined. Reasoning types: COM: comparison, DE: data extraction, LOC: location, VU: visual understanding. Model Context Window CU II RDI MA CA All Expert Performance 72.32 71.11 83.15 76.84 79.17 76. Command R+ Llama-3-70B Mistral-7B PaLM-2 DBRX Gemma-7B Open-Source Large Language Models (LLMs) 128,000 8192 32,768 36,864 32,768 8,192 40.00 31.35 17.10 20.73 18.13 8.89 22.73 35.23 24.09 26.42 19.43 15.15 33.33 22.84 8.89 16.35 13.94 1. 37.91 32.49 25.81 27.65 21.30 13.95 Proprietary Large Language Models (LLMs) GPT-3.5 GPT-4 Claude-3-Haiku Claude-3-Sonnet Claude-3-Opus Gemini-Pro-1.0 16,385 128,000 200,000 200,000 200,000 30,720 22.22 31.11 28.89 25.56 26.67 18.89 33.33 21.21 30.88 21.21 18.18 19. 19.44 23.61 12.50 19.44 20.83 18.06 32.56 32.56 29.07 25.58 26.74 22.09 39.53 35.19 26.72 26.72 22.63 20.93 37.21 31.40 38.10 38.37 30.23 29.07 33.25 31.30 20.45 23.55 19.05 12.25 29.00 28.50 28.25 26.50 25.00 21. Table 3: LLM-based accuracy score on the test set of M3SCIQA in retrieval setting from GPT-4os ranking. The best-performing model in each category is bold, and the second best is underlined. Human expert performance is assessed in an oracle setting, where the correct reference paper is pre-identified. Reasoning types: CU: conceptual understanding, II: implications and Inferences, RDI: results and data interpretation, MA: methodological analysis, CA: critical analysis. : Due to budget constraints, we randomly sampled 200 instances from the test set for evaluation."
        },
        {
            "title": "3.1 Visual Context Evaluation",
            "content": "Task Formulation. The visual context evaluation with LMMs is defined as follows: Given visual context question Qvis, its correspondent scientific image I, and list of reference papers = {d1, d2, , dn}, the objective is to determine ranking of these papers based on their relevance to the question and the image. This ranking is represented by = {r1, r2, , rn}, where ri denotes the ranking of the paper di for each index {1, 2, , n}. We input Qvis, and into each LMM, denoted by fLM , and instruct it to generate ranking of based on their relevance to Qvis and I: = fLM (Qvis, I, d1, d2, , dn) For comparative analysis, simple baselines presented in Table 2 are also assessed for the ranking task. Other than BM25, these baselines employ cosine similarity between query and document embeddings to rank documents. Each query combines the visual context question Qvis and its image caption generated by GPT-4o with one of the documents, represented by its title and abstract. Given visual context question Qvis, its correspondent scientific image I, list of reference papers = {d1, d2, , dn}, an embedding model Embed, and cosine similarity function sim, the ranking process is defined as below: = GPT-4o(I) = Embed(concat(Qvis, C)) di D, hi = Embed(di) = sort(sim(q, h1), , sim(q, hn)) Evaluation Protocol. At the visual context evaluation stage, we assess LMMs ability to accurately retrieve and rank the correct reference paper from complete list of reference papers. Performance is measured using an established information retrieval metric, Mean Reciprocal Rank (MRR), which effectively gauges models ability to identify and prioritize the most relevant reference paper. We also calculate Recall@k and nDCG@k to further analyze LMMs retrieval effectiveness, with results detailed in Table 8 and 9 in Appendix D. Experiment Setup. This stage involves five open-source LMMs, including open-source models, such as LLaVA 1.6 (Liu et al., 2023a), InternVL-Chat-1.1V (Chen et al., 2024), Yi-VL34B (AI et al., 2024), DeepSeek-VL (Lu et al., 2024a), and Qwen-VL-Plus (Bai et al., 2023); six proprietary LMMs, including GPT-4V(ision) (OpenAI, 2024a), GPT-4o (OpenAI, 2024b), Claude 3 Haiku (Anthropic, 2024), Claude 3 Sonnet (Anthropic, 2024), Claude 3 Opus (Anthropic, 2024), and Gemini Vision Pro 1.0 (Team, 2023); and five simple baselines, including BM25, Contriever (Izacard et al., 2021), and OpenAI Embeddings3 (Large, Small, and Ada)."
        },
        {
            "title": "3.2 Reference-Based Evaluation",
            "content": "Task Formulation. The reference-based evaluation is defined as follows: Given combined question Qcomb and ranking of the reference papers obtained in the visual context evaluation stage, the objective is to answer the question based on the top ranked paper in R, denoted by Topk(R) = {R[1], R[2], , R[k]}. Since combined questions contain elements from both visual context and reference-based questions, we instruct LLMs to solely concentrate on the reference-based aspect of Qcomb. The prompts used for this instruction are detailed in Table 15 in Appendix E.3. Accordingly, we input Qcomb and Topk(R) into LLMs, denoted by fLLM , and instruct LLMs to answer Qcomb based on the textual content in top ranked papers: Ans = fLLM (Qcomb, R[1], R[2], , R[k]) Evaluation Protocol. At the reference-based evaluation stage, we assesses how LLMs perform on reference-based questions using the top three ranked papers identified from the visual context evaluation stage as context. Specifically, these papers are ranked by GPT-4o, which is highlighted as the most effective retrieval model in Table 2. GPT4o achieves an MRR of 0.488, suggesting that the correct reference paper typically appears in the 2.1th position, placing it within the top three ranked papers on average. Given that both reference-based question and answer generation utilize plain text extracted from TeX files, we employ the same parsed TeX files as input for LLMs to solve the text-only, reference-based questions. Generative Response Metrics. Following effectiveness of LLMs in evaluating the quality of short AI-generated responses (Wang et al., 2023; Lu et al., 2024b; Dubois et al., 2024; Wang et al., 2024), we utilize strong LLM-evaluator (GPT4) to evaluate the quality of responses generated in the reference-based evaluation stage. Specifically, the LLM-evaluator rates answers generated against the gold answers using scoring scale of 0, 0.5, and 1. To more closely align our scoring scale with expert assessments, we compute Cohens Kappa (McHugh, 2012) to assess the agreement between 3https://platform.openai.com/docs/guides/ embeddings the LLM-evaluator and expert annotators. This comparison is conducted for both the 0-0.5-1 and the 1-2-3-4-5 scales, with prompts utilized for evaluation provided in Table 16 in Appendix E.1. Expert annotators are tasked with rating 200 responses from four different LLMs (Command R+, GPT-4, Mistral, and Gemma) using both scales. Our calculations reveal Cohens Kappa value of 0.520 for the 0-0.5-1 scale and 0.444 for the 1-2-3-4-5 scale. These results demonstrate greater consistency with expert evaluations when using the 00.5-1 scale. Further details and comparative results are presented in Appendix E.1. Thus, we adopt the 0-0.5-1 scoring scale for our evaluations. Additionally, we employ established metrics such as ROUGE (Lin, 2004), BERTScore (Zhang et al., 2020a), and AutoACU (Liu et al., 2023b) to further gauge the quality of the generated responses. Detailed results are provided in Table 10, 11, 12 in Appendix D. Experiment Setup. This stage involves six opensource Text-Only LLMs, including Mistral-7B (Jiang et al., 2023), Llama-3-70B (Meta, 2024), DBRX (Databricks, 2024), PaLM-2 (Anil et al., 2023), Gemma (Team et al., 2024), and Command R+ (CohereForAI, 2024); and six proprietary LLMs, including GPT-3.5 (OpenAI, 2022), GPT4 (OpenAI, 2024a), Claude 3 Haiku (Anthropic, 2024), Claude 3 Sonnet (Anthropic, 2024), Claude 3 Opus (Anthropic, 2024), and Gemini-Pro-1.0 (Team, 2023)."
        },
        {
            "title": "3.3 Main Results",
            "content": "Table 2 and Table 3 present our main results for both open-source and proprietary LMMs and LLMs on the validation and test set of M3SCIQA, focusing on visual context and reference-based questions, respectively. We summarize our key findings as follows: Finding 1: Challenges in Visual Reasoning and Paper Ranking with M3SCIQA. Table 4 provides breakdown of GPT-4os performance in answering the visual context questions, categorized by both reasoning and ranking outcomes. Despite being the overall best-performing retriever, GPT-4o still struggles with the dual challenges: it fails to correctly interpret 42.4% of the scientific images; even when it does produce correct visual reasoning, it falls short in ranking the associated paper within the top three choices. Notably, one interesting error pattern is the scenario reasoning Reasoning Correctness Ranking@Top3 Percentage 33.0% 24.7% 19.7% 22.7% Table 4: Performance distribution for GPT-4o on visual context questions, categorized by Reasoning Correctness and Ranking@Top3. ranking@top3, which accounts for 19.7% of the cases for GPT-4o. While this type of error occurs in both open-source and proprietary LMMs, it is more prevalent in the former. Example error analyses are presented in Figure 4, offering more granular view of these patterns and specific instances where the model underperforms. Finding 2: Inherent Limitations of Open-Source LMMs in Long-Range Ranking Task. The performance of open-source LMMs in long-range ranking tasks is significantly hindered by their fundamental limitations. We identify three primary challenges: (1) Limited Context Window, which necessitates division of large paper clusters into smaller segments, complicating the ranking process and potentially omitting relevant reference papers; (2) Hallucinations, characterized by the erroneous generation and prioritization of irrelevant arXiv webpage URLs, professional NLP terms, repetitive paper IDs, and random numerical values; (3) Formatting Issues, where models disregard specified format and list papers in plain text, complicating the integration of results across rankings from segmented paper clusters. These challenges significantly impede the models ability to provide comprehensive evaluation of their visual reasoning capabilities, suggesting the need for improvements in their basic functionality to handle more complex reasoning and ranking tasks. detailed evaluation of open-source LMMs is presented in Appendix F. Finding 3: Precision-Recall Balance. We evaluate LLMs in retrieval settings using the top ranked papers from the visual context evaluation performed by GPT-4o for the values {1, 2, 3, 4, 5}. As shown in Figure 5, performance generally increases from = 1 to = 3, aligning with an MRR score of approximately 0.488, which places the correct reference paper in the 2.1th position on average. Beyond this point, as more papers are considered, more noise is introduced. Figure 4: Three examples from GPT-4o in answering visual context questions. The general decline in performance after = 3 demonstrates models limitations in retrieval tasks when given more irrelevant information. leading to disproportionately higher evaluations from the LLM-based evaluator. Models GPT-4 GPT-3.5 Llama 3 70B Mistral title-only 7.50 retrieval 28.50 (+21.00) 13.50 29.00 (+15.50) 19.75 28.25 (+8.50) 22.00 19.25 (-2.75) Table 5: Performance of four LLMs in answering reference-based questions in title-only and retrieval setting. Figure 5: Performance scores of Mistral, Llama 3 70B, GPT-3.5, and GPT-4 in different retrieval settings. Finding 4: Challenges in Instruction Compliance for LLMs in Retrieval Task. Our evaluation of four models in both title-only setting, where only the title of the reference paper is provided, and retrieval setting, with the top three ranked papers by GPT-4o, highlights variations in instruction compliance. Models are instructed to answer dont know if definitive answer cannot be derived from the given information. This directive tests the models adherence to instructions, since the task is infeasible with the titles alone and compliant models should exhibit minimal performance. Transition to the retrieval setting should reveal significant increase for the models, as observed with GPT-4 in Table 5. Additionally, employing LLM-based evaluator to assess generative response overlooks models confidence levels. Less compliant models, relying on pre-trained knowledge, often produce tangentially relevant responses rather than the instructed dont know,"
        },
        {
            "title": "4 Related Work",
            "content": "Multi-Modal QA. Multi-modal QA datasets have posed visual reasoning challenges for LMMs. Initially, the focus of benchmarks (Lin et al., 2015; Mobasher et al.; Yagcioglu et al., 2018; Talmor et al., 2021; Lu et al., 2022; Chang et al., 2022; Li et al., 2023; Liu et al., 2023d; Yu et al., 2023) was on conducting QA tasks over simple images, primarily addressing questions such as understanding objects in an image and performing single-hop reasoning. Recently, more complex and nuanced benchmarks (Chen et al., 2022; Lu et al., 2024b) have emerged beyond the scope of understanding simple images to require complex mathematical reasoning over diagrams and plots. Beyond the scope of mathematical reasoning, MMMU (Yue et al., 2023) requires more complex visual reasoning in diverse range of subjects including science, humanities, and engineering. Document QA. Document QA is crucial in the field of NLP, focusing on extracting, synthesizing, and analyzing information from structured and unstructured documents. Early document QA benchmarks (Rajpurkar et al., 2016; Bajaj et al., 2018; Yang et al., 2018) involved short document QA, where questions were posed based on content from web pages such as those in Bings search results or Wikipedia articles. Scientific paper QA benchmarks (Dasigi et al., 2021; Lee et al., 2023) require LLMs to conduct multi-hop reasoning and longcontext information processing. However, notable gap exists in the integration of Multi-modal QA with Document QA, particularly in the context of scientific research, where it encompasses blend of textual and visual data alongside complex textual information. M3SCIQA, bridging this gap, is benchmark for evaluating foundation models abilities in both multi-modal and multi-document reasoning."
        },
        {
            "title": "5 Conclusion",
            "content": "Existing scientific QA benchmarks often overlook the complexity of real research workflows, which require interpreting non-textual data and aggregating information from multiple documents. To bridge this gap, we present M3SCIQA, novel multi-modal multi-document scientific QA benchmark designed to evaluate foundation models. Our evaluation and analysis underscore the challenges LMMs face in scientific diagram understanding and long-range information ranking tasks, highlighting the limitations of current models in handling complex scientific documents. We hope this work paves the way for advancements in multi-modal and longdocument understanding."
        },
        {
            "title": "Limitations",
            "content": "Furthermore, as discussed in Section 3.3, prompting an LMM with set of possible reference papers may be suboptimal due to the challenges models face in ranking large number of papers. An alternative approach could involve assessing the relevance of each paper individually by encoding the paper into textual embedding, then comparing it with the textual embedding with of the visual context question combined with the image representation of the figure. This method could potentially alleviate the challenges of requiring an LMM to sift through large set of possible reference papers and would be an interesting area for future research. Additionally, our approach to ranking papers for certain models, in particular BM25 and Contriever, involves using GPT-4os textual descriptions of images rather than its direct image embedding, which might not accurately capture the nuances of scientific images. Current image embedding models such as LLaVA (Liu et al., 2023a) and CLIP (Radford et al., 2021), while proficient with natural images, are not trained on scientific images. Developing specialized LMM trained specifically on scientific images (Li et al., 2024; Wu et al., 2024) could potentially enhance its performance in interpreting scientific plots, figures, and tables, thereby improving its potential usage in scientific applications."
        },
        {
            "title": "Acknowledgements",
            "content": "This project was supported in part by Tata Sons Private Limited, Tata Consultancy Services Limited, and Titan. We are grateful for the compute support provided by Microsoft Researchs AFMR program. We thank Xinyi Han, Zhongjie Wu, and Amy Zhao for their help in initial stages of this project. The evaluations presented in this study are met with certain limitations due to inherent disparities in the context window of current open-source and proprietary LLMs and LMMs. There is significant difference in context window length between models such as GPT-4 Turbo and Claude-3, which can rank all papers in paper cluster, and models such as InternVL-Chat-V1.1 and QwenVL, which are restricted to handling only two to eight papers in single prompt. This discrepancy may lead to an unfair comparison of their capabilities. Future work could focus on standardizing or extending the context windows in LMMs to mitigate this issue."
        },
        {
            "title": "References",
            "content": "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. Preprint, arXiv:2403.04652. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. Preprint, arXiv:2305.10403. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. Preprint, arXiv:2308.12966. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Preprint, arXiv:2312.14238. CohereForAI. 2024. CommandR+. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. dataset of information-seeking questions and answers anIn Proceedings of the chored in research papers. 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, Online. Association for Computational Linguistics. Databricks. 2024. Dbrx. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2024. Alpacafarm: simulation framework for methods that learn from human feedback. Preprint, arXiv:2305.14387. Mario Giulianelli, Sarenne Wallbridge, and Raquel Fernández. 2023. Information value: Measuring utterance predictability as distance from plausible alternatives. Preprint, arXiv:2310.13676. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3. Preprint, arXiv:2209.12356. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. Ms marco: human generated machine reading comprehension dataset. Preprint, arXiv:1611.09268. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae Lee. 2023. QASA: Advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1903619052. PMLR. Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. Preprint, arXiv:2109.00590. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking multimodal llms with generative comprehension. Preprint, arXiv:2307.16125. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. 2022. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. Preprint, arXiv:2105.14517. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. Preprint, arXiv:2403.00231. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Mary McHugh. 2012. Interrater reliability: The kappa statistic. Biochemia medica : ˇcasopis Hrvatskoga društva medicinskih biokemiˇcara / HDMB, 22:276 82. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2015. Microsoft coco: Common objects in context. Preprint, arXiv:1405.0312. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Preprint, arXiv:2304.08485. Yixin Liu, Alexander Fabbri, Yilun Zhao, Pengfei Liu, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023b. Towards interpretable and efficient automatic reference-based summarization evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1636016368, Singapore. Association for Computational Linguistics. Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. 2023c. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. Preprint, arXiv:2311.09184. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023d. Mmbench: Is your multi-modal model an all-around player? Preprint, arXiv:2307.06281. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. 2024a. Deepseek-vl: Towards real-world vision-language understanding. Preprint, arXiv:2403.05525. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024b. Mathvista: Evaluating mathematical reasoning of In Interfoundation models in visual contexts. national Conference on Learning Representations (ICLR). Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS). Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. Preprint, arXiv:2203.10244. Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Accessed: 06/13/2024. Shaghayegh Mobasher, Ghazal Zamaninejad, Maryam Hashemi, Melika Nobakhtian, and Sauleh Eetemadi. Parsvqa-caps: benchmark for visual question answering and image captioning in persian. Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. 2023. question answering framework for decontextualizing user-facing snippets from scientific documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31943212, Singapore. Association for Computational Linguistics. OpenAI. 2022. Introducing chatgpt. OpenAI. 2024a. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2024b. Hello gpt-4o: Were announcing gpt4o, our new flagship model that can reason across audio, vision, and text in real time. Accessed: 06/13/2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Preprint, arXiv:2103.00020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. Preprint, arXiv:1606.05250. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. Preprint, arXiv:2104.06039. Gemini Team. 2023. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Preprint, arXiv:2403.08295. Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2023. Evaluating open-qa evaluation. Preprint, arXiv:2305.12421. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Preprint, arXiv:2406.18521. Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and Chenghua Lin. 2024. Scimmir: Benchmarking scientific multi-modal information retrieval. Preprint, arXiv:2401.13478. Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, and David Wadden. 2024. Kiwi: dataset of knowledge-intensive writing instructions for answering research questions. Preprint, arXiv:2403.03866. Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. 2018. RecipeQA: challenge dataset for multimodal comprehension of cooking recipes. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 13581368, Brussels, Belgium. Association for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. Preprint, arXiv:2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Preprint, arXiv:2311.16502. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020b. Dialogpt: Large-scale generative pre-training for conversational response generation. Preprint, arXiv:1911.00536. Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2024. DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1610316120, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "13\nA.1 Visual Context Reasoning Definition 13\nA.2 Visual Context Reasoning Examples 13\nA.3 Reference-based Reasoning Defi-\n. . . . . . .",
            "content": "nition and Examples"
        },
        {
            "title": "B Expert Annotation Details",
            "content": "B.1 Expert Annotation for Visual Context Questions . . . . . . . . . . . B.2 Bias Mitigation for Visual Context Questions Annotation . . . . . . . B.3 Expert Annotation for Referencebased Questions . . . . . . . . . . B.4 Expert Annotation for Referencebased Reasoning . . . . . . . . . B.5 Expert Annotation for Reference- . . . . . . . . . . based Answers"
        },
        {
            "title": "C More Dataset Analysis",
            "content": "13 13 13 13"
        },
        {
            "title": "D More Result Analysis",
            "content": ". . . . . . . . . . . text Question . based Question . More Details On the Setup E.1 LLM-Based Evaluator. . . . E.2 Prompt for Evaluating Visual Con- . . E.3 Prompt for Answering Reference- . . E.4 Prompt for Answer Evaluation . . E.5 Prompt for Reference-based Ques- . . . E.6 Model Parameters for Answering . Visual Context Question . E.7 Model Parameters for Answering . . . Reference-based Question . tion Generation . . . . . . . . . . . . . . . . . Comparative Study of LMMs in Answering Visual Context Questions . InternVL-Chat-1.1V . F.1 . . F.2 Qwen-VL-Plus . . F.3 GPT-4V(ision) . . F.4 Claude-3-Opus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 16 17 17 17 17 17 17 17 21"
        },
        {
            "title": "A Data Collection Guidelines",
            "content": "A.1 Visual Context Reasoning Definition Four visual context question reasoning types are defined in Table 6. A.2 Visual Context Reasoning Examples Four visual context reasoning types examples are shown in Figure 6. A.3 Reference-based Reasoning Definition and Examples Five reference-based question reasoning types and examples are defined in Table 7."
        },
        {
            "title": "B Expert Annotation Details",
            "content": "B.1 Expert Annotation for Visual Context"
        },
        {
            "title": "Questions",
            "content": "We employed three computer science graduate students for annotating 300 visual context questions. Being provided with the full list of EMNLP 2023 papers, they were required to: (1) check that each anchor paper has arXiv documentation; (2) find figures or tables that contain comparative information with potential reasoning types described in Table 6; (3) find the potential reference paper in the figure or table and ensure that it has arXiv documentation; and (4) write the visual context question. When they choose figure or table, they were required to fill in the corresponding visual context reasoning type as well as the direct answer to the visual context question. B.2 Bias Mitigation for Visual Context"
        },
        {
            "title": "Questions Annotation",
            "content": "In preparation for the main annotation process, we conduct pilot annotation stage where 20 papers where sampled. Annotators are instructed to generate three distinct questions per paper. These questions are subsequently analyzed by the authors and categorized into four distinct reasoning types: comparison, data extraction, location, and visual understanding. These categories are comprehensive for scientific image understanding. By following the predefined reasoning type definitions in Table 6, we mitigate the risk of annotator bias driven by their own preferences. Additionally, these reasoning types are not specific to NLP and are carefully chosen such that they are applicable in analyzing scientific images in the broader scientific fields. B.3 Expert Annotation for Reference-based"
        },
        {
            "title": "Questions",
            "content": "We require each reference paper to have arXiv documentation. Then, we use the arXiv downloader to obtain the full text of the reference paper and generate subsequent reference-based questions (along with answers, explanations, and evidence) using the prompts described in Section E.5. We test these questions in the oracle setting, use GPT-based evaluators to evaluate if the answer generated in the oracle setting matches the answer generated along with the question. If they do not match, expert annotators proceeded to manually examine these questions and re-write the answers. B.4 Expert Annotation for Reference-based"
        },
        {
            "title": "Reasoning",
            "content": "In Section E.5, we automatically assign reasoning types concurrently with the generation of referencebased questions. To ensure the quality of the generated questions, we prompt GPT-4 with the question and its assigned reasoning type to ask if the question matches the reasoning type. For every question that GPT-4 flags as not matching the assigned reasoning type, expert annotators were instructed to manually examine the reasoning types and correct them when necessary. Figure 6: Examples of four visual context reasoning categories in M3SCIQA. Visual Context Reasoning Description Comparison It focuses on evaluating and contrasting information presented in tables, figures, or other data formats. To answer questions of this type, one must analyze and compare specific subjects or variables within the given dataset. Data Extraction It directly retrieves specific information from table or figure. This approach focuses on pinpointing exact data points or details. Location It is centered on pinpointing spatial or positional information from table or figure. This involves identifying either relative or absolute locations, such as the placement of items in figure or row information in table. Visual Understanding It emphasizes understanding visual information from the figure, such as colors, shapes, and marker types. This approach involves analyzing and extracting visual information. Table 6: Definitions of four visual context reasoning categories in M3SCIQA. Reference-based Reasoning Description & Example Conceptual Understanding Methodological Analysis Results and Data Interpretation Implications and Inferences Critical Analysis Evaluate knowledge of essential concepts, basic theories, and critical definitions related to the subject. Example: What does the hypernetwork in the proposed Hyperdecoders approach generate? Examine and assess the research methodologies and experimental frameworks employed in studies, with an emphasis on their efficacy and constraints. Example: What potential application of the Hyperdecoder approach is suggested by its performance on long-context out-of-domain datasets in the MRQA evaluation? Analyze statistical data, graphs, and tables, focusing on deriving significant insights and conclusions from quantitative and visual information. Example: In the experimental results for the GLUE benchmark using T5large v1.1 + LM as the underlying model, which model configuration achieved the highest average score across tasks? Infer wider implications and practical uses of study outcomes, concentrating on the extensive impact and prospective significance of the results. Example: How does the exponentially weighted pooling method in CET ensure that every embedding receives sufficient training? Assess the studys reasoning, robustness of evidence, and validity of conclusions critically, with focus on logical consistency and the support of empirical data. Example: How does the unified frameworks approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models? Table 7: Definitions of five reasoning categories in M3SCIQA. B.5 Expert Annotation for Reference-based"
        },
        {
            "title": "Answers",
            "content": "Following the two-round answer generation process mentioned in Section 2.3, we manually checked 100 questions for which the first and second round answers matched in order to ensure the gold answers were indeed correct. Out of the 100 sampled questions, 96 questions were marked as correct by expert annotators, demonstrating the high-quality of M3SCIQA benchmark."
        },
        {
            "title": "C More Dataset Analysis",
            "content": "Question Distribution. As illustrated in Table 1, the average question length in M3SCIQA is 41.27 (in tokens), while the maximum number of tokens in question is 78 (in tokens). Figure 7 further illustrates the distribution of token counts in all visual context, reference-based, and combined questions, highlighting the diverse distribution of all three types of questions. In these figures, the red solid line represents the median and the blue dashed line represents the mean. From all three distributions, we note that the median and mean are very close in values, implying our dataset is symmetric or only slightly skewed."
        },
        {
            "title": "D More Result Analysis",
            "content": "Recall@k for Visual Context Evaluation. In addition to the MRR values shown in Table 2, Recall@k is illustrated in Table 8. nDCG@k for Visual Context Evaluation. In addition to the MRR values shown in Table 2, nDCG@k is illustrated in Table 9. Standard Metrics for Reference-based Evaluation. In addition to the LLM-based accuracy results shown in Table 3, ROUGE scores are illustrated in Table 10; AutoACU scores (Liu Model Recall @1 Recall @ Recall @5 GPT-4o GPT-4V(ision) Claude-3-Opus Claude-3-Sonnet Claude-3-Haiku Gemini-Pro-Vision-1.0 0.40 0.30 0.20 0.30 0.09 0.12 0.53 0.45 0.33 0.46 0.25 0.21 0.57 0.51 0.44 0.57 0.29 0.26 Table 8: Recall@k Model nDCG @1 nDCG @3 nDCG @5 GPT-4o GPT-4V(ision) Claude-3-Opus Claude-3-Sonnet Claude-3-Haiku Gemini-Pro-Vision-1.0 0.40 0.30 0.20 0.30 0.09 0. 0.51 0.43 0.31 0.44 0.23 0.19 0.53 0.45 0.36 0.49 0.25 0.21 Table 9: nDCG@k et al., 2023b) are illustrated in Table 12; and each BERTScore (Zhang et al., 2020a) is provided in Table 11. Model ROUGEROUGE-2 ROUGE-l Llama-2-70B Mistral-7B PaLM-2 Gemma-7B DBRX Command R+ GPT-4 GPT-3.5 Gemini-Pro-1.0 Claude-3-Haiku Claude-3-Sonnet Claude-3-Opus 0.125 0.182 0.197 0.073 0.155 0.205 0.237 0.208 0.192 0.176 0.184 0.182 0.056 0.086 0.095 0.032 0.075 0. 0.127 0.101 0.104 0.090 0.086 0.087 0.098 0.143 0.157 0.058 0.122 0.176 0.202 0.171 0.162 0.143 0.144 0.140 Table 10: ROUGE score on test set of M3SCIQA in retrieval setting from GPT-4V(ision)s retrieval. The best-performing model in each category is bold, and the second best is underlined."
        },
        {
            "title": "E More Details On the Setup",
            "content": "E.1 LLM-Based Evaluator. Cohens Kappa results are detailed in Table 13, illustrating the level of concordance between expert annotators and LLM-evaluators. Our result reveals Cohens Kappa value of 0.520 for the 0-0.5-1 scale and 0.444 for the 1-2-3-4-5 scale. While the Cohens Kappa value of 0.520 only indicates weak agreement with humans (McHugh, 2012), the 0-0.5-1 scale demonstrates stronger agreement compared to the 1-2-3-4-5 scale. Additionally, the Figure 7: The distribution of the number of tokens per visual context question in M3SCIQAPart 1 of 3. Figure 7: The distribution of the number of tokens per reference-based question in M3SCIQAPart 2 of 3. Figure 7: The distribution of the number of tokens per combined question in M3SCIQAPart 3 of 3. Model Recall Precision F-1 E.4 Prompt for Answer Evaluation Llama-2-70B Mistral-7B PaLM-2 Gemma-7B DBRX Command R+ GPT-4 GPT-3.5 Gemini-Pro-1.0 Claude-3-Haiku Claude-3-Sonnet Claude-3-Opus 0.852 0.855 0.855 0.359 0.721 0.856 0.865 0.861 0.852 0.855 0.856 0.855 0.807 0.832 0.843 0.355 0.698 0.862 0.851 0.842 0.847 0.827 0.829 0.827 0.828 0.843 0.848 0.357 0.709 0.859 0.858 0.851 0.849 0.840 0.842 0. Table 11: BERTScore on test set of M3SCIQA in retrieval setting from GPT-4V(ision)s retrieval. The bestperforming model in each category is bold, and the second best is underlined. Model Recall Precision F-1 Llama-2-70B Mistral-7B PaLM-2 Gemma-7B DBRX Command R+ GPT-4 GPT-3.5 Gemini-Pro-1.0 Claude-3-Haiku Claude-3-Sonnet Claude-3-Opus 0.212 0.176 0.170 0.097 0.164 0.155 0.226 0.195 0.170 0.217 0.215 0.224 0.091 0.104 0.123 0.198 0.131 0.153 0.164 0.124 0.134 0.113 0.010 0.108 0.111 0.109 0.113 0.107 0.111 0. 0.158 0.118 0.123 0.118 0.110 0.116 Table 12: AutoACU (A3CU) score on test set of M3SCIQA in retrieval setting from GPT-4V(ision)s retrieval. The best-performing model in each category is bold, and the second best is underlined. 0-0.5-1 1-2-3-4-5 Expert Annotators 0. 0.444 Table 13: Cohens Kappa between two LLM-evaluators w.r.t. expert annotations. evaluation prompts used for both the 0-0.5-1 and 1-2-3-4-5 scales are provided in Table 16. E.2 Prompt for Evaluating Visual Context"
        },
        {
            "title": "Question",
            "content": "Prompts used to rank reference papers across multiple LMMs are illustrated in Table 14. E.3 Prompt for Answering Reference-based"
        },
        {
            "title": "Question",
            "content": "Prompts used to answer reference-based questions are illustrated in Table 15. Prompts used to retrieve answers from each text chunk and aggregate answers are illustrated in Table 16. E.5 Prompt for Reference-based Question"
        },
        {
            "title": "Generation",
            "content": "We list our prompt for reference-based question generation in Table 17. E.6 Model Parameters for Answering Visual"
        },
        {
            "title": "Context Question",
            "content": "Model parameters for ranking reference papers from paper cluster are shown in Table 18. E.7 Model Parameters for Answering Reference-based Question Model parameters for answering reference-based questions are exhibited in Table 19."
        },
        {
            "title": "Answering Visual Context Questions",
            "content": "In our experiments, we evaluated numerous LMMs in answering visual context questions, such as Kosmos2, Fuyu-8B, and Qwen-VL-Chat. Our findings indicate that these models severely suffer from both hallucination and formatting errors when analyzing the scientific figures. Thus, we conclude that they lack the basic capabilities to generate valid rankings, which are crucial for calculating MRR. F.1 InternVL-Chat-1.1V InternVL-Chat-1.1V operates with short context window, restriction that makes answering visual context questions particularly difficult. Although pairwise paper rankings were still possible within the token length restrictions, prompting the model with the entire list of possible reference paper titles and abstracts was not possible. Since the vanilla singular prompting method used to test other models with larger context windows (e.g. GPT-4V) on the visual context question dataset could not be applied to InternVL-Chat-1.1V, we used slightly different prompting scheme. Three different ranking settings and methodologies were used to determine the rank of the reference paper for each visual context question. In the first setting, the model was repeatedly prompted to compare the true reference paper against each of the other papers one at time in head-to-head ranking. In this setting, we then considered the true Model Prompt Yi-VL-34B DeepSeek-VL InternVL-Chat-1.1V LLaVa-1.6 Qwen-VL GPT-4o GPT-4V(ision) Gemini-Pro-Vision-1.0 Claude-3-Haiku Claude-3-Sonnet Claude-3-Opus Answer the question from the figure and the reference papers provided only: {question} Additionally, rerank the following reference papers according to their relevance to this question. Each reference paper consists of an S2_id, title, and an abstract. {paper_cluster} Format your answer as python dictionary with keys \"question\", \"answer\", and \"rank\". \"rank\" should be list of S2_id. If no relevant reference papers are provided, return an empty list for \"rank\". Note: The \"rank\" list should only include **question-relevant** reference papers. Do not include irrelevant ones. You are given figure, question, and some paper candidates of titles and abstracts. Your task is to answer the question based on the figure information, then order the paper candidates that provide to you so that the paper that is more relevant to the question comes first in the list. Provide your answer at the end in json file of this format using S2_id only:{\"ranking\": [\"\"] }. Make sure the responded list is in valid format and that it only contains the S2_id. Do not include the title or abstract in the answer list. <question> {question} </question> <paper candidates> {paper_cluster} </paper candidates> Answer the question from the figure and the reference papers provided only: {question} Additionally, rerank the following reference papers according to their relevance to this question. Each reference paper consists of an S2_id, title, and an abstract. {paper_cluster} Format your answer as python dictionary with keys \"question\", \"answer\", and \"rank\". \"rank\" should be list of S2_id. If no relevant reference papers are provided, return an empty list for \"rank\". You are given figure, question, and list of paper candidates of titles and abstracts. Your task is to answer the question based on the figure information and then re-rank the list of paper candidates provided to you. Provide your answer at the end in json format using the S2_id only: {\"ranking\": []}. Only include papers that are relevant. Do not include papers that are irrelevant. Make sure the answer list is properly formatted. <question> {question} </question> <paper candidates> {paper_cluster} </paper candidates> Table 14: Prompts used to rank reference papers across multiple LMMs. Stage Prompt Answers from text chunk Answer aggregation Answer the below question about scientific paper. The question is composed of 2 parts, and the second part of the question can be answered from the paper. will provide you with only chunk of paper. Explain your reasoning. Append the answer at the end of the response in json format {answer: }. You should answer the question in short-answer form. Do not provide long answers. If you do not know the answer, respond with {answer: dont know} <QUESTION> {question} </QUESTION> <CHUNK> {chunk} </CHUNK> will provide you with set of answer candidates for question. Aggregate the information from all the candidates and give me one single answer. Note that if one answer candidate is dont know, you can ignore it. Answer the question based on the answer candidates and summarize the final answer into short answer. <QUESTION> {question} </QUESTION> {answer_candidate_list} Table 15: Prompts used to generate and aggregate answers from text chunk. reference papers rank to be one more than the number of papers individually ranked higher than the true reference paper when compared side-by-side. In the second setting, the model was prompted to assign rating to each of the sampled reference papers; the ratings were then sorted to generate final ranking among the papers. Finally, in the third setting, the model randomly paired papers together, with each of the higher ranked papers in each pair considered to be ranked higher than every lower ranked papers. By then iteratively pairing papers among the set of higher-ranked papers and also iteratively pairing papers among all the initially lower-ranked ones, ranking for the true reference paper was generated."
        },
        {
            "title": "Comparing each pair of sample papers requires",
            "content": "Evaluator Prompt LLM-based Evaluator (0-0.5-1 setting) LLM-based Evaluator (1-2-3-4-5 setting) am testing models performance on open-ended questions. want you to help me in checking to see if the candidate answer has the same meaning as the reference answer for given question. If you think the reference answer and the candidate answer have the same meaning, respond {selection: 1}; otherwise, respond by {selection: 0}. If you think the candidate is partially correct, respond by {selection: 0.5}. If the answer is dont know, rate it to 0. <QUESTION> {question} </QUESTION> <REFERENCE> {reference} </REFERENCE> <CANDIDATE> {candidate} </CANDIDATE> am testing models performance on open-ended questions. want you to help me in checking to see if the candidate answer has the same meaning as the reference answer for given question. Rate the candidate answer from 1, 2, 3, 4, and 5, where 1 means the candidate is the least similar to the reference answer and 5 means the candidate matches to the reference answer perfectly. Respond by {selection: }. If the candidate answer is dont know, rate it to 1. Heres some examples you can consider: Question: Why transformer is better than RNN? Reference: Parallel computation Candidate: Computation Rating: 3 Question: Whats the major advantage of using ALiBi positional embedding? Reference: Effectively handle sequences of varying lengths, particularly beneficial for very long sequences Candidate: It has more freedom to handle input Rating: 2 Question: Whats the models performance on GSK8K dataset? Refernece: 65.65% Candidate: 44.56% Rating: 1 Question: What specific method does this paper propose to solve LLM searching problem? Reference: MCTS Candiate: Monte Carlo Tree Search is proposed in this paper to solve searching when using decomposed prompting method. Rating: 5 Question: How does the performance change when we switch from CoT to ToT in prompting? Reference: Accuracy from 23.50% to 32.87% Candidate: slightly increase Rating: 4 <QUESTION> {question} </QUESTION> <REFERENCE> {reference} </REFERENCE> <CANDIDATE> {candidate} </CANDIDATE> Table 16: Prompts used to evaluate answers generated by LLMs. quadratic number of queries to the model, which requires significant amount of time. Each of the three proposed methods, on the other hand, require number of model queries that is linear in the total number of sample references. However, each of the methodologies have their own potential flaws. The first ranking methodology was asymmetric in that the true reference paper was prompted different number of times; thus, for method with no reasoning or retrieval capabilities, the true reference paper would have 1/2n1 chance of being ranked first, while it would have 1/n chance of being ranked first in the ranking mechanism used in larger models, if there are papers to rank. Since MRR heavily favors smaller ranks, the first ranking methodology would bias the observed MRR downward. The second methodology, with zero-shot prompting, was unstable at times; furthermore, the model generally only chose from set of few possible ratings (i.e. 0, 80, 90, or 100 out of 100), making it hard to differentiate and rank papers with the same rating. The third method is symmetric in its prompting but yields different results depending on initial pairings; we randomize the papers when pairing, and so this method is unbiased. We report the MRR values from the third method in Table 2. Detailed results are illustrated in Table 20. Model Prompt Reference-based Question Generation Prompt Generate 1 short answer question based on the papers full content below. You should follow the reasoning type of {reasoning_type}, with the definition {reasoning_description}. The short answer question should be as hard as possible, and focus on single detail from the paper. The target audience of the short answer question is an expert in the field of natural language processing. The question should be hard for GPT-4 to answer. The answer to the question should be short and must be answerable from the content of the paper. Here are some requirements: <REQUIREMENTS> [Question] should make sense and can be answered from the papers full text. [Answer] should be directly answering the question you generated. [Explanation] should explain why the answer correctly answers the question. [Evidence] should be from the original content from the paper content. This should be an excerpt from the input paper that supports your answer. </REQUIREMENTS> Append the answer at the end of your response in json-like format: {question: , answer: , explanation: , evidence:} <PAPER FULL CONTENT> full_text </PAPER FULL CONTENT> Table 17: Prompt for reference-based question generation. Model Generation Setup LLaVa-1. Yi-VL-6B model = llava-v1.6-mistral-7b, temperature = 0.1, max_tokens = 8192 model = Yi-VL-6B, temperature = 0.1, max_tokens = 8192 DeepSeek-VL model = deepseek-vl-7b-chat, temperature = 0.1, max_tokens = 8192 InternVL-Chat-1.1V model = InternVL-Chat-Chinese-V1-1, temperature = 0.1, max_tokens = 768 Qwen-VL-Plus model = qwen-vl-plus, seed = 1234, max_tokens = 6000 GPT-4o model = gpt-4o, temperature = 0.1, max_tokens = 4096 GPT-4V(ision) model = gpt-4-turbo, temperature = 0.1, max_tokens = 4096 Gemini-Pro-Vision-1.0 model = gemini-pro-vision, temperature = 0.1, max_tokens = 4096 Claude-3-Haiku model = claude-3-haiku-20240307, temperature = 0.1, max_tokens = 4096 Claude-3-Sonnet model = claude-3-sonnet-20240229, temperature = 0.1, max_tokens = Claude-3-Opus model = claude-3-opus-20240229, temperature = 0.1, max_tokens = 4096 Table 18: Parameters of various LMMs in evaluating visual context questions. Model Generation Setup Llama-3-70B temperature = 0.1, max_token = 10,000 Mistral-7B temperature = 0.1, max_token = 40,000 PaLM-2 Gemma DBRX temperature = 0.1, max_token = 40,000 temperature = 0.1, max_token = 12,000 temperature = 0.1, max_token = 40,000 Command R+ temperature = 0.1, max_token = 200,000 GPTmodel = gpt-4-0125-preview, temperature = 0.1, max_tokens = 200,000 Gemini-Pro-1.0 model = gemini-1.0-pro, temperature = 0.1, max_tokens = 40,000 Claude-3-Haiku model = claude-3-haiku-20240307, temperature = 0.1, max_tokens = 250,000 Claude-3-Sonnet model = claude-3-sonnet-20240229, temperature = 0.1, max_tokens = 250,000 Claude-3-Opus model = claude-3-opus-20240229, temperature = 0.1, max_tokens = 250,000 Table 19: Parameters of various LLMs in evaluating reference-based questions. Models validation test Method 1 Method 2 Method 0.07 0.218 0.152 0.07 0.186 0.193 Table 20: MRR for InternVL model (percentage) Rank All Rank Valid (53.1%) Rank Ground Truth (5.0%) QwenVL-Plus 0.047 0.089 0. Table 21: MRR for QwenVL-Plus on the test set across 3 evaluation settings. F.2 Qwen-VL-Plus In the visual context evaluation stage, only 53.1% of Qwen-VL-Pluss rankings are valid, with mere 5.0% including the ground truth paper. MRR for QwenVL-Plus is evaluated based on 3 criteria: (1) Rank All, assigning zero value to any invalid rankings; (2) Rank Valid, considering only valid rankings for MRR computation; and (3) Rank Ground Truth, calculating MRR solely from rankings that include the ground truth. Detailed findings are presented in Table 21, though only Rank Valid is reported in Table 3. Additional Error analysis can refer to Figure 8 below. F.3 GPT-4V(ision) See Figure 9 and Figure 10 below. F.4 Claude-3-Opus See Figure 11 and Figure 12 below. Figure 8: Qwen example output on visual context question - Part 1 of 3. Figure 8: Qwen example output on visual context question - Part 2 of 3. Figure 8: Qwen example output on visual context question - Part 3 of 3. Figure 9: GPT-4V(ision) example output 1 on visual context question. Figure 10: GPT-4V(ision) example output 2 on visual context question. Figure 11: Claude-3-Opus example output 1 on visual context question. Figure 12: Claude-3-Opus example output 2 on visual context question."
        }
    ],
    "affiliations": [
        "Yale University"
    ]
}