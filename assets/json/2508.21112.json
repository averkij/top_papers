{
    "paper_title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control",
    "authors": [
        "Delin Qu",
        "Haoming Song",
        "Qizhi Chen",
        "Zhaoqing Chen",
        "Xianqiang Gao",
        "Xinyi Ye",
        "Qi Lv",
        "Modi Shi",
        "Guanghui Ren",
        "Cheng Ruan",
        "Maoqing Yao",
        "Haoran Yang",
        "Jiacheng Bao",
        "Bin Zhao",
        "Dong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 1 1 1 2 . 8 0 5 2 : r EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Delin Qu , Haoming Song , Qizhi Chen , Zhaoqing Chen , Xianqiang Gao , Modi Shi, Guanghui Ren September Maoqing Yao, Bin Zhao, Dong Wang http://eo-robotics.ai/eo-1 https://github.com/eo-robotics https://huggingface.co/IPEC-COMMUNITY The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models. 1. Introduction Generalist robot policies with open-world capabilities is essential for deploying autonomous robots in real scenarios, where they need to tackle diverse tasks, follow various instructions, adapt to different situations, and even handle unforeseen events. In the realm of digit domains, the paradigm of learning from large-scale multimodal datasets has demonstrated broad proficiency and generalization across various interactive and assistive applications. Extending this idea to the physical world, researchers are now training large vision-language-action models on extensive robotic datasets to realize generalist robots with similar versatility. Although recent efforts have demonstrated impressive progress in achieving dexterous manipulation and long-horizon physical control within constrained environments, open world generalization remains the central challenge for developing general-purpose autonomous AI agents in the physical world (Black et al., 2025, Gemini-Robotics, 2025). To address this challenge, robots must be capable of acquiring comprehensive world knowledge, engaging in human-level reasoning, and executing dexterous actions. Early generalist robot policies (Kim et al., 2024, Black et al., 2024, Pertsch et al., 2025) primarily extend visionlanguage models (VLMs) into vision-language-action (VLA) models with domain-specific robotic data, either through auto-regressive decoding of discrete action tokens or by incorporating additional continuous flow matching modules. However, because these VLA models are trained exclusively on robotic datasets, they are restricted to narrow task domains and specific environments. As result, they suffer from diminished general semantic knowledge inherited from VLMs and exhibit limited instruction-following capabilities. EO-Robotics Team, Shanghai AI Laboratory; Fudan University; AgiBot; Northwestern Polytechnical University EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Recently, several studies (Black et al., 2025, Driess et al., 2025, Lin et al., 2025) have explored co-training VLA models with both web data and robotic data, showing promising generality when interacting with new objects and unseen backgrounds. Nevertheless, existing approaches remain predominantly generating robot action at the end of the VLA output sequence, overlooking the rich temporal dynamics and causal dependencies among vision, language, and action modalities inherent in open-world embodied interactions. Humans, in contrast, exhibit flexible and interleaved synergy between multimodal embodied reasoning and physical action, enabling highly generalizable and dexterous manipulation in the open world, i.e., reasoning guides action and action results inform subsequent reasoning. This raises fundamental research question: How can we design an effective training paradigm for generalist robot policies that support flexible and mutual-informed reasoning-acting integration? Inspired by the impressive results of advanced multimodal understanding and generation systems (Deng et al., 2025, Ma et al., 2025b, Xie et al., 2025, NextStep-1, 2025), which demonstrate the superiority in interleaved multimodal pretraining, we propose unified embodied model to enable flexible and powerful multimodal embodied reasoning and action generation through interleaved embodied pretraining. Achieving this requires not only an effective and unified architecture that excels in mix-modality generation, but also carefully structured multimodal embodied data that jointly integrate texts, images, videos, and robotic actions. To realize this vision, we first established new protocol for scalable data curating, filtering, and construction of high-quality multimodal interleaved embodied data. As for data source, we integrate web vision-language data with real robot episodes, the latter naturally providing actionlevel, temporal, and physical continuity. Then, we employ VLMs and human to annotate real robot episodes with diverse embodied temporal and spatial QA pairs, including physical common sense understanding, task planning, object localization, affordance pointing, and multi-view correspondence. These annotations enable the learning of fine-grained geometric and spatial-temporal representations of the physical world. Finally, the interleaved vision-text-action data is constructed by concatenating these multimodal QA and robot control actions in temporal order. Importantly, while robot actions are fixed at each timestep, we design three flexible interleaved formats to associated random embodied reasoning QA pairs. Consequently, the interleaved embodied data capture rich world knowledge and nuanced cross-modal interactions, providing models with foundational capabilities of action prediction, scene understanding, and complex multimodal reasoning for generalist robot policies. Regarding model architecture, we adopt single unified decoder-only transformer that integrates discrete auto-regressive decoding with continuous flow matching denoising. The unified model is built on pre-trained VLM, thereby inheriting broad visuallanguage knowledge, and its shared parameters are further optimized with modality-specific objectives: next-token prediction for text and flow matching for robotic actions. The model applies causal attention cross the entire interleaved vision-text-action sequence to capture the sequential dependencies between reasoning and acting. In addition, two MLPs are added to encode and decode continuous robotic actions, complementing the original text and visual tokenizers. Unlike prior VLA models (Black et al., 2024, 2025, Driess et al., 2025) that introduce extra action-specific modules to learn action generation, our design enables easier alignment between vision/language and action modalities through circumventing train new action-specific parameters from scratch, thereby achieving more effective cross-modal knowledge transfer in generalist robot policies. Overall, we present the EO-Robotics toolchain for advanced embodied foundation model development, where EO-1, unified embodied foundation model comprising 3B parameters, is trained on the carefully curated interleaved embodied dataset EO-Data1.5M. As generalist VLA, EO-1 exhibits strong generalization capabilities in multimodal embodied reasoning and real robot control across 2 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control diverse set of challenging tasks, including pick-and-place, articulated manipulation, long-horizon planning and dexterous skills. The impressive results validate the effectiveness of unified multimodal modeling for generalist embodied intelligence. Furthermore, EO-Robotics is released with full openness, including model weights, training code, and all components of the interleaved embodied dataset, to serve as forward step for general-purpose automatous robots and facilitate further research in the community. This paper highlights the following contributions: Unified Architecture: EO-1 is unified model that integrates multimodal embodied reasoning and real robot control within shared backbone, enabling seamless cross-modal interaction without introducing extra bottleneck components or action-specific parameters. Interleaved Embodied Dataset: EO-Data1.5M is comprehensive dataset derived from the largest robotic datasets, featuring interleaved embodied reasoning and and robot control through scalable data construction pipeline, thereby enabling interleaved vision-text-action pretraining. Real-world Generalization: Our model surpasses existing open-source models across multiple embodied reasoning and robot control benchmarks, including ERQA, LIBERO, SimplerEnv, and the self-constructed EO-Bench, meanwhile, extensive real-robot evaluations demonstrate its substantially stronger reasoning capabilities and dexterous control in open-world generalization. 2. The EO-1 Model and Training Paradigm In this section, we introduce the architecture of EO-1 and its training strategies. As illustrated in Figure 1, the main architectural idea of EO-1 is to capture the inherent temporal dynamics and causal relationships between vision-text-action modalities in embodied interactions. This is realized by modeling multimodal understanding and robot control with single unified decoder-only transformer, enabling effective cross-modal knowledge transfer and alignment for generalist policies. 2.1. Problem Formulation as unified model EO-1 that integrates Multimodal We formulate the generalist robot policy πθ Embodied Reasoning and Robot Control through synergized autoregressive and denoising paradigm. The EO-1 architecture can flexibly decode both continuous action chunks and tokenized text outputs, enabling seamless text-based reasoning and physical robot control within one model. The distribution represented by the model can be written as πθ(ˆatt+h, ˆℓtot, ℓt, atht), where ot = [I1 , qt] consists of multi-view image observations and robot state, language contexts ℓ are embodied reasoning question-answer text data with respect to current observation (e.g., Q: Based on current image observation, what is the next step to do for the robot cleaning the table. A: pick up the yellow box and place it in the trash box) or overall task prompts (e.g., clear the table), and action sequence is an action chunk. The unified model is trained on an interleaved multimodal dataset 𝒟 to att+h alternately generate text ˆℓt for seamless embodied reasoning and action. Note that the unified model does not generate outputs on vision tokens and robot state tokens. and robot action ˆatt+h , ..., In The generalist robot policy πθ as input and generates sequence of multimodal outputs ˆx1N i=1 p( ˆxix<i). For inputs, each token can be discretized text token xw is instantiated by transformer that takes interleaved multimodal in an autoregressive token sequence x1N manner, i.e., p( ˆx1N) = , continuous image patch token xI , or partially denoised robot action token xa . For output, discrete text tokens are sampled via language-modeling head, while continuous action tokens are sampled by flow-matching head. The language modeling head is implemented with classic logits head appended to the unified transformer to decode text token output ˆxw . For the flow-matching head, rectified flow (Lipman et al., 2022) is utilized to generate , continuous robot state token i 3 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 1: EO-1 Model Architecture. EO-1 model is Vision-Language-Action (VLA) model that adopts single unified decoder-only transformer, equipping with discrete language-modeling head for multimodal embodied reasoning and continuous flow-matching head for robot action generation. The language instruction, image observations, robot state, and noisy action are encoded into an interleaved token sequence of tokens to be processed by the shared transformer backbone, whose weights are initialized from Qwen2.5-VL. The model is trained on interleaved vision-text-action data with combination of flow-matching objective and next-token-prediction objective and capable of seamless embodied reasoning and acting. continuous action signals ˆat according to the forward Euler integration rule: ˆaτ+δ = ˆaτ + δVπθ (ˆaτ , ot, ℓt, atht), (1) represents the parameters of πθ to predict the vector field for robot action generation. The where Vπθ actions are generated by integrating the predicted vector field from τ = 0 to τ = 1, starting with random noise z0 𝒩 (0, I), and δ is the integration step size. The shared parameters in the unified transformer enable the seamless transfer of semantic knowledge from vision-language understanding to action generation, leading to more general reasoning capabilities and control capabilities in embodied scenarios (Deng et al., 2025, Black et al., 2025, Driess et al., 2025). 2.2. Model and Training Model Architecture. The proposed unified model architecture processes interleaved multimodal inputs (i.e., text, images, videos, and actions) through shared transformer backbone that generates both discrete token sequences and continuous action tokens. The model employs text tokenizer and visual encoder to convert text and image patches into input tokens, while the robot state qt is linearly projected into the same transformer embedding space, i.e., xw Rd. Note that the text tokenizer and visual encoder are inherited from pre-trained VLM and the state projector is random initialized. For flow matching action denoising, the input noisy action aτ is obtained by aτ , where zτ 𝒩 (0, I) is random noise. Then, another noisy action linear projector is utilized to embed noisy action and flow matching timestep τ into noisy action token Rd. The unified transformer backbone, initialized from Qwen 2.5 VL (Bai et al., 2025), processes xa this interleaved multimodal input sequence with shared parameters and generates output sequence through two separate heads: language head for text token decoding and flow head for continuous + (1 τ)zτ = τat , xI , 4 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 2: Interleaved rectifying sampling strategy. Our method samples variable-length subsequences from robot action generation segments, enabling efficient training of mixed-modality generation while preserving causal relationships. action denoising. Note that we do not introduce extra action-specific parameters to separately model action denoising as prior VLA models, enabling seamless integration of discrete multimodal embodied reasoning and continuous robot control. Unified Prompting and Attention. Training EO-1 involves three types of data: multimodal understanding data, robot action generation data, and mixed-modality generation data (Section 3). The mixed-modality generation data is formatted as interleaved vision-text-action data, i.e., [BOI] {image patchs} [EOI] [BOS] {text} [EOS] [BOR] {robot state} [EOR] [BOA] {noisy action} [EOA] [BOI] {image patchs} [EOI] [BOS] {text} [EOS] [BOR] , where [BOS], [EOS], [BOI], [EOI], [BOR], [EOR], [BOA], and [EOA] denote the beginning and end of sentence, image, robot state, action, respectively. The multimodal understanding format is [BOI] {image patchs} [EOI] [BOS] {text} [EOS] and the robot control data format is [BOI] {image patchs} [EOI] [BOS] {text} [EOS] [BOR] {robot state} [EOR] [BOA] {noisy action} [EOA]. During training, we employ an omnidirectional attention mask mechanism to process these three type data with causal attention masks and cache key-value (KV) pairs of the generated multimodal context to accelerate mixedmodality generation at inference. Note that prior VLA models are either trained with solo robot control data or co-trained with multimodal understanding and robot control data, missing interleaved vision-text-action data. Interleaved Rectifying Sampling. When training on interleaved vision-text-action data for mixedmodality generation, the denoising process of action generation in interleaved data can disrupt the causal relationships in multimodal token sequences, because the subsequent text, image, or action tokens should attend to the clean action tokens and preceding text/image tokens, rather than noisy action tokens. To address this challenge, we propose rectifying sampling strategy for mixed-modality generation training. As illustrated in Figure 2, we sample + 1 (e.g., = 2) training subsequences from an interleaved sequence containing action generation segments by splitting the sequence with respect to continuous action generation segments. For the subsequences ending with the first action generation segment, there is no additional operation on it. For the subsequences ending with the interleaved action generation segment, i.e., containing another action generation segment in the middle of the sequence, we replace the noisy action tokens in the middle action generation segment with clean action tokens. With this interleaved rectifying sampling, each action generation segment is trained with both flow-matching denoising on noisy action tokens and indirect gradients backpropagation on clear action tokens for subsequent interleaved generation. Training Objective. To learn both auto-regressive decoding and flow-matching denoising with one unified model, we employ two learning objectives: i) next token prediction and ii) denoising vector field prediction. For multimodal understanding and embodied reasoning tasks, the output token and is obtained through original language decoding head. The ˆxi contains only text tokens ˆxw EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control language head and transformer backbone are trained on decoded text tokens using the cross-entropy loss between ground-truth text tokens and predicted logits ℒar(xw = πθ( ˆxx<i) is conditioning on all preceding multimodal tokens. For robot action generation, the output action ˆat is generated by mapping intermediate hidden features of the last transformer layer to denoise vector field z0 through separate flow head. The flow head and transformer backbone are trained on action tokens by minimizing the following loss: ), where ˆxw gt, ˆxw at τ[ ℒ m(θ) = (cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194) is the input noisy action. x<a Vθ (aτ , τx<a) (z0 2 at) ] , (2) = τat + (1 τ)z0 where aτ represents the all preceding multimodal tokens. As in (Black et al., 2024), we sample the flow matching timestep τ from beta distribution that emphasizes lower (noisier) timesteps. With the interleaved multimodal token sequence, we apply next token prediction ℒar(θ) to the language head and flow matching ℒ m(θ) to the flow head for predicting denoising vector field. The unified model is trained end-to-end by optimizing sum of these two objectives: ℒ = ℒar(θ) + ℒ m(θ). 3. Dataset and Benchmark EO-1 is trained on diverse range of datasets across multiple modalities, including text, image, video, and robot control data, to perform embodied reasoning and dextrous control, all through unified multimodal interface. In addition to standard robot control datasets and existing largescale vision-language datasets, we design scalable data construction pipeline and build interleaved embodied datasets from large-scale robot control episodes to capture the rich temporal dynamics and causal relationships inherent in embodied interactions. As illustrated in Table 1, The pre-training data corpus is structured into three main categories: web multimodal data, robot control data, and interleaved embodied data. We summarize the scale and composition of our training data across different modalities. In the following sections, we detail our data sources, interleaved data construction pipeline, and show some samples used in training. Modality Source Web Multimodal Data Robot Control Data LLaVA-1.5 (Liu et al., 2024a), LLaVA-Video-178K (Zhang et al., 2024b), PixMo-Points (Deitke et al., 2024a), RefCOCO (Kazemzadeh et al., 2014a), RoboVQA (Sermanet et al., 2024) AgiBotWorld (Bu et al., 2025), Open X-Embodiment (OpenX-Embodiment et al., 2024), RoboMIND (Wu et al., 2024), SO100-Community (Shukor et al., 2025), IPEC-Franka (Qu et al., 2025) #Data #Tokens 5.7M 7.1B 1.2M (episode) 127.3B Interleaved Embodied Data EO-Data1.5M from AgiBotWorld, Open X-Embodiment and RoboMIND 1.5M 1.0B Table 1: Overview of the data used in EO-1 training. More details in Appendix B. 3.1. Data Source Web Multimodal Data. The web text-image paired data is essential for multimodal understanding. We curated web multimodal dataset comprising 5.7 Million samples with 7.1 Billion tokens, integrating six public sources across three task domains: i) Visual instruction following. We incorporate the LLaVA Visual Instruct Series (Liu et al., 2023b, Sermanet et al., 2024) datasets, which contain GPT-generated multimodal instructions and robotics-oriented visual question answering examples. These sources cover diverse interaction patterns, including image-grounded conversation, multi-step reasoning, and long-horizon task planning. ii) Video understanding and reasoning. We leverage the LLaVA-Video (Zhang et al., 2024a) and RoboVQA (Sermanet et al., 2024), which support temporal multimodal comprehension through detailed video captioning, open-ended QA, and multiple-choice EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control question answering. iii) Referring expression and spatial grounding. To support fine-grained object understanding, we include RefCOCO (subset) (Kazemzadeh et al., 2014b) for natural language referring expression comprehension and PixMo-Points (Deitke et al., 2024b) for spatial localization via human-annotated coordinate grounding. Robot Control Data. Robot control data provides critical supervision for policy learning across embodiment generalization and fine-grained manipulation. We aggregate large-scale real robot control dataset comprising 1.2 Million episodes with 0.13 Trillion tokens from five public sources, forming one of the most comprehensive collections for robot learning to date. The dataset includes AgiBot-World (Bu et al., 2025), featuring dual-arm humanoid demonstrations across diverse bimanual manipulation tasks with rich visuomotor sequences; Open X-Embodiment (OpenX-Embodiment et al., 2024), which spans 22 robot types and 527 skills to support cross-platform generalization; and SO100-Community (Shukor et al., 2025), collection of long-horizon community demonstrations using SO100 platform. It also incorporates RoboMind (Wu et al., 2024), covering wide range of tasks across multiple embodiments (e.g., Panda, UR5e, AgileX, humanoids) with diverse object classes. 3.2. Interleaved Embodied Data Construction Our interleaved embodied data is obtained by annotating existing real robot control data on two aspects, resulting in two components: i) embodied temporal and spatial reasoning data focusing on physical dynamic and spatial relationship understanding of robot execution video (as shown in Figure 3) and ii) interleaved vision-text-action data connecting temporal/spatial reasoning data with robot control data for learning multimodal causal relationships in embodied interactions. 3.2.1. Embodied Reasoning Data Pipeline To enhance the models vision-language capabilities on embodied intelligence, we develop specialized pipeline in Figure 3, to carefully curate robot-specific question-answer data for both temporal and spatial embodied reasoning. Real robot manipulation videos serve as our primary data source. They perform variety of tasks in real-world environments and showcase multiple embodiments. To ensure our curated dataset is rich in terms of diversity, quality, embodiments, and scenarios, the curation pipeline are presented as follows: 1. Robot Video Filter and Curation. As illustrated in Figure 3 and Appendix D, we first filter and sample set of robot videos based on visual similarity to improve the data source diversity. This is because existing robot control datasets comprise thousands of videos collected in several fixed environments performing limited tasks, resulting in high visual similarity between videos. We extract visual features with pretrained vision backbone and cluster them according to feature similarity. The diverse video set is curated by sampling fixed number of videos from each cluster. 2. Video Splitting and Captioning. We employ both human annotators and pretrained VLMs to split videos into short clips containing individual subtasks. Each clip is then processed to extract detailed descriptions of the robots actions. These captions are not only used to construct video captioning QA data, but also serve as prompts for subsequent embodied reasoning annotations. 3. Generating QA Pairs. We prompt VLM to construct free-form questions according to designed templates, using subtask clips with previously generated captions. We construct two kinds of questions: i) temporal reasoning questions that focus on task planning and physical common sense understanding, and ii) spatial reasoning questions that require spatial understanding and object referring. As for answers, we first adopt VLM to generate multiple answers and employ human annotators to select or rewrite the correct answers. 7 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 3: (a) Statistics of EO-Robotics Dataset (EO-Data1.5M) and Benchmark (EO-Bench). (b) Dataset curation pipeline on robot episodes. (c) Interleaved Vision-Text-Action Data Example, including three interleaved formats to concatenate embodied reasoning QA pairs and robot control data along temporal order. EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control 4. Cleaning and Rewriting. Finally, we apply rule-based cleaning to ensure valid answers in spatial reasoning, and prompt an LLM to rewrite question-answer pairs to improve text diversity and inject fuzzy semantics into data. Using the aforementioned pipeline, we curated embodied temporal and spatial reasoning datasets specializing on temporal dynamic understanding and spatial relationship reasoning. The annotation details and dataset statistics are presented as below: Embodied Temporal Reasoning Data. The temporal reasoning data aims to equip the pre-trained vision-language model with capabilities on physical common sense understanding and task planning, specializing on physical commonsense reasoning and task planning. As shown in Figure 3 (a), the Physical Commonsense Understanding data contain 70 thousand QA pairs for describing the effect of various robot actions in the physical world. The task planning data comprises 0.7 million QA pairs across four sub-categories to develop the models capacities including i) Task Planning: generating subtask sequences to complete long-horizon tasks. ii) Episode Captioning: captioning of robot video clips. iii) Affordance QA: assess whether executing specific action is possible. iv) Process Verification: recognizing completed actions in video clips. v) Subtask QA: determining whether subtask has been successfully completed. vi) Failure Detection: identifying unsuccessful subtask executions. Appendix D.3.1 shows question-construction prompt templates used to produce these questions and answers. Embodied Spatial Reasoning Data This kind of data focus on enhancing the models abilities in spatial understanding and reasoning, which contains 1.5 million QA pairs and is organized into five subcategories: relation reasoning, trajectory prediction, object referring, object pointing, and multiview pointing, as illustrated in Figure 3 (a). Specifically, i) Relation Reasoning data are annotated to understand relative spatial relationships among multiple objects in the scene. ii) Trajectory Prediction focuses on anticipating the future motion trajectories of objects or robot gripper within dynamic environments. iii) Object Referring data are constructed by grounding referred objects with bounding boxes among multiple candidates. iv) Object Pointing data aim to identify and point specific objects in multi-object scenarios according to the task instruction. v) Multiview Pointing is labeled by locating the same objects across different view images of the same scene. The question-construction prompt for embodied spatial reasoning is shown in Appendix D.3.2. 3.2.2. Interleaved Vision-Text-Action Data Pipeline To learn the natural sequential coherence among visual, text, and action modalities, we design three flexible formats to concatenate embodied reasoning QA data and robot control data along temporal order in robot videos. They are illustrated in Figure 3 (b), and detailed as follows: Interleaved Temporal Reasoning Format. For certain frame from the robot video, the interleaved temporal reasoning data is constructed as. [image tokens] [next subtask plannning QA] [subtask instruction] [robot action] [image tokens] [task-completion verification QA] . We use predefined template merge next subtask plannning answer into the instruction for action generation, and task-completion verification QA is appended to new image tokens to determine the task progress. Interleaved Spatial Reasoning Format. As for spatial reasoning data, we use the trajectory prediction QA and introduce [trajectory instruction] to connect spatial reasoning QA with robot control data, i.e., [image tokens] [robot trajectory prediction QA] [trajectory instruction] [robot action] [image tokens] [task-completion verification QA] . The robot trajectory predictions answers contain sequence of [x,y] coordinates of robot gripper trajectory for finish task, and its results are merged into [trajectory instruction] for next action generation, i.e., How should the robot execute the trajectory [x1,y1],,[x6,y6] physically?. Interleaved Free Chatting Format. For other temporal and spatial reasoning QA data, we randomly 9 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 4: Examples of EO-Bench, including Multiview Pointing, Physical Common Sense, Trajectory Prediction, Process Verification, Task Planning, and Robot Affordance. select QA pairs and connect them with robot control data through original task instruction: [image tokens] [random reasoning QA] [task instruction] [robot action] [image tokens] [taskcompletion verification QA] . The task instruction is the prompt used in existing VLA models, i.e., What should the robot do to finish the task {task instruction}, where {task instruction} is the original task label from robot control data. With these flexible interleaved formats, we sample the first, last, one random middle image from each segmented subtask video clips and annotate next subtask planning to construct interleaved temporal reasoning data. The interleaved spatial reasoning data is constructed using all robot trajectory prediction QA in embodied spatial reasoning data. The interleaved free chatting format randomly remaining QA data to connect embodied reasoning QA data and robot control data. Totally, we curated 122 thousand interleaved vision-text-action data for mixed-modality generation training. 3.3. Embodied Onevision Reasoning Benchmark Embodied Onevision Benchmark (EO-Bench) aims to construct comprehensive and balanced evaluation suite for open-world embodied reasoning, covering both challenging and accessible tasks. Existing benchmarks often suffer from the drawback that single QA instance may conflate multiple reasoning aspects, such as combining spatial trajectories with extensive commonsense knowledge. This design leads to ambiguous evaluations, making it difficult to attribute performance to specific capabilities. In contrast, our benchmark ensures that each question targets clearly defined reasoning angle rather than mixing multiple dimensions. This enables more faithful diagnosis of models strengths and weaknesses in an interpretable and disentangled manner. As shown in Figure 3(a), EO-Bench is organized into four major categories: spatial understanding, physical commonsense, task reasoning, and state estimation. In total, the benchmark comprises 648 QA pairs manually labeled on diverse robot control data, distributed across categories as follows: 370 for spatial understanding, 140 for task reasoning, 84 for physical dynamic reasoning, and 48 for physical commonsense. This distribution reflects our intention to emphasize spatial reasoning, which EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control is an essential component and the bottleneck of embodied intelligence. Together, these four categories provide structured yet broad evaluation of models ability to reason about space, physics, tasks, and states in embodied intelligence. Each major category is further divided into sub-categories, as illustrated in Figures 3 and 4. Spatial Understanding covers trajectory reasoning (predicting object motion), visual grounding (localizing objects in context), relation reasoning (understanding relative positions among objects), and multiview pointing (identifying objects consistently across multiple viewpoints). Physical Commonsense includes direct influence and counterfactual reasoning, both of which require models to apply fundamental physical principles in embodied interactions. Task Reasoning spans task planning (deriving action sequences toward goals), episode captioning (summarizing task-level events), and process verification (evaluating task completion and progress). State Estimation involves recognizing object states (e.g., open/closed, full/empty), reasoning about robot-object interactions, and assessing action-level consequences. By structuring the benchmark into these categories and sub-categories, we provide principled way to evaluate embodied models across complementary aspects of embodied intelligence, while maintaining interpretability of each evaluation dimension. 4. Experimental Results 4.1. Implementation Details The EO-1 model is trained on large-scale corpus that integrates 1.2M real-robot demonstrations from AgiBotWorld (Bu et al., 2025), Open X-Embodiment (OpenX-Embodiment et al., 2024), RoboMIND (Wu et al., 2024), and SO100-Community (Shukor et al., 2025), together with 5.7M web multimodal samples and 1.5M interleaved embodied data pairs, yielding total of 135B tokens. To optimize on this scale, we train for five epochs with Flash-Attention variable-length packing (average 5 for sequence length of 16,384) and batch size of 1. The backbone learning rates are set to 5 10 5 for the vision encoder, and fixed balance both the language model and multimodal projector, 1 10 factor of 1.0 is applied between language regression and action flow matching losses. Additional settings include an action denoising chunk size of 16, resolution bounds of 50176100352, and the DeepSpeed ZeRO-1 optimizer. At inference time, the policy is conditioned on multi-view camera observations and sub-task instructions. It predicts 16-step action chunk through 10 denoising iterations for robot execution. This design achieves balance between stability and efficiency, while requiring only 6 GB of GPU memory on single NVIDIA RTX 4090, thus enabling real-time evaluation in both simulation and real-world environments. 4.2. Open-sourced Benchmark Evaluation Setups To thoroughly evaluate the embodied reasoning and dexterous manipulation capabilities, we assess our model across two key categories of benchmarks: Embodied Reasoning and Robot Control. They encompass broad spectrum of perceptual, cognitive, and control challenges, spanning from highlevel visual reasoning to low-level motor skill execution, across both simulated and visually complex real-world environments. Embodied Reasoning. This category consists of three reasoning-centric benchmarks: RoboVQA, ERQA, and our self-constructed EO-Bench. They are described in the following items. Specifically, these benchmarks are designed to evaluate distinct and complementary cognitive capabilities. To ensure standardized and consistent evaluation, we employ VLMEvalKit, unified framework that 11 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control facilitates rigorous benchmarking and reliable performance comparison across these tasks. RoboVQA (Sermanet et al., 2024): free-form visual question answering (VQA) benchmark designed to assess high-level reasoning in egocentric embodied scenes. Following prior works, model performance is evaluated using BLEU-4 scores. ERQA (Gemini-Robotics, 2025): curated benchmark that emphasizes spatial reasoning and grounded world knowledge. The tasks are designed around realistic robotic scenarios requiring inference over object relationships, geometry, and agent-centric spatial cues. EO-Bench: We propose EO-Bench to evaluate models in spatial-temporal reasoning. The benchmark consists of 700 multiple-choice VQA tasks, distributed evenly across four reasoning categories: Physical Commonsense, Spatial Understanding, State Estimation, and Task Reasoning. All tasks are grounded on egocentric visual inputs, meticulously constructed to reflect the real-world challenges of robot perception and decision-making. Robot Control. This category includes two manipulation benchmarks: SimplerEnv and LIBERO, which collectively assess wide spectrum of control capabilities across visually diverse, long-horizon, and goal-conditioned tasks. SimplerEnv (Li et al., 2024b): manipulation benchmark featuring WidowX and Google Robots operating in visually diverse environments with variations in lighting, surface textures, and camera viewpoints. It leverages the Bridge Data V2(Walke et al., 2023) for evaluating Real-to-Sim transfer. SimplerEnv includes both short-horizon atomic tasks and visually challenging variants, specifically designed to assess robustness under appearance shifts between real and simulated settings. LIBERO (Liu et al., 2023a): long-horizon manipulation benchmark consisting of temporally extended, multi-stage tasks across various object categories and interaction types. LIBERO emphasizes planning, memory, and semantic understanding in complex simulated environments. 4.3. Benchmark Evaluation Results Embodied Reasoning. To evaluate the embodied reasoning capabilities, we benchmark it on three key tasks: RoboVQA Sermanet et al. (2024), ERQA (Gemini-Robotics, 2025), and EO-Bench. These benchmarks assess diverse range of reasoning skills, including spatial understanding, physical commonsense, and multistep task planning. We compare EO-1 against three categories of baselines: public VLMs (e.g., Qwen2.5 VL (Bai et al., 2025), InternVL2.5 (Chen et al., 2024b)), private VLMs (e.g., GPT-4o, Gemini 1.5 Flash (Gemini et al., 2023b), Claude 3.5), and co-trained visual-language-action models (e.g., ChatVLA (Zhou et al., 2025), Magma (Yang et al., 2025)), with all models evaluated under consistent embodied conditions. Across all the benchmarks presented in Table 2, EO-1 consistently demonstrates robust reasoning capabilities in embodied scenarios in the open world. On RoboVQA, which evaluates long-horizon visuospatial inference in egocentric settings, EO-1 achieves the BLEU-4 score of 58.5, significantly outperforming leading closed-source models, such as GPT-4o (47.2). For ERQA, spatially grounded benchmark derived from real-world visual scenes, EO-1 achieves the accuracy of 45.5, surpassing InternVL2.5 8B (45.2) and outperforming other public and private VLMs. In the suite EO-Bench introduced in this paper, mainstream VLMs achieve modest average score of 32, highlighting the substantial limitations of current methods in handling embodied tasks. On the EO-Bench @ Spatial leaderboard, which evaluates performance across multiview reasoning, trajectory prediction, and visual grounding tasks, EO-1 outperforms other open-source models of comparable size, securing score of 36.4. This highlights EO-1s improved capability in multi-modal scene understanding and spatial reasoning. Furthermore, in the EO-Bench @ Temporal subset, dedicated to multimodal and temporal reasoning in manipulation tasks, EO-1 achieves 38.9, underscoring its strong aptitude for scene-level abstraction and temporal prediction. 12 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Multi-modal Benchmark RoboVQA ERQA EO-Bench @ Spatial EO-Bench @ Temporal Overall Gemini 1.5 Flash (Gemini et al., 2023a) Claude 3.5 GPT-4o 2024-11-20 Qwen2.5 VL 3B (Bai et al., 2025) Qwen2.5 VL 7B (Bai et al., 2025) InternVL2.5 4B (Chen et al., 2024a) InternVL2.5 8B (Chen et al., 2024a) ChatVLA 2B (Zhou et al., 2025) Magma 8B (Yang et al., 2025) EO-1 (3B) 46.0 26.7 47. 55.9 56.9 45.7 43.9 33.7 30.3 58.5 46.3 35.5 40.0 35.3 39.3 36.8 45.2 34.3 29.3 45.5 46.3 24.0 35. 20.0 26.7 37.0 32.8 26.4 29.4 36.4 37.8 34.8 39.3 22.6 31.5 31.9 38.1 21.9 36.7 38.9 44.1 30.3 40. 33.5 38.6 37.9 40.0 29.1 31.4 44.8 Table 2: Performance comparison with standard VLMs and co-trained visual-language-action models on RoboVQA (Sermanet et al., 2024), ERQA (Gemini-Robotics, 2025), and EO-Bench benchmarks. Model LIBERO-Spatial LIBERO-Object SR Rank SR Rank LIBERO-Goal SR Rank LIBERO-Long Overall SR Rank SR Rank OpenVLA (Kim et al., 2024) SpatialVLA (Qu et al., 2025) OpenVLA-OFT (Kim et al., 2025) ThinkAct (Huang et al., 2025) MolmoAct-7B-D (Lee et al., 2025) Diffusion Policy (Chi et al., 2023) Octo (Octo et al., 2024) (Black et al., 2024) π0 -FAST (Pertsch et al., 2025) π0 GR00T N1 (Bjorck et al., 2025) EO-1 (Ours) 84.7 0.9% 88.2 0.5% 97.6 0.9% 88.3 % 87.0 % 78.5 1.1% 78.9 1.0% 96.8 0.8% 96.4 0.7% 94.4 0.9% 99.7 0.2% 5 2 1 3 6 5 2 3 4 1 88.4 0.8% 89.9 0.7% 98.4 0.8% 91.4 % 95.4 % 87.5 0.7% 85.7 0.9% 98.8 0.9% 96.8 0.7% 97.6 1.0% 99.8 0.1% 5 4 1 3 2 5 6 2 4 3 1 79.2 1.0% 78.6 0.6% 97.9 1.0% 87.1 % 87.6 % 73.5 1.2% 84.6 0.9% 95.8 1.1% 88.6 1.0% 93.0 1.2% 99.2 0.3% 4 5 1 3 2 6 5 2 4 3 1 53.7 1.3% 55.5 1.0% 94.5 1.3% 70.9 % 77.2 % 64.8 1.3% 51.1 1.3% 85.2 1.2% 60.2 1.4% 90.6 1.0% 94.8 0.6% 5 4 1 3 4 6 3 5 2 1 76.5 0.6% 78.1 0.7% 97.1 0.6% 84.4 % 86.6 % 76.1 0.7% 75.1 0.6% 94.2 0.9% 85.5 1.0% 93.9 1.1% 98.2 0.3% 5 4 1 3 2 5 6 2 4 3 1 Table 3: Performance comparison with state-of-the-art policies on LIBERO Benchmark (Liu et al., 2023a). Robot Control. We evaluated manipulation performance on two simulation benchmarks: LIBERO (Liu et al., 2023a)and SimplerEnv (Li et al., 2024b). LIBERO spans four subsets, Spatial, Object, Goal, and Long, designed to test generalization across spatial layouts, object categories, goal semantics, and long-horizon planning. SimplerEnv includes Google-VM (Visual Matching), Google-VA (Visual Aggregation), and WindoWX setups with controlled variations in color, texture, lighting, and camera pose, assessing robustness under visual distribution shift. We compare against leading robot foundation models, including the RT series (Brohan et al., 2022, OpenX-Embodiment et al., 2024), Octo (Octo et al., 2024), OpenVLA (Kim et al., 2024), the π-series (Black et al., 2024, Pertsch et al., 2025), and ThinkAct (Huang et al., 2025), among others. As shown in Table 3 and Table 4, our method achieves superior performance across various robotic manipulation tasks. On the LIBERO benchmark, it attains an average success rate of 98.2%, outperforming recent state-of-the-art models, including OpenVLA-OFT (Kim et al., 2025), π0 (Black et al., 2024), and GR00t-N1 (Bjorck et al., 2025) by 1.1%, 4.0%, and 4.3%, respectively, demonstrating robust generalization across spatial, semantic, and long-horizon tasks. On the SimplerEnv (Black et al., 2024) by 3.5%, 5.1%, and 8.3% in the WidowX, Googlebenchmarks, it surpasses π0 VM, and Google-VA variants, respectively, achieving the highest success rates of 72.7%, 76.5%, and 63.0%. These results are obtained with lightweight fine-tuning on modest mixed-modality dataset, underscoring the methods data efficiency, dexterous control, and precise visuomotor grounding. 4.4. Real-world Experiment Evaluation Setups We introduce comprehensive set of robotic manipulation tasks to evaluate the generalist capabilities of the VLA model across diverse array of real-world scenarios. These tasks span pick-and-place, articulated object manipulation, long-horizon and dexterous actions, while also challenging the robots 13 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control WidowX Benchmark Put Spoon on Towel Put Carrot on Plate Stack Blocks Put Eggplant in Basket Overall -FAST (Pertsch et al., 2025) RT-1-X (OpenX-Embodiment et al., 2024) OpenVLA (Kim et al., 2024) SpatialVLA (Qu et al., 2025) Magma (Yang et al., 2025) π0 Octo-Base (Octo et al., 2024) Octo-Small (Octo et al., 2024) RoboVLM (Li et al., 2024a) (Black et al., 2024) π0 ThinkAct (Huang et al., 2025) EO-1 (Ours) 0% 0% 20.8% 37.5% 29.1% 12.5% 47.2% 45.8% 83.8% 58.3% 63.6% 4.2% 0% 20.8% 29.2% 21.9% 8.3% 9.7% 20.8% 52.5% 37.5% 54.5% 0% 0% 25.0% 20.8% 10.8% 0% 4.2% 4.2% 52.5% 8.7% 81.8% 0% 4.1% 70.8% 91.7% 66.6% 43.1% 56.9% 79.2% 87.9% 70.8% 90.9% 1.1% 1.0% 34.4% 44.8% 32.1% 16.0% 29.5% 37.5% 69.2% 43.8% 72.7% Google Robot Benchmark (Matching) Pick Coke Can Move Near OpenClose Drawer Drawer Apple Average -FAST (Pertsch et al., 2025) RT-1 (Brohan et al., 2022) OpenVLA (Kim et al., 2024) TraceVLA (Zheng et al., 2024) SpatialVLA (Qu et al., 2025) Magma (Yang et al., 2025) π0 RT-1-X (OpenX-Embodiment et al., 2024) RT-2-X (OpenX-Embodiment et al., 2024) Octo-Base (Octo et al., 2024) RoboVLM Li et al. (2024a) (Black et al., 2024) π0 ThinkAct (Huang et al., 2025) MolmoAct (Lee et al., 2025) EO-1 (Ours) 85.7% 16.3% 28.0% 86.0% 75.0% 75.3% 56.7% 78.7% 17.0% 77.3% 97.9% 92.0% 77.7% 98.0% 44.2% 46.2% 53.7% 77.9% 53.0% 67.5% 31.7% 77.9% 4.2% 61.7% 78.7% 72.4% 77.1% 83.8% 73.0% 35.6% 57.0% 57.4% 58.9% 42.9% 59.7% 25.0% 22.7% 43.5% 62.25% 50.0% 60.0% 71.3% 6.5% 0% 0% 0% 8.3% 0 40.7% 7.4% 0 24.1% 46.6% 52.8% 52.4% 24.5% 34.7% 55.3% 48.8% 46.4% 47.2% 47.3% 11.0% 51.7% 71.4% 76.5% Drawer Apple Average Google Robot Benchmark (Aggregation) Pick Coke Can Move Near OpenClose Drawer 32.3% 17.7% 31.0% 41.8% 59.0% 31.3% -FAST (Pertsch et al., 2025) 50.0% 47.7% 56.4% 72.7% 78.5% 68.2% 89.8% 54.5% 60.0% 88.0% 68.6% 77.6% RT-1 (Brohan et al., 2022) OpenVLA (Kim et al., 2024) TraceVLA (Zheng et al., 2024) SpatialVLA (Qu et al., 2025) Magma (Yang et al., 2025) π0 RT-1-X (OpenX-Embodiment et al., 2024) RT-2-X (OpenX-Embodiment et al., 2024) Octo-Base (Octo et al., 2024) RoboVLM Li et al. (2024a) (Black et al., 2024) π0 ThinkAct (Huang et al., 2025) MolmoAct EO-1 (Ours) Table 4: Performance comparison with state-of-the-art policies on SimplerEnv Benchmark (Li et al., 2024b). 29.4% 35.3% 1.1% 10.6% 27.6% 47.6% 78.8% 55.0% 49.0% 82.3% 0.6% 75.6% 90.1% 84.0% 76.1% 91.6% 32.3% 79.2% 3.1% 60.0% 80.7% 63.8% 61.3% 81.7% 30.2% 54.4% 1.2% 36.6% 54.7% 63.0% 10.1% 20.6% 0 0 20.5% 23.8% 43.7% 30.0% 36.9% 52.2% 57.5% 44.3% 2.6% 0.0% 0% 6.3% 24.0% 0 abilities to perform embodied reasoning, engage in complex planning, and adapt to varied control types, as illustrated in Figure 5. These tasks are described as follows. 1. Franka Panda Pick-and-Place (7 Tasks). Household tasks such as brewing tea, storing kitchen items, and sorting cubes, demonstrating proficiency in object-to-container pick-and-place and articulated object manipulation. These tasks test fine motor control and adaptability to diverse object geometries in basic household settings. 2. WidowX 250 Out-of-Box (13 Tasks). Out-of-box evaluation in kitchen environment includes tasks involving both object-to-container manipulation (e.g., vegetable preparation, cup arrangement) and articulated handling (e.g., door closing), highlighting the robots flexibility in managing diverse objects and complex actions. 3. Agibot G-1 Long-Horizon Dexterity (4 Tasks). Long-horizon, dexterous tasks requiring sequential planning and fine manipulation, such as folding clothes, making sandwiches, and sorting groceries. These tasks highlight the robots proficiency in multi-step operations in multi-step tasks, demonstrating advanced dexterity and adaptability in long-term execution scenarios. 4. Embodied Reasoning Control (4 Tasks). Physical and abstract tasks including visual object rearrangement, tic-tac-toe, and long-horizon planning tasks (e.g., making breakfast sandwich, roasting beef steak), showcasing its embodied reasoning capabilities. These tasks require not only 14 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 5: Example of real-world evaluation tasks in diverse robots, including Agibot G-1 Long-horizon Dexterous (row 1-4), Franka Panda Pick-and-Place (row 5), WidowX 250 Out-of-Box (row 6), and Embodied Reasoning Control in Franka, Agibot G-1, and Lerobot SO100 (row 1,2,7,8). EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control precise manipulation but also higher-level reasoning, testing the intersection of manipulation and reasoning in real-world contexts. These 28 task settings serve to assess the models performance in four key dimensions: mastering diverse manipulations across multiple robot platforms, specializing in long-horizon dexterous tasks, emerging open-world embodied generalization, and enhanced generalization through unified reasoning capabilities. The results of the four dimensions are presented successively in the following subsections. 4.5. Mastering Diverse Manipulations on Multiple Embodiments Figure 6: Performance comparison on diverse robot platforms and task categories. In this section, we demonstrate that EO-1 can perform wide range of dexterous manipulation tasks across various robotic platforms, showcasing its robustness and adaptability. We evaluate its performance on both short-horizon and long-horizon tasks using different robots, including Franka Panda, WidowX 250 S, Agibot G-1, and Lerobot SO100. These tasks range from basic pick-and-place to complex articulated object manipulation and long-horizon tasks. We compare EO-1 against stateof-the-art baselines, including π0 , and GR00T-N1.5 (Pertsch et al., 2025, Black et al., 2024, Bjorck et al., 2025), highlighting its superior performance across these varied tasks. -Fast, π0 The results reported in Figure 6 show that EO-1 consistently outperforms the baselines across all robot platforms and task categories, achieving performance of 86.0%, compared with 43.0% . Notably, in long-horizon tasks with Agibot for Fast, 71.0% for GR00T-N1.5, and 68.0% for π0 at G-1, EO-1 achieves remarkable completion score of 81.0%, surpassing the performance of π0 67.0%. Similarly, on Franka Pandas pick-and-place tasks, EO-1 scores 94.0%, exceeding GR00TN1.5s 86.0%. On more complex embodied reasoning control tasks, e.g., Tic-Tac-Toe and Visual Rearrangement, EO-1 maintains completion score of 83.0%, which again surpasses that of π0 at 53.0% and GR00T-N1.5 at 62.0%. These results indicate that EO-1 excels at managing multi-step and dexterous tasks that demand high precision and adaptability across various environments and robot platforms. The strong performance of EO-1 across such diverse set of tasks underscores its potential for real-world deployment, where robotic systems are required to operate reliably in dynamic and varied environments. 4.6. Specializing to Long-horizon Dexterity In this section, we investigate the EO-1 models ability to specialize in long-horizon dexterous tasks, which require precise coordination and sustained execution over extended time periods. While the 16 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control model demonstrated strong performance on short-horizon tasks in previous evaluations, these more complex tasks present unique challenges, as they involve sequence of coordinated actions across multiple steps. To explore the models specialization potential, we fine-tune EO-1 using high-quality demonstration data for several real-world tasks that push the limits of both dexterous and long-horizon manipulation. As illustrated in Figure 5 (row 1-4), we select four tasks that require intricate multi-step decisions and fine manipulation across multiple sequential actions: 1. Make Breakfast Sandwich: Tasks involve the precise placement of bread, ham, lettuce, and other ingredients in multi-step sequence using both arms to assemble the sandwich. 2. Roast Beef Steak: Tasks Involve series of steps requiring fine motor skills, including brushing oil on the steak, placing it in the oven, and operating the oven with both arms. 3. Fold Household Clothes: Tasks involve grasping and folding shorts by coordinating both arms for precise control over the fabric, requiring dexterity and spatial awareness. 4. Sort Grocery Items: Tasks involve sort items (e.g., shampoo and green onions) into designated containers based on specific instructions, testing the robots instruction-following and object manipulation skills. Figure 7: Long-horizon dexterity completion rate comparison on the AgiBot G-1 platform. Specializing fine-tuning is performed using 150 demonstration trajectories per task, with each suite comprising approximately 0.5 million frames across 8 steps. The training process spans 25 epochs, with 9:1 sub-task-to-overall task mixture ratio, where 90% of the data is from sub-tasks and 10% from the overall task. During evaluation, 10 test runs per task are conducted, using sub-task instructions to compute the average completion score. As shown in Figure 7, we assess the specialized EO-1 model on four long-horizon tasks, comparing its performance against leading methods. Specifically, on the Roast Beef Steak task, which requires fine motor control and interaction with kitchen appliances, EO-1 reaches completion score of (46.0%). On the Make Breakfast 56.0%, substantially higher than both GR00T-N1.5 (47.0%) and π0 (73.0%) and Sandwich task, EO-1 demonstrates an 85.0% completion score, far surpassing π0 GR00T-N1.5 (72.0%), showcasing its ability to handle complex, multi-step manipulations. Similarly, in the Sorting Grocery Items task, which tests both object manipulation and instruction-following abilities, EO-1 excels with completion score of 95.0%, while π0 and GR00T-N1.5 achieve notably lower success rates (62.0% and 80.0%, respectively). Overall, as shown in Figure 6(c), EO-1 achieves an impressive average completion score of 81.0%, significantly outperforming both GR00T-N1.5 (68.0%) and π0 (67.0%). These results highlight EO-1 ability to excel in long-horizon tasks, showing significant improvements over baseline models, and its capacity to manage complex, multi-step actions in real-world scenarios. By seamlessly integrating regression and flow matching within unified backbone, the model demonstrates the potential for more stable inferences and precise EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 8: Instruction-following in open-world settings. Left: Example scenes from three task types: Instructed Visual Rearrangement (Franka Panda), Sort Grocery Items (Agibot G-1), and WidowX Out-of-Box. Right: Success rates over 18 tasks. decision-making, especially in long-sequence tasks, without relying on action-specific parameters or introducing heuristic bottlenecks. 4.7. Emerging Open-world Embodied Generalization The key challenge for embodied foundation models is generalizing to open-world scenarios where natural language instructions must be grounded into precise, executable actions. To evaluate this capability, we perform generalization assessment using suite of 15 instruction-following tasks across three different embodiments: Franka Panda, Agibot G-1, and WidowX 250 S, all operating in unstructured, object-rich environments. The tasks include Visual Rearrangement (Instructed), which involves 7 instructions over 5 objects to test spatial instruction following and sequential manipulation; Sort Grocery Items, requiring fine-grained recognition and placement of 4 objects under cluttered conditions; and WidowX Out-of-Box, which consists of 13 multi-instruction tasks involving both rigid and articulated objects, assessing adaptability in compact workspaces. Against strong baselines GR00T-N1.5 and π (Pertsch et al., 2025, Black et al., 2024, Bjorck et al., 2025), EO-1 achieves an average completion score of 87.0%, outperforming GR00T-N1.5 (75.0%) and π0 (66.0%). Performance gains are consistent across embodiments and task types, with EO-1 maintaining high accuracy even under precise spatial references and unseen objectinstruction combinations, demonstrating robustness to both language and visual distribution shifts. We observe executes tasks quickly and accurately, it struggles to follow instructions, often prioritizing that while π0 objects closer to itself instead of adhering to the specified commands. This behavior may be due to the negative impact of pre-training on the entire backbone, which can lead to suboptimal decision-making in dynamic environments. In contrast, GR00T-N1.5, which freezes the language backbone during training, shows improved instruction-following capabilities. As shown in Figure 9a, we further extend the evaluation to 12 tasks designed to probe generalization along three orthogonal variation axesVisual Generalization: invariance to changes in object appearance, background, and lighting, tested on WidowX Out-of-Box, Franka Panda Pick-and-Place, and Agibot G-1 household tasks (Fold Clothes, Make Sandwich, Roast Beef). Action Generalization: adaptation to altered object positions, new viewpoints, and unseen object instances with different physical properties. Language Generalization: robustness to typos, paraphrases, and vague or imprecise instruction formulations. The results are shown in Figure 9b, EO-1 attains the highest overall completion score (73.0%) across all settings, outperforming GR00T-N1.5 (60.0%) and π0 (51.0%). The advantage holds across all variation types: 67.0% in Action, 79% in Language, and 72.0% in Visual. The largest relative gain appears in language variations, indicating strong linguistic robustness, which is persistent weakness of many current VLM-based policies. These findings confirm that interleaved visiontextaction training not only strengthens in-distribution manipulation 18 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control (a) Examples of the three generalization axes. Visual: Changes in object appearance, background, and lighting. Action: Altered object positions, novel viewpoints, or new object instances. Language: Typos, rephrasings, and fuzzy descriptions. (b) Generalization performance breakdown. Success rates for EO-1, GR00T-N1.5, and π0 language variations. across visual, action, and Figure 9: Generalization illustration and performance evaluation with visual, action, and language variations. performance but also provides resilience to distribution shifts in perception, control, and instructions, which is critical for real-world deployment of generalist robot policies. We observe that EO-1 demonstrates strong robustness In Action Domain tasks, such as instruction variations like \"Carefully place the aluminum foil box youre holding into the oven on the counter\" versus \"Place the aluminum foil box in right arm into the oven.\", or changes in lighting conditions. This is attributed to the models co-training on interleaved vision-text-action datasets, including grounding boxes, 2D trajectories, instruction fuzzing, and task reasoning corpora. This multi-modal training approach enhances EO-1 generalization ability in open-world scenarios, allowing it to adapt to diverse and dynamic environments. Despite these strengths, we also encounter challenges in generalizing Out of the Action Domain, particularly when working with limited robot control data. For example, in the Make Breakfast Sandwich task, variations in the position of the bread lead to significant drop in success rates. On the other hand, in the WidowX Out-of-Box task with large-scale 19 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Bridge data, where densely annotated interleaved data is available, we see marked improvement in success rates. This suggests that interleaved vision-text-action co-training can fine-tune the model at the feature level, allowing it to map actions better and thus enhancing its generalization potential. 4.8. Enhanced Generalization with Unified Reasoning To evaluate whether sole interleaved visiontextaction policy can seamlessly integrate high-level reasoning with low-level control in real environments, we design reasoning-control benchmark comprising four tasks: Visual Rearrangement, Tic-Tac-Toe, Make Breakfast Sandwich, and Roast Beef Steak. These tasks require joint perception, spatial reasoning, multi-step planning, and bimanual , which manipulation under real-world constraints. Unlike hierarchical baselines, GR00T-N1.5 and π0 use an external LLM planner with separate controller (GPT-4o), EO-1 integrates planning and control within single decoder, aligning reasoning and motor execution in an interleaved sequence. The four tasks are described as follows. 1. Visual Rearrangement. The robot must arrange objects in target layout according to reference image (visual prompt). It retrieves and places items sequentially, considering spatial relationships and maintaining collision-free motion. Success depends on interpreting visual cues and planning precise placements, such as positioning large objects (e.g., penguin) to anchor the scene. 2. Tic-Tac-Toe. The robot perceives the game state, reasons about optimal moves, and places markers to either win or block the opponent. This task evaluates the integration of visual perception, strategic decision-making, and precise placement, requiring real-time dynamic control. 3. Make Breakfast Sandwich (Reasoning Control). The robot assembles sandwich by coordinating both arms to manipulate bread, ham, and lettuce. It plans the order of ingredients, retrieves items, and completes the sandwich. The task emphasizes long-horizon sequencing, bimanual coordination, and maintaining object stability throughout the process. 4. Roast Beef Steak (Reasoning Control). The robot performs cooking sequence involving brushing oil, placing the steak in the oven, and operating the oven controls. It coordinates both arms and times actions to ensure proper cooking, evaluating temporal planning and multi-step manipulation. We carefully constructed reasoning control data for model training. In Visual Rearrangement, the task involves arranging 5 objects across 16 grids, with 800 demonstrations collected for all 80 unique action steps. In Tic-Tac-Toe, the robot plays as red, performing 50 actions per step, resulting in 450 data samples that challenge real-time decision-making and strategic planning. The Make Breakfast Sandwich and Roast Beef Steak tasks rely on control data from section 4.6, emphasizing bimanual coordination and long-horizon task execution. All tasks utilize the interleaved data construction pipeline from Figure 3(b)(c), annotating spatial data (bounding boxes, points, and trajectories) to track object positions and scene state, verify task completion, and guide task planning. During training, EO-1 is fine-tuned with pre-trained weights, combining regression and flow matching. Reasoning Control with EO-1. Figure 10 illustrates qualitative rollouts of EO-1 performing four tasks. In Tic-Tac-Toe, the model perceives the board state, moves to create threats or block the opponent, such as placing piece at (0, 2) to set up circle and later blocking (1, 0) to prevent loss. It executes precise piece placements without hesitation. In Visual Rearrangement, EO-1 parses target layout image, identifies key spatial anchors like placing the gray penguin at (0, 2) to define workspace boundaries, and incrementally matches the arrangement while avoiding collisions. In Roast Beef Steak, the model coordinates both arms: brushing oil on the steak, placing it in the oven at the optimal stage (Step 6), closing the door, and pressing the power button. This showcases its temporal reasoning and efficient role allocation between arms. Finally, in Make Breakfast Sandwich, EO-1 plans the assembly by retrieving ingredientsbread, ham, and lettucein the correct sequence, finishing with the final bread slice to secure the sandwich fillings. 20 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 10: Qualitative rollouts for unified reasoning and control. Each sequence shows how EO-1 perceives the scene, reasons about next steps, and executes precise actions under single interleaved policy: (a) TicTac-Toe, (b) Visual Rearrangement, (c) Roast Beef Steak, (d) Make Breakfast Sandwich. Planning and execution remain aligned throughout, avoiding planact mismatches common in hierarchical pipelines. Quantitative results. As shown in Figure 11, EO-1 achieves the highest average completion score across the four tasks, outperforming GR00T-N1.5 and π0 . Per-task gains are consistent: Make and 66.0%/GR00T-N1.5), Roast Beef Steak (55.0% vs. Breakfast Sandwich (84.0% vs. 70.0%/π0 41.0% and 46.0%), Visual Rearrangement (79.0% vs. 66.0% and 47.0%), and Tic-Tac-Toe (76.0% vs. 24.0% and 36.0%). The largest margin occurs in Tic-Tac-Toe (+40.0 points over the best baseline), reflecting superior game-state reasoning and real-time execution. The next largest is in Make Breakfast Sandwich (+14.0 points), showing strong temporal sequencing and precise control. We attribute these gains to the alignment of natural characteristics achieved by the interleaved vision-text-action training, which unifies symbolic planning and continuous control within cohesive backbone. This eliminates the interface gap between planner and controller, reducing error propagation and enabling smooth, context-aware action execution. Qualitative sequences confirm that EO-1 maintains coherent task strategies while performing fine-grained and stable manipulations, which is an essential capability for robust agents embodied in the open world. 21 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 11: Success rates on the Reasoningcontrol benchmark. EO-1 outperforms both hierarchical baselines on all tasks and on average, with the largest gains in Tic-Tac-Toe and Roast Beef Steak. 4.9. Discussion: Architecture and Data Contributions to Generalization In this section, we examine how the unified architecture and interleaved multimodal pre-training affect model generalization along the three axes defined in Section 4.7: visual, action, and language. Our objective is to isolate the contributions of (i) hybrid decoding mechanism that combines discrete autoregression with continuous flow matching under shared parameters, and (ii) large-scale, interleaved visiontextaction supervision, to overall robustness and scaling behavior in the open world. The benchmark settings include: 1. LIBERO (Liu et al., 2023a): Evaluates the models capacity to fit training distributions and execute long-horizon, multi-stage manipulation, stressing policy learning and temporal credit assignment. 2. Agibot Sandwich, single real-world bimanual task requiring precise sequencing of bread, ham, and lettuce placementused to assess the impact of interleaved embodied data on single-task. 3. WidowX Generalization with BridgeData V2 (Walke et al., 2023): Five task groups selected from the WidowX 250 Prepare Vegetables and Arrange Cups suites. We adopt the generalization protocols in Section 4.7: Visual (changes in object appearance, background, lighting), Action (altered object positions, novel viewpoints, new instances), and Language (typos, rephrasings, fuzzy descriptions). For training, we randomly sample relevant demonstrations from BridgeData at four scales: 50, 500, 5k, and 50k. For the model configurations, we explore the following settings: 1. EO-1 (fast): fully autoregressive baseline using the Fast-tokenizer (Pertsch et al., 2025), trained on Bridge Data with interleaved visiontextaction pairs. 2. EO-1 (base): Our proposed unified hybrid decoding model that is trained solely on robot control data. discrete autoregressive decoding for vision language tokens and continuous flow-matching denoising for robotic actions, sharing parameters across modalities. 3. EO-1 (llava): EO-1 co-trained on robot control data with general visionlanguage instruction datasets (LLaVA-Instruct-150K, RefCOCO) to improve grounding and natural language understanding. 4. EO-1 (interleaved): EO-1 trained with interleaved visiontextaction sequences from corresponding data subsets, emphasizing Physical Common Sense, Spatial Understanding, State Estimation, and Task Reasoning. Unified Decoding Across Modalities Outperforms Autoregression. As shown in Figure 12, our hybrid model (EO-1 (base)) consistently outperforms fully autoregressive EO-1 (fast) across all generalization regimes and data scales. With only 50 demonstrations, EO-1 achieves up to +0.25 22 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 12: WidowX generalization performance across data scales for EO-1 (base), EO-1 (interleaved), and EO-1 (fast) with interleaved visiontextaction data. LIBERO Benchmark (Arch) EO-1 (fast) EO-1 (base) Agibot Sandwich (Data) EO-1 (base) EO-1 (llava) EO-1 (interleaved) WidowX Generalization (Data) EO-1 (base) EO-1 (llava) EO-1 (interleaved) LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long 93.3% 99.7% 95.5% 99.8% 88.0% 99.2% 74.0% 94.8% Overall 88.0% 98.2 % In-distribution Visual Generalization Language Generalization Action Generalization Overall 0.85 0.84 0.89 0.83 0.44 0.88 0.66 0.62 0.83 0.75 0.73 0. 0.77 0.66 0.84 Out-of-Box Visual Generalization Language Generalization Action Generalization Overall 0.87 0.37 0.91 0.73 0.37 0.73 0.59 0.35 0. 0.67 0.28 0.73 0.71 0.34 0.80 Table 5: LIBERO, WidowX and Agibot evaluation for EO-1 (base), EO-1 (interleaved), and EO-1 (fast) with interleaved visiontextaction data. absolute gains in Action Generalization, and the margin persists at scale, reaching 0.91 Out-of-Box at 50k demos while pure autoregression plateaus. In particular, pure AR does not show emerging Language Generalization, stalling at 0.43 accuracy even as data grow, and its Action Generalization saturates at 0.23 from 5 to 50k demonstrations. Similar patterns appear in Table 5: in LIBERO, EO-1 (fast) achieves 88.0% overall success rate compared to 98.2% for EO-1 (base), underscoring the precision gains from incorporating flow-matching into action decoding. Across tasks, discrete-only decoding struggles to capture low-frequency action modes and fails to generalize robustly under distribution shift, even with large-scale data. In contrast, our hybrid architecture unifies discrete autoregression with continuous flow matching, enabling more accurate control and scaling-driven improvements that emerge strongly in open-world scenarios. Scaling Interleaved VisionTextAction Data Boosts Generalization. Scaling interleaved demonstrations from 50 to 50k ( Figure 12) shows that EO-1 (interleaved) consistently outperforms EO-1 (base), with faster and more robust generalization as the data grows. This is especially evident in Language Generalization, where EO-1 (interleaved) improves significantly from 0.27 to 0.82, compared to EO-1 (base), which increases from 0.01 to 0.59. This gap highlights the advantage of incorporating multimodal data aligned with both language and control tasks, enabling faster learning and more effective generalization. However, not all multimodal data are beneficial. As shown in Table 5, incorporating generic instruction-following data, such as EO-1 (llava), leads to performance drop, from 0.77 to 0.66 on Agibot Sandwich and from 0.71 to 0.34 on WidowX Generalization. This suggests that multimodal data misaligned with embodied control tasks biases the model toward linguistic priors, weakening its physical grounding. We observe that Out of the Action Domain generalization struggles on limited datasets, despite dense multimodal annotations. In Agibot Sandwich, performance stays at 0.75 for both the base model and EO-1 (interleaved). In contrast, WidowX, trained on large-scale, diverse Bridge Data, improves significantly (0.73 vs. 0.67), EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control highlighting how multimodal data in multi-task settings enhances action generalization. These results underscore the need for finely annotated, task-aligned multimodal datasets incorporating physical knowledge for effective embodied control. Finally, The superior performance of our unified architecture, which combines discrete autoregression for visionlanguage understanding with continuous flow matching for action generation, demonstrates how joint modeling enhances both control precision and generalization. This approach not only strengthens temporal alignment between multimodal understanding and motor control but also ensures stable, high-performance outcomes even in open-world, distribution-shifted scenarios, proving the power of integrating multiple modalities under unified framework. 5. Related Work Vision-Language-Action Models. Vision-language-action models (VLAs) have emerged as promising approach for generalizable robot control by extending pre-trained vision-language models (VLMs) to action prediction. Autoregressive approaches (Brohan et al., 2023, Kim et al., 2024, Qu et al., 2025, Pertsch et al., 2025, Liu et al., 2024b) discretize continuous actions into tokens with linear bins, spatial grids or quantized DCT byte-pair encoding, naturally aligning with VLM training paradigms and exhibiting inherent advantages in knowledge transfer from web-scale pre-training. Despite these advantages, this discrete paradigm suffers from inference latency and limited resolution for high-frequency control tasks. Alternative works (Octo et al., 2024, Liu et al., 2024b, Black et al., 2024, Bjorck et al., 2025) introduce continuous action experts via diffusion or flow-matching (Ho et al., 2020, Lipman et al., 2022), achieving fast inference and dexterous robotic control. Several efforts (Liu et al., 2025, Kim et al., 2025) attempt to combine discrete autoregression and continuous denoising, employing collaborative action ensemble or two-stage tuning procedure. However, these models suffer from the challenges of knowledge preservation and training efficiency, as the gradients from the action expert degrade the pre-trained VLM backbone (Driess et al., 2025, Black et al., 2025). Co-training strategies have emerged as promising solution that integrates diverse data sources (Black et al., 2025) employs two-stage and enables generalization to new environments. π0.5 co-training approach: first pre-training in multimodal data sources with FAST tokenized actions, then tuning the action expert for low-level control. Building on this foundation, (Driess et al., 2025) formalizes single-stage recipe insulating the VLM backbone during VLA training. Complementary works (Lin et al., 2025, Zhou et al., 2025) further explore reasoning and generalization through π0 -style co-training or purely autoregressive paradigms. Nevertheless, instruction following capabilities remain suboptimal, fundamentally due to architectural bottlenecks and the fact that current approaches are predominantly trained on separate image-text pairs or robotic episodes data, overlooking the rich temporal dynamics and causal relationships inherent in embodied interactions. Addressing these challenges necessitates better co-training strategies, improved cross-modal transfer mechanisms, and larger-scale interleaved datasets that comprehensively capture the full spectrum of robotic episodes. Unified Multimodal Models. Unifying understanding and generation (Deng et al., 2025, Chen et al., 2025b, Liao et al., 2025) has witnessed remarkable progress and emergent properties beyond individual specialized models. Pioneering unified multimodal models (Zhou et al., 2024, Team, 2024, Xie et al., 2024) integrate these capabilities through autoregressive or diffusion modeling in single architecture, fusing both text autoregression and visual generation in the shared backbone. Recent advancements (Deng et al., 2025, Ma et al., 2025a, Chen et al., 2025b, Xie et al., 2025) introduce modality-specific paramaters or dual visual encoding to optimize training pipelines, facilitate joint token semantic and superior performance. Alternative studies (Chen et al., 2025a, Dong et al., 24 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control 2023, Ge et al., 2024, Pan et al., 2025) explored unified models with frozen LLMs, assemble LMMs backbones and visual generative models via adapters or learnable queries. While assembling frozen LLMs struggles to demonstrate promising capabilities in robotic learning (Kim et al., 2024, Bjorck et al., 2025, Qu et al., 2025), unifying understanding and generation inspires architecture design in representative works (Black et al., 2024, 2025, Bjorck et al., 2025) and exhibits significant emergent open-world capabilities. 6. Conclusion and Future Work In this article, we present EO-Robotics, fully open training recipe to foster the research community to develop an advanced embodied foundation model. EO-Robotics is composed of EO-1 model, EOData1.5M dataset, and EO-Bench benchmark, where EO-Data1.5M is collected by scalable pipeline for curating interleaved vision-text-action data to improve EO-1 models open-world generalization. EO-1 effectively synergizes auto-regressive decoding and flow matching denoising in single unified model that is co-trained with web data and curated interleaved embodied data EO-Data1.5M, enabling seamless perception, planning, reasoning, and acting in complex physical environments. Our evaluations across embodied reasoning and robot control experiments demonstrate that EO-1 consistently outperforms strong visionlanguageaction baselines, exhibiting superior multimodal reasoning and dexterous robot control capabilities in open-world generalization. Moreover, EO-Bench presents new benchmark to evaluate embodied reasoning capabilities of embodied foundation models. In summary, EO-Robotics is fully open-sourced to the community and achieves forward step for equipping general-purpose autonomous robots with human-like reasoning and acting abilities. EO-Robotics has made solid step toward developing general embodied AI, featuring with seamless embodied reasoning and action in the open world. While the results with EO-1 demonstrate promising robot control capabilities, there remains key aspects that future work can address. First, we aim to enhance EO-1s reasoning and action ability to handle complex scenarios involving navigation, obstacle avoidance, failure detection and analysis, human intent recognition, and human-robot interaction/cooperation. This requires seamlessly integrate more general multimodal understanding, embodied reasoning, and action abilities into one system, leading to more easy-to-use robots in real life. Second, we plan to explore more efficient design of unified model architecture or asynchronous inference pipeline to enable human-like simultaneous reasoning and action. Finally, we will incorporate more data and robot embodiments (e.g., human data and humanoid robot) into training recipe to enable EO-Robotics to be boarder foundation for general-purpose autonomous robots."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, : vision-language-action flow model for Lachy Groom, Karol Hausman, Brian Ichter, et al. π0 general robot control. arXiv preprint arXiv:2410.24164, 2024. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi_{0.5}: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal modelsarchitecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024b. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024a. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024b. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 26 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Gemini, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023a. Gemini Gemini, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023b. Gemini-Robotics. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787798, Doha, Qatar, October 2014a. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. URL https://aclanthology.org/D14-1086. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787798, Doha, Qatar, October 2014b. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. URL https://aclanthology.org/D14-1086. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-languageaction model. arXiv preprint arXiv:2406.09246, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. 27 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024a. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. In Proceedings of the Conference on Robot Learning (CoRL), 2024b. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. Onetwovla: unified vision-language-action model with adaptive reasoning. arXiv preprint arXiv:2505.11917, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024a. Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, et al. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631, 2025. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024b. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025a. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77397751, 2025b. NextStep-1. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. Octo, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems (RSS), 2024. 28 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control OpenX-Embodiment, Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2024. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 645652. IEEE, 2024. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-languageaction model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Proceedings of the Conference on Robot Learning (CoRL), 2023. Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024a. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024b. 29 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint arXiv:2502.14420, 2025. 30 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control A. Contributors and Acknowledgments A.1. Core Contributors Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Dong Wang, Bin Zhao Model Architecture and Training Delin Qu, Haoming Song, Qizhi Chen Real-Robot Experiments Haoming Song, Delin Qu, Haoran Yang, Jiacheng Bao, Qizhi Chen Simulation and Benchmarking Delin Qu, Haoming Song, Xianqiang Gao, Xinyi Ye Data and Benchmark Curation Zhaoqing Chen, Xianqiang Gao, Qizhi Chen, Delin Qu Research Leads Dong Wang, Bin Zhao A.2. Contributors Qi Lv, Xinyi Ye, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao. A.3. Acknowledgments This work is supported by Shanghai Artificial Intelligence Laboratory. We acknowledge the AgiBot team, including Modi Shi, Cheng Ruan, Guanghui Ren, and Maoqing Yao, for providing hardware support and maintenance of the AgiBot G-1 robots. Special thanks to Xinyi Ye for video editing contributions and Qizhi Chen for webpage design. B. Training Dataset Statistics Dataset Samples Tokens Duration (hr) FPS Camera View Category LLaVA-Video-178K LLaVA-1.5 Pixmo-Points RoboVQA RefCOCO Agibot-Beta DROID (OXE) RT-1 (OXE) Bridge-v2 (OXE) Robo-Mind SO100-Community IPEC-Franka 2.6M 1.1M 1.8M 0.2M 50.1K 5.95B 390.45M 538.92M 218.29M 9.46M 213.8M 116.17B 23.1M 3.7M 2.0M 2.1M 7.9M 0.8M 2560.7 8.41B 428.3 579.32M 338.4 111.1 1.89M 19.3 2.09M 1.19B 72.1 108.52M 19.7 EO-Data1.5M 1.5M 1.0B Total multimodal data Total robot data Total Interleaved data 7.1B 6.2M 253.4M 127.3B 1.5M 1.0B 13.9 13.9 Total 260.6M 135.4B 30 15 3 5 30 30 10 30 30 Third-person Third-person Third-person Egocentric Third-person Web multimodal data Web multimodal data Web multimodal data Web multimodal data Web multimodal data Egocentric, left, right Left, Right, wrist Egocentric Shoulder, left, right, wrist Egocentric, left, right Third-person, wrist Third-person, wrist Real robot Real robot Real robot Real robot Real robot Real robot Real robot Egocentric, left, right Interleaved Embodied Data Egocentric, left, right Interleaved Embodied Data Table 6: Pre-training Dataset Statistics. EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control C. EO-1 Reasoning Examples Figure 13: Object pointing examples. Figure 14: Trajectory prediction examples. 32 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 15: Object referring examples. Figure 16: Multiview reasoning examples. 33 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 17: Task planning examples. Figure 18: Subtask QA examples. 34 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 19: Episode caption examples. Figure 20: Affordance QA examples. 35 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 21: Physical common sense examples. Figure 22: Failure detection examples. Figure 23: Process verification examples. 36 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control D. Data Annotation Pipeline Details In this section, we provide additional details of the data preprocessing pipeline, which complements the description in the main paper. The process is divided into four major components: i) video filtering and curation, ii) video splitting and captioning, iii) QA generation for temporal and spatial reasoning, and iv) cleaning and rewriting. D.1. Video Filtering and Curation The first step focuses on ensuring the diversity and quality of robot videos. Since existing robot datasets often consist of highly redundant scenes collected in constrained environments, we employ feature-based clustering strategy to enhance visual variety. Specifically, pretrained visual backbones are used to extract video features, followed by K-means clustering. From each cluster, we select balanced set of videos, and further apply human-assisted inspection to remove samples from the same scene. Figure 24 illustrates the interface for this process, where clusters are visualized and manually pruned to improve dataset coverage. To further enrich the data for tasks requiring multiview or cross-scene reasoning, we organize all available observations into unified interface (Figure 25). This allows selecting videos with sufficient viewpoint diversity, ensuring that multiview reasoning tasks are well-supported. D.2. Video Splitting and Captioning Once diverse video set is curated, we process them into finer-grained clips. Annotators or pretrained VLMs are instructed to segment videos into meaningful short clips, each containing single subtask. For each clip, descriptive captions are generated to capture the specific robot actions. These captions serve dual purposes: they support video captioning QA data and also provide contextual prompts for subsequent reasoning annotations. Figure 26 and 27 show examples of the prompts used to guide caption generation. D.3. QA Generation for Temporal and Spatial Reasoning The next stage is to construct question-answer pairs that capture both temporal and spatial reasoning abilities. We adopt prompting strategy, where VLMs are guided with carefully designed templates to generate diverse questions, and human annotators refine the final answers. D.3.1. Temporal Reasoning For temporal reasoning, we focus on two key aspects: task planning and physical common sense. Task planning questions require understanding the sequential structure of subtasks, while physical common sense questions evaluate the models ability to reason about object dynamics and feasibility in the physical world. Figure 28 and 29 provide examples of the prompts used to elicit these two categories of questions. D.3.2. Spatial Reasoning For spatial reasoning, we construct four sub-tasks: trajectory prediction, object pointing, object referring, and multiview reasoning. The multiview data are generated using the same prompting strategy as object pointing, without separate prompt design. Figure 30, 31, and 32 show representative prompts used to construct these spatial reasoning tasks. Together, they ensure that the dataset 37 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 24: Web interface for video filtering and clustering. We extract features using pretrained backbone and apply K-means clustering to group visually similar videos. Human verification is then used to remove redundant samples and ensure diversity. captures fine-grained spatial relations necessary for embodied intelligence. Cleaning and Rewriting Finally, to enhance linguistic quality and ensure answer validity, we perform post-processing step. Rule-based filtering is applied to remove invalid or ambiguous spatial answers, and an LLM is prompted to rewrite QA pairs. This step increases textual diversity and introduces natural fuzziness, making the dataset more robust. An example of the rewriting prompt is shown in Figure 33. EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 25: Manual selection of multiview videos. All observations from different datasets are displayed, and the interface allows selecting samples with sufficient viewpoint diversity for multiview reasoning tasks. Figure 26: Video caption prompt. Figure 27: Clip caption prompt. 39 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 28: Task planning prompt. 40 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 29: Physical question prompt. 41 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 30: Trajectory prediction prompt. Figure 31: Object pointing prompt. 42 EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control Figure 32: Object referring prompt. Figure 33: Semantic deblurring prompt."
        }
    ],
    "affiliations": [
        "AgiBot",
        "Fudan University",
        "Northwestern Polytechnical University",
        "Shanghai AI Laboratory"
    ]
}