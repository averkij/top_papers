{
    "paper_title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition",
    "authors": [
        "Jingchao Wang",
        "Haote Yang",
        "Jiang Wu",
        "Yifan He",
        "Xingjian Wei",
        "Yinfan Wang",
        "Chengjin Liu",
        "Lingli Ge",
        "Lijun Wu",
        "Bin Wang",
        "Dahua Lin",
        "Conghui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \\textit{Faithfully Recognize What You've Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 5 5 7 0 . 6 0 5 2 : r GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition Jingchao Wang2, Haote Yang1, Jiang Wu1, Yifan He3, Xingjian Wei1, Yinfan Wang4, Chengjin Liu5, Lingli Ge6, Lijun Wu1, Bin Wang1, Dahua Lin1,7, Conghui He1 1Shanghai Artificial Intelligence Laboratory, 2East China Normal University, 3Peking University, 4Shanghai University, 5Northwestern Polytechnical University, 6Fudan University, 7Chinese University of Hong Kong Correspondence: heconghui@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What Youve Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT."
        },
        {
            "title": "Introduction",
            "content": "Modern chemistry has amassed extensive knowledge, much of it stored in chemical molecular formulas. These formulas are often shown as 2D images in research papers and patents, making them inaccessible to machines and limiting large-scale use[1]. With the rise of Large Language Models (LLMs) and AI, the challenge of using image-based formulas for model training is more pressing. Optical Chemical Structure Recognition (OCSR) technology addresses this by converting images into machine-readable formats like SMILES, crucial for digitizing chemical information. This technology Equal contribution. Project lead. Corresponding author. Preprint. Under review. Figure 1: Comparison of model predictions on molecular graphs with abbreviated structures. Our model accurately retains abbreviations as superatoms, faithfully representing the original image, as indicated by the green circles and text in (d). In contrast, MolScribe[7] and MolNexTR[27] incorrectly expand these abbreviations, as shown by the red and orange circles and text in (b) and (c). Note that in the visualizations of results in (b-d), different types of bonds are not distinguished. drives innovation in pharmaceutical R&D, new material discovery, and chemical production, and advances AI for Chemistry. OCSR is complex pattern recognition task that encounters challenges like intricate molecular structures, varying image quality, diverse drawing styles, and noise. Early methods[2, 3] used image processing rules, effective only for simple, noise-free cases. Recently, deep learning has greatly enhanced OCSR capabilities[4, 5, 6, 7]. However, there is still room for improvement, especially in processing large molecules with many atoms and complex Markush structures[8]. Recently, large Vision-Language Models (VLMs)[9, 10, 11] based on Large Language Models (LLMs)[12, 13, 14] have achieved significant breakthroughs. These models excel in visual perception[15], visual question answering (VQA)[16], and multimodal reasoning[17, 18], and have been applied in medicine[19], autonomous driving[20], remote sensing[21, 22], and OCR[23]. VLMs offer three key advantages over traditional deep learning: they integrate multimodal information, excel in semantic understanding and reasoning, and adapt well across tasks. Recent works like ChemVLM[24], ChemDFM-X[25], and OCSU[26] have applied VLM technology to OCSR, treating it as image captioning to generate SMILES strings from images. However, this approach is less effective compared to graph-parsing methods, and their performance on OCSR tasks needs improvement, as shown in Table 1. After evaluating existing models and identifying key issues, we propose two insights and design principles for applying VLMs to OCSR tasks: (1) Graph Traversal as Visual Chain of Thought: The Chain of Thought (CoT) technique enhances LLM and VLM problem-solving by generating intermediate reasoning steps. In OCSR tasks, visual CoT mechanism that recognizes complex molecules step-by-step can significantly boost performance. Current methods[6, 7, 27] predict atoms and bonds separately, unlike the human approach, leading to redundant computations and increased complexity. To address this, we introduce the Graph Traversal as Visual CoT method, which mimics human graph traversal by interleaving atom and bond predictions in one pass. This streamlines molecular graph parsing, enhancing accuracy and consistency. (2) Faithfully recognize what youve seen: In molecular structure images from papers and patents, abbreviations such as \"Ph\" for phenyl and \"Pr\" for propyl enhance readability but pose challenges for OCSR tasks. Existing methods[7, 27] struggle because their training data contain images with abbreviations but fully expanded annotations, causing learning confusion. To address this, we created data correction pipeline that aligns annotations with images by representing abbreviations as superatoms in the ground truth, improving model accuracy. Building on the above insights and design principles, we present GTR-Mol-VLM, visual large language model tailored for OCSR tasks. GTR-Mol-VLM utilizes the Graph Traversal as Visual Figure 2: The comparison of three paradigms for OCSR. (a) Image-captioning approach: Directly generates SMILES from molecular structure images using either VLMs or specialist models. (b) Graph-parsing approach: Predict atoms and bonds in separate stages to construct molecular graph, which is then converted to SMILES. (c) Our approach: Jointly generates atoms and bonds to form molecular graph, followed by SMILES generation. The molecular graph is then used to construct graph-based SMILES. Chain of Thought approach, which incrementally and interactively navigates all atoms (graph nodes) and chemical bonds (graph edges) in molecule images. We have developed GTR-CoT-1.3M, an SFT dataset specifically designed for VLM OCSR tasks. This dataset contains 1.3 million training samples, each comprising molecular input image, visual CoT process for graph traversal, and the final SMILES string (as shown in Figure 4). portion of the dataset is derived from real images in chemical patents, with annotations taken from the accompanying MOL files. These images often feature many abbreviated structures, whereas the annotations are in fully expanded molecular graph form, creating discrepancies between the images and their annotations, as illustrated in Figure 1. Following the principle of Faithfully recognize what youve seen, we meticulously designed data correction pipeline to systematically correct and filter the annotations of patent training samples (detailed in Section 4.2), resulting in high-quality samples for the patent-derived subset. We introduce MolRec-Bench, the first benchmark designed to evaluate graph parsing accuracy in OCSR tasks for chemical structures. Unlike existing benchmarks that rely on SMILES string comparisons, MolRec-Bench provides precise positional annotations for each atom and bond in molecular images. As demonstrated in Section 2.1, graph-parsing-based OCSR methods outperform image-captioning approaches in several key areas. MolRec-Bench enables detailed evaluation of the advantages. Moreover, traditional SMILES-based evaluations often fail to accurately assess structures with custom functional groups or abbreviations (Section 3.2). MolRec-Bench overcomes this limitation, offering more precise and application-aligned assessment. The contributions of this paper are as follows: (1) We investigate the use of VLM technology for OCSR. To overcome existing limitations, we propose two design principles: Graph Traversal as Visual Chain of Thought and Faithfully Recognize What Youve Seen. These principles, validated through extensive experiments, not only enhance OCSR technology but also have potential applications in related fields like chemical reaction diagram[28, 29] and flowchart analysis[30, 31]. (2) Utilizing these design principles, we created the VLM SFT dataset for OCSR, called GTR-CoT1.3M, and developed specialized multimodal model, GTR-Mol-VLM. The model demonstrates superior performance, especially with complex molecular images, thus advancing OCSR technology to better meet practical needs. (3) We critically assess the limitations of current SMILES-based OCSR evaluation methods and present MolRec-Bench, the first benchmark for assessing graph parsing accuracy in OCSR tasks. MolRec-Bench addresses the shortcomings of previous evaluations by accurately assessing structures with custom functional groups or abbreviations, providing more precise and application-aligned evaluation."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 OCSR Methods OCSR methods are generally categorized into image-captioning and graph-parsing approaches. Image-captioning methods treat OCSR as an image captioning task, directly outputting SMILES strings. These models use an encoder to extract visual features and decoder to generate SMILES[32] or InChI[33] sequences. Early models combined convolutional encoders with recurrent decoders (RNN, GRU, or LSTM), such as MSE-DUDL[34], DECIMER[35], Img2Mol[36], ChemPix[37], and MICER[38]. Later works introduced transformer-based architectures, including DECIMER 1.0[4], DECIMER 2.0[39], SwinOCSR[40], IMG2SMI[41], Image2SMILES[42], Image2InChI[43], and MolParser[5]. Recently, vision-language models (VLM) have been applied to this task, as seen in ChemVLM[24], ChemDFM-X[25], and OCSU[26]. Graph-parsing methods predict atoms, bonds, and additional information (e.g., charges) from images to derive molecular graph structures. Early methods used hand-crafted rules for component detection and graph reconstruction[44, 2, 45, 46, 3, 47, 48]. While effective in simple, noise-free scenarios, these methods struggle with complexity and have high maintenance costs. Recent methods leverage deep learning for component detection or segmentation[49, 50, 51], and more recent approaches use deep learning to construct graphs directly[52, 53]. Existing graph-parsing methods, such as MolGrapher[6], MolScribe[7], and MolNexTr[27], use two-stage approach (Figure 2(b)). They first predict atoms (nodes) and then chemical bonds (edges) using classifiers or graph neural networks. This approach results in redundant computations and increased task complexity. 2.2 OCSR Datasets and Evaluation Benchmarks OCSR datasets and benchmarks are categorized into synthetic and real images. In synthetic datasets, tools like RDKit or Indigo generate molecular images from chemical database SMILES [7, 27, 5]. In [54], RanDepict[55] is used to synthesize images that simulate handwritten forms, enhancing model performance on actual handwritten test sets. Real image datasets contain images from patents and literatures. For example, real data from USPTO patent documents are adopted in [7, 27]. [56] is real handwritten dataset containing 5088 samples. Many evaluation datasets come from real patents or papers. The review article[1] organizes multiple evaluation sets from real patents or papers. [57, 7, 6] also introduce evaluation sets from USPTO patents or journals. Most training sets and all evaluation sets focus on image-captioning methods, containing only images and corresponding SMILES. Thus, even graph-parsing methods rely on comparing predicted SMILES with ground truth, limiting direct evaluation."
        },
        {
            "title": "3 Preliminary",
            "content": "3.1 Comparison of Image-Captioning based and Graph-Parsing based Methods Image-captioning methods treat Optical Chemical Structure Recognition (OCSR) as an imagecaptioning task, directly outputting SMILES strings. In contrast, graph-parsing methods predict atoms, bonds, and other molecular information to derive the graph structure. Compared to imagecaptioning methods, graph-parsing methods offer several advantages: (1) Interpretability: Explicit atom and bond predictions facilitate algorithm optimization and robustness analysis. (2) Manual Verification: Predicted results align with input images, supporting manual checks and semi-automated annotation. (3) Expressive Capability: Molecular graphs represent complex structures like Markush structures. (4) Performance: Graph-parsing methods outperform image-captioning methods with equivalent training data. Thus, we chose the graph-parsing method for this study. 3.2 Limitations of SMILES-based Evaluation The Simplified Molecular Input Line Entry System (SMILES) is string-based language for representing molecular structures and reactions. It encodes atoms and bonds in sequence of characters, aiding chemical information storage and transmission (Appendix D). As noted in Section 2.2, OCSR evaluation datasets compare predicted and ground truth SMILES using exact match or similarity measures. Even for graph-parsing methods, the predicted graphs are converted into SMILES for 4 Figure 3: Illustration of limitations in SMILES-based evaluation. (a) The positive example: both the predicted graph and SMILES match with the ground truth. (b) The counterexample: the graph-parsing OCSR algorithm may correctly interpret the molecular graph with abbreviated structures, but due to canonicalization issues, the SMILES might not match the ground truth and be incorrectly judged as prediction error. comparison. This approach has two key limitations: (1) It doesnt directly evaluate the graph structure, resulting in incomplete assessments. (2) SMILESs limited expressiveness struggles with complex molecular expressions, such as Markush structures. Regarding the second point, single molecular image or graph can correspond to multiple SMILES (Figure 3). To ensure unique evaluation, predicted and ground truth SMILES are converted into deterministic SMILES via canonicalization (Appendix D). However, this process struggles with abbreviated structures, reducing evaluation accuracy. For instance, as shown in Figure 3(b), graph-parsing OCSR algorithm may correctly interpret the molecular graph with multiple abbreviated structures, but due to canonicalization issues, the SMILES might not match the ground truth and be incorrectly judged as prediction error. Moreover, studies like MolScribe[7] and MolNexTR[27] replace abbreviated structures with an asterisk (*) in SMILES, ignoring these elements and resulting in incomplete assessments. Refer to Appendix for more details."
        },
        {
            "title": "4 Method",
            "content": "4.1 Our Insights and Design Principles The graph-parsing based OCSR task can be formulated as an image-to-graph generation task. Given an image Im containing molecule m, we train model to convert Im into molecular structure graph Gm = {a1, a2, . . . , ap, b1, b2, . . . , bq}, where ai represents the i-th atom and bj represents the j-th bond, i.e., Gm = (Im). Compared to existing graph-parsing based methods, we have two insights and corresponding design principles. 4.1.1 Graph Traversal as Visual CoT Existing graph-parsing methods[6, 7, 27] use two-stage approach (Figure 2(b)): first predicting atoms (nodes) and then chemical bonds (edges) with classifiers or graph neural networks. This differs from human cognition, which alternates attention between nodes and edges. This approach has drawbacks: (1) Atom prediction does not consider structural constraints from chemical bonds, thus increasing task difficulty. (2) Bond prediction requires attention connections among all atoms, increasing inference complexity and computational cost. We propose the graph traversal as visual CoT method, which mimics human behavior by parsing molecular graphs through prediction of interleaved atom bonds in single traversal (Figure 2(c)). Specifically, we used depth-first traversal strategy to alternately predict atoms and bonds, creating an ordered structural path. During the traversal, the mutual constraints between atoms and bonds allow bond prediction to depend only on previously identified atoms, thus reducing the task difficulty. Moreover, this method acts as Chain-of-Thought for VLMs in parsing molecular graphs, breaking Figure 4: molecule image and its corresponding Q&A ground truth from GTR-CoT-1.3M. down complex recognition into sequential atom/bond tasks, improving prediction accuracy and consistency. 4.1.2 Faithfully Recognizing What Youve Seen In molecular structure images from papers and patents, many abbreviated structures (e.g., Ph for phenyl, Pr for propyl, Bu for butyl, as shown in Figure 1) are common. While these abbreviations improve readability and conciseness, they present challenges for OCSR tasks. We propose treating these abbreviations as \"super atoms\" rather than expanding them. This faithfully recognizing what youve seen approach ensures consistency between images and annotations, optimizing model learning, and significantly enhancing generalization capability. Existing works[7, 27] have significant shortcomings: their patent training data include images with many abbreviated structures, but the annotations (from MOL files) use fully expanded molecular graphs. This mismatch can confuse models, leading to errors when predicting expanded forms for abbreviations seen in images (Figure 1). To address this, we designed data correction pipeline that aligns annotations with images by representing abbreviated functional groups as superatoms (single nodes) in the ground truth graph (as detailed in Section 4.2.3). This correction resolves the misalignment, providing more accurate supervision for model learning. 4.2 Dataset Construction 4.2.1 GTR-CoT-1.3M Description Building on these insights, we developed GTR-CoT-1.3M, specialized SFT dataset for VLM-based OCSR tasks, defined as = {(Im, Gm, Sm, CoTm)}1.3M m=1 where Im is the molecular image, Sm the SMILES string, Gm the molecular structure graph, and CoTm the Chain-of-Thought process based on our \"Graph Traversal as Visual CoT\" principle, as shown in Figure 2(c). For both synthesized and patent data, we ensure that images and their corresponding ground-truth graphs are strictly consistent. Abbreviated functional groups in images are similarly represented in the graphs, adhering to the \"Faithfully recognize what youve seen\" principle, ensuring alignment between visual data and ground truth. 4.2.2 Constitution of GTR-CoT-1.3M Following [7, 27], GTR-CoT-1.3M is composed of two parts: (1) GTR-CoT-PubChem-1M: Following [7], we selected 1 million molecular SMILES from the PubChem database and used the Indigo tool to convert them into molecular images. The difference is that we chose the offline generation method to save the generated data locally as way to accelerate the training process. (2) GTR-CoT-USPTO-351K: This subset was created from 680k samples from the United States Patent and Trademark Office (USPTO), known as USPTO-680K. The annotations are derived from the MOL files accompanying the patents. The images feature many abbreviated structures, while the annotations are in fully expanded molecular graph form. We developed data correction pipeline 6 to systematically correct and filter these annotations (explained in the next subsection). After this process, we obtained 351k high-quality samples, forming the GTR-CoT-USPTO-351K subset. Examples of samples from both the GTR-CoT-PubChem-1M and GTR-CoT-USPTO-351K subsets are shown in Figure 4. 4.2.3 Filtering and Rectification Pipeline for Patent Datasets The rectification and filtering pipeline for USPTO datasets involves: (1) Image Screening: Original images from USPTO-680K are preprocessed to remove samples with multiple molecular structures, ensuring each image corresponds to single structure to avoid ambiguity. (2) Abbreviation Extraction: OCR technology identifies and extracts chemical abbreviations (e.g., \"Ph\", \"Et\") from images for structural alignment. (3) Structure Replacement and Mapping: Using the extracted abbreviations, SMILES strings, atom sequences, and bond information in the ground truth are replaced. mapping table between common abbreviations and their atomic structures guides this replacement. This process ensures high semantic consistency between image content and structural annotations, providing reliable foundation for model learning. Please refer to Appendix for more details. 4.3 Model and Inference We fine-tuned the Qwen-VL 2.5 3B model using the GTR-CoT-1.3M dataset, creating the GTRMol-VLM model. For each sample in the dataset = {(Im, Gm, Sm, CoTm)}1.3M m=1 , the image Im is input into the VLM with specific query (Figure 2(c)). The model learns the ground truth response Rm = concat(CoTm, Sm). This setup trains the model to first map the molecular structure graph in the image, constructing the graph as visual Chain of Thought, and then predict the SMILES string. During inference, the VLMs output is processed to parse and reconstruct the graph structure, from which the SMILES string is derived using predefined rules. We refer to SMILES directly generated from the VLMs output as generated SMILES, and those derived from the graph structure as graph-based SMILES. 4.4 Benchmark We developed MolRec-Bench to overcome the limitations of existing SMILES-based evaluation datasets as detailed in Section 3.2. This benchmark evaluates not only molecular graph structures but also complex scenarios like Markush structures. MolRec-Bench comprises two subsets: (1) MolRec-USPTO: Based on USPTO[58], it includes 5,423 molecular images from USPTO patents. (2) MolRec-Abb: Derived from MolGrapher[6], it features 9,311 molecular images with abbreviated superatoms from USPTO_10K_abb. The construction of MolRec-Bench is followed as the GTR-CoT-USPTO-351K (Section 4.2.2 and Appendix A). Each sample contains the original molecular image, the corrected molecular graph, and the corrected SMILES. For MolRec-Bench, we defined three evaluation metrics: (1) Gen_SMILES: Calculates the exact match ratio by comparing canonicalized ground truth SMILES with predicted SMILES, designed for image-captioning-based OCSR methods. (2) Gra_SMILES: Similar to Gen_SMILES but uses SMILES generated from the predicted molecular graph, suited for graph-parsing-based OCSR methods. (3) Graph: Measures the exact match ratio between the ground truth and predicted graphs for graph-parsing-based OCSR methods. Details are shown in Appendix B."
        },
        {
            "title": "5 Experiments and Results",
            "content": "5.1 Experiment Setups 5.1.1 Baselines To evaluate our method, we selected three types of baseline models: specialist models, open-source VLMs in the chemical domain, and proprietary general-purpose VLMs. 7 Model MolScribe[7] MolScribe[7] MolNexTR[27] MolNexTR[27] OCSU[26] ChemVLM[24] ChemDFM-X[25] GPT-4o*[9] GPT-4o[9] GPT-4o-mini*[60] GPT-4o-mini[60] Qwen-VL-max*[61] Qwen-VL-max[61] GTR-Mol-VLM (Ours) MolRec-Abb MolRec-USPTO Gen_SMILES Gra_SMILES Graph Gen_SMILES Gra_SMILES Graph 20.11 72.20 19.76 74.60 0.40 4.18 0.76 0.60 0.60 0.20 0.00 0.20 0.20 84.50 19.39 69.63 19.00 70. - - - - 0.00 - 0.00 - 0.00 84.50 19.82 70.60 19.47 71.85 - - - - 0.00 - 0.00 - 0.00 85.49 71.77 85.49 71.75 86. 1.71 42.52 26.94 1.60 0.60 0.20 0.00 1.40 1.00 91.19 72.03 84.66 71.90 85.30 - - - - 0.00 - 0.00 - 0.00 91.67 72.25 86.23 72.14 86. - - - - 0.00 - 0.00 - 0.00 93.45 Table 1: Quantitative results of 2 specialist models, 3 chemical-domain VLMs, and 3 proprietary general-purpose VLMs are reported across three exact-match metrics on two sub-benchmarks. The highest value for each metric is marked in bold. Metrics include: Gen_SMILES, Gra_SMILES, and Graph. indicates trained on GTR-CoT-1.3M; indicates officially released checkpoint; models first predict the graph then SMILES; * models directly predict SMILES. (1)Specialist Models: These are tailored for molecular structure recognition, exemplified by MolScribe[7] and MolNexTR[27]. We evaluated both the open-source checkpoints and versions trained from scratch using our GTR-CoT-1.3M dataset. (2) Open-source VLMs in the Chemical Domain: These fine-tuned models, such as ChemVLM[24], ChemDFM-X[25], and OCSU[26], generate SMILES directly but cannot parse molecular graphs. We tested them on MolRec-Bench, reporting only the Gen_SMILES, using prompts from their papers. (3) Proprietary General-purpose VLMs: We compared models like Qwen-VL-Max-2025-04-08[59], GPT-4o-mini-2024-07-18[60], and GPT-4o-2024-08-06[9]. Using two prompt sets, we evaluated their ability to generate SMILES directly (generated SMILES) and via graph parsing (graph-based SMILES). Due to interface costs, we randomly selected 500 samples from MolRec-Abb and MolRecUSPTO for evaluation. 5.1.2 Training Details The training was performed on 32 NVIDIA A100 GPUs using the AdamW optimizer, with peak learning rate of 1.6 104. We applied cosine decay for the learning rate and linear warm-up for the first 10% of iterations to stabilize training. Each GPU had batch size of 2, and we used gradient accumulation over 16 steps, resulting in an effective batch size of 1024. All parameters in the model were updated during training. We utilized DeepSpeed ZeRO Stage 2 to enhance efficiency and reduce memory usage, allowing for effective training of large multimodal models. The model was trained for 3 epochs, balancing performance with computational resources. 5.2 Main Results 5.2.1 Overall Performance In our evaluation on the MolRec-Bench benchmark, we compared baseline models from Section 5.1.1 with GTR-Mol-VLM, as shown in Table 1. GTR-Mol-VLM achieved the highest performance on both MolRec-USPTO and MolRec-Abb subsets in Gen_SMILES, Gra_SMILES and Graph . On MolRec-Abb, it surpassed the second-best model by 9 percentage points in all 3 metrics, highlighting its strength in handling molecular images with abbreviations. Closed-source general-purpose VLMs like GPT-4o[9] and Qwen[59], despite excelling in general vision-language tasks[59], performed poorly on MolRec-Bench, indicating that OCSR hasnt been focus in their development. Chemical-domain VLMs address the limitations of general-purpose VLMs. However, as discussed, existing chemical-domain VLMs using image-captioning approaches lack the flexibility of graph-based methods and often underperform on SMILES metrics compared to 8 earlier specialist models. This underscores the effectiveness of the approach we present. All of our used Prompts are listed in Appendix E. 5.2.2 Further Analysis We selected two specialist models, MolScribe and MolNexTR, for comparison. As shown in Table 1, these graph-parsing-based models perform well on the MolRec-USPTO subset, achieving over 70% accuracy in Gen_SMILES and Gra_SMILES metrics. However, their accuracy drops below 20% on the MolRec-Abb subset, due to issues with image abbreviations versus expanded ground truth graphs, as discussed in Section 4.1.2. In addition, more experimental results and analysis are demonstrated in Appendix C. CoT Gen_SMILES 66.54 68.85 CoT Strategy Atoms-then-bonds Graph-traversal (ours) Gra_SMILES 79.02 81.88 Graph 80.15 83.26 Table 2: The performances of our models with or without CoT strategies. Table 3: The performances of our models using different CoT strategies. Atoms-then-bonds means our model first predicts atoms and then bonds separately as described in [7]. To ensure fair comparison and analyze GTR-Mol-VLMs superior performance, we retrained MolScribe and MolNexTR from scratch using the GTR-CoT-1.3M dataset and official code. Their performance on MolRec-Bench improved significantly, validating the GTR-CoT-1.3M dataset and method. Nevertheless, GTR-Mol-VLM, based on the powerful QwenVL[59] model, still outperforms these pretrained models. 5.3 Ablation Studies To further validate the effectiveness of the key design components in our proposed method, we conducted two ablation experiments, systematically analyzing the method from multiple perspectives: (1) Effectiveness of CoT: To evaluate the impact of the CoT paradigm on model performance, we conducted the following comparative experiments. For quick conclusions, we trained the model on the GTR-CoT-USPTO-351K subset, employing our graph traversal as visual CoT strategy, and compared it with strategy that directly predicts SMILES without using CoT. As shown in Table 2, the model achieved performance gain of 2.31% on Gen_SMILES when using the CoT strategy. This demonstrates that the CoT approach, which decomposes the complex molecular structure recognition task, effectively enhances model performance and stability. (2) Choice of CoT Strategies: To assess the effectiveness of different graph-parsing strategies for molecular structure recognition, we compared our graph traversal as visual CoT with the Atom-then-bonds method. In this method, the model first predicts all atoms (including types and positions) as graph nodes, then predicts all chemical bonds (including types and connection indices) as graph edges, and finally predicts the SMILES representation. This Atom-then-bonds method simulates the typical structure prediction approach used in specialized models like MolScribe[7] and MolNexTR[27]. As shown in Table 3, our Graph-traversal CoT method achieved performance gain of 2.86% and 3.11% on Gra_SMILES, demonstrating the effectiveness and validity of the graph traversal as visual CoT mechanism."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces GTR-VLM, visual large language model designed for OCSR tasks, along with its accompanying SFT training dataset, GTR-CoT-1.3M. GTR-VLM is developed based on two core principles: Graph Traversal as Visual Chain-of-Thought and Faithfully Recognizing What Youve Seen. The experiments demonstrate that these innovative concepts not only significantly enhance the applicability and flexibility of OCSR models but also effectively improve model performance. Furthermore, GTR-VLM robustly handles challenging scenarios, such as Markush structures. Additionally, our release of the MolRec-Bench addresses the gap in existing OCSR evaluation sets by providing means to assess molecular graph structure parsing results. We anticipate that GTR-VLM 9 and the related dataset will drive OCSR technology toward meeting real-world needs more effectively, thereby advancing the fields of cheminformatics and AI for Science."
        },
        {
            "title": "7 Limitations",
            "content": "While GTR-VLM achieves strong results on MolRec-Abb and MolRec-USPTO, further improvements are possible through the integration of advanced reinforcement learning strategies and the development of higher-quality datasets. Despite these opportunities for enhancement, our approach constitutes pioneering contribution to the field, offering novel, robust solution for OCSR and providing meaningful guidance for downstream applications such as automated chemical literature analysis."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported by National Key R&D Program of China (2022ZD0160201)."
        },
        {
            "title": "References",
            "content": "[1] Kohulan Rajan, Henning Otto Brinkhaus, Achim Zielesny, and Christoph Steinbeck. review of optical chemical structure recognition tools. Journal of Cheminformatics, 12:113, 2020. [2] Igor Filippov and Marc Nicklaus. Optical structure recognition software to recover chemical information: Osra, an open source solution, 2009. [3] Tyler Peryea, Daniel Katzel, Tongan Zhao, Noel Southall, and Dac-Trung Nguyen. Molvec: Open source library for chemical structure recognition. In Abstracts of papers of the American Chemical Society, volume 258. Amer Chemical Soc 1155 16TH ST, NW, WASHINGTON, DC 20036 USA, 2019. [4] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Decimer 1.0: deep learning for chemical image recognition using transformers. Journal of Cheminformatics, 13:116, 2021. [5] Xi Fang, Jiankun Wang, Xiaochen Cai, Shangqian Chen, Shuwen Yang, Haoyi Tao, Nan Wang, Lin Yao, Linfeng Zhang, and Guolin Ke. Molparser: end-to-end visual recognition of molecule structures in the wild. arXiv preprint arXiv:2411.11098, 2024. [6] Lucas Morin, Martin Danelljan, Maria Isabel Agea, Ahmed Nassar, Valery Weber, Ingmar Meijer, Peter Staar, and Fisher Yu. Molgrapher: graph-based visual recognition of chemical structures. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1955219561, 2023. [7] Yujie Qian, Jiang Guo, Zhengkai Tu, Zhening Li, Connor Coley, and Regina Barzilay. Molscribe: robust molecular structure recognition with image-to-graph generation. Journal of Chemical Information and Modeling, 63(7):19251934, 2023. [8] Lucas Morin, Valéry Weber, Ahmed Nassar, Gerhard Ingmar Meijer, Luc Van Gool, Yawei Li, and Peter Staar. Markushgrapher: Joint visual and textual recognition of markush structures. arXiv preprint arXiv:2503.16096, 2025. [9] Hello gpt-4o. https://openai.com/index/hello-gpt-4o/. [10] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 10 [12] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. [13] Jing Shao, Siyu Chen, Yangguang Li, Kun Wang, Zhenfei Yin, Yinan He, Jianing Teng, Qinghong Sun, Mengya Gao, Jihao Liu, Gengshi Huang, Guanglu Song, Yichao Wu, Yuming Huang, Fenggang Liu, Huan Peng, Shuo Qin, Chengyu Wang, Yujie Wang, Conghui He, Ding Liang, Yu Liu, Fengwei Yu, Junjie Yan, Dahua Lin, Xiaogang Wang, and Yu Qiao. Intern: new learning paradigm towards general vision, 2022. [14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [15] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), December 2024. [16] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. [17] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark, 2024. [18] Junyuan Gao, Jiahe Song, Jiang Wu, Runchuan Zhu, Guanlin Shen, Shasha Wang, Xingjian Wei, Haote Yang, Songyang Zhang, Weijia Li, Bin Wang, Dahua Lin, Lijun Wu, and Conghui He. Pm4bench: parallel multilingual multi-modal multi-task benchmark for large vision language model, 2025. [19] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day, 2023. [20] Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, and Zhen Xie. Cityllava: Efficient fine-tuning for vlms in city scenario, 2024. [21] Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, and Conghui He. Vhm: Versatile and honest vision language model for remote sensing image analysis, 2024. [22] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model, 2024. [23] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, and Xiangyu Zhang. General ocr theory: Towards ocr-2.0 via unified end-to-end model, 2024. [24] Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Wei Li, Shufei Zhang, Mao Su, Wanli Ouyang, Yuqiang Li, and Dongzhan Zhou. Chemvlm: Exploring the power of multimodal large language models in chemistry area, 2025. [25] Zihan Zhao, Bo Chen, Jingpiao Li, Lu Chen, Liyang Wen, Pengyu Wang, Zichen Zhu, Danyang Zhang, Yansi Li, Zhongyang Dai, Xin Chen, and Kai Yu. Chemdfm-x: towards large multimodal model for chemistry. Science China Information Sciences, 67(12), December 2024. [26] Siqi Fan, Yuguang Xie, Bowen Cai, Ailin Xie, Gaochao Liu, Mu Qiao, Jie Xing, and Zaiqing Nie. Ocsu: Optical chemical structure understanding for molecule-centric scientific discovery, 2025. [27] Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, and Hanyu Gao. Molnextr: generalized deep learning model for molecular image recognition. Journal of Cheminformatics, 16(1):141, 2024. [28] Yujie Qian, Jiang Guo, Zhengkai Tu, Connor W. Coley, and Regina Barzilay. Rxnscribe: sequence generation model for reaction diagram parsing, 2023. [29] Yufan Chen, Ching Ting Leung, Jianwei Sun, Yong Huang, Linyan Li, Hao Chen, and Hanyu Gao. Towards large-scale chemical reaction image parsing via multimodal large language model, 2025. [30] Junyi Ye, Ankan Dash, Wenpeng Yin, and Guiling Wang. Beyond end-to-end vlms: Leveraging intermediate text representations for superior flowchart understanding, 2024. [31] Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, Spandana Gella, Sai Rajeswar, and Perouz Taslakian. Starflow: Generating structured workflow outputs from sketch images, 2025. [32] David Weininger. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):3136, 1988. [33] Stephen Heller, Alan McNaught, Stephen Stein, Dmitrii Tchekhovskoi, and Igor Pletnev. Inchithe worldwide chemical structure identifier standard. Journal of cheminformatics, 5:19, 2013. [34] Joshua Staker, Kyle Marshall, Robert Abel, and Carolyn McQuaw. Molecular structure extraction from documents using deep learning. Journal of chemical information and modeling, 59(3):10171029, 2019. [35] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Decimer: towards deep learning for chemical image recognition. Journal of Cheminformatics, 12(1):65, 2020. [36] Djork-Arné Clevert, Tuan Le, Robin Winter, and Floriane Montanari. Img2molaccurate smiles recognition from molecular graphical depictions. Chemical science, 12(42):1417414181, 2021. [37] Hayley Weir, Keiran Thompson, Amelia Woodward, Benjamin Choi, Augustin Braun, and Todd Martínez. Chempix: automated recognition of hand-drawn hydrocarbon structures using deep learning. Chemical science, 12(31):1062210633, 2021. [38] Jiacai Yi, Chengkun Wu, Xiaochen Zhang, Xinyi Xiao, Yanlong Qiu, Wentao Zhao, Tingjun Hou, and Dongsheng Cao. Micer: pre-trained encoderdecoder architecture for molecular image captioning. Bioinformatics, 38(19):45624572, 2022. [39] Kohulan Rajan, Henning Otto Brinkhaus, Isabel Agea, Achim Zielesny, and Christoph Steinbeck. Decimer. ai: an open platform for automated optical chemical structure identification, segmentation and recognition in scientific publications. Nature communications, 14(1):5045, 2023. [40] Zhanpeng Xu, Jianhua Li, Zhaopeng Yang, Shiliang Li, and Honglin Li. Swinocsr: end-to-end optical chemical structure recognition using swin transformer. Journal of Cheminformatics, 14(1):41, 2022. [41] Daniel Campos and Heng Ji. Img2smi: translating molecular structure images to simplified molecular-input line-entry system. arXiv preprint arXiv:2109.04202, 2021. [42] Ivan Khokhlov, Lev Krasnov, Maxim Fedorov, and Sergey Sosnin. Image2smiles: Transformer-based molecular optical recognition engine. Chemistry-Methods, 2(1):e202100069, 2022. 12 [43] Da-zhou Li, Xin Xu, Jia-heng Pan, Wei Gao, and Shi-rui Zhang. Image2inchi: Automated molecular optical image recognition. Journal of Chemical Information and Modeling, 64(9):3640 3649, 2024. [44] Syed Saqib Bukhari, Zaryab Iftikhar, and Andreas Dengel. Chemical structure recognition (csr) system: automatic analysis of 2d chemical structures in document images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 12621267. IEEE, 2019. [45] Joe McDaniel and Jason Balmuth. Kekule: Ocr-optical chemical (structure) recognition. Journal of chemical information and computer sciences, 32(4):373378, 1992. [46] Tom Ouyang and Randall Davis. Chemink: natural real-time recognition system for chemical drawings. In Proceedings of the 16th international conference on Intelligent user interfaces, pages 267276, 2011. [47] Noureddin Sadawi, Alan Sexton, and Volker Sorge. Chemical structure recognition: rule-based approach. In Document recognition and retrieval XIX, volume 8297, pages 101109. SPIE, 2012. [48] Viktor Smolov, Fedor Zentsev, and Mikhail Rybalkin. Imago: Open-source toolkit for 2d chemical structure image recognition. In TREC, 2011. [49] Martijn Oldenhof, Adam Arany, Yves Moreau, and Jaak Simm. Chemgrapher: optical graph recognition of chemical compounds by deep learning. Journal of chemical information and modeling, 60(10):45064517, 2020. [50] Youjun Xu, Jinchuan Xiao, Chia-Han Chou, Jianhang Zhang, Jintao Zhu, Qiwan Hu, Hemin Li, Ningsheng Han, Bingyu Liu, Shuaipeng Zhang, et al. Molminer: you only look once for chemical structure recognition. Journal of Chemical Information and Modeling, 62(22):5321 5328, 2022. [51] Xiao-Chen Zhang, Jia-Cai Yi, Guo-Ping Yang, Cheng-Kun Wu, Ting-Jun Hou, and Dong-Sheng Cao. Abc-net: divide-and-conquer based deep learning architecture for smiles recognition from molecular images. Briefings in bioinformatics, 23(2):bbac033, 2022. [52] Yujie Qian, Zhengkai Tu, Jiang Guo, Connor Coley, and Regina Barzilay. Robust molecular image recognition: graph generation approach. Technical report, Technical Report, 2022. [53] Sanghyun Yoo, Ohyun Kwon, and Hoshik Lee. Image-to-graph transformers for chemical structure recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 33933397. IEEE, 2022. [54] Kohulan Rajan, Henning Otto Brinkhaus, Achim Zielesny, and Christoph Steinbeck. Advancements in hand-drawn chemical structure recognition through an enhanced decimer architecture. Journal of Cheminformatics, 16(1):78, 2024. [55] Henning Otto Brinkhaus, Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Randepict: Random chemical structure depiction generator. Journal of cheminformatics, 14(1):31, 2022. [56] Henning Otto Brinkhaus, Achim Zielesny, Christoph Steinbeck, and Kohulan Rajan. Decimerhand-drawn molecule images dataset. Journal of Cheminformatics, 14(1):36, 2022. [57] Joshua Staker, Kyle Marshall, Robert Abel, and Carolyn McQuaw. Molecular structure extraction from documents using deep learning, 2018. [58] Kohulan Rajan, Henning Otto Brinkhaus, Achim Zielesny, and Christoph Steinbeck. review of optical chemical structure recognition tools. Journal of Cheminformatics, pages 113, 2020. [59] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [60] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. [61] Introducing qwen-vl. https://qwenlm.github.io/blog/qwen-vl/."
        },
        {
            "title": "A Dataset",
            "content": "A.1 Construction of GTR-CoT-USPTO-351K Figure 5: The pipeline to construct GTR-CoT-USPTO-351K. Step filtered samples with only one molecule in the image. Step detected all the abbreviations that need to be replaced utilizing the OCR technology. Step replaced the atoms in the SMILES with the abbreviations, and modified the node_coords and the bonds to align with the replacement. Due to the discrepancies between molecular structure images and their corresponding ground truth (SMILES[32], node_coords, and bonds), it is necessary to correct the ground truth in the USPTO680K[58] dataset. The process is illustrated in Figure 5. In step a, subset of 365k samples, each containing single molecule, is selected from the USPTO-680K dataset. In step b, Optical Character Recognition (OCR) is applied to identify all characters in the molecular structure images. In step c, abbreviations identified by OCR that appear in the abbreviation-superatom mapping table are flagged for replacement in the ground truth. The replacement algorithm is rule-based, and its core logic determines whether the atomic combinations corresponding to each abbreviation are present in the SMILES notation, thereby guiding the decision to replace them. The input required for the replacement algorithm includes the original SMILES, node_coords, bonds, the OCR results, and the abbreviation-atom mapping table, which contains common functional groups in organic chemistry. The algorithm then applies the necessary substitutions to the SMILES, node_coords, and bonds to ensure consistency between the molecular structure and the ground truth. Of the 365k samples, 351k had their ground truth successfully corrected. The corrected dataset is referred to as GTR-CoT-USPTO-351K. A.2 Construction of GTR-CoT-PubChem-1M Following MolScribe[7] and MolNexTR[27], we saved the images generated during their training process, because the various data enhancement operations in them may cause RDKit 4 to fail to render the correct images based on the generated SMILES. Therefore, although our total number of seed SMILES is 1,000,000, the final total number of samples obtained is 999,950. Details of the data enhancement and generation process can be found in MolScribe. A.3 Comparison of GTR-CoT-USPTO-351K and USPTO-680K Figure 6 compares the ground truth of GTR-CoT-USPTO-351K and USPTO-680K. The strict alignment between molecular structure images and reconstructed graphs entails precise prediction results. 4http://www.rdkit.org/ 14 Figure 6: The visualization result of USPTO-680K and GTR-CoT-USPTO-351K. The reconstructed graphs of USPTO-351K (the second column) are unaligned with the molecular structure images. GTR-CoT-USPOTO-351K, however, strictly follow the molecular structure in the images. We use different colors to mark SINGLE, DOUBLE, TRIPLE, and AROMATIC bonds, as well as different colored arrows for BEGINWEDGE and BEGINDASH bonds."
        },
        {
            "title": "B Evaluation",
            "content": "B.1 Evaluation details of OCSR methods Figure 7 demonstrates the details of the evaluation of Molscribe and MolNexTR. The abbreviations of the functional groups occurring in the molecular structure images are simply replaced by * during the evaluation, resulting in the degradation of the evaluation accuracy. B.2 Graph-based Metric Figure 8 demonstrates the details of the evaluation of our graph-based metric. The abbreviations of the functional groups are kept as it is. Meanwhile, the predicted and ground truth graphs are compared directly, instead of comparing the SMILES generated by these graphs, leading to more accurate and direct evaluation diagram."
        },
        {
            "title": "C More Experiment Results",
            "content": "C.1 More Prediction Visualization Figure 9 shows more prediction visualization results of MolScribe, MolNexTR, and our method. 15 Figure 7: The evaluation process of MolScribe and MolNexTR. All the abbreviations of functional groups are replaced by * when comparing the predicted and ground truth SMILES. Figure 8: The evaluation process of our Graph-based metric. All the functional groups and Markush structures are kept in their abbreviations when comparing the predicted and ground truth graphs. C.2 Bad Case Analysis Figure 10 shows some failure cases of our method. In case 1, there is problem of incorrectly predicting the order of the abbreviations. In case 2. The left side predicted \"HOOC\" retains the abbreviated form, but on the right side, \"COOH\" does the expansion form. In addition. In case 3, the model misses the topmost \"Ac\". This is mainly because there are too few samples of such writing style, and the model has difficulty in predicting the abbreviations above and below most of the time. In case 4, the superatom \"Cbz\" on the far right is repeatedly positioned on carbon atom."
        },
        {
            "title": "D Chemoinformatics Basics",
            "content": "D.1 SMILES SMILES is method for representing the structure of molecule as an ASCII string. David Weininger initially proposed the concept in the 1980s to represent and store information about chemical molecules in computers. The fundamental principle of SMILES is to describe the molecular topology (i.e., the manner in which atoms are interconnected) in single line of string. This representation has applications in areas such as databases, machine learning, molecular searching, and cheminformatics. Common organic atoms can be used directly with their atomic symbols (e.g. \"C\", \"N\", \"O\", \"S\", \"P\", \"F\", \"Cl\", \"Br\", \"I\"). Special or charged atoms are denoted using square brackets, for example, 16 Figure 9: More prediction visualization of MolScribe, MolNexTR, and our method. We use different colors to mark SINGLE bonds, DOUBLE bonds, TRIPLE bonds, and AROMATIC bonds, as well as different colored arrows for BEGINWEDGE bonds and BEGINDASH bonds. Figure 10: Some typical bad cases of our method. We use different colors to mark SINGLE bonds, DOUBLE bonds, TRIPLE bonds, and AROMATIC bonds, as well as different colored arrows for BEGINWEDGE bonds and BEGINDASH bonds. \"[Na+]\", \"[Fe+3]\", and \"[C@H]\". In SMILES, single bonds are typically not represented, double bonds are indicated by \"=\", triple bonds by \"#\", and aromatic bonds by \":\". Such as ethane: \"CC\", ethylene: \"C=C\", and acetylene: \"C#C\". In SMILES, parentheses are frequently employed to denote branched chains, such as isopropanol: \"CC(C)O\". Numbers are employed in SMILES to denote the commencement and cessation of atoms, thus forming ring. For example: cyclohexane: \"C1CCCC1\", benzene: \"c1ccccc1\", (the lowercase \"c\" represents aromatic carbon). SMILES also supports the description of chiral centres; for example, \"[C@H]\" denotes clockwise configuration, while \"[C@@H]\" denotes counterclockwise configuration. 17 In comparison with structural formulae or molecular diagrams, SMILES representations are characterised by their conciseness and ease of use for database storage, searching, and chemical calculations. Furthermore, the majority of SMILES can be reduced to structural formulas and are supported by numerous chemical software applications (e.g., RDKit, Open Babel5). D.2 SMILES Canonicalization SMILES canonicalization is defined as the process of converting molecular structure into unique and canonical SMILES representation. \"CCO\" and \"OCC\" are correct for ethanol, but they are frequently required to possess single, distinct SMILES for given molecule within databases or algorithms. In this instance, the utilisation of canonical SMILES is imperative to establish benchmark. The implementation of canonical SMILES is essential for the standardisation of SMILES. This approach eliminates the need for duplicated data, expedites the process of retrieving and comparing information, and facilitates the learning and chemical modelling processes. Implementing the canonicalization of SMILES is determined by the algorithm, not by the habits of human-written SMILES. It consists of 3 main steps: 1. Molecular graph renumbering: View molecules as graph structures (atoms are nodes and bonds are edges) and renumber atoms, giving each atom an index. 2. Topology-based sorting: sort the molecules according to their structural rules and chemical properties (e.g. atomic number, connectivity, ring structure, etc.) and choose minimum or optimal order. 3. Follow the path after sorting to generate SMILES string in standard form. However, due to the uncertainty of the chemical nature of the abbreviated structures and the fact that they cannot be exhaustively enumerated to give fixed index, it is not possible to distinguish between the two abbreviated structures when they are both located at the endpoints of the molecule. This results in an Incorrect judgment when using canonical SMILE for Exact Match. Figure 11: Prompt for our model to directly predict SMILES."
        },
        {
            "title": "E All prompts",
            "content": "E.1 GTR-CoTs Prompts The following three figures demonstrate the prompt used by our model. Figure 11 demonstrates the prompt for the direct prediction. Figure 12 demonstrates the prompt for predicting the atoms first, then bonds, and finally SMILES. Figure 13 demonstrates the prompt for predicting the atoms and bonds in graph traversal manner first, then predicting SMILES. E.2 Proprietary VLMs Prompts The following two figures demonstrate the prompt used by three proprietary VLMs (Qwen-VL-Max2025-04-08[59], GPT-4o-mini-2024-07-18[60], and GPT-4o-2024-08-06[9]). Figure 14 demonstrates the prompt for the direct prediction of SMILES. Figure 15 demonstrates the prompt for predicting the atoms and bonds in graph traversal manner first, then predicting SMILES. 5http://openbabel.org/ Figure 12: Prompt for our model to predict the atom first, then bonds, and finally SMILES. Figure 13: Prompt for our model to predict atoms and bonds in graph traversal manner first, then predict SMILES (Our method). 19 Figure 14: Prompt for proprietary LLMs to directly predict SMILES. Figure 15: Prompt for proprietary LLMs to predict the atoms and bonds in graph traversal manner first, then predicting SMILES."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "East China Normal University",
        "Fudan University",
        "Northwestern Polytechnical University",
        "Peking University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai University"
    ]
}