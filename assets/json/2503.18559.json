{
    "paper_title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
    "authors": [
        "Takashi Isobe",
        "He Cui",
        "Dong Zhou",
        "Mengmeng Ge",
        "Dong Li",
        "Emad Barsoum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 9 5 5 8 1 . 3 0 5 2 : r AMD-Hummingbird: Towards an Efficient Text-to-Video Model Takashi Isobe1 He Cui1 Dong Li1 1 Advanced Micro Devices, Inc. 2 Tsinghua University Homepage: https://www.amd.com/en/developer/resources/technical-articles.html Github: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V Dong Zhou1 Emad Barsoum1 Mengmeng Ge1,"
        },
        {
            "title": "Abstract",
            "content": "Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g., iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves 31 speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications."
        },
        {
            "title": "Introduction",
            "content": "Text-to-video (T2V) generation [115] has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. While most existing methods focuses on improving visual quality, the efficiency of T2V diffusion models remains underexplored.In practical deployment scenarios, especially on resource-constrained devices such as iGPUs and NPUs, model size and inference speed are critical factors. Achieving both computational efficiency and high visual performance remains significant challenge. Recent efforts [16, 17, 4, 18] aim to improve efficiency by reducing the number of inference steps. For instance, VideoLCM [16] introduces self-consistency loss in the latent space to align model performance across different step counts. AnimateLCM [17] uses consistency distillation to accelerate generation. Despite these advances, the models remain large, which makes deployment on edge devices difficult. One naïve solution is to prune the network structure. However, this often leads to performance degradation because it disrupts the learned diffusion priors, even when followed by fine-tuning. To address this challenge, we propose two-stage diffusion model distillation pipeline that first prunes model parameters and then restores visual quality through visual feedback learning. Specifically, our method reduces the U-Net size from 1.4 billion parameters, as seen in the widely used VideoCrafter2 [2], to 0.7 billion. This enables high-quality video generation with minimal inference steps. The resulting efficient T2V model, named AMD-Hummingbird, achieves 31 speedup over VideoCrafter2 when deployed on AMD Instinct MI250 accelerators. In addition, on consumer laptop equipped with an iGPU (Radeon 880M) and CPU (Ryzen AI 9 365), the model generates 26-frame video in just 50 seconds*. We also introduce novel data processing pipeline that leverages large language models (LLMs) to recaption text prompts and select high-quality video samples, thereby optimizing training and improving the model performance. The proposed Hummingbird highlights AMDs significant advancements in AI-driven video generation. The main contribution can be summarized as follows: We propose an innovative network pruning strategy for T2V models that significantly reduces model parameters while maintaining visual quality through visual feedback learning. The proposed method can generate high-quality long videos with up to 26 frames. We introduce novel data processing pipeline that leverages large language models (LLMs) to recaption text prompts and employs video quality assessment (VQA) models to select high-quality video samples for training. We publicly release the training code and data processing pipeline, enabling users to easily customize the style of generated videos. This provides flexibility to adapt models to specific needs and preferences. Extensive experiments demonstrate that our method achieves 31 speedup over stateof-the-art models such as VideoCrafter2, while also achieving the highest overall score on VBench."
        },
        {
            "title": "2 Motivation",
            "content": "The development of Hummingbird is driven by several key factors. Firstly, efficiency plays crucial role. Existing open-source T2V models like VideoCrafter2 [2] and CogVideo [10] have made significant strides in visual performance, but they do not yet achieve efficient generation on typical GPUs. There is critical need for smaller models that align with operational constraints while maintaining high performance. Another important consideration is model size and inference speed. Most current research emphasizes improving visual performance, often at the expense of model size and inference speed, which are crucial for practical deployment. AMD approach focuses on balancing these aspects to ensure both high-quality outputs and efficient operations. Customization is also key advantage of building our own model. The proposed structural distillation method allows for adaptable and reward-optimized models, providing users with tailored solutions that meet their specific needs. Data quality is essential for effective model learning. This novel data processing pipeline helps ensure that the model is trained on top-quality visual and textual data, enhancing its performance and reliability. High-quality training data is critical for the model to generate richer textural details and better understand textual input. Finally, by open sourcing the training code, dataset, and model weights, AMD supports the AI community. This enables developers to replicate and explore T2V generation on AMD AI platforms, fostering an open and collaborative approach to AI development, and ensuring that the benefits of AI advancements are widely shared."
        },
        {
            "title": "3 Method",
            "content": "Building on these considerations, we propose an efficient T2V model along with novel two-stage structural pruning pipeline that first reduces network parameters and then enhances visual quality through feedback learning. In addition, we introduce data processing pipeline designed to improve the quality of both text prompts and video samples. *Tested in Nov 2024 by AMD using AMD Radeon 880M and Ryzen AI 9 365 on Ubuntu 6.8.0-51generic. 2 Figure 1: Illustration of the proposed data processing pipeline, which includes video quality assessment, motion filtering, and prompt re-captioning using large language models to improve training data quality. Figure 2: Illustration of the proposed two-stage T2V diffusion model distillation pipeline. The first stage prunes the models parameters to improve efficiency, while the second stage enhances visual quality through feedback learning."
        },
        {
            "title": "3.1 Data Processing Pipeline",
            "content": "The proposed data processing pipeline includes video quality assessment, motion estimation, and prompt re-captioning. Low-quality video data can slow the convergence of the loss function during training and reduce the performance of the T2V diffusion model. To address this issue, the pipeline begins by applying Video Quality Assessment (VQA) models [19] to evaluate each videos quality based on aesthetic and compression-related metrics, as shown in Figure 2. motion estimation module is then used to filter out dolly zoom videos from the set of high-quality samples. To improve the quality of textual inputs, the pipeline leverages the prior knowledge of large language model, specifically LLaMA-8B [20], to perform prompt re-captioning. Experimental results show that training on videos with high visual and textual quality enables the T2V diffusion model to generate more detailed textures and better interpret textual input."
        },
        {
            "title": "3.2 Network Pruning",
            "content": "Model distillation [2123] is proven approach for reducing model size, making it suitable for deployment on resource-constrained hardware. However, it has not been widely explored in the context of text-to-video (T2V) diffusion models. The main challenge lies in effectively transferring knowledge from large, high-performing model to smaller one without causing significant performance degradation. To overcome this, we introduce two-stage distillation framework designed specifically for T2V diffusion models. First Stage: The first step involves constructing smaller model by reducing the number of blocks by half in each layer and removing all middle blocks from the U-Net architecture. Maintaining structural similarity with the original model during this pruning process is essential to support better adaptation of diffusion priors and significantly accelerates training. We also explored various alternative architectural configurations and found that several pruned variants achieved comparable performance after fine-tuning. Based on these findings, we do not apply any task-specific architectural design at this stage. Taking VideoCrafter2 [2] as an example, which contains approximately 1.4 3 billion parameters, our pruned version reduces the parameter count to 0.7 billion. We then fine-tune the smaller model using the outputs of the original model as supervision, following by [24, 16, 25, 26]. The training details are listed below: The model is trained directly on the video outputs generated by the original model after classifier-free guidance (CFG) [27] is applied. We minimize the difference between adjacent points along the Probability Flow Ordinary Differential Equation (PF ODE) trajectory using numerical solvers. Second Stage: The second stage focuses on further enhancing the visual quality of the Hummingbird diffusion model, with improvements in tonal range, color saturation, textural detail, and overall visual fidelity. straightforward approach would be to collect or synthesize additional video data that features high-quality visual effects. For example, some methods utilize video clips from professionally produced films, which benefit from high-end cinematography and extensive postproduction. However, acquiring such data is time-consuming, resource-intensive, and introduces practical limitations related to bandwidth usage and data availability. To address these challenges, we propose reusing the training data from the first stage to further improve visual quality. Inspired by [18], we leverage reward feedback from mixture of reward models to extract valuable visual cues from the existing dataset. This feedback-guided approach enables the model to refine fine-grained visual features without requiring new data. To enhance temporal coherence and visual consistency, we incorporate both image-text and video-text reward models during training. The second-stage training is highly efficient and requires only one GPU-day, utilizing four AMD Instinct MI250 GPUs with 64 GB of memory each."
        },
        {
            "title": "Training Details",
            "content": "We train Hummingbird on the WebVid-10M [29] dataset. WebVid is publicly available, large-scale dataset consisting of 10 million video-text pairs collected from stock footage platforms. It features wide range of motion patterns and diverse scenes, making it suitable for general-purpose T2V training. subset of high-quality video data is selected using our proposed data processing pipeline for the first-stage training. For the second-stage training, smaller subset is randomly sampled from this selection. The first stage is trained for 200K steps with learning rate of 1 104 and batch size of 16. The second stage is trained for 80K steps with learning rate of 1 105 and batch size of 8. During training, we randomly sample 16 frames per video at resolution of 320 512, using dynamic frame stride. The frame rate condition varies depending on the codec of the original video. Training is conducted on AMD Instinct MI250 GPUs, which are based on the CDNA 2 architecture and feature 362.1 TFLOPs of peak FP16 performance, 64 GB of HBM2e memory, 3.2 TB/s peak memory bandwidth, and 500W TDP. The software stack includes Python 3.8, ROCm 5.6.0, PyTorch 2.2.0, and FlashAttention 2.2.0. It is worth noting that we also possess privately collected dataset that can further enhance model performance. However, due to licensing and privacy constraints, this dataset cannot be released. Therefore, all training for the public code release is conducted using the publicly available WebVid-10M dataset. Evaluation Metrics We evaluate the proposed method on the standard benchmark VBench [] which consists of 16 disentangled dimensions and is designed to comprehensively evaluate T2V models. Followed by [18], we also present the Quality Score and Semantic Score. The Quality Score is computed as weighted sum of seven normalized sub-metrics: Subject Consistency [30], Background Consistency [31], Temporal Flickering, Motion Smoothness [32], Aesthetic Quality [33], Dynamic Degree [34], and Image Quality [35]. The Semantic Score is obtained by averaging nine normalized sub-metrics: Object Class [36], Multiple Object [36], Human Action [37], Color [36], Spatial Relationship [38], Scene [39], Appearance Style [40], Temporal Style [40], and Overall Consistency [40]. The Total Score is then calculated as weighted sum of the Quality Score and Semantic Score, providing an overall measure of the models visual and semantic performance. 4 Table 1: VBench [28] Evaluation Results by Dimension. This table compares the performance of 11 video generation models across the 16 individual dimensions defined in VBench. higher score indicates better performance for each dimension. The best result for each dimension is shown in bold, and the second-best result is underlined. Subject Consist. Temporal Flicker. Aesthetic Quality Dynamic Degree Motion Smooth. BG Consist. Quality Score Image Quality Total Score Models 75.75 ModelScopeT2V 77.08 LaVie 78.93 Show-1 79.72 VideoCrafter1 80.40 Pika 80.44 VideoCrafter2 80.58 Gen-2 77.74 AnimateLCM 73.27 VideoLCM 81.01 T2V-Turbo Hummingbird 16 frames (Ours) 81.35 Hummingbird 26 frames (Ours) 80.31 78.05 78.78 80.42 81.59 82.68 82.20 82.47 80.68 77.65 82.57 83.73 83.11 89.87 91.41 95.53 95.10 96.76 96.85 97.61 96.57 96.55 96.28 95.87 96.97 95.29 97.47 98.02 98.04 98.95 98.22 97.61 96.57 97.23 97.02 96.77 97.73 Models Semantic Score Object Class Multiple Objects Human Action ModelScopeT2V LaVie Show-1 VideoCrafter1 Pika VideoCrafter2 Gen-2 AnimateLCM VideoLCM T2V-Turbo Hummingbird 16 frames(Ours) Hummingbird 26 frames(Ours) 66.54 70.31 72.98 72.22 71.26 73.42 73.03 66.00 55.75 74.76 71.84 69. 82.25 91.82 93.07 78.18 87.45 92.55 90.92 87.34 75.40 93.96 96.36 94.86 38.98 33.32 45.47 45.66 46.69 40.66 55.47 34.68 12.50 54.65 35.44 33.99 92.40 96.80 95.60 91.60 88.00 95.00 89.20 83.00 73.00 95.20 94.00 92.00 98.28 98.30 99.12 98.93 99.77 98.41 99.56 98.41 97.33 97.48 95.24 97.64 95.79 96.38 98.24 95.67 99.51 97.73 99.58 98.33 97.01 97.34 96.14 96.97 52.06 54.94 57.35 62.67 63.15 63.13 66.96 63.26 59.93 63.04 68.04 67. 66.39 49.72 44.44 55.00 37.22 42.50 18.89 33.33 5.56 49.17 79.17 50.00 58.57 61.90 58.66 65.46 62.33 67.22 67.42 62.30 66.43 72.49 71.04 69.94 Color 81.72 86.39 86.35 93.32 85.31 92.92 89.49 85.62 82.64 89.90 91.63 94.79 Spatial Relation. Scene Appear. Style Temporal Style Overall Consist. 33.68 34.09 53.50 58.86 65.65 35.86 66.91 40.86 19.85 38.67 38.96 26.87 39.26 52.69 47.03 43.75 44.80 55.29 48.91 46.29 35.10 55.58 52.91 49.49 23.39 23.56 23.06 24.41 21.89 25.13 19.34 19.87 19.87 24.42 22.43 21. 25.37 25.93 25.28 25.54 24.44 25.84 24.12 24.19 22.25 25.51 25.53 24.24 25.67 26.41 27.46 26.76 25.47 28.23 26.17 25.57 23.68 28.16 28.09 27.65 Table 2: Quantitative Comparison of Different Models. Latency is measured on the AMD Instinct MI250 accelerator using the FP16 data type. Metric Steps Unet Params (B) Inference Latency (s) VideoLCM AnimateLCM T2V-Turbo VideoCrafter2 Hummingbird (Ours) 4 1.4 2. 4 1.3 6.4 4 1.4 2.5 50 1.4 44.2 4 0.7 1."
        },
        {
            "title": "4.2 Comparison with State-of-the-arts",
            "content": "Quantitative comparison. To further evaluate the visual performance of Hummingbird, we compare it against 10 state-of-the-art models, including both open-source and closed-source foundation models. The evaluation is conducted using text prompts from VBench [28], which cover wide range of scenes such as natural landscapes, science fiction environments, and urban settings. The models considered in the comparison include ModelScope [15], LaVie [41], Show-1 [8], VideoCrafter1 [1], Pika [6], VideoCrafter2 [2], Gen-2 [42], AnimateLCM [17], VideoLCM [16], and T2V-Turbo [4]. As shown in Table 1, Hummingbird, despite its smaller size, achieves the highest total score among all models, demonstrating its effectiveness in generating high-quality video content with strong computational efficiency. In addition, the proposed Hummingbird model is capable of generating long video sequences with up to 26 frames while maintaining the same performance. Inference speed. Table 2 highlights the inference efficiency of Hummingbird. When evaluated on the AMD Instinct MI250 accelerator, our model achieves approximately 31 speedup compared to VideoCrafter2. This result illustrates Hummingbirds strong balance of speed, computational efficiency, and visual fidelity, setting new baseline for lightweight text-to-video generation models. Qualitative results. Figure 3 presents qualitative examples generated by Hummingbird. The model is capable of producing visually appealing videos with strong temporal consistency. Furthermore, it demonstrates improved alignment between the generated video content and the semantic details of the input text prompts. For optimal viewing quality, we recommend using Adobe Acrobat. 5 Figure 3: Qualitative Results. Video results generated from various text prompts. The proposed Hummingbird model is capable of generating high-quality visual results while accurately following the text prompts and producing semantically consistent content. We recommend viewing the video results using Adobe Acrobat Reader."
        },
        {
            "title": "5 Future Plan",
            "content": "We are preparing to release lightweight image-to-video (I2V) model that supports long video generation with up to 26 frames. This model is designed to offer efficient and high-quality video synthesis from static images, making it suitable for real-world applications on resource-constrained devices. In parallel, we are actively developing lightweight DiT-based models for both T2V and I2V generation. While these models currently rely on privately collected datasets for training, we aim to adapt them to work with publicly available datasets. This effort will improve reproducibility and accessibility for the broader research community."
        },
        {
            "title": "6 Conclusion",
            "content": "The AMD Hummingbird T2V diffusion model marks significant advancement in text-to-video generation, leveraging the capabilities of AMD state-of-the-art hardware and software technologies. The structure distillation method proposed for the T2V diffusion model exemplifies the dedication of AMD to fostering AI innovation, offering exceptional performance and efficiency. Additionally, AMD hardware and software excel in training and inferencing large-scale models like T2V diffusion model, making it key player in advancing AI and empowering developers to push the boundaries of innovation."
        },
        {
            "title": "References",
            "content": "[1] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [2] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 6 [3] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [4] Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv preprint arXiv:2410.05677, 2024. [5] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [6] Pika Labs. Accessed september 25, 2023, 2023. [7] Open-Sora. Open-sora: Democratizing efficient video production for all, 2024. [8] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. [9] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. 2022. [10] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2022. [11] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. [12] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563 22575, 2023. [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [14] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [15] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [16] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023. [17] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH Asia 2024 Technical Communications, pages 15. 2024. [18] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024. [19] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [20] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 7 [21] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [22] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021. [23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [24] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [25] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. International conference on machine learning, 2023. [26] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2023. [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [28] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [29] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. [30] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [32] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. [33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [34] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [35] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [36] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. [37] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: In Proceedings of the IEEE/CVF International Towards training-efficient video foundation models. Conference on Computer Vision, pages 1994819960, 2023. [38] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [39] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. In The Twelfth International Conference on Learning Representations, 2023. [40] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [41] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. [42] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023."
        }
    ],
    "affiliations": [
        "Advanced Micro Devices, Inc.",
        "Tsinghua University"
    ]
}