{
    "paper_title": "MovieCORE: COgnitive REasoning in Movies",
    "authors": [
        "Gueter Josmy Faure",
        "Min-Hung Chen",
        "Jia-Fong Yeh",
        "Ying Cheng",
        "Hung-Ting Su",
        "Yung-Hao Tang",
        "Shang-Hong Lai",
        "Winston H. Hsu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html."
        },
        {
            "title": "Start",
            "content": "MovieCORE: COgnitive REasoning in Movies Gueter Josmy Faure1, Min-Hung Chen2, Jia-Fong Yeh1, Ying Cheng3, Hung-Ting Su1, Yung-Hao Tang4, Shang-Hong Lai3, Winston H. Hsu1 1National Taiwan University, 2NVIDIA, 3National Tsing Hua University, 4National Chengchi University 5 2 0 2 6 2 ] . [ 1 6 2 0 9 1 . 8 0 5 2 : r Figure 1: Beyond Shallow Video Understanding: The proposed benchmark, MovieCORE, challenges visionlanguage models (VLMs) to understand the subtle interplay between emotions (Top, Middle), character dynamics and causality (Middle, Bottom), and psychological complexity (Top, Middle). From empathy to introspection, from wisdom to curiosity MovieCORE tests VLMs ability to comprehend the deeper elements of movies."
        },
        {
            "title": "Abstract",
            "content": "This paper introduces MovieCORE, novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure. github.io/assets/html/moviecore.html"
        },
        {
            "title": "Introduction",
            "content": "Movie audiences consciously or subconsciously absorb information about actors states of mind, body language, and expressions to infer their moods and empathize with their situations. Most people would agree that such inferences are crucial to truly understanding movie. Despite the significance of this deeper level of understanding, existing moviebased VQA datasets have yet to explore this aspect 1 of film comprehension. plexity of VQA datasets. Recent movie-based VQA datasets (Wu and Krahenbuhl, 2021; Song et al., 2024; Rawal et al., 2024) primarily focus on surface-level understanding, neglecting the challenge of comprehending movies at deeper cognitive level. They predominantly address the what by posing questions such as What is the relationship between the actors? or What time does the video take place?, and largely overlook the how, why, and why not questions crucial for achieving profound understanding of movies. While EgoSchema (Mangalam et al., 2023) attempts to delve beyond the obvious, its more profound questions often remain general. We propose MovieCORE, novel VQA dataset designed to engage System-2 thinkingthe slow, deliberate, and logical cognitive processeswhile maintaining strict relevance to specific video content. Unlike existing datasets, MovieCORE embraces the inherent subjectivity of \"why\" and \"why not\" questions as feature rather than limitation, creating both meaningful challenges and research opportunities. To generate comprehensive and faithful question-answer pairs, we develop an agentic brainstorming approach that leverages multiple large language models (LLMs) as interactive thought agents that engage in continuous discussions to refine QA pairs. We validate the quality of the QAs through rigorous human review of representative subset. Additionally, we employ quantitative cognitive metrics to measure our datasets depth and syntactic complexity relative to existing benchmarks. Our evaluation of current VQA models on MovieCOREreveals critical insights about their performance on these challenging cognitive tasks. To address identified limitations and improve existing VLMs deeper cognitive reasoning capabilities, we introduce Agentic Choice Enhancement (ACE), which demonstrates relative performance improvements of up to 25% compared to baseline approaches. Our key contributions are the following: We introduce MovieCORE, VQA dataset focused on thought-provoking questions and answers specific to movie content. We develop an agentic brainstorming approach using multiple LLMs as agents to generate and refine high-quality QA pairs. We implement set of cognitive tests to evaluate the depth, thought-provocation, and comWe design comprehensive evaluation scheme to assess the accuracy, comprehensiveness, depth, and coherence of answers from existing Vision Language Models (VLMs). We evaluate several VLMs on our dataset in both zero-shot and fully-supervised settings, offering insights into their performance on deeper cognitive tasks. We propose post-training \"agentic selection\" plugin to improve existing VLMs and show relative improvement of up to 25% compared to the baseline."
        },
        {
            "title": "2 Related Work",
            "content": "Movie-Based Question-Answering Datasets. Recent video understanding benchmarks are often based on movie scenes because films offer rich blend of multimodal content, combining visual, linguistic, and temporal elements within complex narratives. Early efforts like MovieQA (Tapaswi et al., 2016) explores entire movie understanding but were limited by questions heavily relying on dialogue. TVQA (Lei et al., 2018) requires reasoning over multiple events in short TV series clips, integrating visuals and subtitles. LVU (Wu and Krahenbuhl, 2021) addresses scaling video comprehension to extended sequences, necessitating models to process long temporal contexts. MAD (Soldan et al., 2022) and its extension (Han et al., 2023) focus on scene-level descriptions through audio and visuals but were mainly used for scene annotation tasks with limited narrative comprehension. MoVQA (Zhang et al., 2023) introduces multi-level questions, challenging models in temporal perception, causal reasoning, and narrative synthesis. CinePile (Rawal et al., 2024) automates large-scale question generation across varied scenes and question type and MovieChat-1k (Song et al., 2024) focuses on basic understanding of cinematic contexts. Video Question-Answering Reasoning. Textbased reasoning datasets like DROP (Dua et al., 2019) and GSM8K (Cobbe et al., 2021) handle discrete reasoning tasks, including counting and arithmetic, but are limited to textual inputs and do not address the complexities involved in integrating visual reasoning. Egocentric datasets, such as EpicKitchens (Damen et al., 2018), Ego4D (Grauman et al., 2022), and EgoSchema (Mangalam 2 Figure 2: The Critic Agent, acting as the master of ceremonies (MC), orchestrates interactions among specialized agents using video context and task instructions. It sequentially engages the System II VQA Expert, Skeptical Researcher, Detective, and Meta Reviewer, accumulating insights at each stage. Upon receiving final recommendations from the Meta Reviewer, the MC relays them to the System II VQA Expert for VQA refinement. Subsequently, subset of these refined VQAs undergoes evaluation by human experts for final validation. et al., 2023), challenge models to interpret subjective interactions and continuous activities from first-person perspective, requiring both perceptual understanding and intention reasoning. Perception Test (Patraucean et al., 2024) broadens perceptual reasoning to varied video contexts, assessing highlevel reasoning abilities. Multi-task and complex video benchmarks, such as MVBench (Li et al., 2024), Video-MME (Fu et al., 2024), and MLVU (Zhou et al., 2024), integrate multiple reasoning challenges, requiring predictive reasoning, memory recall, and cross-modal inference over long video sequences. While these datasets have advanced various aspects of video understanding, they predominantly rely on surface-level comprehension of video content. Our work introduces the first dataset specifically designed to evaluate System-2 reasoning in the video domain, requiring models to engage in slow, deliberate, and analytical thinking processes aiming to mirror human approaches to complex movie understanding. use 986 of these clips, as 14 were either unavailable or lacked necessary annotations. MovieChat-1k, already provides high-level information for each video, such as temporal setting (e.g., ancient or modern) and metadata like the movies genre. Although some videos in the original dataset include captions, we observe inaccuracies and imbalanced descriptions. Therefore, we exclude these captions, focusing instead on the existing QA pairs and movie metadata. To provide video context, we utilize MiniCPMv2.6 (Yao et al., 2024), an open-source model with visual capabilities comparable to GPT-4V. We prompt it with carefully curated set of eight questions (shown in Figure S1 in the supplementary material) designed to extract multi-dimensional understanding of the video. These questions address narrative structure, thematic focus, emotional tone, key events, character dynamics, genre, and target audience. The extracted information serves as Data Info priors for our agents."
        },
        {
            "title": "3.2 Agentic Annotation Workflow",
            "content": "To address the challenge of obtaining questionanswer pairs that delve into deeper levels of movie understanding, we propose an agentic annotation workflow. This approach leverages the deliberative capabilities of multiple LLMs acting as specialized agents, each contributing unique perspectives to the annotation process. We start with video context extraction to make sure our text-only annotation agents have enough information about the video."
        },
        {
            "title": "3.1 Video Context Extraction",
            "content": "The videos for our dataset are sourced from MovieChat-1k (Song et al., 2024), collection of 1,000 movie clips averaging 10 minutes each. We Our workflow, illustrated in Figure 2, employs multi-agent system orchestrated by Critic Agent acting as the master of ceremonies (MC). Using the Agentic AI framework autogen (Wu et al., 2024), we deploy instances of GPT4-o for the VQA Expert and Meta Reviewer roles (as these positions demand superior reasoning capabilities), with GPT4o-mini powering the other expert agents. The process begins as the Critic Agent receives task instructions and video context (Data Info) extracted as described in Section 3.1 and sends them to the System II VQA Expert who generates questions that engage System-2 thinking. These initial QA pairs are then scrutinized by the Skeptical Researcher, 3 Figure 3: Comparison of single-pass and agentic annotation. The agentic method (bottom) elicits specific scene details, concrete examples, and detailed story elements, demonstrating the enhanced granularity achieved through multi-agent refinement. Text in blue indicates new, specific details absent in the single-pass version. The single-pass annotation (top), on the other hand, while also attempting to ask deeper questions, remains at more abstract level. who evaluates their contextual relevance and accuracy, often challenging the VQA Expert to provide more concrete evidence. The Detective agent follows, suggesting additional questions to uncover underlying motivations and biases. The Meta Reviewer synthesizes these insights, proposing enhancements to the initial VQAs. The Critic Agent then consolidates this feedback for the VQA Expert to refine the QAs. The process concludes with human expert evaluation of subset of the refined VQAs, assessing their clarity, depth, relevance, and answerability. This agentic annotation workflow mimics collaborative human expert discussions by harnessing collective intelligence and mitigating potential biases of any single agent1. To ensure the quality and reliability of our dataset, we implement rigorous human verification process. Seven graduate students were recruited to assess subset of 30 videos, 30 captions and 150 QA pairs. The final human validation ensures that the resulting VQAs meet the highest standards of quality and depth. We provide more details on the human validation in Appendix II.3. 1Wondering why we chose these specific agents? Please see Appendix II.4 and II."
        },
        {
            "title": "3.3 Agentic versus Single-Pass Annotation",
            "content": "To illustrate the effectiveness of the proposed Agentic Annotation workflow, we compare the quality of the VQAs generated by the System II VQA Expert in the initial round (single-pass) and those produced through our workflow after the agent has gathered feedback and enhancement ideas from other experts (agentic annotation). As shown in Figure 3, the agentic annotation approach demonstrates clear advantages over single-pass annotation. While the single-pass annotation provides general, abstract description of character relationships, the agentic annotation generates questions that ask for and answers that deliver specific, concrete details about key scenes that support the relationship development of the characters including the falling scene, rooftop navigation, and confrontation with the purple heart-shaped entity. The agentic process elicits richer context and more granular evidence, making the annotations more specific and faithful to the movie content. It also makes the dataset much more valuable for training and evaluating AI systems understanding of narrative progression and character dynamics. This suggests that using multiple AI agents as thought partners leads to more detailed and substantive annotations compared to traditional single-pass methods used by other autoDataset MovieChat-1k (Song et al., 2024) ActivityNetQA (Yu et al., 2019) MVBench (Li et al., 2024) EgoSchema (Mangalam et al., 2023) MovieCORE Parse Tree Depth Avg FK Grade Score Avg 3.58 4.24 3.96 6.56 5.38 1.31 0.27 1.71 4.38 6.39 2.45 2.26 2.84 5. 5.88 3.19 2.69 4.74 10.52 -0.39 0.98 1.47 6.08 1.4 1.84 3.11 8.30 12.98 15. 14.03 BT Level HO-QA (%) 1.8 1.9 2.2 3.1 4.9 0.0 0.2 3.4 33.1 99. Table 1: Syntactic Complexity and Cognitive Demand Comparison: Parse tree depth, Flesch-Kincaid (F-K) grade scores, average Blooms Taxonomy (BT) level, and percentage of higher-order questions and answers (HO-QA) across various VQA datasets. and represent questions and answers respectively. Best results are in bold, second-best are underlined. annotated datasets such as (Rawal et al., 2024) and (Mangalam et al., 2023). More comparisons between agentic and single-pass annotation can be found in Appendix II.4. 3.4 Dataset Description MovieCORE is video question-answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. The dataset comprises 986 videos paired with 4,930 corresponding questions and answers and 986 captions. Following the splits of the original MovieChat-1k dataset (Song et al., 2024), we split MovieCORE into 4080 QAs for training (816 videos) and 850 for testing (170 videos). The primary application of MovieCORE lies in training and evaluating VQA models capabilities in deeper cognitive tasks. The questions are specifically designed to assess models abilities to comprehend complex narrative elements, character motivations, and subtle contextual cues skills that are crucial for achieving human-like understanding of cinematic content. wordcloud of MovieCORE is shown in Figure 4 suggesting complex themes regarding character dynamics, emotional resonance, and societal implications through terms like tension, psychological, cultural, and emotional. Also, the prominence of analytical terms such as underscore,depth, and critical, suggests questions that probe deeper interpretations and thematic elements rather than just literal plot descriptions."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Linguistic and Cognitive Complexity To evaluate the effectiveness of MovieCORE in engaging System-2 thinking and promoting deeper cognitive processing, we conduct series of tests designed to assess the complexity, readability, and cognitive demand of our questions and answers. Figure 4: Wordcloud illustrating key themes and concepts of MovieCORE with terms such as \"emotional\", \"character\" and \"influence\" very prominent. These tests include well-established metrics such as parse tree depth, Flesch-Kincaid grade score, and Blooms taxonomy classification. Each provides unique insights into different aspects of our datasets ability to stimulate higher-order thinking. Table 1 presents comparative analysis of MovieCORE against other VQA datasets. Parse Tree Depth measures the syntactic complexity of sentences by analyzing their hierarchical structure. We utilize this metric to assess the structural intricacy of our questions and answers. We employ the spaCy library to generate parse trees for each question and answer in our dataset and recursively compute their depth as follows. Let d(t) be the depth of token in the tree. For token with children C(t), the depth is defined as: d(t) = (cid:40) 0 1 + maxcC(t) d(c) if C(t) = if C(t) = (1) where d(t) = 0 if is leaf node (no children), d(t) = 1 + maxcC(t) d(c) if has children C(t), with maxcC(t) d(c) representing the maximum depth of the children of t. For sentence with multiple tokens, the depth of the parse tree rooted at the token (root of the sentence) is = d(r)."
        },
        {
            "title": "The depth of these trees are then averaged across",
            "content": "5 the dataset. greater parse tree depth often correlates with more complex sentence structures, which typically require more cognitive resources to process. By measuring this, we aim to quantify the linguistic sophistication of our VQAs as compared to existing datasets, hypothesizing that questions and answers with higher parse tree depths are more likely to engage System-2 thinking. Table 1 shows that MovieCORE has the highest average parse tree depth compared to the other VQA datasets. The Flesch-Kincaid (F-K) Grade Score is readability measure that indicates the U.S. grade level needed to understand text. We calculate this score for both questions and answers in our dataset using the standard Flesch-Kincaid formula below F-K Grade Score = 0.39 (cid:0) (cid:1) + 11.8 (cid:0) (cid:1) 15.59 (2) where is the total number of words in the text, S, total number of sentences and the total number of syllables. While our goal is not to make the content unnecessarily difficult, moderately high Flesch-Kincaid score indicates that the QAs require more advanced level of comprehension and thinking. As shown in Table 1, MovieCORE substantially outperforms other datasets with an average grade score of 14.03, with its closest competitor EgoSchema (Mangalam et al., 2023) standing at 8.3. Blooms Taxonomy is hierarchical model used to classify educational learning objectives into levels of complexity and specificity (Mcdaniel, 1970). We prompt GPT-4o-mini with comprehensive breakdown of the Blooms Taxonomy and ask it to classify each question and answer into one of six cognitive levels: Remember (1), Understand (2), Apply (3), Analyze (4), Evaluate (5), and Create (6). Such classification helps us assess the cognitive demand of the QAs. Questions falling into higher levels of Blooms Taxonomy (Analyze, Evaluate, Create) require deeper analysis and critical thinking skills susceptible to trigger System-2 thinking. MovieCORE achieves the highest average Bloom Taxonomy Level (BT Level) of 4.9, indicating that our questions and answers predominantly engage higher-order cognitive skills, significantly surpassing the other datasets. Additionally, we report the percentage of higher-order questions and answers (HO-QA), representing the proportion of both questions and answers that fall into the upper levels of Blooms Taxonomy (levels 4-6). MovieCORE excels in this metric with 99.2% of its questions and answers classified as higher-order. Algorithm 1 ACE: Agentic Choice Enhancement 1: Input: Video , Question Q, Beam width = 5 2: Output: Best response 3: VLM.generate(V, Q, beam_width = k) 4: Llama-3.2.score(C) Score candidates 5: arg maxcC S(c) Select best response 6: return R"
        },
        {
            "title": "5 ACE: Agentic Choice Enhancement",
            "content": "We propose ACE, straightforward yet effective approach to improving existing video language model (VLM) outputs through post-generation refinement. Our approach, detailed in Algorithm 1, uses an existing VLM and leverages beam search with width of 5 to generate diverse candidate responses, which are then re-ranked using the compact 1B-parameter Llama-3.2 (MetaAI, 2024) language model. We hypothesize that, when engaging in task requiring deeper deliberation, it is advisable to have second pair of eyes to refine ones thinking. The lightweight nature of Llama-3.2 (1B) ensures that this enhancement remains computationally efficient while significantly improving the quality of generated responses. We prompt the model without specific evaluation guidelines, allowing it to leverage its inherent understanding of answer quality. Table 2 show that this agentic selection approach paired with HERMES (Faure et al., 2024) (HERMES + ACE) registers an absolute gain of 0.48 compared to the baseline VLM, which translates to roughly 16 percent improvement in answer quality. It also improves InstructBLIP (Dai et al., 2023) by 25% (2.633.29) and MA-LMM (He et al., 2024) by 20% (2.793.35). These results suggest that existing VLMs have untapped potential that can be realized through simple post-generation second pair of eyes strategy, offering practical path to training-free improvement. Table 3 shows similar performance across beam widths (3, 5 and 7) for HERMES, suggesting ACEs effectiveness stems from the agentic selection mechanism itself rather than hyperparameter choices. These results validate our frameworks fundamental premise: lightweight post-generation refinement can unlock significant untapped potential in existing VLMs. 6 Model Accuracy Comprehensiveness Depth Evidence Coherence Avg. Gemini 2.5-flash Gemini-1.5-pro GPT-4o (08-06) InstructBlip (Dai et al., 2023) MA-LMM (He et al., 2024) HERMES (Faure et al., 2024) LongVU (Shen et al., 2024) mPlug-Owl3 (Ye et al., 2024) Qwen2.5-VL (Bai et al., 2025) InternVL2 (IntenVL, 2024) InternVL2.5 (IntenVL, 2024) InstructBlip (Dai et al., 2023) MA-LMM (He et al., 2024) HERMES (Faure et al., 2024) 4.26 3.91 4.18 1.03 1.14 1.77 2.95 3.55 3.78 3.80 3.87 3.25 3.42 3. Proprietary Models 4.50 3.81 4.00 Zero-Shot Results. 0.43 0.63 1.21 2.01 2.75 3.54 3.42 3.54 4.00 3.90 3.98 0.85 0.93 1.41 1.94 2.39 3.36 3.10 3. Fully-Supervised Results 2.43 2.54 2.72 2.47 2.66 2.83 Fully-Supervised Results + ACE (Ours) InstructBlip (Dai et al., 2023) MA-LMM (He et al., 2024) HERMES (Faure et al., 2024) 3.71 3.76 3. 3.15 3.24 3.30 3.02 3.09 3.12 4.03 3.87 3.96 0.33 0.57 1.28 2.06 2.78 3.42 3.37 3.65 2.61 2.81 2.98 3.30 3.39 3. 3.84 3.79 3.96 0.40 0.67 0.37 2.12 2.82 3.50 3.51 3.65 2.38 2.50 2.62 3.25 3.30 3.42 4.13 3.86 4.02 0.61 0.79 1.41 2.22 2.86 3.52 3.44 3. 2.63 2.79 2.93 3.29 (+0.66) 3.35 (+0.56) 3.41 (+0.48) Table 2: Performance Comparison of Video Question-Answering Models. We evaluate various open-source and proprietary Vision-Language Models (VLMs) on five criteria: Accuracy, Comprehensiveness, Depth, Evidence, and Coherence. We use the 7B version of the open-source VLMs (8B for InternVL2.5). w/ ACE Acc. Com. Dep. Evi. Coh. Avg. Beam=3 3.81 3.40 Beam=5 3.81 3.30 Beam=7 3.79 3.29 3.19 3.42 3.43 3.45 3.12 3.38 3.42 3.41 3.08 3.36 3.35 3. Table 3: ACE Beam size ablations on HERMES. ACE improves performance across all evaluation dimensions regardless of the beam size, with no clear winner among the different beam values."
        },
        {
            "title": "6 Quantitative Evaluation",
            "content": "VQA datasets usually use top-1 accuracy as metrics, but valid match has to be perfect match. For instance, there can be one strict answer to the question Does sea appear in the video?, which is Yes or No. However, in the age of LLMs and especially for zero-shot evaluation settings, we might get answers such as it does or no sea appears in the video. In such cases the accuracy would be 0. Recently, LLM-assisted evaluation schemes such as the one introduced by (Maaz et al., 2023), attempt to solve this issue by considering synonyms or paraphrases as valid matches. This works for VQAs where there is perfect answer, and would not work in our case, especially since accuracy for System-2 answer is not binary but exists in spectrum. Furthermore, we posit that accuracy alone is insufficient, therefore we design four other LLM-assisted metrics: depth to assess the depth of reasoning in the answers, comprehensiveness to assess how fully the answer covers all key points and relevant details, coherence and clarity, and evidence to evaluate the quality and relevance of the evidence provided. For all of these metrics, we prompt GPT-4o-mini (OpenAI, 2024) to assign score between 0 to 5 to each. Table 2 presents comprehensive evaluation of model performance across our five assessment criteria. Several key insights emerge from these results: (1) Proprietary models significantly outperform their open-source counterparts. This performance gap indicates that large-scale proprietary training data likely contains more diverse reasoning tasks than those available in public datasets. (2) In the zero-shot setting, most open-source models struggle considerably with complex reasoning, except for the more recent InternVL2.5 and Qwen2.5-VL models. The particularly low scores in Depth and Evidence metrics highlight these models difficulty in formulating multi-step inferences and grounding their responses in specific visual content. (3) Fine-tuning on MovieCORE yields substantial improvements for all models, with HERMES showing 7 Model BLEU-4 CIDEr METEOR InternVL2.5 (8B) mPlugOwl3 (7b) HERMES HERMES + ACE MA-LMM + ACE InstructBLIP + ACE 0.0645 0.0602 0.0308 0.0654 0.0634 0.0605 0.1865 0.1579 0.1230 0.1622 0.1587 0. 0.1026 0.1462 0.0983 0.2138 0.1948 0.1893 Table 4: Traditional Metrics Results. BLEU-4, CIDEr, and METEOR scores for several models on MovieCORE. These results are consistent with the trends observed in our primary evaluation. The top row contains zero-shot results and the bottom row contains fully-supervised results. the strongest performance. However, even with full supervision, these models still underperform compared to proprietary alternatives, suggesting architectural limitations in handling complex reasoning tasks. (4) Our proposed ACE post-generation strategy delivers consistent and substantial improvements across models and metrics."
        },
        {
            "title": "6.1 Evaluation with Traditional Metrics",
            "content": "While our primary evaluation in Table 2 emphasizes metrics tailored for System-2 reasoning, we also report standard VQA and video captioning metrics to enable broader comparison with prior work. Specifically, we compute BLEU-4, CIDEr, and METEOR for several models. These n-gram-based metrics, while limited in capturing the semantic richness and reasoning depth demanded by MovieCORE, provide familiar point of reference relative to traditional VQA benchmarks. As shown in Table 4, the relative ranking of models is consistent with our primary cognitive-oriented evaluation from Table 2: methods enhanced with ACE outperform their baselines, and both zero-shot and fully-supervised models exhibit similar performance trends. 6.2 System-2 vs. System-1 Ablation Study To validate the unique challenges posed by MovieCORE, we conduct comparative evaluation against System-1 baseline using the MovieChat1k dataset, which is built from the exact same set of video clips but contains simpler, surface-level questions such as Does it happen during the day or night?. For MovieChat-1k, we use the officially reported performance of the HERMES model from its original paper. Since MovieChat-1k reports accuracy (using LLM-assisted evaluation), we convert its accuracy scores into an equivalent 05 scale MovieCORE (Score) MovieChat-1k (Acc./Score) Zero-Shot Fully-Supervised 1.14 3.52 78.6% (3.93) 84.9% (4.25) Table 5: Comparison of HERMES on MovieCORE (System-2) (System-1). versus MovieChat-1k MovieChat-1k results are converted to 05 scale for comparability. for direct comparison with our multidimensional MovieCORE evaluation. The results in Table 5 reveal stark performance gap. While HERMES achieves high scores on the surface-level MovieChat-1k benchmark, its performance drops dramatically on MovieCOREs questions, even with identical video content. This substantial gap highlights MovieCOREs emphasis on System-2 reasoning. While HERMES excels on datasets with simple recall (e.g., Do stars appear in the video?), it struggles with MovieCOREs questions that demand deeper causal, motivational, and evidential reasoning, despite being based on the same underlying video content."
        },
        {
            "title": "7 Qualitative Results",
            "content": "Figure 5 provides qualitative comparison between different models responses to questions that require understanding of complex animal behaviors. The figure illustrates how different approaches handle the same queries about cheetah social structures and survival strategies. InternVL-2, strong zero-shot model, provides basic observations but lacks sufficient depth and details. HERMES, fully-supervised model, also struggles with the details and performs worse than InternVL. HERMES+ACE, demonstrates enhanced response quality by incorporating specific visual evidence and richer contextual details. As highlighted in the responses, ACE significantly improves the models ability to reference specific scenes and provide concrete examples to support its assertions."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce MovieCORE, novel VQA dataset that fills critical gap in existing movie-based VQA datasets by emphasizing questions designed to engage System-2 thinking. Our agentic workflow, which leverages brainstorming agents, enables the generation and refinement of high-quality QA pairs. To measure the cognitive depth of VQA datasets, 8 Figure 5: Qualitative Comparison of Model Responses. This figure contrasts responses from InternVL-2 (zeroshot), HERMES (fully-supervised), and HERMES+ACE on two questions about cheetah behaviors. Purple text highlights conceptual understanding while blue text indicates specific visual evidence and contextual details. Note how ACE enhances responses with more precise scene descriptions and behavioral insights. we devise set of tests that demonstrate the superiority of MovieCORE over existing datasets. Additionally, we propose comprehensive evaluation framework to assess the performance of VQA models on this dataset. To tackle the challenges posed by MovieCORE, we propose ACE, lightweight inference-time agentic answer selection plug-in which yields up to 25% relative improvement in answer quality compared to baseline methods, providing insights for future works on this topic. averting potential systematic issues; however, the majority of annotations still rely on automated processes. Second, because the dataset is constructed in part from the MovieChat-1k collection, its genre coverage may be constrained. Certain cinematic genres or narrative styles could be overrepresented, limiting the datasets generalizability. Finally, MovieCORE evaluation is partly LLMassisted, which, while enabling scalability, may inherit the limitations and biases of the judge model."
        },
        {
            "title": "9 Limitations",
            "content": "While MovieCORE offers significant advancement in video question-answering by targeting deeper cognitive understanding, it is not without limitations. First, although we incorporate human verification for subset of the dataset, only 30 videos, and 150 QA pairs were manually verified. This improves dataset quality control by 9 Acknowledgment This work was supported in part by the National Science and Technology Council, Taiwan, under Grant NSTC 113-2634-F-002007. We are grateful to the National Center for High-performance Computing."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint, arXiv:2305.06500. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and 1 others. 2018. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161. Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Winston H. Hsu, and Shang-Hong Lai. 2024. Hermes: temporal-coherent long-form understanding with episodes and semantics. Preprint, arXiv:2408.17443. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, and 1 others. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, and 1 others. 2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012. Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, and Andrew Zisserman. 2023. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1893018940. Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514. IntenVL. 2024. than the InternVL2: BestExpanding Performance Boundaries of OpenSource Multimodal Models with the Progressive Scaling Strategy. Better Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, and 1 others. 2024. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large arXiv preprint vision and language models. arXiv:2306.05424. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244. Rhett Mcdaniel. 1970. Blooms taxonomy. https: //cft.vanderbilt.edu/guides-sub-pages/ blooms-taxonomy/. MetaAI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models ai.meta.com. https://ai.meta.com/blog/ llama-3-2-connect-2024-vision-edge-mobile-devices/. [Accessed 03-11-2024]. OpenAI. 2024. Hello gpt-4o. https://openai.com/ index/hello-gpt-4o/. [Accessed 01-11-2024]. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, and 1 others. 2024. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. 2024. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, and 1 others. 2024. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434. 10 Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. 2022. Mad: scalable dataset for language grounding in videos from movie audio descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50265035. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, and 1 others. 2024. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232. Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46314640. Chao-Yuan Wu and Philipp Krahenbuhl. 2021. Towards long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18841894. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2024. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. In COLM. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, and 1 others. 2024. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134. Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, ZhenHua Ling, Yali Wang, Limin Wang, and Yu Qiao. 2023. Movqa: benchmark of versatile questionanswering for long-form movie understanding. arXiv preprint arXiv:2312.04817. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. 2024. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264. The Supplementary material is organized as follows: 5. Main characters or entities: Focuses on the individuals or groups driving the videos story. Reproducibility Statement 6. Settings and locations: Explores the physical II More Details on MovieCORE III Details on the Blooms Taxonomy IV Evaluation Methodology VI Licence"
        },
        {
            "title": "I Reproducibility Statement",
            "content": "The dataset will be made public as soon as this paper is accepted (or rejected) for publication, as well as the evaluation scheme with clear examples. We will also release the annotation agents used for generating and refining question-answer pairs, including the code and configurations for the large language models (LLMs) employed in the agentic brainstorming process. Additionally, we provide detailed instructions for data preprocessing, agent configuration, and evaluation protocols, enabling reproduction of both the dataset generation process and the evaluation scheme. Our annotation system is scalable and has the potential to inspire other researchers to create massive video benchmarks."
        },
        {
            "title": "II More Details on MovieCORE",
            "content": "II.1 Extracting Video Info\" To generate meaningful interpretations of video content, we employ structured question framework designed to probe various aspects of the videos narrative, emotional tone, and intended purpose. This framework consists of eight prompts, each targeting specific dimensions of video understanding. The prompts and continuation of the sample answers they elicit are listed in Figure S1 and roughly contains the following: 1. Step-by-step explanation: Encourages chronological breakdown of events in the video. 2. Main subject or focus: Identifies the central theme or entity in the video. 3. Overall mood or atmosphere: Captures the emotional tone conveyed by the video. 4. Significant events or actions: Highlights key actions and turning points within the narrative. or contextual backdrop of the video. 7. Genre or category: Classifies the video into relevant category or type. 8. Intended audience: Identifies the target demographic for the video. II.2 Agentic Annotation Details Figure S2 depicts the system messages for the different agents involved in the task of creating system-2 thinking VQAs from system-1 VQAs. The agents and their respective roles are: System-2 Video Question Answering Assistant Responsible for generating up to five system-2 thinking VQA pairs from the given system-1 VQAs. The focus is on creating questions and answers that encourage deeper analysis, critical thinking, and meaningful reflection, while ensuring the insights are grounded in the actual video content. Critic Agent Evaluates the system-2 VQAs created by the System-2 Video Question Answering Assistant and passes them to various Expert Agents for detailed analysis. The Critic Agent then compiles the constructive feedback from the experts and returns it to the System-2 Video Question Answering Assistant, emphasizing the importance of aligning the VQAs with the actual video context. Skeptical Researcher Reviews the questions and answers in the context of the video, analyzing the context and evaluating the system-2 VQAs for their contextual relevance and accuracy. The Skeptical Researcher challenges the assumptions behind the QAs and encourages further evidence-based exploration, providing concise and relevant suggestions. Detective Given the video information and the system-2 VQAs, the Detective identifies additional questions that could uncover underlying causes, motivations, or potential biases. The suggestions should be concise, realistic, and directly relevant to the videos actual content. Meta Reviewer Aggregates the feedback and suggestions from all reviewers (Skeptical Researcher, Detective) and provides final insights and suggestions to refine and improve the system-2 VQAs. The Meta Reviewer ensures the feedback 12 Figure S1: Extracting Detailed Context from Videos: We input each video to MiniCPM-v2.6, prompting it with series of carefully crafted questions (left). The models responses (right) provide rich, multi-faceted details about the video, including narrative flow, character information, setting, mood, and target audience. This extracted information serves as Data Info priors to inform our annotation agents, ensuring comprehensive understanding of the video content before the VQA generation process. Figure S2: System Messages for the Annotation Agents is comprehensive, constructive, and truthful to the videos context and content, filtering out any speculative suggestions. Answerability (1-5): Determines whether the question can be answered solely from the video content II.3 Human Verification Verification Rules To ensure the quality and reliability of our dataset, we implemented rigorous human verification process. Seven qualified evaluators, each holding at least Bachelors degree, were recruited to assess subset of 30 videos and 150 QA pairs. The verification was conducted through standardized evaluation form (Figure S4) that assessed four key dimensions: Relevance (1-5): Evaluates how directly the question/answer relates to the video content Clarity (1-5): Measures the linguistic clarity and absence of ambiguity Depth (1-5): Assesses the level of cognitive analysis required As for the captions, we assessed accuracy, clarity and depth. Evaluators were instructed to watch each video in its entirety and carefully consider the scenes, characters, actions, and dialogues before rating the associated QA pairs. To maintain objectivity, evaluators were required to focus solely on the video content when reviewing the QA pairs and encouraged to replay videos when necessary. The evaluation process also included assessing the accuracy and clarity of video captions to ensure comprehensive content accessibility. Verification Result The human verification process (the rules and interface are illustrated in Figure S4) yields consistently high scores across all evaluated dimensions, as shown in Table S1. QuesFigure S3: parade scene from MovieCORE featuring various cultural and historical elements. This particular QA receives low answerability and relevance scores from one of our reviewers but was still kept following thorough review by human meta-reviewer. Metric Captions Questions Answers II.4 Agentic versus Single-Pass Annotation Accuracy Clarity Depth Relevance Answerability 3.9 4.0 4.1 4.3 4.5 4.0 3.8 4.3 4.2 3.8 4.1 Table S1: Human verification scores across different dimensions for captions, questions, and answers. Scores range from 1 to 5, with 5 being the highest quality. Dashes () indicate metrics not applicable to that content type. The scores, being above 3.8 indicate strong quality across all evaluated dimensions. tions and answers received notably high scores in clarity (4.3) and depth (4.5 and 4.2 respectively), validating our datasets emphasis on deep cognitive understanding. The captions also demonstrate strong quality with scores above 3.8 across applicable metrics. While answerability scores were slightly lower (3.8 for questions), they remain well above acceptable thresholds, confirming that the questions can be reasonably answered from the video content alone. The sample QA pair for the video depicted in Figure S3 received low scores of 2 each for Answerability and Relevance from the human evaluators. However, our human meta-reviewer has determined that the question and answer offer meaningful insights and contextual relevance (underlined in the figure). As shown in Figure S5, the single-pass annotation provides general interpretation of the themes suggested by the presence of the hippopotamus, focusing on human-animal conflict and critiques of captivity. In contrast, the agentic annotation delves deeper by exploring how the hippopotamus functions as symbol throughout the video, detailing its evolution from chaotic force to representation of innocence and victimhood. This nuanced analysis offers specific, concrete details about the symbolic transformation, enhancing the understanding of the narratives thematic complexity. In the other example shown in Figure S6, the single-pass annotation mentions general visual and narrative elements like close-ups and quick scene transitions to build suspense. The agentic annotation specifies how visual techniques such as dramatic lighting, shadow play, and strategic camera angles enhance the emotional weight and suspense of key scenes. By providing detailed exampleslike capturing characters raw emotion through close-ups or creating an ominous atmosphere with dim lightingthe agentic approach offers more granular and faithful depiction of the cinematic techniques used. These comparisons further illustrate that the agentic annotation process elicits richer context and more detailed evidence, reinforcing the idea that using multiple AI agents as thought partners leads to more substan14 Figure S4: Video Question Answering Evaluation Form used in our human verification process. The form assesses four critical dimensions (relevance, clarity, depth, and answerability) on 5-point scale. Each dimension is clearly defined with anchored endpoints to ensure consistent evaluation. The form includes sections for both question/answer assessment and caption verification to ensure comprehensive content quality. Evaluators use this standardized form to systematically review each QA pair while referring to the corresponding video content. 15 Figure S5: Additional Comparison of single-pass and agentic annotation. The agentic method (bottom) delves into specific scene details, such as the hippopotamuss evolution from chaotic force to symbol of innocence, and highlights changes in cinematography that reflect this transformation. The single-pass annotation (top) provides general interpretation of themes like human-animal conflict without specific scene references. Figure S6: Additional Comparison of single-pass and agentic annotation. The agentic method (bottom) specifies visual techniques like dramatic lighting, shadow play, and strategic camera angles that enhance emotional weight and suspense, offering concrete examples like close-up shots capturing raw emotion. The single-pass annotation (top) mentions general visual elements but lacks detailed analysis of how these techniques impact the narrative. tive annotations compared to traditional single-pass methods. Here we provide more explicit, step-by-step illustration of how each agent contributes to the refinement of final question. Step-by-Step Example The following example demonstrates how question evolves as each agent contributes for the example shown in Figure 3: Initial Question (Single-Pass): How does the interaction between the two main charac16 ters evolve throughout the video, and what might this suggest about their relationship? This version is abstract and lacks grounding in the specific video content. spaces.\" (We find similar examples while analyzing the conversations that led to the QAs in S62). Users can swap agents, but we recommend roles that enforce rigor. + Skeptical Researcher: How does the interaction between the two main characters evolve, and can you provide specific scenes as evidence for their relationship? This agent enforces verifiability, pushing for concrete references to the video. + Detective: What are the underlying motivations that drive the two main characters to form partnership? This role introduces causal reasoning, shifting the focus from observable actions to underlying causes. Final Agentic Question (Full Workflow): Can you provide specific scenes that demonstrate the evolution and motivations of the main characters in their relationship? The final result synthesizes evidence-grounding and causal reasoning into more challenging, cognitively rich query. II.5 Why these Specific Agents Careful examination of the agents interactions reveals distinct contributions: For the video in Figure S5, System-2 Video Question Answering Assistant transforms surface observations into deeper inquiries, exemplified by advancing from simply noting the hippopotamus to asking \"How does the hippopotamus function as symbol throughout the video, and how does its portrayal evolve?\" The Critic Agent ensures analytical quality, as evident in the transition from merely identifying \"human-animal conflict\" to explicating how the hippo evolves from \"chaotic and disruptive force\" to \"innocence and victimhood.\" The Skeptical Researcher challenges assumptions, demonstrated by refining the initial \"critique of captivity\" interpretation into more nuanced analysis of \"the growing recognition of the animals plight.\" The Detective uncovers underlying narrative patterns, illustrated by connecting the \"early chaotic scenes giving way to more empathetic portrayals\" with cinematographic techniques. The Meta Reviewer synthesizes these insights into cohesive annotations, balancing the single-pass observation of \"humananimal conflict\" with the richer agentic interpretation of \"intrusion of wild nature into human III Details on the Blooms Taxonomy Figure S7 illustrates Blooms pyramid of cognition levels and Figure S8 relays the prompts we use to ask GPT-4o-mini to score the QAs. Blooms Taxonomy is hierarchical classification of cognitive skills used in education to structure learning objectives. The taxonomy is divided into six levels, progressing from lower-order to higher-order thinking skills: 1. Remembering: Recalling facts and basic concepts. 2. Understanding: Explaining ideas or concepts. 3. Applying: Using information in new situations. 4. Analyzing: Breaking information into parts to explore relationships. 5. Evaluating: Justifying decisions or opinions. 6. Creating: Producing new or original work. Our dataset scores very high in this metric suggesting its propensity to deeply engage the AI system (VLM)s cognitive skills."
        },
        {
            "title": "IV Evaluation Methodology",
            "content": "The MovieCORE benchmark employs comprehensive multi-dimensional evaluation framework for assessing VLMs. The evaluation consists of five key dimensions summarized below. We also include the full prompts for each dimension in Figure S10 and Figure S9. 1. Accuracy Dimension: Evaluates semantic correctness of predicted answers using 6point scoring rubric (05): 5: Perfect semantic match 4: Mostly correct with minor inaccuracies 3: Partially correct, capturing key elements 2Can the reader spot them? 17 Figure S7: Blooms Taxonomy Pyramid. The pyramid illustrates the hierarchical nature of cognitive skills, progressing from lower-order to higher-order thinking. Figure S8: Prompts we use to instruct GPT4-o-mini to compute the Blooms taxonomy level for the different datasets we show in Table 1 of the main paper. truth 4: Deep analysis matching ground truth 3: Moderate depth beyond surface level 2: Limited depth, stating obvious details 1: Superficial analysis 0: No answer or completely irrelevant 3. Comprehensiveness Dimension: Evaluates the thoroughness of answer coverage, scored from 05: 5: Fully comprehensive, covering all key points 4: Mostly comprehensive with minor omissions 3: Moderately comprehensive 2: Limited comprehensiveness 1: Minimal comprehensiveness 0: Not comprehensive or no answer 4. Coherence Dimension: Measures clarity, logical organization, and articulation, scored from 05: 5: Exceptionally coherent, surpassing ground truth Figure S9: Prompt to evaluate the quality and relevance of the evidence provided in the answers. 2: Mostly incorrect but with some relevant information 1: Completely incorrect or unrelated 0: No answer or irrelevant response 2. Depth of Reasoning Dimension: Assesses the level of analytical depth and interpretative insight, scored from 05: 5: Exceptional depth, surpassing ground 4: Very coherent, matching ground truth 18 Figure S10: Evaluation Prompts: These figures illustrate the prompts we use for each of the evaluation methods we employ. The prompt for Evidence is shown in Figure S9. 3: Moderately coherent with minor isevidence sues 2: Somewhat incoherent 1: Largely incoherent 0: Completely incoherent or no answer 5. Evidence Dimension: Assesses quality and relevance of video content evidence, scored from 05: 4: Strong, relevant evidence matching ground truth 3: Moderate evidence with room for improvement 2: Limited, weak evidence support 1: Minimal evidence 0: No evidence or irrelevant support 5: Exceptional use of strong, relevant"
        },
        {
            "title": "Each dimension provides a nuanced evaluation",
            "content": "19 of different aspects of question-answering performance, enabling comprehensive assessment of the systems capabilities."
        },
        {
            "title": "V Licence",
            "content": "The annotations are released under the MIT licence and the videos follow the licence of MovieChat. We do not directly host the videos, those can be found in the MovieChat HuggingFace repository."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National Chengchi University",
        "National Taiwan University",
        "National Tsing Hua University"
    ]
}