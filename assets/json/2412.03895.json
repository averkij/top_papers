{
    "paper_title": "A Noise is Worth Diffusion Guidance",
    "authors": [
        "Donghoon Ahn",
        "Jiwon Kang",
        "Sanghyun Lee",
        "Jaewon Min",
        "Minjae Kim",
        "Wooseok Jang",
        "Hyoungwon Cho",
        "Sayak Paul",
        "SeonHwa Kim",
        "Eunju Cha",
        "Kyong Hwan Jin",
        "Seungryong Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to `guidance-free noise', we uncover that small low-magnitude low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose \\ours, a novel method that replaces guidance methods with a single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noise-space learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how refined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 5 9 8 3 0 . 2 1 4 2 : r a"
        },
        {
            "title": "A Noise is Worth Diffusion Guidance",
            "content": "Donghoon Ahn1 Jiwon Kang1 Sanghyun Lee2 Jaewon Min1 Minjae Kim1 Wooseok Jang Hyoungwon Cho1 Sayak Paul4 SeonHwa Kim1 Eunju Cha3 Kyong Hwan Jin1 Seungryong Kim 1Korea University 2KAIST 3Sookmyung Womens University 4Hugging Face Figure 1. Effectiveness of NoiseRefine. Diffusion models often fail to generate high-quality images without guidance, such as classifierfree guidance (CFG) [13]. We propose NoiseRefine, novel approach to improve image quality without use of guidance by learning to map initial random noise to guidance-free noise space. Results are demonstrated using Stable Diffusion 2.1[36]."
        },
        {
            "title": "Abstract",
            "content": "fined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/. Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifierfree guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to guidance-free noise, we uncover that small low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose NoiseRefine, novel method that replaces guidance methods with single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noisespace learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how re- ,: Equal contribution, : Co-corresponding author 1. Introduction In recent years, Text-to-Image (T2I) diffusion models [3, 6, 32, 36, 39], which generate images conditioned on text prompts, have achieved remarkable advancements. However, their ability to produce high-quality samples largely relies on guidance techniques, such as classifier-free guidance (CFG) [13] and its variants [1, 5, 15, 16]. These methods significantly enhance image quality during inference but double the computational cost. Despite drawbacks such as increased batch size, high guidance scale requirements, oversaturation, and reduced diversity, the dramatic performance gains make CFG the de facto standard. Recent works [4, 37, 38] aim to mitigate these limitations, but the impact of CFG on image quality makes it indispensable in most diffusion pipelines. This raises fundamental question: Can we replace the effects of guidance techniques with minimal changes to the diffusion pipeline? While some works have proposed 1 guidance-free noise space. The overall concept is illustrated in Fig. 2. We analyze the difference between inversion noise and standard Gaussian noise, and we find that the difference arises primarily from subtle variations in pixel values and is concentrated in the low-magnitude, low-frequency component. Our objective is for the noise refining model to learn the mapping from standard Gaussian noise to guidancefree noise. Although we could directly learn the mapping between the initial noises and the inversion noises, error accumulation during the inversion process [8, 29] makes this approach suboptimal. Thus, we mitigate this inversion error by shifting the distance loss from noise space to its denoised image space. We refer to our method as NoiseRefine. Fig. 4 illustrates the overall training process of NoiseRefine. Training noise refining model by backpropagating gradients through the full denoising steps can result in substantial memory consumption and unstable training due to multi-step gradient propagation. To address this challenge, we propose multistep score distillation (MSD), simple yet effective technique enabling efficient full-step model optimization without incurring the high cost of backpropagaInspired by score distillation sampling (SDS) [33], tion. MSD skips gradient computation within the denoising network during the denoising process. Notably, we found that skipping computation of gradient not only reduces computational overhead but also accelerates model convergence. With just prompts and the basic diffusion model, our approach easily accomplish self-distillation without the need for natural images. We demonstrate that sampling from the noise refined by our model produces high-quality images without guidance, as validated across various benchmarks. These results are comparable to images generated using CFG [13] and PAG [1] on the same diffusion model while being approximately twice as fast (compared to CFG alone) or three times as fast (compared to both CFG and PAG). Our contributions are summarized as: We identify the existence of noise space that enables high-quality generation without guidance [1, 13], which we refer to as guidance-free noise space. We show that the mapping between standard normal distribution and guidance-free noise space can be efficiently learned by neural network. To reduce backpropagation costs in training, we propose Multistep Score Distillation that detaches gradients during denoising, accelerating convergence. Our approach achieves 2x speed-up compared to using guidance methods, maintaining comparable quality. Figure 2. Insight of NoiseRefine. We combine inversion methods [8, 29, 46] and guidance methods [1, 13, 15, 16, 21, 38] to establish mapping between standard noise xT and guidance-free noise xGuide . distilling classifier-free guided scores into student models [19, 24, 30], these methods require extensive training data, significant computational resources, and large model capacities to replicate guided denoising trajectories. Instead, we explore the inputs of T2I diffusion pipelines: prompt and an initial noise. single prompt can yield diverse samples depending on the initial noise. In addition, recent studies [41, 50] highlighted the strong influence of initial noise on output quality, correlation between the starting noise and the resulting image. In fact, we observe that, on rare occasions, certain random initial noises can produce high-quality images. This implies that if we could find such noise easily, we would successfully eliminate the need for guidance methods. Thus, we aim to find noise space capable of generating high-quality images, which we term the guidance-free noise space. Throughout this work, we explore how we can find this guidance-free noise space. Some works have proposed to select or optimize noise space to improve perceptual quality [7, 34] or prompt adherence [10]. However, those are not intended to replace the guidance techniques and then still rely on them. Furthermore, they typically require extensive iterations to optimize the input noise [7, 10, 34] and often only work in the few-step diffusion model [7]. Our key insight to find guidance-free noise space lies in leveraging diffusion inversion methods [8, 29, 46] to obtain initial noises that are reconstructed to their corresponding high-quality images without guidance. Building on this, we generate multiple high-quality images using guidance techniques [1, 13] and apply inversion to these images. This process yields collection of initial noises capable of reconstructing images without guidance, constructing the 2 for real-world applications. Another approach [28] involves constructing noise database to generate initial noise during inference but is not generalized to unseen prompts. Our approach overcomes both limitations by learning direct mapping to the learned noise space, enabling efficient mapping with single inference step and providing generalization power for new prompts. 3. Method In this section, we first identify characteristics of mapping from the Gaussian noise space to guidance-free noise space, space of initial noises that can be denoised into high-quality images without guidance (Sec. 3.1). Next, we introduce method for learning this mapping from arbitrary Gaussian noise (Sec. 3.2). Finally, we demonstrate that, with carefully designed dataset construction and filtering process, predicting guidance-free noise using simple single-step neural network can effectively replace traditional guidance techniques [1, 13], enabling efficient and high-quality image generation without guidance (Sec. 3.3). 3.1. Guidance-Free Noise Space To obtain the guidance-free noise space, we emphasize the capability of inversion methods [8, 29, 46] to precisely reconstruct the original image without guidance. In theory, inverting an infinite set of natural images would fully capture this space, but this is infeasible. Instead, we leverage the powerful generation capabilities of text-to-image diffusion models [36], which produce high-quality images with guidance [1, 13]. Specifically, Gaussian noise xT is sampled from standard Gaussian distribution (0, I) and denoised into plausible image xGuide , using text prompt or condition with CFG [13] or any other guidance method [1, 15]. Inverting the image with an inversion method [8, 29] gives us the noise xGuide , defined as: 0 xGuide := Inversion(DenoiseGuide(xT , c)), (1) where Inversion() and DenoiseGuide() denote inversion [8, 29] and denoising with condition and guidance, respectively. more detailed explanation of the notations can be found in supplementary material A.1. Note that the generated image xGuide and inversion noise xGuide are conditioned on context such as the text prompt, but we omit the notation in the paper for simplicity. Now, we can map xT into xGuide . Ideally, if the mapping is consistent or generalizable, neural network can learn to map initial noise to guidance-free noise. This concept is illustrated in Fig. 2. 0 0 We investigate the structure of this mapping by generating {xT , xGuide } pairs via the aforementioned process with 10K randomly selected prompts from the MSCOCO dataset [25]. We employ Stable Diffusion 1.5 [36], 3 Figure 3. Analysis on the relationship between the initial Gaussian noise xT and the guidance-free noise xGuide . (a) shows the histogram of the absolute difference between xT and xGuide . Here, Random denotes the setting where the both noises are replaced with independent gaussian white noise. (b) presents the magnitude difference between the 2D Fourier-transformed frequency components of F(xT ) and F(xGuide ). The difference between xT and xGuide is significantly smaller than in the random case, which mainly corresponds to the low-frequency components. T 2. Related Work Diffusion guidance. Classifier Guidance (CG) [27] enhances fidelity by leveraging trained classifier gradients, albeit at the cost of diversity. CFG [13] models an implicit classifier to achieve similar effects. Ahn et al. [1] and Karras et al. [21] further generalize those guidance methods by intentionally generating lower-quality samples to guide the process toward improved outputs and other guidance techniques [15, 16, 38] generate bad samples in various ways. While effective, these methods double computational and memory costs by requiring degraded sample generation at each step, which is essential to their operation. Diffusion inversion. Denoising Diffusion Implicit Models (DDIM) [46] introduced deterministic sampling, enabling inversion from image to noise. This means that by starting the sampling process from the inverted noise, we can reconstruct the original images. Although DDIM Inversion [46] is the most commonly used inversion method for diffusion models, its reliance on linear approximation often leads to noticeable artifacts in reconstructed images. Several works [8, 29, 31] employ fixed-point iteration to reduce the approximation error. If guidance is used during inversion, then guidance must be applied during sampling to achieve the same generated image and the same holds in reverse. Noise optimization. Optimizing or selecting noises with certain objectives has been key research focus in diffusion models [7, 28, 41]. ReNO [7] optimizes noises based on reward models and Samuel et al. [41] proposed bootstrapbased method to optimize initial noises for rare concept generation. However, these optimization methods require substantial number of iterations, which poses challenge Figure 5. Comparison between noise optimization methods. We compare two methods to optimize noise for target image generation. (a) illustrates direct optimization using inversion noise from the target image, while (b) shows optimization by minimizing the loss between denoised image and the target image. The rightmost column visualizes each optimized noise in lowfrequency area, indicating the similarity between the two noises. age space. To demonstrate the validity of our approach, we investigate whether reducing the image space distance d(x0, xGuide ) also effectively decreases the noise space dis0 tance d(xT , xGuide ) as training progresses, where is distance metric measuring the difference between two data points. This relationship is clarified in Proposition 1 and illustrated in Fig. 4. detailed proof is provided in supplementary material A.2. Proposition 1. Let xT be an initial noise, and suppose that x0 is the image obtained through denoising. Assuming Lipschitz continuity with distance metric d, for every xT , there exists constant κ > 0 such that the following holds: d(xT , xGuide ) < κd(x0, xGuide ). We support our proposition by conducting toy experiments. In Fig. 5, we compare two strategies: directly optimizing the noise xT with the empirical inversion noise xGuide , which we treat as true, and optimizing the loss beT tween denoised image xGuide and the target image x0. To visualize, we plot the low-frequency regions of two optimized noises, revealing their similarity and supporting our proposition. 0 Training pipeline. Our overall training framework is illustrated in Fig. 4. Given randomly sampled Gaussian noise xT and prompt c, diffusion pipeline takes the noise xT as an initial input and generates guided image xGuide by executing denoising steps with guidance [1, 13]. Our noise refining model gϕ() estimates the refined noise ˆxT . Using this ˆxT as initial input, the same diffusion pipeline generates an image ˆx0 through denoising steps without guidance. To reduce the gap between ˆx0 and xGuide , we ap2 ply distance loss d(ˆx0, xGuide 2. In this 0 ) = ˆx0 xGuide 0 0 Figure 4. Training pipeline. We propose training methodology to learn mapping from initial noise to guidance-free noise. Given an initial Gaussian noise xT , the original diffusion model parameterized by θ generates an image xGuide using guidance [1, 13]. Noise refining model refines the initial noise xT to produce ˆxT = gϕ(xT ), which is then input to the original model to generate an image ˆx0 without guidance. By minimizing the distance between two images d(xGuide , ˆx0), noise refining model effectively learns the desired mapping. Note that both noise refining model and original model also receive prompt as input, though this is omitted here for simplicity. 0 0 CFG [13], and ReNoise [8]. Comparing the absolute differences between xT and xGuide to those between ranT dom noise instances, Fig. 3 (a) shows that the differences in {xT , xGuide } pairs are significantly smaller than those of Random pairs. These differences mainly correspond to low-frequency components in the frequency domain as shown in Fig. 3, which plots the magnitude differences between Fourier-transformed noises. This suggests that guiding initial noise with suitable small low-frequency components for given condition can generate high-quality samples without additional guidance during the sampling stage. 3.2. Learning to Map Guidance-Free Noise Space Mitigating inversion error. straightforward approach for learning mapping to guidance-free noise space would be to learn the inversion noise directly. Although possible enough, inversion methods [8, 29, 46] have inherent limitations. They rely on approximations, which means true inversion noise xGuide is not guaranteed. Thus, attempting to learn this approximated inversion noise which includes inversion error may limit the performance. Hence, we try to sidestep this issue, by learning directly in the imT 4 Figure 6. Comparison of optimization results using different loss functions. The orange line represents the optimization process using the full gradient of the MSE loss, while the blue line depicts optimization using MSD loss. The images are sample outputs generated every 1000 optimization steps. The result demonstrates faster convergence but higher image quality when using MSD Loss. way, our model learns to guide the initial noise xT toward guidance-free noise space. For the architecture of the noise refining model gϕ(), we found that by attaching lightweight LoRA [17] to the pre-trained diffusion models, the noise refining model can effectively leverage the diffusion models rich knowledge of text and image information, allowing for faster convergence. Additionally, as shown in Fig. 3 (a), the difference between xT and xGuide is slight; therefore, we incorporate residual connection [11] in the noise refining model gϕ() to enable the model to converge rapidly during training. Multistep score distillation. Naively applying our method incurs high costs from backpropagating through the denoising network up to times and requires significant memory usage. Such requirements are one of the main reasons that recent noise optimization work [7, 22] typically relies on one or few-step models. However, we aim to maximize the number of denoising steps used in our method since the quality of xGuide affects the performance of our model prediction. 0 To circumvent the backpropagation costs of the full-step diffusion model, we propose novel approach, multistep score distillation (MSD), inspired by score distillation sampling [33], where we detach gradients through denoising network during backpropagation. Specifically, the typical denoising process LDenoise(gϕ(xT ), θ, d) is defined as follows: LDenoise(gϕ(xT ), θ, d) := (cid:0)D1 (. . . DT (gϕ(xT ))) , xGuide (2) (cid:1) 0 where Figure 7. Analysis of the relationship between the initial Gaussian noise xT and the refined noise ˆxT . (a) shows histogram of the absolute difference between xT and ˆxT . (b) displays the magnitude difference between the 2D Fourier-transformed frequency components F(xT ) and F(ˆxT ). This demonstrates that the model refines the noise by appropriately adding small, low-frequency components similar to the results shown in Fig. 3. DDIM sampler [46] and defined in supplementary material A.1. In MSD, we perform the typical denoising process but detach the gradients on the denoising network ϵθ at each step. Specifically: LMSD(gϕ(xT ), θ, d) := (cid:0)F1 (. . . FT (gϕ(xT ))) , xGuide 0 (cid:1) , (4) where Ft(x) = atxt + bt SG(ϵ(t) θ (x)), (5) where SG() denotes the stop-gradient (detach) operation. Our noise refining model gϕ() is trained to minimize LMSD(gϕ(xT ), θ, d). Fig. 6 compares optimization results with and without detached gradients, showing that disabling gradients in the denoising network leads to faster convergence and sharper images at substantially lower computational costs. We validate our approach, showing that MSD serves as close approximation to learning with fullgradient objective LDenoise(gϕ(xT ), θ, d). This is clarified in the following proposition. We provide detailed proof in supplementary material A.2. Proposition 2. By approximating the gradients through Multistep Score Distillation (MSD) using detached gradients at each step, we approximate the full-gradient objective with mild assumption. In conclusion, the two gradients can be approximated as follows: ϕLDenoise(gϕ(xT ), θ, d) kϕLMSD(gϕ(xT ), θ, d), (6) where (0, 1) is constant. 3.3. Dataset Construction Dt(x) = atxt + btϵ(t) θ (x), (3) and at and bt are coefficients derived directly from the We observe that some proportion of images generated with CFG [13] in Stable Diffusion 2.1 [36] exhibit low quality, often appearing blurry or displaying distorted facial 5 Figure 8. Qualitative results. Samples starting from Gaussian noise and generated without guidance (left), samples starting from Gaussian noise and generated with sampling guidance (middle), and samples starting from refined noise and generated without guidance (right). features, eyes, and noses. Fortunately, our framework is not constrained to CFG. It can incorporate any qualityenhancing techniques applicable at inference time [15, 16], including PAG [1], which is known to improve structural accuracy. To enhance the quality of samples, we apply PAG along with CFG, as PAG has been shown to reduce blurriness and improve anatomical structure effectively. Additionally, to mitigate the bias of fixed guidance scale, we use both CFG and PAG with randomly varied scale. Furthermore, we filter out low-quality images prone to structural issues or artifacts using human preference model [43], effectively eliminating poor samples. Detailed implementation details are provided in Sec. 4.2. As shown in Tab. 3, this filtering produces superior results compared to training without these enhancements. Note that generating guided image xGuide fline; that is, xGuide putational efficiency. 0 can be done either online or ofcan be pre-generated to enhance com4. Experiment In this section, to show the effectiveness and efficiency of noise refining model, we present extensive qualitative and quantitative results on Stable Diffusion 2.1 [36]. We train noise refining model with text prompts of MS-COCO [25] and Pick-a-pic [23]. CFG [13] and PAG [1] are applied to generating images from those datasets with filtering by aesthetic score [43]. We evaluate our model using text prompts of Drawbench [39], HPDv2 [48], Pick-a-pic [23], and MSCOCO [25]. More implementation details and experimental 6 Dataset Initial Noise Guidance PickScore [23] HPSv2 [48] AES [43] ImageReward [49] CLIPScore [35] DrawBench [39] HPD [48] Pickapic [23] MS-COCO [25] Gaussian Ours Gaussian Gaussian Ours Gaussian Gaussian Ours Gaussian Gaussian Ours Gaussian 19.67 20. 21.29 19.18 20.47 21.02 18.89 20.18 20.67 19.56 21. 21.59 0.1689 0.2306 0.2482 0.1778 0.2386 0.2469 0.1919 0. 0.2559 0.1654 0.2474 0.2504 5.129 5.351 5.475 5.319 5. 5.788 5.226 5.497 5.651 5.138 5.368 5.487 -1.399 -0. -0.020 -1.299 -0.163 0.159 -1.520 -0.304 0.018 -1.134 0. -0.058 25.16 29.40 30.36 26.85 31.21 32.27 24.89 29. 30.53 26.45 30.27 31.29 Table 1. Quantitative comparison of difference metrics across datasets. Starting from refined noise using noise refining model consistently yields higher human preference scores than starting with Gaussian noise, with scores comparable to the guidance case (CFG [13] + PAG [1]). settings can be found in supplementary material D. 4.1. Results Qualitative Comparison. We present our qualitative results evaluated on the aforementioned T2I benchmark datasets on Fig. 8. We observe notably degraded image quality when the initial noise is Gaussian and guidance is not applied. In contrast, when using the noise refined by our model, we observe consistently superior image quality compared to images from Gaussian noise. Moreover, images from our refined noises without guidance show comparable quality to those from guidance. This result demonstrates the effectiveness of noise refining model. Additional qualitative results can be found in supplementary materials C. Quantitative Comparison. To comprehensively evaluate image fidelity and diversity with FID [12] and IS [40], we generate samples from 30K randomly selected prompts on MS-COCO 2014 validation set [25] using three methods: sampling without guidance from Gaussian noise, sampling with guidance from Gaussian noise, and sampling without guidance from noise refined by noise refining model (ours). Tab. 2 presents FID [12] and IS [40] evaluation results and Tab. 1 presents the results of human preference scores [23, 43, 48, 49] and prompt adherence evaluation [35]. Across all metrics, our model shows consistent and substantially improved quality over that of images from Gaussian noise, achieving results comparable to those obtained with guidance. Notably, our model achieves lower FID [12] score than even the guidance setting, addressing concerns about potential declines in image fidelity and diversity when using model-generated data as training data. This result highlights the advantage of our method in enabling efficient training without relying on large-scale image datasets. More details about those datasets for evaluation can be found in supplementary material D. User Study. Tab. 4 shows the results of user study, confirming noise refining models comparable to results starting from Gaussian initial noise without guidance. 45 participants compared 30 image pairs generated with guidance and our method (refined noise without guidance), using generated images for evaluation in Tab. 1, and evaluated visual appealiing and prompt alignment. Initial Noise Guidance FID [12] IS [40] Inference Time Gaussian Ours Gaussian 42.71 11. 13.38 20.86 35.73 37.64 1.357s 1.504s 2.589s Table 2. Quantitative comparison of image quality and computational cost. Parameter FID [12] IS [40] # of steps Filtering 5 10 13.74 13.36 15.27 13.55 30.80 32.81 29.44 31.15 Table 3. Ablation study on the number of denoising steps and dataset filtering during training. Metric Gaussian Noise + CFG Refined Noise (Ours) Image Quality Prompt Adherence 46.04% 48.24% 53.96% 51.76% Table 4. User study on the image quality and prompt adherence of generated images. 4.2. Ablation Study Number of denoising steps. We demonstrate that the number of denoising steps significantly impacts performance. Specifically, we compare cases with denoising steps 7 = 5 and = 10, reporting FID [12], IS [40] in Tab. 3. The results indicate improved performance with bigger number of denoising steps. Considering that high number of steps (e.g., 10) incurs prohibitive backpropagation costs, this supports the necessity of MSD to circumvent backpropagation costs. Dataset filtering. To evaluate the impact of dataset filtering, we generate images using 80K prompts from the MS-COCO [25] dataset and remove those with an aesthetic score (AES) [43] below 6.0, resulting in about 25% of the images remaining. As shown in Tab 3, metrics for generated images are significantly improved in the filtered dataset, demonstrating the substantial impact of dataset filtering on enhancing guidance-free image generation. 5. Analysis and Discussion We analyze what noise refining model learns and identify components in refined noise that contribute to guidancefree generation, discussing the advantages of working in this space. 5.1. What Does noise refining model Learn? In Fig. 7, we show that our model refines the noise by adding mostly small, low-frequency components. The distribution of absolute norm and frequency of the added noise difference is similar to the noise difference between Gaussian noise xT and the inversion noise xGuide shown in Fig. 3, without explicit constraints to achieve this. Low-frequency components aid denoising. Fig. 9 shows that the low-frequency components in the noise difference are dependent on the condition (e.g., text prompt) and act as an initial layout for the synthesized image. These low-frequency signals significantly help diffusion models in forming object shapes in the early steps of denoising. Fig. 10 (a) shows that starting from refined noise, the model quickly establishes plausible images at much earlier stages, allowing the model to focus on adding details within the given layout during denoising. In contrast, as shown in Fig. 10 (b), the diffusion model struggles to create coherent layout in the early denoising steps, resulting in partial details filled in incorrect locations, often leaving ambiguous regions untouched throughout the denoising process. Diversity and generalizability. While these initial layouts might appear to limit diversity, Fig. 9 shows that the generated layouts vary significantly depending on the initial noise. We confirm this with diversity metrics of IS [40], demonstrating greater diversity than guidance-based methods. In addition, our model generalizes beyond the training data, performing well with unseen noise and prompts (Fig. 9, Table. 1), suggesting generalizable mapping between initial and refined noise. Figure 9. Visualization of noise difference between xT and ˆxT . The top row shows the difference, while the bottom row displays the corresponding generated images. The added signal functions as layout, guiding the structure of the image during generation. Here, prompt photo of corgi is used. Figure 10. Visualization of denoising process. (a) Starting from refined noise aids the model in establishing the overall layout early in the generation process, facilitating the successful creation. (b) In contrast, when beginning with Gaussian noise, the model struggles to capture the overall layout, resulting in incomplete or disjointed details rather than producing fully plausible image. Figure 11. Comparison of training noise refining model gϕ (top) and the denoising network ϵθ (bottom). This demonstrates that tuning the input of the diffusion pipeline converges faster than tuning the pipeline (denoising network) within the same number of training iterations. 5.2. Why Learn Noise Mapping? We consider the rise of prompt learning. Large-scale models like CLIP [35], trained on web-scale datasets often contain up to billions of parameters. Fine-tuning such models is impractical and risks disturbing well-learned representations [52]. Instead, tuning the input prompts of the model, method known as prompt learning, has gained popularity as an effective approach [18, 45, 51, 52]. 8 In this context, limiting training to the noise space can be seen as an efficient alternative to tuning the entire denoising pipeline. As seen in Fig. 7 and Fig. 9, certain low-frequency components in the noise space provide critical information, such as image layout, which allows learning with smaller dataset without the need to tune all model parameters. Unlike typical text-to-image diffusion models [36] or their distilled versions [24, 32, 42], which require up to billions of images [44], our model achieves effective training with only 50K noise and self-generated image pairs. We verify this by tuning the diffusion model. However, as shown in Fig. 11, this approach led to slower convergence and frequent loss explosions, making training unstable. Details and discussion of this comparison can be found in supplementary material E. 6. Conclusion In this work, we propose NoiseRefine, an efficient and effective approach to replacing guidance in diffusion sampling with noise refinement by single neural network forward pass. Our noise refining model functions as plug-and-play module based on the original diffusion model and significantly improves image fidelity. Furthermore, our method is highly efficient, which can be trained using lightweight lora, requiring only small set of modelgenerated images for training and remaining feasible on consumer-grade GPUs, thanks to our proposed MSD loss. Beyond its practicality, we believe this work serves as stepping stone toward deeper understanding of the role of guidance and noise in diffusion models."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. arXiv preprint arXiv:2403.17377, 2024. 1, 2, 3, 4, 6, 7, 11, 12, 15 [2] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 16 [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1 [4] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 1 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 [7] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. arXiv preprint arXiv:2406.04312, 2024. 2, 3, 5 [8] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real imarXiv preprint age inversion through iterative noising. arXiv:2403.14602, 2024. 2, 3, 4 [9] Daniel Geng, Inbum Park, and Andrew Owens. Factorized diffusion: Perceptual illusions by noise decomposition. In European Conference on Computer Vision, pages 366384. Springer, 2025. [10] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-to-image diffusion models via initial noise optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93809389, 2024. 2, 16 [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7, 8 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 3, 4, 5, 6, 7, 11, 12, 14, 15, 16, 18 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [15] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. arXiv preprint arXiv:2408.00760, 2024. 1, 2, 3, 6, 11 [16] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7462 7471, 2023. 1, 2, 3, 6, 11, [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 5, 12, 13 [18] Zhengbao Jiang, Frank Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8: 423438, 2020. 8 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 [19] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into condi9 tional gans. In European Conference on Computer Vision, pages 428447. Springer, 2025. els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 9 arXiv preprint [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 2, 19 [21] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 2, 3, 11 [22] Jeeyung Kim, Ze Wang, and Qiang Qiu. Model-agnostic human preference inversion in diffusion models. arXiv preprint arXiv:2404.00879, 2024. 5, 18, 19 [23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 6, 7, [24] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 2, 9 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 3, 6, 7, 8, 12, 13, 19 [26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 19 [27] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided image synthesis via initial image editing in diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 53215329, 2023. [28] Jiafeng Mao, Xueting Wang, Semantic-driven initial image synthesis in diffusion model. arXiv:2312.08872, 2023. 3, 16 and Kiyoharu Aizawa. image construction for guided arXiv preprint [29] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, and Rami Ben-Ari. Fixed-point inversion for text-toimage diffusion models. arXiv preprint arXiv:2312.12540, 2023. 2, 3, 4 [30] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 2 [31] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15912 15921, 2023. [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: 10 [33] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2, 5 [34] Zipeng Qi, Lichen Bai, Haoyi Xiong, et al. Not all noises are created equally: Diffusion noise selection and optimization. arXiv preprint arXiv:2407.14041, 2024. 2 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7, 8, 13 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3, 5, 6, 9, 12 [37] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann Weber. Cads: Unleashing the diversity of diffusion models through condition-annealed sampling. arXiv preprint arXiv:2310.17347, 2023. [38] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024. 1, 2, 3, 11 [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1, 6, 7, 12 [40] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 7, 8 [41] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. Generating images of rare concepts using pretrained diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 46954703, 2024. 2, 3 [42] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 9 [43] Christoph Schuhmann. Improved aesthetic predictor. https : / / github . com / christophschuhmann / improved-aesthetic-predictor, 2022. 6, 7, 8, 12 [44] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 9 [45] Taylor Shin, Yasaman Razeghi, Robert Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020. 8 [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3, 4, 5, 1, 11, 12, 14, 15, 18, 19 [47] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 18 [48] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6, 7, 12 [49] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation.(2023). ArXiv Prepr ArXiv230405977. [50] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Good seed makes good crop: Discovering secret seeds in text-toimage diffusion models. arXiv preprint arXiv:2405.14828, 2024. 2 [51] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language modIn Proceedings of the IEEE/CVF conference on comels. puter vision and pattern recognition, pages 1681616825, 2022. 8, 13 [52] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei InLiu. Learning to prompt for vision-language models. ternational Journal of Computer Vision, 130(9):23372348, 2022."
        },
        {
            "title": "Supplementary Material",
            "content": "In the supplementary material, we clarify the notations and formulations related to diffusion models used in the main paper and provide the proofs for our propositions (Section A), more ablation studies regarding noise refining model (Section B), additional results including qualitative results, comparison with other methods, user study (Section C), implementation details and experimental settings (Section D) and further discussions (Section E). A. Theoretical Background A.1. Preliminaries Denoising Diffusion Probabilistic Models (DDPM). DDPM [14] defines forward process that derives xt by adding Gaussian noise to the image xt1 according to the variance schedule, and reverse process that samples xt1 from xt, both as Markovian chain. The forward process is defined as q(xtxt1) = (cid:114) αt αt (cid:18) xt1, 1 (cid:19) (cid:19) , αt αt1 (cid:18) xt; q(xtx0) = (xt; αt x0, (1 αt)I), (7) (8) with noise rate at timestep as 1 αt/αt1, where αt denotes noise scaling factors up to time step t. The reverse process is defined below. To reparameterize the equation using pθ(xt1xt) = (cid:16) xt1; µ(t) θ (xt), σ2 (cid:17) . xt = αtx0 + 1 αt ϵ for ϵ (0, I), and ϵθ, which is function approximator for predicting ϵ from xt, the inference process becomes xt1 = 1 (cid:113) αt αt1 (cid:32) xt 1 αt αt1 1 αt (cid:33) ϵ(t) θ (xt) + σtz, Where (0, I) and σ2 denotes the variance of Gaussian trainsitions .The objective of DDPM is defined as (cid:104) ϵ ϵ(t) Lsimple(θ) = Et,x0,ϵ θ (xt)2(cid:105) , (9) (10) (11) (12) where the L2 loss between the actual noise ϵ added during training and the noise prediction ϵθ(xt, t) for uniformly sampled {1, ..., }. Denoising Diffusion Implicit Models (DDIM). DDIM [46] consider the following inference distributions: qσ(x1:T x0) := qσ(xT x0) (cid:89) t=2 qσ(xt1xt, x0). with mean function as below. qσ(xt1xt, x0) = (cid:18) (cid:113) αt1x0 + 1 αt1 σ2 xt αtx0 1 αt (13) (14) (cid:19) . , σ2 Distinctively from DDPM, the forward process is Non-Markovian since each xt could depend on both xt1 and x0. Reparameterizing with ϵθ, we can sample xt1 from xt through an equation: xt1 = αt1 (cid:32) xt (cid:124) θ (xt) 1 αt ϵ(t) αt (cid:123)(cid:122) predicted x0 (cid:113) 1 αt1 σ2 ϵ(t) θ (xt) + σtϵt + (cid:33) (cid:125) = atxt + btϵ(t) θ (x), 1 (16) (17) θ (x) with (xt) [1], or where ϵt (0, I) and at = αt1/ αt, bt = 1 αt1 at 1 αt. The objective of DDIM is the same as that of DDPM: LDDIM(θ) = Et,x0,ϵ (cid:104) ϵ ϵ(t) θ (xt)2(cid:105) . (15) Denoising and inversion process. We denote the denoising process as Denoise(xT ). When using the DDIM sampler [46], the denoising process is defined as: Denoise(xT ) := D1 (. . . DT (gϕ(xT ))) , where each step Dt is given by: Dt(x) := atxt + btϵ(t) The guided denoising process, denoted as DenoiseGuide(xT , c), follows the same steps as Eq. 16, but replaces ϵ(t) θ (x). (xt, c) [13], the perturbed-attention guided score ϵPAG θ guided scores, such as the classifier-free guided score ϵCFG combination of both (ϵCFG,PAG θ (xt)). These guided scores are defined in Eqs. 26, 27, and 28. While we utilize the DDIM scheduler in this work, any other diffusion scheduler [14, 20, 46] can be used by appropriately θ modifying at and bt. For the inversion process Inversion(x0, c), we follow the method in [8] to obtain the initial noise xT , which can be denoised back to the given image x0 without employing any guidance methods [1, 13] during inversion. A.2. Derivations Proposition 1. Let xT be an initial noise, and suppose that x0 is the image obtained through denoising. Assuming Lipschitz continuity with distance metric d, for every xT , there exists constant κ > 0 such that the following holds: d(xT , xGuide ) < κd(x0, xGuide ). proofs. The Lipschitz condition is expressed as follows: d(ϵ(t) θ (x), ϵ(t) θ (y)) Ltd(x, y), (18) where Lt is constant dependent on t, and are arbitrary inputs to ϵ(t) follows: θ . DDIM step in terms of xt can be expressed as xt1 = (cid:114) αt1 αt xt + (cid:112)1 αt1 (cid:115) αt1(1 αt) αt ϵ(t) θ (xt). (19) Eq. (19) can be expressed in terms of xGuide equation, which is denoised from xGuide . With those equations, we can get the following t1 = xt1 xGuide (cid:114) αt1 αt (cid:114) αt1 αt (cid:16)(cid:112)αt1(1 αt)/αt where γt = be expressed as follows with Eq. (18): = (xt xGuide (xt xGuide ) + (cid:112)1 αt1 (cid:115) αt1(1 αt) αt ) γt(ϵ(t) θ (xt) ϵ(t) θ (xGuide )), (ϵ(t) θ (xt) ϵ(t) θ (xGuide )) (cid:17) 1 αt1 > 0. If the distance metric have translation invariance, the equation can d(xt1, xGuide ) (cid:114) αt αt1 (1 + γtLt)d(xt, xGuide ). Recursively organizing Eq. (20) for = T, 1, . . . , 1, it can be expressed as follows: d(xT , xGuide ) (cid:32) (cid:89) (1 + γtLt) t= (cid:33) (cid:114) αT α0 d(x0, xGuide 0 ). (20) (21) Since αT is close to 0, using d(x0, xGuide 0 ) is sufficient to directly learn xGuide if d(x0, xGuide 0 ) is small enough. 2 Proposition 2. By approximating the gradients through Multistep Score Distillation (MSD) using detached gradients at each step, we approximate the full-gradient objective with mild assumption. In conclusion, the two gradients can be approximated as follows: ϕLDenoise(gϕ(xT ), θ, d) kϕLMSD(gϕ(xT ), θ, d), (22) where (0, 1) is constant. proofs. Since the only difference between the two losses is the stop gradient in the diffusion model and all other components are identical, it suffices, by the chain rule, to show that the gradient of F1(F2(. . . FT (gϕ(xT )) with respect to ϕ is proportional to the gradient of Denoise(gϕ(xT )) with respect to ϕ. The derivation proceeds as follows: ϕDenoise(gϕ(xT )) = ϕ (cid:32)(cid:114) α0 αT (cid:32)(cid:114) α0 αT = (cid:88) gϕ(xT ) t=1 (cid:114) α0 αt1 (cid:88) t=1 γt (cid:114) α0 αt (cid:33) γtϵ(t) θ (xk) ϵ(t) θ (xt) xt xt gϕ(xT ) (cid:33) gϕ(xT ) ϕ . (23) θ (xk)/xk can be approximated as being proportional to the identity matrix. Additionally, the θ (xk)/xk. Then, each component of ϵ(t) θ (xk)/xk can be approxiθ (xk)/xk) (xk/gϕ(xT )) becomes proportional to the identity matrix. As detailed in B, the term ϵ(t) term xk/gϕ(xT ) can be expressed in terms of ϵ(t) mated by the identity matrix. Consequently, (ϵ(t) Denoting the proportionality constant as ηt, Eq. (23) is simplified as follows: (cid:32)(cid:114) α0 αT (cid:114) α0 αt (23) = (cid:88) γtηt (cid:33) (24) t=1 (cid:32) = 1 (cid:32) = 1 αT αT (cid:88) t=1 (cid:88) t=1 gϕ(xT ) ϕ (cid:33) (cid:114) α0 αT gϕ(xT ) ϕ 1 αt1 γtηt (cid:33) γtηt ϕF1(F2(. . . FT (gϕ(xT ))). 1 αt1 3 B. More Ablation Studies B.1. Diffusion Model Jacobian Approximation In this section, we present experimental results demonstrating that the Jacobian of the diffusion model ϵt θ with respect to the input xt can be approximated as proportional to the identity matrix. Fig. 12 illustrates the Jacobian of ϵt θ. We observe that the Jacobian of diffusion model behaves like the identity regardless of the timestep, except when is significantly small. In such cases, the deviation does not affect our primary analysis. According to the results of Proposition 2, the timestep-dependent constant multiplied to each Jacobian term ηt is expressed as follows: 1 αt1 γt = (cid:114) 1 αt αt (cid:115) 1 αt1 αt . (25) This value can be numerically determined based on the scheduling, and in the case of DDIM [46], it is presented in Fig. 13. The graph shows that the constant decreases as approaches 0, becoming 0. Figure 12. Visualization of Jacobian of denoising network. Starting from = 1000, we performed denoising over 10 steps and plotted the Jacobian heatmap at each timestep.We extracted 500 500 section from the full Jacobian matrix for visualization. Each plot demonstrates that the Jacobian is close to the identity matrix. Figure 13. Visualization of constant values corresponding to Equation (19) across different timesteps. The results numerically demonstrate that for small timesteps, where the Jacobian deviates from the identity matrix, the multiplied constant values are sufficiently close to zero. over timesteps. Visualization of the time-dependent constant value αt1 αt1 γt γt 4 B.2. Utilizing Pretrained Knowledge of Diffusion Models To train noise refining model, we adopt attaching LoRA layers to the original model to effectively leverage its pretrained knowledge. To assess the impact of pretrained knowledge, we conduct an ablation study. The comparison involves noise refining model and the same UNet architecture of the original model, but trained from scratch. We used only filtered MS COCO dataset among both datasets and trained models for 25K steps using two RTX 3090 GPUs. All the other experimental settings are kept consistent. Qualitative results are presented in Fig. 14, and quantitative results are detailed in Tab. 5. Both results indicate that leveraging pretrained knowledge results in superior performance compared to training from scratch. Model UNet trained from scratch Pretrained + LoRA (noise refining model) FID 37.87 13.74 Table 5. Quantitative comparison with noise refining model and UNet trained from scratch. Figure 14. Qualitative comparison with noise refining model (top) and UNet trained from scratch (bottom). 5 C. Additional Results C.1. Additional Qualitative Results We present our additional qualitative results on Fig. 15, 16, 17 and 18. Results show that the performance of using refined noise by noise refining model is comparable to that of using guidance on random Gaussian noise. All the results are selected from images used in Tab. 1 and 2. C.2. User Study We conducted user study to evaluate prompt adherence and image quality by comparing images generated from random Gaussian noise and our refined noise. The results are presented in Tab. 6. The study demonstrates that our method outperformed the baseline in all human evaluation criteria. total of 26 participants anonymously evaluated 20 pairs of images, each pair consisting of an image generated using initial Gaussian noise and our refined noise from noise refining model. The percentage was calculated by dividing the total number of selections for each option by the total number of responses, following the same methodology as in Tab. 4. Participants were provided with the following instructions for each pair of images: 1. Which image has better overall quality? (left/right) 2. Which image more faithfully reflects the given prompt? (left/right)"
        },
        {
            "title": "Metric",
            "content": "Gaussian Noise Refined Noise (Ours)"
        },
        {
            "title": "Image Quality\nPrompt Adherence",
            "content": "3.08% 6.73% 96.92% 93.27% Table 6. User study on the image quality and prompt adherence of generated images. 6 Figure 15. Additional qualitative results. Figure 16. Additional qualitative results. 8 Figure 17. Additional qualitative results. 9 Figure 18. Additional qualitative results. D. Implementation and Experimental Details D.1. Implementation Details Figure 19. Training framework. We provide an annotated illustration of the training framework to clarify the notation in the following discussion. More details of our framework. We can generalize our framework NoiseRefine from pixel-level diffusion models to latentlevel diffusion models, but in our experiments, we use MSE loss in latent space for d(xGuide , ˆx0). We provide our training framework in Fig. 19. It consists of three parts: Guidance T2I Pipeline takes Gaussian noise xT (0, I) and condition (text prompt) as inputs and generates an image xGuide with guidance methods [1, 13, 15, 16, 21, 38]. noise refining model gϕ refines Gaussian noise xT . Guidance-Free T2I Pipeline takes refined noise ˆxT = gϕ(xT ) and condition (text prompt) and generates an image ˆx0 without guidance. For Guidance T2I Pipeline, with the denoising network ϵθ, we can use the guided score ϵCFG (xt, c) for PAG [1] in denoising process as below: (xt, c) for CFG [13] or ϵPAG 0 0 θ θ ϵCFG θ ϵPAG θ ϵCFG,PAG θ (xt, c) = ϵθ(xt, c) + w(ϵθ(xt, c) ϵθ(xt, )), (xt, c) = ϵθ(xt, c) + s(ϵθ(xt, c) ˆϵθ(xt, c)), (xt, c) = ϵθ(xt, c) + w(ϵθ(xt, c) ϵθ(xt, )) + s(ϵθ(xt, c) ˆϵθ(xt, c)), (26) (27) (28) where and denote the guidance scale of CFG [13] and PAG [1], denotes the condition, and denotes the null condition (i.e., empty prompt). Note that the perturbed score ˆϵθ is from perturbing the forward process of the denoising network ϵθ [1]. With the denoising step = 20, we can get the guided image xGuide . Our noise refining model refines Gaussian noise xT with gϕ at timestep = , which is from the reverse step of DDIM [46] in Eq. (15). The output of noise refining model gϕ is denoted as ˆxT = gϕ(xT ) and becomes the input of Guidance-Free T2I Pipeline. In this pipeline, ˆxT is denoised into ˆx0 without guidance using denoising steps. 0 Model details. For noise refining model gϕ, we use Stable Diffusion 2.1 [36] with LoRA [17] rank of 128, applied to all attention, convolutional, and feed-forward layers. We use DDIM [46] scheduler with the same settings as the pre-trained model. For noise refinement, we use an input timestep = 999, and the default denoising step is set to 10. D.2. Experimental Details Training details. The training dataset consists of two parts: 20K images generated with CFG [13] (guidance scale 7.5) using prompts from MS COCO [25], and 30K images generated with both CFG [1] (guidance scale 3.0) and PAG [1] (guidance scale 2.0) using prompts from Pick-a-pic [23]. Only images scoring above 6.0 on the LAION Aesthetics Predictor V2 [43] are selected for training. Furthermore, for training, we use 8 A100 GPUs of 40GB vRAM with batch size of 4 and sample the images for evaluation using weights of 39k training steps. Evaluation details. For sampling images with guidance in Tab. 2, 40% of 30k images are generated with CFG [13] (w = 7.5) and remaining 60% are generated with both CFG [13] (uniformly from [3.0, 5.0]) and PAG [1] (uniformly from [2.0, 3.0]). For Tab. 1, we use 200 prompts from Drawbench [39], 400 prompts from HPDv2 [48], and 500 prompts from test set of Pick-a-pic [23] for generating 5 images per prompt. Here, for MS-COCO [25], we use 5k generated images selected from those used in Tab. 2. The qualitative results for Fig. 8 are from the images used in Tab. 1 and 2. Additionally, Inference time in Tab 2 is computed by averaging time per image across 30K images generated with the inference step of 20 and batch size of 1 on RTX 3090. Ablation study settings. For the ablation study on the number of denoising steps, we use the training dataset which consists of MS COCO [25] and Pick-a-pic [23] used in training noise refining model. The models are trained in two V100 GPUs for 100K steps. In the case of training dataset filtering, we only use MS-COCO [25] dataset, and the models are trained in two RTX 3090 GPUs for 25K steps. For the unfiltered case, entire 80K images are used. For the filtered case, we use the same filtering criteria detailed in D.1, resulting in 20K images. All the other training details are kept consistent. E. Discussion In this section, we compare the performance between training the noise refining model gϕ and the denoising network ϵθ in the denoising process without guidance (Sec. E.1). In addition, we present our hypothesis on why refined noise eliminates the need for guidance methods, explaining it step by step (Sec. E.2). We further analyze the impact of initial noise and prompt on the generated image (Sec. E.3). E.1. Effectiveness of Prompt Learning As shown in the training framework of our method  (Fig. 19)  , the noise refining model can be trained using the loss d(xGuide , ˆx0), but the denoising network ϵθ within the Guidance-Free T2I pipeline can also be trained. Instead of directly 0 training the model itself (e.g., fine-tuning models like CLIP [35]), our method demonstrates the efficiency of learning the noise input to the Guidance-Free T2I pipeline, akin to prompt learning, which optimizes prompts instead of models. Specifically, similar to conditional prompt learning such as CoCoOp [51], noise prompts are conditionally generated based on different inputs (Gaussian noise xT and text prompt c). By leveraging the knowledge of pretrained denoising networks, noise prompts can be generated efficiently, as verified in Sec. B.2. Here, we examine the case of training Guidance-Free T2I pipeline itself. Specifically, following the settings of ablation study on the number of denoising steps (Tab. 3) where we use filtered MS COCO [25] dataset, we compare the performance of models trained using our learning method  (Fig. 19)  with models where ϵθ is fine-tuned on xT as input without using the noise refining model. Instead of directly fine-tuning ϵθ, we train LoRA [17] module with the same rank and layers as gϕ (used in the noise refining model). The results in Fig. 11 clearly show that training the noise prompt leads to significantly faster convergence and higherquality outputs at the same training steps. In the figure, the first row represents the outputs from models trained with the noise refining model ϵθ, while the second row shows outputs from models where ϵθ was fine-tuned using LoRA [17]. E.2. Why does refined noise help denoising? To identify which refined noise components contribute to guidance-free generation, we first decompose the refined noise into multiple frequency components. In this study, we utilize two-dimensional Fourier transform to break down both the refined noise and the initial noise into their respective frequency components. Each frequency component is represented by frequency band, denoted as (a, b), which corresponds to the frequency range from to b. Note that although we explored other decomposition methods, such as dividing the noise into patches, they did not yield interpretable results. Figure 20. Visualization of denoised images according to the cutoff band. Both refined and initial noise were transformed into the frequency domain using Fourier transforms. The frequency domain of the initial noise, normalized such that the maximum radius is 1. (a) The frequency divided into intervals of 0.1. For each interval, the corresponding frequency components were replaced with those from the refined noise, followed by denoising. The results show that only when the (0, 0.1) frequency band was replaced does an image generated by the refined noise emerge. (b) Visualization of denoised images by incrementally increasing the cutoff radius from 0 in steps of 0.01 and replacing the corresponding components of the initial noise with refined noise. The results demonstrate that images denoised using refined noise are obtained starting at cutoff radius of 0.03. 13 Figure 21. Visualization of the norm based on the frequency-filtered radius of refined noise. This visualization demonstrates the increase in the norm as the cutoff radius in the frequency domain is expanded. The refined noise was transformed into the frequency domain using Fourier transform, and the norm corresponding to each cutoff radius was calculated and plotted. Low-frequency components matter. Using 2D Fourier transforms, we transform both refined and initial noise into the frequency domain. The initial and refined noise frequency domain is normalized into (0, 1). We synthesize new noise signal by replacing specific frequency bands of the initial noise with the corresponding bands from the refined noise. Fig. 20 (a) presents the generated images corresponding to different frequency bands, demonstrating that the low-frequency components of the refined noise predominantly influence the generation process. In Fig. 20 (b), images are generated by varying the band length within the low-frequency region. The results indicate that, despite the low magnitude of the low-frequency components, which can be confirmed through Fig. 21, they are sufficient to reconstruct the image effectively. Figure 22. Denoised images using only low(top) / high(bottom) frequency components. Diffusion models can generate the overall structure of the image using only the low-frequency bands of the refined noise. We use DDIM [46] with 20 steps for denoising without CFG, and the prompt was photo of corgi. Diffusion models can generate images using only low-frequency components. In Fig. 22, we examine how well diffusion models can denoise when specific frequency bands of refined noise are retained, and the values of the remaining bands are set to zero (using ideal high/low pass filters). The top row shows the results of applying 2D Fourier transform to the refined noise, normalizing the FFT frequency domain into (0, 1), and sequentially retaining lower frequency bands, such as (0, 0), (0, 0.1), (0, 0.2), ..., (0, 1), while setting the remaining bands to zero. These noise inputs are then denoised without CFG [13]. The figure demonstrates that the diffusion model begins forming recognizable corgi shape even when only the lower 50% of frequency bands of the refined noise are present. In contrast, noise containing only high-frequency bands fails to generate coherent images. 14 Figure 23. Denoised images using only low (top) / high (bottom) frequency components with reinitialization. We use DDIM [46] with 20 steps for denoising without CFG, and the prompt was photo of corgi. High-frequency components contribute details. Here, we use the same noise decomposition process of refined noise as Fig. 22 but following [9], we reinitialize the frequency components that were set to zero with corresponding components from standard Gaussian noise, then denoise again. The results, shown in Fig. 23, indicate that when all frequency components are present, the diffusion model can generate clear and complete images. Randomly reinitialized high-frequency components appear to add details onto the structure formed by the low-frequency components. While refined noise retaining only the lower 10%20% of frequencies can still reconstruct the original image when the rest is reinitialized, noise retaining only the high-frequency components fails to do so. This suggests that low-frequency components alone carry the significant information needed for image generation. (a) Low-frequency components of refined noise (b) High-frequency components of refined noise Figure 24. Different denoised images using only low(a) / high(b) frequency components for different seeds. Here we use 8 different seeds. From the top rows, it visualizes 8 images using only the lower (a) / higher (b) 5%, 10%, 20%, and 30% (from the top to the last rows) frequency components of the refined noise. In Fig. 24, each row visualizes images generated with only the lower 5%, 10%, 20%, and 30% (from the top rows to last rows) frequency components of the refined noise, while the bottom row shows images generated with only the upper 5%, 10%, 20%, and 30% frequency components. These results confirm that low-frequency components encode the overall layout and structure, whereas high-frequency components lack meaningful information. From these observations, we infer that the poor quality of unguided diffusion model outputs is due to their failure to form appropriate low-frequency components during denoising. High-frequency details added on poorly formed layouts result in artifacts that are perceived as unnatural. How do guidance methods form plausible initial layouts? As highlighted in [1], classifier-free guidance (CFG) [13] enhances the difference between conditional and unconditional predictions at each step, amplifying signals that can only be generated with the condition (e.g., features like the eyes or nose of corgi in photo of corgi). This effectively strengthens salient features corresponding to low-frequency components in the early denoising steps. From this, we deduce that guidance methods [1, 13, 16] add appropriate low-frequency components during inference, aiding the formation of high-quality layouts. 15 Figure 25. Visualization of 11th layer cross attention map. Token corresponding to cat is used for visualization among the prompt photo of cat. First and second row is the case starting from random Gaussian noise where guidance is not used for the first row and used for the second row. Third row is the starting from refined noise by the noise refining model. Noise Delta means the difference between initial Gaussian noise xT and refined noise ˆxT . When guidance is not used, failure to create meaningful attention map across all timestep is notable, leading to completely broken generation. However when guidance or our refined noise is used, meaningful cross attention map is observed, leading to successful generation. Notably thanks to noise delta, better aligned cross attention map is observed even in earlier step (t = ) when the refined noise is used. How does the noise refining model form low-frequency layouts? Interestingly, the noise refining model naturally forms low-frequency layouts even though our training framework does not explicitly enforce learning them as can be seen in Fig. 7. To understand this, we analyze cross-attention and self-attention maps across denoising steps. Fig. 25 visualizes these maps at different timesteps. Gaussian noise fails to form meaningful cross-attention maps in early steps due to its near-zero signal-tonoise ratio (SNR), which is expected. However, this failure persists in later steps, indicating an inability to form well-aligned layouts (Fig. 25 first row). Research [2, 10, 28] has shown that reducing noisy artifacts in cross-attention maps and aligning them with object regions during inference improves performance. This suggests that the failure of cross-attention maps to align is key reason for the diffusion models inability to create coherent layouts. When using CFG [13] (second row) or refined noise (third row), the cross-attention maps align well with the prompt, resulting in better outputs. Notably, cross-attention maps for refined noise exhibit accurate object shapes from the very first step, implying that the diffusion model can form plausible layouts from the beginning of the denoising process. This is further supported by Fig. 10, which visualizes x0 predictions at each denoising step. Implications for guidance-free generation. Without guidance methods or noise refiners aiding the formation of lowfrequency layouts, diffusion models fail to create plausible initial layouts. Random low-frequency components lead to artifacts that are perceived as unnatural. An interesting avenue for future research would be identifying why diffusion models struggle to form low-frequency components without guidance and developing training techniques to eliminate the need for guidance during the training stage. 16 E.3. Impact of Initial Noise and Prompt on Generated Image Figure 26. Visualization of denoised image using different prompt for noise refinement ϵθ and denoising gϕ. We previously demonstrated how refined noise affects initial layouts and how guidance and refined noise contribute to forming these layouts effectively. In this section, we investigate how the layout and the prompt influence the final generated image during the denoising process. Specifically, we explore what happens when the prompt used to generate the initial layout (P1, one of the inputs to the noise refining model gϕ) differs from the prompt used during denoising (P2, one of the inputs to the denoising network ϵθ in the Guidance-Free T2I Pipeline shown in Fig. 19). Does the model prioritize one prompt over the other? Or does it attempt to harmonize both? We investigate this question through the results shown in Fig. 26. Fig. 26 (a) visualizes the predicted x0 term in Eq. 15 during the denoising process when no layout is provided (starting 17 from Gaussian noise). The leftmost image corresponds to the predicted x0 at = , and subsequent images are visualized every three steps. Due to the noisy and ambiguous nature of the initial layout of Gaussian noise, the diffusion model fails to form coherent lion layout from the initial structure. Instead, it partially adds features such as fur, mane, nose, or mouth, resulting in poor perceptual quality. In contrast, (b) shows that in the case of P1 = P2, refined noise effectively forms the lion layout from the beginning. The diffusion model accurately places the overall lion shape, including its mane, eyes, nose, and mouth, in appropriate positions during the denoising process. (c) shows the results when the denoising prompt P2 is set to an empty prompt (null prompt). Despite this, the model successfully generates feline animal based solely on unconditional generation, as the layout sufficiently captures the overall structure of the object. This can be interpreted as the information embedded in the refined noise. (d) demonstrates the case where the denoising prompt P2 is set to prompt similar to the initial layout prompt (a photo of tiger in the wild). When similar prompt is used, the image retains the layout provided by the refined noise while also adhering to the prompt. In (e), P2 is set to an entirely independent prompt (a laptop computer on desk). Here, the model fails to generate coherent image corresponding to the layout or the prompt. The diffusion model attempts to form laptop on the existing lion or feline layout but fails to align with the laptop prompt, leading to failure. Finally, (f) shows that applying CFG [13] in the settings of (e) allows the diffusion model to disregard the initial layout and generate laptop. This partially explains why CFG consistently produces high-quality images. Randomly generated initial noise is unlikely to align with the prompt (as shown in (a)), and CFG helps the model ignore such initial noise and generate images consistent with the given prompt. Figure 27. Images from interpolated refined Gaussian noise. Interpolation between refined noise. To evaluate whether the noise refining model effectively learns noise mapping, we follow [46, 47] to perform spherical interpolation on initial noise samples, generating multiple interpolated noises. We then refine each interpolated noise using the noise refining model and verify that the refined noises effectively interpolate natural images. In Fig. 27, (a) shows the images denoised by the diffusion model without any guidance method, starting from spherical interpolations of two random Gaussian noises. Specifically, each interpolated noise is obtained by performing slerp(xT1, xT2, a) for various interpolation ratios a, where slerp performs spherical interpolation between two Gaussian noise at ratio of a. Fig. 27 (b) shows the results of denoising the refined versions of these interpolated noises without guidance. The results demonstrate that the refined noises effectively interpolate between the two images. This indicates that the noise refining model does not simply memorize specific low-frequency signals while ignoring the input noise. Instead, it effectively learns mapping from Gaussian noise space to guidance-free noise space where semantic interpolation between guidance-free images is possible. E.4. Comparison with related work recent study [22] exists under the category of noise manipulation. To the best of our knowledge, this work is unique in its focus on learning the noise space itself, rather than optimizing or selecting. Therefore, we compare our proposed approach 18 with this methodology PAHI (Prompt Adaptive Human preference Inversion) [22] in this section. There are several key differences between the two approaches. First, the tasks being addressed are distinct. While PAHI [22] aims at generating outputs aligned with human preferences, our objective is to replace conventional guidance mechanisms entirely. Second, our method offers much greater flexibility. PAHI [22] assumes that sampling from certain (µ, Σ) instead of standard normal Gaussian distribution is more beneficial and predict µ and Σ. However, this assumption lacks strong theoretical foundation. In contrast, our approach aims to learn gaussian-free noise space without imposing such constraints. Additionally, while PAHI [22] is limited to few-step models due to the computational overhead of backpropagation, our approach leverages MSD loss, enabling the use of full-step models without modification. Although the official code for PAHI [22] is unavailable, we adhere to the guidelines presented in their paper as possible and compare with our method. Specifically, we compare the noise refining model with the setup that samples noise from (µ, Σ) where µ and Σ is predicted by MLP for given prompt. Both models are trained with filtered 20K MS COCO[25] dataset for 25K steps using two RTX 3090 GPUs. Example qualitative results of employing MLP are presented in Fig. 28, and quantitative comparisons are shown in Tab.7. Across both evaluations, the noise refining model outperforms the other setup by significant margin, showing the effectiveness of our proposed method. Figure 28. Qualitative results when employing shallow 2-layer MLP for estimating Gaussian parameters, as proposed by [22]. The results are significantly blurry, indicating that the simple approach of predicting µ and Σ under the assumption that the optimal noise lies within (µ, Σ) performs poorly. Method FID MLP [22] estimating Gaussian parameters Noise refining model (ours) 217.30 13.74 Table 7. Quantitative results when employing shallow 2-layer MLP for estimating Gaussian parameters, as proposed by [22]. E.5. Robustness to the number of denoising steps and schedulers Since the noise refining model is trained with fixed scheduler (DDIM [46]) and denoising steps (10), concerns arise regarding its performance when using different schedulers or denoising steps. To examine the impact of varying schedulers and denoising steps, we conduct experiments comparing qualitative results across diverse configurations. For comparison, we select DPM++ SDE [26], DPM++ 2M [26], and EDM [20], using the prompt photo of cat. The results, presented in Fig. 29, show that our refined noise consistently produces reliable outputs regardless of the denoising timestep or scheduler. This demonstrates the robustness of the noise refining model across diverse schedulers and denoising step configurations. 19 Figure 29. Inference results on our refined noise in various denoising steps and scheduler settings. (a), (b), and (c) present inference results employing different schedulers at denoising steps of 10, 20, and 50, respectively. The consistency observed across these results highlights the robustness of our refined noise to variations in both denoising steps and schedulers."
        }
    ],
    "affiliations": [
        "Hugging Face",
        "KAIST",
        "Korea University",
        "Sookmyung Womens University"
    ]
}