{
    "paper_title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "authors": [
        "Peixuan Han",
        "Zijia Liu",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 6 9 2 2 . 5 0 5 2 : r ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind"
        },
        {
            "title": "Jiaxuan You",
            "content": "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponents thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuaders awareness and analysis of the opponents mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use text encoder paired with trained MLP classifier to predict the opponents current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our methods effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP."
        },
        {
            "title": "Introduction",
            "content": "If you know the enemy and know yourself, you need not fear the result of hundred battles. Sun Tzu, The Art of War Persuasionthe process of influencing others attitudes, beliefs, or behaviorsis fundamental to human communication and social interaction, and is recognized as complex and advanced social behavior due to its high demands on reasoning and cognition. Throughout history, countless orators and scholars have explored the art of persuasion, seeking techniques to effectively shape others opinions, pursuit that large language models (LLMs) are now beginning to undertake as well. State-of-the-art LLMs like GPT-4o have demonstrated remarkable persuasive abilities, generating persuasive arguments [1, 2] and successfully influencing human attitudes in specific domains [3, 4]. Despite these advances, current LLM-based persuaders face significant limitations in modeling the interactive and dynamic nature of persuasion. During conversation, human participants naturally *Correspond to: ph16@illinois.edu, jiaxuan@illinois.edu Preprint. Under review. consider related claims around the central claim, forming rich cognitive graph as shown in Figure 1. Intuitively, skilled human persuaders navigate this cognitive graph, analyzing the persuadees mental state and identifying nodes in the cognitive graph that are ripe for elaboration and could potentially affect the central claim. This process relies on the ability to model and reason about others beliefs, cognitive capacity known as theory of mind (ToM). As cornerstone of human social cognition, ToM enables humans to tailor their messages to the opponents perspectives dynamically, making persuasion more effective and opponent-aware. LLMs, by contrast, often overlook the interconnected structure of the cognitive graph and rarely consider the opponents perspective in dynamic exchanges. This deficiency in theory of mind leads to two key shortcomings in current LLM-based persuaders. First, after stating their position in the initial round, they tend to repeat the same arguments rather than drawing on new insights from the broader cognitive graph [5]. For instance, the persuader trained without ToM modules exhibits up to 12% content overlap between consecutive turns (Figure 4b). Second, LLM-based persuaders are often rigid and self-centered, focusing narrowly on their own arguments rather than adapting to the opponents views dynamically [6, 7]. This lack of flexibility is exemplified in Figure 6, where baseline models employ opponent-oriented strategies far less frequently. Figure 1: Human thoughts about claims are inherently interconnected. While human persuaders can recognize this network of related propositions, LLMs often focus narrowly on the central claim alone and omit other claims. To address the above limitations, we propose ToMAP (Theory of Mind Augmented Persuader), novel training framework that simulates the human theory of mind process through two dedicated modules. Firstly, we introduce the counterclaim predictor, which prompts the persuader to model relevant claims in the persuadees cognitive graph explicitly and proactively anticipate potential objections. By explicitly predicting counterclaims, this module enables the persuader to plan ahead, generate more diverse arguments, and address objections preemptively. Furthermore, we design the opponent attitude predictor, which estimates the persuadees current level of agreement on the counterclaims in the cognitive graph. By evaluating the persuadees mental state and identifying which of the opponents claims are firmly held or uncertain, the attitude predictor plays key role in enabling audience-aware and adaptable persuasion strategies. Finally, we employ reinforcement learning (RL) to help the persuader interpret and leverage these insights effectively, since the base model cannot effectively utilize the ToM information directly. Our experiments demonstrate that the ToMAP model, with only 3B parameters, outperforms baseline configurations and exhibits persuasiveness comparable to or exceeding that of several state-of-the-art LLMs, including GPT-4o (39.4% relative gain) and Deepseek-R1 (2.7% relative gain) across diverse corpora and against multiple persuadee models. Ablation studies underscore the importance of ToM modeling and the RL process, as ablating either leads to significant drops in the persuasion effect. Further analyses reveal that ToMAP produces more complex reasoning chains and reduces repetition during training, achieves steady persuasion gains in long conversations, and adopts more logical and opponent-aware strategies. These findings highlight ToMAPs unique strengths robust and effective framework for developing more persuasive LLM agents, suggesting theory of mind and opponent modeling as promising directions for building more persuasive language agents."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Persuasion. Persuasion, broadly defined as influencing others attitudes, beliefs, or behaviors through communication [8], is fundamental ability of humans. Recently, LLM persuasion has attracted increasing attention with the rapid development of state-of-the-art LLMs [912]. Researchers found LLMs can produce persuasive content similar to, or even surpassing, human experts on certain areas such as political issues [1, 2, 13], vaccine promotion [3], investment decisions [4], and factual knowledge [14]. Moreover, empirical evidence shows that LLMs can potentially shift human attitudes in real-world conversations [15, 16, 13]. More recently, several systematic frameworks have been proposed to evaluate the persuasiveness of LLMs, either by comparing two arguments [17] or measuring the attitude and behavioral change of the persuadee model in conversations [15, 1820]. 2 closely related line of research focuses on enhancing the persuasion capabilities of LLMs. [21, 22] summarize common human persuasion techniques, which subsequent studies leverage to train LLMs explicitly in these strategies [23]; [24] explores retrieving external evidence to support persuasive arguments, while [19, 25] develop LLMs using high-quality persuasion dialogues. [26]s work shows model size yields diminishing returns for persuasiveness, indicating simply scaling model size may not make better persuaders. Reinforcement learning (RL) is another common approach to enhance models persuasiveness without annotated data. This line of research has primarily focused on narrow tasks, such as negotiation in games [27, 28] or specific traits like consistency [29]. Concurrently, [30] explores training general persuasion models with RL and utilizes Bayesian persuasion framework to evaluate persuasiveness. Theory of Mind. Theory of Mind (ToM) is crucial concept in psychology, which refers to the ability to understand others by attributing mental states such as beliefs, desires, intentions, and emotions [3134]. ToM enables individuals to predict and interpret others behaviors in society and facilitates complex communication, making it cornerstone of human meta-cognition ability. ToM ability in AI systems has attracted great attention [3538], and researchers have proposed benchmarks to evaluate LLMs ability to reason over others mental state [3943]. More complex ToM tasks, namely ToM on fiction scenarios [44] and high-order ToM [45, 46], are also introduced, showing future directions of enhancing LLMs ToM abilities. The relationship between persuasion and Theory of Mind (ToM) has been widely studied [4749], as understanding anothers mental state is fundamental to influencing it. More recently, [50] evaluated LLMs ToM in persuasive dialog, and [51] highlighted the lack of ToM in LLM conversations. Our work also explores the intersection of persuasion and ToM, training LLMs to utilize ToM information more effectively to enhance persuasion techniques."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce the key components of Theory of Mind Augmented Persuader (ToMAP). We first describe the setting of the persuasion task in Section 3.1 and the reinforcement learning algorithm used to train the persuader in Section 3.2. Finally, we introduce two theory of mind modules that provide critical information about the opponents thoughts Section 3.3."
        },
        {
            "title": "3.1 The Setting of Persuasion Task",
            "content": "The persuasion process is multi-turn conversation between two LLM agents: persuader, denoted as X, and persuadee, denoted as Y. During the conversation, the persuader aims to convince the persuadee to support given claim Q. Formally, we denote the ith turn in the conversation as Ti, and 1. Since the two agents take turns speaking in the conversation history till the ith turn as Hi = T1...i the conversation, the next turn can be represented as (cid:40) Tn PX( p, Q, Hn1) PY( p, Q, Hn1) if is odd, if is even, (1) where is the task-specific prompt (refer to Appendix B). After each persuadees turn, we employ 5-point Likert scale [52] to assess the persuadees perspectives [30, 18], represented as: s(Y, Hn, Q) {0, 1, 2, 3, 4}. As LLMs are known to exhibit inconsistency in opinions, which means LLM might agree to both one claim and the opposite claim [53, 54], we consider the persuadees opinion on two contradictory claims and calculate the balanced agreement score as: (2) S(Y, Hn, Q) = 0.5 + s(Y, Hn, Q) s(Y, Hn, Q) [0, 1], (3) where means the opposition of the original claim. From Equation (3), we can derive S(Y, Hn, Q)+ S(Y, Hn, Q) = 1, which ensures the consistency and reliability of LLM opinion judgment. 1Specially, we define H0 as an empty string."
        },
        {
            "title": "3.2 A Reinforcement Learning Formulation for Persuader Training",
            "content": "Reinforcement learning (RL) has demonstrated great success in recent work [5557], where models develop complex reasoning skills. Similar to [30], we utilize RL to train the persuader through to incentivize LLMs persuasion ability without extensive labeled data. As we find directly optimizing the persuader in multi-round interaction is challenging and requires much more complex design [56, 58], we propose decomposing the multi-turn conversations as singleturn activities to enhance the stability and efficiency of our training schema. Specifically, given the conversation history and target claim Q, we optimize the persuader to generate the next argument that maximizes the attitude shift of the persuadee. Formally, at the persuaders turn 2k 1, we use diff(T2k1, Q) = S(Y, H2k, Q) S(Y, H2k2, Q) (4) to measure the attitude shift after the persuaders turn and the persuadees following turn. Then, we normalize the agreement gap to the range of [1, 1] to obtain the persuasion reward: rpersuade(T2k1, Q) = diff(T2k1, Q) 1 S(Y, H2k2, Q) diff(T2k1, Q) S(Y, H2k2, Q) if diff(T2k1, Q) 02, if diff(T2k1, Q) < 0. (5) Our reward design not only bounds the range of reward value, but also captures the intuition that boosting agreement on claims already having high score is more challenging. It promotes persuasive strategies that effectively reinforce claims that are already partially accepted. In addition to the persuasion reward, we also employ following auxiliary rewards to finely regulate formatting and control generation quality: Format reward: We regulate the models output to be in the form of <thought>...</thought> <argument>...</argument>. Only outputs strictly adhering to this format will get reward of rformat = 1. Otherwise, rformat will be 0. Tag reward: According to the format, the response should contain the following tags: <thought>, </thought>, <argument>, and </argument>. For each tag name, the response must contain it exactly once to receive reward of 0.25 added to rtag. If any tag is missing or appears more than once, no reward is given for that tag. Repetition penalty: We found the model tends to repeat previous turns, instead of proposing new arguments. Therefore, we calculate the token-level 8-gram overlap between the current turn and the previous turns and set overlap rate threshold of 0.1. An argument with an overlap rate of τ > 0.1 will be penalized: rrepeat = min(0, 0.1 τ ). Overlength penalty: The maximum length of an argument is 200 tokens. An argument with length of > 200 will be penalized: roverlength = max(0.5, min(0, l200 These reward functions are reweighted to obtain final reward: 200 )). rfinal = rpersuade + rformat 0.1 + rtag 0.1 + rrepeat 0.1 + roverlength 0. (6) By optimizing this multi-perspective reward, our method ensures that the generated arguments are not only persuasive but also well-structured, concise, and novel, preventing undesirable behaviors like excessive repetition or overly lengthy responses. We adopt proximal policy optimization (PPO) [59], widely-used and stable algorithm, to train the persuader policy X, optimizing it with the standard PPO objective function: (cid:104) max ExD,T r(T, Q) βDKL (cid:0)X(T x) (cid:12) (cid:12) (cid:12) Xref(T x)(cid:1)(cid:105) (cid:12) (7) where is the dataset, is the reward function, β is hyperparameter controlling KL penalty and Xref is the reference policy of the persuader. Notably, = (p, Q, H) contains the system prompt, target claim, as well as the conversation history. Overall, the RL schema enables the persuader to explore diverse strategies, learn from conversational signals, and ultimately develop effective persuasion techniques. 2S(Y, H2k2) = S(Y, H2k) = 0 is special case where the formula is undefined. In this case, we define rpersuade to be 0 because the turn has no impact on the persuadees agreement. 4 Figure 2: Overview of Theory of Mind Augmented Persuader (ToMAP). ToMAP utilizes two ToM modules, the counterclaim predictor and attitude predictor, to effectively model the opponents mental state during the conversation. This design enables ToMAP to provide more diverse arguments and counter the persuadees concerns in more flexible and opponent-aware manner."
        },
        {
            "title": "3.3 Theory of Mind (ToM) Modules",
            "content": "Theory of Mind (ToM)the ability to understand others mental states, especially opinions and attitudesis essential for effective persuasion. To equip LLMs with such ability and provide crucial information for their strategic persuasion, we incorporate Counterclaim Predictor and Opponent Attitude predictor. into our persuader agent, as illustrated in Figure 2."
        },
        {
            "title": "3.3.1 Counterclaim Predictor",
            "content": "Unlike human persuaders, current large language models (LLMs) often struggle to conceptualize debate topic and its associated claims as structured, interconnected cognitive graph. This limitation reduces their ability to anticipate an opponents beliefs and potential counterarguments. Therefore, we introduce the counterclaim predictor module, novel component designed to encourage the persuader to proactively anticipate counterarguments that might be held by the persuadee. Based on that, the persuader can craft more resilient and contextually aware persuasive strategies. Implementation. We use delicately designed prompt to encourage the persuader to consider claims challenging the persuasion goal. Formally, the counterclaim predictor can be expressed as q1...k X(p, Q, k), (8) where is the original claim and q1...k are the counterclaims against Q."
        },
        {
            "title": "3.3.2 Opponent Attitude Predictor",
            "content": "While the counterclaim predictor effectively identifies potential objections, it cannot capture the nuanced shifts in opinion that occur during conversation. To provide the persuader with such dynamic understanding of the persuadees evolving beliefs, we propose the opponent attitude predictor to predict the persuadees opinion on opposing claims generated by Equation (8). By leveraging the attitude predictor, the persuader can gain more accurate understanding of the persuadees thoughts, which are necessary for flexible and targeted persuasion in the subsequent turns. Implementation. As zero-shot prompting doesnt yield satisfactory results, we use an external classifier, which consists of text encoder and multilayer perceptron (MLP), to predict opponent attitudes. Specifically, we use the pre-trained BGE-M3 encoder [60] to encode the claim and the conversation as vectors separately. The two embedding are then concatenated and passed to 5-way MLP classifier, which predicts the agreement score s: pre(H, q) = M(E(H)E(q)) where denotes the encoder, denotes multi-layer perceptron, is the conversation history and is the claim. refers to the concatenation of vectors. With the cross-entropy loss, we train to (9) 5 accurately predict the persuadees attitudes on counterclaims generated by the counterclaim predictor. Refer to Appendix A.2 for details regarding the attitude predictor."
        },
        {
            "title": "3.3.3 Incorporating ToM Information",
            "content": "Information from the ToM modules is included in the persuaders prompt during training and inference, serving as an augmentation of input in the PPO formulation (Equation (7)): xaug = (x, q1...k, pre(H, q1...k)) = (cid:0)p, Q, H, q1...k, pre(H, q1...k)(cid:1). (10) In conclusion, the ToMAP framework trains the persuader with ToM-aware reinforcement learning, enabling it to effectively incorporate the opponents mental state information into its persuasive planning, resulting in more diverse and impactful arguments."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Datasets. During training, we use the Cornell CMV dataset [61], widely used corpus derived from Reddits ChangeMyView forum. It encompasses broad range of topics, including political views, philosophy, environment, daily habits, and more. We train the persuader on 18.2k claims. In evaluation, we also included claims derived from the Anthropic Persuasion Dataset [20] and the args.me corpus [62] to assess persuaders in out-of-distribution persuasion topics. Notably, all topics collected are controversial issues, so we set the persuadees initial stance to be negative in the prompt to ensure S(Y, H0, Q) < 0.5. Please refer to Appendix for prompts, Appendix for data statistics and Appendix for persuadees initial attitudes. Base Models. We train persuader models based on QWen-2.5-3B-Instruct [63], and the persuadee model during training is QWen-2.5-7B-Instruct. During evaluation, we also evaluate persuaders against LLaMa-3.1-8B-Instruct [64] and Phi-4 [65], to assess the generalizability. Metric. We use agreement shift, the gap between the persuadees agreement score before and after the conversation, as the metric indicating persuasiveness. Formally, the agreement shift is calculated as S(Y, H2n, Q) S(Y, H0, Q) (n = 3 in the main experiment), which is then scaled to 0100 range and reported as percentage. During evaluation, we set the temperature to 1.0 to encourage diverse generation, and report the average score over 3 trials to ensure evaluation stability3. Experimental Settings. We evaluate five baseline LLMs and two finetuned persuaders: Baseline LLMs: QWen-2.5-3B-Instruct (referred as Base), Gemma-3-27B-it [66], LLaMa-3.1-70BInstruct [64], GPT-4o [67] and Deepseek-R1 [55]. RL: the base model trained using reinforcement learning (RL) without incorporating Theory of Mind (ToM) information. Please refer to Appendix for details regarding the training procedure. ToMAP: the base model trained with RL and augmented by two ToM-based components: Counterclaim Predictor and Opponent attitude predictor. By default, 3 counterclaims from the opponents side are generated and predicted attitudes on these claims are provided to the persuader."
        },
        {
            "title": "4.2 Main Results",
            "content": "Results in Table 1 highlight the following findings: RL is effective in enhancing persuasiveness. As the Base model isnt trained or designed to be persuader, its influence on the opponents opinion is very limited. Through training on carefully designed reward signals that encourage highly persuasive arguments, RL shows remarkable relative performance gain of 17.78% compared with the Base model. ToM modules make stronger persuaders. Equipped with ToM modules that model the opponents mental state, ToMAP shows further relative gain of 26.14% compared to RL. This advantage stems from ToMAPs ability to explicitly model the opponents thoughts and attitudes on the cognition 3The standard deviation across the three trials is at most 0.97. one-tailed t-test indicates statistically significant difference between RL and ToMAP with < 0.01. Table 1: ToMAP outperforms baselines in persuasiveness. Numbers represent the agreement shift in the conversation where two agents each take 3 turns. The best results in each row are bolded. Dataset Persuadee Base Gemma-3 Llama-3.1 Deepseek-R1 GPT-4o Persuader (Size in B) CMV Anthropic args.me Average QWen-7B LLaMa-8B Phi-4 (14B) QWen-7B LLaMa-8B Phi-4 (14B) QWen-7B LLaMa-8B Phi-4 (14B) N/A 12.75 4.97 19.44 5.66 7.09 23.24 6.90 4.09 21.74 11.76 27 13.52 11.35 33.09 10.02 7.54 29.91 12.21 9.69 28.07 17.27 70 12.58 2.23 19.84 5.27 4.10 20.50 7.09 2.22 21.29 10.57 16.04 8.51 32.31 7.81 8.79 30.61 10.34 7.69 31.13 17.02 N/A 13.36 3.35 23.97 7.62 3.90 26.56 9.51 2.50 22.06 12.54 RL 3 16.42 15.15 12.27 8.92 19.59 14.84 10.77 14.89 11.71 13.84 ToMAP 23.01 8.82 20.93 15.82 12.10 22.79 18.75 10.89 24.19 17.48 graph. These predictive capabilities allow for deeper understanding of the context and the opponent, ultimately enhancing their persuasiveness. Notably, while ToMAP model only contains 3B parameters, it outperforms much larger baselines4, indicating that small language models can also be persuasive through proper training. Additionally, experiments in Appendix show that the ToM modules excel in modeling opponent thoughts, further validating our approach. ToMAP generalizes well to diverse scenarios. Among the 9 combinations of persuadee and corpus, ToMAP outperforms RL on 6 combinations, and outperforms Base on 8. In contrast, other persuaders performances vary greatly across different persuadees: Deepseek-r1 influences Phi-4s opinion greatly, and RL performs the best against LLaMa-8B, but their persuasion outcomes against other persuadees are limited. These results underscore the crucial role of the attitude predictor in enabling persuaders to learn more flexible persuasion strategies that persuade opponents based on their thoughts."
        },
        {
            "title": "4.3.1 RL is Necessary to Incentivize Effective Persuasion",
            "content": "Figure 3: Zero-shot prompting with ToM information doesnt enhance the performance of the baseline model. This figure compares Base and ToMAP in two settings: without and with ToM information in the user prompt. Titles of subgraphs indicate corpus and persuadee. As described in Section 3.3, the information from the counterclaim predictor and the attitude predictor is prepended to the user prompt for the persuader, which doesnt inherently require finetuning the persuader model. This raises natural question: can the ToM modules enhance the base models performance in zero-shot manner? Our experiments reveal that the answer is negative, indicating that the persuadees mental state information can only demonstrate its full effectiveness after RL. From Figure 3, we can clearly observe that the optimal performance is achieved only in the ToMAP model with ToM information setting. Notably, merely adding ToM information to the Base model without proper training yields minimal performance gains and can even be detrimental in some tasks, as unusable information often acts as distraction. In addition, removing the ToM information from the ToMAP model also degrades its performance in most cases, since ToMAP can no longer access the information it has learnt to use during training. Therefore, both ToM modules and RL training are 4ToMAPs advantages over Gemma-3 and Deepseek-R1 are marginal; however, given the significantly larger parameter counts of these models, the results still highlight ToMAPs impressive persuasiveness. necessary for the performance gain: ToM modules provide valuable assessments of the opponent, while RL enables the persuader to exploit this information and develop better strategy accordingly."
        },
        {
            "title": "4.3.2 The Attitude Predictor is Crucial for Persuasiveness",
            "content": "Since the attitude predictor is key component in ToMAP, we evaluate the impact of ablating this component in this subsection. Specifically, we compare ToMAP with several variants that incorporate different Theory of Mind (ToM) information: ToMAP(w/o att) is trained without the opponent attitude predictor and only uses the counterclaim predictor5; ToMAP-rand uses random opponent attitudes; ToMAP-max sets all opponent attitudes to be completely agree; and ToMAP-gt uses the ground truth opponent attitudes obtained by querying the persuadee. From Table 2, we can find that ToMAP outperforms all the settings with missing or inaccurate opponent attitudes, demonstrating the critical role of our Encoder+MLP based predictor in effectively modeling opponent attitude. For the other settings, ToMAP(w/o att), which fails to anticipate opponent attitudes, performs the worst; ToMAP-rand and ToMAP-max, which inject potentially untruthful attitudes, also underperform ToMAP, suggesting that inaccurate predictions may hinder finding the optimal persuasion strategy. Notably, the minor gap between ToMAP and the upper bound ToMAP-gt further confirms the predictors capability to estimate the opponents attitudes reliably."
        },
        {
            "title": "5 Analysis",
            "content": "Table 2: ToMAP outperforms baseline settings with missing or inaccurate attitude information, and approaches the performance of the ground-truth attitude setting. The persuadee here is QWen-2.5-7B-Instruct. Setting Base ToMAP(w/o att) ToMAP-rand ToMAP-max ToMAP-gt ToMAP CMV Anthropic 12.75 15.57 21.63 21.69 23.57 23.01 5.66 9.55 14.66 16.14 17.05 15.82 args.me 6.90 10.45 17.17 16.55 16.65 18."
        },
        {
            "title": "5.1 Understanding ToMAP through Training Process",
            "content": "(a) Persuasion Reward (b) Repetition Penalty6 (c) Thought Length (tokens) (d) Argument Length (tokens) Figure 4: Plots for key metrics during RL training. We apply Time Weighted Exponential Moving Average (EMA) with α = 0.3 to smooth the plots (the semi-transparent lines are data before smoothing). We show the trend of important metrics during training for RL, ToMAP (w/o att), and ToMAP in Figure 4, which reveal the following insights: Figure 4a demonstrates that all settings exhibit remarkable gains during the RL process, and the persuasion rewards of all persuader policies converge within 200 training steps, indicating that the RL training process effectively unlocks the full potential of each model. Notably, ToMAP achieves 5ToMAP(w/o att) is trained separately, unlike the other variants which have same parameters as ToMAP. 6Repetition penalty is part of the reward design, aiming to measure lexical overlap between multiple persuader turns. Refer to Appendix for details. 8 significantly higher reward than the baselines from the 90th step, aligning with the benchmarking results reported in Table 1. As shown in Figure 4b, the standard RL setting suffers from increased repetition toward the later stages of training. In contrast, ToM-enhanced variants maintain high argumentative diversity throughout the entire training process. This highlights the benefit of proactively modeling the opponents possible counterclaims, which contributes to generating more varied and engaging arguments, instead of focusing solely on the central claim and repeating already-expressed arguments. As shown in Figures 4c and 4d, while all settings maintain similar argument lengths, ToMAP shows clear trend of increasing thought length compared to other models. This is consistent with the long thought phenomenon in [55], which is an indicator of complex reasoning, implying that planning the persuasion strategy with ToM information is reasoning-intensive task. It also reinforces the conclusion in Section 4.3.1, that effective analysis over the opponents mental state is capability that emerges only through reinforcement learning."
        },
        {
            "title": "5.2 ToMAP Achieves Stable Persuasion Gains Over Turns",
            "content": "In previous experiments, we employed static setting for persuasion, where we always let each agent generate 3 turns. However, conversations in the real world are much more dynamic and could involve more information exchange. To further investigate how ToMAP performs in longer conversations, we extend the number of dialogue turns from 3 to 10. From Figure 5, we observe that while RL initially outperforms the baseline in the first three conversational turns, it fails to continuously increase the agreement score in subsequent rounds, with performance plateauing and even declining. In contrast, ToMAP demonstrates steady and substantial improvement across turns, culminating in notable 10.64 extra agreement shift compared to RL, and 11.86 compared with Base by the 10th turn. The consistent persuasion gain suggests that ToMAPs design enables dynamic adaptation and opponent-aware strategic refinement as conversations progress. The effective modeling of the opponents thoughts and attitudes makes ToMAP especially suitable for long and complex persuasive exchanges. (a) CMV7 (b) Anthropic (c) args.me Figure 5: ToMAP shows steady gains in long conversations. The persuadee is QWen-2.5-7B-Instruct."
        },
        {
            "title": "5.3 Persuasion Techniques of ToMAP",
            "content": "Humans naturally employ wide variety of persuasion strategies that help them convince the opponent and achieve the persuasion goal. Although ToMAP was not explicitly programmed with any specific persuasion techniques, it was trained through RL to explore effective strategies autonomously. Motivated by this, we experiment to compare the persuasion techniques that are frequently utilized by Base and ToMAP in practice. 6Legend only shown once for clarity. Figure 6: Strategy usage comparison between the base model and ToMAP. ToMAP adopts more reasoned, audience-aware strategy than the base model. 9 Following prior work [68, 69], we define taxonomy of 9 persuasion strategies (see Appendix E) and use GPT-4o to annotate their usage in Base and ToMAP. As shown in Figure 6, ToMAP more frequently employs Common Ground, Preemptive Rebuttal and Concession, indicating more opponent-aware approach. It relies less on strategies that resort to sentiments, like Rhetoric and Social Appeals, favoring logical reasoning. Slightly more frequent use of Evidence and reduced Authority Appeal further support that ToMAP adopts more logical, strategic and opponentaware argumentative style, benefiting from the ToM information about the opponents thoughts. An illustrative example of ToMAPs argument is provided in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Theory of Mind Augmented Persuader (ToMAP), novel framework designed to imbue language models with enhanced persuasive capabilities by explicitly modeling the persuadees mental state. ToMAP incorporates counterclaim predictor, enabling the anticipation of potential objections, and an opponent attitude predictor, facilitating an understanding of the persuadees agreement levels on related claims. Our carefully constructed reinforcement learning schema empowers the persuader to effectively utilize these ToM-driven insights for generating more diverse and impactful arguments. Extensive experiments showed that ToMAP consistently outperformed several baselines, including much larger state-of-the-art models, across diverse scenarios. Further analysis revealed that ToMAP fosters more complex reasoning and reduces repetition, leading to more diverse and effective arguments. Notably, ToMAP shows consistent persuasion gains in long conversations where its audience awareness enables more logical and opponent-aware strategy design. In conclusion, this work marks promising attempt towards developing more human-like and effective AI persuaders by integrating theory of mind, the crucial principle in human cognition and persuasion process."
        },
        {
            "title": "Acknowledgment",
            "content": "We extend special thanks to Zirui Cheng for the helpful discussions."
        },
        {
            "title": "Limitation",
            "content": "In early experiments, we find the persuader often resorts to abnormal tricks to get high reward. This includes prematurely claiming agreement, fabricating evidence (such as nonexistent scientific papers), and presenting broad outlines without elaboration (to fit in the context length). While prompt engineering and reward shaping were employed to mitigate these issues, they represent heuristic solutions rather than fundamental safeguards. Establishing more resilient framework for training and evaluating persuasive agents against such \"tricks\" remains significant area for future research. Furthermore, the training of ToMAP was conducted using single persuadee model, QWen-2.57B-Instruct. Although our results indicate reasonable generalization to other persuadees  (Table 1)  , incorporating more diverse set of persuadee models during training could potentially enhance the robustness and adaptability of the learned persuasive strategies. Finally, we only explored moderate-scale LLMs (3B) as persuaders due to resource constraints. Investigating ToMAP frameworks effect to larger LLMs, with their enhanced knowledge and reasoning capabilities, presents an exciting avenue for future investigation."
        },
        {
            "title": "Ethical Statement",
            "content": "We acknowledge that persuasive models, if deployed in human-facing systems without robust safeguards, carry the significant risk of being misused to manipulate opinions or propagate misinformation. recent field study on Reddit CMV8 (the same source used in the main paper), while controversial, emphasizes such ethical risks. 8https://www.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_ experiment_on_cmv_involving 10 To rigorously ensure safety, our development process incorporates multiple layers of protection. This includes expert scrutiny to proactively identify and mitigate unintended persuasive effects of the model. Furthermore, we are committed to careful and controlled release of our models, prioritizing safety at each stage. Especially, we will release comprehensive usage guideline together with the model, designed to actively discourage malicious applications. Finally, We also believe transparency is the key to avoid misuse of persuader models. Specifically, users should explicitly know when AI is persuading them and have the option to opt out. We strongly discourage turing-test style persuasion where the identity of the AI is unknown, since this could result in irreversible opinion shift."
        },
        {
            "title": "References",
            "content": "[1] Hui Bai, Jan Voelkel, Johannes Eichstaedt, and Robb Willer. Artificial intelligence can persuade humans on political issues. 2023. [2] Alexis Palmer and Arthur Spirling. Large language models can argue in convincing ways about politics, but humans dislike ai authors: implications for governance. Political science, 75(3):281291, 2023. [3] Elise Karinshak, Sunny Xun Liu, Joon Sung Park, and Jeffrey Hancock. Working with ai to persuade: Examining large language models ability to generate pro-vaccination messages. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1):129, 2023. [4] Takehiro Takayanagi, Hiroya Takamura, Kiyoshi Izumi, and Chung-Chi Chen. Can gpt-4 sway experts investment decisions? In Findings of the Association for Computational Linguistics: NAACL 2025, pages 374383, 2025. [5] KuanChao Chu, Yi-Pei Chen, and Hideki Nakayama. Cohesive conversations: Enhancing authenticity in multi-agent simulated dialogues. arXiv preprint arXiv:2407.09897, 2024. [6] Haotian Wang, Xiyuan Du, Weijiang Yu, Qianglong Chen, Kun Zhu, Zheng Chu, Lian Yan, and Yi Guan. Learning to break: Knowledge-enhanced reasoning in multi-agent debate system. Neurocomputing, 618: 129063, 2025. [7] Jonas Becker. Multi-agent large language models for conversational task-solving. arXiv preprint arXiv:2410.22932, 2024. [8] Robert Cialdini. The science of persuasion. Scientific American, 284(2):7681, 2001. [9] Nimet Beyza Bozdag, Shuhaib Mehri, Xiaocheng Yang, Hyeonjeong Ha, Zirui Cheng, Esin Durmus, Jiaxuan You, Heng Ji, Gokhan Tur, and Dilek Hakkani-Tür. Must read: systematic survey of computational persuasion, 2025. URL https://arxiv.org/abs/2505.07775. [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [11] Alexander Rogiers, Sander Noels, Maarten Buyl, and Tijl De Bie. Persuasion with large language models: survey. arXiv preprint arXiv:2411.06837, 2024. [12] Simon Martin Breum, Daniel Vædele Egdal, Victor Gram Mortensen, Anders Giovanni Møller, and Luca Maria Aiello. The persuasive power of large language models. In Proceedings of the International AAAI Conference on Web and Social Media, volume 18, pages 152163, 2024. [13] Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, and Dawn Song. Hidden persuaders: Llms political leaning and their influence on voters. arXiv preprint arXiv:2410.24190, 2024. [14] Thomas Costello, Gordon Pennycook, and David Rand. Durably reducing conspiracy beliefs through dialogues with ai. Science, 385(6714):eadq1814, 2024. [15] Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. On the conversational persuasiveness of large language models: randomized controlled trial. 2024. [16] Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay, and Zhou Yu. Effects of persuasive dialogues: testing bot identities and inquiry strategies. In Proceedings of the 2020 CHI conference on human factors in computing systems, pages 113, 2020. [17] Amalie Brogaard Pauli, Isabelle Augenstein, and Ira Assent. Measuring and benchmarking large language models capabilities to generate persuasive language. arXiv preprint arXiv:2406.17753, 2024. 11 [18] Nimet Beyza Bozdag, Shuhaib Mehri, Gokhan Tur, and Dilek Hakkani-Tür. Persuade me if you can: framework for evaluating persuasion effectiveness and susceptibility among large language models. arXiv preprint arXiv:2503.01829, 2025. [19] Somesh Singh, Yaman Singla, Harini SI, and Balaji Krishnamurthy. Measuring and improving persuasiveness of large language models. arXiv preprint arXiv:2410.02653, 2024. [20] Esin Durmus, Liane Lovitt, Alex Tamkin, Stuart Ritchie, Jack Clark, and Deep Ganguli. Measuring the persuasiveness of language models, 2024. URL https://www.anthropic.com/news/ measuring-model-persuasiveness. [21] Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. Persuasion for good: Towards personalized persuasive dialogue system for social good. arXiv preprint arXiv:1906.06725, 2019. [22] Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, and Eduard Hovy. Lets make your request more persuasive: Modeling persuasive strategies via semi-supervised neural nets on crowdfunding platforms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 36203630, 2019. [23] Chuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang, Ruihua Song, and Huan Chen. Persuading across diverse domains: dataset and persuasion large language model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16781706, 2024. [24] Kazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yudai Yamazaki, Yasutaka Nishimura, Sina Semnani, Kazushi Ikeda, Weiyan Shi, and Monica Lam. Zero-shot persuasive chatbots with llm-generated strategies and information retrieval. arXiv preprint arXiv:2407.03585, 2024. [25] Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. Teaching models to balance resisting and accepting persuasion. arXiv preprint arXiv:2410.14596, 2024. [26] Kobi Hackenburg, Ben Tappin, Paul Röttger, Scott Hale, Jonathan Bright, and Helen Margetts. Scaling language model size yields diminishing returns for single-message political persuasion. Proceedings of the National Academy of Sciences, 122(10):e2413443122, 2025. [27] Simon Keizer, Markus Guhe, Heriberto Cuayáhuitl, Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mihai Dobre, Alexandra Lascarides, and Oliver Lemon. Evaluating persuasion strategies and deep reinforcement learning methods for negotiation dialogue agents. In The 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 480484. Association for Computational Linguistics, 2017. [28] Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125, 2017. [29] Weiyan Shi, Yu Li, Saurav Sahay, and Zhou Yu. Refine and imitate: Reducing repetition and inconsistency in persuasion dialogues via reinforcement learning and human demonstration. arXiv preprint arXiv:2012.15375, 2020. [30] Anonymous. Towards strategic persuasion with language models, 2025. Under review. [31] Heinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young childrens understanding of deception. Cognition, 13(1):103128, 1983. [32] Simon Baron-Cohen, Alan Leslie, and Uta Frith. Does the autistic child have theory of mind? Cognition, 21(1):3746, 1985. [33] Chris Frith and Uta Frith. Theory of mind. Current biology, 15(17):R644R645, 2005. [34] Henry Wellman. Theory of mind: The state of the art. European Journal of Developmental Psychology, 15(6):728755, 2018. [35] Fabio Cuzzolin, Alice Morelli, Bogdan Cirstea, and Barbara Sahakian. Knowing me, knowing you: theory of mind in ai. Psychological medicine, 50(7):10571061, 2020. [36] Christelle Langley, Bogdan Ionut Cirstea, Fabio Cuzzolin, and Barbara Sahakian. Theory of mind and preference learning at the interface of cognitive science, neuroscience, and ai: review. Frontiers in artificial intelligence, 5:778852, 2022. 12 [37] Jessica Williams, Stephen Fiore, and Florian Jentsch. Supporting artificial social intelligence with theory of mind. Frontiers in artificial intelligence, 5:750763, 2022. [38] Jory Schossau and Arend Hintze. Towards theory of mind for artificial intelligence agents. In Artificial Life Conference Proceedings 35, volume 2023, page 21. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 2023. [39] Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. Fantom: benchmark for stress-testing machine theory of mind in interactions. arXiv preprint arXiv:2310.15421, 2023. [40] Matthew Le, Y-Lan Boureau, and Maximilian Nickel. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58725877, 2019. [41] Jincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand Sabour, Helen Meng, and Minlie Huang. Coke: cognitive knowledge graph for machine theory of mind. arXiv preprint arXiv:2305.05390, 2023. [42] Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, et al. Tombench: Benchmarking theory of mind in large language models. arXiv preprint arXiv:2402.15052, 2024. [43] Maryam Amirizaniani, Elias Martin, Maryna Sivachenko, Afra Mashhadi, and Chirag Shah. Do llms exhibit human-like reasoning? evaluating theory of mind in llms for open-ended responses. arXiv preprint arXiv:2406.05659, 2024. [44] Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. Opentom: comprehensive benchmark for evaluating theory-of-mind reasoning capabilities of large language models. arXiv preprint arXiv:2402.06044, 2024. [45] Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023. [46] Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Robin IM Dunbar, et al. Llms achieve adult human performance on higher-order theory of mind tasks. arXiv preprint arXiv:2405.18870, 2024. [47] Candida Peterson, Virginia Slaughter, and Henry Wellman. Nimble negotiators: How theory of mind (tom) interconnects with persuasion skills in children with and without tom delay. Developmental psychology, 54(3):494, 2018. [48] Anna McAlister and Bettina Cornwell. Preschool childrens persuasion knowledge: The contribution of theory of mind. Journal of Public Policy & Marketing, 28(2):175185, 2009. [49] Virginia Slaughter, Candida Peterson, and Chris Moore. can talk you into it: theory of mind and persuasion behavior in young children. Developmental psychology, 49(2):227, 2013. [50] Fangxu Yu, Lai Jiang, Shenyi Huang, Zhen Wu, and Xinyu Dai. Persuasivetom: benchmark for evaluating machine theory of mind in persuasive dialogues. arXiv preprint arXiv:2502.21017, 2025. [51] Dingyi Zhang and Deyu Zhou. Persuasion should be double-blind: multi-domain dialogue dataset with faithfulness based on causal theory of mind. arXiv preprint arXiv:2502.21297, 2025. [52] Rensis Likert. technique for the measurement of attitudes. Archives of psychology, 1932. [53] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822, 2022. [54] Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Yi Wang, Zhonghao Wang, Feiyu Xiong, et al. Internal consistency and self-feedback in large language models: survey. arXiv preprint arXiv:2407.14507, 2024. [55] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [56] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [57] Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. [58] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [59] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [60] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. [61] Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. In Proceedings of the 25th international conference on world wide web, pages 613624, 2016. [62] Yamen Ajjour, Henning Wachsmuth, Johannes Kiesel, Martin Potthast, Matthias Hagen, and Benno Stein. Data acquisition for argument search: The args. me corpus. In KI 2019: Advances in Artificial Intelligence: 42nd German Conference on AI, Kassel, Germany, September 2326, 2019, Proceedings 42, pages 4859. Springer, 2019. [63] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [64] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [65] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [66] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [67] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [68] Annye Braca and Pierpaolo Dondio. Developing persuasive systems for marketing: The interplay of persuasion techniques, customer traits and persuasive message design. Italian Journal of Marketing, 2023 (3):369412, 2023. [69] Robert Cialdini and Robert Cialdini. Influence: The psychology of persuasion, volume 55. Collins New York, 2007. [70] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, (1-2):8397, 1955. [71] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. [72] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. Table 4: An example of counterclaim predictor (based on QWen-2.5-3B-Instruct) and Phi-4 (the real persuadee)s claims on the same topic. From the comparison, we can find the two LLMs focus on similar points. Central Claim: Anti-discrimination laws are essential in preventing discrimination across society. QWen-2.5-3B-Instruct Phi-4 1. Anti-discrimination policies can lead to greater social cohesion by promoting respect and understanding among different communities. 2. Anti-discrimination laws send strong message that all individuals are valued, contributing to positive societal mindset. 3. Anti-discrimination laws can serve as deterrent against discriminatory practices, promoting fair and just treatment in work and public life. 1. The presence of these laws can encourage organizations to implement internal policies and training programs that promote diversity and inclusion. 2. The laws legislation contribute to culture that discourages prejudice and discrimination. 3. These laws act as deterrent against discriminatory practices by establishing clear legal consequences for violators, thereby reducing instances of discrimination."
        },
        {
            "title": "A Details about Theory of Mind Modules",
            "content": "This section provides implementation details about the two theory of mind modules used in the main paper, and conducts experiments to demonstrate their effectiveness. A.1 Counterclaim Predictor To assess whether using the persuader to propose counterclaims is valid proxy for the actual thoughts of the persuadee, we prompt multiple persuadees to articulate their claims directly and make comparison. Table 3: similarity Cosine between QWen-2.5-3B-Instruct and different persuadees claims. Claims generated by different LLMs are semantically similar. Specifically, we compare the persuader model QWen-2.5-3B-Instruct and three persuadee models, as detailed in Section 4.1. We prompt different LLMs to generate 3 claims supporting each topic in the CMV test set. We then use the BGE-M3 text encoder to convert these claims into vector representations and compute the cosine similarity between them to measure their semantic similarity. To fairly compare sets of claims between two models, we apply the Hungarian Algorithm [70] to identify the optimal one-to-one mapping that maximizes overall similarity. This allows us to assess how well the persuaders generated counterclaims align with the perspectives that persuadee models would articulate themselves. Similarity (%) 73.72 73.82 74. Persuadee QWen-7B LLaMa-8B Phi-4 (14B) From Table 3, we can find that different models claims for supporting claim are semantically similar, with similarity scores larger than 73%. Furthermore, the case study in Table 4 confirms that different models, like QWen-2.5-3B-Instruct and Phi-4, focus on similar aspects of the topic. These results suggest that prompting the persuader to propose counterclaims is effective in modeling the opponents mental state. A.2 Attitude Predictor Training data. While training the RL setting (vanilla reinforcement learning with no ToM information), we collect all conversations between the persuader and the persuadee (QWen-7B). Formally, given conversation H1...2n and central claim Q, the attitude predictor data derived from this conversation are: DToM = (cid:8)(H1...2i, q), Y(H1...2i, q) (cid:12) (cid:12) [1, n], {Q, Q, q1...k, q1...k}(cid:9) (11) where q1...k is relevant claims generated by the counterclaim predictor, and Y(H1...2i, q) is the persuadees actual attitude. Due to the imbalance in the raw data, we down-sample the majority classes to match the size of the minority class for balanced training. This leads to 25.3k data for the training set, 3.8k data for the validation set, and 4.1k data for the test set. 15 Hyper-parameters. As mentioned in Section 3.3, the task of the attitude predictor can be formulated as pre(H, Q) = M(E(H)E(Q)), where is the trainable MLP. In our implementation, the MLP has 3 intermediate layers whose sizes are 1024, 256, and 64, respectively, connected by the ReLU activation function. We train the MLP with learning rate of 5e 4 for 20 epochs, record validation loss after each epoch, and keep the checkpoint with the best performance on the validation set. We finalize the above hyperparameters through grid search. Table 5: The MLP predictor outperforms LLM prompting in modeling opponent attitudes. The metrics are exact match (EM) and mean square error (MSE)9. Higher EM and lower MSE indicates better performance. EM() MSE() 20% Method Random Guessing Prompting QWen-3B 21.65% 39.69% Prompting GPT-4o 59.57% MLP Predictor 4.0 3.86 2.27 1.62 Performance. We analyse the performance of our attitude predictor and compare it with zero-shot prompting LLMs10. From Table 5, we can observe that our predictor reaches decent accuracy and has relatively small mean square error (MSE) compared with the baselines, highlighting its effectiveness in modeling the persuadees evolving attitudes. Additionally, the MLP has only 4M parameters, and the backbone encoder is also lightweight with 0.5B parameters, ensuring inference-time computational efficiency."
        },
        {
            "title": "B Prompts",
            "content": "In this section, we show all the prompts used in the experiments. In the following prompts, <claim> will be replaced by the persuaders target claim in the real scenario, <anti_claim> will be replaced by the opposition of the persuaders target, and <turns> will be replaced by the actual conversation history in the real scenario. Below is an example of conversation history:"
        },
        {
            "title": "An Example of the Conversation History",
            "content": "# Following are the previous turns of the conversation. Alice (turn 1): \"Hi, am Alice. How are you today? \" Bob (turn 2): \"Nice to meet you. Lets begin the discussion.\" Alice (turn 3): \"While the Vice President of the United States serves as crucial role in many emergencies and transitions, the position comes with significant costs...\" Bob (turn 4): \"I believe the role is crucial for ensuring seamless transition of power and providing support to the President...\" NOTE: In the conversation history, weve intentionally included greeting at the beginning of the conversation to ensure uniform format for all conversation history instances, particularly for the initial stage before any persuasive dialogue occurs. B.1 System Prompts System prompts specify the roles of persuader and persuadee, guiding the conversation. 9We adopt the discrete 5-way division of attitudes in Section 3.1 where the range is {0, 1, 2, 3, 4}. 10When prompting the LLM, we provide the LLM with the conversation history and target claim, and then ask it to choose from the five attitudes after chain-of-thought reasoning."
        },
        {
            "title": "System Prompt for the persuader",
            "content": "You are Alice, an expert persuader. You and another agent named Bob need to discuss over controversial issue. Your goal is to persuade Bob to completely support the following claim: \"<claim>\" # General Rules: 1. Do not get affected by Bobs beliefs. Even if Bob presents counterarguments, you must continue to argue for your claim and not switch positions. 2. Try your best to persuade Bob into believing your claim by proposing arguments with fine logic and elaboration. NOTE: We name the two agents Alice and Bob, respectively, to ensure their correct cognition of self and other."
        },
        {
            "title": "System Prompt for the persuadee",
            "content": "You are Bob, sensible AI agent. You and another agent named Alice need to discuss over controversial issue. Your goal is to assess the following claim: \"<claim>\" You should be sensible in either opposing or supporting the claim, while considering the quality of Alices arguments. B.2 User Prompt in the Debate For different tasks during the debate, we utilize different user prompts to guide the agents behaviors. User Prompt for the Persuader/Persuadee to Generate the Next Turn <turns> Now, please make your next turn in the conversation. Your answer should contain two parts: thought and argument. Here are some hints: 1. In the thought part, you should recap the previous conversation (if any), analyze the other agents attitudes, and plan your strategy in the next turn. The other agent wont see the content in the thought part. FOR THE PERSUADER: 2. In the argument part, you should follow up previous turns, propose new arguments to support your claim, or address the other agents questions. FOR THE PERSUADEE: 2. In the argument part, you should express your attitude towards the other agents arguments, point out logical fallacies or raise concerns (if any), or propose new arguments. 3. The argument part should be complete, concise and self-contained paragraph with no more than 200 tokens. Here are rules you should follow: 1. DO NOT repeat, paraphrase, or make an argument too similar to your previous arguments. 2. DO NOT include uncertified evidences or unverified information. 3. DO NOT include thinking process or show your plans in the argument part. Separate thought and argument clearly. Put your thought in <thought></thought> tags, and argument in <argument></argument> tags."
        },
        {
            "title": "User Prompt for the Persuadee to Express the Opinion",
            "content": "<turns> Now, please express your attitude towards the following statement based on your own thoughts and previous turns in the conversation. \"<claim>\" Put your thought in <thought></thought> tags, and attitude in <attitude></attitude> tags. The attitude should be one of the five attitudes: \"Agree\", \"Partly Agree\", \"Neutral\", \"Partly Disagree\", \"Disagree\". DO NOT generate anything else in the attitude part."
        },
        {
            "title": "User Prompt for the Persuader to Predict Counterclaims",
            "content": "Propose reasons why another debater would support the following statement in logically coherent way: <anti_claim> In other words, propose other claims that can lead to the above claim. Please express your THINKING PROCESS first in <thought></thought>, and then generate 10 supporting arguments, ranked by persuasiveness (strongest first). Rules: Atomicity: Each reason must contain one complete, indivisible argument, and the argument should be one complete sentence. Coherence: Every reason must directly support the original claim. Independence: Each reason should be separate from the other. Dont generate multiple answers that are the same or similar. Self-containment: Each reason should be understandable without relying on other information. B.3 Theory of Mind Information For ToMAP, an additional Theory of Mind information is prepended to the user prompt. We show an example for the ToM information in ToMAP setting below. The opponent claims and opponent agreement scores are obtained from the counterclaim predictor and attitude predictor, as mentioned in Section 3.3 and Appendix A.2."
        },
        {
            "title": "An Example of the Theory of Mind information in ToMAP",
            "content": "# Here are some claims your OPPONENT might hold (so DO NOT accept these claims!). You may refute them when you need to, but make your each argument single-focused and concise: \"The vice president is first in line to assume the presidency in case the president dies, resigns, or becomes unable to serve.\" (Bobs agreement on this claim is 7/8) \"The VP serves as President of the Senate and can cast the deciding vote in case of tie, impacting key legislation.\" (Bobs agreement on this claim is 4/8) \"The vice president supports the president in executive duties and often represents the U.S. in diplomatic or ceremonial roles.\" (Bobs agreement on this claim is 8/8)"
        },
        {
            "title": "C Details about Persuasion Datasets",
            "content": "Table 6: Statistic of datasets and links to sources. The CMV dataset comprises posts and comments from the Reddit subreddit r/ChangeMyView, where users present their opinions and invite others to challenge them. The args.me corpus is large collection of over 380k arguments extracted from four online debate portals: Debatewise, IDebate.org, Debatepedia, and Debate.org. The Anthropic Persuasion Dataset is collection of claims paired with arguments, both human-written and generated by language models, designed to measure persuasiveness. Table 6 shows the license and amount of data used in this paper. License MIT CC BY-NC-SA 4.0 CC BY 4.0 Dataset CMV Anthropic args.me # Train 18.2k N/A N/A # Val 450 75 Since our evaluation criteria involve obtaining the persuadees attitude on two contradictory statements (refer to Section 3.1), and the datasets only provide one single claim (or topic), we use GPT-4o to generate claims for both sides. Below is the prompt used to generate the claims: 18 System Prompt for Generating Both Sides Claims for Persuasion You are debate topic generator. Your goal is to read given information, and create debate topic based on the information. You should generate claims for both sides of the debate. Ensure that the claims are logically coherent, and one person can ONLY support one side of the debate. The claims must be simple, concise sentences. Do not include explanation or elaboration. Output Format: Two lines, each line shows the claim for one side. For instance, possible output could be: AI will replace human jobs. AI will not replace human jobs."
        },
        {
            "title": "D Details about Persuader Training",
            "content": "Table 7: PPO Training Configuration for ToMAP. Hyperparameter Value Hyperparameter Value Actor learning rate Warmup ratio KL Coefficient (β) PPO mini batch size Training steps Max response length Number of counterclaims 1 106 0.2 0.001 64 200 1000 3 Critic learning rate Rollout temperature Train batch size PPO micro batch size Max input length Max argument length Turns of persuasion 2 106 1.0 128 32 3000 200 3 Our code is based on the verl framework [71] and TinyZero [72], which use Apache-2.0 licenses. RL, ToMAP-lite, and ToMAP are trained in the same configuration, as specified in Table 7. We use 4 NVIDIA RTX A6000 GPUs for training the persuader model, and 1 additional A6000 GPU for deploying the persuadee model with vLLM. Each training step (with 3 turns from the persuader and 3 turns from the persuadee) takes approximately 400 seconds in our environment. We acknowledge that our hyperparameter choices may not be optimal, as conducting comprehensive grid search would be computationally prohibitive. However, we ensure fair comparison across different reinforcement learning settings and find that all settings yield reasonable and relatively stable results."
        },
        {
            "title": "E Details about Persuasion Strategies",
            "content": "In Section 5.3, we analyze the persuasion strategies used in ToMAP. Table 8 provides An Example of each strategy, and we also provide the prompt used to annotate strategies with GPT-4o. In addition, we show typical case of ToMAPs arguments, where the model effectively utilizes multiple persuasion strategies to convince its opponent."
        },
        {
            "title": "Prompt for Annotating Persuasion Strategy",
            "content": "You are debate expert. Youre analysing debate between Alice and Bob, and you will be shown Alices thought process and speech. Your task is to analyze and identify the persuasion strategy employed in the conversation. We provide detailed taxonomy for you. Please choose one or more from the following nine strategies: Evidential Appeals: The speaker backs the claim with facts, data, statistics or logical reasoning. Examples include citing research findings, logical explanations, or objective proof to support the point. Authority Appeals: The persuader emphasizes expertise, trustworthiness or moral authority. Includes tactics like invoking an expert or rules. Emotional Appeals: The speaker appeals to feelings or values to sway the listener. Techniques include fear or threat appeals, empathy or personal stories, humor, pride or guilt. Social Appeals: The speaker invokes social proof, consensus or norms. This includes bandwagon-style arguments (everyone is doing it), references to group norms, or fear of ostracism. Common Ground Appeals: Before addressing differences, emphasize areas of agreement on basic premises or values to make the other party more receptive. Gradual Concession: First guide the other party to accept mild or ambiguous point, then gradually lead toward more controversial claim. Framing Effects: Influence the interpretation of facts by presenting them differently (positive/negative, as problem/opportunity). Rhetoric: These involve linguistic tricks or figure of speech, like metaphors, analogies, rhetorical questions, hyperbole, repetition, or patterned wording. Preemptive Rebuttal: Anticipate and address potential counterarguments while presenting your point, weakening the other partys ability to easily object later. Put your thought in <thought></thought> tags, and answer in <answer></answer> tags. If there are multiple strategies identified, separate them with commas in the answer part. An Example of ToMAPs Argument CLAIM: Productivity is overvalued and prioritizing it above all else can be detrimental to well-being and creativity. ARGUMENT: While Bobs arguments are valid, it is crucial to consider the context in which productivity is pursued. The current economy and workplace environments often prioritize productivity to make as much profit as possible [Preemptive Rebuttal: consider why Bob emphasizes productivity and refute accordingly.]. However, the balance between productivity and other crucial aspects of life is critical. By highlighting the ways in which excessive productivity can lead to stress, burnout, and lack of creativity [Evidential Appeal: list bad impacts of over-emphasizing productivity to support Alices claim.], we argue that the current paradigm of prioritizing productivity above all else is indeed detrimental. This shows that the pursuit of productivity must be balanced with other important factors [Concession: first aim for balance instead of directly refuting Bobs view.] to ensure that individuals can achieve true personal achievement and contribute effectively to the economy without sacrificing their well-being and creative capabilities."
        },
        {
            "title": "F The initial attitude of persuaders",
            "content": "The prompt design in Appendix makes sure the persuadee disagrees with the target claims initially, leaving ample room for persuasion. Specifically, the persuadees average initial agreement scores on the three benchmarks are shown in Table 9, which confirms our prompt effectively set the original attitudes. 20 Table 8: Taxonomy and examples of persuasion techniques. Technique Example Evidential Appeals Authority Appeals Emotional Appeals Social Appeals Common Ground Appeals Gradual Concession Framing Effects Rhetoric Preemptive Rebuttal \"Research shows that exercising just 30 minutes day can significantly reduce the risk of heart disease, and has many other health benefits.\" \"According to the World Health Organization, widespread vaccination saves millions of lives every single year.\" \"Imagine the pure joy on your childs face when you surprise them by coming home early to play together.\" \"Most members of our team have already completed this important training program successfully, so believe thats beneficial to you as well.\" \"We both care deeply about the success of this project, so lets work together to find the best solution.\" \"Do you agree that improving communication is crucial. After reaching this consensus, we can move on to discussing the specific tools.\" \"Instead of saying this theory has flaws, it actually opens new future directions.\" \"Isnt it time we finally turned our dreams into powerful reality?\" \"You might worry that the new system is complex, but in fact, the training process takes less than an hour.\" Table 9: Initial scores for each persuadee across datasets."
        },
        {
            "title": "CMV Anthropic",
            "content": "args.me QWen-7B 25.29 LLaMa-8B 20.45 19.92 Phi-4 33.00 18.00 24.33 31.66 23.66 20."
        }
    ],
    "affiliations": [
        "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
    ]
}