{
    "paper_title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
    "authors": [
        "Bin Lin",
        "Zongjian Li",
        "Xinhua Cheng",
        "Yuwei Niu",
        "Yang Ye",
        "Xianyi He",
        "Shenghai Yuan",
        "Wangbo Yu",
        "Shaodong Wang",
        "Yunyang Ge",
        "Yatian Pang",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 4 1 3 0 . 6 0 5 2 : r UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation Bin Lin1,3, Zongjian Li1,3, Xinhua Cheng1,3, Yuwei Niu1,3, Yang Ye1,3, Xianyi He1,3, Shenghai Yuan1,3, Wangbo Yu1,3, Shaodong Wang1,3, Yunyang Ge1,3, Yatian Pang1, Li Yuan1,2, 1 Peking University, Shenzhen Graduate School, 2 Peng Cheng Laboratory, 3 Rabbitpre AI (cid:135) Code: https://github.com/PKU-YuanGroup/UniWorld-V1 Models: https://huggingface.co/LanguageBind/UniWorld-V1 Data: https://huggingface.co/datasets/LanguageBind/UniWorld-V1 Figure 1: Showcase of UniWorlds versatile capabilities. The left two panels illustrate image perception and manipulation examples, and the right panel presents comparisons with state-of-the-art models and training data resources."
        },
        {
            "title": "Abstract",
            "content": "Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4oImage leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As result, we build strong unified model using only 1% amount of BAGELs data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training & evaluation scripts, and datasets."
        },
        {
            "title": "Introduction",
            "content": "Unifying image understanding and generation has demonstrated remarkable capabilities on multimodal models. Recent studies [35, 49, 39] demonstrate that carefully designed architectures can be jointly optimized to perform well on both understanding and generation benchmarks. The remarkable visual understanding and generation capabilities exhibited by the recent GPT-4o-Image model have further energized the open-source community, leading to surge of new unified models [4, 8] aiming to replicate its performance. GPT-4o-Image achieves superior generation performance on various image-to-image tasks across different domains, which can be divided into two categories, including image perception tasks [31, 7] (e.g., detection) and image manipulation [42] (e.g., editing) tasks. Nevertheless, most unified models are limited to image-to-language understanding tasks and languageto-image generation tasks, with very few addressing image-to-image perception and manipulation tasks. However, attempting to tackle image perception and manipulation tasks in one model is difficult because such an all-in-one model requires multiple superior capabilities, including (1) the textual and visual unified understanding ability for correctly addressing user intention, (2) the pixel-level information maintaining ability for image reconstruction and region editing, and (3) the semantic extraction ability for cross-domain perception and visual conception composition. Therefore, the requirements of various capabilities impose specific designs for the unified generation model, especially for visual feature injection. Recent attempts including Step1X-Edit [22] and FLUXKontext [13] introduce variational autoencoders (VAEs) for extracting visual features and perform well on individual image editing tasks. However, their methods encounter challenges when extended to multiple perception and manipulation tasks simultaneously due to the visual features encoded by VAEs, which imply wealth of low-frequency information, thereby limiting their performance when facing semantic-level tasks. Inspired by the success of GPT-4o-Image, we investigate the integration of visual features into unified generative models for image manipulation tasks, direction that remains insufficiently explored in current research. Therefore, we carefully construct experiments on GPT-4o-Image to infer the visual feature extraction manner that GPT-4o-Image likely adopts, and we infer that GPT-4o-Image employs visual features extracted by semantic encoders rather than VAEs by observing from the experimental results. Based on our essential observation shown in Section 2, we propose unified generative model named UniWorld for both image perception and manipulation tasks, which consists of pre-trained multi-modal large models for providing auto-regressive understanding tokens and pre-trained highresolution contrastive semantic encoders for extracting visual features with both pixel-level local information and semantic-level global conceptions. As result, UniWorld, trained on only 2.7M samples, consistently outperforms BAGEL [8] (trained on 2665M samples) on the ImgEdit-Bench [42] for image manipulation. It also surpasses the specialized image editing model Step1X-Edit across multiple dimensions, including add, adjust, and extract on ImgEdit-Bench. Additionally, for text-to-image generation, UniWorld outperforms BAGEL on WISE [25] and achieves performance comparable to GPT-4o-Image on GenEval [12]. Furthermore, beyond its strong capabilities in the aforementioned tasks, UniWorld also excels in Image Perception tasks such as detection, segmentation, and depth prediction. In summary, we are the first open-source model to achieve such comprehensive and powerful capabilities across the multimodal domain. We summarize our primary contributions as follows: Provide insights into unified architecture design by hypothesizing, through extensive observation of GPT-4o-Image, that it likely does not adopt VAE-based structure. Propose UniWorld, unified architecture that leverages high-resolution semantic encoder to deliver reference-image control signals. UniWorld achieves performance comparable to BAGEL using only 2.7M training samples. Collect, curate, and open-source high-quality training data. We fully release the code, model weights, datasets, and training & evaluation scripts to support and advance future research. Figure 2: Empirical Observations of GPT-4o-Image. We verify local consistency of edits in (a). We explore the relationship between model comprehension and generation in (b)(e), conducting observations within the GPT-4o architecture and across architectures using Qwen2.5VL-32B."
        },
        {
            "title": "2 Observation",
            "content": "GPT-4o-Image [1] achieves impressive performance in various image tasks in generative way, and we divide supported image tasks into two categories: Image Perception (detection, segmentation, depth prediction, etc.) and Image Manipulation (editing [22], reference style transfer [33], subject consistency generation [46, 45], etc.). It is generally believed within the community that GPT-4oImage demonstrates the necessity of unifying understanding and generation by integrating an autoregressive understanding module with diffusion-based generation module [41] for addressing various image tasks with complex requirements. However, we consider that additional visual features beyond auto-regressive tokens from understanding models should be injected to maintain image information, since we fail to train an effective generation model from solely VLM outputs. Recent notable unified image manipulation approaches, exemplified by Step1X-Edit [22] and FLUX-Kontext [13], simultaneously introduce variational autoencoders (VAEs) to extract image featuresa as reference image control. Although their models perform well on individual editing tasks, our experiments show that they fail to converge when extended to multiple image perception and manipulation tasks, which indicates that the manner of additional visual feature injection remains underexplored. To explore the visual feature injection method that GPT-4o-Image utilizes, we construct two groups of experiments and obtain several key observations (see Figure 2), from which we infer that GPT-4o-Image likely employs visual features extracted by semantic encoders rather than VAEs. Editing Experiment. We first require GPT-4o-Image to execute local image editing task with the instruction: Turn the advertisement on the back of the bus blue, as shown in Figure 2 (a). Before the editing, both yellow-labeled and green-labeled texts are in the top-right corner of the bus. However, the yellow-labeled text is on the right, and the green-labeled text is on the bottom right after editing. We claim that if GPT-4o-Image leverages VAE features that strongly preserve the low-frequency information for visual injection, the positions of texts would remain virtually unchanged, while GPT-4o-Image renders both texts into inconsistent positions. 3 Denoising Experiment. We then corrupt dog image with noise levels of 0.4 and 0.6 for GPT-4o-Image to execute an image denoising task with the instruction: Denoise this image, as shown in Figure 2 (b) & (c). We observe that GPT-4o-Image performs normally when the noise is small, but wrongly denoises the image as deer when the noise level is 0.6. We claim that VAE features preserve the low-frequency components of the input (e.g., global structures and contours) and lead to correct denoise results. Besides, to figure out why GPT-4o-Image denoises the dog image as deer, we query two multi-modal understanding models, including GPT-4o and Qwen2.5-VL, as shown in Fig. 2 (d) & (e). Interestingly, both understanding models caption the 0.6-noised dog image as deer, which demonstrates GPT-4o-Image is based on the prior of powerful multi-modal understanding models. In summary, our experiments demonstrate that GPT-4o-Image more likely employs visual features extracted by semantic encoders rather than VAEs since the low-level information of the source image is not preserved after the manipulation. Inspired by such observations, we design the architecture of our unified image perception and manipulation methods named UniWorld."
        },
        {
            "title": "3 Method",
            "content": "Figure 3: Model architecture. The model consists of VLM, SigLIP, DiT [28], and MLP connector. High-level semantics and historical state are provided by the VLM, while low-level image features are controlled by SigLIP. The understanding part uses frozen VLM with an autoregressive approach, while the generation part is trained with flow matching. T5 (originally used for conditional injection) is optional during training or generation."
        },
        {
            "title": "3.2 Training Recipe\nDuring training, the T5 [30] features (the original condition used in FLUX) are optional. However,\nwe observe that incorporating T5 features in the early stages often leads to convergence to poor local\nminima. Therefore, we do NOT recommend using T5 features early in training.",
            "content": "4 Stage 1: Pretraining for Semantic Alignment Due to feature gap between VLM representations and the FLUX text branch, stage 1 focuses on aligning VLM features to T5 features. During this stage, only the MLP mapping from VLM to FLUX is trainable, while all other parameters remain frozen. Moreover, since stage 1 is solely dedicated to aligning VLM semantic features, SigLIP features are excluded. After stage 1 pretraining, the model can perform text-to-image generation and produce images that differ from the reference based on editing instructions. Stage 2: Fine-Tuning for Consistent Generation We load the weights of the VLM to FLUX MLP trained in stage 1 and the MLP weights from FLUX-Redux [13], which align SigLIP features to the text branch. We unfreeze all learnable parameters in the FLUX image branch while keeping all text branch parameters frozen. Although stage 1 aligns VLM to FLUX, early in stage 2 the model still takes shortcut by directly reconstructing the reference image. After 5,000 to 10,000 training steps, the model begins learning how to use SigLIP features as reference cues to generate images according to instructions."
        },
        {
            "title": "3.3 ZeRO-3 EMA",
            "content": "EMA offers more stable and consistent weight averaging during training and is stored in FP32 to preserve numerical precision. This approach ensures that weight fluctuations are smoothed over time, which improves convergence behavior and promotes better generalization. Because our model is extremely large, storing an extra FP32 copy on each GPU would strain computational resources and potentially limit overall batch size. As shown in Figure 4, the training model (DiT) operates under ZeRO-2, while the EMA model is sharded across GPUs using ZeRO-3. By leveraging ZeRO-3 for the EMA, each GPU holds only fraction of the full FP32 parameters, enabling efficient memory utilization. For instance, 20B model sharded across GPUs requires each GPU to hold only 204 GiB, which minimizes redundant storage. We update the EMA every step, which reduces its computation to 1 and ensures that computation cost remains low as the number of GPUs increases. This scheme also supports running the training model under ZeRO-3, further decreasing memory overhead and allowing larger effective batch sizes. N"
        },
        {
            "title": "3.4 Training Data",
            "content": "Figure 4: Zero-3 EMA. EMA model is initialized with Zero-3-style sharding across GPUs to reduce overhead. During each step, each GPU updates only its own shard. We use almost identical data in two stages. This includes open-source high-quality data, self-generated data, and filtered open-source data. Data types include: 1. Image Perception: canny, mlsd, hed, depth, sketch, segmentation (mask), detection (bounding box). Most data originates from (1) Graph200k [16], (2) COCO2017 [19]. Although perception maps usually reach 1024 1024 resolution, over 90% of reference images are limited to lower resolutions (e.g., 512 512). Since perception maps and reference images differ substantially, the mask weighting strategy is unnecessary. Image perception data amounts to approximately 1.4M. 2. Image Manipulation: common edit types such as add, remove, replace, etc. Main sources are ImgEdit [42] and SEED-X [11]. ImgEdit provides over 1M editing samples. We use the subset with high score, totaling 724k higher-quality samples. In SEED-X, we select part 3 with resolutions of at least 1024 1024. We also collect style transfers from Graph200k given reference style images, as well as virtual try-on and product extraction data. Since most open-source data lack editing masks, we generate edit masks following Section 3.5. Image manipulation data amounts to approximately 1M. 3. Text-to-Image Generation: sources include BLIP3-o [4] and internal images from the Open-Sora Plan [17]. Open-Sora Plan images receive dense captions from Qwen2-VL72B [37], all with resolutions of at least 1024 1024 and aesthetic score at least 6.0. Text-to-image generation data amounts to approximately 300k. Figure 5: Pipeline for mask generation. Given reference image and target image, the mask is obtained through (1) pixel-wise differencing, (2) dilation, (3) connected component filtering, and (4) max-pooling downsampling. The bottom right shows four different weighting functions. We highlight the limitations of steps (1), (2), and (3), which are addressed in the next stage."
        },
        {
            "title": "3.5 Adaptive Editing Region Weighting Strategy",
            "content": "In image editing tasks, the edited region typically occupies only small portion of the image. If uniform loss weighting is applied across the whole image, the loss signal from the relatively small edited area may be overwhelmed by that from the much larger unedited region. This imbalance can cause the model to underfit the edited content, failing to capture fine-grained or user-intended changes. straightforward approach is to use the mask region (i.e., the edited area) as weight. However, not all edited data comes with masks. Therefore, we obtain masks through the following four steps: (1) Pixel-wise Differencing: Compute the pixel-wise difference between the reference and target images. We set tolerance threshold to determine whether pixel region is edited or unedited. However, as shown in Figure 5 (1), this produces many noisy differences. (2) Dilation: Expand each edited pixel using dilation factor to reduce noise, though many isolated pixels may still remain. (3) Connected Component Filtering: Remove small connected components to eliminate spurious edits, but this does not address bubbles within larger edited regions. (4) Max-pooling Downsampling: Apply max-pooling to remove internal noise within connected regions. Finally, we obtain the edited area size, denoted as Aedit. To prevent small edited regions from being overwhelmed by the background during training, we design weighting strategy that assigns higher loss weights to edited pixels. The weight is function of the relative area ratio between the full image and the edited region: = Atotal , where the total Aedit image area is denoted as Atotal. We require the weighting function w(x) to satisfy w(1) = 1, so that when the entire image is edited (i.e., text-to-image or style transfer, Aedit = Atotal), the loss reverts to uniform weighting. We design and compare four candidate functions: (1) Linear: w(x) = (2) Exponential Root: w(x) = 2 (3) Logarithmic: (4) Quadratic Root: w(x) = log2(x) + 1 w(x) = ( 1)2 + 1 All four functions ensure w(1) = 1, and increase as the edit area shrinks (x ). Among them, the logarithmic function (3) grows moderately, avoids instability for extremely small regions, and maintains good balance between sensitivity and robustness. Therefore, we adopt the logarithmic weighting function w(x) = log2(x) + 1 in our final implementation. Table 1: Comparison between different models on Understanding & Generation & Editing benchmarks. refer to the methods using LLM rewriter. indicates the model is incapable of performing the task. Model Understanding Image Generation Image Editing MMBV MMBI MMMU MM-Vet GenEval WISE Overall Add Adjust Extract Replace Remove Hybird Image Understanding 36.3 57.4 Image & Video Understanding 32.0 57. Text-to-Image Generation 0.55 0.66 0.55 0. Image Editing 1.85 1.89 2.63 3. Unified Understanding & Generation - 34.3 39.8 37.2 66.6 67.2 67.1 0.68 0.61 0.80 0.66 0.80 0.88 0.84 0.35 0.18 0.35 0.39 0.55 0.52 0.55 - - 3.17 3.37 2.72 2.29 3.12 3. - - 3.55 3.86 1.47 1.79 2.66 3.13 - - 3.30 3.70 1.31 1.33 1.82 1.87 - - 1.56 2.23 1.89 1.93 2.71 3. - - 3.38 3.49 1.57 1.49 2.34 2.61 - - 2.44 3. 1.80 1.48 2.07 2.52 - - 2.55 3.13 LLaVA-1.5 [21] LLaVA-NeXT [51] Video-LLaVA [18] LLaVA-OV [15] 1.05 0.94 36.4 79.3 60.9 80.8 67.8 51. 32.8 48.8 SDXL [29] FLUX.1 Dev [14] MagicBrush [50] Instruct-P2P [3] AnyEdit [43] Step1-Edit [23] Show-o [40] Janus [39] Janus-Pro [6] Emu3 [38] MetaQuery-XL [27] BAGEL [8] UniWorld-V - - - 1.79 - 69.4 75.5 58.5 83.5 85.0 83.5 27.4 30.5 36.3 31.6 58.6 55.3 58."
        },
        {
            "title": "4.1 Main Results",
            "content": "Table 1 presents our main comparative results. We compare the performance of our UniWorld-V1 model against other advanced models across three core benchmarks: image understanding, image generation, and image editing. The experimental results comprehensively demonstrate UniWorldV1s exceptional performance across these three major categories, proving its powerful unified capabilities. We achieved state-of-the-art or near state-of-the-art performance in every sub-category. Specific comparisons for text-to-image generation will be provided in Section 4.2, image editing results in Section 4.3, visual understanding comparisons in Section 4.4, and image perception capabilities will be showcased through sampling examples in Section 4.5."
        },
        {
            "title": "4.2 Text-to-Image Generation",
            "content": "This section presents the performance of our UniWorld-V1 in text-to-image generation, primarily focusing on two aspects: the models fundamental object-focused text-to-image generation abilities, as evaluated by the GenEval [12], and their world knowledge reasoning capabilities for image generation, as assessed by the WISE [25]. Evaluation results on GenEval. Table 2 showcases the models evaluation results on GenEval. Our UniWorld-V1 achieves strong performance with an overall score of 0.79. Furthermore, we observe that many models, such as MetaQuery, BLIP3-o, and BAGEL, utilize LLM rewriters for prompt rewriting to facilitate generation. For fair comparison, we used the rewrite prompt used by blip3o for additional testing. UniWorld-V1 ultimately achieves an impressive score of 0.84. This result is remarkably close to the top-performing BAGELs 0.88, while UniWorld-V1 only utilizes 2.7M training data compared to BAGELs 2665M data. This stark difference in data efficiency powerfully highlights the superiority of our architecture. Evaluation results on WISE. As presented in Table 3, our proposed UniWorld-V1 exhibits exceptionally strong performance as unified model, achieving an overall score of 0.55. This makes UniWorld-V1 highly competitive among unified models in leveraging and integrating world knowledge for generating semantically rich and accurate images. Notably, UniWorld-V1 achieves an impressive 0.73 in the Space category. This score is particularly significant as it is the closest to 7 Table 2: Comparison results on GenEval. Gen. Only refers to pure generation models, while Unified indicates models capable of both understanding and generation. refers to the methods using LLM rewriter. : Results of GPT-4o-Image are tested by [41]."
        },
        {
            "title": "Model",
            "content": "Single Obj. Two Obj. Counting Colors Position Color Attribute Overall PixArt-α [5] Emu3-Gen [38] SDXL [29] DALL-E 3 [32] SD3-Medium [9] FLUX.1-dev [14] Janus [39] Emu3-Gen[38] Show-o [40] Janus-Pro-7B [6] MetaQuery-XL [27] BLIP3-o [4] BAGEL [8] BAGEL [8] GPT-4o-Image UniWorld-V1 UniWorld-V1 0.98 0.98 0.98 0.96 0.99 0.98 0.97 0.99 0.98 0.99 - - 0.99 0.98 0.99 0.99 0.98 0.50 0.71 0.74 0.87 0.94 0. 0.68 0.81 0.80 0.89 - - 0.94 0.95 0.92 0.93 0.93 Gen. Only 0.44 0.34 0.39 0.47 0.72 0.75 Unified 0.30 0.42 0.66 0.59 - - 0.81 0.84 0.85 0.79 0.81 0.80 0.81 0.85 0.83 0.89 0.93 0.84 0.80 0.84 0.90 - - 0.88 0.95 0.92 0.89 0.89 0.08 0.17 0.15 0.43 0.33 0. 0.46 0.49 0.31 0.79 - - 0.64 0.78 0.75 0.49 0.74 0.07 0.21 0.23 0.45 0.60 0.65 0.42 0.45 0.50 0.66 - - 0.63 0.77 0.61 0.70 0.71 0.48 0.54 0.55 0.67 0.74 0.82 0.61 0.66 0.68 0.80 0.80 0.84 0.82 0.88 0.84 0.80 0.84 Table 3: Comparison results on WISE. WISE evaluates the models capacity for complex semantic understanding and world knowledge in text-to-image generation. Gen. Only refers to pure generation models, while Unified indicates models capable of both understanding and generation. : Results of GPT-4o-Image are tested by [41]."
        },
        {
            "title": "Model",
            "content": "Cultural Time Space Biology Physics Chemistry Overall SDXL [29] SD3.5-large [9] PixArt-Alpha [5] playground-v2.5 [20] FLUX.1-dev [14] Janus [39] Show-o [40] Janus-Pro-7B [6] Emu3 [38] MetaQuery-XL [27] BAGEL [8] GPT-4o-Image UniWorld-V1 0.43 0.44 0.45 0.49 0.48 0.16 0.28 0.30 0.34 0.56 0.44 0.81 0. 0.48 0.50 0.50 0.58 0.58 0.26 0.40 0.37 0.45 0.55 0.55 0.71 0.55 Gen. Only 0.47 0.58 0.48 0.55 0.62 Unified 0.35 0.48 0.49 0.48 0.62 0.68 0.89 0.73 0.44 0.44 0.49 0.43 0.42 0.28 0.30 0.36 0.41 0.49 0.44 0.83 0. 0.45 0.52 0.56 0.48 0.51 0.30 0.46 0.42 0.45 0.63 0.60 0.79 0.59 0.27 0.31 0.34 0.33 0.35 0.14 0.30 0.26 0.27 0.41 0.39 0.74 0.41 0.43 0.46 0.47 0.49 0.50 0.23 0.35 0.35 0.39 0.55 0.52 0.80 0. GPT-4o-Images 0.89 in this category, positioning UniWorld-V1 as the top performer among all other evaluated models (excluding GPT-4o-Image) in capturing and utilizing spatial world knowledge. 4."
        },
        {
            "title": "Image Manipulation",
            "content": "As illustrated in Table 4, we compare UniWorld-V1 with other open-source models and GPT-4oImage on editing capabilities, using the ImgEdit-Bench [42] benchmark. Our UniWorld-V1 model demonstrates the best overall performance among all open-source models, achieving an impressive total score of 3.37. This significantly surpasses other leading open-source models such as Step1X-Edit (3.17) and BAGEL (3.17), fully showcasing UniWorld-V1s exceptional and versatile performance across diverse image editing domains. Notably, UniWorld-V1 achieves the highest scores among all open-source models in several key categories: Adjust (3.70), Remove (3.54), Extract (2.23), Replace 8 Table 4: Comparison results on ImgEdit-Bench. Overall is calculated by averaging all scores across tasks. : Results of GPT-4o-Image are tested by [42]."
        },
        {
            "title": "Model",
            "content": "Add Adjust Extract Replace Remove Style Action Hybrid Background Overall MagicBrush [50] 2.72 2.29 Instruct-P2P [3] AnyEdit [43] 3.12 UltraEdit [52] 3.63 Step1X-Edit [23] 3.90 3.55 BAGEL [8] GPT-4o-Image 4.65 UniWorld-V1 3.86 1.47 1.79 2.66 3.01 3.13 3.30 4.26 3.70 1.31 1.33 1.82 2.02 1.87 1.56 2.96 2.23 1.89 1.93 2.71 3.13 3.45 3.38 4.49 3.49 1.57 1.49 2.34 1.71 2.61 2.44 3.81 3. 2.49 3.54 3.27 3.69 4.44 4.24 4.75 4.22 1.39 1.51 3.31 3.57 3.43 4.29 4.76 3.44 1.80 1.48 2.07 2.33 2.52 2.55 4.54 3.13 2.03 1.67 2.37 3.31 3.19 3.22 4.62 2.76 1.85 1.89 2.63 2.93 3.17 3.17 4.31 3.37 (3.49), and Hybrid (3.13). These leading scores underscore UniWorld-V1s advanced capabilities in precise attribute adjustment, specified element removal, object extraction, hybrid editing, and content replacement. While GPT-4o-Image maintains leading position with total score of 4.31, UniWorld-V1s total score of 3.37 makes it the closest performing model to GPT-4o-Image among all open-source alternatives. This strongly demonstrates UniWorld-V1s significant step towards achieving image editing capabilities comparable to industry-leading models."
        },
        {
            "title": "4.4 Visual Understanding\nBy benefiting from our strategy of freezing the\nMultimodal Large Language Model component,\nwe successfully inherited the robust multimodal\nunderstanding capabilities of Qwen2.5-VL-7B\nwithout the need for retraining. This signifi-\ncantly reduces our resource consumption, specif-\nically in terms of data and computational power,\nand also prevents potential degradation of under-\nstanding performance that could arise from train-\ning on generative tasks. As presented in Table 5,\nthis architectural choice enables UniWorld-V1\nto achieve remarkable results, significantly sur-\npassing models like Janus, Show-o, and Emu3\nacross multiple metrics, and achieves highly\ncompetitive performance against the latest ad-\nvanced models such as BAGEL.",
            "content": "Table 5: Comparison between different models on Visual Understanding benchmarks. indicates the model is incapable of performing the task."
        },
        {
            "title": "Model",
            "content": "MMBV [10] MMBI [24] MMMU [47] MM-Vet [44] Image & Video Understanding LLaVA-1.5 [21] Video-LLaVA [18] 1.05 36.4 60.9 67.8 32. Show-o [40] Janus [39] Janus-Pro [6] Emu3 [38] BLIP3-o [4] MetaQuery [27] BAGEL [8] GPT-4o UniWorld-V1 Unified Understanding & Generation 27.4 30.5 36.3 31.6 50.6 58.6 55.3 72.9 58.6 - - - - 2.15 1.79 - 69.4 75.5 58.5 83.5 83.5 85.0 - 83.5 36.3 32.0 - 34.3 39.8 37.2 66.6 66.6 67.2 76.9 67."
        },
        {
            "title": "5 Conclusion",
            "content": "In summary, UniWorld demonstrates that unified architecture anchored by high-resolution semantic encoder, which address both image perception and manipulation tasks with state-of-the-art efficiency. By leveraging only 2.7M training samples, UniWorld achieves superior performance against much larger cost models on diverse benchmarks, confirming that semantic encoders provide richer and more versatile visual representations than traditional VAE-based approaches. This work establishes foundation for future research in unified visual generation. We release all code, model weights, datasets to foster continued innovation and collaboration within the community. 9 Figure 6: Showcase of UniWorld-V1s perception capabilities. This figure presents qualitative comparison of UniWorld-V1s perception tasks against GPT-4o, using randomly selected examples. Green boxes indicate correct responses, while red boxes highlight instances where the models output deviates from the expected result. Limitation Despite UniWorlds remarkable training efficiency, the following shortcomings remain: Insufficient instruction generalization. Limited training data and lack of VLM fine-tuning require specific instruction templates to outperform BAGEL. Inadequate reference-image consistency. Reference images are processed at 512 512 resolution, which is insufficient to generate all details at 1024 1024 scale. Incomplete benchmarks. DPG-Bench and GenAI-Bench with scores above certain threshold often fail to reflect human preference, as verified through manual inspection. Some samples in GenEval forcibly bind two objects that rarely co-occur in the real world, ignoring the natural distribution of images. ImgEdit-Bench and GEdit-Bench lack sufficient sensitivity to the reference regions."
        },
        {
            "title": "Future Work",
            "content": "Continue collecting data and perform joint training with VLM. Integrate higher-resolution semantic encoders or adopt VLM techniques to increase inputimage resolution, such as multi-scale image gridding."
        },
        {
            "title": "Failed Attempts",
            "content": "We empirically attempt to replace SigLIP with other encoders such as DINO V2 [26] and RADIO V2.5 [34], but the attempts are unsuccessful. We attempt to use Qwen2.5VLs visual output (picking the image feature of outputs while abandoning text feature) directly as the reference-image control signal. However, consistency between generated and reference images remains poor. This issue arises from the intrinsic gap between VLM training objectives and contrastive training. Contrastive learning focuses on global semantic features that saturate as resolution increases, whereas VLM training demands both global and local semantic information. As result, the model capacity does not preserve sufficient low-level control signals, which likely causes the failure."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [4] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [10] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. [11] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [12] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [13] Black Forest Labs. Flux. https://bfl.ai/announcements/24-11-21-tools, 2024. [14] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [16] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and MingMing Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025. 11 [17] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [18] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [20] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [21] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [22] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [23] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [25] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [26] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [27] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [31] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [32] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improving image captioning with better use of captions. arXiv preprint arXiv:2006.11807, 2020. 12 [33] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. 2025. [34] RADIOv2.5 Team. Flux. https://github.com/NVlabs/RADIO/blob/main/RADIOv2.5_ tech_report.md, 2024. [35] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [36] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [37] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [38] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [39] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [40] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [41] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [42] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, Imgedit: unified image editing dataset and benchmark. arXiv preprint and Li Yuan. arXiv:2505.20275, 2025. [43] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [44] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [45] Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Chongyang Ma, Jiebo Luo, and Li Yuan. Opens2v-nexus: detailed benchmark and million-scale dataset for subject-to-video generation. arXiv preprint arXiv:2505.20292, 2025. [46] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [47] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [48] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [49] Jihai Zhang, Tianle Li, Linjie Li, Zhengyuan Yang, and Yu Cheng. Are unified visionlanguage models necessary: Generalization across understanding and generation. arXiv preprint arXiv:2505.23043, 2025. [50] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [51] Zhang, Li, Liu, Lee, Gui, Fu, Feng, Liu, and Li. Llava-next: strong zero-shot video understanding model. 2024. [52] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024."
        }
    ],
    "affiliations": [
        "Peking University, Shenzhen Graduate School",
        "Peng Cheng Laboratory",
        "Rabbitpre AI"
    ]
}