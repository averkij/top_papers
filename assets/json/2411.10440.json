{
    "paper_title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
    "authors": [
        "Guowei Xu",
        "Peng Jin",
        "Li Hao",
        "Yibing Song",
        "Lichao Sun",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct."
        },
        {
            "title": "Start",
            "content": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step Peng Jin1,3,4 Guowei Xu2 Li Hao1,3,4 Yibing Song5 1School of Electronic and Computer Engineering, Peking University 2 Institute for Interdisciplinary Information Sciences, Tsinghua University 3Peng Cheng Laboratory 4AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School Li Yuan1,3,4* Lichao Sun 4 2 0 2 5 1 ] . [ 1 0 4 4 0 1 . 1 1 4 2 : r 5Alibaba DAMO Academy 6Computer Science and Engineering, Lehigh University *Correspondence: yuanli-ece@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAIs o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex viIn this work, we introsual question-answering tasks. duce LLaVA-o11, novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inferencetime scaling. Remarkably, with only 100k training samples and simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closedsource models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. 1. Introduction Large language models, represented by OpenAI o1 [63], demonstrate strong capabilities for systematic and in-depth reasoning, validating the efficacy of inference-time scaling 1There are similar names of recent VLM works. To clarify, LLaVA-o1 is built upon Llama-3.2-Vision model [40], rather than LLaVA [32]. Figure 1. Performance of LLaVA-o1 and other models across six multimodal reasoning benchmarks. Although LLaVA-o1 is fine-tuned from the Llama-3.2-11B-Vision-Instruct [40] model (which has the lowest average score), it outperforms many larger open-source models and even some closed-source models. Detailed benchmark results are shown in Table 7. for language models [47]. However, vision is equally important for enabling models to fully understand the world and extend their cognitive abilities [6]. Therefore, developing multimodal model that integrates both language and vision while facilitating effective, systematic, and deep reasoning holds substantial significance. Early open-source vision language models (VLMs) mainly employ direct prediction approach [21, 30, 32], generating brief answers immediately in response to question. The main limitation of this direct-response paradigm is its lack of structured reasoning process, making it less effective for tasks demanding logical reasoning [62]. Recent studies have shown that incorporating Chain-of-Thought (CoT) reasoning encourages the model to reason step by 1 Figure 2. Comparison of the base model and LLaVA-o1. As shown, the base model Llama-3.2-11B-Vision-Instruct exhibits obvious flaws in reasoning, with several errors occurring throughout the reasoning process. In contrast, LLaVA-o1 begins by outlining the problem, interprets relevant information from the image, proceeds with step-by-step reasoning process, and ultimately reaches well-supported conclusion. step, significantly improving its question-answering capabilities [52]. However, even with CoT reasoning, most VLMs frequently produce errors or hallucinated outputs during the reasoning progress [24, 31, 50]. Our findings suggest that significant cause of these issues is the insufficiently systematic and structured nature of the reasoning process in existing VLMs. Specifically, by referring systematic, we mean that the model does not generate direct reasoning chain but instead engages in multistage reasoning. Structured, on the other hand, refers to the models ability to clearly identify the reasoning stage it is in and understand the primary task to be addressed at each stage. We observe that VLMs often initiate responses without adequately organizing the problem and the available information. Moreover, they frequently deviate from logical reasoning toward conclusions, instead of presenting conclusion prematurely and subsequently attempting to justify it. Given that language models generate responses tokenby-token, once an erroneous conclusion is introduced, the model typically continues along flawed reasoning path. OpenAI o1 [63] addresses these issues effectively by enabling the model to independently engage in systematic and structured reasoning through language. Building on this insight, we design LLaVA-o1. While the community has made some preliminary explorations into the underlying mechanisms of OpenAI o1 [42, 54], the model remains black box, with its technical details largely unknown. This work demonstrates potential way to enhance models ability to perform autonomous, stage-by-stage reasoning by employing supervised fine-tuning. Specifically, LLaVA-o1 is capable of generating four distinct stages: summary, caption, reasoning, and conclusion. Each stage serves unique purpose in the reasoning process. Summary: brief outline in which the model summarizes the forthcoming task. Caption: description of the relevant parts of an image (if present), focusing on elements related to the question. Reasoning: detailed analysis in which the model systematically considers the question. Conclusion: concise summary of the answer, providing final response based on the preceding reasoning. To enhance the understanding of CoT processes in LLM, LLaVA-o1 marks each stage with dedicated tag (e.g., <SUMMARY>...</SUMMARY>) to denote the beginning and end of each stage. Those taggings enable the model to maintain clarity throughout the reasoning process. Unlike traditional CoT reasoning, which allows the model to think freely, our method promotes structured thinking by first or2 ganizing the problem and known information, followed by detailed thought process, and then deriving conclusion. To achieve this, we construct the LLaVA-o1-100k dataset by generating responses stage by stage using GPT-4o [3] and then train the model using supervised fine-tuning. The structured reasoning in LLaVA-o1 also facilitates efficient inference time scaling. In contrast to conventional scaling methods, such as best-of-N sampling [4, 51] and sentence-level beam search [16, 49], LLaVA-o1 employs novel stage-level beam search method that generates multiple candidate results at each stage and selects the best one to continue the generation process. We conduct experiments on several multimodal reasoning benchmarks, including MMStar [9], MMBench [33], MMVet [60], MathVista [35], AI2D [23], and HallusionBench [17], and observed that LLaVA-o1 offers two primary advantages: First, enabling the model to perform structured reasoning independently substantially outperforms traditional CoT prompting, particularly in complex reasoning tasks that require systematic analysis. Second, our stage-level beam search method is scalable and improves performance reliability, making it more effective in achieving stable and accurate results. Our contributions are summarized as follows: We introduce LLaVA-o1, visual language model designed for systematic reasoning, demonstrating exceptional performance on tasks that require structured thinking and reasoning. We demonstrate that LLaVA-o1, using stage-level beam search, is inference-time scalable. This means that with increased computational resources, the performance of our approach can be further enhanced, making it applicable to more complex tasks and scenarios. Extensive experiments on various benchmarks demonstrate that our method achieves superior performance relative to larger and closed-source models, underscoring the effectiveness of LLaVA-o1 for multimodal reasoning. 2. Related Works 2.1. Visual reasoning with large language models Visual reasoning demands the models visual perception capability and the high-level cognition ability [22, 37]. Several tasks have been applied to evaluate the visual reasoning ability of VLMs, including VQA [20, 26] requiring models to answer from visual contents and textual questions, and Visual Entailment [48] requiring models to determine the consistency of text descriptions and visual contents, etc. Traditional vision-language models employ neural symbolic approaches [5, 11] to explicitly model the visual reasoning process. With the development of LLMs, visionlanguage models leverage the advanced reasoning abilities of LLMs to interpret visual tasks [32, 59]. Some vision-language models enhance visual reasoning by optimizing the visual encoding strategy [21, 29, 32] to produce cognition-focused visual tokens. Some VLMs, e.g. VISPROG [18], position the LLM as decision-making agent, enhancing visual reasoning under complex visual tasks by invoking various task-specific visual modules. Additionally, instructing learning techniques for language models, including prompt tuning [61], in-context learning, and supervised fine-tuning [46], also contribute to improvements in visual reasoning capabilities. 2.2. Chain-of-thought in large language models Chain-of-thought prompting offers step-by-step reasoning trajectory when LLM faces hard questions including commonsense reasoning [44], logical reasoning [27, 55], etc. Specifically, CoT prompting decomposes the question into group of reasoning steps and builds chain to guide the model to generate the results of complex problems step-bystep [12]. Recent works have demonstrated [15, 52] that CoT prompting substantially improves the LLMs capability on both reasoning and interpretability. 2.3. Inference time scaling Existing methods for inference time scaling fall into two main categories: those that rely on an external verifier for selection [25, 56] and those that operate independently of any external verifier [19, 53]. The external verifier selection methods can be employed in prevalent methods. On the other hand, inference time scaling methods that do not rely on an external verifier primarily include majority voting [19], best-of-N search [4, 51], and sentence-level beam search [16, 49]. Majority voting is effective for certain types of problems that have standard answers, but it is not suitable for open-ended tasks. Best-of-N search generates complete answers and allows the model to select the best response. However, generating full answers for selection can complicate the evaluation of their accuracy. Sentencelevel beam search generates multiple candidate sentences, selects the best one, and iteratively continues this process. However, this approach operates at too granular level, which makes it difficult for the model to effectively assess the quality of its responses on per-sentence basis. 3. Proposed Method Our LLaVA-o1facilitates progressive, step-by-step reasoning process that enhances the reasoning capabilities of Vision-Language Models (VLMs) and allows for effective inference time scaling [47]. Using structured thinking, LLaVA-o1 achieves systematic and efficient reasoning process. Its inference-time reasoning framework enables it to outperform existing methods in inference time scalability. This design ensures both robustness and accuracy in complex tasks requiring reasoning, which separates it 3 from traditional approaches. Figure 1 illustrates our general framework of the reasoning process. 3.1. Enhancing Reasoning Capability through"
        },
        {
            "title": "Structured Thinking",
            "content": "Our goal during training time is to develop visual language model capable of extended chains of reasoning, allowing it to engage in systematic and in-depth reasoning. 3.1.1. Reasoning Stages Our proposed model, LLaVA-o1, decomposes the answer generation process into four structured reasoning stages: Summary Stage. In this initial phase, LLaVA-o1 provides high-level summary interpretation of the question, outlining the primary aspects of the problem it intends to address. Caption Stage. If an image is present, LLaVA-o1 offers concise overview of the visual elements relevant to the question, helping to understand multimodal input. Reasoning Stage. Building on the initial summary, LLaVA-o1 conducts structured, logical reasoning to derive preliminary answer. Conclusion Stage. In this final stage, LLaVA-o1 synthesizes an answer based on the preceding reasoning. Here, the output from the conclusion stage is the direct response provided to the user, while the prior three stages are internal hidden stages representing LLaVA-o1s reasoning process. The output at this stage adapts to the users requirements: for instance, if the user requests brief answer, the conclusion will be concise; if detailed explanations are desired, the conclusion provides thorough, comprehensive response. Each stage is initiated at the models discretion, without external prompt engineering frameworks or addiSpecifically, we provide the model tional prompting. with four pairs of special tags: <SUMMARY></SUMMARY>, <CAPTION></CAPTION>, <REASONING></REASONING>, and <CONCLUSION></CONCLUSION>. These tags correspond to summarizing the response approach, describing relevant image content, conducting reasoning, and preparing final answer, respectively. Upon training, the model autonomously selects these tags as needed, activating each stage based on its own judgment. As with OpenAI o1 [63], all stages are completed by the model in single inference pass. This structured approach enables the model to independently manage its reasoning process, improving its adaptability and performance on complex reasoning tasks. 3.1.2. Data Preparation and Model Training Most existing VQA datasets lack detailed reasoning processes needed to train the LLaVA-o1 model. Therefore, we compile new dataset, integrating samples from several widely used VQA datasets, resulting in total of 99k Figure 3. Process flow for generating the LLaVA-o1-100k dataset. We prompt GPT-4o to generate responses in separate stages, and filter its outputs to ensure quality. Dataset ShareGPT4V [8] ChartQA [38] A-OKVQA [45] AI2D [23] GeoQA+ [7] ScienceQA [34] DocVQA [39] PISC [28] CLEVR [22] CLEVR-Math [13] Type General VQA General VQA General VQA Science-Targeted VQA Science-Targeted VQA Science-Targeted VQA General VQA General VQA General VQA Science-Targeted VQA Size 31.3k 17.2k 16.1k 11.4k 11.4k 5.6k 4.0k 1.0k 0.5k 0.5k Table 1. The number of samples selected from each benchmark. image QA pairs (each pair may include one or multiple rounds of questioning). As shown in Figure 3, since no multimodal model currently exists that can directly produce systematic, structured reasoning, we use GPT-4o [3] to generate detailed reasoning processes, including summary, caption, reasoning, and conclusion, and compile these into the LLaVA-o1-100k dataset, which we plan to release for public use. We include data from both general-purpose VQA datasets and science-targeted VQA datasets specified blow: General VQA Datasets. We include several generalpurpose VQA datasets with distinct focuses. ShareGPT4V [8] provides multi-turn question-answering data from GPT4V [57] interactions. ChartQA [38] focuses on interpreting charts and graphs. A-OKVQA [45] emphasizes external knowledge beyond visible content. DocVQA [39] involves document-based questions requiring textual comprehension. We also include PISC [28] to understand social relationships, and CLEVR [22] to address object properties, spatial relationships, and counting tasks. Science-Targeted VQA Datasets. These datasets include GeoQA+ [7] for geometric reasoning, along with AI2D 4 Figure 4. An illustration of inference approaches. Best-of-N search generates complete responses and selects the best one among them; Sentence-level Beam Search generates multiple candidate options for each sentence and chooses the best one. In contrast, our Stagelevel Beam Search generates candidates for each reasoning stage (e.g., summary, caption, reasoning, and conclusion) and selects the best option at each stage. Best-of-N search operates at coarse level, while Sentence-level Beam Search is overly granular, and our method achieves an optimal balance and achieves the best performance. Figure 5. Comparison of LLaVA-o1 performance with and without stage-level beam search. Our stage-level beam search is effective in selecting better reasoning during model inference. [23] and ScienceQA [34], which target scientific questions. CLEVR-Math [13], an extension of CLEVR, focuses on arithmetic analysis in visual contexts. Table 1 shows the number of QA pairs selected from each dataset. Model Training. The LLaVA-o1-100k dataset we construct can be used to further conduct Supervised FineTuning (SFT) on any existing model to enhance reasoning capabilities. In this work, we select the Llama-3.2-11BVision-Instruct [40] model as the base model, and perform full parameter fine-tuning by using the LLaVA-o1-100k dataset. The training is conducted on single node with 8 H100 GPUs. 3.2. Effective Inference Time Scaling using Stagelevel Beam Search After training, our objective is to further enhance the models reasoning ability during inference. Specifically, we leverage the stage-based outputs of LLaVA-o1, which provides an ideal granularity for inference time scaling. Our method follows the steps below: Sample responses for the first stage in the solution. Randomly sample 2 responses and let the model determine which is better, keeping the better response. Repeat for 1 times, retaining the best response. Sample responses for the next stage, then repeat steps 2-4 until all stages are processed. it is Notably, the structured output design of LLaVA-o1 that makes this approach feasible, enabling efficient and accurate verification at each stage. This validates the effectiveness of structured output in improving inference time scaling. An illustration of the three approaches is shown in Figure 4. We provide an example in Figure 5. When inference time scaling is not applied, although the model generates correct reasoning steps, it fails to arrive at concrete answer during the reasoning process. This causes the model to make guess in the conclusion phase, leading to an incorrect result. In contrast, with inference time scaling, the model retains the reasoning steps leading to the final result, ensuring the correctness of the answer. Model MMStar MMBench MMVet MathVista AI2D Hallusion Average Base Model Llama-3.2-11B-Vision-Instruct Our Models LLaVA-o1 (with Direct Training) LLaVA-o1 (w/o Structured Tags) LLaVA-o1 49.8 54.3 55.7 57. 65.8 76.2 74.2 75.0 57.6 49.9 57.0 60.3 48.6 49.5 54.1 54. 77.3 91.4 87.2 85.7 40.3 42.9 45.0 47.8 56.6 60.7 62.2 63. Table 2. Experimental results of different models on the benchmark. Here, LLaVA-o1 (with Direct Training) refers to the model trained directly on the original VQA datasets Q&A pairs, while LLaVA-o1 (w/o Structured Tags) represents the model trained on the LLaVA-o1-100k dataset with the structured tags removed. LLaVA-o1 refers to the model trained on the complete LLaVA-o1-100k dataset, including the structured tags. Model CP FP IR LR Math Science & Technology Average Base Model Llama-3.2-11B-Vision-Instruct Our Models LLaVA-o1 (with Direct Training) LLaVA-o1 (w/o Structured Tags) LLaVA-o1 66. 46.4 57.6 50.8 45.2 68.4 68.4 68.8 48.0 48.0 46. 65.6 60.0 63.2 52.0 55.2 58.0 51.6 64.4 64.0 32.8 40.0 38.0 44.8 49. 54.3 55.7 57.6 Table 3. Performance of different models on the MMStar benchmark across various skill areas. Here, CP represents coarse perception, FP represents fine-grained perception, IR represents instance reasoning, and LR represents logical reasoning. As shown in the table, our model demonstrates substantial improvement over the base model in instance reasoning, logical reasoning, math, and science & technology, indicating that structured reasoning can significantly enhance the models reasoning capabilities. 4. Post-Training Performance In this section, we compare LLaVA-o1 with the base model, Llama-3.2-11B-Vision-Instruct, on six commonly used multimodal benchmarks to demonstrate the effectiveness of our approach during the training phase. Following this comparison, we conduct ablation studies to evaluate the contribution of each component within our method, addressing the following three key questions: (1) Is our LLaVA-o1-100k dataset more effective than directly using the original datasets Q&A pairs? (2) What is the impact of structured tags on the performance? Specifically, we explore whether LLaVA-o1 can function without tags by implicitly segmenting different stages of the response. (3) In which specific areas does our model show the most improvement compared to the base model, and does it genuinely enhance reasoning capabilities? evaluated using the testmini set, and the remaining datasets each have single test set. To ensure fairness and reproducibility, all evaluations are conducted using VLMEvalKit [14], an open-source evaluation toolkit for large visionlanguage models. The performance metrics of all baseline models are derived from VLMEvalKits testing results [1]. 4.2. Benchmark Results We found that LLaVA-o1 achieves significant performance improvements, despite using only 100k data. According to Table 2, compared to the base model, Llama-3.2-11BVision-Instruct, LLaVA-o1 demonstrates notable improvements across general VQA, mathematical reasoning, scientific VQA, and hallucination control tasks, with an average benchmark score increase of 6.9%, thereby validating the effectiveness of our approach. 4.1. Experimental Setup 4.3. Ablation Study We selected six widely used and challenging benchmarks for our experiments: MMStar [9], MMBench V1.1 [33], MMVet [60], MathVista [35], AI2D [23], and HallusionBench [17]. MMStar, MMBench, and MMVet primarily evaluate the general visual question-answering capabilities of models, while MathVista, and AI2D focus on models proficiency in mathematical and scientific reasoning. HallusionBench specifically assesses the models handling of language hallucinations and visual illusions. For MMBench, we use the V1.1 version of the test set, MathVista is Effectiveness of LLaVA-o1-100k Compared to Original Datasets. To demonstrate the effectiveness of our improved LLaVA-o1-100k dataset, we present comparison between LLaVA-o1 and the model trained on the original Q&A pairs across different benchmarks in Table 2. Although the model trained directly on the original Q&A pairs shows some overall improvement on the base model, its average performance remains significantly lower. In particular, on the MMVet benchmark, which requires more detailed responses, its performance is even worse than the base Model MMStar MMBench MMVet MathVista AI2D Hallusion Average Base Model Llama-3.2-11B-Vision-Instruct Our Models LLaVA-o1 LLaVA-o1(BS = 2) 49.8 57.6 58. 65.8 75.0 75.6 57.6 60.3 61.7 48.6 54.8 56. 77.3 85.7 87.5 40.3 47.8 48.2 56.6 63.5 64. Table 4. Experimental results during inference time. LLaVA-o1 (BS = 2) denotes the model using stage-level beam search with beam size of 2. The results show that stage-level beam search can achieve further significant performance improvements. Method Number of Beam MMVet Score Method Number of Beam MMVet Score No Inference Scaling Best-of-N Search Sentence-level Beam Search Stage-level Beam Search 1 10 2 4 60.3 60.9 58.4 62.9 No Inference Scaling Stage-level Beam Search Stage-level Beam Search Stage-level Beam Search 1 2 3 4 60.3 61.7 62.3 62.9 Table 5. Comparison to Baseline Methods. Our stage-level beam search outperforms both best-of-N and sentence-level beam search under comparable inference time compute. Table 6. Scaling Trend of LLaVA-o1. As the the number of candidate responses increases, the models performance consistently improves. model. This result underscores the importance of the multistage format of our LLaVA-o1-100k dataset for training models capable of advanced reasoning. Structured Tags are Essential for Enhanced Performance. To examine whether the four tags we introduced improve the models performance, we compare LLaVA-o1 with the model trained on the LLaVA-o1-100k dataset with structured tags removed. As shown in Table 2, our results show significant drop in performance when the tags are removed, indicating that the structured tagging facilitates reasoning and improves model performance. To the best of our knowledge, LLaVA-o1 is the first attempt to successfully enhance models reasoning ability and overall performance through structured reasoning with tags. Performance Gains Primarily in Reasoning-Intensive Areas. To analyze the specific areas in which LLaVA-o1 has improved compared to the base model, we conduct detailed assessment of the models performance across different skills on the MMStar benchmark. MMStar is designed to evaluate six key capabilities: coarse perception, finegrained perception, instance reasoning, logical reasoning, math, and science & technology. In Table 3, we compare the base model with LLaVA-o1. Our analysis reveals that LLaVA-o1 demonstrates notable improvements in tasks requiring systematic reasoning, such as instance reasoning, logical reasoning, math, and science & technology, while showing relatively smaller gains in coarse perception and fine-grained perception. This suggests that our method can mainly improve reasoning capabilities of the model. 5. Inference Time Scaling In this section, we aim to compare the effectiveness of our stage-level beam search approach with traditional methods like best-of-N and sentence-level beam search under comparable computational constraints. The experimental setup mirrors that used in the previous section, with evaluations conducted across the same six benchmarks: MMStar, MMBench V1.1, MMVet, MathVista, AI2D, and HallusionBench. All methods are evaluated using VLMEvalKit to ensure reproducibility. 5.1. Benchmark Results As shown in Table 4, stage-level beam search demonstrates substantial effectiveness in leveraging the structured reasoning stages of LLaVA-o1. By evaluating outputs at each reasoning stage, this approach strikes balance between rigorous quality control and computational efficiency, yielding higher inference accuracy on complex reasoning tasks without significant computational overhead. These findings suggest that stage-level beam search, which is made possible by the structured output design of LLaVA-o1, is an effective and powerful approach for inference time scaling. 5.2. Comparison to Baseline Methods We compare our method with baseline inference scaling methods on the MMVet benchmark to evaluate relative performance. For fair comparison, our stage-level beam search method and the baseline models are evaluated using comparable levels of inference time compute. Specifically, we set = 10 for the best-of-N method, generate 4 candidate responses per stage for our stage-level beam search, and use sentence-level beam search generating 2 candidates per sentence. As shown in Table 5, the best-of-N method yields only modest improvement of 0.6%, while sentence-level beam search even shows 1.9% decrease in performance. We examine the sub-scores and found that the main reason for the performance drop in sentence-level beam search is the excessively granular sentence-level apModel MMStar-R MMBench-R MMVet-R MathVista AI2D Hallusion Average Closed-Source Models GPT-4o-0806 [3] Claude3.5-Sonnet-0620 [2] Gemini-1.5-Pro [43] GPT-4o-mini-0718 [41] Open-Source Models InternVL2-Llama3-76B [10] InternVL2-8B [10] Ovis1.5-Gemma2-9B [36] MiniCPM-V2.6-8B [58] Llama-3.2-90B-Vision-Instruct [40] VILA-1.5-40B [30] Base Model Llama-3.2-11B-Vision-Instruct [40] Our Models LLaVA-o1 LLaVA-o1 (BS = 2) 66.0 64.2 56.4 54.9 68.9 62.5 58.7 57.1 51.1 53.2 46.6 57.5 57.8 82.4 75.4 71.5 76.9 84.4 77.4 76.3 75.7 76.8 75. 64.9 73.1 73.8 80.8 68.7 71.3 74.6 62.5 56.9 50.9 56.3 74.1 44.4 63.8 66.7 71. 62.7 61.6 57.7 52.4 65.6 58.3 65.6 60.6 58.3 49.5 48.6 54.8 56.1 84.7 80.2 79.1 77.8 87.6 83.6 84.5 82.1 69.5 77. 77.3 85.7 87.5 54.2 49.9 45.6 46.1 55.4 45.0 48.2 48.1 44.1 40.9 40.3 47.8 48. 71.8 66.7 63.6 63.8 70.7 64.0 64.0 63.3 62.3 56.9 56.9 64.3 65.8 Table 7. Experimental results of LLaVA-o1 and state-of-the-art models on reasoning benchmarks. Here, LLaVA-o1 refers to the model without inference scaling, while LLaVA-o1 (BS = 2) denotes the model using stage-level beam search with beam size of 2. proach, which struggles to effectively address open-ended In contrast, our stage-level beam search imquestions. proved performance by 2.6%, highlighting the superiority of stage-based search. 5.3. Scaling Trend of Stage-level Beam Search To better illustrate the effectiveness of our stage-level beam search as inference time compute increases, we evaluate LLaVA-o1 with different beam sizes on the MMVet benchmark. As shown in Table 6, we test the performance of the model by generating 1 (ie, no inference time scaling), 2, 3, and 4 candidate responses at each reasoning stage, allowing the model to select the best answer from these options. Our findings show that as the number of candidate responses increases, the models performance consistently improves, confirming that our stage-level beam search approach is scalable. Due to computational resource constraints, we only test beam size of 2 across all benchmarks. However, it is expected that increasing the beam size will lead to even more significant improvements. 6. Comparison to State-of-the-Art VLMs As shown in Table 7, we compare LLaVA-o1 with other state-of-the-art open-source and closed-source vision language models (VLM) across six benchmarks that require advanced reasoning capabilities: MMStar-R, MMBench-R, MMVet-R, MathVista, AI2D, and HallusionBench. MMStar-R, MMBench-R, and MMVet-R are custom benchmarks derived from MMStar, MMBench V1.1, and MMVet, respectively, with tasks requiring only coarse perception, fine-grained perception, and OCR removed. These filtered benchmarks retain tasks that demand complex reasoning. MathVista, AI2D, and HallusionBench inherently focus on advanced reasoning, so we retained all tasks within these benchmarks. [40], and VILA-1.5-40B [30]. Our results show that LLaVA-o1 consistently outperforms many open-source models of similar or even larger sizes, such as InternVL2-8B [10], Ovis1.5-Gemma29B [36], MiniCPM-V2.6-8B [58], Llama-3.2-90B-VisionRemarkably, Instruct LLaVA-o1 even surpasses certain closed-source models like GPT-4o-mini [41] and Gemini-1.5-pro [43], underscoring the effectiveness of our structured reasoning approach. This comparison validates the advantages of our method, particularly in benchmarks that heavily depend on reasoning skills, and highlights LLaVA-o1 as competitive model in the domain of reasoning-intensive VLM tasks. 7. Conclusion In this paper, we present LLaVA-o1, novel vision language model that performs structured, autonomous reasoning in multiple stages. By introducing four distinct stagessummary, caption, reasoning, and conclusionLLaVA-o1 achieves systematic reasoning process. Our contributions are twofold: first, the creation of the LLaVA-o1-100k dataset with detailed reasoning annotations, which supports training on systematic, structured responses; and second, the proposal of stage-level beam search method, enabling effective inference time scaling. Overall, LLaVA-o1 establishes new standard for multimodal reasoning in VLMs, offering robust performance and scalability, especially in inference time. Our work paves the way for future research on structured reasoning in VLMs, including potential expansions with external verifiers and the use of reinforcement learning to further enhance complex multimodal reasoning capabilities."
        },
        {
            "title": "References",
            "content": "[1] Detailed results on openvlm leaderboard. https : / / opencompass . openxlab . space / assets / OpenVLM.json. 6 [2] Claude 3.5 sonnet, 2024. Available at: https://www. anthropic.com/news/claude-3-5-sonnet. 8 [3] OpenAI (2024). Gpt-4o system card, 2024. 3, 4, 8 [4] Afra Amini, Tim Vieira, and Ryan Cotterell. Variational best-of-n alignment, 2024. 3 [5] Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, and Kazuhito Koishida. Neuro-symbolic visual reasoning: Disentangling. In International Conference on Machine Learning, pages 279290. Pmlr, 2020. [6] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational models defining new era in vision: survey and outlook. arXiv preprint arXiv:2307.13721, 2023. 1 [7] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 15111520, Gyeongju, Republic of Korea, 2022. International Committee on Computational Linguistics. 4 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision (ECCV), 2024. 4 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024. 3, 6 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2418524198, 2024. 8 [11] Minkyu Choi, Harsh Goel, Mohammad Omama, Yang, Shah, and Chinchali. Towards neuro-symbolic video unIn Proceedings of the European Conference derstanding. on Computer Vision (ECCV), Milan, Italy, pages 913. Springer, 2024. 3 [12] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. Navigate through enigmatic labyrinth survey of chain of thought reasoning: Advances, frontiers and future. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11731203, 2024. 3 [13] Adam Dahlgren Lindstrom and Savitha Sam Abraham. Clevr-math: dataset for compositional language, viIn International Joint sual and mathematical reasoning. Conference on Learning and Reasoning, 16th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy 2022), Windsor, UK, September 28-30, 2022, pages 155170. Technical University of Aachen, 2022. 4, 5 [14] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An opensource toolkit for evaluating large multi-modality models, 2024. 6 [15] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use lapa question answering benchmark with implicit reatop? soning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. 3 [16] Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012. 3 [17] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, and et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1437514385, 2024. 3, 6 [18] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. 3 [19] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. [20] Md Farhan Ishmam, Md Sakib Hossain Shovon, Muhammad Firoz Mridha, and Nilanjan Dey. From image to language: critical analysis of visual question answering (vqa) approaches, challenges, and opportunities. Information Fusion, page 102270, 2024. 3 [21] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 1, 3 [22] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3, 4 [23] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. pages 235251, 2016. 3, 4, 5, 6 [24] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukoˇsiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared 9 Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning, 2023. [25] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheusvision: Vision-language model as judge for fine-grained evaluation. arXiv preprint arXiv:2401.06591, 2024. 3 [26] Hao Li, Xu Li, Belhal Karimi, Jie Chen, and Mingming Sun. Joint learning of object graph and relation graph for visual question answering. In 2022 IEEE International Conference on Multimedia and Expo (ICME), pages 0106. IEEE, 2022. 3 [27] Hao Li, Jinfa Huang, Peng Jin, Guoli Song, Qi Wu, and Jie Chen. Weakly-supervised 3d spatial reasoning for text-based visual question answering. IEEE Transactions on Image Processing, 32:33673382, 2023. 3 [28] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankanhalli. People in social context (pisc) dataset, 2017. Data set. 4 [29] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. 3 [30] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2668926699, 2024. 1, [31] Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning, 2023. 2 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023. 1, 3 [33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. 3, 6 [34] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 4, 5 [35] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 3, 6 [36] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. [37] Mikołaj Małkinski and Jacek Mandziuk. review of emerging research directions in abstract visual reasoning. Information Fusion, 91:713736, 2023. 3 [38] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 4 [39] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2021. 4 [40] Meta AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https://ai.meta. com/blog/llama-3-2-connect-2024-visionedge-mobile-devices/, 2024. 1, 5, 8 [41] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https : / / openai . com / index / gpt - 4o - mini - advancing - cost - efficient - intelligence/, 2024. 8 [42] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: strategic progress report part 1, 2024. [43] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 8 [44] Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. Commonsense reasoning for natural language processing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 2733, 2020. 3 [45] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge, 2022. 4 [46] Ming Shen. Rethinking data selection for supervised finetuning. arXiv preprint arXiv:2402.06094, 2024. 3 [47] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. 1, 3 [48] Haoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and Furu Wei. Clip models are few-shot learners: Empirical studies on vqa and visual entailment. arXiv preprint arXiv:2203.07190, 2022. [49] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. 3 [50] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023. 2 [51] Xiaofei Wang, Jinhua Li, and Yifan Zhang. Improved value alignment in large language models using variational bestof-n techniques, 2024. 3 [52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 10 Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, [53] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification, 2023. 3 [54] Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, and J. H. Liu. comparative study on reasoning patterns of openais o1 model, 2024. 2 [55] Siheng Xiong, Yuan Yang, Ali Payani, James Kerce, and Faramarz Fekri. Teilp: Time prediction over knowledge graphs via logical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1611216119, 2024. 3 [56] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llavacritic: Learning to evaluate multimodal models, 2024. 3 [57] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023. 4 [58] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 8 [59] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224, 2024. [60] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024. 3, 6 [61] JD Zamfirescu-Pereira, Richmond Wong, Bjoern Hartmann, and Qian Yang. Why johnny cant prompt: how In non-ai experts try (and fail) to design llm prompts. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 121, 2023. 3 [62] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Improve vision language model chain-ofYiming Yang. thought reasoning, 2024. 1 [63] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, and et al. Evaluation of openai o1: Opportunities and challenges of agi, 2024. 1, 2,"
        }
    ],
    "affiliations": [
        "School of Electronic and Computer Engineering, Peking University",
        "Institute for Interdisciplinary Information Sciences, Tsinghua University",
        "Peng Cheng Laboratory",
        "AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School",
        "Alibaba DAMO Academy",
        "Computer Science and Engineering, Lehigh University"
    ]
}