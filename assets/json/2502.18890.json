{
    "paper_title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens",
    "authors": [
        "Tong Wu",
        "Junzhe Shen",
        "Zixia Jia",
        "Yuxuan Wang",
        "Zilong Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 9 8 8 1 . 2 0 5 2 : r February 27, 2025 From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Tong Wu , Junzhe Shen , Zixia Jia , Yuxuan Wang and Zilong Zheng (cid:0) NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University Equal contribution. Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target models inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3ˆ speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as scalable and effective solution at unprecedented lengths. Code can be found at github.com/bigai-nlco/TokenSwift. Figure 1. Comparison of the time taken to generate 100K tokens using autoregressive (AR) and TOKENSWIFT with prefix length of 4096 on LLaMA3.1-8b. As seen, TOKENSWIFT accelerates the AR process from nearly 5 hours to just 90 minutes. 1. Introduction Recent advances in large language models (LLMs), amplified by their long context capacities (Wu et al., 2024; Ding et al., 2024), have demonstrated remarkable proficiency in intricate reasoning (Jaech et al., 2024; Guo et al., 2025), agentic thinking (Shinn et al., 2023; Yao et al., 2023; Li et al., 2024a), and creative writing (Wang et al., Correspondence to: Zilong Zheng <zlzheng@bigai.ai>. 2025 BIGAI. All rights reserved. Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens 2023; Mikhaylovskiy, 2023), etc. These advancements necessitate the ability to generate lengthy sequences, e.g., o1-like (Jaech et al., 2024) reasoning tends to generate protracted chain-of-thought trajectories before reaching final conclusions. However, critical challenge impeding the practical deployment of such applications is the extensive time required to produce ultra-long sequences. For instance, generating 100K tokens with LLaMA3.18B can take approximately five hours (Figure 1), duration that is impractically long for the development of sophisticated applications, let alone recent gigantic models such as LLaMA3.1-405B (AI@Meta, 2024) and DeepSeek-600B (Liu et al., 2024a). Addressing this bottleneck is essential for harnessing the full potential of LLMs in real-world scenarios. straightforward solution is to take advantage of recent success in speculative decoding (SD) (Leviathan et al., 2023; Chen et al., 2023), which employs draft-then-verify strategy to expedite generation while preserving lossless accuracy; see Appendix and Section 5.1 for detailed background and relevant literature. However, these methods are generally tailored for generating short sequences, e.g., TriForce (Sun et al., 2024a) and MagicDec (Chen et al., 2024a) are limited to generating 256 and 64 tokens, respectively. Directly extending their generation length to 100K tokens would inevitably encounter failures due to KV cache budget constraints. Furthermore, even when applied to optimized KV cache architectures such as Group Query Attention (GQA), these methods yield only marginal acceleration gains for short-sequence generation, as evidenced in Tables 1 and 3. This observation leads to pivotal research question: Is it possible to achieve model-agnostic lossless accelerations, akin to those seen in short-sequence SDs, for generating ultra-long sequences, with minimal training overhead? To answer this question, we conduct an in-depth analysis (2) and identify three key challenges: (1) frequent model reloading: frequently reloading model for each token generation introduces significant delay, primarily due to memory access times rather than computation. (2) Prolonged Growing of KV Cache, the dynamic management of key-value (KV) pairs, which grow with the sequence length, adds complexity in maintaining model efficiency. (3) repetitive content generation, the issue of repetitive generation becomes more pronounced as the sequence length increases, leading to degraded output quality. Building on these insights, we introduce our framework TOKENSWIFT, which utilizes n-gram retrieval and dynamic KV cache updates to accelerate ultra-long sequence generation. Specifically, we employ multi-token generation and token reutilization to enable the LLM (i.e. target model) to draft multiple tokens in single forward pass, alleviating the first challenge of frequent model reloading (3.2). As the generation progresses, we dynamically update the partial KV cache at each iteration, reducing the KV cache loading time (3.3). Finally, to mitigate the issue of repetitive outputs, we apply contextual penalty to constrain the generation process, ensuring the diversity of output (3.4). In 4, we conduct extensive experiments to evaluate TOKENSWIFT across different model scales and architectures. In summary, we highlight our advantages as: 1. To the best of our knowledge, TOKENSWIFT is the first to accelerate ultra-long sequence generation up to 100K with lossless accuracy of target LLMs, while demonstrating significant superiority over enhanced baselines. 2. TOKENSWIFT consistently achieves over 3ˆ speedup compared to AR across varying prefix lengths, model architectures, and model scales in generating 100K tokens, reducing the AR process from nearly 5 hours to 90 minutes on LLaMA3.1-8b. 3. TOKENSWIFT achieves progressively higher speedup compared to AR as the generation length increases, while enhancing diversity in ultra-long sequence generation (as measured by Distinct-n (Li et al., 2016)). 2. Challenges Accelerating long sequence generation is nevertheless non-trivial task, even built upon prior success in speculative decoding (SD). In this section, we identify critical challenges encountered in accelerating ultra-long sequence generation. Challenge I: Frequent Model Reloading One fundamental speed obstacle lies in the autoregressive (AR) generation scheme of LLM. For each token, the entire model must be loaded from GPUs storage unit to the computing unit (Yuan et al., 2024), which takes significantly more time than the relatively small amount of 2 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens computation performed (as shown in Table 2). Consequently, the primary bottleneck in generation stems from I/O memory access rather than computation. Table 1. Experimental results of TriForce (Sun et al., 2024a) and MagicDec (Chen et al., 2024a) with default parameters on LLaMA3.1-8b. The Batch Size of MagicDec is set to 1. Method Gen. Len. Draft Form Speed Up TriForce MagicDec 256 64 Standalone Draft Self-Speculation Standalone Draft 1. 1.20 1.06 Table 2. Taking NVIDIA A100 80G and LLaMA3.1-8b as example, MAX refers to the scenario with maximum context window 128K. The calculation method is from Yuan et al. (2024). MEMORY COMPUTATION Bandwidth: 2.04e12 B/s Model Weights: 15.0 GB MAX Operations: 83.9 GB BF16: 312e12 FLOPS Loading Time: 7.4 ms MAX Computing Time: 0.3 ms When generating ultra-long sequence, such as 100K tokens, the GPU must reload the model weights over 100,000 times. This repetitive process poses the challenge: How can we reduce the frequency of model reloading? Challenge II: Prolonged Growing of KV Cache Previous studies, such as TriForce (Sun et al., 2024a) and MagicDec (Chen et al., 2024a) have demonstrated that, small KV cache budget can be used during the drafting phase to reduce the time increase caused by the loading enormous KV cache. While their one-time compression strategy at the prefill stage can handle scenarios with long prefixes and short outputs, it fails to address cases involving ultra-long outputs, as the growing size of KV cache would far exceed the allocated length budget. To dynamically manage partial KV cache within limited budget during ultra-long sequence generation, the challenge lies in determining when and how to dynamically update the KV cache. Challenge III: Repetitive Content Generation The degeneration of AR in text generation tasks characterized by output text that is bland, incoherent, or gets stuck in repetitive loops is widely studied challenge (Holtzman et al., 2020; Nguyen et al., 2024; Hewitt et al., 2022). When generating sequences of considerable length, e.g., 100K, the model tends to produce repetitive sentences (Figure 5). Since our objective is lossless acceleration and repetition is an inherent problem in LLMs, eliminating this issue is not our focus. However, it is still essential and challenging to mitigate repetition patterns in ultra-long sequences. 3. TOKENSWIFT To achieve lossless acceleration in generating ultra-long sequences, we propose tailored solutions for each challenge inherent to this process. These solutions are seamlessly integrated into unified framework, i.e. TOKENSWIFT. 3.1. Overview The overall framework is depicted in Figure 2. TOKENSWIFT generate sequence of draft tokens with selfdrafting, which are then passed to the target (full) model for validation using tree-based attention mechanism (See Appendix for more tree-based attention details). This process ensures that the final generated output aligns with the target models predictions, effectively achieving lossless acceleration. TOKENSWIFT is lightweight because the draft model is the target model itself with partial KV cache. This 3 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Figure 2. Illustration of TOKENSWIFT Framework. First, target model (LLM) with partial KV cache and three linear layers outputs 4 logits in single forward pass. Tree-based attention is then applied to construct candidate tokens. Secondly, top-k candidate 4-grams are retrieved accordingly. These candidates compose draft tokens, which are fed into the LLM with full KV cache to generate target tokens. The verification is performed by checking if draft tokens match exactly with target tokens (Algorithm 1). Finally, we randomly select one of the longest valid draft tokens, and update n-gram table and KV cache accordingly. eliminates the need to train separate draft LLM; instead, only γ linear layers need to be trained, where γ + 11 represents the number of logits predicted in single forward pass. In addition, during the verification process, once we obtain the target tokens from the target model with full KV cache, we directly compare draft tokens with target tokens sequentially to ensure that the process is lossless (He et al., 2024). 3.2. Multi-token Generation and Token Reutilization Multi-token Self-Drafting Inspired by Medusa (Cai et al., 2024), we enable the LLM to generate multiple draft tokens in single forward pass by incorporating γ additional linear layers. However, we empirically note that the additional linear layers should not be independent of each other. Specifically, we propose the following structure: h1 = f1(h0) + h0, h2 = f2(h1) + h1, h3 = f3(h2) + h2, l0, l1, l2, l3 = g(h0), g(h1), g(h2), g(h3), (1) where h0 denotes the last hidden state of LLM, fi() represents the i-th linear layer, hi refers to the i-th hidden representation, g() represents the LM Head of target model, and li denotes output logits. This structure aligns more closely with the AR nature of the model. Moreover, this adjustment incurs no additional computational cost. Token Reutilization Given the relatively low acceptance rate of using linear layers to generate draft tokens, we propose method named token reutilization to further reduce the frequency of model reloads. The idea behind token reutilization is that some phrases could appear frequently, and they are likely to reappear in subsequent generations. Specifically, we maintain set of tuples t(G, )u, where = txi+1, ..., xi+nu represents an n-gram and denotes its corresponding frequency within the generated token sequence = tx0, x1, ..., xt1u by time step (t ě n). After obtaining tp0, . . . , p3u as described in 3.4, we retrieve the top-k most frequent n-grams beginning with 1The target model itself can also predict one logit, making the total number of logits γ + 1. We take γ = 3. 4 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens token arg max p0 to serve as additional draft tokens. Although this method can be applied to tasks with long prefixes, its efficacy is constrained by the limited decoding steps, which reduces the opportunities for accepting n-gram candidates. Additionally, since the long prefix text is not generated by the LLM itself, distributional discrepancy exists between the generated text and the authentic text (Mitchell et al., 2023). As result, this method is particularly suitable for generating ultra-long sequences. 3.3. Dynamic KV Cache Management Dynamic KV Cache Updates Building upon the findings of Xiao et al. (2024), we preserve the initial KV pairs within the cache during the drafting process, while progressively evicting less important KV pairs. Specifically, we enforce fixed budget size B, ensuring that the KV cache at any given time can be represented as: KV = t(K0, V0), ..., (KS, VS), (KS+1, VS+1), ..., (KB1, VB1)u, where the first pairs remain fixed, and the pairs from position to 1 are ordered by decreasing importance. As new tokens are generated, less important KV pairs are gradually replaced, starting from the least important ones at position 1 and moving towards position S. Once replacements extend beyond the position, we recalculate the importance scores of all preceding tokens and select the most relevant S pairs to reconstruct the cache. This process consistently preserves the critical information required for ultra-long sequence generation. Importance Score of KV pairs We rank the KV pairs based on the importance scores derived from the dot product between the query (Q) and key (K), i.e. QKT. In the case of Group Query Attention (GQA), since each corresponds to group of = tQ0, ..., Qg1u, direct dot-product computation is not feasible. Unlike methods such as SnapKV (Li et al., 2024c), we do not replicate the K. Instead, we partition the Q, as shown in Equation (2): ((i+1)g)1ÿ importance scorei = Qj Ki, j=ig (2) where for position i, Qj in the group Qi are dot-product with the same Ki, and their results are aggregated to obtain the final importance score. This approach enhances memory saving while preserving the quality of the attention mechanism, ensuring that each query is effectively utilized without introducing unnecessary redundancy. 3.4. Contextual Penalty and Random N-gram Selection Contextual Penalty To mitigate repetition in generated text, we have explored various sampling strategies. However, with the significantly larger sequence length, the likelihood of repetition increases significantly (2). As result, we decided to apply an additional penalty to the generated tokens to further mitigate repetition. The penalized sampling approach proposed in (Keskar et al., 2019) suggests applying penalty to all generated tokens. However, when generating ultra-long sequences, the set of generated tokens may cover nearly all common words, which limits the ability to sample appropriate tokens. Therefore, we propose an improvement to this method. Specifically, we introduce fixed penalty window and apply penalty value θ to the most recent tokens, denoted as W, generated up to the current position, as illustrated in Equation (3): pi = exp (cid:0)li/(t I(li))(cid:1) ř exp (cid:0)lj/(t I(lj))(cid:1) , I(l) = θ if else 1.0, θ (1, 8), (3) where denotes temperature, li and pi represent the logit and probability of i-th token. This adjustment aims to maintain diversity while still mitigating repetitive generation. 5 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Algorithm 1 TOKENSWIFT Require: Prompt p, target model M, decoding tree T, n-gram candidate number k; max budget size of partial cache, cache initial size S. if (C ull e) ą S then 1: Prefill target model with KV cache ull Ð Pre illM(p), s.t. ull = len(p); 2: Prefill partial KV cache Cp Ð tC ull[0 : S], Top-KBS(C ull)u w.r.t Equation (2), where Cp = B; 3: st Ð 0, Ð len(p). 4: while st ď target length do 5: 6: 7: 8: Multi-token Parallel Generation: Get penalized probability pď3 9: 10: Dynamic KV Cache Update: Cp Ð tC ull[: S], Top-KBS(C ull)u, Ð ull. 2with partial cache Cp. ď3ug end if Tree-based Attention: Construct groups of candidate draft tokens txi Token Reutilization: Select n-gram candidates tai Parallel Verification: Let draft tokens tdiuk+g i=1 := tai probabilities tqiuk+g i=1 . Sample target tokens tyiuk+g Random select the longest accepted length of draft tokens Let st Ð st + len(yi); yield: yi Evict Cp to ensure the size of Cp is and update ull. i=1 tqiuk+g i=1 . ď3uk ď3uk ď3ug ďm tdďmdi 11: 12: 13: 14: 15: 16: end while ďm = yi ďmuk+g i=1 by exactly match. i=1 using decoding tree and pď3. i=1 with highest frequency, where a0 = arg max p0 (3.2). i=1 txi i=1, and send tdu to to get penalized verification Random n-gram Selection In our experiments, we observe that the draft tokens provided to the target model for parallel validation often yield multiple valid groups. Building on this observation, we randomly select one valid n-gram to serve as the final output. By leveraging the fact that multiple valid n-grams emerge during verification, we ensure that the final output is both diverse and accurate. In summary, the overall flow of our framework is presented in Algorithm 1. 4. Experiments In this section, we demonstrate the capability of TOKENSWIFT in accelerating ultra-long sequences generation. 4.1. Setup We conduct experiments on variety of models, including YaRN-LLaMA2-7b-128k (Peng et al., 2024), LLaMA3.1-8b (AI@Meta, 2024) and Qwen2.5-(1.5b,7b,14b) (Qwen et al., 2025). For all models, we use the Base version, as the output length of Instruct version is limited (Bai et al., 2024). The inference experiments are performed on the test set of PG-19 (Rae et al., 2020). Training and Inference Details We train linear layers in Section 3.2 using the first 8K tokens of training data, for datasets longer than 8K tokens, from PG-19 (Rae et al., 2020). The number of extra decoding heads is set to 3 across all models. Inference is performed on single NVIDIA A100-SXM4-80GB. When generating 100K tokens, the models are prefilled with 2K, 4K or 8K tokens as prompt from random sample of the PG-19 test set (See Appendix F.2 for ablation on prefill length). The maximum budget of the partial cache is determined by the length of the prompt. For further training and inference details, please refer to Appendix B. Evaluation Metrics We evaluate the overall acceptance rate and speedup for all methods. Unlike Leviathan et al. (2023)3, our acceptance rate α is defined as: 2The subscript ď 3 here denotes tuple with indices 0, 1, 2, 3. The notation will be used similarly hereafter. 3The two can be converted into each other through computation. 6 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Table 3. Experimental results for LLaMA2 and LLaMA3.1 under varying prefix lengths, generating sequences from 20K to 100K tokens. α denotes the acceptance rate of all draft tokens (Equation (4)), while ˆ represents the speedup ratio relative to AR (Equation (5)). TriForce* refers to our improved version, and Medusa* indicates the model we retrained (4.1). Prefill Len. 2048 Prefill Len. Prefill Len. 8192 Prefill Len. 2048 Prefill Len. 4096 Prefill Len. 8192 Method Gen. Len. YaRN-LLaMA2-7b-128k (MHA) LLaMA3.1-8b (GQA) Medusa* TriForce* TOKENSWIFT Medusa* TriForce* TOKENSWIFT Medusa* TriForce* TOKENSWIFT Medusa* TriForce* TOKENSWIFT Medusa* TriForce* TOKENSWIFT 20K 40K 60K 80K 100K α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) 0.43 0.80 0.730.09 0.52 0.84 0.820.06 0.59 0.85 0.870.04 0.61 0.84 0.890.03 0.62 0.82 0.900.02 0.96 1.50 2.110. 1.08 1.64 2.600.05 1.18 1.76 2.920.04 1.17 1.86 3.130.04 1.15 1.94 3.250.05 0.39 0.89 0.680.09 0.42 0.93 0.790. 0.47 0.95 0.850.04 0.51 0.95 0.880.04 0.52 0.96 0.900.03 0.85 1.51 2.020.20 0.86 1.67 2.560.09 0.95 1.83 2.890. 0.99 1.98 3.100.06 0.99 2.14 3.230.06 0.40 0.92 0.640.08 0.43 0.96 0.790.05 0.45 0.97 0.850.04 0.47 0.97 0.880. 0.47 0.97 0.900.02 0.83 1.36 1.910.12 0.88 1.49 2.500.07 0.91 1.62 2.840.05 0.93 1.74 3.050.03 0.91 1.86 3.200. 0.35 0.89 0.640.08 0.35 0.93 0.720.07 0.35 0.94 0.750.06 0.35 0.95 0.770.04 0.35 0.95 0.790.03 1.20 1.13 1.870. 1.26 1.18 2.390.16 1.29 1.21 2.730.13 1.30 1.23 2.960.07 1.31 1.25 3.130.07 0.39 0.89 0.650.07 0.40 0.94 0.730. 0.40 0.95 0.790.06 0.40 0.95 0.820.06 0.41 0.96 0.840.05 1.29 1.08 1.930.18 1.39 0.99 2.470.22 1.42 0.96 2.880. 1.43 0.94 3.130.16 1.45 0.92 3.270.19 0.34 0.99 0.720.09 0.34 0.99 0.810.10 0.34 1.00 0.850.08 0.34 1.00 0.880. 0.34 0.99 0.900.06 1.21 1.16 1.990.20 1.26 1.18 2.540.22 1.29 1.19 2.930.17 1.29 1.21 3.190.13 1.29 1.22 3.380. ř i=1 ai (γ + 1) ˆ , α = (4) where ai represents the number of tokens accepted at the i-th time step, γ + 1 denotes the number of draft tokens generated at each time step, and represents the total number of time steps. The speedup is denotes as ˆ, which is the ratio of AR latency to TOKENSWIFT latency, given by: ˆ = latencyAR latencyTOKENSWIFT , (5) where latency refers to the average time required to generate single token. We use Distinct-n (Li et al., 2016) to measure the diversity of generated content, i.e., repetition. higher value indicates greater diversity and lower repetition  (Table 6)  . Baselines We compare TOKENSWIFT with two baselines: TriForce*: The original TriForce (Sun et al., 2024a) employs static KV update strategy, which cannot accelerate the generation of 100K tokens. The results in Table 3 correspond to our improved version of TriForce, which incorporates dynamic KV update 4. Medusa*: To ensure losslessness, we adopt the Medusa (Cai et al., 2024) training recipe and incorporate the verification method of TOKENSWIFT. Both Medusa heads and tree structure are consistent with TOKENSWIFT. The recent released MagicDec (Chen et al., 2024a) primarily focuses on acceleration for large throughput, and when the batch size is 1, LLaMA3.1-8b does not exhibit any acceleration for short text generation, let alone for ultra-long sequences. Therefore, it is excluded from our baseline. 4.2. Main Results The experimental results are presented in Table 3 and Table 4. We evaluate TOKENSWIFT at different generation lengths of 20K, 40K, 60K, 80K and 100K, reporting speedup ˆ and acceptance rate α by taking the average and standard deviation of 5 experiments to avoid randomness. Notably, the results for TOKENSWIFT and Medusa* show balanced trade-off between speed and quality, in contrast to TriForce*, which suffers from low quality due to the absence of any repetition penalty. TOKENSWIFT significantly outperforms all baselines across generation lengths. As shown in Table 3, across all lengths, TOKENSWIFT demonstrates superior acceleration performance compared to all baselines on models with 4To compare with LLaMA3.1-8b, we pretrained draft model based on LLaMA3.1-8b. See Appendix for details. 7 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Table 4. Experimental results of TOKENSWIFT for Qwen2.5 across different scales under prefix length 4096, generating sequences from 20K to 100K tokens. TAR and TTOKENSWIFT denote the actual time required (in minutes) for AR and TOKENSWIFT, respectively. represents the number of minutes saved by TOKENSWIFT compared to AR. Gen. Len. α Qwen2.5-1.5B TAR ˆ(ą 1) TTOKENSWIFT α Qwen2.5-7B TAR ˆ(ą 1) TTOKENSWIFT α Qwen2.5-14B TAR ˆ(ą 1) TTOKENSWIFT 20K 40K 60K 80K 100K 0.690.11 1.690.17 12.00 0.800.06 2.310.09 36.00 0.850.04 2.690.07 73.80 0.870.03 2.950.06 124.20 0.890.07 3.130.07 187.80 7.20 15.60 27.60 42.00 60. -4.80 0.640.07 2.000.16 15.60 -20.40 0.770.05 2.640.10 47.40 -46.20 0.780.08 2.860.25 95.40 -82.20 0.800.09 3.070.30 161.40 -127.80 0.820.09 3.230.28 244.20 7.80 18.00 33.60 52.80 75.60 -7.80 0.670.06 2.120.13 29.40 -29.40 0.780.03 2.680.10 89.40 -61.80 0.820.02 3.010.13 184.20 -108.60 0.830.02 3.200.13 312.60 -168.60 0.840.02 3.340.10 474.60 13.80 33.60 61.20 97.80 142.20 -15.60 -55.80 -123.00 -214.80 -332.40 Figure 3. Upper: The acceptance rate α for = 20 and = 0, along with the n-gram acceptance rate β for = 20, plotted against varying generation lengths. Lower: The speedup ˆ achieved at different generation lengths. different architectures (MHA, GQA). Moreover, TOKENSWIFT demonstrates remarkable robustness, showing virtually no impact when tested with varying prefix lengths. Longer generations amplify the speedup benefits. As the generation length increases, the speed improvement of TOKENSWIFT becomes increasingly evident. Two key factors drive this trend: Firstly, AR experiences longer KV cache loading times as the number of tokens grows, whereas TOKENSWIFT mitigates this issue by utilizing dynamic KV pruning. Secondly, the acceptance rate improves as the number of tokens increases, primarily due to the higher n-grams acceptance rate. As the n-grams pool composed of generated tokens grows larger, the candidate n-grams become more diverse and accurate (Figure 3). Larger models yield greater speedup benefits. The impact of frequent model reloading varies with model scale, as larger models require more time due to the increased parameters. As shown in Table 4, TOKENSWIFT demonstrates robust performance across models of different scales, with the acceleration advantage becoming more pronounced for larger models. In particular, when generating 100K tokens, TOKENSWIFT saves up to 5.54 hours for 14B model. 4.3. Ablation Studies We conduct comprehensive ablation studies on TOKENSWIFT using LLaMA3.1-8b. For all experiments, the prefix length is 4096. 8 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens 4.3.1. TOKEN REUTILIZATION We define the n-gram acceptance rate β similarly to Equation (4). Let a1 candidate at iteration i. Then β is given by: denote the length of accepted n-gram ř i=1 bi (γ + 1) ˆ β = , where, bi = # a1 i, 0, a1 = ai a1 ă ai . (6) From Figure 3, we observe that removing token reutilization (k = 0) leads to significant decrease in both acceptance rate α and speedup ˆ. Furthermore, as the generation length increases, the acceptance rate α for = 0 slightly drops. This trend stems from the fact that, in ultra-long sequences, the KV cache cannot be compressed indefinitely. In contrast, TOKENSWIFT (k = 20) shows an increasing acceptance rate as the sequence length grows, demonstrating the effectiveness of token reutilization in reducing the frequency of model reloading. Table 5. The ablation experiment results on KV management. Gen. Len. Full Cache Partial Cache Dynamic Partial Cache 20K 40K 60K 80K 100K α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) 0.42 1.36 0.42 1.42 0.42 1.45 0.42 1.46 0.42 1.47 0.19 0. 0.16 1.03 0.18 1.19 0.19 1.31 0.21 1.44 4.3.2. DYNAMIC KV UPDATES 0.45 1. 0.43 1.75 0.42 1.88 0.42 1.97 0.40 1.96 Table 6. The ablation experiment results on contextual penalty using different sampling methods. Light cell represents the settings adopted by TOKENSWIFT. We take θ = 1.2, = 1024. Distinct-1 Distinct-2 Distinct-3 Distinct-4 AVG. ˆ top-p w/o. penalty η-sampling w/o. penalty min-p w/o. penalty 0.15 0.09 0.25 0.06 0.41 0.07 0.25 0.15 0.43 0. 0.71 0.11 0.29 0.18 0.49 0.12 0.81 0.14 0.31 0.20 0.53 0. 0.82 0.15 0.25 0.16 0.43 0.11 0.69 0.12 3.42 3.53 3.42 3. 3.27 3.58 To evaluate the effectiveness of TOKENSWIFTs dynamic KV update policy, we experiment with three different strategies of managing KV cache during drafting: Full Cache: Retaining full KV cache throughout drafting. Partial Cache: Updating partial KV cache only once during the prefill phase. Dynamic Partial Cache: Dynamically updating KV cache as described in 3.3 For fair comparison, token reutilization is disabled (i.e. = 0). As shown in Table 5, Partial Cache leads to low acceptance rate, resulting in reduced speedup. While Full Cache achieves higher acceptance rate, its computational overhead negates the speedup gains. In contrast, Dynamic Partial Cache adopted by TOKENSWIFT strikes balanced trade-off, achieving both high acceptance rate and significant speedup. As result, Dynamic Partial Cache can effectively manage partial KV under ultra-long sequence generation. 4.3.3. CONTEXTUAL PENALTY As an orthogonal method to min-p, top-p, and η-sampling for mitigating the repetition, contextual penalty demonstrates effectiveness across different sampling methods. As shown in Table 6, without contextual penalty, the diversity of generated sequences is significantly lower for all sampling methods. The most striking improvement emerges in min-p sampling (See Appendix for more sampling details), where the average Distinct-n score surges from 0.12 to 0.69 with only an 8% compromise in speedup. These results clearly highlight the impact of contextual penalty in mitigating repetitive token generation. It can seamlessly integrate with existing sampling methods to enhance the quality of ultra-long sequence generation. In addition, we can find that the higher the diversity, the lower the speedup. Therefore, if TriForce is combined with context penalty, the speedup in Table 3 will drop further. 9 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Figure 4. Upper: The acceptance rate α and n-gram acceptance rate β versus varying k. Lower: The speedup ˆ versus varying k. Table 7. Acceptance rate α (k = 0) and speedup ˆ across different tree configurations. Each configuration is represented by 4-digit array: they represent the number of candidates for different decoding heads in 3.2. Gen. Len. [3,3,3,3] [1,9,9,9] [1,3,3,3] (Ours) 20K 40K 60K 80K 100K α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) 0.44 1.34 0.43 1. 0.43 1.75 0.43 1.85 0.42 1.91 0.50 0.53 0.52 0.67 0.53 0. 0.55 0.88 0.57 0.96 0.45 1.56 0.43 1.75 0.42 1.88 0.42 1. 0.40 1.96 Table 8. Distinct-n score across different penalty value θ. 1.0 indicate that no penalty is applied. We take = 1024 (See Appendix F.3 for ablation on W). θ 1.0 1.1 1.2 1.3 1.4 1.5 Distinct-1 Distinct-2 Distinct-3 Distinct-4 AVG. 0.07 0.08 0.41 0.57 0.52 0. 0.11 0.13 0.71 0.86 0.73 0.96 0.14 0.15 0.81 0.93 0.76 0.98 0.15 0.16 0.82 0.95 0.77 0.99 0.12 0.13 0.69 0.83 0.70 0.92 4.4. Discussions In this section, we explore the effects of different hyperparameters on TOKENSWIFT. 4.4.1. TREE CONFIGURATION Due to the time-consuming nature of finding the optimal tree in Medusa (Cai et al., 2024) and its limited impact on accelerating ultra-long sequences generation, we employ simple 3-ary tree in tree attention. See Appendix for the tree structure. As shown in Table 7, [1,9,9,9] has the highest acceptance rate but the lowest speedup. This is because more candidates increase the acceptance rate, but also increase the verification burden. Similarly, by comparing [1,3,3,3] and [3,3,3,3], we can find that the first head (i.e., the original head of target model) achieves relatively high prediction accuracy when using KV compression, so choosing the top-1 token as candidate is sufficient. To balance the trade-off of acceptance rate and verification efficiency, we adopt [1,3,3,3] as the configuration of TOKENSWIFT. 10 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens 4.4.2. N-GRAM CANDIDATES As illustrated in Figure 4, increasing enhances the n-gram acceptance rate β due to larger pool of n-gram candidates. However, an excessive number of candidates can strain the verification process, leading to reduced speedup ˆ. Interestingly, lower does not always result in lower β. For instance, = 5 achieves higher β than = 20, resulting in both higher acceptance rate α and greater speedup ˆ. However, at = 5, the lack of diversity among the candidates leads to increased repetition, which in turn degrades the quality of generation. 4.4.3. PENALTY VALUE θ As key component of TOKENSWIFT, contextual penalty significantly reduces repetition in generated text. We examine the effect of two parameters present in contextual penalty, i.e. penalty value θ and penalty window W. Table 8 presents the impact of introducing contextual penalty on diversity. Without any penalty (θ = 0), the generated sequences exhibit severe repetition, with an average Distinct-n score of only 0.12. As the value of θ increases gradually to 1.2, the diversity improves significantly, highlighting the effectiveness of contextual penalty in enhancing the diversity of ultra-long sequence generation. 4.4.4. CASE STUDY Figure 5 presents case study on the impact of the contextual penalty. Without the Contextual Penalty, repetitions appear at about 5K tokens, compared to 60K with the penalty applied. Additionally, generation without the penalty exhibits word-for-word repetition, whereas generation with the penalty primarily demonstrates semanticlevel repetition, highlighting its effectiveness in mitigating redundancy. 5. Related Works 5.1. Speculative Decoding Recent advancements in speculative decoding have significantly accelerated large language model (LLM) inference through diverse methodologies. Speculative decoding (Leviathan et al., 2023; Chen et al., 2023) traditionally leverages smaller draft models to propose candidate tokens for verification by the target model. Early works like SpecTr (Sun et al., 2023) introduced optimal transport for multi-candidate selection, while SpecInfer (Miao et al., 2024) and Medusa (Cai et al., 2024) pioneered tree-based structures with tree-aware attention and multi-head decoding to enable parallel verification of multiple candidates. Subsequent innovations, such as Sequoia (Chen et al., 2024b) and EAGLE-2 (Li et al., 2024d), optimized tree construction using dynamic programming and reordering strategies, while Hydra (Ankner et al., 2024) and ReDrafter (Cheng et al., 2024) enhanced tree dependencies through sequential or recurrent heads. Hardware-aware optimizations, exemplified by SpecExec (Svirschevski et al., 2024) and Triforce (Sun et al., 2024a), further improved efficiency by leveraging hierarchical KV caching and quantized inference. Self-speculative approaches eliminate the need for external draft models by exploiting internal model dynamics. Draft&Verify (Zhang et al., 2024) and LayerSkip (Elhoushi et al., 2024) utilized early-exit mechanisms and Bayesian optimization to skip layers adaptively, whereas Kangaroo (Liu et al., 2024b) integrated dual early exits with lightweight adapters. Sun et al. (2024b) and SpecDec++ (Huang et al., 2024) introduced theoretical frameworks for block-level token acceptance and adaptive candidate lengths. Parallel decoding paradigms, such as PASS (Monea et al., 2023) and MTJD (Qin et al., 2024), employed look-ahead embeddings or joint probability modeling to generate multiple candidates in single pass, while CLLMs (Kou et al., 2024) and Lookahead (Fu et al., 2024) reimagined autoregressive consistency through Jacobi decoding and n-gram candidate pools. Retrieval-augmented methods like REST (He et al., 2024), and NEST (Li et al., 2024b) integrated vector or phrase retrieval to draft context-aware tokens, often combining copy mechanisms with confidence-based attribution. Training-centric strategies, including TR-Jacobi (Wang et al., 2024a), enhanced parallel decoding capability via noisy training or self-distilled multi-head architectures. System-level optimizations such as PipeInfer (Butler et al., 2024) and Narasimhan et al. (2024) addressed scalability through asynchronous pipelines and latencyaware scheduling, while Goodput (Liu et al., 2024c) focused on dynamic resource allocation and nested model 11 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Figure 5. Case Study on LLaMA3.1-8b. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The blue text is repetition part. See Appendix for more cases. deployment. Approaches such as Triforce (Sun et al., 2024a) and MagicDec (Chen et al., 2024a) incorporate KV cache compression during the drafting phase. However, their applicability is limited to scenarios characterized by long prefixes and short outputs, making them unsuitable for ultra-long sequence generation tasks. In such tasks, which are the focus of our work, the need for efficient inference spans both extended input contexts and lengthy outputs, presenting challenges that existing methods fail to address. 5.2. Long Sequence Generation Recent advances in long sequence generation have focused on addressing the challenges of coherence, efficiency, and scalability in producing extended outputs. pivotal contribution is the LongWriter (Bai et al., 2024) framework, which introduces task decomposition strategy to generate texts exceeding 20,000 words. Complementing this, Temp-Lora (Wang et al., 2024b) proposes inference-time training with temporary Lora modules to dynamically adapt model parameters during generation, offering scalable alternative to traditional 12 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens KV caching. Similarly, PLANET (Hu et al., 2022) leverages dynamic content planning with sentence-level bag-ofwords objectives to improve logical coherence in opinion articles and argumentative essays, demonstrating the effectiveness of structured planning in autoregressive transformers. In addition, lightweight decoding-side sampling strategies have emerged for repetition mitigation. The foundational work on Nucleus Sampling (Holtzman et al., 2020) first demonstrated that dynamically truncating low-probability token sets could reduce repetitive outputs while maintaining tractable decoding latency. Building on this, Hewitt et al. (2022) introduced η-sampling explicitly linking candidate set reduction to repetition mitigation by entropy-guided token pruning. Recent variants like Min-p (Nguyen et al., 2024) optimize truncation rules in real-timescaling thresholds to the maximum token probability. And Mirostat Sampling (Basu et al., 2021) further integrate lightweight Bayesian controllers to adjust η parameters on-the-fly. Our work systematically analyzing how parameterized sampling (e.g., Top-p Min-p, η-sampling) balances computational overhead and repetition suppression in ultra-long sequence generation pipelines. 6. Conclusion In this study, we introduce TOKENSWIFT, novel framework designed to achieve lossless acceleration in generating ultra-long sequences with LLMs. By analyzing and addressing three challenges, TOKENSWIFT significantly enhances the efficiency of the generation process. Our experimental results demonstrate that TOKENSWIFT achieves over 3ˆ acceleration across various model scales and architectures. Furthermore, TOKENSWIFT effectively mitigates issues related to repetitive content, ensuring the quality and coherence of the generated sequences. These advancements position TOKENSWIFT as scalable and effective solution for ultra-long sequence generation tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Haoyi Wu from ShanghaiTech University, Xuekai Zhu from Shanghai Jiaotong University, Hengli Li from Peking University for helpful discussions on speculative decoding and language modeling. This work presented herein is supported by the National Natural Science Foundation of China (62376031)."
        },
        {
            "title": "References",
            "content": "AI@Meta. The llama 3 herd of models, 2024. URL https://ai.meta.com/research/publications/the-llama -3-herd-of-models. Ankner, Z., Parthasarathy, R., Nrusimha, A., Rinard, C., Ragan-Kelley, J., and Brandon, W. Hydra: Sequentiallydependent draft heads for medusa decoding. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=FbhjirzvJG. Bai, Y., Zhang, J., Lv, X., Zheng, L., Zhu, S., Hou, L., Dong, Y., Tang, J., and Li, J. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. Basu, S., Ramachandran, G. S., Keskar, N. S., and Varshney, L. R. {MIROSTAT}: {neural} {text} {decoding} {algorithm} {that} {directly} {controls} {perplexity}. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=W1G1JZEIy5_. Butler, B., Yu, S., Mazaheri, A., and Jannesari, A. Pipeinfer: Accelerating llm inference using asynchronous pipelined speculation. In SC24: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 119, 2024. doi: 10.1109/SC41406.2024.00046. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. In Forty-first International Conference on Machine Learning, volume abs/2401.10774, 2024. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. 13 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Chen, J., Tiwari, V., Sadhukhan, R., Chen, Z., Shi, J., Yen, I. E.-H., and Chen, B. Magicdec: Breaking the LatencyThroughput Tradeoff for Long Context Generation with Speculative Decoding. arXiv, abs/2408.11049, 2024a. Chen, Z., May, A., Svirschevski, R., Huang, Y.-H., Ryabinin, M., Jia, Z., and Chen, B. Sequoia: Scalable and robust speculative decoding. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=rk2L9YGDi2. Cheng, Y., Zhang, A., Zhang, X., Wang, C., and Wang, Y. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919, 2024. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. LongroPE: Extending LLM context window beyond 2 million tokens. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=ONOtpXLqqw. Elhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B., Wasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal, S., Roman, A., Aly, A., Chen, B., and Wu, C.-J. LayerSkip: Enabling early exit inference and self-speculative decoding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1262212642, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.681. URL https://aclant hology.org/2024.acl-long.681/. Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the sequential dependency of LLM inference using lookahead decoding. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/for um?id=eDjvSFOkXw. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, Z., Zhong, Z., Cai, T., Lee, J., and He, D. REST: Retrieval-based speculative decoding. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 15821595, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.88. URL https://aclanthology.org/2024.naacl-long.88/. Hewitt, J., Manning, C. D., and Liang, P. Truncation sampling as language model desmoothing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 34143427, 2022. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygGQyrF vH. Hu, Z., Chan, H. P., Liu, J., Xiao, X., Wu, H., and Huang, L. PLANET: Dynamic content planning in autoregressive transformers for long-form text generation. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 22882305, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.163. URL https://aclanthology.org/2022.acl-long.163/. Huang, K., Guo, X., and Wang, M. Specdec++: Boosting speculative decoding via adaptive candidate lengths. arXiv preprint arXiv:2405.19715, 2024. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. Ctrl: conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019. Kou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. CLLMs: Consistency large language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=8uzBOVmh8H. 14 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. diversity-promoting objective function for neural conversation models. In Knight, K., Nenkova, A., and Rambow, O. (eds.), Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 110 119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://aclanthology.org/N16-1014/. Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., and Zheng, Z. Ram: Towards an ever-improving memory system by learning from communications, 2024a. URL https://arxiv.org/abs/2404.12045. Li, M., Chen, X., Holtzman, A., Chen, B., Lin, J., tau Yih, W., and Lin, X. V. Nearest neighbor speculative decoding for LLM generation and attribution. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=Ni9kebsSTt. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. URL https://openreview.net/forum?id=poE54GOq2l. Li, Y., Wei, F., Zhang, C., and Zhang, H. EAGLE-2: Faster inference of language models with dynamic draft trees. In Empirical Methods in Natural Language Processing, 2024d. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, F., Tang, Y., Liu, Z., Ni, Y., Tang, D., Han, K., and Wang, Y. Kangaroo: Lossless self-speculative decoding for accelerating LLMs via double early exiting. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=lT3oc04mDp. Liu, X., Daniel, C., Hu, L., Kwon, W., Li, Z., Mo, X., Cheung, A., Deng, Z., Stoica, I., and Zhang, H. Optimizing speculative decoding for serving large language models using goodput. arXiv preprint arXiv:2406.14066, 2024c. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, pp. 932949, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703867. doi: 10.1145/3620666.3651335. URL https://doi.org/10.1145/3620666.3651335. Mikhaylovskiy, N. Long story generation challenge. In Mille, S. (ed.), Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges, pp. 1016, Prague, Czechia, September 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.inlg-genchal.2/. Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., and Finn, C. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, pp. 2495024962. PMLR, 2023. Monea, G., Joulin, A., and Grave, E. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023. Narasimhan, H., Jitkrittum, W., Rawat, A. S., Kim, S., Gupta, N., Menon, A. K., and Kumar, S. Faster cascades via speculative decoding. arXiv preprint arXiv:2405.19261, 2024. Nguyen, M., Baker, A., Kirsch, A., and Neo, C. Min sampling: Balancing creativity and coherence at high temperature. arXiv e-prints, pp. arXiv2407, 2024. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.n et/forum?id=wHBfxhZu1u. 15 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Qin, Z., Hu, Z., He, Z., Prakriya, N., Cong, J., and Sun, Y. Optimized multi-token joint decoding with auxiliary model for llm inference. arXiv preprint arXiv:2407.09722, 2024. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=SylKikSYDH. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 86348652. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e9 0-Paper-Conference.pdf. Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. In First Conference on Language Modeling, 2024a. URL https://openre view.net/forum?id=HVK6nl3i97. Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and Yu, F. Spectr: Fast speculative decoding via optimal transport. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3022230242. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/6034a661584af6c28fd97a6f23e56c0 a-Paper-Conference.pdf. Sun, Z., Ro, J. H., Beirami, A., and Suresh, A. T. Optimal block-level draft verification for accelerating speculative decoding. arXiv preprint arXiv:2403.10444, 2024b. Svirschevski, R., May, A., Chen, Z., Chen, B., Jia, Z., and Ryabinin, M. Specexec: Massively parallel speculative decoding for interactive LLM inference on consumer devices. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=JAhNsZ9dvG. Wang, Y., Yang, K., Liu, X., and Klein, D. Improving pacing in long-form story planning. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=KUSzNKRI2g. Wang, Y., Luo, X., Wei, F., Liu, Y., Zhu, Q., Zhang, X., Yang, Q., Xu, D., and Che, W. Make some noise: Unlocking language model parallel inference capability through noisy training. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1291412926, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.718. URL https://aclanthology.org/2024.emnlp-main.718/. Wang, Y., Ma, D., and Cai, D. With greater text comes greater necessity: Inference-time training helps long text generation. In First Conference on Language Modeling, 2024b. URL https://openreview.net/forum?id=dj9x 6JuiD5. Wu, T., Zhao, Y., and Zheng, Z. An efficient recipe for long context extension via middle-focused positional encoding. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=aNHEqFMS0N. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?i d=NG7sS51zVF. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. 16 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Yuan, Z., Shang, Y., Zhou, Y., Dong, Z., Zhou, Z., Xue, C., Wu, B., Li, Z., Gu, Q., Lee, Y. J., et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G., and Mehrotra, S. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11263 11282, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl -long.607. URL https://aclanthology.org/2024.acl-long.607/. 17 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens A. Lossless Nature of Speculative Decoding The speculative decoding (Leviathan et al., 2023; Chen et al., 2023) can easily be justified to be lossless and identical to sample from qtarget alone, i.e., pSD = qtarget. Note that, given prefix X1:j, the next token sampled from: # xj+1 pdra t(xX1:j), norm(max(0, qtarget(xX1:j) pdra t( ˆxX1:j))), otherwise, if (0, 1) ą α, where α is the acceptance rate given by (cid:32) α(x) = min 1.0, qtarget(x) pdra t(x) (cid:33) . If the draft token is accepted, we have pSD(xX1:j; accepted) = pdra t(xX1:j)α(xX1:j) = min(pdra t, qtarget). If the token is rejected, we have pSD(xX1:j; rejected) = (1 α(xX1:j))norm(max(0, qtarget(xX1:j) pdra t( ˆxX1:j))) = (1 α) qtarget min(pdra t, qtarget) 1 α = qtarget min(pdra t, qtarget) Therefore, the overall probability is given by pSD(xX1:j) = pSD(xX1:j; accepted) + pSD(xX1:j; rejected) = qtarget((xX1:j) Proved. B. Additional Training and Inference Details. B.1. Training Details During training, only three linear layers are fine-tuned, while the parameters of the LLM remained fixed. The model was trained on an NVIDIA A100-SXM4-80GB GPU. The specific training parameters are outlined in Table 9. Table 9. Additional training details. Note that these hyperparameters do not require extensive tuning. LLaMA3.1-8b YaRN-LLaMA2-7b-128k Qwen2.5-1.5b Qwen2.5-7b Qwen2.5-14b optimizer betas weight decay warmup steps learning rate scheduler num. GPUs gradient accumulation steps batch size per GPU num. steps learning rate AdamW (0.9, 0.999) 0.1 50 cosine 4 10 3 200 5e-3 1 600 1e-3 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Table 10. stands for the maximum number of retrieved n-grams in token reutilization temp. top-p min-p penalty penalty len. LLaMA3.1-8b YaRN-LLaMA2-7b-128k Qwen2.5-1.5b Qwen2.5-7b Qwen2.5-14b 20 1.0 - 0.9 0.9 - - 0.1 - - 0.05 0.05 1.2 1.15 1.15 1.15 1.13 1024 B.2. Inference Details For inference, we used 4-grams to maintain consistency with multi-token generation. The specific inference parameters are presented in Table 10. For the tree attention mechanism, we selected simple ternary full tree configuration, as depicted in Appendix B.2. C. Pre-training Details of the Llama3.1 Draft Model To serve as the draft model for LLaMA3.1-8b in TriForce, we pretrain tiny version of 250M parameters with the same tokenizer from LLaMA3.1-8b. The model configuration is listed in Table 11. We train the model on Wikipedia (20231101.en) 5 and part of C4-en6 for 1 epoch. Table 11. Configuration of Llama 3.1 205M. hidden size hidden act intermediate size max position embeddings num attention heads num key value heads rope theta vocab size 768 silu 3072 2048 12 12 500000 128256 5https://huggingface.co/datasets/wikimedia/wikipedia 6https://huggingface.co/datasets/allenai/c 19 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens D. Different Sampling Method D.1. Introduction of Different Sampling Algorithms Given probability distribution P(xtx1, x2, . . . , xt1) over the vocabulary at position t, top-p sampling (Holtzman et al., 2020) first sorts the tokens in descending order of their probabilities. It then selects the smallest set of tokens whose cumulative probability exceeds predefined threshold p, where (0, 1]. Formally, let Vp Ă be the smallest set such that: ÿ P(xt = vx1, x2, . . . , xt1) ě p. vPVp The next token ˆxt is then randomly sampled from this reduced set Vp according to the renormalized probabilities: ˆxt ř P(xt = vx1, . . . , xt1) v1PVp P(xt = v1x1, . . . , xt1) for Vp. Nguyen et al. (2024) introduced min-p sampling, which uses relative probability threshold pbase (0, 1] to scale the maximum token probability pmax to determine the absolute probability threshold pscaled. Sampling is then performed on tokens with probability greater than or equal to pscaled. Formally, given the maximum probability over the token distribution pmax = maxvPV P(xt = vx1, x2, . . . , xt1), the absolute probability threshold pscaled is calculated as: pscaled = pbase ˆ pmax. The sampling pool Vmin is then defined as the set of tokens whose probability is greater than or equal to pscaled: Vmin = tv P(vx1, x2, . . . , xt1) ě pscaledu. Finally, the next token ˆxt is randomly sampled from the set Vmin according to the normalized probabilities: ˆxt ř P(vx1, . . . , xt1) v1PVmin P(v1x1, . . . , xt1) for Vmin. The sampling pool of η-sampling (Hewitt et al., 2022) is defined as Vη = tv P(vx1, x2, . . . , xt1) ě ηu, η = min (cid:0)ϵ, α exp(hθ,xăi )(cid:1) . where hθ,xăi is the entropy of P(Vx1, x2, . . . , xt1), α and ϵ are hyperparameters. D.2. Impact of Different Sampling Algorithms We also explored the impact of different sampling algorithms with disable token reutilization, including top-p sampling (Holtzman et al., 2020), min-p sampling (Nguyen et al., 2024), and η-sampling (Hewitt et al., 2022). As summarized in Table 12, TOKENSWIFT consistently demonstrates strong robustness across these methods. This versatility underscores its compatibility with wide range of decoding strategies, making it suitable for diverse applications and use cases. E. Tree-Based Attention Tree attention is mechanism designed to process multiple candidate continuations during speculative decoding efficiently. Instead of selecting single continuation as in traditional methods, tree attention leverages multiple candidates to increase the expected acceptance length in each decoding step, balancing computational demands and performance. 20 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Table 12. Ablation results on various sampling methods with disable token reutilization. Gen. Len. top-p (p = 0.9) min-p (p = 0.1) η-sampling (ϵ =2e-4) 20K 40K 60K 80K 100K α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) 0.68 2.10 0.81 2.80 0.84 3.07 0.86 3.28 0.87 3. 0.66 2.01 0.75 2.58 0.79 2.94 0.81 3.15 0.82 3.26 0.56 1. 0.71 2.59 0.78 2.99 0.81 3.24 0.84 3.42 The mechanism uses tree structure where each branch represents unique candidate continuation. For example, if two heads generate top-2 and top-3 predictions, the Cartesian product of these predictions results in 6 candidates, forming tree with 6 branches. Each token in the tree attends only to its predecessors, and an attention mask ensures that this constraint is upheld. Positional indices are also adjusted to align with the tree structure. The tree structure is constructed by taking the Cartesian product of the predictions across all heads. If head has sk top predictions, then the tree structure consists of all possible combinations of predictions across the heads. Each combination forms unique branch in the tree. Let the total number of candidates (i.e., branches) in the tree be denoted as C, which is the product of the number of predictions for each head: Kź = sk. k=1 Each candidate is distinct sequence of tokens formed by selecting one token from each set of predictions from the heads. To ensure that tokens only attend to their predecessors (tokens generated earlier in the continuation), an attention mask is applied. The attention mask for the tree structure ensures that for each token at level k, it can attend only to tokens in levels t0, 1, . . . , 1u. This guarantees that each tokens attention is directed solely towards its predecessors in the tree. Formally, the attention mask Mk for each token at level is defined as: # Mk(i, j) = 1 if token is predecessor of token i, 0 otherwise. where Mk(i, j) = 1 means that the token at position can attend to the token at position i, and Mk(i, j) = 0 means no attention is allowed from to i. F. More Ablation Experiments F.1. Ablation of Temperature Table 13 presents the results of an ablation experiment investigating the effect of varying temperature settings on the generation length, acceptance rate, and speedup during text generation. The experiment uses top-p sampling with fixed of 0.9 and evaluates generation lengths ranging from 20K to 100K tokens, with temperature values spanning from 0.4 to 1.2. 21 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens From the results, it is evident that as temperature increases, acceptance rate generally decreases across all generation lengths. Specifically, acceptance rate drops from 0.79 at temperature of 0.4 to 0.52 at temperature of 1.2 for 20K-length generation, and similar trend is observed for longer sequences. This suggests that higher temperatures result in more diverse but less accurate output. On the other hand, speedup tends to remain relatively stable or slightly decrease with higher temperatures. The highest speedups, reaching around 3.4, are observed across all generation lengths with temperatures around 0.6 and 1.0, indicating that moderate temperature settings offer the best balance between speed and quality. Table 13. Ablation results on varying temperatures. Using top-p sampling, with set to 0.9. Gen. Len. 0.4 0.6 0.8 1. 1.2 20K 40K 60K 80K 100K α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) α ˆ(ą 1) 0.79 2. 0.85 2.76 0.87 3.07 0.88 3.26 0.89 3.39 0.84 2.34 0.88 2. 0.89 3.10 0.90 3.29 0.90 3.41 0.56 1.80 0.73 2.60 0.80 3. 0.83 3.29 0.85 3.45 0.68 2.10 0.81 2.80 0.84 3.07 0.86 3. 0.87 3.42 0.52 1.72 0.69 2.52 0.77 2.96 0.81 3.22 0.83 3. F.2. Ablation of Prefill Length We disable token reutilization and conduct ablation study on the different prefix length, as shown in Table 14. The experiment explores the impact of varying prefix lengths on the generation of sequences of different lengths (from 20K to 100K). The results include two key metrics: acceptance rate (α) and speedup factor (ˆ). As the prefix length increases, the acceptance rate tends to stabilize, generally hovering around 0.35 to 0.39 across different sequence lengths, with slight fluctuation depending on the specific prefix length. This suggests that while the acceptance rate does not dramatically change with longer sequences, it remains relatively consistent. Table 14. Ablation results on different prefill length disable token reutilization. Prefill Len. 20K 40K 60K 80K 100K α ˆ α ˆ α ˆ α ˆ α ˆ 2048 3072 4096 5120 6144 7168 8192 0.35 0.31 0.35 0.32 0.39 0.36 0.36 1.41 1.23 1.32 1.29 1.46 1.42 1.21 0.35 0.31 0.35 0.31 0.39 0.37 0. 1.63 1.42 1.54 1.46 1.66 1.62 1.42 0.35 0.31 0.35 0.31 0.39 0.36 0.36 1.76 1.55 1.69 1.57 1.80 1.74 1.58 0.35 0.31 0.35 0.31 0.39 0.36 0.36 1.83 1.64 1.76 1.65 1.88 1.82 1.69 0.34 0.30 0.35 0.31 0.39 0.36 0. 1.87 1.69 1.85 1.70 1.94 1.88 1.77 In terms of speedup, it shows that with longer prefix lengths, the model achieves progressively higher acceleration. For instance, prefix length of 2048 achieves speedup of 1.41 for 20K tokens, but with 8192, the speedup reaches up to 1.77 for 100K tokens. This indicates that increasing the prefix length contributes to better acceleration, especially for longer sequences, while maintaining relatively stable acceptance rate. The findings demonstrate the tradeoff between prefix length and model efficiency, where larger prefix lengths tend to result in greater speed. 22 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens F.3. Ablation of Penalty Window We investigate the effect of penalty window size (W) on the performance of model generating sequences of varying lengths (from 20K to 100K tokens). For each sequence length, we apply penalty to generated tokens within sliding window of size W, and evaluate the impact on two key metrics: acceptance rate (α) and acceleration factor (ˆ). Additionally, we assess the diversity of the generated sequences using the Distinct-n metric, where higher values indicate greater diversity. Table 15. Ablation results on penalty length (W). Penalty Len. (W) 20K 40K 60K 80K 100K α ˆ α ˆ α ˆ α ˆ α ˆ 20 50 128 256 512 1024 0.82 0.83 0.59 0.78 0.75 0.66 0.69 2.25 2.30 1.75 2.17 2.15 2.01 1.99 0.90 0.89 0.70 0.86 0.84 0.75 0.79 2.85 2.83 2.38 2.76 2.73 2.58 2.58 0.93 0.91 0.75 0.89 0.88 0.79 0.82 3.20 3.14 2.75 3.11 3.07 2.94 2. 0.94 0.92 0.80 0.91 0.89 0.81 0.84 3.42 3.35 3.07 3.33 3.28 3.15 3.14 0.95 0.93 0.82 0.92 0.90 0.82 0.86 3.58 3.52 3.29 3.48 3.43 3.26 3.31 Table 16. Distinct-n score with different penalty length W. Penalty Len. (W) Distinct-1 Distinct-2 Distinct-3 Distinct-4 AVG. 20 50 128 256 512 1024 2048 0.85 0.91 0.95 0.83 0.90 0.79 0.67 0.86 0.91 0.77 0.91 0.86 0.86 0.84 0.73 0.85 0.57 0.88 0.74 0.77 0.86 0.70 0.77 0.48 0.83 0.65 0.71 0.84 0.79 0.86 0.69 0.86 0.79 0.78 0. The results in Table 15 and Table 16 show clear trade-off between the penalty window size and the models performance. For smaller penalty window sizes, such as = 20, the model achieves higher acceptance rates and better acceleration, but this comes at the cost of lower diversity in the generated sequences (as indicated by lower Distinct-n values). As the penalty window size increases (e.g., = 256 or = 2048), the acceptance rate slightly decreases, but the model exhibits better diversity and still maintains significant speedup relative to the AR baseline. These findings suggest that larger penalty windows can help reduce repetitiveness and improve the diversity of long sequence generation, but they may also slightly reduce the models efficiency and acceptance rate. Table 15 also reveals that for each penalty window size, increasing the sequence length (from 20K to 100K tokens) generally results in higher acceleration and better diversity, with some fluctuations in acceptance rates. G. More Cases H. Training Loss Curve 23 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens Figure 6. Case Study on YaRN-LLaMA2-7b-128k. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The blue text is repetition part. 24 Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens (a) Cross Entropy Loss Training Curve of the First Linear Layer (b) Cross Entropy Loss Training Curve of the Second Linear Layer (c) Cross Entropy Loss Training Curve of the Third Linear Layer Figure 7. Cross Entropy Loss Training Curve of Linear Layers"
        }
    ],
    "affiliations": [
        "NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University"
    ]
}