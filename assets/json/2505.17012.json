{
    "paper_title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding",
    "authors": [
        "Haoning Wu",
        "Xiao Huang",
        "Yaohui Chen",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 1 0 7 1 . 5 0 5 2 : r SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding Haoning Wu1,2, Xiao Huang1,3, Yaohui Chen1, Ya Zhang1,2, Yanfeng Wang1,2, Weidi Xie1,2 1School of Artificial Intelligence, Shanghai Jiao Tong University 2Shanghai AI Laboratory 3Tianjin University https://haoningwu3639.github.io/SpatialScore"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This paper investigates critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as rigorous benchmark for the next evolution of MLLMs. All codes and data will be publicly available."
        },
        {
            "title": "Introduction",
            "content": "With the rapid development of multimodal representation learning [43, 71] and large language models, multimodal large language models (MLLMs) have demonstrated remarkable progress in processing text, images, and videos, enabling applications across diverse scenarios [8, 15, 27, 45, 46]. While modern MLLMs have excelled in general semantic-related QA tasks [25, 35, 70] (e.g., answering who, what, and where questions) and reasoning-based problems like math [37, 57, 74, 77], their abilities to interpret visual geometry, such as 3D spatial relationships, camera dynamics, and depth perception, remain underexplored. This gap is critical for real-world applications like embodied AI and autonomous navigation, where human-like spatial reasoning is essential. In traditional computer vision research, 3D visual geometry perception has been regarded as solved problem (though relying on optimization), with well-established tools [48] and rigorous mathematical foundations [18]. Recent literature [56, 58, 73] has revitalized these classical approaches with feed-forward neural networks. However, these advances remain confined to vision-only paradigms, lacking integration with language understanding and unified evaluation protocols. Building on this *: These authors contribute equally to this work. : Corresponding author. Preprint. Under review. Figure 1: Overview. (a): SpatialScore thoroughly assesses spatial reasoning abilities of current models via question-answering (judgment, multi-choice, and open-ended QA); (b): Performance of representative models on our proposed VGBench and SpaitalScore. progress, it is foreseeable that integrating semantic comprehension with spatial geometric perception will emerge as the next evolutionary step for MLLMs. In this paper, we ask the question: to what extent do existing MLLMs possess 3D spatial perception and understanding capabilities? While some studies have begun exploring this direction, they remain nascent, primarily suffering from two critical limitations: (i) simplistic tasks: existing benchmarks [3, 22, 32, 54, 66] predominantly focus on superficial spatial-related queries (e.g., basic object presence or positional relationships), neglecting rigorous visual geometry understanding (e.g., camera pose and dynamics); and (ii) narrow evaluation scope: prior assessments [22, 29, 32, 38, 66] are typically fragmented, considering naive questions (e.g., Yes/No judgment), single-modality inputs (e.g., static images), or isolated skills (e.g., distance or size estimation), lacking unified framework to measure holistic spatial reasoning. To tackle these challenges, we first introduce VGBench, specialized benchmark designed to assess fine-grained visual geometry perception abilities of MLLMs through diverse tasks, such as object orientation, camera motion, and distance estimation. Building upon this, we further integrate VGBench with spatial-relevant data from 11 existing datasets to construct SpatialScore, comprehensive benchmark for spatial understanding (as depicted in Figure 1), featuring 28K samples that cover diverse tasks (e.g., metric-based distance measurement and homography matrix estimation), modalities (images/videos), question formats (judgment, multi-choice, and open-ended QA), along with carefully curated subset, SpatialScore-Hard, further emphasizing challenging scenarios. To empower MLLMs with robust spatial reasoning, we propose SpatialAgent, an advanced multiagent framework that orchestrates 9 specialized tools (e.g., monocular depth estimator [67], optical flow estimator [53], camera pose estimator [56]). This agent system enables off-the-shelf MLLMs to perform spatial reasoning through two distinct paradigms: (i) Plan-Execute: hierarchical approach that decomposes complex spatial understanding tasks into structured sub-task plans for sequential tool invocation; and (ii) ReAct: an interleaved reasoning-and-action method that iteratively refines understanding via context-aware tool interactions. By leveraging these paradigms, SpatialAgent dynamically processes multimodal inputs (textual queries paired with images or videos) through tool orchestration, significantly improving the spatial understanding capabilities of MLLMs. Overall, our contributions can be summarized as follows: (i) we present VGBench, specialized benchmark for evaluating visual geometry perception and understanding abilities of MLLMs; (ii) we introduce SpatialScore, comprehensive multimodal spatial understanding benchmark via integrating VGBench with 11 existing datasets, comprising 28K samples across various tasks (e.g., orientation and size estimation), modalities (images and videos), and question formats (judgment, multi-choice, and open-ended QA), along with more challenging subset, termed SpatialScore-Hard; (iii) we propose SpatialAgent, novel multi-agent framework equipped with 9 specialized tools to improve spatial understanding capabilities of MLLMs via Plan-Execute (hierarchical task decomposition) or ReAct (interleaved reasoning-action) paradigms; (iv) we conduct comprehensive evaluations, which reveal that SpatialAgent significantly improves spatial understanding performance, yet current MLLMs still lag behind humans, underscoring the need for fundamental architectural innovations. To our knowledge, this work introduces the most comprehensive and diverse spatial understanding benchmark to date, offering potential insights and rigorous testbed for future MLLMs development. 2 Figure 2: Dataset Construction and Statistics. (a) The data construction pipeline for VGBench, SpatialScore, and SpatialScore-Hard; (b) Representative examples from distinct categories in SpatialScore; (c) Data distribution statistics across VGBench, SpatialScore, and SpatialScore-Hard."
        },
        {
            "title": "2 Dataset",
            "content": "In this section, we start with the construction of VGBench in Sec. 2.1; Then we elaborate on our proposed comprehensive spatial understanding benchmark, SpatialScore, detailing the integrated diverse data sources and curated challenging subset, SpatialScore-Hard, in Sec. 2.2; Finally, we present thorough statistical analysis of the collected data and discussions in Sec. 2.3."
        },
        {
            "title": "2.1 VGBench",
            "content": "Just as mathematics [37, 57, 74, 77] is widely employed to evaluate the logical reasoning capabilities of MLLMs, visual geometry, which can be regarded as the mathematics of computer vision research [18], offers natural framework for assessing their visual reasoning abilities. Thus, to bridge the gap in spatial understanding evaluation in existing MLLM benchmarks, particularly in 3D visual geometry perception (e.g., camera pose estimation, point tracking, and depth estimation), we introduce VGBench, the first benchmark dedicated to visual geometry perception and understanding. Concretely, we employ controllable and scalable pipeline to generate high-quality QA pairs by leveraging precise 3D annotations (e.g., depth and 3D bounding boxes) from existing datasets. As depicted in Figure 2(a), we first randomly sample 300 scenes from multiple 3D reconstruction and detection datasets (ScanNet [10], ScanNet++ [69], WildRGB-D [63], and CA-1M [23]), each with accurate 3D geometric annotations. Subsequently, we construct open-ended QA pairs by combining predefined question templates with LLM-based rewriting (e.g., prompting DeepSeek-v3 [31] to transform basic questions into more varied forms), ensuring the diversity of questions. To facilitate convenient quantitative evaluation, we convert subset of these open-ended QA pairs into judgment and multi-choice formats, with three strategies for generating plausible and challenging distractors: (i) we randomly sample same-category annotations (e.g., depth or distance) within proper numeric range from the same scene or other scenes; (ii) we introduce minor perturbations within justifiable margin of ground truth (e.g., homography matrix); and (iii) we also leverage DeepSeek-v3 [31] to produce valid yet confusing distractors. As result, VGBench is populated with 6K high-quality samples, spanning judgment, multi-choice, and open-ended QA formats."
        },
        {
            "title": "2.2 SpatialScore",
            "content": "To enable holistic evaluation of spatial reasoning capabilities, we further integrate existing spatialrelated datasets, to construct the most comprehensive and diverse spatial understanding benchmark to date, termed SpatialScore. This dataset incorporates our proposed VGBench, along with 3 specialized benchmarks for spatial intelligence (including MMVP [55], RealWorldQA [9], SpatialSense [66], SpatialBench [3], QSpatialBench [29], CV-Bench [54], VSR [32], 3DSRBench [38], and VSI-Bench [65]), as well as spatial-relevant subsets from general multi-image question-answering benchmarks (e.g., 6 categories from BLINK [14] and 10 categories from MMIU [40]). This integration yields 28K samples spanning 8 categories (counting, object localization, 3D positional relation, depth & distance, object properties, camera & image transformation, point/object tracking, and others), with representative examples for each category illustrated in Figure 2(b). Moreover, to further advance spatial intelligence research, we curate SpatialScore-Hard, more challenging subset of 1.4K samples, selected through rigorous process: we first identify 4.3K candidate samples, where at least 16 MLLMs (ranging from 1B to 78B) fail to give correct answers, with the additional constraint that at least two large models (32B+ parameters) fail on each sample. These candidates are then manually verified and balanced across categories to establish focused set that better reveals the limitations of current MLLMs in spatial understanding."
        },
        {
            "title": "2.3 Statistics & Discussion",
            "content": "We present the task-specific data distributions of VGBench, SpatialScore, and SpatialScore-Hard in Figure 2(c), highlighting their large-scale, diverse, and comprehensive coverage of spatial reasoning tasks. This curated design enables rigorous evaluation of MLLMs spatial reasoning capabilities and offers actionable insights for model development. Additional data details will be further discussed in Sec. E. To our knowledge, this is the most comprehensive benchmark for spatial intelligence to date. We expect it to be valuable resource for the community and catalyst for progress in this domain."
        },
        {
            "title": "3 Methodology",
            "content": "We introduce SpatialAgent, multi-agent system designed to enhance spatial understanding via the integration of nine specialized visual geometry perception tools. To be specific, Sec. 3.1 presents the problem formulation, Sec. 3.2 elaborates on the expert tools, and Sec. 3.3 describes the system architecture and interaction paradigms."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We consider question-answering paradigm for spatial understanding. Specifically, given textual question (q) paired with visual input (v), which can be single image, multi-frame sequence, or video, the core process of our framework can be represented as: = A(q, v; ) (1) where denotes free-text outputs from the model, refers to our proposed multi-agent system, SpatialAgent, which supports both Plan-Execute and ReAct reasoning paradigms (detailed in Sec. 3.3), and constitutes our specialized toolbox containing various agents, formulated as = {t1, t2, . . . tn}, where each ti represents distinct tool for visual geometry perception."
        },
        {
            "title": "3.2 Toolbox",
            "content": "As illustrated in Figure 3(a), our framework integrates comprehensive toolbox (T ) comprising nine specialized modules designed to enhance spatial understanding. Each tool is rigorously specified with clearly defined functionality, input/output formats, and illustrative usage examples. The modules are implemented through strategic combinations of existing models and are organized into four core categories: 2D perception, motion & transformation, camera & geometry, and auxiliary utilities. Notably, our toolbox exclusively employs open-source expert models, ensuring easy reproducibility and continuous improvement potential as these underlying tools evolve. 2D Perception. To endow SpatialAgent with core 2D perception capabilities, we integrate suite of open-set visual understanding models. Specifically, RAM++ [20] is used for open-vocabulary object recognition, while OWLv2 [41] provides accurate object detection and localization with bounding boxes. These detections serve as spatial prompts for SAM2 [47], which performs instance segmentation to refine localization and quantify object proportions. When combined with depth cues and real-world priors, this enables reliable physical size estimation. 4 Figure 3: Architecture and Workflow of SpatialAgent. (a) Specialized spatial understanding tools integrated in SpatialAgent; (b) The Plan-Execute paradigm for hierarchical task decomposition and stepwise execution; (c) The ReAct paradigm for iterative interaction and dynamic strategy refinement. Motion & Transformation. To support dynamic spatial reasoning in multi-frame sequences or videos, we incorporate RAFT [53] for dense optical flow estimation. This facilitates camera motion analysis and, when combined with 2D perception modules, enables objectand region-level motion tracking. For geometric transformations across views, we leverage SIFT [36] in OpenCV, supporting robust feature matching and homography estimation for point tracking and image alignment tasks. Camera & Geometry. To enable 3D spatial reasoning, we integrate specialized geometric tools. Specifically, VGGT [56] estimates camera parameters (intrinsic and extrinsic) from single or multiframe inputs. DepthAnythingV2 [67] provides metric depth estimation using domain-specific models (indoor/outdoor), which interface seamlessly with our 2D perception modules to yield accurate depth for detected objects or regions. Additionally, OrientAnything [60] estimates 3D object orientations, facilitating fine-grained spatial relationship inference. Auxiliary Utilities. To support tool interaction and orchestration, we implement general-purpose utilities, including basic image operations (e.g., cropping, resizing) and numerical computations. dedicated Terminate action consolidates tool outputs and signals the completion of reasoning. Moreover, we employ targeted prompt engineering to enhance the step-by-step reasoning capabilities of open-source MLLMs (e.g., Qwen2.5-VL [2], InternVL3 [76]) when serving as the agent core."
        },
        {
            "title": "3.3 SpatialAgent",
            "content": "As mentioned in Sec. 3.1, our proposed SpatialAgent, driven by specific instructions and prompts, supports two distinct reasoning paradigms: Plan-Execute (PE) and ReAct, which we elaborate below. Plan-Execute Paradigm. As shown in Figure 3(b), in this paradigm, our SpatialAgent comprises three parts: planner, executor, and summarizer, expressed as APE = {Aplan, Aexec, Asum}, which obtains the final response (rPE) via sequential feedforward. Given question (q) and visual input (v), along with detailed specifications about the toolbox (T ), the planner (Aplan) first generates sequential tool invocation plan (p) of steps, each with specific tool ti and its parameters (argsi): = Aplan(q, v; ) = {(t1, args1), (t2, args2), . . . , (tk, argsk)} (2) 5 Then the executor (Aexec) sequentially executes the plan and obtain the tool output set (Y) consisting of results at each step (yi), denoted as: = {y1, y2, . . . , yk} = Aexec(p) (3) Finally, the summarizer (Asum) produces the final response (rPE) by reasoning according to the tool outputs (Y) and original inputs, formulated as: rPE = Asum(Y, q, v) (4) ReAct Paradigm. As depicted in Figure 3(c), this paradigm employs an interleaved reasoning process, with our SpatialAgent composed of observer, executor, and summarizer, formulated as AReAct = {Aobs, Aexec, Asum}. Here, we maintain memory module (M) that records all intermediate interactions between the observer (Aobs) and the executor (Aexec). At step i, the memory state Mi stores the complete history of observer decisions (o) and execution results (y), represented as: Mi = {m1, m2, . . . mi1} = {(o1, y1), (o2, y2), . . . , (oi1, yi1)}, with M1 = (5) The observer (Aobs) at step i, generates the next action (oi) based on the original inputs (q, v) and the full interaction history Mi, while the executor (Aexec) processes accordingly, expressed as: oi = Aobs(Mi, q, v); yi = Aexec(oi) (6) The iterative process continues until the observer outputs Terminate action, triggering the summarization phase, where the summarizer (Asum) synthesizes the final response (rReAct) by consolidating all accumulated evidence in memory (M) and the original inputs, i.e., rReAct = Asum(M, q, v). Both paradigms are driven by carefully designed prompts (as detailed in Sec. D.4), with distinct characteristics: The Plan-Execute paradigm excels at efficiently formulating and executing plans, though its predetermined execution path may sacrifice precision in complex scenarios. Conversely, the ReAct paradigm demonstrates better flexibility through dynamic planning that adapts to intermediate outputs, albeit at the cost of reduced efficiency due to its iterative nature."
        },
        {
            "title": "4 Experiments",
            "content": "This section begins with the experimental settings in Sec. 4.1, including baseline models, metrics, and implementation details, followed by quantitative and qualitative assessments of existing MLLMs on SpatialScore, as well as comparisons with our SpatialAgent in Sec. 4.2 and 4.3, respectively."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Baselines. To thoroughly evaluate spatial understanding abilities of MLLMs, we conduct extensive experiments on our proposed SpatialScore across 25 representative models spanning scales from 1B to 78B parameters, including: general MLLMs such as InternVL2.5 [6] (1B, 4B, 8B, 38B, and 78B), InternVL3 [76] (1B, 8B, 14B, 38B, and 78B), Kimi-VL [52] (3B), Qwen2.5VL [2] (7B, 32B, and 72B), LLaVA-OneVision [24] (7B and 72B), Cambrian [54] (8B), LLaMA-3.2V [16] (11B), LLaMA3.2V-CoT [64] (11B), and LLaVA-1.5 [33] (13B), as well as models specifically fine-tuned for spatial understanding: SpaceQwen2.5VL [4] (3B), SpatialBot [3] (3B), and SpaceLLaVA [4] (13B). Evaluation Metrics. We employ accuracy as our primary measure, with tailored scoring protocols for distinct question types: for judgment, multi-choice, and simple open-ended questions (e.g., counting), we directly compare model responses against ground truth; And for complex open-ended QAs in VSI-Bench [65] and metric-based size/distance estimation, we apply δ = 2 tolerance threshold following [29] (regarding answers within 0.5 to 2 of ground truth as correct). Implementation Details. All experiments are conducted on 4 Nvidia A100 GPUs with float16 precision, except for Qwen2.5VL [2], which uses bfloat16 to prevent numerical overflow. FlashAttention2 [11] is enabled for all supported models to optimize efficiency. To ensure reproducibility, we fix the random seed and set the temperature to 0 throughout all evaluations. For models limited to single-image inputs, we only feed the first frame when processing multi-frame or video inputs. 6 Table 1: Quantitative Results on VGBench. Here, Homo., Pose-Est., 3D-Recon., Tracking, and Obj-Pos. denote Homography Matrix, Pose Estimation, 3D Reconstruction, Point Tracking, and Object Position, respectively. Results with the best and second best results are bolded and underlined. Methods Overall Homo. Pose-Est. Camera 3D-Recon. Distance Depth Tracking Obj-Pos. InternVL2.5-1B InternVL3-1B 29.28 28.82 22.80 22.70 40.91 30.91 40.00 39. 30.19 30.00 28.67 29.50 20.19 21.36 28.45 28.36 38.50 41.00 1B models SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrian-8B InternVL2.5-8B InternVL3-8B 25.63 26.68 38.10 28.25 32.60 35.10 35.60 35.75 27.75 34.90 33.22 LLaMA-3.2V-11B 29.75 LLaMA-3.2V-11B-CoT 31.42 28.05 LLaVA-1.5-13B SpaceLLaVA-13B 13.60 InternVL3-14B 40.95 25.00 23.10 25.20 23.20 25.40 28. 25.40 25.90 23.20 27.30 24.20 24.90 24.30 25.30 14.20 25.40 3B and 4B models 30.91 32.73 48.55 35.82 53.64 49.64 30.60 34.20 55.20 27.60 46.40 39.40 7B and 8B models 55.27 49.64 38.18 44.18 34.91 62.80 40.80 28.60 45.40 44.80 19.09 29.64 31.82 28.18 27.64 31.27 31.27 30.55 28.18 32.36 27.45 11B, 13B, and 14B models 36.36 38.55 30.91 3.27 62. 28.60 38.00 40.80 35.60 57.60 26.91 32.55 31.45 9.82 30.55 32B and 38B models 29.50 21.83 28.50 20.83 28.50 32.33 44.17 29.33 30.33 29.33 26.00 26.17 24.83 26.83 32.83 36. 15.27 16.36 18.91 19.64 24.73 30.09 17.27 20.18 22.64 25.91 23.82 26.36 22.00 18.82 7.82 26.91 26.36 28.82 56.91 36.55 31.55 36.00 36.91 51.27 22.27 36.73 43.18 31.73 35.64 25.91 1.00 58. Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 41.78 42.43 44.88 26.10 23.40 26.40 70.55 64.73 72.00 52.60 51.80 53.80 32.18 31.09 30. 29.33 34.67 44.00 20.91 24.36 23.45 64.82 68.27 70.91 72B and 78B models LLaVA-OneVision-72B 37.72 Qwen2.5-VL-72B 42.92 42.78 InternVL2.5-78B 43.53 InternVL3-78B 22.40 27.10 25.10 25. 44.36 70.91 62.18 61.09 64.20 51.20 52.00 49.20 30.36 30.36 28.55 30.00 37.83 31.67 39.67 44.50 23.00 22.55 27.09 28.00 49.00 67.18 66.00 72. 37.50 38.00 51.83 38.33 38.83 43.83 38.50 46.50 41.50 51.33 48.50 41.50 46.33 38.33 21.67 40.50 49.83 49.83 48.67 48.00 52.33 49.17 38."
        },
        {
            "title": "4.2 Quantitative Results",
            "content": "Table 1 presents the performance of existing MLLMs on VGBench, the first benchmark specifically designed for evaluating visual geometry perception and understanding abilities. While larger models generally achieve better overall accuracy, their performance remains suboptimal on this challenging benchmark, particularly in tasks involving homography matrix, camera parameters, 3D reconstruction, and distance/depth estimation. These results reveal significant gap between MLLMs strong semantic/logical reasoning capabilities and their limited visual geometry perception abilities. Based on the results in Table 2, we summarize the following key observations: (i) on model scale vs. performance, larger models generally exhibit improved spatial reasoning capabilities, with InternVL3-78B achieving the highest overall accuracy of 60.28%. However, absolute performance remains modest, indicating substantial room for improvement in comprehensive spatial understanding; (ii) on limitation of fine-tuning, models fine-tuned on limited spatial-relevant data (e.g., SpaceQwen2.5VL-3B, SpaceLLaVA-13B) show poor generalization, performing poorly on our diverse and challenging SpatialScore benchmark; (iii) on 2d vs. 3d reasoning, while existing models handle basic 2D spatial tasks (e.g., object localization) moderately well, they significantly struggle with 3D spatial reasoning, particularly in tasks requiring visual geometry perception, such as camera parameter prediction and image transformation (homography matrix). More results and detailed analysis on other source datasets within SpatialScore will be presented in Sec. F. We further report performance on the SpatialScore-Hard subset  (Table 3)  , comprising both opensource and proprietary models (e.g., commercial APIs). This subset is rigorously constructed through 7 Table 2: Quantitative Results on SpatialScore. Here, Count., Obj-Loc., Pos-Rel., Dist., Obj-Prop., and Cam.&IT. refer to Counting, Object Localization, 3D Positional Relation, Depth & Distance, Object Properties, and Camera & Image Transformation, respectively. Results with the best and second best results are bolded and underlined, respectively. Methods Overall Count. Obj-Loc. Pos-Rel. Dist. Obj-Prop. Cam.&IT. Tracking Others InternVL2.5-1B InternVL3-1B 42.32 42.26 44.41 58.07 46.90 46.61 56.45 52. 32.04 32.57 26.24 34.40 30.78 27.81 28.47 28.35 43.47 45.13 1B models SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrian-8B InternVL2.5-8B InternVL3-8B 42.31 41.65 51.48 44.60 47.90 49.82 51.24 51.19 41.46 52.39 52.46 LLaMA-3.2V-11B 42.80 LLaMA-3.2V-11B-CoT 48.80 42.29 LLaVA-1.5-13B SpaceLLaVA-13B 31.80 56.40 InternVL3-14B 45.01 53.25 52.58 46.95 46.62 53. 58.07 47.02 40.05 49.30 63.50 44.27 39.58 44.68 42.13 62.22 3B and 4B models 49.78 54.32 61.99 55.22 55.55 62.02 57.88 55.40 61.34 58.93 62.23 62.82 27.36 27.12 38.27 28.38 37.53 42. 7B and 8B models 59.08 64.11 51.58 65.50 65.09 61.86 63.58 58.64 64.17 64.06 40.08 38.07 29.66 43.60 43.30 11B, 13B, and 14B models 49.45 60.04 49.88 41.10 65. 53.86 59.15 51.94 41.73 64.91 34.05 40.18 34.53 19.11 46.89 32B and 38B models 34.11 26.10 46.47 35.07 32.59 27.00 50.52 41.22 21.18 47.81 45.47 37.69 50.05 40.22 34.83 50. Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 54.65 57.20 59.02 49.16 59.08 67.31 61.50 66.68 68.46 64.31 66.59 67.06 43.28 46.20 48. 46.18 50.95 48.66 72B and 78B models LLaVA-OneVision-72B 54.82 56.82 Qwen2.5VL-72B 57.81 InternVL2.5-78B 60.17 InternVL3-78B 60.15 49.23 57.74 65.91 64.08 70.42 67.46 70.42 63.66 64.72 67.97 68. 44.04 44.06 48.15 50.37 55.10 52.10 50.10 50.43 26.34 24.21 35.73 28.65 35.85 32.49 39.51 32.85 25.59 32.16 29.28 28.11 31.35 28.89 15.39 40.92 43.65 40.23 43. 38.01 44.34 39.15 42.45 26.44 27.57 56.28 37.20 36.90 37.02 37.62 49.28 17.40 38.22 39.77 30.08 37.32 23.39 8.31 56.76 65.19 67.28 69.32 51.26 67.64 66.45 71. 43.58 41.66 47.36 42.73 42.19 48.85 45.87 50.40 45.18 49.81 48.11 42.78 47.47 43.31 38.52 51.68 49.65 49.65 51.68 51.68 49.65 50.29 53.12 voting and manual verification to ensure correctness while enormously increasing task difficulty. The low accuracy of existing state-of-the-art models on this subset highlights its challenging nature, while SpatialAgent demonstrates substantial improvements. Remarkably, even using compact models like Qwen2.5VL-7B and InternVL-8B as agent cores, SpatialAgent boosts their spatial understanding abilities to surpass all open-source models, even outperforming proprietary systems in several categories. These improvements are attributed to SpatialAgents structured, tool invocation, validating that its systematic, tool-augmented reasoning framework offers decisive advantages in complex spatial tasks, particularly those requiring precise visual geometry and multi-step reasoning."
        },
        {
            "title": "4.3 Qualitative Results",
            "content": "Figure 4 presents representative case studies for comparing SpatialAgents reasoning trajectories, under both the Plan-Execute and ReAct paradigms, against various baseline MLLMs. These visualizations highlight SpatialAgents structured and interpretable reasoning process, which systematically decomposes complex tasks and dynamically invokes appropriate tools for more accurate solutions. While SpatialAgent exhibits strong performance across diverse spatial reasoning tasks, occasional failures still occur, typically due to suboptimal tool execution or misinterpretation of intermediate results (e.g., confusing depth with object distance). Such limitations are expected to diminish as MLLMs improve in comprehension and as the toolbox design becomes more precise and robust. 8 Table 3: Quantitative Results on SpatialScore-Hard. Our SpatialAgent demonstrates substantially greater performance improvements on this carefully curated, challenging subset, highlighting its specialized capabilities for spatial understanding tasks. Methods Overall Count. Obj-Loc. Pos-Rel. Dist. Obj-Prop. Cam.&IT. Tracking Others InternVL2.5-1B InternVL3-1B 11.64 10.50 14.79 16.90 12.00 5.71 14.95 11.21 9.71 17.14 18.29 17. 1B models SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrain-8B InternVL2.5-8B InternVL3-8B SpatialAgent-Qwen-ReAct SpatialAgent-Qwen-PE SpatialAgent-Intern-ReAct SpatialAgent-Intern-PE LLaMA-3.2V-11B LLaMA-3.2V-11B-CoT LLaVA-1.5-13B SpaceLLaVA-13B InternVL3-14B 17.21 19.87 16.29 26.36 16.86 13.71 15.60 15.21 21.93 13.00 12.86 30.29 35.30 39.51 46. 21.93 23.50 26.50 17.86 16.14 20.42 19.72 12.68 19.72 18.31 6.34 14.08 4.93 11.27 7.04 17.61 10.56 13.38 26.56 26.76 20.42 14.08 16.90 0.70 14.08 3B and 4B models 18.86 27.59 18.86 28.00 7.43 21. 27.10 25.23 12.62 38.79 16.82 15.42 7B and 8B models 13.14 5.71 41.71 21.14 2.86 41.71 40.57 43.53 44.31 16.36 20.56 38.79 11.68 16.36 63.55 67.31 58.88 59.62 6.86 8.00 17.71 27.43 19.43 14.86 20.57 17.71 11.43 12.57 12.57 34.86 29.14 38.82 56. 11B, 13B, and 14B models 36.57 24.57 40.57 33.14 7.43 27.57 35.98 33.64 27.57 12.62 15.43 24.57 18.86 6.29 20.00 32B and 38B models Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 14.36 11.64 16.00 11.27 7.04 23.94 11.43 10.29 12.00 14.49 13.55 12.62 20.00 21.71 14.86 72B and 78B models LLaVA-OneVision-72B Qwen2.5VL-72B InternVL2.5-78B InternVL3-78B 15.29 17.79 13.43 21.79 16.20 9.86 11.27 21.13 8.00 22.86 19.43 20.00 12.62 20.56 11.21 27.57 18.86 19.43 15.43 27. 12.57 23.43 22.29 32.57 31.43 12.00 12.00 8.00 15.43 10.86 10.86 23.43 26.29 46.86 41.86 20.57 38.86 38.86 36.57 20.00 6.29 6.86 13.71 18.29 9.71 4.00 8.57 Proprietary Models (Commercial APIs) Gemini-2.0 Flash Claude-3.5-Haiku GPT-4o 28.92 30.00 30.57 28.87 20.42 23.94 40.57 54.29 54.86 23.36 48.13 29.91 21.71 15.43 24. 21.71 21.71 25."
        },
        {
            "title": "5 Related Work",
            "content": "12.00 10.29 8.57 13.71 18.86 14.86 15.43 16.00 21.14 21.14 16.57 20.57 20.57 26.86 31.43 42.86 58.29 17.71 20.00 12.00 6.29 19.43 14.29 7.43 17.71 21.71 21.71 17.71 18. 23.43 27.43 24.57 7.10 2.37 4.00 3.43 23.08 17.16 11.83 18.34 14.79 12.43 17.75 21.30 11.24 8.86 8.86 16.57 18.34 24.34 27.38 21.89 9.47 22.49 11.83 10. 25.44 15.38 20.12 15.98 21.30 14.20 27.81 49.70 31.95 39.05 18.86 22.86 15.43 26.86 11.43 9.14 9.04 19.43 22.86 13.14 13.14 13.14 45.35 24.57 46.45 13.71 15.43 25.14 14.86 25. 11.43 9.71 15.43 11.43 14.86 14.29 22.29 24.00 14.86 21.14 Multimodal Benchmarks. Recent breakthroughs in MLLMs [2, 6, 24, 30, 34, 76], exemplified by GPT-4o [21], Claude [1], and Gemini [51], have spurred demand for comprehensive evaluation benchmarks. Existing benchmarks like MMMU [70], MMBench [35], and Seed-Bench [25], have established broad assessments on visual-language understanding, while MMIU [40] and BLINK [14] focus on multi-image analysis, and VideoMME [13] targets video comprehension. Additional benchmarks specialize in domain-specific tasks, such as physics [8], healthcare [75], and sports [44, 62]. Despite these efforts, existing frameworks still lack systematic evaluation of visual geometry perception, critical gap we aim to bridge by introducing VGBench in this work. Spatial Understanding. Prior work has explored diverse aspects, including positional relationship [9, 22, 32, 55, 66], size/orientation/distance estimation [3, 9, 38, 54], and metric-based question answering [4, 12, 29]. Recent efforts, such as Open3DVQA [72] and Spatial457 [59], leverage simulators for scalable and controllable data construction, enabling targeted fine-tuning of MLLMs on spatial understanding tasks, while SpatialRGPT [7] introduces extra mask/bbox inputs for region-based Figure 4: Qualitative Results. We present the comprehensive reasoning process of SpatialAgent against the direct responses of other models. While occasional errors occur due to tool execution or interpretation mistakes, these limitations are expected to diminish as MLLMs continue to advance. analysis. The VSI-Bench [65] further extends spatial intelligence research to video inputs, facilitating dynamic scene understanding such as analyzing object appearance order and route planning. However, current benchmarks suffer from limitations such as restricted task complexity, narrow evaluation scopes, and fragmented protocols. To address this, we consolidate VGBench with spatial-relevant data from 11 existing datasets, establishing SpatialScore, the most comprehensive and diverse spatial understanding benchmark to date, which aims to promote progress in spatial intelligence research. Multi-Agent Systems. Multi-Agent Systems (MAS) have emerged as powerful paradigm for tackling complex tasks [26, 28, 39], demonstrating broad applicability across domains such as scientific problem solving [15, 27], software development [42], collaborative workflows [5, 68], and robotics [17, 50]. Recent frameworks like ReAct [68] and Reflexion [49] enable iterative reasoning through dynamic tool use, while platforms such as AutoGen [61] and MetaGPT [19] facilitate multiagent collaboration. Given that spatial understanding inherently requires such reasoning abilities, we introduce SpatialAgent, novel framework that integrates 9 specialized tools for spatial reasoning. By dynamically invoking appropriate tools and maintaining interpretable, step-by-step solutions, SpatialAgent significantly improves existing MLLMs in spatial intelligence."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper aims to investigate the 3D spatial understanding capabilities of current multimodal large language models (MLLMs). Specifically, we introduce VGBench, specialized benchmark for visual geometry perception and understanding. Built upon this, we further propose SpatialScore, the most comprehensive and diverse spatial reasoning benchmark to date, via integrating VGBench with spatial-relevant data from 11 existing datasets, which features 28K samples across various tasks, modalities, and QA formats, along with curated challenging subset, SpatialScore-Hard. To facilitate more robust spatial reasoning, we present SpatialAgent, multi-agent framework equipped with 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms. Extensive evaluations demonstrate the efficacy of SpatialAgent while revealing persistent challenges for current MLLMs in spatial understanding. We believe this comprehensive benchmark and framework offer rigorous foundation for advancing spatial intelligence in the future."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Weidi would like to acknowledge the funding from Scientific Research Innovation Capability Support Project for Young Faculty (ZY-GXQNJSKYCXNLZCXM-I22)."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In IEEE International Conference on Robotics and Automation, 2025. [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [5] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In Proceedings of the International Conference on Learning Representations, 2024. [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [7] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In Conference on Neural Information Processing Systems, volume 37, pages 135062135093, 2025. [8] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. In Proceedings of the International Conference on Learning Representations, 2025. [9] X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model, 2024. [10] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 58285839, 2017. [11] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In Proceedings of the International Conference on Learning Representations, 2024. [12] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, et al. Mm-spatial: Exploring 3d spatial understanding in multimodal llms. arXiv preprint arXiv:2503.13111, 2025. [13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [14] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In Proceedings of the European Conference on Computer Vision, pages 148166, 2024. [15] Alireza Ghafarollahi and Markus Buehler. Sciagents: Automating scientific discovery through bioinspired multi-agent intelligent graph reasoning. Advanced Materials, page 2413523, 2024. [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun Wu, Huazheng Wang, Thomas Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized teams. arXiv preprint arXiv:2403.12482, 2024. [18] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision, volume 665. Cambridge university press, 2003. 11 [19] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multi-agent collaborative framework. In Proceedings of the International Conference on Learning Representations, 2024. [20] Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. arXiv preprint arXiv:2310.15200, 2023. [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [22] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats up with vision-language models? investigating their struggle with spatial reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2023. [23] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. [25] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [26] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Conference on Neural Information Processing Systems, 2023. [27] Junkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin Zhang, Weizhi Ma, et al. Agent hospital: simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024. [28] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. In Conference on Neural Information Processing Systems, 2024. [29] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1702817047, 2024. [30] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. [31] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [32] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Conference on Neural Information Processing Systems, 2023. [35] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216233, 2024. [36] David Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60:91110, 2004. 12 [37] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In Proceedings of the International Conference on Learning Representations, 2024. [38] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Alan Yuille, and Jieneng Chen. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [39] Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, et al. Taco: Learning multi-modal action models with synthetic chains-of-thought-and-action. arXiv preprint arXiv:2412.05479, 2024. [40] Fanqing Meng, Chuanhao Li, Jin Wang, Quanfeng Lu, Hao Tian, Tianshuo Yang, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. In Proceedings of the International Conference on Learning Representations, 2025. [41] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In Conference on Neural Information Processing Systems, volume 36, pages 7298373007, 2023. [42] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Association for Computational Linguistics, 2024. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, 2021. [44] Jiayuan Rao, Zifeng Li, Haoning Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Multi-agent system for comprehensive soccer understanding. arXiv preprint arXiv:2505.03735, 2025. [45] Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards universal soccer video understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [46] Jiayuan Rao, Haoning Wu, Chang Liu, Yanfeng Wang, and Weidi Xie. Matchtime: Towards automatic soccer game commentary generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2024. [47] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In Proceedings of the International Conference on Learning Representations, 2025. [48] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. [49] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Conference on Neural Information Processing Systems, volume 36, pages 86348652, 2023. [50] Sinan Tan, Weilai Xiang, Huaping Liu, Di Guo, and Fuchun Sun. Multi-agent embodied question answering in interactive environments. In Proceedings of the European Conference on Computer Vision, pages 663678, 2020. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [53] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Proceedings of the European Conference on Computer Vision, pages 402419, 2020. [54] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In Conference on Neural Information Processing Systems, volume 37, pages 8731087356, 2024. 13 [55] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [56] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [57] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [58] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [59] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Spatial457: diagnostic benchmark for 6d spatial reasoning of large multimodal models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [60] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv:2412.18605, 2024. [61] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [62] Haotian Xia, Zhengbang Yang, Junbo Zou, Rhys Tracy, Yuqing Wang, Chi Lu, Christopher Lai, Yanjun He, Xun Shao, Zhuoqing Xie, et al. Sportu: comprehensive sports understanding benchmark for multimodal large language models. In Proceedings of the International Conference on Learning Representations, 2025. [63] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2237822389, 2024. [64] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [65] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [66] Kaiyu Yang, Olga Russakovsky, and Jia Deng. Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition. In Proceedings of the International Conference on Computer Vision, pages 20512060, 2019. [67] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In Conference on Neural Information Processing Systems, volume 37, pages 21875 21911, 2024. [68] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In Proceedings of the International Conference on Learning Representations, 2023. [69] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the International Conference on Computer Vision, pages 1222, 2023. [70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the International Conference on Computer Vision, pages 1197511986, 2023. 14 [72] Weichen Zhan, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, and Xiao-Ping Zhang. Open3dvqa: benchmark for comprehensive spatial reasoning with multimodal large language model in open space. arXiv preprint arXiv:2503.11094, 2025. [73] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3r: simple approach for estimating geometry in the presence of motion. In Proceedings of the International Conference on Learning Representations, 2025. [74] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In Proceedings of the European Conference on Computer Vision, pages 169186, 2024. [75] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Development of large-scale medical visual question-answering dataset. Communications Medicine, 4(1):277, 2024. [76] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [77] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic In visual benchmark for evaluating mathematical reasoning robustness of vision language models. Proceedings of the International Conference on Learning Representations, 2025."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Dataset 2.1 VGBench . . 2.2 SpatialScore . . . . . . . . . . . . . 2.3 Statistics & Discussion . 3 Methodology 3.1 Problem Formulation . 3.2 Toolbox . . . 3.3 SpatialAgent . . . . . . . . . . . 4 Experiments 4.1 Experimental Settings . 4.2 Quantitative Results . 4.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Related Work 6 Conclusion Broader Impacts Limitations Future Work More Implementation Details D.1 SpaitalScore-Hard Selection . D.2 Data Construction . . . . D.3 SpatialScore Evaluation . . . D.4 SpatialAgent Development D.5 Toolbox Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Data Details More Experiment Results 1 3 3 4 4 4 4 5 6 7 8 9 10 17 17 17 17 17 20 25 31"
        },
        {
            "title": "A Broader Impacts",
            "content": "The potential impact of our work on society lies in several aspects. While VGBench establishes the first visual geometry perception QA benchmark and SpatialScore unifies various spatial reasoning tasks, our current benchmark primarily focuses on publicly available datasets, potentially overlooking assessment of in-the-wild generalization, which may lead to subsequent research becoming overly tailored to existing data. Similarly, though SpatialAgent demonstrates the promise of multi-agent systems for spatial reasoning, it remains an interim solution, and fundamental advances in visual geometry perception capabilities of MLLMs remain needed. Nevertheless, on the other hand, our VGBench, SpatialScore, and SpatialAgent also have some positive impacts on the research and industry of computer vision and embodied AI: The establishment of comprehensive and diverse benchmark for spatial understanding will stimulate progress in vision systems, especially in designing models with human-like 3D perception abilities, which is critical for real-world applications such as robotics and autonomous navigation. Additionally, the development of the specialized multi-agent system aligns with industry trends, offering practical and scalable solution for complex spatial reasoning tasks."
        },
        {
            "title": "B Limitations",
            "content": "While SpatialScore provides comprehensive and diverse evaluation framework for spatial understanding, and SpatialAgent demonstrates promising improvement with multi-agent system, our work is not without its limitations. Specifically, though SpatialScore covers evaluation across single-image, multi-frame sequences, and videos, it primarily relies on RGB frames, still lacking samples that directly take point clouds, depth maps, or surface normals as input. Moreover, while SpatialAgent significantly boosts spatial understanding capabilities of existing MLLMs by leveraging tools dedicated to visual geometry perception, its current toolbox remains relatively rudimentary. Further integration with more advanced tools, such as those for 3D object detection or 3D grounding, could further improve its performance. These gaps are left to be bridged in future work."
        },
        {
            "title": "C Future Work",
            "content": "To tackle the potential limitations, we outline promising directions for advancing spatial understanding: (i) Beyond RGB images/videos from existing datasets, incorporating diverse in-the-wild data and direct 3D inputs (e.g., point clouds, depth maps) will enhance the evaluation of spatial reasoning capabilities and drive progress in related research areas; (ii) Enriching the toolbox with more 3D scene understanding expert models (e.g., 3D object detection) and facilitating better multi-agent collaboration will yield more robust and scalable solutions for spatial reasoning tasks; (iii) While current training-free approaches demonstrate promise, substantial improvements in the spatial understanding of multimodal large language models still require foundational research, such as curated training datasets or enhanced visual geometry perception mechanisms."
        },
        {
            "title": "D More Implementation Details",
            "content": "In this section, we present more technical details about our work. First, we introduce the selection process of the SpaitalScore-Hard subset in Sec. D.1; Then we elaborate on the construction of VGBench in Sec. D.2; Finally, we provide detailed descriptions of the evaluation on SpatialScore along with development details of SpatialAgent and its toolbox in Sec. D.3, Sec. D.4, and Sec. D.5, particularly highlighting instruction prompts. D.1 SpaitalScore-Hard Selection As described in Sec. 2.2, we obtain the final 1,400 samples in SpatialScore-Hard through voting process involving 20 MLLMs of varying architectures and scales, followed by manual curation. Specifically, these 20 MLLMs include: InternVL2.5 [6] (1B, 4B, 8B, 38B, and 78B), InternVL3 [76] (1B, 8B, 14B, 38B, and 78B), Kimi-VL [52] (3B), Qwen2.5VL [2] (7B, 32B, and 72B), LLaVA-OneVision [24] (7B and 72B), LLaMA3.2V [16] (11B), LLaMA-3.2V-CoT [64] (11B), and SpaceQwen2.5VL [4] (3B). We exclude models such as Cambrian [54] (8B), LLaVA-1.5 [33] (13B), SpatialBot [3] (3B), SpaceLLaVA [4] (13B), and Kimi-VLThinking [52] (3B), mainly due to their inability to support multi-frame sequences and video inputs (limited to single or dual images), or their poor instruction following capability, which may introduce noise into the reposenses. The initial voting yields approximately 4,300 samples, which are further refined through human verification to produce the final 1,400 representative and challenging samples within SpatialScore-Hard. D.2 Data Construction As mentioned in Sec. 2.2, we construct our VGBench with approximately 300 scenes sourced from ScanNet [10], ScanNet++ [69], CA-1M [23], and WildRGB-D [63], to evaluate the visual geometry perception capabilities of current MLLMs through question-answering. Considering that ScanNet [10] has been widely utilized in previous work [14, 29, 40] and may have already been included in models training data, we deliberately reduce its proportion in our dataset construction. After extracting essential key information from the scenes, we initially generate corresponding questions and contexts using predefined templates for each category, as presented below. Task 1: Camera Parameter Estimation. The ground truth camera parameters are derived directly from the intrinsic matrix, which contains the focal lengths (fx, fy) in pixel units, principal point coordinates (cx, cy), HFOV (horizontal field of view), and VFOV (vertical field of view). The distractor options are synthesized by LLMs such as DeepSeek-v3 [31] or uniformly sampling from intervals (0.15, 0.05) and (0.05, 0.15). \"question\": \"What is the camera parameter for this image?\", \"context\": \"Please select the correct answer based on the given camera parameters.\" {option_str} Task 2: Metric Depth Estimation. The metric-based depth values are directly obtained from the depth map within datasets to serve as ground truth, while three equally-sized regions are randomly selected as distractors for multi-choice questions. \"question\": \"What is the depth (in meters) of the region with BBOX (x1, y1, x2, y2) = ({x1}, {y1}, {x2}, {y2})?\", \"context\": \"Your task is to estimate the depth of the region with BBOX (x1, y1, x2, y2) = ({x1}, {y1}, {x2}, {y2}). Please provide the average depth in meters.\" Task 3: Relative Depth Estimation. \"question\": \"Which of the marked regions has the greatest depth?\", \"context\": \"Four regions are marked in the image. Your task is to identify which region has the greatest average depth.\" {option_str} Task 4: Distance Estimation. We construct distance estimation samples using CA-1M [23]. The ground truth distance is computed by randomly selecting two objects from the instance annotations and calculating the Euclidean distance between their 3D centroids. For judgment and multi-choice questions, we generate distractor distances by applying uniformly random perturbations within (50%, 50%) of the correct distance. \"question\": \"What is the metric distance between the center of the object inside the 2D region ({x1a}, {y1a}, {x2a}, {y2a}) and the center of the object inside the 2D region ({x1b}, {y1b}, {x2b}, {y2b}) in the image?\", \"context\": \"Calculate the real-world distance in 3D space between two 2D regions. Answer using meters as the unit.\" Task 5: Image to Homography Matrix Estimation. We construct homography QA samples by first randomly selecting source image and generating four distinct random homography matrices, which are then applied to produce four corresponding warped images. The evaluation questions are formulated in two complementary formats: (i) matrix-to-image matching, where models must select the correct warped image given specific homography matrix, and (ii) image-to-matrix matching, requiring identification of the correct transformation matrix given warped image. \"question\": \"Which image was generated using the following homography matrix? {matrix_str} \"context\": \"The first image is the original image. {option_str} Task 6: Homography Matrix to Image Estimation. 18 \"question\": \"Which homography matrix was used to transform the first image into the second image? \" \"context\": \"The first image is the original image. {option_str} Task 7: Object Location. We generate object location QA samples by randomly selecting objects from CA-1M frames [23] and extracting their 2D/3D bounding boxes. We then create two question types: (i) multi-choice questions with 3D box options (correct answer plus distractors), and (ii) judgments assessing whether provided 3D box is accurate. Distractors are generated through three methods: adding slight noise to correct boxes, substituting boxes from other objects in the scene, or prompting LLMs (e.g., DeepSeek-v3 [31]) to synthesize plausible alternatives. \"question\": \"Which of the following 3D bounding boxes corresponds to the object inside the 2D region ({x1}, {y1}, {x2}, {y2}) in the image?\" \"context\": \"Your task is to determine which of the provided 3D bounding boxes (in world coordinates) correctly defines the object inside the specified 2D region. {option_str}\" Task 8: Pose Estimation. We construct pose estimation QA samples by randomly selecting two images from scene, calculating the relative camera pose (rotation and translation parameters) between them, and then creating corresponding questions about the spatial transformation between views. The distractors are generated by randomly sampling relative camera poses of other images within the same scene. \"question\": \"What is the relative camera pose between these two images?\" \"context\": \"Your task is to determine the relative camera pose between these two images. Please select from the following options. {option_str} Task 9: 3D Reconstruction. The 3D reconstruction QA samples are generated through three-step procedure: We first randomly select pair of source and target images from the same scene, and then compute the camera transformation matrix between these two viewpoints. Based on this transformation, we formulate questions that ask models to identify which image corresponds to the geometrically transformed version of the source view, with other randomly selected images of the same scene as distractors. \"question\": \"What is the image after transformation {transform} from the first image?\" \"context\": \"Your task is to estimate the image after the pose transformation. Select the correct answer from the following options. {option_str} Task 10: Point Tracking. We first randomly select two images from scene with overlap, and sample random point in the first image and compute its corresponding coordinate in the second image through geometric transformation as ground truth. The other distractors are generated by randomly selecting points (with reasonable distances from the ground truth) from the second image. \"question\": \"In the first image, there is red marker point. In the second image, which point corresponds to the same 3D point as the red marker in the left image?\" \"context\": \"Your task is to identify which point in the second image corresponds to the same 3D point marked in red in the left image. Please select from the following options. {option_str} Moreover, to boost the linguistic diversity of QA pairs while preserving semantic integrity, we employ DeepSeekV3 [31] with the following prompt, to systematically paraphrase both questions and contextual information. You are helpful assistant that paraphrases VQA questions and answers. You will receive an original question and an original context. Your task is to paraphrase both. 19 **Constraint on Placeholders (e.g., \"{option_str}\"):** placeholder should only appear in the paraphrased output (question or context) if, and only if, it was present in the corresponding original input (the original question or the original context, respectively). **Crucially, do not transfer or add placeholders to the paraphrased question if they were only present in the original context.** Paraphrase naturally, otherwise. D.3 SpatialScore Evaluation To ensure reproducibility, we standardize the following configurations: all models adopt deterministic sampling (TEMPERATURE=0.0, DO_SAMPLE=False) and maximum output length of 512 tokens, except for reasoning models like KimiVL-3B-Thinking [52] and LLaMA-3.2V-11B-CoT [64], which are allocated 2048 tokens. For SpatialAgent, we set the maximum attempt limit to 3 iterations under the Plan-Execute paradigm and permit 10 dialogue turns for ReAct interactions. To accommodate the extended reasoning requirements in multi-agent collaboration, the token limit is correspondingly increased to 2048 for these cases. Since SpatialScore encompasses samples from 12 spatial understanding datasets, covering judgments, multichoice, and open-ended questions, we carefully design tailored system prompts for each dataset to ensure models can properly follow instructions and provide correctly formatted answers. The detailed prompts are as follows: For multi-choice questions from CV-Bench [54], MMIU [40], BLINK [14], 3DSRBench [38], and MMVP [55], we expect models to concisely output their selected option, with the corresponding prompt shown below: **Please select the most appropriate answer from options (A), (B), (C), (D), (E), or (F).** **Respond ONLY with the letter and its parentheses, for example: (A)** Question: {question} For samples from SpatialBench [3], VSR [32], SpatialSense [66], RealWorldQA [9], VSI-Bench [65], and VGBench, which typically include judgments, multi-choice, and simple open-ended questions (e.g., counting), we employ the following general prompt to guide models in directly providing their determined correct answers: **Answer concisely with single word, number, or option (e.g., yes, no, 5, 2.2, A).** Question: {question} For metric-based open-ended QAs in QSpatialBench [29] and VGBench, following observations in [29], we adopt the prompt below to instruct models to identify reference objects, perform reasoning, and ultimately provide numerical answers for object sizes or distances along with appropriate measurement units: You will be provided with question and 2D image. The question involves measuring the precise distance in 3D space through 2D image. You will answer the question by providing numeric answer consisting of scalar and distance unit in the format of **scalar {scalar} distance_unit {distance unit}** at the end of your response. Lets think step by step and start by finding good reference objects or object parts in the image. Question: {question} Notably, we have meticulously designed answer-parsing functions for all models to ensure accurate extraction of final answers from their responses. However, certain responses still fail to comply with instruction prompts or result in refusal to answer (primarily occurring in some fine-tuned models and smaller-scale architectures). Considering that instruction-following capability itself constitutes an important metric for evaluating model performance, we deliberately abstain from introducing additional human intervention for correction, instead objectively reporting all corresponding results. D.4 SpatialAgent Development To build SpatialAgent, multi-agent system tailored for spatial understanding, we have meticulously designed series of instruction prompts to guide SpatialAgent to think step-by-step in two distinct paradigms: Plan-Execute and ReAct, with details presented below. 20 For the Plan-Execute paradigm, SpatialAgent primarily consists of three components: planner, executor, and summarizer. First, we use the following prompt to guide the planner in formulating detailed tool invocation plan based on the descriptions in the toolbox: [BEGIN OF GOAL] Generate JSON-formatted tool-calling plan to solve visual understanding questions about given 2D images. [END OF GOAL] [BEGIN OF TOOLBOX] {action_details} [END OF TOOLBOX] [BEGIN OF TASK INSTRUCTIONS] Generate step-by-step plan to answer the given visual understanding question about given 2D images. ***Use ONLY the tools listed in the TOOLBOX section (e.g., GetObjectOrientation, EstimateObjectGeometryProperties, SelfReasoning, DetectObjects, LocalizeObjects, EstimateObjectDepth)*** ***Follow their argument specifications EXACTLY as defined in the toolbox, and try to give detailed and comprehensive instructions in queries.*** Do NOT invent new tools or modify the existing tool interfaces. The plan should strictly follow what these tools can and cannot do. [END OF TASK INSTRUCTIONS] [BEGIN OF FORMAT INSTRUCTIONS] You are helpful assistant tasked with solving visual reasoning questions. Think step by step. ***Return JSON list of tool calls inside json tags, where each call is dictionary with name and arguments. The name MUST match exactly one of the tool names provided in the toolbox. The arguments MUST include ALL required parameters for that specific tool with EXACT parameter names. The image argument must be specified as image-0, image-1, and image-2, to refer to the provided images. Do not answer the question directly, and do not use absolute paths for the image argument. [END OF FORMAT INSTRUCTIONS] [BEGIN OF EXAMPLES] Example for Which is closer to the camera, the dog or the cat?: json [ {\"name\": \"LocalizeObjects\", \"arguments\": {\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"]}}, {\"name\": \"EstimateObjectDepth\", \"arguments\": {\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"], \"indoor_or_outdoor\": \"outdoor\"}}, ] [END OF EXAMPLES] *** Do not answer the question directly. Instead, think step-by-step, and output the tool-calling plan inside json tags. *** Next, the executor follows the prompt below to sequentially execute tool invocations according to the plan. [BEGIN OF GOAL] Generate Chain of Thought (CoT) reasoning process using the provided tool execution results. [END OF GOAL] [BEGIN OF TASK INSTRUCTIONS] You are helpful assistant tasked with solving visual reasoning questions. Analyze the given question and tool execution results. Think step by step. Generate step-by-step reasoning process that shows how the tools contribute to solving the question. Use ONLY the tools and results provided, following their specifications STRICTLY. 21 The results of tool calls can sometimes be incomplete or incorrect, so please be critical and decide how to make use of them. If tool failed, note the failure and proceed with your prior knowledge and reasoning. Repeat for each tool result in order. [END OF TASK INSTRUCTIONS] [BEGIN OF FORMAT INSTRUCTIONS] *** Output CoT with: - <thinking> Explain why this tool was used and how its result contributes to the answer. </thinking> - <tool> The tool call in JSON format, e.g., {{\"name\": \"LocalizeObjects\", \"arguments\": {{\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"]}}}}. </tool> - <observation>: The tool result as string. </observation> Repeat for each tool result in order. *** [END OF FORMAT INSTRUCTIONS] [BEGIN OF EXAMPLES] Example for In image-0, which is closer to the camera, the dog or the cat?: <thinking> To determine which object is closer to the camera, need first localize the dog and cat in the image. </thinking> <tool> {{\"name\": \"LocalizeObjects\", \"arguments\": {{\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"]}}}} </tool> <observation> {{\"results\": [{{\"label\": \"dog\", \"region\": [0.5, 0.6, 0.6, 0.8], \"confidence\": 0.95}}, {{\"label\": \"cat\", \"region\": [0.4, 0.5, 0.45, 0.7], \"confidence\": 0.87}}]}} </observation> <thinking> The bounding box for the dog is [0.5, 0.6, 0.6, 0.8], and for the cat is [0.4, 0.5, 0.45, 0.7]. Then, need estimate the depth of them to reflect their distances to the camera. </thinking> <tool> {{\"name\": \"EstimateObjectDepth\", \"arguments\": {{\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"], \"indoor_or_outdoor\": \"outdoor\"}}}} </tool> <observation> {{\"results\": [{{\"object\": \"dog\", \"depth\": 1.0, \"error\": null}}, {{\"object\": \"cat\", \"depth\": 1.2, \"error\": null}}]}} </observation> [END OF EXAMPLES] Tool Plan: {tool_plan} Tool Results: {tool_results} *** **Notably, you should AVOID outputting terms like <final_thining>, <answer>, or <final_answer> here.** **Now, output your reasoning between <thinking> and </thinking>, the tool call in JSON format between <tool> and </tool>, and the observation between <observation> and </observation>.** *** Finally, the summarizer consolidates the tool execution results and produces the final reasoning and answer, guided by the following instruction prompt: [BEGIN OF GOAL] Generate final REASONING and ANSWER for {GOAL_TYPE} about given images, based on tool results and prior Chain of Thought (CoT) steps. {NUMERIC_ANSWER} [END OF GOAL] [BEGIN OF TASK INSTRUCTIONS] You are helpful assistant tasked with solving visual reasoning questions. {TASK_SPECIFIC} Given the question, tool execution results, and CoT steps, synthesize the information to provide final REASONING and ANSWER. **The results of tool calls can sometimes be incomplete or incorrect, so please be critical and decide how to make use of them.** If tool results are unclear or contradictory, use your prior knowledge to think the problem step-by-step. {ANSWER_FORMAT} 22 {CRITICAL_NOTE} [END OF TASK INSTRUCTIONS] [BEGIN OF FORMAT INSTRUCTIONS] *** Output: <thinking> complete analysis synthesizing all tool results and CoT steps to derive the answer. </thinking> <answer> The final answer, formatted as: {OUTPUT_ANSWER} </answer> *** [END OF FORMAT INSTRUCTIONS] CoT Steps: {cot_steps} *** {CRITICAL_NOTE} {FINAL_INSTRUCTION} *** Notably, as discussed in Sec. D.3, we have designed tailored system prompts to handle diverse question formats from multiple data sources in SpatialScore. Accordingly, when constructing SpatialAgent, we also incorporate some specific variables (including NUMERIC_ANSWER, TASK_SPECIFIC, ANSWER_FORMAT, OUTPUT_ANSWER, CRITICAL_NOTE, and FINAL_INSTRUCTION) to handle these distinct questions, whose values vary across datasets as detailed below. For samples from CV-Bench, MMIU, BLINK, 3DSRBench, and MMVP: { NUMERIC_ANSWER = \"\" TASK_SPECIFIC = \"Think step by step.\" ANSWER_FORMAT = \"Select the most appropriate answer from options based on reasoning (e.g., A, (b)).\" OUTPUT_ANSWER = \"[The option letter of your final answer] (e.g., (A), B)\" CRITICAL_NOTE = \"**CRITICAL: You MUST always provide reasonable answer with the corresponding option. Never respond with cannot be determined, none of the above, or similar phrases.**\" FINAL_INSTRUCTION = \"Now, output **your thinking** between <thinking> and </thinking>, and **your answer with the corresponding option** between <answer> and </answer>.\" } For samples from Spatialbench, VSR, SpatialSense, VSI-Bench, RealWorldQA, and VGBench: { NUMERIC_ANSWER = \"\" TASK_SPECIFIC = \"Think step by step.\" ANSWER_FORMAT = \"Answer with an exact number, or option (e.g., YES, no, 5, A, (B), c).\" OUTPUT_ANSWER = \"**exact option or number** (e.g., YES, no, 5, A, (B), (c))\" CRITICAL_NOTE = \"**CRITICAL: You MUST always provide reasonable answer with the corresponding option. Never respond with cannot be determined, none of the above, or similar phrases.**\" FINAL_INSTRUCTION = \"Now, output **your answer with the corresponding option or exact number** between <answer> and </answer>.\" } For samples from QSpatialBench: { NUMERIC_ANSWER = \"**CRITICAL: You MUST provide specific numerical answer with measurement units, even if uncertain. Use your knowledge of typical object sizes as reference when needed.**\" TASK_SPECIFIC = \"Think step by step and start by finding good reference objects or object parts in the image.\" ANSWER_FORMAT = \"Ensure the answer uses STANDARD measurement units: meter, centimeter, feet, or inches (e.g., 3.25 meters). Look for clues such as familiar objects of known size to establish scale in the image.\" OUTPUT_ANSWER = \"**exact number with specific units** (e.g., **5.0 centimeters**, **2.3 meters**, **8.5 inches**)\" CRITICAL_NOTE = \"**CRITICAL: You MUST always provide specific numerical answer with units. Never respond with cannot be determined or similar phrases.**\" FINAL_INSTRUCTION = \"Now, output **your thinking** between <thinking> and </thinking>, and **your precise answer with STANDARD measurement units (meter, centimeter, feet, or inches) between <answer> and </answer>.**\" } Moreover, in the Plan-Execute paradigm, scenarios may arise where either (i) the planner fails to generate correct plan, or (ii) the executor encounters tool invocation failures. To address this, we set maximum attempt threshold (default to 3). When the system exceeds this limit without completing the Plan-Execute reasoning process, SpatialAgent will bypass the workflow and directly answer the question using the following prompt: [BEGIN OF GOAL] Provide direct ANSWER to spatial understanding questions about given 2D images without external tools. {NUMERIC_ANSWER} [END OF GOAL] [BEGIN OF TASK INSTRUCTIONS] You are helpful assistant tasked with solving visual reasoning questions. {TASK_SPECIFIC} Answer the visual understanding question by reasoning about the provided image. Provide direct answer by reasoning logically based on typical spatial relationships and visual cues in the image. {ANSWER_FORMAT} {CRITICAL_NOTE} [END OF TASK INSTRUCTIONS] [BEGIN OF FORMAT INSTRUCTIONS] Output your response in the format: <thinking> [Your reasoning here] </thinking> <answer> OUTPUT_ANSWER </answer> [END OF FORMAT INSTRUCTIONS] {CRITICAL_NOTE} {FINAL_INSTRUCTION} For the ReAct paradigm, we employ the following instruction prompt, and the ASSISTANT_PROMPT used here is identical to the one defined in Sec. D.3. [BEGIN OF GOAL] You are helpful assistant, and your goal is to solve the # USER REQUEST #. You can either rely on your own capabilities or perform actions with external tools to help you. list of all available actions is provided to you below. [END OF GOAL] [BEGIN OF ACTIONS] {for each action in actions} [END OF ACTIONS] [BEGIN OF TASK INSTRUCTIONS] 1. You must only select actions from # ACTIONS #. 2. You can only call one action at time. 3. If no action is needed, please make actions an empty list (i.e. \"actions\": []). 4. You must always call **Terminate** with your final answer at the end. [END OF TASK INSTRUCTIONS] 24 [BEGIN OF FORMAT INSTRUCTIONS] Your output should be in strict JSON format as follows: { } \"thought\": \"the thought process, or an empty string\", \"actions\": [{\"name\": \"action1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}}] If you terminate with an answer, please follow the rules below: For multi-choice questions, output **your answer with the corresponding option** between <answer> and </answer>. For numeric questions, output **your answer with the specific unit (like meter or centimeter)** between <answer> and </answer>. [END OF FORMAT INSTRUCTIONS] [BEGIN OF EXAMPLES] {for each demo in demo_examples} [END OF EXAMPLES] {ASSISTANT_PROMPT} D.5 Toolbox Specifications To facilitate our proposed multi-agent system, SpatialAgent, to perform visual reasoning for spatial understanding questions effectively through tool invocation and collaboration, we have designed detailed input-output descriptions for each tool, accompanied by concrete examples. These specifications serve as contextual information for the agent core to select and invoke proper expert tools, with the details elaborated as follows. For 2D perception tools, we first implement the DetectObjects action using the RAM++ [20] model, with the detailed tool specification presented below: description = \"\"\" Detect objects present in an image using the RAM++ model. Return list of object labels found in the image. Useful for identifying what objects exist in scene before detailed analysis. \"\"\" args_spec = {\"image\": \"The image to analyze.\"} rets_spec = {\"objects\": \"List of the object names detected in the image.\"} examples = [{\"name\": \"DetectObjects\", \"arguments\": {\"image\": \"image-0\"}}] Next, we adopt Owlv2 [41] to create LocalizaObjects function, which can localize objects based on given text prompts, with the following functionality explanation: description = \"\"\" Detect and localize specific objects in an image. This tool identifies objects matching your text descriptions and returns their bounding boxes. For each object category, return the highest confidence detection to avoid duplicates. Useful for: Finding specific objects within scene; Getting object positions for spatial reasoning; Preparing regions for more detailed analysis. \"\"\" args_spec = { \"image\": \"The image to analyze.\", \"objects\": \"A list of object names to detect, e.g., [dog, cat, chair].\" } rets_spec = { \"regions\": \"A list of detected regions, each represented by dictionary containing the label (object name), bounding box (pixel coordinates [left, top, right, bottom]), and confidence score.\" } examples = [ {\"name\": \"LocalizeObjects\", \"arguments\": {\"image\": \"image-0\", \"objects\": [\"dog\", \"cat\"]}}, {\"name\": \"LocalizeObjects\", \"arguments\": {\"image\": \"image-1\", \"objects\": [\"chair\", \"table\", \"lamp\"]}} 25 ] Then, we integrate Owlv2 [41] for advanced object localization and SAM2 [47] for precise segmentation, creating GetObjectMask function, with the following tool description: description = \"\"\" Generate precise masks for objects in images using Owlv2 + SAM2. This tool identifies objects in an image and generates pixel-level segmentation masks for each object. It returns the mask area ratios and bounding boxes for each detected object. Useful for analyzing object shapes, sizes, and measuring coverage. \"\"\" args_spec = { \"image\": \"The image to generate masks.\", \"objects\": \"A list of object descriptions of the objects to localize and segment (e.g., [dog, cat]).\" } rets_spec = { \"results\": \"A list of dictionaries, each containing the mask area ratio and bounding box for an object. Format: [{object: str, mask_area: float, bbox: [left, top, right, bottom], error: str or None}]\" } examples = [ {\"name\": \"GetObjectMask\", \"arguments\": {\"image\": \"image-0\", \"objects\": [\"coffee mug\", \"microwave\"]}}, {\"name\": \"GetObjectMask\", \"arguments\": {\"image\": \"image-1\", \"objects\": [\"sofa\", \"television\"]}} ] To further equip SpatialAgent with motion understanding and image transformation analysis, we have developed specialized expert tools such as EstimateOpticalFlow action implemented with RAFT [53]: description = \"\"\" Estimate optical flow between two images using RAFT. This tool analyzes motion (in pixels) between consecutive frames and returns the average pixel displacement in both horizontal (x) and vertical (y) directions. The first image occurs earlier in time, and the second image occurs later. - Positive mean_flow_x indicates objects move left; negative indicates right. - Positive mean_flow_y indicates objects move up; negative indicates down. - For camera motion, the interpretation is reversed: positive mean_flow_x indicates camera moves right, negative indicates left; positive mean_flow_y indicates camera moves down, negative indicates up. - This tool is useful for analyzing camera movement, object motion detection, and 3D spatial reasoning. \"\"\" args_spec = { \"image\": \"A list of exactly two image paths to compute optical flow between. The first image is earlier in time.\" } rets_spec = { \"output\": \"Dictionary containing mean_flow_x (average horizontal pixel displacement) and mean_flow_y (average vertical pixel displacement).\" } examples = [ {\"name\": \"EstimateOpticalFlow\", \"arguments\": {\"image\": [\"image-0\", \"image-1\"]}, {\"name\": \"EstimateOpticalFlow\", \"arguments\": {\"image\": [\"image-1\", \"image-2\"]}} ] The MatchImagesSIFT functionality performs keypoint extraction and feature matching between images using SIFT [36] implemented via OpenCV. The detailed specifications are as follows: description = \"\"\" Match keypoints between two images using SIFT (Scale-Invariant Feature Transform). 26 Detects distinctive features in both images and finds corresponding pairs. Returns matched coordinate pairs between the images for applications like image alignment, object recognition, or structure from motion. \"\"\" args_spec = { \"image\": \"A list of two image paths to detect keypoints and perform matching.\", \"num_keypoints\": \"Maximum number of keypoints to detect per image (default: 1200).\", \"ratio_th\": \"Ratio test threshold for keypoint matching (default: 0.75).\" } rets_spec = { \"matches\": \"List of matched coordinate pairs [[x1,y1], [x2,y2]], where [x1,y1] is from the first image and [x2,y2] is from the second.\", \"num_matches\": \"Total number of matches found between the images.\" } examples = [ {\"name\": \"MatchImagesSIFT\", \"arguments\": {\"image\": [\"image-0\", \"image-1\"], \"num_keypoints\": 1200, \"ratio_th\": 0.75}}, {\"name\": \"MatchImagesSIFT\", \"arguments\": {\"image\": [\"image-2\", \"image-3\"], \"num_keypoints\": 1500, \"ratio_th\": 0.8}} ] The EstimateHomographyMatrix tool, also implemented via OpenCV, calculates the homography transformation matrix between two images based on extracted keypoints, with the following function description: description = \"\"\" Compute the homography transformation matrix between two images using SIFT features and RANSAC. homography is 3x3 matrix that maps points from one image to corresponding points in another image, useful for image alignment, perspective correction, and measuring planar transformations. \"\"\" args_spec = { \"image\": \"A list of two image paths to compute the homography between.\", \"num_keypoints\": \"Maximum number of keypoints to detect per image (default: 1200).\", \"ratio_th\": \"Ratio test threshold for keypoint matching (default: 0.75).\", \"ransac_reproj_threshold\": \"Maximum allowed reprojection error in RANSAC to treat point pair as an inlier (default: 5.0).\" } rets_spec = { \"homography_matrix\": \"3x3 homography matrix mapping points from the first image to the second image.\", \"inliers_count\": \"Number of inlier matches used to compute the homography.\", \"total_matches\": \"Total number of matches found between the images.\", \"status\": \"Success or failure status of the homography estimation.\" } examples = [ {\"name\": \"EstimateHomographyMatrix\", \"arguments\": {\"image\": [\"image-0\", \"image-1\"], \"num_keypoints\": 1200, \"ratio_th\": 0.75, \"ransac_reproj_threshold\": 5.0}}, {\"name\": \"EstimateHomographyMatrix\", \"arguments\": {\"image\": \"num_keypoints\": 1500, \"ratio_th\": 0.8, \"ransac_reproj_threshold\": 3.0}} ] [\"image-2\", \"image-3\"], Additionally, to empower SpatialAgent with 3D spatial reasoning capabilities, we have integrated specialized visual geometry models. The EstimateObjectGeometryProperties function is implemented via the integration of SAM2 [47], DepthAnythingv2 [67], and VGGT [56] to obtain detailed spatial and geometry properties of objects in the given image, with the following tool specification: description = \"\"\" Get detailed spatial and geometry properties of objects in an image, including object bounding boxes, mask areas, depth measurements, and camera parameters. Uses advanced computer vision techniques to provide accurate measurements of object properties. 27 Camera parameters include intrinsic and extrinsic matrices for 3D geometry applications. ALL DEPTH VALUES ARE RETURNED IN METERS, not in arbitrary units or pixels. \"\"\" args_spec = { \"image\": \"The image to analyze (file path).\", \"object_descs\": \"List of strings describing the objects to analyze (e.g., [dog, cat]).\" } rets_spec = { \"results\": \"List of dictionaries with object details. Format: [{object: str, bbox: [left, top, right, bottom], mask_area: float, depth: float, error: str or None}]\", \"camera_parameters\": \"Dictionary with intrinsic (3x3 matrix) and extrinsic (3x4 matrix) parameters. Format: {intrinsic: [[float]], extrinsic: [[float]]}\" } examples = [ {\"name\": \"EstimateObjectGeometryProperties\", \"arguments\": {\"image\": \"image-0\", \"object_descs\": [\"coffee cup\", \"keyboard\"]}}, {\"name\": \"EstimateObjectGeometryProperties\", \"arguments\": {\"image\": \"image-1\", \"object_descs\": [\"dog\", \"cat\"]}}, {\"name\": \"EstimateObjectGeometryProperties\", \"arguments\": {\"image\": \"image-2\", \"object_descs\": [\"the red car\", \"tree\"]}} ] We have also implemented EstimateRegionDepth action with DepthAnythingv2 [67] for scene depth estimation and region-specific average depth calculation based on given bounding boxes, with the following tool description: description = \"\"\" Estimate the metric depth (in meters) of specific region in an image using DepthAnythingV2, reflecting how far the region is from the camera position (in meters). This tool supports both indoor (range: 0-20m) and outdoor scenes (range: 0-80m), and returns precise depth measurements for the specified region. It works with both single and multiple bounding boxes in pixel coordinates. NOTE: Depth represents the distance from camera to object, NOT distances between objects or object sizes. ALL DEPTH VALUES ARE RETURNED IN METERS, not in arbitrary units or pixels. \"\"\" args_spec = { \"image\": \"The image to analyze.\", \"bboxes\": \"List of bounding boxes [[left, top, right, bottom], ...] in pixel coordinates. For single region, you can use single list [left, top, right, bottom].\", \"indoor_or_outdoor\": \"Scene type (indoor or outdoor) to select appropriate depth model.\", \"mode\": \"Method for depth calculation: mean (average over region) or center (center point only). Default: mean.\" } rets_spec = { \"depths\": \"List of dictionaries with bounding box and estimated depth (in meters), format: [{bbox: list, depth: float, error: str or None}]\", \"unit\": \"The unit of the depth value (always meters).\" } examples = [ {\"name\": \"EstimateRegionDepth\", \"arguments\": {\"image\": \"image-0\", \"bboxes\": [100, 50, 200, 150], \"indoor_or_outdoor\": \"indoor\"}}, {\"name\": \"EstimateRegionDepth\", \"arguments\": {\"image\": \"image-1\", \"bboxes\": [50, 20, 150, 120], \"indoor_or_outdoor\": \"outdoor\"}}, {\"name\": \"EstimateRegionDepth\", \"arguments\": {\"image\": \"image-2\", \"bboxes\": [[100, 50, 200, 150], [150, 100, 250, 200]], \"indoor_or_outdoor\": \"indoor\"}} ] By combining Owlv2 [41] and DepthAnythingv2 [67], we also facilitate EstimateObjectDepth, with the corresponding specification as follows: 28 description = \"\"\" Estimate the metric depth (in meters) of objects in an image using DepthAnythingV2, reflecting how far objects are from the camera position (in meters). This tool supports both indoor (0-20m range) and outdoor scenes (0-80m range), and returns precise depth measurements for multiple objects. Here, smaller values indicate objects are closer to the camera, but do not indicate their own sizes. NOTE: Results represent camera-to-object distances, NOT distances between objects. ALL DEPTH VALUES ARE RETURNED IN METERS, not in arbitrary units or pixels. \"\"\" args_spec = { \"image\": \"The image to analyze.\", \"objects\": \"List of object descriptions to measure distance to (e.g., [dog, cat]).\", \"indoor_or_outdoor\": \"Scene type (indoor or outdoor) to select appropriate depth model.\" } rets_spec = { \"results\": \"A list of dictionaries, each containing the object description and its estimated depth (in meters). Format: [{object: str, depth: float, error: str or None}]\" } examples = [ {\"name\": \"EstimateObjectDepth\", \"arguments\": {\"image\": \"image-0\", \"objects\": [\"the red car\"], \"indoor_or_outdoor\": \"outdoor\"}}, {\"name\": \"EstimateObjectDepth\", \"arguments\": {\"image\": \"image-1\", \"objects\": [\"dog\", \"cat\"], \"indoor_or_outdoor\": \"outdoor\"}}, {\"name\": \"EstimateObjectDepth\", \"arguments\": {\"image\": \"image-2\", \"objects\": [\"coffee cup\", \"keyboard\"], \"indoor_or_outdoor\": \"indoor\"}} ] OrientAnything [60] model is employed for the GetObjectOrientation functionality: description = \"\"\" Estimate 3D orientation of objects in images using Orient-Anything model. This tool: - Measures three key angles of objects in an image - Azimuth: Horizontal rotation angle (0-360 clockwise) - Polar: Vertical inclination angle (0-180) - Rotation: In-plane rotation (-180 to +180) - Returns confidence score for reliability assessment Useful for 3D understanding, pose estimation, and spatial reasoning tasks. \"\"\" args_spec = { \"image\": \"The image to analyze.\", \"objects\": \"List of object descriptions to analyze. For single object, you can pass string or single-element list.\" } rets_spec = { \"results\": \"List of objects with orientation data, format: [{object: str, angle_data: {azimuth: float, polar: float, rotation: float, confidence: float}, error: str or None}]\" } examples = [ {\"name\": \"GetObjectOrientation\", \"arguments\": {\"image\": \"image-0\", \"objects\": \"a red car\"}}, {\"name\": \"GetObjectOrientation\", \"arguments\": {\"image\": \"image-1\", \"objects\": [\"a red car\", \"a blue chair\"]}} ] Furthermore, we have developed suite of general-purpose tools to assist SpatialAgent in tool invocation and reasoning processes, such as the Calculate function below: description = \"Calculate mathematical expression.\" args_spec = {\"expression\": \"The mathematical expression to calculate (e.g., 2 + 2, 10 * 5.5).\"} rets_spec = {\"result\": \"The numerical result of the calculation.\"} 29 examples = [ {\"name\": \"Calculate\", \"arguments\": {\"expression\": \"2 + 2\"}}, {\"name\": \"Calculate\", \"arguments\": {\"expression\": \"4 * 9 * 84\"}}, {\"name\": \"Calculate\", \"arguments\": {\"expression\": \"5 - 4 / 2\"}}, ] Moreover, we selectively integrated fundamental OpenCV-based image processing tools, such as Crop and Resize, into specialized expert models to enhance their functionality. description = \"\"\" Crop an image to focus on specific region of interest with the bounding box. It labels the cropped region with bounding box and crops the region with some margins around the bounding box to help with contextual understanding of the region. \"\"\" args_spec = { \"image\": \"The image to crop.\", \"bbox\": \"Bounding box coordinates [left, top, right, bottom] as normalized values between 0.0 and 1.0 representing percentage of image dimensions.\" } rets_spec = {\"image\": \"The cropped image as base64-encoded string.\"} examples = [{\"name\": \"Crop\", \"arguments\": {\"image\": \"image-0\", \"bbox\": [0.33, 0.21, 0.58, 0.46]}}] Finally, we design dedicated Terminate action to formally conclude the reasoning process and give structured final answers. description = \"\"\" Use this function ONLY when you are completely confident in your final answer. For multiple-choice questions: Specify the letter of the correct option (e.g., A, B, C, D) at the beginning. For numerical answers: Include both the specific value and appropriate unit of measurement (e.g., meter or centimeter). For yes/no questions: Clearly state Yes or No followed by your explanation. DO NOT call this function if you are uncertain or need to perform additional analysis. Double-check your answer before terminating! \"\"\" args_spec = {\"answer\": \"The final answer with proper formatting. For multiple choice: include letter (e.g., A. explanation or (B)). For numerical answers: include units (e.g., 3.25 meters).\"} rets_spec = {\"answer\": \"The final answer that will be submitted.\"} examples = [ {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"A. Yes, because the geometric relationship shows the objects are adjacent.\"}}, {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"(B) The second object is twice as large as the first.\"}}, {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"B. 3.25 meters, measured from the front edge of the table to the wall.\"}}, {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"(A) 2 inches, which is the minimum clearance required.\"}}, {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"The distance between objects is 47.3 centimeters.\"}}, {\"name\": \"Terminate\", \"arguments\": {\"answer\": \"The angle between the two planes is 38.2 degrees.\"}}, ] SelfThinking module is designed to guide MLLMs in self-reflecting on questions through meticulous prompt engineering, thereby fully leveraging their inherent potential to better tackle spatial understanding tasks. description = \"\"\" Query model itself with both text and image (REQUIRED) to perform detailed reasoning about the visual inputs. **Note: detailed and comprehensive instruction or question is helpful for this tool to provide in-depth reasoning and analysis.** GOOD FOR: Visual reasoning; Multi-step analysis; Detailed explanations; Complex pattern recognition; Hypothesis generation. \"\"\" args_spec = { \"image\": \"The image to analyze (REQUIRED).\", \"query\": \"The text question or reasoning task about the image (REQUIRED).\", } rets_spec = {\"response\": \"The models detailed reasoning response about the image.\"} examples = [ {\"name\": \"SelfThinking\", \"arguments\": {\"query\": \"Analyze the spatial relationship between objects in this image.\", \"image\": \"image-0\"}}, {\"name\": \"SelfThinking\", \"arguments\": {\"query\": \"Reason step by step about the composition of this scene.\", \"image\": \"image-0\"}}, {\"name\": \"SelfThinking\", \"arguments\": {\"query\": \"What can you infer about the physical structure of these objects?\", \"image\": \"image-0\"}}, {\"name\": \"SelfThinking\", \"arguments\": {\"query\": \"Compare the relative positions of elements in the scene.\", \"image\": \"image-1\"}}, {\"name\": \"SelfThinking\", \"arguments\": {\"query\": \"Explain the potential causes of the visual patterns observed.\", \"image\": \"image-0\"}}, ]"
        },
        {
            "title": "E Additional Data Details",
            "content": "In this section, we provide further details about our established VGBench and SpatialScore benchmarks. Concretely, we first provide details about the distribution of question formats and input modalities, as well as data sources across VGBench and SpatialScore, in Table 4 and Table 5, respectively. Subsequently, Figure 5 and Figure 6 present statistical overviews of data sources and task categories in VGBench and SpatialScore, respectively. Furthermore, we showcase representative examples from each category of VGBench in Figure 7, which primarily focuses on diverse visual geometry perception tasks. By integrating these datasets, SpatialScore encompasses various spatial reasoning tasks of diverse questionanswering formats (judgment, multi-choice, and open-ended) and input modalities (single-image, multi-frame sequence, and video), establishing the most comprehensive and heterogeneous benchmark for spatial understanding to date. This makes it an effective testbed for evaluating the spatial reasoning capabilities of existing MLLMs. We believe this advancement will further propel research progress in the field of spatial intelligence. Table 4: Question Format and Input Modality Distribution of VGBench and SpatialScore. VGBench SpatialScore QA Format Num. Modality Num. QA Format Num. Modality Num. Judgment 1,132 Single-image 2,800 Judgment Multi-choice 4,032 Multi-image 3,200 Multi-choice 17,833 Multi-image Open-ended Open-ended Video 4,353 836 5,907 Single-image 17,058 5,905 5,130 Video Table 5: Data Sources Distribution of VGBench and SpatialScore. VGBench SpatialScore Sources Num. Sources Num. Sources Num. Sources Num. ScanNet ScanNet++ 1,214 SpatialBench 873 QSpatialBench CA-1M 3,289 WildRGB-D 624 MMVP BLINK 174 RealWorldQA 765 SpatialSense 3,622 1,222 VSI-Bench 5,130 271 2,133 3DSRBench 5,157 300 6,000 2,638 VGBench 681 VSR MMIU CV-Bench Figure 5: Data sources and task category statistics visualization of VGBench."
        },
        {
            "title": "F More Experiment Results",
            "content": "In this section, we will present additional experimental details and results, followed by comprehensive and in-depth analysis and discussion about the source datasets within SpatialScore. First, we present the performance of current MLLMs on CV-Bench [54] and 3DSRBench [38] in Table 6. The results illustrate that current models excel at 2D positional relationship perception and simple 3D understanding tasks in CV-Bench, but demonstrate limited capabilities when handling complex 3D perception tasks involving orientation awareness and multi-object reasoning in 3DSRBench. Thus, by integrating all of them, our constructed SpatialScore thoroughly covers spatial understanding samples with varying focuses and difficulty levels, thereby establishing the most comprehensive and diverse spatial reasoning evaluation benchmark to date. 32 Figure 6: Data sources and task category statistics visualization of SpatialScore. Then, in Table 7, Table 8, we showcase the performance on two general multi-image evaluation benchmarks, MMIU [40] and BLINK [14], with focus on their spatial-related categories. Notably, certain models, such as Qwen2.5VL-72B [2] and InternVL3-78B [76] achieve exceptionally high performance on the 3D pose estimation task in MMIU, primarily because the distractors in their questions lack sufficient complexity, allowing the models to leverage the powerful logical reasoning capabilities of their language components to find shortcuts from textual perspective. In contrast, when constructing VGBench, we meticulously design noise perturbations and employ LLM rewriting strategies to create numerically plausible yet highly deceptive distractors, ensuring the high quality of our benchmark data. Subsequently, Table 9 presents the performance on VSI-Bench [65], where video serves as input. Since we focus on general MLLMs without specialized designs for video tasks, we uniformly sample 8 frames from each video by default to ensure compatibility across most models. For the open-ended, continuous value prediction tasks in this dataset, we follow its proposed Mean Relative Accuracy metric. In contrast, when evaluating these samples in our SpatialScore, we adopt δ = 2 tolerance threshold following QSpatialBench [29] (regarding answers within 0.5 to 2 of ground truth as correct). Regarding overall accuracy, the results in Table 9 adhere to the original calculation approach in [65], which represents the mean of accuracies across each category. Whereas SpatialScore employs more objective and balanced evaluation methodology, which computes it as the proportion of correct predictions across all samples. By incorporating these multi-frame image sequences and video-based samples into our SpatialScore, we further diversify its question formats and input modalities, thereby ensuring more robust and professional evaluation of spatial understanding capabilities. Finally, in Table 10, we present results across multiple benchmarks, including RealWorldQA [9] and MMVP [55] for comprehensive evaluation, VSR [32] and SpatialSense [66] for spatial relation judgment, SpatialBench [3] for embodied AI-related spatial reasoning, and QSpatialBench for open-ended distance/size estimation. 33 Figure 7: Representative examples from distinct categories in VGBench. Here, for MMVP [55], we adhere to its original evaluation protocol, where correctness requires accurate answers to both samples of the same question (i.e., question-level accuracy), while in our SpatialScore (Sec. 4), we adopt sample-level accuracy for the included MMVP samples. Regarding VSR [32], we report results under both data partitions (Zero-Shot and Random). To prevent potential data leakage from models being trained on related data, we exclusively use the Zero-Shot partition for evaluation in SpatialScore. As shown in Table 10, while current MLLMs achieve impressive performance on these relatively simple tasks, there remains notable room for improvement on open-ended QAs within QSpatialBench [29], particularly its Plus subset containing diverse in-the-wild scenarios, unlike ScanNet data that may have been included in model training. Accordingly, when constructing VGBench, we carefully control the proportion of potentially pre-trained data while prioritizing more recent datasets to ensure evaluation robustness. The integration of VGBench with these diverse datasets ensures that SpatialScore encompasses spatial understanding samples across various input modalities, question formats, and data sources. To the best of our knowledge, this represents the most extensive and diverse benchmark for spatial understanding to date. We anticipate that this work will provide valuable insights for the future development of MLLMs and serve as catalyst for advancements in spatial intelligence research. 34 Table 6: Quantitative Results on CV-Bench and 3DSRBench. Here, Multi-obj. refers to mutliobject reasoning. Results with the best and second best results are bolded and underlined respectively. Methods Overall 2D-count 2D-relation 3D-depth 3D-distance Overall Height Location Orientation Multi-obj. CV-Bench 3DSRBench InternVL2.5-1B InternVL3-1B 54.09 53.37 59.90 65.10 SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B-Instruct InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B-Instruct Cambrain-8B InternVL2.5-8B InternVL3-8B LLaMA-3.2V-11B LLaMA-3.2V-11B-CoT LLaVA-1.5-13B SpaceLLaVA-13B InternVL3-14B Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B LLaVA-OneVision-72B Qwen2.5-VL-72B InternVL2.5-78B InternVL3-78B 51.40 55.95 71.04 57.47 68.46 74.07 73.20 76.61 62.24 76.95 82.49 58.19 72.78 58.19 58.19 82. 79.68 83.32 84.00 79.68 82.26 85.33 85.71 62.18 60.28 65.48 60.53 62.56 68.02 69.16 63.71 60.66 68.65 73.98 59.01 59.14 56.09 56.09 70.18 68.91 74.75 74. 70.18 69.67 76.65 76.52 1B models 53.33 55.50 48.33 39.17 47.80 46.60 49.86 52. 48.10 46.69 3B and 4B models 45.38 54.17 74.50 43.50 78.00 80.67 50.00 37.50 71.17 44.00 64.67 69.17 7B and 8B models 81.67 85.50 55.00 79.50 85. 65.00 72.67 50.50 77.17 82.17 11B, 13B, and 14B models 67.33 78.83 66.83 66.83 88.83 50.50 78.00 53.33 53.33 80.67 32B and 38B models 47.74 52.05 54.61 51.19 53.38 55. 58.37 56.91 49.06 58.00 58.33 47.10 57.07 51.43 44.87 58.64 51.30 52.46 49.71 55.80 50.87 54.78 58.55 49.57 41.30 51.01 56.38 52.46 60.58 59.42 44.78 54.20 86.50 84.67 89. 85.83 83.33 83.67 57.59 59.98 60.89 54.06 52.03 54.35 72B and 78B models 82.50 88.50 88.83 92.83 79.00 82.50 83.50 84. 59.76 60.67 61.66 64.40 63.04 55.51 55.36 60.72 58.43 60.80 63.84 54.24 61.91 67.52 68.29 68.93 51.61 70.39 68.40 56.29 65.24 61.09 48.73 68.58 65.42 73.03 71. 69.87 73.38 73.49 77.76 53.08 50.31 45.38 69.38 74.46 79.08 70.31 79.85 77.85 87.69 81.69 84.46 90.46 55.85 78.92 57.23 57.23 91.54 80.77 92.46 91. 89.23 91.54 94.31 91.54 42.57 41.22 40.32 44.88 51.93 47.78 46.91 47.88 52.03 49.52 45.66 50.29 50.39 42.76 49.71 42.57 41.22 51.64 48.36 53.67 53. 50.97 53.96 56.56 55.98 49.83 47.56 42.66 47.50 49.01 48.37 49.83 48.20 52.26 52.38 51.68 53.14 53.89 38.44 51.97 43.96 43.31 54.76 56.79 54.01 57. 53.72 54.18 55.52 57.67 35 Table 7: Quantitative Results on MMIU. Here, V-Reason., V-Corr., Homo-Est., Img-Trans., Tracking, 3D-Track., Multi-Reason., 3D-Depth, 3D-Pose, and 3D-Det., refer to Visually Grounded Reasoning, Visual Correspondence, Homography Estimation, Image Spatial Transformation Estimation, Point Tracking, 3D Object Tracking, Multiview Reasoning, 3D Depth Estimation, 3D Pose Estimation, and 3D Object Detection, respectively. Results with the best and second best results are bolded and underlined, respectively. Methods Overall V-Reason. V-Corr. Homo-Est. Img-Trans. Tracking 3D-Track. Multi-Reason. 3D-Depth 3D-Pose 3D-Det. InternVL2.5-1B InternVL3-1B 34.65 33.99 70.00 72.75 20.50 19.50 26.00 24.50 25.50 24.50 35.50 37. 1B models SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrain-8B InternVL2.5-8B InternVL3-8B 34.88 24.19 43.65 39.15 40.32 36.24 35.54 38.12 21.47 36.47 35.21 LLaMA-3.2V-11B 29.02 LLaMA-3.2V-11B-CoT 36.90 22.93 LLaVA-1.5-13B 19.69 SpaceLLaVA-13B InternVL3-14B 48. Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 48.10 46.55 53.16 LLaVA-OneVision-72B 43.13 55.84 Qwen2.5VL-72B 45.80 InternVL2.5-78B 56.73 InternVL3-78B 69.00 71.75 67.50 69.75 59.75 66.00 63.50 65.00 47.00 70.25 68.25 39.75 60.00 46.25 19.69 68. 68.75 67.25 66.75 69.50 67.25 69.75 66.75 28.00 16.00 34.50 31.00 27.50 27.50 32.00 29.50 9.50 23.00 20.50 29.00 26.50 18.50 51.00 38.00 39.00 41.00 53. 41.50 46.00 44.50 56.00 3B and 4B models 24.00 24.00 28.50 25.00 24.50 27.00 24.00 22.50 75.00 36.50 75.50 46.50 7B and 8B models 23.00 27.00 14.50 26.50 27. 56.00 58.00 8.00 60.00 39.50 25.00 7.00 26.00 23.50 24.00 23.50 26.00 24.00 19.50 17.50 26.50 11B, 13B, and 14B models 27.00 27.50 0.00 27.50 23.50 27.00 28.00 27. 27.00 29.50 25.00 28.50 25.00 20.50 21.50 1.00 34.50 22.50 54.00 0.00 24.00 67.00 32B and 38B models 32.50 34.50 35.00 75.00 79.50 67. 72B and 78B models 29.50 36.50 41.00 43.50 59.50 80.50 78.00 64.50 23.50 26.50 26.50 18.50 59.00 40.50 25.00 45.50 4.50 38.00 23.50 26.50 41. 29.50 31.00 0.00 3.00 56.00 11.00 13.00 70.00 9.50 85.00 23.50 83.00 45.11 44.36 44.36 0.00 54.14 50.38 44.36 44.36 51.13 55.64 19.55 44.36 45. 50.38 51.13 51.13 42.11 53.38 38.35 47.37 45.86 30.08 46.62 41.35 49.62 27.00 26.50 22.00 5.50 19.50 23. 26.50 26.50 23.50 26.50 26.50 26.00 27.00 27.00 15.50 25.50 28.50 25.00 28.00 22.00 0.00 27.00 26.50 26.50 25.50 28.00 24.00 26.50 25.50 21.50 0.00 20.00 34.50 53.00 0. 22.00 7.50 9.50 2.00 0.00 13.00 28.50 26.00 7.50 61.50 98.00 66.50 91.00 79.00 100.00 54.00 100.00 29.00 0.00 28.00 27.00 25.00 28.50 27.50 28.50 22.00 38.00 25. 25.50 28.50 21.00 3.00 37.50 41.00 41.50 34.00 27.00 28.50 29.00 37.50 36 Table 8: Quantitative Results on BLINK. Here, Count., Multi-Reason., Obj-Loc., Rel-Depth, Spa-Rela., and V-Corr., represent Counting, Multi-view Reasoning, Object-Location, Relative-Depth, Spatial-Relation, and Visual Correspondence, respectively. Results with the best and second best results are bolded and underlined, respectively."
        },
        {
            "title": "Methods",
            "content": "Overall Count. Multi-Reason. Obj-Loc. Rel-Depth Spa-Rela. V-Corr. InternVL2.5-1B InternVL3-1B 48.28 47.67 46.67 52.50 56.39 56.39 1B models SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrain-8B InternVL2.5-8B InternVL3-8B 48.40 53.81 63.51 56.39 56.39 64.99 62.41 63.51 43.86 62.41 61.43 61.67 53.33 72.50 60.83 61.67 65.83 70.83 64.17 54.17 70.00 70. 3B and 4B models 44.36 44.36 42.11 45.86 41.35 44.36 7B and 8B models 55.64 54.14 36.84 44.36 45.11 45.90 38.52 45.08 54.10 62.30 47.54 56.56 48. 58.20 55.74 50.00 59.02 58.20 11B, 13B, and 14B models LLaMA-3.2V-11B 52.21 LLaMA-3.2V-11B-CoT 58.60 48.89 LLaVA-1.5-13B 47.27 SpaceLLaVA-13B 69.41 InternVL3-14B 55.83 64.17 46.67 55.08 70.00 44.36 45.11 44.36 42.75 55.64 32B and 38B models Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 69.90 70.27 73.10 65.83 74.17 75.00 44.36 44.36 53.38 72B and 78B models LLaVA-OneVision-72B 69.16 73.83 Qwen2.5VL-72B InternVL2.5-78B 74.69 75.92 InternVL3-78B 77.50 74.17 75.83 76.67 53.38 53.38 54.14 54.89 62.30 61.48 51.64 47.54 66.39 54.10 50.82 59.84 60.66 60.66 57.38 57.38 56.45 55. 51.61 66.13 67.74 65.32 65.32 66.94 75.00 74.19 51.61 77.42 76.61 62.90 68.55 55.65 50.81 83.06 75.00 84.68 79.84 78.23 81.45 82.26 81.45 59.44 59. 29.65 28.49 65.73 69.93 82.52 72.03 81.12 83.92 83.92 86.01 74.83 89.51 83.92 65.73 76.22 63.63 65.73 89.51 86.01 86.71 86.71 78.32 88.81 93.00 91. 27.91 38.95 55.81 49.42 37.21 43.60 37.79 49.42 6.40 40.12 40.70 41.28 34.30 26.79 54.65 86.63 77.33 80.23 67.44 80.81 81.40 87.79 Table 9: Quantitative Results on VSI-Bench. Here, Rel-Dist., Route-Plan., Appr-Order., Abs-Dist., Count., Obj-Size, Room-Size, and Rel-Dirc. represent Object Relative Distance, Route Planning, Object Appearance Order, Absolute Distance, Object Counting, Object Size Estimation, Room Size Estimation, and Object Relative Direction, respectively. Results with the best and second best results are bolded and underlined, respectively. Methods Overall Rel-Dist. Route-Plan. Appr-Order. Abs-Dist. Count. Obj-Size Room-Size Rel-Direc. 1B models 23.30 24.60 22.27 27. 21.17 48.81 10.93 26.31 3B and 4B models InternVL2.5-1B InternVL3-1B 24.59 31.20 33.94 36. SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrain-8B InternVL2.5-8B InternVL3-8B 23.28 24.33 34.62 24.90 25.78 27.47 32.23 28.82 17.44 33.78 34.90 LLaMA-3.2V-11B 21.79 LLaMA-3.2V-11B-CoT 28.12 25.13 LLaVA-1.5-13B 14.56 SpaceLLaVA-13B 39.43 InternVL3-14B Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B 31.29 36.74 39.64 LLaVA-OneVision-72B 36.35 31.53 Qwen2.5VL-72B 35.15 InternVL2.5-78B 39.23 InternVL3-78B 34.65 30.70 36.34 31.41 36.48 36.20 38.73 37.89 26.34 36.06 37.04 30.28 31.69 31.27 18.59 38.87 38.73 39.44 40. 37.61 44.65 44.23 41.69 9.10 14.72 11.53 4.24 33.33 6.01 27.36 22.81 30.62 30.28 0.00 34.41 33.68 3.72 17.67 14.69 5.56 47.47 36.18 38.85 36. 39.58 40.03 30.35 36.04 43.01 41.15 40.28 36.53 38.73 37.59 42.80 41.42 36.88 38.99 30.73 41.16 41.75 28.30 28.92 30.87 13.58 36.18 30.42 38.39 38. 30.53 27.60 40.33 40.57 16.83 42.28 29.40 24.88 20.16 29.10 39.24 19.13 7.03 16.96 46.88 20.48 6.46 27.29 11.81 48.73 17.35 34.24 55.36 42.07 14.34 26.58 48. 26.08 10.44 49.19 26.99 16.65 2.91 48.04 34.49 8.12 46.18 38.15 32.47 51.03 31.24 34.07 46.93 40.04 49.93 44.87 51.54 50.60 48.55 44.26 32.99 29. 29.38 29.38 32.47 28.35 27.84 30.93 26.29 24.74 29.90 28.87 26.80 24.60 24.60 26.70 23.79 19.42 32.52 7B and 8B models 22.82 37.86 26.21 34.79 29.13 2.88 16.49 30.80 20.17 15.50 23. 15.19 7.15 11.21 31.40 25.79 11B, 13B, and 14B models 29.38 37.11 18.56 0.00 25.77 15.86 26.70 22.98 21.68 39.97 32B and 38B models 34.54 36.60 29. 31.07 27.18 34.79 72B and 78B models 28.87 28.35 31.96 27.32 35.44 31.39 29.94 40.61 13.81 25.37 24.15 11.21 31.51 22.03 30.25 36. 25.16 15.28 29.24 35.10 38 Table 10: Quantitative Results on RealWolrdQA, MMVP, VSR, SpatialSense, SpatialBench, and QSpatialBench. Here, we consider two data split strategies for VSR: Zero-Shot (ZS) and Random (Rand). Moreover, QSB-Plus and QSB-ScanNet denote QSpatialBench-Plus and QSpatialBenchScanNet, respectively. Results with the best and second best results are bolded and underlined. Methods RealWorldQA MMVP VSR-ZS VSR-Rand SpatialSense SpatialBench QSB-Plus QSB-ScanNet 1B models 19.33 32.67 65.55 58.83 65.47 63.14 InternVL2.5-1B InternVL3-1B SpaceQwen2.5VL-3B SpatialBot-Phi2-3B Kimi-VL-3B Kimi-VL-3B-Thinking Qwen2.5-VL-3B InternVL2.5-4B LLaVA-OneVision-7B Qwen2.5-VL-7B Cambrain-8B InternVL2.5-8B InternVL3-8B LLaMA-3.2V-11B LLaMA-3.2V-11B-CoT LLaVA-1.5-13B SpaceLLaVA-13B InternVL3-14B Qwen2.5-VL-32B InternVL2.5-38B InternVL3-38B LLaVA-OneVision-72B Qwen2.5VL-72B InternVL2.5-78B InternVL3-78B 57.91 56.21 43.14 57.78 67.45 54.64 60.78 64.31 67.45 67.97 63.00 66.54 67. 55.16 56.99 52.03 21.57 69.54 69.28 72.68 73.73 71.90 71.50 74.25 78.69 12.67 20.00 46.00 46.00 17.33 45.33 54.67 48.00 31.33 52.00 53.33 40.67 44.00 30.00 21.33 56. 54.00 68.00 71.33 68.67 61.33 70.00 67.33 54.60 51.72 51.15 62.07 61.49 60.92 62.07 63.79 68.97 66.67 63.79 67.24 62.64 57.47 55.75 54.60 42.53 65. 66.09 74.14 68.97 67.82 67.82 70.11 67.82 25.74 18.81 33.66 0.00 5.94 7.92 25.74 45.54 57.43 32.67 14.85 47.52 44.55 40.59 41.58 41.58 51.49 43. 48.51 53.47 59.41 57.43 63.37 43.56 50.50 18.24 13.53 24.12 3.53 4.12 28.82 28.82 43.53 52.35 45.88 21.76 56.47 58.82 51.18 48.82 51.18 59.41 60. 61.11 56.47 65.29 58.24 68.82 70.00 72.94 3B and 4B models 74.47 67.35 78.56 75.94 81.01 83.31 68.38 70.93 79.59 75.58 79.27 80.68 7B and 8B models 77.50 85.43 71.52 81.51 83.88 77.86 81.00 73.17 80.41 80.41 64.08 59.11 65.16 62.56 69.13 66.76 70.62 71.98 69.66 71.12 68.03 72.53 71.20 11B, 13B, and 14B models 68.74 75.70 67.35 39.36 85.35 59.86 72.98 69.79 36.22 82.87 32B and 38B models 88.05 88.54 89.20 84.05 85.19 85.97 72B and 78B models 83.55 89.28 88.95 89.85 81.91 83.55 86.47 86.15 65.71 67.12 58.42 48.59 73.19 73.58 75.40 74.74 73.47 73.58 76.34 76."
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "Shanghai AI Laboratory",
        "Tianjin University"
    ]
}