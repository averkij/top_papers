{
    "paper_title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation",
    "authors": [
        "Xiuyu Yang",
        "Shuhan Tan",
        "Philipp Krähenbühl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen"
        },
        {
            "title": "Start",
            "content": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation Xiuyu Yang* Shuhan Tan* Philipp Krähenbühl"
        },
        {
            "title": "UT Austin",
            "content": "5 2 0 2 0 2 ] . [ 1 3 1 2 7 1 . 6 0 5 2 : r Figure 1. Long-term traffic simulation with InfGen and prior SOTA [31]. InfGen keeps scene layout realistic while [31] becomes empty."
        },
        {
            "title": "Abstract",
            "content": "An ideal traffic simulator replicates the realistic longterm point-to-point trip that self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, unified nexttoken prediction model that performs interleaved closedloop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-theart in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen. 1. Introduction Traffic simulation is cornerstone of the extensive and safe development of self-driving systems. The ultimate goal of traffic simulation is to create realistic trip-level driving experiences that faithfully reflect real-world self-driving con- *Equal contribution. Work done while Xiuyu interned at UT Austin. ditions [1, 7, 9, 13, 30]. simulator should provide realistic model of the environment, the ego-vehicle, and all other traffic agents throughout the trip. Existing simulators easily handle an expansive static environment [1, 9, 30] and intricate ego-vehicle dynamics [7, 13]. However, they often lack stable long-term simulation of non-ego traffic agents. In this paper, we introduce InfGen, long-term traffic simulator: Given short (1 second) driving-log, InfGen simulates realistic traffic flow around the ego agent for up to 30 seconds. This long-term setting leads to new challenges. The ego agent may move outside its initial simulation area, leading to logged agents moving out of sight and becoming irrelevant. Furthermore, when the ego agents drive into new map area not covered in the log, these street areas have no agents. Gradually, scenario becomes sparser and eventually empty (Fig. 1 bottom row). This is clearly unrealistic. InfGen models this by combining closed-loop motion simulation with scene generation to remove exiting agents and spawn new agents according to the spatial scene layout. InfGen  (Fig. 2)  is unified autoregressive transformer with interleaved token prediction. It handles temporal motion simulation and spatial scene generation in unified model. We design set of tokenizers to convert task-specific behaviors of motion simulation and scenario generation into discrete tokens. We then add mode-control tokens to mark 1 recently, GIGAFLOW [7] shows strong agent performance emerges from large-scale self-play in simulation. For this direction, nuPlan [1] and WOSAC [14] provide data and benchmark for fair comparisons. All these works focus on simulating motions of agents existed in the history, leading to unrealistic scene layouts under long-term rollout. InfGen solves this issue with interleaved scene generation, maintaining realistic scene layout across the rollout horizon. Traffic Scenario Generation. This line of work focuses on generation realistic and interesting traffic scenarios. Early works like SceneGen [23] and TrafficGen [10] generates agent initial poses on an empty map. Another popular direction is to generate near-collision scenarios with adversarial optimization [4, 17, 29, 33, 35]. More recent works like LCTGen [24] enables better customizations of the generated scenarios in forms of text [24, 44], scenario queries [8] or cost functions [45]. SLEDGE [6] combine generative models with rule-based traffic simulation to synthesize dynamic driving scenarios. These works either focus on static scene layout initialization, or only generate short-term open-loop scenarios. In contract, InfGen conducts dynamic scenario layout generation during closedloop rollout, enabling stable long-term simulation. Concurrent works like SceneDiffuser++ [27] and ScenarioDreamer [19] introduces vectorized latent diffusion approach to generate realistic and diverse driving simulation environments. Interleaved Next-Token Prediction. Recent advances in vision-language models have sparked works that unify generation and understanding tasks with interleaved mixedmodal token sequences. For example, Chameleon [3] proposes to train LLMs on interleaved text and image tokens in any arbitrary sequences. This line of works show strong performance on traditional multimodal tasks, but also longform mixed modal generation that interleaves between image and text generation [3, 12, 28, 32, 37]. InfGen follows the same philosophy but focus on different pair of modalities: temporal agent motion and spatial agent layout. 3. Problem Formulation The goal of traditional traffic simulation [14] is to predict future agent trajectories given historical observations (with time span TH) and static map. Specifically, at timestep t0, we are given the static map and the history states of all the agents A0:t0 = {a1 }, where each agent ai has history states up to timestep t0. The standard task is to predict future agent states over fixed horizon , formulated as estimating the conditional distribution: p(At0+1:T M, A0:t0). Prior work [25, 31] factorize the simulation on the time axis and turn it to an autoregres- , . . . , aN , a2 0:t0 0:t0 0:t Figure 2. Overview of InfGen interleaved next-token-prediction process. Colors mark different token modalities. the task switch between the two tasks, indicating what the current task is and when to switch. This design allows us to convert each real log into single ordered sequence of tokens containing interleaved data of both tasks. We directly train InfGen with the next token prediction objective endto-end on real data. Noticeably, thanks to the next token prediction formulation, we can train InfGen on short-term driving logs and produce stable long-term rollouts, up to 6 longer than the training horizon (Fig. 1 top). We show detailed pipeline of InfGen in Fig. 3. We show in Section 5 that InfGen significantly outperforms prior SOTA models [31, 41] in 30-second long-term traffic simulation for in terms of both motion and scenario realism, showing its strong capacity for stable long-horizon rollout. We contrast different models rollout with visualizations  (Fig. 5)  . Furthermore, InfGen achieves strong performance even on the standard short-term Sim Agent setting [14], showing its strong adaptivity to different rollout horizons. In addition, we provide comprehensive set of analysis on the long-term traffic simulation task to shed light on research towards trip-level driving simulators. 2. Related Work Closed-loop Motion Simulation. In traffic simulators, motion simulation aims to model realistic multi-agent interactions that mimic real-world data. Early works like Wayformer [15] focus on open-loop imitation learning from real logs [20, 36, 38, 40, 43]. Other works like ProSim [25] and CAT-K [41] instead focus on modeling closed-loop interaction between agents [2, 22, 39]. CtRL-Sim [18] learns reactive agents by applying offline reinforcement learning to diverse traffic scenarios. More recent works like SMART [31] discover the effectiveness of modeling this task as an autoregressive next-token-prediction problem [11, 16, 21, 42, 46]. ProSim [25] enables multimodal prompts to control behavior semantics of any agent. Most Figure 3. Pipeline of InfGen interleaved motion simulation (blue flow) and scene generation (green flow). For either task, we first pass its query feature through blocks of attention layers and feed it to task-specific head and control head. We then sample from both heads to obtain motion token or pose token, as well as control token, which determines determines which task to execute next. sive prediction task: 4.1. Tokenization p(At0+1:T M, A0:t0) = 1 (cid:89) t=t p(At+1 M, A0:t). (1) However, this formulation assumes fixed set of agents throughout the prediction horizon, which does not hold true for long-term simulations with the horizon ( ). In realistic scenarios (e.g., [14]), agents dynamically enter and leave the observable region around the ego vehicle. To address this issue, we model two interleaved processes at each timestep: 1) motion simulation: predicts future motions for existing agents; 2) scene generation: dynamically inserting new agents and removing agents exiting the scenario. At each timestep t, we first performs motion simulation: At+1 pmotion(At+1 M, 0:t), followed by scene genert+1 pscene(A ation: t+1 M, At+1). Here, At+1 contains the predicted motions of existing agents, while t+1 represents the updated set of agents after adding new agents and removing exiting agents. Then, we formulate the longterm traffic simulation task as: p(A t+1:T M, A0:t0 ) = 1 (cid:89) pscene(A t+1 M, At+1) t=t0 pmotion(At+1 M, 0:t). (2) 4. InfGen InfGen is unified autoregressive model for long-term traffic simulation. Regarding the inputs, it first tokenizes all scene context information (M and A0:t0) into sequences of discrete tokens, see Sec. 4.1. It then uses an autoregressive model for interleaved next-token prediction, see Sec. 4.2. And Secs. 4.3 and 4.4 provide the details on the model architecture and training process of InfGen. We aim to convert all agent motions, layouts and the map in real log into sequence of discrete tokens. We tokenize each modality differently. Map Tokenizer. We adapt the map tokenizer from [31]: we uniformly segment all the road elements into set of fixed-length road vectors. Each vector contains the corresponding features, including start/end points, directions and road type. We collect all road vectors to the map token set Vmap. Agent Motion Tokenizer. We follow the motion tokenizer from prior works [31, 41].1 Specifically, we segment the continuous trajectories of all agents in the dataset with fixed time span of 0.5 seconds. Then we use k-disks algorithm [31] to cluster these trajectories into the set of motion vocabulary Vmotion. Finally, at every 0.5-second interval, we convert the continuous trajectory into discrete token with the index of its nearest neighbor in the motion vocabulary. Agent Pose Tokenizer. When new agent is inserted into the scenario, its initial pose (position and heading) is given. We tokenize the pose of each inserted agent as pair of discretized position and heading tokens. For position, we construct grid with radius of R, centered on the ego agents location with x-axis aligned with ego agents heading, resulting in position token set Vpos. To get the position token, we obtain the index of the grid closest to the agent based on L2 distance. For heading, we divide the 360 range at the interval of θ, resulting in heading token set Vhead. To get the heading token, we similarly obtain the index of the heading interval closest to the agent heading. For simplicity, we refer the pair of position and heading tokens as pose token. 1Please refer to [31] for the details of motion tokenizer and k-disks approach. 3 Mode Control Tokenizer. We model long-term traffic simulation as interleaved motion simulation and scenario generation. We design the control tokens Vcontrol, consists of 4 special tokens, to mark mode transition between tasks: 1) <BEGIN MOTION>: the next token is an agent motion token to simulate current existing agents; 2) <ADD AGENT>: the next token is an agent pose token to insert new agent; 3) <KEEP AGENT>: the current agent is kept in the scenario; 4) <REMOVE AGENT>: the current agent will be removed in the next timestep. In the next section we will show how to use Vcontrol to control the interleaved simulation process. Tokenizing real log into discrete token sequence allows us to convert the complex mixture-task simulation process into simple interleaved next token prediction task. 4.2. Interleaved Next Token Prediction Dynamic Agent Matrix. Traffic simulation can be represented by an agent matrix shown in Figure 3. The horizontal axis represent temporal lifecycle of each agent: being inserted, active moving and finally exit the scenario. The length of the temporal axis equals to the rollout horizon. On the other hand, the vertical axis represent spatial agent layout at each timestep, where the width represent the number of active agents at each step. When new agent is inserted, new row is created and append to the matrix. Conversely, when current agent gets removed, its row gets deleted from the matrix. For long-term scenarios, because agents are frequently being inserted and removed from the scenario, the number of rows of the matrix are also constantly changing. Hence, we term it the dynamic agent matrix. As shown in Figure 3, we represent the long-term traffic simulation task as extending the dynamic agent matrix on different axis. Motion simulation (the upper blue flow) extends the temporal axis by adding new columns with predicted motion tokens. In contrast, scenario generation (the lower green flow) extends the spatial axis by adding new rows with pose tokens of new agents, and removing current rows of exiting agents. The control tokens determine how to interleave these two processes. Temporal Motion Simulation. We show this process in the blue flow of Figure 3. For the ith active agent at timestep t, we use its current motion token mt to 2. Specifically, we input qm to obtain its motion feature fm Temporal Attention layer to attend to the key and value of all its own past motion tokens within tw timesteps (within the same row as the query token): as the query qmt mt = MHSAt(qmt, {kmtτ }tw τ =1, {vmtτ }tw τ =1). (3) The output is then sent to an Agent-Agent Attention layer to 2We omit the and in this section when possible for simplicity. attend to all the other active agents within valid range raa at the same timestep t: mi = MHCAaa(q mi , {kmj }N j=1, {vmj }N j=1). (4) Finally, the query goes through Map-Agent Attention layer to attend to the Nr precomputed map tokens within valid range rma: fmi = MHCAma(q mi , {kmj }Nr j=1, {vmj }Nr j=1). (5) The motion head and control head separately take fm and output the probabilities over the motion and control tokens, from which we sample motion token and control token for each active agent. In this subtask, we enforce the control token to be sampled from <KEEP AGENT> and <REMOVE AGENT>. If control token is <KEEP AGENT>, we add the sampled motion tokens to the next column of each agent. Otherwise, if the control token is <REMOVE AGENT>, we add this control token to the next column and discard the motion token. The above process is conducted for all the current active agents in parallel in training and inference. After this, we switch to the scene generation step. Spatial Scene Generation. We show this process in the green flow of Figure 3. After each motion simulation step, we use learnable agent query a0 to obtain the scene generation feature fa0 . Same as the motion query, the agent query is also sent through three attention layers to collect the context information. The latter two layers are the same as the motion query, while the Temporal Attention layer is replaced by Grid Attention layer. This layer allows the agent query to attend to the occupancy grid tokens of total size Ng = Np = Vpos derived from the position tokens: a0 = MHCAg(qa0, Γ({kgj }Ng j=1), Γ({vgj }Ng j=1)), (6) where Γ() presents the transformation preceding the attention calculation for efficiency. We then pass a0 through the other two layers (Eq. 4 and Eq. 5) to produce fa0. Differently, a0 will have various visible range for the active agents rqa at current timestep and for the map tokens rqm, please refer to Appendix for more details. Then the pose head and control head take fa0 and output distributions for pose and control tokens respectively, from which we sample pose token and control token for each generation step (Please refer to Appendix for more details). We enforce the control token to be sampled from <ADD AGENT> and <BEGIN MOTION> for this subtask. Controlled by <ADD AGENT>, we append new row to the agent matrix and assign the sampled pose token to the current timestep. Then, conditioned on the all active agents, including the newly inserted one, we repeat the above step to autoregressively insert another new agents. This process is terminated when the sampled control token is <BEGIN MOTION>. In this case, we end the scene generation process and move to motion simulation of the next timestep. Finally, we remove any row that has <REMOVE AGENT> token at the current timestep from the agent matrix. 4.3. Model Architecture Token Embedding. Our model takes tokens of different modalities. To model them with single token sequence, we take different MLP layers to embed different kinds of tokens into the same latent dimension before entering the model. Please refer to Appendix for more details. Modeling Layer. Our transformer model is composed of blocks of attention layers. As mentioned in Section 4.2, each block contains 4 attention layers: Temporal Attention, Agent-Agent Attention, Map-Agent Attention and Grid Attention. Here the first layer are implemented with multihead self attention layer (MHSA), while the other layers are multi-head cross attention layer (MHCA). Furthermore, we apply the position-aware attention from prior works [25, 31] to explicitly model the relative positions between tokens. Please refer to the Appendix for the details. Occupancy Grid Encoder. We obtain occupancy grid features fg of the current scenario with the agent position tokens and map tokens via Γ(). Specifically, given the vocabulary size Np of position tokens Vpos, we directly assign each token with occupation indication {0, 1}, leading to an agents occupancy grid g1Np {0, 1}. We utilize the occupancy maps of agents in decoding pose token process, to make the agent query efficiently infer the spatial distributions of agents. We use an MLP Layer to convert g1Np to its features NpD before feed it into Grid Attention layers. 4.4. Training Ground-truth Sequence. As described in Section 3, realworld traffic scenario log [14] naturally contains data of agent motion, insertion and removal behaviors. To train our model with the interleaved NTP problem explained in Section 4.2, we convert each ground-truth log into an ordered sequence of token labels. To this end, we enforce specific ordering to chain tokens from different modalities. Specifically, at each timestep we arrange each type of tokens with fixed order: 1) motion tokens from all the current agents; 2) control tokens <REMOVE AGENT> and <KEEP AGENT> for the current agents; 3) pose tokens and <ADD AGENT> for any inserted agents; 4) <BEGIN MOTION> that mark the transition to the next timestep. For the same type of tokens we order them following to the agents distance to the ego agent from near to far. With this rule we obtain sequence of ground-truth (GT) token labels from each real log to train our model. Please refer to Appendix for more details about the GT tokens. Table 1. Short-term traffic simulation in WOSAC [14] (). Method Composite Kinematic Interactive Map TrafficBots [40] GUMP [11] SMART-7M [31] CatK [41] InfGen 0.6976 0.7404 0.7521 0.7603 0. 0.3994 0.4773 0.4799 0.4611 0.4754 0.7103 0.7872 0.8048 0.8103 0.7936 0.8342 0.8339 0.8573 0.8732 0.8502 Learning Objective. As shown in Figure 2, given the GT input tokens as input, our model predicts the distributions of different kinds of tokens at the corresponding position. We then train our model with set of standard NTP objective for each type of tokens. For example, the loss function for the motion token is: Lmotion = 1 (cid:88) t=1 log pθ(mt+1 c1:t), (7) where is the total number of timesteps, mt+1 is the GT motion token in the next timestep, pθ(mt+1 c1:t) is the model-predicted probability of the GT token. Here, c1:t is the ensemble of all history context the model attends to, including history motions of the same agent, positions of other agents in the current timestep, and the map. We formulate the loss function in the same way for agent pose tokens Lpose and mode control tokens Lcontrol. We also have supervision of shapes and types for those new agents. Our total training loss can be written as: = λ1Lmotion + Lpose + λ4Lcontrol + Lattr, (8) where Lpose = λ2Lpos + λ3Lhead and Lattr = λ5Lshape + λ6Ltype. We directly end-to-end train InfGen with on the fully tokenized dataset. During training InfGen not only learns how to conduct the two tasks respectively, but also learns to automatically and seamlessly switch between them. Please refer to Appendix for more training explanations. 5. Experiments In this section, we validate our work from two aspects: 1) How does InfGen compare to the SOTA baselines on conventional short-term traffic simulation task in standard benchmarks? 2) How does InfGen perform on our mainly introduced long-term traffic simulation task? Dataset. We train and validate InfGen on Waymo Open Motion Dataset (WOMD) [9], with 480K scenarios for training and 44K scenarios for validation. Each scenario consists of = 9.1 recorded rollout, with the first TH = 1.1 historical frames and the subsequent 8 future frames. Training. To train InfGen, we use total batch size of 8 on 8 NVIDIA A5000 GPUs with the AdamW optimizer 5 Table 2. Long-term traffic simulation evaluation () on WOMD validation split measured by the metrics introduced in Sec. 5.2."
        },
        {
            "title": "Composite Kinematic",
            "content": "Interactive Map-based SMART-7M [31] CatK [41] InfGen 0.6519 0.6584 0.6606 0.5839 0.5850 0.5966 0.7542 0.7584 0.7619 0.8102 0.8186 0. overall 0.4324 0.4424 0.4542 Placement-based D+ N+ 0.5713 0.5842 0. 0.4964 0.5233 0.5635 0.3371 0.3371 0.3169 0.3248 0.3248 0.3092 and cosine annealing learning rate scheduler. The initial learning rate is 0.0005. For the loss function in Eq. 8, we set λ1 = λ3 = 1, λ2 = λ4 = 10, λ5 = 0.2, λ6 = 5. 5.1. WOMD Sim Agent Challenge We first evaluate InfGen on the standard short-term traffic simulation task in WOSAC [14]. For this setting, we enforce InfGen to skip all the scene generation steps, and predict the rollout for only 8s. We then evaluate the rollouts under the official WOSAC metrics and compare the results with the top-performing methods in Table. 1. The results show InfGen performs very competitive even under the short-term setting without any task-specific tuning, achieving similar performance to SOTA [41] and outperforms strong models [11, 40]. 5.2. Long-term Traffic Simulation Setup Rollout Setup. To be compatible with the map size of WOMD, we set the total rollout duration of our long-term traffic simulation experiments as = 31.1 with FPS = 10. The first 1.1 corresponds to the historical segment, from which we simulate 300 future steps. WOSAC Metric Adaption. The standard WOSAC metric [14] evaluates short-term traffic simulation by comparing simulated rollouts directly with ground-truth (GT) logs over an 8-second horizon. Specifically, it computes 9 metrics that assess aspects such as kinematic realism, interaction realism, and map adherence, assuming one-to-one correspondence between simulated and logged agents. However, in our long-term simulation setting, the rollout duration extends significantly beyond the standard 8-second window, reaching up to 30 seconds. This creates two critical challenges: (1) there is no direct one-to-one correspondence between simulated agents and logged agents over the entire long-term rollout, as agents dynamically enter and exit the scene; (2) the evaluation window (8 seconds in WOSAC) is shorter than our simulation horizon (30 seconds). Therefore, we need to adapt the original WOSAC metric to suit our long-term simulation setting. Specifically, we adapt the WOSAC evaluation as folGiven long-term simulated rollout σ = lows. (M, 0 : ), we extract short segments using sliding window approach. Specifically, we slide window of Figure 4. Agent Count Error (ACE) curves of InfGen against baselines over 30s long-term simulation rollouts. length Tw = TH (matching the standard 8 evaluation window) at fixed interval throughout the entire simulated rollout. Each sliding window generates short-term segment that matches the length of : t+Tw standard evaluation segments. Finally, we get 0:T = {A }P i=1 with the number of segments equal to , and s(P 1) + Tw = . s(i1) : s(i1)+Tw Since the number of simulated agents in these segments may differ from the logged agents , we cannot directly apply the original WOSAC evaluation. Instead, we compute agent-level Negative Log-Likelihood (NLL) scores for all simulated agents by evaluating their behaviors against global distribution learned from the entire validation dataset ( 48K scenarios). Concretely, we first estimate empirical distributions for agent motions, interactions, and placements from the entire validation dataset, and then measure how well our simulated agent behaviors conform to these reference distributions. This modified approach ensures fair and consistent evaluation of long-term simulation realism with varying numbers of agents and rollout durations. Placement-based Metrics. The standard WOSAC metrics evaluate simulation realism through multiple components: kinematic-based, interaction-based, and map-based. These metrics are then aggregated into an overall realism composite metric using predefined weights. However, these metrics assume fixed set of agents throughout the evaluation horizon and thus fail to capture the realism of agent insertion and removal events that are essential in long-term traffic simulations. We then propose the placement-based component, which comprises 4 types of statistics: the numTable 3. Ablation study of InfGen on long-term traffic simulation (). indicates remaining unchanged as in Table. 2. Cont. token Pos. token Head. token Composite Kinematic Interactive Map-based Placement-based 0.6328 0.6564 0.6509 0.6297 0.6674 0.5493 0.5580 0.5866 0.5422 0.5921 0.7043 0.7768 0.7276 0.6962 0.7688 0.7961 0.8077 0.8107 0.7939 0.8003 0.4513* 0.4378 0.4445 0.4499 0.4503 * We take the heuristic approach to remove the agents. Table 4. Long-term motion prediction evaluation (). Method Composite Kinematic Interactive Map SMART-7M [31] CatK [41] InfGen 0.7428 0.7316 0.7432 0.5413 0.5216 0. 0.7626 0.7347 0.7685 0.8349 0.8495 0.8213 ber of placement N+, the number of removal N, the distance of placement D+ and the distance of removal D, where the distances here are relative to the ego agent. We aim to use the placement-based metrics to assess the realism of InfGen in modeling the entry and exit of agents during the long-term rollout. Similarly, we calculate the NLLs of the placement-based statistics under the logged distributions in the same way as the other components. Agent Count Error Metrics. In addition to WOSAC metrics extensions, we also introduce new Agent Count Error (ACE) metric to evaluate scene realism during long-horizon rollouts. For each sliding window over the 30 rollout, we compute the mean absolute difference in agent count between simulation and the ground-truth distribution of the validation set. We summarize the results with two scalar metrics (lower is better): Mean ACE (overall error) and ACE Slope (error growth rate via linear regression). With the above setup we are now ready to evaluate different methods for long-term traffic simulation. 5.3. Long-term Traffic Simulation Evaluation Baselines. Since existing works do not focus on long-term rollout, to fairly compare, our baselines are derived by improving upon the SOTA simulation methods SMART [31] and CatK [41]. SMART leverages vectorized map and agent trajectory data, predicting motion sequences through decoder-only transformer architecture. CatK further introduces closed-loop supervised fine-tuning technique and achieve better WOSAC simulation performance. First, we extend their rollout duration to = 31.1s and then calculate the kinematic, interactive and map-based metrics. For the placement-based metrics, we design heuristic approach: we obtain the entered and exited agents by partitioning the distance between agents and the ego 0 : }N agent by the radius R, which corresponds to the distance placement and distance removal of placement-based measurements, introduced in Sec. 5.2. During the rollout process, we have the distances from each agent to the ego agent {D i=1. At step t, the agents that first run within the range R, whose Dt given D0 : t1 > R, are considered as entered agents, while those agents with Dt > and D0 : t1 R, are considered as exited agents. Throughout the experiment, we adjust to achieve the highest placement-based score for baselines on validation set. In this case, we assign additional validity values to each agent at each timestep, thus only those valid agents will be included in baseline inferences and evaluation metrics. We follow the default settings in their papers. Quantitative Results. Under the settings and proposed evaluation metrics in Sec. 5.2, Table. 2 shows the results of long-term traffic simulation under extended WOSAC metrics. As can be seen, InfGen achieves better realism performance than baselines. For placement-based metrics, our method significantly outperforms the baselines, demonstrating that our approach effectively models the spatial sequences of agents. The lower scores on map-based metrics may be attributed to the insertion of agents in regions outside driving lanes or near road boundaries, leading to less realistic motion trajectories. Figure 4 shows the curve of comparison results under our introduced ACE metrics, where our method significantly outperforms prior methods: InfGen achieves Mean ACE of 8.1 while baselines (CatK [41],SMART [31]) have scores of 12.2 and 12.0, which reflects the improvements of our dynamically scenario generation; InfGen has ACE Slope: 0.15, however, baselines accumulate such errors with slopes of 0.32 and 0.31. This significantly confirms the much better long-term stability observed in Figure 5. Qualitative Analysis. We further show InfGen through the visualizations of the long-term rollouts. As depicted in Figure. 5, the results highlight two core properties of InfGen: long-term and closed-loop. The first scenario depicts bidirectional driving road, where InfGen successfully simulates oncoming traffic. In the third scenario, when the rollout reaches = 18s, most of active agents 7 Figure 5. Qualitative results of long-term closed-loop rollouts for 5 scenarios. We compare rollouts of InfGen and SMART [31] here. are the initially placed agents, are the new agents inserted by InfGen, and are the ego agents. 8 run outside the scenario, resulting in an empty region, as reflected by prior works. Under the control of InfGen, new agents enter the scenario, allowing continued agentagent and agent-map interactions with high realism. While in baselines, those regions around the ego agent remains empty. Moreover, from the forth scenario, we observe our model can place new agents not only on driving roads but also in parking lots or other open areas. Motion-only Analysis. In Section 3 we formulate the long-term traffic simulation task as product of motion simulation and scene generation (Equation 2). Here we investigate the performance of different models in long-term traffic simulation when we only consider motion simulation. Specifically, we disable agent insertion and removal for all the methods, and simply let them rollout for 30 seconds. We then evaluate the rollouts with the adapted WOSAC metrics. As shown in Table 4, all the compared methods have very similar performance. This indicate that simply extending rollout horizon will not reveal important aspects of longterm traffic simulation, like the empty scenarios we see for SMART rollouts in Figure 5. 5.4. Ablation Study We conduct various ablation studies to validate our methods. We ablate the impact of designed control tokens, position tokens and heading tokens on our task. Due to the high cost of local evaluation, following [41], we use 5% (2204 out of 44K scenarios) of the validation split in this part. Effect of Control Token. As discussed in Sec.4.1, we introduce the control tokens to determine the spatial scene generation sequence. The baselines, to some extent, can be regarded as versions without the <ADD AGENT> token, and naturally, the <REMOVE AGENT> token is also absent. To fully validate the control tokens, we additionally conduct longterm rollout tests while retaining the <ADD AGENT> token but removing the <REMOVE AGENT> token. Note that we use the heuristic approach to remove agents with distances exceed the R, for two reasons: 1) Adding agents without removing any results in an unrealistic scenario that would not naturally exist; and 2) to ensure fairer comparison. As shown in Table 3, removing <REMOVE AGENT> token in long-term rollout severely degrades the kinematic and interactive metrics. Unsurprisingly, as the continuously increasing number of agents over time significantly impacts bother their motion states and internal interactions. Effect of Position Token. We take position tokens to efficiently capture the environment information of local regions, which are ultimately aggregated into the agent query in spatial scene generation. We have an ablation experiment by completely removing the position tokens along with its token embedding, and we instead directly predict the (x, y) locations of agents. The results reveal that the position tokens help the model better address the placement-related issues, as grids simplify the search space. Additionally, through position tokens embedding, the agent query can more efficiently perceive the spatial distribution of the environment. Effect of Heading Token. The initialization of newlyentered agents poses, is essential to the their subsequent motions and further the interactions with others. Similarly, we validate replacing the head token prediction with the direct prediction of continuous angle values. The results at table. 3 also reflects that heading tokens can slightly improve the interactive and placement-based performance. 6. Conclusion In this work we propose InfGen, unified next-token prediction model for long-term traffic simulation. InfGen learns to automatically switch between temporal motion simulation and spatial scene generation. Our experiments show InfGen significantly outperforms prior methods in long-term simulation, while keeping strong performance on standard short-term simulation. We believe InfGen is steady step towards realistic trip-level traffic simulation. Limitations and Future Works. The main limitation of this paper is that we did not evaluate InfGen under real trip-level rollout duration (> 5 minutes). Our main constraint is the map areas available in WOSAC scenarios are too small: even in our 30s rollout the ego agent often drive to areas where no map is available. We plan to further explore with suitable data. Another limitation is that InfGen is trained purely with supervised learning on logged realworld data, which may lead to failures due to overfitting and the model capturing spurious causal relationships. Given the interactive nature of this task and its autoregressive generation process, in future work we plan to explore interactive reinforcement learning to further encourage realistic agent interactions and scenario generation through environment feedback [5, 26]."
        },
        {
            "title": "References",
            "content": "[1] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: closed-loop mlbased planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 1, 2 [2] Sergio Casas, Cole Gulino, Shuai Suo, Katie Luo, Renjie Liao, and Raquel Urtasun. Implicit latent variable model for In European Conferscene-consistent motion forecasting. ence on Computer Vision (ECCV), 2020. 2 [3] Chameleon Team. Chameleon: Mixed-Modal Early-Fusion Foundation Models. arXiv preprint arXiv:2405.09818, 2024. 2 9 [4] Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, Safe-sim: and Manmohan Chandraker. Safety-critical closed-loop traffic simulation with diffusioncontrollable adversaries. arXiv preprint arXiv:2401.00391, 2024. 2 [5] Keyu Chen, Wenchao Sun, Hao Cheng, and Sifa Zheng. Rift: Closed-loop rl fine-tuning for realistic and controllable traffic simulation, 2025. [6] Kashyap Chitta, Daniel Dauner, and Andreas Geiger. Sledge: Synthesizing driving environments with generative models and rule-based traffic. In European Conference on Computer Vision, pages 5774. Springer, 2024. 2 [7] Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Krähenbühl, and Vladlen Koltun. Robust autonomy emerges from self-play. arXiv preprint arXiv:2502.03349, 2025. 1, 2 [8] Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao, and Marco Pavone. Realgen: Retrieval augmented genarXiv preprint eration for controllable traffic scenarios. arXiv:2312.13303, 2023. 2 [9] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 97109719, 2021. 1, 5 [10] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. Trafficgen: Learning to generate diverse and realistic traffic scenarios. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 35673575, 2023. 2 [11] Yihan Hu, Siqi Chai, Zhening Yang, Jingyu Qian, Kun Li, Wenxin Shao, Haichao Zhang, Wei Xu, and Qiang Liu. Solving motion planning tasks with scalable generative model. In European Conference on Computer Vision, pages 386 404. Springer, 2024. 2, 5, 6 [12] Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads. arXiv preprint arXiv:2412.00127, 2024. [13] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement IEEE Transactions on Pattern Analysis and Malearning. chine Intelligence, 2022. 1 [14] Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nicholas Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Yang, Shimon Whiteson, et al. The waymo open sim agents challenge. Advances in Neural Information Processing Systems, 36:5915159171, 2023. 2, 3, 5, 6 [15] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S. Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. In 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023. 2 [16] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Traffic modeling as next-token prediction. arXiv preprint arXiv:2312.04535, 2023. 2 [17] Davis Rempe, Jonah Philion, Leonidas Guibas, Sanja Fidler, and Or Litany. Generating useful accident-prone drivIn Proceedings of ing scenarios via learned traffic prior. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1728417294, 2022. 2 [18] Luke Rowe, Roger Girgis, Anthony Gosselin, Bruno Carrez, Florian Golemo, Felix Heide, Liam Paull, and Christopher Pal. Ctrl-sim: Reactive and controllable driving agents with offline reinforcement learning. arXiv preprint arXiv:2403.19918, 2024. [19] Luke Rowe, Roger Girgis, Anthony Gosselin, Liam Paull, Christopher Pal, and Felix Heide. Scenario dreamer: Vectorized latent diffusion for generating driving simulation environments. arXiv preprint arXiv:2503.22496, 2025. 2 [20] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In European Conference on Computer Vision (ECCV), 2020. 2 [21] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8579 8590, 2023. 2 [22] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafficsim: Learning to simulate realistic multiagent behaviors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1040010409, 2021. 2 [23] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: In IEEE/CVF Learning to generate realistic traffic scenes. Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [24] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. Language conditioned traffic In 7th Annual Conference on Robot Learning generation. (CoRL), 2023. 2 [25] Shuhan Tan, Boris Ivanovic, Yuxiao Chen, Boyi Li, Xinshuo Weng, Yulong Cao, Philipp Krähenbühl, and Marco Pavone. In 8th Annual Promptable closed-loop traffic simulation. Conference on Robot Learning (CoRL), 2024. 2, 5, 6 [26] Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp KrähenInteractive post-training for vision-language-action bühl. models. arXiv preprint arXiv:2505.17016, 2025. 9 [27] Shuhan Tan, John Lambert, Hong Jeon, Sakshum Kulshrestha, Yijing Bai, Jing Luo, Dragomir Anguelov, Mingxing Tan, and Chiyu Max Jiang. Scenediffuser++: City-scale traffic simulation via generative world model. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 15701580, 2025. 2 [28] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, and Jifeng Dai. 10 [41] Zhejun Zhang, Peter Karkus, Maximilian Igl, Wenhao Ding, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Closedloop supervised fine-tuning of tokenized traffic models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3, 5, 6, 7, 9 [42] Jianbo Zhao, Jiaheng Zhuang, Qibin Zhou, Taiyu Ban, Ziyao Xu, Hangning Zhou, Junhe Wang, Guoan Wang, Zhiheng Li, and Bin Li. Kigras: Kinematic-driven generative model for realistic agent simulation. arXiv preprint arXiv:2407.12940, 2024. 2 [43] Tianyang Zhao, Yuke Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu. Multi-agent tensor fusion for contextual trajectory prediction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [44] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray. Language-guided traffic simulation via scene-level diffusion. In Conference on Robot Learning, pages 144177. PMLR, 2023. 2 [45] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulaIn 2023 IEEE International Conference on Robotics tion. and Automation (ICRA), pages 35603566. IEEE, 2023. 2 [46] Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and Chun Jason Xue. Behaviorgpt: Smart agent simulation for autonomous driving with next-patch prediction. arXiv preprint arXiv:2405.17372, 2024. 2 Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024. 2 [29] Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat, Sergio Casas, Mengye Ren, and Raquel Urtasun. Advsim: Generating safety-critical sceIn Proceedings of the narios for self-driving vehicles. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99099918, 2021. 2 [30] Cathy Wu, Abdelrahman Kreidieh, Karthik Parvate, Eugene Vinitsky, and Alexandre Bayen. Flow: modular learning framework for mixed autonomy traffic. In IEEE Transactions on Robotics, pages 16771689, 2021. [31] Wei Wu, Xiaoxin Feng, Ziyan Gao, and Yuheng KAN. Smart: Scalable multi-agent real-time motion generation via In Advances in Neural Information next-token prediction. Processing Systems, pages 114048114071. Curran Associates, Inc., 2024. 1, 2, 3, 5, 6, 7, 8 [32] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2 [33] Yuting Xie, Xianda Guo, Cong Wang, Kunhua Liu, and Long Chen. Advdiffuser: Generating adversarial safetycritical driving scenarios via guided diffusion. arXiv preprint arXiv:2410.08453, 2024. 2 [34] Zixun Xie, Sicheng Zuo, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jie Zhou, Jiwen Lu, and Shanghang Zhang. Gpd1: Generative pre-training for driving, 2024. 6 [35] Chejian Xu, Ding Zhao, Alberto Sangiovanni-Vincentelli, and Bo Li. Diffscene: Diffusion-based safety-critical scenario generation for autonomous vehicles. AdvML-Frontiers 2023, 2023. 2 [36] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. In 2023 Bits: Bi-level imitation for traffic simulation. IEEE International Conference on Robotics and Automation (ICRA), page 29292936. IEEE, 2023. [37] Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, and Lifu Huang. Modalityspecialized synergizers for interleaved vision-language generalists. In International Conference on Learning Representations (ICLR), 2025. 2 [38] Xiuyu Yang, Zhuangyan Zhang, Haikuo Du, Sui Yang, Fengping Sun, Yanbo Liu, Ling Pei, Wenchao Xu, Weiqi Sun, and Zhengyu Li. Rmmdet: Road-side multitype and multigroup sensor detection system for autonomous driving. arXiv preprint arXiv:2303.05203, 2023. 2 [39] Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, Simon Suo, and Raquel Urtasun. Learning realistic traffic agents in closed-loop. In 7th Annual Conference on Robot Learning, 2023. 2 [40] Zhejun Zhang, Christos Sakaridis, and Luc Van Gool. Trafficbots v1. 5: Traffic simulation via conditional vaes and transformers with relative pose encoding. arXiv preprint arXiv:2406.10898, 2024. 2, 5, 6 11 Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Supplementary Videos Table 5. Main hyperparameters of InfGen model. We provide additional videos for better demonstration of InfGen. In this video, we showcase the existing problem of current baselines, as introduced in Sec. 1, and the comparison between baselines and our method. Then we have more qualitative examples. Please refer to our project page for details. B. Model Details In this section, we provide more details of InfGen model. We give an overview of the main hyperparameters of the model architecture, and explain the setup of agent embedding and query which are directly operated by transformer decoder. Then we describe more about how to decode various outputs. Hyperparameters. We extend the base model from SMART-7M [31] to train out InfGen model. We also have the detailed descriptions about the transformer decoders in Sec. 4.2, and we list all the main hyperparameters of the model architecture and our implementations in Table. 5. Note that we also inherit the road network from [31] (See Sec.3.3 of their paper) in our NTP pre-training process (where the Map-Map Attention Layers are incorporated). Since we adopt the map tokenizer and transform maps into discrete tokens, this network will help the model to understand the topological connectivity and continuity of unordered map tokens. Finally, the total model size of InfGen is around 11M. B.1. Agent Feature Learning Agent Embedding. In our stacked attention layers, we directly operate on different tokens according to each specific task. For practical purposes, we construct comprehensive aggregated agent embedding Fagent RT (where denotes the rollout horizon) to process them uniformly as token sequences in the transformer layers, as mentioned in Sec. 4.3. Specifically, we obtain the km, vm in Eq. 3, 4, 5 from tokens of different modalities through direct fusion, as Eq. 9. We first concatenate all the features on channel dimension and then produce the agent embedding through 1-layer MLP which is directly operated by transformer layers: MLP(Concat(Fmotion, Fposition, Fvalidity, Fattribute)), (9) where validity denotes if the agent at current timestep is visible by the environment, which is labeled in real log data."
        },
        {
            "title": "Hyperparameter",
            "content": "Transformer Decoder attention head dimension number of attention heads number of motion transformer blocks number of scene transformer blocks number of map transformer blocks D, feature dimension of token embedding number of frequency bands tw, Agent temporal Attention radius raa, Agent-Agent Attention radius rma, Map-Agent Attention radius rmm, Map-Map Attention radius rqa, Query-Agent Attention radius rqm, Query-Map Attention radius Tokenizer R, radius of position grids g, grid interval of Vpos θ, angle interval of Vhead Vmotion, vocabulary size of motion tokens Vmap, vocabulary size of map tokens Vpos, vocabulary size of position tokens Vhead, vocabulary size of heading tokens Vcontrol, vocabulary size of control tokens"
        },
        {
            "title": "Value",
            "content": "16 8 6 3 3 128 64 12 60 30 10 10 75,10 75 3 3 2048 1024 1849 120 4 And Fattribute aggregates the shape and type values of each agent. Notably, validity RT 1 here reflects not only the state of agent but also implicitly embeds the control tokens. To get all these features from their low-dimension values, we use MLP Layers for those in continuous space (e.g., agent shapes) and learnable embeddings for those in discrete space. Finally, we have the dynamic agent matrix tensor FA RAT (as the matrix in Fig. 3) which is continuously updated during the rollout in an interleaved autoregressive manner. Agent Query. As we explained in Sec. 4.2, we start from an agent query a0 to autoregressively insert the agents in spatial scene generation. Ideally, we consider that the spatiotemporal features of the agent query are fully aligned with the ego agent, i.e., it follows the ego agents position and motion. Since we are primarily concerned with the environment around the ego agent in this task, and the position tokens Vpos are also centered at the ego agent. To build this query, we take exactly the same approach as Eq. 9. For its 1 motion token, we directly use another new special token instead of any existing token from Vmotion. For its position token, we fix it as the centroid of the Vpos. For its validity, we set it as invalid. And we use new special agent type and shape values as its inherent attributes. B.2. Modeling Layer Position-aware Attention. We explicitly model the relative spatial-temporal positions between input tokens in attention calculation (as in Eq. 3, 4, 5). For each querycontext token pair (e.g., agent-agent or agent-map), we add the relative positional encoding from context token Fc to query token Fq (Fc, Fq RD are from FA). Specifically, we incorporate 3 types of descriptors: relative distance p, the relative direction d, the relative heading θ, where R2 and θ (π, π) denotes the positions and headings of input tokens. For temporal attention module (Eq. 3), we add additional time span to formulate 4D descriptors: pcq = pc pq2, θcq = θc θq, tcq = tc tq, dcq = atan2(pc,y pq,y, pc,x pq,x) θq, (10) which are formulated to rij RD, the relative positional encodings of tokens Fi,Fj and then added to keys, values in geometric attention layers (Eq.3,4,5): sampling. Secondly, we get the updated agent query 0 by locating it at the accurate position which is translated from the position token, with its heading same as the one of the ego agent. And again let 0 to attend to its environment and output refined feature , then send it to the heading head 0 to get another probability distribution over the heading tokens Vhead. The headings of the new agents are determined by sample the token with the highest probability. In step of decoding headings of new agents, only Agent-Agent Attention and Map-Agent Attention will be employed, since we consider the headings are highly related to the surrounding agents and maps, but not the occupancy grids. Note that, in Map-Agent Attention Layers, we use different visible range when decoding position tokens and heading tokens. For position tokens, we use an = 75m while for heading tokens, = 10m (also specified in Table 5). We autoregressively repeat such position-heading decoding step which is controlled by control tokens. Decoding Attributes. As mentioned in Eq. 8, for those newly-inserted agents, we also calculate the losses of their shapes and types, since these attributes also have inherent relationship with their initial pose. We directly predict the continuous shapes value (l, h, w) through 3-layer MLP head. To get the types, we predict the categorical distributions over 3 defined types in WOMD (vehicle, cyclist and pedestrian) and take top-1 sampling. rij = PE([pij, dij, θij, tij]), = Attn(ϕq(F l1 A ), ϕk(F l1 ), ϕv(F l1 ), r, I). (12) (11) C. Token Details In Equation 11, PE is the Fourier embedding layers. In Equation 12, (i, j) and NK2 indicates the directed or symmetric index set of total involved context-query token pairs which are determined through distance thresholds rqc (visible range centered on query token specified in Table 5) with the upper limitation of number of total context tokens Nc. While RKD denotes the stacked positional encodings of total pairs. And ϕ represents the projection function to query, key and value, reflects the index of attention layer. Decoding Pose Token. As described in Sec. 4.2 and Fig. 3, we sample new pose token from the categorical distributions from the updated query feature fa0 at each autoregressive step, simultaneously with the control token. We detail the decoding process of the pose tokens: we formulate the pose head as the sequentially connected position head and heading head, in this case we decode pose token through two separate steps. Given an agent query a0, we attach it to the ego agent to attend to environments with position-awareness, and produce the scene generation feature a0. We then first input it to position head to get the distribution over the position tokens Vpos, from which we get the position token of the new agent candidate through top-K In this section, we have more details regarding the design of our tokenization mechanisms, especially the control tokens, and the formulations of GT tokens used for training. Temporal Tokenization. Regarding the tokens associated with temporal transitions, e.g., motion tokens, control tokens, we first tokenize the time axis at time span of δ = 5 step (0.5 at 10 FPS), as Eq. 13. Accordingly, real log from WOMD [14] (n = 91 steps for 9.1 s) results in = n/δ = 18 discrete tokens. Tokenize({x0, x1, , xn1}) = {ˆx0, ˆx1, , ˆxN 1}, (13) where denotes features with temporal transitions, e.g., motions xm, validities xv. Importantly, the value of each ˆx is defined based on different rules. For motions, each ˆxm is aggregated motions vector xm δk : δ(k+1) over the k-th time segment, as mentioned in Sec. 4.1. Furthermore, regarding the validity of agents, we consider both the starting and ending steps within its time segment: ˆxv is considered True if and only if both xv δ(k+1) are True, i.e., ˆxv δk xv = xv δ(k+1), as motion token is meaningful only when ˆx is valid. δk and xv Control Tokens. We aim to explain how to derive the control tokens from real logs. Starting from the token-wise validity sequences (Eq. 13), we define the <ADD AGENT> token as: k1 when ˆxv ˆxc k1 = True and 0 < k1, ˆxv = False, (14) and the <REMOVE AGENT> token ˆxc is symmetrically dek2 fined as the last valid token such that all subsequent ones are invalid, i.e., k2 < 1, ˆxv = False. Thus, we set all tokens between <ADD AGENT> and <REMOVE AGENT>, ˆxc (k1 < < k2), as the <KEEP AGENT> token. Notably, for two special cases: 1) we force the ˆxc k1 with k1 = 0 of Eq. 14 to be <ADD AGENT>; 2) for any <REMOVE AGENT> token ˆxc with k2 = 1, we force it to be instead k2 <KEEP AGENT>. As we explained in Sec. 4.2, the tokens <ADD AGENT> and <BEGIN MOTION> only present when we examine the dynamic agent matrix column-wise. Therefore, when we organize the tokens sequence according to the spatial layout (as opposed to Eq. 13), <BEGIN MOTION> is defined as the next token after all the <ADD AGENT> tokens: when < l, ˆxc ˆxc is <ADD AGENT>. (15) Moreover, for these <ADD AGENT> tokens of the GT spatial sequence, they are ordered according to the distances from the ego agent, as explained in Sec. 4.4. Empty Token. Since we incorporate the invalid steps/agents, e.g., those with ˆxv = False, into InfGen model, we also involve the empty tokens shown in Figure 3, as part of dynamic agent matrix tensor FA RAT D. To build these invalid agent embeddings, we follow the similar methods of the agent query, but set their positions and headings to zeros. Introducing these invalid values can bring unexpected noise within the modeling layers (in Sec. 4.2), specifically, interactions between tokens from different timesteps or different agents when any side of them has ˆxv = False, which arise from two sources: 1) the construction of dynamic agent matrix tensor FA (Eq. 9); 2) position-aware attention layers (Eq. 10, 11). We address them through applying the rules: ˆxinvalid ˆxinvalid zinvalid, ˆxvalid ˆxinvalid ztrans, ˆxinvalid ˆxvalid ztrans, (16a) (16b) (16c) where ˆx denotes tokens of various features, e.g., motion tokens ˆxm, heading tokens ˆxh, and ˆxinvalid, ˆxvalid reflect their corresponding ˆxv are False, True, respectively. We force these values to be our predefined constant values to eliminate such noises due to the non-constant ˆxvalid. Since the model actually only needs the qualitative characteristic of the transition between invalid and valid states, rather than the specific quantitative values. In our experiments, we set ztrans = 1 and zinvalid = 2. Without Equation 16, InfGen will suffer from the disruptive noises, preventing effective modeling of the control sequence. D. Training Details As we summarized in Sec. 4.4, we efficiently end-to-end train InfGen model on multimodal token sequences. Basically, we parallelly train temporal motion simulation and spatial scene generation as standard NTP task of each individual token modality, and perform interleaved autoregression in inference stage. We break down the details of the each aspect for training process in this section. D.1. Temporal Simulation }N 1 k=0 Vmotion, validities {ˆxv Temporal Motions Training. We train on temporal motion tokens similar to prior works [31], and additionally deal with the transition of before and after an agent is inserted or removed. Given the temporal discrete tokens of one agent in Eq. 13, we have their GT motion tokens {ˆxm k=0 {0, 1} (0 = False, 1 = True), and, furthermore, the control tokens. From the view of the temporal axis, <ADD AGENT> denotes the start of the sequence (BOS) while <REMOVE AGENT> denotes the end of the sequence (EOS). Therefore, we refer to the states of the agent as its validities combined with BOS and EOS. k}N 1 To supervise the motion tokens tensor RN predicted by motion head, we derive the motion mask BN from the states3 (Note that masks are temporally aligned with prediction sequences, not groud-truth sequences). Assuming the step of the BOS and EOS are sBOS, sEOS, then we have: 1) MsBOS = 1: the step of BOS; sBOS+2 (with xv 2) MsBOS+1 = xv sBOS+1 = 1): the = xv sBOS next step after BOS; xv s1 xv 3) Ms = xv s+1, s, sBOS + 1 < < sEOS: the steps between the step after BOS and EOS (not included) only when the corresponding GT motions are valid. Otherwise, the steps not satisfy the conditions above have Ms = 0, including the EOS. Let := {ˆxm k=0 , then the total loss for motion tokens is: }N 1 Lm 1:N = 1 (cid:88) kI CE(cid:0)Y , k (cid:1), = {k = 1}, (17) where CE is the CrossEntropy loss, as also described in Eq. 7. Temporal Controls Training. simulation, we train on temporal control tokens {ˆxc In the part of temporal k=0 k}N 3We omit the superscript of in this section when possible for simplicity. 3 {<NULL>, <KEEP AGENT>, <REMOVE AGENT>} similar to motion ones. We have described how to derive the control tokens <KEEP AGENT> and <REMOVE AGENT> from the validities in Sec. C. And we have <NULL> as the placeholder token to indicate those steps without any control operations, which allows ct to fully represent the entire GT temporal token sequence. To supervise the control tokens tensor ct RN predicted by control head, we derive the temporal control mask ct BN from the states. Here we have: 1) Ms<sBOS = 0: the steps before BOS (not included); 2) MsBOS = 1: the step of BOS; sBOS+2 (with xv 3) MsBOS+1 = xv sBOS+1 = 1): the = xv sBOS next step after BOS; 4) MssEOS = 0: the steps after EOS (included); 5) Ms = xv s+1, s, sBOS + 1 < < sEOS: the steps between the step after BOS and EOS (not included) only when the corresponding GT motions are valid. s1 xv xv Then the total loss Lct 1:N for the entire temporal control token sequence is calculated similar to Equation 17 which takes ct,X ct,M ct as inputs. Note that the steps with Ms = 1(sBOS < < sEOS 1) correspond to the <KEEP AGENT> tokens, while those with Ms = 1(s = sEOS 1) correspond to the <REMOVE AGENT> tokens. To alleviate the imbalance of these two control tokens, we set the label weights: w(<KEEP AGENT>) = 0.1 and w(<REMOVE AGENT>) = 0.9 when calculating the CrossEntropy Loss. D.2. Spatial Generation }L Spatial Controls Training. For the spatial scene generation, we train on the control sequence {ˆxcs k=0 {<NULL>, <ADD AGENT>, <BEGIN MOTION>}. And denotes the total number of agents (including those existing and to be added) in real log and we force = 32 in training process for saving memory. We also use <NULL> as the placeholder for those agents without controls (e.g., existing and not to be added ones), allowing cs to fully represent the entire GT spatial token sequence. To formulate the spatial token sequence cs, we reorganize the all agents along the spatial axis, as explained in Sec. C, the first sequence (BOS) when scene generation begins, while <BEGIN MOTION> denotes sequence (EOS). Hence, the tokens between BOS and EOS (not included) are all <ADD AGENT> tokens, and <NULL> only present after EOS which corresponds to those agents currently in motion simulation. As considerable number of the trailing tokens in ct are <NULL> that cannot be trained, we further truncate the trailing part of ctonly consider the first = 10 tokens in training for more efficiency. And the size of cs in inference is scalable since we train in an autoregressive way. To supervise the control tokens tensor cs RL predicted by control head, we also have the mask ct BL following: 1) MsBOS = 1: the step of BOS; 2) MssEOS = 0: the steps after EOS (included); 3) Ms = 1, s, sBOS < < sEOS: the steps between BOS and EOS (not included). Then the total loss Lcs 1:M for the spatial control tokens is obtained similar to Equation 17. We also have label weights: w(<ADD AGENT>) = 0.1 and w(<BEGIN MOTION>) = 0.9 to deal with the class imbalance. Spatial Hybrid Attention. To model such spatial sequence ct in an autoregressive manner, we adapt the causal attention mechanism (in Agent-Agent Attention layers) similar to the Temporal Attention layers. , control token ˆxc Specifically, when predicting token ˆxt (e.g., motion token ˆxm t) in temporal simulation, it will only involve the history tokens {ˆxtτ }tw τ =1 within the time window as the context (as Eq. 3). Therefore, given the spatial token sequence cs RL and there exist agents already in motion simulations, we construct hybrid mask ct 1) For those agents that already exist, they will not be hybrid BL(A+L) which consists of two parts: masked out: ct hybrid[1 : L, 1 : A] = 0LA. 2) For those agents to be predicted (may not all correhybrid[1 : L, A+1 : spond to <ADD AGENT>), we have ct A+L] to be standard causal mask to exclude the futures in attention layers. We can wite it as: cs hybrid[i, j] = (cid:40) if or < 0, , otherwise , (18) where [1, L], [1, + L]. Note that the ultimate context which query features attend to is determined by jointly applying cs hybrid and other possible masks (e.g., from the visible range). In this way, we train on spatial token sequence with smaller context length = 10 while extend it to scalable number (> 10) with an upper limit of 128. E. Additional Results Long-term Traffic Simulation. We show more qualitative comparison results of InfGen and the baselines [24,32] in Fig. 9, 10,and 11. In these scenario, we again demonstrate the strengths of our approach. As the ego agent travels for away from the initial locations, new agents appear in our examples while maintaining great realism, seamlessly continuing the interaction process. This indicates that InfGen can effectively one of the major challenges of longterm traffic simulation task. Metrics Curve for Long-term Simulation. We further investigate how simulation realism evolves over the duration of long-term rollouts by plotting metric scores for each 4 Figure 6. Metrics (adapted WOSAC) curve of InfGen against SMART [31] over the 30s long-term simulation rollouts. sliding window index in Figure 6. Specifically, we show the evolution of three WOSAC metric components (Composite, Kinematic, and Placement-based) for both our method and SMART [24]. As expected, we observe gradual decrease in realism scores across all methods, reflecting the increasing difficulty of maintaining realism over extended simulation periods. However, our method consistently outperforms SMART by exhibiting notably slower decline in all metrics, particularly in the placement-based component. This significant improvement highlights the effectiveness of our proposed interleaved scene generation and motion simulation approach, enabling sustained realism by dynamically handling agent insertions and removals. These quantitative results strongly align with our qualitative observations, further emphasizing the importance of explicitly modeling agent placement and removal to achieve realistic long-term traffic simulations. F. Limitations and Future Direction Although InfGen has achieved promising results on longterm traffic simulation, our method is limited in some aspects, as briefly described in Sec. 6. In this section, we have detailed discussions on these terms. Failure Cases. We have some failure cases existed: (1) Unreasonable inserted agents. In some examples, our method may have unreasonable newly entered agents in traffic scenario. As shown in Fig. 7, at = 6 and = 12 s, the agents highlighted by red boxes occupy the road boundaries, which is unrealistic in real-world scenarios. It indicates that our method lacks sufficient control at such fine-grained level. Notably, we did not impose any explicit constraints on this kind of cases, such as regularization losses. (2) Incorrect initial motion inferring. During the spatial sequence prediction, InfGen first observes overall traffic scenario before placing new agents in potential locations. When these agents are located in complex road situation, they may fail to accurately infer their initial velocity (or moFigure 7. Failure case #1: newly-entered agents appear unreasonably on the boundary of the road. Figure 8. Failure case #2: In the region with complex map structure (highlighted by red box), newly-entered agents fail to correctly infer their initial velocity (or motion) and remain stationary, which is unrealistic. tion) for the subsequent rollout. For example, as shown in Fig. 8, some new agents incorrectly remain stationary on the driving lanes, which also impacts the motions of other agents, ultimately reducing the realism. (3) Agents flickering modeling. According to our observations, the phenomenon of agents flickering is prevalent in real-world data, particularly in regions farther from the ego agent. And it arises due to the instability of the ego agents remote perception. In our work, we handle these flickering agents in two ways: first, we discard agents with presence duration shorter than 0.5 s; second, we change the flickering frequency, which can be attributed to the resolution of discretization on time axis (as introduced in Sec. C). Specifically, the minimum temporal granularity that each token can represent is 0.5s. As result, InfGen inherently struggles to generate highly realistic agents exhibiting flickering behavior. potential solution is to introduce another format of tokens Vvalidity which reflects the frame-wise validity within even one 0.5 token, and make InfGen learn it in temporal simulation. Given that each 0.5 segment contains 5 valid timesteps, with each step has 2 possible states (invalid, and valid), we can derive the vocabulary size: 25 = 32. But the task becomes more complex under such conditions. We leave this as future direction. Future Directions. In addition to addressing the limitations discussed, some other potential interesting improvements may be as below: (1) Long context understanding and learning. While InfGen effectively addresses the challenge faced by Longterm Sim Agentmodeling the insertion and deletion of agents interleaved with their motions in continuously evolving traffic scenarios to maintain high realism over extended rollout durationswe believe that another critical bottleneck for even longer horizons is the accumulation of errors over the rollout process. In our work, we adhere to unified next-token prediction paradigm for end-to-end training, with reference to historical information constrained by the temporal length. Some hard cases in long-long-term rollout may fail: in busy intersection where the lateral traffic passes, followed by the longitudinal traffic while other different lanes remain open, the execution intervals of different actions can be significantly long, and the rules can be greatly complex. Some works [25, 41] utilize closed-loop training or finetuning for improvements, which also cannot be solution. Thus, it remains an other challenge. (2) Driving Map Generation. The duration of long-term rollout controlled by InfGen is significantly constrained by the size of the map region-without this limitation, it would be possible to extend the rollout even further. Therefore, integrating the map generation would be substantial improvement, enabling longer and more flexible simulations. Some concurrent works, such as GPD-1 [34], have some progress in this point, making it promising direction for near future. G. License The Waymo Open Motion Dataset (WOMD) [14] we used in our work is licensed under Waymo Dataset License Agreement for Non-Commercial Use4. The implementations of official WOSAC metrics5 based on which we develop our extended metrics are licensed under the Apache License, Version 2.0. 4https://waymo.com/open/terms/ 5https://github.com/waymo-research/waymo-open-dataset/ 6 Figure 9. More qualitative comparison results #1. 7 Figure 10. More qualitative comparison results #2. 8 Figure 11. More qualitative comparison results #3."
        }
    ],
    "affiliations": [
        "UT Austin"
    ]
}