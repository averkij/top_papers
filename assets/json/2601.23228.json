{
    "paper_title": "Scaling Multiagent Systems with Process Rewards",
    "authors": [
        "Ed Li",
        "Junyu Ren",
        "Cat Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision."
        },
        {
            "title": "Start",
            "content": "Ed Li * Junyu Ren * Cat Yan 6 2 0 2 0 3 ] . [ 1 8 2 2 3 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.017.5pp on AIME and +7.817.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision. 1. Introduction Large reasoning models trained with reinforcement learning have emerged as the dominant paradigm for state-of-theart AI systems (OpenAI, 2024; DeepSeek-AI, 2025; Snell et al., 2024). Augmented with external tools such as code interpreters and search engines (Schick et al., 2023), these models achieve remarkable performance on mathematical reasoning (Shao et al., 2024), coding (Jimenez et al., 2024), and scientific problems (Novikov et al., 2025). Building on these capabilities, multiagent systemswhere multiple language model-based agents interact to solve complex tasksoffer an intuitive path forward as specializations and *Equal contribution . Correspondence to: Ed Li <ed.li@yale.edu>. Preprint. February 2, 2026. 1 Figure 1. Multiagent architectures with separate weights enable specialization through end-to-end training, sidestepping catastrophic forgetting that limits single-model scaling just like mixtureof-experts (MoE) architecture. the diversity of perspectives can tackle problems better than any single agent. (Wu et al., 2023; Hong et al., 2023). The bulk of existing multiagent work implements specialization through prompt engineering: assigning different system prompts, personas, or tool access to agents within the system. While effective for orchestrating pre-trained models, the true strength of multiagent systems lies in the ability to modify the weights of individual agents, enabling specialization without interference. Recent evidence suggests such specialization naturally emerges through reinforcement learning: reasoning models trained solely for accuracy spontaneously develop diverse internal personas and expertise (Kim et al., 2026). By separating weights across agents, improving one agents capabilities cannot degrade anothers performance, allowing scaling without catastrophic forgetting (Figure 1). Recent work has begun exploring this direction of finetuning agents in multiagent systems simultaneously (Subramaniam et al., 2025; Liao et al., 2025; Park et al., 2025; Liu et al., 2025). However, two significant challenges remain: 1. Credit assignment: How do we attribute the performance of the overall system to actions taken by individual agents? 2. Computational cost: Rollouts of the entire multiagent system can take minutes or even hours, yet yield only single outcome reward signal."
        },
        {
            "title": "We address these challenges through finetuning MultiAgent",
            "content": "MAPPA: Scaling Multiagent Systems with Process Rewards system with Per-action Process rewards from AI feedback (MAPPA). Rather than assigning single sparse reward at trajectorys end according to fixed instructions in LLM-as-aJudge, we leverage language models as coaches to assess the quality of each agents action given its role, inputs, and environment feedback such as tool execution results. This yields dense learning signal throughout the trajectory, enabling effective training even when tasks fail entirely. Crucially, the coach performs implicit credit assignment: when downstream agent encounters file-not-found error for an artifact that an upstream agent should have produced, the coach assigns low scores to the upstream agents faulty actions rather than penalizing the downstream agent. Furthermore, the number of training signals now scales with the number of actions taken, dramatically improving sample efficiency over outcome-based methods. We validate this approach on two separate domains: mathematical reasoning with code execution (MathChat) and end-to-end data science pipelines (DSBench (Jing et al., 2025)). On MathChat, our three-agent system achieves improvements of +5.017.5pp on AIME and +7.817.2pp on AMC across two model configurations. On DSBench, where multiagent pipelines must engineer features, train models, and generate predictions, training improves success rate by +12.5pp while quality metrics improve by up to 30%. We present general framework for training multiagent systems on complex, tool-augmented tasks via coach-guided reinforcement learning. The key componentsagent topology, reward structure, and training pipelineare domainagnostic and can be configured to support diverse applications. Our source code is available at https://github. com/ltjed/multiagent-coaching. Overall, our results suggest that scaling the number of specialized agents represents promising new dimension for improving performance on complex, long-horizon tasks. 2. Methodology 2.1. Problem Setting MAPPA finetunes tool-augmented multiagent systems where LLM agents collaborate to solve tasks. Each agent is initialized from pretrained LLM with independent policy parameters trained separately, enabling specialization. All agents live in terminal environment and execute in pre-defined topology (e.g., sequential pipelines, debate, mixture-of-agents). Within each agents turn, it generates one or more actions that may include tool calls. When tool calls are generated as part of an agents action, they are processed and executed in sandboxed environment, the agent will then receive environment feedback (e.g., stdout, stderr, error messages). Depending on the task, the agent may also need to call tools to read, write files or other artifacts in the terminal environment. 2.2. Process Rewards from AI Feedback Multi-turn tool use with inter-agent dependencies creates challenging credit assignmentwhich agents actions led to success or failure? Unlike existing RL approaches that rely on sparse outcome rewards (success/failure for entire trajectories), we leverage coach LLM to perform peraction credit assignment, providing dense process-quality rewards on 010 scale.1 We use the term coach rather than judge to emphasize that while LLM-as-a-judge evaluates final outputs against fixed rubrics, coach provides context-aware feedback on each intermediate action to guide improvement throughout the trajectory. The coach evaluates each action holistically based on: (1) the agents role and responsibilities, (2) the input context the agent observed, (3) the agents action, and (4) tool execution results (stdout, stderr, error messages) when the output contains tool calls. This enables the coach to identify the responsible agent when failures occur: if downstream agent encounters missing file error, the coach assigns low scores to the upstream agent that should have produced it, not the downstream agent that correctly reported the issue. See Appendix for example coach evaluations demonstrating this context-aware feedback. Crucially, MAPPA can work with or without ground truth/verifier that most other alternatives RL approaches such as RLVR (Lightman et al., 2023; DeepSeek-AI, 2025) require. When ground truth is available (e.g., ROC-AUC, F1, RMSE for DSBench), the coach incorporates it to inform process scores via intelligently synthesizing multidimensional metrics into single reward; When ground truth is unavailable, the coach simply evaluates how sensible it is for the agent to take that action given its context to the best of the coachs ability. In this work, ground truth is only provided for the last action of the last agent in the multiagent system. Figure 2. Agent execution loop with per-action coach evaluation. 1We found the 010 scale empirically outperforms continuous 01 scores, likely due to the prevalence of 010 rating samples in the pre-training corpus, i.e., 7 or 4, carries more semantic meaning than decimal values like 0.305. 2 MAPPA: Scaling Multiagent Systems with Process Rewards 2.3. Training Algorithm As shown in Figure 2, each agent action generates trajectory tuple (agent id, input, action, reward) where the reward is provided by the coach evaluation. We then finetune each agents underlying model with its actions corresponding tuples via REINFORCE++ (Hu et al., 2025). We choose REINFORCE++ rather than Group-Relative Policy Optimization (GRPO) (DeepSeek-AI, 2025) for fundamental reason: GRPO normalizes advantages within groups of samples sharing the same prompt, assuming identical input states. This assumption breaks in end-to-end multiagent trainingeach agents input depends on upstream agents stochastic outputs, so even rollouts from the same initial prompt produce different intermediate states. REINFORCE++ instead applies global batch normalization across all experiences, naturally handling state diversity. Global normalization also reduces variance from noisy coach rewards compared to deterministic ground truth metrics. Each action receives an independent coach reward rcoach [0, 1], with KL penalties computed per-token. The KL-penalized reward and advantage are: rt = rcoach β DKL(πθπref), At = (cid:88) τ rτ (1) where β = 0.01 and At is the undiscounted return-to-go, propagating downstream rewards to earlier actions. Advantages are then globally normalized across all agents and experiences in the batch: ˆAt = (At µ)/σ. The policy gradient uses the standard PPO clipped objective with ϵ = 0.2. See Appendix for full derivations. 2.4. Scalable Distributed Training Architecture On-policy RL requires fresh trajectories from the current policy for each gradient update, creating an efficiency challenge for multiagent systems where rollouts are expensive. Our training infrastructure, extended from MARTI (Zhang et al., 2025), addresses this through tight coupling: each iteration consists of (1) parallel trajectory collection, (2) coach evaluation and experience preparation, and (3) synchronous gradient updatesthen immediately proceeds to the next iteration. Training uses Ray for distributed coordination, vLLM for inference, and DeepSpeed ZeRO-3 for memory-efficient updates. Each agent maintains independent actor groups initialized in parallel: vLLM engines for inference, reference models for KL computation, and policy actors for training, with optional model co-location on shared GPUs to reduce memory overhead. During rollout, prompts are sharded across workers that execute multiagent workflows independently, with coach evaluation overlapped asynchronously to reduce wall-clock time. Completed trajectories are routed to originating agents for gradient computation; NCCL broadcasts updated weights to vLLM engines after each training step. All agents share the training loop but maintain independent policy parameters. See Appendix for details. 3. Results We demonstrate our approach on two separate multiagent systems across two domains: competition math with code execution (MathChat) and end-to-end data science pipelines (DSBench). While competition math can be solved by single agents (OpenAI, 2024; DeepSeek-AI, 2025), applying MAPPA to multiagent systems with different underlying models and constraints tests the generality and robustness. DSBench, in contrast, more closely resembles real-world complex tasks that multiagent systems are built for. 3.1. MathChat: Competition Math with Code Execution Task and Dataset. MathChat involves solving AIME (American Invitational Mathematics Examination) competition math problems with code execution. We randomly sample 512 problems from 19832024 for training and use disjoint held-out sets for evaluation: 30 AIME problems from 2025 and 32 AMC (American Mathematics Competition, similar to but slightly simpler than AIME) problems. Multiagent Configuration. We implement three-agent sequential pipeline designed to separate reasoning, computation, and verification: Problem Solver: Drafts step-by-step reasoning for downstream agents. The prompt instructs: Your job is to draft solution to the problem... Code Executor: Has the option to write and execute Python code to verify and compute solutions. The prompt instructs: You can execute Python code. Write code in python blocks and it will be automatically executed. Verifier: Synthesizes information from upstream agents and outputs the final answer. The prompt instructs: You are the last agent. The system succeeds only if YOU output the correct answer. The final answer must be formatted as boxed{answer} to be counted as correct. Each agent is initialized from the same pretrained checkpoint but finetuned separately, enabling specialization. We experiment with two reasoning models: DeepSeekR1-Distill-Qwen-1.5B (DeepSeek-AI, 2025) and Qwen34B (Team, 2024). All agents use reasoning model inference with <think> tags, allowing internal deliberation before generating outputs. Each agent has 4K token output limit after which the response is cut off, content within <think> tags included. See Appendix for complete agent prompts. 3 MAPPA: Scaling Multiagent Systems with Process Rewards Table 1. MathChat performance on held-out competition math problems. R1-Distill-Qwen-1.5B Qwen3-4B AMC Baseline Best AIME Baseline Best 60.9% 78.1% +17.2pp 24.2% 29.2% +5.0pp 78.1% 85.9% +7.8pp 49.2% 66.7% +17.5pp Training Configuration. We finetune the multiagent systems using REINFORCE++ with Gemini 2.5 Flash as the coach on 8 NVIDIA H100 GPUs. Key hyperparameters: actor learning rate 106, rollout batch size 32, training temperature 1.0, evaluation temperature 0.6, KL penalty coefficient β = 0.01. With 512 training problems processed in rollout batches of 32, one epoch equals 16 training steps (one complete pass through all problems). We evaluate every 4 steps (every 0.25 epochs) on held-out problems. Each evaluation samples every problem 4 times and reports mean accuracy to reduce variance from stochastic generation. See Appendix G.9 for computational cost details. Results. As shown in Table 1, MAPPA improves performance across both model configurations, with gains ranging from +5.0pp to +17.5pp depending on model capacity and task difficulty. The larger Qwen3-4B achieves the biggest improvement (+17.5pp) on AIME, while the smaller DeepSeek-R1-Distill-Qwen-1.5B shows asymmetric gains: +17.2pp on the more accessible AMC but only +5.0pp on AIME 2025, suggesting capacity limitations on harder reasoning tasks. Interestingly, behavioral metrics reveal divergent learning patterns between the two models (Figure 3). The larger Qwen3-4B shows dramatic behavioral adaptation: successful tool call rate increases substantially while response lengths decrease across all three agents. In contrast, the smaller 1.5B model maintains relatively stable behavioral metrics throughout training. This suggests that with greater capacity, Qwen3-4B learns to leverage tool calls more effectively through MAPPA, while the 1.5B model improves accuracy without developing such qualitative behavioral changesdemonstrating that process rewards can drive improvement even when model capacity limits behavioral adaptation. We also evaluate under partial information constraints, where each agent observes only the immediately preceding agents output with no access to earlier context; results in Appendix A.1 show MAPPA still achieves consistent improvements (+3.95.8pp), further demonstrating robustness. Figure 3. Behavioral metrics during MathChat training. Code Executor Tool Call % (top-left) shows the percentage of Code Executor actions that contain tool call; the other three panels show average response length (in tokens) for each agent at each training step. Faint lines show raw data; solid lines show exponential moving average (α = 1/3). Qwen3-4B shows dramatically increased tool usage and more concise responses, while R1-DistillQwen-1.5B maintains stable behavior throughout training. 3.2. DSBench: Data Science Pipelines Task and Dataset. DSBench presents 72 Kaggle-style machine learning tasks requiring complete end-to-end pipeline execution. Each task provides training/test CSV files, task type (classification/regression), and ground truth labels for evaluation. We use 64 tasks for training and 6 for held-out evaluation (4 classification, 2 regression)an 8.6% heldout ratio that is reasonable given the limited size of existing data science benchmarks. Multiagent Configuration. We implement three-agent sequential pipeline where each agent specializes in distinct phase: the Data Engineer performs exploratory data analysis, preprocessing, and feature engineering (maximum 3 turns); the Modeler handles algorithm selection, model training, and hyperparameter tuning (maximum 5 turns); and the Analyst generates final predictions and formats submissions (maximum 2 turns). Each agent is initialized from the same pretrained checkpoint (Qwen3-4B) with independent policy parameters. Agents communicate via base64-encoded file passing: the Data Engineer produces train.pkl, train.pkl, test.pkl; the Modeler consumes these and produces model.pkl; the Analyst loads both to generate submission.csv. This explicit file dependency enables the coach to assign blame when pipeline failures occur. Figure 4 illustrates this threeagent pipeline; see Appendix for complete agent prompts. Training Configuration. We train using REINFORCE++ with Gemini 2.5 Pro as the coach. All agents have access to 4 MAPPA: Scaling Multiagent Systems with Process Rewards Figure 4. DSBench three-agent pipeline. Each agent executes Python code via shared sandbox, reading inputs from and writing outputs to shared file workspace. The Data Engineer preprocesses raw CSV files into pickle artifacts; the Modeler trains and saves model; the Analyst generates the final submission for evaluation. SandboxFusion for Python code execution. When ground truth labels are available, we compute task-specific metrics (Accuracy, F1 for classification; RMSE, MAE for regression) and provide them to the coach. Rather than naively averaging these metrics or picking single one, the coach intelligently weighs them in contextpenalizing reward hacking (e.g., optimizing one metric at the expense of others), attributing poor metrics to the responsible agent (Data Engineers preprocessing vs. Analysts predictions), and producing fair process score that reflects overall execution quality. Training uses Ray, vLLM, and DeepSpeed ZeRO-3 on 8 NVIDIA H100 GPUs. Key hyperparameters: actor learning rate 106, rollout batch size 16, training temperature 1.0, evaluation temperature 0.6. Checkpoints are evaluated every 2 steps on 6 held-out tasks. Results. The pipeline structure creates natural dependencies: the Modeler cannot train without properly formatted data from the Data Engineer, and the Analyst cannot predict without the trained model from the Modeler. This makes DSBench an ideal testbed for evaluating multiagent credit assignment and process-focused training. We train for 21 epochs (84 steps) and evaluate checkpoints every 2 steps. Table 2 presents metrics at three checkpoints: baseline (epoch 0), peak success rate (epoch 11), and late training (epoch 21). Figure 5 shows the full training dynamics across all 84 steps. Training initially improves both success rate (+16.7pp) and quality across task types (Accuracy +28.8%, RMSE 41.4% fair), validating that coach rewards translate to downstream metric improvements. Extended training (epoch 1121) shows pattern that might appear as overfitting: classification success drops from 56.2% back to baseline 43.8% (12.4pp), and classification quality metrics decline (Accuracy-Fair 6.8%, F1-Fair 33.9%). However, reFigure 5. DSBench training dynamics over 84 steps (21 epochs). Light points show raw metrics; solid line shows EMA (α=0.3). Dashed lines mark peak raw values. Classification metrics peak early (steps 4448) then decline, while regression RMSE continues improving through step 84, illustrating specialization to regression tasks. gression tells different storysuccess maintains its peak (87.5%) while quality continues improving: RMSE drops from 9.5% to 8.0% (15.8%). This asymmetry suggests specialization to regression rather than simple overfitting; we investigate this phenomenon in Section 4.1. Fair metrics enable reliable model selection by identifying epoch 11 as optimal for balanced performance across both task types. The per-agent coach scores in Table 2 demonstrate that credit assignment enables targeted learning, with each agent receiving feedback specific to its role rather than shared outcome-based rewards. MAPPA: Scaling Multiagent Systems with Process Rewards Table 2. DSBench evaluation comparing baseline (epoch 0), peak success rate (epoch 11), and late training (epoch 21). Epoch 11 maximizes success rate and fair metrics across both task types. By epoch 21, the model specializes in regression: RMSE continues improving while classification metrics regress to baseline. Fair metrics penalize failures (0.5 for Accuracy, 0 for F1, 50% for MAE/RMSE). Each epoch consists of 4 training steps. Metric Epoch 0 Epoch 11 0 Epoch 21 1121 Success Rate (24 eval samples: 16 classification + 8 regression) Classification Regression Total 56.2% 87.5% 66.7% 43.8% 62.5% 50.0% Classification Quality Accuracy (Raw) Accuracy (Fair) F1 (Raw) F1 (Fair) Regression Quality MAE (Raw) MAE (Fair) RMSE (Raw) RMSE (Fair) 0.690 0.583 0.288 0.126 7.1% 23.2% 9.8% 24.9% Coach Scores by Agent (010 scale) Data Engineer Modeler Analyst 5.43 4.55 5.56 0.889 0.719 0.309 0. 6.9% 12.3% 9.5% 14.6% 5.09 5.28 7.24 +12.4pp +25.0pp +16.7pp +28.8% +23.3% +7.3% +38.1% 2.8% 47.0% 3.1% 41.4% 6.3% +16.0% +30.2% 43.8% 87.5% 58.3% 0.889 0.670 0.263 0.115 5.7% 11.2% 8.0% 13.2% 5.60 5.10 7.17 12.4pp 0pp 8.4pp 0% 6.8% 14.9% 33.9% 17.4% 8.9% 15.8% 9.6% +10.0% 3.4% 1.0% 4. Discussion Despite the potential of multiagent systems, recent work studying inference-time coordination finds that naively scaling agent count without changing weights actually degrades performance on sequential reasoning tasks by 3970% (Kim et al., 2025). Our results show that finetuning multiagent systems with process rewards achieves substantial improvements on both MathChat and DSBench. In this section, we first investigate an unexpected specialization pattern in DSBench where training improves regression performance while classification regresses, tracing this to systematic biases in coach scoring. We then discuss coach model selection and compare our approach with prior multiagent finetuning work. Finally, we outline limitations and future directions including trainable coaches and reward backpropagation. for regression tasks, leading to the observed specialization. This pattern illustrates fundamental limitation of stateless evaluation: our coach judges each action in isolation without awareness that its scores drive gradient updates, and thus cannot detect or correct emergent imbalances in its own scoring behavior. We discuss how future coaches might overcome this limitation in Section 4.4. Table 3. Coach score delta (Regression Classification) by agent across training epochs. Positive values indicate higher scores for regression tasks. The Data Engineers widening delta correlates with the observed specialization. Agent Epoch 0 Epoch 11 Epoch 21 Data Engineer Modeler Analyst +1.15 +0.51 +1.80 +1.40 +1.06 +1.17 +1.67 +1.02 +0.77 4.1. Investigating Specialization 4.2. Coach Model Choice The DSBench results reveal an unexpected pattern: while regression performance continues improving, classification metrics regress to baseline. To understand this specialization, we stratify coach scores by task type  (Table 3)  . The analysis reveals systematic pattern: the coach assigns higher scores to regression tasks across all agents, with deltas ranging from +0.51 to +1.80. Notably, the Data Engineers delta widens from +1.15 to +1.67 over training, while the Analysts narrows from +1.80 to +0.77. We hypothesize that agents learn to exploit the coachs preference Effective credit assignment in multiagent systems requires more than observing actionsit requires reasoning about causality. When DSBench pipeline fails to produce predictions, the coach must determine whether the Data Engineer failed to save required files, the Modeler chose an inappropriate algorithm, or the Analyst made errors in the final processing stage. This root-cause analysis demands strong logical reasoning capabilities, substantial context windows to process full agent transcripts, and the ability to interpret tool outputs (code execution results, file operations, error messages). Additionally, the coachs informa6 MAPPA: Scaling Multiagent Systems with Process Rewards tion asymmetryaccess to environment feedback invisible to agentsenables credit assignment that would be impossible from trajectory data alone. We expect weaker models to also function as coaches, given two fundamental asymmetries: (1) information asymmetry the coach observes tool outputs and environment feedback that agents cannot see, and (2) task asymmetrycritiquing an action upon reflection is easier than proposing one under uncertainty. The advantage of stronger coaches is primarily quantitative rather than qualitative: frontier models like GPT-4, Claude-3.5-Sonnet, and Gemini-2.5 assign appropriate rewards in more situations with higher reliability, reducing noise in the training gradient signal and making training more sampleand epoch-efficient. Weaker coaches would still provide directionally correct feedback, but with higher variance. This design also aligns with our perspective that scaling specialized agents represents new dimension for improving performance (Figure 1). Strong general-purpose models serve as coaches to fine-tune swarm of smaller, specialized agents. Once trained, the multiagent system can collectively outperform the single strong model in both efficiency and performance ceiling: smaller models are cheaper to run in parallel, and specialized agents can develop domain expertise that generalist models lack. The coach bootstraps capabilities it cannot itself achieve through direct inference. 4.3. Comparison with Prior Work Most existing multiagent frameworks like AutoGen (Wu et al., 2023), MetaGPT (Hong et al., 2023), and CAMEL (Li et al., 2023) focus on collaboration through prompt engineering but do not update agent weights. Recently, more work has begun experimenting with finetuning specific multiagent system setups: Subramaniam et al. (2025) propose Multiagent Finetuning, establishing that training multiple agents outperforms training single agent by preserving diverse reasoning chains; however, their approach uses supervised fine-tuning rather than RL, and focuses specifically on debate where homogeneous agents generate competing solutions to the same problema narrower setting than our sequential pipelines with heterogeneous specialized agents. Liao et al. (2025) introduce MARFT, providing theoretical foundations for adapting MARL to LLM-based multiagent systems; our work is complementary, focusing on the practical reward design challenge rather than algorithmic formalism. Park et al. (2025) present MAPoRL, which co-trains agents through multi-turn debate where verifier scores both answer correctness and discussion quality (rewarding corrective and persuasive exchanges). While MAPoRL demonstrates that co-training is essentialtraining individual LLMs alone fails to induce collaborationtheir approach requires ground truth labels to train the verifier, uses sparse outcome-based rewards augmented with discussion incentives rather than dense per-action process feedback, and focuses on homogeneous debate agents rather than heterogeneous specialized pipelines. Our coach provides dense supervision at every action without requiring ground truth, enabling learning even when tasks fail entirely. Liu et al. (2025) formalize LLM collaboration as DecPOMDP and propose MAGRPO, extending GRPO to multiagent multi-turn settings; however, GRPOs same-state assumption breaks in heterogeneous pipelines where upstream stochasticity causes state divergence (see Section 2.3). We use REINFORCE++ with global batch normalization, and our coachs context-aware evaluation enables stable gradient estimation even when actions arise from divergent trajectories. 4.4. Future Directions Evaluation Limitations. Our experiments use standard benchmark sizes: 32 AMC problems, 30 AIME 2025 problems, and 6 DSBench modeling tasks for held-out evaluation. While these sets are small, this reflects domain constraints DSBench contains only 72 total modeling tasks, yielding an 8.6% held-out ratio that is reasonable for such limited benchmarks. We prioritize training on AIME problems given their difficulty and relevance to mathematical reasoning. To mitigate variance from small evaluation sets, each problem is evaluated multiple times per checkpoint (4 for MathChat, 2 for DSBench) with accuracy computed as the mean across attempts. Due to computational constraints, we report results from single training runs rather than confidence intervals across multiple seeds. Tables report peak accuracy at the best-performing checkpoint for each benchmark, which may introduce optimistic bias. Future work should validate with multiple seeds as compute permits. Mitigating Coach Biases. Using LLMs to assess LLM outputs has well-documented biases that affect evaluation reliability (Ye et al., 2024; Chen et al., 2024). Verbosity bias leads to preference for longer responses regardless of quality. Self-enhancement bias emerges when LLMs favor outputs similar to their own generations (Wataoka et al., 2024). Our results demonstrate the MAPPA is effective but such biases may still influence scores. In the future, we believe more investigation is warranted, with potential mitigation strategies including ensembling multiple coach models (e.g., combining Claude, Gemini, and GPT-4). Beyond Scalar Rewards. Currently, our approach only uses scalar rewards from AI feedback, but the coaches are capable of much richer feedback. For instance, when an agents action is suboptimal, the coach could generate corrected action that the agents should have taken, enabling supervised finetuning (SFT) on improved demonstrations or preference learning via DPO (Rafailov et al., 2023), that 7 MAPPA: Scaling Multiagent Systems with Process Rewards combine these with RL. Extracting richer signal from each coach interaction could further improve sample efficiency, especially when agents perform task to low success rate even though this improvement may come at the expense of reduced exploration compared to pure RL as in our current work. Trainable Coach. The coach itself could be trainable agent inside the multiagent system. This raises fundamental question: what signal should train the coach? Options include meta-evaluation from stronger external model, agreement with outcome-based verification when available, or human feedback on coach decisions. Whether fully selfcontained systemwhere agents and coaches co-evolve without external supervisioncan avoid degenerate equilibria remains an open question. Reward Backpropagation. Our current approach assigns process rewards independently at each stepa bottom-up method where each agent is critiqued on everything that could be improved, without knowing which improvements actually matter. more efficient approach would backpropagate outcome information: given specific outcome, trace backward to identify which agent made which mistake, or which agents action saved the team. At each backward step, the judge attempts to explain the outcome from the current agents actions; if reasonable explanation exists, credit or blame is attributed there, otherwise the judge peels off minor contributions and passes the residual to the previous agent. This mirrors formal backpropagation, where each layer provides gradient direction for the layer below based on the current loss. We formalize this in Appendix F. Agent-as-a-Coach. Our current coach has full context about each agents task, behavior, and consequences, but lacks awareness that its scores will be used to fine-tune models. This stateless evaluation underexploits LLM intelligence. true agent-as-a-coach would have access to training history and performance trends, enabling it to discover patterns like have been scoring regression tasks +1.2 higher on averageam creating imbalance? or classification success has stagnated while regression keeps improving. Beyond self-reflection, an agentic coach could implement strategic training: First establish reliable task completion by rewarding successful runs even with mediocre quality; once success rates stabilize, shift focus to quality metrics. This mirrors how professional coaches build fundamentals before advanced techniques. The coach could use tools to compute statistics across training history, run code to verify correctness, or inspect intermediate artifactsmaking evaluation decisions informed by the full training context rather than isolated action judgments. See Appendix E.4 for detailed discussion of strategic multi-objective judging. This paradigm shift from LLM-as-judge (passive, state8 less) to agent-as-a-coach (active, strategic) scales with model capability: stronger coaches can implement more sophisticated curriculum strategies, detect and correct their own biases, and adaptively balance across task types based on observed training dynamics. Scaling to Large-Scale Multiagent Systems. MAPPA simplifies end-to-end training of multiagent systems to crafting an effective LLM coach and providing it with relevant information to make accurate assessments. The true potential of this approach lies in scaling to large multiagent systems with dozens or more agents for complex, long-horizon tasks such as scientific research, where multiagent systems start to show promise (Li et al., 2025). key challenge, however, is reward hacking: coach-assigned scores may improve while overall system success stagnates or declines. We believe collecting behavioral metrics throughout training, such as response length and tool call rate shown in Results, is key. These metrics serve as sanity checks and provide interpretable signals for diagnosing what is happening inside multiagent system as it evolves. 5. Conclusion MAPPA demonstrates that multiagent systems can be effectively trained end-to-end using process rewards from AI feedback. Dense per-action supervision solves credit assignment, improves sample efficiency, and generalizes across domainssuggesting that scaling specialized agents through finetuning, rather than prompting alone, represents promising frontier for complex, long-horizon tasks. We are entering an era where autonomous AI agents interacting with each other is increasingly the default, from enterprise workflows to scientific discovery, with minimal human intervention. As these multiagent systems grow in capability and deployment, evaluating and improving them becomes critical challenge. Our work takes first step toward addressing this challenge."
        },
        {
            "title": "Impact Statement",
            "content": "This work advances methods for training multiagent AI systems with minimal human supervision, contributing to the broader goal of building self-improving AI systems. While this could accelerate progress on complex tasks that benefit society, training AI with AI supervisors raises value alignment concerns, as trained agents may inherit biases from coach models. We recommend auditing coach models for biases and monitoring behavioral metrics to detect reward hacking. MAPPA: Scaling Multiagent Systems with Process Rewards"
        },
        {
            "title": "References",
            "content": "Chen, G. H., Chen, S., Liu, Z., Jiang, F., and Wang, B. Humans or LLMs as the judge? study on judgement bias. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. Hu, J., Liu, J. K., Xu, H., and Shen, W. REINFORCE++: Stabilizing critic-free policy optimization with global advantage normalization. arXiv preprint arXiv:2501.03262, 2025. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. SWE-bench: Can language models resolve real-world GitHub issues? In International Conference on Learning Representations, 2024. Jing, L., Huang, Z., Wang, X., Yao, W., Yu, W., Ma, K., Zhang, H., Du, X., and Yu, D. DSBench: How far are data science agents to becoming data science experts? In International Conference on Learning Representations, 2025. Kim, J., Lai, S., Scherrer, N., Aguera Arcas, B., and Evans, J. Reasoning models generate societies of thought. arXiv preprint arXiv:2601.10825, 2026. Kim, Y., Gu, K., Park, C., Park, C., Schmidgall, S., Heydari, A. A., Yan, Y., Zhang, Z., Zhuang, Y., Malhotra, M., Liang, P. P., Park, H. W., Yang, Y., Xu, X., Du, Y., Patel, S., Althoff, T., McDuff, D., and Liu, X. Towards science of scaling agent systems. arXiv preprint arXiv:2512.08296, 2025. Li, E., Ren, J., Pan, X., Yan, C., Li, C., Bergemann, D., and Yang, Z. Build your personalized research group: multiagent framework for continual and interactive science automation. arXiv preprint arXiv:2510.15624, 2025. Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. CAMEL: Communicative agents for mind exploration of large language model society. In Advances in Neural Information Processing Systems, 2023. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, S., Chen, T., Liang, Z., Lyu, X., and Amato, C. LLM collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652, 2025. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., and Balog, M. AlphaEvolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. OpenAI. OpenAI o1 system card. arXiv preprint arXiv:2412.16720, 2024. Park, C., Han, S., Guo, X., Ozdaglar, A. E., Zhang, K., and Kim, J.-K. MAPoRL: Multi-agent post-co-training for collaborative large language models with reinforcement learning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, 2023. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2023. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, X., Zhang, Y., Bi, X., Guo, D., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Subramaniam, V., Du, Y., Tenenbaum, J. B., Torralba, A., Li, S., and Mordatch, I. Multiagent finetuning: Self improvement with diverse reasoning chains. arXiv preprint arXiv:2501.05707, 2025. Team, Q. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Liao, J., Wen, M., Wang, J., and Zhang, W. MARFT: Multi-agent reinforcement fine-tuning. arXiv preprint arXiv:2504.16129, 2025. Wataoka, K., Takahashi, T., and Ri, R. Self-preference bias in LLM-as-a-judge. arXiv preprint arXiv:2410.21819, 2024. 9 MAPPA: Scaling Multiagent Systems with Process Rewards Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al. Autogen: Enabling next-gen LLM applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. Ye, J., Wang, Y., Huang, Y., Chen, D., et al. Justice or prejudice? quantifying biases in LLM-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. Zhang, K., Liu, R., Zhu, X., Tian, K., Zeng, S., Jia, G., Fan, Y., Lv, X., Zuo, Y., Jiang, C., Liu, Z., Wang, J., Wang, Y., Zhao, R., Hua, E., Wang, Y., Wang, S., Gao, J., Long, X., Sun, Y., Ma, Z., Cui, G., Bai, L., Ding, N., Qi, B., and Zhou, B. MARTI: framework for multi-agent LLM systems reinforced training and inference. https: //github.com/TsinghuaC3I/MARTI, 2025. 10 MAPPA: Scaling Multiagent Systems with Process Rewards A. Extended Experimental Results A.1. MathChat: Partial Information Results We also evaluate MAPPA under partial information constraints, where each agent observes only the immediately preceding agents output (except the Problem Solver, which sees the problem statement), with no access to earlier context or the original question. This tests whether MAPPA can train agents that must operate with limited context. Table 4. MathChat performance under partial information constraints (Qwen3-4B). Despite degraded baseline performance compared to full context (512pp), MAPPA achieves consistent improvements. Benchmark Baseline Best AMC (32) AIME 2025 (30) 72.7% 37.5% 76.6% +3.9pp 43.3% +5.8pp As expected, partial information constraints degrade baseline performance: AMC drops from 78.1% to 72.7% (5.4pp) and AIME from 49.2% to 37.5% (11.7pp) compared to full context. However, MAPPA still achieves consistent improvements of +3.9pp on AMC and +5.8pp on AIME, demonstrating that per-action process rewards provide effective supervision even when agents have limited observability. Figure 6 shows behavioral metrics for this run. Under partial information constraints, agents start with higher tool usage and shorter responses compared to full-context runs, then further optimize during training: tool calls increase while response lengths continue to decrease. Figure 6. Behavioral metrics for the partial information run (Qwen3-4B). From left to right, top to bottom: Code Executor tool calls, Problem Solver response length, Code Executor response length, Verifier response length, AMC accuracy, and AIME 2025 accuracy. Despite limited context, agents show similar behavioral adaptations as full-context runs. A.2. DSBench: Extended Quality Metrics Figure 7 presents the complete set of quality metrics tracked during DSBench training, showing both raw metrics (computed only over successful samples) and fair metrics (which penalize failures: 0.5 for Accuracy, 0 for F1, 50% for MAE/RMSE). The vertical green line marks step 44 (epoch 11), where overall success rate peaks. Key observations: (1) Classification metrics (Accuracy, F1, ROC-AUC) peak around step 44 then decline, while regression metrics (MAE, RMSE) continue improving through step 84. (2) Fair metrics show larger improvements than raw metrics because they capture both quality improvement and increased success rate. (3) The divergence between raw and fair metrics 11 MAPPA: Scaling Multiagent Systems with Process Rewards after step 44 reflects the models specialization toward regression tasksraw classification metrics remain stable (successful runs maintain quality) while fair metrics decline (fewer successful runs). Figure 7. DSBench quality metrics over 84 training steps. Top row: success rates by task type. Middle rows: classification metrics (Accuracy, F1, ROC-AUC) in raw and fair variants. Bottom row: regression metrics (MAE, RMSE) in raw and fair variants. Green vertical line and markers indicate step 44 (peak success rate). Fair metrics penalize failures (Accuracy: 0.5, F1: 0, MAE/RMSE: 50%), capturing both quality and reliability. 12 MAPPA: Scaling Multiagent Systems with Process Rewards B. MathChat Agent Prompts This section provides the complete prompts used by each agent in the MathChat pipeline. All agents receive the original math problem and role-specific instructions. B.1. Problem Solver Prompt You are Problem Solver in 3-agent system: Problem Solver (you) -> Code Executor -> Verifier. The system succeeds only if the Verifier (final agent) outputs the correct answer. Your job is to draft solution to the problem. You have strict 4k token limit (your thinking inside <think> and </think> tags also counts). Anything beyond that will be truncated. ## Problem {problem} B.2. Code Executor Prompt You are Code Executor in 3-agent system: Problem Solver -> Code Executor (you) -> Verifier. The system succeeds only if the Verifier (final agent) outputs the correct answer. Your job is to compute/verify the solution using Python code. You can execute Python code. Write code in python blocks and it will be automatically executed by the user on your behalf, based on which you can iterate further or output final answers. You have strict 4k token limit (your thinking inside <think> and </think> tags also counts). Anything beyond that will be truncated. ## Problem {problem} ## Input from Problem Solver {solution} B.3. Verifier Prompt You are Verifier, the final agent in 3-agent system: Problem Solver -> Code Executor -> Verifier (you). You are the last agent. The system succeeds only if YOU output the correct answer. Evaluate the information below and provide the final answer. You have strict 4k token limit (your thinking inside <think> and </think> tags also counts). Anything beyond that will be truncated. Output your final answer as: **boxed{answer}** ## Problem {problem} ## Input from Code Executor {execution} MAPPA: Scaling Multiagent Systems with Process Rewards B.4. Coach Evaluation Prompt The coach evaluates each agent action using the following prompt template: You are evaluating specific agent in multiagent system that collaboratively solves math competition problem. The system has 3 specialized agents working sequentially: 1. **Problem Solver**: Reasons through the problem step-by-step 2. **Code Executor**: Writes and executes Python code to verify/compute solutions 3. **Verifier**: Synthesizes the code executors output and provides the final answer. The success of the overall system is determined by the Verifiers final answer. **Workflow Constraints:** - Each agent has 4K token output limit. Outputs beyond this are truncated. - All agents can see the original problem statement. When evaluating, consider whether the agent: - (IMPORTANT) Performed well given their role and the information provided - Stayed within output length limit **Problem that the overall system is solving:** {problem} **Current agent being evaluated:** {agent_role} **What the agent received as input:** {agent_input} **What the agent outputted:** {agent_output} **Environment feedback for the agents tool call (if applicable):** {tool_observation} **Ground truth answer (if applicable):** {ground_truth_answer} Evaluate the agents decision quality based on: - Given the agents role and the information it had as input, how helpful is its output towards the overall systems success? - For Code Executor: Did it output code in the right format? Is the code syntactically correct and does it solve the problem? - For Verifier: Did it correctly synthesize the information and output the final answer? Strictly assigns score from 0 (terrible) to 10 (perfect). Additionally, if ground truth is provided (not \"N/A\") and the agent is \"Verifier\", you MUST also verify answer correctness: Provide binary correctness as: 14 MAPPA: Scaling Multiagent Systems with Process Rewards - ANSWER_CORRECT: 1 (if the Verifiers final answer matches the ground truth) - ANSWER_CORRECT: 0 (if the Verifiers final answer is incorrect) Structure your output in the following format exactly: PROCESS_SCORE: [0 to 10] ANSWER_CORRECT: [0 or 1, but only if ground truth provided and agent is Verifier] 15 MAPPA: Scaling Multiagent Systems with Process Rewards C. DSBench Agent Prompts This section provides the complete prompts used by each agent in the DSBench pipeline. Prompts include placeholders for task-specific information and file system state. C.1. Data Engineer Prompt You are Data Engineer in data science pipeline. **CRITICAL: Keep your <think> reasoning BRIEF (under 1000 words). Then IMMEDIATELY write executable code in python block. Do NOT describe code in thinking - just write it!** **Task Description:** {task_description} ## CRITICAL FILE SYSTEM OBSERVATION **Files currently available in your workspace:** {available_files_list} NOTE: The file names shown above are the ACTUAL files you can access. Always check this list! **Your Responsibilities:** 1. **Load Data**: Read training data from train.csv and test data from test.csv 2. **Exploratory Data Analysis (EDA)**: - Inspect schema (dtypes, shape, missing values) - Analyze distributions, correlations - Identify data quality issues 3. **Data Preprocessing**: - Handle missing values (imputation/removal) - Encode categorical variables - Scale numerical features - Engineer new features based on domain insights 4. **CRITICAL: Save ALL Required Artifacts**: - X_train.pkl - preprocessed training features - y_train.pkl - training labels - X_test.pkl - preprocessed TEST features (CRITICAL!) - scaler.pkl or similar - fitted transformers 5. **Output Summary**: Print what files you saved ## CRITICAL: You MUST save X_test.pkl! The Analyst agent downstream will load X_test.pkl to generate predictions. If you dont save X_test.pkl, the entire pipeline will FAIL. **Constraints:** - Apply SAME transformations to both train and test data - Do NOT use test data for fitting (no data leakage!) - Make preprocessing deterministic (set random_state) - ALWAYS save X_test.pkl - downstream Analyst needs it! C.2. Modeler Prompt You are Modeler in data science pipeline. **CRITICAL: Keep your <think> reasoning BRIEF (under 1000 words). Then IMMEDIATELY write executable code in python block.** **Task Description:** {task_description} 16 MAPPA: Scaling Multiagent Systems with Process Rewards ## CRITICAL FILE SYSTEM OBSERVATION **Files currently available in your workspace (saved by previous agents):** {available_files_list} NOTE: The file names shown above are the ACTUAL files you can load. You MUST use these exact file names! **Preprocessing Summary from Data Engineer:** {preprocessing_summary} **Your Responsibilities:** 1. **Check Available Files FIRST**: Verify which files exist before loading 2. **Load Data**: Load the preprocessed data saved by Data Engineer 3. **Algorithm Selection**: - Analyze the problem type (classification/regression) - Consider dataset characteristics - Select appropriate algorithms (RandomForest, XGBoost, LightGBM) 4. **Model Training**: - Train selected models with cross-validation - Tune hyperparameters - Evaluate performance 5. **Model Selection**: Choose best model based on validation metrics 6. **CRITICAL: Save Model**: Save the trained model as model.pkl - the Analyst needs this! 7. **Output**: Print files saved and performance metrics **Constraints:** - Check available files BEFORE trying to load them - Set random_state for reproducibility - Use cross-validation for model selection - Report validation metrics honestly - ALWAYS save your trained model to model.pkl C.3. Analyst Prompt You are an Analyst in data science pipeline. **CRITICAL: Keep your <think> reasoning BRIEF (under 1000 words). Then IMMEDIATELY write executable code.** **Task Description:** {task_description} ## CRITICAL FILE SYSTEM OBSERVATION **Files currently available in your workspace:** {available_files_list} NOTE: The file names shown above are the ACTUAL files you can load. You MUST use these exact file names! **Preprocessing Summary from Data Engineer:** {preprocessing_summary} **Model Summary from Modeler:** {model_summary} **Your Responsibilities:** 1. **CHECK AVAILABLE FILES FIRST**: Look at the file 17 MAPPA: Scaling Multiagent Systems with Process Rewards list above CAREFULLY! - You NEED model.pkl to make predictions - You NEED X_test.pkl with preprocessed test features - If these files are MISSING, report the issue! 2. **Load Saved Artifacts**: Load the model and preprocessed test data 3. **Generate Predictions**: Make predictions on test set 4. **Get IDs from test.csv**: Load original test.csv to get the ID column 5. **Create Submission**: - Save predictions to submission.csv - MUST include correct ID column and prediction column - Follow competition format 6. **Quality Checks**: - Validate no missing predictions - Check value ranges - Verify column names and order ## CRITICAL: Check file availability before loading! If X_test.pkl or model.pkl is NOT in the available files list, you CANNOT create submission.csv. **Constraints:** - CHECK THE FILE LIST ABOVE before trying to load - If required files are missing, report which ones - Load the model from model.pkl - Load preprocessed test data from X_test.pkl - Get IDs from original test.csv - Ensure predictions match test set size - Save predictions to submission.csv C.4. Coach Evaluation Prompt The DSBench coach uses detailed prompt that includes root-cause analysis for pipeline failures. Key excerpts: You are evaluating specific agent in multiagent data science system. The system has 3 specialized agents: 1. **Data Engineer**: Performs EDA, data cleaning, feature engineering - MUST save: X_train.pkl, y_train.pkl, X_test.pkl 2. **Modeler**: Selects algorithms, trains models - MUST save: model.pkl 3. **Analyst**: Generates predictions, creates submission - MUST save: submission.csv (FINAL DELIVERABLE) ## CRITICAL: OUTCOME-BASED ROOT CAUSE ANALYSIS If ground_truth_answer shows \"ERROR: No predictions found\" or similar failure, this means submission.csv was NOT created. This is PIPELINE FAILURE. You MUST perform root cause analysis: 1. **Check \"FILES SAVED BY PREVIOUS AGENTS\"**: - Did Data Engineer save X_test.pkl? - Did Modeler save model.pkl? - If these files are missing, the UPSTREAM AGENT is at fault! 2. **Check \"tool_observation\" for errors**: - FileNotFoundError for X_test.pkl -> Data Engineers fault - FileNotFoundError for model.pkl -> Modelers fault 18 MAPPA: Scaling Multiagent Systems with Process Rewards - KeyError, TypeError -> Likely Analysts own bug 3. **Assign blame to the responsible agent**: - Data Engineers fault if: X_test.pkl is NOT in \"FILES SAVED BY PREVIOUS AGENTS\" - Modelers fault if: model.pkl is NOT in \"FILES SAVED BY PREVIOUS AGENTS\" - Analysts fault if: Both files were available but Analysts code is buggy ## Scoring Guidelines **Score 10**: EXCEPTIONAL - Agent exceeded expectations **Score 8-9**: Code executed successfully, all required files saved, methodology is sound **Score 6-7**: Code executed, most files saved, minor issues **Score 4-5**: Agent tried but code had errors **Score 2-3**: Code crashed due to upstream failures **Score 0-1**: Agent produced no useful output ## Using Ground Truth Metrics (Analyst Only) If ground truth metrics are provided, apply HARD CAPS: **Classification Tasks:** ROC-AUC >= 0.85 Max 9 ROC-AUC 0.75-0.85 Max 8 ROC-AUC 0.65-0.75 Max 6 ROC-AUC 0.55-0.65 Max 4 ROC-AUC < 0.55 Max 3 **Regression Tasks (RMSE as % of range):** RMSE < 10% Max 9 RMSE 10-25% Max 8 RMSE 25-50% Max 6 RMSE > 50% Max 4 Output format: PROCESS_SCORE: [0 to 10] 19 MAPPA: Scaling Multiagent Systems with Process Rewards D. Example Coach Evaluations This section presents real coach evaluation responses from training runs, illustrating how the coach provides dense, context-aware feedback that goes beyond simple correctness checking. D.1. MathChat: Problem Solver Evaluation The following coach response evaluates Problem Solver agent attempting an AMC geometry problem about counting rectangles inscribed in regular dodecagon. This example demonstrates how the coach rewards metacognitive monitoring the agents ability to test and revise its own hypotheses. PROCESS_SCORE: 0.8 **Evaluation:** 1. **Reasonableness of the Action:** The agents action, which is to provide step-by-step reasoning process, is perfectly aligned with its role as Problem Solver. The reasoning itself is logical and demonstrates good attempt to solve the problem. 2. **Quality of Reasoning:** * **Initial Insight:** The agent has crucial insight early on: rectangle inscribed in circle (which the dodecagon is) must have its diagonals as diameters of the circle. This is the correct path to the solution. * **Flawed Premise:** The agent then makes common mistake, assuming that for the quadrilateral to be rectangle, the two diameters must be perpendicular. This is incorrect; the quadrilateral formed by the endpoints of *any* two distinct diameters is always rectangle. * **Excellent Self-Correction:** The agent then intelligently uses the example provided in the asy code to test its own hypothesis. It correctly identifies the vertices of the example rectangle and determines that its diagonals are not perpendicular. This leads it to correctly conclude that its premise was wrong. This act of testing hypothesis against evidence and revising its thinking is sign of very strong reasoning process. * **Getting Stuck:** After correctly abandoning the \"perpendicular diameters\" idea, the agent unfortunately also abandons the \"diameters\" idea altogether. It switches to much more complex method of analyzing perpendicular sides and gets stuck. It failed to realize that its original insight was correct and only the \"perpendicular\" constraint needed to be removed. **Conclusion:** The agent demonstrated strong, logical thought process. It identified the most promising approach, made mistake, but then impressively corrected that mistake based on the provided evidence. Its main failure was not revisiting the original, correct approach after removing the flawed constraint. Despite not reaching the final answer, the quality of the step-by-step reasoning and self-correction is high. This example illustrates how the coach rewards metacognitive monitoring: when the agent tests its hypothesis against 20 MAPPA: Scaling Multiagent Systems with Process Rewards evidence and revises its thinking, this is explicitly praised as sign of very strong reasoning process. The coach also distinguishes between the quality of the initial insight (correct), the intermediate error (flawed premise), the recovery attempt (excellent self-correction), and the final outcome (got stuck). This granular feedback enables agents to learn which reasoning behaviors to reinforce. D.2. DSBench: Data Engineer Evaluation The following coach response evaluates Data Engineer agent preprocessing healthcare dataset for stroke prediction. This example demonstrates how the coach identifies subtle implementation bugs that would silently corrupt the pipeline. PROCESS_SCORE: 0.6 **Evaluation of agent_data_engineer:** **1. Reasoning and Plan:** The agents thought process is logical and structured. It correctly identifies the key responsibilities: loading data, performing EDA, handling missing data, encoding categoricals, scaling numericals, and engineering new features. The agent correctly reasons about the need for deterministic preprocessing and considers the appropriate order of operations. **2. Code Implementation and Preprocessing:** * **Strengths:** - The use of sklearn.pipeline.Pipeline and sklearn.compose.ColumnTransformer is best practice. It encapsulates preprocessing steps cleanly and helps prevent data leakage. - The choice of preprocessing steps is appropriate: mean imputation for numerical, constant fill for categorical, StandardScaler for numerical, and OneHotEncoder for categorical. - The agent correctly includes handle_unknown=ignore in the OneHotEncoder, which is crucial for handling potential new categories in the test set. - The agent correctly sets random_state=42 to ensure the process is deterministic. - The feature engineering ideas (age_group, bmi_over_age) are plausible and relevant for stroke prediction task. * **Weaknesses / Bugs:** - **Critical Flaw:** The agent defines numerical_cols and categorical_cols *before* creating the new features (age_group, bmi_over_age). Consequently, the ColumnTransformer is configured to only process the *original* columns. The newly created features will be dropped by the transformer, completely nullifying the feature engineering step. The column lists should have been defined *after* the new columns were added to the DataFrame. - **Redundancy:** The agent performs imputation twice. First, it imputes directly on the DataFrame using .fillna(). Then, it includes SimpleImputer again within the pipelines of the ColumnTransformer. While not fatal error, it is inefficient and indicates slight misunderstanding of how to use the pipelines effectively. **3. Summary Output:** The agent correctly structures the final output according 21 MAPPA: Scaling Multiagent Systems with Process Rewards to the specified PREPROCESSING_SUMMARY format. **Conclusion:** The agent demonstrates strong conceptual understanding of the data engineering role and utilizes modern, robust tools (sklearn pipelines). However, the implementation contains critical logical error in the order of operations that prevents the engineered features from being correctly processed. This is significant bug that would negatively impact the final model. The plan was excellent, but the execution was flawed. This example demonstrates the coachs ability to identify subtle implementation bugs that would silently corrupt the ML pipeline. The Critical Flawdefining column lists before creating new features, causing engineered features to be silently droppedis exactly the kind of bug that passes syntax checks but produces incorrect results. The coach also evaluates domainspecific best practices: using ColumnTransformer for clean preprocessing, setting handle unknown=ignore for robust handling of unseen categories, and ensuring deterministic behavior with random state. The conclusion the plan was excellent, but the execution was flawed provides actionable feedback that helps the agent learn to verify implementation details match high-level intentions. D.3. MathChat: Strategy vs. Implementation The following example evaluates Problem Solver on grid coloring combinatorics problem, demonstrating how the coach distinguishes between excellent high-level strategy and flawed low-level modeling. PROCESS_SCORE: 0.6 This is very strong attempt at complex combinatorial problem. The agents overall strategy is excellent, but there is critical flaw in its understanding of the grids structure that prevents it from reaching the correct conclusion. **Evaluation of the Agents Reasoning:** 1. **Problem Decomposition (Excellent):** The agent correctly identifies the 12 line segments and astutely categorizes them into 8 \"outer\" segments (on the perimeter) and 4 \"internal\" segments (shared between squares). This is key insight that simplifies the problem. 2. **Strategic Approach (Excellent):** The agent devises powerful strategy: - First, consider all possible colorings of the 4 internal segments. - Second, for each internal coloring, calculate the number of ways the 8 outer segments can be colored to satisfy the constraints for each of the four squares. - Finally, sum the results over all possible internal colorings. This is systematic and valid way to solve the problem. 3. **Modeling the Grid Structure (Flawed):** This is where the agent makes critical error. It incorrectly assumes the grid can be split into two independent columns. - **Agents flawed model:** It states that the top-left and bottom-left squares share two internal edges, and the top-right and bottom-right squares share the other two internal edges. This would mean 22 MAPPA: Scaling Multiagent Systems with Process Rewards the left column is completely independent of the right column. - **Correct model:** The internal edges connect all four squares. For example, the top-middle vertical segment is shared by the top-left and top-right squares, linking the two columns. 4. **Execution of the Flawed Model (Good):** Given its incorrect assumption, the agents subsequent reasoning is logical and well-executed. It correctly sets up the calculation based on its flawed model. If the initial model had been correct, this method would have yielded the right answer for that model. **Conclusion:** The agent demonstrated sophisticated approach to problem-solving. The strategy of breaking down the problem based on the more constrained \"internal\" edges is exactly the right way to tackle this. However, the fundamental misunderstanding of the grids connectivity is major flaw that makes the final answer incorrect. The high-level strategy is excellent, but the low-level implementation is based on false premise. This example shows the coachs nuanced evaluation: Problem Decomposition (Excellent), Strategic Approach (Excellent), but Modeling the Grid Structure (Flawed). The coach explicitly notes that the agents execution of its flawed model was internally consistentif the initial model had been correct, this method would have yielded the right answer. This distinction between strategic quality and modeling accuracy provides precise feedback for improvement. D.4. DSBench: Analyst Pipeline Failure The following example evaluates an Analyst agent that fundamentally misunderstands how to apply trained ML pipeline to test dataa critical conceptual error in production ML systems. PROCESS_SCORE: 0.1 **Critique of the Agents Action:** 1. **Reproducing the Pipeline (Major Failure)**: The agents primary responsibility is to apply the *exact* same preprocessing and use the *exact* same trained model from the previous steps. The agent fails critically on this point: - **LabelEncoder Error**: The code le = LabelEncoder() and le.fit_transform(test_df[...]) creates *new* encoder and fits it *only on the test data*. This is incorrect. The encoder must be the one that was fitted on the training data to ensure consistent mapping of categories to numbers. - **TfidfVectorizer Error**: The code initializes new, unfitted vectorizer. Calling .transform() without first calling .fit() on the training data will cause NotFittedError. The agent should have used the vectorizer object that was already fitted by the Data Engineer/Modeler. - **Model Retraining Error**: The code re-initializes new RandomForestClassifier and then attempts to retrain it with model.fit(X_train, y_train). This is severe misunderstanding of the agents role. The Analyst should use the *already trained* model from the Modeler. Furthermore, the variables X_train and y_train are not defined within the 23 MAPPA: Scaling Multiagent Systems with Process Rewards Analysts script, meaning the code would crash with NameError. 2. **Generating Predictions (Failure)**: Due to the errors mentioned above, the provided script is not executable. It would fail at the tfidf.transform step, and even if it didnt, it would fail at the model.fit step. Therefore, it cannot generate any predictions. 3. **Creating Submission (Partial Success)**: The logic for creating the final submission DataFrame and printing it in the required format is syntactically correct. However, this is useless since the predictions themselves cannot be generated. **Conclusion:** The agent demonstrated fundamental misunderstanding of how to apply trained machine learning pipeline to new, unseen data. The concepts of using pre-fitted transformers (like LabelEncoder and TfidfVectorizer) and pre-trained model were completely missed. The resulting code is non-functional and conceptually wrong. This example demonstrates the coach identifying fundamental conceptual errors about ML deployment. The agents mistakerefitting encoders on test data and retraining the modelwould cause category mapping inconsistencies and invalidate the entire pipeline. This is precisely the kind of error that distinguishes production-ready ML practitioners from novices. The coachs detailed breakdown (LabelEncoder error, TfidfVectorizer error, Model Retraining error) provides specific, actionable feedback about each conceptual gap. MAPPA: Scaling Multiagent Systems with Process Rewards E. Training Algorithm Details This appendix provides additional technical details for the training algorithm described in Section 2.3. E.1. Why REINFORCE++ Over GRPO Group-Relative Policy Optimization (GRPO) normalizes advantages within groups of samples sharing the same prompt, which assumes identical input states across the group. This assumption is valid for single-agent settings where all samples from the same prompt begin in the same state. However, in end-to-end multiagent training, each agents input context depends on the stochastic output of upstream agents. Consider two rollouts of the same math problem: the Problem Solver may generate different reasoning chains, causing the Code Executor to observe different inputs despite originating from the same initial prompt. This state divergence makes within-group comparisons ill-defined for GRPO. An alternative approachtraining each agent separately with frozen upstream contextwould restore the same-state assumption. However, this requires separate rollout passes for each agent, significantly reducing efficiency and preventing agents from co-adapting during training. Instead, we train all agents end-to-end from complete trajectories using REINFORCE++, which applies global batch normalization across all collected experiences rather than per-prompt normalization. E.2. Advantage Estimation The advantage estimation proceeds in two steps. First, for each action at with coach reward rcoach KL-penalized reward: [0, 1], we compute the rt = rcoach β DKL(πθπref) (2) where β = 0.01 is the KL penalty coefficient and DKL measures divergence from the frozen reference policy. The advantage for each action is then computed as undiscounted return-to-go: At = (cid:88) τ rτ (3) which propagates downstream rewards back to earlier actions within each agents trajectory. This choice of γ = 1 (no discounting) reflects the finite-horizon nature of multiagent workflows where all actions contribute equally to the final outcome. Second, we collect advantages At from all agents and all experiences in the current batch, then compute global statistics: µ = (cid:80) m(i) i,t A(i) (cid:80) i,t m(i) , σ2 = (cid:80) µ)2 m(i) i,t(A(i) (cid:80) i,t m(i) (4) σ2 + ϵ with where m(i) ϵ = 108 for numerical stability. are action masks indicating valid actions. Advantages are then normalized: ˆAt = (At µ)/ E.3. Policy Gradient Objective The policy gradient objective follows PPO with clipped surrogate: (cid:104) Lpolicy = min (cid:16) ρt(θ) ˆAt, clip(ρt(θ), 1ϵ, 1+ϵ) ˆAt (cid:17)(cid:105) (5) where ρt(θ) = πθ(atst)/πold(atst) is the probability ratio between the current and old policy, and ϵ = 0.2 is the clipping range. The clipping prevents excessively large policy updates that could destabilize training, which is particularly important when training multiple agents simultaneously where instability in one agent can cascade through the pipeline. E.4. Strategic Multi-Objective Judging Our current coach evaluates each action independently without memory of previous training epochs or access to aggregate performance statistics. Each action is scored based solely on its intrinsic quality, without considering whether the training process would benefit more from improving reliability versus refinement at that stage. strategic coach with memory could balance objectives dynamically. For example, such coach could: 25 MAPPA: Scaling Multiagent Systems with Process Rewards Assign higher rewards to successful task completion early in training when the pipeline is unreliable Shift focus to quality improvements once success rate stabilizes Restore emphasis on reliability if it degrades Such coach could access metrics like rolling success rate, quality trends, and per-agent failure modes to make context-aware reward decisions across the training trajectory. Implementing strategic coaching in our framework would be straightforward: the coach prompt already accepts arbitrary metadata fields that could include per-epoch statistics. We leave implementation and empirical evaluation of strategic multi-objective judging to future work. 26 MAPPA: Scaling Multiagent Systems with Process Rewards F. Reward Backpropagation This appendix formalizes Reward Backpropagation, an approach for outcome-aware process reward decomposition referenced in Section 4.4. F.1. Motivation Process rewards without outcome information are inherently inefficient: without knowing what actually mattered, coach must critique everything that could be improvedsome feedback may be valid, some may be mere stylistic preference. This bottom-up approach asks each agent to improve my part to my best effort without knowing which improvements actually matter for the final result. Outcome-aware evaluation inverts this. Given specific outcome (e.g., incorrect predictions, or surprisingly good results despite upstream errors), it becomes much easier to trace backward and identify which agent made which mistake, or whose action saved the team. This top-down approach asks: given this outcome, who is the real contributor to success or failure? The linear backpropagation structure provides an ordered flow of attributionnot chaotic discussionwhere each step receives credit or blame only for the residual not yet explained by downstream actions. Reward Backpropagation formalizes this shift from local, bottom-up optimization to global-aware, top-down attribution. F.2. Method Overview Given trajectory τ = (s1, s2, . . . , sT ) and final outcome RT Rk (potentially vector-valued), Reward Backpropagation proceeds in two phases: Forward Pass (Process Rewards). For each step t, query coach to produce process reward pt evaluating local decision quality without knowledge of the final outcome. Backward Pass (Outcome Decomposition). Starting from RT , run backward chain of coach calls: 1. At step , the coach attributes part of RT to step , producing contribution and passing residual rT 1 backward 2. At step < , the coach receives residual rt, attributes part to step (producing t), and passes rt1 = rt at backward 3. This continues to step 1, yielding per-step outcome-aware contributions {t} Implementation Note. The notation above is conceptual rather than literal. In practice, the residual rt and subtraction rt1 = rt at are not scalar or vector arithmetic but rather natural language attribution. Each backward step asks the coach to explain, in natural language, how the output of agent 1 (which forms part of the input to agent t) contributed to the outcome being evaluated. For example, the coach might produce feedback such as: Agent 1s output contained attribute X, which caused issue at agent t. consider to be problematic decision, worth 3 out of 10. The subtraction thus represents conceptual decompositionthe coach articulates what portion of the outcome can be attributed to each step, with the residual being the remaining unexplained outcome passed backward for further attribution. This natural language grounding makes the process interpretable and allows rich, multi-dimensional feedback beyond what scalar rewards can capture. F.3. Combining Process and Outcome Signals The final step-level reward combines both signals: ft = α pt + β (6) where α, β 0 are hyperparameters. Process reward pt captures local quality; outcome contribution captures how the final result validates or refutes that quality. Steps whose local quality did not affect the outcome receive small t. 27 MAPPA: Scaling Multiagent Systems with Process Rewards F.4. Consistency Constraints We encourage approximate decomposition consistency: (cid:88) t=1 v(RT ) (7) where : Rk scalarizes the outcome vector. This can be enforced via prompting or post-hoc normalization. F.5. Benefits Reward Backpropagation provides: (1) causally plausible credit assignmentlocally good actions in failed trajectories receive negative t; (2) interpretable narratives about which steps mattered; (3) robustness to process-reward hacking since anchors rewards to actual outcomes. The approach is compatible with REINFORCE++ and can be distilled into lightweight Process Reward Model for efficiency. 28 MAPPA: Scaling Multiagent Systems with Process Rewards G. Distributed Training Implementation This appendix provides detailed implementation specifications for the distributed training architecture described in Section 2.4. G.1. Parallel Agent Initialization Each agent is initialized via ThreadPoolExecutor to parallelize model loading across agents: with ThreadPoolExecutor(max_workers=len(agent_configs)) as executor: futures = [executor.submit(self._init_agent, agent_id, config, args) for agent_id, config in agent_configs] agents = [f.result() for in futures] Each agent instantiates independent Ray actor groups for: vLLM Engines: num engines tensor parallel size Ray actors for autoregressive generation with PagedAttention and KV-cache optimization Actor Policy: PPORayActorGroup with actor num nodes actor num gpus per node workers for trainable policy parameters Reference Model: Same structure as actor, frozen copy for KL divergence computation Critic Model: Value function estimation distributed across ranks (optional, not used with REINFORCE++) When colocate all models=True, placement groups bundle multiple models on shared GPUs with fractional allocation (num gpus per actor=0.2), reducing peak memory by enabling actor, reference, and vLLM models to share GPU memory. G.2. Distributed Rollout Generation Prompts are sharded evenly across world size workers: chunk_size = (num_prompts + world_size - 1) // world_size for rank in range(world_size): start_idx, end_idx = rank * chunk_size, min((rank+1) * chunk_size, num_prompts) chunked[rank] = prompts[start_idx:end_idx] Each rank receives its shard and executes workflows via Ray remote calls: all_refs = [] for rank in range(world_size): ref = generate_samples_remote.remote(world, chunked[rank], rank, world_size) all_refs.append(ref) results = ray.get(all_refs) # Barrier: wait for all ranks Within each workflow, LLM engines are assigned cyclically: mod num engines; otherwise engines are striped across ranks. Requests are batched across available engines: if fewer engines than ranks, engine handles rank batch_size = (len(prompts) + len(llms) - 1) // len(llms) for i, llm in enumerate(llms): batch = prompts[i * batch_size : (i + 1) * batch_size] refs.append(llm.generate.remote(batch, sampling_params)) G.3. Experience Preparation and Agent Routing Completed trajectories are processed by ExperienceMaker, which computes per-action rewards, KL divergence, and advantages. For multiagent training, each action is routed to its originating agent: sharded_data = [[None] * world_size for _ in range(num_agents)] for rank in range(world_size): for agent_id in range(num_agents): agent_data = all_results[rank][agent_id] sharded_data[agent_id][rank] = ray.put(agent_data) # Extract agents batch 29 MAPPA: Scaling Multiagent Systems with Process Rewards This enables each agent to train on its own trajectory segments while maintaining batch synchronization across the distributed system. Variable Turn Count Handling. When agents have variable turn counts (e.g., Code Executor in MathChat with up to 5 turns), different workers may produce different numbers of samples, causing NCCL deadlocks during gradient synchronization. The filter agents data option truncates all workers to the minimum sample count: # Count actual samples per rank for each agent actual_samples_per_rank = [[count_samples(rank, agent) for agent in range(num_agents)] for rank in range(world_size)] # Truncate to minimum across ranks to ensure equal gradient syncs min_samples = [min(rank[agent] for rank in actual_samples_per_rank) for agent in range(num_agents)] This is critical for MathChat where coder max turns=5 causes variable action counts across workers. Global Batch Normalization (REINFORCE++). Advantages are normalized across the entire batch, not per-prompt: all_advantages = torch.cat([exp.advantages for exp in experiences]) mean = (all_advantages * action_masks).sum() / num_actions var = ((all_advantages - mean).pow(2) * action_masks).sum() / num_actions rstd = var.clamp(min=1e-8).rsqrt() normalized = (all_advantages - mean) * rstd G.4. Weight Synchronization After each training step, updated weights must be broadcast from DeepSpeed actors to vLLM engines. The synchronization mechanism depends on colocate all models: Standard NCCL Broadcast. When models are on separate GPUs, dedicated NCCL process group connects DeepSpeed rank 0 with all vLLM engine ranks: # Process group: [DeepSpeed rank 0, vLLM engine 0, engine 1, ...] world_size = vllm_num_engines * vllm_tensor_parallel_size + 1 # Broadcast each parameter (ZeRO-3 requires AllGather first) for name, param in model.named_parameters(): with deepspeed.zero.GatheredParameters([param], enabled=zero_stage==3): torch.distributed.broadcast(param.data, src=0, group=model_update_group) CUDA IPC (Co-located Models). When colocate all models=True (default for both MathChat and DSBench), the system automatically selects CUDA IPC for zero-copy parameter sharing, which is faster than NCCL for models on the same GPU: # Automatically enabled when colocate_all_models=True and backend=\"nccl\" weight = param.data.clone() ipc_handle = reduce_tensor(weight) ipc_handle = {get_physical_gpu_id(): ipc_handle} torch.distributed.all_gather_object(ipc_handle_list, ipc_handle) # vLLM engines reconstruct tensor from IPC handle without data copy # Get CUDA IPC handle G.5. Memory Optimization Prefix Caching. vLLM reuses KV cache across requests with common prefixes (e.g., system prompts shared across agents). Cache is reset after weight updates: engine.reset prefix cache.remote(). Sleep/Wake Mode. vLLM engines can offload to CPU during training to free GPU memory: 30 MAPPA: Scaling Multiagent Systems with Process Rewards def sleep(self, level=1): self.llm.sleep(level=level) # Offload to CPU def wake_up(self): self.llm.wake_up() # Reload to GPU DeepSpeed ZeRO-3. Optimizer states, gradients, and parameters are partitioned across GPUs. Parameters are gathered on-demand during forward/backward passes, enabling training of models larger than single-GPU memory. G.6. Synchronization Points Table 5 summarizes the key synchronization barriers in the training loop. Table 5. Communication patterns and synchronization points in distributed training. Component Protocol Frequency Ray put/get Ray remote/get Prompt sharding Rollout collection Reference model inference Ray remote Gradient reduction Weight broadcast Metric aggregation NCCL all reduce NCCL broadcast NCCL all reduce Per iteration Per iteration Per trajectory Per training step Per training step Per training step G.7. Configuration Parameters Table 6 lists key hyperparameters controlling distributed training behavior. Table 6. Key configuration parameters for distributed training. Parameter Default Description actor num nodes actor num gpus per node vllm num engines vllm tensor parallel size vllm gpu memory utilization colocate all models zero stage vllm sync backend 1 2 2 1 0.7 True 3 nccl Nodes for policy training GPUs per node for policy vLLM inference engines Tensor parallelism per engine GPU memory for KV cache Share GPUs across model types DeepSpeed ZeRO level Weight sync backend G.8. Task-Specific Configuration Table 7 compares the configurations used for MathChat and DSBench experiments. Table 7. Configuration differences between MathChat and DSBench. Parameter MathChat DSBench rollout batch size samples per prompt num episodes generate max len prompt max len coach model coder max turns filter agents data vllm gpu memory utilization 32 2 8 4096 24576 gemini-2.5-flash 5 True 0.7 16 2 30 16384 24576 gemini-3-pro-preview N/A True 0.6 Key differences: DSBench uses longer generation limits (16K vs 4K) to accommodate multi-step data science pipelines, trains for more episodes (30 vs 8) due to smaller dataset size, and uses more capable coach model (Gemini 3 Pro) for complex ML evaluation. Both use filter agents data=True to handle variable turn counts across workers. 31 MAPPA: Scaling Multiagent Systems with Process Rewards G.9. Computational Cost Hardware. All experiments use single node with 8 NVIDIA H100 GPUs. Training Time. Wall-clock training time is approximately 812 hours for 106 training steps on MathChat, with coach API calls being the primary bottleneck (each coach evaluation adds 25 seconds latency). Each rollout generates 39 coach calls (3 agents 13 turns per agent), totaling 3,00010,000 coach calls per training run. DSBench training (30 episodes 128 rollouts) incurs similar per-episode costs but with longer per-task execution times due to multi-turn tool use. API Cost. Using Gemini 2.5 Flash at approximately $0.075 per 1M input tokens and $0.30 per 1M output tokens, the estimated API cost is $50150 per training run."
        }
    ],
    "affiliations": [
        "Yale University"
    ]
}