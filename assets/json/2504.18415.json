{
    "paper_title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs",
    "authors": [
        "Hongyu Wang",
        "Shuming Ma",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 5 1 4 8 1 . 4 0 5 2 : r BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs Hongyu Wang Shuming Ma Furu Wei https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference. Theres Plenty of Room at the Bottom. Richard Feynman (a) Wo of BitNet b1.58 (b) Wdown of BitNet b1.58 (c) Wo of BitNet v2 (d) Wdown of BitNet Figure 1: Top: Overview of BitNet v2 and H-BitLinear. Bottom: The distribution of the activation of output projection Wo in attention and down projection Wdown in FFN. BitNet v2 utilizes H-BitLinear to eliminate the large amount of outlier channels in the intermediate states. The Hadamard transformation reshapes the original sharp distribution into more Gaussian-like form. Equal contribution. Corresponding author. S. Ma and F. Wei are with Microsoft Research. H. Wang is with University of Chinese Academy of Sciences."
        },
        {
            "title": "Introduction",
            "content": "The field of deep learning is rapidly embracing quantization-aware training and low-bit inference, driven by hardware advancements like next-generation GPUs (e.g., GB200) offering native support for 4-bit computations. This promises significant efficiency gains for large-scale models. Pioneering work like BitNet b1.58 [MWM+24] demonstrated that 1.58-bit LLMs can match full-precision performance while drastically reducing inference costs (latency, memory, throughput, energy) [WZS+25]. However, while BitNet b1.58 quantizes weights to 1.58 bits, alleviating memory bandwidth bottlenecks, it retains 8-bit activations. This reliance on 8-bit precision prevents these models from fully leveraging the 4-bit computational capabilities of emerging hardware, shifting the efficiency bottleneck towards computation itself. Achieving lower bit-width activations is crucial for maximizing hardware utilization, particularly for efficient kernel design in batched inference scenarios. Research [WMW24, LPC+24] highlights key challenge: the non-uniform distribution of activations within LLMs. While inputs to attention and feed-forward network (FFN) layers often exhibit Gaussian-like distributions amenable to quantization, their intermediate states (outputs before final projection) contain significant outliers, hindering aggressive low-bit quantization. BitNet a4.8 [WMW24] attempted to address this by selectively using 4-bit quantization for inputs and 8-bit sparsification for intermediate states. While achieving minimal performance loss compared to 8-bit activations, sparsification is less suited for maximizing throughput in batched inference, where dense computations are often preferred for hardware efficiency. To bridge this gap and unlock the full potential of 4-bit computation for 1.58-bit LLMs, we introduce BitNet v2. Our framework enables native 4-bit activations across the model. The core innovation is H-BitLinear, novel linear layer replacing the standard output projections in attention and down projections in FFNs. H-BitLinear applies an online Hadamard transformation before activation quantization. This strategically reshapes the sharp, outlier-prone distributions of intermediate states into more manageable, Gaussian-like forms, significantly reducing the impact of outliers in 1.58-bit models. We train BitNet v2 from scratch using 8-bit activations, achieving negligible performance loss compared to BitNet b1.58 [MWM+24]. Subsequently, the model can be efficiently fine-tuned with small amount of data to operate with native 4-bit activations. Extensive experiments demonstrate that our 4-bit BitNet v2 variant achieves performance comparable to BitNet a4.8 while offering superior computational efficiency for batched inference scenarios."
        },
        {
            "title": "2 BitNet v2: Native 4-bit Activations",
            "content": "We illustrate the architecture of BitNet v2 in Figure 1. We implement BitNet v2 using LLaMAlike components, including RMS normalization [ZS19], SwishGLU [Sha20] and removing all bias. Compared to BitNet [WMD+23], we use H-BitLinear for Wo in attention and Wdown in FFN layers to deal with outlier channels of intermediate states. BitNet v2 is trained with 1.58-bit weights and INT8 activations from scratch, then continue-trained with INT4 activations for all linear layers except input/output embedding. 2.1 H-BitLinear Following [WMD+23, MWM+24], as for weight quantization, we use per-tensor absmean function to quantize the weights into ternary values, i.e., {-1, 0, 1}: Qw(W) = αRoundClip( α + ϵ , 1, 1), α = mean(W) RoundClip(X, a, b) = min(max(round(X), a), b) (1) (2) For low-bit activations, previous works [WMW24, LPC+24] have shown that the distributions of the inputs to the attention and feed-forward-network layers (i.e., the activations of Wqkv and Wup,gate) tend to exhibit Gaussian-like shape, while the intermediates states (i.e., the activations of Wo and Wdown) have more outlier channels and massive amount of entries around zero. 2 (a) Wqkv of BitNet b1. (b) Wo of BitNet b1.58 (c) Wup,gate of BitNet b1.58 (d) Wdown of BitNet b1.58 (e) Wqkv of BitNet v2 (f) Wo of BitNet v2 (g) Wup,gate of BitNet v2 (h) Wdown of BitNet Figure 2: The activation distribution of BitNet b1.58 and BitNet v2 with 8-bit activations. Therefore, we introduce H-BitLinear for Wo in attention and Wdown in FFN layers. H-BitLinear employs Hadamard transformation before activation quantization to first reduce the number of outlier channels. The Hadamard transformation satisfies that: Hadamard(X) = HmX Hm = 1 (cid:18) Hm1 Hm1 Hm1 Hm1 (cid:19) , H0 = (1) (3) (4) where Hm is 2m 2m matrix and Rn, = 2m. We use fast-hadamard-transform2 to perform the matrix multiplication, which has O(n log n) computational complexity. As shown in Figure 2 and Figure 3, with Hadamard transformation, the distribution of the intermediate states becomes closer to Gaussian-like distribution, which significantly reduces the number of outliers and make it more suitable for INT4 quantization. For 8-bit and 4-bit activations, we adopt per-token absmax and absmean function, respectively. The activation quantization can be formulated as: QINT8(X) = γ 127 RoundClip( QINT4(X) = β 7 RoundClip( 127 γ + ϵ 7 β + ϵ X, 128, 127), γ = max(X) X, 8, 7), β = mean(X) Above all, the matrix multiplication of H-BitLinear can be written as: = Qw(W) QINT8/4(Xr), Xr = Hadamard(LN(X)) where LN denotes the layer normalization. 2.2 Training (5) (6) (7) Following [WMD+23], we employ the straight-through estimator (STE) [BLC13] for gradient approximation and use mixed-precision training to update the parameters. During backward propagation, 2https://github.com/Dao-AILab/fast-hadamard-transform 3 (a) Wdown of BitNet b1.58 (b) Wdown of BitNet v2 (c) Wo of BitNet b1. (d) Wo of BitNet v2 Figure 3: The activation distribution of Wdown in FFN and Wo in attention of BitNet b1.58 and BitNet v2 with 8-bit activations. Models Size PPL ARCc ARCe HS PQ WGe LBA Avg BitNet b1.58 BitNet a4.8 BitNet v2 (a8) BitNet v2 (a4) BitNet b1.58 BitNet a4.8 BitNet v2 (a8) BitNet v2 (a4) BitNet b1.58 BitNet a4.8 BitNet v2 (a8) BitNet v2 (a4) BitNet b1.58 BitNet a4.8 BitNet v2 (a8) BitNet v2 (a4) 400M 1.3B 3B 7B 13.37 13.61 13.50 13.78 11.02 11.15 11.14 11.33 9.71 9.80 9.72 9.85 9.09 9.16 9.14 9. 24.32 24.15 23.29 23.29 27.90 27.47 27.90 27.56 28.84 29.01 30.55 28.92 31.74 31.91 32.94 32.42 43.01 41.75 43.06 41.46 49.58 49.20 49.96 49. 54.80 55.01 55.56 55.01 59.51 59.09 58.54 58.00 39.51 64.91 39.48 65.18 39.06 64.74 38.33 65.45 48.85 69.80 48.72 69.64 48.37 69.42 48.00 68.23 56.39 71.44 55.92 71.76 57.19 71.33 56.59 71.65 61.49 74.37 61.06 74.16 61.08 74.10 60.71 74. 51.93 53.59 50.59 50.59 55.80 56.51 57.22 55.49 59.35 59.59 58.72 59.67 59.98 59.67 61.48 60.85 45.51 44.34 45.26 44.56 54.12 53.85 54.14 53. 60.47 59.85 60.90 60.74 61.63 61.54 64.22 63.52 44.87 44.75 44.33 43.95 51.01 50.90 51.17 50.41 55.22 55.19 55.71 55.43 58.12 57.91 58.73 58. Table 1: Perplexity and results of BitNet v2, BitNet a4.8 and BitNet b1.58 on the end tasks. we bypass the non-differentiable functions in quantization. To support mixed-precision training, we maintain full-precision latent weight to accumulate parameter updates. For the backward pass through the Hadamard transformation, we apply the transformation to the gradients as well, leveraging the orthogonality of the transformation matrix Hm. Specifically, the backward propagation is formulated as: X = Hadamard( ) Hadamard(X) (8) Similar to BitNet a4.8, BitNet v2 with 4-bit activations can be continue-trained from its 8-bit activation counterpart using small number of training tokens, while incurring negligible performance loss. The optimizer states are reused for continue-training."
        },
        {
            "title": "3 Experiments",
            "content": "We compared BitNet v2 to BitNet b1.58 and BitNet a4.8 of various model sizes. All models were trained with 1.58-bit weights. BitNet b1.58 has fully INT8 activations for all linear layers. BitNet a4.8 is continue-trained from BitNet b1.58 with hybrid quantization and sparsification for activations, where the inputs to the sublayers are quantized into 4-bit integers and the intermediate states uses top-K sparsification [WMWW24] and squared ReLU. We adopted the two-stage weight decay and learning rate scheduling following the training recipe of BitNet b1.58 [MWM+24]. All models were trained with 100B tokens from the RedPajama 4 Models Size ARCc ARCe HS PQ WGe LBA Avg BitNet v2 (a8) w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV BitNet v2 (a8) w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV 3B 7B 30.55 29.52 30.63 29.69 32.94 33.02 32.76 32. 55.56 55.18 55.22 55.22 58.54 58.67 58.46 58.29 57.19 57.17 57.15 56.22 61.08 61.04 61.01 60.85 71.33 70.95 71.16 71.49 74.10 73.61 74.10 73. 58.72 58.56 58.96 57.62 61.48 61.88 60.85 60.77 60.90 60.84 60.49 59.01 64.22 64.06 63.87 62.99 55.71 55.37 55.60 54.88 58.73 58.71 58.51 58. Table 2: The zero-shot accuracy of BitNet v2 with 8-bit activations and QKV states varying bit-widths on the end tasks. Models Size ARCc ARCe HS PQ WGe LBA Avg BitNet v2 (a4) w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV BitNet v2 (a4) w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV 3B 7B 28.92 29.52 28.58 29.18 32.42 32.94 33.11 32.08 55.01 54.46 55.43 55.51 58.00 58.12 57.91 57. 56.59 56.36 56.32 55.85 60.71 60.33 60.78 60.29 71.65 71.49 71.16 71.60 74.27 74.21 74.05 73.23 59.67 58.17 57.93 58.41 60.85 61.01 61.17 59. 60.74 60.14 60.70 59.54 63.52 63.65 62.93 62.97 55.43 55.02 55.02 55.02 58.30 58.38 58.33 57.69 Table 3: The zero-shot accuracy of BitNet v2 with 4-bit activations and QKV states varying bit-widths on the end tasks. dataset [Com23] to ensure fair comparison. For BitNet v2 (a4) and BitNet a4.8, we first trained the model with 8-bit activations for 95B tokens. Then we reused the optimizer states and continue-train the model with 4-bit activations for 5B tokens. More details can be found in the Appendix B. We evaluated the zero-shot accuracy for these models on range of language tasks using the lm-evaluation-harness toolkit [GTA+24], including ARC-Easy (ARCe) [YBS19], ARCChallenge (ARCc) [YBS19], Hellaswag (HS) [ZHB+19], Winogrande (WGe) [SBBC20] and PIQA (PQ) [BZB+19] and LAMBADA (LBA) [PKL+16]. We also reported the perplexity on the validation set of C4 [RSR+19] dataset. 3.1 Main Results We present the detailed results of BitNet v2 and the baselines in Table 1. Introducing the Hadamard transformation before the quantization in attention and FFN layers results in minimal perplexity degradation. For 8-bit activations, BitNet v2 surpasses BitNet b1.58 with an average accuracy improvement of 0.16%, 0.49%, and 0.61% on end tasks for the 1.3B, 3B, and 7B model sizes, respectively. Additionally, BitNet v2 enables native 4-bit activations across all linear layers, enhancing efficiency for batched inference. With INT4 activations, BitNet v2 achieves perplexity comparable to BitNet a4.8 while demonstrating superior performance on downstream tasks for the 3B and 7B models. Table 2 and Table 3 summarize detailed results of BitNet v2 (a8) and BitNet v2 (a4) with low-bit attention, respectively. We adopt post-RoPE quantization for QKV states. The QKV heads were directly quantized to unsigned integers using the absmax function, without the need of any calibration dataset. We retain the KV heads of [BOS] token as 8-bit precision. As shown in Table 2 and Table 3, BitNet v2 with 3-bit KV Cache achieves accuracy comparable to its counterpart with full-precision KV cache in 3B and 7B models. 3.2 Comparison with Post-Training Quantization. We compared BitNet v2 (a4) with post-training quantization baselines, including SpinQuant [LZF+24] and QuaRot [AMC+24], in 1.3B models. QuaRot employs randomized Hadamard 5 Models PPL ARCc ARCe HS PQ WGe LBA Avg w/o fusing rotary matrix to Wqkv,up,gate QuaRot SpinQuant 13.52 13.52 26.28 25.60 QuaRot SpinQuant BitNet v2 (a4) 20.83 19.80 11. 24.74 24.74 27.56 47.43 47.35 40.78 40.19 49.58 45.92 45.52 40.54 40.77 48.00 65.89 67. 62.89 62.73 68.23 51.46 52.49 49.33 52.09 55.49 42.34 42.52 36.89 39.24 53.58 46.55 46. 42.53 43.29 50.41 Table 4: Perplexity and zero-shot accuracy of BitNet v2, QuaRot and SpinQuant on the end tasks. Methods #Bits 1.3B 3B Acc. PPL Acc. PPL No rotatation Weight & activation rotatation Activation rotatation No rotatation Weight & activation rotatation Activation rotatation W1.58A W1.58A4 diverged diverged 50.47 51.16 11.14 11.14 55.55 55. 9.69 9.72 diverged diverged 50.09 50.41 11.33 11.33 54.98 55. 9.81 9.85 Table 5: Ablations on the Hadamard transformation of H-BitLinear across various sizes. transformations to mitigate outlier features, while SpinQuant uses the learnable rotary matrix. Then they adopt GPTQ and absmax function to quantize the weight and activations into 4-bit, respectively. Since the weights of BitNet b1.58 were already trained to be ternary values from scratch, we adopted absmean function used in the training of BitNet rather than GPTQ for weight quantization. For activation quantization of the baselines, we retained rotary transformations for activations as an online operation; Furthermore, we removed the fusion of RMS normalization scales into projections. Following [LZF+24], we tuned the rotation of SpinQuant with 800 samples from WikiText2 dataset. We report the results of BitNet v2 (a4) and BitNet b1.58 with SpinQuant and QuaRot in Table 4. All models were quantized to 1.58-bit weight and 4-bit activations. BitNet v2 (a4) significantly surpasses these baselines in terms of perplexity on valid set of C4 and accuracy on downstream tasks. Additionally, we observed that ternary models are more sensitive to the fusion of rotary matrix and latent weights. Specifically, we removed rotary matrix fusion for Wqkv in attention modules and Wup, gate in FFNs, i.e., using Qw(W )RT rather than Qw(W RT ). Removing this fusion notably enhances baseline performance: the perplexity decreases from 19.80 to 13.52 for SpinQuant. However, they still trails substantially behind BitNet v2 (a4). Moreover, this adjustment forces these projections to revert to full precision (W16A4), thus sacrificing inference efficiency. 3.3 Ablation Study We conduct ablation studies on the Hadamard transformation in H-BitLinear at the 1.3B and 3B model scales. All models are trained on the same dataset to ensure fair comparison. We report perplexity on the C4 validation set and zero-shot accuracy across range of language tasks, including ARC-Easy [YBS19], ARC-Challenge [YBS19], Hellaswag [ZHB+19], Winogrande [SBBC20], PIQA [BZB+19], and LAMBADA [PKL+16]. As shown in Table 5, removing the rotary transformation leads to model divergence. Moreover, while applying the Hadamard transformation to both the weights and activations results in faster convergence, it achieves similar performance to applying it only to the activations as training progresses. Therefore, for simplicity, we apply the Hadamard transformation only to the activations in H-BitLinear. Detailed results can be found in Appendix A."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduce BitNet v2, enabling native 4-bit activations within 1-bit LLMs. This is achieved using our proposed H-BitLinear layer in place of standard attention output and FFN down projections. H-BitLinear employs an online Hadamard transformation before activation quantization to effectively 6 suppress outlier channels by reshaping the activation distribution. Our experiments show BitNet v2 with 8-bit activations matches BitNet b1.58 performance. Subsequently, BitNet v2 can be trained for native 4-bit activation use. This 4-bit variant, BitNet v2(a4), maintains comparable performance to the 8-bit version while significantly boosting efficiency in batched inference scenarios."
        },
        {
            "title": "References",
            "content": "[AMC+24] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. CoRR, abs/2404.00456, 2024. [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019. [Com23] Together Computer. Redpajama: an open dataset for training large language models, 2023. [GTA+24] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [LPC+24] James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, and Ben Athiwaratkun. Training-free activation sparsity in large language models. CoRR, abs/2408.14690, 2024. [LZF+24] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization with learned rotations. CoRR, abs/2405.16406, 2024. [MWM+24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. CoRR, abs/2402.17764, 2024. [PKL+16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. CoRR, abs/1910.10683, 2019. [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 87328740, 2020. [Sha20] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. CoRR, abs/2310.11453, 2023. 7 [WMW24] Hongyu Wang, Shuming Ma, and Furu Wei. Bitnet a4.8: 4-bit activations for 1-bit llms. CoRR, abs/2411.04965, 2024. [WMWW24] Hongyu Wang, Shuming Ma, Ruiping Wang, and Furu Wei. Q-sparse: All large language models can be fully sparsely-activated. CoRR, abs/2407.10969, 2024. [WZS+25] Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, and Furu Wei. Bitnet. cpp: Efficient edge inference for ternary llms. arXiv preprint arXiv:2502.11880, 2025. [YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, EMNLP-IJCNLP, 2019. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: can machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 47914800, 2019. [ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems, pages 1236012371, 2019."
        },
        {
            "title": "A More results",
            "content": "Methods Size ARCc ARCe HS PQ WGe LBA Avg No rotatation Weight & activation rotatation Activation rotatation No rotatation Weight & activation rotatation Activation rotatation 1.3B 27.13 27.90 50.29 49.96 3B 30.03 30.55 55.72 55.56 diverged 48.32 69.04 48.37 69. diverged 56.81 71.65 57.19 71.33 54.30 57.22 53.76 54.14 50.47 51.17 59.43 58.72 59.65 60. 55.54 55.71 Table 6: Ablations on the Hadamard transformation of H-BitLinear across various sizes. All models have 1.58-bit weights and 8-bit activations. Methods Size ARCc ARCe HS PQ WGe LBA Avg No rotatation Weight & activation rotatation Activation rotatation No rotatation Weight & activation rotatation Activation rotatation 1.3B 27.22 27.56 49.12 49.58 3B 29.44 28. 54.46 55.01 diverged 47.77 69.37 48.00 68.23 diverged 56.57 71.93 56.59 71.65 54.54 55.49 52.49 53.58 50.09 50. 57.85 59.67 59.64 60.74 54.98 55.43 Table 7: Ablations on the Hadamard transformation of H-BitLinear across various sizes. All models have 1.58-bit weights and 4-bit activations. Hyper-parameters Size Hidden Size GLU Size #Heads #Layers Batch Size # Tokens Seq Length 400M 1.3B 3B 7B 1024 2048 4096 4096 8192 8192 16384 16 32 32 32 24 18 20 24 1M 1M 1M 1M 100B 100B 100B 100B 2048 2048 2048 Table 8: Model configurations for the BitNet models. Model BitNet Learning Rate Size 400M 1.8 103 1.2 103 1.2 103 8 104 1.3B 1.2 103 6.4 104 3B 1 103 6 104 7B Weight Decay Warm-up Adam β 0.1 0 0.1 0 0.1 0 0.1 0 375 375 375 375 (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) Table 9: Hyper-parameters for both BitNet v2 training."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of Chinese Academy of Sciences"
    ]
}