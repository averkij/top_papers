{
    "paper_title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "authors": [
        "Sankalp KJ",
        "Ashutosh Kumar",
        "Laxmaan Balaji",
        "Nikunj Kotecha",
        "Vinija Jain",
        "Aman Chadha",
        "Sreyoshi Bhaduri"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the intricacies of Indian languages. IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks' design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models."
        },
        {
            "title": "Start",
            "content": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding Sankalp KJ1, Ashutosh Kumar2, Laxmaan Balaji3, Nikunj Kotecha3, Vinija Jain4*, Aman Chadha5, Sreyoshi Bhaduri6 1Artificial Intelligence Institute, University of South Carolina 2Rochester Institute of Technology 3Independent Researcher 4Meta AI 5Amazon Gen AI 6Amazon sjajee@email.sc.edu, ak1825@rit.edu, laxmaanb@gmail.com, kotecha.nikunj95@gmail.com, hi@vinija.ai, hi@aman.ai, sreyoshibhaduri@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the inIndicMMLUtricacies of Indian languages. Pro provides standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models. As publicly available resource, IndicMMLU-Pro1 is set to contribute significantly to advancements in Indic languagebased technologies and serve as valuable tool for the NLP community."
        },
        {
            "title": "Introduction",
            "content": "With over 1.5 billion speakers, Indic languages constitute substantial component of the worlds *Work done outside position at Meta Work done outside position at Amazon Gen AI. Work done outside position at Amazon. 1https://huggingface.co/datasets/LinguaLift/IndicMMLUPro linguistic tapestry, showcasing the incredible diversity of the Indian subcontinent, where languages from the Indo-Aryan and Dravidian families have evolved over centuries, shaped by shared cultural and historical context. As an integral part of daily life, these languages facilitate communication and accessibility in various aspects of society, including education, government, media, healthcare, and social services. Further, by developing language-specific benchmarks and conducting research on Indic languages, we can increase the downstream impact on these languages on inclusion, ultimately leading to better language support and enhanced accessibility for the deaf and hard-of-hearing communities across the region who use Indic sign languages (Ananthanarayana et al., 2021b). This, in turn, can enable greater social inclusion, improved education and employment opportunities, and more effective participation in civic life for sign language users. The disparity in Natural Language Processing (NLP) research and resources across languages is striking with survey of existing literature showing how resources and research on Indic languages lagging (KJ et al., 2024a). In their work, (Joshi et al., 2020) revealed stark imbalance, where mere 28 percent of languages are considered \"winners,\" while staggering 88 percent are \"left behind.\" This disparity is exemplified by the contrast between English and Bengali, languages with comparable speaker populations (Lane, 2019). Despite this, English dominates Bengali regarding available resources, with hundreds of times more visibility on platforms like the Linguistic Data Consortium, Wikipedia, and academic publication venues of significance. Thus, Indic languages have historically received less attention in the field of NLP compared to more globally dominant languages (Das et al., 2024). The disparity in NLP resources for Indic languages has been attributed to their remark5 2 0 J 8 2 ] . [ 2 7 4 7 5 1 . 1 0 5 2 : r Figure 1: IndicMMLU-Pro Dataset Construction and Evaluation Pipeline. The diagram illustrates the end-toend process of creating and validating the IndicMMLU-Pro dataset across nine Indic languages. Starting with the English MMLU-Pro dataset, content is translated using IndicTrans2 (1B parameters) and undergoes rigorous quality assurance through back-translation and multiple metric evaluations (chrF++, BLEU, METEOR, TER, and SacreBLEU). Only translations meeting quality thresholds proceed to the final dataset. The workflow also shows the comprehensive evaluation process including expert proofreading involving 13 reviewers who assess semantic accuracy, fluency, and linguistic style. This systematic approach ensures the creation of high-quality, multilingual benchmark dataset that maintains the integrity of the original MMLU-Pro while adapting to the linguistic nuances of Indic languages. able linguistic diversity, intricate morphology, and the scarcity of annotated datasets (Kakwani et al., 2020a; Marreddy et al., 2022). Nevertheless, driven by the increasing demand for NLP applications in Indic languages and the prospect of groundbreaking technological innovations (Ananthanarayana et al., 2021a), there is an urgent need for rigorous evaluation benchmarks to assess the performance of AI models in this domain accurately. To address this gap, we introduce IndicMMLUPro, benchmark built upon the principles of the recently released MMLU-Pro (Wang et al., 2024) by Tiger Labs. IndicMMLU-Pro adapts the robust multi-task principles to the unique context of Indic languages, providing comprehensive evaluation framework that assesses the linguistic understanding, reasoning abilities, and generative capabilities of AI models. This paper makes three key contributions. Primarily, we introduce IndicMMLU-Pro, novel benchmark for evaluating AI models across wide range of tasks and multiple Indic languages. Next, the design principles, task taxonomy, and data collection methodology of IndicMMLU-Pro are presented in detail, to ensure that the benchmark accurately captures the complex linguistic and cultural characteristics of Indic languages. Furthermore, we establish baseline results on IndicMMLU-Pro using state-of-the-art multilingual models, laying the groundwork for future research and development of Indic languages-based AI models."
        },
        {
            "title": "2 Methodology",
            "content": "To create IndicMMLU-Pro, benchmark for Indic languages equivalent to MMLU Pro, we adopted process similar to the prior work on IndicMMLU, organizing our approach into two main steps: dataset creation and baseline benchmarking. 2.1 Dataset Creation Our goal was to provide MMLU-Pro in nine Indic languages: Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Urdu, Kannada and Punjabi. To achieve this, we used IndicTrans2 (Gala et al., 2023), state-of-the-art machine translation model specifically designed for Indic languages. As shown in Figure1, we use IndicTrans2 to convert the questions and corresponding options from the original English MMLU Pro dataset into each of the target languages. This approach allowed us to maintain the structure and content of the original benchmark while adapting it to the linguistic characteristics of Indic languages. The experimental settings for IndicTrans2 were as follows: Model Size: 1B parameters Quantization: None Batch Size: 8 We applied these settings consistently across all nine language translations to ensure uniformity in the translation process. 2.2 Quality Assurance To maintain the integrity and accuracy of the translated content, we implemented rigorous quality assurance process: Back-translation: For subset of the data, we performed back-translation to English and compared it with the original text to identify any significant discrepancies. Validation: After back-translation, we verified the back-translated dataset with the original MMLU-Pro on numerous metrics, such as chrF++ BLEU, METEOR, TER & SacreBLEU to ensure the datasets quality and consistency, also described in Section 3. This multi-metric evaluation provides comprehensive assessment of the translations accuracy and fluency. 2.3 Dataset Structure The resulting IndicMMLU-Pro dataset maintains the same structure as the original MMLU-Pro dataset, with separate subsets for each of the nine Indic languages. The dataset retains the original categories and task types from MMLU Pro, thus ensuring identical usage and facilitating comprehensive evaluation in various domains and cognitive skills. 2.4 Dataset Availability The IndicMMLU-Pro dataset is publicly available on the Hugging Face Hub (KJ et al., 2024b). This allows for easy access and reproducibility of our results. Researchers and practitioners can directly use or adapt this dataset for their studies and applications in the processing of Indic languages. 2.5 Baseline Benchmarking To establish baseline performance metrics for the IndicMMLU-Pro benchmark, we evaluated stateof-the-art multilingual language models: GPT-4o, GPT-4o-mini, Llama-3.1-8B-Instruct, IndicBERT (Kakwani et al., 2020a), IndicBART (Dabre et al., 2021), RemBERT (Chung et al., 2020), MuRIL (Khanuja et al., 2021), and XLM-RoBERTa (Conneau et al., 2020), Navarasa, Airavata (Gala et al., 2024), OpenHathi, TamilLlama (Balachandran, 2023), and MahaMarathi. These models were chosen due to their demonstrated capabilities in handling multiple languages and their specific design for the inclusion of Indic languages in their training data. The benchmarking process was conducted as follows: Model Selection: To provide benchmark of performance over the various languages contained in IndicMMLU-Pro, we leverage the following language models with Indic language understanding capabilities. IndicBERT and IndicBART: Specifically designed for Indic languages, offering robust performance in this domain. RemBERT: multilingual model with strong performance across diverse languages, suitable for cross-lingual tasks. MuRIL: multilingual model with focus on Indian languages, providing comprehensive coverage of Indic languages. XLM-RoBERTa: large-scale multilingual model is known for its crosslingual performance and ability to handle multiple languages efficiently. GPT-4o and GPT-4o-mini: State-ofthe-art models with advanced capabilities in handling multilingual tasks. Llama-3.1-8B-Instruct: model designed for multilingual (Hindi) instruction tasks. Navarasa, Airavata, OpenHathi, TamilLlama, and MahaMarathi: Models specifically designed or finetuned for Indic languages, enhancing their performance in this domain. Data Preparation: We used the test split of the IndicMMLU-Pro dataset for each of the nine Indic languages. The data was preprocessed to match the input format required by each model. Evaluation Process: We used accuracy as the primary metric, calculated as the percentage of correct predictions across all tasks in the benchmark. We acknowledge the importance of using multiple evaluation metrics to garner comprehensive understanding of text data (Bhaduri et al., 2024a), as different metrics can capture distinct aspects of model performance (Bedemariam et al., 2025). Nevertheless, for this study, accuracy served as suitable baseline metric. Notably, our evaluation was conducted separately for each language, enabling languagespecific performance analysis and facilitating more nuanced understanding of model strengths and weaknesses across diverse linguistic contexts. Computational Resources: The benchmarking was performed on cluster of NVIDIA A100 GPUs, with each model evaluation taking approximately 24 hours per language. The results of this baseline benchmarking are presented in Table 1 of the Results section, showing the accuracy scores for each model across all nine languages. This benchmarking process provides solid foundation for understanding the current capabilities of multilingual models in Indic languages and sets baseline for future research and improvements in this area. 2.6 Proofreading and Setup We conducted proofreading exercise on 9,000 sentence pairs (English and Indic), where 1,000 of these pairs were from the test split of each of the nine languages in the IndicMMLU-Pro dataset. These sentences pairs were stratified equally across the 14 categories and their respective 4 sources in each of the nine languages. These sentence pairs were shuffled randomly across all Indic languages to eliminate any bias and ensure there was no preservation for each source and category after the selection process. The three criteria that were defined for proofreading scores were: Semantic Accuracy and Correctness: ensuring the translation conveys the exact meaning of the original text without errors, omissions, or additions. Fluency and Readability: check if the translation reads naturally, smoothly, and is easy to understand in the target language. Linguistic and Stylistic Appropriateness: ensuring the tone, style, and language fit the purpose, audience, and cultural context of the text. total of 13 experts, representing all 9 Indic languages, participated in the proofreading exercise. The expert distribution ensured that each language had at least one expert, with four languages having an additional expert to facilitate workload sharing. Using standardized evaluation framework, each expert assessed sentence pairs based on three criteria, assigning scores on 5-point scale (1 = lowest, 5 = highest). Comprehensive guidelines, accompanied by illustrative samples, were provided to all experts to ensure consistency in scoring. The participating experts were native speakers of their respective languages, with fluency in English and formal education in their native languages. To ensure consistency, experts scores were systematically recorded in spreadsheets, which included sentence pairs and evaluations based on the three criteria. We extend our sincere gratitude to the experts listed below under Acknowledgements, for generously volunteering their time and expertise to support this research endeavor."
        },
        {
            "title": "3 Results",
            "content": "Table 3 presents the evaluation metrics for the IndicMMLU-Pro dataset using back-translation techniques. The chrF++ (Popovic, 2017) and BLEU (Papineni et al., 2002) scores are provided for each of the nine Indic languages (Bengali, Gujarati, Hindi, Kannada, Marathi, Punjabi, Tamil, Telugu, and Urdu). These metrics assess the quality and accuracy of the translated content in the IndicMMLU-Pro benchmark, providing insights into the datasets linguistic fidelity across different Indic languages. Table 1 and 2 present the accuracy scores (in percentages) of various pre-trained language models evaluated on the IndicMMLU-Pro benchmark. The benchmark covers nine major Indic languages: Bengali, Gujarati, Hindi, Kannada, Marathi, Punjabi, Tamil, Telugu, and Urdu. The scores reflect the models ability to handle diverse linguistic challenges specific to Indic languages. 3.1 Overall Performance Performance Range: The accuracy scores across all models and languages now fall within broader range of 9.90% to 44.80%. This wider range reflects the significant performance differences between the newer, more advanced models (like GPT-4o) and the previously evaluated models. Model Comparison: GPT-4o consistently outperforms all other models across all languages, achieving the highest scores ranging from 38.46% to 44.80%. GPT-4o mini follows as the secondbest performer with scores ranging from 25.75% to 35.08%. Among the previously evaluated models, XLM-RoBERTa remains the top performer, consistently outperforming other models across most languages with scores ranging from 11.92% to 13.16%. Language-Model Variability: There is now much more pronounced variability in performance across different languages and models. This suggests that language-specific characteristics and model architectures play significant roles in performance outcomes. 3.2 Language-wise Performance Breaking down the performance for each language reveals interesting patterns: Hindi: GPT-4o leads with 44.80%, followed by GPT-4o mini (32.33%). Among other models, Navarasa (12.43%) and XLMRoBERTa (12.33%) perform best. Bengali: GPT-4o achieves 44.38%, with GPT-4o mini at 31.11%. XLM-RoBERTa (12.68%) and Navarasa (12.08%) lead among other models. Telugu: GPT-4o scores 41.34%, GPT-4o mini 26.78%. XLM-RoBERTa (12.62%) outperforms Navarasa (11.77%) and TamilLlama (11.53%). Marathi: GPT-4o reaches 42.20%, GPT-4o mini 27.13%. RemBERT (12.93%) performs best among earlier models, with Navarasa at 11.88% and MahaMarathi at 11.60%. Tamil: GPT-4o mini shows strong performance at 35.08%, close to GPT-4os 38.46%. Navarasa (12.38%) slightly outperforms XLM-RoBERTa (12.34%), with TamilLlama at 11.54%. Gujarati: GPT-4o scores 41.77%, GPT-4o mini 28.29%. IndicBART (12.14%) and RemBERT (12.13%) perform well among earlier models. Urdu: GPT-4o achieves 44.18%, GPT-4o mini 31.13%. XLM-RoBERTa (12.53%) leads among other models. Kannada: GPT-4o scores 38.97%, GPT-4o mini 25.75%. XLM-RoBERTa significantly outperforms other models (13.16%) among earlier models. Punjabi: GPT-4o reaches 40.60%, GPT-4o mini 26.25%. XLM-RoBERTa (12.59%) performs best among other models. Language GPT-4o GPT-4o mini Llama-3.1-8B IndicBART IndicBERT RemBERT MuRIL Hindi Bengali Punjabi Marathi Urdu Gujarati Telugu Tamil Kannada 44.80 44.38 40.60 42.20 44.18 41.77 41.34 38.46 38. 32.33 31.11 26.25 27.13 31.13 28.29 26.78 35.08 25.75 18.61 N/A N/A N/A N/A N/A N/A N/A N/A 11.21 12.52 11.78 11.65 12.11 12.14 12.05 11.70 11.51 10.78 10.39 10.36 10.59 11.63 11.06 11.36 10.96 11.71 11.41 12.00 11.06 12.93 11.32 12.13 10.20 10.98 10.87 10.87 9.90 10.36 11.79 11.20 10.79 9.96 11.00 10. Table 1: Performance comparison of language models on the IndicMMLU-Pro benchmark across nine Indic languages, including Indo-Aryan (Hindi, Bengali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidian (Telugu, Tamil, and Kannada) languages. Accuracy scores are shown as percentages. Models compared include GPT-4o, GPT-4o mini, IndicBART, IndicBERT, RemBERT, MuRIL, and Llama-3.1-8B-Instruct. Language XLM-RoBERTa Navarasa Airavata OpenHathi TamilLlama MahaMarathi Hindi Bengali Punjabi Marathi Urdu Gujarati Telugu Tamil Kannada 12.33 12.68 12.59 12.57 12.53 11.92 12.62 12.34 13. 12.43 12.08 11.95 11.88 10.73 11.53 11.77 12.38 11.88 11.60 - - - - - - - - 11.65 - - - - - - - - - - - - - - 11.53 11.66 - - - - 11.60 - - - - - Table 2: Comparison of language model performance across Indian languages, both Indo-Aryan (i.e., Hindi, Bengali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidan (i.e., Telegu, Tamil, and Kannada). Scores are shown for Llama 3.1, Navarasa, Airavata, OpenHathi, TamilLlama, and MahaMarathi models where available. 3.2.1 Cross-linguistic Analysis Indo-Aryan languages (Hindi, Bengali, Punjabi, Gujarati, Urdu, Marathi) and Dravidian languages (Kannada, Tamil, Telugu) exhibit distinct performance patterns. Script-based Patterns: Languages using the Devanagari script (Hindi, Marathi) exhibit similar performance patterns, while Urdu (using the Perso-Arabic script) shows more varied performance. Performance Patterns: GPT-4o and GPT-4o mini consistently outperform all other models across both language families.Among other models, XLM-RoBERTa generally performs best across both language families. Navarasa shows competitive performance, particularly for the Dravidian language Language-specific Models: TamilLlama and MahaMarathi show promise but fail to surpass the performance of top multilingual models like XLM-RoBERTa and Navarasa. Consistency: GPT-4o and GPT-4o mini demonstrate relatively consistent performance across different languages, suggesting robust multilingual capabilities. Navarasa also shows consistency, particularly among Dravidian languages. Performance Gap: significant performance gap exists between GPT-4o/GPT-4o mini and other models, highlighting the advantages of larger, more advanced models in handling the complexities of Indic languages. 3.3 Dataset Quality Assessment In order to heuristically assert the quality of Indic language datasets, we convert the dataset from the respective Indic language back to English again by leveraging IndicTrans2 as in Section 2.1. The English translations are then scored by the following metrics: chrF++ (Popovic, 2017) (threshold > 50%): language and token agnostic, character ngram F1 score. We use n=6, as it is the standard set in the chrF++ paper. Language chrF++ BLEU METEOR Hindi Gujarati Tamil 78.06 77.67 74.32 0.59 0.58 0.54 0.56 0.55 0. TER 42.27 43.09 46.41 SacreBLEU 59.07 58.28 53.64 Table 3: Back-translation evaluation metrics for the IndicMMLU-Pro dataset for 3 Indic languages. BLEU (Papineni et al., 2002) (threshold > 25-30%): score obtained by measuring the precision of n-grams of the candidate translation compared to the reference translation, with brevity penalty to penalize short translations. METEOR (Banerjee and Lavie, 2005) (threshold = 50-60%): metric that evaluates translation quality by considering precision and recall, along with harmonic mean of unigram matches, with additional features such as stemming and synonymy matching. TER (Snover et al., 2006) (threshold < 4050%): score that measures the number of edits required to change system-generated translation to exactly match reference translation. SacreBLEU (Post, 2018) (threshold > 2530%): variant of BLEU that ensures standard reference text processing and tokenization. Table 3 provides insights into the quality of the IndicMMLU-Pro dataset through back-translation evaluation metrics for three languages: Hindi, Gujarati, and Tamil. The metrics include chrF++, BLEU, METEOR, TER, and SacreBLEU scores. Hindi shows the highest overall quality with chrF++ score of 78.06, BLEU of 0.59, METEOR of 0.56, TER of 42.27, and SacreBLEU of 59.07. Gujarati follows closely with very similar scores: chrF++ of 77.67, BLEU of 0.58, METEOR of 0.55, TER of 43.09, and SacreBLEU of 58.28. Tamil, while still demonstrating good quality, shows slightly lower scores across all metrics: chrF++ of 74.32, BLEU of 0.54, METEOR of 0.52, TER of 46.41, and SacreBLEU of 53.64. The high chrF++ scores (above 74) for all three languages indicate good overall translation quality and semantic preservation. The BLEU and SacreBLEU scores, ranging from 0.54 to 0.59 and 53.64 to 59.07 respectively, suggest reasonably good translation quality, though theres room for improvement. The METEOR scores (0.52-0.56) also indicate good semantic similarity between the original and back-translated texts. The Translation Error Rate (TER) scores, ranging from 42.27 to 46.41, suggest that moderate amount of editing is required to match the reference translation, with Tamil requiring slightly more editing than Hindi or Gujarati. It is important to note that data for the other six languages (Bengali, Punjabi, Kannada, Telugu, Urdu, and Marathi) is missing from Table 2. This limitation in the dataset quality assessment makes it challenging to draw comprehensive conclusions about the overall quality of the IndicMMLU-Pro dataset across all nine languages. In summary, the available metrics indicate good translation quality for Hindi, Gujarati, and Tamil, with Hindi showing the highest quality across all metrics. However, the lack of data for the remaining languages highlights the need for more comprehensive evaluation of the entire dataset. To better illustrate the meaning of chrF++ scores in practice, lets examine sample translations for Hindi (highest score), Gujarati (middle score), and Tamil (lowest score) as shown in Figure 2, 3, 4 respectively. Hindi (chrF++: 78.06): The chrF++ score for Hindi is the best among the three languages indicating high structural and semantic similarity between the original text and the back-translated text. Gujarati (chrF++: 77.67): While slightly this score still represents high-quality lower, translation with minor variations. Tamil (chrF++: 74.32): This lower score might reflect slight changes in word choice or structure, but the core meaning is preserved. These examples demonstrate that even with the lowest chrF++ score in our dataset (74.32 for Tamil), the translations maintain good semantic fidelity. The differences in scores often reflect Figure 2: The original text sample, its Hindi translation, and the corresponding back-translated text Figure 3: The original text sample, its Gujarati translation, and the corresponding back-translated text Figure 4: The original text sample, its Tamil translation, and the corresponding back-translated text Language Questions Choices Hindi Bengali Telugu Marathi Tamil Gujarati Urdu Kannada Punjabi 0.9109 0.9172 0.9193 0.9126 0.9194 0.9164 0.9121 0.9149 0.9177 0.9250 0.9251 0.9287 0.9242 0.9255 0.9320 0.9302 0.9238 0.9254 Table 4: Cosine similarity scores between LaBSE embeddings of IndicMMLU-Pro languages and English MMLU-Pro for questions and multiple-choice options. These scores are used as measure of semantic similarity, with higher values suggesting closer meaning alignment across languages. nuances in word choice, sentence structure, or slight variations in conveying the same meaning, rather than significant errors in translation. This analysis supports our earlier observation that the IndicMMLU-Pro dataset maintains good overall translation quality across the evaluated languages. Preliminary assessments of the other languages (Bengali, Punjabi, Kannada, Telugu, Urdu, and Marathi) suggest similar trends in translation quality, with chrF++ scores ranging from 73 to 79. Complete metrics for these languages are being compiled and will be included in future publications. 3.4 Cosine Similarity Scores We evaluated the semantic similarity between Indic languages from the IndicMMLU-Pro dataset and the English MMLU-Pro dataset using the LaBSE (Language-agnostic BERT Sentence EmIt loads both datasets and probedding) model. cesses all the samples from each. For each pair of corresponding questions, it generates embeddings for the questions and their multiple-choice options using the LaBSE model. The script then calculates cosine similarity between these embeddings to measure how semantically similar Indic languages and their english versions are. It computes two main metrics: average question similarity, and average choice similarity. The question and choice similarities indicate how close the meanings are between the two languages. This analysis helps assess how accurately Indic languages in the IndicMMLU-Pro represent the original English MMLU-Pro in terms of meaning and structure, which is crucial for ensuring the quality and consistency of multilingual datasets in machine learning and natural language processing tasks."
        },
        {
            "title": "4 Related Work",
            "content": "The study of multilingual language models has been significantly advanced by efforts to create and utilize datasets that encompass wide range of languages. Indic languages, with their diverse scripts and rich morphological structures, present unique challenges and opportunities for multilingual NLP research. Multilingual datasets have been crucial in developing language models capable of understanding and generating text in multiple languages. The WikiAnn dataset, for instance, provides valuable resource for named entity recognition across many languages, including several Indic languages (Pan et al., 2017). The IndicNLP Corpus (Kunchukuttan, 2020), developed as part of the IndicNLP Library, provides extensive monolingual corpora for various Indic languages. This dataset has been instrumental in training and evaluating models specifically designed for these languages. IndicCorp (Joshi et al., 2022), large-scale dataset of Indic languages, and the IndicNLPSuite(Kakwani et al., 2020b), which includes pre-trained models and linguistic resources, have further enhanced the development of multilingual NLP models. These resources have provided solid foundation for various NLP tasks such as machine translation, text classification, etc. IndicGLUE(Kakwani et al., 2020c) is comprehensive benchmark designed to evaluate the performance of NLP models on variety of tasks across multiple Indic languages. It includes tasks such as sentiment analysis, natural language inference, and question answering, making it valuable resource for assessing the capabilities of multilingual models. (Ahuja et al., 2023) introduce Megaverse, comprehensive benchmark designed to evaluate large language models across variety of lanThis guages, modalities, models, and tasks. benchmark aims to provide holistic assessment of language models performance, emphasizing their capabilities and limitations in handling diverse linguistic and contextual scenarios. Megaverse is instrumental in identifying the strengths and weaknesses of current models and guiding future improvements in multilingual and multimodal language processing. (Holtermann et al., 2024) developed MultiQ, benchmark aimed at evaluating the elementary multilingual capabilities of large language models. MultiQ focuses on assessing basic language understanding and generation tasks across multiple languages, providing insights into the foundational multilingual capabilities of these models. This benchmark is critical for identifying gaps in language model training and performance, particularly for less-represented languages. (Aggarwal et al., 2022) introduce IndicXNLI, benchmark specifically designed to evaluate the natural language inference capabilities of multilingual models for Indian languages. This benchmark includes diverse set of inference tasks, reflecting the linguistic richness and complexity of Indic languages. IndicXNLI is crucial for advancing the understanding and processing of Indian languages in NLP, providing standard for evaluating inference models in this context. (Kumar et al., 2022) develop the IndicNLG benchmark, which provides multilingual datasets for variety of natural language generation (NLG) tasks in Indic languages. This benchmark addresses the unique challenges posed by the linguistic diversity of Indic languages, offering datasets for tasks such as machine translation, summarization, and text generation. IndicNLG is instrumental in improving the performance and capabilities of NLG models for Indic languages, fostering advancements in multilingual NLP. MMLU (Massively Multilingual Language Understanding) (Liang et al., 2020; Hu et al., 2020) is comprehensive benchmark designed to evaluate the performance of language models across multiple languages and wide range of tasks. This benchmark has been instrumental in assessing the capabilities of multilingual models and has provided insights into their strengths and limitations."
        },
        {
            "title": "5 Key Findings and Implications of",
            "content": "IndicMMLU-Pro IndicMMLU-Pro represents significant step forward in the development and evaluation of AI models for Indic languages. Through this comprehensive benchmark, we have shed light on the current capabilities and limitations of state-of-theart multilingual models in handling the linguistic diversity and complexity of the Indian subcontinent. This section details implications based on some of our key findings. 5.1 Performance Across Models 5.1.1 GPT-4o Dominance GPT-4o consistently outperformed all other models across all languages, with accuracy scores ranging from 38.46% to 44.80%. This suggests that large, advanced language models have significant potential for handling Indic languages. 5.1.2 Performance Tiers clear hierarchy emerged among the models: Elite Tier: GPT-4o Advanced Tier: GPT-4o mini Specialized Tier: XLM-RoBERTa, Navarasa, and other specialized models Foundation Tier: Earlier models like IndicBART, RemBERT, and IndicBERT, MuRIL 5.1.3 Specialized vs. General Models While GPT-4o and GPT-4o mini showed superior performance, specialized models like XLMRoBERTa and Navarasa consistently outperformed other models in the foundation tier. This highlights the importance of both scale and specialized training for Indic languages. 5.2 Language-Specific Insights 5.2.1 Performance Variability Significant variability in model performance was observed across different Indic languages, emphasizing the need for language-specific approaches in NLP. 5.2.2 Script and Language Family Impact Languages using similar scripts (e.g., Devanagari for Hindi and Marathi) showed similar performance patterns. Urdu, using different script, exhibited more varied performance across models. 5.2.3 Indo-Aryan vs. Dravidian Languages While performance varied across both language families, some models showed more consistency in Dravidian languages, suggesting potential differences in how models handle these distinct language groups. 5.3 Dataset and Evaluation Quality 5.3.1 Translation Quality Back-translation evaluation metrics for Hindi, Gujarati, and Tamil demonstrated good overall translation quality and semantic preservation, with chrF++ scores above 74 for all three languages. 5.3.2 Metric Discrepancies Differences between chrF++ and BLEU scores suggest that while meaning is generally preserved, there may be variations in phrasing and structure compared to the original English text. 5.3.3 Comprehensive Evaluation Need The lack of evaluation metrics for six out of nine languages highlights the need for more thorough assessment across all included Indic languages. 5.4 Methodological Insights 5.4.1 Benchmark Design to create The adaptation successful IndicMMLU-Pro approach to developing comprehensive, multitask benchmarks for specific language groups. of MMLU-Pro demonstrates 5.4.2 Translation Approach The use of IndicTrans2 for dataset creation, combined with rigorous quality assurance processes, provides template for developing high-quality multilingual datasets. 5.4.3 Evaluation Metrics The use of multiple metrics (chrF++, BLEU, METEOR, TER, SacreBLEU) for assessing translation quality offers more nuanced understanding of dataset quality. These findings and implications highlight the significant advancements made by IndicMMLUPro in evaluating and understanding the performance of language models across Indic languages, while also pointing to crucial areas for future research and development in multilingual NLP. 5.5 Future Directions Based on our findings, we propose several key areas for future research and development: Data Collection: There is pressing need for more high-quality, diverse datasets across all Indic languages, particularly for lowresource languages. This will in turn enable more robust model training and evaluation. Future Model Development: research should focus on developing models that can better handle the unique linguistic features of Indic languages, including their complex morphology and script diversity. This may involve innovative architecture designs or novel pre-training techniques. Cross-lingual Transfer: Exploring techniques to improve knowledge transfer between related Indic languages will help boost performance, especially for low-resource languages. This could involve leveraging linguistic similarities or developing more effective multilingual training strategies. Task-specific Fine-tuning: Developing strategies for effective fine-tuning of large multilingual models on specific language tasks lead to significant performance improvements (Balne et al., 2024). Thus future work in this area may include investigating optimal fine-tuning techniques or developing Indic-specific pre-training tasks. Evaluation Metrics: Further refinement of evaluation metrics to account for the linguistic and cultural nuances of Indic languages is necessary for more accurate performance assessment. Existing metrics often overlook the linguistic and cultural nuances inherent to Indic languages. IndicMMLU-Pro sets new standard for evaluating AI models in the context of Indic languages. By providing comprehensive, multitask, and multilingual benchmark, it enables researchers and developers to assess and improve the capabilities of their models across wide range of Indic languages and domains. As the field of NLP continues to advance, we hope that IndicMMLUPro will catalyze increased research and development of technologies based on Indic languages. This benchmark not only highlights the current state of the art but also points the way forward for creating more linguistically diverse, culturally sensitive, and capable AI systems that can serve the vast and diverse population of the Indian subcontinent. To foster more comprehensive understanding of Indic language models, it is essential to intentionally integrate multi-disciplinary perspectives into engineering and science workstreams (Bhaduri et al., 2024b). Collaborations between engineers, scientists, and experts from social sciences can provide nuanced insights into the complex interplay between language, culture, and technology (Mackenzie et al., 2024). By embracing interdisciplinary approaches and refining evaluation metrics, we can work towards more inclusive and accurate assessment of Indic language models, ultimately driving progress in natural language processing for diverse languages and communities. In conclusion, while significant challenges remain in the field of Indic language NLP, IndicMMLU-Pro provides robust framework for measuring progress and guiding future research efforts. As we continue to push the boundaries of whats possible in multilingual AI, benchmarks like IndicMMLU-Pro will play crucial role in ensuring that technological advancements benefit all language communities, fostering greater inclusivity and accessibility in the digital age."
        },
        {
            "title": "6 Acknowledgements",
            "content": "We extend our deepest gratitude to the experts who volunteered their time and effort to evaluate our dataset. Unfortunately, Indic languages are often neglected in the development and evaluation of Large Language Models, making initiatives like ours reliant on the generosity and expertise of individuals like those listed below. The list of experts and the language they reviewed are: Telugu Experts: Dushyanth Reddy and Sai Krishna Bolla Tamil Expert: Veeramanohar Avudaiappan Hindi Expert: Ojasmitha Pedirappagari Gujarati Expert: Ashwini Thosar Punjabi Expert: Shivan Bhatia Marathi Experts: Nityanand Bhadra Sameer Jadhav and Kannada Expert: Nikhil Ragunath Bengali Experts: Sounak Datta and Nirjhar Barua Urdu Experts: Md Mahtab and Risha Fatema Expert feedback has significantly enhanced the quality of our dataset, and we are grateful for their dedication to promoting Indic languages in the global LLM community."
        },
        {
            "title": "References",
            "content": "Divyanshu Aggarwal, Vivek Gupta, and Anoop Indicxnli: Evaluating mularXiv inference for indian languages. Kunchukuttan. 2022. tilingual preprint arXiv:2204.08776. Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, et al. 2023. Megaverse: Benchmarking large language models across languages, modalities, models and tasks. arXiv preprint arXiv:2311.07463. Tejaswini Ananthanarayana, Nikunj Kotecha, Priyanshu Srivastava, Lipisha Chaudhary, Nicholas Wilkins, and Ifeoma Nwogu. 2021a. Dynamic cross-feature fusion for american sign language translation. In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021), pages 18. IEEE. Tejaswini Ananthanarayana, Priyanshu Srivastava, Akash Chintha, Akhil Santha, Brian Landy, Joseph Panaro, Andre Webster, Nikunj Kotecha, Shagan Sah, Thomastine Sarchet, et al. 2021b. Deep learning methods for sign language translation. ACM Transactions on Accessible Computing (TACCESS), 14(4):130. Abhinand Balachandran. 2023. Tamil-llama: new arXiv tamil language model based on llama 2. preprint arXiv:2311.05845. Charith Chandra Sai Balne, Sreyoshi Bhaduri, Tamoghna Roy, Vinija Jain, and Aman Chadha. 2024. Parameter efficient fine tuning: comprehenarXiv preprint sive analysis across applications. arXiv:2404.13506. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572. Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, and Naumaan Nayyar. 2025. Potential and perils of large language models as judges of unstructured textual data. arXiv preprint arXiv:2501.08167. Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Anshul Mittal, and Rutu Mulkar. 2024a. Reconciling methodological paradigms: Employing large language models as novice qualitative research assistants in talent management research. arXiv preprint arXiv:2408.11043. Sreyoshi Bhaduri, Kenneth Ohnemus, Jess Blackburn, Anshul Mittal, Yan Dong, Savannah Laferriere, Robert Pulvermacher, Marina Dias, Alex Gil, Shahriar Sadighi, et al. 2024b. (multi-disciplinary) teamwork makes the (real) dream work: Pragmatic recommendations from industry for engineering classrooms. Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. 2020. Rethinking embedding coupling in pre-trained language models. Preprint, arXiv:2010.12821. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. Preprint, arXiv:1911.02116. Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra, and Pratyush Kumar. 2021. Indicbart: pre-trained model for natural language generation of indic languages. Preprint, arXiv:2109.02903. Dipto Das, Shion Guha, Jed Brubaker, and Bryan Semaan. 2024. Thecolonial impulse\" of natural language processing: An audit of bengali sentiment analysis tools and their identity-based biases. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 118. Jay Gala, Pranjal A. Chitale, Raghavan AK, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. Preprint, arXiv:2305.16307. Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh Khapra, Raj Dabre, Rudra Murthy, Anoop Kunchukuttan, et al. 2024. Airavata: Introducing hindi instruction-tuned llm. arXiv preprint arXiv:2401.15006. Carolin Holtermann, Paul Röttger, Timm Dill, and Anne Lauscher. 2024. Evaluating the elementary multilingual capabilities of large language models with multiq. arXiv preprint arXiv:2403.03814. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv preprint arXiv:2003.11080. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the nlp world. arXiv preprint arXiv:2004.09095. Sachin Joshi et al. 2022. Indiccorp: large-scale In Proceedings of the dataset for indic languages. 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. IndicNLPKhapra, and Pratyush Kumar. 2020a. Suite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages. In Findings of EMNLP. Divyanshu Kakwani, Anoop Kunchukuttan, Raj Shah, et al. 2020b. Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages. arXiv preprint arXiv:2004.09095. Divyanshu Kakwani et al. 2020c. Indicglue: benchmark for evaluating indic language understanding. arXiv preprint arXiv:2004.09095. Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, and Partha Talukdar. 2021. Muril: Multilingual representations for indian languages. Preprint, arXiv:2103.10730. Sankalp KJ, Vinija Jain, Sreyoshi Bhaduri, Tamoghna Roy, and Aman Chadha. 2024a. Decoding the diversity: review of the indic ai research landscape. arXiv preprint arXiv:2406.09559. Copenhagen, Denmark. Association for Computational Linguistics. Matt Post. 2018. call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. study of translation edit rate with targeted human annotaIn Proceedings of the 7th Conference of the tion. Association for Machine Translation in the Americas: Technical Papers, pages 223231. Sankalp KJ, Ashutosh Kumar, Laxmaan Balaji, Nikunj Kotecha, Vinija Jain, Aman Chadha, and Sreyoshi Bhaduri. 2024b. IndicMMLU-Pro: Comprehensive Benchmark for Evaluating Multilingual AI Models across Indic Languages. https://huggingface.co/datasets/ LinguaLift/IndicMMLU-Pro. 2024-07-12. Accessed: Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlupro: more robust and challenging multi-task Preprint, language understanding benchmark. arXiv:2406.01574. Aman Kumar, Himani Shrotriya, Prachi Sahu, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh Khapra, and Pratyush Kumar. 2022. Indicnlg benchmark: Multilingual datasets for diverse nlg tasks in indic languages. arXiv preprint arXiv:2203.05437. Anoop Kunchukuttan. 2020. The indicnlp library. James Lane. 2019. The 10 most spoken languages in the world. Babbel Magazine, 6(09). Yaobo Liang et al. 2020. Xglue: new benchmark dataset for cross-lingual pre-training, understanding and generation. arXiv preprint arXiv:2004.01401. Tammy Mackenzie, Leslie Salgado, Sreyoshi Bhaduri, Victoria Kuketz, Solenne Savoia, and Lilianny Virguez. 2024. Beyond the algorithm: Empowering In 2024 ai practitioners through liberal education. ASEE Annual Conference & Exposition. Mounika Marreddy, Subba Reddy Oota, Lakshmi Sireesha Vakada, Venkata Charan Chinni, and Radhika Mamidi. 2022. Am resource-poor language? data sets, embeddings, models and analysis for four different nlp tasks in telugu language. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(1):134. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461958. Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. Maja Popovic. 2017. chrF++: words helping characIn Proceedings of the Second Conter n-grams. ference on Machine Translation, pages 612618,"
        },
        {
            "title": "7 Appendix",
            "content": "7.1 Choice of Models The selection of models for our benchmark evaluation was based on their relevance to Indic languages, their prominence in multilingual NLP research, and their potential to handle the diverse linguistic features of Indic languages. Heres detailed explanation of our choices: GPT-4o: highly advanced multimodal model that offers improved speed and costeffectiveness compared to GPT-4 Turbo, while also featuring enhanced vision capabilities. With its 128K context window and knowledge cutoff in October 2023, GPT4o represents the cutting edge in large language models, making it an excellent candidate for benchmarking across diverse Indic languages. GPT-4o-mini: Designed as more costefficient alternative to larger models, GPT4o-mini offers balance between performance and resource utilization. Despite its smaller size, it outperforms GPT-3.5 Turbo in intelligence while maintaining lower costs. Its inclusion in our benchmark allows us to assess the trade-offs between model size, performance, and efficiency in processing Indic languages. IndicBERT: transformer-based multilingual model specifically pre-trained on 12 Developed by major Indian languages. AI4Bharat, IndicBERT is designed to capture the nuances of Indic languages through its training on large corpus of Indian language text. Its inclusion in our benchmark is crucial for evaluating performance on tasks that require deep understanding of Indian language structures and semantics. IndicBART: sequence-to-sequence model tailored for Indian languages, IndicBART extends the capabilities of mBART by incorporating additional pre-training on Indic language corpora. It excels in generation tasks such as summarization, translation, and text completion across multiple Indian languages. Its inclusion allows us to assess performance on more complex, generative NLP tasks specific to Indic languages. RemBERT: Developed by Google Research, RemBERT (Rebalanced Multilingual BERT) is an advanced multilingual model that addresses the limitations of previous multilingual models. It uses novel rebalancing technique during pre-training to improve performance across wide range of languages, including low-resource ones. RemBERTs inclusion in our benchmark provides strong baseline for assessing how well general multilingual models perform on Indic language tasks compared to Indic-specific models. MuRIL: (Multilingual Representations for Indian Languages): Developed by Google Research India, MuRIL is specifically designed to address the unique challenges of Indian languages. Its trained on significantly larger amounts of Indian language data compared to general multilingual models, covering 17 Indian languages including lowresource ones. MuRILs architecture incorporates techniques to handle the linguistic diversity and script complexity of Indian languages, making it crucial benchmark for evaluating specialized Indic language performance. XLM-RoBERTa: large-scale multilingual model trained on 100 languages, XLMRoBERTa represents significant advancement in cross-lingual language understanding. Developed by Facebook AI, it uses self-supervised learning approach on vast Its inclusion amount of web-crawled data. in our benchmark allows us to assess how well general multilingual model, trained on diverse set of languages including Indic ones, performs on specific Indic language tasks compared to models with more focused Indic language training. OpenHathi: An open-source large language model focused primarily on Hindi and other Indian languages. Developed as part of an initiative to create freely accessible AI resources for Indic languages, OpenHathi is trained on diverse corpus of Indian language texts, including literature, news, and Its inclusion in our benchweb content. mark allows us to evaluate the performance of community-driven, open-source efforts in Indic language modeling, particularly for Hindi, and compare it with proprietary and general multilingual models. Airavata: Developed by AI4Bharat, Airavata is specialized large language model focused on Hindi and other Indian languages. It represents significant effort in creating instruction-tuned models specifically for Indic languages. Trained on diverse corpus of Hindi text and fine-tuned on carefully curated instruction datasets, Airavata aims to understand and generate contextually appropriate responses in Hindi. Its inclusion in our benchmark allows us to evaluate the effectiveness of language-specific instruction tuning for Indic languages, particularly in tasks requiring nuanced understanding of cultural and linguistic contexts unique to Hindi. TamilLlama: pioneering language model specifically designed for Tamil, one of the classical languages of India. Developed by team of researchers and language enthusiasts, TamilLlama is built upon the LLaMA architecture but fine-tuned extensively on It incorporates large corpus of Tamil text. unique features to handle the agglutinative nature of Tamil and its complex morphology. By including TamilLlama in our benchmark, we aim to assess the advantages of highly specialized, language-specific models in capturing the intricacies of individual Indic languages, particularly for tasks that require deep understanding of Tamil linguistic structures and cultural nuances. 7.2 Model Output & Evaluation IndicBERT: The IndicBERT model, when used with AutoModelForMultipleChoice, outputs logits for each option in multiplechoice question. These logits represent the models confidence scores for each option. The model produces logits for each option. torch.argmax(logits, dim=1) selects the index of the option with the highest logit (i.e., the models predicted answer). This index is converted to letter (A, B, C, D) using the index_to_letter dictionary. The predicted answer (letter) is directly compared with the correct answer. IndicBART: The IndicBART model is being loaded with AutoModelForSeq2SeqLM framework to construct input texts by combining the question with each of its options in the format: Question: <question> Answer: <option>. These input texts are encoded into format that the model can process. The model generates responses for each encoded input text, effectively simulating an answer for each option. scoring heuristic, such as the length of the generated response, is then applied to determine the relevance or quality of each simulated answer. The option with the highest score is selected as the models predicted answer. This method leverages the models ability to generate coherent and contextually appropriate responses, allowing it to adapt to different questions and options effectively. It iterates through all questions, applies this process, and calculates the accuracy by comparing predicted answers with the correct ones, thus evaluating the models performance in handling multiple-choice questions across various languages. MURIL: The MURIL (Multilingual Representations for Indian Languages) model is being used for sequence classification. The models output is single logit value for each question-option pair, representing the models confidence or relevance score for that pair. The evaluation process works as follows: For each question, the script passes the question text paired with each option to the model separately. The model produces score for each pair. These scores are collected for all options of question. The option with the highest score is selected as the predicted answer. This approach assumes that the option most relevant or coherent with the question (as determined by the models score) is likely to be the correct answer. While this method can work for some types of questions, its not the standard way to use MURIL for multiple-choice questions. MURIL is primarily designed for tasks like text classification, not specifically for multiple-choice question answering. more appropriate approach would be to fine-tune MURIL on multiple-choice QA task, or to use model specifically designed for this purpose. The current method might work to some extent due to MURILs pretraining on diverse tasks, but it may not be as accurate or reliable as properly fine-tuned model for multiplechoice questions. RemBERT: The RemBERT model, which is designed for multiple-choice tasks, is being used. The models output is set of logits, one for each option in the multiplechoice question. These logits represent the models confidence scores for each option. The evaluation process works as follows: For each question, the script encodes the question paired with each option and passes this to the model. The model then outputs logits for all options simultaneously. The option with the highest logit score is selected as the predicted answer. This approach is correct and appropriate for multiple-choice question-answering tasks, as it allows the model to consider all options together in context, rather than evaluating them independently. The model has been trained to understand the relationship between the question, the correct answer, and the distractors, making it well-suited for this type of task. This method of using RemBERT for multiplechoice questions is standard and effective approach in natural language processing for question-answering tasks. XLM-RoBERTa: The XLM-RoBERTa model is being used for sequence classification. The models output is single logit value for each question-option pair, Figure 5: Model Accuracy Across Different Languages of question. The option with the highest score is selected as the predicted answer. This approach assumes that the option most relevant or coherent with the question (as determined by the models score) is likely to be the correct answer. While this method can work to some extent, its not the optimal way to use XLM-RoBERTa for multiple-choice questions. XLM-RoBERTa is primarily designed for tasks like text classification or sequence pair classification, not specifically for multiple-choice question answering. more appropriate approach would be to finetune XLM-RoBERTa on multiple-choice QA task, or to use model architecture specifically designed for multiple-choice tasks, which would consider all options simultaneously rather than in pairs. Airavata We evaluate the performance of the Airavata language model on the Hindi It subset of the IndicMMLU-Pro dataset. loads specified model using the Hugging Face transformers library, and then processes multiple-choice questions from the dataset. Figure 6: Average Question and Option Lengths in # Words and # Tokens representing the models relevance or compatibility score for that pair. The evaluation process works as follows: For each question, the script passes the question text paired with each option to the model separately. The model produces score for each pair. These scores are collected for all options For each question, it creates prompt in chat format, feeds it to the model, and compares the models single-token prediction to the correct answer. The script calculates the overall accuracy of the models predictions across all questions in the dataset. Finally, it saves this accuracy score to text file and prints it to the console, providing quantitative measure of the models capability in understanding and answering Hindi multiplechoice questions."
        },
        {
            "title": "8 Additional Data Samples",
            "content": "To further illustrate the performance of the machine translation workflow, we provide examples of translations and back-translations in three different languages: Hindi, Gujarati, and Tamil. Figure 2 showcases the original text sample, its translation into Hindi, and the corresponding backtranslated text, demonstrating the transformation and consistency of the translation process. For additional insights, Figure 7, 8, 9 includes broader set of examples across Hindi, Gujarati, and Tamil, highlighting the systems ability to handle linguistic diversity and maintain semantic fidelity across these languages. These examples emphasize the efficacy and robustness of the translation pipeline in preserving the original meaning during both forward and reverse translation. Figure 7: Additional examples showcasing the machine translation workflow, including the original text samples, their Hindi translations, and the corresponding back-translated texts. Figure 8: Additional examples showcasing the machine translation workflow, including the original text samples, their Gujarati translations, and the corresponding back-translated texts. Figure 9: Additional examples showcasing the machine translation workflow, including the original text samples, their Tamil translations, and the corresponding back-translated texts."
        }
    ],
    "affiliations": [
        "Amazon Gen AI",
        "Artificial Intelligence Institute, University of South Carolina",
        "Independent Researcher",
        "Meta AI",
        "Rochester Institute of Technology"
    ]
}