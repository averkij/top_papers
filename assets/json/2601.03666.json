{
    "paper_title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "authors": [
        "Haonan Chen",
        "Sicheng Gao",
        "Radu Timofte",
        "Tetsuya Sakai",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B."
        },
        {
            "title": "Start",
            "content": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings Haonan Chen1, Sicheng Gao2, Radu Timofte2, Tetsuya Sakai3, Zhicheng Dou 1 1Gaoling School of Artificial Intelligence, Renmin University of China 2University of Würzburg 3Waseda University {hnchen,dou}@ruc.edu.cn {sicheng.gao,radu.timofte}@uni-wuerzburg.de tetsuya@waseda.jp 6 2 0 2 9 ] . [ 2 6 6 6 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Modern information systems often involve different types of items, e.g., text query, an image, video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched firstand second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match crossmodal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https:// huggingface.co/Haon-Chen/e5-omni-7B."
        },
        {
            "title": "Introduction",
            "content": "Modern information retrieval increasingly needs to handle multimodal queries and results in web Co-first authors Corresponding authors Figure 1: Implicit vs. explicit alignment for omnimodal embeddings. (a) Implicit alignment leads to modality-dependent sharpness, negative hardness imbalance, and unstable ranking. (b) e5-omni performs explicit alignment with three lightweight modules to calibrate cross-modality similarities. search, multimedia question answering, and longdocument understanding (Jiang et al., 2025; Meng et al., 2025; Ma et al., 2024). Recent vision language models (VLMs) have shown strong multimodal capabilities, and bi-modal embedding models fine-tuned from VLMs perform well on text image retrieval (Chen et al., 2025b; Zhang et al., 2024). However, many real-world retrieval scenarios involve additional modalities beyond text and images, such as audio and video. Motivated by this, we study an omni-modal setting where heterogeneous items (text, images, audio, and video) are embedded into shared space for consistent similarity comparison. Recent work has started to move from bi-modal to omni-modal embedding models, e.g., by adopting VLM-centered bi-encoder architecture (Ma et al., 2025; Xu et al., 2025b; Xiao et al., 2025). However, these methods still largely rely on implicit modality alignment inherited from the generative pretraining of VLMs. Contrastive fine-tuning provides only limited corrective signal, and it may not fully calibrate modality-specific similarity scales or the local geometry of the shared space. As shown in the left part of Fig. 1, omni-modal contrastive training is challenging because different modalities induce similarity scores with different scales, hardness profiles, and embedding geometry. This often leads to three issues: (1) Modalitydependent sharpness: using single global temperature can produce overly sharp logits for some modality compositions but overly flat logits for others, which creates imbalanced contrastive gradients; (2) Negative hardness imbalance: negative hardness varies across modalities and shifts during training; when all in-batch negatives are treated equally, optimization can be dominated by many trivially easy negatives, weakening the late-stage learning signal and limiting fine-grained discrimination in mixed-modality batches; and (3) Unstable ranking: inconsistent geometry across heterogeneous inputs makes rankings sensitive to small score changes, even when all items are embedded in single space. These issues become more significant in the omni-modal setting, because minibatches naturally mix diverse modalities and thus amplify scale and geometry mismatch. In this work, we propose lightweight explicit alignment method (e5-omni) that turns VLM backbones into robust omni-modal embedding models without modifying the architecture. As shown in the right part of Fig. 1, e5-omni has three components: (1) Modality-aware Temperature Calibration. We introduce lightweight calibration module that uses trainable per-modality scaling vector to calibrate similarity logits across modalities. This adaptive rescaling helps balance contrastive training signals across different modality compositions. (2) Controllable Negative Curriculum. We select in-batch negatives using quantile-based threshold and gradually increase hardness after warmup period. To mitigate the bias introduced by negative selection, we further incorporate debiased contrastive objective (Chuang et al., 2020), which stabilizes optimization in mixed-modality batches. (3) Batch Whitening and Covariance Alignment. We apply batch whitening transform (Ermolov et al., 2021) to the embeddings and add CORALstyle regularizer (Sun and Saenko, 2016) to align mini-batch covariances across modalities. This harmonizes second-order geometry in the shared space and leads to more consistent similarity comparisons across diverse omni-modal inputs. Empirically, e5-omni delivers consistent improvements on diverse embedding benchmarks, including MMEB-V2 (Meng et al., 2025) and the AudioCaps textaudio retrieval benchmark (Kim et al., 2019). Moreover, the gains grow with model scale: upgrading the backbone from 3B to 7B yields larger improvements. We also find that e5-omni transfers well to other VLM backbones. In summary, our contributions are threefold: We propose e5-omni, lightweight explicit alignment recipe that adapts generative VLMs into omni-modal embedding models without changing the backbone architecture. We introduce three simple plug-in components: modality-aware logit calibration, controllable negative curriculum with debiased contrastive learning, and batch whitening with covariance alignment, making omni-modal contrastive training more robust under mixedmodality batches. We achieve consistent gains on MMEB-V2 and AudioCaps over strong baselines, and show that the recipe benefits more from larger backbones and transfers to other VLMs."
        },
        {
            "title": "2 Method: e5-omni",
            "content": "In this section, we present e5-omni, lightweight explicit-alignment framework that transforms VLM into unified omni-modal embedding model. As shown in the left part of Fig. 2, e5-omni preserves the backbone and introduces simple training recipe. Our framework comprises three components: (1) Modality-aware temperature calibration, which balances contrastive sharpness across modalities; (2) controllable negative curriculum, which maintains strong and stable learning signal; and (3) Batch whitening and covariance alignment, which harmonizes the shared-space geometry under heterogeneous inputs. Together, these components improve the robustness of omni-modal embeddings without architectural modifications. 2.1 Preliminaries We study omni-modal embedding with base modality set M0 = {T, I, A, V}, corresponding to text, image, audio, and video. Training data consist of tuples (q, p+, ), where is query, p+ is its matched target, and is set of negIn this setting, q, p+, and each ative targets. can each take any non-empty modality composition: = xmq , p+ = xm+, = Figure 2: Overview of e5-omni. Given omni-modal inputs, e5-omni augments VLM backbone with three lightweight components: (1) Modality-aware temperature calibration computes modality-composition indicator w(x) and applies learnable modality-specific temperatures τ to calibrate logits ℓ(q, p); (2) Controllable negative curriculum progressively masks easy negatives and optimizes masked debiased objective LDCL; (3) Batch whitening and covariance alignment whitens batch embeddings and adds CORAL-style covariance regularizer. xm, mq, m+, 2M0 {}, where we denote an input with modality composition as xm. We consider two common sources of negatives: (i) in-batch negatives, e.g., other positive targets in the same mini-batch, and (ii) hard negatives, mined by retriever or constructed via heuristics. Our objective is to learn single embedding function e() : RD, where denotes the space of omni-modal inputs. The embedding maps any omni-modal input into shared representation space, so that matched pairs (q, p+) are close while mismatched pairs (q, p) are well separated. 2.2 Modality-aware Temperature Calibration single global temperature in contrastive learning implicitly assumes that all inputs induce similarity logits with comparable sharpness (Chen et al., 2020). In the omni-modal setting, different modalities can exhibit very different levels of ambiguity and noise. As result, fixed temperature can make some modality compositions overly sharp while leaving others overly flat, leading to imbalanced gradients and unstable optimization. To address this issue, we introduce lightweight modality-aware temperature module implemented as learnable per-modality scaling vector τ RM0. As shown in the top right part of Fig. 2, for any input x, we construct normalized modalityindicator weight w(x) M01 based on its modality composition. Concretely, w(x) assigns non-zero mass to every modality present in and is normalized by the number of active modalities. We then define an instance temperature as weighted sum τ (x) = max(cid:0)w(x)τ , 106(cid:1). Given any pair (q, p), we use symmetric pairwise temperature τ (q, p) = (τ (q) + τ (p)) /2, and compute the calibrated similarity logit as ℓ(q, p) = sim(e(q), e(p)) τ (q, p) . (1) We use ℓ(, ) consistently in the contrastive learning objective. Intuitively, larger τ flattens logits while smaller τ sharpens logits, so noisier/ambiguous modalities tend to learn larger temperatures. This stabilizes contrastive gradients in mixed omni-modal batches. 2.3 Controllable Negative Curriculum In omni-modal contrastive training, the hardness of negatives varies widely across batches and modalities, causing either weak supervision (overly easy negatives) or unstable optimization (false negatives). To address this hardness imbalance, we adopt simple curriculum that gradually increases the focus on confusing negatives. However, hardnegative selection is prone to selection bias, since the hardest samples often include near-duplicates or semantically related items (i.e., false negatives), which can destabilize learning. As shown in the middle right of Fig. 2, we therefore combine negative selection with Debiased Contrastive Learning (DCL) (Chuang et al., 2020), which down-weights the aggregated negative to reduce the impact of potential false negatives. }B )}B , For mini-batch = {(qi, p+ i=1, we form similarity matrix RB(B+K), where the first columns correspond to the in-batch paired targets {p+ j=1, i.e., in-batch negatives (Sij = ℓ(qi, p+ )), and the remaining columns correspond to mined hard negatives (Si,B+k = ℓ(qi, i,k)). The diagonal Sii is the positive for row i. We then mask negatives by keeping only the top fraction of the most confusing items per row. Concretely, at training step we use quantile threshold ρt [0, 1) and keep the hardest (1 ρt) fraction among all negatives in each row: for each i, let Ni(ρt) be the set of indices of the largest = (1ρt)(B+K1) values in {Sij}j=i. This produces boolean mask {0, 1}B(B+K) where Mij = 1 iff Ni(ρt) (and Mii = 0). Masked DCL objective. We apply the mask inside DCL loss by summing only over selected negatives. Let Ωi = {i}Ni(ρt). DCL corrects the bias of (masked) negative sampling by subtracting scaled positive term from the negative aggregate: LDCL = 1 (cid:88) i=1 log exp(Sii) exp(Sii) + (cid:101)Ni , (2) (cid:101)Ni = max (cid:16) (cid:88) exp(Sij) γ+ exp(Sii), ϵ (cid:17) , jΩi{i} where γ+ (0, 1) is the debiasing coefficient and ϵ is small constant for numerical stability. Here is computed from the calibrated logit ℓ(, ), hence no extra global temperature is required. Curriculum schedule. Recall that ρt denotes the mask ratio of negatives, so larger ρt means fewer negatives are kept (and the remaining set is harder). We therefore start with smaller ρt (keeping larger fraction of negatives, including easier ones), and linearly increase it after warmup period to gradually focus on fewer, harder negatives: ρt = ρinit + (ρfinal ρinit) clip (cid:18) t0 (cid:19) , 0, 1 , where is the training step, t0 is the warmup cutoff (the step after which we start increasing ρt, i.e., hardness), is the total number of training steps, and clip(x, 0, 1) = min(max(x, 0), 1). This curriculum lets the model first learn coarse omni-modal alignment with stable gradients, and then progressively resolve fine-grained confusions with genuinely hard negatives, while DCL mitigates the bias amplified by negative selection together yielding stable optimization and cleaner alignment under mixed-modality batches. 2.4 Batch Whitening and Covariance Alignment Representations produced by an omni-modal backbone often exhibit mismatched second-order statistics, which can distort similarity geometry and destabilize ranking (Su et al., 2021), especially when batches mix diverse modality compositions. To explicitly regularize the shared embedding space, we apply batch whitening (Ermolov et al., 2021) and add CORAL-style covariance alignment loss (Sun and Saenko, 2016) as regularizer. i=1, we compute embeddings with the shared encoder e() i=1 RBD, = and stack them as = [e(qi)]B [e(p+ i=1 RBD. We compute single whitening transform from the concatenation of query and target embeddings in the mini-batch, and apply the same transform to both sets: (cid:98)Q = (Q) and (cid:98)P = (P). We then define the empirical covariance operator for any batch embedding matrix RBD (e.g., (cid:98)Q or (cid:98)P) as For mini-batch = {(qi, p+ , )}B )]B Cov(X) = 1 1 (X X)(X X) , (3) where is the feature-wise mean over the batch. The CORAL loss is Lcoral = (cid:13) 2 (cid:13) (cid:13) With Eq.(2) and Eq.(4), the final objective is (cid:13) (cid:13) (cid:13)Cov( (cid:98)Q) Cov( (cid:98)P) 1 4D2 . (4) = LDCL + λcoralLcoral , (5) where λcoral controls the strength of covariance regularization. In practice, we implement () with numerically stable routine (e.g., group-wise computation with small jitter) to avoid instability under high-dimensional embeddings. By reducing second-order mismatch between (cid:98)Q and (cid:98)P, this regularizer improves geometric consistency in the shared space and stabilizes similarity-based ranking in omni-modal retrieval. Table 1: Overall results on MMEB-V2, which contains 78 tasks spanning three major groups: Image (36 tasks), Video (18 tasks), and VisDoc (24 visual document tasks). We report group-wise averages and the overall score across all tasks. We group prior baselines by their supported modality sets (bi-modal, tri-modal, and omni-modal) and additionally report e5-omni w/o. explicit alignment (vanilla contrastive) as strong baseline. We highlight the best score in bold and the second-best with an underline. denotes our model outperforms all baselines significantly in paired t-test at < 0.01 level (with Bonferroni correction). Model Size Image Video VisDoc All Text + Image (Bi-modal) ColPali-v1.3 (Faysse et al., 2025) VLM2Vec-v1 (Jiang et al., 2025) GME (Zhang et al., 2024) CAFe (Yu et al., 2025) Text + Image + Video VLM2Vec-v2 (Meng et al., 2025) UniME-V2 (Gu et al., 2025) UME-R1 (Lan et al., 2025) Text + Image + Video + Audio (Omni-modal) Tevatron-Omni (Ma et al., 2025) LCO-EMB (Xiao et al., 2025) Omni-Embed-Nemotron (Xu et al., 2025b) 3B 7B 7B 7B 2B 7B 7B 7B 7B 3B Ours w/o. explicit alignment (vanilla contrastive) e5-omni-vanilla-3B 3B e5-omni-vanilla-7B 7B Ours (Omni-modal) e5-omni-3B e5-omni-7B 3B 7B 34.9 65.5 56.0 67.6 64.9 71.8 71. 37.1 44.0 43.7 65.8 69.0 67.6 71.2 28.2 33.7 38.4 42.4 34.6 39.0 47.5 35.1 38.2 36. 39.3 41.8 40.6 43.5 71.0 46.4 75.2 63.9 65.4 56.7 67.1 74.5 69.8 74.2 72.0 74. 73.2 76.1 44.4 52.3 57.8 60.6 58.0 59.6 64.5 48.1 50.6 51.5 62.0 64.4 63.1 66."
        },
        {
            "title": "3 Experiments",
            "content": "3.1.2 Implementation Details. 3.1 Experimental Setup 3.1.1 Training Datasets. We train e5-omni on heterogeneous mixture of omni-modal retrieval pairs that cover diverse modality compositions. In particular, the training data include: (1) text-only contrastive pairs from BGE-m3 (Chen et al., 2024); (2) textimage pairs from the MMEB-V1 training set (Jiang et al., 2025) and PixMo-Docs (Deitke et al., 2025); (3) text video retrieval pairs from MSR-VTT (Xu et al., 2016) and the MMEB-V2 training set (Meng et al., 2025); (4) textaudio retrieval pairs from AudioCaps (Kim et al., 2019); (5) visual-document retrieval pairs from MMEB-V2 (Meng et al., 2025). Overall, this mixture exposes the model to queries/ targets that are uni-modal, bi-modal, or composed of multiple modalities, which is critical for learning unified omni-modal embedding space. We adopt Qwen2.5-Omni (Xu et al., 2025a) as the VLM backbone and fine-tune it with LoRA (Hu et al., 2022) for parameter-efficient adaptation. We set the maximum sequence length to 512 tokens for both queries and targets, and train for one epoch with learning rate of 1 104 and warmup ratio of 0.005. Training is performed on 8 H100 GPUs with per-device batch size of 20 and gradient accumulation of 2. For each dataset, we use two dataset-provided hard negatives per query. For our method-specific components, we initialize the trainable modality scaling vector τ to 0.02. After training, the learned temperatures are τ = [0.0130, 0.0127, 0.0219, 0.0223] for {T, I, A, V}, respectively. We set the debiasing coefficient γ+ = 0.1, ρinit = 0.1, ρfinal = 0.5, and use t0 = 4000 warmup steps before increasing hardness. For batch whitening and covariance alignment, we set λcoral = 0.05. We report the settings of e5-omni-7B. More details are in Appendix B. Table 2: Overall results on AudioCaps. We only include omni-modal embedding models that are trained with audio data. denotes our model outperforms all baselines significantly in paired t-test at < 0.05 level (with Bonferroni correction). Model Size Recall@1 Baselines Tevatron-Omni LCO-EMB Omni-Embed-Nemotron Ours e5-omni-3B e5-omni-7B 7B 7B 3B 3B 7B 34.0 24.2 20.5 34.3 37.7 3.1.3 Evaluation. We evaluate e5-omni on two benchmarks. First, we use MMEB-V2 (Meng et al., 2025), largescale multimodal embedding benchmark covering text, image, video, and visual documents. It contains 9 meta-tasks and 78 tasks spanning diverse categories such as retrieval, classification, question answering, and visual document retrieval. Following the MMEB-V2 protocol, we use Hit@1 as the primary metric for image/video tasks and report NDCG@5 for visual-document tasks. Second, we evaluate audio retrieval on AudioCaps (Kim et al., 2019), which contains about 4.4K textaudio pairs. We report Recall@1, i.e., the fraction of text queries whose matched audio is ranked at the top. 3.2 Overall Results Table 1 reports overall performance on MMEB-V2, and Table 2 reports AudioCaps textaudio retrieval results. e5-omni outperforms strong bi-modal and omni-modal baselines on both benchmarks, supporting the effectiveness of our framework. We highlight three observations: (1) Broad gains across modalities and tasks. On MMEBV2, e5-omni improves image/video retrieval and also yields clear gains on the visual-document retrieval subset, suggesting that explicit alignment remains beneficial under heterogeneous, long-form (2) Improved audio retrieval. visual inputs. On AudioCaps, e5-omni-7B achieves higher Recall@1 than omni-modal baselines, indicating that our alignment components generalize beyond vision-language and improve the stability of audio embeddings. (3) Scaling with model size. The alignment gains grow when scaling from 3B to 7B (All: +1.1 vs. +2.0), suggesting that explicit alignment is complementary to model scaling. Table 3: Ablation of e5-omni. We remove one component at time from the full model and report results on MMEB-V2 and AudioCaps. Model e5-omni-7B w/o. Modality-aware Temp. w/o. Curriculum Schedule w/o. DCL (w/ Curriculum) w/o. Whitening & CORAL MMEB-V2 AudioCaps 66.4 65.7 65.7 66.1 65. 37.7 36.6 36.7 37.0 36.3 3.3 Ablation Study To understand the contribution of each component in e5-omni, we conduct ablation studies by removing one design choice at time while keeping all other settings fixed. We report results on MMEBV2 and AudioCaps in Table 3. Overall, removing any component degrades performance, indicating that the proposed techniques are complementary. Modality-aware Temperature Calibration. Disabling modality-aware temperature calibration (i.e., using single global temperature of 0.02, matching the initialization of τ ) causes clear performance drop. This suggests that calibrating modalitydependent logit sharpness stabilizes contrastive optimization under mixed omni-modal batches. Controllable Negative Curriculum. We ablate the negative curriculum by (i) removing the curriculum schedule and using fixed threshold 0.3, or (ii) removing DCL while keeping curriculum. Both variants underperform e5-omni, indicating that progressively increasing negative hardness helps avoid unstable early training while improving finegrained discrimination later. We further observe additional degradation when removing DCL, suggesting that debiasing is important when hard-negative selection amplifies false-negative bias. Batch Whitening and Covariance Alignment. Removing whitening and the covariance regularizer also reduces performance on both benchmarks. This supports our motivation that regularizing second-order statistics improves geometric consistency in the shared embedding space and stabilizes similarity-based ranking. 3.4 Embedding-space Modality Alignment Diagnostics Retrieval metrics summarize ranking quality but do not reveal whether the embedding space is distributionally consistent across modalities. We therefore run lightweight distribution-level diagnostic on VOC2007, clean textimage setting for inspecting Figure 3: PCA overlap on VOC2007. Left: e5-omni w/o. alignment. Right: e5-omni. We project embeddings into shared 2D PCA space and overlay 2σ covariance ellipses. We report centroid (distance between the query/target mean embeddings) and covgap (Frobenius gap between their covariance matrices). Figure 4: Covariance-difference heatmap on VOC2007. Left: e5-omni w/o. alignment. Right: e5-omni. After fixed 32D random projection, we visualize covdiff : the entrywise magnitude of the querytarget covariance difference matrix. cross-modal embedding mismatch. We compare e5-omni with e5-omni w/o. alignment (7B), which disables all three techniques in Sec. 2 while keeping the same backbone and embedding dimension. Setup. We sample 1k text queries and 2k image targets, extract embeddings with e(), and stack them as RNqD and RNpD. Image targets are drawn from the intersection of candidate pools for both checkpoints. We compute empirical means and Cov() as in Eq. (3). Diagnostics. PCA overlap. We fit single PCA basis on the union of and from both models, project all embeddings to 2D, and visualize the query/target clouds with 2σ covariance ellipses  (Fig. 3)  . We quantify distribution mismatch using the centroid gap µQ µP2 and the covariance gap Cov(Q)Cov(P)F . Covariance-difference heatmap. To inspect second-order mismatch more directly, we apply fixed random Gaussian projection to 32D and visualize the entrywise magnitude  (Fig. 4)  , together with its Frobenius norm. Results and interpretation. Compared to e5-omni w/o. alignment, the full e5-omni yields smaller querytarget mismatch in embedding space: the centroid gap and the covariance gap decrease in Fig. 3. The covariance-difference heatmap in Fig. 4 further indicates reduced secondorder discrepancy with fewer high-magnitude entries. While the heatmap mainly reflects second-order geometry (most directly tied to whitening/covariance regularization), the PCA overlap reflects both firstand second-order effects and may also be influenced by training dynamics (e.g., modality-aware logit calibration and more these stable negative supervision). Overall, diagnostics provide distribution-level evidence that the combined design of e5-omni promotes more consistent shared embedding space, complementing the benchmark results. 3.5 Hyperparameter Analysis We analyze the sensitivity of e5-omni to key hyperparameters in our explicit alignment recipe. We tune these hyperparameters on 1K-sample validation splits drawn from the corresponding training sets. For consistency with prior experiments, we report results on the MMEB-V2 test sets in Fig. 5. Additional studies are deferred to Appendix C. Modality-aware temperature initialization. Our modality-aware calibration introduces trainable per-modality scaling vector τ (Sec. 2.2). We sweep the initialization value τ0 (i.e., initializing all entries of τ to the same constant τ0). We observe trade-off: overly small τ0 yields overly sharp logits early in training, while large τ0 produces overly flat logits and slows convergence. Overall, initializing τ in small range (e.g., τ0 [0.015, 0.03]) is robust, and we use τ0 = 0.02 by default. DCL debiasing coefficient. We sweep the DCL parameter γ+ (Sec. 2.3), which controls the strength of debiasing under negative selection. Moderate values work best: too small γ+ provides limited correction, while too large γ+ can over-correct and weaken supervision. In our experiments, γ+ [0.1, 0.2] yields consistent improvements on MMEB-V2, and we use γ+ = 0.1 by default. Covariance regularization weight. Finally, we vary the CORAL weight λcoral (Sec. 2.4). Increasing λcoral generally improves cross-modal geometric consistency, but overly large weights can interfere with the retrieval objective. We observe that small auxiliary weight (e.g., λcoral [0.02, 0.1]) provides the best trade-off. Table 4: Generalization of e5-omni across different VLM backbones on MMEB-V2. w/o. alignment disables all alignment components. VLM Backbone Size Method MMEB-V2 Qwen2.5-VL Qwen2-VL LLaVA-OV 3B 2B 7B w/o. alignment e5-omni w/o. alignment e5-omni w/o. alignment e5-omni 61.5 62.9 59.2 60.5 63.7 65.4 Gu et al., 2025). For example, UME-R1 (Lan et al., 2025) explores reasoning-driven generative embedding paradigm that leverages reinforcement learning. More recent efforts further expand to video and audio modalities (Ma et al., 2025; Xu et al., 2025b; Xiao et al., 2025). For example, Omni-Embed-Nemotron (Xu et al., 2025b) builds unified embedding model on Qwen-Omni backbone to support omni-modal retrieval in single shared space. Similarly, LCO-Embed (Xiao et al., 2025) proposes language-centric omni-modal embedding framework. While these omni-modal models broaden coverage, they largely rely on VLM pretraining to provide implicit alignment and typically lack explicit calibration and alignment mechanisms. This limitation becomes more critical as the number of supported modalities grows. Unlike these models, e5-omni introduces lightweight, plug-and-play explicit alignment recipe to improve robustness under mixed-modality training."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced e5-omni, lightweight explicitalignment recipe that turns off-the-shelf VLM backbones into unified omni-modal embedding models. e5-omni keeps the backbone unchanged and adds three simple components: modality-aware temperature calibration to balance logit sharpness across modality compositions; controllable negative curriculum (with debiasing) to stabilize contrastive optimization under mixed-modality batches; and batch whitening with covariance alignment to harmonize second-order geometry in the shared space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong baselines, indicating that explicit alignment yields more robust omnimodal embeddings with minimal overhead. Figure 5: The performances of e5-omni-7B under different training settings on MMEB-V2. We report the overall scores using same metric as in Table 1. 3.6 Generalization to Other VLM Backbones To validate that e5-omni is not tied to specific backbone, we apply our explicit alignment recipe to multiple off-the-shelf VLMs with different sizes and architectures. For each backbone, we first train strong baseline w/o. alignment (as defined in Sec. 3.4), and then train the full e5-omni by enabling all three techniques. We report results on MMEB-V2 in Table 4. We omit AudioCaps because only limited number of public VLM backbones support audio inputs out of the box, making broad backbone comparison infeasible. Overall, e5-omni consistently improves over the corresponding baselines across all backbones, demonstrating that our method is plug-andplay strategy for turning diverse VLM backbones into robust omni-modal embedding models."
        },
        {
            "title": "4 Related Work",
            "content": "Bi-modal Embedding Models. Early vision language embedding models primarily focus on aligning text and images in shared space, typically via dual-encoder contrastive learning (Jia et al., 2021; Radford et al., 2021; Li et al., 2022, 2023). For example, CLIP (Radford et al., 2021) learns image and text encoders by matching paired imagecaption data at scale. More recently, line of work builds retrieval-oriented bi-encoders on top of modern VLMs (Jiang et al., 2025; Zhang et al., 2024; Chen et al., 2025b; Thirukovalluru et al., 2025; Chen et al., 2025a), converting generative or instruction-following backbones into embedding models. For example, mmE5 (Chen et al., 2025b) improves textimage embedding quality through data synthesis. Despite strong progress on text image representations, these bi-modal models are not designed to fully meet modern retrieval needs that increasingly involve additional modalities. Omni-modal Embedding Models. Moving beyond textimage, recent work has started to incorporate video (Meng et al., 2025; Lan et al., 2025;"
        },
        {
            "title": "Limitations",
            "content": "While e5-omni achieves strong omni-modal embedding performance with minimal overhead, several limitations remain: 1. Scope of alignment. Our recipe primarily targets similarity geometry and optimization dynamics for retrieval. It does not explicitly improve higher-level reasoning or compositional understanding, so gains may be smaller on tasks that require multi-step inference beyond embedding similarity. 2. Dependence on batch statistics. Batch whitening and covariance alignment rely on mini-batch estimates, whose quality can vary with batch size, modality composition, and distributed training settings. Although we apply stabilizing tricks (e.g., jitter and shrinkage), these estimates may still be noisy for small or highly imbalanced omni-modal batches. 3. Training and evaluation coverage. We evaluate on MMEB-V2 and AudioCaps, which cover text/image/video and textaudio retrieval. Broader evaluation on more diverse audio/video domains, long-horizon retrieval settings, and real-world multimodal corpora would further strengthen the conclusions."
        },
        {
            "title": "References",
            "content": "Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, and Zhicheng Dou. Moca: Modality-aware continual pre-training makes better bidirectional multimodal embeddings. CoRR, abs/2506.23115, 2025a. doi: 10.48550/ARXIV.2506. 23115. URL https://doi.org/10.48550/arXiv. 2506.23115. Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. CoRR, abs/2502.08468, 2025b. doi: 10.48550/ARXIV.2502.08468. URL https://doi.org/10.48550/arXiv.2502.08468. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. doi: 10.48550/ARXIV. 2402.03216. URL https://doi.org/10.48550/ arXiv.2402.03216. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597 1607. PMLR, 2020. URL http://proceedings. mlr.press/v119/chen20j.html. Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 63c3ddcc7b23daa1e42dc41f9a44a873-Abstract. html. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, Yen-Sung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross B. Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language In IEEE/CVF Conference on Computer models. Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 91104. Computer Vision Foundation / IEEE, 10.1109/CVPR52734.2025.00018. 2025. https://openaccess.thecvf.com/ URL content/CVPR2025/html/Deitke_Molmo_and_ PixMo_Open_Weights_and_Open_Data_for_ State-of-the-Art_CVPR_2025_paper.html. doi: Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for selfsupervised representation learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 30153024. PMLR, URL http://proceedings.mlr.press/ 2021. v139/ermolov21a.html. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with In The Thirteenth Intervision language models. national Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/ forum?id=ogjBpZ8uSi. Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Tom Weidong Cai, Jiankang Deng, and Lidong Bing. Unimev2: Mllm-as-a-judge for universal multimodal embedding learning. CoRR, abs/2510.13515, 2025. doi: 10.48550/ARXIV.2510.13515. URL https: //doi.org/10.48550/arXiv.2510.13515. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= nZeVKeeFYf9. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904 4916. PMLR, 2021. URL http://proceedings. mlr.press/v139/jia21b.html. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= TE0KOzWYAF. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 119132. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1011. URL https://doi.org/ 10.18653/v1/n19-1011. Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. UME-R1: exploring reasoningdriven generative multimodal embeddings. CoRR, abs/2511.00405, 2025. doi: 10.48550/ARXIV.2511. 00405. URL https://doi.org/10.48550/arXiv. 2511.00405. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 12888 12900. PMLR, 2022. URL https://proceedings. mlr.press/v162/li22n.html. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 2329 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2023. URL https: //proceedings.mlr.press/v202/li23q.html. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 64926505. Association for Computational Linguistics, 2024. URL https://aclanthology. org/2024.emnlp-main.373. Xueguang Ma, Luyu Gao, Shengyao Zhuang, Jiaqi Samantha Zhan, Jamie Callan, and Jimmy Lin. Tevatron 2.0: Unified document retrieval toolkit across scale, language, and modality. In Nicola Ferro, Maria Maistro, Gabriella Pasi, Omar Alonso, Andrew Trotman, and Suzan Verberne, editors, Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2025, Padua, Italy, July 1318, 2025, pages 40614065. ACM, 2025. doi: 10. 1145/3726302.3730135. URL https://doi.org/ 10.1145/3726302.3730135. Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Yingbo Zhou, Wenhu Chen, and Semih Yavuz. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. CoRR, abs/2507.04590, 2025. doi: 10.48550/ARXIV.2507.04590. URL https://doi. org/10.48550/arXiv.2507.04590. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms, 2024. URL http://arxiv.org/abs/2412.16855. July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748 8763. PMLR, 2021. URL http://proceedings. mlr.press/v139/radford21a.html. Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better semantics and faster retrieval. CoRR, abs/2103.15316, 2021. URL https://arxiv.org/ abs/2103.15316. Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation. In Gang Hua and Hervé Jégou, editors, Computer Vision - ECCV 2016 Workshops - Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III, volume 9915 of Lecture Notes in Computer Science, pages 443450, 2016. doi: 10.1007/978-3-319-49409-8_35. URL https:// doi.org/10.1007/978-3-319-49409-8_35. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293, 2025. Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, and Yu Rong. Scaling language-centric omnimodal representation learning, 2025. URL https://arxiv.org/abs/2510. 11693. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. CoRR, abs/2503.20215, 2025a. doi: 10.48550/ARXIV.2503. 20215. URL https://doi.org/10.48550/arXiv. 2503.20215. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSRVTT: large video description dataset for bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 52885296. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.571. URL https://doi.org/ 10.1109/CVPR.2016.571. Mengyao Xu, Wenfei Zhou, Yauhen Babakhin, Gabriel de Souza Pereira Moreira, Ronay Ak, Radek Osmulski, Bo Liu, Even Oldridge, and Benedikt Schifferer. Omni-embed-nemotron: unified multimodal retrieval model for text, image, audio, and video. CoRR, abs/2510.03458, 2025b. doi: 10.48550/ ARXIV.2510.03458. URL https://doi.org/10. 48550/arXiv.2510.03458. Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, and Hanchao Yu. Cafe: Unifying representation and generation with contrastiveautoregressive finetuning. CoRR, abs/2503.19900, 2025. doi: 10.48550/ARXIV.2503.19900. URL https://doi.org/10.48550/arXiv.2503.19900. controllable negative selection (Sec. 2.3). Unless otherwise specified, all sweeps are conducted on the 7B backbone with fixed training budget. We select hyperparameters on validation sets containing 1K samples drawn from the corresponding training sets, and report results on the MMEB-V2 test set for consistency with the main experiments  (Fig. 6)  . Curriculum start ρinit. We vary the initial quantile threshold ρinit, which controls how many negatives are kept at the beginning of training. Very small ρinit retains many easy negatives and yields weaker supervision, while overly large ρinit can introduce hard (and potentially noisy) negatives too early. As shown in Fig. 6, performance peaks at moderate start (ρinit = 0.10) and drops on both sides, indicating that balanced early hardness level is important. Curriculum end ρfinal. We sweep the final threshold ρfinal, which determines the hardness level near the end of training. Increasing ρfinal improves performance up to ρfinal = 0.50, while more aggressive settings yield diminishing returns and eventually degrade performance. We use ρfinal = 0.50 by default. Warmup cutoff t0. Finally, we vary the warmup cutoff t0 (in steps) before increasing hardness. Starting the curriculum too early (t0 = 0) underperforms, while moderate warmup improves stability and final accuracy. We observe the best performance at t0 = 4000 steps, and use t0 = 4000 by default."
        },
        {
            "title": "Appendix",
            "content": "A Detailed Results on MMEB-V2 We report detailed results of e5-omni and strong baselines on MMEB-V2, which contains 78 tasks spanning three modality groups: Image, Video, and Visual Document (VisDoc). Table 5 summarizes overall performance and provides breakdown of the Image group (36 tasks; Hit@1), including both meta-task averages and per-dataset results. Table 6 presents detailed Video results (18 tasks; Hit@1). Table 7 reports detailed VisDoc results (24 tasks; NDCG@5), covering documentlevel retrieval benchmarks such as ViDoRe. For readability and to fit the appendix layout, we split the detailed results into three tables."
        },
        {
            "title": "B Implementation Details",
            "content": "This section complements the main implementation setup in Sec. 3.1.2 by documenting practical details that are important for faithful reproduction beyond scalar hyperparameters. Whitening and covariance regularization. We compute batch statistics from the concatenation of query and target embeddings, derive single whitening transform, and apply it to both sets before regularizing second-order mismatch via CORAL-style Frobenius penalty (Eq. 4). To ensure numerical stability under high-dimensional embeddings, we implement the whitening operator with robust routine that performs group-wise computation and injects small jitter δ = 104 into the covariance estimate when needed. Debiased negative training. Our contrastive objective uses debiased contrastive learning to mitigate false-negative bias under negative selection. In practice, we apply the debiasing term jointly with the curriculum-based masking strategy: early training avoids overly aggressive hard negatives, while later stages gradually increase negative pressure. Hyperparameter selection protocol. For all hyperparameter sweeps reported in the paper, we tune on lightweight validation splits consisting of 1K samples drawn from the corresponding training sets, and report results on the MMEB-V2 test set(s) for consistency with prior work."
        },
        {
            "title": "C Additional Hyperparameter Studies",
            "content": "This appendix complements Sec. 3.5 with additional sweeps on the negative curriculum used for Table 5: Detailed results on the full MMEB-v2 benchmark: summary and image tasks (36 tasks, Hit@1). GME VLM2Vec-v1 VLM2Vec-v2 CAFe UME-R1 LCO-EMB Omni-Embed-Nemotron Model Size Avg - All (78 tasks) Avg - Image (36 tasks, Hit@1) Avg - Video (18 tasks, Hit@1) Avg - Visdoc (24 tasks, NDCG@5) I-CLS (10) I-QA (10) I-RET (12) I-VG (4) V-CLS (5) V-QA (5) V-RET (5) V-MR (3) VD-Vidore-V1 (10) VD-Vidore-V2 (4) VD-VisRAG (6) VD-OOD (4) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country211 OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS MSCOCO RefCOCO RefCOCO-Matching Visual7W-Pointing 7B 57.8 56.0 38.4 75.2 57.7 34.7 71.2 59.3 37.4 50.4 28.4 37.0 89.4 55.6 85.0 44.4 64.6 50.5 53.6 80.3 69.5 39.1 41.2 83.9 69.0 24.8 33.2 21.0 41.4 20.3 17.8 22.2 28.0 39.0 76.9 46.8 60.8 54.9 79.7 83.6 71.2 57.7 67.6 91.4 37.8 78.2 75.1 96.0 31.4 60.9 78.4 66.5 7B 52.3 65.5 33.7 46.4 62.7 56.9 69.4 82.2 39.1 30.0 29.0 38.9 56.9 9.4 59.1 38.1 80.1 79.7 69.7 80.7 77.4 37.4 58.1 73.9 40.1 29.8 56.8 47.3 89.7 60.0 56.9 52.7 38.5 39.9 55.1 71.6 81.9 51.1 80.5 81.2 77.2 73.9 67.6 88.3 17.1 62.3 66.5 85.7 75.7 87.6 84.6 81. 2B 58.0 64.9 34.6 65.4 62.9 56.3 69.5 77.3 39.3 34.3 28.8 36.8 75.5 44.9 79.4 39.4 80.8 72.9 56.3 85.0 71.0 35.9 47.4 89.3 65.2 25.2 51.5 43.6 90.1 58.8 47.4 52.9 38.2 43.3 64.9 72.2 82.7 57.5 74.5 78.2 75.3 71.4 68.6 90.6 19.5 66.9 64.3 84.1 67.1 87.1 85.8 69.2 7B 60.6 67.6 42.4 63.9 63.6 61.7 69.1 87.6 35.8 58.7 34.4 39.5 70.7 49.6 79.5 38.1 77.3 83.2 78.7 89.8 79.9 45.0 55.2 88.0 22.5 16.7 67.3 63.8 79.2 53.3 48.8 52.5 65.4 43.8 65.7 76.8 82.7 60.4 69.5 79.4 75.4 73.1 66.7 89.3 39.0 61.2 60.8 71.3 84.7 89.4 83.0 93. 7B 64.5 71.3 47.5 67.1 67.1 69.2 71.9 84.9 48.6 60.7 38.2 39.3 75.7 50.5 83.7 37.6 80.4 82.3 79.0 90.8 80.3 46.8 53.9 90.1 42.3 25.0 71.7 58.7 93.8 79.2 75.1 55.2 53.7 51.6 69.3 83.5 80.7 55.3 76.8 82.0 78.3 71.4 68.1 90.9 23.4 72.5 71.4 92.0 72.7 91.4 91.1 84.2 7B 50.6 44.0 38.2 69.8 56.3 16.9 52.2 57.0 39.3 57.6 24.8 26.5 80.4 56.4 79.7 41.8 73.0 37.5 45.1 64.9 72.0 40.8 52.2 88.9 61.7 26.5 14.4 10.9 17.5 7.4 12.7 10.8 26.7 31.8 11.5 25.1 51.1 8.4 61.4 59.5 66.1 59.5 65.2 53.2 3.5 64.2 59.9 74.4 33.8 50.9 98.5 44. 3B 51.5 43.7 36.9 74.2 46.0 20.5 58.0 52.9 40.5 44.3 32.7 25.6 85.7 61.3 84.3 43.3 57.0 36.4 48.0 47.9 53.1 30.7 37.0 80.9 44.9 24.1 18.6 11.9 17.3 9.1 13.1 7.4 25.3 34.8 31.2 36.6 51.5 14.8 58.8 56.3 60.7 56.5 65.3 89.8 6.2 87.7 67.9 80.5 32.8 59.7 65.9 53.1 e5-omni 3B e5-omni 7B 63.1 67.6 40.6 73.2 64.7 62.1 70.5 79.6 40.2 48.5 33.1 40.7 83.1 59.5 85.5 43. 76.5 76.7 66.2 84.8 75.1 41.7 58.6 85.5 53.0 29.1 64.9 54.8 91.2 52.8 52.7 59.8 43.6 53.1 65.4 82.8 78.9 54.9 74.6 77.7 78.1 72.5 68.4 89.2 19.5 74.9 76.1 81.7 70.9 85.3 75.8 86.5 66.4 71.2 43.5 76.1 66.7 68.5 73.0 83.9 46.6 52.9 36.7 34.2 87.6 62.4 87.5 44.3 79.4 77.2 72.3 83.9 76.1 44.3 57.2 84.5 57.7 34.7 70.5 59.7 93.3 62.9 62.8 65.2 56.4 56.3 72.1 86.2 82.4 54.9 75.3 79.8 76.6 76.9 68.6 89.3 24.5 79.4 78.7 89.6 74.8 89.4 87.2 84.3 Figure 6: Sensitivity to negative curriculum settings on MMEB-V2 (7B backbone). Table 6: Detailed results on the full MMEB-v2 benchmark: video tasks (18 tasks, Hit@1). GME VLM2Vec-v1 VLM2Vec-v2 CAFe UME-R1 LCO-EMB Omni-Embed-Nemotron Model Size 7B 39.7 K700 SmthSmthV2 30.6 47.9 HMDB51 54.7 UCF101 14.3 Breakfast 46.6 MVBench 39.2 Video-MME 53.6 NExTQA EgoSchema 46.8 ActivityNetQA 65.6 26.4 DiDeMo 31.8 MSR-VTT 49.7 MSVD 24.9 VATEX 9.1 YouCook2 59.5 QVHighlight 14.0 Charades-STA 37.4 MomentSeeker 7B 35.5 32.1 42.2 61.8 23.8 28.5 27.8 20.3 21.8 51.4 29.3 34.5 46.7 25.5 9.0 57.7 19.8 39.3 2B 38.0 42.8 40.9 60.0 14.8 33.7 30.7 20.9 34.0 52.3 30.4 28.3 48.1 26.5 10.6 49.4 20.2 40.8 7B 40.1 35.8 46.9 39.6 16.6 48.9 46.0 62.4 60.0 76.0 37.8 36.5 56.4 32.0 9.5 58.4 18.7 41.4 7B 42.8 50.4 58.3 70.0 21.5 58.2 47.3 69.6 52.4 76.0 40.0 38.9 60.8 32.6 18.5 54.9 21.9 41.1 7B 40.2 33.3 37.3 66.0 19.9 58.1 48.5 65.5 40.4 75.3 26.9 22.0 50.7 18.8 5.4 20.0 30.0 29.5 3B 36.0 33.4 49.1 60.5 23.3 40.6 38.6 41.2 37.6 63.4 34.7 35.4 54.0 25.7 13.8 29.1 11.6 36.1 e5-omni 3B e5-omni 7B 41.9 34.4 42.2 58.0 24.5 45.3 43.6 54.4 36.6 62.8 34.8 36.0 52.7 28.6 13.7 62.6 18.2 41. 48.0 41.3 54.4 66.0 23.3 50.3 45.6 62.1 39.0 67.7 36.1 42.1 57.2 33.5 14.5 42.4 17.9 42.3 Table 7: Detailed results on the full MMEB-v2 benchmark: VisDoc tasks (24 tasks, NDCG@5). GME VLM2Vec-v1 VLM2Vec-v2 CAFe UME-R1 LCO-EMB Omni-Embed-Nemotron Model Size ViDoRe_arxivqa ViDoRe_docvqa ViDoRe_infovqa ViDoRe_tabfquad ViDoRe_tatdqa ViDoRe_shiftproject ViDoRe_artificial_intelligence ViDoRe_energy ViDoRe_government_reports ViDoRe_healthcare_industry ViDoRe_esg_reports_human_labeled_v2 ViDoRe_biomedical_lectures_v2_multilingual ViDoRe_economics_reports_v2_multilingual ViDoRe_esg_reports_v2_multilingual VisRAG_ArxivQA VisRAG_ChartQA VisRAG_MP-DocVQA VisRAG_SlideVQA VisRAG_InfoVQA VisRAG_PlotQA ViDoSeek-page ViDoSeek-doc MMLongBench-page MMLongBench-doc 7B 86.9 57.5 91.6 94.6 74.1 96.8 99.6 95.3 98.8 99.3 63.4 49.5 54.2 55.4 87.4 81.9 89.2 94.5 93.5 63.4 23.2 83.9 16.2 54.3 7B 60.2 34.7 70.4 78.2 27.6 38.6 67.7 60.4 61.8 69.9 6.8 5.1 13.9 11.9 52.6 70.2 52.8 72.8 72.0 34.4 22.3 77.8 11.8 40.5 2B 80.6 44.9 83.7 89.2 43.8 60.8 88.5 86.5 85.0 92.2 45.6 44.3 43.0 46.6 76.9 84.4 71.8 91.5 85.7 66.1 21.9 80.2 11.9 43.7 7B 73.3 38.3 80.6 80.7 37.8 52.0 86.0 84.8 85.0 88.4 50.7 50.9 54.3 42.3 74.0 82.7 75.1 87.6 87.9 69.4 22.5 73.8 13.3 42.6 7B 73.6 41.1 80.8 90.2 46.7 65.0 89.5 85.7 89.8 94.3 50.4 50.7 57.8 43.2 80.5 85.0 83.4 91.5 89.2 72.7 21.3 75.3 12.3 41.3 7B 79.1 50.8 84.5 92.0 58.7 72.2 92.6 88.1 91.6 94.8 60.6 60.8 57.2 47.0 76.6 86.4 78.0 90.3 87.7 59.5 23.0 80.8 14.9 48.6 3B 85.3 57.8 89.8 91.7 70.2 80.2 97.0 92.6 95.6 96.7 68.0 60.2 63.5 53.3 81.3 86.5 85.1 95.1 91.6 66.1 21.9 83.5 15.1 52.6 e5-omni 3B e5-omni 7B 82.2 52.0 88.9 88.9 58.8 80.1 98.2 93.2 93.2 95.9 61.8 60.9 64.4 51.0 84.5 86.8 85.5 95.4 92.5 68.2 22.4 83.3 15.0 52.8 87.6 58.0 92.3 93.2 70.9 85.8 98.2 95.0 96.3 98.7 64.0 64.5 60.3 60.7 88.0 89.0 89.1 96.0 93.5 69.2 23.1 83.4 15.8 55."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "University of Würzburg",
        "Waseda University"
    ]
}