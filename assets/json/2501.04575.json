{
    "paper_title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
    "authors": [
        "Yuhang Liu",
        "Pengxiang Li",
        "Zishu Wei",
        "Congkai Xie",
        "Xueyu Hu",
        "Xinchen Xu",
        "Shengyu Zhang",
        "Xiaotian Han",
        "Hongxia Yang",
        "Fei Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \\url{https://github.com/Reallm-Labs/InfiGUIAgent}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 7 5 4 0 . 1 0 5 2 : r InfiGUIAgent: Multimodal Generalist GUI Agent with Native Reasoning and Reflection Yuhang Liu1, Pengxiang Li2, Zishu Wei1, Congkai Xie3, Xueyu Hu1, Xinchen Xu1, Shengyu Zhang1, Xiaotian Han 4, Hongxia Yang5, Fei Wu1 1Zhejiang University, 2Dalian University of Technology, 3Reallm Labs, 4ByteDance Inc, 5The Hong Kong Polytechnic University sy_zhang@zju.edu.cn, xiaotian.han@bytedance.com, hongxia.yang@polyu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce InfiGUIAgent, an MLLM-based GUI Agent trained with twostage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectationreflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. InfiGUIAgent achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at https://github. com/Reallm-Labs/InfiGUIAgent."
        },
        {
            "title": "Introduction",
            "content": "Graphical User Interface (GUI) Agents have emerged as powerful tools for automating tasks on computing devices, including mobile phones and computers. These agents can understand and interact with GUIs to execute complex operations, significantly enhancing user productivity and expanding the scope of automated task completion (Hu et al., 2024b; Hong et al., 2024; Zhang and Zhang, 2023; Qi et al., 2024; Xie et al., 2024; Vu et al., 2024; Yu et al., 2024; Wen et al., 2023). Recent developments in multimodal large language models (MLLMs) (Bai et al., 2023b; Li et al., 2024c; Team et al., 2024; Dai et al., 2022) have significantly advanced the potential of GUI Agents. MLLMs possess powerful visual understanding capabilities and can reason based on visual information, making them promising foundation for building sophisticated GUI Agents. These models can 1 interpret complex interface elements and adapt to wide range of tasks, leading to more efficient and robust automation (Hong et al., 2024; Jiang et al., 2023; You et al., 2025; Nong et al., 2024; Vu et al., 2024). However, current MLLM-based GUI Agents face several critical challenges. key limitation lies in their reasoning capabilities (Zhang and Zhang, 2023; Qi et al., 2024; Yu et al., 2024). While many existing GUI Agents can perform basic single-step reasoning, they struggle to effectively leverage information from previous steps. This lack of reflection on past experiences can lead to repetitive errors during task execution. Another significant challenge lies in the reliance on the additional information of the GUIs. Many prior GUI Agent implementations rely on accessibility trees or Set-of-Marks (Yang et al., 2023a), to represent or augment the GUIs visual information. However, GUIs are inherently visual, and representing them primarily through text can lead to information loss or redundancy. Augmenting visual input with textual descriptions can also increase computational overhead. Furthermore, the availability and consistency of these textual representations vary across platforms, hindering practical deployment. To address these limitations, we propose InfiGUIAgent, which is MLLM-based GUI Agent trained through two-stage supervised fine-tuning (SFT) methods with robust fundamental capabilities and native reasoning abilities. In stage 1, we collect data covering multiple tasks, such as visionlanguage understanding, GUI-specific QA, and tool use to improve fundamental capabilities such as GUI understanding and instruction grounding of the agents. In stage 2, we recognized two essential reasoning skills for GUI Agents: (1) Hierarchical reasoning, and (2) Expectation-reflection reasoning, and integrate these skills into the SFT data synthesized by MLLMs based on existing trajectories. Our main contributions are threefold: We propose two-stage supervised finetuning pipeline to comprehensively improve both the fundamental abilities and advanced reasoning abilities of GUI Agents. We synthesize SFT data with two advanced reasoning skills: hierarchical reasoning and expectation-reflection reasoning, enabling the agents to natively perform complex reasoning. We build InfiGUIAgent by supervised finetuning model using our SFT data and conduct experiments on several GUI benchmarks, demonstrating that our model achieves competitive performance."
        },
        {
            "title": "2.1 Multimodal LLMs",
            "content": "Large Language Models (LLMs) (Floridi and Chiriatti, 2020; Touvron et al., 2023; Bai et al., 2023a; Xiao et al., 2021) have significantly enhanced the capabilities of AI systems in tackling wide range of tasks (Hu et al., 2024c; Li et al., 2024d), thanks to their exceptional ability to process complex semantic and contextual information. The remarkable power of LLMs has also inspired exploration into their potential for processing multimodal data, such as images. Typically, the architecture of Multimodal Large Language Models (MLLMs) consists of three main components: pre-trained large language model, trained modality encoder, and modality interface that connects the LLM with the encoded modality features. Various vision encoders, such as ViT (Dosovitskiy et al., 2021), CLIP (Radford et al., 2021), and ConvNeXt (Liu et al., 2022), extract visual features, which are integrated using techniques like adapter networks (Liu et al., 2023), cross-attention layers (Alayrac et al., 2022), and visual expert modules (Wang et al., 2023). These methods have facilitated the development of high-performing MLLMs, such as Qwen-VL (Bai et al., 2023b), GPT-4 Vision (OpenAI, 2023), BLIP-2 (Li et al., 2023) and InfiMM (Liu et al., 2024), thus opening new avenues for LLMs in processing GUI tasks."
        },
        {
            "title": "2.2 MLLM-based GUI Agents",
            "content": "Agents are AI systems that perceive their environments, make decisions, and take actions to complete specific tasks. LLMs reaching human-level intelligence have greatly enhanced the ability to build agents. For GUI tasks, LLMs that read HTML code to perceive GUIs are developed (Wen et al., 2023). However, various works have shown that learning to interact with the visual form of the GUIs can show superior performance (Hu et al., 2024b). Therefore, MLLM-based GUI Agents are developed. ILuvUI (Jiang et al., 2023) finetuned LLaVA to enhance general GUI understanding, while AppAgent (Zhang et al., 2023) explored app usage through autonomous interactions. CogAgent (Hong et al., 2024) integrated high-resolution vision encoders, and Ferret-UI-anyres (You et al., 2025) employed an any-resolution approach. Building upon these works, our study focuses on developing more lightweight agent with simplified architecture for GUI tasks, aiming to improve ease of deployment."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce our two-stage supervised fine-tuning strategy for building InfiGUIAgent, as shown in Figure 1. In stage 1, we focus on improving fundamental abilities such as understanding and grounding, particularly considering the complexity of GUIs. In stage 2, we move on to improve the native reasoning abilities of agents for handling complicated GUI tasks."
        },
        {
            "title": "Abilities",
            "content": "Considering the complexity of GUIs, which involve diverse data formats such as HTML code, highresolution interfaces cluttered with small icons and text, general MLLMs lack fundamental abilities in both understanding GUI and grounding the actions. To address this, we first collected range of existing visual-language and GUI datasets for supervised fine-tuning in stage 1. We gathered data covering several GUI tasks from multiple sources to ensure comprehensive capabilities improvement (see Table 1). The datasets can be categorized into five parts: GUI Understanding. Datasets focusing on GUI element recognition, layout comprehension, and semantic interpretation, including Screen2Words (Wang et al., 2021) and Screen Annotation (Baechler et al., 2024). Grounding. Datasets capture various user interaction sequences and operation patterns, including GUIEnv (Chen et al., 2024), RICO Semantic Annotation (Sunkara et al., 2022), SeeClickWeb (Cheng et al., 2024), RICO SCA (Li et al., 2 Figure 1: InfiGUIAgent is trained in two stages. Stage 1 cultivates fundamental abilities using diverse datasets covering GUI understanding (element recognition and layout comprehension), question answering, instruction grounding, general knowledge, and tool usage. Stage 2 introduces native advanced reasoning, employed during both training and inference. This stage follows cyclical process at each step, consisting of Reflection, Hierarchical Reasoning (strategic and tactical layers), Action, and Expectation. Each step receives the overall task, the history of previous screenshots and reasoning, and the current environment as input. Reflection assesses the previous actions outcome against its expectation, while Expectation predicts the outcome of the current action for subsequent reflection. 2020a), Widget Caption (Li et al., 2020b), UIBert Reference Expression (Bai et al., 2021) and OmniAct-Single Click (Kapoor et al., 2024). Question Answering. Datasets contain GUIspecific QA tasks, including GUIChat (Chen et al., 2024), ScreenQA (Hsiao et al., 2022) and Complex QA (Yin et al., 2023). General Knowledge. Multimodal datasets maintain models general capabilities, including LLaVA-OneVision (Li et al., 2024a) and PixMo (MDeitke et al., 2024). Tool Usage. Datasets cover general tool using, including Glaive-function-calling (Glaive AI, 2024). Due to the diversity of our data sources, we implemented comprehensive format standardization across all datasets. Additionally, we adopted the Reference-Augmented Annotation format (see Section 3.1.2) to enhance the models ability to ground visual elements with textual descriptions, enabling precise spatial referencing while maintaining natural language flow."
        },
        {
            "title": "3.1.1 Data Preprocessing and Standardization",
            "content": "Given the diversity of our data sources, we implemented comprehensive preprocessing steps to standardize the data format across all datasets. We normalized the coordinate system by following (Wang et al., 2024), mapping all spatial coordinates to relative scale of [0, 1000]. This standardization facilitates consistent representation of both point and box annotations in JSON format, with points expressed as {\"x\" : x, \"y\" : y} and bounding boxes as {\"x1\" : x1, \"y1\" : y1, \"x2\" : x2, \"y2\" : y2}. In this coordinate system, the origin {\"x\" : 0, \"y\" : 0} is located at the screens top-left corner, with the xaxis extending rightward and the y-axis downward. The bottom-right corner corresponds to coordinates {\"x\" : 1000, \"y\" : 1000}. To enhance data quality, we implemented two additional preprocessing steps: Instruction Enhancement. For datasets with ambiguous instructions, we developed standardized instruction templates to establish clear correspondence between commands and their expected outcomes. Response Refinement. For entries with complex or inconsistent response formats, we utilized Qwen2-VL-72B (Bai et al., 2023b) to reformulate responses while preserving their semantic content. Each reformulation underwent validation to ensure accuracy and consistency."
        },
        {
            "title": "3.1.2 Reference-Augmented Annotation\nTo better leverage the spatial information available\nin our collected datasets and enhance the model’s\nvisual-language understanding of GUIs, we imple-\nmented a reference-augmented annotation format.",
            "content": "3 Table 1: Training datasets used in stage 1 of supervised fine-tuning."
        },
        {
            "title": "Category",
            "content": "# of Samples Webpage GUI-related Datasets GUIEnv (Chen et al., 2024) RICO Semantic Annotation (Sunkara et al., 2022) Mobile SeeClick-Web (Cheng et al., 2024) RICO SCA (Li et al., 2020a) Widget Caption (Li et al., 2020b) GUIChat (Chen et al., 2024) ScreenQA (Hsiao et al., 2022) UIBert Reference Expression (Bai et al., 2021) Screen2Words (Wang et al., 2021) Complex QA (Yin et al., 2023) Screen Annotation (Baechler et al., 2024) OmniAct-Single Click (Kapoor et al., 2024) Grounding Grounding Grounding Grounding Grounding QA QA Grounding Understanding QA Understanding Webpage Mobile Mobile Webpage Mobile Mobile & Mobile Mobile Mobile Mobile Webpage & Desktop Grounding Non-GUI Datasets LLaVA-OneVision (Li et al., 2024a) PixMo (MDeitke et al., 2024) Glaive-function-calling (Glaive AI, 2024) - - - General General Tool Usage 150,000 150,000 100,000 100,000 70,000 40,000 17,000 16,000 12,000 11,000 5,400 4,800 250,000 68,800 5,000 This format enables bidirectional referencing between GUI elements and textual responses. Specifically, we adopted the following structured notation: The format consists of several key components: the reference type (either \"box\" for rectangular regions or \"point\" for specific locations), coordinate specifications (x1, y1, x2, y2 for boxes or x, for points), optional annotative notes, and the corresponding textual content. To generate training data in this format, we prompted Qwen2-VL-72B (Bai et al., 2023b) to seamlessly integrate GUI spatial information with original responses, maintaining natural language flow while preserving precise spatial references."
        },
        {
            "title": "3.2 Stage 2: Training for Native Reasoning",
            "content": "Building upon the foundational capabilities such as understanding and grounding, GUI Agents must also master advanced reasoning skills to effectively handle complex tasks. We identify two crucial reasoning skills : (1) Hierarchical reasoning, which enables planning and task decomposition, helping agents structure complex tasks into manageable subtasks and execute them efficiently (Huang and Chang, 2023; Zhang et al., 2024b; Huang et al., 2024), and (2) Expectation-reflection reasoning, which fosters adaptive self-correction and reflec4 tion (Shinn et al., 2023; Yao et al., 2023; Hu et al., 2024a), enabling agents to learn from past actions and improve decision-making consistency. These reasoning skills are integrated into the training datasets of agents, so that they can reason with these skills natively without any extra prompting. To achieve this, we generate SFT data incorporating these reasoning skills based on existing trajectory data (see Table 2) and continue fine-tuning the model from stage 1."
        },
        {
            "title": "3.2.1 Hierarchical Reasoning",
            "content": "Effective execution of GUI tasks demands both overarching strategic planning and meticulous tactical execution. To achieve this, we synthesize trajectory data with hierarchical reasoning with two distinct layers: Strategic Layer. Strategic layer is responsible for high-level task decomposition and sub-goal planning. This layer analyzes the overall task objective and determines the sequence of subtasks needed for completion. Tactical Layer. Tactical layer handles the selection and grounding of concrete actions. Based on the strategic layers planning, agent select appropriate GUI operations and adjusts their parameters to match the target."
        },
        {
            "title": "3.2.2 Expectation-Reflection Reasoning",
            "content": "action consistency and foster To enhance autonomous incorporate self-correction, we Expectation-reflection reasoning into the training datasets. This iterative process enhances the Table 2: UI action reasoning datasets used in the training process"
        },
        {
            "title": "Platform",
            "content": "# of Samples Webpage & Mobile GUIAct (Chen et al., 2024) AMEX (Chai et al., 2024) Mobile Android in the Zoo (Zhang et al., 2024a) Mobile Composition: Stage 1-aligned - 10,000 3,000 2,000 30,000 Category Operations Single-point operations Two-point operations Directional operations Text input Parameterless operations State settings tap, click, hover, select swipe, select_text scroll input, point_input remember, enter, home, back set_task_status Table 3: Categorization of actions in the action space. agents ability to adapt and learn from its actions through structured reflection cycle: Reasoning. After reflection (except the first step), the agents conduct hierarchical reasoning. Action. After the reasoning, the agent takes the action. Expectation. Following each action, the agent generates expected outcomes which are used to be verified at the next step. Reflection. The agent evaluates whether its actions achieved the expected results and generating textual summary of the reflection."
        },
        {
            "title": "3.2.3 Agent-Environment Interface\nWe formulate the GUI interaction as a process\nwhere an agent interacts with a mobile environment.\nLet st ∈ S denote the environment state at step t,\nwhere S represents the state space. The agent can\nobserve the state through a screenshot observation\not and performs actions at ∈ A, where A is the\naction space. The environment transitions from\nst to st+1 following st+1 ∼ P (·|st, at), where P\nrepresents the transition probability function.",
            "content": "The agent receives task goal and maintains access to history window of size n. At each step t, the agents input consists of: Goal Current observation ot Historical context Ht = {(oi, ri, ai)}t1 where ri represents the reasoning process i=tn, Based on these inputs, the agent generates reasoning process rt and predicts an action at. The interaction follows standard protocol using function calls and responses:"
        },
        {
            "title": "3.2.4 Modular Action Space",
            "content": "Given the diverse action spaces across collected datasets, we categorized and standardized the actions by unifying their names and parameters, merging similar operations where appropriate. The resulting action space consists of independent, composable operations that can be flexibly combined based on task requirements, as shown in Table 3. This modular design allows for dynamic action space configuration while maintaining consistent interface across different platforms and scenarios."
        },
        {
            "title": "3.2.5 Reasoning Process Construction",
            "content": "To construct high-quality reasoning data to stimulate the models native reasoning capabilities, we leverage more capable MLLMs (e.g. Qwen2-VL72B) to generate structured reasoning processes based on existing interaction trajectories. The construction process involves several key components: Screenshot Description. For each observation ot in the trajectory, we generate detailed description dt. This step addresses the limitation that some MLLM models do not support inter5 leaved image-text input formats well. To establish clear correspondence between observations (screenshots) and steps, we generate detailed descriptions to replace the screenshots, which helps facilitate the subsequent reasoning process construction. 45K samples based on trajectories from datasets shown in Table 2. We continual supervised finetune Qwen2-VL-2B (Bai et al., 2023c). We leverage ZeRO0 (Rajbhandari et al., 2020) technology to enable full parameter fine-tuning of the model across 8 A800 80GB GPUs. Reflection. Given the previous expectation et1 and current observation ot, we generate reflection ft that evaluates the outcome of the previous action. Strategic Layer. The strategic reasoning consists of two parts: First, summary is generated based on the n-step history Ht = {(oi, ri, ai)}t1 i=tn and current observation ot. Then, the planning component is generated with access to the actual action at to ensure alignment with the trajectory. Tactical Layer. This layers reasoning is constructed using the generated reflection ft and strategic layer output. The actual action at from the trajectory is incorporated to ensure the tactical reasoning leads to appropriate action selection. Expectation. For each state-action pair (st, at), we generate an expectation et based on current observation ot, reasoning process rt, and action at. Notably, we deliberately avoid using the next state st+1 in this generation process. Although using st+1 could improve the agents accuracy in modeling state transitions, while using st+1 could lead to perfect expectations, such an approach might impair the agents ability to handle expectation mismatches during deployment. While we avoid using st+1 in expectation generation to maintain robustness, we also explore the possibility of improving state transition modeling through parallel next-state prediction task. Using the trajectory data, we construct additional training examples where the agent learns to predict the next state description dt+1 given the current observation ot and action at. This auxiliary task helps the agent learn state transition dynamics, while keeping the expectation generation process independent of future states."
        },
        {
            "title": "4.1.2 Evaluation Benchmarks\nScreenSpot. ScreenSpot (Cheng et al., 2024) is\nan evaluation benchmark for GUI grounding, con-\nsisting of over 1,200 instructions from iOS, An-\ndroid, macOS, Windows, and Web environments,\nwith annotated element types.",
            "content": "AndroidWorld. AndroidWorld (Rawles et al., 2024) is fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. We find that Android World uses Set-of-Marks (SoM) (Yang et al., 2023b) to enhance the agents grounding ability. However, when humans operate smartphones, their brains do not label elements on the screen. Over-reliance on SoM can lead to insufficient focus on pixel-level grounding ability. Therefore, in our experiments, agents respond to the raw image rather than the annotated image."
        },
        {
            "title": "4.2 Main Results",
            "content": "ScreenSpot. Table 4 provides the results of different models across three platforms (Mobile, Desktop and Web) and two element types (Text and Icon) on ScreenSpot (Cheng et al., 2024). InfiGUIAgent2B achieves highest accuracy of 76.3%, surpassing several strong baselines such as ShowUI (Lin et al., 2024) (75.1%) and UGround-7B (Gou et al., 2024) (73.3%), which is even with larger parameters size. AndroidWorld. Table 5 compares the success rates of InfiGUIAgent with open-source models on AndroidWorld (Rawles et al., 2024). InfiGUIAgent2B achieves an overall success rate of 0.09, outperforming open-source models of similar size, such as ShowUI-2B (Lin et al., 2024) (0.07), and model with much more parameters such as LLaVa-OV-7B (Li et al., 2024b) (0.00) and Qwen2-VL-72B (Bai et al., 2023b) (0.05)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose InfiGUIAgent, novel MLLM-based GUI Agents. By constructing comprehensive training datasets with two-stage supervised fine-tuning, we enhance the models ability to understand, reason, and interact with GUIs. Our"
        },
        {
            "title": "Model",
            "content": "Proprietary Models GPT-4o1 (OpenAI, 2024) Gemini-1.5-pro2 (Team et al., 2024) Open-source Models Qwen2-VL-2B (Wang et al., 2024) Qwen2-VL-7B (Wang et al., 2024) CogAgent (Hong et al., 2024) SeeClick (Cheng et al., 2024) UGround-7B (Gou et al., 2024) ShowUI-2B (Lin et al., 2024) Ours InfiGUIAgent-2B Accuracy (%) Avg. Mobile Desktop Web Text Icon Text Icon Text Icon 30.5 76.2 23.2 54.1 20.6 65.5 19.4 39. 11.1 52.2 7.8 32.0 18.8 53.2 24.2 61.3 67.0 78.0 82.8 92.3 10.0 39.3 24.0 52.0 60.3 75.5 1.4 52.0 74.2 72.2 82.5 76. 9.3 45.0 20.0 30.0 63.6 61.1 8.7 33.0 70.4 55.7 80.4 81.7 2.4 21.8 28.6 32.5 70.4 63.6 9.3 42.9 47.4 53.4 73.3 75.1 88.6 74. 85.6 65.0 79.1 64.6 76.3 Table 4: Performances on various platforms (Mobile, Desktop, Web) on Screenshot. All experiments were conducted using raw screenshot information. Results marked in bold represent the best performance, and those underlined indicate the second-best performance. Model Success Rate Easy Middle Hard Overall Qwen2-VL-2B (Wang et al., 2024) Qwen2-VL-7B (Wang et al., 2024) Qwen2-VL-72B (Wang et al., 2024) LLaVa-OV-7B (Li et al., 2024b) ShowUI-2B (Lin et al., 2024) 0.00 0.00 0.08 0.00 0.18 0.00 0.00 0.00 0.00 0. 0.00 0.05 0.05 0.00 0.00 0.00 0.05 0.05 0.00 0.07 Ours InfiGUIAgent-2B 0.25 0.00 0. 0.09 Table 5: Performances on AndroidWorld. evaluation, conducted using raw screenshots without relying on additional GUI metadata, demonstrates the models applicability to real-world scenarios. Experimental results show that our model performs well on GUI tasks and surpass several open-source baselines."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. 2024. Screenai: vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615. Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera Arcas. 2021. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a. Qwen technical report. ArXiv. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. Qwen-vl: frontier large vision-language model with versatile abilities. ArXiv. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023c. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. 2024. Amex: Android multiannotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490. 7 Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. Yong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang, Zhangyin Feng, Fan Zhang, Xueyu Hu, and Shuming Shi. 2022. One model, multiple modalities: sparsely activated approach for text, sound, image, video and code. Preprint, arXiv:2205.06126. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Luciano Floridi and Massimo Chiriatti. 2020. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681694. Glaive AI. 2024. Glaive function calling dataset. https://huggingface.co/datasets/glaiveai/ glaive-function-calling. Accessed: 2024-0108. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2024. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290. Yu-Chung Hsiao, Fedir Zubach, Gillbune Baechler, Victor Carbune, Jason Lin, Maria Wang, Srinivas Sunkara, Yun Zhu, and Jindong Chen. 2022. Screenqa: Large-scale question-answer pairs over mobile app screenshots. arXiv preprint arXiv:2209.08199. Xueyu Hu, Kun Kuang, Jiankai Sun, Hongxia Yang, and Fei Wu. 2024a. Leveraging print debugging to improve code generation in large language models. Preprint, arXiv:2401.05319. Xu, Shawn Wang, Xinchen Xu, Shuofei Qiao, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, and Fei Wu. 2024b. Os agents: survey on mllm-based agents for general computing devices use. Preprints. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024c. Infiagent-dabench: Evaluating agents on data analysis tasks. arXiv preprint arXiv:2401.05507. Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. Preprint, arXiv:2212.10403. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716. Yue Jiang, Eldon Schoop, Amanda Swearngin, and Jeffrey Nichols. 2023. Instruction-tuned language-vision modeling of uis from machine conversations. arXiv preprint arXiv:2310.04869. Iluvui: Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakutdinov. 2024. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, YanLiu Li, Ziwei Liu, and Chunyuan Li. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03329. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024b. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. 2024c. Aria: An open multimodal native mixture-of-experts model. Preprint, arXiv:2410.05993. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. 2024d. Infibench: Evaluating 8 the question-answering capabilities of code large language models. arXiv preprint arXiv:2404.07940. OpenAI. 2023. Gpt-4v(ision) system card. OpenAI. 2024. Gpt-4o. Accessed: 2025-01-03. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020a. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776. Yang Li, Luheng Li, Gangaand He, Jingjie Zheng, Hong Li, and Zhiwei Guan. 2020b. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. 2024. Showui: One visionlanguage-action model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents. Haogeng Liu, Quanzeng You, Yiqi Wang, Xiaotian Han, Bohan Zhai, Yongfei Liu, Wentao Chen, Yiren Jian, Yunzhe Tao, Jianbo Yuan, Ran He, and Hongxia Yang. 2024. Infimm: Advancing multimodal understanding In with an open-sourced visual language model. Annual Meeting of the Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. In Proceedings of the convnet for the 2020s. IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986. Matt MDeitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bramsom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli van der Bilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Kuo-Hao Gupta, Tanmay sna Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. arXiv preprint arXiv:2409.17146. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, et al. 2024. Webrl: Training llm web agents via self-evolving online curarXiv preprint riculum reinforcement learning. arXiv:2411.02337. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. Preprint, arXiv:1910.02054. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo CampbellAjala, et al. 2024. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Abhanshu Sharma, James Stout, et al. 2022. Towards better semantic understanding of mobile interfaces. arXiv preprint arXiv:2210.02663. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv. Minh Duc Vu, Han Wang, Jieshan Chen, Zhuang Li, Shengdong Zhao, Zhenchang Xing, and Chunyang Chen. 2024. Gptvoicetasker: Advancing multi-step mobile task efficiency through dynamic interface exploration and learning. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pages 117. Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. 2024. Mobileflow: multimodal llm for mobile gui agent. arXiv preprint arXiv:2407.04346. Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2words: Automatic mobile ui summarization with multimodal learning. arXiv preprint arXiv:2108.03353. 9 Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024a. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. 2024b. Llm as mastermind: survey of strategic reasoning with large language models. Preprint, arXiv:2404.01230. Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079. Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2023. Autodroid: Llmpowered task automation in android. arXiv preprint arXiv:2308.15272. Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021. Lawformer: pre-trained language model for chinese legal long documents. AI Open, 2:7984. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. arXiv preprint arXiv:2404.07972. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023a. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. Preprint, arXiv:2310.11441. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023b. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2023. Agent lumos: Unified and modular training for open-source language agents. arXiv preprint arXiv:2311.05657. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. 2025. Ferret-ui: Grounded mobile In Euroui understanding with multimodal llms. pean Conference on Computer Vision, pages 240 255. Springer. Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. 2024. Exact: Teaching ai agents to explore with reflectivearXiv preprint mcts and exploratory learning. arXiv:2410.02052."
        },
        {
            "title": "A Cases",
            "content": "A.1 Stage 1: Fundamental Abilities We demonstrate the fundamental abilities trained in Stage 1 through three cases: GUI Understanding (Figure 2), Grounding (Figure 3), and Question Answering (Figure 4). A.2 Stage 2: Native Reasoning We provide two representative cases to demonstrate the reasoning and interaction process of InfiGUIAgent. Reply to Message Figure 5 illustrates step where the agent needs to reply to specific message in messaging application. The reasoning process involves identifying the \"Start chat\" button and grounding the action to initiate the reply process. Creating New Contact Figure 6 and Figure 7 demonstrate sequential steps for creating new contact. In the first step (Step K), the agent navigates to the \"Contacts\" section by reasoning and groundIn the ing the action to the corresponding tab. following step (Step K+1), the agent initiates the contact creation process by identifying and tapping the \"Create new contact\" button. These sequential steps highlight the agents hierarchical reasoning and grounding abilities. 11 Figure 2: Case of GUI Understanding. Figure 3: Case of Grounding. 12 Figure 4: Case of Question Answering. Figure 5: Case of Native Advanced Reasoning. The agents goal is to reply to message 13 Figure 6: Case of Native Advanced Reasoning. The agents goal is to create new contact. Figure 7: Case of Native Advanced Reasoning. The agents goal is to create new contact."
        }
    ],
    "affiliations": [
        "ByteDance Inc",
        "Dalian University of Technology",
        "Reallm Labs",
        "The Hong Kong Polytechnic University",
        "Zhejiang University"
    ]
}