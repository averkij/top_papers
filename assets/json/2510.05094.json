{
    "paper_title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
    "authors": [
        "Ziqi Huang",
        "Ning Yu",
        "Gordon Chen",
        "Haonan Qiu",
        "Paul Debevec",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 4 9 0 5 0 . 0 1 5 2 : r VChain: Chain-of-Visual-Thought for Reasoning in Video Generation Ziqi Huang, Ning Yu(cid:66), Gordon Chen, Haonan Qiu, Paul Debevec, Ziwei Liu(cid:66) https://eyeline-labs.github.io/VChain Figure 1: Overview of VChain. We introduce VChain, an inference-time tuning framework for reasoning in video generation. Given user-provided prompt (e.g., rock and feather are falling from the sky towards the ground.), VChain leverages large multimodal models to generate Chain of Visual Thoughts, which are sparse set of causally important keyframes to guide the video generator via Sparse Inference-Time Tuning. VChain effectively improves reasoning in video generation without extensive re-training."
        },
        {
            "title": "Abstract",
            "content": "Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains dedicated (cid:66) Corresponding Authors. Project Lead. Ziqi Huang, Gordon Chen, Haonan Qiu, and Ziwei Liu are with Nanyang Technological University. Email: {ziqi002, chen2008, haonan002, ziwei.liu}@ntu.edu.sg Ning Yu and Paul Debevec are with Eyeline Labs. Email: {ning.yu, debevec}@scanlinevfx.com 1 pipeline that leverages large multimodal models to generate sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos."
        },
        {
            "title": "Introduction",
            "content": "Video generation (Yang et al., 2024; Gen, 2024; kli, 2024; Kong et al., 2024; Wan et al., 2025; Team, 2025; Agarwal et al., 2025; Min, 2023) aims to synthesize coherent and realistic visual sequences, either from scratch or based on user-provided inputs such as text prompts, reference images, motion cues, or other forms of control. In recent years, this field has made remarkable progress, driven by powerful generative models such as diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021b; Ho et al., 2020), and supported by large-scale video datasets and increasing computational resources. Modern video generation models have achieved impressive results in generating smooth and visually appealing video clips. However, they still struggle to reflect the intrinsic dynamics of the real world, especially when it comes to generating sequences that involve meaningful state transitions or coherent chains of consequences. As result, current methods often fail to capture how visual states evolve over time in logically consistent and causally grounded manner. For example, given prompt like person drops cup, it hits the ground, and the liquid splashes out, many models may render smooth deformations between frames but omit key causal steps, such as the cup deforming on impact or the splash propagating outward, resulting in scenes that are logically inconsistent or physically implausible. In contrast, large language and multimodal models excel precisely in the areas where video generation models tend to struggle. Models such as GPT4o (Hurst et al., 2024) have made rapid progress in general reasoning and cross-modal understanding. These models show strong capabilities in following instructions, multi-step reasoning, and aligning semantics across text and vision. Although they do not explicitly simulate visual dynamics over time, they are effective in inferring likely transitions between visual states. For instance, they can reason that if glass tips over, it may shatter, or that if person jumps, they will eventually land. This ability to suggest causally and logically consistent progressions offers promising signal that current video generators lack. natural question is raised: can we leverage this reasoning ability from large multimodal models to guide video generation models towards more coherent chains of visual consequences? To this end, we propose VChain, novel inference-time tuning framework that introduces high-level reasoning into video generation. The core idea is to represent the evolution of scenario as sparse sequence of Visual Thoughts - keyframes that capture critical intermediate states that reasoning agent might anticipate. These visual thoughts are automatically generated using large multimodal models and serve as guidance signals for the video generator. VChain mainly consists of two main components. 1) Visual Thought Reasoning: We design dedicated pipeline that leverages large multimodal models to decompose user-provided text prompt into concise set of causally important Visual Thoughts. These keyframes capture the intended chain of visual outcomes and act as blueprint for the temporal structure of the video. 2) Sparse Inference-Time Tuning: Then, the pre-trained video generator is quickly and efficiently fine-tuned using only the Visual Thought keyframes. The model is adjusted in focused manner at these critical visual states, allowing it to capture the intended visual state transitions. Compared to tuning on video data, this approach is significantly faster and more practical for deployment. As an inference-time tuning method, VChain offers several benefits. (1) Self-contained: All supervision is synthesized on the fly during inference by prompting large multimodal model, with no need for external annotations, curated datasets, or retrieval systems. (2) Efficient: The tuning is only supervised by few keyframes with limited iterations, and thus introduces minimal overhead relative to the cost of sampling the video itself. (3) Effective: We evaluate VChain on complex, multi-step video generation tasks that require strong causal reasoning. Across these scenarios, VChain consistently improves the dynamic fidelity of generated videos, leading to sequences that better reflect logical consequences, smooth transitions, and coherent visual narratives. Beyond specific technique, VChain offers new pathway: treating multimodal models as reasoning modules that complement, rather than replace, generative models in constructing causally coherent visual narratives. More generally speaking, VChain encourages the community to rethink how reasoning can be integrated into video generation - not through model retraining or dense supervision, but by transforming general-purpose multimodal intelligence into chain-of-visual-thought guidance at inference time. In summary, our contributions are: We introduce VChain, novel framework that uses chain-of-visual-thought from large multimodal models to bring high-level reasoning into video generation. We design the Visual Thought Reasoning pipeline, GPT-guided pipeline that synthesizes sparse, causally grounded keyframes for guiding video generation. Extensive experiments demonstrate that sparse supervision on these keyframes improves models ability to produce videos with coherent visual consequences and interpretable state transitions. Our method operates entirely at inference time, requires no external training data, and adds minimal computational overhead."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Generation Video generation has seen rapid progress (Yang et al., 2024; Gen, 2024; kli, 2024; Kong et al., 2024; Wan et al., 2025; Team, 2025; Agarwal et al., 2025; Min, 2023), driven by advances in diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021b; Ho et al., 2020; Song et al., 2021a; Zhang and Agrawala, 2023; Blattmann et al., 2023; Esser et al., 2024; Mou et al., 2023; Ding et al., 2021, 2022; Ho et al., 2022), variational autoencoderbased compression (Kingma and Welling, 2013; Van Den Oord et al., 2017; Esser et al., 2021; Podell et al., 2023; Yu et al., 2023), and transformer-based backbones (Dosovitskiy et al., 2020; Peebles and Xie, 2022). Despite impressive progress in visual fidelity, smooth motion, and temporal alignment, most existing video generation methods remain limited to surface-level coherence (Zheng et al., 2025). They typically fail to capture deeper aspects such as causal dynamics, physical interactions, and meaningful state transitions. These models often overlook how actions lead to consequences, how objects behave under physical laws, or how scene states evolve with internal logic. To address this gap, we introduce an inference-time reasoning framework that injects high-level semantic supervision obtained from large multimodal models into the generation process. This approach enables pretrained video generators to produce outputs that are not only visually plausible but also causally and physically grounded. such as Transfusion (Zhou et al., 2024) incorporate these capabilities into multimodal pipelines for image generation. LMD (Lian et al., 2023a) and LVD (Lian et al., 2023b) leverage large language model to generate coarse layouts to guide visual synthesis. However, these approaches typically treat LLMs as static prompt interpreters or highlevel planners, or alternatively require dense retraining and architectural modifications. Unlike existing methods, we propose lightweight inferencetime reasoning framework for video generation that leverages off-the-shelf multimodal models and pre-trained video generators. Our method avoids dense retraining and instead injects high-level reasoning signals through sparse visual supervision, enabling more causally consistent and semantically grounded video generation with minimal overhead."
        },
        {
            "title": "3 The VChain Framework",
            "content": "VChain is an inference-time reasoning framework designed to enhance the causal and physical coherence of video generation. Built on top of pre-trained video generator, it aims to improve the models ability to reflect reasoning, physics, causality, and commonsense understanding, producing videos that are more physically grounded and causally consistent. As shown in Figure 2, the VChain framework has three key stages: (1) Visual Thought Reasoning, which uses large multimodal model to infer key events and their consequences as sparse sequence of visual snapshots; (2) Sparse InferenceTime Tuning, which injects these Visual Thoughts from stage 1 into the pre-trained video generator via lightweight LoRA adaptation; and (3) Video Sampling, which produces the final video by using both the stage-1 thoughts and the stage-2 tuned generator. 2.2 Multimodal Models for Understanding 3.1 Preliminaries and Generation Large language models (LLMs) like GPT-4 (OpenAI, 2023) and multimodal models such as Gemini (Team et al., 2023) and GPT-4o (Hurst et al., 2024) have shown strong capabilities in visionlanguage tasks, including instruction-following, visual question answering, and interactive reasoning. These models can perform reasoning about visual scenes, and more importantly, understand and generate grounded visual content. Recent works 3 Diffusion Models. Diffusion models are class of generative models that reconstructs data x0 such as natural images or videos by iteratively denoising starting from the Gaussian prior xT . widely used training loss (Ho et al., 2020) is LDM(θ) := Et,x0,ϵ , where xt is noisy image or video obtained by adding noise ϵ (0, I) to the original visual x0. The network ϵθ() learns to estimate this added noise. To generate new data x0, the trained model ϵθ() deϵ ϵθ(xt, t)2(cid:105) (cid:104) noises xt iteratively from = to = 0, using the predicted noise at each step. Video Diffusion Models. Our work builds on Wan (Wan et al., 2025), state-of-the-art video generation foundation model trained on mix of video and image datasets, supporting both video and image generation. Recent progress in diffusionbased video generation has been shifting from UNet (Ronneberger et al., 2015) architectures to Diffusion Transformers (DiTs) (Peebles and Xie, 2022) with Flow Matching (Lipman et al., 2022). Wan adopts this newer paradigm, design now common in text-to-video (T2V) systems (Kong et al., 2024). DiTs offer scalability advantages, while Flow Matching enables faster and more stable training convergence. Wan includes three main components: 1) Wan-VAE: spatio-temporal variational autoencoder; the 2) video diffusion transformer, and the 3) text encoder. Given video R(1+T )HW 3, Wan-VAE compresses it into VAE latent R(1+T /4)H/8W/8. The compression is spatial (by factor of 8 8) for all frames, and temporal (by factor of 4) for all frames except the first, which is only spatially compressed. The Wan video generation model uses the flow matching (Lipman et al., 2022; Esser et al., 2024) training objective in the Wan-VAEs latent space. Given the video (or image) latent x1 and noise x0 (0, I), the noised latent is defined by the linear interpolation: xt = tx1 + (1 t)x0, (1) where the timestep [0, 1] is sampled from logit-normal distribution. The model is trained to predict the velocity: vt = dxt dt = x1 x0, (2) using the objective: L(θ) = Ex0,x1,c,t uθ(xt, t, c) vt2 , (3) where uθ is the denoising model and represents the embedded text prompt. The text encoder transforms the input text prompt into token embeddings, which we refer to as for brevity. Low-Rank Adaptation (LoRA). LoRA (Hu et al., 2022) is parameter-efficient fine-tuning technique. It freezes the original model weights, and injects trainable low-rank decomposition matrices into network layers, largely reducing the number of trainable parameters. Specifically, for pre-trained Algorithm 1 Visual Thought Reasoning 1: given user-provided text prompt 2: 3: % generate first frame 4: txt, consequence = chat(p) 5: img = image_generate(txt) 6: chainvis = [img] % init chain-of-visual-thought 7: chaintxt = [txt] % init chain-of-textual-thought 8: 9: % iteratively generate subsequent frames 10: repeat 11: 12: 13: 14: 15: until flag==terminate 16: 17: return chainvis, chaintxt txt, flag = perception(chainvis, consequence, p) img = image_edit(chainvis, txt) chainvis.append(img) chaintxt.append(txt) weight matrix W0 Rdk, LoRA reparametrizes the update as W0 + = W0 + BA, where Rdr, Rrk, and min(d, k). Only and are updated during training. Given an input x, the modified forward computation is = W0x + = W0x + BAx. Because of the low-rank property, LoRA offers both computational and memory efficiency, making it strong fit for fine-tuning large video diffusion models. 3.2 Visual Thought Reasoning Given user-provided text prompt for video generation, we leverage the powerful multimodal reasoning capabilities of GPT-4o (Hurst et al., 2024) to generate sequence of images, referred to as the Chain of Visual Thoughts, that capture the key moments of the intended video. The steps and definitions of Visual Thought Reasoning are listed in Algorithm 1. We first prompt GPT-4o to reason about the likely outcome implied by the user input prompt p. As illustrated in Figure 2, given prompt, piece of ice on brown piece of paper sitting under the sun, GPT-4o infers that the ice will melt due to the heat, forming puddle that soaks the paper. This step establishes the ground-truth trajectory of the intended video, referred to as the consequence, which serves as the basis for constructing the key transitions of the unfolding scene. We then instruct GPT-4o to generate caption txt0 describing the first frame in the Chain of Visual Thoughts, which is transformed into an image img0 using GPT-4os native image generation module. After that, GPT-4o predicts an editing instruction txti to produce the key moment at time step in our chain, conditioned Figure 2: VChain Framework. An overview of our three-stage inference-time pipeline for reasoning in video generation. (a) Visual Thought Reasoning: Given user-provided text prompt, large multimodal model (GPT-4o) infers causal chain of events and generates sequence of keyframes, termed the Chain of Visual Thoughts, via iterative reasoning and image synthesis. (b) Sparse Inference-Time Tuning: These visual thoughts (paired with their corresponding textual thoughts) serve as sparse supervision for fine-tuning pre-trained video generator via LoRA. (c) Video Sampling: The full sequence of textual thoughts is concatenated to form single prompt, which is used to prompt the fine-tuned model in generating the final video output. the consequence, and the Chain of Vion p, sual Thoughts at the current timestep, chainvis = [img0, img1, . . . , imgi1]. Then txti is used to generate the subsequent image imgi. This process continues iteratively, where GPT-4o predicts an editing instruction and generates corresponding image, and terminates only when the consequence has been fully captured by chainvis. The resulting output is coherent sequence of keyframes, or Chain of Visual Thoughts [img0, img1,. . . , imgN 1] paired with its corresponding textual thoughts [txt0, txt1, . . . , txtN 1], that captures the temporal evolution implied by the user prompt. This approach also allows users to generate causally consistent image sequences without having to explicitly anticipate or specify the underlying consequences of the described scenario. Please refer to the Appendices for detailed descriptions of the Visual Thought Reasoning process, including system prompts, intermediate outputs, and workflow details. 3.3 Sparse Inference-Time Tuning Given the sparse and causally grounded Chain of Visual Thoughts generated from the previous stage, we perform lightweight inference-time tuning on pre-trained video generator. We only use these keyframes as supervision, treating them as anchor points that encode important state changes (e.g., melting, breaking, or object movement). Formally, img1,. . . , let chainvis = [img0, imgN 1] be the sequence of Visual Thoughts (keyframes), and chaintxt = [txt0, txt1, . . . , txtN 1] be their corresponding Textual Thoughts. Each imgi is treated as one-frame video, paired with the caption txti. These pairs (imgi, txti) serve as the training data for tuning the video diffusion model using the same flow-matching objective as Equation 3: Lvchain(θ) = Ex0,x1,c,t uθ(xt, t, c) vt2 , (4) where x1 = imgi, x0 (0, I), [0, 1] is sampled from logit-normal distribution, xt = 5 Table 1: Quantitative Evaluation. VChain is compared with existing methods and ablation variants, achieving comparable or superior performance across all evaluation metrics. Method VBench Quality Score Frame Quality Temporal Smoothness Video-Text Alignment Physics Commonsense Reasoning Causal Reasoning T2V T2V + Prompt Aug Without Visual Thought Without Sparse Tuning VChain (Ours) 76.21% 77.51% 78.47% 73.35% 78.49% 43.65% 57.24% 50.59% 55.47% 52.93% 64.26% 44.07% 29.19% 71.67% 65.82% 32.03% 40.04% 38.09% 47.66% 44.14% 54.69% 42.97% 33.24% 67.77% 58.01% 32.42% 38.48% 43.75% 34.57% 60.16% 32.81% 41.99% 47.51% 34.46% 62.12% tx1 + (1 t)x0 as in the flow-matching setup, and is the text embedding of txti. This sparse tuning scheme offers two key benefits: 1) Focused supervision: By concentrating only on keyframes that encode the critical moments (e.g., object breaking, melting, or appearing), we guide the model to focus on inferring causal outcomes and key visual state transitions. 2) Efficiency: Since the tuning is image-only, tuning is fast and memory-efficient. This makes our method practical for inference-time adaptation. Additionally, our tuning does not require additional databases or labels. The entire supervision signal is generated internally from the Visual and Textual Thoughts (Section 3.2), making VChain easily pluggable into general pre-trained video generators. 3.4 Video Sampling Following Sparse Inference-Time Tuning, we concatenate every textual thought txti from the Chain of Textual Thoughts chaintxt into single composite prompt txtconcat. This final prompt is used as the input to the fine-tuned video generator to produce the output video. The resulting generation reflects both the inferred sequence of events and the adapted capabilities of the model."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup For Visual Thought Reasoning, we use the GPT family (Hurst et al., 2024) as our large multimodal model. Specifically, we use gpt-4o for chat and perception, and gpt-image-1 for steps involving image generation and editing. Our main experiments are conducted using the state-of-the-art pretrained video generator Wan2.1-T2V-1.3B (Wan et al., 2025). We design 20 diverse test scenarios for both human evaluations and quantitative comparisons. We list the implementation details, test cases, and the time cost breakdown in Appendices. 4.2 Comparison Methods We compare our proposed method VChain against several baselines and ablation variants. Baseline Comparison. We include the following baselines: - T2V: The original pre-trained text-to-video generation model without any modification. - T2V + Prompt Aug: The input text prompt is enhanced using GPT-based prompt augmentation. Ablation Study. To further understand the impact of each component in VChain, we design the following ablation settings: - Without Visual Thought: We use our Visual Thought Reasoning pipeline to produce both composite text prompts txtconcat and visual thoughts chainvis, but only feed txtconcat to the video generator, omitting the visual thoughts for sparse tuning. This ablation evaluates the necessity of performing chain-of-thought reasoning visually, showing that text-only thoughts are insufficient for reasoning in video generation. - Without Sparse Tuning: We use GPT-generated keyframes as-is for direct video interpolation, without fine-tuning the video generator. This variant evaluates the necessity of sparse tuning to align the dynamics with the inferred reasoning signals. - VChain (Ours): Our full framework, which combines both Visual Thought Reasoning and Sparse Inference-Time Tuning to enable reasoning in video generation. 4.3 Quantitative Comparisons We introduce the aspects used for experimental comparisons. VBench Quality Score. To evaluate VChains impact on fundamental video quality, independently of its reasoning or causal capabilities, we conduct quantitative evaluations using VBench (Huang et al., 2024a,b), an evaluation framework designed to assess key technical dimensions of video generation, such as frame-level fidelity, temporal con6 Figure 3: Qualitative Results - Baseline Comparison. T2V fails to capture the key causal interaction: the pins remain mostly static or jitter slightly, with no meaningful collision, revealing lack of physical reasoning despite temporal coherence. T2V + Prompt Aug introduces relevant elements and motion, but the dynamics are erratic and implausible. Pins deform unnaturally, visual artifacts appear, and later frames become unstable, indicating poor spatial consistency. In contrast, VChain (Ours) produces coherent and physically realistic sequence: the ball strikes the pins with plausible force, leading to consistent knockdown. Object geometry and material properties are well preserved across frames. These results show that VChain not only enables causal reasoning about the outcome of physical interactions, but also stabilizes spatial transitions. sistency, and motion dynamics etc. As shown in Table 1, VChain achieves comparable or slightly better scores than both the original pre-trained generator and other baselines. We also perform complementary human evaluations focused on three core aspects of video quality: Frame Quality. Evaluates the visual quality of individual frames, including aesthetics, imaging sharpness, and realism. Temporal Quality. Assesses motion smoothness, temporal consistency, and overall dynamic realism across frames. Video-Text Alignment. Evaluates how faithfully the generated video reflects the user-provided text prompt. While VChain is primarily designed to enhance high-level reasoning in video generation (e.g., comthe results monsense, causality, and physics), shown in Table 1 confirm that it does not compromise basic visual quality. In fact, it often brings modest improvements. To directly assess VChains reasoning capabilities, we conduct targeted human studies along the following dimensions: Physics. Evaluates whether the video follows physical laws, like gravity and air friction (e.g., rocks fall faster than feathers in the air). Participants rate how well the video obeys the laws of physics. Commonsense Reasoning. Assesses whether events in the video reflect everyday real-world knowledge. For instance, blue paint mixed with yellow turning green, or oil floating on water. Users rate how well the video reflects common sense. Causal Reasoning. Evaluates whether the video captures appropriate cause-and-effect relationships. Examples include stone causing splash when dropped in water, ball failing to bounce on pillow, or switch turning on light. Participants are asked: How well does the video reflect the causal consequences of the initial setup? Human evaluators were presented with generated videos alongside their corresponding input prompts. The outputs from our method and the baselines were shown in randomized order to avoid bias. total of 32 evaluators rated each video on scale from 1 to 5 for each evaluation dimension. The scores are then averaged and normalized to percentage scale, as reported in Table 1. VChain consistently outperforms the baseline methods, particularly in reasoning-related dimensions such as physics, commonsense, and causality. These improvements demonstrate the effectiveness of the integration of our framework in inferencetime reasoning for video generation. 4.4 Qualitative Comparisons Extensive qualitative results and comparisons are also provided in the Appendices. Baseline Comparison. We present qualitative comparisons against baseline methods in Figure 3. In the T2V baseline, the model fails to produce any meaningful physical interaction: the pins remain mostly static or exhibit minor jittering, with no visible impact or knockdown. Although temporally stable, the output is semantically misaligned 7 Figure 4: Qualitative Results - Ablation Study. We compare VChain with two ablated variants. (1) Without Visual Thought: Although the model recognizes that the video should be in first-person perspective based on the textual prompt, it fails to capture the correct visual pattern for ball-catching viewpoint. In contrast, VChain leverages the reasoned Visual Thoughts to render step-by-step intermediate visual states of the throw-and-catch process. (2) Without Sparse Tuning: While Visual Thoughts are included, the model performs direct frame interpolation without tuning, leading to warping artifacts due to spatial misalignments among individual frames in Visual Thoughts. VChain (Ours) produces the most coherent and physically grounded interaction, correctly depicting the ball being thrown and caught from first-person perspective. Removing either component degrades video synthesis quality. with the input prompt, lacking the key causal event of bowling ball knocking down pins. The T2V + Prompt Aug variant introduces the ball and pins, showing some degree of collision and motion. However, the dynamics are chaotic and physically implausible. Pins deform or scatter in erratic ways, and the scene suffers from visual artifacts and temporal instability, particularly in later frames. In contrast, VChain (Ours) produces coherent and physically grounded sequence. The bowling ball hits the pins with realistic impact, and the pins fall in directions consistent with expected physical behavior. This outcome is enabled by chain-of-visualthought reasoning, which provides the model with structured, causal progression of events. Furthermore, object geometry and material properties are well preserved. Pins and the ball are visually distinct and accurately rendered. Ablation Study. In Figure 4, we compare VChain with two ablated variants: 1) Without Visual Thought, while it understands that the first-person perspective should be generated from the Textual Thoughts, it fails to envision the correct visual pattern of ball-catching POV. In contrast, our method benefits from directly seeing the Visual Thoughts, enabling accurate spatial understanding and rendering of the interaction. 2) Without Sparse Tuning, which includes Visual Thoughts directly performs frame interpolation, and warping artifacts emerge when attempting to bridge spatial misalignments between Visual Thought keyframes. VChain (Ours) produces the most coherent and physically grounded interaction, accurately depicting the ball being thrown and caught. Removing either component leads to degraded video synthesis. Figure 11(b) in Appendices highlights another example of rubber duck and rock falling into water. Without Visual Thought, the duck appears submerged in water, violating the basic physical intuition that rubber ducks are supposed to float. In contrast, our method correctly depicts the duck floating on the waters surface. This underscores the importance of having Visual Thoughts (versus Textual Thoughts only) at inference time: its important to view the Visual Thoughts during inference - to actually see how the rubber duck floats on the water surface rather than sinks. Our demo video provides more intuitive comparison."
        },
        {
            "title": "5 Limitations",
            "content": "5.1 Limitations of Visual Thought Generation Our framework inherits some limitations from the current state of the GPT-4o image generation model. First, we observe that gpt-image-1 tends to slightly oversaturate and over-smooth edited images. Since each generated image is passed back into the model as input to produce the next frame in the image sequence, this effect can accumulate iteratively, leading to yellow color cast and oversmoothness across the image sequence. The artifact slightly undermines the photorealism of later frames and introduces slight color inconsistencies 8 across the sequence. Qualitative examples are provided in the Appendices. Another limitation is the API cost. Each generated keyframe requires two calls to GPT-4o. Thus, the total number of API calls scales linearly with the image sequence length, and the token consumption would be quadratic. Hence, the reliance on proprietary models might limit accessibility and reproducibility for those without access to sufficient compute budget or API quotas. Despite these concerns, the overhead in practice is modest: inferencetime reasoning for video typically requires only 3-6 images, which keeps the cost relatively low. 5.2 Limitations of Sparse Inference-Time Tuning Our method fine-tunes pre-trained video generator using only small number of keyframes, referred to as Visual Thoughts, as supervision. This sparse tuning introduces an inherent trade-off: optimizing too strongly on static keyframes may reduce motion dynamics, since the model adapts primarily to still images, while insufficient optimization may weaken the reasoning signals injected into the generator, producing results closer to the untuned baseline. Despite the potential trade-off, this sparse tuning strategy offers two main advantages: (1) Focused adaptation: the model concentrates its capacity on semantically critical transitions (e.g., melting, breaking, or object interactions) rather than reconstructing entire video sequences. (2) Efficiency, as it eliminates the need for dense videos, significantly reducing both data preparation and computational overhead. This makes our approach well-suited for inference-time integration into existing pipelines. Overall, while sparse supervision cannot fully capture the dynamics in video samples, the improvements in semantic alignment and causal coherence generally outweigh the loss in dynamics. This paradigm also challenges the conventional assumption that full video sequences are required for fine-tuning, showing that carefully selected set of keyframes can already provide sufficient guidance for adapting video generators to new prompts or scenarios."
        },
        {
            "title": "6 Ethical Considerations",
            "content": "While both large multimodal models and video generators can produce vivid and compelling content, users should exercise caution when using AIgenerated media. Outputs may inherit and amplify safety concerns and biases from the multimodal models and video generators they rely on. We strongly advocate for the responsible and ethical use of generative models. Potential Risks. VChain is intended as research contribution, but its ability to improve causal and physical coherence also increases the realism of synthetic videos. This realism could be misused for harmful purposes such as producing disinformation, deepfakes, or fabricated evidence. Moreover, because VChain depends on large multimodal models and pretrained generators, it might propagate their biases into more coherent video narratives, which may reinforce stereotypes or exclusion. We emphasize that VChain is designed for controlled research and creative exploration, not deployment in sensitive or adversarial settings."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we present VChain, general inference-time framework that integrates multimodal reasoning into video generation. By representing scenario as sparse sequence of Visual Thoughts - keyframes capturing critical intermediate states inferred by large multimodal models - VChain injects causal and commonsense reasoning signals directly at inference time. This paradigm enables video generators to model meaningful state transitions without dense annotations or costly retraining. Experiments on complex, multi-step scenarios show that VChain substantially improves the coherence, causal consistency, and rationality of generated videos, while maintaining efficiency and visual quality. More broadly, VChain demonstrates how the reasoning capabilities of large multimodal models can be effectively combined with the rendering and motion priors of video generators. We view this framework as step toward bridging reasoning and generation, and hope to inspire further research on reasoning for video generation."
        },
        {
            "title": "Acknowledgments",
            "content": "This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221-0012, MOE-T2EP20223-0002). This research is also supported by cash and in-kind funding from NTU S-Lab and industry partner(s), and Eyeline Labs."
        },
        {
            "title": "References",
            "content": "2022. Langchain. Accessed March 31, 2025 [Online] https://www.langchain.com. 2023. Minmax team. Accessed August 31, 2024 [Online] https://hailuoai.com/. 2024. Gen-3. Accessed June 17, 2024 [Onhttps://runwayml.com/research/ line] introducing-gen-3-alpha. 2024. Kling. Accessed December 9, 2024 [Online] https://klingai.kuaishou.com/. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, and 1 others. 2025. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, and 1 others. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and 1 others. 2021. CogView: Mastering text-to-image generation via transformers. In NeurIPS. Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. 2022. Cogview2: Faster and better text-to-image generation via hierarchical transformers. In NeurIPS. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and 1 others. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, and 1 others. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In ICML. Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In CVPR. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, and 1 others. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, and 1 others. 2024a. Vbench: Comprehensive benchmark suite for video generative models. In CVPR. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2024b. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Diederik Kingma and Max Welling. 2013. AutoarXiv preprint encoding variational bayes. arXiv:1312.6114. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, and 1 others. 2024. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2023a. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655. Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. 2023b. Llm-grounded video diffusion models. arXiv preprint arXiv:2309.17444. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. William Peebles and Saining Xie. 2022. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039. diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical In International Conference image segmentation. on Medical image computing and computer-assisted intervention. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021a. Denoising diffusion implicit models. In ICLR. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021b. Score-based generative modeling through stochastic differential equations. In ICLR. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. StepFun Team. 2025. Preprint, arXiv:2502.10248. [link]. Aaron Van Den Oord, Oriol Vinyals, and 1 others. 2017. Neural discrete representation learning. In NeurIPS. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, and 1 others. 2025. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, and 1 others. 2024. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072. Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and 1 others. 2023. Magvit: Masked generative video transformer. In CVPR. Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. 2025. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755."
        },
        {
            "title": "Appendices",
            "content": "We provide additional implementation details in Appendix A, and qualitative results in Appendix B. demo video is also available at this link."
        },
        {
            "title": "A Additional Implementation Details",
            "content": "A.1 Implementation Details of Visual Thought Reasoning Given user-provided input prompt describing video, our Visual Thought Reasoning pipeline synthesizes sequence of keyframes which form the crucial moments of the video. The implementation details are as follows. We first prompt GPT-4os chat completions API with the system message shown in Figure 7 to instruct the model to reason about the videos likely spatial layout and anticipated causal consequences based on the user-provided input prompt. We employ LangChain (lan, 2022) to convert GPT-4os unstructured textual outputs into structured schemaaligned responses containing: 1. Context Frame: richly detailed prompt used to generate the first frame in the Chain of Visual Thoughts. 2. Concise Prompt: concise version of the Context Frame prompt (the full version is too long, so the first image is paired with this concise prompt during sparse inference-time tuning). 3. Consequences: sequence of inferred physical outcomes that define the expected trajectory of the generated video. The Context Frame is passed to GPTs gpt-image-1 API to produce the corresponding image. To generate subsequent keyframes in the Chain of Visual Thoughts, we concatenate all previously generated images in our chain into single composite image, as shown in Figure 2. This stitched chain of images, together with the user input prompt and the inferred consequences, is passed to GPT-4os chat completion API using the system message in Figure 8. GPT-4o predicts the next key moment in the sequence. Specifically, the output contains: 1) an image-editing instruction and 2) boolean flag indicating whether terminal state has been reached. We pass the same inputs as before along with the editing instruction to the gpt-image-1 API to generate the next keyframe. We repeat this process iteratively, where we predict the next key moment and generate the corresponding image, until the boolean flag signals that the full sequence of consequences have been realized by the chain. All outputs, including keyframe captions and reasoning chains, are stored in structured JSON file (see Figure 9). We then generate CSV file where each row contains an image file path and its corresponding caption, forming the image-text pairs used to fine-tune the video generation model. The first image is paired with the concise prompt, while each subsequent image is paired with its keyframe (example in Figure 10). Figure 5 shows an example of Chain of Visual Thoughts generated by our pipeline. A.2 Time Cost Table 2 summarizes the average runtime of each stage in VChain, providing detailed breakdown of the overall computational cost. A.3 Implementation Details of Sparse Inference-Time Tuning Our main experiments are conducted using the state-of-the-art pre-trained video generator Wan2.1T2V-1.3B (Wan et al., 2025). We use the learning rate of 1e 4, and fine-tune with train_lora_rank of 16, and train_lora_alpha of 16. A.4 Details of Test Cases We design twenty test cases to support both human and quantitative evaluations. Each case depicts simple, physically grounded scenario that requires causal reasoning to generate coherent outcomes. rock and feather falling from the sky towards the ground. An egg falling from the sky towards concrete ground. An ice cream cone is left out in the sun. rubber duck and rock fall into water tank. steel ball is dropped into water. Milk is poured into cup of black coffee. man falls off pile of bricks. steel ball falling through the air onto ice. ball is dropped onto pillow. sandwich rotting over time. An elderly blows out cake filled with candles. Red and yellow paint are mixed together with brush. Concentrated sulfuric acid is poured onto wooden table. 12 Figure 5: Example of Visual Thoughts. We show the reasoned Visual Thoughts of the input prompt: Concentrated sulfuric acid is poured onto wooden table. The sequence illustrates our pipelines inferred causal progression across keyframes. An egg is dropped onto pillow. mailbox rusting over time in broad daylight. man blows into deflated balloon. Oil is poured into glass of milk. chameleon eats flying insect. Blue and yellow paint are mixed together with brush. cup of water is falling towards the ground on its side. Figure 6: GPT Keyframe Limitations. Qualitative examples showing the accumulated saturation and smoothness artifacts produced by gpt-image-1 during iterative keyframe generation. As each generated image is recursively used as part of the input for the next step, slight over-saturation and over-smoothing compound over time, leading to slight color shifts (e.g., yellow cast) and reduced photorealism in later frames."
        },
        {
            "title": "B Additional Qualitative Results",
            "content": "We present additional qualitative examples illustrating the saturation limitations of Visual Thought Generation in Figure 6. Further qualitative comparisons are shown in Figures 13, 14, 15, 16, 17, 18, and 19."
        },
        {
            "title": "First Frame System Message",
            "content": "You are given text prompt, which describes video. You are to perform the following tasks: 1. Infer the objects/people/elements present in the scene, the perspective of the camera, the spatial relationship between the objects in the scene as well as details not explicitly mentioned in the text prompt. Create detailed, movie-like description of the scene that evokes visuals with strong 2. detail and composition cues. It should clearly describe the objects/people/elements present in the scene, the perspective of the camera, and the spatial relationships between the objects in it as well as the details not explicitly mentioned in the text prompt. This is the Context Frame. The context frame must depict the initial state of the scene, before any action occurs, and must not foreshadow the input prompt. Given an input prompt \"A man throws ball\", the context frame should depict man holding ball at his side, not in mid-throw. Given an input prompt \"A man squeezes ball in his hand\", the context frame should depict man gently holding ball in his palm, not squeezing it yet. Given an input prompt \"A dolphin emerges from the water\", the context frame should depict calm ocean. The dolphin should not be visible yet. The context frame should be written as if it is depicting an image, not video. Hence, it should not foreshadow what will happen next. 3. Create concise version of the context frame. This should be short, one-sentence description of the context frame. 4. Infer sequence of consequences/changes from the text prompt, even if it is not explicitly mentioned. Use assertive languange to clearly describe the changes in appearance, shape, color, size, and position that may occur as result. Example: Input Prompt: \"A cat pushes glass of water off table.\" Thoughts: In order for the cat to tip the glass off the table, the cat is sitting on the table next to the glass of water. In order for the glass of water to fall off the table, it should be placed precariously on the edge of the table. side view perspective would capture the table, the cat, the glass of water in one frame. Context Frame: side view of sleek tabby cat sitting upright on wooden table in kitchen. The glass of water is placed precariously at the very edge of the table. The cat gazes intently at the glass, its tail curled around its body. The camera is at mid-height, framing the cat, table, glass, and floor clearly in the shot. Concise Prompt: cat sits next to glass of water on table. Consequences: The cat will touch the glass of water, causing it to tip over the edge of the table and fall towards the ground. The glass of water will touch the ground and shatter as result. The water will spill everywhere and glass shards will be on the floor. Additional Examples Figure 7: First Frame System Message."
        },
        {
            "title": "Next Frame System Message",
            "content": "You are given an input prompt, which describes video. You are also given sequence of keyframes (1 or more keyframes), meant to depict key moments of the video. You are also given hint, describing what happens throughout the video. You are to predict the next keyframe in the sequence. Use precise language to clearly describe the changes that may occur in this next key. You must predict what may happen within the next 5 seconds of the video. Hence, do not predict too far into the future. keyframe is still image that captures either the start, peak/intermediate stage, the end, or the consequence of an event. The next predicted keyframe MUST ONLY depict either the start, peak, end, or consequence of an event and NEVER combination of them. e.g. The key moments of kicking ball into goal are (1) The moment the foot makes contact with the ball. The ball should not have moved at this point., (2) The moment the ball is inside the goal. e.g. The key moments of ice melting are (1) When the ice is fully solid (2) The moment the ice cube is half melted (3) The moment the ice cube is completely melted with large puddle of water. e.g. The key moments of glass of water falling off table are (1) The moment the glass of water is on the edge of the table, (2) The moment the glass of water is falling midair towards the ground (3) The moment the glass of water makes contact with the ground but is still in one piece. (4) The moment the glass of water shatters on the ground and the water spills everywhere. If the next key moment involves contact between two objects, then the next keyframe must depict the moment of contact. The objects must be touching in the next predicted key rame description. Your caption for the next keyframe should not use comparative language to describe relative change in position, distance, or size (e.g. towards, away from). Instead, it should describe the absolute position, distance, or size of the objects involved. If possible, use spatial prepositions to clarify the relationship between objects (e.g. inside of, on top of, and below). The next keyframe should describe the image as if it not in motion. Hence, avoid using phrases like about to, going to. Describe the next keyframe as if it is still image. Finally, return True if the next predicted keyframe is the last frame of this video, otherwise, return False. If nothing significant happens after the next predicted keyframe, return True. Example: Input Prompt: \"A cat pushes glass of water off table\" Hint: The glass of water will fall off and shatter on the floor. The water will spill everywhere and glass shards will be on the floor. Keyframe 1: [A cat is sitting on the table, and the glass of water is on the edge of the table.] Next Predicted Keyframe: The cats paw is touching the glass of water sitting on the table. Last Frame: False Additional Examples Figure 8: Next Frame System Message."
        },
        {
            "title": "Reasoning Output Example",
            "content": "\" sulfuric_acid \": { \" input_prompt \": \" Concentrated sulfuric acid is poured onto wooden table .\" , \" thoughts \": \" The scene involves wooden table , likely in laboratory or workshop setting , where concentrated sulfuric acid is about to be poured . The acid is typically stored in glass or plastic container , and the person pouring it might be wearing protective gear such as gloves and goggles . The camera should capture side view to show the table , the container of acid , and the person pouring it . The table is initially dry and intact , with visible wood grain .\" , \" consequences \": \" As the sulfuric acid is poured onto the wooden table , it will react with the wood , causing it to char and emit smoke . The wood will darken and potentially start to disintegrate where the acid makes contact , creating burnt , uneven surface . The reaction may produce heat and release fumes , necessitating proper ventilation and safety precautions .\" , \" context_frame \": \" In well - lit laboratory , sturdy wooden table stands at the center of the scene , its surface smooth and polished , with visible wood grain patterns . person , wearing protective gloves and goggles , stands beside the table , holding glass container filled with concentrated sulfuric acid . The container is tilted slightly , poised to pour . The camera captures side view , framing the table , the container , and the person , highlighting the contrast between the clear , viscous liquid and the warm tones of the wood .\" , \" concise_prompt \": \"A person stands beside wooden table , holding container of concentrated sulfuric acid .\" , \" key_frames \": [ \" The area where the concentrated sulfuric acid makes contact with the wooden table starts to darken and emit smoke . The wood grain appears charred and blackened , with visible smoke rising from the surface . The edges of the darkened area are irregular , indicating the beginning of disintegration . \" , \" The concentrated sulfuric acid has been poured onto the wooden table . small , blackened , and charred area is visible on the table where the acid has made contact . Smoke is rising from the reaction site , and the wood grain around the area has started to darken and disintegrate slightly , illustrating the corrosive impact of the acid .\" , \" The concentrated sulfuric acid creates deep , blackened mark on the wooden table where it has been poured . The wood is significantly charred with smoke wafting upwards , forming small plume . The surrounding area of the wood appears darker , with slight disintegration at the center of the spill , indicating intense chemical reaction . \" , \" The concentrated sulfuric acid has reacted with the wooden table , and the area of contact has expanded . The wood appears darkened and severely burnt , with visible smoke and fumes rising prominently into the air . The wooden surface is visibly damaged , with large burnt patches and disintegrated material , showing an uneven and charred texture . The person remains focused on observing the reaction , and the container of acid is still slightly tilted above the table .\" ] } Figure 9: Reasoning Output Example."
        },
        {
            "title": "CSV Output Example",
            "content": "\"file_name\",\"text\" \"sulfuric_acid_0.png\", \"A person stands beside wooden table, holding container of concentrated sulfuric acid.\" \"sulfuric_acid_1.png\",\"The area where the concentrated sulfuric acid makes contact with the wooden table starts to darken and emit smoke. The wood grain appears charred and blackened, with visible smoke rising from the surface. The edges of the darkened area are irregular, indicating the beginning of disintegration.\" \"sulfuric_acid_2.png\",\"The concentrated sulfuric acid has been poured onto the wooden table. small, blackened, and charred area is visible on the table where the acid has made contact. Smoke is rising from the reaction site, and the wood grain around the area has started to darken and disintegrate slightly, illustrating the corrosive impact of the acid.\" \"sulfuric_acid_3.png\",\"The concentrated sulfuric acid creates deep, blackened mark on the wooden table where it has been poured. The wood is significantly charred with smoke wafting upwards, forming small plume. The surrounding area of the wood appears darker, with slight disintegration at the center of the spill, indicating intense chemical reaction.\" \"sulfuric_acid_4.png\", \"The concentrated sulfuric acid has reacted with the wooden table, and the area of contact has expanded. The wood appears darkened and severely burnt, with visible smoke and fumes rising prominently into the air. The wooden surface is visibly damaged, with large burnt patches and disintegrated material, showing an uneven and charred texture. The person remains focused on observing the reaction, and the container of acid is still slightly tilted above the table.\" Figure 10: CSV Output Example. Table 2: Time Cost Breakdown. Breakdown Time Cost Comments Visual Thought Reasoning initial reasoning image generation image perception image editing 3 min 3 sec 14 sec 1 min 7 sec 16 sec 1 min 26 sec API: gpt-4o chat completions, called once for every sequence, CPU API: gpt-image-1 generate, called once for every sequence, CPU API: gpt-4o vqa, called 2.5 times (Averaged across 35 sequences), CPU API: gpt-image-1 edit, called 2.5 times (Averaged across 35 sequences), CPU Sparse Inference-Time Tuning pre-process visual thoughts for fine-tuning load model fine-tuning 5 min 36 sec Wan2.1-T2V-1.3B, 480832, 81 frames, NVIDIA A100 GPU 30 sec 6 sec 5 min including checkpoint saving Sparse Inference-Time Tuning pre-process visual thoughts for fine-tuning load model fine-tuning 6 min 56 sec Wan2.1-T2V-14B, 480832, 81 frames, NVIDIA A100 GPU 30 sec 20 sec 6 min 6 sec including checkpoint saving Video Sampling model loading sampling VAE decoding & video saving Video Sampling model loading sampling VAE decoding & video saving Wan2.1-T2V-1.3B, 480832, 81 frames, NVIDIA A100 GPU could save time by not saving then re-loading checkpoint upon tuning 3 min 9 sec 14 sec 2 min 46 sec 9 sec 14 min 48 sec Wan2.1-T2V-14B, 480832, 81 frames, NVIDIA A100 GPU 33 sec 14 min 06 sec 9 sec could save time by not saving then re-loading checkpoint upon tuning 17 Figure 11: More Qualitative Comparisons. 18 Figure 12: More Qualitative Comparisons. 19 Figure 13: Additional Qualitative Comparisons - Egg Fall. Figure 14: Additional Qualitative Comparisons - Pillow. Figure 15: Additional Qualitative Comparisons - Rocket Feather. 20 Figure 16: Additional Qualitative Comparisons - Cup. Figure 17: Additional Qualitative Comparisons - Egg Pillow. Figure 18: Additional Qualitative Comparisons - Oil Milk. Figure 19: Additional Qualitative Comparisons - Orange."
        }
    ],
    "affiliations": [
        "Eyeline Labs",
        "Nanyang Technological University"
    ]
}