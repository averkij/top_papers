{
    "paper_title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models",
    "authors": [
        "Liang Wang",
        "Yu Rong",
        "Tingyang Xu",
        "Zhenyi Zhong",
        "Zhiyuan Liu",
        "Pengju Wang",
        "Deli Zhao",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 5 8 6 0 . 7 0 5 2 : r DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models Liang Wang1,2,3, Yu Rong4,5, Tingyang Xu4,5, Zhenyi Zhong6, Zhiyuan Liu3, Pengju Wang4,5, Deli Zhao4,5, Qiang Liu1,2, Shu Wu1,2, Liang Wang1,2 1NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences. 2School of Artificial Intelligence, University of Chinese Academy of Sciences. 3National University of Singapore. 4DAMO Academy, Alibaba Group. 5Hupan Lab. 6College of Intelligence and Computing, Tianjin University. Contributing authors: liang.wang@cripac.ia.ac.cn; yu.rong@hotmail.com; xuty 007@hotmail.com; zhenyi zhong@tju.edu.cn; acharkq@gmail.com; weichang.wpj@alibaba-inc.com; zhaodeli@gmail.com; qiang.liu@nlpr.ia.ac.cn; shu.wu@nlpr.ia.ac.cn; wangliang@nlpr.ia.ac.cn; Abstract Molecular structure elucidation from spectra is foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, transformer-based spectral encoder that captures intraand inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation. Keywords: Molecular Structure Elucidation, Molecular Spectra, Diffusion Models"
        },
        {
            "title": "1 Introduction",
            "content": "Molecular structure elucidation represents one of the fundamental challenges in chemistry, materials science, and biology [13], requiring the accurate determination of atomic connectivity, stereochemistry, and three-dimensional (3D) arrangements based on experimental data. Traditional approaches often 1 require expert interpretation and substantial manual effort to propose and validate structural hypotheses. Although these methods have proven invaluable over decades of chemical research, the increasing complexity of natural products, synthetic compounds, and pharmaceutical intermediates demands more sophisticated computational approaches that can efficiently navigate the vast and complex chemical space of possible structures. The integration of machine learning into spectroscopic analysis has opened new avenues for automating and accelerating molecular structure determination. Early applications of machine learning to molecular structure elucidation primarily adopted retrieval or search-based paradigms [1, 4, 5]. These approaches treat the task as matching problem: given an experimental spectrum, the model searches within predefined molecular library to retrieve candidate structures whose spectra best align with the observed one. Although efficient in practice, these methods are inherently constrained by the coverage and quality of the reference library. To mitigate this limitation, some efforts have proposed spectral prediction models, which simulate spectra from known molecular structures [612]. These predicted spectra are then used to expand the searchable library and improve retrieval coverage. However, such methods still rely on the existence of finite reference library and fundamentally struggle to generalize to novel molecules that are not well represented in the library, limiting their applicability in open-ended or exploratory chemical analyses. To overcome the reliance on molecular libraries and enable fully data-driven structure elucidation, recent research has turned to more expressive neural architectures capable of de novo structure inference directly from spectra. Initial attempts in this direction framed the task as predictive problem, training models to infer coarse structural features from spectra, such as molecular properties [13], fingerprints [14], fragments [15, 16], chemical formulas [17], or other approximate structure profiles [1821]. Although informative, these coarse predictions do not achieve complete molecular structure elucidation. In shift toward complete structure elucidation, subsequent works introduced generative models that take spectral data as input and produce discrete molecular representations, such as SMILES [22] or molecular graphs [23]. These models typically follow autoregressive architectures, generating molecular structures token by token using RNNs [2426] or Transformers [4, 16, 2730]. Despite their promise, they face two fundamental limitations. First, generative models built on discrete SMILES sequences or graphs inherently lack geometric inductive biases and cannot effectively capture 3D geometric information. This is critical because many spectral modalities, such as infrared (IR), Raman, and ultraviolet-visible (UVVis), are inherently dependent on the molecules geometry and associated energy states [31]. Second, all existing methods are limited to processing single type of spectrum in isolation, whereas molecular structure elucidation in practice relies on the joint interpretation of multiple spectral modalities, each offering complementary structural cues [32]. Current models lack support for such multi-modal spectral reasoning, thereby constraining their accuracy and generalizability. These limitations highlight the need for more principled and unified generative framework that can integrate both geometric reasoning and multi-modal spectral conditioning. The inherent limitations of existing approaches motivate the development of more sophisticated generative models that can effectively bridge the gap between spectral observations and molecular structure determination. Diffusion models have recently attracted significant attention in generative modeling due to their ability to produce high-quality samples through iterative denoising processes [33, 34]. This framework has demonstrated remarkable success in modeling complex, multi-modal distributions and generating diverse outputs, and has also shown particular promise in molecular applications [3, 35]. The ability of diffusion models to generate chemically meaningful structures and handle multi-modal distributions makes them particularly well-suited for molecular structure elucidation. In this work, we introduce DiffSpectra, novel framework that leverages diffusion models to directly generate molecular structures from multi-modal spectra. Our approach addresses the structure elucidation problem as conditional generation task, where spectral measurements serve as conditioning information to guide the iterative denoising process towards generating plausible molecular structures. At the core of our framework lies DiffSpectra, joint 2D and 3D diffusion model that simultaneously generates both molecular graphs and atomic coordinates within unified diffusion framework. Each molecule is represented as graph comprising node features (e.g., atom types), edge features (e.g., bond types), and 3D coordinates. This unified representation captures both the topological structure and spatial geometry. DiffSpectra formulates structure elucidation as continuous-time conditional diffusion process, where noise is progressively added and then removed under the condition of spectral constraints. By simultaneously modeling both topological and geometric components, the model ensures that generated structures satisfy chemical bonding rules while preserving spatial consistency. This dualspace generation is essential for accurately reconstructing molecules whose spectra reflect both bonding 2 connectivities and 3D conformations, effectively leveraging the complementary strengths of 2D topology and 3D geometry to ensure structurally consistent outputs. To parameterize the denoising process, we introduce the Diffusion Molecule Transformer (DMT), an SE(3)-equivariant architecture designed for molecular structure generation. DMT consists of stack of transformer-style blocks operating on three interleaved streamsnode features, edge features, and coordinatesallowing for rich interaction between topological and geometric representations. Through geometry-aware multi-head attention and equivariant updates, DMT effectively models spatial relationships and maintains chemical symmetry. To effectively condition the generation process on multi-modal spectra, we employ SpecFormer, transformer-based spectral encoder specifically designed to process and integrate multiple types of molecular spectra simultaneously, including IR, Raman, and UV-Vis. SpecFormer partitions each spectrum into local patches, embeds them into unified latent space, and employs multi-head self-attention to capture intraand inter-spectrum dependencies. To enhance the quality of spectral embeddings, we pre-train SpecFormer using masked patch reconstruction and contrastive alignment with molecular structures. The resulting spectral embeddings provides powerful conditioning signals for DiffSpectra, effectively guiding generation toward candidates that are consistent with the measured spectra. We extensively evaluate DiffSpectra to assess its effectiveness in structure elucidation from multimodal spectra. Our results show that DiffSpectra generates chemically valid and stable molecular structures. When conditioned on spectra, our model attains top-1 exact structure accuracy of 16.01%, and accurately reconstructs key functional groups with over 96% similarity. We further show that using pre-trained SpecFormer significantly improves structure elucidation accuracy, and that combining multiple spectral modalities yields better performance than using any single spectrum alone. Importantly, we find that sampling multiple candidates greatly enhances the hit rate: the top-20 accuracy rises to 96.86%, indicating that correct structures are consistently ranked among the plausible outputs. This highlights the practical value of diffusion-based sampling in structure elucidation, where generating shortlist of candidates enables reliable downstream validation. Our ablations further confirm the advantage of model-based SE(3) equivariance and demonstrate that moderate sampling temperatures strike good balance between diversity and accuracy. These findings highlight the potential of spectrum-guided diffusion models for accurate molecular structure elucidation. To the best of our knowledge, this is the first work to leverage diffusion models for elucidating complete 3D molecular structures directly from multi-modal spectral data."
        },
        {
            "title": "2.1 Overview of DiffSpectra Framework",
            "content": "To achieve accurate molecular structure elucidation from molecular spectra, we propose DiffSpectra, spectrum-conditioned diffusion framework that generates molecular structures guided by spectral information. The overall paradigm is illustrated in Figure 1. Formally, molecule characterized by its structure and spectra is represented as = (G, S). The molecular structure is modeled as graph = (H, A, X), where RN d1 denotes node-level attributes such as atom types and charges, RN d2 encodes pairwise edge features including bond existence and bond types, and RN 3 corresponds to the 3D coordinates of the atoms. Here, is the number of atoms, d1 is the node feature dimensionality, and d2 is the edge feature dimensionality. The atomic ordering is consistent across and to preserve the correspondence between atom types and spatial positions. Meanwhile, the molecular spectra are denoted as = (s1, . . . , sS), where = 3 in our setting, covering UV-Vis, IR, and Raman spectra. Concretely, s1 R601 records the UV-Vis spectrum from 1.5 to 13.5 eV with 0.02 eV intervals; s2 R3501 is the IR spectrum spanning 5004000 cm1 at 1 cm1 resolution; and s3 R3501 is the Raman spectrum on the same range. Together, these modalities provide comprehensive description of the molecular signatures. 3 Fig. 1: (A) Overview of the DiffSpectra framework, illustrating the continuous-time forward diffusion process and reverse denoising process. The Diffusion Molecule Transformer (DMT) is used as the denoising network, and spectra encoded by SpecFormer serve as conditional input. (B) Architecture of the DMT, which processes node features, edge features, and atomic coordinates through three parallel streams with shared condition encoding, relational multi-head attention, and equivariant updates. (C) Architecture and pre-training strategy of SpecFormer, which encodes multi-modal spectra (UV-Vis, IR, and Raman) through unified transformer encoder. It is pre-trained via masked patch reconstruction (MPR) and contrastive learning with 3D molecular structures. Within DiffSpectra, continuous-time variational diffusion model progressively perturbs the molecular graph with Gaussian noise during the forward process, then denoises it in reverse, conditioned on spectra-derived information. dedicated backbone, the Diffusion Molecule Transformer, parameterizes the denoising process while maintaining permutation and SE(3) equivariance. DMT is guided by spectral embeddings extracted by SpecFormer, Transformer-based encoder pre-trained with both masked 4 reconstruction and contrastive alignment to capture spectrumstructure correlations. This design allows the framework to achieve faithful and chemically meaningful structure elucidation, supported by physically grounded diffusion and rich spectral priors. More details of the full architecture and training objectives are presented in Section 4. Table 1: Evaluation of basic metrics for molecular generation on the QM9S dataset. Metrics include stability, validity, uniqueness, and novelty, as well as distribution-based and 3D geometry-based metrics. The Train row reports statistics from the training set as reference distribution. DiffSpectra is compared to unconditional molecular generative models. Metric-2D Atom stable Mol stable V&C V&U V&U&N FCD SNN Frag Scaf Train CDGS [36] JODO [37] DiffSpectra 99.9% 99.7% 99.9% 99.9% 98.8% 95.1% 98.8% 98.2% 98.9% 98.9% 95.1% 93.6% 99.0% 96.0% 0.0% 89.8% 89.5% 0.063 0.798 0.138 0.490 0.493 0. 0.992 0.973 0.986 0.946 0.784 0.934 98.4% 96.8% 93.7% 0.088 0.531 0.992 0.943 Metric-3D Atom stable Mol stable FCD Bond Angle Dihedral Train E-NF [38] G-SchNet [39] G-SphereNet [40] EDM [41] MDM [42] GeoLDM [43] JODO [37] DiffSpectra 99.4% 84.7% 95.7% 67.2% 98.6% 99.2% 98.9% 99.2% 99.2% 95.3% 4.5% 68.1% 13.4% 81.7% 89.6% 89.7% 93.4% 92.9% 0. 5.44e4 4.65e4 1.78e4 4.452 2.386 6.659 1.285 4.861 1.030 0.885 0.943 0.6165 0.3622 0.1511 0.1303 0.2735 0.2404 0. 0.4203 0.0727 0.3537 0.0182 0.0660 0.0100 0.0121 0.0056 0.0042 0.0129 6.64e-4 0.0239 6.59e-4 6.29e-4 0.1792 0.0054 2.01e-"
        },
        {
            "title": "2.2 DiffSpectra generates valid and stable molecular structures",
            "content": "As summarized in Table 1, our proposed DiffSpectra consistently achieves strong results in generating chemically valid and stable molecular structures under spectrum-conditioned settings. In the 2D evaluation, DiffSpectra attains 99.9% atom stability and 98.2% molecular stability, demonstrating performance comparable to the training data distribution, while outperforming or matching general molecular generative models such as CDGS [36] and JODO [37]. Moreover, DiffSpectra maintains high structural uniqueness and novelty, with V&U (valid and unique) at 96.8% and V&U&N (valid, unique, and novel) at 93.7%, significantly surpassing prior methods. In distribution-based metrics, DiffSpectra achieves the lowest FCD, indicating close distributional match to the reference test set. It also reaches the best SNN, Frag, and Scaf scores, demonstrating strong capacity to cover diverse chemical substructures and scaffolds while maintaining validity. In the 3D evaluation, DiffSpectra demonstrates competitive geometric stability, with atom stability and molecular stability again comparable to the training set. Notably, DiffSpectra achieves the best angle and dihedral MMDs, suggesting accurate recovery of bond angles and torsional angles, and achieves bond distance MMD comparable to the top-performing baselines. This highlights DiffSpectra effectiveness in reconstructing not only 2D connectivity but also realistic 3D molecular conformations. The Train row in Table 1 reports the statistics of the training dataset as reference distribution. For some metrics that measure similarity to the test set, such as SNN, Frag, Scaf, or geometric MMDs, the training data does not necessarily achieve the best possible scores, since the test distribution may differ from the training distribution. All other baselines listed in Table 1, including CDGS [36], JODO [37], EDM [41], and others, are unconditional molecular generative models, which sample from the general molecular space without explicit external guidance. In contrast, DiffSpectra is specifically designed for molecular structure elucidation conditioned on spectra. The use of spectral signals as conditions provides external structural priors, enabling the model to generate results that better match the test set distribution. Therefore, the higher V&U, V&U&N, SNN, Frag, Scaf, and MMD metrics achieved by DiffSpectra compared to unconditional models are expected, as the conditioning naturally constrains the generation process toward structures consistent with the observed test samples. 5 Fig. 2: Visualization of molecular structure elucidation results using DiffSpectra under different configurations. We compare single-spectrum inputs (IR, Raman, UV-Vis), multi-modal spectra, and the effect of incorporating the pre-trained SpecFormer. Ground-truth structures are shown on the left for reference."
        },
        {
            "title": "2.3 DiffSpectra accurately elucidates molecular structures from spectra",
            "content": "As shown in Table 2 and Figure 2, DiffSpectra demonstrates strong performance in the challenging task of molecular structure elucidation from spectra. First, the top-1 accuracy (ACC@1) reaches 16.01%, meaning that the model is able to recover the exact target structure among its predictions in nontrivial fraction of cases. Beyond strict exact matches, the average MCES score is 1.3552, indicating that even when the model fails to generate an exact structure, it still preserves substantial portion of the molecular graph connectivity. In terms of fingerprint-based similarity metrics, DiffSpectra achieves Tanimoto similarity of 0.7837 and cosine similarity of 0.8421 over Morgan fingerprints, reflecting high agreement on local structural features. The Tanimoto similarity over MACCS keys is even higher at 0.9227, highlighting good coverage of predefined chemical substructures. These results suggest that DiffSpectra captures the key functional groups and local motifs characteristic of the target molecules. 6 Table 2: Structure elucidation performance of DiffSpectra on the QM9S dataset. We compare two configurations: one using pre-trained SpecFormer as the spectral condition encoder, and one using an untrained SpecFormer. Reported metrics include exact structure recovery, graph overlap, fingerprintbased similarities, fragment-level similarity, and functional group similarity. Model DiffSpectra Pre-trained SpecFormer ACC@1 MCES TaniSimMG CosSimMG TaniSimMA FraggleSim FGSim 0.9618 0.9490 16.01% 1.3552 1.7795 14.11% 0.7837 0.7205 0.9481 0.9383 0.8421 0.7938 0.9227 0. Furthermore, the fragment-based Fraggle similarity reaches 0.9481, showing that large chemically meaningful fragments are well recovered. The functional group similarity (FGSim) is also high at 0.9618, confirming that the predicted molecules retain nearly all functional group types present in the ground-truth structures. Overall, these metrics collectively indicate that DiffSpectra is able to accurately elucidate molecular structures from spectra, recovering both global connectivity and local functional features with high fidelity."
        },
        {
            "title": "2.4 Pre-trained SpecFormer facilitates more accurate structure elucidation",
            "content": "To investigate the effect of pre-training the spectrum encoder, we conduct an ablation study comparing DiffSpectra with and without pre-trained SpecFormer module, as summarized in Table 2. When equipped with the pre-trained SpecFormer, DiffSpectra achieves top-1 accuracy of 16.01%, compared to 14.11% without pre-training, indicating clear improvement in correctly recovering the ground-truth molecular structures from spectra. Similarly, the MCES decreases from 1.7795 to 1.3552, suggesting that the molecular graphs predicted with pre-trained SpecFormer retain more of the correct connectivity of the target structures. In addition, the fingerprint-based similarity scores also improve with pre-training. For example, the Tanimoto similarity over Morgan fingerprints rises from 0.7205 to 0.7837, and cosine similarity improves from 0.7938 to 0.8421. The Tanimoto similarity over MACCS keys also increases from 0.8924 to 0.9227. These gains indicate that the molecular structures generated with pre-trained SpecFormer more accurately capture both local substructural patterns and functional group motifs. Moreover, the Fraggle similarity improves from 0.9383 to 0.9481, and FGSim from 0.9490 to 0.9618, confirming that larger chemical fragments and functional groups are better preserved. Overall, these results demonstrate that pre-training the SpecFormer encoder on spectral data provides beneficial inductive biases, allowing DiffSpectra to more effectively align spectral representations with molecular graph structures during conditional generation. This highlights the value of leveraging well-trained spectrum encoder to facilitate accurate molecular structure elucidation from spectra. Table 3: Effect of spectral modalities on structure elucidation performance. We report DiffSpectras results on the QM9S dataset using different types of spectra as conditional input, including IR, Raman, UV-Vis, and their combination. Metrics include exact structure recovery, graph overlap, fingerprintbased similarities, fragment-level similarity, and functional group similarity. Model DiffSpectra Spectral Modalities ACC@1 MCES TaniSimMG CosSimMG TaniSimMA FraggleSim FGSim 0.9490 0.9269 0.9315 0.4567 14.11% 1.7795 2.4812 10.97% 2.1708 12.51% 8.7909 0.10% All Spectra Only IR Only Raman Only UV-Vis 0.7938 0.7188 0.7612 0.2625 0.9383 0.9197 0.9343 0.5581 0.7205 0.6246 0.6778 0. 0.8924 0.8460 0.8701 0."
        },
        {
            "title": "2.5 Multi-modal spectra outperform single-modality spectra for structure",
            "content": "elucidation To further investigate the contribution of different spectral modalities, we conduct an ablation study where DiffSpectra is conditioned on individual or combined spectra types. As summarized in Table 3, using all spectra modalities (IR, Raman, UV-Vis) together achieves the best structure elucidation results, with top-1 accuracy of 14.11% and the lowest MCES of 1.7795, indicating improved graph connectivity recovery. The fingerprint-based metrics also show higher similarities when leveraging all spectra, with TanimotoMG at 0.7205 and CosineMG at 0.7938, suggesting better reconstruction of local 7 molecular patterns. Likewise, the Tanimoto similarity on MACCS keys reaches 0.8924, and the Fraggle similarity is 0.9383, indicating strong preservation of meaningful chemical fragments. Among individual modalities, Raman spectra alone outperform IR and UV-Vis, with ACC@1 of 12.51% and reasonable similarity scores. IR spectra alone still provide useful chemical information, achieving 10.97% top-1 accuracy, while UV-Vis alone performs poorly (0.1% top-1 accuracy), reflecting its limited ability to uniquely identify molecular structures in the QM9S dataset. Overall, these results highlight that combining multiple spectra modalities provides complementary structural priors, enabling DiffSpectra to generate molecular structures with higher fidelity and consistency to ground-truth targets."
        },
        {
            "title": "2.6 Sampling multiple candidates improves structural hit accuracy",
            "content": "Due to the inherently stochastic nature of diffusion models, each sampling process can produce distinct yet plausible molecular structure. While single sample may not always match the ground-truth structure exactly, generating multiple candidates increases the likelihood that at least one of them will align with the correct structure. This motivates the use of Top-K Accuracy (ACC@K) as more comprehensive evaluation metric, which is formally defined in Appendix C.2. We present in Figure 3 the performance of different model variants under varying numbers of samples. Specifically, ACC@K measures the proportion of test molecules for which the correct structure appears among the top generated candidates. For instance, the ACC@1 reflects the exact match rate under single-sample generation, while ACC@K (K > 1) demonstrate that allowing multiple guesses significantly boosts hit probability. Across all model variants, we observe consistent upward trend in Accuracy@K as increases. As increases, the model has more opportunities to generate correct structure among its top-K outputs. For example, the full model with all spectra and pre-trained SpecFormer improves from 16.01% at = 1 to 96.86% at = 20, showing that even small sampling budget can significantly improve overall accuracy. Fig. 3: Accuracy@K with increasing number of sampled candidates. We evaluate the top-K accuracy (ACC@K) as the number of generated candidates increases, comparing different variants of our model. Across all settings, Accuracy@K consistently improves with larger K, confirming that multiple sampling significantly increases the chance of recovering the correct molecular structure. These results highlight an important property of diffusion-based molecular elucidation: even when the model may not always generate the correct structure in one shot, it maintains strong ability to place the correct structure within small number of plausible candidates. This aligns well with practical use cases, where ranked list of likely structures can be provided for downstream validation or experimental testing. The effectiveness of multi-sample inference further demonstrates how spectral conditioning constrains the generative space, enabling the model to consistently place the correct structure among top candidates. Table 4: Evaluation of different SE(3) equivariance strategies for DiffSpectra. We compare model-based equivariant architecture to data-based equivariance approaches, with or without data augmentation, on molecular structure elucidation metrics. Here, data aug refers to data augmentation. Model SE(3) Equivariance DiffSpectra model-based data-based (w/ data aug) data-based (w/o data aug) ACC@1 MCES TaniSimMG CosSimMG TaniSimMA FraggleSim FGSim 14.11% 1.7795 1.8785 12.98% 3.6036 7.47% 0.9490 0.9456 0.8607 0.7938 0.7877 0. 0.7205 0.8882 0.5117 0.9383 0.9389 0.8665 0.8924 0.8882 0."
        },
        {
            "title": "2.7 Comparing model-based and data-based SE(3) equivariance strategies",
            "content": "Since our model performs diffusion generation on molecular 3D structures, it is essential to ensure SE(3) equivariance so that the models predictions are consistent under rigid-body transformations of the input. To this end, we design and evaluate two strategies for achieving SE(3) equivariance: model-based equivariance and data-based equivariance. In the model-based design, geometric inductive biases are explicitly incorporated into the network architecture, for example through pairwise distance encoding and equivariant coordinate update mechanisms, ensuring equivariant behavior. In contrast, the databased design does not include SE(3)-equivariant inductive biases in the model architecture. Instead, it relies on random rotation and translation data augmentation during training, encouraging the model to empirically learn equivariance from data. As shown in Table 4, the model-based SE(3)-equivariant implementation of DiffSpectra achieves superior performance across most metrics. It attains top-1 accuracy of 14.11% and the lowest MCES (1.7795), indicating stronger recovery of molecular graph connectivity. The data-based approach with data augmentation achieves slightly lower top-1 accuracy (12.98%) but shows competitive fingerprintbased and functional group similarities, with TanimotoMG, FraggleSim, and FGSim comparable to the model-based design. Notably, when data augmentation is removed from the data-based design, performance degrades substantially across all metrics, confirming the critical role of data augmentation in enabling the model to generalize over molecular rotations and translations. Overall, these results demonstrate that while data-based equivariance with augmentation can help achieve some level of SE(3)-aware behavior, explicitly enforcing geometric inductive biases through model-based equivariance is more robust and effective strategy for molecular structure elucidation from spectra. Table 5: Effect of sampling temperature on structure elucidation performance. We report DiffSpectra results with different sampling temperature coefficients τ , which scale the injected noise during diffusion sampling. Moderate values of τ help balance diversity and accuracy, while extremely low or high τ degrade performance. Model Temperature ACC@1 MCES TaniSimMG CosSimMG TaniSimMA FraggleSim FGSim DiffSpectra τ =1.2 τ =1.0 τ =0.8 τ =0.6 τ =0.4 τ =0.2 15.45% 16.01% 16.30% 16.05% 15.86% 14.56% 1.4224 1.3552 1.3643 1.4009 1.5081 1.9842 0.7739 0.7837 0.7848 0.7794 0.7629 0.7005 0.8348 0.8421 0.8429 0.8387 0.8261 0.7774 0.9197 0.9227 0.9235 0.9220 0.9120 0. 0.9468 0.9481 0.9491 0.9480 0.9442 0.9297 0.9604 0.9618 0.9621 0.9627 0.9577 0."
        },
        {
            "title": "2.8 Sampling temperature balances diversity and accuracy",
            "content": "Since given molecular spectrum corresponds to unique molecular structure, the task of molecular structure elucidation from spectra requires high accuracy in the generated results. However, diffusion models inherently introduce stochasticity during sampling due to the injection of random noise, which is essential for promoting diversity in the generated results. To better understand the trade-off between stochasticity and accuracy, we investigate how controlling the level of stochasticity affects performance. Specifically, we introduce sampling temperature parameter τ in Eq. (6), which modulates the scale of the injected noise during the sampling process. Lower τ values reduce the influence of stochasticity, resulting in more deterministic outputs, while higher τ allows for greater exploration of the molecular space. As shown in Table 5, we evaluate DiffSpectra under varying τ values ranging from 0.2 to 1.2. We observe that moderate temperature values (τ = 0.8 and τ = 1.0) achieve the best balance between diversity and correctness. In particular, τ = 0.8 obtains the highest top-1 accuracy of 16.30%, while maintaining competitive similarity metrics, including TanimotoMG of 0.7848, CosineMG of 0.8429, and TanimotoMA of 0.9235. Extremely low temperatures (e.g., τ = 0.2) result in overly deterministic generations with reduced diversity and higher MCES (1.9842), while extremely high temperatures (e.g., τ = 1.2) also degrade performance due to excessive stochasticity. Overall, these results suggest that selecting moderate τ helps DiffSpectra strike an effective tradeoff between generating structurally diverse candidates and maintaining high similarity to the groundtruth molecules."
        },
        {
            "title": "3 Conclusion",
            "content": "In this work, we present DiffSpectra, novel spectrum-conditioned diffusion framework for molecular structure elucidation. By integrating continuous-time variational diffusion model with Transformerbased molecular graph backbone, DiffSpectra is capable of generating molecular graphs with consistent 2D and 3D structures under spectral conditions. To encode spectral conditions, we introduce SpecFormer, multi-spectrum Transformer encoder, which is further pre-trained via masked patch reconstruction and contrastive learning to align spectra with molecular structures. Through extensive experiments, we demonstrate that DiffSpectra achieves superior performance in recovering molecular structures from spectra, benefiting from the incorporation of pre-trained SpecFormer and multi-modal spectral inputs. In addition, our analysis shows that model-based SE(3) equivariant architecture provides robust geometric consistency, while proper sampling temperature helps balance stochasticity and accuracy in generation. Our work paves the way for several promising directions for future research. Firstly, expanding the diversity and quantity of molecular spectra data could further boost the performance of spectralconditioned generation. Secondly, extending the framework to handle even more complex spectra modalities, such as NMR or mass spectra, could generalize the method toward broader applications in analytical chemistry. Lastly, adapting DiffSpectra to larger biomolecular or materials systems may open opportunities for molecular identification in drug discovery and materials science, especially when combined with high-throughput experimental pipelines. We believe DiffSpectra represents significant step toward trustworthy and accurate molecular structure elucidation conditioned on spectral data."
        },
        {
            "title": "4.1 Diffusion Model for Molecular Structure Elucidation",
            "content": "We adopt continuous-time diffusion probabilistic model to generate molecular graphs in joint space that integrates 2D topology and 3D geometry. Each molecule is represented as graph = (H, A, X), where RN d1 denotes node-level attributes such as atom types and charges, RN d2 encodes pairwise edge features such as bond existence and bond types, and RN 3 corresponds to the 3D coordinates of the atoms. Here, represents the number of atoms in the molecule, d1 denotes the dimensionality of the node features, and d2 denotes the dimensionality of the edge features. We adopt the mathematical formulation of the Variational Diffusion Model (VDM) [4446] to define our diffusion and sampling processes, which follow continuous-time diffusion probabilistic framework, as illustrated in Figure 1(A). To streamline the presentation, we consider generic graph component, namely the node features H, the edge features A, or the atomic coordinates X, which we uniformly denote by vector-valued variable Rd. The subsequent formulas can then be applied independently to each component in consistent manner."
        },
        {
            "title": "4.1.1 Forward Diffusion Process",
            "content": "In the forward diffusion process, Gaussian noise Gϵ (0, I) is gradually added to the data over continuous time from = 0 to = 1. The noised sample Gt is obtained by: where αt and σt are signal and noise scaling functions, typically determined by cosine or linear schedules. The signal-to-noise ratio (SNR) at time is defined as: Gt = αtG0 + σtGϵ, (1) SNR(t) = α2 σ2 . (2) The SNR is strictly decreasing from = 0 to = 1, ensuring that the data is gradually corrupted into pure noise as 1. Given two times 0 < 1, the conditional distribution of noised graph at time given less-noised graph at time s, denoted q(Gt Gs), is also Gaussian: q(Gt Gs) = (αtsGs, σ2 tsI), (3) 10 where the intermediate scaling factor and conditional variance are defined as: αts = αt αs , ts = σ2 σ α2 tsσ2 . (4) This formulation allows the forward process to be implemented using analytically tractable Gaussian transitions, which also facilitates efficient computation of training objectives and reverse-time sampling."
        },
        {
            "title": "4.1.2 Reverse Denoising Process",
            "content": "The reverse-time denoising process starts from standard Gaussian sample G1 (0, I) and proceeds backward from = 1 to = 0. Let 0 < 1 denote two consecutive time steps. At each step, data prediction model dθ takes as input the noised sample Gt, self-conditioning [47] estimate G0, and the log signal-to-noise ratio log SNR(t). The model predicts ˆG0 = dθ(Gt, G0, log SNR(t)), which is then used to compute the denoised sample Gs by: Gs = Gt + αtsσ2 σ2 σsσts σt αsσ2 ts σ2 ˆG0, Gs = Gs + τ Gϵ, Gϵ (0, I), (5) (6) where τ is sampling temperature parameter, modulating the amount of stochasticity in the sampling process. Lower values of τ < 1 reduce the influence of noise, leading to more deterministic outputs. The self-conditioning mechanism, where the model reuses its previous prediction G0 as an additional input in the next step, enhances training stability and generation quality [47]."
        },
        {
            "title": "4.1.3 Training Objective",
            "content": "To recover the original sample, we train data prediction model dθ that takes as input the noised graph Gt, self-conditioning estimate G0, and the log signal-to-noise ratio log SNR(t). The model predicts the clean sample ˆG0, where ˆG0 refers to any component of ˆG0 = ( ˆH, ˆA, ˆX). Therefore, the model is trained to minimize weighted mean squared error: = Et,G0 (cid:16) (cid:20)(cid:114) αt σt λ1 ˆA A02 2 + λ2 ˆX ˆX0 2 + λ3 ˆH H02 2 (cid:17)(cid:21) , (7) where ˆX0 is obtained by aligning X0 to Xt using the Kabsch algorithm [48] to preserve SE(3)- equivariance. The loss coefficients λi balance the relative importance of each component."
        },
        {
            "title": "4.2 Diffusion Molecule Transformer (DMT)",
            "content": "To parameterize the data prediction module dθ, we employ specialized architecture named Diffusion Molecule Transformer (DMT) [37, 49] as its backbone, to jointly model the noisy molecular graphs node features, edge features, and 3D coordinates in our diffusion framework. The architecture is motivated by the need to recover the correlations among these three components, which are independently corrupted by noise during the forward diffusion process. DMT is specifically tailored for molecular generation tasks and respects both permutation equivariance and SE(3)-equivariance. At each time [0, 1] during the reverse process, DMT takes the noisy molecular graph Gt = (Ht, At, Xt) as input, along with self-conditioned estimate G0, and predicts clean graph ˆG0 = ( ˆH, ˆA, ˆX). The model performs denoising inference conditioned on both the current time step and the molecular spectra. The overall model architecture is illustrated in Figure 1(B). For the sake of clarity, the self-conditioning mechanism is omitted in the figure. time embedding, obtained via sinusoidal encoding of log(α2 ), is passed through learnable projections. Simultaneously, spectra embedding is generated by the SpecFormer, which will be detailed in Section 4.3. These two embeddings are then concatenated to form the conditioning vector C, which is used throughout the network. /σ2 Initial embeddings. The DMT module is composed of stacked Transformer-style equivariant blocks. In the first layer, the node features Ht and the edge features At are first concatenated with their corresponding self-conditioning estimates H0 and A0, respectively. For edge features, distance features 11 D0 computed from the self-conditioned coordinates X0 are also concatenated. These concatenated features are then passed through linear projections to obtain the initial embeddings: H(1) = Linearnode (cid:0)(cid:104) Ht, H0 (cid:105)(cid:1) RN dh, A(1) = Linearedge (cid:0)(cid:104) At, A0, D0 (cid:105)(cid:1) RN da, (8) (9) where [, ] denotes the concatenation operation, and dh and da denote the hidden embedding dimensions for node and edge embeddings, respectively. The coordinate stream initializes with the noisy positions X(1) = Xt. In addition, adjacency matrices derived from chemical bonding patterns and distance cutoffs computed from X0 are extracted and concatenated to serve as auxiliary attention masks in subsequent layers. Three-stream update. Each DMT block consists of three interacting streams that are responsible for updating node features, edge features, and coordinates, respectively. These streams exchange information throughout the block. In the node stream, information is propagated across the molecular graph via relational multi-head attention (MHA) mechanism [50, 51], enabling flexible message passing over fully connected molecular graphs. In particular, pairwise geometric distances computed from coordinates are transformed into radial basis encodings ρ(l) ij , and these are combined with edge features and geometric distances such as: (cid:104) A(l) ij = A(l) ij ; X(l) X(l) 2; ρ(l) ij (cid:105) , (10) forming geometry-aware relational representation. The queries, keys, and values for MHA are obtained by applying learnable linear projections to node features: = H(l)WQ, = H(l)WK, = H(l)WV , (11) where WQ, WK, and WV are learned projection matrices. The attention weights are then defined as: aij = ij )(cid:1) Qi tanh(cid:0)ϕ0( A(l) dk , = softmax(a), and the node-level aggregation proceeds as: Attn(H(l), A(l))i = (cid:88) j=1 aij tanh(cid:0)ϕ1( A(l) ij )(cid:1) Vj, (12) (13) with ϕ0 and ϕ1 as learnable projections. We extend the above attention mechanism to multi-head framework, yielding the aggregated representation M(l) RN dm, which serves as the intermediate node embeddings after relational multi-head attention. To incorporate time-dependent and spectra-dependent conditioning, we apply adaptive layer normalization (AdaLN) to each stream, conditioned on learned conditioning embedding C: AdaLN(h, C) = (1 + FFNscale(C)) LN(h) + FFNbias(C), and use an adaptive scaling function: Scale(h, C) = FFN scale(C) to modulate feature amplitudes. The node update is performed by: H(l+1) = Scale(cid:0)M(l), C(cid:1) + H(l), H(l+1) = Scale (cid:16) FFN(cid:0)AdaLN(H(l+1) (cid:17) , C)(cid:1), + H(l+1) . Similarly, edge features are updated by first fusing node messages: (cid:16) ˆA(l) ij = with subsequent normalization and scaling: M(l) + M(l) (cid:17) W1, A(l+1) ij = Scale(cid:0) ˆA(l) ij , C(cid:1) + A(l) ij , 12 (14) (15) (16) (17) A(l+1) ij = Scale (cid:16) FFN(cid:0)AdaLN(A(l+1) ij (cid:17) , C)(cid:1), + A(l+1) ij . (18) The parameter W1 is learnable weight matrix that couples node and edge information. For the coordinate stream, equivariant updates are achieved using directional vector fields, combining signals from node and edge streams: e(l+1) ij = AdaLN (cid:16) (cid:104) H(l+1) , H(l+1) , A(l+1) ij , X(l) X(l) 2 (cid:105) (cid:17) , , X(l+1) = X(l) + (cid:88) j=i γ(l) X(l) X(l) X(l) X(l) tanh(cid:0)FFN(e(l+1) ij )(cid:1), (19) (20) where W2 is learned projection and γ(l) is trainable scalar that stabilizes the directional updates. Finally, coordinates are shifted to have zero center-of-mass by mean centering, ensuring translation invariance. Overall, the DMT stacks such equivariant blocks, which collectively refine the graph features and spatial coordinates. Final output heads predict discrete atom and bond types as well as coordinates aligned to the canonical centered frame. Its design guarantees permutation equivariance and SE(3) equivariance, enabling robust molecule generation across both topological and geometric levels."
        },
        {
            "title": "4.3 Spectra Transformer (SpecFormer) for Spectra Encoding",
            "content": "To enable DMT to incorporate molecular spectral information as conditional input for molecular structure elucidation, we propose the Spectra Transformer (SpecFormer) [31] to encode molecular spectra. We further pre-train SpecFormer using both molecular structure data and spectra data."
        },
        {
            "title": "4.3.1 Architecture of SpecFormer",
            "content": "For each type of spectrum, we first segment the data independently into patches and encode these patches. The patch embeddings from all spectra are then concatenated and collectively processed by Transformer-based encoder. Patching. Rather than encoding each frequency point individually, we divide each spectrum into several patches. This design choice offers two key benefits: (i) It allows the model to capture local semantic featuressuch as absorption peaksmore effectively by grouping adjacent frequency points; and (ii) it lowers the computational cost for the following Transformer layers. Formally, spectrum si RLi (where = 1, . . . , S) is split into patches of length Pi with stride Di. If 0 < Di < Pi, patches overlap by Pi Di points; if Di = Pi, patches are non-overlapping. The patching process yields sequence pi RNiPi, where Ni = + 1 is the number of patches produced from si. (cid:107) (cid:106) LiPi Di Patch and position encoding. Each patch sequence from the i-th spectrum is projected into latent space of dimension via learnable linear transformation Wi RPid. To preserve the sequential order, learnable position encoding Wpos RNid is added: , yielding the encoded RNid for each spectrum. These representations serve as input to the SpecFormer representation encoder. = piWi + Wpos SpecFormer: multi-spectrum Transformer encoder. Existing encoders such as CNN-AM [52] utilize one-dimensional convolutions and are typically designed to process single type of spectrum. In contrast, our model simultaneously considers multiple molecular spectra (e.g., UV-Vis, IR, Raman) as input. This multi-spectrum strategy is motivated by the presence of both intra-spectrum dependenciesrelationships among peaks within the same spectrumand inter-spectrum dependenciescorrelations between peaks across different spectral modalities. These dependencies have been well documented, for example, in studies of vibronic coupling [53]. 1 To effectively model these dependencies, we concatenate the encoded patch sequences from all spectra: ˆp = i=1 Ni)d. This combined sequence is then passed through Transformer encoder (see Figure 1). In each attention head = 1, . . . , H, queries, keys, and values are computed as Qh = ˆpWQ Rd , Kh = ˆpWK . The attention output for each head is: respectively, with WQ Rddk and WV , and Vh = ˆpWV R((cid:80)S , WK Oh = Attention(Qh, Kh, Vh) = Softmax (cid:19) (cid:18) QhK dk Vh. (21) 13 Batch normalization, feed-forward networks, and residual connections are integrated as illustrated in Figure 1. The outputs from all attention heads are combined to form R((cid:80)S i=1 Ni)d. Finally, flattening layer and projection head are applied to produce the molecular spectra representation zs Rd."
        },
        {
            "title": "4.3.2 Pre-training of SpecFormer",
            "content": "To enable more effective encoding of molecular spectra, we introduce masked reconstruction pretraining objective. In addition, to overcome the scarcity of spectral pre-training data by leveraging largescale molecular structure pre-training, we incorporate contrastive learning objective to align spectral and structural representations. The complete pre-training framework is illustrated in Figure 1(C). Masked patches reconstruction pre-training for spectra. To ensure that the spectrum encoder can effectively extract and represent information from molecular spectra, we adopt masked patches reconstruction (MPR) pre-training strategy. Inspired by the effectiveness of masked reconstruction across multiple domains [5459], MPR guides the learning of SpecFormer by encouraging it to reconstruct masked portions of spectral data. After segmenting the spectra into patches, we randomly mask fraction of these patchesaccording to predefined ratio αby replacing them with zero vectors. The masked patch sequences then undergo patch and position encoding, which conceals their semantic content (such as absorption intensities at certain wavelengths) but retains their positional information, aiding the reconstruction task. Once processed by SpecFormer, the encoded representations corresponding to the masked patches are passed through reconstruction head specific to each spectrum. The original values of the masked patches are then predicted, with the mean squared error (MSE) between the reconstructed and true patch values serving as the training objective: LMPR = (cid:88) i=1 pi,j (cid:101)Pi ˆpi,j pi,j2 2, (22) where (cid:101)Pi is the set of masked patches for the i-th type of spectrum, and ˆpi,j denotes the reconstruction for the masked patch pi,j. Contrastive learning between 3D structures and spectra. To align spectral and 3D molecular representations, we introduce contrastive learning objective in addition to the MPR. The 3D embeddings are learned under the guidance of denoising objective [31, 60]. Here, the 3D embedding zx Rd and spectra embedding zs Rd of the same molecule are treated as positive pairs, while all other pairings are considered negative. The contrastive objective is designed to maximize similarity between positive pairs while minimizing similarity with negatives, using the InfoNCE loss [61]: LContrast = 1 2 Ep(zx,zs)[log exp(fx(zx, zs)) exp(fx(zx, zs)) + (cid:80) exp(fx(zj x, zs)) + log exp(fs(zs, zx)) exp(fs(zs, zx)) + (cid:80) exp(fs(zj s, zx)) ], (23) where zj implemented here as the inner product: fx(zx, zs) = fs(zs, zx) = zx, zs. denote negative samples, and fx(zx, zs) and fs(zs, zx) are scoring functions, and zj Two-stage pre-training pipeline. Although spectral datasets are scarce, there exists wealth of large-scale unlabeled molecular structure datasets. By leveraging our proposed contrastive alignment framework between spectra and structures, we can transfer knowledge from large-scale pre-training on molecular structures to enhance the learning of SpecFormer. To fully exploit both spectral and structural information, we adopt two-stage training protocol: the first stage performs pre-training on large dataset [62] containing only 3D structures using the denoising objective, while the second stage utilizes dataset with available spectra to jointly optimize the overall objective: = βDenoisingLDenoising + βMPRLMPR + βContrastLContrast, (24) where βDenoising, βMPR, and βContrast are weights for each component. In the second stage, SpecFormer is pre-trained exclusively on the QM9S training set to avoid data leakage."
        },
        {
            "title": "References",
            "content": "[1] Cao, L., Guler, M., Tagirdzhanov, A.M., Lee, Y.-Y., Gurevich, A.A., Mohimani, H.: Moldiscovery: learning mass spectrometry fragmentation of small molecules. Nature Communications 12 (2020) [2] Alberts, M., Schilter, O., Zipoli, F., Hartrampf, N., Laino, T.: Unraveling molecular structure: multimodal spectroscopic dataset for chemistry. In: NeurIPS (2024) [3] Li, Z., Cao, B., Jiao, R., Wang, L., Wang, D., Liu, Y., Chen, D., Li, J., Liu, Q., Rong, Y., Wang, L., Zhang, T., Yu, J.X.: Materials generation in the era of artificial intelligence: comprehensive survey. arXiv abs/2505.16379 (2025) [4] Kanakala, G.C., Sridharan, B., Priyakumar, U.D.: Spectra to structure: Contrastive learning framework for library ranking and generating molecular structures for infrared spectra. Digital Discovery (2024) [5] Bushuiev, R., Bushuiev, A., Jonge, N.F., Young, A., Kretschmer, F., Samusevich, R., Heirman, J., Wang, F., Zhang, L., Duhrkop, K., Ludwig, M., Haupt, N.A., Kalia, A., Brungs, C., Schmid, R., Greiner, R., Wang, B., Wishart, D.S., Liu, L., Rousu, J., Bittremieux, W., Rost, H., Mak, T.D., Hassoun, S., Huber, F., Hooft, J.J.J., Stravs, M.A., Bocker, S., Sivic, J., Pluskal, T.: Massspecgym: benchmark for the discovery and identification of molecules. In: NeurIPS (2024) [6] Wei, J.N., Wei, J.N., Belanger, D., Adams, R.P., Sculley, D.: Rapid prediction of electronionization mass spectrometry using neural networks. ACS Central Science 5, 700708 (2018) [7] Schutt, K.T., Unke, O.T., Gastegger, M.: Equivariant message passing for the prediction of tensorial properties and molecular spectra. In: International Conference on Machine Learning (2021) [8] Zou, Z., Zhang, Y., Liang, L., Wei, M., Leng, J., Jiang, J., Luo, Y., Hu, W.: deep learning model for predicting selected organic molecular spectra. Nature Computational Science 3, 957964 (2023) [9] Young, A., Rost, H.L., Wang, B.: Tandem mass spectrum prediction for small molecules using graph transformers. Nature Machine Intelligence 6, 404416 (2024) [10] Goldman, S., Bradshaw, J., Xin, J., Coley, C.W.: Prefix-tree decoding for predicting mass spectra from molecules. In: NeurIPS (2023) [11] Park, J.V., Jo, J., Yoon, S.: Mass spectra prediction with structural motif-based graph neural networks. Scientific Reports 14 (2023) [12] Zong, Y., Wang, Y., Qiu, X., Huang, X., Qiao, L.: Deep learning prediction of glycopeptide tandem mass spectra powers glycoproteomics. Nature Machine Intelligence 6(8), 950961 (2024) [13] Hu, T., Zhang, Y., Hu, W.: Leveraging deep learning for accurate and automated interpretation of molecular ir and raman spectra. Progress in Natural Science: Materials International (2025) [14] Heirman, J., Bittremieux, W.: Annotating metabolite mass spectra with domain-inspired chemical formula transformers. Nature Machine Intelligence 6(11), 12961302 (2024) [15] Duhrkop, K., Nothias, L.-F., Fleischauer, M., Reher, R., Ludwig, M., Hoffmann, M.A., Petras, D., Gerwick, W.H., Rousu, J., Dorrestein, P.C., Bocker, S.: Systematic classification of unknown metabolites using high-resolution fragmentation mass spectra. Nature Biotechnology 39, 462471 (2020) [16] Hu, F., Chen, M.S., Rotskoff, G.M., Kanan, M.W., Markland, T.E.: Accurate and efficient structure elucidation from routine one-dimensional nmr spectra using multitask machine learning. ACS Central Science 10, 21622170 (2024) [17] Hong, Y., Li, S., Ye, Y., Tang, H.: Fiddle: deep learning method for chemical formulas prediction from tandem mass spectra. bioRxiv (2025) 15 [18] Deng, Y., Yao, Y., Wang, Y., Yu, T., Cai, W., Zhou, D., Yin, F., Liu, W., Liu, Y., Xie, C., Guan, J., Hu, Y., Huang, P., Li, W.: An end-to-end deep learning method for mass spectrometry data analysis to reveal disease-specific metabolic profiles. Nature Communications 15 (2024) [19] Nunes, J.B., Ijsselsteijn, M.E., Abdelaal, T., Ursem, R., Ploeg, M., Everts, B., Mahfouz, A., Heijs, B., Miranda, N.F.C.C.: Integration of mass cytometry and mass spectrometry imaging for spatially resolved single-cell metabolic profiling. Nature Methods 21, 17961800 (2023) [20] Lancaster, N.M., Sinitcyn, P., Forny, P., Peters-Clarke, T.M., Fecher, C., Smith, A.J., Shishkova, E., Arrey, T.N., Pashkova, A., Robinson, M.L., Arp, N.L., Fan, J., Hansen, J.K., Galmozzi, A., Serrano, L.R., Rojas, J., Gasch, A.P., Westphall, M.S., Stewart, H.I., Hock, C., Damoc, E., Pagliarini, D.J., Zabrouskov, V., Coon, J.J.: Fast and deep phosphoproteome analysis with the orbitrap astral mass spectrometer. Nature Communications 15 (2024) [21] Guo, G., Goldfeder, J., Lan, L., Ray, A., Yang, A.H., Chen, B., Billinge, S.J.L., Lipson, H.: Towards end-to-end structure determination from x-ray diffraction data using deep learning. npj Computational Materials (2024) [22] Stravs, M.A., Duhrkop, K., Bocker, S., Zamboni, N.: Msnovelist: de novo structure generation from mass spectra. Nature Methods 19, 865870 (2021) [23] Bohde, M., Manjrekar, M., Wang, R., Ji, S., Coley, C.W.: Diffms: Diffusion generation of molecules conditioned on mass spectra. In: ICML (2025) [24] Litsa, E.E., Chenthamarakshan, V., Das, P., Kavraki, L.E.: An end-to-end deep learning framework for translating mass spectra to de-novo molecules. Communications Chemistry 6 (2023) [25] French, E., Deng, X., Chen, S., Ju, C.-W., Cheng, X., Zhang, L., et al.: Revolutionizing spectroscopic analysis using sequence-to-sequence models i: From infrared spectra to molecular structures. ChemRxiv (2025) [26] Vavekanand, R.: Nmrgen: generative modeling framework for molecular structure prediction from nmr spectra. IECE Transactions on Emerging Topics in Artificial Intelligence (2025) [27] Yilmaz, M., Fondrie, W.E., Bittremieux, W., Nelson, R., Ananth, V., Oh, S., Noble, W.S.: Sequence-to-sequence translation from mass spectra to peptides with transformer model. Nature Communications 15 (2024) [28] Alberts, M., Laino, T., Vaucher, A.C.: Leveraging infrared spectroscopy for automated structure elucidation. Communications Chemistry 7 (2024) [29] Lu, X., Ma, H., Li, H., Li, J., Zhu, T., Liu, G.-k., Ren, B.: Vib2mol: from vibrational spectra to molecular structures-a versatile deep learning model. arXiv abs/2503.07014 (2025) [30] Wu, W., Leonardis, A., Jiao, J., Jiang, J., Chen, L.: Transformer-based models for predicting molecular structures from infrared spectra using patch-based self-attention. The Journal of Physical Chemistry 129(8), 20772085 (2025) [31] Wang, L., Liu, S., Rong, Y., Zhao, D., Liu, Q., Wu, S., Wang, L.: Molspectra: Pre-training 3d molecular representation with multi-modal energy spectra. In: ICLR (2025) [32] Guo, K., Nan, B., Zhou, Y., Guo, T., Guo, Z., Surve, M., Liang, Z., Chawla, N.V., Wiest, O., Zhang, X.: Can llms solve molecule puzzles? multimodal benchmark for molecular structure elucidation. In: NeurIPS (2024) [33] Sohl-Dickstein, J., Weiss, E.A., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: ICML (2015) [34] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020) [35] Wang, L., Song, C., Liu, Z., Rong, Y., Liu, Q., Wu, S., Wang, L.: Diffusion models for molecules: 16 survey of methods and tasks. arXiv abs/2502.09511 (2025) [36] Huang, H., Sun, L., Du, B., Lv, W.: Conditional diffusion based on discrete graph structures for molecular graph generation. In: AAAI (2023) [37] Huang, H., Sun, L., Du, B., Lv, W.: Learning joint 2-d and 3-d graph diffusion models for complete molecule generation. IEEE Transactions on Neural Networks and Learning Systems (2024) [38] Satorras, V.G., Hoogeboom, E., Fuchs, F.B., Posner, I., Welling, M.: E(n) equivariant normalizing flows. In: NeurIPS (2021) [39] Gebauer, N.W.A., Gastegger, M., Schutt, K.T.: Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. In: NeurIPS (2019) [40] Luo, Y., Ji, S.: An autoregressive flow model for 3d molecular geometry generation from scratch. In: ICLR (2022) [41] Hoogeboom, E., Satorras, V.G., Vignac, C., Welling, M.: Equivariant diffusion for molecule generation in 3d. In: ICML (2022) [42] Huang, L., Zhang, H., Xu, T., Wong, K.: MDM: molecular diffusion model for 3d molecule generation. In: AAAI (2023) [43] Xu, M., Powers, A.S., Dror, R.O., Ermon, S., Leskovec, J.: Geometric latent diffusion models for 3d molecule generation. In: ICML (2023) [44] Kingma, D.P., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. In: NeurIPS (2022) [45] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. In: ICLR (2022) [46] Hoogeboom, E., Salimans, T.: Blurring diffusion models. In: ICLR (2023) [47] Chen, T., Zhang, R., Hinton, G.E.: Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv abs/2208.04202 (2022) [48] Kabsch, W.: solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section 32, 922923 (1976) [49] Liu, Z., Luo, Y., Huang, H., Zhang, E., Li, S., Fang, J., Shi, Y., Wang, X., Kawaguchi, K., Chua, T.: Next-mol: 3d diffusion meets 1d language modeling for 3d molecule generation. In: ICLR (2025) [50] Yuan, C., Zhao, K., Kuruoglu, E.E., Wang, L., Xu, T., Huang, W., Zhao, D., Cheng, H., Rong, Y.: survey of graph transformers: Architectures, theories and applications. CoRR abs/2502.16533 (2025) [51] Li, Z., Wang, L., Sun, X., Luo, Y., Zhu, Y., Chen, D., Luo, Y., Zhou, X., Liu, Q., Wu, S., Wang, L., Yu, J.X.: GSLB: the graph structure learning benchmark. In: NeurIPS (2023) [52] Tao, S., Feng, Y., Wang, W., Han, T., Smith, P.E.S., Jiang, J.: machine learning protocol for geometric information retrieval from molecular spectra. Artificial Intelligence Chemistry (2024) [53] Kong, F.-F., Tian, X.-J., Zhang, Y., Yu, Y.-J., Jing, S.-H., Zhang, Y., Tian, G., Luo, Y., Yang, J., Dong, Z., Hou, J.G.: Probing intramolecular vibronic coupling through vibronic-state imaging. Nature Communications 12 (2021) [54] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In: NAACL (2019) [55] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.B.: Masked autoencoders are scalable vision learners. In: CVPR (2022) 17 [56] Hou, Z., Liu, X., Cen, Y., Dong, Y., Yang, H., Wang, C., Tang, J.: Graphmae: Self-supervised masked graph autoencoders. In: KDD (2022) [57] Xia, J., Zhao, C., Hu, B., Gao, Z., Tan, C., Liu, Y., Li, S., Li, S.Z.: Mole-bert: Rethinking pretraining graph neural networks for molecules. In: ICLR (2023) [58] Wang, L., Tao, X., Liu, Q., Wu, S., Wang, L.: Rethinking graph masked autoencoders through alignment and uniformity. In: AAAI (2024) [59] Nie, Y., Nguyen, N.H., Sinthong, P., Kalagnanam, J.: time series is worth 64 words: Long-term forecasting with transformers. In: ICLR (2023) [60] Zaidi, S., Schaarschmidt, M., Martens, J., Kim, H., Teh, Y.W., Sanchez-Gonzalez, A., Battaglia, P.W., Pascanu, R., Godwin, J.: Pre-training via denoising for molecular property prediction. In: ICLR (2023) [61] Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv abs/1807.03748 (2018) [62] Nakata, M., Shimazaki, T.: Pubchemqc project: large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling 57 6, 13001308 (2017) [63] Ramakrishnan, R., Dral, P.O., Dral, P.O., Rupp, M., Lilienfeld, O.A.: Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data 1 (2014)"
        },
        {
            "title": "A Derivation of the Reverse Sampling Process",
            "content": "This appendix derives the reverse sampling formula for our continuous-time diffusion model, as applied to molecular graph generation. Our objective is to obtain an expression for sampling less-noisy graph state Gs from more-noisy state Gt = (Ht, At, Xt), given model prediction of the original clean graph ˆG0 = ( ˆH0, ˆA0, ˆX0). We assume continuous time interval [0, 1], with 0 < 1. To streamline the derivation, we consider single graph componenteither node features H, edge features A, or atomic coordinates Xdenoted generically as vector-valued variable Rd. The full derivation extends naturally by applying the result independently to each component. A.1 Forward Diffusion Process Following the formulation of VDM [4446], the forward diffusion process is defined as: q(Gt G0) = (αtG0, σ2 I), (25) where the graph Gt at time is noisy version of the clean graph G0. Equivalently, it can be reparameterized as: Gt = αtG0 + σtGϵ, Gϵ (0, I). (26) Because αt decreases monotonically while σt increases, the information from G0 is gradually destroyed as increase. Assuming the process defined by Eq. (25) is Markovian, its transition distribution between two intermediate steps and with 0 < is: q(Gt Gs) = (αtsGs, σ2 tsI), (27) tsσ2 and σ2 α2 ts = σ2 where αts = αt . convenient property of this framework is that the time αs grid can be defined arbitrarily and does not depend on the particular spacing of and t. We set = 1 to denote the final diffusion step, where q(GT G0) (0, I) approximates standard normal distribution. Unless otherwise specified, time steps are assumed to lie in the unit interval [0, 1]. This formulation describes the distribution of more-noised state Gt conditioned on less-noised state Gs, and serves as key component of the variational framework. A.2 Reverse Denoising Process Posterior q(Gs Gt, G0) We now derive the posterior distribution q(Gs Gt, G0), which describes the distribution of an intermediate state Gs conditioned on both the noisy future Gt and the original clean graph G0. This distribution underlies the training of the reverse process. From standard Bayesian inference for Gaussians, suppose prior (µp, Σp) and likelihood (Az, Σl). Then the posterior p(z y) is Gaussian with: Σpost = (cid:0)Σ1 µpost = Σpost + AΣ1 (cid:0)Σ1 A(cid:1)1 µp + AΣ , y(cid:1) . In our case: Applying this, we obtain: with: = Gs, = Gt, µp = αsG0, Σp = σ2 = αtsI, Σl = σ2 tsI. I, q(Gs Gt, G0) = (µQ, σ2 QI), µQ = σ2 (cid:32) 1 σ2 αsG0 + (cid:33) Gt = αts σ2 ts αtsσ2 σ2 Gt + αsσ2 ts σ2 G0, σ2 = (cid:33)1 (cid:32) 1 σ2 + α2 ts σ2 ts = σ2 σ2 ts σ2 . 19 (28) (29) (30) (31) (32) (33) The posterior mean is convex combination of Gt and G0, with weights determined by their respective noise scales. This provides the optimal denoised estimate of Gs and forms the basis of the reverse sampling process. A.3 Reverse Denoising Process Approximation At inference time, the true G0 is unavailable, so we replace it with model estimate ˆG0 = dθ(Gt, G0, log SNR(t)). This yields the approximate reverse-time transition: where: p(Gs Gt) ( Gs, σ2 QI), Gs = αtsσ2 σ2 Gt + αsσ2 ts σ2 ˆG0, σ2 = σ2 σ2 ts σ2 . We then sample from this distribution as: Gs = Gs + σsσts σt Gϵ, Gϵ (0, I). Combining the above expressions, the complete reverse sampling step from to becomes: Gs = αtsσ2 σ2 Gt + αsσ2 ts σ2 ˆG0 + σsσts σt Gϵ. (34) (35) (36) (37) (38) This closed-form expression enables efficient ancestral sampling in the reverse-time generative process. A.4 Sampling with Temperature To control the stochasticity in the sampling process, we introduce temperature parameter τ > 0, which scales the noise component during sampling: Gs = αtsσ2 σ2 Gt + αsσ2 ts σ2 ˆG0 + τ σsσts σt Gϵ. (39) The temperature parameter τ modulates the amount of stochasticity in the sampling process. When τ = 1, the sampling follows the standard formulation. Lower values of τ < 1 reduce the influence of noise, leading to more deterministic and potentially sharper outputs. Conversely, higher values of τ > 1 increase the noise contribution, encouraging greater diversity at the cost of higher stochasticity. This flexibility allows practitioners to trade off between sample fidelity and variability based on downstream objectives."
        },
        {
            "title": "B Datasets",
            "content": "We conduct experiments on the QM9S dataset [8, 31], which augments the original QM9 [63] molecular dataset with simulated spectra. For each molecule, we extract its structure and corresponding IR, Raman, and UV-Vis spectra, forming multi-modal spectral condition for our structure elucidation task. The dataset contains 133,885 molecules in total. Following the original setting in DetaNet [8], we adopt the same training, validation, and test split strategy, which partitions the dataset into 90% for training, 5% for validation, and 5% for testing."
        },
        {
            "title": "C Metrics",
            "content": "Aiming to generating chemically valid and complete molecules, modeling accurate molecular distributions, and achieving achieve accurate sturcture elucidation from spectra, we elaborately design series of evaluation metrics to reflect the generation quality. 20 Specifically, we introduce two categories of evaluation metrics. The first focuses on general molecular generation quality, emphasizing the validity and chemical soundness of the generated structures. The second targets molecular structure elucidation, evaluating the alignment between the predicted molecular structuresderived from spectral dataand the corresponding ground truth structures. C.1 Basic Metrics for Molecular Generation In the context of conditional molecule generation, these fundamental evaluation criteria serve as prerequisite to guarantee that the generated structures are chemically meaningful and sufficiently diverse before evaluating their spectrum-conditioned structural accuracy. Molecular Structure Validity Evaluation The primary consideration is the chemical rationality of the generated molecular structures. The Validity metric ensures that the molecular structures conform to basic chemical rules, including chemically plausible valences (e.g., carbon atoms typically form four covalent bonds), and the overall bonding patterns are chemically reasonable (e.g., avoiding unrealistic bonds between atoms, maintaining aromatic delocalization). Building upon this, we introduce the Validity and Complete (V&C) indicator, which not only requires chemical validity but also guarantees that the molecular graph forms single connected entity instead of fragmented parts. This is particularly important for evaluating the models ability to generate fully connected, functional molecules. To further assess the models capacity for innovation, we measure the proportions of molecules satisfying Validity and Unique (V&U) and Validity, Unique, and Novelty (V&U&N) criteria. Uniqueness is determined by canonicalizing molecular representations and removing duplicates, thus ensuring diversity among the generated structures. Novelty is evaluated by comparing the generated molecules against the training set, reflecting the models ability to produce genuinely novel chemotypes rather than simply memorizing training data. Molecular Structure Stability Evaluation While structural validity is necessary condition, it may not fully capture the chemical plausibility of molecule. We additionally introduce more stringent indicator of molecular stability. Atom Stability assesses whether each atom in the molecule has achieved an appropriate valence coordination, fully considering the influence of formal charges on the permissible bonding patterns. Mol Stability further requires that all atoms in the entire molecule satisfy the stability criteria, thereby providing more comprehensive assessment of the chemical soundness of the generated molecules. Distribution-based Structure Evaluation To evaluate the models ability to learn the data distribution of real molecules, we adopt several distribution-based metrics. The Frechet ChemNet Distance (FCD) measures the similarity between the distributions of generated molecules and reference molecules in high-dimensional learned feature space. The Similarity to Nearest Neighbor (SNN) metric assesses the representativeness of the generated set by measuring the Tanimoto similarity between each generated molecule and its nearest neighbors in the test set, reflecting both diversity and coverage. We additionally employ metrics based on molecular structural features: Fragment Similarity (Frag), based on BRICS decomposition, evaluates distributional alignment at the functional group level; while Scaffold Similarity (Scaf ), using BemisMurcko scaffold analysis, quantifies similarity and diversity at the core-structure level. These complementary metrics jointly reflect the models ability to capture both local functional groups and global structural patterns, thereby characterizing the complexity of the generated molecular structures. 3D Geometry-based Structural Evaluation Beyond chemical rules in 2D representations, assessing the 3D geometric consistency of generated molecules is critical, especially for downstream applications such as molecular docking or molecular structure elucidation from spectra. We therefore introduce set of geometric evaluation metrics based on fundamental molecular geometry features: bond lengths, bond angles, and dihedral angles. Bond evaluation compares the distributions of bond lengths in the generated set against those in the reference set (test set molecules), capturing whether typical interatomic distances are preserved. Angle evaluation similarly assesses the distributions of bond angles, reflecting the local spatial arrangements of connected atoms and ring systems. Dihedral evaluation analyzes the distributions of torsional 21 (dihedral) angles, providing insights into conformational consistency and stereochemical plausibility of the generated molecules. To quantify the similarity between these geometric features of the generated and reference molecules, we employ maximum mean discrepancy (MMD), which measures the distance between two distributions in kernel space. lower MMD indicates that the generated molecules more faithfully reproduce realistic 3D geometries. It is important to note that for all distribution-based and geometric metrics, the test set serves as the reference, while the novelty metric uses the training set as the reference to measure whether generated molecules are unseen. This distinction ensures clear separation between distributional fidelity and generative innovation. C.2 Metrics for Molecular Structure Elucidation In the task of molecular structure elucidation, we introduce set of precise metrics to evaluate the similarity between the ground-truth target structures and the generated candidate molecules under spectrum-conditioned generation. Here, molecular structure is represented as graph = (H, A, X), where RN d1 denotes node-level attributes such as atom types and charges, RN d2 encodes pairwise edge features such as bond existence and bond types, and RN 3 corresponds to the 3D coordinates of the atoms. Top-K Accuracy The Top-K Accuracy quantifies the models ability to recover the exact target structure among its top-K generated candidates. For each condition characterized by set of spectra, the model is sampled times to generate molecular structure candidates. If any of the candidates exactly matches the ground-truth structure, the prediction is considered correct. Formally, the metric is defined as: ACC@ = EG (cid:16) (cid:104) 1 {1, . . . , K}, ˆGi = (cid:17)(cid:105) , (40) where 1() is the indicator function, denotes the ground-truth molecular graph, and ˆGi denotes the i-th generated candidate. This metric reflects the probability that the correct structure appears at least once among the top-K model outputs. Maximum Common Edge Subgraph (MCES) To quantify graph-structural overlap, we adopt the Maximum Common Edge Subgraph (MCES) metric: (cid:20) MCES = EG (cid:21) max HG, ˆG E(H) , (41) where E(H) denotes the set of edges in the common subgraph H. This indicator captures the extent of exact subgraph matching between the target and predicted molecular graphs, with higher values reflecting greater degree of structural recovery. Fingerprint-based Similarity Metrics We adopt several fingerprint-based similarity measures. Denote by aMorgan and ˆaMorgan the binary Morgan fingerprint vectors of the target and predicted molecules, respectively, and by aMACCS and ˆaMACCS their MACCS fingerprints. The Tanimoto Similarity over Morgan fingerprints (TaniSimMorgan) is defined as: TaniSimMorgan = EG (cid:20) aMorgan ˆaMorgan aMorgan ˆaMorgan (cid:21) , (42) where and denote bitwise AND and OR over the fingerprint vectors. This score quantifies the overlap in local structural patterns. The Cosine Similarity over Morgan fingerprints (CosSimMorgan) is defined as: CosSimMorgan = EG (cid:20) aMorgan ˆaMorgan aMorgan ˆaMorgan (cid:21) . Likewise, the Tanimoto Similarity over MACCS fingerprints (TaniSimMACCS) is: TaniSimMACCS = EG (cid:20) aMACCS ˆaMACCS aMACCS ˆaMACCS (cid:21) . 22 (43) (44) These fingerprints capture interpretable functional patterns based on set of predefined substructure keys, complementing the Morgan-based representations. Fragment-based Similarity (Fraggle) The Fraggle Similarity metric decomposes into set of chemically meaningful fragments = {f1, f2, . . . , fn} by strategies including double acyclic or exocyclic bond cuts with specified fragment size constraints. For each fragment fi, Fraggle computes two Tanimoto similarities over RDKit fingerprints between the target molecule and the predicted molecule ˆG: one based on the standard RDKit fingerprint over the entire molecule, and another based on masked RDKit fingerprint where atoms outside fi with Tversky similarity below 0.8 are replaced with wildcards. The fragment-level score is then taken as the maximum of these two values: si = max (TaniSimRDKit(aRDKit, ˆaRDKit), TaniSimRDKit(aRDKit,mask, ˆaRDKit,mask)) . (45) The final Fraggle similarity is defined as: FraggleSim = EG (cid:20) (cid:21) . max fiF si (46) This dual approach captures both global and local matching patterns Functional Group-based Similarity Lastly, to evaluate functional group consistency, we define set of chemically significant functional groups (e.g., alkane, alcohol, amine, carboxylic acid, etc.) described by SMARTS patterns. For pair of molecules (ground truth) and ˆG (prediction), we extract their respective functional group sets G(G) and G( ˆG). The Functional Group Similarity (FGSim) is then computed as: FGSim = EG (cid:12) (cid:12) (cid:12)F G(G) G( ˆG) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)F G(G) G( ˆG) (cid:12) (cid:12) (cid:12) . (47) This interpretable metric is independent of molecular size, computationally efficient via substructure matching, and well-suited for comparative evaluation."
        }
    ],
    "affiliations": [
        "College of Intelligence and Computing, Tianjin University",
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
        "National University of Singapore",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}