{
    "paper_title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
    "authors": [
        "Vansh Sharma",
        "Venkat Raman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 8 8 8 1 . 6 0 5 2 : r Steering Conceptual Bias via Transformer Latent-Subspace Activation Vansh Sharma University of Michigan"
        },
        {
            "title": "Abstract",
            "content": "This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. static neuron-attribution method, perturbing the highest activated MLP weight for C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (06) improving the probe classification accuracy by 61.5 % compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces modest inference overhead, it remains practical by steering only subset of layers and enables reproducible model behavior. These results demonstrate scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems. Keywords: Activation Steering, Latent Space Activation, Activation Patching, Neuron Attribution, Privileged Basis, Code Generation, Reproducibility Corresponding Author: vanshs@umich.edu Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have rapidly evolved into sophisticated natural language processors, enabling the development of agentic systems that autonomously orchestrate complex workflows [Wang et al., 2024a, Sharma and Raman, 2024]. particularly striking trend is the adoption of LLM-driven agents for automated code generation. By incorporating plugin architectures that expose external APIs, these agents extend beyond text synthesis: agents can invoke specialized tools [Wang et al., 2024b] and execute command-line operations [OpenAI, 2021, Lu et al., 2024]. This expanded action space has already powered variety of real-world applications - from LLM-based robotic controllers [Wang et al., 2023, Kannan et al., 2024, Yang et al., 2024] to automated scientific experimentation [Gao et al., 2024, Gridach et al., 2025] platforms - highlighting the remarkable success of these agents. Despite the growing utility of LLM agents for high-level scripting and automation, their application to scientific code generation remains unexplored. Scientific software predominantly relies on C++ or CPP, CUDA and other low-level languages that are sparsely represented in most pretraining datasets. As result, LLM-generated implementations often exhibit syntactic or semantic errors, leading to compilation failures or unstable behavior at runtime [Zhang et al., 2023]. In addition, current agents are heavily dependent on user-defined control primitives and meticulously engineered prompts, which can be misinterpreted and give rise to unpredictable execution paths [Kim et al., 2025, Krishnan, 2025, Gridach et al., 2025]. One possible solution is to augment function definitions and syntax using retrieval augmented generation (RAG) framework; however, Xiao et al. [2023] and Yona et al. [2025] report unexplained vulnerability linked to \"attention sinks\" or first token bias, causing model behavior to diverge due to token repetition in long interactions, commonly seen in RAG. Another failure mode arises from the use of LMs that have been trained or fine-tuned on undisclosed corpora and then subjected to opaque alignment procedures, especially for MoE models [Nishu et al., 2025]. Such processes can inadvertently skew the models output toward particular programming language or coding style, further eroding its ability to generate correct generalizable code across the various low-level languages prevalent in scientific computing during agentic applications with long repeated interactions. To investigate these bottlenecks, curated benchmark of scientific coding challenges is introduced to reveal the implicit language preference of an LLM when presented with given problem. Targeted probing techniques are then applied to identify subgraphs or subspaces in the model whose activation strongly correlates with that preference. In the privileged basis of the model [Zhang and Nanda, 2024], often termed native MLP activation axes, each coordinate encodes distinct functional characteristic (here the coding language preference). This basis is consequence of elementwise non-nonlinearities (e.g., ReLU, GELU) breaking rotational symmetry and the coordinate-wise biases introduced by optimizers such as Adam [Nelson Elhage and Olah, 2023]. Since these axes remain (approximately) disentangled, selectively amplifying or suppressing single coordinate produces clear causal shifts in token probabilities [Elhage et al., 2022]. In this basis, an effective weight vector for any neuron can be derived and its direct influence on features can be decoded. The external perturbation of this neurons activation along the identified axis then reliably steers the model toward generating code in the desired programming language. Contribution of this work: 1. curated suite of simple and complex scientific programming prompts designed to systematically evaluate programming language selection behavior across targeted coding tasks. 2. proof-of-concept method that locates the single MLP weight most correlated with the target concept (CPP language in this case) and perturbs it to bias model preference, demonstrating the feasibility and fragility of manual neuron edits for style control. 3. scalable method that trains lightweight per-layer probes refined during inference via cross-entropy loss. This approach reliably steers LLMs toward the CPP language (or any target domain) with minimal runtime overhead and embeds reproducible transformation matrices for consistent behavior across multiple model deployments."
        },
        {
            "title": "2 Related Work",
            "content": "Reverse engineering neural networks to drive/steer specific behavior, known as mechanistic interpretability, is an emerging field [Wang et al., 2022, Templeton et al., 2024, Olsson et al., 2022]. Its primary objective is to identify causal relationships within model activations Jiang et al. [2024], Chen et al. [2024], Rodrigues et al. [2024] thereby revealing complex functional roles [Condori and Bruno, 2021, Szandała, 2023] and enabling the targeted modulation [Huang et al., 2025] or ablation of individual neurons. Supervised fine-tuning, weight modulation techniques, and RLHF represent direct intervention strategies for model steering [Dathathri et al., 2019, Meng et al., 2022]. Although effective, these methods impose substantial computational overhead and can inadvertently compromise the robustness and general performance of the model [Brown et al., 2023]. The method using corrupted inputs as baseline distribution to resample neuron activation, known as Activation Patching, has been widely used to achieve fine-grained control over the model output [Kramár et al., 2024, Geiger et al., 2022, Meng et al., 2022]. typical causal attribution method quantifies the negative impact on model output by deleting specific neuron activations, more trivial approach compared to Activation Patching. In such methods, extensive model sweeps are conducted to evaluate the results of neuron modification, leading to millions of model evaluations [Kramár et al., 2024]. Studies such as [Park et al., 2025, Yeo et al., 2025], have focused on suppressing hallucinations in LLMs within different modalities using activation patching. Recent studies using similar forms of neuron attribution have typically been applied to multiple choice question benchmarks [Wang et al., 2025, Davies, 2025], rather than real-world deployment scenarios. In contrast to previous work, this study presents different approach for the inference-time adaptive steering algorithm [Wang et al., 2025] by incorporating gradient-based refinement, offering more efficient and precise mechanism for steering large-scale models."
        },
        {
            "title": "3 Methodology",
            "content": "Autoregressive decoder-only transformers convert embedded tokens into high-dimensional latent representations that are successively refined by the attention mechanism [Vaswani et al., 2017] and deep MLP blocks for output generation. Residual connections carry persistent residual stream: combining each MLP input with its output to preserve and enhance contextual features before the LM head decodes the final latent stream into tokens. In particular, individual neurons operate directly on the residual stream, central information pathway through transformer layers. Neurons specialized in the detection of specific features within this stream exhibit strong alignment between their weight vectors and the corresponding features, characterized by high cosine similarity [Meng et al., 2022]. This property forms the basis for the methods explained in the following sections."
        },
        {
            "title": "3.1 Neuron Attribution: Static Method",
            "content": "Building upon the previously mentioned alignment property between weights and features, the static method involves directly decoding the neuron weight vectors through the LM-head, thus converting the neuron weights into interpretable token-level probability distributions. This approach aligns conceptually with the Golden Gate\" investigation by Templeton et al. [2024] and work by Davies [2025]. transformer with layers processes an input token sequence x1, . . . , xn (vocabulary ) through residual-stream architecture. Each token xi is first embedded into d-dimensional vector h(0) . In layer {1, . . . , L}, the residual update at position is given by (cid:1) , + (j)(cid:0)h(j1) , . . . , h(j1) = h(j1) h(j) 1 where (j) denotes the combined self-attention and feedforward operations of layer j. After such updates, the final residuals h(L) are passed through learned linear head (LM-head or \"output head\" or \"prediction head\") and softmax to produce next token probabilities: (cid:1) . (cid:0)xi+1 x1:i (cid:1) = softmax(cid:0)W h(L) i To identify neurons selectively responsive to particular target token, the decoded probability corresponding to that token is normalized by the average decoded probability of the top tokens for 3 each neuron. Upon identifying candidate feature-selective neurons, their functional significance is validated by systematically modifying their activations [Vig et al., 2020]. For pre-trained models, the role of individual neurons can be probed through precisely crafted prompts that elicit the desired features, followed by controlled variations in neuron outputs to measure corresponding changes in token probabilities. The neuron with the highest activation is identified and then its activation value is increased by fixed amount, while all other coordinates remain unchanged. The modified residual vector is passed unchanged through the remaining layers of the transformer. The subsequent steps provide detailed description of this approach. Here, the residual stream dimension (final hidden state) is and the vocabulary dimension is denoted as . 1. Extraction from Transformer Layers Iterate over each transformer layer, accessing the corresponding MLP components. 2. Computation of Effective Neuron Weights Calculate effective neuron weights using an elementwise multiplication of up-projection weights Wup and the sigmoid activation (σ) of gate-projection weights Wgate: Weff = Wup σ(Wgate) (1) 3. Decoding via LM Head Decode each row (weff) of Weff using the LM head (WLM RV D), mapping it into vocabulary logits: Compute probability distribution over the vocabulary using the softmax function: Logits = WLM weff + bLM = softmax(Logits) 4. Normalized Activation Score Calculation Determine probability Pt for the token of interest t: Pt = [t] Calculate average probability Pavg over the top-k (here, = 100) tokens: Pavg = 1 (cid:88) i=1 (i) Obtain the normalized activation score (AN ): AN ="
        },
        {
            "title": "3.2 Neuron Attribution: Adaptive Method",
            "content": "(2) (3) (4) (5) (6) The algorithm presented above targets individual neuron activations directly, but in large-scale models the notion that single neuron uniquely encodes high-level concept is often invalid: neurons are commonly polysemantic, responding to many different features or tokens. To achieve consistent concept steering, one must, therefore, identify all neurons associated with given concept - across every layer - and apply coordinated adjustments. Building on the Adaptive Activation Steering (ACT) framework [Wang et al., 2025], an adaptive, multi-neuron gradient-based algorithm is proposed that, at inference time, activates the entire set of concept-linked neurons. The algorithm (0) extends the original three-step ACT framework Wang et al. [2025] by introducing lightweight probe-refinement stage. First, (1) per-prompt styledifference vectors are extracted, (2) cluster these into compact set of steering centroids, and (3) train separate probe at each layer to assign new activations to these clusters at inference. In the second phase, probes are iteratively refined via gradient descent: autoregressive inference is performed under gradient tracking, and at each layer (a) the selected centroid as residual adjustment is applied and (b) accumulate cross-entropy loss comparing the probes prediction to the known cluster label. Finally, the loss is propagated back exclusively through the probe parameters, keeping the weights of the base model fixed, as shown in Fig. 1. This targeted refinement produces more accurate layer-specific classifiers and yields stronger 4 Figure 1: Schematic of Gradient-Refined ACT (G-ACT). Small per-layer probes tap each decoder blocks residual (post-attention or post-FFN) to predict steering-vector cluster, which is then injected back into that same residual streambiasing the model toward the target subject (CPP code) before the final LM head produces token logits. and more consistent steering, while preserving the interpretability of the method and the minimal runtime cost. Notations: Let be the pretrained language model to be steered, and let = {(Qi, A+ , )}N i=1 denote the set of prompts Qi, each paired with target-style answer A+ . We write for the number of transformer layers in M, for the number of steering clusters discovered by K-means, for the number of refinement epochs, and α for the scalar steering strength. For prompt at layer ℓ, let and baseline answer be the activations under the two styles, and define the difference vector i,ℓ, h+ i,ℓ RD"
        },
        {
            "title": "Cluster centroids are denoted",
            "content": "and each probe is linear classifier that maps hi,ℓ to logits i,ℓ = h+ i,ℓ i,ℓ. {ck}C k=1, ck,ℓ RD, πℓ : RD RC zi,ℓ = πℓ(hi,ℓ), used both to select the centroid cki,ℓ and to compute the cross-entropy loss CE(zi,ℓ, ki) against the true cluster label ki. Finally, denotes the steered output generated by after all residual injections."
        },
        {
            "title": "3.3 Generating Dataset",
            "content": "A dataset of coding challenges, inspired from the SciCode corpus [Tian et al., 2024], was assembled to isolate the natural language selection behavior of an LLM. Each example provides problem statement (e.g., Implement the Gauss-Seidel method or Generate the nth Fibonacci sequence) in addition to problem description without mention of programming language. The tasks span both general-purpose algorithms and domain-specific scientific routines, ensuring broad coverage as shown in the Snippet 3.3. Each problem instance in this dataset 5 i=1: prompts with two reference answers , Algorithm Gradient-Refined Adaptive Activation Steering (G-ACT) Require: M: pretrained language model Require: = {(Qi, A+ )}N Require: C: number of clusters Require: E: number of probe refinement epochs Require: α: steering strength Ensure: Steered output 1: Initialize empty list 2: Initialize empty list 3: for = 1 to do 4: 5: 6: 7: 8: end for 9: Partition into clusters D1, . . . , DC KMeans(V ) 10: for = 1 to do 11: 12: 13: end for Run on (Qi, A+ Run on (Qi, Compute vi µ+ Append vi to πj TRAINPROBE(Dj) Append πj to = MeanAct(M) = MeanAct(M) ), let µ+ ), let µ µ Run forward pass of on Qi, collecting per-layer activations {hi,ℓ} for ℓ = 1 to do Zero gradients of all probes in Set total loss 0 for each prompt (Qi, A+ , ) do 14: Probe Refinement 15: for = 1 to do 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end for 28: M.generate(Qtest) end for Compute logits zi,ℓ πℓ(hi,ℓ) += CE(cid:0)zi,ℓ, ji (cid:1) Inject steering: hi,ℓ hi,ℓ + α cji,ℓ end for Backpropagate and update probes in represents distinct scientific or coding scenario; consequently, even after randomly partitioning into training and test subsets (e.g., an 80%/20% split), every held-out example remains effectively out-of-distribution. In other words, there is no natural in-distribution test set, since no two problems share identical underlying characteristics. Prompts are cast as direct questions to instruction-tuned models, and qualitative comparisons of their outputs are used to assess shifts in chosen implementation language. Sample Task 1 - Problem Name: Gauss Seidel Create function to solve the matrix equation Ax = using the Gauss-Seidel iteration. The function takes matrix and vector as inputs. The method involves splitting the matrix into the difference of two matrices, = . For Gauss-Seidel, = L, where is the diagonal component of and is the lower triangular component of A. The function should implement the corresponding iterative solvers until the norm of the increment is less than the given tolerance, xk xk1l2 < ϵ. 6 Sample Task 2 - Problem Name: Non-Uniform Meshing Write code that automatically generates one-dimensional non-uniform mesh with cells (grid points) between locations y0 and yn. The cell size yj = yj yj1 varies according to geometric progression: yj+1 = α yj for = 1 to 1. Implement the mesh generation with = 256 using stretching factor α = 1.08, between y0 = 0 and yn = 1."
        },
        {
            "title": "4 Results",
            "content": "In this study, five instruction-tuned LLMs were evaluated: Llama-3.2-3B-Instruct, compact 3 billion-parameter model; Llama-3.3-70B-Instruct, its 70 billion-parameter counterpart [Grattafiori et al., 2024]; Qwen2.5-Coder-32B-Instruct, 32 billion-parameter model specialized for coding; Qwen2.5-14B-Instruct-1M, 14 billion-parameter variant fine-tuned with context window of one million tokens; and QwQ-32B, general-purpose 32 billion-parameter model [Qwen et al., 2025]. Models were evaluated on the 84 benchmark questions at sampling temperature (T) of 1.0. To verify statistical stability, each modelprompt pair was sampled both 100 and 25 times; observed performance differences between these two sampling sizes were under 1%. Consequently, 25 repetitions per prompt were deemed sufficient and used for all reported results."
        },
        {
            "title": "4.1 Language Preferences",
            "content": "Figure 2: Programming-language preferences of Llama-3.2-3B-Instruct on set of coding tasks (25 runs per task at = 1.0). Figure 3: Programming-language preferences of Qwen2.5-14B-Instruct-1M on set of coding tasks (25 runs per task at = 1.0). Figure 2 illustrates the programming-language distribution for Llama-3.2-3B-Instruct across 84 benchmark problems. Python (green) predominates, accounting for roughly 7080% of outputs on nearly every task. Java (blue) appears most frequently on classical algorithmic challenges, such as sorting routines, string manipulations, and data structure implementations, peaking at about 30%. CPP (red) is chosen only sporadically (<10%), typically for numerically intensive or performance-critical operations such as prime-number checks and binary conversions. Julia (purple) applies exclusively to small subset of domain-specific scientific problems (e.g. differential equation solvers, linear algebra benchmarks), also at low frequency (<10%). Figure 3 presents the analogous results for Qwen2.5-14B-Instruct-1M. In this case, Julia is the predominant output language ( 67%), 7 with Python as the secondary preference ( 33%) and negligible use of Java or CPP. The results of the remaining models are presented in A.1. Figure 4: Aggregate programming-language preferences of five LLMs on the coding benchmark (25 repetitions per task at sampling temperature 1.0). Each pie-chart shows the percentage of generated solutions in Python (green), CPP (red), Java (blue), and Julia (purple) for the corresponding model. The summary of results for the five models is presented in Figure 4. Llama-3.2-3B-Instruct defaults strongly to Java (76.2%) with the remainder in Python (23.8%), whereas the larger Llama-3.3-70B-Instruct model reverses that biasPython accounts for 73.8% of outputs and Julia for 26.2%. Among the Qwen family, Qwen2.5-Coder-32B-Instruct generates Python most often (59.5%), followed by Julia (35.7%) and small 4.8% share of CPP; Qwen2.5-14B-Instruct-1M exhibits pronounced Julia preference (66.7%) over Python (33.3%); and the general-purpose QwQ-32B again favors Python (77.4%) with Julia at 17.9% and CPP at 4.8%. These findings illustrate that variations in model scale, architectural design, and fine-tuning data collectively impart distinct, reproducible biases in each models code generation behavior. In particular, smaller, distilled models are often fine-tuned on corpora focused on web development and other routine applications. Based on these findings, to bias code generation towards the CPP language for scientific computing, there is need for targeted alignment, through prompt engineering or domain-specific fine-tuning of the model."
        },
        {
            "title": "4.2 Static Method Analysis",
            "content": "Figure 5: Activation maps for A. Llama-3.2-3B-Instruct and B. Qwen2.5-14B-Instruct-1M models respectively. Neuron with highest activation probability is marked with red circle for CPP token. The procedure outlined in Sec. 3.1 was applied across all models. Figure 5 shows the activation maps for Llama-3.2-3B-Instruct and Qwen2.5-14B-Instruct-1M, with the neuron exhibiting the highest probability for the CPP token located in layer 27 of the Llama model and layer 31 of the Qwen model. Activation maps for the remaining three models are shown in Figure 15."
        },
        {
            "title": "4.2.1 Inducing Language Preference Bias",
            "content": "To evaluate the causal role of the identified CPP neuron, Llama-3.2-3B-Instruct was rerun on the benchmark with artificially amplified activation of the same neuron (L27 N6859). The resulting shift in language selection frequencies, demonstrating an increase in CPP output, is shown in Fig. 6. Figure 6: Programming-language preferences of Llama-3.2-3B-Instruct on set of coding tasks (25 runs per task at = 1.0) with activated neuron for CPP. In the baseline configuration, Java was the dominant output language for Llama-3.2-3B-Instruct. Compared to baseline  (Fig. 2)  , where CPP never exceeded 10 %, the red bars now dominate almost all problems, often approaching or reaching the output of CPP 100 %. Python (green) and Java (blue) virtually disappear, and Julia (purple) is completely eliminated. In converse experiment, by applying the same algorithm to amplify the neuron associated with the Python token, the model was re-evaluated on the identical 84 task benchmark. As shown in Fig. 7, this single neuron drives the model to select Python for virtually every prompt, green bars reach nearly 100 %, while CPP, Java and Julia outputs drop to negligible levels. These complementary tests confirm that selective activation of individual MLP neurons exerts strong causal control over the models programming-language choice, effectively overriding its native bias. The next section examines whether selective activation of the identified neuron can causally enforce CPP-only code generation regime, irrespective of the original task context. Figure 7: Programming-language preferences of Llama-3.2-3B-Instruct on set of coding tasks (25 runs per task at = 1.0) with activated neuron for Python."
        },
        {
            "title": "4.2.2 Code Generation Testing",
            "content": "To establish baseline, the Llama-3.2-3B-Instruct model was tasked (prompt in Sec.B) with generating code for each problem in the dataset by restricting its language choice exclusively to CPP and Python. The first two tokens of the generated output contain the markdown style format of the code: python or cpp that indicate the code preference. The overall results are shown in Fig. 8, where two distinct regimes emerge. First, Python-leaning tasks (e.g., high-level array manipulations, string processing) see Python outputs ranging from 4080 %. Second, CPP-dominant tasks (e.g., performance-critical routines, bitwise operations, numeric solvers) exhibit CPP output exceeding 6090 %. Across all problems, the model defaults to CPP ( 73 %) more often than Python, but still resorts to Python (about 26 %) for substantial minority of prompts. This variability reflects the native bias of the model towards low-level implementations on computationally intensive tasks, while retaining flexibility to generate Python for simpler algorithmic challenges. 9 Figure 8: Code output of Llama-3.2-3B-Instruct on set of coding tasks (25 runs per task at = 1.0) without activated neuron. Figure 9: Code output of Llama-3.2-3B-Instruct on set of coding tasks (25 runs per task at = 1.0) with activated neuron for CPP. Activated Neuron Llama-3.2-3B Model Standard Llama-3.2-3B Model def gauss_seidel(A, b, tolerance=1e-10, max_iter=1000): = len(b) = [0.0 for in range(n)] for in range(n): x[i] = b[i] for in range(i): x[i] -= A[i][j] * x[j] x[i] /= A[i][i] for iter in range(max_iter): temp = [0.0 for in range(n)] for in range(n): temp[i] = x[i] for in range(i): temp[i] -= A[i][j] * x[j] temp[i] /= A[i][i] norm_x = sum([pow(x_i - t_i, 2) for x_i, t_i in zip(x, temp)]) norm_x = pow(norm_x, 0.5) if norm_x < tolerance: break = temp return # Example usage = [[3, 2, 1], [2, 4, 1], [1, 1, 6]] = [8, 12, 20] = gauss_seidel(A, b) print(x) #include <iostream> #include <vector> #include <cmath> using namespace std; void gauss_seidel( const vector<vector<double>>& A, vector<double>& b, double tolerance, int max_iter) { int = A.size(); vector<double> x(n); for (int = 0; < n; i++) { for (int = 0; < n; k++) { double sum = 0.0; for (int = 0; < i; j++) { sum += A[i][j] * x[j]; } } x[i] = (b[i] - sum) / A[i][i]; } for (int iter = 0; iter < max_iter; iter++) { vector<double> temp = x; for (int = 0; < n; i++) { double sum = 0.0; for (int = 0; < n; k++) { if (i!= k) { sum += A[i][k] * temp[k]; } } x[i] = (b[i] - sum) / A[i][i]; } double norm_x = 0.0; for (int = 0; < n; i++) { norm_x += pow(x[i]-temp[i], 2); } norm_x = sqrt(norm_x); if (norm_x < tolerance) break; } } 10 For the identical prompt, activating the CPP neuron produces an overwhelmingly CPP-centric output profile (red) as shown in Fig. 9. In almost every problem, CPP now constitutes about 80-100 % of the 25 samples, while Python (green) retreats to at most 1020 % on handful of tasks. This confirms that up-weighting the identified unit exerts causal, near-complete override of the models native language preferences, driving it to generate CPP code almost exclusively, while small residual Python ( 5%) tail on few prompts suggests minimal contextual resistance. For brevity, the code generated by the model for the Gauss-Seidel problem is provided in Snippet 4.2.2. While the generated CPP implementation allocates the solution array, performs relaxation updates in nested loops, monitors convergence against the prescribed tolerance, and terminates once the residual falls below the threshold, comprehensive validation of its correctness and performance has not been conducted and is beyond the scope of this study. Additionally, the model tends to generate Python code for the photon angular momentum problem in Fig. 8 that shifted to CPP after neuron activation. The generated code is shown in Snippet A.4. This shift from Python to CPP without modifying the prompt demonstrates that neuron-level activation can potentially override the models language choice while maintaining functional correctness. What is the activation value for single neuron? All experiments so far apply fixed perturbation to single neuron to induce programming-language bias. However, cross-model evaluations reveal that both the optimal perturbation magnitude and the most effective neuron are strongly dependent on the model architecture and prompt phrasing. In addition to LM-head, activating neurons within MLP or attention-head sublayers incurs minimal memory and computation overheads and is thus recommended [Kramár et al., 2024]. We advise systematically tuning the perturbation amplitude and, where feasible, testing multiple candidate neurons, to identify an optimal configuration for any given prompt and model architecture."
        },
        {
            "title": "4.3 Adaptive Method Analysis",
            "content": "To build the framework, the initial step involves splitting (70/30%) the dataset of the coding problem prompts 3.3 into disjoint training and test sets. For each prompt in training set, two forward passes through the frozen model (here Llama-3.2-3B-Instruct) are performed, one requesting CPP solution and one requesting Python solution, and the final token activations h+ i,ℓ at each layer ℓ are stored. The per-layer difference (i,ℓ) vectors were then used to characterize the \"style shift\" between CPP and Python, following the original ACT concept [Wang et al., 2025]. i,ℓ and In Figure 10, left plot shows the mean ℓ2 norm of the vectors (ℓ2) as function of the layer index (Li). It is observed that the style signal is negligible in the earliest layers but grows steadily, reaching its maximum in the final blocks, which guided the decision to focus the steering efforts on the later layers. While the full hidden-state differences (blue) carry the largest absolute style signal in late layers, the attention-head differences (orange) already exhibit clear and much lower dimensional rise beginning around layer 5. Therefore, head outputs were selected to probe because they strike practical balance between signal strength, interpretability, and computational cost: probing the entire hidden state or MLP output would require fitting and storing far larger clusters (and risks overfitting on spurious components of the style vector). Figure 10: Results for Llama-3.2-3B-Instruct model. Left: mean ℓ2-norm of per-layer CPPPython difference vectors as function of layer index. Right: UMAP projection of the flattened head-difference vectors, colored by four K-means clusters; crosses mark the cluster centroids. The right plot in figure 10 visualizes the head-difference vectors, in particular their four distinct modes when projected via UMAP [McInnes et al., 2018] and clustered by K-means [Arthur and Vassilvitskii, 2007]. Each cluster centroid (cross) represents family of prompts that induce very similar shifts (e.g., algorithmic or I/O intense problems) in the attention-head activations when input switches between CPP and Python. Few outliers form their own small clusters because their style change is qualitatively different from the bulk of the tasks. Cluster 2 comprises about 20 straightforward coding tasks such as binary search, removing duplicates, array sorting, etc. which sit far from the other clusters. Cluster 3 (upper left) groups more complex algorithmic problems, while Cluster 1 contains prompts with descriptions of scientific variables (values, units) rather than pure code tasks. Although users may choose to cluster hidden-state or MLP i,ℓ vectors instead, the attention-head clusters offer compact set of steering directions for the inference step."
        },
        {
            "title": "4.3.1 Per-Layer Activation Probes",
            "content": "Per-layer probe results for biasing generation toward CPP style are presented in this section. The standard ACT and the gradient-based ACT algorithms train their respective probes to optimize different objectives under different regimes. ACT fits separate logistic regression on the flattened diffs of each layer, minimizing CE directly on that static dataset until convergence. The gradient-refined ACT embeds the probes inside an autoregressive generation loop: Each probe is updated by summing CE losses across every token step, while also injecting its chosen centroid back into the model. Here, the gradient-refined probes are trained for 50 epochs. Both sets of probes are evaluated on the testing set of prompts, which due to their uniqueness can be considered out-of-distribution (see Sec.3.3). To mitigate sensitivity to prompt phrasing, each test problem is evaluated using ten distinct CPP templates (e.g., \"Problem: . . .\", \"Implement the following in CPP: . . .\", etc.) and ten analogous Python templates. This ensemble of paraphrases ensures that the performance metrics reflect true style shifts rather than idiosyncrasies of any single prompt formulation. Evaluation Metrics For each prompt and each transformer layer, the i,ℓ vector is computed for every template and averaged, thus smoothing out surface-level variability, and then the probe predicts its nearest centroid. The probe classification accuracy and macro-F1 scores serve as the comparison metric. This comparison method provides clear, layer-wise view of which method more reliably recognizes the correct steering mode by isolating probe quality from downstream generation effects. (cid:80)N i=1 1(cid:0)ˆyi = yi Accuracy: Fraction of test prompts for which the per-layer probe correctly predicts the cluster ID (assigned during training). Given test examples with true labels yi and predicted labels ˆyi, accuracy is Acc = 1 Macro-F1: Unweighted average of the F1 scores across all clusters, giving equal importance to small and large clusters. For C-class problem, let TPc, FPc, FNc be the true-positive, false-positive, and false-negative counts for class c. Define per-class precision, recall, and F1 as (cid:1), where 1() is the indicator function. Precisionc = TPc TPc + FPc , Recallc = TPc TPc + FNc , F1c ="
        },
        {
            "title": "2 Precisionc Recallc\nPrecisionc + Recallc",
            "content": ". (cid:80)C c=1 F1c. As mentioned earlier, Macro-F1 is then the unweighted mean of the per-class F1 scores: Macro-F1 = 1 In Fig.11, the gradient-refined ACT probes deliver strong improvements on Llama 3B but collapse when applied to Llama 70B. For the 3B model, gradient-based refinement increases the accuracy of the early layers 06 from 0% to 61. 5% (and 0 to 0.254 macroF1), improving what were previously uninformative probes. Midlayers (7, 11-12, 15-17, 20, 22-27) see modest to significant gains, while few layers (8, 10, 13, 19) tend to under-predict with refinement, suggesting overfitting or class collapse in those specific blocks. Layers that were already strong in standard ACT (9, 16, 21, 23, 25, 27) remain unchanged at peak performance. Averaged across all 28 layers, the accuracy increases from 0.405 to 0.556 (+15%) and the macro-F1 from 0.165 to 0.238 (+7.3%), demonstrating that the gradient-refined ACT probes more reliably recognize the correct steering mode, especially in layers that originally carried smaller or noisy style signal. In contrast, on the much larger 70B model, where only hidden-state differences were used and clusters reduced to three (see Fig. 16), both standard ACT and gradient ACT probes underperform: the mean accuracy is only 6.3% for vanilla and 19.1% for refined; mean macro-F1 is 0.041 vs. 0.111. In other words, the head-state activations that carry clear style signal in 3B become too diffuse or noisy in 70B model, so clustering and probes trained on those attention-head features lose their discriminative power; instead hidden-state features (see Fig. 10) can be used in such scenarios. Furthermore, the probe logic does come with non-trivial runtime cost. As shown in Table 1, on Llama-3B single no-probe\" forward pass takes approx. 3.25 s, but inserting standard ACT injections inflates that to about 6.78 (nearly double). Gradient-refined ACT falls in between ( 4.55 s) as it must evaluate the lightweight classifier of each layer during generation but does not rerun the clustering offline. The 70B model shows the same pattern at scale: base generation costs about 27.8 s, standard ACT around 38.7 s, and gradient-refined ACT takes 32.6 s. In practice, each per-layer probe entails small extra linear pass (and, when using gradients, backward step), so total latency increases compared to unmodified inference. Gradient-ACT skips the expensive nearest-centroid search at every layer. In standard ACT, i,ℓ of each prompt token must be compared (L2 distance) with all centroids to select which steering vector to add, resulting in an O(KD) operation per layer per token. In contrast, gradient-ACT uses smaller linear probe (single matrix multiplication) to predict the cluster, so it avoids repeatedly computing distances. As result, even though gradient-ACT still evaluates Figure 11: Results for A. Llama-3.2-3B-Instruct model and B. Llama-3.3-70B-Instruct model. AccRef and AccGR denote the proportion of prompts correctly assigned to their nearest centroid by the standard and refined probes, respectively. 1Ref and 1GR are the corresponding macro-averaged F1 scores."
        },
        {
            "title": "Method",
            "content": "Llama-3.2-3B-Instruct Llama-3.3-70B-Instruct Vanilla Model Standard ACT Gradient-Refined ACT 3.2502 0.0033 6.7840 0.0049 4.5526 0.0367 27.7948 0.0025 38.7461 0.0177 32.6276 0.0965 Table 1: Inference time performance of per-layer probes under standard and gradient-based ACT methods compared to no-probe setting for different Llama family models. Average timings are reported across 25 inference runs, after discarding the first 5 warm-up iterations. one extra linear layer per token, it ends up faster than full K-means lookup at inference time, and hence the intermediate runtime."
        },
        {
            "title": "5 Conclusion",
            "content": "Five causal language models were evaluated on suite of scientific coding prompts to quantify coding language bias. While smaller models preferred Java or Julia inconsistently, both variants of the LLaMA models exhibited clear separability in their internal activations when prompted to select one of four language options. static neuron-attribution approach, identifying individual LM-head neurons correlated with the CPP style and manually perturbing them, is shown to produce CPP code in limited cases. However, this method proved fragile, with performance highly sensitive to choice of neuron, prompt formulation, and model scale. To overcome these limitations, gradient-refined version of ACT is introduced. Per-prompt difference\" vectors (CPP minus Python activations) are clustered into small set of representative steering directions, and lightweight per-layer probes are trained, then further refined online via cross-entropy loss, during generation. In LLaMA-3.2 3B, the average probe classification accuracy of the probe increased, with the layers (06) improving from 0 % to 61.5 % accuracy. Even on the larger LLaMA-3.3 70B model, where head-state signals become more diffuse, G-ACT 13 still increases accuracy, demonstrating that targeted injections at key layers can reliably bias generation toward CPP despite overall weak activations. Although adding per-layer probes incurs modest runtime overhead (approximately 1.31.4 slower than base generation), practical deployments can accommodate this cost by steering only subset of layers, caching subtokens, or accepting slightly longer latencies. Even imperfect probes suffice to steer the model output toward the desired style, yielding substantial qualitative gains in CPP code generation, and can be extended to other concepts of interest. While static neuron perturbation may only be viable in narrow scenarios, G-ACT provides scalable, interpretable, and efficient mechanism for steering LLMs toward CPP (or any target subject), with acceptable inference costs. Beyond steering concepts, this approach embeds persistent transformation matrices that guarantee identical model behavior across different users, fostering new paradigm of output reproducibility."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The authors acknowledge support from Los Alamos National Laboratory under the project Algorithm/Software/Hardware Co-design for High Energy Density Applications\" at the University of Michigan."
        },
        {
            "title": "References",
            "content": "David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 07, page 10271035, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245. Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu, and Henry Kvinge. Robustness of edited neural networks. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. Jinyin Chen, Guohan Huang, Haibin Zheng, Hang Du, and Jian Zhang. Robust explanations for graph neural network with neuron explanation component. Information Sciences, 654:119785, 2024. Rayner HM Condori and Odemir Bruno. Analysis of activation maps through global pooling measurements for texture classification. Information Sciences, 555:260279, 2021. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019. Harry Davies. Flash interpretability: Decoding specialised feature neurons in large language models with the lm-head, 2025. URL https://arxiv.org/abs/2501.02688. Nelson Elhage et al. Toy Models of Superposition. Transformer Circuits Thread, 2022. URL https:// transformer-circuits.pub/2022/toy_model/index.html#motivation-privileged. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery with ai agents. Cell, 187(22):61256151, 2024. Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In International Conference on Machine Learning, pages 73247338. PMLR, 2022. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong 15 Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Mourad Gridach, Jay Nanavati, Christina Mack, Khaldoun Zine El Abidine, and Lenon Mendes. Agentic AI for scientific discovery: survey of progress, challenges, and future directions. In Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation, 2025. URL https: //openreview.net/forum?id=TyCYakX9BD. Lan Huang, Yangguang Shao, Wenju Hou, Hui Yang, Yan Wang, Nan Sheng, Yinglu Sun, and Yao Wang. Pcg-cam: Enhanced class activation map using principal components of gradients and its applications in brain mri. Information Sciences, 708:122046, 2025. Nick Jiang, Anish Kachinthaya, Suzie Petryk, and Yossi Gandelsman. Interpreting and editing vision-language representations to mitigate hallucinations. arXiv preprint arXiv:2410.02762, 2024. 16 Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multi-agent robot task planning using large language models. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1214012147, 2024. doi: 10.1109/IROS58592.2024.10802322. Juhee Kim, Woohyuk Choi, and Byoungyoung Lee. Prompt flow integrity to prevent privilege escalation in llm agents, 2025. URL https://arxiv.org/abs/2503.15547. János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. Atp*: An efficient and scalable method for localizing llm behaviour to components. arXiv preprint arXiv:2403.00745, 2024. János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. Atp*: An efficient and scalable method for localizing llm behaviour to components, 2024. URL https://arxiv.org/abs/2403.00745. Naveen Krishnan. Ai agents: Evolution, architecture, and real-world applications, 2025. URL https://arxiv. org/abs/2503.12687. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/abs/2408.06292. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=-h6WAS6eE4. Robert Lasenby Nelson Elhage and Christopher Olah. Privileged Bases in the Transformer Residual Stream. Transformer Circuits Thread, 2023. URL https://transformer-circuits.pub/2023/ privileged-basis/index.html. Kumari Nishu, Sachin Mehta, Samira Abnar, Mehrdad Farajtabar, Maxwell Horton, Mahyar Najibi, Moin Nabi, Minsik Cho, and Devang Naik. From dense to dynamic: Token-difficulty driven moefication of pre-trained llms. arXiv preprint arXiv:2502.12325, 2025. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. OpenAI. OpenAI Codex, 2021. URL https://github.com/openai/codex. Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, and Yixuan Li. Steer llm latents for hallucination detection. arXiv preprint arXiv:2503.01917, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Caroline Mazini Rodrigues, Nicolas Boutry, and Laurent Najman. Unsupervised discovery of interpretable visual concepts. Information Sciences, 661:120159, 2024. Vansh Sharma and Venkat Raman. reliable knowledge processing framework for combustion science using foundation models. Energy and AI, 16:100365, 2024. ISSN 2666-5468. doi: https://doi.org/10.1016/j.egyai. 2024.100365. Tomasz Szandała. Unlocking the black box of cnns: Visualising the decision-making process with prism. Information Sciences, 642:119162, 2023. Adly Templeton et al. Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/ scaling-monosemanticity. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: research coding benchmark curated by scientists, 2024. URL https://arxiv.org/abs/2407.13168. 17 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. Advances in Neural Information Processing Systems, 2017. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1238812401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, 2023. URL https: //arxiv.org/abs/2305.16291. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024a. Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu Chu, Junyi Gao, Yasha Wang, and Liantao Ma. Adaptive activation steering: tuning-free llm truthfulness improvement method for diverse hallucinations categories. In Proceedings of the ACM on Web Conference 2025, WWW 25, page 25622578, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400712746. doi: 10.1145/3696410.3714640. URL https://doi.org/10.1145/3696410.3714640. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In ICML, 2024b. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Ziyi Yang, Shreyas S. Raman, Ankit Shah, and Stefanie Tellex. Plug in the safety chip: Enforcing constraints for llm-driven robot agents. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1443514442, 2024. doi: 10.1109/ICRA57147.2024.10611447. Wei Jie Yeo, Rui Mao, Moloud Abdar, Erik Cambria, and Ranjan Satapathy. Debiasing clip: Interpreting and correcting bias in attention heads. arXiv preprint arXiv:2505.17425, 2025. Itay Yona, Ilia Shumailov, Jamie Hayes, Federico Barbero, and Yossi Gandelsman. Interpreting the repeated token phenomenon in large language models. arXiv preprint arXiv:2503.08908, 2025. Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=Hf17y6u9BC. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL."
        },
        {
            "title": "A Results",
            "content": "A.1 Model Preference (Extended) The following figure illustrates the code-generation language distributions for Llama-3.3-70B-Instruct, Qwen2.5Coder-32B-Instruct, and QwQ-32B. In each model, Julia and Python dominate the outputs, while Java and CPP appear only marginally. Figure 12: Programming-language preferences of Llama-3.3-70B-Instruct on set of coding tasks (25 runs per task at = 1.0). Figure 13: Programming-language preferences of Qwen2.5-Coder-32B-Instruct on set of coding tasks (25 runs per task at = 1.0). Figure 14: Programming-language preferences of QwQ-32B on set of coding tasks (25 runs per task at = 1.0). A.2 Activation Maps Figure 15: Activation maps for C. Llama-3.3-70B-Instruct, D. QwQ-32B and E. Qwen2.5-Coder32B-Instruct models respectively. Neuron with highest activation probability is marked with red circle for CPP token. A.3 Steering Vectors for Llama-3.3-70B-Instruct Model Figure 16 illustrates two complementary views of the CPP Python steering signal in the 70B LLaMA model. On the left, the mean norm of per-layer difference vectors grows steadily with depth, showing that later layers carry substantially stronger language-style cues (hidden-state MLP attention-head). On the right, UMAP embedding of the flattened head-difference vectors reveals three well-separated clusters of prompts, each centroid (x) capturing distinct style subgroup. Together, these plots confirm that (1) deeper layers encode more pronounced style differences and (2) prompt-level style variations can be compactly represented by small number of steering directions. Figure 16: Results for Llama-3.3-70B-Instruct model. Left: mean ℓ2-norm of per-layer CPPPython difference vectors as function of layer index. Right: UMAP projection of the flattened head-difference vectors, colored by four K-means clusters; crosses mark the cluster centroids. 21 A.4 Code Generation Examples Activated Neuron Llama-3.2-3B-Instruct Model #include <iostream> #include <vector> #include <cmath> #include <complex> #include <queue> void boson_distribution(double temperature, double* omega_r, double* omega_i, double* sum = nullptr, int N, int total_modes = -1, int phonons = -1) { for (int nu = 0; nu < N; nu++) { for (int = 0; < phonons; i++) { double numerator = std::exp(-omega_r[i] / (temperature * TMath_Pow(2.718281828459045, TMath_Exp(1.0)))+1.0); double denominator = 1; double exponent = 0.0; for (int q_index1 = 0; q_index1 < phonons; q_index1++) { for (int n1 = 0; n1 <= 2; n1++) { exponent += std::exp(-(omega_i[q_index1] * n1)); } } denominator = std::sqrt(exponent); if (sum!= nullptr) { sum[i] = numerator / denominator; } else { std::cout << numerator / denominator << std::endl; } } } } void calculate_phonon_angular_momentum(const double *frequencies, const double polarization_vector[3][3], int index_x, int index_y, int index_z, double temperature) { double N_Bohr_Q *= 0; int phonons = 3; double n_boson = 100.0; double omega_r[100] = {0}; double omega_i[10] = {0}; for (int = 0; < phonons; ++i) { omega_r[i] = frequencies[i]; for (int = 0; < 3; ++k) { for (int = 0; < 3; ++j) { omega_i[i] += (1 / n_boson) * std::sqrt(std::pow(polarization_vector[k][j][0] * frequencies[i] + n_Bohr_Q * omega_r[i] / sqrt(1 + 1),3)+ std::pow(polarization_vector[k][j][1] * frequencies[i] + n_Bohr_Q * omega_r[i] / sqrt(1 +orus),3) + std::pow(polarization_vector[k][j][2] * frequencies[i] + n_Bohr_Q * omega_r[i] / sqrt(1 + std::sqrt(omega )))); } } } double sum = 0; for (int q_index1 = 0; q_index1 < 3; ++q_index1) { double temp_sumsum[q_index1] = 0; for (int nu = 0; nu < phonons; ++nu) { temp_sumsum[q_index1] += omega_i[nu] + 0.5; temp_sumsum[q_index1] *= polarization_vector[q_index1][0][0]; temp_sumsum[q_index1] *= polarization_vector[q_index1][1][1]; temp_sumsum[q_index1] *= polarization_vector[q_index1][2][2]; } sum += temp_sumsum[q_index1]; } double M_alpha[3][3] = {{0, -1, 0}, {1, 0, 0}, {0, 0, -1}}; double unit_matrix[3][3] = {1,0,0,0,1,0,0,0,1, 0,1,0,0,0,1,0,0,1, 0,0,1,0,0,0,1,0,1}; for (int alpha = 0; alpha < 3; alpha++) { double unit_matrix_new[3][3] = {}; for (int = 0; < 3; ++i) { for (int = 0; < 3; ++j) { unit_matrix_new[i][j] = unit_matrix[i][j] * M_alpha[alpha][i] * polarization_vector[alpha][j]; } } if (alpha == index_x) sum /= 16; if (alpha == index_y) sum /= 16; if (alpha == index_z) sum /= 16; } double L_alpha = 1.054571817*sum; printf(\"%.15lf\", L_alpha); } 22 Standard Llama-3.2-3B-Instruct Model import numpy as np import matplotlib.pyplot as plt from scipy.constants import Boltzmann from numpy.fft import fft, ifft def bose_distribution(omega, beta, n=0): return (np.exp(beta * omega) - 1) / (np.exp(beta * omega) - 1) + def calculate_phonon_angular_momentum(phonon_frequencies, phonon_polarization_vectors, matrix_elements, temperature): = (np.eye(len(phonon_frequencies)) * np.eye(3) + np.vandermonde([1, np.pi, np.pi/2], beta = 1 / (Boltzmann * temperature) Lx = np.sum(phonon_polarization_vectors[:, :, np.newaxis, np.newaxis] @ np.vandermonde([2, np.pi/2, np.pi/2]))) @ phonon_polarization_vectors [np.newaxis, : , np.newaxis, np.newaxis] * bose_distribution(phonon_frequencies, beta, n=0) + (1/2) * np.eye(3) @ np.sum( phonon_polarization_vectors[:, np.newaxis, np.newaxis, np.newaxis] * phonon_polarization_vectors[np.newaxis, :, np.newaxis, :], Ly = np.sum(phonon_polarization_vectors[:, np.newaxis, : axis=2) * bose_distribution(phonon_frequencies, beta, n=0), axis=2)) , np.newaxis] @ @ phonon_polarization_vectors[:, ::-1, :, np.newaxis] * bose_distribution(phonon_frequencies, beta, n=0) + (1/2) * np.sum(phonon_polarization_vectors[:, :, np.newaxis, np.newaxis] * phonon_polarization_vectors[np.newaxis, ::-1, :, np.newaxis] * bose_distribution(phonon_frequencies, beta, n=0), axis=2)) Lz = np.sum(phonon_polarization_vectors[:, np.newaxis, : , np.newaxis] @ np.swapaxes( M, axis=0) @ phonon_polarization_vectors[:, ::-1, :, np.newaxis] * bose_distribution(phonon_frequencies, beta, n=0) + (1/2) * np.sum( phonon_polarization_vectors[:, :, np.newaxis, np.newaxis] @ phonon_polarization_vectors[np.newaxis, :, :, np.newaxis] * bose_distribution(phonon_frequencies, beta, n=0), axis=2)) return Lx, Ly, Lz # Test the function omega = np.linspace(1, 10, 100) photon polarization_vectors = np.random.rand(10, 3, 3) np.savez(compress=True,\"phonon_frequency.npy\", phonon_frequencies=omega) np.savez( compress=True, phonon_polarization_vectors=photon_polarization_vectors) = np.load(\"M.npy\") Lx, Ly, Lz = calculate_phonon_angular_momentum( np.load(\"phonon_frequency.npy\"), np.load(\"phonon_polarization_vectors.npz\") )[\"phonon_frequencies\"]. squeeze(), np.load(\"phonon_frequency.npy\"), np.load(\"phonon_polarization_vectors.npz\") plt.plot(Lx, Ly) plt.show() 23 A.5 Code Generation with G-ACT G-ACT is used to activate the CPP code generation behavior in the Llama 3B and 70B models for the following prompts: P1: Create function to solve the matrix equation Ax = using the Gauss-Seidel iteration. The function takes matrix and vector as inputs. The method involves splitting the matrix into the difference of two matrices, = . For Gauss-Seidel, = L, where is the diagonal component of and is the lower triangular component of A. The function should implement the corresponding iterative solvers until the norm of the increment is less than the given tolerance, xk xk1l2 < ϵ. And P2: Write me CFD code for the compressible Euler system of equations for flow over wedge at 30 degrees. Simply provide the code only. Assume air as the fluid at mach 3 inlet velocity and isentropic relations are used. As shown in the following code blocks, the models autonomously generate CPP code directly from the prompt, without any additional user guidance. Across repeated trials, it produces only CPP implementations, underscoring the robustness and effectiveness of the proposed framework. G-ACT + Llama-3.2-3B-Instruct Model for P1 #include <iostream> #include <vector> // Function to calculate the L2 norm (Euclidean norm) double l2_norm(const std::vector<double>& vec) { double sum = 0; for (const auto& val : vec) { // const auto& allows us to avoid copying elements fromvec sum += val * val; } return sqrt(sum); } // Function template to perform Gauss-Seidal iterations on Ax=b with split into M=N=A-D template<typename T> void gauss_seidel(T* A, int n, T* b, tol=1e-10, int max_iter=10000) { if(n==1){ x[1]={}; x[0]=b[0]/A[0]; std::cout << \"Solution: \"; for(int i=0;i<n;++i){std::cout<<x[i]<<\" \";}std::endl; return ; } T* x=new T[n]; // Allocate memory for solution vector bool converged=true; for(int iter=0;iter<max_iter&&converged;iter++){ for (int j=n-1;j>=0;j--) { sum=0; for ( int k=j+1;k<n;k++ ) { sum+=A[j*n+k]*x[k]; } if(j!=n-1){ sum-=A[j*(j+1)]*x[j]; } x[j]=(b[j]-sum)/A[j*j]; } inc_l2=l2_norm(x); if(inc_l2<tol){ converged=false; } } delete[] x; // Deallocate memory after use } G-ACT + Llama-3.3-70B-Instruct Model for P2 #include <iostream> #include <fstream> #include <vector> #include <cmath> #include <iomanip> 24 using namespace std; // Define constants const double PI = 3.14159265358979323846; const double GAMMA = 1.4; // Adiabatic index for air const double MACH = 3.0; // Mach number const double THETA = 30.0 * PI / 180.0; // Angle of wedge in radians const double = 287.0; // Gas constant for air const double CP = 1005.0; // Specific heat capacity at constant pressure for air const double CV = CP - R; // Specific heat capacity at constant volume for air // Define grid parameters const int NX = 100; // Number of grid points in x-direction const int NY = 100; // Number of grid points in y-direction const double LX = 10.0; // Length of domain in x-direction const double LY = 10.0; // Length of domain in y-direction const double DX = LX / (NX - 1); // Grid spacing in x-direction const double DY = LY / (NY - 1); // Grid spacing in y-direction // Define flow parameters const double P0 = 101325.0; // Stagnation pressure const double T0 = 288.0; // Stagnation temperature const double U0 = MACH * sqrt(GAMMA * * T0); // Inlet velocity const double P1 = P0 * pow(1.0 + (GAMMA - 1.0) / 2.0 * MACH * MACH, -GAMMA / (GAMMA - 1.0)); // Inlet pressure const double T1 = T0 / (1.0 + (GAMMA - 1.0) / 2.0 * MACH * MACH); // Inlet temperature const double RHO1 = P1 / (R * T1); // Inlet density // Define time parameters const double DT = 0.001; // Time step const double T_END = 10.0; // End time // Define variables double rho[NX][NY]; // Density double u[NX][NY]; // x-velocity double v[NX][NY]; // y-velocity double p[NX][NY]; // Pressure double e[NX][NY]; // Total energy // Function to calculate fluxes void calculateFluxes(double &Fx, double &Fy, double rho, double u, double v, double p, double e) { Fx = rho * u; Fy = rho * v; } // Function to calculate source terms void calculateSourceTerms(double &Sx, double &Sy, double rho, double u, double v, double p, double e) { Sx = 0.0; Sy = 0.0; } // Function to update variables void updateVariables(double &rho, double &u, double &v, double &p, double &e, double Fx, double Fy, double Sx, double Sy) { rho += DT * (Sx - (Fx / DX + Fy / DY)); += DT * (Sx / rho - (Fx * / (rho * DX) + Fy * / (rho * DY))); += DT * (Sy / rho - (Fx * / (rho * DX) + Fy * / (rho * DY))); += DT * (Sx * - (Fx * / (rho * DX) + Fy * / (rho * DY))); += DT * (Sx * - (Fx * / (rho * DX) + Fy * / (rho * DY))); } int main() { // Initialize variables for (int = 0; < NX; i++) { for (int = 0; < NY; j++) { rho[i][j] = RHO1; u[i][j] = U0; v[i][j] = 0.0; p[i][j] = P1; e[i][j] = P1 / (GAMMA - 1.0) + 0.5 * RHO1 * (U0 * U0 + 0.0 * 0.0); } } // Time loop for (double = 0.0; < T_END; += DT) { // Calculate fluxes and source terms for (int = 0; < NX; i++) { for (int = 0; < NY; j++) { double Fx, Fy, Sx, Sy; calculateFluxes(Fx, Fy, rho[i][j], u[i][j], v[i][j], p[i][j], e[i][j]); calculateSourceTerms(Sx, Sy, rho[i][j], u[i][j], v[i][j], p[i][j], e[i][j]); // Update variables updateVariables(rho[i][j], u[i][j], v[i][j], p[i][j], e[i][j], Fx, Fy, Sx, Sy); } } // Apply boundary conditions for (int = 0; < NY; j++) { rho[0][j] = RHO1; 25 u[0][j] = U0; v[0][j] = 0.0; p[0][j] = P1; e[0][j] = P1 / (GAMMA - 1.0) + 0.5 * RHO1 * (U0 * U0 + 0.0 * 0.0); } for (int = 0; < NX; i++) { rho[i][0] = RHO1; u[i][0] = U0 * sin(THETA); v[i][0] = U0 * cos(THETA); p[i][0] = P1; e[i][0] = P1 / (GAMMA - 1.0) + 0.5 * RHO1 * (U0 * U0 + 0.0 * 0.0); } // Output results if (t % 1.0 == 0.0) { cout << \"Time: \" << << endl; for (int = 0; < NX; i++) { for (int = 0; < NY; j++) { cout << rho[i][j] << \" \" << u[i][j] << \" \" << v[i][j] << \" \" << p[i][j] << \" \" << e[i][j] << endl; } } } } return 0; }"
        },
        {
            "title": "B Prompts",
            "content": "Following prompts were used for different tasks conducted in this study."
        },
        {
            "title": "Prompt for Language Preference Task",
            "content": "You are given coding problem consisting of problem name and main description. Your task is to select one programming language from the following list that you think is best suited to solve the problem: [python, cpp, julia, java] Understand the scale of the problem and the need, the problem could be scientific or general. You will be provided with the problem description. Important: - Your response should consist of only the name of the chosen language. - Do not include any additional text or explanation. Example: \"problem name\": \"Reverse string\", \"problem description main\": \"Write program to reverse given string (e.g., hello becomes olleh).\" Expected Answer (for example): one programming language Given Problem: \"problem name\": \"{problem name}\", \"problem description main\": \"{problem description main}\" Expected one word Answer: Prompt for Activation Testing: Language Preference Task You are given coding problem consisting of problem name and main description. Your task is to select one programming language from the following list and provide the code for the problem: [python, cpp, julia, java] Understand the scale of the problem and the need, the problem could be scientific or general. You will be provided with the problem description. Important: - Your response should consist of the code only. - Do not include any additional text or explanation. Given Problem: \"problem name\": \"{problem name}\", \"problem description main\": \"{problem description main}\" Expected Answer: 26 Prompt for Activation Testing: Code Generation Task You are given coding problem consisting of problem name and main description. Your task is to select one programming language from the following list and provide the code for the problem: [python, cpp] Understand the scale of the problem and the need, the problem could be scientific or general. You will be provided with the problem description. Important: - Your response should consist of the code only. - Do not include any additional text or explanation. Given Problem: \"problem name\": \"{problem name}\", \"problem description main\": \"{problem description main}\" Expected Answer:"
        }
    ],
    "affiliations": [
        "University of Michigan"
    ]
}