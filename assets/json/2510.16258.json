{
    "paper_title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
    "authors": [
        "Claire McLean",
        "Makenzie Meendering",
        "Tristan Swartz",
        "Orri Gabbay",
        "Alexandra Olsen",
        "Rachel Jacobs",
        "Nicholas Rosen",
        "Philippe de Bree",
        "Tony Garcia",
        "Gadsden Merrill",
        "Jake Sandakly",
        "Julia Buffalini",
        "Neham Jain",
        "Steven Krenn",
        "Moneish Kumar",
        "Dejan Markovic",
        "Evonne Ng",
        "Fabian Prada",
        "Andrew Saba",
        "Siwei Zhang",
        "Vasu Agrawal",
        "Tim Godisart",
        "Alexander Richard",
        "Michael Zollhoefer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 individual hours of 3D motion data from 439 participants collected in a multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features a wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and a separate audio track for each participant."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 5 2 6 1 . 0 1 5 2 : r Embody 3D: Large-scale Multimodal Motion and Behavior Dataset Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer Codec Avatars Lab, Meta Author list not sorted by contribution The Codec Avatars Lab at Meta introduces Embody 3D, multimodal dataset of 500 individual hoursa of 3D motion data from 439 participants collected in multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and separate audio track for each participant. Date: October 21, 2025 Dataset: https://www.meta.com/emerging-tech/codec-avatars/embody-3d aindividual hours refers to number of person-hours in the collection. If we collect one hour of conversation between two participants, it would amount to two individual hours of motion data."
        },
        {
            "title": "1 Introduction",
            "content": "The development of robust motion understanding and synthesis systems critically depends on the availability of high-quality, large-scale motion datasets. However, current datasets face fundamental trade-offs: they either achieve scale at the expense of quality and completeness [19, 10, 3, 1], or provide high-quality data in limited quantities [13, 5, 4, 9, 14, 16, 6, 17, 8, 11], see Table 1. This limitation has become significant bottleneck in human motion and behavior research. Limitations of 2D motion datasets lie in their quality and comprehensiveness. While it is easy to scale 2D video data to thousands of hours, it is challenging to convert such data into high quality 3D motion data. Monocular tracking suffers from depth ambiguity, motion blur, limited resolution around critical areas like the hands, occlusions and partial visibility of the human in the frames, as well as the inability to establish common 3D world space when multiple actors appear in video. While 2D datasets can provide breadth, 1 they lack the precision and spatial consistency required for many applications. 3D motion datasets address the quality concerns through sophisticated multi-view systems that enable accurate 3D tracking. However, scaling collections in such specialized, complex systems is costly, such that existing 3D datasets remain relatively small in volume. Furthermore, even among 3D datasets, completeness is often compromised: many existing datasets fail to provide hand tracking or body shapes. Domain specialization plagues both 2D and 3D datasets. Even large scale video-based datasets typically focus on either conversations [1] or locomotion [3], but rarely both. In smaller scale 3D motion datasets, this task specialization is even more severe, rendering them unfit to model generic human motion and behavior. Additionally, existing datasets frequently lack critical modalities that are strongly linked to human motion, such as speech or text annotations. Embody 3D aims to overcome these limitations and provides single, large-scale dataset with comprehensive coverage of human motion and behavior. Our dataset consists of 500 individual hours of human motion, tracked in multi-view collection system with 80 cameras with 24 mega-pixels each. We provide full body tracking including hands and body shape, together with additional data modalities such as audio and text annotations. Instead of specializing on single task, we provide motion and behavior data on comprehensive set of tasks, including single-person collections such as charades, locomotion, or hand interactions, as well as multi-person collections like conversations, collaborative activities, furniture and object interactions, and co-living scenarios. Overall, the dataset has more than 54 million 3D motion frames and 439 participants. We provide body and hand tracking as well as body shape parameters in SMPL-X [15] format, audio that is separated per participant using beamforming over 640-channel microphone array, fine-grained text annotations created by human annotators, and segment-level annotations through prompting (like play tennis for charades or be angry for conversational settings). dataset size tracking data type modalities individual hours subjects full body shape hands locomotion conversation audio text multiperson 2D motion datasets (monocular tracking from video) SHOW [19] Motion-X [10] MotionMillion [3] Seamless Interaction [1] 27 144 2,000 8, 4 - - 4,284 HUMOTO [13] ZeroEGGS [5] Trinity [4] AIST++ [9] Audio2Photoreal [14] KIT-ML [16] HumanML3D [6] BABEL [17] BEAT [11] Talking with Hands [8] Embody 3D 2 2 4 5 8 11 29 43 76 100 500 1 1 1 30 4 111 - 346 30 439 3D motion datasets Table 1 Overview of existing datasets. 2D motion datasets scale in volume, but suffer from low tracking quality, depth ambiguity, and lack of consistent 3D space. 3D motion datasets are of high tracking quality and grounded in defined 3D world space, but typically are limited in size. All existing datasets lack important aspects of human motion (hands, shape, upper and lower body tracking), types of motion (locomotion vs. conversational, gesture-focused motion), interactions between multiple people, or additional modalities like audio or text annotations. Embody 3D provides the first comprehensive high quality 3D motion dataset that checks all these boxes."
        },
        {
            "title": "2 Dataset Details",
            "content": "Our dataset features seven different subcategories of motion with different properties. 2 Charades contains prompted motion. Participants are instructed to perform specific motion, like jumping or shooting an arrow, and are being recorded for 15 seconds per motion prompt. We provide the tracked motion and the text prompt for the motion. Overall, this subcategory has 88.9h of motion data from total of 221 participants. Hand Interactions. portion of the dataset is collected with special emphasis on hand motion and hand-body interactions. Each participant was instructed to perform several hand and arm motions. Many of these motions have self-contact between both hands or between the hands and body. In total, the dataset has 111.3h of hand-focused motion segments from total of 137 participants. Locomotion. In this category, participants were instructed to move in specific way, such as different styles of jumping, walking, or running. The dataset has total of 21h of locomotion-focused data from 46 participants. Dyadic Conversations. Humans are social animals, and as such communication between humans plays large role in our everyday lives. To enable building authentic conversational virtual humans, we put special emphasis on conversational collections. We collected total of 59.4 individual hours of such conversational motion data with total of 86 participants. The conversations were guided by instructions and prompts. Participants were asked to have conversations about various topics in different emotions like anger, happiness, sadness and others, as well as unguided free-form conversations. Multi-Person Conversations. Moving beyond single person and dyadic data, the dataset offers 125.2 individual hours of multi-person conversations with total of 210 participants. Besides extending the dyadic case to multiple participants, we also add furniture interaction to the data collection and allow participants to use chairs, high-table, or couch as opportunities to sit down during their conversations. Scenarios is subcategory in which multiple participants perform given scenario or collaborative or competitive activity. These scenarios include playing games, assembling furniture, or competing on given tasks. Notably, this section contains object and furniture interactions. Overall, we provide 49.2 individual hours of motion data from 77 participants in this subcategory. Day in the Life is the most challenging motion subcategory in our dataset. Three to four participants interact with each other in small apartment-like setup with different objects and furniture. We instructed participants with different goals that focus around typical co-living, hosting, or group activity scenarios. In total, this category consists of 46.4 individual hours of motion data from total of 77 participants. Table 2 provides summary of amount of data and available assets per subcategory. Figure 1 shows an example frame from each of the subcategories. hours of motion data number of participants audio text annotations Charades Hand Interactions Locomotion Dyadic Conversations Multi-person Conversations Scenarios Day in the Life 88.9 111.3 21.0 59.4 125.2 49.2 46.4 221 137 46 86 210 77 77 () () Table 2 Available data per subcategory of the dataset. () refers to high-level information only, for instance dyadic conversations might have an emotion label but no fine-grained text annotations."
        },
        {
            "title": "3 Collection System and Dataset Acquisition",
            "content": "System. The Embody 3D collection system is multimodal collection system of 6m by 6m by 3.6m high. The capture area covered by cameras is 3.6m by 3.6m. The room is outfitted with custom anechoic acoustic treatment and pipe grid system to mount equipment. The camera array uses high-end global shutter machine vision cameras with 24.47-megapixel resolution (5320 4600) as collect data at 30fps. There are 80 cameras distributed around the covered volume. The 64 body-tracking cameras are equipped with 815mm F4 EF lenses, and the exposure is set to 4 milliseconds. The 16 face-tracking cameras are equipped with 35mm 3 Charades Hand Interactions Dyadic Conversations Figure 1 Example frames from each dataset subcategory. For single-person categories like Charades, Hand Interactions, and Locomotion, we collect multiple participants at the same time for better throughput in terms of individual participant hours. For interactive subcategories like conversations or scenarios, on the contrary, all participants in the scene interact with each other. Multi-person Conversations Day in the Life Locomotion Scenarios Figure 2 End to end processing pipeline for generating multi-person poses and audio from raw collected data. F1.4 EF lenses, and the exposure is set to 2 milliseconds. We arranged 14 LED panels, equally distributed throughout the collection system, achieving an average illuminance of approximately 650 lux throughout the volume, similar to bright indoor room. The microphone system is made from 5 custom, in-house-designed MEMS microphone arrays. Each array consists of 128 MEMS microphone elements arranged in spherical array, effectively recording 10th-order ambisonics. The five 128-mic MEMS arrays combine to create larger array, totaling 640 audio channels. Data collection. All data collection sessions in this dataset have been supervised by research assistants who ensure participants are properly prepared and briefed for the tasks they are given during the collection. Prior to the collection, each participant has been informed about the use of their data for research purposes and has signed consent form. All participants complete calibration session for body shape estimation. Research assistants then guide the participants through their sessions, give instructions, and provide feedback. Additionally, each recording is monitored in real time by research assistant to flag quality issues. Text Annotations. We provide detailed, human-generated text annotations for all segments from the Scenarios and Day-in-the-Life subcategories. These annotations include scene level information that describe the scene on higher level, as well as detailed pose and motion annotations for each person in the scene. We additionally asked annotators to assign labels to each participant that describe their emotional state based on their facial expression, their pose, and their speech."
        },
        {
            "title": "4 Data Processing",
            "content": "In the following, we describe the data processing pipeline from raw collected data to tracked 3D motion, body shapes, and speech-separated audio channels. schematic overview of the process is illustrated in Figure 2."
        },
        {
            "title": "4.1 Synchronization and Calibration",
            "content": "Multi-Camera and Audio Synchronization. During recording session, timestamps for each camera frame are stored alongside the raw image data. While all cameras are co-triggered by the same clock and trigger source, frames still need to be synchronized in post to handle dropped frames. The timestamps are combined across all the cameras and clustered to generate global frame list. Any cameras whose timestamps are above certain threshold from the median timestamp for each cluster are filtered out. Additionally, any frames where too few cameras are present are also filtered out. This leaves global frame mapping and final list of cameras that are in sync with one another. An encoding of the timestamp is embedded into the audio files, allowing us to synchronize the audio stream to the camera frames. Each collection is recorded in series of segments. For each segment, we take the start and end timestamps generated by the camera synchronization stage and synchronize to the audio samples. Frame drops in the audio stream are replaced with zeros. Multi-Camera Geometric Calibration. For multi-camera calibration, we developed custom fiducial tracking board mounted on wheeled cart. The operator moves the fiducial tracking board around the room to build an extrinsic connection graph. We also built smaller fiducial tracking rig for camera intrinsic calibration. On average, our p50 re-projection error is under 0.2px, with p99 error about 0.8px on average. We mounted custom fiducial tracking system onto the microphone array systems to align the microphone coordinate system with the camera coordinate system. Finally, floor fiducial targets were added to estimate the floor plane. Microphone positions and floor tags are detected and triangulated using RANSAC and PnP pose estimation. Finally, plane is fit to the detected floor points, providing floor-centric world coordinate system that is consistent among all collection sessions."
        },
        {
            "title": "4.2 Participant Shape Estimation",
            "content": "Calibration Poses Dense Keypoint Detection Body Shape Estimation Figure 3 Each participant performs calibration poses to obtain person specific body shape and reference face images. During the collections, we ask participants to perform series of four calibration poses, from which we extract their mean body shape. These calibration poses comprise A, T, C, and T-Rex poses. Each of these poses are performed by single participant at time for few seconds per pose. On these calibration poses, we run dense keypoint model and optimize shape coefficients of the linear 3D human shape model. See Figure 3 for reference example."
        },
        {
            "title": "4.3 Multi-Person Pose Estimation",
            "content": "Multi-Person Keypoint Detection. In each image from each synchronized camera, we run person bounding box detector. We sort by detection confidence and keep the top bounding boxes, where is the number of participants for given collection session. For each bounding box instance, we run the Sapiens-1B keypoint pose detector model [7], providing 308 keypoints for face and body. Example outputs can be seen in Figure 4a. Keypoint Matching. Given the set of multi-person keypoints, the matching problem consists of finding the detections associated with each participant. We solve the matching problem following bottom-up approach. First we create per-frame spatial clusters (one for each participant) by lifting 2D detections into ray bundles and grouping based on ray-to-ray distance. Then, we propagate clusters across consecutive frames by comparing similarity of their respective 2D detections. This give us spatio-temporal clusters of 2D keypoints. We match 5 (a) Keypoint Detection (b) Keypoint Matching Figure 4 We generate 2d keypoint detections within the bounding boxes of multi-person detector (left). Keypoints are then matched to participants using geometric and appearance cues (right). these spatio-temporal clusters with participants using an off-the-shelf face embedding model [18]. Within each cluster, we sample five face crops from images with the highest detection confidence. Then, we compute cosine similarity between the embeddings of reference face image from each participant and the embeddings of cluster samples. We match identities by applying the Hungarian algorithm to the similarity matrix of face embeddings. See Figure 4b for an example of keypoint matching. Keypoint Triangulation. We obtain initial estimations of 3D keypoints from RANSAC: for each keypoint we compute triangulations from random camera pairs and keep the one with lowest reprojection error on the remaining cameras. We further refine the keypoint positions by minimizing an energy that incorporates point-to-ray distance, temporal smoothness, and bone length constrains. Pose Tracking. To obtain the skeleton state at each frame we train pose encoder model mapping shape and 3D keypoints to joint rotations. During training we minimize the distance between 3D keypoints and joint positions [12]. Additionally, we regularize joint rotations through pre-trained pose prior model. The pose encoder model consists of Procrustes module that aligns torso joints to 3D keypoints, followed by refinement modules mapping joint-to-keypoint 3D residuals into joint rotation residuals [2]."
        },
        {
            "title": "4.4 Beamforming",
            "content": "We developed beamforming algorithm over the 640 MEMS microphone channels to separate speech from each participant in the scene. Based on human annotations for noise, bleed, and distortions of the beamforming algorithm, we optimized its hyper-parameters to achieve best separation while minimizing distortion. We release the non-separated audio from reference microphone in the center of the collection system as well as separated speech channel for each participant in each segment."
        },
        {
            "title": "4.5 Quality Assurance",
            "content": "Human annotators reviewed the entire dataset to ensure high tracking quality. Each participants tracked motion has been overlaid with the original video data from four different camera views to spot tracking errors, severe jitter, or other inconsistencies. Annotators scored tracking quality and accuracy on Likert scale from one to five and we discarded all segments with an average score of lower than 2.5. We observed annotator ratings above 2.5 to be free of significant errors, and rating penalties mostly being rooted in minor misalignments in subtle details."
        },
        {
            "title": "References",
            "content": "[1] Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony DAvirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Zhiyuan Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, and Michael Zollhoefer. Seamless interaction: Dyadic audiovisual motion modeling and large-scale dataset. 2025. https://ai.meta.com/research/publications/ seamless-interaction-dyadic-audiovisual-motion-modeling-and-large-scale-dataset/. [2] João Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Jitendra Malik. Human pose estimation with iterative error feedback. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 47334742, 2015. https://api.semanticscholar.org/CorpusID:10111903. [3] Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, and Jingbo Wang. Go to zero: Towards zero-shot motion generation with million-scale data. arXiv preprint arXiv:2507.07095, 2025. [4] Ylva Ferstl and Rachel McDonnell. Iva: Investigating the use of recurrent motion modelling for speech gesture generation. In IVA 18 Proceedings of the 18th International Conference on Intelligent Virtual Agents, Nov 2018. https://trinityspeechgesture.scss.tcd.ie. [5] Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus Troje, and Marc-André Carbonneau. Zeroeggs: Zero-shot example-based gesture generation from speech. In Computer Graphics Forum, volume 42, pages 206216. Wiley Online Library, 2023. [6] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161, 2022. [7] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision ECCV 2024, pages 206228, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-73235-5. [8] Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha Srinivasa, and Yaser Sheikh. Talking with hands 16.2 m: large-scale dataset of synchronized body-finger motion and audio for conversational motion analysis and synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 763772, 2019. [9] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1340113412, 2021. [10] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280, 2023. [11] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Beat: large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis. In European conference on computer vision, pages 612630. Springer, 2022. [12] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, October 2015. [13] Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya, Qixing Huang, and Yi Zhou. Humoto: 4d dataset of mocap human object interactions. arXiv preprint arXiv:2504.10414, 2025. 7 [14] Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. From audio to photoreal embodiment: Synthesizing humans in conversations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10011010, 2024. [15] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. [16] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4): 236252, 2016. [17] Abhinanda Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael Black. Babel: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 722731, 2021. [18] Yujiang Wang and Jie Shen. Intelligent behaviour understanding group. https://github.com/ibug-group/face_ embedding. [19] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In CVPR, 2023."
        }
    ],
    "affiliations": [
        "Codec Avatars Lab, Meta"
    ]
}