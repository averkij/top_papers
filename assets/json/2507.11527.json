{
    "paper_title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering",
    "authors": [
        "Yinsheng Li",
        "Zhen Dong",
        "Yi Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 2 5 1 1 . 7 0 5 2 : r DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering Yinsheng Li Department of Civil Engineering McGill University yinsheng.li@mail.mcgill.ca Zhen Dong UC Santa Barbara and NVIDIA zhendong@berkeley.edu Yi Shao Department of Civil Engineering McGill University yi.shao2@mcgill.ca"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at Github-DrafterBench, with the test set hosted at Huggingface."
        },
        {
            "title": "Introduction",
            "content": "Recently, Large Language Models(LLMs) have been demonstrated with remarkable capabilities in planning (Wu et al., 2023a), problem-solving (Kim et al., 2024), tool calling (Kim et al., 2023), programming (Li et al., 2022), etc. There is growing trend for the integration of general LLMs in real scenarios, where one of the most promising fields is automating tasks by LLM agents in the industry (Ahn et al., 2022; Song et al., 2023). Such automation solutions are urgently needed in Civil Engineering, as there are bunches of monotonous, low-tech, and high-labor-intensity tasks from the construction stage (Liu et al., 2019) to the design stage (EvolveLab, 2025). They benefit users by helping them focus on more complex and skill-intensive work and create more value in the same work time. Systematic evaluation and comprehensive analysis from the perspective of industrial applications are critical to gain deeper understanding of the capabilities of models and to identify targeted improvements. However, few benchmarks put them in the shoes of real-world industrial tasks, especially those of Civil Engineering. Preprint. Under review. Figure 1: The workflow and possible results of simple drawing revision task in civil engineering. (a) The whole process involves series of operations, starting from opening the file, locating the annotation, selecting the expected lines, executing deletion, and ending with saving the changes. (b)(c)(d) illustrate the final outcomes of different WRONG operations. The innateness of industrial tasks brings unique challenges for both AI agents and benchmarks compared to those developed for general tasks. To illustrate them, we take the drawing revision as an example, which is one of the most labor-intensive and low-tech tasks that needs automation in Civil Engineering (according to our interviews with more than ten construction companies in North America). First, industrial tasks require skilled worker to provide complete solution by integrating available tools, prior knowledge, and implicit policies rather than simply calling functions following instructions. Figure 1a illustrates the complete process of performing simple revision task. It is worth noting that after deleting the lines as instructed, the changes should be saved as new file named according to the company or the users policies. Usually, the prior knowledge and policies are common sense for skilled human practitioner and are not mentioned in the instructions. But it is challenge for LLMs who lack them to know them and act on them like skilled worker. Second, high robustness is crucial in industrial tasks, which means that an exact drawing is expected even if an instruction is expressed in varying language styles and expressions from different individuals. Third, it is essential to ensure the accuracy of every detail in the workflow. task fails even if simple operation is omitted (forget to save changes, Figure 1b), parameter value is in error (delete all lines rather than only boxed lines, Figure 1c), or an unexpected parameter is specified (an extra point appears, Figure 1d). Fourth, it is difficult to assess the quality of the LLMs performance directly from the revised drawings. This is because not all steps in solution make any visible changes, resulting in what can be seen is not what has been done. It is possible that an unclean solution with invisible extra operations outputs drawing that is the same as the ground truth. Existing benchmarks actively explore the intelligence limit of LLMs in various scenarios (Srivastava et al., 2022; Wu et al., 2023b; Zhuo et al., 2024; Xue et al., 2024), but they fall short of automating industrial tasks. They assess the capabilities of LLMs with the tasks of getting answers by following 2 instructions straightforwardly and paying less attention to implicit policies. Besides, the interactive style of multi-turn and multi-round is trend to evaluate the serviceability of agent assistants in the real world (Ma et al., 2024; Wang et al., 2023). However, agents in these styles are sometimes found to be unstable and overly creative, which conflicts with the high-robustness requirement in industrial tasks (Kim et al., 2023; Xu et al., 2023; Mialon et al., 2023). Meanwhile, they usually require an observable environment or human feedback to promote the mandate process. However, for autonomous tasks in industry, building an observable environment may not be easy, and it is desirable to minimize human involvement to maximize their productivity. In addition, many benchmarks shed light on the completion of the final result, obscuring the comprehensive analysis of where failure stems from (Yin et al., 2024a). The latter is hugely helpful for having deeper and clearer insight into the capabilities of models. This is especially true given that in industry tasks, ground truth results can be obtained both through ground truth paths in various forms and contaminated paths in accidents. In this work, we introduce DrafterBench, an open-source automatic toolkit, to evaluate AI agents in automating representation task of civil engineering, drawing revisions. By analyzing real drawing revision documents, 12 types of tasks are summarized across four types of operations (adding, content modification, mapping, and format updating) and three objects (text, table, and vector). Forty-six tools for revising drawings in PDF files are customized and provided with the necessary prior knowledge and implicit policy in the system information. As result, 1920 tasks in dynamic instruction quality were prepared and verified by humans to simulate the real scenario. DrafterBench assesses four essential capabilities of models: structured data understanding, function execution, instruction following, and critical reasoning. It offers systematic analysis of task accuracy and error statistics on six subtasks. To accurately assess the models, dual functions are designed to record ground operation paths, which may be expressed in various coding styles by different models in different responses. The operation paths, instead of the output drawings, were compared with the ground truth paths to grade the performance of the models and analyze detailed errors. Our contributions are as follows: We introduce DrafterBench, which provides comprehensive analysis of the strengths and limitations of LLM agents to automate monotonous and low-tech tasks for industry scenarios, especially civil engineering. fully automated evaluation toolkit is released, providing stable and accurate evaluation on models, resisting stochastic variation in models response style and manner, avoiding the instability that may be encountered in interactive tasks. We conducted experiments for different mainstream LLMs. The results demonstrate that DrafterBench can give clear view of their strengths and deficiencies. We hope that the proposed benchmark can provide insight for the future development of LLM agents, especially for integrating LLMs in engineering applications."
        },
        {
            "title": "2 Related Works",
            "content": "LLM-based Agent Significant advances in LLM agents have been made with the development of the reasoning enhancement framework for LLM (Wei et al., 2022; Yao et al., 2024, 2022; Besta et al., 2023). By accomplishing relatively simple subtasks one by one, agents can solve complex tasks (Masterman et al., 2024; Srinivasan et al., 2023). ReAct is the most popular agent construction approach, which commands the agent to observe the environment to think deeply, complete simple operation each turn, and gradually move toward the final answer (Yao et al., 2022). Based on the structure, agents can be categorized as single-agent or multi-agent. single-agent accomplishes observing, planning, selecting, executing, and other actions by self-asking and answering (Paranjape et al., 2023), while multi-agent system has independent modules responsible for each of the above four or more aspects and accomplishes the task through inter-module communication (Shi et al., 2024). Despite the ReAct-style agents power, in industrial application scenarios, there is likely no environment for interaction, and agents that can accomplish tasks in single turn are preferred. Function-calling Benchmarks There are number of benchmarks that have been built to evaluate the performance of agents, especially function-calling agents (Xu et al., 2023; Qin et al., 2023; Li et al., 2023), on basic capabilities such as understanding, reasoning, planning, problem-solving, and their behavior in real tasks (Song et al., 2023; Yin et al., 2024b). The benchmarks have included 3 Figure 2: Expected workflow of an automation agent for drawing revision: The engineers or drafters leave their modification instructions and mark the related locations in the target files. Then, preprocessing will extract instructions and prepare files containing only the position marker for the agent. The LLM agent receives the instructions and files and automatically takes action to execute the modification. Finally, the workflow ends with human inspection of the updated files. variety of tasks such as math, games, science quizzes, code, etc (Ma et al., 2024; Zhuo et al., 2024). However, few benchmarks have been built on the real engineering tasks that need to be automated. Meanwhile, recent benchmarks have focused chiefly on multi-step, multi-turn problem solving (Yan et al., 2024; Ma et al., 2024) and not so much on one-turn, long-sequence high-level function callings, which are much more efficient in some tasks in the industry."
        },
        {
            "title": "3 The DrafterBench",
            "content": "DrafterBench is designed to evaluate LLM models when serving as an automation agent to save engineers or drafters effort on low-tech monotonous work in industrial scenarios like drawing revision. Thus, tasks in DrafterBench are limited to the part of the work that can be completed by calling functions according to instructions and implicit policies. From practical application perspective, this assumed automation agent has workflow shown in Figure 2. This benchmark skips the preprocessing process and simulates the situation in which the agent receives the extracted instruction and prepared files and starts to take action. The action is calling the tools/functions provided by coding (which is more flexible and stable than other function calling methods (Wang et al., 2024)) to implement the revision instruction in one turn without human participation. 3.1 Task Collection Over 100+ real-world drawing revision files (provided by design firms and construction companies) were collected and comprehensively analyzed. The target tasks were filtered out and categorized into three elements: text, table and vector entity, and four operations: adding, content modification, mapping, and format updating, in total 12 types. Detailed descriptions of them can be found in Appendix B. The difficulty of each task varies greatly and is generally controlled by six parameters in four dimensions: Difficulty in understanding structured data Since these documents come from different companies and are handled by different people, the language style or expression of the instructions varies greatly, leading to dynamic difficulty in understanding embedded data. They can be either extremely clear and concise sentences, which are named structured language in this work, or verbose paragraphs providing more and sometimes abundant information, namely unstructured language. Difficulty in function execution There are different function calling pipelines for each type of task. But their complexity varies greatly. Figure 3ab illustrates the pipelines for adding text and adding vectors, respectively, as examples. It can contain only straightforward steps in relatively easier tasks, like adding text, or graph structure with richer nodes and edges for tasks like adding vectors. Difficulty of instruction following The number of objects and revisions involved makes the length and complexity of an instruction fluctuate greatly. The impact of multiple objects and multiple revision operations on the implementation of an instruction is shown in Figure 3c. 4 Figure 3: An illustration of function calling pipelines for different situations: (a) standard pipeline for adding texts. (b) standard pipeline for adding vectors. (c) Execution details for an instruction with multiple objects and long operation chains in one turn. Difficulty of critical reasoning Due to the stochastic nature of human manual work, the quality of instructions is not always maintained at constant standard. There are two types of errors that frequently appear. Sometimes, detailed values are not clearly specified, like Move the table to the left little, that is, vaguely defining the value. It also happens that some necessary information is not specified, such as add the missing dimension without specifying the content or value to be added, which is an incomplete(error) instruction. The fully automated workflow expects the agent to respond to the above errors in two ways. The agent should think carefully and self-correct the ambiguous details with reasonable values. It is better to record incomplete instructions in the logging files using specified tools to alert the user rather than implement them anyway and make things worse. The six parameters are summarized in Table 1. When constructing the benchmark, all filtered instructions were first grouped according to the six parameters. Tasks were sampled directly for groups with adequate tasks as candidate tasks or supplied with synthesized instructions for groups with fewer tasks. Then, the candidate tasks were verified by humans to ensure the solvability of the instructions and the uniformity of the value distribution. As result, 1920 tasks were obtained with five tasks for each combination of difficulty parameters, as shown in Table 1. Table 1: Task Specifications Items Parameter details Number 12 types of tasks Single object/Multiple objects Single operation/Multiple operations Task category Object per instruction Operation per instruction Information completeness Complete/Incomplete Value specifying style Language style Tasks per situation Total Precisely/Vaguely Structured/Unstructured 12 2 2 2 2 2 5 3.2 Tool Preparation Drawing revision tools/functions The tools/functions provided for the agent were specifically tailored for PDF drawing revision in this work, total of 46 tools. The well-known PDF editing libraries, including PyMuPDF (Artifex, 2025) and Reportlab (ReportLab, 2025), and computer vision libraries, such as cv2 (OpenCV-Team, 2025) and pyresseract (Hoffstaetter, 2025), are introduced when designing them. 5 Dual tools/functions As it is difficult to conduct an accurate evaluation of the performance of models directly from the output drawings, dual tools/functions are introduced. They have function names, calling methods, arguments, and output types the same as the corresponding original functions. When the agent executes the generated code, the dual functions are executed instead of the original functions. They do not modify the drawing but record the argument details and the ground operations path, following the rules, such as repeated operations being recorded as one. They allow and record common coding errors that are not allowed in the original functions, such as parameter type errors. These records will be compared with the ground truth for analysis at the operational level. It can clearly estimate the operation quality, eliminate the noise introduced by coding styles, and easily distinguish unclean paths even though they have output drawings that are the same as the ground truth paths. In summary, the dual tools facilitate an accurate and comprehensive assessment of the mode while ensuring the uniqueness of the ground truth and considering the flexibility of the coding. 3.3 Default Prompt After several experiments and adjustments, we have obtained prompt framework that can effectively solve the tasks of drawing revision by coding with function calling, following the instructions and implicit policies in this benchmark. The default prompts for each task can be found in Appendix D. We also release the editing privileges of the prompts to encourage users to develop their own prompts to achieve higher performance."
        },
        {
            "title": "4 Evaluation Metric",
            "content": "The evaluation metric was developed to evaluate both the task accuracy and the error statistics of the agents performance. The operation paths recorded by dual functions are compared with the ground truth paths. For each task, its score is graded by three parts: code executability, target completeness (completeness of the instruction). The evaluation has two levels: Level 1, check whether generated code can be run with dual functions. The dual functions are compatible with some common coding errors. Thus, there must be significant errors in an unrunnable code string and it will receive score of 0. Level 2, when code string can be run with dual functions. Each function execution is monitored for executability and will be recognized as executable if and only if (a) all required arguments are specified and (b) all specified arguments are in the correct data format. The executability score of the response is graded as 0 if any function is non-executable; otherwise, it is 30. The target completeness is assessed in six subtasks described below. Argument defining: define arguments according to the details in the instruction. Variables transferring: transfer intermediate variables between functions. Function calling: call functions or tools. (Single) Tool selection: select suitable tool from the tool library. Multi-tool selection: select series of tools in sequence from the tool library. Plan execution: The critical details of sequence of functions corresponding to an objects revision operations. Figure 4 illustrates the six subtasks. The scores for the first five subtasks were calculated using Equation 1. P + 70 (cid:88) (pp,pg) IoU (pp, pg) 70 6 mean (si) (100 min (si)) (1) (2) (3) Figure 4: Subtasks when calling function by coding to revise drawing. 6 Table 2: Average task scores for different task sets and results of the comprehensive evaluations. Bold and underlined numbers are the first and second-highest performances in each category. OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct Language style Details ambiguity Structured language Unstructured language Precise details Vague details Instruction completeness Complete instruction Objects number Maximum operation chain length Incomplete instruction Single object Multiple objects Single operation Multiple operations Average tasks score Comprehensive score 81. 82.26 89.82 74.02 81.43 82.41 83. 80.60 82.06 81.63 81.92 79.90 75. 73.84 79.46 69.52 79.63 69.36 73. 75.11 75.46 72.56 74.49 71.76 74. 78.20 81.15 71.39 82.00 70.54 75. 77.19 76.15 76.52 76.27 73.79 76. 75.84 79.25 72.86 82.74 69.37 75. 76.83 77.21 73.74 76.05 73.09 75. 74.23 76.18 73.36 84.49 65.06 75. 74.47 76.26 71.79 74.77 72.05 70. 70.62 73.50 67.84 80.96 60.38 69. 71.77 72.52 66.97 70.67 67.55 TP refers to the number of subtasks in response that are the same as the ground truth, and vice versa for FN. FP is the number of subtasks that are not in the ground truth, but are performed in the response. The drawing revision task is result-sensitive task. If an agent performs unexpected operations, they are likely to introduce new errors and make things worse. Therefore, FP is introduced as penalty when scoring the response. The intersection over Union (IoU) was employed to score the plan execution. is sequence of revision operations performed on an object. pg refers to the ground truth, and pp refers to the response of the agent. Equation 2 was employed to score plan execution. The total score for all subtasks is 70. The sum of executability and target completeness score is the total score of the agents response to an instruction. The comprehensive score of the agent is calculated by Equation 3, where si is the average score for the i-th type of task. The second part is introduced to better consider the weak points of the agent. If we simply take the average score of the 12 tasks as the comprehensive score, it is impossible to distinguish between the following two situations: 1) Agents scored high in some sets and low in others. 2) Agents scored evenly on each type of task. These two situations may have similar comprehensive scores, but the latter is preferred in the industry scenario."
        },
        {
            "title": "5 Experimental Results",
            "content": "We test various state-of-the-art commercial and open-source language models for agents through their APIs: OpenAI GPT API (o1, gpt-4o-2024-08-06) (OpenAI, 2024), Anthropic Claude API (claude-35-sonnet-20241022) (Anthrop, 2024), Deepseek API (deepseek-chat) (Deepseek, 2024), Deepinfra API (Qwen/Qwen2.5-72B-Instruct, Meta-Llama-3-70B-Instruct) (Qwen-Team, 2024; Meta, 2024). 5.1 Task Accuracy and Robustness Table 2 illustrates the main results. First, OpenAI o1 leads the performance in almost all task sets with obvious dominance compared to other models. There are two models in the second tier, Claude3.5-sonnet and Deepseek-v3-685B, which are neck-and-neck for the second-highest performance. Second, even the widely recognized powerful OpenAI o1 fails to earn around 20 points for simple, monotonous, and low-tech industrial tasks, highlighting the necessity of evaluation centered on industrial applications. Third, the performance of all models fluctuates in tasks with different difficulty, especially for tasks with some sensitive parameters. 7 Figure 5: An illustration of performance degradation in terms of six parameters (left) and detailed performance of each task type (right). The values in the graph are the percentage of the models performance in the weaker task set versus its performance in the stronger task set. To dissect the capabilities of LLMs to automate drawing revision tasks and to gain insight into their robustness, the agents performance in tasks of different complexity is shown in Figure 5. From the robustness of structured data understanding in different language styles, almost all models show great stability with an average degradation of 1%, except for the Claude3.5-sonnet (5%). For the robustness of function execution, the performance degradation of each type of task versus the best type of task for each model is around 9%. No significant gap was observed between the models. The relatively most stable model, OpenAI o1, decreases by 7%, and the least stable model, Claude3.5-sonnet, decreases by 11%. There is an interesting observation in Figure 5 that the best performance occurs in tasks having the least number of arguments (deleting vectors o1, adding vectors Deepseek-v3-685B, Qwen2.5-72B-Instruct) or having the most simple pipeline (adding text ChatGPT-4o-2024-08-06, Claude3.5-sonnet, updating vector format - Llama-3-70B-Instruct). From the robustness of instruction following, the good news is that all models exhibit good stability with performance degradation of less than 4% when dealing with tasks involving multiple objects compared to only involving single object. Meanwhile, when dealing with tasks having multiple operations, OpenAI o1 and Claude3.5-sonnet exhibit excellent stability with degradation of less than 1%, while that of others is 6% on average. From the robustness of critical reasoning, all models somehow struggle to complete the tasks. When details are vaguely specified, the degradation of all models is great, 5% for Qwen2.5-72B-Instruct and 12% for others. When there are errors in the instructions, performance degradation reaches 18% for all models except OpenAI o1, which shows great resilience, in contrast. 5.2 Error Analysis To have clear understanding of where the errors stem from and to have deeper understanding of the limitations of the models, we calculated the accuracy of the six subtasks mentioned in Section 4, as shown in Figure 6. Similar to the comprehensive score, OpenAI o1 leads the way in accuracy in almost all subtasks, followed by Claude3.5-sonnet and Deepseek-v3-685B. To benefit the readers reading experience, only comprehensive analysis is provided here; more detailed discussion, such as how the performance of six subtasks varies at task sets in different difficulties, can be found in Appendix C. In Figure 6, there is an obvious gap between the accuracy of plan execution and the other five subtasks. It is interesting that the difference is constantly around 20% for all models, though the value of their subtask accuracy varies greatly. Figure 6: Subtasks accuracy(%) of tested models. 8 OpenAI o1, one of the most advanced models in the world, shows the same gap, even though it exceeds other models in almost all other subtasks. The plan execution was scored by checking whether all critical details of sequence of tools to modify an object were consistent with the ground truth. Therefore, it can be deduced that the models are trying their best to complete each subtask with high accuracy, but it is difficult for them to pay enough attention to every detail. The models are able to understand significant portion of the task objectives, but still lack the ability to avoid all errors throughout the modification process of an object. 5.3 Limitations and Future Directions To fulfill the requirements of automating monotonous, low-tech, and high-labor-intensity tasks in the industry, like drawing revision tasks, there are still several challenges that need to be considered in the future development of LLMs. Interactive style In our experiments, when LLMs encounter an incomplete instruction, they prefer to ask the user for more information or to fill the missing information with placeholder to give the output rather than record it according to the polices in the system information. This contributes to the low accuracy in the incomplete instruction task set. It also corroborates the fact that current LLMs are paranoid about instantaneous interaction with humans. Instantaneous interaction is indeed an important scenario for the application of LLMs. But in real engineering applications, especially when automating industrial tasks, there are also scenarios that prefer no human involvement, such as the drawing revision task considered in this benchmark. Although LLMs are evolving toward super-intelligence and seamless interaction with humans, it is also important to maintain compatibility with other modes of interaction, such as delayed, indirect interaction through preprocessing and postprocessing files between LLMs and humans. Understanding of details LLMs are in trouble when dealing with instructions that vaguely define key arguments. Common errors include filling in placeholders or directly filling in the description words in the instructions for vaguely defined arguments. Therefore, it is necessary to strengthen LLMs to speculate, reason, and understand the users exact intention behind the instruction details rather than literally executing them and giving response for reference only. Implementation of new policies LLMs perform quite stubbornly when they meet instructions that are in some kind of conflict with their intrinsic policy. For example, during the testing of this benchmark, the system information in the prompt repeatedly emphasizes the users expected response when encountering detail-ambiguous instructions and error instructions. They can obey the police in some tasks, but also ignore the new policies in significant number of subtasks and give the unexpected responses mentioned above, in accordance with their habits or intrinsic policy. Having and remaining loyal to intrinsic policies is necessary competency for LLMs, especially when it comes to cybersecurity or illegal risk. However, in some other less serious and stereotypical aspects, there is need to improve LLMs tolerance and implementation of new policies, especially considering the diverse demands and polices of industrial application scenarios. More benchmarks are also needed to illustrate more requirements for integrating LLMs in industrial scenarios."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce DrafterBench, an open-source toolkit designed to offer systematic and comprehensive evaluation of the capabilities of LLMs to automate monotonous, low-tech, and high-labor-intensity tasks from industry. We emphasize the challenges and rigorous requirements for AI agents from an industrial perspective. DrafterBench is under the context of representation civil engineering task, drawing revision, total of 1920 drawing revision tasks are collected from real documents. The complexity of tasks is controlled by six parameters to investigate four essential capabilities of LLMs, namely structured data understanding, function execution, instruction following, and critical reasoning. The automatic evaluation toolkit assesses the performance of LLMs with task accuracy and error statistics. We conducted experiments on mainstream LLMs to reveal their strengths and deficiencies in automating industry tasks. From the result, we posit that DrafterBench is useful metric that can provide valuable evaluation and useful insight for the future development of LLMs from the perspective of industry, especially Civil Engineering."
        },
        {
            "title": "References",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, and Karol Hausman. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Anthrop. Claude3.5-sonnet, 2024. URL https://www.llama.com/. Artifex. Pymupdf, 2025. URL https://pypi.org/project/PyMuPDF/. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, and Piotr Nyczyk. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2023. ISBN 2374-3468. Deepseek. Deepseek v3, 2024. URL https://www.deepseek.com/. EvolveLab. Glyph copilot, 2025. URL https://www.evolvelab.io/glyph. S. Hoffstaetter. pytesseract, 2025. URL https://pypi.org/project/pytesseract/. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36, 2024. Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael Mahoney, Kurt Keutzer, and Amir Gholami. An llm compiler for parallel function calling. arXiv preprint arXiv:2312.04511, 2023. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, and Agustin Dal Lago. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. ISSN 0036-8075. Jiepeng Liu, Liu, Feng, Wu, and Lan. Automated clash resolution of rebar design in rc joints using multi-agent reinforcement learning and bim. In ISARC. Proceedings of the International Symposium on Automation and Robotics in Construction, volume 36, pages 921928. IAARC Publications, 2019. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llm agents. ArXiv, abs/2401.13178, 2024. Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: survey. ArXiv, abs/2404.11584, 2024. Meta. Llama3.3, 2024. URL https://www.llama.com/. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. ArXiv, abs/2311.12983, 2023. OpenAI. Openai o1, 2024. learning-to-reason-with-llms/. URL https://openai.com/index/ OpenCV-Team. Opencv, 2025. URL https://pypi.org/project/opencv-python/. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, and Bill Qian. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 10 Qwen-Team. Qwen2.5, 2024. URL https://qwen2.org/qwen2-5/. ReportLab. Reportlab, 2025. URL https://docs.reportlab.com/. Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive agents. arXiv preprint arXiv:2403.03031, 2024. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, and Rong Yao. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624, 2023. Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, and Jian Zhang. Nexusraven: commercially-permissive language model for function calling. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, and Adrià Garriga-Alonso. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. ArXiv, abs/2309.10691, 2023. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. Plan, eliminate, and tracklanguage models are good teachers for embodied agents. arXiv preprint arXiv:2305.02412, 2023a. Yue Wu, Xuan Tang, Tom Mitchell, and Yuanzhi Li. Smartplay: benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023b. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023. Siqiao Xue, Tingting Chen, Fan Zhou, Qingyang Dai, Zhixuan Chu, and Hongyuan Mei. Famma: benchmark for financial domain multilingual multimodal question answering. arXiv preprint arXiv:2410.04526, 2024. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard, 2024. URL https://gorilla.cs. berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, and Aonan Zhang. Mmau: holistic benchmark of agent capabilities across diverse domains. arXiv preprint arXiv:2407.18961, 2024a. Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, and Aonan Zhang. Mmau: holistic benchmark of agent capabilities across diverse domains. arXiv preprint arXiv:2407.18961, 2024b. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, and Indraneil Paul. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A Limitation and Computing Resources",
            "content": "A.1 Limitation The tasks involved in this work are all in English. Multilingual input will be included in future work to consider the ability of LLMs to automate industrial tasks in non-English environments. At the same time, this work covers only drawing revision tasks in the field of civil engineering, and more types of tasks are yet to be performed in future work. A.2 Computing Resources This benchmark calls LLMs via APIs and has no special requirements for hardware devices. In this experiment, the benchmark runs on an i9-13900F CPU for about 30 min to get all the results."
        },
        {
            "title": "B Description of Drawing Revision Tasks",
            "content": "The detailed description of the 12 drawing revision tasks is shown in Table 3. Table 3: 12 types of tasks for the drafter agent. Object element Operation Description Text Table Vector entity Add new texts to the target area. Adding Content modification Delete or replace words in the target text strings. Mapping Format updating Move, rotate or scale the target texts. Modify the format of target texts, including but not limited to text color, font type, font size, and alignment. Add new tables to the target area. Adding Content modification Delete, clear, or replace cells in the target tables. Mapping Format updating Move, rotate or scale the target tables. Modify the format of target tables, including but not limited to font type, font size, boundary width, and alignment. Adding Content modification Delete target strokes, including rebars, columns, Add standard strokes to the target position. Mapping Format updating and lines. Move, rotate or scale target strokes. Modify the format of target strokes, including but not limited to line color, line thickness, and line type."
        },
        {
            "title": "C Performance of Agents for Tasks in Different Difficulty",
            "content": "C.1 Structured Data Understanding Table 4: Accuracy of subtasks in different language styles in percentage. STR means structured language, and U-STR means unstructured language. OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct STR U-STR STR U-STR STR U-STR STR U-STR STR U-STR STR U-STR Argument defining Variable transferring Function calling (Single) Tool selection Multi-tool selection Plan execution 59.84 71.03 73.20 68.31 70.15 49.94 63.31 73.13 76.85 72.28 73.86 49. 58.86 70.95 71.63 63.28 75.38 48.09 60.81 70.62 72.24 61.57 77.40 45.09 62.81 72.67 73.78 67.30 76.22 47.11 63.94 66.70 71.56 69.87 58.68 42.45 59.72 72.92 72.28 62.58 75.89 45.29 58.51 67.64 71.27 59.31 74.20 43. 57.05 68.44 70.92 57.44 77.91 41.64 53.09 62.68 69.08 53.72 75.72 38.27 54.67 63.36 68.58 52.72 77.07 39.57 46.45 49.97 57.43 52.72 37.10 31.75 13 Table 4 lists agents performance for instructions in structured and unstructured language. The stable scores of all LLMs show their excellent adaptability to different language styles. The difference in both comprehensive scores and subtasks between the two language styles is around 5%. C.2 Instruction Following Objects per instruction The number of objects per instruction determines whether the pipeline will be implemented in parallel. As shown in Table 5, the good news is that LLMs perform stably for both single object per instruction and multiple objects per instruction. An interesting observation is that models except OpenAI o1 perform better for multi-object instruction, which is actually more difficult. However, the increase in accuracy in all subtasks is not due to better performance but to lower requirement for Recording incomplete instructions. For an incomplete instruction with single object, the only ground truth is to call the Recording function; it will be 100% false if it is not called. In contrast, for multi-object instruction, Recording should only be called for objects that lack necessary information, and the standard pipeline should be executed normally for others. Therefore, LLMs can get higher scores even though none of the Recording is called in the multiple objects instructions. Table 5: Accuracy of subtasks for instructions with single object (SIN) or multiple objects (MULT). OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct SIN MULT SIN MULT SIN MULT SIN MULT SIN MULT SIN MULT Argument defining Variable transfering Function calling (Single) Tool selection Multi-tool selection Plan execution 62.65 72.45 76.64 74.73 67.95 41.13 60.93 71.90 74.13 67.00 73.00 54.80 53.77 67.93 68.22 62.10 67.52 38.17 63.40 72.17 73.98 62.66 78.57 51.64 60.72 70.73 73.95 70.48 68.38 37.41 64.93 69.17 71.97 67.18 67.23 48. 50.70 64.44 66.04 58.62 59.40 34.23 64.05 73.12 74.94 62.66 79.31 50.34 46.25 61.34 65.13 54.84 64.10 31.58 60.24 67.61 72.68 56.13 79.94 45.17 34.89 37.68 47.08 49.94 0.00 24.97 59.76 65.90 71.78 54.78 79.20 42. Operations per object The number of operations per object changes the execution loop of substeps in the standard pipeline. The LLMs demonstrate some adaptability to varying operations. The difference between instructions that perform only one operation on an object and those that perform multiple operations is about 5% for all subtasks. It is noted that some local drops also occur in multi-tool selection, as shown in Table 6. Table 6: Accuracy of subtasks for instructions with single operation per object(SIN) or multioperations per object(MULT). OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct SIN MULT SIN MULT SIN MULT SIN MULT SIN MULT SIN MULT Argument defining Variable transfering Function calling (Single) Tool selection Multi-tool selection Plan execution 64.64 71.20 75.16 70.14 72.71 53.45 59.58 72.74 74.93 70.41 71.57 47. 58.50 72.51 73.07 64.14 76.79 44.51 61.86 68.47 70.22 59.91 75.76 49.42 61.52 67.96 70.66 69.81 59.62 41.93 66.23 72.00 75.71 66.79 73.36 48.82 61.87 67.77 71.85 59.54 77.73 46.37 57.33 72.15 71.73 61.90 73.90 42. 55.29 68.76 71.92 58.51 77.88 38.56 54.63 61.28 67.09 51.30 75.11 41.92 52.49 57.02 63.60 48.39 68.34 39.63 49.29 56.40 62.61 55.67 36.54 32.75 C.3 Critical Reasoning As mentioned in the main body, there are two situations that require LLMs critical reasoning to give correct response, which are incomplete instructions and vague details. For most LLMs, switching from specifying values precisely to vaguely decreases the accuracy of all subtasks  (Table 7)  . The most obvious difficulty introduced is the need to assume arguments in reasonable way. The most common error is simply filling in the description text instead of assuming reasonable value. For Table 7: Performance of agents for instructions specifying values precisely (P)/vaguely (VA). OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct VA Argument defining Variable transfering Function calling (Single) Tool selection Multi-tool selection Plan execution 84.53 90.58 93.15 91.04 90.47 63.14 38.87 55.48 58.17 50.73 55.71 38.39 69.88 79.26 79.46 68.91 86.87 54.90 VA 49.91 63.18 64.94 56.30 67.14 38. 72.12 75.38 76.72 73.47 70.86 51.04 VA 54.72 64.57 68.91 63.98 64.44 38.17 65.94 72.79 75.46 63.26 82.19 49. VA 52.38 68.03 68.35 58.75 69.37 39.06 57.43 65.38 69.99 54.66 78.06 43.79 VA 52.72 65.72 70.01 56.45 75.71 36. 51.16 52.64 58.88 53.11 38.67 38.37 VA 49.96 60.28 66.84 52.35 73.65 32.45 14 Figure 7: An example of incorrectly recoding vague instruction. example, when the instruction asks for general font color, the LLMs will define variable as follows: fontcolor=general color. Besides, OpenAI o1 is the most sensitive model to vague instructions. Considerable decrease exists not only in argument defining, but also in function calling and tool selection. This is because some values vaguely described are wrongly recognized as missing necessary information. Some vague instructions are incorrectly treated as error instructions. Here is an example response generated by OpenAI o1 shown in Figure 7. Consistent with the analysis in the main body, there is very significant decrease in the performance of the models on the tasks set with error instructions in almost all subtasks, except OpenAl o1  (Table 8)  . However, the reason for OpenAI o1s stable comprehensive score is not stable accuracy for all subtasks, but there is significant decrease in plan execution and small increase in the other subtasks. This means that, although OpenAI o1 can apply the correct response pattern to erroneous instructions, it remains or even becomes more challenging for o1 to fully align with the instructions in detail. This results in more imperfect modifications. Table 8: Performance of agents for complete instructions (Com) and incomplete instructions (ERR). OpenAI o1 ChatGPT-4o-2024-08-06 Claude3.5-sonnet Deepseek-v3-685B Qwen2.5-72B-Instruct Llama3-70B-Instruct Com ERR Com ERR Com ERR Com ERR Com ERR Com ERR Argument defining Variable transferring Function calling (Single) Tool selection Multi-tool selection Plan execution 60.04 71.41 72.98 66.69 69.97 59.66 63.86 73.52 78.54 75.59 76.15 35.87 68.66 77.09 78.41 72.27 78.64 60.51 46.64 57.18 60.85 47.95 71.79 30.11 71.43 74.56 76.74 78.23 63.69 56.90 51.33 59.16 65.69 54.41 75.13 29. 72.78 82.40 82.95 78.61 79.15 61.18 38.68 44.13 52.64 34.97 67.69 26.40 75.72 82.62 85.94 80.52 84.42 59.36 24.19 28.75 42.68 18.94 61.28 21.97 74.09 78.91 84.24 78.78 82.91 57.72 15.37 8.64 26.63 14.41 4.87 17."
        },
        {
            "title": "D Default Prompts",
            "content": "All default prompts have four components: task background, standard pipelines(task plan), available functions, and examples: Task background: This section describes the role of LLM and provides prior knowledge and implicit policy. It includes definitions of terminology, terms defined by the tool designer or user, and other necessary information. Standard pipeline: This section is important in improving the agents performance, especially in integrating and utilizing functions. It describes graph containing multi-mode operations to implement the revision of an object, such as branch shift, step skipping, single tool selection, and multi-tool selection from sub-library. Instead of working as plan for particular task, it works for all possible situations. It supports parallel implementation for instructions with multiple objects, or loop execution for an object with long operations. Available functions: Detailed information about the available functions is provided in this section, such as function names, calling statements, application scenarios, and the functions essential arguments, optional arguments, input data types, and output results. It is also possible to provide complementary implicit policies if necessary. 15 Examples: After experimentation, it was found that it is still challenge for LLMs to follow the pipeline and call functions to complete the instruction in zero-shot manner. Thus, one or several examples are recommended to help illustrate how pipelines are implemented. Here are the default prompts: D.1 Adding Text 16 D.2 Revising Text 17 D.3 Mapping Text 19 20 D.4 Updating Text Format 21 D.5 Adding Table 22 23 D.6 Revising Table 24 25 D.7 Mapping Table 26 27 D.8 Updating Table Format 28 D.9 Adding Vectors 30 D.10 Deleting Vectors 31 32 D.11 Mapping Vectors 34 35 D.12 Updating Vector Format 36 37 Here are all the default prompts. We encourage users to develop their own prompts to achieve higher scores."
        }
    ],
    "affiliations": [
        "Department of Civil Engineering McGill University",
        "NVIDIA",
        "UC Santa Barbara"
    ]
}