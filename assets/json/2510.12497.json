{
    "paper_title": "Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance",
    "authors": [
        "Jincheng Zhong",
        "Boyuan Jiang",
        "Xin Tao",
        "Pengfei Wan",
        "Kun Gai",
        "Mingsheng Long"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 9 4 2 1 . 0 1 5 2 : r MITIGATING THE NOISE SHIFT FOR DENOISING GENERATIVE MODELS VIA NOISE AWARENESS GUIDANCE Jincheng Zhong1, Boyuan Jiang2, Xin Tao2, Pengfei Wan2, Kun Gai2, Mingsheng Long1(cid:0) 1School of Software, BNRist, Tsinghua University, China 2Kling Team, Kuaishou Technology, China {zhongjinchengwork, jiangsutx}@gmail.com {jiangboyuan,wanpengfei}@kuaishou.com mingsheng@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify long-overlooked yet pervasive issue in this family of models: misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce classifier-free variant of NAG, which jointly trains noiseconditional and noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Denoising-based generative models, such as diffusion models (Ho et al., 2020; Peebles & Xie, 2023) and flow-based models (Lipman et al., 2023), have demonstrated remarkable scalability and achieved state-of-the-art results across wide range of visual generation tasks, including image synthesis (Ho et al., 2020), video generation (Ho et al., 2022), and cross-modal generation (Saharia et al., 2022; Rombach et al., 2022). The core principle of these models is to progressively recover target sample from pure noise. At each iteration, neural network processes an intermediate state, which consists of both signal and noise mixed in pre-defined proportions, and updates it to the next state according to the network output and pre-defined coefficients. During iterative sampling, the model is repeatedly applied and inevitably accumulates errors from multiple sources, including imperfect network approximation, discretization in numerical integration, and other stochastic factors. Recent studies have primarily focused on the discretization aspect, aiming to accelerate generation by reducing the number of denoising steps (Geng et al., 2025; Song et al., 2023; Lu et al., 2022), or on designing more effective diffusion architectures to increase model capacity (Peebles & Xie, 2023; Ma et al., 2024; Karras et al., 2022). Nevertheless, accumulated errors in such complex system are unavoidable. key manifestation of these errors is that the noise level inherently encoded in intermediate states may deviate from the pre-defined schedule. This misalignment, long overlooked by the community, is both widespread and rooted in the collective effect of diverse error sources. We refer to this phenomenon as noise shift, which often leads to fundamental mismatch between training and inference in denoising networks. In this work, we demonstrate that the noise shift manifests as systematic drift toward larger noise levels t. We conduct an empirical analysis on recent advanced diffusion models (Ma et al., 2024) Work done during internship at Kling Team, Kuaishou Technology 1 Figure 1: Empirical observation of noise shift. Denoising generative models suffer from traininginference misalignment, where the posterior estimation during sampling tends to lean toward larger noise levels. The yellow curves indicate the estimated probability density of the posterior pϕ,t(t ˆx) for sampled intermediate states ˆx, while the orange curves indicate the posterior pϕ,t(t x) for intermediate states stochastically interpolated from training data x0 pdata(x0) on ImageNet. The (a), (b), and (c) show comparisons between posterior estimates obtained at inference and training, for prior noise levels = 0.7, 0.5, and 0.3, respectively. All density functions are estimated via kernel density estimation with 5,000 samples. for ImageNet generation. As illustrated in Figure 1, the noise shift issue is widespread and can be directly observed using an external posterior noise-level estimator gϕ. This observable noise shift δ indicates clear mismatch: the actual noise encoded in intermediate states is not consistent with the pre-defined noise levels, exhibiting systematic tendency toward larger noise levels = + δ. To quantify noise shift, we compare the posterior estimation gϕ(t ˆx) of intermediate states during sampling with the posterior estimation gϕ(t xt) of intermediate states from the forward process in training, along with the reference of the corresponding pre-defined prior t. This misalignment can lead to sub-optimal results in two ways: 1) noise shift introduces out-ofdistribution generalization issues, since the trained model is applied to shifted intermediate state sθ(xt+δ, t) rather than the intended sθ(xt, t). 2) noise shift causes sub-optimal denoising operations, as the next state is computed using inaccurate pre-defined coefficients. To address this issue, we propose Noise Awareness Guidance (NAG), novel guidance correction approach designed to mitigate the noise shift phenomenon. The key idea of NAG is to enable denoising models to recognize the inherent noise level of given intermediate state during sampling and to generate guidance signal that steers shifted samples back toward the accurate pre-defined noise level. However, as discussed in prior works (Ho & Salimans, 2021; Dhariwal & Nichol, 2021), gradient-based guidance signals that rely on external classifiers suffer from several drawbacks, including vulnerability to adversarial-like gradient manipulation, complex training pipelines, and the need for additional costly training on noisy inputs. Inspired by the success of classifier-free guidance (CFG) (Ho & Salimans, 2021), we further propose classifier-free variant of NAG. Instead of relying on the gradient of separately trained noise estimator, classifier-free NAG combines the score estimates of noise-conditional diffusion model with those of jointly trained noise-unconditional model. This approach removes the dependency on external classifiers by applying noise-condition dropout during training. Empirically, we show that NAG substantially alleviates the noise shift issue, consistently leading to significant improvements in the generation quality of mainstream denoising-based generative models. Our comprehensive evaluations are conducted across two widely used base models: DiT (Peebles & Xie, 2023) for diffusion models and SiT (Ma et al., 2024) for flow-based models. To demonstrate both the effectiveness and generality of NAG, our evaluations cover two mainstream use cases of modern denoising generative models: 1) We show that NAG can be directly incorporated into DiT and SiT to improve ImageNet conditional generation, highlighting that foundation model development can benefit from our approach. 2) We conduct supervised fine-tuning experiments on small downstream datasets, verifying the effectiveness of NAG in supervised fine-tuning scenarios. Overall, our contributions can be summarized as follows: We identify the noise shift issue, which is widespread in existing denoising generative models but has long been overlooked. Through empirical analysis with an external noise estimator on ImageNet generation tasks, we reveal the severity of this issue. 2 We propose novel and concise approach, Noise Awareness Guidance (NAG), to mitigate the noise shift issue. We further introduce its classifier-free variant, which can be more easily incorporated into mainstream denoising generative models. We conduct comprehensive experiments validating the effectiveness and generality of NAG, providing strong evidence that it mitigates the noise shift issue and leads to significant improvements in both ImageNet generation and supervised fine-tuning tasks."
        },
        {
            "title": "2 PRELIMIARY",
            "content": "We begin by reviewing denoising generative models under the unified framework of stochastic interpolants (Albergo et al., 2023). Throughout this section, we adopt the notation of Ma et al. (2024). Both diffusion and flow-based models can be understood as stochastic processes that gradually transform noise sample from simple prior distributions, typically standard Gaussian ϵ (0, I), into data sample from the complex target distribution x0 pdata(x0). Forward process. Let x0 pdata(x0) be sample from the data distribution. We define continuous-time stochastic interpolant over [0, ]: xt = αtx0 + σtϵ, α0 = σT = 1, αT = σ0 = 0, (1) where αt is monotonically decreasing and σt is monotonically increasing (Lipman et al., 2023; Ma et al., 2024). This formulation interpolates smoothly between the clean data (t = 0) and pure noise (t = ). Probability flow ODE. Given the forward process, the dynamics of xt can be equivalently described by probability flow ordinary differential equation (PF ODE): xt = v(xt, t), where the velocity field is given by v(x, t) = E[ xt xt = x] = αt E[x0 xt = x] + σt E[ϵ xt = x]. In practice, the velocity is parameterized by neural network vθ(x, t), trained with the objective Lv(θ) := Ex0,ϵ,t (cid:104)(cid:13) (cid:13)vθ(xt, t) αtx0 σtϵ(cid:13) (cid:13) 2(cid:105) . (2) (3) (4) Since the ODE solution at time matches the marginal distribution pt(x) of xt, samples can be generated by integrating Equation 2 backward from xT = ϵ (0, I) using standard ODE solvers. Reverse-time SDE. Equivalently, the marginals pt(x) are consistent with the reverse-time stochastic differential equation (SDE): dxt = v(xt, t) dt 1 2 wts(xt, t) dt + wt ˆwt, (5) where ˆwt is reverse-time Wiener process, wt > 0 is diffusion coefficient, and s(x, t) = log pt(x) is the score function. The score can be expressed either as conditional expectation or equivalently in terms of the velocity field: s(x, t) = σ1 E[ϵ xt = x], s(x, t) = σ αtv(x, t) αtx αtσt αt σt . (6) (7) Thus, data can also be generated by solving Equation 5 with the same velocity model vθ(x, t). 3 Conditional generation Let pt(x y) is the density that xt is condtioned on some variable y. If pt(y x) is known, we can sample from pt(x y) by solving conditional reverse-time SDE where the conditional score defined as: s(x, y) = log pt(x y) = log pt(x) + log pt(y x). (8) In practice, we can build seperate neural network to model pt(y x) on noisy data, following classifier guidance (Dhariwal & Nichol, 2021; Song et al., 2020). Note that pt(y x) pt(x y)p1 (x), we can derive the classifier-free guidance sampling (Ho & Salimans, 2021). Empirically, classifier-free guidance achieves significant performance. For simplicity, we primarily consider the linear interpolant with = 1, αt = 1 t, and σt = t, following Ma et al. (2024). Nevertheless, our analysis extends naturally to other formulations such as DDPM (Ho et al., 2020), which employ discretized dynamics, alternative schedules (αt, σt), or different model parameterizations."
        },
        {
            "title": "3 NOISE SHIFT ISSUE IN THE DENOISING PROCESS",
            "content": "We identify misalignment between the training distribution pt(x), obtained from clean data samples x0 pdata(x0), and the intermediate distribution pt(ˆx) encountered during the numerical solution of the SDE or ODE. Conceptually, this misalignment can be diagnosed by comparing the posterior pt(t x) inferred from perturbed states with the pre-defined prior p(t). In practice, accumulated errors from multiple sourcessuch as imperfect network approximation, discretization error, and other modeling inaccuraciescan be viewed as an additional Gaussian perturbation applied to xt, where ˆxt = xt + e, where (0, σ2 I). This perturbation increases the effective variance from σ2 , making the perturbed state behave as if it were sampled at shifted noise level = + δ, where to σ2 + σ2 + σ2 . We refer to the discrepancy δ = as the noise shift. t+δ = σ2 σ (9) Statement 1 (Relation between noise shift and additive error). Given the forward process defined in Equation 1, consider an additive error (0, σ2 is small, the shift δ admits first-order approximation: I). When the error variance σ2 δ (cid:112)σ2 σt + σ2 σt , (10) where σt = dσt/dt. (See Appendix for full derivations.) Intuitively, Statement 1 shows that accumulated errors push the effective variance in ˆxt toward later noise level = + δ, where δ > 0, causing systematic bias. For example, in the linear interpolation case σt = t, the shift reduces to δ = (cid:112)σ2 σt, illustrating that perturbed states tend to be interpreted as noisier than intended. Although based on simplified assumptions, this analysis qualitatively captures the nature of noise shift in practical denoising processes. + σ Empirical analysis. To better illustrate the noise shift issue, we conduct empirical simulations on ImageNet at 256 256 resolution using the pre-trained SiT-XL/2 model, which was trained for 1,400 epochs Previous studies (Sun et al., 2025; Stahl et al., 2000) suggest that for high-dimensional data such as images, the posterior pt(t x) concentrates sharply (similar to Dirac delta), making the noise level encoded in reliably estimable. Motivated by this, we train noise estimator gϕ(t x) on the ImageNet 256 256 dataset1. Empirical comparisons between the estimated posterior distributions pϕ,t(t ˆx) are shown in Figure 1. Consistent with Statement 1, we observe that the estimated posterior distribution pϕ,t(t ˆx) (yellow curve) shifts toward larger values of the pre-defined prior t, demonstrating that the noise shift phenomenon is widespread in the denoising stage. Additionally, the orange curve shows the posterior estimation on samples generated from ImageNet through the forward process in Equation 1, serving as evidence of the accuracy of gϕ on ground-truth intermediate states. 1Implementation details of the noise estimator are provided in the Appendix B.2 4 Figure 2: Conceptual comparison of guidance behaviors based on class information and noise awareness. (a) conceptual example of noise shift, where ˆxt is drifted to larger noise level by δ. (b) Class-conditional guidance pushes the trajectory toward regions aligned with the class condition c. (c) Noise-aware guidance instead pushes ˆxt toward the position better aligned with the intended noise level from the pre-defined prior. NAG explicitly targets the noise shift issue. In particular, intermediate states with mid-level noise exhibit substantial systematic overestimation by gϕ, highlighting clear misalignment between the training and inference distributions. Further results at more noise levels can be found in the Appendix C. The effect of noise shift δ. While our empirical analysis is constrained by the accuracy of the noise estimator, the observed noise shift δ can still be regarded as sufficient but not necessary condition for indicating sub-optimal behavior in the denoising stage. This pervasive noise shift affects the entire sampling trajectory in two primary ways: 1) The learned velocity field vθ(x, t) suffers from out-of-distribution errors, since the model operates on perturbed intermediate states with shifted noise levels δ. If the noise-conditioned network vθ(x, t) satisfies Lipschitz condition in x, the resulting model error can be bounded by Lxe, where Lx is the Lipschitz constant. 2) The misalignment in introduces errors in the SDE coefficients αt and σt during reverse-time integration. Consequently, the denoising process becomes sub-optimal under the influence of noise shift. As discussed above, δ can be interpreted as collection of errors originating from various sources, making it unrealistic to eliminate completely. Notably, reducing δ to zero is not sufficient condition for generating better images. For instance, if an intermediate sample corresponds to an image that is entirely out of distribution, generation will still fail due to the limited capability of the model. Nevertheless, since the existence of noise shift always induces some degree of misalignment, our qualitative findings provide valuable insights into the design of corrective methods."
        },
        {
            "title": "4 NOISE AWARENESS GUIDANCE",
            "content": "In this section, we introduce the core concept of Noise Awareness Guidance (NAG), which directly addresses the noise shift issue. We interpret the shift δ as the misalignment between the sampled state ˆxt and its intended noise condition t. Inspired by conditional guidance methods (Dhariwal & Nichol, 2021; Song et al., 2020), we propose mechanism that explicitly steers the sampling trajectory to remain consistent with the pre-defined noise schedule. Our key insight is that by reinforcing the conditioning on t, the posterior pt(t ˆx) along the reverse-time SDE (or ODE) trajectory remains closer to the pre-defined t, thereby mitigating the noise shift δ. Noise awareness guidance. The noise-conditional score can be written as s(x t) = log pt(x t) = log pt(x) + log pt(t x). Analogous to Equation 8, if pt(t x) were available, we could sample from pt(x t) by solving the conditional reverse-time SDE in Equation 11. As discussed in Section 3, the posterior pt(t x) can be reliably estimated from noisy data point xt. Intuitively, we can guide the sampling trajectory with log gϕ(t x) as the guidance signal, where gϕ is the posterior estimator model in Section 3. Since it relies on being aware of the accurate noise level encoded in an intermediate state. We refer to this approach as Noise Awareness Guidance (NAG). As the gradient log gϕ(t x) is provided by an external posterior estimator gϕ, we call this formulation classifier-based NAG. (11) 5 Classifier-free noise awareness guidance. Despite its effectiveness, classifier-based NAG inherits the drawbacks of classifier guidance (Dhariwal & Nichol, 2021; Song et al., 2020), including the high computational cost of training an external posterior estimator for t, increased pipeline complexity, and the risk of adversarial-like behavior in explicit classifiers. To address these issues, we extend the idea of classifier-free guidance (CFG) (Ho & Salimans, 2021) to NAG. Noting that pt(t x) pt(x t)/pt(x), we can utilize score mixture to approximate the gradient of an implicit noise predictor as swnag(x t) = (wnag + 1) s(x t) wnag s(x), where wnag is the guidance parameter for NAG. Importantly, modern denoising models already accept the noise level along with the intermediate state x, inherently defining the conditional score s(x t). Thus, we only need access to the unconditional score s(x), without explicitly training separate noise-level predictor. To implement NAG, we follow the training strategy of CFG: during training, the noise condition is randomly dropped with fixed probability, allowing the model to share weights between conditional and unconditional objectives. (12) Discussion and relation to CFG. The mechanism of NAG can be intuitively understood in analogy to CFG. From the perspective of conditional generation, sampling without NAG corresponds to relying solely on the conditional score model. By strengthening the conditioning on t, NAG guides the trajectory toward lower-temperature regions where the model produces higher-confidence samples, ensuring that each intermediate state remains aligned with its intended noise level. As illustrated in Figure 2, the noise-level conditioning axis introduced by NAG is orthogonal to the conditional axis of CFG, providing complementary control over the sampling process. It is worth noting that because the noise shift δ arises from various sources, CFG empirically mitigates it to some extent, as it biases sampling toward lower-temperature regions where models are more confident. However, compared to this indirect effect of CFG, NAG directly targets the reduction of δ and thereby constructs improved sampling trajectories. Figure 4 visualizes the mitigating effect on noise shift by different methods."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we present comprehensive empirical analysis to demonstrate the effectiveness and generality of NAG. Our study considers two settings: (1) standard ImageNet generation benchmarks (Section 5.1) and (2) supervised fine-tuning off-the-shelf models on small, fine-grained datasets (Section 5.2). These experiments provide evidence of NAGs compatibility with two widely used scenarios: large-scale foundation model training and supervised fine-tuning. Section 5.3 presents more discussion on empirical analysis of noise shift δ. 5.1 NAG FOR IMAGENET GENERATION Implementation details. Our experiments are conducted on two representative variants of denoising generative models: DiTs (Peebles & Xie, 2023) for diffusion-based models and SiTs (Ma et al., 2024) for flow-based models. We faithfully follow the experimental setups described in the DiT and SiT papers, unless otherwise specified. All experiments are performed at resolution of 256 256 (denoted as ImageNet 256 256), where 32 32 4 latent vectors are obtained using the pre-trained Stable Diffusion VAE tokenizer (Rombach et al., 2022). For model configurations, we adopt the S/2, B/2, L/2, and XL/2 variants introduced in the DiT and SiT papers (Peebles & Xie, 2023; Ma et al., 2024), all of which process inputs with patch size of 2. For experiments trained from random initialization, we train for 80 epochs and apply 10% dropout probability on the noise conditions. Due to computational limitations, evaluations on fully converged XL/2 models are instead conducted by fine-tuning for an additional 10 epochs on off-the-shelf checkpoints pre-trained for 1,400 epochs with 20% noise dropout. Additional experimental details are provided in Appendix B. Evaluation. For experiments with DiT, we follow the default setup using 250 DDPM sampling steps (Peebles & Xie, 2023). For SiT, consistent with its original setup, we always adopt the SDEEulerMaruyama sampler with 250 sampling steps (Ma et al., 2024). For experiments across different architectures of DiTs and SiTs, we report the Fréchet Inception Distance (FID) (Heusel 6 Table 1: Converged comparsions on ImageNet 256 256 with DiT-XL/2 and SiT-XL/2. We fine-tune off-the-shelf DiT-XL/2 and SiT-XL/2 checkpoints for an additional 10 epochs to support NAG sampling, with and without classifier-free guidance (CFG), following the setup in the original papers (Peebles & Xie, 2023; Ma et al., 2024). All metrics are reported on 50k generated images. Model Training Epoches Generation w/o CFG Generation w/ CFG FID Prec. Rec. FID Prec. Rec. DiT-XL/2 (Peebles & Xie, 2023) +NAG (ours) 1400 10+(1400) SiT-XL/2 (Ma et al., 2024) +NAG (ours) 1400 10+(1400) 9.62 2.59 8.61 2. 0.67 0.79 0.68 0.75 0.67 0.60 0.67 0.66 2.27 2.14 2.06 1. 0.83 0.80 0.82 0.77 0.57 0.61 0.59 0.66 Figure 3: FID comparison of vanilla DiTs and SiTs on ImageNet 256 256 after 80 epochs of training. Classifier-free guidance (CFG) is not used. All metrics are computed with 10K samples. et al., 2017) computed with 10,000 samples. For converged results, to enable direct comparison with the original papers, we report FID, precision (Prec.), and recall (Rec.) (Kynkäänniemi et al., 2019) computed with 50,000 samples by default. Comparison. Figure 3 presents the results of training DiTs and SiTs from scratch across various architectures. The results show that NAG consistently brings substantial improvements over the baselines. An interesting observation is that DiTs benefit more from NAG than SiTs when trained for 80 epochs. This may arise from the different training schedules: the DDPM-style setup used in DiTs could lead to better training of the noise-unconditional branch, thereby providing more accurate guidance direction for NAG. Notably, for extensively pre-trained models, it is sufficient to fine-tune only the noise-unconditional branch at small fraction of the original cost (e.g., 10% additional epochs, approximately 0.7% of the full 1,400-epoch pre-training cost) to enable the model to apply NAG. Remarkably, using NAG alone allows the model to achieve generation quality close to that of CFG-guided model. Moreover, when combined with CFG, NAG continues to provide additional improvements, demonstrating that its mechanism is complementary and orthogonal to CFG. 5.2 NAG FOR SUPERVISED FINE-TUNING Implementation Details. Supervised fine-tuning of an off-the-shelf pre-trained checkpoint to new domain is fundamental task in generative modeling. To further demonstrate the general effectiveness of NAG, we conduct supervised fine-tuning evaluations following the setups in Zhong et al. (2025; 2024). Specifically, we evaluate NAG on fine-tuning DiT-XL/22 across seven wellestablished fine-grained downstream datasets: Food101 (Bossard et al., 2014), SUN397 (Xiao et al., 2010), DF20-Mini (Picek et al., 2022), Caltech101 (Griffin et al., 2007), CUB-200-2011 (Wah et al., 2011), ArtBench-10 (Liao et al., 2022), and Stanford Cars (Krause et al., 2013). We fine-tune for 24,000 steps with batch size of 32 at 256 256 resolution for each task. The compared baselines include vanilla generation, generation with classifier-free guidance (CFG), and Domain Guidance (DoG) (Zhong et al., 2025). Notably, DoG is guidance method specifically designed for fine-tuning scenarios. To demonstrate both the fundamental effect and generality of NAG, we directly apply it on top of these baselines without any modifications, except for introducing noise-dropout training to support the noise-unconditional branch. Detailed implementation information is provided in Appendix B. 2https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-256x256.pt 7 Table 2: FID Comparisons on fine-tuning tasks with pre-trained DiT-XL-2-256x256. Dataset Method Food SUN Caltech CUB Bird Stanford Car DF-20M ArtBench Average FID Fine-tuning (w/o CFG) 16.04 21.41 31.34 9.81 + NAG (ours) 11.18 14.95 24.32 5.68 11.29 5.92 Fine-tuning (with CFG) 10.93 14.13 23.84 5.37 21.87 3.52 + NAG (ours) 5.78 8.81 Fine-tuning (with DoG) 9.25 11.69 23.05 3.52 21.88 3.41 + NAG (ours) 6.45 8.24 6.32 3.91 4.38 4.21 17.92 14.79 15.29 12.55 12.22 11. 22.76 19.22 19.94 15.69 16.76 14.80 18.65 13.72 13.69 10.31 11.55 10. Figure 4: Comparisons of the estimated posterior pϕ(t x) on ImageNet 256 256 with converged SiT/XL-2 model. (a) Noise shift across the entire sampling process, computed as the difference between the estimated posterior pϕ(t ˆx) and the pre-defined prior t. The visualization shows that noise shift δ becomes increasingly severe as sampling progresses. (b) Noise shift measured between the estimated pϕ(t ˆx) and pϕ(t x), where is generated from real data. This comparison reflects the traininginference misalignment while accounting for the inherent inaccuracy of gϕ. Evaluations. Followling the setup in (Zhong et al., 2025), all results are generated with 50 DDPM sampling steps. and the FIDs are computed with 10,000 samples. Results. The FID comparisons across various fine-tuning tasks are summarized in Table 2. The results indicate that NAG is highly general and exhibits strong compatibility across different baselines, benchmarks, and guidance approaches. Consistent with the ImageNet results, NAG alone achieves performance comparable to sampling with CFG. Furthermore, Table 2 shows that both CFG-guided sampling and DoG-guided sampling can be substantially improved by NAG. This broad compatibility highlights that the noise shift issue is indeed widespread in denoising-based generation, and that NAG, by directly addressing this issue, can consistently improve generation quality across various baselines. Notably, Domain Guidance (DoG) (Zhong et al., 2025), CFG variant specifically designed for supervised fine-tuning, also directly benefits from NAG, with significant improvements observed in Table 2. 5.3 EMPIRICAL OBSERVATIONS OF NOISE SHIFT WITH NAG This section presents detailed empirical analysis based on the posterior estimator gϕ, as further expansion beyond Section 3. As the sampling process progresses, the noise shift can be divided into two stages. In the first stage, the shift increases steadily until it reaches threshold (e.g., when the signal-to-noise ratio is around 1). In the second stage, the shift plateaus, remaining relatively stable as the actual noise level decreases from 0.5 to 0. When intermediate states approach the data distribution at very low noise levels, the estimated noise shift relative to the pre-defined prior tends to be overestimated. This occurs because gϕ applied to intermediate states generated from real data suffers from larger estimation errors due to its limited capability in this regime. This overestimate can be viewed in Figure fig: nag mitigates shift(a) and released by mean normalization in Figure 4(b). 8 Figure 5: Empirical observations of NAG mitigating the noise shift δ. (ab) Effects of NAG without interference from CFG. (cd) Compatibility of NAG under CFG, showing that NAG addresses the noise shift directly, rather than relying on the indirect effects of CFG. As shown in Figure 4, NAG primarily influences the sampling process when the signal-to-noise ratio is larger than 1 (roughly 0.5), effectively reducing the noise shift in this range. In contrast, its effect is less pronounced in the early denoising stage, where the signal-to-noise ratio is low. Figure 5 further illustrates that NAG shifts the density of intermediate states toward the posterior pϕ,t(t x) estimated from real data, and hence closer to the pre-defined prior t. Classifier-free guidance (CFG) (Ho & Salimans, 2021) is known to steer the sampling trajectory toward low-temperature regions associated with the target class, thereby producing higher-quality samples within high-confidence regions. This can be interpreted as reduction of model fitting errors. Since noise shift δ reflects the accumulation of errors from multiple sources, CFG also reduces noise shift to some extent (as observed in Figure 1(ab)). However, its effect remains indirect and limited, as CFG primarily mitigates errors along the class-conditional dimension. In contrast, Figures 4 and 5(cd) demonstrate that NAG can be directly applied on top of CFG-guided models, substantially reducing the remaining noise shift. It is important to clarify that eliminating the estimated noise shift δ is not sufficient condition for achieving optimal generation, since potential pitfalls may lie in the imperfect accuracy of the noise estimator or in other complex factors. Nevertheless, the presence of distinguishable noise shift during sampling is sufficient condition for sub-optimal generation. This observation motivates us to address the noise shift issue directly."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Denoising generative models. Denoising generative models, including diffusion models and flowbased models (Ho et al., 2020; Song & Ermon, 2019; Song et al., 2020; Lipman et al., 2023), generate high quality samples from pure noise through an iterative denoising process. Recent progress in this field has primarily focused on noise schedules (Nichol & Dhariwal, 2021; Karras et al., 2022), training objectives (Salimans & Ho, 2021), and model architectures (Peebles & Xie, 2023; Ma et al., 2024), which aim to reduce approximation errors caused by limited model capacity. Another important direction is the development of faster denoising methods with fewer iterative steps, such as high order solvers (Bao et al., 2022; Lu et al., 2022) and improved interval modeling (Frans et al., 2025; Geng et al., 2025; Song et al., 2023). These works primarily address numerical errors introduced by discretized integration. In contrast, most prior studies have focused on eliminating specific sources of error. In this paper, we instead highlight pervasive issue, namely noise shift, and demonstrate how addressing it alleviates the persistent sub optimality in the generation process. Guidance techniques for condition generations. Guidance has been shown to play central role in conditional generation (Dhariwal & Nichol, 2021; Ho & Salimans, 2021), significantly improving alignment between generated samples and conditioning information. More recently, Kynkäänniemi et al. (2024); Karras et al. (2024) proposed techniques to further improve the practical effectiveness of classifier free guidance. Our proposed Noise Awareness Guidance also falls into this category. To the best of our knowledge, it is the first method to explicitly use the noise level itself as guidance signal, directly enhancing alignment with the intended noise condition."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This paper presents novel pespetive that observing the behavior of posterior noise level pt(t ˆx), and finds out the nosie shift issue that the empirical estimated posterior noise level pϕ,t(t ˆx) have 9 tendency to larger noise level. We anaylsis that the noise shift issue is manifestation caused via collection of errors from various sources and is widespread in the current denoising sampling process, and perform iterative denoising sampling under noise shifts leads to sub-optimal generations. We further provide noise awareness guidance apporach and its classifer-free varients to directly release the noise shift issue and achieve significant improvement on by reducing the noise shift gap. We hope that our work would attract researchers to pay attention to the widespread training and inference misalignment in denoising generation and facilitate many posible future research directions, including and theoretical or empirical analysis on the noise shift issue, building generative models that are robust to inference shift in sampling stages, exploring the boundary of high quality generation, or faster sampling."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In ICLR, 2022. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, 2014. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In ICLR, 2025. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In NeurIPS, 2024. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV, 2013. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 2019. Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. 10 Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models with artworks. arXiv preprint arXiv:2206.11404, 2022. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 2022. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Lukáš Picek, Milan Šulc, Jiˇrí Matas, Thomas Jeppesen, Jacob Heilmann-Clausen, Thomas Læssøe, and Tobias Frøslev. Danish fungi 2020-not just another image recognition dataset. In WACV, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2021. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. Volker Stahl, Alexander Fischer, and Rolf Bippus. Quantile based noise estimation for spectral subtraction and wiener filtering. In CASSP, 2000. Qiao Sun, Zhicheng Jiang, Hanhong Zhao, and Kaiming He. Is noise conditioning necessary for denoising generative models? In ICML, 2025. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. Jincheng Zhong, Xingzhuo Guo, Jiaxiang Dong, and Mingsheng Long. Diffusion tuning: Transferring diffusion models via chain of forgetting. In NeurIPS, 2024. Jincheng Zhong, XiangCheng Zhang, Jianmin Wang, and Mingsheng Long. Domain guidance: simple transfer approach for pre-trained diffusion model. In ICLR, 2025. 11 DERIVATION OF STATEMENT 1 We derive the expected noise shift δ in the presence of additive Gaussian error. Recall that the forward process is defined for noise level [0, ] as xt = αtx0 + σtϵ, where ϵ (0, I). (13) Influence of error e. Consider an intermediate state perturbed by additive error: ˆxt = xt + e, (14) where RD is assumed to follow zero-mean Gaussian distribution with unknown variance, (0, σ I)."
        },
        {
            "title": "The perturbed state can be rewritten as",
            "content": "ˆxt = αtx0 + (cid:0)σtϵ + e(cid:1). Since ϵ and are independent zero-mean Gaussians, their weighted sum is also Gaussian with variance (15) Var(σtϵ + e) = σ2 + σ2 = (σ2 + σ )I. Thus, the distribution of ˆxt is ˆxt (cid:0)αtx0, (σ2 + σ2 )I(cid:1) . The perturbed state ˆxt can be expressed in terms of the initial data x0: ˆxt = (αtx0 + σtϵ) + = αtx0 + (σtϵ + e). (16) (17) (18) Definition of noise shift. This distribution coincides with that of an intermediate state from the original forward process but evaluated at shifted noise level = + δ. By definition, δ satisfies t+δ = σ2 σ2 + σ2 , (19) and the noise shift is defined as δ = t. (20) First-order approximation. Assume that σt is differentiable in and that the error variance σ2 small, so that δ is also small. first-order Taylor expansion of σt+δ around gives is σt+δ σt + σt δ, where σt = dσt dt . By construction, σt+δ = (cid:112)σ2 + σ2 . Substituting yields σt + σt δ (cid:113) σ2 + σ2 . Result. Solving for δ gives the following approximation for the noise shift: δ"
        },
        {
            "title": "B IMLEMENTATIONS",
            "content": "(cid:112)σ2 + σ2 σt σt . (21) (22) (23) All experiments are conducted in PyTorch, based on the official DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024) codebases. B.1 IMPLEMENTATION TO MAIN RESULTS Architecture configurations. We follow the transformer architectures defined in DiT, using four different configurations for various model sizes: Small (S), Base (B), Large (L), and XLarge (XL). All models employ patch size of 2, and latent states are obtained using the pre-trained Stable Diffusion tokenizer (Rombach et al., 2022). The detailed model architectures are provided in Table 3. Table 3: Configurations on DiTs and SiTs. configs S/2 B/2 L/2 XL/ params (M) FLOPs (G) depth hidden dim heads patch size latent encoder 130 23.0 12 768 12 22 676 33 118.6 6.0 28 12 1152 384 16 6 2 2 22 SD-VAE(Rombach et al., 2022) 458 80.7 24 1024 16 2 2 Sampler. For DiT, we directly adopt the DDPM sampler from the official implementation3. For SiT, we use the EulerMaruyama sampler from its official implementation4, with the default setting wt = σt in Equation 5, and the final step size set to 0.04. Guidance weights. For all baselines with CFG, we keep the setting consistent with the original results, using wcfg = 1.5. For all results of NAG without CFG, we use wnag = 3.0 by default. For NAG combined with CFG, we set wcfg = 1.2 and wnag = 2.0 by default. Training configurations. We retain most training configurations from DiT and SiT (Peebles & Xie, 2023; Ma et al., 2024), without modifying decay schedules, warmup schedules, AdamW hyperparameters, or applying additional data augmentation or gradient clipping. All results are reported using an exponential moving average (EMA) of model weights with decay of 0.9999. Our training setup includes two scenarios on ImageNet: (1) training from random initialization (Figure 3); and (2) fine-tuning off-the-shelf pre-trained models (1400 epochs) with an unconditional noise branch  (Table 1)  . Detailed configurations are summarized in Table 4. Table 4: Training Configurations on ImageNet configs from scratch (Figure 3) fine-tuning  (Table 1)  training iterations batch size optimizer ((β1, β2) noise dropout learning rate 400K 256 AdamW (0.9,0.999) 10% 1 104 50K 256 AdamW (0.9,0.999) 20% 1 105 Fine-tuning with noise condition dropout on ImageNet. Compared to training from scratch, fine-tuning requires more careful handling to avoid catastrophic forgetting of learned generative capability. Following the strategy for class-unconditional inputs, we introduce pseudo noise level (i.e., 1001 for DiT, 1.001 for SiT) that remains consistent across inputs, rather than discarding noise embeddings directly. In addition, we reduce the learning rate to one tenth of the original value (1 105 instead of 1 104) and double the noise dropout ratio to 20%. When training from scratch, the choice of unconditional implementation has only minor effect on training dynamics. Fine-tuning on new datasets. We strictly follow the setup in Domain Guidance (Zhong et al., 2025), using constant learning rate of 1 104 and batch size of 32 with the AdamW optimizer for 24,000 iterations across all datasets. For NAG, we apply 10% noise dropout. FID calculation. For fair comparison across benchmarks, we strictly follow the FID calculation protocol used in the original implementation of each task. For ImageNet generation, we compute 3https://github.com/facebookresearch/DiT 4https://github.com/willisma/SiT 13 FID scores between generated images (10K or 50K) and all available real images in the ImageNet training set, using ADMs TensorFlow evaluation suite5 (Dhariwal & Nichol, 2021). For fine-tuning experiments on downstream datasets, we observe small performance variations between different FID implementations. To ensure consistency with results reported in (Zhong et al., 2025), we compute FID scores using PyTorch implementation6, comparing 10K generated images against all available images in the test set for each downstream task. B.2 IMPLEMENTATION OF EMPIRICAL POSTERIOR ESTIMATOR gϕ. To empirically identify the noise shift issue, we rely on an external posterior estimator gϕ. Here we describe the construction of the estimator gϕ used in Section 3 and Section 5.3. All related code will be made publicly available. To reduce computational costs, we fine-tune the existing SiT-XL/2 checkpoint (the same model used for ImageNet generation) by replacing its final layer with noise level regressor. The regressor is implemented as two-layer MLP applied to the globally averaged token: the first layer projects the hidden state from 1152 to 576 dimensions with SiLU activation (Elfwing et al., 2018), and the second layer outputs the predicted noise level. We inherit the training pipeline and hyperparameters from the noise-condition fine-tuning setup on ImageNet described in Section B.1, including learning rate of 1 105, the same batch size, AdamW optimizer settings, and identical data preprocessing. The key difference is that the noise level is used as the prediction target rather than as an input condition. The model parameters ϕ are optimized by minimizing the L2 loss between the predicted and true noise levels, with the noise condition input masked by pseudo condition (set to 1.001 in practice). The posterior model operates in the latent space obtained from the SD-VAE (Rombach et al., 2022), avoiding the need to transform noisy latent states back to image space. We train gϕ on ImageNet 256 256 for 40 epochs (approximately 200K iterations), reaching training loss of 0.0002. No EMA is applied to gϕ. All probability density functions in this paper are plotted using kernel density estimation (KDE) with 5,000 samples. The samples are constructed in two steps. First, we randomly sample 5,000 images from ImageNet and generate 5,000 noise samples. We then linearly interpolate the images and noise following the linear schedule, producing 5,000 forward trajectories in which intermediate states share the same clean data point x0 and noise ϵ. Second, we generate 5,000 reverse trajectories using the EulerMaruyama SDE solver with 20 steps, incorporating the same class information, and save all intermediate states. In both cases, intermediate states within the same trajectory are tied to the same clean data point and noise. Finally, we compute the densities via KDE for samples associated with the same prior and the same generation process."
        },
        {
            "title": "C MORE VISUALIZATION RESULTS WITH KERNEL DENSITY ESTIMATION",
            "content": "In this section, we provide the full probability density results of the estimated posterior t, as an extension of Figure 1 and Figure 5."
        },
        {
            "title": "D SENSITIVITY TO HYPERPARAMETERS OF NAG",
            "content": "We analyze the sensitivity of NAG hyperparameters in Figure 8, including the guidance weight wNAG and the number of sampling steps for SiT-XL/2 with wNAG = 3.0. 5https://github.com/openai/guided-diffusion/tree/main/evaluations 6https://github.com/mseitzer/pytorch-fid 14 Figure 6: More visualization of noise shift. The yellow curves indicate the estimated probability density of the posterior pϕ,t(t ˆx) for sampled intermediate states ˆx, while the orange curves indicate the posterior pϕ,t(t x) for intermediate states stochastically interpolated from training data x0 pdata(x0) on ImageNet. The black indicator is the pre-defined t. 15 Figure 7: Additional visualization of how NAG mitigates noise shift. The yellow curves represent the estimated probability density of the posterior pϕ,t(t ˆx) for sampled intermediate states ˆx. The blue curve shows the density influenced by CFG, while the pale gold curve highlights the mitigating effect of NAG. The orange curves correspond to the posterior pϕ,t(t x) for intermediate states stochastically interpolated from training data x0 pdata(x0) on ImageNet. The black indicator denotes the pre-defined t. 16 Figure 8: Hyperparameter sensitivity of NAG. (a) Effect of wNAG, measured by FID-10K. (b) Effect of the number of sampling steps."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology, China",
        "School of Software, BNRist, Tsinghua University, China"
    ]
}