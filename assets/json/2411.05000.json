{
    "paper_title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?",
    "authors": [
        "Jonathan Roberts",
        "Kai Han",
        "Samuel Albanie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 0 0 0 5 0 . 1 1 4 2 : r Preprint. Under review. NEEDLE THREADING: CAN LLMS FOLLOW THREADS THROUGH NEAR-MILLION-SCALE HAYSTACKS? Jonathan Roberts1, Kai Han2, Samuel Albanie 1University of Cambridge, 2The University of Hong Kong jdr53@cam.ac.uk, kaihanx@hku.hk, samuel.albanie.academic@gmail.com https://needle-threading.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly comparedthey often correspond to substantially different numbers of written characters. We release our code and long context experimental data."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, LLMs and multimodal LLMs have been shown to possess remarkable capabilities (Bubeck et al., 2023) across tasks including software engineering (Hou et al., 2023), geospatial reasoning (Roberts et al., 2023a;b), medicine (Wu et al., 2023), mathematical and scientific figure understanding (Yue et al., 2024) and finance (Liu et al., 2023b). An expansion of compute resources, coupled with technical innovations (Liu et al., 2023a), is enabling contemporary frontier models to be trained on ever increasing volumes of data and longer context limitsthe maximum number of tokens they can process at once. To contextualise the number of tokens leading models can process simultaneously, at just over 300k tokens1, the classic novel Moby-Dick (Melville, 1851) could fit into the reported 2M token context window of Gemini 1.5 Pro (Reid et al., 2024) almost 5 times. As shown in Fig. 1, most books and even book series contain fewer tokens than the longest model context windows. longer context offers potential benefits to performance, for example, many-shot in-context Figure 1: Contextualising context lengths of LLMs and classic literature1. Books sourced from Project Gutenberg (2024). 1Using the LLaMA-3.1 tokenizer (Dubey et al., 2024). Preprint. Under review. learning (Agarwal et al., 2024) in which hundreds or thousands of examples are appended to the model input. Another consequence is the wider range of possible applications and attainable downstream tasks. In particular, with longer context, models can better perform real-world scenarios, such as legal document retrieval, academic research, understanding tax frameworks, and solving crimes and puzzles. In these cases, decisions are made and conclusions drawn based on large quantities of information distributed across many sources and formats. The ability to hold information on the scale of multiple full-length novels or hundreds of academic papers and documents incontext, makes models well-suited to this type of task. The rate of development of longer context models has outpaced the understanding of how well they use their long context and can navigate it. Moreover, current benchmarks are considered inadequate and lacking (Bai et al., 2023; Zhang et al., 2024). Specifically, we identify three limitations of the extant literature related to long context understanding. (1) Performance saturation: Building on the needle in haystack test (Kamradt, 2023), numerous benchmarks focus on simple retrieval-based experiments. Frontier models can perform these tasks excellently, achieving perfect or near-perfect scores (Reid et al., 2024; Anthropic, 2024a; Dubey et al., 2024), leaving little headroom and useful insights to be gained. (2) Limited context length: In most long-context benchmarks, evaluations are limited to sub-100k contexts, falling short of the context limit of frontier LLMs by an order of magnitude. (3) Lack of granular takeaways: Due to the use of real documents or tendency to aggregate multiple tasks into an overall metric in most works, isolating specific trends is challenging other than the macro-trend that performance degrades as context length increases. As such, there is opportunity for set of challenging experiments, suitable to reach the limits of frontier models. To this end, we design and conduct series of retrieval-based long context experiments of varying degrees of difficulty, across range of context sizes up to 900k (Gemini 1.5) tokens. Our investigation includes novel needle threading tasks, which entail following thread of linked pieces of information across different parts of the context and retrieving the final value. We also explore more difficult multi-threading variation, which requires tracking multiple threads simultaneously, and assess whether the LLMs are thread-safe. We evaluate suite of 17 LLMs on these tasks and observe performance decreases in longer contexts. Coupled with the finding that tokenization differs significantly between models, we introduce task-specific effective context limit metric. In summary, our core contributions are: (1) We introduce challenging multi-step threading and multi-threading retrieval tasks and evaluate 17 leading LLMs on these tasks. (2) For simple needle retrieval tasks, we show that increased context length reduces retrieval performance, while increasing the number of needles retrieved concurrently has relatively limited influence on stronger models. (3) We show that many leading LLMs are remarkably thread-safe - their thread following perfor- (4) We conduct comparison of tokenizers, mance is largely unaffected by concurrent queries. highlighting significant differences in token counting. (5) We propose task-specific and configurable model-agnostic effective context limit metric."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Evaluation of the long context capabilities of large language models is recent yet burgeoning field of research. Numerous works focus on evaluating LLMs at long-document understanding tasks, such as question answering (An et al., 2023; Bai et al., 2023; Dong et al., 2023; Kuratov et al., 2024; Shaham et al., 2023; Li et al., 2023; Yuan et al., 2024), in which performance is generally found to decrease with increasing context length. Related tasks involve the summarisation and citation of insights across documents (Laban et al., 2024) and claim verification (Karpinska et al., 2024), which proves challenging for frontier models. While these benchmarks provide robust evaluations across variety of tasks, they typically focus on smaller context lengths, with most including only limited explorations beyond 100k. Although there are benefits to realism by using real documents for these tasks, there are drawbacks. Specifically, timely annotation and curation are required, making it difficult to decompose performance as function of variables such as context depth and length. Other works focus on more abstract retrieval tasks (e.g., Kamradt (2023)), allowing clearer takeaways at the cost of real-world relevance. An influential work is Liu et al. (2024), which empirically demonstrated that the position of relevant information within an LLMs context significantly impacts performance, with the best performances attained when information is at the beginning or end of the context. Similar behaviour is reported in some subsequent works (Xu et al., 2023; An et al., 2024; 2 Preprint. Under review. Dong et al., 2023; Hsieh et al., 2024b; Laban et al., 2024) (and in some cases (Levy et al., 2024)) but others have failed to replicate the findings (Zhang et al., 2024; Song et al., 2024). Song et al. (2024) introduces retrieval paradigm involving the accumulation of information throughout the context window, along with more challenging variant that includes misleading information. Despite revealing interesting behaviour, there is limited headroom for frontier models on these tasks. Some recent related works include more challenging retrieval experiments, involving multiple steps. One example is the Ancestral Trace Challenge (Li et al., 2024), which proves challenging but is evaluated to relatively modest context lengths (up to 2k tokens). Another example is Variable Tracking (Hsieh et al., 2024a), however, results on these tasks are included as part of wider set of experiments rather than being analysed in detail separately. We evaluate our difficult needle threading tasks to context lengths up to 630k tokens and comprehensively ablate and decompose the results."
        },
        {
            "title": "3 TASKS",
            "content": "Taking inspiration from prior works (Liu et al., 2024; Hsieh et al., 2024a; Zhang et al., 2024), we focus our experimentation on abstract tasks containing synthetically generated data. By using synthetic data, (1) we avoid potentially expensive question-and-answer curation and annotation, (2) we can ensure our data is high-quality and noise-free, and (3) we gain fine-grained control over the sequence length and other task parameters, allowing direct influence on difficulty. The abstract setting removes almost all natural language semantics, constraining the experimental variables and enabling the derivation of insights more closely linked to the parameters of the context window. We use string-serialised JSON objects containing key-value pairs of random UUIDs for all our experiments. Each UUID is unique 32-character, 128-bit value string. The prompts used for each task follows this general structure: <Task description> {9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423} <Output format instructions> Key(s): d64b2470-8749-3be3-e6e8-11291f2dd06e Corresponding value(s): In the following subsections, we outline our long-context understanding tasks. To complement the textual descriptions, we also include schematic of each task in Fig 2. We conduct each experiment on set of haystacks of different sequence lengths, m, where each haystack (H) is set of keyvalue pairs: = {(Ki, Vi) {1, 2, 3, ...m}}. Single Needle In this simple, motivating task the goal is to provide the corresponding value (Vi) to single specified key (Ki). For each haystack, we place needles at fixed set of placement depths. Multiple Needles Building on the previous task, the goal of this task is to provide all the corresponding values to specified set of between 2 and 25 keys. We repeat the task with two different placement methods: (1) Random - keys are randomly sampled (without replacement). (2) Clustered - after randomly sampling an initial key, all subsequent keys are sampled adjacently (motivated by the observation that informative cues for given query often cluster together in real world applications). Conditional Needles An extension to the Multiple Needles task in which, rather than providing specific keys, the goal is to retrieve the values corresponding to all keys matching specified criteria. In this case, we modify target keys by replacing randomly selected character with special character such as * or &. The expected values are those corresponding to keys containing the special character. Threading We define Threading Task by initially selecting subset of indices = {j1, j2, ..., jn} from H, where jk {1, 2, ..., m}. We then iterate over the indices for > 1, replacing in H, Kjk Vjk1, 3 Preprint. Under review. to form thread. Given single start key (Kj1), the end goal is to find the value at the end of the thread (Vjn). We evaluate thread lengths up to n=25 steps and experiment with different thread directions: (i) Forward - where the position of each subsequent pair in the thread occurs later in (i.e., j1 < j2 < ... < jn), (ii) Backward - where the positions of subsequent pairs occurs earlier in (i.e., j1 > j2 > ... > jn) and (iii) Random - where each subsequent pair in the thread can occur at any available position in H, regardless of direction. Multi-Threading For this task, we modify to include more than one thread. The goal is to determine the final value of each thread, given only the starting keys. We experiment with different combinations of thread lengths and number of threads, as well as investigating the effect of thread direction. Branched Threading We propose final variation of the threading task in which we add branches to each link in the thread. In this case, at each index in the thread (except the first key), we modify 2 or more keys (number based on the specified branching factor, b) to equal one of the previous values. At each step, there are possible continuations, only one of which continues. The overall goal is to determine the final value of the longest thread. Figure 2: Schematics for our long-context key-value retrieval tasks. See 3 for descriptions. 4 Preprint. Under review."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Baselines. To build comprehensive characterisation of the capabilities of current frontier long context models, we evaluated set of 17 LLMs on our challenging long context retrieval experiments. Since the majority of frontier long context models are closed-source, we centre our evaluation on closed-source baselines. However, we also evaluate subset of open-source models as comparison. Where possible, we focus on chat or instruction-tuned variants of each LLM as their greater tendency to follow instructions enables broader range of tasks and eases automatic evaluation. Specifically, we evaluate models from the closed-source GPT-4 (OpenAI, 2023; 2024a), Gemini 1.0 (Gemini Team et al., 2023) and 1.5 (Reid et al., 2024), Claude 3 (Anthropic, 2024a) and 3.5 (Anthropic, 2024b), and Reka (Ormazabal et al., 2024) series and the open-source Jamba 1.5 (Team et al., 2024), Mistral (AI, 2024a), and LLaMA 3.1 (Dubey et al., 2024) model series. Reported context lengths for each model are shown in Fig. 1. Prompting. We used simple prompting strategy throughout our experimentation that consisted of single basic user prompt containing the question and output format instructions for each task. In keeping with prior works (Roberts et al., 2024a;b; OpenAI, 2024c), we do not modify the system prompt or tailor the prompt for each model. With the exception of providing examples of the desired output format, we do not use few-shot examples or explicitly encourage reasoning. We include the specific prompts used in each task in the Appendix. Inference. All inference was carried out in zero-shot setting. To aid reproducibility, we set model hyperparameters that encourage as deterministic generation as possible. Concretely, we use greedy search decoding strategies in which the most probable token is selected from the model vocabulary at each step, conditional on the preceding tokens i.e., wn+1 = arg maxwV (ww1, w2, . . . , wn). We achieve this by specifying random seeds and setting the temperature parameter to zero. We evaluate the LLMs via the VertexAI (Google, 2024) {Gemini, Claude, Jamba, LLaMA 3.1, and Mistral}, OpenAI (OpenAI, 2024b) {GPT}, and Reka (AI, 2024b) {Reka} APIs. We aimed to evaluate each model as close to their context limits as possible, however, due to API restrictions this was not always feasible, see the Appendix for more details. Evaluation. Following recent work (Roberts et al., 2024b), we use strong LLM (Gemini 1.5 Flash) to parse the output from the evaluated LLMs into specific format before evaluation via exact matching with the expected answer. As most models exhibit strong output following abilities, this LLM-based reformatting and evaluation has been demonstrated to correlate strongly with other evaluation measures in (Roberts et al., 2024a). For most models, this was only necessary for tasks requiring multiple values as the answer. For tasks requiring values as answers, we only evaluate the top answers provided by the models, any other additional answers were disregarded. Tokenization. Context limits are typically reported in tokens and models are compared as though this is consistent, modelagnostic metric. However, although minor variations in tokenization schemes might be expected across tokenizers, our preliminary experiments revealed significant differences, as outlined in Fig. 3. UUID pair is represented by 50 tokens by GPT4o while Gemini 1.5 uses 75. Over longer contexts this difference is notable: Gemini 1.5 Flashs reported context limit of 1M tokens is equivalent to 700k GPT-4o tokens. References to token counts throughout this section refer to text tokenized using the LLaMA 3.1 tokenizer. Figure 3: Tokenization. LLMs tokenize UUIDs at significantly different granularities. In the following subsections, we report the results on the tasks outlined in 3. Experiments were carried out on haystacks of 12 different sizes ranging from 1k to 630k tokens (measured in LLaMA 3.1 tokens). For most models, we repeat each experiment on 5 different sets of haystacks and report the average performance, however, in some cases, only 1 repeat was feasible due to rate limit restrictions. More details and results can be found in the Appendix. 5 Preprint. Under review. Figure 4: Single Needle overall performance with 95% Wilson confidence intervals. 4.1 SINGLE NEEDLE As motivating task, we evaluate the ability of the models to accurately retrieve values corresponding to keys at fixed depths in 10% increments in the haystacks. We show the overall depth-averaged model performance on this task in Fig. 4 and heatmaps for subset of the models in Fig. 5. At shorter contexts, the models perform this simple task well. However, in most cases, the retrieval accuracy decreases for longer context lengths. This suggests that while the models can perform inference on inputs up to their context limits, most have smaller effective context limit from which they can accurately extract information. Notable exceptions are GPT-4o and Jamba-1.5 Large, which attain perfect scores throughout. From the heatmaps, it is apparent that for the majority of models, accuracy decreases towards the middle of the context, supporting the findings of Liu et al. (2024). Figure 5: Single Needle heatmaps. For most models, the effective context length is less than the context limit. At longer contexts, retrieval precision decreases towards the middle of the context. 4.2 MULTIPLE NEEDLES Building on the previous task, we test the capabilities of the LLMs to simultaneously retrieve the values corresponding to [1,2,3,4,5,10,15,20,25] input keys from the haystacks of key-value pairs. As before, we report overall results averaged over all numbers of needles for each context size in Fig. 6 and heatmaps for selected models in Fig. 7, showing decomposition of performance as function of the number of needles and needle placement (randomly placed or clustered). Considering the overall result, we observe similar macro-average trend as in the single needle task, where performance decreases at larger context sizes. However, in this case, owing to the higher degree of difficulty the performance drop-off is steeper, with several models accuracy reduced to below 20% as their context limits are approached. This faster performance degradation suggests the effective context limits for this task are even shorter than when retrieving single needle. As before, GPT-4o achieves near-perfect score. As shown in the heatmaps for Gemini 1.5 Flash, retrieval accuracy is unaffected by the relative placement of the needles. Furthermore, context length has far larger impact on performance than the number of needles which has very limited impact on performance for the stronger models. 6 Preprint. Under review. Figure 6: Overall accuracy for Multiple Needles (left) and Conditional Needles (right). Shaded regions show 95% confidence intervals. Figure 7: Multiple Needles heatmaps. Context length has substantially greater effect on performance than needle placement positions or the number of needles. Figure 8: Conditional Needles heatmaps. Needles prove easier to retrieve when clustered. 4.3 CONDITIONAL NEEDLES Sharing similar structure to the multiple needles tasks, the conditional needles task assesses the ability to retrieve the values corresponding to [1,2,3,4,5,10,15,20,25] unspecified input keys that meet the condition of containing the * character. Compared to the multiple needles task, similar overall trend is observed. Fig. 6 shows an arguably steeper initial performance decrease at shorter context lengths followed by shallower decline towards the longer context lengths, resulting in lower overall scores. More differences between the tasks can be seen in the heatmaps in Fig. 8. One clear observation is that the placement of the conditional needles directly impacts the ability of the models to retrieve the corresponding values: retrieval accuracy is higher when the relevant key-value pairs are clustered rather than randomly placed. Also, when randomly placed, performance noticeably decreases when the number of needles increases. We found similar model performance with different conditional characters, though it was notably lower for .. 4.4 THREADING Having demonstrated the models capabilities to perform single-step retrieval-based tasks (at least at shorter context lengths), we now move towards challenging multi-step reasoning-based retrieval. Concretely, at each context size, we test how accurately each model can retrieve the final value from threads of length: [2,3,4,5,6,7,8,9,10,15,20,25]. Threading introduces directionality the relative position in the context window of subsequent pieces of the thread. We repeat each evaluation on threads going in forward, backward and random directions (see Fig. 2). Overall results are displayed 7 Preprint. Under review. Figure 9: Overall accuracy for Threading (left) and Multi-threading (right). Shaded regions show 95% confidence intervals. Figure 10: Threading. For most models, forward-travelling threads are easier to follow. in Fig. 9 and example heatmaps are shown in Fig. 10. Average accuracies are significantly lower for this task reflecting the added difficulty of following the thread through the context. For many models, e.g., Gemini 1.5 Flash (darker red) and Claude 3 Haiku (darker blue), the accuracy plateaus to nearly zero at higher context lengths. The heatmaps reveal two clear trends. Firstly, performance decreases both with increasing context length and thread length. Second, the direction of the thread matters. Except for Claude 3.5 Sonnet, all models achieve much better accuracies on threads moving forward through the context compared to threads travelling backwards. 4.5 MULTI-THREADING We extend the threading task by adding extra threads for the models to simultaneously retrieve the final values from. We test performance on thread lengths of [2,3,4,5,10] for [2,3,4,5] separate threads. This is repeated for the following thread directions: all forwards, all backwards, all random, and random directions. The averaged accuracies for each context size are shown in Fig. 9 and example heatmaps in Fig. 11. The lack of clear differences between the heatmaps for 2 threads vs 5 threads suggests that within the experimental range of thread lengths, the models are thread-safe and performance is not significantly degraded by simultaneously following additional threads. This is further illustrated in Fig. 12, in which Claude 3.5 Sonnet shows no performance degradation up to 25 threads and GPT-4o and Gemini 1.5 Pro show gradual decline. 4.6 AGGREGATING HAYSTACK METRICS To directly compare the overall performance of the models, we take an equally weighted average over the Single Needle, Multiple Needles, Conditional Needles, Threading and Multi-threading task scores. The results are presented in Tab. 1. We find that the best model depends on the context size: for the smallest contexts GPT-4o is best, at the longer contexts Gemini 1.5 Pro is superior, and Claude 3.5 Sonnet is the best performing from 2.5 to 32k. Across the board, the closed-source models outperform the open-source models. 4.7 EFFECTIVE CONTEXT LENGTH As mentioned above, the macro-trend shown across all the tasks of reduced performance at longer context windows implies the models ability to fully use their context window weakens as it grows. In short, there is context size beyond which the models are unable to effectively reason over Preprint. Under review. Figure 11: Multi-threading. Concurrently following threads does not degrade performance. Figure 12: Frontier LLMs are thread-safe. Each point represents an average over 10 repeats retrieving randomly directed threads with length of 3 in 20k LLaMA 3.1 token haystack. and retrieve from. We propose an effective context length metric for each task that leverages the granularity of our experiments rather than simply estimating an average. For each task, we create dense grid of points along the two key experimental variables (see axes of heatmaps) and interpolate the average accuracy at each point. We then determine contour corresponding to given threshold level of accuracy (taken here to be 75%). This contour represents the effective frontier, beyond which retrieval is unreliable. We derive specific task metrics from these effective frontiers. For the Single Needle task, we conservatively take the minimum value of the contour to provide metric that is independent of context position. For the other tasks we take the corresponding contour value at specific point on the x-axis, for example, where Num. Needles = 10 or Thread Length = 5. Example contour plots are shown in Fig. 13. Tab. 2 contains the computed effective context length metrics for each task. Given the discrepancies between tokenizers, we base our metric on the model-agnostic Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 1.2k 87.7 80.7 70.8 55.4 91.5 82.0 71.8 93.2 75.7 59.8 58.8 54.9 78.1 76.7 59.7 2.5k 81.1 73.3 63.5 50.4 88.7 73.7 65.7 86.1 67.9 53.8 43.5 49.8 68.9 77.1 46.9 5k 76.7 70.1 60.2 44.8 84.9 67.9 62.8 81.6 64.7 17.0 31.2 45.3 66.0 70.5 42.5 10k 78.6 67.5 57.5 39.0 80.9 52.0 59.3 74.1 61.8 33.5 29.8 40.9 61.9 69.8 40.9 20k 74.8 65.7 47.1 33.3 79.4 44.6 53.3 71.9 58.3 29.6 26.8 33.6 57.1 62.8 27.8 Accuracy (%) 32k 72.7 60.1 43.9 30.4 75.9 44.7 50.3 68.6 56.3 27.0 25.4 29.0 52.5 55.2 - 64k 69.2 53.9 43.4 27.2 63.2 39.9 43.0 64.9 51.3 24.9 20.4 26.0 38.5 39.3 - 128k 65.2 53.3 40.4 20.4 50.6 38.8 37.2 60.9 42.9 - 14.1 13.7 4.5 19.6 - 180k - 46.1 - - 48.0 37.6 37.4 - - - - - - - - 250k - 37.4 - - - - - - - - - - - - - 500k - 21.3 - - - - - - - - - - - - - 630k - 19.7 - - - - - - - - - - - - - Table 1: Overall results averaged across the Single Needle, Multiple Needles, Conditional Needles, Threading and Multi-threading tasks. The highest scoring models at each context size is bold. 9 Preprint. Under review. Figure 13: Contour plots showing effective context length frontiers for the Single Needle (left) and Multiple Needles (right) tasks. Raw contours were used for the determination of the effective context lengths in Tab. 2. To improve visual clarity, the contours displayed have been smoothed using Gaussian filter with Ïƒ=1.5. Model Context Limit (1k chars) Effective Context Size (1k chars) (proportion of limit, %) Single Needle Multiple Needles Conditional Needles Threading Multi-threading Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 2472 1236 295 295 309 309 309 214 214 214 214 214 214 214 38 @10 needles @10 needles 315 (13%) 132 (11%) 295 (100%) 87 (29%) 169 (55%) 309 (100%) 87 (28%) 214 (100%) 120 (56%) 5 (2%) 5 (2%) 14 (7%) 22 (10%) 138 (64%) 24 (63%) 430 (17%) 294 (24%) 295 (100%) 17 (6%) 309 (100%) 309 (100%) 201 (65%) 214 (100%) 176 (82%) 5 (2%) 9 (4%) 22 (10%) 114 (53%) 124 (58%) 31 (82%) 220 (9%) 44 (4%) 10 (3%) 10 (3%) 121 (39%) 14 (5%) 18 (6%) 14 (7%) 43 (20%) 3 (1%) 3 (1%) 34 (16%) 34 (16%) 60 (28%) 0 (0%) @5 steps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 4 (1%) 0 (0%) 0 (0%) 7 (3%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) @5 steps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3 (1%) 0 (0%) 0 (0%) 3 (1%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3 (1%) 0 (0%) Table 2: Effective context lengths. @X indicates the effective limit on the task when the named parameter equals X. number of characters in the input rather than the token count. The results show that even for the less challenging tasks, most models have an effective context length far less than their advertised context limit."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We introduce set of retrieval experiments covering simple single-needle retrieval, more difficult multiple-needle and conditional-needle retrieval and finally, challenging needle threading and multithreading retrieval. All experiments are carried out on haystacks consisting of key-value pairs of UUIDs, where the distractor text is from the same distribution as the relevant text. By focusing purely on this abstract, synthetic setting, we can closely control the experimental conditions and perform studies across specific independent variables to high degree of granularity. This enables us to decompose the key variables affecting performance and extract the following interesting takeaways after evaluating 17 LLMs on the tasks. (i) At long context lengths, the retrieval precision of frontier LLMs decreases towards the middle of the context; (ii) Clustering needles has little effect when tasked with retrieving specific needles but noticeably increases performance when retrieving all needles meeting condition; (iii) Most LLMs achieve higher accuracies when retrieving threads moving forwards through the context versus backwards directed threads; (iv) The evaluated LLMs show proficiency at keeping track of multiple threads simultaneously. Thus, we go further than most prior long context benchmarks, which average across tasks, and provide only coarse, macro-trends. After revealing notable differences between tokenizers and observing poorer performances on larger haystacks, we sought to derive an effective context limit metric. In particular, we propose contourbased task-specific metric that is independent of tokenization. For given task setting, the metric 10 Preprint. Under review. defines the maximum context size at which the model can effectively perform. We release our code and tasks for the community to use and we hope that our findings encourage further long context understanding research."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by the UKRI Centre for Doctoral Training in Application of Artificial Intelligence to the study of Environmental Risks (reference EP/S022961/1), an Isaac Newton Trust grant, research gift from Google, an EPSRC HPC grant, the Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27208022), and HKU Seed Fund for Basic Research. Samuel would like to acknowledge the support of Z. Novak and N. Novak in enabling his contribution."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. Mistral AI. Mistral Large 2. https://mistral.ai/news/mistral-large-2407/, July 2024a. Reka AI. Reka AI API. https://platform.reka.ai/dashboard, 2024b. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your llm fully utilize the context. arXiv preprint arXiv:2404.16811, 2024. Anthropic. Introducing the next generation of Claude. https://www.anthropic.com/ news/claude-3-family, Mar 2024a. Anthropic. Claude 3.5 Sonnet. claude-3-5-sonnet, Jun 2024b. https://www.anthropic.com/news/ Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Google. Vertex AI. https://cloud.google.com/vertex-ai/, 2024. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: systematic literature review. arXiv preprint arXiv:2308.10620, 2023. Preprint. Under review. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024a. Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, et al. Found in the midarXiv preprint dle: Calibrating positional attention bias improves long context utilization. arXiv:2406.16008, 2024b. Greg Kamradt. Llmtest needleinahaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2024-09-09. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: novel challenge for long-context language models. arXiv preprint arXiv:2406.16264, 2024. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in 10m haystack: Recurrent memory finds what llms miss. arXiv preprint arXiv:2402.10790, 2024. Philippe Laban, Alexander Fabbri, Caiming Xiong, and Chien-Sheng Wu. Summary of haystack: challenge to long-context llms and rag systems. arXiv preprint arXiv:2407.01370, 2024. Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. arXiv preprint arXiv:2402.14848, 2024. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939, 2023. Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window? arXiv preprint arXiv:2407.11963, 2024. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889, 2023a. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. Fingpt: Democratizing internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485, 2023b. Herman Melville. Moby-Dick; or, The Whale. Project Gutenberg, 1851. https://www. gutenberg.org/ebooks/2701. OpenAI. GPT-4V(ision) System Card. https://cdn.openai.com/papers/GPTV_ System_Card.pdf, 2023. OpenAI. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, July 2024a. OpenAI. API Reference. https://platform.openai.com/docs/api-reference, 2024b. OpenAI. simple-evals. https://github.com/openai/simple-evals, 2024c. Accessed: 15-05-2024. Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et al. Reka Core, Flash, and Edge: Series of Powerful Multimodal Language Models. arXiv preprint arXiv:2404.12387, 2024. Project Gutenberg. Project gutenberg. https://www.gutenberg.org, 2024. Accessed: 2024-09-23. 12 Preprint. Under review. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Jonathan Roberts, Timo Luddecke, Sowmen Das, Kai Han, and Samuel Albanie. GPT4GEO: How Language Model Sees the Worlds Geography. arXiv preprint arXiv:2306.00020, 2023a. Jonathan Roberts, Timo Luddecke, Rehan Sheikh, Kai Han, and Samuel Albanie. Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. arXiv preprint arXiv:2311.14656, 2023b. Jonathan Roberts, Kai Han, and Samuel Albanie. GRAB: Challenging GRaph Analysis Benchmark for Large Multimodal Models. arXiv preprint arXiv:2408.11817, 2024a. Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. SciFIBench: Benchmarking large multimodal models for scientific figure interpretation. Neural Information Processing Systems, 2024b. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023. Mingyang Song, Mao Zheng, and Xuan Luo. Counting-stars: simple, efficient, and reasonable strategy for evaluating long-context large language models. arXiv preprint arXiv:2403.11802, 2024. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Jamba-1.5: Hybrid transformerAviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. mamba models at scale. arXiv preprint arXiv:2408.12570, 2024. Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023. Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, et al. Lv-eval: balanced long-context benchmark with 5 length levels up to 256k. arXiv preprint arXiv:2402.05136, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, 2024. 13 Preprint. Under review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROMPTS A.1.1 SINGLE NEEDLE Extract the value corresponding to the specified key in the JSON object below. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the corresponding value, nothing else. Key: <key> Corresponding value: A.1.2 MULTIPLE NEEDLES Extract the values corresponding to the specified keys in the JSON object below. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the list of corresponding values in square brackets, nothing else. Keys: [<keys>] Corresponding values: A.1.3 CONDITIONAL NEEDLES Extract the values corresponding to the keys that contain the character <char> in the JSON object below. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the list of corresponding values in square brackets, nothing else. Corresponding values: A.1.4 THREADING The specified key corresponds to value in the JSON object below. However, that value might equal another key in the JSON object. The value corresponding to this new key might also equal another key in the JSON object. This chain could continue beyond. Extract the final value in the chain. If the value corresponding to the first key does not equal another key, then the final value is the value corresponding to the first key. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the corresponding value at the end of the chain, nothing else. Key: <key> Corresponding final value: 14 Preprint. Under review. A.1.5 MULTI-THREADING The specified keys each correspond to values in the JSON object below. However, the values might equal others key in the JSON object. The value corresponding to each new key might also equal another key in the JSON object. This chain could continue beyond. Extract the final values in each the chain. If the value corresponding to the first key does not equal another key, then the final value is the value corresponding to the first key. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the corresponding values at the end of each chain in square brackets, nothing else. Keys: <keys> Corresponding final values: A.1.6 BRANCHED THREADING The specified key corresponds to value in the JSON object below. However, that value might equal other keys in the JSON object. The values corresponding to these new keys might also equal other keys in the JSON object. This branched chain could continue beyond. Follow the longest chain and extract the final value at the end of the chain. { 9a159850-2f26-2bab-a114-4eefdeb0859f: 5de8eca9-8fd4-80b8-bf16-bd4397034f54, d64b2470-8749-3be3-e6e8-11291f2dd06e: 1f22fcdb-9001-05ab-91f1-e7914b66a4ea, . . ., bae328a1-44f3-7da1-d323-4bd9782beca1: 1183e29c-db7a-dccf-6ce8-c0a462d9942c, 5d88d112-e4ec-79a1-d038-8f1c58a240e4: ea8bf5c3-1ede-7de0-ba05-d8cd69393423, } Only write the corresponding value at the end of the longest chain, nothing else. Key: <key> Corresponding final value: A.1.7 LLM-REFORMATTING SINGLE VALUE OUTPUT generative model has answered question to which the answer is 32-character hexadecimal string UUID.n The output from the model answering the question is <unformatted model response>.n Extract just the 32-character hexadecimal UUID string from the output. Keep the dashes but remove any whitespace, other characters (such as punctuation or quotes), and any additional text and explanation.n Return only the extracted 32-character hexadecimal UUID, without any additional text or explanation. If no answer is provided, return None.n A.1.8 LLM-REFORMATTING MULTIPLE VALUE OUTPUT generative model has answered question to which the answer is list of 32-character hexadecimal strings.n The output from the model answering the question is <unformatted model response>.n Extract just the list of 32-character hexadecimal UUID strings from the output. Keep the dashes but remove any whitespace, other characters (such as punctuation or quotes), and any additional text and explanation.n Format the list as list of strings, with each string in the list being 32-character hexadecimal UUID string. For example: [12345678-1234-5678-1234-567812345678, 87654321-43218765-4321-876587654321]n Return only the extracted list, without any additional text or explanation. Do not include any additional syntax, like python, in your answer. If no answer is provided, return None.n A.2 MODEL VERSIONS Closed-source model API versions GPT-4o mini: gpt-4o-mini-2024-07-18 GPT-4o: gpt-4o-2024-08-06 15 Preprint. Under review. Gemini-Pro: gemini-1.0-pro-002 Gemini 1.5 Flash: gemini-1.5-flash-preview-0514 Gemini 1.5 Pro: gemini-1.5-pro-preview-0514 Claude 3 Haiku: claude-3-haiku@20240307 Claude 3 Sonnet: claude-3-sonnet@20240229 Claude 3.5 Sonnet: claude-3-5-sonnet@20240620 Reka Flash: reka-flash-20240904 Reka Core: reka-core-20240415 A.3 RESULTS TABLES Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Mistral Large Mistral Nemo 1.2k 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 96.4 100.0 100.0 100.0 100.0 100.0 2.5k 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 98.2 96.4 100.0 100.0 100.0 100. Accuracy (%) 5k 100.0 100.0 100.0 98.2 100.0 100.0 100.0 100.0 100.0 0.0 76.4 100.0 96.4 100.0 100.0 100.0 100.0 10k 100.0 100.0 100.0 98.2 100.0 100.0 100.0 100.0 100.0 94.5 83.6 94.5 98.2 100.0 98.2 100.0 100.0 20k 100.0 100.0 100.0 96.4 100.0 100.0 98.2 100.0 100.0 87.3 85.5 98.2 96.4 98.2 76.4 98.2 12.7 32k 98.2 94.5 100.0 100.0 100.0 100.0 100.0 100.0 98.2 89.1 76.4 89.1 89.1 100.0 - - - 64k 98.2 83.6 100.0 94.5 98.2 100.0 94.5 100.0 94.5 87.3 56.4 87.3 89.1 100.0 - - - 128k 96.4 89.1 100.0 78.2 90.9 100.0 74.5 100.0 80.0 61.8 50.9 50.9 18.2 80.0 - - - 180k 94.5 89.1 100.0 72.7 87.3 94.5 83.6 - - - - - - - - - - 250k 76.4 74.5 - - - - - - - - - - - - - - - 500k 45.5 34.5 - - - - - - - - - - - - - - - 630k 30.9 32.7 - - - - - - - - - - - - - - - Table 3: Single Needle depth-averaged results. Reka Core 0.0 at 5k is likely due to safety restraints (output is not generated due to context). Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 1.2k 100.0 100.0 99.6 71.9 100.0 100.0 99.9 100.0 99.9 97.6 94.9 98.0 100.0 16.7 99.8 2.5k 100.0 98.9 99.4 67.0 100.0 100.0 100.0 100.0 99.8 82.7 77.9 94.7 100.0 55.6 99.9 5k 100.0 100.0 99.5 63.0 100.0 100.0 99.4 100.0 99.0 64.7 68.2 88.1 100.0 88.2 98.2 10k 100.0 100.0 98.0 56.6 99.9 100.0 99.7 100.0 98.6 50.0 55.2 78.3 99.9 98.6 97. Accuracy (%) 64k 97.4 86.3 88.4 21.4 99.1 97.0 94.9 99.9 85.5 31.6 45.0 40.9 73.2 77.3 - 32k 99.8 86.7 92.6 35.0 99.6 98.6 96.9 100.0 95.6 42.9 49.8 51.8 91.2 88.2 - 20k 100.0 99.9 95.5 46.4 99.7 99.5 98.5 100.0 97.2 54.8 48.1 63.6 97.7 94.0 58.5 128k 96.3 84.0 83.9 13.5 97.3 93.8 80.2 99.8 70.5 0.0 19.4 16.8 1.9 17.7 - 180k 94.7 67.7 - - 85.9 91.7 67.0 - - - - - - - - 250k 76.7 46.3 - - - - - - - - - - - - - 500k 34.6 18.5 - - - - - - - - - - - - - 630k 30.0 10.0 - - - - - - - - - - - - - Table 4: Multiple Needles overall results. 16 Preprint. Under review. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Accuracy (%) 1.2k 98.6 96.3 98.0 80.5 88.9 99.9 99.2 100.0 98.2 56.9 68.8 52.9 97.2 100.0 54.0 2.5k 98.3 96.9 92.4 66.3 92.2 99.9 94.3 99.8 98.3 61.2 37.7 51.2 98.4 100.0 17.4 5k 95.2 94.6 85.4 46.0 89.8 98.1 90.2 99.2 92.9 16.9 6.7 34.1 99.1 99.8 11.0 10k 97.3 94.3 71.0 30.7 88.3 45.0 84.9 97.5 88.9 21.7 6.6 31.0 97.1 98.5 8. 20k 93.6 90.2 30.7 19.6 87.1 16.1 60.9 91.2 80.1 4.7 0.2 4.9 85.4 94.7 1.1 32k 95.7 86.8 25.0 15.9 87.7 17.0 50.8 92.8 77.4 2.8 0.0 2.5 80.5 85.6 - 64k 92.4 78.8 27.1 20.3 71.4 0.0 21.8 89.9 76.7 5.6 0.0 0.4 30.0 16.7 - 128k 85.6 78.8 17.1 10.6 45.3 0.1 28.9 82.3 63.9 - 0.0 0.0 1.8 0.2 - 180k 77.9 66.7 - - 51.4 0.0 33.5 - - - - - - - - 250k 86.2 64.1 - - - - - - - - - - - - - Table 5: Conditional Needles overall results. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Mistral Large Mistral Nemo Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Mistral Large Mistral Nemo 1.2k 57.8 46.7 23.9 5.6 78.3 40.0 25.6 75.0 37.2 27.8 19.4 13.2 38.0 75.0 23.3 68.9 12. 1.2k 82.2 60.5 32.5 18.9 90.1 69.9 34.1 90.9 43.0 16.8 11.1 14.0 55.1 91.6 21.6 71.3 19.0 2.5k 42.2 33.9 12.2 7.8 72.2 26.7 10.0 61.1 22.8 22.2 0.0 1.4 21.3 58.3 8.9 45.0 7.2 2.5k 65.1 36.9 13.5 10.8 79.1 42.1 24.2 69.5 18.6 2.9 1.7 3.3 28.3 71.5 8.2 49.2 14.4 5k 35.0 25.6 8.3 3.3 61.7 17.2 7.2 51.1 14.4 0.0 2.8 0.7 13.0 20.8 2.2 31.1 2.2 10k 37.8 18.3 5.6 1.7 53.3 7.2 3.3 30.0 8.3 0.0 2.8 0.0 7.4 29.2 0.6 10.6 0.0 20k 29.4 16.7 5.6 1.7 52.2 6.7 1.7 23.3 5.0 0.0 0.0 0.0 1.9 12.5 1.1 1.1 0. Accuracy (%) 32k 25.0 13.9 0.6 0.0 43.9 2.8 0.0 16.1 0.0 0.0 0.0 0.0 0.0 0.0 - - - 64k 23.3 10.0 1.1 0.0 13.3 1.1 1.7 14.4 0.0 0.0 0.0 0.0 0.0 0.0 - - - 128k 23.3 6.7 0.0 0.0 5.6 0.0 0.6 7.2 0.0 - 0.0 0.0 0.0 0.0 - - - Table 6: Threading overall results. 5k 53.2 30.4 8.0 13.6 72.8 24.2 17.4 57.5 17.3 3.5 2.0 3.5 21.6 43.7 1.3 34.9 9.7 10k 57.9 25.1 13.0 7.9 62.8 7.6 8.7 42.9 13.1 1.5 0.7 0.9 6.7 22.7 0.3 14.4 7. 20k 50.7 21.9 3.8 2.5 58.2 1.0 7.4 44.9 9.3 1.3 0.2 1.1 4.1 14.5 1.9 8.7 3.1 Accuracy (%) 32k 44.9 18.5 1.2 1.0 48.5 5.1 4.0 34.1 10.3 0.0 0.6 1.5 1.8 2.2 - - - 64k 34.6 10.5 0.6 0.0 33.9 1.5 2.3 19.9 0.0 0.2 0.8 1.6 0.3 2.4 - - - 128k 24.6 7.8 1.2 0.0 13.8 0.0 1.6 15.2 0.0 - 0.0 0.6 0.4 0.3 - - - 180k - 2.8 - - 4.4 0.0 1.1 - - - - - - - - - - 180k - 4.0 - - 11.1 1.6 1.6 - - - - - - - - - - 250k - 0.0 - - - - - - - - - - - - - - - 250k - 2.2 - - - - - - - - - - - - - - - 500k 59.9 52.2 - - - - - - - - - - - - - 500k - 1.1 - - - - - - - - - - - - - - - 500k - 0.3 - - - - - - - - - - - - - - - 630k - 54.8 - - - - - - - - - - - - - 630k - 0.6 - - - - - - - - - - - - - - - 630k - 0.5 - - - - - - - - - - - - - - - Table 7: Multi-Threading overall results. 17 Preprint. Under review. Figure 14: Branched threading. Shaded regions display 95% Wilson confidence intervals. A.4 BRANCHED THREADING We carried out small-scale branched threading investigation focused on subset of the bestperforming models from the previous tasks. Across 5 haystack sizes, we evaluate the models ability to accurately retrieve the final value of threads of length [2,3,4,5,6,7,8,9,10] where there is branch at each step. We repeat this for branching factors of [2,3,4,5,6,7,8,9,10] and present the averaged results in Fig. 14. After an initial drop, the performance does not significantly decrease with context length though it does with thread length and branching factor. We conducted 5 repeats per context size for each model. A.5 LIMITATIONS We note several limitations to our work. First, we restrict our study to the use of synthetic data. While this has significant benefits (fine-grained controllability, automatic provision of perfect ground truth), our benchmark does not capture differences in LLM behaviour that are domainspecific (for instance, LLMs may be more performant on some distributions than others). Second, as discussed below, the scale of our experiments (particular the number of experimental repeats) was limited by cost for the larger models. A.6 API RESTRICTIONS The design of our experiments was guided in part by the following API-based restrictions and limitations: Cost. For the most expensive models (e.g., Gemini 1.5 Pro, Claude 3.5 Sonnet), running just single repeat on one task could cost hundreds of dollars. Therefore, in some cases, the evaluation of these models could not be repeated extensively, limiting the statistical strength of our experiments. Context restrictions. Some models were only available for API-based inference in limited capacity (e.g., Mistral), in which it was not possible to provide inputs that approach the context limit. As such, we could only evaluate these models as close to the context limit as we could. Latency. As result of latency introduced by low server throughput or indirectly via low rate limits at the time of writing, for some models (e.g., LLaMA 3.1), it was not possible to extensively conduct repeats. A.7 REPEATS 18 Preprint. Under review. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Mistral Large Mistral Nemo 1.2k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2.5k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 10k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 20k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 Num. Repeats 64k 32k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 - - - - - - 128k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 - - - 180k 5 5 1 1 5 5 5 - - - - - - - - - - 250k 5 5 - - - - - - - - - - - - - - - Table 8: Number of repeats carried out for the Single Needle task. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 1.2k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 5 2.5k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 5 5k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 5 10k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 20k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 5 Num. Repeats 64k 32k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 2 2 2 2 1 1 - - 128k 5 5 5 5 5 5 5 5 5 1 1 2 2 1 - 180k 1 5 - - 5 5 5 - - - - - - - - 250k 1 5 - - - - - - - - - - - - - Table 9: Number of repeats carried out for the Multiple Needles task. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 1.2k 5 5 5 5 5 5 5 5 5 1 1 1 1 1 5 2.5k 5 5 5 5 5 5 5 5 5 1 1 1 1 1 5 5k 5 5 5 5 5 5 5 5 5 1 1 1 1 1 5 10k 5 5 5 5 5 5 5 5 5 1 1 1 1 1 20k 5 5 5 5 5 5 5 5 5 1 1 1 1 1 5 Num. Repeats 64k 32k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 - - 128k 5 5 5 5 5 5 5 5 5 - 1 1 1 1 - 180k 1 5 - - 5 5 5 - - - - - - - - 250k 1 5 - - - - - - - - - - - - - 500k 5 5 - - - - - - - - - - - - - - - 500k 1 5 - - - - - - - - - - - - - 500k 1 5 - - - - - - - - - - - - - Table 10: Number of repeats carried out for the Conditional Needles task. 630k 5 5 - - - - - - - - - - - - - - - 630k 1 5 - - - - - - - - - - - - - 630k - 5 - - - - - - - - - - - - - 19 Preprint. Under review. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro Mistral Large Mistral Nemo 1.2k 5 5 5 5 5 5 5 5 5 1 1 4 3 1 5 5 5 2.5k 5 5 5 5 5 5 5 5 5 1 1 4 3 1 5 5 5k 5 5 5 5 5 5 5 5 5 1 1 4 3 1 5 5 5 10k 5 5 5 5 5 5 5 5 5 1 1 4 3 1 5 5 5 20k 5 5 5 5 5 5 5 5 5 1 1 4 3 1 5 5 5 Num. Repeats 64k 32k 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 4 4 3 3 1 1 - - - - - - 128k 5 5 5 5 5 5 5 5 5 - 1 4 2 1 - - - 180k - 5 - - 5 5 5 - - - - - - - - - - 250k - 5 - - - - - - - - - - - - - - - 500k - 5 - - - - - - - - - - - - - - - 630k - 5 - - - - - - - - - - - - - - - Table 11: Number of repeats carried out for the Threading task. Model Gemini 1.5 Pro Gemini 1.5 Flash Jamba 1.5 Large Jamba 1.5 Mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku GPT-4o GPT-4o mini Reka Core Reka Flash LLaMA 3.1 8b LLaMA 3.1 70b LLaMA 3.1 405b Gemini 1.0 Pro 1.2k 1 5 1 1 5 1 5 1 1 1 1 1 1 1 5 2.5k 1 5 1 1 5 1 5 1 1 1 1 1 1 1 5 5k 1 5 1 1 5 1 5 1 1 1 1 1 1 1 5 10k 1 5 1 1 5 1 5 1 1 1 1 1 1 1 5 20k 1 5 1 1 5 1 5 1 1 1 1 1 1 1 5 Num. Repeats 64k 32k 1 1 5 5 1 1 1 1 5 5 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 - - 128k 1 5 1 1 5 1 5 1 1 - 1 1 1 1 - 180k - 5 - - 1 1 5 - - - - - - - - 250k - 5 - - - - - - - - - - - - - 500k - 5 - - - - - - - - - - - - - 630k - 5 - - - - - - - - - - - - - Table 12: Number of repeats carried out for the Multi-threading task."
        }
    ],
    "affiliations": [
        "University of Cambridge",
        "The University of Hong Kong"
    ]
}