{
    "paper_title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
    "authors": [
        "Yun Qu",
        "Qi Wang",
        "Yixiu Mao",
        "Heming Zou",
        "Yuhang Jiang",
        "Weijie Liu",
        "Clive Bai",
        "Kai Yang",
        "Yangkun Chen",
        "Saiyong Yang",
        "Xiangyang Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 0 7 9 1 0 . 2 0 6 2 : r 2026-02Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models Yun Qu1,2, Qi Wang1,, Yixiu Mao1, Heming Zou1,2,, Yuhang Jiang1, Weijie Liu2, Clive Bai2, Kai Yang2, Yangkun Chen2, Saiyong Yang2,, Xiangyang Ji1, 1Department of Automation, Tsinghua University 2LLM Department, Tencent (cid:66) cheemswang@mail.tsinghua.edu.cn, stevesyang@tencent.com, xyji@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using lightweight generative model trained on the shared optimization history. Intermediatedifficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPSs substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
        },
        {
            "title": "Introduction",
            "content": "Recent advances have witnessed the impressive reasoning abilities of large language models (LLMs) on complex problem-solving, such as mathematics and programming (Shao et al., 2024; Luo et al., 2025a). Behind the remarkable success is key post-training technology, reinforcement learning with verifiable rewards (RLVR) (Jaech et al., 2024; Guo et al., 2025), which prompts the LLM to generate long Chain-of-Thoughts (CoTs) (Wei et al., 2022) and performs policy optimization with verified rewards. Despite its success, RLVR is widely known to be expensive in computations and memory usage, as it requires substantial rollouts for LLM policy evaluation and updates (Zheng et al., 2025b; Lin et al., 2025). Pressing demands of online prompt selection. In LLMs training, data curation is an indispensable process, as the prompt chosen to optimize directly influences convergence and efficiency. In particular, the prompts in RLVR contribute unevenly: overly easy or extremely hard prompts tend to yield vanishing gradients (Yu et al., 2025), while intermediate-difficulty prompts are more informative to update (Zeng et al., 2025b; Chen et al., 2025). Consequently, the naive use of random prompt sampling can be ineffective and prolong the learning process. To this end, several researchers have shifted focus to online prompt selection (Yu et al., 2025; Zhang et al., 2025a; Bae et al., 2025), which evaluates prompt difficulty in larger candidate set via additional rollouts and selects the subset for RLVR. While such strategy improves performance and reduces the number of training steps, it incurs substantial computational overhead from additional rollouts (Zheng et al., 2025b). Opportunities and challenges of prompt difficulty prediction. Recognizing the high computational cost of prompt evaluation, recent works (Zheng et al., 2025b; Zhang et al., 2025b; Qu et al., 2025a) propose to predict prompt difficulty from accumulated history and employ selective rollouts to improve training efficiency. However, existing approaches to difficulty estimation, e.g., MoPPS (Qu et al., 2025a), rely on prompt predictive model (PPM) that tracks difficulty beliefs independently for arbitrary prompt, imposing several limitations. Note that prompt-specific modeling isolates samples during belief updates; hence, only frequently optimized prompts enable reliable difficulty estimates. Other prompts either suffer from stale information or receive few belief updates at all, considering that these prompt-specific PPMs cannot generalize across prompts. Moreover, they neglect the batch-level structure in prompt selection, leading to redundant training signals within selected batches and limited exploration of the prompt space. All of these raise the research question: Can we design lightweight yet generalizable PPM, together with more effective subset acquisition strategies, to achieve efficient optimization? Small generalizable PPMs and comprehensive batch acquisition boost computational efficiency. This work positively answers the research question and proposes generalizable predictive prompt selection (GPS), which designs generalizable PPM and crafts principled batch acquisition strategy in RLVR. (i) Unlike isolated prompt difficulty modeling (Qu et al., 2025a; Zhang et al., 2025b; Zheng et al., 2025b), we construct lightweight Work completed during an internship at Tencent. Corresponding Authors generative PPM that exploits the entire optimization histories and shares experience across prompts. This way derives the generalizable difficulty prior and improves the accuracy of the prompt difficulty estimate through temporal extrapolation. (ii) Armed with the generalizable PPM, we incorporate the intermediate difficulty prioritization and history-anchored diversity principles into the acquisition criteria for batch selection. This design aims to reduce sampling redundancy and preserve prompt coverage. In addition, the proposed generalizable PPM reserves the potential of guiding efficient test-time computation allocation. Contributions and Empirical Findings. The primary contribution of this work is threefold: 1. We develop generalizable PPM that leverages the shared optimization history to generalize difficulty prediction across prompts at negligible computational cost for RLVR. 2. The proposed prompt batch acquisition strategy combines difficulty guidance with history-aware diversity to reduce redundancy and improve prompt coverage. 3. The generalizable PPM after RL post-training supports the test-time computation allocation, improving performance under compute budget constraints. Extensive experiments on large-scale mathematical and logical reasoning benchmarks across diverse LLM backbones witness several advantages of our scheme in both training and test time: (i) GPS reliably predicts prompt difficulty and improves the quality of the selected prompt batch. (ii) GPS accelerates RL post-training, delivering up to 2.0 speedup over uniform sampling while consistently improving performance. Moreover, GPS matches or outperforms evaluation-based selection with up to 69% lower rollout cost. (iii) At test time, the learned PPM enables effective computation reallocation, reducing inference cost by up to 36.4% without performance loss or improving accuracy by up to 3.2% under fixed budgets."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Notations i=1 is selected to generate rollouts, i.e., long CoT responses, for optimization. We consider reasoning tasks in which prompts τ take the form of mathematical or logical problems, e.g., If 991 + 993 + 995 + 997 + 999 = 5000 N, then N? from Deepscaler (Luo et al., 2025b). Let = {τi}N i=1 denote the full prompt pool. At iteration t, the LLM πθt serves as the policy. In each iteration, batch of prompts = {τt,i}B For each prompt τ t,j}k j=1, with each verified by t,j}k reward function r(τ, y) to yield the reward set rτ j=1. Following common practice (Guo et al., 2025), we focus on binary correctness rewards, while our formulation does not rely on this restriction (Appendix E.5). We denote γτ ] as the latent success rate of prompt τ at step t, which acts as an intrinsic difficulty attribute of the prompt and represents the probability that the current model solves τ. The batch-level feedback at t-th step is RB , the LLM generates independent responses yτ = {rτ i=1, corresponding to the generative process: = E[rτ = {yτ = {r τt,i }B p(RB , θt) = t (cid:90) i=1 We define the observation history of prompt τ as Hτ defined as the collection of all prompt-specific history Ht = (cid:83) For brevity, we use τ to denote both the natural language prompt and its corresponding embedding representation. The choice of encoding mechanism is orthogonal to our core studies, and we adopt the WordLlama toolkit implementation (Miller, 2024) to ensure efficient processing, with further discussion provided in Appendix B.3. and t}. The full optimization history is τ τT Hτ = {T τt,i, θt)dγ τt,i )p(γ = {rτ , RB }t i=1. τt,i τt,i τt,i p(r γ . 2.2 Reinforcement Learning with Verifiable Rewards RLVR aims to optimize the LLM policy πθ to maximize the expected verifiable reward: max θ τT , yπθ(τ) where πθ(yτ) is the models conditional distribution over responses for prompt τ, and r(τ, y) is verifiable reward evaluating the quality of response y. (cid:2)r(τ, y)(cid:3), (1) Group Relative Policy Optimization (GRPO). Shao et al. (2024) introduces GRPO to stablize RL post-training via the group-normalized advantage while removing the value network used in PPO (Schulman et al., 2017): JGRPO(θ) = τT , {yτ }k i=1πθold (τ) 1 i=1 1 yτ yτ t=1 (cid:0)min (cid:0)ρi,t ˆAi, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi (cid:1) β KL(πθπref)(cid:1) (2) 2 Figure 1: Framework overview. Unlike prompt-specific modeling like MoPPS (Qu et al., 2025a), GPS proposes generalizable prompt predictive model (PPM) to estimate difficulty and track LLM evolution. The comprehensive prompt batch acquisition accounts for intermediate difficulty prioritization and batch-level diversity, improving RL post-training efficiency while mitigating PPM overfitting. where ρi,t = πθ(yi,tτ,yi,<t) (yi,tτ,yi,<t) is the importance ratio, and πref is fixed reference policy. The KL divergence term penalizes deviation from πref, with β controlling the regularization strength. The group-relative advantage for the i-th response is computed as πθold ˆAi = 2.3 Online Prompt Selection for RLVR mean({rτ rτ }k std({rτ }k i=1) i=1) . (3) RLVR enhances LLM reasoning but comes with high computational costs, driving the need for prompt curation. Previous studies show that prompts affect optimization unevenly. For algorithms like GRPO, rewards with zero variance can lead to vanishing policy gradients (Yu et al., 2025; Zhang et al., 2025b). In contrast, prompts of intermediate difficulty provide more informative training signals (Chen et al., 2025; Bae et al., 2025; Zeng et al., 2025b). To mitigate ineffective prompts and enhance training, online prompt selection methods (Yu et al., 2025; Qu et al., 2025a; Zhang et al., 2025a; Zheng et al., 2025b) have been proposed to adaptively choose prompt batches during optimization. The key to the prompt evaluation is to precisely estimate the success rate γτt under an arbitrary τt and θt at the t-th iteration. Evaluation-based selection. Methods such as Dynamic Sampling (DS) (Yu et al., 2025) and SPEED-RL (Zhang et al., 2025a) over-sample and evaluate candidate set ˆB of size ˆB with real model rollouts, and then select the batch: (cid:110) = τ ˆB (cid:12) (cid:12) std({rτ }k i=1) > 0 (cid:111) . (4) This approach ensures informative prompts but exhausts computations from large-scale exact prompt evaluations (Zheng et al., 2025b). Prediction-based selection. To circumvent extra prompt evaluation, several methods (Zhang et al., 2025b; Zheng et al., 2025b; Gao et al., 2025) adopt the strategy of predicting prompt difficulty. This requires constructing series of PPMs as p(γτ τ, θt) iteratively. Take MoPPS (Qu et al., 2025a) as example, it employs Beta posterior for each prompt and utilizes specific history data to estimate success rates. Nevertheless, the PPM in MoPPS hardly keeps pace with the evolving model state θt and treats prompts independently. These raise the generalization issue due to the failure of sharing information across prompts and update delay degrades the success rate accuracy."
        },
        {
            "title": "3 Method",
            "content": "This section presents GPS to construct generalizable PPM, along with principled acquisition criterion for efficient RL post-training. Meanwhile, we show that test-time compute allocation can benefit from the online learned PPM. 3.1 The Curse of Prompt-Specific PPMs The family of prompt-specific PPMs (Zhang et al., 2025b; Zheng et al., 2025b; Qu et al., 2025a) estimates γτ prompt-specific histories, maintaining independent posteriors per prompt: with 3 Algorithm 1: Generalizable Predictive Prompt Selection (GPS) Input: Prompt pool = {τi}N parameters (ψ1, η1, ϕ1); Diversity weight λ; Training steps T. i=1; Batch size B; LLM πθ1 with parameters θ1; Generalizable PPM with Output: LLM πθ Initialize history H0 ; for = 1 to do // Prompt Difficulty Prediction foreach ˆτ do Estimate ˆγ ˆτ via Eq. (10) using Ht1; ; // Prompt Batch Selection Initialize selected batch partial while partial < do Select prompt τ via Eq. (14); partial partial ; foreach τ do partial {τ}; = {rτ t,j}k j=1 ; j=1 using πθt ; Generate responses yτ Compute rewards rτ = {yτ t,j}k // Policy Update in RLVR Update LLM θt θt+1 with {(τ, yτ // History and PPM Update Ht Ht1 {(τ, rτ )}τT Update (ψt, ηt, ϕt) by maximizing ELBO in Eq. (8); )}τT , rτ ; ; p({γτ }τT Ht1) = τT p(γτ τ, Hτ t1). (5) While this formulation is computationally efficient, it raises significant concerns when handling large-scale prompts and rapidly changing LLM dynamics: Cross-Prompt Generalization Bottleneck. Independent modeling in Eq. (5) implicitly assumes that difficulty is non-transferable, precluding information sharing between related prompts. This encounters cold-start bottleneck. Reliable estimates are only available for frequently sampled prompts. In highdimensional prompt spaces, independent estimators often collapse toward uninformative priors, failing to leverage the semantic structure of the prompt. Accumulated Estimation Error from Delayed Reward Signals. Standard approaches often assume local stationarity, extrapolating future performance from past difficulty in the absence of sufficient promptspecific observations. However, because the underlying model evolves continuously during training, prompt difficulty is inherently non-stationary. Without mechanism to model these dynamics, predictors tend to suffer from lagged adaptation, failing to perceive performance shifts before they occur. Moreover, such methods also fail to generalize to unseen prompts after training, as summarized in Table 1. Note that the prompt difficulty depends jointly on the prompt τ itself and the evolving θt, while the accumulated history Ht1 reflects the change of LLM dynamics. Hence, distinguished from Eq. (5), this work turns to more natural way of estimating the prompt difficulty, which uses the complete history Ht1 in modeling, i.e., (6) For an arbitrary prompt τ, this approach can circumvent delayed reward signals and potentially improve predictive accuracy with the help of the optimization signals from other relevant prompts. τ, Ht1), τ . p(γτ 3.2 Generalizable Prompt Predictive Model Building on the previous analysis, we investigate how to construct generalizable PPMs. The basic idea is to leverage optimization history and compress difficulty-aware information into the latent variable shared across prompts. Generative Modeling of PPMs with Latent Variables. Here, we treat the prompt feedback and associated optimization signals at the t-th step as the conditional generative result from previous accumulated experience Ht1. 4 Table 1: Comparison of typical online prompt selection methods. Method DS (Yu et al., 2025) MoPPS (Qu et al., 2025a) GPS (Ours) Prompt Evaluation Exact Predictive Predictive Difficulty Modeling Prompt Batch Selection - Threshold Filtering Independent Max-Sum (Top-B) Compute Efficiency Dataset Scalability Test-time Computation Global Max-Sum Diversity Specifically, global latent variable zt, referred to as the difficulty context, is introduced to provide foresight of generalizable prompt difficulty information from Ht1. Then the marginal distribution of the whole optimization history can be factorized into: p(H0:T) = p(H0) (cid:90) t=1 pψ(Htzt)pη(ztHt1)dz1:T, (7) where pη(ztHt1) is conditional prior, and pψ(Htzt) τt,i, zt) is shared likelihood model for prompt difficulty estimation. The function of global latent variables is akin to classical conditional generative modeling (Sohn et al., 2015; Garnelo et al., 2018) to represent the context that allows information to transfer across prompts, while the evolving prior enables predictions to adapt to the non-stationary optimization trajectory. i=1 pψ(γ = τt,i Variational Inference. To handle the intractability of the marginal likelihood, we introduce variational posterior qϕ(ztHt). Applying Jensens inequality yields the evidence lower bound (ELBO): ln p(HtHt1) = ln qϕ(ztHt) (cid:20) pη(ztHt1)pψ(Htzt) qϕ(ztHt) (cid:2) ln pψ(Htzt)(cid:3) DKL (cid:21) qϕ(ztHt) (cid:2)qϕ(ztHt)pη(ztHt1)(cid:3) = LELBO(ψ, ϕ, η) (8) Maximizing the ELBO jointly optimizes: (i) the decoder pψ, which captures the shared mapping from context to difficulty; (ii) the encoder qϕ, which extracts current contextual information; and (iii) the history-dependent prior pη, which acts as temporal summarizer of the optimization history. Implementation details are deferred to Appendix D.4. Predictive Distribution. The difficulty of candidate prompt τt at step is quantified via the predictive distribution p(γτt, Ht1) = (cid:90) pψ(γτt, zt)pη(ztHt1)dzt. In practice, we use Monte Carlo approximation: ˆγτt 1 m=1 pψ(γτt, (m) ), with (m) pη(Ht1). (9) (10) This mechanism enables the model to infer the difficulty of even unvisited prompts, while the time-varying prior pη ensures the predictions remain synchronized with the evolving policy. Further, we provide theoretical justification for global difficulty modeling, suggesting that conditioning on full optimization history yields strictly lower predictive mean squared error than independent estimators based solely on per-prompt observations. formal proof is provided in Appendix C. Theorem 3.1 (Better Prediction with Shared History). Let ˆγτ,ind := E[γτ only on prompt-specific history, and ˆγτ,shr := E[γτ history, where Hτ t1] be the optimal predictor based Ht1] be the optimal predictor based on the full optimization )2] admits the orthogonal decomposition t1 Ht1. The prediction risk R( ˆγ) := E[( ˆγ γτ Hτ R( ˆγτ,shr) = R( ˆγτ,ind) C(τ), (11) where C(τ) = non-redundant predictive information about γτ (cid:104)(cid:0) ˆγτ,shr ˆγτ,ind(cid:1)2(cid:105) beyond Hτ t1. 0 is the estimation gap. Moreover, C(τ) > 0 if and only if Ht1 provides 3.3 Difficulty-Diversity Unified Prompt Batch Selection Prior methods (Chen et al., 2025; Qu et al., 2025a; Gao et al., 2025) select prompts based on independent promptwise scoring, without considering batch-level structure. Instead, we formulate prompt selection as batch-level optimization problem that explicitly balances prompt-wise difficulty with batch-level diversity, seeking informative and diverse prompt batches. 5 Figure 2: Spearmans rank correlation and p-value during training between predicted prompt difficulty and empirical success rate. Unified Batch Utility. Specifically, at training step t, our objective is to select subset following batch-level utility: that maximizes the arg max T U(T u( ˆγτ ) ) = τT (cid:124) (cid:125) (cid:123)(cid:122) Difficulty Utility +λ ; D(T (cid:123)(cid:122) (cid:124) History-Anchored Diversity t1) (cid:125) , (12) where u( ˆγτ ) measures the informativeness of an individual prompt, D() promotes diversity at the batch level, and λ > 0 controls the trade-off. Following common practice (Xu et al., 2025; Sun et al., 2025), we prioritize prompts with intermediate difficulty and adopt u( ˆγτ 0.5)2. ) = ( ˆγτ The diversity term encourages the batch to span broad and non-repetitive region of the prompt space: D(T ; t1) = dist(τi, τj) τi,τjT (cid:124) (cid:125) (cid:123)(cid:122) Intra-batch Dispersion dist(τ, τ) + τT (cid:124) τT t1 (cid:123)(cid:122) Inter-step Exploration (cid:125) , (13) where dist(, ) is distance measure. This provides dual benefits: it (i) improves LLM training by reducing redundant signals (Jin et al., 2025; Qu et al., 2025b) and (ii) mitigates overfitting of the PPM by encouraging exposure to broader region of the prompt space, as discussed in Appendix B.2. Greedy Search Prompt Batch. Maximizing U(T ) is max-sum diversification problem, which is NPhard (Borodin et al., 2017). We adopt standard greedy approximation (Borodin et al., 2017; Wang et al., 2023), which iteratively selects the prompt with the largest marginal gain τ = arg max {τ}) U(T partial (14) )(cid:3), (cid:2)U(T partial t τT partial where partial computational efficiency, enabling scalable prompt selection for large-scale LLM training. denotes the batch constructed so far. This greedy heuristic effectively balances selection quality with 3.4 Extension to Test-Time Computation Allocation Recent work shows that uniformly allocating test-time computation leads to inefficient budget usage (Damani et al., 2024; Wang et al., 2025c). Considering Best-of-N sampling (Cobbe et al., 2021; Lightman et al., 2023), the marginal gain of one extra sample to prompt τ is k(τ) = pass@(k + 1)(τ) pass@k(τ) = (1 γτ)kγτ, (15) which motivates difficulty-aware allocation. Fortunately, unlike prompt-specific modeling, generalizable PPM captures global and transferable notion of prompt difficulty, making it feasible to generalize across unseen prompts for guiding test-time computation allocation. Specifically, following the allocation strategy of (Damani et al., 2024), we partition test prompts into three bins based on predicted difficulty and allocate lower budgets to easy or likely-unsolvable prompts, while assigning more computation to promising prompts that are challenging yet solvable (details in Appendix D.5). This reuse does not introduce additional training and can offer improvements over uniform allocation under fixed budget, naturally complementing the training-time framework."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate GPS on representative reasoning tasks: mathematical and logical reasoning, using large-scale benchmarks. To assess the generality, we experiment with diverse set of LLM backbones spanning different 6 Figure 3: Training curves of GPS and baselines across different scenarios and backbone models versus training steps. DS serves an oracle baseline with respect to training steps, but incurs substantially higher rollout costs. Training curves plotted against the number of rollouts are provided in Fig. 7. Table 2: Evaluation on mathematics benchmarks. + denotes finetuning with the corresponding method. Avg. reports the average accuracy, and Runtime indicates total training time. Bold and underlined values indicate the best and second-best results, respectively. In-Distribution Out-of-Distribution Method DSR-1.5B +Uniform +PCL +GRESO +MoPPS +DS (Oracle) +GPS (Ours) DSR-7B +Uniform +PCL +GRESO +MoPPS +DS (Oracle) +GPS (Ours) MATH500 Olympiad. Minerva. AMC23 AIME24 Avg@32 Avg@32 Avg@ Avg@1 Avg@4 Avg. MMLU-Pro ARC-c GPQA Avg@8 Avg@8 Avg@8 Avg. Runtime 77.8 86.2 84.8 86.2 86.2 87.2 88.0 87.2 91.6 93.2 91.8 92.0 93.2 93.2 36.6 49.0 49.6 50.6 49.0 51.0 51.3 47.5 57.9 58.0 58.2 58.3 60.0 62.1 25.6 30.8 33.0 30.7 29.9 30.4 31.3 35.8 37.5 38.4 39.3 39.1 40.4 39. 57.7 76.1 77.7 77.1 77.8 79.2 78.1 76.5 89.5 90.1 89.7 89.5 90.5 90.5 20.8 32.4 34.2 31.5 34.0 34.7 33.8 38.8 50.9 52.8 49.8 50.8 51.0 51.8 43.7 54.9 55.8 55.2 55.4 56.5 56.5 57.1 65.5 66.5 65.8 65.9 67.0 67. 21.2 22.3 24.0 24.7 21.4 24.5 23.3 50.5 49.8 50.6 52.2 52.4 52.0 52.8 43.1 45.4 46.3 46.7 44.6 46.7 46.9 76.1 74.1 74.4 77.1 76.7 78.2 79.7 22.8 27.5 28.5 26.4 27.5 26.8 30.4 19.2 17.3 22.4 25.0 23.4 19.6 22. 29.0 31.7 33.0 32.6 31.2 32.7 33.5 48.6 47.1 49.1 51.4 50.9 49.9 51.5 - 16h 17h 27h 17h 30h 16h - 40h 50h 53h 42h 77h 49h model sizes, including both base models and distilled models. For RLVR algorithm, we primarily adopt the GRPO algorithm within the verl framework (Sheng et al., 2024), though GPS is compatible with other algorithms as shown in Sec. 4.3.1. Model performance is measured by test accuracy, reported as the average pass@1 over multiple rollouts per problem, where the number of generations varies across benchmarks. Mathematical Reasoning. We train on the DeepScaler dataset (Luo et al., 2025b), which contains 40.3k competitionlevel math problems. Following prior work (Luo et al., 2025b; Qu et al., 2025a), we use DeepSeek-R1 distilled models: R1-Distill-1.5B (DSR-1.5B) and R1-Distill-7B (DSR-7B). Evaluation is conducted on suite of standard mathematical benchmarks, namely AIME24, AMC23, MATH500 (Lightman et al., 2023), Minerva Math (Minerva.) (Lewkowycz et al., 2022), and OlympiadBench (Olymp.) (He et al., 2024). To assess out-of-distribution generalization, we evaluate on general reasoning benchmarks: MMLU-Pro Wang et al. (2024), ARC-c Clark et al. (2018), and GPQA-diamond (GPQA) Rein et al. (2024). Logical Reasoning. We adopt the Countdown Number Game, combinatorial arithmetic reasoning task that requires composing given numbers with basic operations to match target value. Training is performed on 20k-instance subset of the Countdown-34 (CD34) dataset (Pan et al., 2025). Models are evaluated on both CD34 and more challenging variant, Countdown-4 (CD4). We use two base LLMs, Qwen3-4B-Base and Qwen3-8B-Base (Yang et al., 2025), and one instruct LLM Llama-3.2-3B-Instruct Grattafiori et al. (2024) in Appendix E.7. Baselines. We compare GPS with several representative sampling strategies: (1) Uniform samples prompts uniformly; (2) MoPPS (Qu et al., 2025a) maintains prompt-specific Beta posteriors to estimate difficulty from historical observations; (3) PCL (Gao et al., 2025) estimates prompt difficulty using LLM; (4) GRESO (Zheng et al., 2025b) maintains per-prompt historical reward statistics to guide probabilistic filtering; and (5) Dynamic Sampling (DS) (Yu et al., 2025), which oversamples prompts and filters out uninformative ones using exact evaluation. We consider DS as an Oracle baseline since it relies on real evaluation. Our focus is on reducing computational overhead relative to DS rather than outperforming it in accuracy. Additional implementation details are provided in Appendix D, together with extended experimental results in 7 Figure 4: Test-time computation allocation results across benchmarks. pass@k versus the number of generated samples is shown, with insets reporting Spearmans rank correlation ρ (* < 0.05, ** < 0.01, *** < 0.001) and other statistical metrics. Appendix and prompt examples in Appendix F. 4.2 Main Results 4.2.1 Reliable Difficulty Prediction To evaluate the quality of difficulty prediction, we adopt Spearman rank correlation coefficient (Sedgwick, 2014) ρ, which measures the strength of monotonic relationship between two sequences and is invariant to monotonic transformations, making it suitable for assessing relative difficulty ordering. Formally, it is defined as: (cid:16) (cid:17) , cov ρ = rank( ˆΓB), rank((cid:101)ΓB) σrank( ˆΓB ) σrank((cid:101)ΓB ) where ˆΓB = { ˆγτ}τT and (cid:101)ΓB = { (cid:101)γτ}τT denote the predicted and empirical success rates for the batch B, and rank() returns the rank ordering. Furthermore, we report the p-value under the null hypothesis testing that ˆγτ and (cid:101)γτ are independent to assess statistical significance. As shown in Fig. 2, GPS rapidly learns to predict prompt difficulty within only few optimization steps, well before completing full epoch, achieving extremely low p-values. The correlation steadily improves as histories accumulate. This validates that the proposed generalizable PPM enables reliable difficulty prediction with crossprompt generalization. Furthermore, Fig. 6 shows that GPS attains higher prediction quality than MoPPS and consistently higher effective sample ratio than Uniform, which can be attributed to our proposed comprehensive batch acquisition strategy. (16) 4.2.2 Efficient and Improved RLVR To examine whether improved prompt selection translates into efficient RLVR training, we compare GPS with baselines across diverse scenarios and backbone models. Fig. 3 shows training curves with respect to training steps, while Tables 2 and 4 summarize the final performance. Overall, GPS enables both more efficient and effective RLVR training. Compared to Uniform, GPS consistently accelerates training, yielding 1.42.0 speedup in terms of training steps. It also delivers average gains of 1.61.9 points on mathematical tasks and 4.15.7 points on logical tasks. These improvements come at minimal additional cost: the lightweight PPM introduces negligible overhead (Appendix E.1), and the modest increase in runtime is due to longer response generation  (Fig. 12)  . Compared to DS, GPS achieves comparable performance while substantially reduced cost, requiring up to 69% fewer rollouts  (Fig. 7)  and reducing training time by 28%47%. Benefiting from the design of generative PPM and unified prompt batch selection, GPS outperforms prediction-based baselines. Moreover, its strong performance on out-of-distribution benchmarks indicates improved general reasoning ability. 4.2.3 Test-Time Generalization and Efficiency To investigate whether the learned generalizable PPM generalizes to test-time prompts, we conduct offline batch evaluations on logical and mathematical benchmarks. Fig. 4 shows that the PPM retains statistically significant correlation ρ with empirical success rates on most unseen test benchmarks. Building on these predictions, we 8 Figure 5: (a) Training curves on Countdown with PPO and Reinforce++. GPS is compatible with both algorithms and consistently outperforms Uniform. (b) Ablation study on Countdown, including removing history-anchored diversity (w/o hisdiv), removing inter-step exploration only (w/o his), and replacing the PPM with deterministic PPM without latent variables (w/o z). evaluate difficulty-guided computation allocation and compare against Default (fixed samples per prompt), and Random (the same allocation configurations with random difficulty predictions). As shown in Fig. 4, the proposed method yields up to 3.2% relative improvement over Default at fixed budget, or reduces computation by up to 36.4% with no loss in performance. Notably, this introduces no additional computational overhead. The results suggest that training-time difficulty prediction can be reused for test-time, enabling coherent trainingtest pipeline. 4.3 Additional Analysis 4.3.1 Algorithm Compatibility Although primarily evaluated with GRPO, GPS is compatible with other RLVR algorithms. We integrate it with two alternative algorithms, PPO (Schulman et al., 2017) and Reinforce++ (Hu, 2025) on Countdown. As shown in Fig. 5(a), GPS consistently improves training efficiency and final performance over Uniform under both algorithms. Notably, GPS is compatible with PPO despite its single-response generation per prompt. This is enabled by the generalizable predictive model, in contrast to prior evaluation-based methods such as DS, which rely on multiple responses and are therefore inapplicable in this regime. These results position GPS as broadly applicable solution for enhancing sample efficiency across diverse RLVR pipelines. GPS is also applicable to continuous-reward settings (see Appendix E.5). 4.3.2 Ablation Studies We conduct ablation studies to evaluate the contributions of key components in GPS, including history-anchored diversity and the latent difficulty context in the generative PPM. As shown in Fig. 5(b), removing history-anchored diversity (Ours w/o hisdiv) causes substantial performance drop, while ablating only the inter-step exploration term (Ours w/o his) leads to smaller decline. This indicates that both intra-batch dispersion and inter-step exploration play important roles in effective prompt batch selection. Replacing the generative PPM with deterministic PPM that directly maps prompts to success rates, i.e., removing the latent difficulty context (Ours w/o z), also degrades performance, highlighting the benefit of modeling shared latent difficulty context for cross-prompt generalization and policy evolution adaptation. Overall, these results confirm that each design component contributes meaningfully. Additional results are provided in Appendix E.6. 4.3.3 Hyperparameters The effects of the diversity weight λ and the candidate batch size are evaluated in Appendix E.9 and E.10."
        },
        {
            "title": "5 Conclusion",
            "content": "This work presents Generalizable Predictive Prompt Selection to accelerate RL post-training of large reasoning models. The small generative PPM is effective enough in prompt difficulty estimate, allowing for cross-prompt generalization and adaptation to policy evolution. The acquisition of difficulty and batch-level diversity integration helps improve LLM and PPM training. Additionally, the learned PPM aids in test-time computation allocation. Discussion. This work first verifies the feasibility of efficiently steering LLMs RL post-training with small generative PPMs. Future work can be exploring more refined generative modeling for online prompt selection. In 9 addition, this work has preliminarily revealed the opportunity of PPM-guided test-time computation allocation; more advanced designs may further enhance its effectiveness."
        },
        {
            "title": "References",
            "content": "Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. Allan Borodin, Aadhar Jain, Hyun Chul Lee, and Yuli Ye. Max-sum diversification, monotone submodular functions, and dynamic updates. ACM Transactions on Algorithms (TALG), 13(3):125, 2017. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piche, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint arXiv:2505.14970, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, and Jacob Andreas. Learning how hard to think: Input-adaptive allocation of lm computation. arXiv preprint arXiv:2410.04707, 2024. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219, 2025. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning via reinforcement learning. arXiv preprint arXiv:2504.05185, 2025. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025. URL https: //arxiv.org/abs/2508.15260. Zhaolin Gao, Joongwon Kim, Wen Sun, Thorsten Joachims, Sid Wang, Richard Yuanzhe Pang, and Liang Tan. Prompt curriculum learning for efficient llm post-training. arXiv preprint arXiv:2510.01135, 2025. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasonerzero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Renren Jin, Pengzhi Gao, Yuqi Ren, Zhuowen Han, Tongxuan Zhang, Wuwei Huang, Wei Liu, Jian Luan, and Deyi Xiong. Revisiting entropy in reinforcement learning for large reasoning models, 2025. URL https: //arxiv.org/abs/2511.05993. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, et al. Deepcoder: fully open-source 14b coder at o3-mini level. Notion Blog, 2025a. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025b. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR, 2025. D. Lee Miller. Wordllama: Recycled token embeddings from large language models, 2024. URL https: //github.com/dleemiller/wordllama. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/JiayiPan/TinyZero, 2025. Accessed: 2025-01-24. Yun Qu, Qi Wang, Yixiu Mao, Vincent Tao Hu, Bjorn Ommer, and Xiangyang Ji. Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models?, 2025a. URL https://arxiv.org/abs/ 2507.04632. Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, and Xiangyang Ji. Fast and robust: Task sampling with posterior and diversity synergies for adaptive decision-makers in randomized environments. arXiv preprint arXiv:2504.19139, 2025b. Vinod Raman, Hilal Asi, and Satyen Kale. Abon: Adaptive best-of-n alignment. arXiv preprint arXiv:2505.12050, 2025. Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason Lee, and Sanjeev Arora. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Philip Sedgwick. Spearmans rank correlation coefficient. Bmj, 349, 2014. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015. Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, and Huan Zhang. Improving data efficiency for llm reinforcement fine-tuning through difficulty-targeted online data selection and rollout replay. arXiv preprint arXiv:2506.05316, 2025. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025a. Yanhao Wang, Michael Mathioudakis, Jia Li, and Francesco Fabbri. Max-min diversification with fairness constraints: Exact and approximation algorithms. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pp. 9199. SIAM, 2023. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Youkang Wang, Jian Wang, Rubing Chen, Tianyi Zeng, Xiao-Yong Wei, and Qing Li. Optpo: Optimal rollout allocation for test-time policy optimization, 2025c. URL https://arxiv.org/abs/2512.02882. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chainof-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. Not all rollouts are useful: Down-sampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818, 2025. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. URL https://arxiv.org/abs/2504.14945. 12 An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025a. Yongcheng Zeng, Zexu Sun, Bokai Ji, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Haifeng Zhang, Xu Chen, and Jun Wang. Cures: From gradient analysis to efficient curriculum learning for reasoning llms, 2025b. URL https://arxiv.org/abs/2510.01037. Ruiqi Zhang, Daman Arora, Song Mei, and Andrea Zanette. Speed-rl: Faster training of reasoning models via online curriculum learning, 2025a. URL https://arxiv.org/abs/2506.09016. Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, et al. Srpo: cross-domain implementation of large-scale reinforcement learning on llm. arXiv preprint arXiv:2504.14286, 2025b. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts, 2025b. URL https://arxiv.org/abs/2506.02177. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. slime: An llm post-training framework for rl scaling. https://github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv."
        },
        {
            "title": "Appendix Overview",
            "content": "This appendix provides supplementary discussions, theoretical analyses, and experimental details that support the main results. The appendix is organized as follows: Appendix (Related Works): reviews related works on RL post-training of LLMs and online prompt selection for RLVR. Appendix (Additional Discussions): offers clarifications on the contributions of the proposed method and further discusses the importance of history-anchored diversity, as well as the relationship between semantic representations and prompt difficulty. Appendix (Theoretical Proof): presents detailed theoretical proof. Appendix (Implementation Details): provides comprehensive implementation details, including tasks, models, training details, and the realization of the generalizable prompt predictive model, as well as its extension to test-time computation allocation. Appendix (Extended Experimental Results): reports additional experimental results and analyses, including computational complexity analysis, extended evaluations, ablation studies, hyperparameter sensitivity analyses, training dynamics, and evaluations across different LLM families. Appendix (Data Examples): presents representative data examples used in the experiments."
        },
        {
            "title": "A Related Works",
            "content": "RL Post-Training of LLMs. Reinforcement learning has become central technique for aligning pretrained LLMs with desired behaviors and task objectives. Early successes are largely driven by Reinforcement Learning with Human Feedback (RLHF), which improves instruction following, safety, and alignment through preferencebased optimization (Dong et al., 2024; Dai et al., 2023; Sun et al., 2023; Zheng et al., 2023). More recent progress highlights Reinforcement Learning with Verifiable Rewards (RLVR), where reward signals are automatically verified, leading to substantial gains in structured reasoning domains such as mathematics and programming (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025; Chu et al., 2025; Pan et al., 2025; Luo et al., 2025a;b). From an algorithmic perspective, Proximal Policy Optimization (PPO) (Schulman et al., 2017) remains standard choice, while Group Relative Policy Optimization (GRPO) (Shao et al., 2024) reduces computational overhead by eliminating the value network and estimating advantages through group-normalized rewards. Building on these foundations, growing body of work focuses on stabilizing training, reducing bias, lowering variance, and improving sample efficiency (Yuan et al., 2025; Yue et al., 2025; Liu et al., 2025b; Yu et al., 2025; Kazemnejad et al., 2024; Hu, 2025; Zheng et al., 2025a). In parallel, large-scale empirical studies demonstrate the effectiveness of RL post-training across model sizes and application domains (Luo et al., 2025b; Dang & Ngo, 2025; Luo et al., 2025a; Zeng et al., 2025a; Meng et al., 2025; Xu et al., 2024), supported by increasingly mature systems for scalable RL training (Sheng et al., 2024; Zhu et al., 2025; Wang et al., 2025a). Online Prompt Selection for RLVR Beyond algorithmic improvements, data curation have emerged as critical factors in determining the efficiency of RL post-training. line of offline approaches pre-select training prompts based on heuristics such as difficulty, diversity, or solution length (Ye et al., 2025; Li et al., 2025; Wen et al., 2025; Hu et al., 2025; Yang et al., 2024; Fatemi et al., 2025; Wang et al., 2025b). While effective in reducing training cost, these methods typically require additional preprocessing and remain static throughout training, making them fail to adapt to evolving LLM policy (Qu et al., 2025a; Gao et al., 2025). To address this limitation, recent work explores online prompt selection that adapt to the current policy. Some methods perform evaluation-based filtering by discarding uninformative prompts or prioritizing moderate difficulty (Yu et al., 2025; Liu et al., 2025a; Cui et al., 2025; Meng et al., 2025; Bae et al., 2025), often at the expense of additional LLM evaluations. An alternative direction is prediction-based prompt selection, which seeks to estimate prompt difficulty without explicit rollout-based evaluation (Zhang et al., 2025b; Zheng et al., 2025b; Gao et al., 2025; Qu et al., 2025a; Zeng et al., 2025b; Chen et al., 2025). representative method is MoPPS (Qu et al., 2025a), which models each prompt independently using prompt-specific Beta posterior to estimate its success rate based on historical observations. While this design avoids additional inference cost, it does not explicitly account for the continually evolving model state during online optimization, nor does it allow information sharing across prompts. As result, prompt-specific modeling may suffer from lagged adaptation and limited generalization, particularly in large prompt pools where many prompts are sparsely observed."
        },
        {
            "title": "B Additional Discussions",
            "content": "B.1 Contribution Clarification Efficiently guiding data selection for LLM optimization is fundamentally challenging problem with substantial practical significance. In RL post-training, evaluating candidate prompts using the target model itself is often prohibitively expensive, which makes evaluation-based strategies difficult to scale. As result, designing mechanisms that can steer LLM optimization using limited auxiliary computation remains an open and practically important challenge. Prior work has predominantly approached this problem through lightweight prompt-specific modeling or by leveraging large models to assess prompt difficulty. In contrast, this work represents an early attempt to use shared and lightweight prompt predictive model to guide the optimization of much larger LLM. This small-steerlarge paradigm, in which small predictive model captures transferable difficulty signals to drive LLM training, constitutes key conceptual contribution and opens promising design space for scalable optimization. We emphasize that the specific choice of conditional generative model for the prompt predictive model is not intended as primary technical contribution. Rather, it serves as concrete instantiation of the broader idea of shared, history-conditioned modeling. Our framework does not rely on the optimality of particular generative architecture, and alternative formulations could be explored within the same paradigm. Empirically, we find that even relatively simple generative predictive model is sufficient to extract useful signals for prompt selection, highlighting the potential of this direction rather than claiming architectural optimality. B.2 Discussion on the Importance of History-Anchored Diversity The motivation for history-anchored diversity arises from the coupled nature of prompt selection, PPM learning, and LLM optimization. PPM predictions parameterize the acquisition criterion, which shapes the prompt distribution used for LLM optimization, while the optimization process in turn generates new feedback that is used to update the PPM. This creates an intrinsic feedback loop between sampling, prediction, and policy learning, analogous to chicken-and-egg problem. Without explicit diversity regularization, the acquisition criteria tends to over-exploit narrow subset of prompts that are estimated to be most informative. While this can yield short-term gains, it may result in progressively concentrated training distribution for the PPM, increasing the risk of overfitting and weakening its ability to generalize difficulty estimates to underexplored regions of the prompt space. As the predictor becomes increasingly biased toward frequently sampled prompts, such miscalibration can in turn reduce the diversity of training signals received by the LLM, potentially reinforcing sampling collapse and adversely affecting the performance of joint optimization. History-anchored diversity explicitly counteracts this effect by encouraging coverage across both the current batch and historical selections. By maintaining more balanced and diverse prompt distribution, it improves the effectiveness and robustness of the PPM. This, in turn, supports more effective LLM optimization through sustained exposure to diverse and appropriately challenging prompts. In Appendix E.6, the ablation results and the observed increase in the effective sample ratio provide empirical evidence for this role. We note that history-anchored diversity is introduced as general regularization principle rather than specific instantiation, and alternative diversity measures or regularizers could be incorporated within the same framework. B.3 Discussion on Semantic Representations and Prompt Difficulty Prompt difficulty is naturally determined jointly by the prompt itself and the evolving policy model. In our implementation, these two factors are explicitly considered: the former is represented by fixed semantic embedding τ, while the latter is captured by time-evolving latent context zt that summarizes the optimization history. Formally, prompt difficulty γτ is modeled as (τ, zt) . To enable cross-prompt generalization, the framework implicitly relies on weak structural assumption: under fixed LLM policy, semantic representations provide coarse organization of the prompt space in which difficulty is not entirely arbitrary. Importantly, this does not require semantic similarity to be strong or deterministic predictor of difficulty, nor does it imply that prompt embeddings alone can determine difficulty. Rather, semantic representations serve as shared coordinate system that supports information transfer across prompts. Empirically, on DeepScaler, we conduct simple baseline that estimates difficulty by weighting success rates using embedding similarity achieves correlation of approximately 0.2 under fixed policy. While this level of correlation is insufficient for accurate difficulty prediction in isolation, it demonstrates that semantic structure provides non-trivial signals. At the same time, its weakness highlights the necessity of incorporating history-dependent difficulty context, motivating the use of the dynamically updated latent variable zt. Overall, semantic representations in our framework act as stable but coarse anchor for cross-prompt generalization, 15 while the latent context zt captures the dominant, time-varying factors induced by model optimization. Their combination enables effective prompt difficulty modeling. Finally, we emphasize that the specific choice of semantic embedding (e.g., WordLLaMA) is practical instantiation rather than conceptual requirement; the framework itself is agnostic to the particular embedding model used."
        },
        {
            "title": "C Theoretical Proof",
            "content": "Theorem 3.1 (Better Prediction with Shared History). Let ˆγτ,ind := E[γτ only on prompt-specific history, and ˆγτ,shr := E[γτ history, where Hτ t1] be the optimal predictor based Ht1] be the optimal predictor based on the full optimization )2] admits the orthogonal decomposition t1 Ht1. The prediction risk R( ˆγ) := E[( ˆγ γτ Hτ R( ˆγτ,shr) = R( ˆγτ,ind) C(τ), (11) where C(τ) = non-redundant predictive information about γτ (cid:104)(cid:0) ˆγτ,shr ˆγτ,ind(cid:1)2(cid:105) beyond Hτ t1. 0 is the estimation gap. Moreover, C(τ) > 0 if and only if Ht1 provides Proof. The proof follows standard argument by interpreting conditional expectation as an orthogonal projection in the L2 Hilbert space of random variables. We include it to formalize the role of shared history conditioning in our framework. Step 1: Orthogonality and MSE decomposition. We decompose the independent estimation error as: Let ˆγτ,ind = (γτ γτ ˆγτ,shr) + ( ˆγτ,shr ˆγτ,ind). ϵ := γτ ˆγτ,shr, δ := ˆγτ,shr ˆγτ,ind. Since ˆγτ,shr = E[γτ Ht1], we have E[ϵ Ht1] = E[γτ Moreover, δ is fully determined by Ht1, so we can write ˆγτ,shr Ht1] = 0. E[ϵ δ] = E(cid:2)E[ϵ δ Ht1](cid:3) = E(cid:2)δ E[ϵ Ht1](cid:3) = 0. The Pythagorean identity ϵ + δ2 = ϵ2 + δ2 directly yields R( ˆγτ,ind) = R( ˆγτ,shr) + C(τ), where C(τ) = E[δ2] 0. (17) (18) (19) Step 2: Risk reduction. The term C(τ) 0 represents the variance of the shared predictor unexplained by prompt-specific history. It is strictly positive if and only if ˆγτ,shr cannot be determined solely from prompt-specific feedback, i.e., when the full optimization history provides additional predictive information about γτ . Remark C.1 (Realization via Global Latent Context). The optimal shared estimator ˆγτ,shr introduced above is generally intractable, as it requires conditioning on the full, high-dimensional optimization history Ht1. Our framework therefore adopts practical surrogate by encoding global difficulty context into time-evolving latent variable zt. Specifically, the history-dependent prior pη(zt Ht1) is designed to capture coarse-grained properties of the optimization trajectory, while the shared decoder pψ(γ τ, zt) enables experience sharing across prompts. This construction is motivated by the theoretical advantage of shared conditioning, without assuming that the latent representation fully characterizes the optimization history. In practice, even coarse contextual signal can be sufficient to provide reasonably stable relative ordering for prompt selection, which is the primary objective of our framework. Empirically, we observe consistent correlation between predicted and observed prompt difficulty, supporting this behavior."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Tasks D.1.1 Mathematics Reasoning Training Dataset. Following Luo et al. (2025b); Gao et al. (2025), we train models on the DeepScaler dataset (Luo et al., 2025b), which consists of 40,315 competition-level mathematics problems. We use the public version hosted at https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset. 16 Evaluation Benchmarks. We evaluate mathematical reasoning performance on suite of benchmarks, including AIME24, AMC23, MATH500 (Lightman et al., 2023), Minerva Math (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024), using the datasets hosted at https://huggingface.co/datasets/math-ai. Training curves report the average performance across all math benchmarks. To further assess generalization beyond the training distribution, we additionally evaluate on three general reasoning benchmarks: MMLU-Pro (Wang et al., 2024), ARC-c (Clark et al., 2018), and GPQA-diamond (GPQA) (Rein et al., 2024), using datasets provided by LUFFY (Yan et al., 2025). Reward Function. Following the default configuration in verl (Sheng et al., 2024), we adopt binary reward scheme that assigns reward of 1 to correct responses and 0 otherwise. D.1.2 Logical Reasoning Training Dataset. We consider the Countdown Number Game, symbolic reasoning task that requires combining given numbers via basic arithmetic operations to reach target value. For training, we use 20,000problem subset of the Countdown-34 dataset from https://huggingface.co/datasets/Jiayi-Pan/ Countdown-Tasks-3to4, where each problem provides either three or four source numbers. Evaluation Benchmarks. Evaluation is conducted on two held-out benchmarks: 512-problem split from Countdown-34 (CD-34), and 512-problem subset from Countdown-4 (CD-4), more challenging variant available at https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-4. Compared to CD-34, CD-4 consistently provides four source numbers per instance, substantially enlarging the search space and increasing problem difficulty. Training curves are reported on the average performance on CD-34 and CD-4. Reward Function. Following Pan et al. (2025), we incorporate format-aware reward function: = 1 0.1 0 if the response is correct, if the response is incorrect but properly formatted, otherwise. (20) D.2 Models We evaluate five LLMs spanning different architectures and parameter scales. All models are obtained from their official Hugging Face repositories and used without modification: DeepSeek-R1-Distill-Qwen-1.5B: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1. 5B; DeepSeek-R1-Distill-Qwen-7B: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B; Qwen3-4B-Base: https://huggingface.co/Qwen/Qwen3-4B-Base; Qwen3-8B-Base: https://huggingface.co/Qwen/Qwen3-8B-Base; Llama-3.2-3B-Instruct: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct. D.3 Training Details We adopt GRPO (Shao et al., 2024) as the default RLVR algorithm, implemented within the verl framework (Sheng et al., 2024). At each training step, = 8 responses are sampled per prompt to estimate advantages, using temperature 1.0 and top = 1.0. Evaluation is performed using pass@1, computed from multiple independent generations per prompt, where the number of generations varies across benchmarks. Following Luo et al. (2025b); Qu et al. (2025a), evaluation generations use temperature 0.6 and top = 0.95. We disable the KL penalty by setting β = 0, consistent with Yu et al. (2025). The training batch size is set to 256 for both DeepScaler and Countdown, with mini-batch sizes of 128 and 64, respectively. The maximum response length is 8192 tokens for DeepScaler and 1024 tokens for Countdown. Entropy regularization is applied with coefficient of 0.001 for Countdown and disabled for DeepScaler. Optimization is performed using Adam (Kingma & Ba, 2014) with learning rates of 1e6 for Countdown and 4e6 for DeepScaler, following Qu et al. (2025a); Gao et al. (2025), with β = (0.9, 0.999), and weight decay 0.01. We apply the ClipHigher strategy from DAPO (Yu et al., 2025), which decouples clipping ranges with ϵlow = 0.2 and ϵhigh = 0.28. For Countdown, we apply post-rollout advantage normalization to stabilize gradient magnitudes under low effective sample ratios. For each benchmark, training is conducted under fixed computational budget; in practice, this corresponds to roughly one pass over the dataset, by which point performance has largely stabilized. All experiments are conducted on 8 NVIDIA H20 GPUs. 17 Regarding hyperparameters, we set λ = 1 for Countdown and λ = 0.5 for DeepScaler. The candidate batch consists of the entire prompt pool. Regarding the diversity implementation in Eq. equation 13, we use Euclidean distance in the embedding space as simple and computationally efficient distance measure. D.4 Implementation of the Generalizable Prompt Predictive Model We implement the prompt predictive model using variational formulation optimized via the ELBO in Eq. equation 8, jointly learning the encoder, decoder, and history-conditioned prior. Input Representation. Prompt embeddings are extracted using the WORDLLAMA toolkit (Miller, 2024), selected for its computational efficiency and stable semantic representations. Encoder and Prior. The variational encoder qϕ(zt Ht) and the history-conditioned prior pη(zt Ht1) are implemented using lightweight Transformer encoders. They operate over short sequences that aggregate prompt embeddings and their associated success rate feedback. For computational efficiency, the prior conditions on the most recent training batch, while the encoder conditions on the current batch. This design leverages the recursive structure of the latent formulation: information from recent history is progressively compressed into the latent state through posterior-to-prior regularization, while longer-term effects are reflected in the evolving parameters of the predictive model as result of training on preceding data. Consequently, conditioning on recent observations suffices to capture short-term non-stationarity in the optimization trajectory without explicitly re-encoding the full history at each step. The Transformer outputs are pooled to parameterize the mean and variance of Gaussian latent context zt. Decoder. The shared decoder pψ(Ht zt) is parameterized as multilayer perceptron (MLP), which maps the latent context zt together with the prompt embedding τt to predicted difficulty ˆγτt . This shared parameterization enables experience transfer across prompts while keeping the predictive model lightweight. Overall, this design remains consistent across all experimental settings and LLM backbones, enabling an efficient and stable implementation. The model introduces negligible computational overhead, with only 20M parameters, less than 1% of most LLMs. The detailed computational complexity analysis is deferred to Appendix E.1. Discussion. The design choices of the predictive model, including the specific variational formulation, Transformerbased encoder, and lightweight MLP decoder, are not claimed to be optimal. Rather, the primary goal of this work is to validate the feasibility and effectiveness of global, history-conditioned difficulty modeling for prompt difficulty prediction under realistic computational constraints. Accordingly, we intentionally adopt simple and stable architectures without extensive architecture search or scaling studies. Exploring alternative model classes, varied predictive model capacities, or more sophisticated temporal modeling strategies may further improve predictive accuracy, but such investigations are orthogonal to the central contributions of this paper and are left for future work. D.5 Extension to Test-Time Computation Allocation After LLMs post-training, reasoning performance can be further improved by scaling test-time computation (Snell et al., 2024; Fu et al., 2025). common instantiation is best-of-N sampling (Cobbe et al., 2021; Lightman et al., 2023), which generates independent responses per prompt and selects the best one. Under binary correctness reward, performance is measured by the pass@k metric: pass@k(τ) = 1 (1 γτ)k. (21) Necessity of Computation Allocation. Typical test-time scaling methods allocate fixed number of samples to each prompt, i.e., k(τ) for all τ . Under global computation budget, this is suboptimal since the marginal benefit of additional samples varies across prompts Damani et al. (2024); Wang et al. (2025c). Specifically, the marginal gain of one extra sample to prompt τ is k(τ) = pass@(k + 1)(τ) pass@k(τ) = (1 γτ)kγτ. As increases, easy prompts quickly saturate, and extremely hard ones with γτ 0 yield negligible gains. Challenging yet solvable prompts retain the largest marginal gains, which motivates difficulty-guided computation allocation. (22) Computation Allocation with Predicted Difficulty. Accurately estimating γτ at test time is costly and hinders parallelism. Prior methods propose pre-training predictors Damani et al. (2024); Snell et al. (2024) or sequential pre-sampling Raman et al. (2025) to estimate the prompt difficulty. We reuse the PPM learned during RLVR, providing new scheme for test-time allocation. Independent modeling methods such as MoPPS do not generalize to unseen benchmarks and are therefore unsuitable in this setting. 18 Specifically, we formulate test-time computation allocation as difficulty quantile-based function. Given an evaluation set and global budget τT k(τ) = , each prompt is assigned minimum number of samples kmin, and the remaining computation is distributed according to k(τ) = kmin + α w(q( ˆγτ), k) , (23) where q( ˆγτ) [0, 1] denotes the empirical quantile of ˆγτ within { ˆγτ }τT , w() is an allocation function defined over quantiles, and α is determined by the budget constraint. This formulation depends only on relative ordering and is therefore robust to miscalibration on unseen benchmarks. Inspired by Damani et al. (2024), we adopt simple window function in this work, w(q( ˆγτ), k) = 1[q1(k) < q( ˆγτ) < q2(k)] , which provides coarse approximation to the marginal utility structure of pass@k: prompts that are either too easy or unsolvable receive little computation, while challenging yet solvable prompts are prioritized. (24) Scope and Implementation Notes. We emphasize that the detailed computation allocation strategy is not technical contribution of this work, nor is it intended to be optimal. Our goal here is solely to demonstrate that generalizable PPM, once learned during RLVR, can be naturally reused at test time to guide computation allocation. The specific functional form of w() and the choice of hyperparameters are deliberately kept simple and are adopted as concrete instantiation rather than carefully optimized design, and primarily serve to instantiate this idea in practice. The empirical results should therefore be interpreted as evidence of the applicability of generalizable PPMs to test-time computation allocation, rather than as an exhaustive study of optimal allocation strategies. In practice, we set kmin = 0.1 across all scenarios. The quantile thresholds are selected via lightweight calibration under low-computation regimes: we evaluate pass@k over small range of values and choose thresholds that yield stable improvements. While not optimal, this lightweight calibration is to demonstrate the feasibility of difficulty-guided allocation in practice. Across benchmarks and model sizes, the resulting thresholds (q1, q2) typically focus on difficult prompts (e.g., (0, 0.5) for CD34 and (0, 0.6) for MATH500). In settings with substantial distribution shift, an interesting direction for future work is to treat the learned PPM as pretrained initialization and adapt it via lightweight finetuning before test-time allocation."
        },
        {
            "title": "E Extended Experimental Results",
            "content": "E.1 Computational Complexity Analysis Table 3 summarizes the per-step runtime of different operations for Countdown and DeepScaler under varied LLM scales. For reference, we report the per-step average runtime of LLM training and rollout generation under uniform prompt sampling. Evaluation-based methods such as DS incur additional overhead by performing rollout-based evaluation over enlarged candidate sets; specifically, the DS sampling cost is computed as the rollout multiplicative factor relative to the standard batch size, multiplied by the average rollout time. In contrast, the proposed GPS performs prompt selection using lightweight predictive model and does not require any additional LLM inference. As result, the extra cost introduced by GPS remains negligible (< 1%) compared to the overall cost of LLM training and rollout generation. Additionally, to assess scalability under large candidate pools, we evaluate GPS on synthetic prompt set containing 1M prompts, with candidate batch size of the same scale. In this setting, the combined cost of GPS sampling and PPM updates remains < 16s, indicating that the overhead grows sublinearly and remains negligible relative to LLM training. Notably, the current implementation runs the PPM on single GPU and further acceleration is straightforward via data or model parallelism in large-scale settings. Moreover, the candidate batch size can be chosen substantially smaller to further reduce computational cost without affecting performance, as shown in Appendix E.10. Table 3: Per-step computational cost of different operations across tasks and LLM scales. All measurements are taken during RL post-training on 8H20 GPUs with batch size 256. Countdown DeepScaler Component LLM Training LLM Rollout DS Sample Cost GPS (Sample + PPM Update) 4B 72 32 8B 116 39 1.5B 203 157 7B 610 290 332 339 3157 3.6290 1 1 1.6 1.6 19 Table 4: Evaluation results on Countdown. + denotes finetuning with the corresponding method. Avg. reports the average accuracy, and Runtime indicates total training time. Bold and underlined values indicate the best and second-best results, respectively. Method Qwen3-4B +Uniform +PCL +GRESO +MoPPS +DS +GPS (Ours) Qwen3-8B +Uniform +PCL +GRESO +MoPPS +DS +GPS (Ours) CD4 CD34 Avg@16 Avg@16 Avg. Runtime 1.3 51.1 51.0 53.8 52.9 56.1 57.2 2.1 52.5 53.5 54.1 55.5 58.7 59.4 3.5 73.8 72.8 73.8 73.9 76.3 76.0 3.9 73.3 74.9 75.1 76.0 78.2 77.9 2.4 62.5 61.9 63.8 63.4 66.2 66.6 3.0 62.9 64.2 64.6 65.7 68.5 68. - 2.9h 3.2h 4.9h 2.8h 4.9h 3.4h - 4.3h 5.1h 6.2h 4.3h 6.9h 5.0h Figure 6: Difficulty prediction quality and effective sample ratio during training. The top row shows Spearmans rank correlation between predicted difficulty and empirical success rate. The bottom row reports the effective sample ratio achieved by different online prompt selection methods. E.2 Additional Evaluation Results Table 4 report the final evaluation results on Countdown. E.3 Quality of Difficulty Prediction and Prompt Selection To further demonstrate that the generalizable PPM yields more accurate predictions than prompt-specific difficulty modeling, we compare Spearmans rank correlation with the representative prompt-specific method MoPPS. Notably, to isolate prediction quality from sampling-induced distributional effects, all correlations are computed under uniform prompt sampling. As shown in the top row of Fig. 6, GPS consistently achieves higher correlation than MoPPS. In contrast, MoPPS struggles to provide meaningful predictions under large-scale training, due to the sparsity of prompt-specific observations. These results indicate that the proposed generalizable PPM enables more accurate and stable difficulty estimation. To assess prompt selection effectiveness, we further report the Effective Sample Ratio (ESR) and compare against Uniform and the prompt-specific baseline MoPPS. ESR is defined as the fraction of prompts with non-zero reward variance: ESR(T B) = (cid:104) 1 1 τT Var({rτ }k j=1) > 0 (cid:105) , (25) 20 Figure 7: Training curves of GPS and baseline methods across different scenarios and backbone models, plotted against the number of generated rollouts to reflect computational overhead. Figure 8: Evaluation on Countdown with PRIME using continuous process rewards. GPS achieves consistent improvements over uniform prompt selection and demonstrates reliable reward-variance prediction, indicating its applicability beyond binary reward settings. where 1[] is the indicator function. higher ESR indicates more informative prompts in the batch. As shown in the bottom row of Fig. 6, GPS maintains consistently higher ESR throughout training, which is attributed to the reliable difficulty prediction and unified batch selection strategy. This indicates that GPS selects more informative prompt batches, which in turn contributes to its superior performance. E.4 Performance versus Rollout Number To explicitly illustrate the rollout efficiency of GPS relative to evaluation-based methods such as DS, we plot training performance against the number of generated rollouts. As shown in Fig. 7, prediction-based methods such as GPS introduce no additional rollouts beyond those required for training, whereas evaluation-based method DS incur substantially higher rollout costs due to the evaluation of larger candidate prompt sets. GPS achieves comparable performance while requiring only 31%34% of the rollouts used by DS. These results highlight the rollout efficiency enabled by prompt difficulty prediction. E.5 Applicability to Continuous Process Rewards Most prior works on online prompt selection focus on tasks with binary rewards, which is also the primary setting studied in this work. Nevertheless, the proposed prompt predictive model does not rely on the binary reward assumption and is applicable to more general reward functions. However, an open challenge lies in defining appropriate acquisition criteria for selecting informative prompts when rewards are continuous and process-based. In contrast to binary rewards, where prompts with intermediate difficulty γ 0.5 are most informative, there is no canonical target value for continuous rewards. Inspired by prior studies (Xu et al., 2025; Razin et al., 2025), we adopt simple yet effective hypothesis: prompts that exhibit higher reward variance across rollouts provide more informative learning signals. Notably, this criterion naturally subsumes the binary-reward case, where reward variance is maximized at γ = 0.5. Consequently, we replace difficulty-based utility u( ˆγτ) in Eq. (12) with reward variance Var(rτ), prioritizing prompts with greater variance in continuous process rewards. Correspondingly, the prediction target of the PPM is changed from the success rate γτ to the reward variance Var(rτ). To empirically assess the applicability of GPS beyond binary feedback, we conduct preliminary study by integrating our method with PRIME (Cui et al., 2025), which provides continuous process rewards. As shown in Figure 9: Ablation study of key components in GPS on Countdown and DeepScaler, including the generative predictive model and history-anchored diversity. Figure 10: Effect of history-anchored diversity on the effective sample ratio during training. Fig. 8, GPS consistently outperforms uniform prompt selection under continuous process rewards and maintains reliable prediction of reward variance. These results provide initial evidence that the proposed framework extends beyond binary reward settings and can accommodate more general forms of feedback. E.6 Additional Ablation Study and the Role of History-Anchored Diversity In addition to the ablation study on Countdown presented in the main paper, we further conduct the same set of ablations on the DeepScaler. As shown in Fig. 9, the relative performance trends on DeepScaler closely mirror those observed on Countdown. In particular, removing history-anchored diversity (GPS w/o hisdiv) results in pronounced performance degradation, while ablating the latent difficulty context in the generative predictive model (GPS w/o z) also consistently harms performance. These results confirm that both components contribute robustly across datasets. To more directly examine the role of history-anchored diversity, we further evaluate its impact on the effective sample ratio. As shown in Fig. 10, introducing history-anchored diversity consistently increases ESR. This observation supports the explanation given in the main text: by explicitly encouraging diversity, history-anchored diversity mitigates overfitting of the PPM, leading to more reliable difficulty estimation and more effective prompt selection. E.7 Evaluation with Different LLM families Beyond the Qwen and DeepSeek series, we evaluate GPS and baseline methods on the Countdown using Llama3.2-3B-Instruct, an instruction-tuned model from the LLaMA family that differs substantially in architectural design and training paradigm. As shown in Fig. 11, GPS continues to achieve reliable difficulty prediction and consistently better performance. These results indicate that the proposed method is not specialized to particular LLM series, but has the potential of generalizing across heterogeneous model families. E.8 Training Dynamics Fig. 12 illustrates the training dynamics of response length, entropy, and training reward under different prompt selection strategies. Response Length. Regarding response length, the proposed method exhibits trend highly similar to DS, while both consistently produce longer responses than uniform sampling. This suggests that sampling prompts of Figure 11: Evaluation on Countdown with Llama-3.2-3B-Instruct, demonstrating the effectiveness of the proposed method across different LLM families. Figure 12: Training dynamics, showing response length, entropy, and training reward throughout training. intermediate difficulty tends to elicit longer reasoning chains. This effect also explains the longer wall-clock training time of our method compared to uniform sampling. Entropy. For entropy, no single monotonic pattern emerges across all settings. In the DeepScaler experiments, both DS and GPS lead to increased entropy during training, indicating enhanced exploration. Notably, our method consistently induces stronger entropy increase, suggesting more diverse exploration behavior, which we leave for future investigation. Training Rewards. Finally, training rewards align well with expectations. Compared to uniform sampling, GPS maintains the average reward close to 0.5 throughout training, reflecting sustained focus on informative prompts. This behavior empirically validates the effectiveness of the proposed difficulty estimation and prompt selection strategy. E.9 Sensitivity to Diversity Weight λ We study the effect of diversity weight λ in GPS, which controls the difficulty-diversity trade-off. As shown in Fig. 13a, very small values of λ underemphasize diversity, resulting in redundant prompt selection and limited coverage of the prompt space. In contrast, excessively large λ over-prioritizes diversity at the expense of utility, leading to degraded training performance. Importantly, GPS exhibits relatively stable performance over broad intermediate range of λ, indicating robustness to the choice of this hyperparameter. 23 (a) Effect of diversity weight λ. (b) Effect of candidate batch size. Figure 13: Hyperparameter sensitivity analysis on Countdown 8B. E.10 Effect of Candidate Batch Size We further investigate the effect of the candidate ratio, defined as the ratio between the candidate batch size and the actual batch size used for training. By default, we use the full prompt pool as the candidate set, which incurs negligible overhead due to the lightweight PPM. As shown in Fig. 13b, increasing the candidate batch size tends to improve performance, as larger pool provides more flexibility for prompt batch selection. The gains gradually saturate once the candidate size exceeds moderate threshold, suggesting diminishing returns beyond this point. This trend is consistent with prior observations (Qu et al., 2025a)."
        },
        {
            "title": "F Data Examples",
            "content": "For the DeepScaler dataset and mathematics benchmarks, prompts are constructed by appending chain-ofthought instruction (Wei et al., 2022) together with formatting constraint: Lets think step by step and output the final answer within boxed{}. For general reasoning benchmarks, we follow the evaluation protocol of Yan et al. (2025) and adopt PRIMEs prompt template. For the Countdown task, we use the prompt format introduced in Pan et al. (2025). DeepScaler & Mathematics Benchmarks example Prompt: The operation is defined for all nonzero numbers by = a2 Lets think step by step and output the final answer within boxed{}. Ground-Truth Answer: 2 3 . Determine [(1 2) 3] [1 (2 3)]. General Reasoning Benchmarks example or offence. , and must not cause Prompt: The following are multiple choice questions (with answers) about $. Think step by step and then finish your answer with boxed{X} where is the correct letter choice. Question: Typical advertising regulatory bodies suggest, for example that adverts must not: encourage unnecessary Options: A. Unsafe practices, Wants, Jealousy, Serious B. Unsafe practices, Distress, Joy, Trivial C. Unsafe practices, Distress, Fear, Serious D. Safe practices, Wants, Jealousy, Trivial E. Unsafe practices, Wants, Fear, Trivial F. Safe practices, Wants, Fear, Serious G. Safe practices, Distress, Fear, Trivial H. Safe practices, Distress, Jealousy, Serious I. Safe practices, Fear, Jealousy, Trivial Ground-Truth Answer: , cause 24 Countdown example Prompt: conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. User: Using the numbers [2, 54, 17], create an equation that equals 35. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> < /think> tags. And return the final answer in <answer> < /answer> tags, for example <answer> (1 + 2)/3 </answer>. Assistant: Let me solve this step by step. <think>"
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "LLM Department, Tencent"
    ]
}