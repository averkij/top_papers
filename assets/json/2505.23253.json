{
    "paper_title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
    "authors": [
        "Yixun Liang",
        "Kunming Luo",
        "Xiao Chen",
        "Rui Chen",
        "Hongyu Yan",
        "Weiyu Li",
        "Jiarui Liu",
        "Ping Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 5 2 3 2 . 5 0 5 2 : r UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes Yixun Liang1,2* Kunming Luo1,2* Xiao Chen2* Rui Chen1 Hongyu Yan1 Weiyu Li1,2 Jiarui Liu1,2 Ping Tan1,2 1HKUST 2Light Illusion Figure 1. UniTEX generates high-quality and complete textures for both artist-created low-polygon mesh (the van shell) and generative high-polygon meshes (robots, toy bear, bust and roadsign)."
        },
        {
            "title": "Abstract",
            "content": "We present UniTEX, novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)a continuous, volumetric representation that maps any 3D point to texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy *Equal contribution. Work done during internship at Light Illusion. Corresponding authors. for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/ YixunLiang/UniTEX 1. Introduction High-quality texture generation plays critical role in 3D asset creation, as it directly impacts both the visual realism and semantic fidelity of rendered models. In modern applications such as gaming, virtual reality, and digital content creation, photorealistic textures are essential for delivering immersive and visually engaging experiences. However, producing detailed and aesthetically refined textures typically requires significant manual effort and domain expertise, making the process time-consuming and resourceintensive. Recently, diffusion models trained on large-scale datasets [11, 31] have brought about revolutionary transformation in the field of image and video generation. Inspired by this, there is an expectation that 3D texture generation can also be greatly simplified, enabling direct generation from text descriptions or single reference image. Prior works [1, 7, 25, 40] have primarily addressed this by adapting 2D generative models for multi-view texture generation and then projecting these multi-view textures onto the 3D mesh. While this approach leverages strong 2D priors and achieves some promising results, the generated textures often suffer from issues of self-occlusion and multi-view inconsistencies, leading to textures on the 3D surface that are incomplete or fragmented. To mitigate this, some early attempts [1, 39, 40] introduce second stage of UV-based inpainting process to merge the partial views into coherent texture map to handle those incomplete or inconsistent areas after multi-view projection. Although these methods provide completed texture, in practice, UV-based inpainting often struggles to handle meshes created from generative AI pipelines [19, 20, 47] as shown in Fig. 2. This practical observation highlights the generalization bottlenecks of UV-based inpainting for real-world 3D texture generation. This challenge stems from fundamental limitation of UV mapping, what we refer to as topological ambiguity. Specifically, single 3D mesh can correspond to multiple valid UV layouts that are not uniquely determined by the geometry but are also highly sensitive to vertice/face distributions and UV unwarping algorithms. This ambiguity poses critical challenge for UV-based inpainting models because there is no guarantee that the UV parameterization used during inference will match that of the training stage. In practice, mesh topology varies significantly due to differences in modeling conventions across designers. Consequently, the UV parameterization becomes inherently inconsistent. This problem is especially pronounced when texturing meshes produced by generative pipelines [19, 20, 47], which are extracted using Marching Cubes [26]. The resulting domain gap severely limits generalization during the inference of current UV-based inpainting models. To address this challenge, we propose novel approach to represent textures in unified 3D functional space so as to bypass the limitations of refinement operations in UV space. Specifically, we introduce Texture Functions (TFs)a continuous representation that maps any 3D spatial point to texture value. For each query point in space, we determine the closest surface point on the mesh using method analogous to computing Unsigned Distance Functions (UDF) and then retrieve the corresponding texture value from that location. While the texture is sampled based on surface proximity, TFs are defined throughout the entire 3D volume, enabling volumetric texture represenFigure 2. UV-based texturing models perform well on indomain, artist-created meshes (first column), but struggle with out-of-distribution, generated meshes (second column). We take Paint3D [40] and TexGEN [39] as representative examples: while effective on large, continuous regions, they fail to handle small, fragmented areas due to training biases toward clean, large-region In contrast, our method operates outside the UV UV layouts. space, enabling better generalization across diverse mesh types. Additional comparisons are provided in Sec. 4.2.2. tation. Unlike UV maps, which rely on mesh-specific face distribution and suffer from topological ambiguity, TFs are invariant to mesh face distributions and depend solely on surface position, which bypasses the problem we mentioned above. Also, this formulation allows texture to be treated as smooth, continuous field over 3D spaceanalogous in spirit to how SDFs or UDFs represent geometry, but instead modeling appearance only on the surface like previous methods [15, 27]. This brings more complete training for texture generation, as detailed in Sec. 4.4.2. With this formulation, we frame texture inpainting/prediction as native 3D regression task like [23, 24], where the model takes image and geometry inputs to directly predict the corresponding 3D texture functions. To realize this, we introduce transformer-based architectureLarge Texturing Model (LTM)which is detailed in Sec. 3.3 to perform this task. By eliminating reliance on UV layouts, our approach reduces the domain gap between training data and real-world application, providing generalizable and scalable second stage in the 3D texturing pipeline. To further enhance the overall quality of the texture generation pipeline, we propose an advanced LoRA-based training strategy to efficiently adapt large-scale diffusion transformers (DiTs) for multiview synthesis conditioned on geometry and reference images. Since the first-stage texture generation heavily relies on the capabilities of 2D diffusion modelsand large-scale DiT models currently dominate 2D generative modelingour approach is designed to efficiently leverage these strengths. It facilitates scalable adaptation of powerful foundation models such as FLUX and SD3 [11], thereby improving generative texture quality and offering transferable insights for other vision tasks. We evaluate our method through extensive experiments on multiple settings, demonstrating its superiority over existing approaches in terms of both visual quality and texture integrity. Overall, our contributions are summarized as: We propose Texture Functions (TFs), continuous 3D texture representation that bypasses UV mapping and models texture as completed spatial field. We design novel Large Texturing Model (LTM) based on transformer architecture to predict TFs directly from images and geometry inputs. We develop LoRA-based strategy to efficiently adapt large diffusion transformers (DiTs) for downstream tasks, enabling high-quality multiview synthesis for texture generation using large-scale Diffusion Transformers. 2. Related work 2.1. 3D Texturing using Diffusion Models Most existing texturing methods leverage 2D priors from text-to-image (T2I) diffusion models [31] to generate textures from text or images. Methods like Text2Tex [3] and TEXture [30] iteratively paint meshes from multiple views but often lack multi-view consistency. Another line of work uses optimization-based approaches with Score Distillation Sampling (SDS) to directly refine texture maps, as seen in Fantasia3D [4], Magic3D [21], and DreamMat [45]. FlashTex [9] improves light disentanglement by introducing light-conditioned diffusion model in twostage pipeline. However, SDS-based methods typically rely on single-view diffusion models, suffer from view inconsistency (e.g., Janus problem), and require expensive iterative optimization, limiting their scalability. Subsequent works aim to improve multiview consistency and texture quality by fine-tuning diffusion models [7, 40, 43, 47], introducing advanced sampling strategies [2, 25, 42], or designing 2D diffusion models to adopt UV refinement [1, 39, 40]. TexFusion [2] uses denoising sampler that fuses images across views in UV space at each step. Paint3D [40] refines textures through UV inpainting model. Meta 3D TextureGen [1] leverages geometry-aware T2I model for consistent multi-view synthesis, and Hunyuan3D 2.0 [47] proposes model to generate multi-view textures simultaneously. Although these UV-based methods can leverage the powerful priors of 2D diffusion models, the UV representation inherently suffers from topological ambiguity, which limits the generalization capabilities of these methods. In this paper, we propose Texture Functions (TFs), continuous 3D texture representation, aiming to bypass the limitations of UV unwrapping and achieve complete and high-quality texturing. 2.2. 3D Native Texturing Another line of work trains generative models directly on 3D data with ground-truth textures [22, 27, 3739]. Texture Field [27] learns implicit color fields on 3D surfaces, while Texturify [32] uses face convolutions and adversarial rendering losses for per-face texture prediction. Recent diffusion-based methods such as PointUV [38], TexGEN [39], and TexOct [22] generate point cloud colors mapped to UV textures. TexGaussian [37] introduces an octree-based 3D Gaussian representation trained to predict textures from text and geometry and then bake them to the original mesh. Although these native 3D approaches offer better completion compared with 2D diffusion-based methods, they are limited by the scarcity of 3D data and limited generalizability. In this paper, we reposition native 3D texturing as refinement module in second-stage refinement and get more generative conditions (multi-view images) from 2D foundation models like [17]. By integrating the strong generative capabilities of 2D diffusion models, our method achieves more robust performance under limited data and produces higher-quality results. 3. Methods 3.1. Overview In this section, we present detailed design of our texturing pipeline, which is two-stage pipeline that fully considers the combination of the 2D diffusion models for Multiview generation and 3D texturing methods for texture completion. As shown in Fig. 3, given single input image and textureless 3D mesh, our system first finetunes two large-scale diffusion transformers (Flux 1 *) using an efficient LoRA-based training strategy (Sec. 3.2) to generate six orthographic, illumination-free views conditioned on the meshs normals and canonical coordinate map (CCM) [18, 36], which is special type of rendered image where each pixel encodes the 3D coordinate of the surface point it corresponds to). These generated images can optionally be used for super-resolution [10]. After reprojection and blending, the synthesized views, along with the partially textured geometry, are fed into our Large Texturing Model (LTM) (Sec. 3.3), which predicts the corresponding *https://huggingface.co/black-forest-labs/FLUX.1-dev Figure 3. Overall pipeline of UniTEX. Given textureless geometry and reference image, UniTEX first generates high-fidelity multi-view image through 3 steps (RGB generation, delighting, and super-resolution (SR)) using finetuned DiTs (detailed in Sec. 3.2). The texture will be reprojected to partial textured mesh and sent to the Large Texturing Model (Detailed in Sec. 3.3) with generated images to predict the corresponding complete texture functions (Detailed in Sec. 3.3.2). The final texture is then synthesized by blending the predicted texture functions with the partial textured geometry. for better results. However, effectively adapting 2D diffusion models for 3D texturing presents unique challenges. Unlike traditional 2D generation, our task requires significantly more conditioning signals. For instance, generating six-view images at 512512 resolution demands reference image and corresponding geometric information (e.g., normals and CCMs) for all six views. Moreover, since diffusion transformers (DiTs) rely heavily on in-context learning [33, 46]where all inputs and conditions are jointly encoded as tokensthe resulting volume of token inputs substantially increases training cost and slows convergence. Recent studies, such as MVDiffusion++[34] and LongLoRA[6], suggest that it is not strictly necessary to preserve the full forward computation pattern during both training and inference when fine-tuning large-scale foundation models. Specifically, Long-LoRA proposes truncating attention windows during training while restoring full attention at inference time to save resources and achieve long context finetuning. Similarly, MVDiffusion++ demonstrates that training on fewer views and inferring more views at test time can still yield strong performance. This invites rethinking of why such behaviors can work and what finetuning is truly optimizing for. Using texture generation as an example, the image quality does not need to be trained during finetuning, the pattern we want the model to learn is multi-view consistency and correspond to conditions. Learning such pattern may not require simultaneous access to all input tokens. Building on this insight, we propose drop training strategy: during each training step, only subset of all tokens is retained, and the diffusion transformer is conditioned and generated solely on these selected tokens rather than the full input tokens. This apFigure 4. Pipeline of the Large Texturing Model. Given partially textured geometry and six input views, we first unify them into shared triplane-cube token representation. transformerbased architecture then processes these tokens to extract geometryaware features, which are subsequently decoded into colors using lightweight MLP. complete texture functions(Sec. 3.3.2). The final texture is then synthesized by blending the predicted texture functions with the initial partial textured geometry. 3.2. Efficient DiT Tuning for MV Generation In this section, we present the details of the first stage in Fig 3. We fine-tune two large-scale diffusion transformers using in-context learning to adapt the model to our specific task 3D texturing. Specifically, the first Flux takes reference images, normal maps, and canonical coordinate maps (CCMs) of textureless mesh as input and generates corresponding shaded images. The second Flux is used to delight and generate diffuse color for final texturing. The generated diffuse color images can be used with super-resolution [10] proach reduces the dependency on complete image tokens, allowing the model to learn from partial information while maintaining task-relevant needs. As shown in Sec. 4.4.1, This method achieves comparable generation quality to fullinput fine-tuning at the same iterations while significantly accelerating training and reducing the computational cost. 3.3. Large Texturing Model Previous two-stage texturing methods primarily relied on UV-based inpainting, which often suffers from topological ambiguities and leads to suboptimal texture quality. In our work, we propose Large Texturing Model (LTM) to regress the texture in 3D functional space to bypass the topology ambiguity and serve as the second stage in Fig. 3. The visualization of our Large Texturing Model (LTM) is shown in Fig. 4. Based on the generated images from Sec. 3.2 with textureless/ incompleted texture geometry, we introduce the Large Texturing Model (LTM), which architecture design is detailed in Sec. 3.3.1), which is designed to regress the Texture Functions (Detailed in Sec. 3.3.2) for the second stage of our texturing pipeline. 3.3.1. Achitechture Design We first initialize our triplane-cube with set of learnable tokens, which contain high-resolution triplane R33232. and low-resolution cube R888, which have been extensively demonstrated to offer superior representation compared to triplanes [12, 29]. Then, we encode 6 orth-view images, CCM, and alpha maps to triplane like CRM [36] and add them together to initial triplane tokens. After that, we encode geometry information to such representation following Shape2VecSet [41]. Specifically, we sample the colored point cloud in the partial textured geometry and use the triplane-cube tokens to query them through cross-attention. After integrating the information of 2D images and 3D geometry, we use transformer architecture with selfattention to process these flatted features of triplane-cube. After processing such representation. The final output is first reshaped into triplane and cube representations, then upsampled using 2D and 3D deconvolution layers respectively. During sampling, We implement an MLP, denoted as MLPθ to predict RGB queried from the triplane-cube features denoted as C. Given query 3D point x, we predict the correspondence color as: Figure 5. Visualized example of the Texture Functions (TF) for represent the texture for the whole 3D space. (a) textured mesh. (b) Unsigned Distance Function (UDF) samples representing 3D geometry. (c) Inspired by UDF, we define texture as continuous function over 3D space (details in Sec. 3.3.2), enabling volumetric texture representation. tance Fields (SDF/UDF) in 3D functional space as continuous and complete supervision signals [19, 20, 47], texture is traditionally defined only on the surface of 3D objects. Prior works [13, 15, 17, 27, 32] typically rely on surface or volume rendering to supervise the texture representation via 2D projections. However, this form of supervision is inherently sparse and limited in coverage compared to the dense volumetric supervision available in geometry tasks. As shown in geometry generation, complete 3D supervisionsuch as that provided by SDFs/UDFsconsistently outperforms sparse alternatives like LRM-based methods. Motivated by this insight, we propose to extend texture from surface-restricted signal to continuous volumetric function defined throughout the 3D space. This allows us to supervise the texture model using densely sampled points across the volume, providing richer and more complete training signals. Formally, we define Texture Function as mapping over 3D coordinates R3, where each texture value is obtained by orthogonally projecting onto the closest point on the mesh surface Ω and querying its corresponding color. This definition aligns naturally with volumetric geometry representations and enables unified learning across the entire 3D domain. For better understanding, Fig. 5 provides visual comparison between our proposed texture function and the traditional unsigned distance function (UDF). With such Texture functions, the training objectives of LTM is defined as: ˆc(x, C) = MLPθ(gridsample(x, C)), (1) Ltexture = ExΩ (cid:2)ˆc(x, C) c(x)2(cid:3) + λLtv(T ), (2) where ˆc R3 is the predicted RGB color for point x. 3.3.2. Training Objective Texture Functions: Unlike native 3D geometry generation or reconstruction, which benefits from well-defined Signed or Unsigned Diswhere is the set of surface points, ˆc(x) is the predicted color, and c(x) is the ground truth color derive from texture functions. Ltv denote as total-variance loss to regulize the variance of grid-based 3D representation. The λ denotes the weight, which we set as 0.0005 in our experiments. To be Figure 6. Qualitative comparison of our method against state-of-the-art (Paint3D [40], Hunyuan2.0-Paint [47]) and commercial proprietary (Rodin, Meshy) texturing approaches. Our method consistently achieves superior texture quality and generalization across diverse mesh sources. (best viewed by zoom in) noticed, we use truncated texture functions like truncated SDF that are used for training in [5, 20, 47]. We set the threshold as 0.025. For those points that are out of the truncated value, we use background color as the ground truth for training. Methods. Artist-created Mesh Generative Mesh CMMD FIDCLIP CLIPscore LPIPS CLIPscore User-perf. Paint3D TexPainter TexGaussians Hunyuan3D-Paint Ours 1.196 1.632 1.290 0.909 0.826 20.52 27.13 21.08 20.38 16.03 0.840 0.804 0.836 0.824 0.844 0.107 0.112 0.095 0.091 0.090 0.737 0.773 0.718 0.805 0. 6.82% 1.36% 4.55% 21.36% 65.91% In addition to providing surface point supervision similar to previous methods, our approach further extends supervision to non-geometric regions in 3D space. Specifically, the colored regions are expanded into thin shell (after truncation) surrounding the geometry. This brings notable advantages: during color prediction, the model is implicitly encouraged to build volumetric understanding of the 3D object. The introduction of the thin shell alleviates the need for highly precise mesh modeling, as the model can still correctly query colors without relying on exact geometry. Moreover, fully defined supervision signal is beneficial for learning well-structured latent space and enhancing the models generalizability. This leads to improved predicted texture quality, as demonstrated by our experiment results in Sec. 4.4.2. Table 1. Quantitative comparison between artist-created and generative mesh on different texturing methods. 4. Experiments 4.1. Experimental Setup Baselines In this section, we compare our overall pipeline with current open-source state-of-the-art (SoTA) methods, including Hunyuan3D-Paint [47], TexPainter [42], TexGaussians [42], and Paint3D [40] in quantitative comparison, in the qualitative comparison, we further compare our method with the closed-source method including Tripo, Hyper3D-Rodin . To be noticed, for the pipelines that can only receive text prompt as input, we use GPT-4o to caption the image and use it as input prompt, otherwise, use the reference image as input prompt. The API was invoked from Tripo/Rodin platform in May 2025 and some of the meshes are downloaded from gallery. Figure 7. Qualitative comparison of refinement stage across different methods. In the first case, automatically unwrapping makes fragmented and noisy UV layout on the face. UV-based methods such as Paint3D and TexGen struggle with these (blue boxs). In contrast, our method generates smooth and coherent textures that more respect to geometry (glasses in the first row and the ribcage and emblem in the second row). Methods. PSNRuv PSNRuv PSNR SSIM LPIPS Paint3D-UVpaint TexGEN Ours 18.61 20.47 23. 15.22 17.07 19.89 23.90 25.19 30.45 0.971 0.977 0.989 0.042 0.040 0.023 indicates the Table 2. Refinement Stage Comparision, PSNRuv PSNR calculated exclusively in the invisible regions, whereas PSNRuv is computed over the entire set of valid UV regions. For the second stage comparison, we use the same partial textured model reprojected from six views and reference image as input and use different methods to paint/complete this partial texture. We primarily compare our second stage with Paint3D-2nd stage [40]. Additionally, we observe that the settings of TexGen [39] can also be viewed as secondstage model for UV-based inpainting, and thus, we include it in our second-stage comparisons as well. Unlike previous methods [1, 47] contains several settings and some of them solely compare the texture generation ability in artist-created meshes. We set two benchmarks that consider both artist-created meshes and generative meshes. For artist-created meshes, we randomly select 78 meshes from objaverse [8] and the test list provided by MetaTexGEN [1]. For generative meshes, we select 30 images and generate mesh using Craftsman [19] for texturing. This enables us to fully evaluate the texture generation ability in various real-world applications. [14] for finer fidelity assessment. eration. For semantic similarity, we use CLIP-based FID (F IDCLIP ) via Clean-FID [28], and CLIP-MMD (CMMD) CLIPScore [48] measures alignment with input prompts, while LPIPS [44] evaluates perceptual similarity to ground truth. We further include human evaluation metric (User-perf.) to assess the perceptual quality of generated textures. For tasks with strictly ground-truth supervision, such as second-stage evaluation from gt views, we assess texture quality using PSNR measured on the predicted surface via sampled UV points (denoted as PSNRuv). To evaluate the perceptual quality of the rendered views, we additionally report PSNR, SSIM [35], and LPIPS [44]. 4.2. Qualitative Comparision 4.2.1. Texture Generation Comparision We evaluate our method visually across diverse set of meshes, including raw scans, artist-created models, and outputs from mainstream proprietary 3D generation pipelines such as Tripo, Rodin, and Hunyuan 2.5. We compare against state-of-the-art image-based texturing approaches, including Paint3D [40] and Hunyuan2.0-Paint [47]. Additionally, we include comparisons with proprietary methods such as Rodin and Meshy to further assess texturing quality. As demonstrated in Fig. 6, our method consistently achieves better performance across all baselines and Evaluation Criteria We adopt several widely used image-level metrics for fair evaluation of texture genhttps://hyper3d.ai/omnicraft/texture?lang=zh https://www.meshy.ai/workspace Figure 8. Qualitative results of stylized texturing. We evaluate our method with various input images and our method can robustly generate high-fidelity stylized textures.(Mesh generated by Hunyuan2.5) mesh sources. Our model effectively extracts information from shaded images to restore fine-grained texture details, as clearly observed in the mask and Buddha examples. Secondly. It also better respects structural geometry, accurately recovering the door frames and handles of the vehicle while maintaining the desired rusted style capability lacking in other methods. Additionally, our textures exhibit richer details and improved visual coherence, particularly in the cases shown in the fourth and fifth columns. 4.2.2. Refinement Stage Comparision In this section, we evaluate the effectiveness of the second stage of our overall texturing pipeline. Specifically, we use the generated images from the first stage as input and compare different methods for texture refinement. We benchmark our approach against state-of-the-art UV-based methods, including Paint3D [40] and TexGen [39]. As shown in Fig. 7 (best viewed by zooming in), our method consistently gains better results. In the first column, automatic UV unwrapping results in numerous small, fragmented regions on the face. UV-based methods struggle to refine these areas, leading to noticeable color inconsistencies and poor results. In contrast, our approach produces smooth and coherent textures. Furthermore, our model better respects the semantic consistency of texture sets. For example, in the first case, the glasses are seamlessly completed, and in the second case, both the ribs and the small emblem are accurately and coherently inpainted by our method. 4.2.3. Stylized Texturing In this section, we demonstrate that our method can be applied to stylized texturing for 3D assets. Stylized texturing refers to the process of generating texture maps that not only align with the geometry of the 3D object but also reflect the visual style of given reference imagesuch as oil painting, metallic finish, or cartoon shading. As illustrated in Fig. 8, our approach enables the creation of realistic and coherent textures while faithfully adapting to the appearance characteristics of different style exemplars. 4.3. Quantitative Comparision 4.3.1. Texture Generation Comparision As shown in Tab. 1, our method outperforms the current state-of-the-art across both benchmarks. While Paint3D achieves comparable CLIP score on artist-created meshes, its performance degrades significantly on generative models, highlighting the limited generalizability of UV-based mapping methods discussed in the introduction. In contrast, our approach consistently yields stronger results among two-stage pipelines. Additionally, TexGaussians performs relatively well in real track but poorly on in-the-wild generative data, likely due to the limited availability of large scale high-quality native 3D texture datasets. These findings indicate that native 3D approaches are currently better suited Figure 9. Visualization of the effectiveness of Texture Function Supervision (TFS). Under identical training iterations, models trained with TFS yield significantly higher-quality and completed textures compared to those supervised solely on surface signals. (Best viewed when zoomed in). for refinement or inpainting, rather than as standalone generative solutions. Therefore, leveraging 2D diffusion priors remains critical. Our method achieves highly competitive results across diverse methods, demonstrating its robustness in texturing 3D shapes completely with varying topologies. 4.3.2. Refinement Stage Comparision We also conduct quantitative evaluation of our refinement stage on the artist-created mesh benchmark to rigorously assess the completeness and quality of the texture produced by our pipeline. As shown in Tab. 2, the results demonstrate that our second stage achieves the best performance in both preserving visible regions and completing occluded areas. In addition, our method demonstrates strong performance in terms of rendered image quality, further validating the visual fidelity of the generated textures. These results suggest that our approach provides more effective refinement paradigm compared to UV-space operations or methods that treat texture purely as 2D image. This is because texture appearance is often closely tied to the underlying geometry, and operating in 3D functional space allows our method to better incorporate geometric structure during refinement. Overall, the findings further support our claim that operate texture in 3D functional space is more suitable to represent texture in UV-space. 4.4. Ablation Study In this section, we systematically evaluate all our proposed key components, which includes comparison of the effectiveness of the patch dropout strategy (Sec.4.4.1), and the impact of texture function supervision (Sec.4.4.2). It is important to note that, due to limitations in computational resources, certain ablation experiments were performed using fewer training iterations than the full model. Further implementation details and discussion are in our Appendix. 4.4.1. Drop Training Strategy In this section, we evaluate our proposed drop training strategy. We first evaluate our drop training strategy on texture generation task. In addition, we introduce the normal Methods. Normal Estimation MV Texture Generation mean RMSE 11.25 22.5 CMMD FIDCLIP CLIPscore LPIPS Mem. Cost Speed w/o Drop Training w/ Drop Training 22.57 22. 29.42 29.79 44.85 45.13 55.82 55.48 0.912 0.826 17.49 16.03 0.839 0. 0.087 0.090 69.2GB 53.6GB 38.76s/it 21.50s/it Table 3. Ablation studies of MV texture generation and normal estimation using drop training strategy. Methods. PSNRuv PSNR w/o Texture Function Supervision Texture Function Supervision 20.31 20.99 25.81 27.01 highest PSNRuv scores indicate significant enhancement in texture completeness. These results demonstrate the effectiveness of our proposed TFs. Table 4. Ablation studies on Texture function. 5. Conclusion estimation task as complementary evaluation to investigate advanced training strategies for large-scale diffusion transformers. To assess the impact of our strategy, we conduct controlled experiment. We train the same model on the same dataset for an identical number of iterations (e.g., 1500), with the only variable being the presence or absence of our drop training strategy. For texture generation, we adopt the Artist-Created Mesh comparison setting described in Sec. 4.1. For normal estimation, we follow the evaluation protocol used by GeoWizard [16], which employs metrics as shown in Tab. 3. Further details on the settings can be found in the Appendix. As shown in Tab. 3, our experiments indicate that using dropout during training can produce comparable performance in 3 tasks we select, while accelerating training by significant percentage specifically, drop 50% tokens in MV texture generation task can save 22.5% memory cost and speed up our training about 44.5%, (evaluate using A800 bs=4). These experimental results in various tasks indicate that our method is useful plug-and-play training strategy for finetuning diffusion models. 4.4.2. Texture Functions Supervision v.s. Surface Supervision In this section, we evaluate the effectiveness of using texture functions (TFs) as supervision within our LTM framework. straightforward alternative is to directly supervise the LTM using rendered images or surface-sampled RGB values, as adopted in prior works such as [15, 27, 38]. For TFbased supervision, we utilize queries from the completed 3D volume, leveraging its well-defined volumetric representation. In contrast, the baseline approach limits supervision to surface points only. It is worth noting that, unlike the setup in Tab. 2, this evaluation textures the entire model directly through LTM query points, without blending with partial texture geometry. This allows for comprehensive assessment of the underlying representations effectiveness. As shown in Tab. 4 and Fig. 9, supervising with texture functions consistently yields superior performance, achieving the highest PSNR and lowest LPIPS, which reflects imIn particular, the proved fidelity and perceptual quality. We presented UniTEX, universal and general framework to generate high fidelity texture for 3D shapes. Unlike previous UV-inpainting based methods, which are limited by inherent problems like topological ambiguity in UV mapping, we proposed to bypass UV unwarping entirely. This is achieved through our Texture Functions (TFs), topology-agnostic and continuous 3D texture representation that models texture as complete spatial field. We then designed Large Texturing Model (LTM) to these TFs directly from geometries, effectively predict multi-view inputs and partial textured models, enabling robust and high-fidelity texture completion. Furthermore, we developed an efficient LoRA-based strategy to adapt large-scale Diffusion Transformers (DiTs) for Extensive exadvanced multi-view texture synthesis. periments on benchmarks including both artist-created and challenging generative 3D models demonstrate UniTEXs quality, consistency, and scalability compared to prior 3D texturing methods. perceptual superior"
        },
        {
            "title": "References",
            "content": "[1] Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, and Oran Gafni. Meta 3d texturegen: Fast and consistent texture generation for 3d objects. arXiv preprint arXiv:2407.02430, 2024. 2, 3, 7 [2] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41694181, 2023. 3 [3] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven In Proceedings of texture synthesis via diffusion models. the IEEE/CVF international conference on computer vision, pages 1855818568, 2023. 3 [4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In ICCV, 2023. 3 [5] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024. 6 [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient finetuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. 4 [7] Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, and Liang Pan. Mvpaint: Synchronized multi-view diffusion for painting anything 3d. arXiv preprint arxiv:2411.02336, 2024. 2, 3 [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 7 [9] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Jun-Yan Zhu, Tinghui Zhou, and Maneesh Ramanan, Agrawala. Flashtex: Fast relightable mesh texturing with In European Conference on Computer Vilightcontrolnet. sion, pages 90107. Springer, 2024. 3 [10] Linwei Dong, Qingnan Fan, Yihong Guo, Zhonghao Wang, Qi Zhang, Jinwei Chen, Yawei Luo, and Changqing Zou. Tsd-sr: One-step diffusion with target score distillation arXiv preprint for real-world image super-resolution. arXiv:2411.18263, 2024. 3, [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 3 [12] Jingyu Guo, Sensen Gao, Jia-Wang Bian, Wanhu Sun, Heliang Zheng, Rongfei Jia, and Mingming Gong. Hyper3d: Efficient 3d representation via hybrid triplane and octree feature for enhanced 3d shape variational auto-encoders. arXiv preprint arXiv:2503.10403, 2025. 5 [13] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 5 [14] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 9307 9315, 2024. 7 [15] Lutao Jiang, Jiantao Lin, Kanghao Chen, Wenhang Ge, Xin Yang, Yifan Jiang, Yuanhuiyi Lyu, Xu Zheng, and Yingcong Chen. Dimer: Disentangled mesh reconstruction model. arXiv preprint arXiv:2504.17670, 2025. 2, 5, 10 [16] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9492 9502, 2024. [17] Sixu Li, Chaojian Li, Wenbo Zhu, Boyang Yu, Yang Zhao, Cheng Wan, Haoran You, Huihong Shi, and Yingyan Lin. Instant-3d: Instant neural radiance field training towards onIn Proceedings of the 50th device ar/vr 3d reconstruction. Annual International Symposium on Computer Architecture, pages 113, 2023. 3, 5 [18] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023. 3 [19] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2, 5, 7 [20] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. 2, 5, 6 [21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. 3 [22] Jialun Liu, Chenming Wu, Xinqi Liu, Xing Liu, Jinbo Wu, Haotian Peng, Chen Zhao, Haocheng Feng, Jingtuo Liu, and Errui Ding. Texoct: Generating textures of 3d models with In Proceedings of the IEEE/CVF octree-based diffusion. Conference on Computer Vision and Pattern Recognition (CVPR), pages 42844293, 2024. [23] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. 2 [24] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. arXiv preprint arXiv:2408.10198, 2024. 2 [25] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2, 3 [26] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 2 [27] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texIn Proceedings of ture representations in function space. the IEEE/CVF international conference on computer vision, pages 45314540, 2019. 2, 3, 5, 10 [28] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1141011420, 2022. 7 ral fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. [42] Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, and Xifeng Gao. Texpainter: Generative mesh texturing with multi-view consistency. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3, 6 [43] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [44] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [45] Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, et al. Dreammat: High-quality pbr material generation with geometry-and light-aware diffusion models. ACM Transactions on Graphics (TOG), 43(4):118, 2024. 3 [46] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 4 [47] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 3, 5, 6, 7 [48] SUN Zhengwentai. clip-score: CLIP Score for Pyhttps : / / github . com / taited / clip - Torch. score, 2023. Version 0.2.1. 7 [29] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for realtime view synthesis in unbounded scenes. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 5 [30] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 3 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [32] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In European Conference on Computer Vision, pages 7288. Springer, 2022. 3, 5 [33] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. [34] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: dense highresolution multi-view diffusion model for single or sparseview 3d object reconstruction. In European Conference on Computer Vision, pages 175191. Springer, 2024. 4 [35] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 7 [36] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh arXiv preprint with convolutional reconstruction model. arXiv:2403.05034, 2024. 3, 5 [37] Bojun Xiong, Jialun Liu, Jiakui Hu, Chenming Wu, Jinbo Wu, Xing Liu, Chen Zhao, Errui Ding, and Zhouhui Texgaussian: Generating high-quality pbr mateLian. rial via octree-based 3d gaussian splatting. arXiv preprint arXiv:2411.19654, 2024. 3 [38] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with pointuv diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42064216, 2023. 3, 10 [39] Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, Jianhui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, and Xiaojuan Qi. Texgen: generative diffusion model for mesh textures. ACM Transactions on Graphics (TOG), 43(6):114, 2024. 2, 3, 7, [40] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42524262, 2024. 2, 3, 6, 7, 9 [41] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neu-"
        }
    ],
    "affiliations": [
        "HKUST",
        "Light Illusion"
    ]
}