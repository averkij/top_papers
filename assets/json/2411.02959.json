{
    "paper_title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
    "authors": [
        "Jiejun Tan",
        "Zhicheng Dou",
        "Wen Wang",
        "Mang Wang",
        "Weipeng Chen",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 9 5 9 2 0 . 1 1 4 2 : r HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Jiejun Tan Gaoling School of Artificial Intelligence Renmin University of China Beijing, China zstanjj@ruc.edu.cn Zhicheng Dou Gaoling School of Artificial Intelligence Renmin University of China Beijing, China dou@ruc.edu.cn Mang Wang Baichuan Intelligent Technology Beijing, China songmu@baichuan-inc.com Weipeng Chen Baichuan Intelligent Technology Beijing, China chenweipeng@baichuan-inc.com Wen Wang Baichuan Intelligent Technology Beijing, China wangwen@baichuan-inc.com Ji-Rong Wen Gaoling School of Artificial Intelligence Renmin University of China Beijing, China Abstract Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design twostep block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems 1. This work was done when Jiejun Tan was doing internship at Baichuan Intelligent Technology. Corresponding author. 1Code and datasets are available at https://github.com/plageon/HtmlRAG. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, June 0305, 2025, Woodstock, NY 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/25/06 https://doi.org/XXXXXXX.XXXXXXX CCS Concepts Information systems Web search engines. Keywords HTML, Retrieval-Augmented Generation, Large Language Model ACM Reference Format: Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and JiRong Wen. 2024. HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems. In Proceedings of TheWebConf 2025 (Conference acronym XX). ACM, New York, NY, USA, 14 pages. https://doi. org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nLarge Language Models (LLMs) have been proven to have powerful\ncapabilities in various natural language processing tasks [42, 44, 46].\nHowever, at the same time, LLMs show deficiencies such as forget-\nting long-tailed knowledge [28], offering outdated knowledge [3],\nand hallucination [38, 39, 74]. Retrieval-augmented generation\n(RAG) utilizes a retrieval system to fetch external knowledge and\naugment the LLM. It has proved effective in mitigating hallucina-\ntions of LLMs [41, 76]. Many RAG systems, such as Perlexity [47]\nand SearchGPT [43], have been developed, and they commonly use\nWeb search engines as the underlying retrieval systems.",
            "content": "Traditional RAG pipelines typically use plain text as the format for retrieved knowledge [21, 63]. HTML documents from the Web are often converted into plain text and concatenated with the users query before being fed into the LLM. We found that converting HTML to plain text leads to the loss of structural and semantic information. Figure 1 illustrates that web page containing tabular form becomes disordered when converted to plain text. Even worse, original HTML tags, such as <code> and <a>, denoting important information, are discarded during conversion. Thus, in this paper, we tend to investigate an intuitive idea: Can we take HTML as the format of external knowledge in RAG systems to preserve the information in HTML documents to larger extent? Taking HTML as the format of external knowledge offers several advantages beyond preserving the information inherent in HTML documents. During pre-training, LLMs have encountered HTML Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. Pruning module, which functions upon the intrinsic tree structure of HTML. The pruning process is comprised of the following steps: (1) Building Block Tree. Each HTML document can be parsed into DOM tree [58]. We do not simply prune HTML on the DOM tree because it is too finely-grained [16, 62], which brings much computational cost. Instead, we propose to build corresponding block tree, in which the original DOM tree nodes are merged into hierarchical blocks. The granularity of the block tree can be adjusted by the degree of merging. (2) Pruning Blocks based on Text Embedding. We then prune the block tree using an on-the-shelf embedding model, because it is simple but effective way to calculate the blocks relevance scores with the users query based on their embedding similarity. We apply greedy pruning algorithm that removes blocks with lower similarity scores, and gets pruned block tree. However, we observe that the embedding model may fail to work well with the fine-grained blocks because embeddings learned for these small blocks are usually vague and inaccurate, so this pruning step is limited to coarse-grained block trees. (3) Generative Fine-grained Block Pruning. To prune the block tree further, we expand the leaf nodes of the pruned block tree and build finer-grained block tree. Since the generative model has longer context window, it can model the block tree globally and is not limited to modeling one block at time. Thus we further develop generative model to prune HTML over the fine-grained blocks. The generative model is supposed to calculate the score for each block, which is given by the generation probability of unique sequence indicating the block. The sequence is given by the path of HTML tags, starting from the root tag and walking down to the blocks tag and text (e.g., <html><body><div><p>block content...). Finally, according to the block scores, we apply similar greedy pruning algorithm to get the final pruned HTML. We conduct extensive experiments on six datasets including ambiguous QA, natural QA, multi-hop QA, and long-form QA. Experimental results confirm the superiority of HTML as the format of external knowledge over plain text. Our contributions are threefold: (1) We propose to take HTML as the format of knowledge in RAG systems, which retains information of the original HTML; (2) We propose simple but effective HTML cleaning algorithm; (3) We propose two-stage HTML pruning algorithm. This can be applied to most RAG systems and strikes balance between efficiency and effectiveness."
        },
        {
            "title": "2 Related Works\n2.1 Retrieval-Augmented Generation (RAG)\nRAG systems augment LLM with external knowledge. A typical\nRAG pipeline includes components such as a query rewriter [55],\na retriever [32, 53], a reranker [53, 63], a refiner [19, 22, 66], and a\nreader [5, 77]. This typical pipeline is widely used by mainstream\nRAG frameworks, such as LangChain [8] and LlamaIndex [35].\nMany works aim to optimize components in the pipeline, and pre-\nvious works also manage to enhance the performance of RAG in\nother ways. Some methods devise new RAG frameworks, like re-\ntrieving external knowledge actively when internal knowledge is\nmissing [5, 20, 55], or letting the LLM plan the retrieval process in\na straight line or a tree structure [27, 52]. However, most existing",
            "content": "Figure 1: Information loss in HTML to plain text conversion. documents [6, 15, 17], which means that they inherently possess the ability to understand HTML without requiring further finetuning [26, 73]. Recently, both proprietary and open source LLMs have begun to support increasingly longer input windows, making it feasible to input more extensive HTML documents [11, 69, 72]. Furthermore, documents in Latex, PDF, and Word formats can be converted to HTML with minimal loss, expanding the potential application of HTML as the format of external knowledge [7, 61, 64]. However, employing HTML as the knowledge format for LLMs also presents the challenge of handling longer input sequences and noisy contexts. Our preliminary experiments show that real HTML document from the Web contains over 80K tokens on average, among which over 90% of the tokens are CSS styles, JavaScript, Comments, or other meaningless tokens. Compared to the common maximum context window of current LLMs, which ranges from 32K to 128K, an individual document length of 80K is unacceptable. The noisy tokens. The aforementioned meaningless tokens in HTML documents can also affect the generation quality of LLMs. To solve this problem, in this paper, we devise HTML Cleaning module to remove semantically irrelevant content in HTML documents, while keeping the main content intact. We also adjust the HTML tree structure without losing semantic information, for example, merging multiple layers of single nested HTML tags and removing empty tags. These processes reduce the length of the HTML to 6% of its original size. Even after cleaning, HTML documents remain relatively long (over 4K each) to LLMs. To shorten the input context and remove the noise contained in the original retrieved documents, existing RAG systems have utilized different types of post-retrieval result refiners [19, 22, 66, 75]. These refiners extract the relevant text chunks or key sentences from the documents, regarding the users query and LLMs preference, and discard other content. These plaintext-based refiners cannot be directly applied to HTML because simply chunking HTML without considering its structure may generate unreasonable chunks. Hence, we further design an HTML HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY RAG systems take plain text as the format of external knowledge. Instead, we propose to take HTML as the format of external knowledge, and we believe using HTML can keep richer semantics in retrieved results."
        },
        {
            "title": "2.2 Post-Retrieval Process of RAG\nRAG systems usually apply post-retrieval processes (i.e., result refin-\ners) to extract only the useful content to shorten the input context\nsent to LLMs. The chunking-based refiner is a widely used solution,\nwhich first chunks the text according to certain rules, and then uses\na reranking model to select top chunks with high relevance [25, 40].\nAnother solution is abstractive refiner, which utilizes a text-to-\ntext language model to generate abstracts of results [14, 19, 66].\nSome works use off-the-shelf abstractive models [70, 71] or fine-\ntuned abstractive models [19] to summarize retrieved results in\na segmented and hierarchical manner. Others leverage the logits\nof language models to determine the importance of words within\ndocuments [33, 37].",
            "content": "The aforementioned post-retrieval result refiners are all based on plain text. The existing chunking-based methods cannot be directly applied to HTML because simply chunking HTML without considering its structure may generate unreasonable chunks. Furthermore, the abstractive refiners may have problems such as difficulty in dealing with excessively long HTML, high computational cost, or limited understanding of HTML. To alleviate these problems, in this paper, we propose to prune HTML based on its DOM structure."
        },
        {
            "title": "3 Methodology\nIn this paper, we propose HtmlRAG, which uses HTML instead of\nplain text as the format of retrieved knowledge in RAG systems,\naiming to keep richer semantic and structured information that is\nmissing in plain text. We emphasize that HTML is a popular data\nformat for documents in a knowledge base and other document\nformats can be easily converted into HTML.",
            "content": "Taking HTML as the format of external knowledge presents new challenge of excessively long context. Hence, in HtmlRAG, we propose to prune the original HTML documents into shorter ones progressively. We first apply an HTML cleaning module (3.2) to remove useless elements and tags. We then propose two-step structure-aware pruning method to further refine the resulting HTML (3.4). More specifically, we delete less important HTML blocks with low embedding similarities with the input query (3.4.1), and then conduct finer block pruning with generative model (3.4.2). The overview of our method is shown in Figure 2."
        },
        {
            "title": "3.2.1 HTML Content Cleaning. The HTML documents retrieved\nfrom the Web contain a large amount of extra content that is invis-\nible to human users, such as HTML tags, CSS, JavaScript, etc. Most\nof the HTML tags provide rich structural information that helps\nthe LLM understand the HTML, while CSS and JavaScript content\nprovide limited assistance. So the specific cleaning steps, which are\nalmost lossless, are as follows: (1) We remove CSS styles, Comments,\nand JavaScript; (2) We clear lengthy HTML tag attributes.",
            "content": "Lossless Structural Compression. We find that in most HTML 3.2.2 documents, their original HTML structure contains redundancies. We can conduct the following compression to the HTML structure without losing semantic information: (1) We merge multiple layers of single-nested tags. For example, we simplify <div><div><p>some text</p></div></div> to <p>some text</p>; (2) We removed empty tags, such as <p></p>."
        },
        {
            "title": "Construction",
            "content": "To prune all retrieved HTML documents as whole, we first concatenate all retrieved HTML documents together, and use Beautiful Soup [50] to parse the concatenated HTML document to single DOM tree. Pruning HTML using the DOM tree is the most natural way, but the DOM tree is so finely-grained that numerous nodes and the deep tree structure bring huge computational costs. Considering the above problem, we propose an optimized tree structure that models HTML, which is not so fine-grained. Ideally, the granularity of the tree structure can be adjusted for different pruning requirements. We term it as block tree, and we set the maximum number of words per block, 𝑚𝑎𝑥𝑊 𝑜𝑟𝑑𝑠 to control the granularity of the block tree. In terms of block tree construction, we start from DOM tree, and we merge fragmented child nodes Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. Figure 2: HTML for RAG pipeline overview into their parent and treat them as block. We can recursively merge blocks or child nodes into their parent to form bigger block under the condition that the number of words in block does not exceed 𝑚𝑎𝑥𝑊 𝑜𝑟𝑑𝑠. After merging, original leaf nodes that are unable to be merged are also regarded as blocks. Algorithm details are demonstrated in Appendix B."
        },
        {
            "title": "3.4.1 Pruning Blocks based on Text Embedding. The refining pro-\ncess is expected to shorten the retrieval results while preserving\nkey information as much as possible. A straightforward idea is to\nextract plain text in the block and calculate a similarity score with\nthe user’s query using text embeddings. Then we use a greedy algo-\nrithm to prune the block tree by deleting low-similarity blocks and\nretraining higher ones. In practice, we keep deleting the block with\nthe lowest relevance until the total length of the HTML documents\nsatisfies the context window we set. After block deleting, redun-\ndant HTML structures will re-appear, so we re-adjust the HTML\nstructure, meaning multiple layers of single-nested tags are merged\nand empty tags are removed. The detailed pruning algorithm is\ndemonstrated in Appendix B.",
            "content": "The embedding-based HTML pruning algorithm is lightweight but effective. It adapts to the HTML format better compared to plaintext-based refiners. However, it still has limitations, mainly reflected in the following aspects: (1) The embedding models context window is limited to the scope of text within the block each time. It does not directly compare candidate blocks in single inference. Thus the embedding model lacks global view of the document information; (2) The embedding model cannot handle block trees with finer granularity, because the text within most blocks is not long enough for the embedding model to obtain semantic features."
        },
        {
            "title": "3.4.2 Generative Fine-Grained Block Pruning. To further prune\nblocks with a finer granularity, we expand the leaf nodes of the\npruned block tree and get a finer-grained block tree. Given the lim-\nitations of the embedding-model-based block pruning, we propose\nto use a generative model because it has a long context to cover\nthe whole block tree and is not limited to modeling one block at a\ntime. Yet processing the cleaned HTML directly with a generative\nmodel is inappropriate because the cleaned HTML is long (60K on\naverage), which brings much computational cost. Similarly, the gen-\nerative model is supposed to calculate scores for blocks. Inspired by\nCFIC [48], which takes the text chunk’s sequence generation prob-\nability as the score for that chunk, we propose to use a sequence\nof tags to identify a block. Specifically, the sequence consists of\ntags starting from the root tag and walking down to the block’s\ntag, and we term this sequence as “block path”. In the inference\nphase, the generative model follows the structure of the block tree\nand calculates the scores of blocks in the block tree. The scores of\nblocks are derived from the token logits, as displayed in Figure 3.\nAt last, we use the same block pruning operation as we mention in\n§3.4.1 to obtain the refined HTML document.",
            "content": "The details of the generative fine-grained block pruning module are introduced in the remaining section. (1) Training Path-aware Generative Model. Long-context LLMs are capable of modeling long-context input containing HTML format and following instructions [10, 36]. Considering the computational cost, we employ an existing lightweight long-context LLM as the foundation model. The model input is the concatenation of an HTML, the query, and an instruction, as demonstrated in Figure 4. The instruction is specially designed to help the LLM understand this path generation task, but we find that the unfinetuned LLM does not meet our requirements. We attribute this to the fact that existing LLMs have not encountered similar tasks or HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY Figure 3: Block score calculation. The block tree is transformed into the token tree with tokenizer, and corresponding HTML tags and tokens are marked with the same colors. Token generation probabilities are in the upper right corner, and tokens in dashed boxes do not require inference. In the upper right corner of the block tree, the block probabilities are displayed, which can be derived from the corresponding token probabilities. instructions in either pre-training data or instruction fine-tuning data, because the path generation task is proposed for the first time. Thus we fine-tune the generative model to align with the target of generating the path for the most relevant block. So we design the output format as shown in Figure 4: the block path, followed by the block content. The block content is appended to provide an extra supervising signal that helps the generative model learn the features of the most relevant block. Additionally, to discriminate between children with the same tag name, we append number to the end of the original tag name. For example, two children with the same <div> tag are renamed as <div1> and <div2>. We collect small amount of supervised data to enhance the models capability in block path generation. Following the typical SFT process [49], the steps for training data collecting, filtering, and constructing are as follows: First, we sample queries from the training set of several open-source QA datasets. For each query, we retrieve couple of related HTML documents using the online search engine Bing. Then we clean the retrieved HTML, and prune the HTML with the embedding model. By adjusting the output length in HTML pruning, we get pruned HTML documents of various lengths, ranging from 2K tokens to 32K tokens. After that, we build block tree from each HTML document pruned by the embedding model, and calculate the exact match score for the content within blocks with the gold answer. To ensure the data quality, we discard samples in which no blocks content exactly matches the gold answer, meaning highly relevant HTML documents are not retrieved. More training details are discussed in Appendix A. (2) Efficient Tree-Based Inference with Dynamic Skipping. During inference, the generative model is supposed to calculate block scores, and the score for block 𝑏 is Score(𝑏). Each block has block path, and we first tokenize it to tokens {𝑡1, 𝑡2, , 𝑡𝑁 )}, suppose it has 𝑁 tokens in total (e.g., <html><div> is tokenized to {<, html, ><, div, >}). Given the models input sequence 𝑖𝑛𝑝𝑢𝑡 and 𝑛 1 already generated tokens, the generative model GenModel calculates the logit of the 𝑛-th token 𝑡𝑛 in the output {HTML} **{Question}** Input: **HTML**: **Question**: Your task is to identify the most relevant text piece to the given question in the HTML document. This text piece could either be direct paraphrase to the fact, or supporting evidence that can be used to infer the fact. The overall length of the text piece should be more than 20 words and less than 300 words. You should provide the path to the text piece in the HTML document. An example for the output is: <html1><body><div2><p>Some key information... Output: <html1><body><div2><p>At the historic 2018 Royal Rumble, Shinsuke Nakamura won the Mens Royal Rumble. . . Figure 4: The prompt for the generative model. sequence as below: Logits(𝑡𝑛) = GenModel(𝑡𝑛 {𝑖𝑛𝑝𝑢𝑡, 𝑡1, , 𝑡𝑛1}). (1) We propose an efficient tree-based inference, and the tree is termed as the token tree, which has one-to-one correspondence with the block tree, given specific tokenizer. We merge tokenized block paths to get the block tree, as Figure 3 shows. For example, {<, html, ><, nav, >} and {<, html, ><, div, >} share the same prefix, {<, html, ><}, and can be merged. Ultimately, the 𝑖-th token in the tokenized block path will appear at the 𝑖-th level of the token tree. After the token tree construction, we calculate the probabilities of tokens in the token tree. The calculation has the following conditions: (1) The probability of the root node is 1.0, which is often <, depending on the tokenizer; (2) The probabilities of singleton child nodes, which have no siblings, are 1.0; (3) The probabilities of other nodes are calculated by the generative model 𝐺𝑒𝑛𝑀𝑜𝑑𝑒𝑙. Suppose token 𝑡𝑛 has 𝐾 siblings, which Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. 𝑛, 𝑡 are the 𝑛-th token in the output sequence, we get the logits of siblings {𝑡 1 𝑛, } by Equation (1) and take the softmax of logits as probabilities. In summary, the probability of token 𝑡𝑘 𝑛 (the 𝑛-th token in the tokenized block path, and the 𝑘-th sibling) is given by: P(𝑡𝑘 𝑛 ) = 1.0, exp(Logits(𝑡𝑘 𝑛 ) ) 𝑖=1 exp(Logits(𝑡𝑖 (cid:205)𝐾 𝑛 ) ) if 𝑛 = 1 or 𝐾 = 1; overwise. , (2) In the first two conditions, it is needless to infer with the generative model, meaning many tokens can be skipped. This brings down the inference computational cost. Apart from token skipping, the order of token logit calculation also matters lot in computational cost. We apply depth-first algorithm to traverse the token tree and calculate token logits so that the tokens that are calculated sequentially share the longest prefix sequence. This strategy reuses the KV cache of prefix sequences at most. Algorithm details are displayed in Appendix B. At last, we transform the generation probabilities from the token tree back to the block tree so that we can calculate block scores. To prevent precision overflow, we take the sum of the logarithm of token probabilities as the score of the block 𝑏: Score(𝑏) = 𝑁 𝑖=1 log(P(𝑡𝑖 )). (3) After we get the block scores, we reuse the greedy block pruning algorithm introduced in 3.4.1 to get the finely pruned HTML."
        },
        {
            "title": "4.1 Datasets\nWe select six datasets, including: (1) ASQA [54]: a QA dataset con-\nsists of ambiguous questions that can be answered by multiple\nanswers supported by different knowledge sources; (2) Hotpot-\nQA [67]: a QA dataset consists of multi-hop questions; (3) NQ [29]:\nA QA dataset containing real user’s queries collected by Google; (4)\nTrivia-QA [24]: a QA dataset containing real user’s questions; (5)\nMuSiQue [56]: A synthetic multi-hop QA dataset; (6) ELI5 [13]: A\nlong-form QA dataset with questions collected from Reddit forum.\nWe randomly sample 400 questions from the test set (if any) or\nvalidation set in the original datasets for our evaluation.",
            "content": "To simulate the real industrial web search environment, we require real web pages from the Web in HTML format as retrieved documents. However, the widely used Wikipedia search corpus mainly consists of pre-processed passages in plain text format. So, we apply Bing search API in the US-EN region to search for relevant web pages, and then we scrap static HTML documents through URLs in returned search results. We provide the URLs and corresponding HTML documents in our experiments for reproduction."
        },
        {
            "title": "4.2 Evaluation Metrics\nOur method aims to enhance the overall performance of RAG, so\nwe evaluate the LLM’s response as the end-to-end result. We choose",
            "content": "different evaluation metrics for datasets according to their questionand-answer formats. For Hotpot-QA and MuSiQue, in which each question is annotated with single short answer, we report Exact Match. For ASQA, NQ, and Trivia-QA, whose questions are annotated with several short answers, we report Exact Match and Hit@1. Hit@1 means at least one answer of the annotated answers finds the exact match in the LLMs response. ELI5 is annotated with long-form answers, and we report ROUGE-L [34] and BLEU [45]."
        },
        {
            "title": "4.5 Experimental Results\nMain experimental results are demonstrated in Table 1. Our method,\nHtmlRAG meets or exceeds the baselines across all metrics on the\nsix datasets. This demonstrates the effectiveness of HTML pruning.\nAdditionally, we make the following observations:",
            "content": "(1) For chunking-based refiners, we followed LangChains [8] chunking rule, which chunks according to HTML tag headings (h1, h2, etc.). Although this chunking strategy considers certain HTML structures, it does not utilize the structural information as effectively as our method. Moreover, converting the final output to plain text still results in loss of HTML structural and semantic information. Among the three rerankers we applied, the sparse retriever BM25 is inferior to two dense retrievers. Among two dense retrievers, the encoder-based BGE performs better than the decoderbased e5-mistral, despite the latter having more parameters. HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY Table 1: Results of HtmlRAG and baselines under the short-context setting. Hit@1 is the proportion of instances where at least one short answer matches. The best and second best results are in bold and underlined. The symbol signifies that our model achieves superior results among baselines in statistically significant manner (t-test, 𝑝-value < 0.05). Method ASQA Hotpot-QA NQ Trivia-QA MuSiQue ELI5 Hit@ EM EM Hit@1 EM Hit@1 EM EM ROUGE-L BLEU Llama-3.1-8B-Instruct-4K BM25 BGE E5-Mistral LongLLMLingua JinaAI Reader 45.00 68.50 62.50 59.25 53. 19.84 31.47 28.51 26.34 23.14 36.25 43.25 38.50 40.75 34.00 40.75 59.00 56.50 55.25 47.25 30.66 44.59 41.73 41.82 34.41 84.75 92.25 90.00 90.00 84.75 26.17 27.50 27.05 27.02 24. HtmlRAG 71.75 33.31 43.75 61.75 45.90 91.75 27.82 Llama-3.1-70B-Instruct-4K BM25 BGE E5-Mistral LongLLMLingua JinaAI Reader 49.50 68.00 63.00 62.50 55.25 21.95 30.57 28.75 27.74 23. 38.25 41.75 36.75 45.00 34.25 47.00 59.50 59.50 56.75 48.25 35.56 45.05 44.07 42.89 35.40 88.00 93.00 90.75 92.50 90.00 HtmlRAG 68.50 30.53 46.25 60.50 45.26 93.50 25.63 27.04 26.27 27.23 25. 27.03 5.75 10.00 9.00 9.00 6.75 8.75 9.50 12.50 11.00 10.25 9.25 15.90 15.87 15.77 16.08 15.80 15. 16.15 16.20 16.17 15.84 16.06 13.25 16.33 6.56 6.30 5.85 6.45 5.65 5.84 6.99 6.64 6.72 6.39 6. 6.77 Table 2: Results of HtmlRAG without pruning and baselines under the long-context setting. Hit@1 is the proportion of instances where at least one short answer matches. The best and second best results are in bold and underlined. The symbol signifies that our method achieves superior results among baselines in statistically significant manner (t-test, 𝑝-value < 0.05). Method ASQA Hotpot-QA NQ Trivia-QA MuSiQue ELI5 Hit@1 EM EM Hit@1 EM Hit@1 EM EM ROUGE-L BLEU Llama-3.1-8B-Instruct-128K Vanilla HTML Plain Text Markdown 47.75 61.50 61.75 20.08 27.82 26.70 HtmlRAG w/o Prune 61.00 26.70 28.75 39.25 37.50 39.50 47.25 59.25 57.50 36.09 44.31 42. 85.00 94.00 91.50 24.85 28.23 26.67 59.00 43.46 92.00 27.50 Vanilla HTML Plain Text Markdown 44.00 59.75 56.00 17.52 25.16 24.00 28.00 41.00 39.00 46.75 59.75 57.00 36.06 44.11 42. 81.50 93.50 92.00 22.58 26.75 26.43 Llama-3.1-70B-Instruct-128K 6.00 7.75 7.50 8.75 3.25 8.75 8. HtmlRAG w/o Prune 58.75 25.28 42.25 58.00 43.65 95.00 27.21 10.75 16.13 16.02 16.12 15.62 15.69 16.88 16. 16.57 6.28 6.35 5.91 5.87 5.16 7.44 6.74 6.32 (2) Among the abstractive refiners, LongLLMLingua is not optimized for HTML documents, so its extraction ability is affected when dealing with HTML. Additionally, the plain text output loses structural information, resulting in inferior performance compared to our method. The JinaAI-reader generates the refined Markdown given the HTML input. However, token-by-token decoding with long input and output lengths is not only challenging for end-to-end generative models, but also has high computational cost."
        },
        {
            "title": "4.6 Further Analysis\n4.6.1 The Effectiveness of HTML Cleaning. To validate the priority\nof HTML as the format of retrieved knowledge, we compare our\nHTML cleaning module, namely the results of HtmlRAG without",
            "content": "pruning, with other rule-based cleaning strategies, including (1) Vanilla HTML; (2) Plain Text: The plain text extracted with an onthe-self package BeautifulSoup [50]; (3) Markdown: The Markdown converted by an on-the-self converter Markdownify [2]. Additional experiments on token count show that HTML-Clean drops over 94.07% tokens of the original HTML, while the number for plain text and Markdown conversion are 96.71% and 90.32% respectively. The cleaned HTML is still long, so we conduct experiments under long-context setting (128K), as shown in Table 2. When HTML is taken as the format of external knowledge, HtmlRAG without pruning meets or outperforms plain text and Markdown on most datasets, demonstrating its validity. Besides, we make the following observations: (1) Unprocessed HTML documents contain Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. Table 3: Ablation studies for HtmlRAG. ASQA Hotpot-QA NQ Trivia-QA MuSiQue Method HtmlRAG Hit@ 68.50 59.50 (9.00%) EM 30.53 EM 46.25 Hit@ 60.50 56.25 (4.25%) EM 45.26 Hit@1 93.50 EM 27.03 EM 13.25 w/o Block Tree 42.07 (3.19%) 92.00 (1.50%) 26.59 (0.44%) 8.00 (5.25%) w/o Prune-Embed 56.75 (11.75%) 24.05 (6.48%) 37.50 (8.75%) 49.50 (11.00%) 37.27 (7.99%) 91.75 (1.75%) 26.02 (1.01%) 9.75 (3.50%) 42.91 (2.35%) 89.50 (4.00%) 25.55 (1.48%) 7.00 (6.25%) w/o Prune-Gen 25.50 (5.03%) 40.25 (6.00%) 26.74 (3.79%) 38.75 (7.50%) 57.75 (2.75%) 62.00 (6.50%) Figure 5: Experimental results for the impact of block tree granularity. The results of Prune-Embed and Prune-Gen are represented in bar chart, with red dashed horizontal line indicating the performance of the strong baseline method, chunking-based refiner with BGE (BGE-Chunk-Rerank). large amount of irrelevant content, so all cleaning algorithms show improvements over vanilla HTML. (2) more capable LLM (70B) performs better than less capable one (8B) when taking HTML as the format of external knowledge. This indicates that more powerful models are better at understanding the complex information within HTML."
        },
        {
            "title": "4.6.2 Ablation Study. We conduct ablation studies to demonstrate\nthe effectiveness of each component in HtmlRAG, including block\ntree construction (Block Tree), HTML pruning with the embedding\nmodel (Prune-Embed), and HTML pruning with the generative\nmodel (Prune-Gen). From the results in in Table 3, we can see: (1) In\nthe ablation study for block tree construction, we use the DOM tree\ninstead of the block tree. Units in the DOM tree are so fragmented\nthat the embedding model fails to capture sufficient semantic fea-\ntures, thus causing a drop in performance. The performance of\nthe generative model is also affected due to the increase in the\nlength of block paths. (2) In the ablation study for pruning with\nthe embedding model, we only use the generative model to prune\nthe cleaned HTML. Without the basically pruned HTML by the\nembedding model, the input to the generative model becomes very\nlong (exceeds 32K), resulting in high computational costs and poor\nperformance. (3) In the ablation study for pruning with the genera-\ntive model, we only use the embedding model to prune the cleaned\nHTML. The result is inferior compared to the further pruned HTML\nusing the generative model, because the embedding model’s global\nunderstanding and ability to process finely-grained block trees are\ninferior to the generative model.",
            "content": "Impact of Block Tree Granularity. The most critical hyper4.6.3 parameter in HTML pruning is granularity. coarse granularity reduces the flexibility of pruning, while fine granularity makes it difficult to extract text embeddings for small blocks, and leads Result Length # Params Storage # In-Tokens # Out-Tokens BGE Prune-Embed Prune-Gen LLM Chat 200M 200M 3B 70B 2.5G 2.5G 7.2G 131G 93.54K 152.5K 6750 740.3 2653 28.70 182.9 Table 4: Analysis of inference cost on ELI5 dataset We compare the chunking-based refiner using BGE (BGE), the two HTML pruning steps basing on the text embedding (PruneEmbed) and the generative model (Prune-Gen) in HtmlRAG, and LLM chatting (LLM Chat) by model parameters, storage, average input tokens, and average output tokens. to overly long block paths for the generative model, so we need to find balancing points. In Figure 5, we experiment with HTML pruning under different granularity ranging from 64 to 512 words, and compare their result with strong baseline. Prune-Embed stands for using the basically pruned HTML by the embedding model, and Prune-Gen stands for using the finely pruned HTML by the generative model. It can be observed that the generative model adapts to finer granularity than the embedding model and generally outperforms the embedding model. This validates the rationality of our two-stage pruning method."
        },
        {
            "title": "4.6.4 Light Weight HTML Pruning. To show that our HTML prun-\ning method does not significantly increase the computational cost\ndespite using an LLM with 3B parameters, we conduct an efficiency\nanalysis. Table 4 shows the computational cost of our method com-\npared to the baseline and the cost of the LLM’s inference. We can\nsee that the HTML pruning with the embedding model still main-\ntains a similar computational cost to the chunking-based refiner.",
            "content": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY The computational cost of the generative model is bit higher than the baseline but still much lower than the cost of the LLM for chatting. Additional experiments show that there are over 45% of nodes that can be skipped, explaining the little increase in the generative models computational cost. Analysis of token counts shows the average token count for all retrieved knowledge in HTML format is 1.6M, suppose we retrieve 20 HTML documents. HTML cleaning reduces the token count to 135K, HTML pruning based on text embedding reduces it to 8K, and generative HTML pruning reduces it to 4K. In typical RAG scenarios, since the computational cost of HTML pruning is much less than the inference cost of the LLM, we recommend using complete HTML pruning to achieve the best results. Meanwhile, in some resource-limited scenarios where the cost of HTML pruning is also concern, we suggest using only the basically pruned HTML from the embedding model. Basically pruned HTML also yields performance that meets or surpasses the chunking-based refiner, as we can observe from Figure 5."
        },
        {
            "title": "5 Conclusion and Future Works\nIn this work, we propose taking HTML as the format of external\nknowledge in RAG systems. To tackle the additional tokens brought\nby HTML, we design HTML cleaning and HTML pruning to shorten\nHTML while retaining key information. Experiments show that\nHtmlRAG outperforms existing post-retrieval processes based on\nplain text, and validates the priority of HTML as the format of\nretrieved knowledge. Moreover, this work opens up a new research\ndirection and provides a simple and effective solution. We believe\nas LLMs become more powerful, HTML will be more suitable as the\nformat of external knowledge. We also hope that future works will\npropose better solutions for processing HTML in RAG systems.",
            "content": "References [1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. CoRR abs/2404.14219 (2024). https://doi.org/10.48550/ARXIV.2404.14219 arXiv:2404.14219 [2] AlexVonB, Matthew Dapena-Tretter, and André van Delft. 2024. pythonmarkdownify. https://github.com/matthewwithanm/python-markdownify [3] Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. 2024. Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, LunWei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 64166432. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.383 [4] Ryan Aponte, Ryan A. Rossi, Shunan Guo, Jane Hoffswell, Nedim Lipka, Chang Xiao, Gromit Yeuk-Yin Chan, Eunyee Koh, and Nesreen K. Ahmed. 2023. ML-based Approach for HTML-based Style Recommendation. In Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben (Eds.). ACM, 913. https://doi.org/10.1145/ 3543873.3587300 [5] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum? id=hSyW5go0v [6] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023. Emergent and Predictable Memorization in Large Language Models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 59404fb89d6194641c69ae99ecdf8f6d-Abstract-Conference.html [7] Deyan Ginev Bruce R. Miller, mailto:bruce.miller@nist.gov. 2024. LaTeXML. https://github.com/brucemiller/LaTeXML [8] Harrison Chase. 2022. LangChain. https://github.com/langchain-ai/langchain [9] Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. 2022. XDoc: Unified Pre-training for Cross-Format Document Understanding. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 10061016. https: //doi.org/10.18653/V1/2022.FINDINGS-EMNLP.71 [10] Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. WebSRC: Dataset for Web-Based Structural Reading Comprehension. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 41734185. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.343 [11] Zican Dong, Tianyi Tang, Junyi Li, and Wayne Xin Zhao. 2023. Survey on Long Text Modeling with Transformers. CoRR abs/2302.14502 (2023). https: //doi.org/10.48550/ARXIV.2302.14502 arXiv:2302. [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The Llama 3 Herd of Models. CoRR abs/2407.21783 (2024). https://doi.org/10.48550/ARXIV.2407.21783 arXiv:2407.21783 [13] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, 35583567. https://doi.org/10.18653/V1/P19-1346 [14] Henry Gilbert, Michael Sandborn, Douglas C. Schmidt, Jesse Spencer-Smith, and Jules White. 2023. Semantic Compression with Large Language Models. In Tenth International Conference on Social Networks Analysis, Management and Security, SNAMS 2023, Abu Dhabi, United Arab Emirates, November 21-24, 2023. IEEE, 18. https://doi.org/10.1109/SNAMS60348.2023.10375400 [15] Dirk Groeneveld, Iz Beltagy, Evan Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. OLMo: Accelerating the Science of Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1578915809. https://doi.org/10.18653/V1/2024.ACL-LONG.841 [16] Yu Guo, Zhengyi Ma, Jiaxin Mao, Hongjin Qian, Xinyu Zhang, Hao Jiang, Zhao Cao, and Zhicheng Dou. 2022. Webformer: Pre-training with Web Pages for Information Retrieval. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022, Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 15021512. https://doi.org/10.1145/ 3477495.3532086 [17] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin V. Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. Understanding HTML with Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 28032821. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.185 [18] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. CoRR abs/2310.06825 (2023). https: //doi.org/10.48550/ARXIV.2310.06825 arXiv:2310.06825 [19] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 16581677. https://doi.org/10.18653/V1/2024.ACL-LONG.91 [20] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane DwivediYu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 79697992. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.495 [21] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024. FlashRAG: Modular Toolkit for Efficient Retrieval-Augmented Generation Research. CoRR abs/2405.13576 (2024). https://doi.org/10.48550/ARXIV.2405. 13576 arXiv:2405. [22] Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou. 2024. BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 750761. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.42 [23] JinaAI. 2024. Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown. https://jina.ai/news/reader-lm-small-language-modelsfor-cleaning-and-converting-html-to-markdown/. [Online; accessed 2024-1005]. [24] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, 16011611. https://doi.org/10.18653/V1/P17-1147 [25] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 67696781. https://doi.org/10.18653/ V1/2020.EMNLP-MAIN.550 [26] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language Models can Solve Computer Tasks. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 7cc1005ec73cfbaac9fa21192b622507-Abstract-Conference.html [27] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. 2023. Tree of Clarifications: Answering Ambiguous Questions with RetrievalAugmented Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 9961009. https://doi.org/10.18653/V1/2023.EMNLPMAIN.63 [28] Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2024. Understanding Catastrophic Forgetting in Language Models via Implicit Inference. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id= VrHiF2hsrm [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics 7 (2019), 452466. https://doi.org/10.1162/TACL_A_00276 [30] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. 2024. AutoWebGLM: Bootstrap And Reinforce Large Language Model-based Web Navigating Agent. CoRR abs/2404.03648 (2024). https://doi.org/10.48550/ARXIV. 2404.03648 arXiv:2404.03648 [31] Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2021. StructuralLM: Structural Pre-training for Form Understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 63096318. https://doi.org/10.18653/V1/2021.ACLLONG.493 [32] Xiaoxi Li, Zhicheng Dou, Yujia Zhou, and Fangchao Liu. 2024. CorpusLM: Towards Unified Language Model on Corpus for Knowledge-Intensive Tasks. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 2637. https://doi.org/10.1145/3626772.3657778 [33] Yucheng Li. 2023. Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering. CoRR abs/2304.12102 (2023). https://doi.org/10.48550/ARXIV.2304.12102 arXiv:2304.12102 [34] Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 7481. https://aclanthology.org/W04-1013 [35] Jerry Liu. 2022. LlamaIndex. https://doi.org/10.5281/zenodo.1234 [36] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. 2024. VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? CoRR abs/2404.05955 (2024). https://doi.org/10.48550/ARXIV.2404.05955 arXiv:2404.05955 [37] Yang Liu. 2019. Fine-tune BERT for Extractive Summarization. abs/1903.10318 (2019). arXiv:1903.10318 http://arxiv.org/abs/1903.10318 [38] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 98029822. https://doi.org/10.18653/V1/2023.ACL-LONG.546 [39] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Finegrained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 12076 12100. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.741 [40] Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Özyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, and Hong Yu. 2024. ReALM: Reference Resolution as Language Modeling. In Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL 2024, Kyoto, Japan, September 18 - 20, 2024, Tatsuya Kawahara, Vera Demberg, Stefan Ultes, Koji Inoue, Shikib Mehri, David M. Howcroft, and Kazunori Komatani (Eds.). Association for Computational Linguistics, 5165. https://aclanthology.org/2024.sigdial-1.5 [41] Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When Do LLMs Need Retrieval Augmentation? Mitigating LLMs Overconfidence Helps Retrieval Augmentation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1137511388. https://doi.org/10.18653/V1/2024.FINDINGS-ACL. [42] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). https: //doi.org/10.48550/ARXIV.2303.08774 arXiv:2303.08774 [43] OpenAI. 2024. SearchGPT Prototype. https://www.perplexity.ai/ [Online; accessed 2024-10-14]. [44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in CoRR HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html [45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311318. https://doi.org/10.3115/1073083. 1073135 [46] Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2023. Bidirectional Language Models Are Also Few-shot Learners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https: //openreview.net/forum?id=wCFB37bzud [47] PerplexityAI. 2024. Perplexity. https://openai.com/index/searchgpt-prototype/ [48] Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou. 2024. Grounding Language Model with Chunking-Free In-Context Retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 12981311. https://doi.org/10.18653/V1/2024.ACL-LONG.71 [49] Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, and Xing Sun. 2024. Unleashing the Power of Data Tsunami: Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models. CoRR abs/2408.02085 (2024). https://doi.org/10.48550/ARXIV.2408.02085 arXiv:2408.02085 [50] Leonard Richardson. 2024. Beautiful Soup. https://www.crummy.com/software/ BeautifulSoup/ [51] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333389. https://doi.org/10.1561/1500000019 [52] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 92489274. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.620 [53] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: RetrievalAugmented Black-Box Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, Kevin Duh, Helena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational Linguistics, 83718384. https://doi.org/10.18653/V1/2024.NAACL-LONG. [54] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 82738288. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.566 [55] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 44204436. https://doi.org/10.18653/V1/2024.ACL-LONG.242 [56] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. Trans. Assoc. Comput. Linguistics 10 (2022), 539554. https://doi.org/10.1162/TACL_A_ 00475 [57] Shin-Rong Tsai, Hsi-Yu Schive, and Matthew Turk. 2024. Libyt: Tool for Parallel In Situ Analysis with yt, Python, and Jupyter. In Proceedings of the Platform for Advanced Scientific Computing Conference, PASC 2024, Zurich, Switzerland, June 3-5, 2024, Katherine Evans and Olaf Schenk (Eds.). ACM, 25:125:10. https: //doi.org/10.1145/3659914.3659939 [58] W3Schools. 2024. What is the HTML DOM? https://www.w3schools.com/ whatis/whatis_htmldom.asp [Online; accessed 2024-10-14]. [59] Haochen Wang, Kai Hu, Haoyu Dong, and Liangcai Gao. 2024. DocTabQA: Answering Questions from Long Documents Using Tables. In Document Analysis and Recognition - ICDAR 2024 - 18th International Conference, Athens, Greece, August 30 - September 4, 2024, Proceedings, Part (Lecture Notes in Computer Science, Vol. 14804), Elisa H. Barney Smith, Marcus Liwicki, and Liangrui Peng (Eds.). Springer, 470487. https://doi.org/10.1007/978-3-031-70533-5_27 [60] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving Text Embeddings with Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1189711916. https://doi.org/10.18653/V1/2024.ACLLONG.642 [61] Lucy Lu Wang, Jonathan Bragg, and Daniel S. Weld. 2023. Paper to HTML: Publicly Available Web Tool for Converting Scientific Pdfs into Accessible HTML. SIGACCESS Access. Comput. 134, Article 1 (Jan. 2023), 1 pages. https: //doi.org/10.1145/3582298.3582299 [62] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. 2022. WebFormer: The Web-page Transformer for Structure Information Extraction. In WWW 22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, Frédérique Laforest, Raphaël Troncy, Elena Simperl, Deepak Agarwal, Aristides Gionis, Ivan Herman, and Lionel Médini (Eds.). ACM, 31243133. https://doi.org/10.1145/3485447.3512032 [63] Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. 2024. RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation. CoRR abs/2406.12566 (2024). https://doi.org/ 10.48550/ARXIV.2406.12566 arXiv:2406. [64] Michael Williamson, Jonathan Lehman, and Jacob Wang. 2024. mammoth.js. https://github.com/mwilliamson/mammoth.js [65] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. CoRR abs/2309.07597 (2023). https://doi.org/10.48550/ARXIV.2309.07597 arXiv:2309.07597 [66] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving RetrievalAugmented LMs with Context Compression and Selective Augmentation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id= mlJLVigNHp [67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, 23692380. https://doi.org/10.18653/V1/D18-1259 [68] Huaying Yuan, Zhicheng Dou, Yujia Zhou, Yu Guo, and Ji-Rong Wen. 2023. VILE: Block-Aware Visual Enhanced Document Retrieval. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023, Ingo Frommholz, Frank Hopfgartner, Mark Lee, Michael Oakes, Mounia Lalmas, Min Zhang, and Rodrygo L. T. Santos (Eds.). ACM, 31043113. https://doi.org/10.1145/3583780. [69] Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. ChatGLM: Family of Large Language Models from GLM-130B to GLM-4 All Tools. CoRR abs/2406.12793 (2024). https://doi.org/10.48550/ARXIV.2406.12793 arXiv:2406.12793 [70] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023. Extractive Summarization via ChatGPT for Faithful Summary Generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 32703278. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.214 [71] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023. SummIt: Iterative Text Summarization via ChatGPT. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 10644 10657. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.714 [72] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. ınftyBench: Extending Long Context Evaluation Beyond 100K Tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1526215277. https://doi.org/10.18653/V1/2024.ACLLONG.814 [73] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is Generalist Web Agent, if Grounded. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=piecKJ2DlB [74] Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, and José Hernández-Orallo. 2024. Larger and more instructable Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. language models become less reliable. Nature (2024), 18. [75] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip S. Yu. 2024. Trustworthiness in Retrieval-Augmented Generation Systems: Survey. arXiv:2409.10102 [cs.IR] https://arxiv.org/abs/2409.10102 [76] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. 2024. Metacognitive Retrieval-Augmented Large Language Models. In Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024, Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W. Lauw, and Roy Ka-Wei Lee (Eds.). ACM, 14531463. https://doi.org/10.1145/3589334.3645481 [77] Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng Dou. 2024. INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 27822809. https://doi.org/10.18653/V1/2024.ACL-LONG. Generative Model Training Details Here we introduce several critical hyper-parameters that define the training process of the generative model. The models max training context window is set to 35000 tokens. The model is trained for 3 epochs. The training is conducted on 4 computing nodes, with 32 Nvidia A800 GPUs, each having 80G memory. To manage memory usage and computational efficiency, 𝑝𝑒𝑟 _𝑑𝑒𝑣𝑖𝑐𝑒_𝑡𝑟𝑎𝑖𝑛_𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒 is set to 1, while 𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡_𝑎𝑐𝑐𝑢𝑚𝑢𝑙𝑎𝑡𝑖𝑜𝑛_𝑠𝑡𝑒𝑝𝑠 is set to 8, effectively simulating larger batch size during backpropagation. For parallelism, 𝑠𝑒𝑞_𝑝𝑎𝑟𝑎𝑙𝑙𝑒𝑙_𝑠𝑖𝑧𝑒 is set to 8, indicating that the model will distribute its computations across 8 devices if available. The 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔_𝑟𝑎𝑡𝑒 is set to 2e-5, striking balance between rapid convergence and avoiding divergence. The learning rate scheduler (𝑙𝑟 _𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒𝑟 _𝑡𝑦𝑝𝑒) is set to constant, meaning the learning rate remains unchanged throughout the training unless manually adjusted. For optimization, the Adam optimizer parameters (𝑎𝑑𝑎𝑚_𝑏𝑒𝑡𝑎1, 𝑎𝑑𝑎𝑚_𝑏𝑒𝑡𝑎2, and 𝑎𝑑𝑎𝑚_𝑒𝑝𝑠𝑖𝑙𝑜𝑛) are chosen as 0.9, 0.98, and 1e-8 respectively, to ensure stable gradient updates. The 𝑚𝑎𝑥_𝑔𝑟𝑎𝑑_𝑛𝑜𝑟𝑚 is set to 1.0 to prevent exploding gradients by clipping them if they exceed this norm. weight decay (𝑤𝑒𝑖𝑔ℎ𝑡_𝑑𝑒𝑐𝑎𝑦) of 1e-4 is used to regularize the model and prevent overfitting. 𝑤𝑎𝑟𝑚𝑢𝑝_𝑟𝑎𝑡𝑖𝑜 of 0.01 indicates that the learning rate will be gradually increased during the initial 1% of the training process before settling at the base learning rate. 𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡_𝑐ℎ𝑒𝑐𝑘𝑝𝑜𝑖𝑛𝑡𝑖𝑛𝑔 is enabled to save memory at the cost of increased computation time. DeepSpeed is configured for efficient distributed training. For ZeRO optimization (𝑧𝑒𝑟𝑜_𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑎𝑡𝑖𝑜𝑛), stage 3 is selected, which represents the highest level of parameter partitioning and offloading. Gradient clipping (𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡_𝑐𝑙𝑖𝑝𝑝𝑖𝑛𝑔) is set to 1.0, ensuring that the gradients do not grow too large, thus preventing potential issues like exploding gradients. The 𝑤𝑎𝑙𝑙_𝑐𝑙𝑜𝑐𝑘_𝑏𝑟𝑒𝑎𝑘𝑑𝑜𝑤𝑛 option is set to false, indicating that DeepSpeed will not provide detailed breakdown of the training time spent on different components of the training loop, which can be useful for profiling but may add some overhead. Mixed precision training using bfloat16 is set to auto, indicating that DeepSpeed will decide whether to use bfloat16 based on the capabilities of the system and the requirements of the model. Key Algorithms In this appendix section, we present all the algorithms mentioned in the main text using pseudo code, including the algorithm for constructing the block tree, the pruning algorithm using the embedding model, and the pruning algorithm using the generative model. To make it clear, we first define elements under certain node as follows: All sorts of elements under the node are referred to as node.content; Text wrapped by child tags is referred to as node.children; Text directly attached to the node is referred to as node.text. We show an example accordingly in Figure 6. To discriminate between children with the same HTML tag, we append number to the end of the original tag name. For example, two children with the same <div> tag are renamed as <div1> and <div2>. The block tree construction algorithm is demonstrated in Algorithm 1, which transforms DOM Tree 𝑇 into Block Tree 𝑇 . In the block tree, block is the smallest unit that be pruned in HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems Conference acronym XX, June 0305, 2025, Woodstock, NY Algorithm 1 Construct Block Tree 𝑇 from DOM Tree 𝑇 Algorithm 3 Token Probability Calculation 3: 1: procedure ConstructBlockTree(𝑇 ) Declare queue 𝑛𝑜𝑑𝑒𝑄𝑢𝑒𝑢𝑒 2: 𝑅 root node of 𝑇 Enqueue 𝑅 into 𝑛𝑜𝑑𝑒𝑄𝑢𝑒𝑢𝑒 while 𝑛𝑜𝑑𝑒𝑄𝑢𝑒𝑢𝑒 is not empty do 4: 5: 𝑛𝑜𝑑𝑒 Dequeue from 𝑛𝑜𝑑𝑒𝑄𝑢𝑒𝑢𝑒 if 𝑛𝑜𝑑𝑒 is leaf node then 𝑛𝑜𝑑𝑒.𝑏𝑙𝑜𝑐𝑘 node.content 𝑛𝑜𝑑𝑒.𝑖𝑠𝐿𝑒𝑎𝑓 True else if 𝑛𝑜𝑑𝑒.𝑐𝑜𝑛𝑡𝑒𝑛𝑡 < 𝑚𝑎𝑥𝑇𝑜𝑘𝑒𝑛𝑠 then Merge descendant nodes of 𝑛𝑜𝑑𝑒 𝑛𝑜𝑑𝑒.𝑏𝑙𝑜𝑐𝑘 node.content 𝑛𝑜𝑑𝑒.𝑖𝑠𝐿𝑒𝑎𝑓 True else Expand children of 𝑛𝑜𝑑𝑒 for each child of 𝑛𝑜𝑑𝑒 do Enqueue child into 𝑛𝑜𝑑𝑒𝑄𝑢𝑒𝑢𝑒 end for if 𝑛𝑜𝑑𝑒.𝑡𝑒𝑥𝑡 is not empty then 𝑛𝑜𝑑𝑒.𝑏𝑙𝑜𝑐𝑘 node.text 𝑛𝑜𝑑𝑒.𝑖𝑠𝐿𝑒𝑎𝑓 False end if end if end if end while return 𝑇 27: 28: end procedure Algorithm 2 Greedy Block Pruning 1: procedure GreedyBlockTreePruning(T) 𝑛𝑜𝑑𝑒𝑠 all nodes with blocks from 2: for each 𝑛𝑜𝑑𝑒 in 𝑛𝑜𝑑𝑒𝑠 do 3: 𝑛𝑜𝑑𝑒.𝑠𝑐𝑜𝑟𝑒 𝑅𝑒𝑙 (𝑞, 𝑛𝑜𝑑𝑒.𝑏𝑙𝑜𝑐𝑘) calculate semantic similarity between node 𝑛𝑜𝑑𝑒 and user request end for Sort 𝑛𝑜𝑑𝑒𝑠 by key 𝑛𝑜𝑑𝑒.𝑠𝑐𝑜𝑟𝑒 in ascending order while each 𝑛𝑜𝑑𝑒 in 𝑛𝑜𝑑𝑒𝑠 do 𝑛𝑜𝑑𝑒 the node with the lowest score if 𝑛𝑜𝑑𝑒.𝑖𝑠𝐿𝑒𝑎𝑓 then 𝑝𝑎𝑟𝑒𝑛𝑡 𝑛𝑜𝑑𝑒.𝑝𝑎𝑟𝑒𝑛𝑡 delete 𝑛𝑜𝑑𝑒 while 𝑝𝑎𝑟𝑒𝑛𝑡 .𝑐𝑜𝑛𝑡𝑒𝑛𝑡 is empty do 𝑝𝑎𝑟𝑒𝑛𝑡 𝑝𝑎𝑟𝑒𝑛𝑡 .𝑝𝑎𝑟𝑒𝑛𝑡 delete 𝑝𝑎𝑟𝑒𝑛𝑡 end while else delete 𝑛𝑜𝑑𝑒.𝑡𝑒𝑥𝑡 end if end while 19: 20: end procedure 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 1: procedure TraverseTokenTree 2: Declare queue 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 𝑡1 root node of 𝑇 𝑡1.𝑠𝑐𝑜𝑟𝑒 1.0 Push 𝑡1 into 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 while 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 is not empty do 𝑡𝑛1 Pop from 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛 Expand children of node 𝑝 : (𝑡 1 if 𝐾 = 0 (𝑝 is leaf node) then Set the score of 𝑡1 as 1.0 𝑛, 𝑡 𝑛, , 𝑡𝐾 𝑛 ) continue else if 𝐾 = 1 then 𝑡 1 𝑛.𝑠𝑐𝑜𝑟𝑒 1.0 Push the singleton child 𝑡 0 𝑛 into 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 else 𝑛 in 𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛 do 𝑝𝑟𝑒 𝑓 𝑖𝑥 {𝑖𝑛𝑝𝑢𝑡, 𝑡1, . . . , 𝑡𝑛1} for each 𝑡𝑘 𝑡𝑘 𝑛 .𝑠𝑐𝑜𝑟𝑒 Push 𝑡𝑘 (cid:205)𝐾 𝑛 into 𝑛𝑜𝑑𝑒𝑆𝑡𝑎𝑐𝑘 𝑒𝑥𝑝 (Logits(𝑡𝑘 𝑛 ) ) 𝑖=1 𝑒𝑥𝑝 (Logits(𝑡𝑖 𝑛 ) ) end for 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end if end while 21: 22: end procedure Figure 6: Node content explained subsequent steps. We use breadth-first algorithm to traverse all nodes in the DOM tree. Leaf nodes that are visited are directly considered as blocks. If the total number of tokens of all content under node is less than the number we set (maxWordss), we merge all the content of the node and consider it as block. Otherwise, we check the content of the node. The nodes children are to be visited in subsequent steps. The nodes text will be considered as block. It is noteworthy that if there are only children but no text under the node, it will not be considered as block. This algorithm merges fragmented nodes as block, until the number of tokens exceeds maxWords. Another key algorithm is greedy block pruning, as demonstrated in Algorithm 2. We greedily delete the block with the lowest score until the length of the HTML document meets the context window we set. To elaborate, when deleting block, if the block is leaf Conference acronym XX, June 0305, 2025, Woodstock, NY Tan et al. node, we delete the block directly. Otherwise, if the block consists of directly attached text under parent node, we delete only those text. After block is deleted, the algorithm recursively checks if the parent node is empty. If the parent node is empty, it is to be deleted. The last key algorithm is token probability calculation, as demonstrated in Algorithm 3. We use depth-firth algorithm to traverse tokens in the token tree so that tokens visited sequentially share the longest prefix sequences. The probability of the root token and singleton child tokens are directly set to 1.0, and does not require calculation. Received 20 February 2007; revised 12 March 2009; accepted 5 June"
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence Renmin University of China",
        "Baichuan Intelligent Technology"
    ]
}