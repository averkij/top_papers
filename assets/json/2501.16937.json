{
    "paper_title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models",
    "authors": [
        "Makoto Shing",
        "Kou Misaki",
        "Han Bao",
        "Sho Yokoi",
        "Takuya Akiba"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 7 3 9 6 1 . 1 0 5 2 : r Published as conference paper at ICLR 2025 TAID: TEMPORALLY ADAPTIVE INTERPOLATED DISTILLATION FOR EFFICIENT KNOWLEDGE TRANSFER IN LANGUAGE MODELS Makoto Shing1, Kou Misaki1, Han Bao2, Sho Yokoi345, Takuya Akiba1 1Sakana AI, 2Kyoto University, 3NINJAL, 4Tohoku University, 5RIKEN {mkshing,kou.misaki,takiba}@sakana.ai, bao@i.kyoto-u.ac.jp, yokoi@ninjal.ac.jp"
        },
        {
            "title": "ABSTRACT",
            "content": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, widely-used technique for transferring knowledge from large teacher model to small student model, presents promising approach for model compression. significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce Temporally Adaptive Interpolated Distillation (TAID), novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the students initial distribution towards the teachers distribution. We provide theoretical analysis demonstrating TAIDs ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAIDs superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAIDs practical impact by developing two stateof-the-art compact foundation models: TAID-LLM-1.5B for language tasks and TAID-VLM-2B for vision-language tasks. These results demonstrate TAIDs effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models are too large. Causal language models (LMs) are increasingly becoming essential tools across various sectors (Malinka et al., 2023; Wu et al., 2023; Zhang et al., 2023a; He et al., 2024). Scaling data size, model size, and training steps has been the primary approach to improve LM performance (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI et al., 2024), leading to rapid advancements in both proprietary and open-source LMs (Touvron et al., 2023; Abdin et al., 2024; Yang et al., 2024). However, the success of large LMs creates challenges: they are too large for edge devices (Qu et al., 2024; Thawakar et al., 2024; Liu et al., 2024), have decoding times too long for real-time applications (Wan et al., 2023; Leviathan et al., 2023; Miao et al., 2024), and consume significant energy resources (Luccioni et al., 2023; Faiz et al., 2024). This paradox of scale hinders the widespread deployment and use of LMs despite their potential and high demand. Knowledge distillation offers promising prescription. One promising approach to developing compact yet high-performing models is knowledge distillation (KD) (Hinton et al., 2015). KD aims to transfer the knowledge, specifically the predicted distributions, from well-trained, high-capacity teacher model to more compact student model, often achieving better performance than small models trained solely (Buciluundefined et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015). In the context of compressing large LMs, KD is becoming mainstream approach, with many specialized KD methods actively being developed (Xu et al., 2024; Team et al., 2024; Muralidharan et al., 2024). 1 Published as conference paper at ICLR 2025 Figure 1: Comparison of standard KD and TAID. (Left) Standard KD methods typically employ direct optimization towards fixed teacher distribution. (Right) TAID creates dynamic bridge through adaptive, time-dependent intermediate teacher distributions (green dashed lines), enabling gradual optimization of the student. This approach facilitates flexible transition from the students initial distribution towards the teachers distribution over time, effectively addressing the capacity gap and balancing knowledge transfer across varying model sizes. The formidable, unresolved challenge of teacher-student differences. Nevertheless, KD is not flawless method, and two significant issues remain, both stemming from the differences between teacher models and the student models. (i) Capacity gap the substantial capacity gap between large teacher model and compact student model makes effective knowledge transfer more difficult (Mirzadeh et al., 2020; Cho & Hariharan, 2019; Zhang et al., 2023b). As LMs continue to grow in size and complexity, this capacity gap becomes increasingly pronounced, making it even more challenging to distill knowledge effectively. (ii) Mode averaging and mode collapse due to the disparity in model capacity, KD methods often struggle with mode-averaging and mode-collapse issues, where student models either fail to oversmooth rich output distributions of teacher model or become overly focused on specific modes (Wen et al., 2023; Gu et al., 2024; Agarwal et al., 2024). new method to overcome the teacher-student difference. To overcome the fundamental issue of differences between teacher and student models, we introduce Temporally Adaptive Interpolated Distillation (TAID), new approach to KD for LMs. TAID reduces the gap between teacher and student model throughout the training process by dynamically introducing an intermediate teacher that interpolates teacher and student models to provide target distribution with modest capability (see Figure 1). This simple technique allows for learning higher-quality student model than with existing KD methods (Section 6), scales students performance with teachers size even under large capacity gaps (Section 6.3.2), and suppresses mode-averaging and mode-collapse issues theoretically and empirically (Section 4 and 6.3.3). Our main contributions to this paper are as follows: We introduce TAID (Section 3), new knowledge distillation method that reimagines the distillation process as dynamic, adaptive knowledge transfer from student to teacher distributions. This approach addresses common challenges in distilling large language models. We provide theoretical analysis of TAID (Section 4) with regression model as proxy to the language modeling objective, demonstrating its ability to prevent mode collapse in the distillation process. This theoretical guarantee sets TAID apart from traditional self-distillation methods, which can suffer from mode collapse. We conduct extensive experiments (Section 6) across various model sizes and architectures, demonstrating TAIDs superiority in both instruction tuning and pre-training scenarios. Moreover, we experimentally reveal TAIDs robustness to capacity gaps (Section 6.3.2), and its ability to balance between mode averaging and mode collapse, unlike existing KD methods (Section 6.3.3). We demonstrate TAIDs practical impact by developing two state-of-the-art compact models (Section 7): TAID-LLM-1.5B achieves the best performance for language models under 2B parameters, while TAID-VLM-2B outperforms vision-language models up to 4B parameters, showcasing TAIDs effectiveness across different domains. 2 Published as conference paper at ICLR"
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Problem setting for language model distillation. language model is defined as probability distribution over token sequences = (y1, y2, . . . , yS) S, where is the vocabulary and is the sequence length. The distribution is obtained by applying the softmax function to logit values: p(ys y<s) = softmax(logitp(ys y<s)) = exp(logitp(ysy<s))/(cid:80) Y exp(logitp(yy<s)). The model satisfies the autoregressive property: p(y) = (cid:81)S s=1 p(ys y<s) where y<s := (y1, y2, . . . , ys1), and p(ys y<s) = p(y1) for = 1. In KD for language models, we aim to transfer knowledge from well-trained teacher model to parametric student model qθ. The objective is to find parameters θ that minimize distance measure between their distributions. s=1 (cid:80) (cid:80)S ysY p(ys y<s) log p(ysy<s) Traditional knowledge distillation approaches. Hinton et al. (2015) introduced KD using the KullbackLeibler (KL) divergence, which is formulated for language models as: JKL(p, qθ) := 1 qθ(ysy<s) . However, KD based on the standard KL divergence often suffers from the mode-averaging problem, where student model attempts to aggressively cover all modes of teacher distribution despite being incapable, potentially resulting in oversmoothed and less accurate distribution (Wen et al., 2023; Gu et al., 2024). To address this, Wen et al. (2023) proposed using the Reverse KL (RKL) divergence: JRKL(p, qθ) := JKL(qθ, p). While this approach mitigates the mode-averaging problem, it can lead to mode collapse, where the student model focuses only on the dominant modes of the teacher distribution. Curse of capacity gap. Mirzadeh et al. (2020), Cho & Hariharan (2019), and Zhang et al. (2023b) reported curse of capacity gap, where an excessively large model can negatively impact the performance of the student model. This phenomenon poses significant challenge in KD, particularly for language models. As state-of-the-art language models continue to grow in size and complexity, the capacity gap becomes increasingly critical in developing high-performing and compact student models. Addressing the capacity gap is crucial for effectively transferring knowledge from largescale language models to more portable ones without sacrificing performance. Our experiments (Section 6.3.2) provide empirical evidence of the capacity gap and demonstrate how our proposed method addresses this challenge."
        },
        {
            "title": "3 PROPOSED METHOD: TAID",
            "content": "We introduce Temporally Adaptive Interpolated Distillation (TAID), novel knowledge distillation method for large language models. TAID uses dynamic, time-dependent intermediate teacher to bridge the gap between student and teacher models (see Figure 1). This approach facilitates smoother knowledge transfer, addressing the capacity gap and balancing mode-averaging and modecollapse issues. We show how TAID mitigates these issues in Sections 6.3.2 and 6.3.3, respectively. 3.1 TEMPORALLY INTERPOLATED DISTRIBUTION The key idea behind TAID is to employ time-dependent intermediate teacher to bridge the gap between student and teacher models. We formally define the intermediate distribution as follows: Definition 3.1 (TAID Interpolated Distribution). For any input sequence y<s s1 and any output token ys Y, the TAID interpolated distribution pt is defined as: pt(ysy<s) := softmax (cid:16) (1 t) logitq θ (ysy<s) + logitp(ysy<s) (cid:17) (1) where [0, 1] is time-dependent interpolation parameter, logitq represents detached version of the student logits (i.e., treated as constant without being backpropagated), and logitp represents the teacher logits. θ The interpolation is performed at the logit level to preserve relative confidence between predictions. The TAID objective function with the interpolation parameter is defined as the KL divergence between the intermediate distribution pt and the student distribution qθ: 3 Published as conference paper at ICLR 2025 Definition 3.2 (TAID Objective). The TAID objective function at time is defined as: (t) TAID(p, qθ) := JKL(pt, qθ) ="
        },
        {
            "title": "1\nS",
            "content": "S (cid:88) (cid:88) s=1 ysY pt(ysy<s) log pt(ysy<s) qθ(ysy<s) . (2) We gradually increase the interpolation parameter from 0 to 1 during training so that the intermediate distribution pt adaptively transitions from the students initial distribution towards the teachers distribution. Refer to Section 3.2 for the scheduling of the interpolation parameter. The detached θ in pt ensures that we only optimize the student model qθ in the denominator of the KL divergence, effectively treating the intermediate distribution as target. The intermediate distribution provides crucial advantage in addressing the capacity gap and modeaveraging/collapse issues. By smoothly transitioning from the students initial distribution to the teachers distribution, TAID facilitates gradual transfer of knowledge. This approach effectively mitigates issues associated with significant capacity gaps between teacher and student models. This can be understood as follows: When is small, the student model is encouraged to focus on its own modes, reinforcing its unique characteristics. In this phase, TAID behaves similarly to selfdistillation (using the student model as the teacher), which amplifies generalization by sparsifying the model (Mobahi et al., 2020). Thus, the student model tends to capture dominant features of the students distribution. As increases, the student gradually incorporates the teachers knowledge, capturing more nuanced and rich signals from the teacher distribution. This balanced approach results in student model that not only captures the essential knowledge from the teacher but also maintains its ability to generalize effectively. Despite TAIDs relevance to self-distillation, the interpolation parameter is essential to avoid mode collapse, which self-distillation cannot escape. We will theoretically demonstrate it in Section 4. 3.2 ADAPTIVE INTERPOLATION PARAMETER UPDATE While TAID demonstrates effectiveness even with simple linear increase of the interpolation parameter t, we propose an adaptive update mechanism to achieve more efficient learning and improved accuracy. The key motivation is to dynamically adjust based on the students learning progress. The adaptive update strategy is designed to aggressively increase in the early stages when the interpolated distribution pt is close to the student model qθ, as the model fitting is not challenging in this phase. As the student model approaches the teacher model, the increase in becomes more gradual, allowing for careful fitting to the more complex teacher distribution. TAID (tn) TAID)/(J (tn1) TAID + ϵ), where (tn) Our adaptive update strategy is based on the relative change in the objective function: δn := (J (tn1) TAID is the value of the TAID objective function at interpolation parameter tn, tn is the interpolation parameter at step n, and ϵ is small constant to prevent division by zero. We update tn using momentum-based approach to smooth out short-term fluctuations: mn = βmn1 + (1 β)δn, where β is the momentum coefficient. The interpolation parameter is then updated as: tn min(1.0, max(tlinear, tn1 + α sigmoid(mn))), where α is the step size for t, and tlinear is linear increase schedule as lower bound for t. To allow for flexible initialization, is set to start value tstart, which is hyperparameter. The complete TAID training procedure is summarized in Algorithm 1 in Appendix A. This update mechanism allows for more aggressive increases in during the early stages of training when the student is learning rapidly (high δt), and more gradual increases as the student model approaches the teachers complexity (low δt). The sigmoid function bounds the update, ensuring stable learning, while the max and min operations guarantee monotonic increase within the predefined range. detailed analysis of how different α values affect the behavior of and the learning dynamics is presented in Section 6.3.1."
        },
        {
            "title": "4 THEORETICAL ANALYSIS",
            "content": "TAID distills from the intermediate distribution pt, partially containing the student model qθ as the mixture component. This may apparently cause the collapse because students modes are amplified repeatedly during the fitting recursion. Such collapse phenomenon has been theoretically observed 4 Published as conference paper at ICLR 2025 for self-distillation, where the teacher and student models are identical (Mobahi et al., 2020). We aim to demonstrate that TAID avoids mode collapse, unlike self-distillation. We borrow the analysis framework of Mobahi et al. (2020) to study least-square regression as proxy to language modeling. In each training step, the student model is updated by fitting to the interpolated label (1 t)yt + tyteacher, where yt and yteacher are the labels of the current student and teacher models, respectively, and is the interpolation parameter (being linearly increased) at the current step. Here, we suppose the student model achieves ϵ-interpolation of the training signals so that the regression loss is minimized near-perfectly in each time step. Theorem 4.1 (Non-collapse Nature (Informally)). Suppose we run distillation for steps in total. If the teacher model has sufficiently large signals so that the label is at least as large as Ω( ϵ), then the student model does not collapse for any time t. Notably, self-distillation inevitably collapses for sufficiently large steps (Mobahi et al., 2020, Proposition 4), corroborating the benefit of the intermediate distribution and its adaptive update. The formal statement and more discussions can be found in Appendix B."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "(cid:80) Improving objective functions. To address the mode-averaging and mode-collapse issues that the traditional KL divergence-based methods (Section 2) face, various alternative objective functions have been applied to knowledge distillation. Wen et al. (2023) applied the Total Variation Distance, formulated at the sequence level similar to Kim & Rush (2016): JTVD(p, qθ) := 1 p(y) qθ(y). Agarwal et al. (2024) utilized the Generalized JensenShannon (JS) Diver2 gence: JGJSD(p, qθ) := λJKD(p, r) + (1 λ)JRKD(p, r), where r(y) = λp(y) + (1 λ)qθ(y) and λ [0, 1]. Additionally, Ko et al. (2024) employed the Skew KL Divergence: JSKD(p, qθ) := JKL(p, r). They also defined the Skew Reverse KL Divergence as JSRKD(p, qθ) := JKL(qθ, r). These approaches aim to balance preserving teacher knowledge and allowing student generalization. However, they typically use fixed teacher distribution throughout distillation, potentially hindering knowledge transfer when there is significant capacity gap between teacher and student. In contrast, our TAID method introduces time-dependent intermediate distribution, gradually transitioning from the students initial distribution to the teachers, mitigating the capacity gap issue and enabling more stable learning. While Skew KL divergence also adopts an intermediate distribution, its approach differs significantly from TAID. Skew KL divergence uses fixed intermediate distribution and transfers the teachers knowledge to it, whereas TAID employs time-dependent intermediate distribution and transfers it to the student. This distinction, particularly the dynamic nature of TAIDs intermediate distribution, makes TAID more suitable for adaptive updates of the student model as the interpolation parameter changes over time (see Appendix for detailed comparison). Utilizing student-generated outputs (SGOs). Recent research in KD for language models has explored utilizing on-policy data sampled from teacher and student models during training (Gu et al., 2024; Zhang et al., 2024b). Within this approach, some studies have specifically focused on leveraging student-generated outputs (SGOs) (Agarwal et al., 2024; Ko et al., 2024). While these methods show promise in improving distillation performance and addressing the distribution mismatch between training and inference due to the autoregressive nature of LMs when training on fixed dataset (Pomerleau, 1991; Ross & Bagnell, 2010), they are computationally expensive for large-scale models. TAID achieves superior performance without relying on on-policy data or SGOs, offering improved computational efficiency for large-scale datasets and models (see Section 6.1). Future work could explore combining TAID with on-policy approaches to potentially achieve even better performance. KD methods from image classification. KD has been extensively studied in image classification tasks, with some logit-based methods being applicable to language model distillation. Notable examples include CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), which have shown remarkable performance using standard KL divergence. CTKD shares similar curriculum learning approach with TAID, gradually increasing task difficulty. CTKD achieves this through learnable temperature parameter that modifies both student and teacher distributions. In contrast, TAID modifies only the 5 Published as conference paper at ICLR Table 1: Evaluating distillation methods for LLM instruction tuning. The MT-Bench scores after training are listed, where higher scores indicate better conversational performance. For each of the three teacher-student pairs, different distillation algorithms, including the proposed TAID method, are compared. The highest score in each column is highlighted in bold. Method Teacher Llama-2 (6.7B) Student TinyLlama (1.1B) TinyLlama (1.1B) Phi-3-mini (3.8B) StableLM Zephyr (2.8B) Pythia (0.4B) SFT KL (Hinton et al., 2015) RKL (Wen et al., 2023; Gu et al., 2024) TVD (Wen et al., 2023) Adaptive KL (Wu et al., 2024) GKD (Agarwal et al., 2024) DistiLLM (Ko et al., 2024) CTKD (Li et al., 2023b) DKD (Zhao et al., 2022) (Ours) TAID w/o adaptive update (Ours) TAID 2.00 2.71 3.48 3.27 3.27 2.24 3.23 1.78 2.70 3.44 4.05 3.94 3.99 3.92 3.64 3.77 3.82 3.97 2.84 4.14 4.18 4.27 2.57 2.74 2.53 2.57 2.64 2.59 2.97 1.39 2.90 2.88 3.05 teacher distribution through interpolation, potentially preserving more of the students learned information. DKD decomposes KL divergence into target and non-target class components, allowing for better weight adjustment in tasks of varying difficulty. However, these image classification-based methods are not sufficiently effective in language modeling due to the unique characteristics of the language domain. We experimentally verified it in Section 6.3.4. TAID addresses these challenges through its adaptive interpolation, while remaining flexible enough to be combined with methods like DKD for simpler tasks."
        },
        {
            "title": "6 EMPIRICAL ANALYSIS",
            "content": "We evaluate TAID across instruction tuning and pre-training scenarios, using various model sizes and architectures. Our experiments compare TAID against state-of-the-art methods, demonstrating its superior performance and efficiency, while providing insights into its behavior across different capacity gaps and its ability to balance mode-averaging and mode-collapse issues. 6.1 INSTRUCTION TUNING for training. Experimental setup. For the instruction-following task, we used the UltraChat 200k dataset (Ding et al., 2023) Performance was assessed using MT-Bench (Zheng et al., 2023), benchmark designed to evaluate models instruction-following ability, with scoring conducted by GPT-4. For our experiments, we utilized three teacher-student pairs: Phi-3-mini-4k-instruct (Abdin et al., 2024) as teacher with TinyLlama (Zhang et al., 2024a) as student, Llama-2-7b-chat (Touvron et al., 2023) as teacher with TinyLlama as student, and StableLM Zephyr 3B (Team, 2023) as teacher with Pythia-410M (Biderman et al., 2023) as student. To evaluate the pure effectiveness of our distillation method, we focused solely on distillation using instruction data, unlike previous studies (Gu et al., 2024; Agarwal et al., 2024; Ko et al., 2024) that often perform supervised fine-tuning (SFT) before distillation or include additional cross-entropy loss on pre-training corpora. Furthermore, to simulate more practical scenario, we used powerful teacher models trained on in-house data with open weights for distillation to smaller student models. We compared TAID against prior works, including KL divergence (Hinton et al., 2015), RKL (Wen et al., 2023), Total Variation Distance (TVD) (Wen et al., 2023), Adaptive KL (Wu et al., 2024), as well as methods utilizing SGOs such as Generalized KD (GKD) (Agarwal et al., 2024) and DistiLLM (Ko et al., 2024). Additionally, we included two methods originally proposed for image classification tasks: CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), to assess their effectiveness in language model distillation. We also included supervised fine-tuning (SFT) baseline to demonstrate the benefits of knowledge distillation. To isolate the impact of our adaptive update mechanism, we evaluated TAID both with and without this feature, where TAID without adaptive update uses linear increase of the interpolation parameter with respect to the 6 Published as conference paper at ICLR 2025 Table 2: Evaluating distillation methods for LLM continued pre-training. The Open LLM Leaderboard scores after training are listed, with higher scores indicating better performance. The average score across the 6 tasks (Average column) is commonly used as an indicator of overall language proficiency. The highest score in each column is highlighted in bold. Method ARC HellaSwag MMLU TrustfulQA Winogrande GSM8K Average SFT KL (Hinton et al., 2015) TVD (Wen et al., 2023) Adaptive KL (Wu et al., 2024) GJS (Agarwal et al., 2024) Skew KL (Ko et al., 2024) Skew RKL (Ko et al., 2024) (Ours) TAID 41.38 44.97 43.52 43.77 44.71 44.62 44.11 45.48 63.66 65.43 64.50 63.09 65.67 65.25 64.80 65.43 25.89 25.11 25.95 26.04 25.27 25.79 26.07 25.43 35.64 37.95 36.38 36.42 37.76 37.45 36.76 37.92 61.25 63.22 63.14 63.22 62.12 62.51 62.83 63.38 1.21 2.80 2.96 2.12 3.34 3.41 3.03 2. 38.17 39.91 39.41 39.11 39.81 39.84 39.60 40.10 training steps. Detailed hyper-parameters and implementation specifics for TAID and all baseline methods are provided in Appendix D.1. Results. Table 1 presents the MT-Bench scores for all methods across the three different teacherstudent pairs. Our proposed TAID method consistently outperforms all baseline methods, including those proposed for image classification (CTKD and DKD) and methods utilizing SGOs such as GKD and DistiLLM. Notably, TAID achieves superior performance without relying on expensive SGO sampling strategies, resulting in significantly faster training timesapproximately 2 times faster than DistiLLM and 10 times faster than GKD. This combination of superior performance and computational efficiency, achieved without SGOs, makes TAID particularly attractive for real-world applications where both model quality and training speed are crucial. An ablation study comparing TAID with and without adaptive updates shows improvements ranging from 2.2% to 17.7% across different teacher-student pairs, underlining the importance of our proposed adaptive mechanism. 6.2 PRE-TRAINING Experimental setup. Due to the limited resources, we performed continued pre-training, initializing the student model with pre-trained model and further refining it through additional pre-training using distillation. We used the first 10% of the SmolLM-Corpus (Ben Allal et al., 2024) dataset, amounting to approximately 20 billion tokens. We used Phi-3-medium-4k-instruct (Abdin et al., 2024) as the teacher model and TinyLlama as the student model. Similar to our instruction tuning experiments, we focused solely on distillation without additional supervised fine-tuning or pre-training losses. Due to the computational cost associated with sampling from the student model in large-scale pre-training and the absence of prompts as in instruction-following tasks, we adapted the baseline methods to use only their objective functions without SGOs. We compared TAID against these modified baselines, including KL divergence, TVD, Adaptive KL, GJS (used in GKD), and Skew KL/RKL (used in DistiLLM). To evaluate the pre-trained models, we followed the Open LLM Leaderboard (Beeching et al., 2023) methodology, which is commonly used to assess the underlying capabilities of models through few-shot evaluation. This methodology includes six diverse tasks, with evaluation settings and metrics adhering to the Open LLM Leaderboard standards. Detailed hyperparameters and implementation specifics are provided in Appendix D.2. Results. Table 2 presents the results of our pre-training experiments. Following the standard practice in the LLM community, we reported the average scores across diverse tasks. TAID achieves the highest average score across all six tasks, outperforming all baseline methods. This superior average performance demonstrates TAIDs effectiveness in transferring knowledge from the teacher to the student model across diverse range of tasks. While TAID shows the best overall performance, it is worth noting that it achieves the highest scores on two individual tasks (ARC and Winogrande) and competitive performance on the others. The consistently strong performance across tasks, coupled with the highest average score, underscores TAIDs robustness and effectiveness in knowledge distillation for large language models. 7 Published as conference paper at ICLR 2025 Figure 2: Analysis of TAIDs behavior and performance. (Left) Interpolation parameter behavior: Higher α values lead to faster initial growth compared to linear increase, allowing for more aggressive knowledge transfer in early stages when the capacity gap is small. (Middle) Objective value comparison: TAID exhibits more stable objective value with lower variance compared to standard KL divergence throughout training, indicating consistent learning difficulty that aligns with the students evolving capabilities. (Right) Performance across different teacher sizes: TAID shows monotonic improvement and outperforms other methods as teacher size increases, demonstrating its effectiveness in addressing the curse of capacity gap. 6.3 ANALYSIS 6.3.1 ANALYSIS OF INTERPOLATION PARAMETER AND TRAINING STABILITY We analyzed TAIDs interpolation parameter and learning dynamics to validate its design. Figure 2 (Left) shows how different learning rates α affect ts behavior over time under the setting of Section 6.1, with tstart set to 0.4. We can confirm that is smoothly increasing thanks to our adaptive update mechanism. Higher α values lead to faster initial growth of t, enabling more aggressive early knowledge transfer, which is particularly beneficial when the capacity gap between student and teacher models is small. Figure 2 (Middle) compares the objective value of TAID (using the intermediate distribution) with the standard KL divergence between the teacher and student during training. TAID demonstrates constant value with low variance throughout the training process, in contrast to the higher and more variable loss of standard KL. This stability in loss indicates that TAIDs adaptive interpolation mechanism keeps the learning task at consistent level of difficulty, aligning with the students current capabilities. This controlled learning environment potentially leads to more efficient and stable knowledge transfer throughout the training process."
        },
        {
            "title": "6.3.2 PERFORMANCE ACROSS VARIOUS CAPACITY GAPS",
            "content": "TAIDs design, which gradually transfers knowledge from the teacher model, is expected to address the curse of capacity gap described in Section 2. To evaluate this, we conducted an experiment using fixed-size student model (70m) trained with teachers of varying capacities (410M to 6.9B) from the Pythia Suite (Biderman et al., 2023). Models were trained on random 1B token subset of the SmolLM-Corpus for 1 epoch, due to computational cost constraints. We chose the LAMBADA dataset (Paperno et al., 2016) for evaluation, as it tests models ability to predict the final word of passage, directly assessing language modeling capability without relying on specific knowledge, making it suitable for comparing models with small-scale training. Figure 2 (Right) shows that TAID consistently outperforms both KL and RKL divergence methods across all teacher model sizes. Notably, TAID exhibits consistent upward trend in performance as the teacher model size increases while KL and RKL methods show inconsistent performance trends. This inconsistency in KL and RKL methods aligns with the curse of capacity gap, where larger teacher models do not always lead to better student performance, described Section 2. TAIDs consistent improvement with larger teachers indicates its robustness in handling varying capacity gaps, making it particularly suitable for distilling knowledge from state-of-the-art large language models into more compact and deployable student models. 8 Published as conference paper at ICLR"
        },
        {
            "title": "6.3.3 BALANCING MODE AVERAGING AND MODE COLLAPSE",
            "content": "To demonstrate TAIDs effectiveness in balancing mode-averaging and mode-collapse issues, we analyzed the distributions of student models trained using KL divergence, RKL divergence, and TAID. We used the trained models of the Phi-3-mini-4k-instruct (teacher) and TinyLlama (student) pair in Section 6.1, with distributions calculated from the UltraChat 200k train set. Table 3 presents summary of our analysis, showing the probability mass distribution for the head and tail of the vocabulary as ranked by the teacher model. We observe that TAID consistently maintains probability masses between those of KL and RKL for both the head and tail of the distribution. In the head, TAID captures dominant vocabulary in the teachers distribution more than KL, effectively avoiding the mode-averaging issue. While RKL captures the dominant vocabulary more than TAID, it significantly fails to capture low-frequent vocabulary in the tail of the teacher distribution, which TAID captures reasonably, preventing the modecollapse issue. These results indicate that TAID successfully navigates the trade-off between mode averaging and mode collapse, achieving more balanced and faithful representation of the teachers distribution across both common and rare tokens. This balanced approach contributes to TAIDs superior performance in knowledge distillation tasks, as it more effectively captures the full spectrum of the teachers knowledge while maintaining focused distribution. 6.3.4 COMPARISON WITH IMAGE CLASSIFICATION TASKS Table 3: Probability mass distribution analysis. Head: sum of probabilities for top-10 tokens. Tail: sum of probabilities for tokens in the 80 100th percentile.1 Method Head Tail KL RKL TAID 0.216 0.227 0.218 40.2 107 8.1 107 39.0 107 Our experiments revealed that KD methods developed for image classification, such as CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), underperform in language model distillation. We hypothesize that this is due to fundamental differences in the distributions between language modeling tasks and image classification tasks. Figure 3 illustrates the entropy of the distribution and the probabilities of groundtruth classes (target-class probabilities) for two representative models: ResNet-56 (He et al., 2016) for image classification and GPT-2 (Radford et al., 2019) for language modeling.2 Image classification typically involves predicting one-hot distribution with high target-class probability and low entropy. In contrast, language modeling predicts more diverse probability distribution, resulting in lower target-class probabilities and higher entropy. These characteristics lead to two key challenges in language model distillation. First, there is an increased susceptibility to mode collapse, as the model can easily be pulled toward non-target modes. Second, language modeling poses significant challenge for smaller models with limited capacity: predicting extremely low-frequency classes. This difficulty is compounded by power law distribution of word frequencies (Zipfs law), resulting in large number of extremely low-frequency classes in the long tail of the distribution. To test this hypothesis and to assess TAIDs flexibility, we evaluated TAID on multiple image classification tasks (results in Appendix D.3). While gains were modest on CIFAR-100, TAID consistently outperformed CTKD and DKD on the more complex ImageNet task. This aligns with our observation that ImageNet (entropy: 6.67, target-class probability: 0.00130) presents more challenging distribution compared to CIFAR-100 (entropy: 0.485, target-class probability: 0.613). These findings highlight the need for distillation methods tailored to language modelings unique challenges. TAIDs strong performance Figure 3: Comparison between image classification and language modeling tasks. Language modeling (GPT-2) exhibits significantly higher entropy and lower target-class probabilities compared to image classification (ResNet-56). These fundamental differences highlight the unique challenges in language model distillation. 1Typically, probabilities range from 101 to 102 for Head tokens and from 1010 to 1011 for Tail tokens. 2For this analysis, we used the CIFAR-100 (Krizhevsky, 2009) dataset for ResNet-56 and the OpenWebText (Gokaslan & Cohen, 2019) dataset for GPT-2. 9 Published as conference paper at ICLR 2025 Table 4: Performance of TAID-LLM-1.5B, our new state-of-the-art LLM for models under 2B parameters. See Table 9 for task breakdown. Table 5: Performance of TAID-VLM-2B, our new state-of-the-art VLM for models up to 4B parameters. See Table 10 for task breakdown. Model LightEval () Model Open-VLM-LB () Qwen2-1.5B (Yang et al., 2024) Phi-1.5B (Li et al., 2023a) StableLM-2-1.6B (Bellagente et al., 2024) SmolLM-1.7B (Allal et al., 2024) TAID-LLM-1.5B 46.19 50.39 51.24 51.31 52. PaliGemma (Beyer et al., 2024) MiniCPM-V-2 (Yao et al., 2024) Phi-3-Vision (Abdin et al., 2024) InternVL2-2B (Chen et al., 2024) TAID-VLM-2B 46.56 47.93 53.60 53.96 56.43 across domains, particularly in complex tasks, demonstrates its potential as versatile approach to knowledge distillation. Future work could explore its application to other tasks involving long-tail distributions or complex probability predictions beyond language modeling."
        },
        {
            "title": "7 APPLICATION TO STATE-OF-THE-ART MODEL DEVELOPMENT",
            "content": "Building upon our systematic evaluation of TAID, we further demonstrate its effectiveness in developing state-of-the-art models. We introduce two models: TAID-LLM-1.5B and TAID-VLM-2B, which have achieved state-of-the-art performance in their respective size categories for large language models (LLMs) and vision-language models (VLMs). TAID-LLM-1.5B. We developed TAID-LLM-1.5B, new 1.5B-parameter language model, using our TAID method. Following recent conventions in evaluating language models of this size (Allal et al., 2024), we evaluated it using LightEval 3, comprehensive benchmark suite for small language models. Table 4 shows that TAID-LLM-1.5B achieves the highest score, setting new state-of-the-art for models with fewer than 2 billion parameters. Detailed settings and results can be found in Appendix E.1. TAID-VLM-2B. To showcase TAIDs versatility, we developed TAID-VLM-2B, new 2Bparameter vision-language model. We evaluated it following the Open VLM Leaderboard protocol (OpenCompass Contributors, 2023)4. As shown in Table 5, TAID-VLM-2B achieves the highest score among state-of-the-art vision-language models up to 4B parameters, even surpassing the performance of larger models like Phi-3-Vision (4.2B parameters). This success highlights TAIDs capability in transferring multimodal knowledge across significant capacity gaps. Detailed settings and results can be found in Appendix E.2."
        },
        {
            "title": "8 CONCLUSION",
            "content": "We introduced Temporally Adaptive Interpolated Distillation (TAID), novel knowledge distillation approach that effectively addresses the challenges of compressing large language models. Our experiments demonstrated TAIDs superior performance across various model sizes and architectures, consistently outperforming state-of-the-art methods. The development of TAID-LLM-1.5B and TAID-VLM-2B, achieving state-of-the-art performance in their categories, underscores TAIDs practical impact. TAIDs dynamic bridge mechanism effectively mitigates mode-averaging and mode-collapse problems, leading to more stable and efficient training. These advantages contribute to more accessible deployment of advanced language technologies in resource-constrained environments. Future research could extend TAID to other distance metrics, explore non-linear interpolations, adapt it for multi-teacher distillation (Wan et al., 2024), and investigate its application in other modalities and tasks beyond classification. In conclusion, TAID represents significant advancement in knowledge distillation, offering both theoretical insights and practical benefits. As AI evolves, techniques like TAID will be crucial in making these advancements more accessible and deployable in real-world applications. 3https://huggingface.co/blog/smollm 4https://huggingface.co/spaces/opencompass/open_vlm_leaderboard 10 Published as conference paper at ICLR"
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "Makoto Shing and Takuya Akiba initiated this project. Makoto Shing is the main contributor who conceptualized and proposed the TAID method, designed and conducted all experiments, performed theoretical analysis, implemented the main code, wrote the initial draft of the manuscript, and was responsible for data analysis and interpretation of results. Consistently led and executed all aspects of the project from inception to completion. Kou Misaki contributed to data processing for the TAID-LLM-1.5B model. Han Bao provided crucial feedback on theoretical interpretations and analysis. Sho Yokoi offered valuable insights and feedback, especially based on his expertise in Natural Language Processing. Takuya Akiba served as the primary advisor throughout the project, offering guidance, technical insight, advice, and supervision from inception to completion. All authors reviewed and edited the final manuscript."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The authors would like to thank Masanori Suganuma and Tianyu Zhao for providing valuable discussions and feedback while drafting the text. This work is based on results obtained from project, JPNP20017, subsidized by the New Energy and Industrial Technology Development Organization (NEDO). This work was supported by JSPS KAKENHI Grant Number 22H05106."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, and Matthieu Geist. On-policy distillation of language models: Learning from self-generated mistakes. In International Conference on Learning Representations, 2024. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Smollm - blazingly fast and remarkably powerful, 2024. Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. Edward Beeching, Clementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/open-llm-leaderboard-old/ open_llm_leaderboard, 2023. Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von URL https://huggingface.co/datasets/ Smollm-corpus, 2024. Werra. HuggingFaceTB/smollm-corpus. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, pp. 23972430. PMLR, 2023. 11 Published as conference paper at ICLR 2025 Cristian Buciluundefined, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 535541. Association for Computing Machinery, 2006. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, June 2024. Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Empirical Methods in Natural Language Processing, 2023. Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, and Fan Chen. LLMCarbon: Modeling the end-to-end carbon footprint of large language models. In International Conference on Learning Representations, 2024. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In International Conference on Learning Representations, 2024. Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694, 2024. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, pp. 3001630030. Curran Associates, Inc., 2022. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, and Qian Liu. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. Ying Jin, Jiaqi Wang, and Dahua Lin. Multi-level logit distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognitio, pp. 2427624285, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Empirical Methods in Natural Language Processing, pp. 13171327. Association for Computational Linguistics, 2016. Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. DistiLLM: Towards streamlined distillation for large language models. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), International Conference on Machine Learning, pp. 2487224895. PMLR, 2024. 12 Published as conference paper at ICLR 2025 Alex Krizhevsky. Learning multiple layers of features from tiny images. Masters thesis, Department of Computer Science, University of Toronto, 2009. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023a. Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, and Jian Yang. Curriculum temperature for knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 15041512, 2023b. Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. MobileLLM: Optimizing sub-billion parameter language models for on-device use cases. In International Conference on Machine Learning, 2024. Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, 176b parameter language model. Journal of Machine Learning Research, pp. 115, 2023. Kamil Malinka, Martin Peresıni, Anton Firc, Ondrej Hujnak, and Filip Janus. On the educational impact of chatgpt: Is artificial intelligence ready to obtain university degree? In Innovation and Technology in Computer Science Education V. 1, pp. 4753, 2023. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pp. 932949, 2024. Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 51915198, 2020. Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in Hilbert space. Advances in Neural Information Processing Systems, pp. 33513361, 2020. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2024. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In Katrin Erk and Noah A. Smith (eds.), Association for Computational Linguistics, pp. 15251534. Association for Computational Linguistics, 2016. Dean A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 1991. Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, and Xianhao Chen. Mobile edge intelligence for large language models: contemporary survey. arXiv preprint arXiv:2407.18921, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. 13 Published as conference paper at ICLR 2025 Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 661668. PMLR, 2010. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Stability AI Language Team. Stablelm zephyr 3b, 2023. URL https://huggingface.co/ stabilityai/stablelm-zephyr-3b. Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, and Eric P. Xing. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In International Conference on Learning Representations, 2024. Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, et al. Efficient large language models: survey. arXiv preprint arXiv:2312.03863, 2023. Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Association for Computational Linguistics, pp. 1081710834. Association for Computational Linguistics, 2023. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, and David Rosenberg. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564, 2023. Taiqiang Wu, Chaofan Tao, Jiahao Wang, and Zhe Zhao. Rethinking kullback-leibler divergence in knowledge distillation for large language models. arXiv preprint arXiv:2404.02657, 2024. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, and Dacheng Tao. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488, 2023a. Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. Lifting the curse of capacity gap in distilling language models. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), ional Linguistics. Association for Computational Linguistics, 2023b. Peiyuan Zhang, Guangtao Zeng, and Tianduo Wang. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024a. 14 Published as conference paper at ICLR Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, and Chao Zhang. PLaD: Preference-based large language model distillation with pseudo-preference pairs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Association for Computational Linguistics, pp. 1562315636. Association for Computational Linguistics, 2024b. Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1195311962, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, et al. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023."
        },
        {
            "title": "A TAID TRAINING ALGORITHM",
            "content": "Algorithm 1 provides detailed description of the TAID training procedure, including the adaptive update mechanism for the interpolation parameter t. The TAID algorithm utilizes several key Algorithm 1 TAID training algorithm 1: Input: Learning rate η, learning rate of the interpolation parameter α, momentum coefficient β, total iterations , start value tstart, end value tend 2: Initialize student model parameters θ 3: Initialize t1 = tstart, m0 = 0, (t0) TAID = 4: for each training iteration = 1 to do 5: 6: 7: , yj)}B j=1 from dataset Compute linear increase value: tlinear = tstart + (tend tstart) n/N Sample batch {(y<s Compute ptn(ysy<s) using Eq. (1) Compute (tn) TAID using Eq. (2) Update θ: θ θ ηθJ (tn) TAID δn = (J (tn1) TAID (tn) mn = βmn1 + (1 β)δn = α sigmoid(mn) tn+1 min(tend, max(tlinear, tn + t)) TAID)/(J (tn1) TAID + ϵ) 8: 9: 10: 11: 12: 13: 14: end for hyperparameters that control the behavior of the interpolation parameter and the adaptive update mechanism. We discuss the effects of these parameters below: α (learning rate of t): This parameter controls the speed of the adaptive update for t. Figure 2 (Left) shows the behavior of for different values of α, including linear increase for comparison. As α increases, we observe that grows more rapidly in the early stages when the student model is close to the initial interpolation distribution. This allows for more efficient learning when the task is relatively easy for the student. β (momentum coefficient): This parameter controls the smoothness of the adaptive update. higher value of β results in more stable updates by reducing the impact of short-term fluctuations in the objective function. In our experiments, we found that β value around 0.99 worked well across different scenarios. tstart (initial value of t): This parameter determines the starting point of the interpolation. It is particularly useful for skipping the initial stages of learning when the task is very easy for the student. The choice of tstart should be based on the intuitive gap between the initial student and teacher models. In our experiments, we found that values between 0.2 and 0.4 often yield good results, depending on the initial similarity between the student and teacher models. tend (maximum value of t): This parameter sets the upper limit for t, typically set to 1.0 to ensure that the final distribution matches the teacher model. 15 Published as conference paper at ICLR 2025 The algorithm uses linear increase schedule (tlinear) as lower bound for t, ensuring that increases at least linearly over the course of training. This approach maintains the adaptive nature of TAID while guaranteeing minimum rate of progression towards the teacher distribution. In our experiments, TAID demonstrated robust performance across various tasks with minimal hyperparameter tuning. We usually used β = 0.99 and α = 5e4, with tstart typically ranging between 0.2 and 0.4, depending on the initial student-teacher similarity. While these default values often yield good results, practitioners may achieve further improvements by fine-tuning these parameters for their specific tasks and model architectures, particularly in cases that differ significantly from our experimental settings."
        },
        {
            "title": "B THEORETICAL ANALYSIS OF MODE COLLAPSE",
            "content": "In this section, we formally study the mode-collapse behavior of TAID. B.1 ANALYSIS MODEL To study the collapse phenomenon, we leverage the analysis framework used by Mobahi et al. (2020). We study the regression problem in the interpolation regime:5 := arg min R(f ) s.t. F 1 (cid:88) (f (xi) yi)2 ϵ, i=1 (3) i=1 is finite training set with d-dimensional covariates xi Rd where := {(xi, yi)}N and one-dimensional outcome yi R, ϵ > 0 is desired loss tolerance parameter, R(f ) is regularization functional, and RX is hypothesis space. Since we are interested in large model regime, is reasonably assumed to be encompassing all measurable functions. The meansquared loss is used in (3) instead of the KL divergence, which is convenient to obtain analytical solutions later. The regularizer in the following form is considered: (cid:90) R(f ) = u(x, x)f (x)f (x)dxdx, (4) where is symmetric kernel inducing R(f ) 0 with equality only when = 0. The interpolation problem (3) may collapse depending on the teacher signals. Let us stack labels into vector: := [y1 y2 . . . yN ] RN . When y2 ϵ holds, the problem (3) has trivial solution = 0. Such collapse may happen particularly in the self-distillation paradigm because the teacher signals are (partially) given by our hypothesis itself. Thus, it is crucial to investigate when and whether the non-collapse condition y2 > ϵ is satisfied to ensure that our hypothesis learns meaningful signals. Variational problem. The Lagrangian variational problem of (3) is given as follows: λ := arg min 1 (cid:88) (f (xi) yi)2 + λ (cid:90) i=1 u(x, x)f (x)f (x)dxdx, where 1 N (cid:88) (f λ(xi) yi)2 ϵ = 0, i=1 (5) and λ1 > 0 is the Lagrange multiplier. The solution to the variational problem (5) can be analytically written down. Let be the Green function of the linear operator [Lf ](x) := (cid:82) u(x, x)f (x)dx such that (cid:90) u(x, x)g(x, x0)dx = δ(x x0), (6) 5The interpolation regime must be distinguished from the time interpolation used in the proposed TAID. 16 Published as conference paper at ICLR where δ(x) is the Dirac delta. Let RN and gx RN be Gi,j :="
        },
        {
            "title": "1\nN",
            "content": "g(xi, xj) and gx,i :="
        },
        {
            "title": "1\nN",
            "content": "g(x, xi) for all i, [N ]. Then, the analytical solution to (5) is given as follows (Mobahi et al., 2020, Proposition 1): λ(x) = (λI + G)1y. (7) If we diagonalize (which is positive definite) as = VDV, the prediction vector over the training inputs x1, . . . , xN is given as := [f λ(x1) . . . λ(xN )] = VD(λI + D)1Vy. (8) The solution (8) is essentially nonlinear extension of the ridge estimator. Note that RN is an orthogonal matrix and = diag(d1, . . . , dN ) has positive eigenvalues solely. (cid:80) Importantly, (7) is the solution to the variational problem (5), which is parametrized by λ satisfying 1 λ(xi) yi)2 ϵ = 0. Solving this in λ is hard because of its non-linearity, but Mobahi et al. (2020, Eq. (24)) evaluate its upper and lower bound: i(f α ϵ ϵ λ = for some α [dmin, dmax], (9) where dmax := maxi di and dmin := mini di. Thus, the analytical solution (7) with this range of λ is solution to the original interpolation problem (3), too. Remark on connection to language modeling. The interpolation formulation (3) is based on the standard (one-dimensional) regression problem, which obviously deviates from the language modeling problem introduced in (2). Nonetheless, we believe that this formulation is not only beneficial for our transparent understanding owing to its simplicity but also has connection to multi-categorical distributions. In distributional modeling, student model qθ outputs probability distribution over Y, and falls into mode collapse when qθ has only few numbers of non-zero probabilities, that is, {c qθ(y = c) > 0} Y. To deal with the multi-categorical outputs, we can extend the one-dimensional problem (3) as follows: Y, := arg min fcF R(fc) s.t. 1 N (cid:88) (fc(xi) yi,c)2 ϵ, i=1 where teacher signal yi,c is given in the one-hot format such that (cid:80) cY yi,c = 1 and yi,c {0, 1} for all Y. We can follow the subsequent analysis straightforwardly. In this multi-categorical problem, model (fc)cY is regarded as falling into mode collapse if fc = 0 for many Y. This is measured by the teacher signal condition yc2 ϵ for each c, where yc {0, 1}N is the stacked labels for class c. Thus, studying (3) is directly relevant to mode collapse in language modeling. B.2 FORMAL THEORETICAL STATEMENT To study TAID in fashion of the interpolation problem (3), we consider the following learning procedure listed in Algorithm 2. Here, the input signals y0 are deemed as the well-trained teacher we can deem y1 as the well-trained teacher, but the resulting distillation dynamics would not change much. Theorem B.1. Let κ := dmax/dmin( 1) be the condition number of G. The prediction vector yt+1 does not collapse, namely yt+1 = 0 cannot be solution to the interpolation problem (3), if for some γ [0, 1], either of the following holds: < min (cid:26) 1 γ + κ (r0 γ) + o(1), (cid:27) γ T or 1 r0 < t, (10) where r0 := y0/ ϵ > 1 and o(1) is an asymptotic term in the large r0 limit. 17 Published as conference paper at ICLR 2025 Algorithm 2 TAID learning procedure for least-square regression Input: number of iterations, y0 RN input signals 1: 0 2: while < do yt (1 )yt + y0 3: ϵ/(yt λt αt yt+1 VD(λtI + D)1Vyt + 1 ϵ) 4: 5: 6: 7: end while Compose intermediate teacher Choose an appropriate λt by (9) Solve the variational problem with teacher yt and λt To make the asymptotics in r0 work well, we need to ensure sufficiently strong initial signals y0 and/or near-interpolation (small ϵ). The first bound in (10) is non-vacuous when = Ω(r0). Though it is rather strong requirement, the asymptotic term becomes negligible numerically with moderate magnitude of r0 (like 5 to 10). To see how TAID benefits from the intermediate teacher, compare the non-collapse condition (10) with that of self-distillation (Mobahi et al., 2020, Proposition 4): r0 1 κ . (11) We have two observations. First, TAID is beneficial in the latter phase of recursion (namely, step closer to ), where self-distillation can never escape from collapse eventually. This is an intuitive feature of TAID because the intermediate teacher partly consists strong signals y0 that does not depend on learned student predictors. Second, TAID is worse in the early phase of recursion (namely, step closer to 1) than self-distillation by constant factor. Specifically, TAID and self-distillation have critical steps of collapse = O(r0/(γ + κ)) and = O(r0/κ), respectively. To ensure that TAID learns meaningful features in the early phase, γ should be reasonably bounded away from 0, leading to worse critical point than self-distillation. This is price that TAID has to pay for the stabilization in the latter phase. By setting γ = 1 in (10), we get more interpretable corollary, which is the formal version of Theorem 4.1. Corollary B.1.1. If initialization y0 satisfies y0 = Ω (cid:18) 1 + (cid:112)1 + 4T (1 + κ) 2 (cid:19) ϵ , the prediction vector yt+1 does not collapse for any t. B.3 PROOF Proof of Theorem B.1. Subsequently, we use the change-of-variable zt := Vyt, where the norm is preserved zt = yt. We also write zt := Vyt and rt := zt/ ϵ for convenience. At each time t, the non-collapse criterion is given by zt2 > ϵ( rt > 1): if it holds, the next update in Line 5 would not collapse. Let At := D(λtI + D)1. We first show the second case, namely, the Published as conference paper at ICLR 2025 prediction avoids collapse when 1 r0 < t. Then, zt is recursively expanded. zt = 1 (cid:18) (cid:18) = 1 (cid:18) = 1 (cid:18) = 1 T T (cid:19) (cid:19) (cid:19) zt + z0 At1zt1 + (cid:20)(cid:18) At1 1 z0 1 (cid:19) zt1 + 1 z0 (cid:19) (cid:18) 1 (cid:19) 1 (cid:20)(cid:18) At1zt1 + 1 z0 (cid:21) + (cid:19) 1 At1 + (cid:21) t (12) = . . . (cid:34) (cid:89) = τ =0 (cid:18) 1 τ (cid:40) = ! t+1 (T 1)! =: Atz0. τ =0 (cid:34)t1 (cid:89) τ =0 (cid:19)(cid:35) (cid:34)t1 (cid:89) (cid:35) Aτ z0 + t1 (cid:88) (cid:34)τ 1 (cid:89) (cid:18) τ = s=0 1 (cid:19)(cid:35) τ (cid:34) τ (cid:89) s=1 (cid:35) Ats z0 + z (cid:35) Aτ + t1 (cid:88) τ =1 (t τ ) (T + τ 1)! τ +1 (T 1)! (cid:34) τ (cid:89) s=1 (cid:35) Ats + (cid:41) z0 To evaluate At, we first look at Aτ for τ [0, 1]. Since Aτ is diagonal matrix, its k-th element of Aτ can be expressed as follows: (Aτ )k = dk λτ + dk (cid:18) = ατ /dk zτ / ϵ 1 (cid:19) + 1 (cid:16) (cid:16) 1/κ zτ / ϵ1 κ ϵ zτ / (cid:17)1 (cid:17)1 + 1 + 1 0 , (13) where ατ is given in (9). The last inequalities can be formally shown by induction in τ [0, 1]. Thus, the minimum singular value of At is evaluated as follows: ! t+1 (T 1)! (cid:34)t1 (cid:89) (cid:35) Aτ + t1 (cid:88) τ =1 (t τ ) (T + τ 1)! τ +1 (T 1)! (cid:35) Ats + (cid:34) τ (cid:89) s=1 τ =0 (cid:34)t1 (cid:89) τ =0 Aτ (cid:35)(cid:33) + σmin (cid:32)t1 (cid:88) τ = (t τ ) (T + τ 1)! τ +1 (T 1)! (cid:34) τ (cid:89) s=1 Ats (cid:33) I (cid:35)(cid:33) σmin(At) (cid:32) = σmin (cid:32) = σmin + σmin (cid:18) σmin = , ! t+1 (T 1)! (cid:18) (cid:19) (cid:19) where the second identity holds because all matrices evaluated are diagonal. This implies zt σmin(At)z0 z0 = z0. ϵ/z0)T = ( The last equality uses z0 = z0. Thus, the non-collapse criterion zt > > ( Next, supposing is small enough such that γ r0 avoids collapse when < ( 1 with γ (0, 1), we show that the prediction 2 + o(1))(r0 γ). To see the non-collapse criterion rt > 1, we first ϵ/y0)T . ϵ holds as long as 19 Published as conference paper at ICLR 2025 derive lower bound of rt: rt At1 zt1 ϵ (cid:13) zt1 (cid:13) (cid:13) (cid:13) ϵ + T σmin(At1)rt1 (cid:13) z0 (cid:13) (cid:13) (cid:13) ϵ (cid:13) z0 (cid:13) (cid:13) (cid:13) ϵ r (cid:13) (cid:13) (cid:13) (cid:13) 1 At1 (cid:18) 1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) (12)= (a) (cid:18) 1 (cid:18) 1 (cid:18) (13) 1 (cid:19) (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:19) T γ r0 γ r0 σmin(At1)rt1 γ (cid:19) rt1 κ rt11 + 1 γ (cid:18) (b) 1 (cid:19) γ r0 (β0rt1 β1) γ, where (a) is due to the reverse triangle inequality and (b) is due to Mobahi et al. (2020, Eq. (137)) (which is essentially linear lower bound of convex function in r0) with r2 (r0 1)2 + κ(2r0 1) 0κ (r0 1 + κ)2 . (r0 1 + κ)2 and β1 := β0 := By recursively lower bounding rt, we obtain the following bound: (cid:20)(cid:18) rt 1 (cid:19) (cid:21)t β r0 γ r0 (cid:16) 1 γ r0 (cid:20)(cid:16) (cid:17) β1 1 γ r0 (cid:17)t (cid:21) βt 0 1 (cid:17) (cid:16) 1 γ r0 (cid:16) β0 1 (cid:17) (cid:16) (cid:17) where β0 := rt = 1 to derive the critical t, which is equivalent to β0 and β1 := 1 γ r0 1 γ r0 γ =: βt 0r0 β1 βt 0 1 β0 1 γ =: rt, β1. To derive the non-collapse condition, we solve log = (cid:16) (1+γ)(1 β0)+ β1 β1+r0(1 β0) log β0 (cid:17) . By simple algebra, (cid:18) log = γ[r2 γ2[r0+2(κ1) κ1 r0 0 +(κ2)r0(κ1)]+(κr 0 +κ(κ1)r0) ]+γ(κ1)(κ+2r0 1 r0 log (cid:17) (cid:16) 1 1 γ r0 + log (cid:18) 1 1 κ(κ1) (r01+κ)2 (cid:19) )+κ(κ1+r2 0 ) (cid:19) 1 γ2[r0+2(κ1) κ1 r0 γ[r2 (cid:104) 1 1 γ r0 1 (cid:105) ]+γ(κ1)(κ+2r0 1 r0 0 +κ(κ1)r0) 0 +(κ2)r0(κ1)]+(κr2 (cid:20) (cid:21) )+κ(κ1+r2 0 ) + 1 1 κ(κ1) (r01+κ)2 = = κ(κ1)(r01)+γ(r2 γ[r2 0 +(2κ3)r0(κ1)(κ+3)+ κ1 r0 0 +(κ2)r0(κ1)]+[κr2 0 +κ(κ1)r0] )γ2[r0+2(κ1) κ1 ] 1 r0 γ 1 + 1 κ(κ1) 1 (r01+κ) γr2 0 +[2κ3γ+κ(κ1)]r0(κ1)[κ+γ(κ+3+2γ)]+ γ(κ1)(1+γ) r0 (γ+κ)r2 γr2 0 +[γ(κ2)+(κ1)]κr0γ(κ1) 0 +(2γ+κ)(κ1)r0(κ+1)(κ1)γ (r0γ)[r 0 +2(κ1)r0(κ1)] where the inequality is due to 1 1 (in large r0) expressed as follows: log 1. The last lower bound can be asymptotically γ+o(1) γ+κ+o(1) γ+o(1) (r0γ)(1+o(1)) = 1 γ + κ (r0 γ) + o(1). 20 Published as conference paper at ICLR 2025 Table 6: Performance comparison between TAID and Skew KL across different teacher sizes. TAID shows consistent improvement with larger teachers, while Skew KLs performance degrades. Method 410M 1B 2.8B 6.9B"
        },
        {
            "title": "TAID\nSKL",
            "content": "20.82 18.65 21.17 18.50 21.70 18.28 22.01 18.20 Thus, the non-collapse condition in the second case is < 1 γ+κ (r0 γ) + o(1). Proof of Corollary B.1.1. By the non-collapse criterion (10) with γ = 1, 1 1 + κ (r0 1) + o(1) 1 r0 suffices for yt not being collapsed for any t. By solving this quadratic inequality, we can verify the statement."
        },
        {
            "title": "C DETAILED COMPARISON WITH SKEW KL",
            "content": "We provide detailed comparison between TAID and Skew KL to highlight their fundamental differences, focusing on two key aspects: the direction of knowledge flow and the nature of interpolation design. The first key difference lies in the direction of knowledge flow, which can be understood through their objective functions. The TAID objective is formulated as JTAID(p, qθ) = JKL(pt, qθ), while the Skew KL objective takes the form JSKD(p, qθ) = JKL(p, r), where r(y) = λp(y) + (1 λ)qθ(y) and λ [0, 1]. In TAID, the interpolated distribution pt teaches the student model qθ, creating direct path for knowledge transfer from the interpolated distribution to the student. Conversely, in Skew KL, the teacher teaches the interpolated distribution r, establishing an indirect path where the students knowledge is mixed into the target distribution. The second fundamental difference is in the design of the interpolation mechanism. TAID employs time-dependent parameter that gradually changes during training, enabling adaptive knowledge transfer that evolves with the students learning progress. In contrast, Skew KL uses fixed interpolation parameter λ throughout the training process, maintaining constant mixing ratio between teacher and student distributions. Our empirical study validates the benefits of these design choices, particularly in handling the capacity gap between teacher and student models. Table 6 shows the performance comparison across different teacher sizes, demonstrating that TAID achieves consistent improvement as teacher size increases from 410M to 6.9B parameters, while Skew KLs performance degrades with larger teachers."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 INSTRUCTION TUNING EXPERIMENTS For our instruction tuning experiments, we utilized the UltraChat 200k dataset. We preprocessed the dataset by removing samples exceeding maximum length of 2048 tokens, resulting in approximately 150k training samples and 2k validation samples. All models were trained for 5 epochs using batch size of 64. We employed the AdamW optimizer with learning rate of 1e4 and cosine learning rate scheduler. To select the best checkpoint for evaluation, we calculated the ROUGE-L score on the validation set after each epoch and chose the checkpoint with the highest score. For our proposed TAID method, we used momentum coefficient (β) of 0.99 across all experiments. The learning rate of (α) was set to 5e4. The initial value of (tstart) was set to 0.4 for the Published as conference paper at ICLR 2025 Table 7: Top-1 accuracies (%) on the CIFAR-100 dataset. Results for different teacher-student pairs are shown. Method Teacher ResNet56 ResNet110 ResNet32 Student ResNet20 ResNet324 ResNet84 WRN-40-2 WRN-16WRN-40-2 WRN-40-1 VGG13 VGG8 KL (Hinton et al., 2015) CTKD (Li et al., 2023b) DKD (Zhao et al., 2022) MLKD (Jin et al., 2023) (Ours) TAID 70.66 71.19 71.97 72.19 72.25 73.08 73.52 74.11 74.11 73.51 73.33 73.39 76.32 77.08 74. 74.92 75.45 76.24 76.63 75.81 73.54 73.93 74.81 75.35 74.51 72.93 73.52 74.68 75.18 74.38 Phi-3-mini-4k-instruct pair and 0.2 for the other two pairs. The final value of (tend) was set to 1.0 for all experiments. Regarding baseline methods, we implemented GKD using Generalized Jensen-Shannon Divergence (GJSD) with λ = 0.1 as the objective function and student data fraction of 0.5. For DistiLLM, we used Skew KL divergence with λ = 0.1 and an initial student data fraction of 0.0. We selected the better performing skew divergence between Skew Forward KL and Skew Reverse KL based on the best ROUGE-L score. Following the original DistiLLM paper, we calculated the validation loss twice per epoch, totaling 10 times, to leverage the Adaptive SGO scheduler. For Adaptive KL, our implementation was used since no official implementation was available. For CTKD and DKD, we followed their settings used in the training on ImageNet (Deng et al., 2009). In terms of computational efficiency, we observed significant differences in training times among the different methods. TAID completed its training in approximately 0.7 hours per epoch on our hardware setup using 8 NVIDIA H100 GPUs. In comparison, DistiLLM required about 2 hours per epoch, while GKD took approximately 9.8 hours per epoch under the same conditions. These differences in training time are primarily attributed to the computational complexity of methods utilizing SGOs. TAIDs ability to achieve competitive performance without relying on SGOs contributes to its faster training times. D.2 PRE-TRAINING EXPERIMENTS For our pre-training experiments, we used the first 10% of the SmolLM-Corpus (Ben Allal et al., 2024) dataset, which amounted to approximately 20 billion tokens. The pre-training was conducted for 1 epoch using distributed setup with 80 NVIDIA H100 GPUs, each processing batch size of 8, resulting in an effective batch size of 640. We used the AdamW optimizer with learning rate of 1e4 and cosine learning rate scheduler. The TAID-specific parameters for the pre-training experiments were kept consistent with those used in the Phi-3mini-4k-instruct pair in the instruction tuning experiments. Also, the baseline methods in the pre-training experiments were implemented similarly to the instruction tuning experiments, with adjustments made to exclude SGOs due to the computational constraints of largescale pre-training. Specifically, for methods like DistiLLM, we only used the core divergence components without the SGO-based additions. D.3 IMAGE CLASSIFICATION RESULTS To explore TAIDs applicability beyond language models, we conducted experiments on image classification tasks using the CIFAR-100 and ImageNet datasets. D.4 CIFAR-100 RESULTS We evaluated TAID on the CIFAR-100 dataset, which consists of 100 classes. Table 7 presents the top-1 accuracies achieved by TAID and other knowledge distillation methods on various teacherstudent model pairs. 22 Published as conference paper at ICLR 2025 Table 8: Top-1 accuracies (%) on the ImageNet validation set. Results for different teacherstudent pairs are shown. Method ResNet34 Teacher Student ResNet18 ResNet50 MN-V1 KD (Hinton et al., 2015) CTKD (Li et al., 2023b) DKD (Zhao et al., 2022) MLKD (Jin et al., 2023) (Ours) TAID 71.03 71.38 71.70 71.90 72.10 70.50 71.16 72.05 73.01 72. As shown in Table 7, TAID performs competitively on CIFAR-100, consistently outperforming KL divergence across all model pairs. However, the gains are modest compared to state-of-the-art methods specifically designed for image classification, such as MLKD. Interestingly, based on the analysis of DKD, we can interpret that for simpler tasks like CIFAR-100, where the teachers target class probabilities are close to 1, the weight of the NCKD component in DKD becomes small. This suggests that combining TAID with DKD could potentially lead to further performance improvements, leveraging the strengths of both approaches in handling different aspects of the distillation process. D.5 IMAGENET RESULTS To assess TAIDs performance on larger-scale image classification task, we conducted experiments on the ImageNet dataset, which contains 1000 classes. Table 8 presents the top-1 accuracies achieved by TAID and other methods on ImageNet. On ImageNet, TAID shows more pronounced improvements, consistently outperforming CTKD and DKD across both teacher-student pairs. For the ResNet34-ResNet18 pair, TAID achieves the highest accuracy among all methods. For the ResNet50-MobileNet-V1 pair, TAID performs competitively, outperforming CTKD and DKD, and achieving results close to MLKD. These results on ImageNet demonstrate that TAIDs performance improves relative to other methods as the task complexity increases. With its larger number of classes and more diverse images, ImageNet presents more challenging scenario where TAIDs adaptive interpolation mechanism shows more significant gains. This aligns with our observations in the main text that TAIDs strengths are particularly evident in tasks with higher complexity and entropy."
        },
        {
            "title": "E MODEL DETAILS",
            "content": "E.1 TAID-LLM-1.5B For the development of TAID-LLM-1.5B, we utilized the full SmolLM-Corpus dataset. The training process consisted of 2 epochs, employing the AdamW optimizer with cosine learning rate scheduler. We set the initial learning rate to 1e5. experiment, we used Qwen2-72B-Instruct as and In this Qwen2-1.5B-Instruct as the student model. For the TAID-specific parameters, we used momentum coefficient (β) of 0.99 and learning rate of (α) of 5e5. The initial value of (tstart) was set to 0.4, and the final value (tend) was set to 1.0. teacher model the To enhance training efficiency, we pre-computed the probabilities from the teacher model. Furthermore, to manage storage costs effectively, we only utilized the top 50 probabilities. This approach allowed us to balance computational resources and model performance, enabling efficient knowledge transfer from the large teacher model to the smaller student model. Table 9 presents the detailed results for TAID-LLM-1.5B and other state-of-the-art small language models across various tasks as evaluated using the LightEval benchmark suite (Allal et al., 2024). 23 Published as conference paper at ICLR 2025 Table 9: Performance of TAID-LLM-1.5B, our new state-of-the-art LLM for models under 2B parameters. Model MMLU TriviaQA ARC PIQA Hellaswag OBQA Winogrande Average Qwen2-1.5B (Yang et al., 2024) Qwen2.5-1.5B (Qwen Team, 2024) Phi-1.5B (Li et al., 2023a) StableLM-2-1.6B (Bellagente et al., 2024) SmolLM-1.7B (Allal et al., 2024) TAID-LLM-1.5B 37.91 41.15 35.92 36.21 39.97 39.96 1.38 0.68 6.06 29.59 22.56 22.96 48.12 58.41 60.53 53.57 59.95 58.14 75.30 76.01 75.62 76.77 76.06 77. 63.87 66.40 60.72 66.60 62.91 67.15 36.80 40.00 46.00 37.20 42.80 41.40 59.98 59.35 67.88 58.72 54.91 58.88 46.19 48.86 50.39 51.24 51.31 52.27 Table 10: Performance of TAID-VLM-2B, our new state-of-the-art VLM for models up to 4B parameters. Model MMBench V11 MMStar MMMU VAL MathVista OCRBench AI2D HallusionBench MMVet Average PaliGemma-3B-mix-448 (Beyer et al., 2024) MiniCPM-V-2 (Yao et al., 2024) Phi-3-Vision (Abdin et al., 2024) InternVL2-2B (Chen et al., 2024) TAID-VLM-2B 65.6 65.8 65.2 69.6 70.7 48.3 39.1 47.7 49.8 49.5 34.9 38.2 46.1 36.3 35.1 28.7 39.8 44.6 46.0 51. 61.4 60.5 63.7 78.1 78.6 68.3 62.9 78.4 74.1 74.0 32.2 36.1 39.0 38.0 56.8 33.1 41.0 44.1 39.7 35.1 46.6 47.9 53.6 54.0 56.4 LightEval is designed to comprehensively assess the capabilities of small language models through series of seven zero-shot tasks. Note that the scores in Table 4 denotes the average scores in Table 9. As shown in Table 9, TAID-LLM-1.5B achieves competitive or superior performance across all tasks, with particularly strong results in PIQA and Hellaswag. This demonstrates the effectiveness of our distillation approach in creating compact model that maintains high performance across diverse range of language tasks. E.2 TAID-VLM-2B For TAID-VLM-2B, we trained on the Mantis-Instruct dataset (Jiang et al., 2024). The training process spanned 3 epochs, using the AdamW optimizer with cosine learning rate scheduler. The initial learning rate was set to 1e6. In this vision-language model distillation task, we employed InternVL2-8B (Chen et al., 2024) as the teacher model and InternVL2-2B as the student model. The TAID-specific parameters remained largely consistent with those used for TAID-LLM-1.5B, with momentum coefficient (β) of 0.99 and tstart of 0.4. However, we adjusted the learning rate of to 5e4 to accommodate the characteristics of vision-language model training. The tend value was maintained at 1.0. Table 10 presents the detailed results for TAID-VLM-2B and other state-of-the-art small visionlanguage models across various tasks. Note that the scores in Table 5 denotes the average scores in Table 10. As shown in Table 10, TAID-VLM-2B achieves competitive or superior performance across most tasks, with particularly strong results in MMStar, and HallusionBench. This demonstrates the effectiveness of our distillation approach in creating compact vision-language model that maintains high performance across diverse range of multimodal tasks."
        }
    ],
    "affiliations": [
        "Kyoto University",
        "NINJAL",
        "RIKEN",
        "Sakana AI",
        "Tohoku University"
    ]
}